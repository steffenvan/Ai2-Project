<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.017225">
<title confidence="0.998338">
PORTAGE: with Smoothed Phrase Tables
and Segment Choice Models
</title>
<author confidence="0.957459">
Howard Johnson Fatiha Sadat, George Foster, Roland Kuhn,
National Research Council Michel Simard, Eric Joanis and Samuel Larkin
</author>
<affiliation confidence="0.996917">
Institute for Information Technology National Research Council
Interactive Information Institute for Information Technology
</affiliation>
<address confidence="0.881715333333333">
1200 Montreal Road Interactive Language Technologies
Ottawa, ON, Canada K1A 0R6 101 St-Jean-Bosco Street
Howard.Johnson@cnrc-nrc.gc.ca Gatineau, QC, Canada K1A 0R6
</address>
<email confidence="0.819155">
firstname.lastname@cnrc-nrc.gc.ca
</email>
<sectionHeader confidence="0.997311" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999895375">
Improvements to Portage and its partici-
pation in the shared task of NAACL 2006
Workshop on Statistical Machine Trans-
lation are described. Promising ideas in
phrase table smoothing and global dis-
tortion using feature-rich models are dis-
cussed as well as numerous improvements
in the software base.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999908071428572">
The statistical machine translation system Portage is
participating in the NAACL 2006 Workshop on Sta-
tistical Machine Translation. This is a good opportu-
nity to do benchmarking against a publicly available
data set and explore the benefits of a number of re-
cently added features.
Section 2 describes the changes that have been
made to Portage in the past year that affect the par-
ticipation in the 2006 shared task. Section 3 outlines
the methods employed for this task and extensions
of it. In Section 4 the results are summarized in tab-
ular form. Following these, there is a conclusions
section that highlights what can be gleaned of value
from these results.
</bodyText>
<sectionHeader confidence="0.995545" genericHeader="introduction">
2 Portage
</sectionHeader>
<bodyText confidence="0.999813272727273">
Because this is the second participation of Portage in
such a shared task, a description of the base system
can be found elsewhere (Sadat et al, 2005). Briefly,
Portage is a research vehicle and development pro-
totype system exploiting the state-of-the-art in sta-
tistical machine translation (SMT). It uses a custom
built decoder followed by a rescoring module that
adjusts weights based on a number of features de-
fined on the source sentence. We will devote space
to discussing changes made since the 2005 shared
task.
</bodyText>
<subsectionHeader confidence="0.993843">
2.1 Phrase-Table Smoothing
</subsectionHeader>
<bodyText confidence="0.99990956">
Phrase-based SMT relies on conditional distribu-
tions p(s1t) and p(t1s) that are derived from the joint
frequencies c(s, t) of source/target phrase pairs ob-
served in an aligned parallel corpus. Traditionally,
relative-frequency estimation is used to derive con-
ditional distributions, ie p(slt) = c(s, t)/ K, c(s, t).
However, relative-frequency estimation has the
well-known problem of favouring rare events. For
instance, any phrase pair whose constituents occur
only once in the corpus will be assigned a probabil-
ity of 1, almost certainly higher than the probabili-
ties of pairs for which much more evidence exists.
During translation, rare pairs can directly compete
with overlapping frequent pairs, so overestimating
their probabilities can significantly degrade perfor-
mance.
To address this problem, we implemented two
simple smoothing strategies. The first is based on
the Good-Turing technique as described in (Church
and Gale, 1991). This replaces each observed joint
frequency c with cg = (c + 1)n,+1/n,, where n,
is the number of distinct pairs with frequency c
(smoothed for large c). It also assigns a total count
mass of n1 to unseen pairs, which we distributed
in proportion to the frequency of each conditioning
</bodyText>
<page confidence="0.973426">
134
</page>
<subsectionHeader confidence="0.398518">
Proceedings of the Workshop on Statistical Machine Translation, pages 134–137,
New York City, June 2006. c�2006 Association for Computational Linguistics
</subsectionHeader>
<bodyText confidence="0.329152">
E,
</bodyText>
<equation confidence="0.968421">
s cg(s, t) + p(t)n1
</equation>
<bodyText confidence="0.9328048">
where p(t) = c(t)/ Et c(t). The estimates for
pg(t|s) are analogous.
The second strategy is Kneser-Ney smoothing
(Kneser and Ney, 1995), using the interpolated vari-
ant described in (Chen and Goodman., 1998):1
</bodyText>
<equation confidence="0.99687875">
c(s, t) − D + D n1+(*, t) pk(s)
pk(s|t) =
E
s c(s, t)
</equation>
<bodyText confidence="0.995805083333333">
where D = n1/(n1 + 2n2), n1+(*, t) is the num-
ber of distinct phrases s with which t co-occurs, and
pk(s) = n1+(s, *)/ Es n1+(s, *), with n1+(s, *)
analogous to n1+(*, t).
Our approach to phrase-table smoothing contrasts
to previous work (Zens and Ney, 2004) in which
smoothed phrase probabilities are constructed from
word-pair probabilities and combined in a log-linear
model with an unsmoothed phrase-table. We believe
the two approaches are complementary, so a combi-
nation of both would be worth exploring in future
work.
</bodyText>
<subsectionHeader confidence="0.99808">
2.2 Feature-Rich DT-based distortion
</subsectionHeader>
<bodyText confidence="0.999886555555556">
In a recent paper (Kuhn et al, 2006), we presented a
new class of probabilistic ”Segment Choice Models”
(SCMs) for distortion in phrase-based systems. In
some situations, SCMs will assign a better distortion
score to a drastic reordering of the source sentence
than to no reordering; in this, SCMs differ from the
conventional penalty-based distortion, which always
favours less rather than more distortion.
We developed a particular kind of SCM based on
decision trees (DTs) containing both questions of a
positional type (e.g., questions about the distance
of a given phrase from the beginning of the source
sentence or from the previously translated phrase)
and word-based questions (e.g., questions about the
presence or absence of given words in a specified
phrase).
The DTs are grown on a corpus consisting of
segment-aligned bilingual sentence pairs. This
</bodyText>
<footnote confidence="0.554345">
1As for Good-Turing smoothing, this formula applies only
to pairs s, t for which c(s, t) &gt; 0, since these are the only ones
considered by the decoder.
</footnote>
<bodyText confidence="0.999900684210526">
segment-aligned corpus is obtained by training a
phrase translation model on a large bilingual cor-
pus and then using it (in conjunction with a distor-
tion penalty) to carry out alignments between the
phrases in the source-language sentence and those
in the corresponding target-language sentence in a
second bilingual corpus. Typically, the first corpus
(on which the phrase translation model is trained) is
the same as the second corpus (on which alignment
is carried out). To avoid overfitting, the alignment
algorithm is leave-one-out: statistics derived from
a particular sentence pair are not used to align that
sentence pair.
Note that the experiments reported in (Kuhn et
al, 2006) focused on translation of Chinese into En-
glish. The interest of the experiments reported here
on WMT data was to see if the feature-rich DT-based
distortion model could be useful for MT between
other language pairs.
</bodyText>
<sectionHeader confidence="0.941191" genericHeader="method">
3 Application to the Shared Task: Methods
</sectionHeader>
<subsectionHeader confidence="0.988311">
3.1 Restricted Resource Exercise
</subsectionHeader>
<bodyText confidence="0.982165230769231">
The first exercise that was done is to replicate the
conditions of 2005 as closely as possible to see the
effects of one year of research and development.
The second exercise was to replicate all three of
these translation exercises using the 2006 language
model, and to do the three exercises of translat-
ing out of English into French, Spanish, and Ger-
man. This was our baseline for other studies. A
third exercise involved modifying the generation
of the phrase-table to incorporate our Good-Turing
smoothing. All six language pairs were re-processed
with these phrase-tables. The improvement in the
results on the devtest set were compelling. This be-
came the baseline for further work. A fourth ex-
ercise involved replacing penalty-based distortion
modelling with the feature-rich decision-tree based
distortion modelling described above. A fifth ex-
ercise involved the use of a Kneser-Ney phrase-
table smoothing algorithm as an alternative to Good-
Turing.
For all of these exercises, 1-best results after de-
coding were calculated as well as rescoring on 1000-
best lists of results using 12 feature functions (13
in the case of decision-tree based distortion mod-
elling). The results submitted for the shared task
phrase. The resulting estimates are:
</bodyText>
<equation confidence="0.9138995">
pg(s|t) =
cg (s, t)
</equation>
<page confidence="0.986815">
135
</page>
<bodyText confidence="0.999736">
were the results of the third and fourth exercises
where rescoring had been applied.
and tokenization procedures. English preprocessing
simply included lower-casing, separating punctua-
tion from words and splitting off ’s.
</bodyText>
<subsectionHeader confidence="0.998843">
3.2 Open Resource Exercise
</subsectionHeader>
<bodyText confidence="0.999450214285714">
Our goal in this exercise was to conduct a com-
parative study using additional training data for the
French-English shared task. Results of WPT 2005
showed an improvement of at least 0.3 BLEU point
when exploiting different resources for the French-
English pair of languages. In addition to the training
resources used in WPT 2005 for the French-English
task, i.e. Europarl and Hansard, we used a bilingual
dictionary, Le Grand Dictionnaire Terminologique
(GDT) 2 to train translation models and the English
side of the UN parallel corpus (LDC2004E13) to
train an English language model. Integrating termi-
nological lexicons into a statistical machine transla-
tion engine is not a straightforward operation, since
we cannot expect them to come with attached prob-
abilities. The approach we took consists on view-
ing all translation candidates of each source term or
phrase as equiprobable (Sadat et al, 2006).
In total, the data used in this second part of our
contribution to WMT 2006 is described as follows:
(1) A set of 688,031 sentences in French and En-
glish extracted from the Europarl parallel corpus (2)
A set of 6,056,014 sentences in French and English
extracted from the Hansard parallel corpus, the offi-
cial record of Canada’s parliamentary debates. (3) A
set of 701,709 sentences in French and English ex-
tracted from the bilingual dictionary GDT. (4) Lan-
guage models were trained on the French and En-
glish parts of the Europarl and Hansard. We used
the provided Europarl corpus while omitting data
from Q4/2000 (October-December), since it is re-
served for development and test data. (5) An addi-
tional English language model was trained on 128
million words of the UN Parallel corpus.
For the supplied Europarl corpora, we relied on
the existing segmentation and tokenization, except
for French, which we manipulated slightly to bring
into line with our existing conventions (e.g., convert-
ing l ’ an into l’ an, aujourd ’ hui into aujourd’hui).
For the Hansard corpus used to supplement our
French-English resources, we used our own align-
ment based on Moore’s algorithm, segmentation,
</bodyText>
<footnote confidence="0.901539">
2http://www.granddictionnaire.com/
</footnote>
<sectionHeader confidence="0.999274" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999966111111111">
The results are shown in Table 1. The numbers
shown are BLEU scores. The MC rows correspond
to the multi-corpora results described in the open re-
source exercise section above. All other rows are
from the restricted resource exercise.
The devtest results are the scores computed be-
fore the shared-task submission and were used to
drive the choice of direction of the research. The
test results were computed after the shared-task sub-
mission and serve for validation of the conclusions.
We believe that our use of multiple training cor-
pora as well as our re-tokenization for French and
an enhanced language model resulted in our overall
success in the English-French translation track. The
results for the in-domain test data puts our group at
the top of the ranking table drawn by the organizers
(first on Adequacy and fluency and third on BLEU
scores).
</bodyText>
<sectionHeader confidence="0.995737" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999981388888889">
Benchmarking with same language model and pa-
rameters as WPT05 reproduces the results with a
tiny improvement. The larger language model used
in 2006 for English yields about half a BLEU. Good-
Turing phrase table smoothing yields roughly half
a BLEU point. Kneser-Ney phrase table smooth-
ing yields between a third and half a BLEU point
more than Good-Turing. Decision tree based distor-
tion yields a small improvement for the devtest set
when rescoring was not used but failed to show im-
provement on the test set.
In summary, the results from phrase-table
smoothing are extremely encouraging. On the other
hand, the feature-rich decision tree distortion mod-
elling requires additional work before it provides a
good pay-back. Fortunately we have some encour-
aging avenues under investigation. Clearly there is
more work needed for both of these areas.
</bodyText>
<sectionHeader confidence="0.997751" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9995265">
We wish to thank Aaron Tikuisis and Denis Yuen
for important contributions to the Portage code base
</bodyText>
<page confidence="0.999025">
136
</page>
<tableCaption confidence="0.999596">
Table 1: Restricted and open resource results
</tableCaption>
<table confidence="0.999146515151515">
fr −+ en es −+ en de −+ en en −+ fr en −+ es en −+ de
devtest: with rescoring
WPT05 29.32 29.08 23.21
LM-2005 29.30 29.21 23.41
LM-2006 29.88 29.54 23.94 30.43 28.81 17.33
GT-PTS 30.35 29.84 24.60 30.89 29.54 17.62
GT-PTS+DT-dist 30.09 29.44 24.62 31.06 29.46 17.84
KN-PTS 30.55 30.12 24.66 31.28 29.90 17.78
MC WPT05 29.63
MC 30.09 31.30
MC+GT-PTS 30.75 31.37
devtest: 1-best after decoding
LM-2006 28.59 28.45 23.22 29.22 28.30 16.94
GT-PTS 29.23 28.91 23.67 30.07 28.86 17.32
GT-PTS+DT-dist 29.48 29.07 23.50 30.22 29.46 17.42
KN-PTS 29.77 29.76 23.27 30.73 29.62 17.78
MC WPT05 28.71
MC 29.63 31.01
MC+GT-PTS 29.90 31.22
test: with rescoring
LM-2006 26.64 28.43 21.33 28.06 28.01 15.19
GT-PTS 27.19 28.95 21.91 28.60 28.83 15.38
GT-PTS+DT-dist 26.84 28.56 21.84 28.56 28.59 15.45
KN-PTS 27.40 29.07 21.98 28.96 29.06 15.64
MC 26.95 29.12
MC+GT-PTS 27.10 29.46
test: 1-best after decoding
LM-2006 25.35 27.25 20.46 27.20 27.18 14.60
GT-PTS 25.95 28.07 21.06 27.85 27.96 15.05
GT-PTS+DT-dist 25.86 28.04 20.74 27.85 27.97 14.92
KN-PTS 26.83 28.66 21.36 28.62 28.71 15.42
MC 26.70 28.74
MC+GT-PTS 26.81 29.03
</table>
<bodyText confidence="0.993711">
and the OQLF (Office Qu´eb´ecois de la Langue
Franc¸aise) for permission to use the GDT.
</bodyText>
<sectionHeader confidence="0.999237" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9958308125">
S. F. Chen and J. T. Goodman. 1998. An empirical
study of smoothing techniques for language modeling.
Technical Report TR-10-98, Computer Science Group,
Harvard University.
K. Church and W. Gale. 1991. A comparison of the en-
hanced Good-Turing and deleted estimation methods
for estimating probabilities of English bigrams. Com-
puter speech and language, 5(1):19–54.
R. Kneser and H. Ney. 1995. Improved backing-off for
m-gram language modeling. In Proc. International
Conference on Acoustics, Speech, and Signal Process-
ing (ICASSP) 1995, pages 181–184, Detroit, Michi-
gan. IEEE.
R. Kuhn, D. Yuen, M. Simard, G. Foster, P. Paul, E. Joa-
nis and J. H. Johnson. 2006. Segment Choice Models:
Feature-Rich Models for Global Distortion in Statisti-
cal Machine Translation (accepted for publication in
HLT-NAACL conference, to be held June 2006).
F. Sadat, J. H. Johnson, A. Agbago, G. Foster, R. Kuhn,
J. Martin and A. Tikuisis. 2005. PORTAGE: A
Phrase-based Machine Translation System In Proc.
ACL 2005 Workshop on building and using parallel
texts. Ann Arbor, Michigan.
F. Sadat, G. Foster and R. Kuhn. 2006. Syst`eme de tra-
duction automatique statistique combinant diff´erentes
ressources. In Proc. TALN 2006 (Traitement Automa-
tique des Langues Naturelles). Leuven, Belgium, April
10-13, 2006.
R. Zens and H. Ney. 2004. Improvements in phrase-
based statistical machine translation. In Proc. Human
Language Technology Conference / North American
Chapter of the ACL, Boston, May.
</reference>
<page confidence="0.99781">
137
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.666540">
<title confidence="0.9964375">PORTAGE: with Smoothed Phrase Tables and Segment Choice Models</title>
<author confidence="0.999959">Howard Johnson Fatiha Sadat</author>
<author confidence="0.999959">George Foster</author>
<author confidence="0.999959">Roland Kuhn</author>
<affiliation confidence="0.902159333333333">Research Council Simard, Eric Joanis and Samuel Larkin Institute for Information Technology National Research Council Interactive Information Institute for Information Technology</affiliation>
<address confidence="0.991657">1200 Montreal Road Interactive Language Technologies Ottawa, ON, Canada K1A 0R6 101 St-Jean-Bosco Street QC, Canada K1A 0R6</address>
<email confidence="0.99788">firstname.lastname@cnrc-nrc.gc.ca</email>
<abstract confidence="0.995987111111111">Improvements to Portage and its participation in the shared task of NAACL 2006 Workshop on Statistical Machine Translation are described. Promising ideas in phrase table smoothing and global distortion using feature-rich models are discussed as well as numerous improvements in the software base.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S F Chen</author>
<author>J T Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1998</date>
<tech>Technical Report TR-10-98,</tech>
<institution>Computer Science Group, Harvard University.</institution>
<marker>Chen, Goodman, 1998</marker>
<rawString>S. F. Chen and J. T. Goodman. 1998. An empirical study of smoothing techniques for language modeling. Technical Report TR-10-98, Computer Science Group, Harvard University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Church</author>
<author>W Gale</author>
</authors>
<title>A comparison of the enhanced Good-Turing and deleted estimation methods for estimating probabilities of English bigrams. Computer speech and language,</title>
<date>1991</date>
<pages>5--1</pages>
<contexts>
<context position="2990" citStr="Church and Gale, 1991" startWordPosition="450" endWordPosition="453">owever, relative-frequency estimation has the well-known problem of favouring rare events. For instance, any phrase pair whose constituents occur only once in the corpus will be assigned a probability of 1, almost certainly higher than the probabilities of pairs for which much more evidence exists. During translation, rare pairs can directly compete with overlapping frequent pairs, so overestimating their probabilities can significantly degrade performance. To address this problem, we implemented two simple smoothing strategies. The first is based on the Good-Turing technique as described in (Church and Gale, 1991). This replaces each observed joint frequency c with cg = (c + 1)n,+1/n,, where n, is the number of distinct pairs with frequency c (smoothed for large c). It also assigns a total count mass of n1 to unseen pairs, which we distributed in proportion to the frequency of each conditioning 134 Proceedings of the Workshop on Statistical Machine Translation, pages 134–137, New York City, June 2006. c�2006 Association for Computational Linguistics E, s cg(s, t) + p(t)n1 where p(t) = c(t)/ Et c(t). The estimates for pg(t|s) are analogous. The second strategy is Kneser-Ney smoothing (Kneser and Ney, 19</context>
</contexts>
<marker>Church, Gale, 1991</marker>
<rawString>K. Church and W. Gale. 1991. A comparison of the enhanced Good-Turing and deleted estimation methods for estimating probabilities of English bigrams. Computer speech and language, 5(1):19–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kneser</author>
<author>H Ney</author>
</authors>
<title>Improved backing-off for m-gram language modeling.</title>
<date>1995</date>
<booktitle>In Proc. International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</booktitle>
<pages>181--184</pages>
<publisher>IEEE.</publisher>
<location>Detroit, Michigan.</location>
<contexts>
<context position="3593" citStr="Kneser and Ney, 1995" startWordPosition="550" endWordPosition="553">ch and Gale, 1991). This replaces each observed joint frequency c with cg = (c + 1)n,+1/n,, where n, is the number of distinct pairs with frequency c (smoothed for large c). It also assigns a total count mass of n1 to unseen pairs, which we distributed in proportion to the frequency of each conditioning 134 Proceedings of the Workshop on Statistical Machine Translation, pages 134–137, New York City, June 2006. c�2006 Association for Computational Linguistics E, s cg(s, t) + p(t)n1 where p(t) = c(t)/ Et c(t). The estimates for pg(t|s) are analogous. The second strategy is Kneser-Ney smoothing (Kneser and Ney, 1995), using the interpolated variant described in (Chen and Goodman., 1998):1 c(s, t) − D + D n1+(*, t) pk(s) pk(s|t) = E s c(s, t) where D = n1/(n1 + 2n2), n1+(*, t) is the number of distinct phrases s with which t co-occurs, and pk(s) = n1+(s, *)/ Es n1+(s, *), with n1+(s, *) analogous to n1+(*, t). Our approach to phrase-table smoothing contrasts to previous work (Zens and Ney, 2004) in which smoothed phrase probabilities are constructed from word-pair probabilities and combined in a log-linear model with an unsmoothed phrase-table. We believe the two approaches are complementary, so a combinat</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>R. Kneser and H. Ney. 1995. Improved backing-off for m-gram language modeling. In Proc. International Conference on Acoustics, Speech, and Signal Processing (ICASSP) 1995, pages 181–184, Detroit, Michigan. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kuhn</author>
<author>D Yuen</author>
<author>M Simard</author>
<author>G Foster</author>
<author>P Paul</author>
<author>E Joanis</author>
<author>J H Johnson</author>
</authors>
<title>Segment Choice Models: Feature-Rich Models for Global Distortion in Statistical Machine Translation (accepted for publication in HLT-NAACL conference, to be held</title>
<date>2006</date>
<contexts>
<context position="4319" citStr="Kuhn et al, 2006" startWordPosition="676" endWordPosition="679">s|t) = E s c(s, t) where D = n1/(n1 + 2n2), n1+(*, t) is the number of distinct phrases s with which t co-occurs, and pk(s) = n1+(s, *)/ Es n1+(s, *), with n1+(s, *) analogous to n1+(*, t). Our approach to phrase-table smoothing contrasts to previous work (Zens and Ney, 2004) in which smoothed phrase probabilities are constructed from word-pair probabilities and combined in a log-linear model with an unsmoothed phrase-table. We believe the two approaches are complementary, so a combination of both would be worth exploring in future work. 2.2 Feature-Rich DT-based distortion In a recent paper (Kuhn et al, 2006), we presented a new class of probabilistic ”Segment Choice Models” (SCMs) for distortion in phrase-based systems. In some situations, SCMs will assign a better distortion score to a drastic reordering of the source sentence than to no reordering; in this, SCMs differ from the conventional penalty-based distortion, which always favours less rather than more distortion. We developed a particular kind of SCM based on decision trees (DTs) containing both questions of a positional type (e.g., questions about the distance of a given phrase from the beginning of the source sentence or from the previ</context>
<context position="5984" citStr="Kuhn et al, 2006" startWordPosition="939" endWordPosition="942">del on a large bilingual corpus and then using it (in conjunction with a distortion penalty) to carry out alignments between the phrases in the source-language sentence and those in the corresponding target-language sentence in a second bilingual corpus. Typically, the first corpus (on which the phrase translation model is trained) is the same as the second corpus (on which alignment is carried out). To avoid overfitting, the alignment algorithm is leave-one-out: statistics derived from a particular sentence pair are not used to align that sentence pair. Note that the experiments reported in (Kuhn et al, 2006) focused on translation of Chinese into English. The interest of the experiments reported here on WMT data was to see if the feature-rich DT-based distortion model could be useful for MT between other language pairs. 3 Application to the Shared Task: Methods 3.1 Restricted Resource Exercise The first exercise that was done is to replicate the conditions of 2005 as closely as possible to see the effects of one year of research and development. The second exercise was to replicate all three of these translation exercises using the 2006 language model, and to do the three exercises of translating</context>
</contexts>
<marker>Kuhn, Yuen, Simard, Foster, Paul, Joanis, Johnson, 2006</marker>
<rawString>R. Kuhn, D. Yuen, M. Simard, G. Foster, P. Paul, E. Joanis and J. H. Johnson. 2006. Segment Choice Models: Feature-Rich Models for Global Distortion in Statistical Machine Translation (accepted for publication in HLT-NAACL conference, to be held June 2006).</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sadat</author>
<author>J H Johnson</author>
<author>A Agbago</author>
<author>G Foster</author>
<author>R Kuhn</author>
<author>J Martin</author>
<author>A Tikuisis</author>
</authors>
<title>PORTAGE: A Phrase-based Machine Translation System In</title>
<date>2005</date>
<booktitle>Proc. ACL</booktitle>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="1655" citStr="Sadat et al, 2005" startWordPosition="248" endWordPosition="251">le data set and explore the benefits of a number of recently added features. Section 2 describes the changes that have been made to Portage in the past year that affect the participation in the 2006 shared task. Section 3 outlines the methods employed for this task and extensions of it. In Section 4 the results are summarized in tabular form. Following these, there is a conclusions section that highlights what can be gleaned of value from these results. 2 Portage Because this is the second participation of Portage in such a shared task, a description of the base system can be found elsewhere (Sadat et al, 2005). Briefly, Portage is a research vehicle and development prototype system exploiting the state-of-the-art in statistical machine translation (SMT). It uses a custom built decoder followed by a rescoring module that adjusts weights based on a number of features defined on the source sentence. We will devote space to discussing changes made since the 2005 shared task. 2.1 Phrase-Table Smoothing Phrase-based SMT relies on conditional distributions p(s1t) and p(t1s) that are derived from the joint frequencies c(s, t) of source/target phrase pairs observed in an aligned parallel corpus. Traditional</context>
</contexts>
<marker>Sadat, Johnson, Agbago, Foster, Kuhn, Martin, Tikuisis, 2005</marker>
<rawString>F. Sadat, J. H. Johnson, A. Agbago, G. Foster, R. Kuhn, J. Martin and A. Tikuisis. 2005. PORTAGE: A Phrase-based Machine Translation System In Proc. ACL 2005 Workshop on building and using parallel texts. Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sadat</author>
<author>G Foster</author>
<author>R Kuhn</author>
</authors>
<title>Syst`eme de traduction automatique statistique combinant diff´erentes ressources.</title>
<date>2006</date>
<booktitle>In Proc. TALN 2006 (Traitement Automatique des Langues Naturelles).</booktitle>
<location>Leuven, Belgium,</location>
<contexts>
<context position="8701" citStr="Sadat et al, 2006" startWordPosition="1370" endWordPosition="1373">n to the training resources used in WPT 2005 for the French-English task, i.e. Europarl and Hansard, we used a bilingual dictionary, Le Grand Dictionnaire Terminologique (GDT) 2 to train translation models and the English side of the UN parallel corpus (LDC2004E13) to train an English language model. Integrating terminological lexicons into a statistical machine translation engine is not a straightforward operation, since we cannot expect them to come with attached probabilities. The approach we took consists on viewing all translation candidates of each source term or phrase as equiprobable (Sadat et al, 2006). In total, the data used in this second part of our contribution to WMT 2006 is described as follows: (1) A set of 688,031 sentences in French and English extracted from the Europarl parallel corpus (2) A set of 6,056,014 sentences in French and English extracted from the Hansard parallel corpus, the official record of Canada’s parliamentary debates. (3) A set of 701,709 sentences in French and English extracted from the bilingual dictionary GDT. (4) Language models were trained on the French and English parts of the Europarl and Hansard. We used the provided Europarl corpus while omitting da</context>
</contexts>
<marker>Sadat, Foster, Kuhn, 2006</marker>
<rawString>F. Sadat, G. Foster and R. Kuhn. 2006. Syst`eme de traduction automatique statistique combinant diff´erentes ressources. In Proc. TALN 2006 (Traitement Automatique des Langues Naturelles). Leuven, Belgium, April 10-13, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Zens</author>
<author>H Ney</author>
</authors>
<title>Improvements in phrasebased statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proc. Human Language Technology Conference / North American Chapter of the ACL,</booktitle>
<location>Boston,</location>
<contexts>
<context position="3978" citStr="Zens and Ney, 2004" startWordPosition="624" endWordPosition="627"> New York City, June 2006. c�2006 Association for Computational Linguistics E, s cg(s, t) + p(t)n1 where p(t) = c(t)/ Et c(t). The estimates for pg(t|s) are analogous. The second strategy is Kneser-Ney smoothing (Kneser and Ney, 1995), using the interpolated variant described in (Chen and Goodman., 1998):1 c(s, t) − D + D n1+(*, t) pk(s) pk(s|t) = E s c(s, t) where D = n1/(n1 + 2n2), n1+(*, t) is the number of distinct phrases s with which t co-occurs, and pk(s) = n1+(s, *)/ Es n1+(s, *), with n1+(s, *) analogous to n1+(*, t). Our approach to phrase-table smoothing contrasts to previous work (Zens and Ney, 2004) in which smoothed phrase probabilities are constructed from word-pair probabilities and combined in a log-linear model with an unsmoothed phrase-table. We believe the two approaches are complementary, so a combination of both would be worth exploring in future work. 2.2 Feature-Rich DT-based distortion In a recent paper (Kuhn et al, 2006), we presented a new class of probabilistic ”Segment Choice Models” (SCMs) for distortion in phrase-based systems. In some situations, SCMs will assign a better distortion score to a drastic reordering of the source sentence than to no reordering; in this, SC</context>
</contexts>
<marker>Zens, Ney, 2004</marker>
<rawString>R. Zens and H. Ney. 2004. Improvements in phrasebased statistical machine translation. In Proc. Human Language Technology Conference / North American Chapter of the ACL, Boston, May.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>