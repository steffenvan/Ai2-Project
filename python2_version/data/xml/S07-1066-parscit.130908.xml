<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.028743">
<title confidence="0.982281">
SW-AG: Local Context Matching for English Lexical Substitution
</title>
<author confidence="0.999338">
George Dahl, Anne-Marie Frassica, Richard Wicentowski
</author>
<affiliation confidence="0.95362">
Department of Computer Science
Swarthmore College
</affiliation>
<address confidence="0.56226">
Swarthmore, PA 19081 USA
</address>
<email confidence="0.997952">
{george.dahl, afrassi1}@gmail.com, richardw@cs.swarthmore.edu
</email>
<sectionHeader confidence="0.995624" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999675">
We present two systems that pick the ten
most appropriate substitutes for a marked
word in a test sentence. The first system
scores candidates based on how frequently
their local contexts match that of the marked
word. The second system, an enhancement
to the first, incorporates cosine similarity us-
ing unigram features. The core of both sys-
tems bypasses intermediate sense selection.
Our results show that a knowledge-light, di-
rect method for scoring potential replace-
ments is viable.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999940827586207">
An obvious way to view the problem of lexical sub-
stitution is as a sense disambiguation task. For
example, one possible approach is to identify the
sense of the target word and then to pick a synonym
based on the identified sense. Following Dagan et
al. (2006), we refer to this as an indirect approach.
A system using an indirect approach must have ac-
cess to a list of senses for each target word, and each
sense must have a corresponding list of synonyms.
Though one can use a predefined sense inventory,
such as WordNet, the granularity of the sense inven-
tory may not be appropriate for the task. If the sense
inventory is too fine-grained, then picking the cor-
rect sense may be needlessly difficult. Conversely,
if it is too coarse, picking the correct sense may not
narrow down the list of potential substitutions suffi-
ciently.
To avoid these problems, we propose a direct ap-
proach, which will break the problem into two steps:
for each target word, generate a list of candidate syn-
onyms; then rank each synonym for its quality as
a replacement. Although our second system makes
use of some sense information, it is used only to re-
rank candidates generated using a direct approach.
We describe two systems: the first is a purely di-
rect method based on local context matching, and
the second is a hybrid of local context matching
and wider context bag-of-words matching. Both are
knowledge-light and unsupervised.
</bodyText>
<sectionHeader confidence="0.993028" genericHeader="introduction">
2 Methods
</sectionHeader>
<bodyText confidence="0.999989">
As mentioned above, we divide the task into two
steps: compiling a list of synonyms and then, for
each test instance, ranking the list of appropriate
synonyms. Both of our systems create lists of can-
didate synonyms in the same way and only differ in
the way they arrive at a ranking for these candidates.
</bodyText>
<subsectionHeader confidence="0.999641">
2.1 Compiling a substitution lexicon
</subsectionHeader>
<bodyText confidence="0.999942357142857">
We begin by compiling a list of candidate synonyms
for each target word. Following Dagan et al. (2006),
we will refer to this list of synonyms as our substitu-
tion lexicon. The performance of our system is lim-
ited by the substitution lexicon because it can only
pick the correct replacements if they are in the lexi-
con. The substitution lexicon available to our scor-
ing system therefore determines both the maximum
attainable recall and the baseline probability of ran-
domly guessing a correct replacement.
One approach to generating a substitution lexicon
is to query WordNet for lists of synonyms grouped
by the senses of each word. While WordNet has its
advantages, we aimed to create a knowledge-light
</bodyText>
<page confidence="0.989721">
304
</page>
<bodyText confidence="0.993431326086957">
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 304–307,
Prague, June 2007. c�2007 Association for Computational Linguistics
system. A more knowledge-free system would have
used a machine readable dictionary or a large nat-
ural language sample to retrieve its synonyms (see,
for example, Lin (1998)), but our system falls short
of this, relying on Roget’s New Millennium The-
saurus1 (henceforth RT) as a source of synonyms.
Though this thesaurus is similar to WordNet in some
ways, it does not contain semantic relationships be-
yond synonyms and antonyms. One important ad-
vantage of a thesaurus over WordNet is that it is eas-
ier to obtain for languages other than English.
We used the trial data to ensure that the quality
of the list compiled from RT would be satisfactory
for this task. We found that by using the synonyms
in WordNet synsets2 as our substitution lexicon, we
could achieve a maximum recall of 53% when using
an oracle to select the correct synonyms. However,
using the synonyms from RT as the substitution lex-
icon led to a maximum recall of 85%.
Querying RT for the synonyms of a word returns
multiple entries. For our purposes, each entry con-
sists of a Main Entry, a part of speech, a definition,
and a list of synonyms. Many of the returned entries
do not list the query word as the Main Entry. For
instance, given the query “tell”, RT returns 115 en-
tries: 7 whose Main Entry is “tell”, an additional 3
that contain “tell” (e.g. “show and tell”), and the re-
maining 105 entries are other words (e.g. “gossip”)
that list “tell” as a synonym. Where the Main Entry
matches the query, RT entries roughly correspond to
the traditional notion of “sense”.
In order to reduce the number of potentially spu-
rious synonyms that could be picked, we created
a simple automatic filtering system. For each RT
query, we kept only those entries whose Main Entry
and part of speech matched the target word exactly3.
In addition, we removed obscure words which we
believed human annotators would be unlikely to
pick. We used the unigram counts from the Web
1T 5-gram corpus (Brants and Franz, 2006) to de-
termine the frequency of use of each candidate syn-
onym. We experimented with discarding the least
frequent third of the candidates. Although this fil-
tering reduced our maximum attainable recall from
</bodyText>
<footnote confidence="0.99450625">
1http://thesaurus.reference.com
2excluding the extended relations such as hyponyms, etc.
3Since RT was not always consistent in labeling adjectives
and adverbs, we conflated these in filtering.
</footnote>
<bodyText confidence="0.942735">
85% to 75% on the trial data, it significantly raised
our precision.
</bodyText>
<subsectionHeader confidence="0.999748">
2.2 Ranking substitutions
</subsectionHeader>
<bodyText confidence="0.9999424">
We created two systems (and submitted two sets of
results) for this task. The first system is fully de-
scribed in Section 2.2.1. The second system includes
the first system and is fully described in the remain-
der of Section 2.2.
</bodyText>
<subsectionHeader confidence="0.622185">
2.2.1 Local context matching (LCM)
</subsectionHeader>
<bodyText confidence="0.999970228571428">
Our first system matches the context of target
words to the context of candidate synonyms in a
large, unannotated corpus. If the context of a candi-
date synonym exactly matches the context of a target
word, it is considered a good replacement synonym.
Context matches are made against the Web 1T cor-
pus’ list of trigrams. Though this corpus provides
us with a very large amount of data4, to increase
the likelihood of finding an appropriate match, we
mapped inflected words to their roots in both the cor-
pus and the test data (Baayen et al., 1996).
The context of a target word consists of a set of up
to 3 trigrams, specifically those trigrams in the test
sentence that contain the target word. For example,
the context of “bright” in the sentence5 “... who was
a bright boy only ...” is the set was a bright”, “a
bright boy”, “bright boy only”}.
Once we identified the set of context trigrams,
we filtered this set by removing all trigrams which
did not include content words. To identify content
words, we used the NLTK-Lite tagger to assign a
part of speech to each word (Loper and Bird, 2002).
We considered open class words (with the exception
of the verb to be) and pronouns to be content words.
We call the filtered set of trigrams the test trigrams.
From the above example, we would remove the tri-
gram “was a bright” since it does not contain a con-
tent word other than the target word.
We match the test trigrams against trigrams in the
Web 1T corpus. A corpus trigram is said to match
one of the test trigrams if the only difference be-
tween them is that the target word is replaced with a
candidate synonym.
A scoring algorithm is then applied to each can-
didate. The scoring algorithm relies on the test tri-
</bodyText>
<footnote confidence="0.999966">
4There are over 967 million unique trigrams in this corpus
5Excerpted from trial instance 1.
</footnote>
<page confidence="0.997807">
305
</page>
<bodyText confidence="0.999180333333333">
grams, denoted by T, the set of candidate synonyms,
C, and the frequencies of the trigrams in the corpus.
Let m(t, c) be the frequency of the corpus trigram
that matches test trigram t, where the target word in
t is replaced with candidate c. The score of a candi-
date c is given by:
</bodyText>
<equation confidence="0.99522">
�score(c) =
tET
</equation>
<bodyText confidence="0.99992775">
The normalization factor prevents high frequency
test trigrams from dominating the score of candi-
dates. The candidates are ranked by score, and the
top ten candidates are returned as substitutions.
</bodyText>
<subsectionHeader confidence="0.626912">
2.2.2 Nearest “synonym” neighbor
</subsectionHeader>
<bodyText confidence="0.999988038461538">
In some cases, the words in the local context did
not help identify a replacement synonym. For ex-
ample, in test instance 391 the trigrams used in the
local context model were: “by a coach”, “a coach
and”, and “coach and five”. The first two trigrams
were removed because they did not contain content
words. The final trigram does not provide conclu-
sive evidence: the correct synonym in this case can
be determined by knowing whether the next word is
“players” (coach = instructor) or “horses” (coach =
vehicle). Without backoff, extending the local con-
text model to 5-grams led to sparse data problems.
Rather than match exact n-grams, we use a near-
est neighbor cosine model with unigram (bag of
words) features for all words in the target sentence.
For each instance, an “instance vector” was created
by counting the unigrams in the target sentence.
Since the Web 1T corpus does not contain full
sentences, we matched each of the instance vectors
against vectors derived from the British National
Corpus6. For each candidate synonym, we created
a single vector by summing the unigram counts in
all sentences containing the candidate synonym (or
one of its morphological variants). We ranked each
candidate by the cosine similarity between the can-
didate vector and the instance vector.
</bodyText>
<subsectionHeader confidence="0.870521">
2.2.3 Nearest “sense” neighbor
</subsectionHeader>
<bodyText confidence="0.999614333333333">
Manual inspection of the trial data key revealed
that, for many instances, a large majority (if not
all) of the human-selected synonyms in that instance
</bodyText>
<footnote confidence="0.835933">
6http://www.natcorp.ox.ac.uk/
</footnote>
<bodyText confidence="0.999832947368421">
were found in just one or two RT entries. This not
altogether unexpected insight led to the creation of a
second nearest neighbor cosine model.
We first created instance vectors, following the
method described above. However, instead of cre-
ating a single vector for each candidate synonym,
we created a single vector for each “sense” (RT en-
try): for each RT entry, we created a single vector by
summing the unigram counts in all BNC sentences
containing any of that entry’s candidate synonyms
(or morphological variants). We ranked each candi-
date sense by the cosine similarity between the sense
vector and the instance vector.
This method is not used on its own but rather to
filter the results (Section 2.2.4) of the nearest “syn-
onym” neighbor method. Also note that while we
used the “senses” provided by RT for this method,
we could have used an automatic method, e.g. Lin
and Pantel (2001), to achieve the same goal.
</bodyText>
<subsectionHeader confidence="0.988855">
2.2.4 Filtering by sense
</subsectionHeader>
<bodyText confidence="0.99999780952381">
The nearest synonym neighbor method underper-
formed the local context matching method on the
trial data. This result led us to filter the nearest
neighbor results by keeping only those words listed
as synonyms of the highest ranked senses, as deter-
mined by the nearest sense neighbor model. This
proved successful, increasing accuracy from .41 to
.44 (for instances which had a mode) when we kept
only those synonyms found in the top half of the
senses returned by the nearest sense model.7
We attempted the sense filtering method on the
local context model but found that it was less suc-
cessful. No matter what threshold we set for filter-
ing, we always did best by not doing the filtering
at all. However, applying the filtering to only noun
instances, keeping only those synonyms belonging
to the single most highly ranked sense, increased
our accuracy on nouns from .51 to .57 (for instances
which had a mode). This surprising result, used in
the following section, requires further investigation
which was not possible in the limited time provided.
</bodyText>
<subsectionHeader confidence="0.927543">
2.2.5 Model Combination
</subsectionHeader>
<bodyText confidence="0.9929">
A straightforward model combination using the
relative ranking of synonyms by the filtered local
</bodyText>
<footnote confidence="0.971547">
7We rounded up if there were an odd number of senses, and
we always kept a minimum of two senses.
</footnote>
<equation confidence="0.710174">
m(t, c)
E-EC m(t, x)
</equation>
<page confidence="0.892256">
306
</page>
<table confidence="0.998637">
P R Mode P Mode R
all 35.53 32.83 47.41 43.82
Further Analysis
NMWT 37.49 34.64 49.11 45.35
NMWS 38.36 35.67 49.41 45.70
RAND 36.94 34.52 48.94 45.72
MAN 33.83 30.85 45.63 41.67
</table>
<tableCaption confidence="0.998304">
Table 1: SWAG1:OOT results
</tableCaption>
<sectionHeader confidence="0.998437" genericHeader="method">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.99574575">
partially responsible. For example, both LCM (used
in SWAG1 and SWAG2) and the nearest neigh-
bor cosine comparison algorithm (used in SWAG2)
performed poorly on verbs on the trial data. The
voter described in the SWAG2 discussion always
performed better on verbs than either system did in-
dividually, so this may account for part of the higher
precision and recall.
</bodyText>
<table confidence="0.998544714285714">
P R Mode P Mode R
all 37.80 34.66 50.18 46.02
Further Analysis
NMWT 39.95 36.51 52.28 47.78
NMWS 40.97 37.75 52.25 47.98
RAND 39.74 36.36 53.61 48.78
MAN 35.56 32.79 46.34 42.88
</table>
<tableCaption confidence="0.996392">
Table 2: SWAG2:OOT results
</tableCaption>
<bodyText confidence="0.999964">
context matching (FLCM) model8 and the filtered
nearest neighbor (FNN) model yielded results which
were inferior to those provided by the FLCM model
on its own. Examination of the results of each model
showed that the FLCM model was best on nouns and
adjectives, the FNN model was best on adverbs, and
the combination model was best on verbs. Though
limited time prohibited us from doing a more thor-
ough evaluation, we decided to use this unorthodox
combination as the basis for our second system.
</bodyText>
<sectionHeader confidence="0.999979" genericHeader="evaluation">
3 Results
</sectionHeader>
<bodyText confidence="0.982011043478261">
We submitted two sets of results to this task: the first
was our local context matching system (SWAG1)
and the second was the combined FLCM and FNN
hybrid system (SWAG2).
Our systems consistently perform better when a
mode exists, which makes sense because those are
instances in which the annotators are in agreement
(McCarthy and Navigli, 2007). In these cases it is
more likely that the most appropriate synonym is
clear from the context and therefore easier to pick.
It is hard to say exactly why SWAG2 outperforms
SWAG1 because we haven’t had enough time to
fully analyze our results. Our decision to choose dif-
ferent systems for each part of speech may have been
8Filtering was done only on nouns as described above.
Our results show that direct methods of lexical sub-
stitution deserve more investigation. It does indeed
seem possible to successfully do lexical substitution
without doing sense disambiguation. Furthermore,
this task can be accomplished in a knowledge-light
way. Further investigation of this method could in-
clude generating the list of synonyms using a com-
pletely knowledge-free approach.
</bodyText>
<sectionHeader confidence="0.999102" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999822576923077">
R.H. Baayen, R. Piepenbrock, and L. Gulikers. 1996.
CELEX2. LDC96L14, Linguistic Data Consortium,
Philadelphia.
T. Brants and A. Franz. 2006. Web 1T 5-gram, ver. 1.
LDC2006T13, Linguistic Data Consortium, Philadel-
phia.
I. Dagan, O. Glickman, A. Gliozzo, E. Marmorshtein,
and C. Strapparava. 2006. Direct word sense match-
ing for lexical substitution. In Proceedings of the 44th
Annual Meeting of the Association for Computational
Linguistics.
D. Lin and P. Pantel. 2001. Induction of semantic
classes from natural language text. In Proceedings of
the 7th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining.
D. Lin. 1998. Automatic retrieval and clustering of simi-
lar words. In Proceedings of the 36th Annual Meeting
of the Association for Computational Linguistics.
E. Loper and S. Bird. 2002. NLTK: The Natural
Language Toolkit. In Proceedings of the ACL-02
Workshop on Effective Tools and Methodologies
for Teaching Natural Language Processing and
Computational Linguistics.
D. McCarthy and R. Navigli. 2007. SemEval-2007 Task
10: English lexical substitution task. In Proceedings
of SemEval-2007.
</reference>
<page confidence="0.9986">
307
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.903292">
<title confidence="0.999455">SW-AG: Local Context Matching for English Lexical Substitution</title>
<author confidence="0.999961">George Dahl</author>
<author confidence="0.999961">Anne-Marie Frassica</author>
<author confidence="0.999961">Richard Wicentowski</author>
<affiliation confidence="0.9993045">Department of Computer Science Swarthmore College</affiliation>
<address confidence="0.935761">Swarthmore, PA 19081 USA</address>
<email confidence="0.999855">richardw@cs.swarthmore.edu</email>
<abstract confidence="0.99723">We present two systems that pick the ten most appropriate substitutes for a marked word in a test sentence. The first system scores candidates based on how frequently their local contexts match that of the marked word. The second system, an enhancement to the first, incorporates cosine similarity using unigram features. The core of both systems bypasses intermediate sense selection. Our results show that a knowledge-light, direct method for scoring potential replacements is viable.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R H Baayen</author>
<author>R Piepenbrock</author>
<author>L Gulikers</author>
</authors>
<date>1996</date>
<booktitle>CELEX2. LDC96L14, Linguistic Data Consortium,</booktitle>
<location>Philadelphia.</location>
<contexts>
<context position="6685" citStr="Baayen et al., 1996" startWordPosition="1113" endWordPosition="1116">ribed in the remainder of Section 2.2. 2.2.1 Local context matching (LCM) Our first system matches the context of target words to the context of candidate synonyms in a large, unannotated corpus. If the context of a candidate synonym exactly matches the context of a target word, it is considered a good replacement synonym. Context matches are made against the Web 1T corpus’ list of trigrams. Though this corpus provides us with a very large amount of data4, to increase the likelihood of finding an appropriate match, we mapped inflected words to their roots in both the corpus and the test data (Baayen et al., 1996). The context of a target word consists of a set of up to 3 trigrams, specifically those trigrams in the test sentence that contain the target word. For example, the context of “bright” in the sentence5 “... who was a bright boy only ...” is the set was a bright”, “a bright boy”, “bright boy only”}. Once we identified the set of context trigrams, we filtered this set by removing all trigrams which did not include content words. To identify content words, we used the NLTK-Lite tagger to assign a part of speech to each word (Loper and Bird, 2002). We considered open class words (with the excepti</context>
</contexts>
<marker>Baayen, Piepenbrock, Gulikers, 1996</marker>
<rawString>R.H. Baayen, R. Piepenbrock, and L. Gulikers. 1996. CELEX2. LDC96L14, Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Brants</author>
<author>A Franz</author>
</authors>
<date>2006</date>
<booktitle>Web 1T 5-gram, ver. 1. LDC2006T13, Linguistic Data Consortium,</booktitle>
<location>Philadelphia.</location>
<contexts>
<context position="5382" citStr="Brants and Franz, 2006" startWordPosition="897" endWordPosition="900">remaining 105 entries are other words (e.g. “gossip”) that list “tell” as a synonym. Where the Main Entry matches the query, RT entries roughly correspond to the traditional notion of “sense”. In order to reduce the number of potentially spurious synonyms that could be picked, we created a simple automatic filtering system. For each RT query, we kept only those entries whose Main Entry and part of speech matched the target word exactly3. In addition, we removed obscure words which we believed human annotators would be unlikely to pick. We used the unigram counts from the Web 1T 5-gram corpus (Brants and Franz, 2006) to determine the frequency of use of each candidate synonym. We experimented with discarding the least frequent third of the candidates. Although this filtering reduced our maximum attainable recall from 1http://thesaurus.reference.com 2excluding the extended relations such as hyponyms, etc. 3Since RT was not always consistent in labeling adjectives and adverbs, we conflated these in filtering. 85% to 75% on the trial data, it significantly raised our precision. 2.2 Ranking substitutions We created two systems (and submitted two sets of results) for this task. The first system is fully descri</context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>T. Brants and A. Franz. 2006. Web 1T 5-gram, ver. 1. LDC2006T13, Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>O Glickman</author>
<author>A Gliozzo</author>
<author>E Marmorshtein</author>
<author>C Strapparava</author>
</authors>
<title>Direct word sense matching for lexical substitution.</title>
<date>2006</date>
<booktitle>In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1025" citStr="Dagan et al. (2006)" startWordPosition="151" endWordPosition="154">based on how frequently their local contexts match that of the marked word. The second system, an enhancement to the first, incorporates cosine similarity using unigram features. The core of both systems bypasses intermediate sense selection. Our results show that a knowledge-light, direct method for scoring potential replacements is viable. 1 Introduction An obvious way to view the problem of lexical substitution is as a sense disambiguation task. For example, one possible approach is to identify the sense of the target word and then to pick a synonym based on the identified sense. Following Dagan et al. (2006), we refer to this as an indirect approach. A system using an indirect approach must have access to a list of senses for each target word, and each sense must have a corresponding list of synonyms. Though one can use a predefined sense inventory, such as WordNet, the granularity of the sense inventory may not be appropriate for the task. If the sense inventory is too fine-grained, then picking the correct sense may be needlessly difficult. Conversely, if it is too coarse, picking the correct sense may not narrow down the list of potential substitutions sufficiently. To avoid these problems, we</context>
<context position="2637" citStr="Dagan et al. (2006)" startWordPosition="430" endWordPosition="433"> on local context matching, and the second is a hybrid of local context matching and wider context bag-of-words matching. Both are knowledge-light and unsupervised. 2 Methods As mentioned above, we divide the task into two steps: compiling a list of synonyms and then, for each test instance, ranking the list of appropriate synonyms. Both of our systems create lists of candidate synonyms in the same way and only differ in the way they arrive at a ranking for these candidates. 2.1 Compiling a substitution lexicon We begin by compiling a list of candidate synonyms for each target word. Following Dagan et al. (2006), we will refer to this list of synonyms as our substitution lexicon. The performance of our system is limited by the substitution lexicon because it can only pick the correct replacements if they are in the lexicon. The substitution lexicon available to our scoring system therefore determines both the maximum attainable recall and the baseline probability of randomly guessing a correct replacement. One approach to generating a substitution lexicon is to query WordNet for lists of synonyms grouped by the senses of each word. While WordNet has its advantages, we aimed to create a knowledge-ligh</context>
</contexts>
<marker>Dagan, Glickman, Gliozzo, Marmorshtein, Strapparava, 2006</marker>
<rawString>I. Dagan, O. Glickman, A. Gliozzo, E. Marmorshtein, and C. Strapparava. 2006. Direct word sense matching for lexical substitution. In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
<author>P Pantel</author>
</authors>
<title>Induction of semantic classes from natural language text.</title>
<date>2001</date>
<booktitle>In Proceedings of the 7th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.</booktitle>
<contexts>
<context position="10876" citStr="Lin and Pantel (2001)" startWordPosition="1835" endWordPosition="1838">ndidate synonym, we created a single vector for each “sense” (RT entry): for each RT entry, we created a single vector by summing the unigram counts in all BNC sentences containing any of that entry’s candidate synonyms (or morphological variants). We ranked each candidate sense by the cosine similarity between the sense vector and the instance vector. This method is not used on its own but rather to filter the results (Section 2.2.4) of the nearest “synonym” neighbor method. Also note that while we used the “senses” provided by RT for this method, we could have used an automatic method, e.g. Lin and Pantel (2001), to achieve the same goal. 2.2.4 Filtering by sense The nearest synonym neighbor method underperformed the local context matching method on the trial data. This result led us to filter the nearest neighbor results by keeping only those words listed as synonyms of the highest ranked senses, as determined by the nearest sense neighbor model. This proved successful, increasing accuracy from .41 to .44 (for instances which had a mode) when we kept only those synonyms found in the top half of the senses returned by the nearest sense model.7 We attempted the sense filtering method on the local cont</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>D. Lin and P. Pantel. 2001. Induction of semantic classes from natural language text. In Proceedings of the 7th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3584" citStr="Lin (1998)" startWordPosition="581" endWordPosition="582"> baseline probability of randomly guessing a correct replacement. One approach to generating a substitution lexicon is to query WordNet for lists of synonyms grouped by the senses of each word. While WordNet has its advantages, we aimed to create a knowledge-light 304 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 304–307, Prague, June 2007. c�2007 Association for Computational Linguistics system. A more knowledge-free system would have used a machine readable dictionary or a large natural language sample to retrieve its synonyms (see, for example, Lin (1998)), but our system falls short of this, relying on Roget’s New Millennium Thesaurus1 (henceforth RT) as a source of synonyms. Though this thesaurus is similar to WordNet in some ways, it does not contain semantic relationships beyond synonyms and antonyms. One important advantage of a thesaurus over WordNet is that it is easier to obtain for languages other than English. We used the trial data to ensure that the quality of the list compiled from RT would be satisfactory for this task. We found that by using the synonyms in WordNet synsets2 as our substitution lexicon, we could achieve a maximum</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>D. Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Loper</author>
<author>S Bird</author>
</authors>
<title>NLTK: The Natural Language Toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics.</booktitle>
<contexts>
<context position="7235" citStr="Loper and Bird, 2002" startWordPosition="1213" endWordPosition="1216"> their roots in both the corpus and the test data (Baayen et al., 1996). The context of a target word consists of a set of up to 3 trigrams, specifically those trigrams in the test sentence that contain the target word. For example, the context of “bright” in the sentence5 “... who was a bright boy only ...” is the set was a bright”, “a bright boy”, “bright boy only”}. Once we identified the set of context trigrams, we filtered this set by removing all trigrams which did not include content words. To identify content words, we used the NLTK-Lite tagger to assign a part of speech to each word (Loper and Bird, 2002). We considered open class words (with the exception of the verb to be) and pronouns to be content words. We call the filtered set of trigrams the test trigrams. From the above example, we would remove the trigram “was a bright” since it does not contain a content word other than the target word. We match the test trigrams against trigrams in the Web 1T corpus. A corpus trigram is said to match one of the test trigrams if the only difference between them is that the target word is replaced with a candidate synonym. A scoring algorithm is then applied to each candidate. The scoring algorithm re</context>
</contexts>
<marker>Loper, Bird, 2002</marker>
<rawString>E. Loper and S. Bird. 2002. NLTK: The Natural Language Toolkit. In Proceedings of the ACL-02 Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McCarthy</author>
<author>R Navigli</author>
</authors>
<title>SemEval-2007 Task 10: English lexical substitution task.</title>
<date>2007</date>
<booktitle>In Proceedings of SemEval-2007.</booktitle>
<contexts>
<context position="13867" citStr="McCarthy and Navigli, 2007" startWordPosition="2344" endWordPosition="2347">l was best on nouns and adjectives, the FNN model was best on adverbs, and the combination model was best on verbs. Though limited time prohibited us from doing a more thorough evaluation, we decided to use this unorthodox combination as the basis for our second system. 3 Results We submitted two sets of results to this task: the first was our local context matching system (SWAG1) and the second was the combined FLCM and FNN hybrid system (SWAG2). Our systems consistently perform better when a mode exists, which makes sense because those are instances in which the annotators are in agreement (McCarthy and Navigli, 2007). In these cases it is more likely that the most appropriate synonym is clear from the context and therefore easier to pick. It is hard to say exactly why SWAG2 outperforms SWAG1 because we haven’t had enough time to fully analyze our results. Our decision to choose different systems for each part of speech may have been 8Filtering was done only on nouns as described above. Our results show that direct methods of lexical substitution deserve more investigation. It does indeed seem possible to successfully do lexical substitution without doing sense disambiguation. Furthermore, this task can be</context>
</contexts>
<marker>McCarthy, Navigli, 2007</marker>
<rawString>D. McCarthy and R. Navigli. 2007. SemEval-2007 Task 10: English lexical substitution task. In Proceedings of SemEval-2007.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>