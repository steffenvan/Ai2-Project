<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005835">
<title confidence="0.992615">
Variant Transduction: A Method for Rapid Development of
Interactive Spoken Interfaces
</title>
<author confidence="0.980986">
Hiyan Alshawi and Shona Douglas
</author>
<affiliation confidence="0.972475">
AT&amp;T Labs Research
</affiliation>
<address confidence="0.969872">
180 Park Avenue
Florham Park, NJ 07932, USA
</address>
<email confidence="0.999425">
{hiyan,shona}@research.att.com
</email>
<sectionHeader confidence="0.980176" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999954545454546">
We describe an approach (&amp;quot;vari-
ant transduction&amp;quot;) aimed at reduc-
ing the effort and skill involved
in building spoken language inter-
faces. Applications are created
by specifying a relatively small set
of example utterance-action pairs
grouped into contexts. No interme-
diate semantic representations are
involved in the specification, and
the confirmation requests used in
the dialog are constructed automat-
ically. These properties of vari-
ant transduction arise from combin-
ing techniques for paraphrase gen-
eration, classification, and example-
matching. We describe how a spo-
ken dialog system is constructed
with this approach and also provide
some experimental results on vary-
ing the number of examples used to
build a particular application.
</bodyText>
<sectionHeader confidence="0.997257" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99971125862069">
Developing non-trivial interactive spoken lan-
guage applications currently requires signifi-
cant effort, often several person-months. A
major part of this effort is aimed at coping
with variation in the spoken language input
by users. One approach to handling varia-
tion is to write a large natural language gram-
mar manually and hope that its coverage is
sufficient for multiple applications (Dowding
et al., 1994). Another approach is to cre-
ate a simulation of the intended system (typ-
ically with a human in the loop) and then
record users interacting with the simulation.
The recordings are then transcribed and an-
notated with semantic information relating to
the domain; the transcriptions and annota-
tions can then be used to create a statistical
understanding model (Miller et al., 1998) or
used as guidance for manual grammar devel-
opment (Aust et al., 1995).
Building mixed initiative spoken language
systems currently usually involves the design
of semantic representations specific to the ap-
plication domain. These representations are
used to pass data between the language pro-
cessing components: understanding, dialog,
confirmation generation, and response gener-
ation. However, such representations tend to
be domain-specific, and this makes it difficult
to port to new domains or to use machine
learning techniques without extensive hand-
labeling of data with the semantic represen-
tations. Furthermore, the use of intermediate
semantic representations still requires a final
transduction step from the intermediate rep-
resentation to the action format expected by
the application back-end (e.g. SQL database
query or procedure call).
For situations when the effort and exper-
tise available to build an application is small,
the methods mentioned above are impracti-
cal, and highly directed dialog systems with
little allowance for language variability are
constructed.
In this paper, we describe an approach to
constructing interactive spoken language ap-
plications aimed at alleviating these prob-
lems. We first outline the characteristics of
the method (section 2) and what needs to
be provided by the application builder (sec-
tion 3). In section 4 and section 5 we ex-
plain variant expansion and the operation of
the system at runtime, and in section 6 we
describe how confirmation requests are pro-
duced by the system. In section 7 we give
some initial experimental results on varying
the number of examples used to construct a
call-routing application.
</bodyText>
<sectionHeader confidence="0.452166" genericHeader="introduction">
2 Characteristics of our approach
</sectionHeader>
<bodyText confidence="0.9999655">
The goal of the approach discussed in this pa-
per (which we refer to as &amp;quot;variant transduc-
tion&amp;quot;) is to avoid the effort and specialized
expertise used to build current research pro-
totypes, while allowing more natural spoken
input than is handled by spoken dialog sys-
tems built using current commercial practice.
This led us to adopt the following constraints:
</bodyText>
<listItem confidence="0.996526789473684">
• Applications are constructed using a rel-
atively small number of example inputs
(no grammar development or extensive
data collection).
• No intermediate semantic representa-
tions are needed. Instead, manipulations
are performed on word strings and on ac-
tion strings that are final (back-end) ap-
plication calls.
• Confirmation queries posed by the sys-
tem to the user are constructed automat-
ically from the examples, without the use
of a separate generation component.
• Dialog control should be simple to spec-
ify for simple applications, while allowing
the flexibility of delegating this control
to another module (e.g. an &amp;quot;intelligent&amp;quot;
back-end agent) for more complex appli-
cations.
</listItem>
<bodyText confidence="0.999845235294118">
We have constructed two telephone-based
applications using this method, an applica-
tion to access email and a call-routing appli-
cation. These two applications were chosen
to gain experience with the method because
they have different usage characteristics and
back-end complexity. For the e-mail access
system, usage is typically habitual, and the
system&apos;s mapping of user utterances to back-
end actions needs to take into account dy-
namic aspects of the current email session.
For the call-routing application, the back-end
calls executed by the system are relatively
simple, but users may only encounter the sys-
tem once, and the system&apos;s initial prompt is
not intended to constrain the first input spo-
ken by the user.
</bodyText>
<sectionHeader confidence="0.98397" genericHeader="method">
3 Constructing an application with
example-action contexts
</sectionHeader>
<bodyText confidence="0.999898545454545">
An interactive spoken language application
constructed with the variant transduction
method consists of a set of contexts. Each
context provides the mapping between user
inputs and application actions that are mean-
ingful in a particular stage of interaction be-
tween the user and system. For example the
e-mail reader application includes contexts for
logging in and for navigating a mail folder.
The actual contexts that are used at run-
time are created through a four step process:
</bodyText>
<listItem confidence="0.660844">
1. The application developer specifies (a
small number of) triples (e, a, c) where
e is a natural language string (a typical
user input), a is an application action
(back-end application API call). For in-
</listItem>
<bodyText confidence="0.980476625">
stance, the string read the message from
John might be paired with the API call
mailAgent.getWithSender(&amp;quot;jsmith@att.com&amp;quot;).
The third element of a triple, c, is an
expression identifying another (or the
same) context, specifically, the context
the system will transition to if e is the
closest match to the user&apos;s input.
</bodyText>
<listItem confidence="0.9977024">
2. The set of triples for each context is ex-
panded by the system into a larger set
of triples. The additional triples are of
the form (v, a&apos;, c) where v is a &amp;quot;variant&amp;quot;
of example e (as explained in section 4
below), and a&apos; is an &amp;quot;adapted&amp;quot; version of
the action a.
3. During an actual user session, the set of
triples for a context may optionally be
expanded further to take into account
</listItem>
<bodyText confidence="0.997519314285714">
the dynamic aspects of a particular ses-
sion. For example, in the mail access ap-
plication, the set of names available for
recognition is increased to include those
present as senders in the user&apos;s current
mail folder.
4. A speech recognition language model is
compiled from the expanded set of ex-
amples. We currently use a language
model that accepts any sequence of sub-
strings of the examples, optionally sepa-
rated by filler words, as well as sequences
of digits. (For a small number of exam-
ples, a statistical N-gram model is inef-
fective because of low N-gram counts.) A
detailed account of the recognition lan-
guage model techniques used in the sys-
tem is beyond the scope of this paper.
In the current implementation, actions are
sequences of statements in the Java language.
Constructors can be called to create new ob-
jects (e.g. a mail session object) which can be
assigned to variables and referenced in other
actions. The context interpreter loads the re-
quired classes and evaluates methods dynam-
ically as needed. It is thus possible for an
application developer to build a spoken inter-
face to their target API without introducing
any new Java classes. The system could eas-
ily be adapted to use action strings from other
interpreted languages.
A key property of the process described
above is that the application developer needs
to know only the back-end API and English
(or some other natural language).
</bodyText>
<sectionHeader confidence="0.964314" genericHeader="method">
4 Variant compilation
</sectionHeader>
<bodyText confidence="0.999907522727273">
Different expansion methods can be used in
the second step to produce variants v of an
example e. In the simplest case, v may be
a paraphrase of e. Such paraphrase vari-
ants are used in the experiments in section 7,
where domain-independent &amp;quot;carrier&amp;quot; phrases
are used to create variants. For example, the
phrase I&apos;d like to (among others) is used as a
possible alternative for the phrase I want to.
The context compiler includes an English-to-
English paraphrase generator, so the applica-
tion developer is not involved in the expan-
sion process, relieving her of the burden of
handling this type of language variation. We
are also experimenting with other forms of
variation, including those arising from lexical-
semantic relations, user-specific customiza-
tion, and those variants uttered by users dur-
ing field trials of a system.
When v is a paraphrase of e, the adapted
action a&apos; is the same string as a. In the more
general case, the meaning of variant v is dif-
ferent from that of e, and the system attempts
(not always correctly) to construct a&apos; so that
it reflects this difference in meaning. For ex-
ample, including the variant show the message
from Bill Wilson of an example read the mes-
sage from John, involves modifying the ac-
tion mailAgent.getWithSender(&amp;quot;jsmith@att.com&amp;quot;)
to mailAgent.getWithSender(&amp;quot;wwilson@att.com&amp;quot;).
We currently adopt a simple approach to
the process of mapping language string vari-
ants to their corresponding target action
string variants. The process requires the
availability of a &amp;quot;token mapping&amp;quot; t between
these two string domains, or data or heuristics
from which such a mapping can be learned au-
tomatically. Examples of the token mapping
are names to email addresses as illustrated in
the example above, name to identifier pairs in
a database system, &amp;quot;soundex&amp;quot; phonetic string
spelling in directory applications, and a bilin-
gual dictionary in a translation application.
The process proceeds as follows:
</bodyText>
<listItem confidence="0.922451791666667">
1. Compute a set of lexical mappings be-
tween the variant v and example e. This
is currently performed by aligning the
two string in such a way as that the align-
ment minimizes the (weighted) edit dis-
tance between them (Wagner and Fis-
cher, 1974).
2. The token mapping t is used to map
substitution pairs identified by the align-
ment ((read, show) and (John, Bill Wil-
son) in the example above) to corre-
sponding substitution pairs in the action
string. In general this will result in a
smaller set of substitution strings since
not all word strings will be present in
the domain of t. (In the example, this re-
sults in the single pair (jsmith@att.com,
wwilson@att.com).)
3. The action substitution pairs are applied
to a to produce a&apos;.
4. The resulting action a&apos; is checked for
(syntactic) well-formedness in the action
string domain; the variant v is rejected if
a&apos; is ill-formed.
</listItem>
<sectionHeader confidence="0.803078" genericHeader="method">
5 Input interpretation
</sectionHeader>
<bodyText confidence="0.999854714285715">
When an example-action context is active
during an interaction with a user, two com-
ponents (in addition to the speech recognition
language model) are compiled from the con-
text in order to map the user inputs into the
appropriate (possibly adapted) action:
Classifier A classifier is built with training
pairs (v, a) where v is a variant of an
example e for which the example action
pair (e, a) is a member of the unexpanded
pairs in the context. Note that the clas-
sifier is not trained on pairs with adapted
examples a&apos; since the set of adapted
actions may be too large for accurate
classification (with standard classifica-
tion techniques). The classifiers typically
use text features such as N-grams ap-
pearing in the training data. In our ex-
periments, we have used different classi-
fiers, including BoosTexter (Schapire and
Singer, 2000), and a classifier based on
Phi-correlation statistics for the text fea-
tures (see Alshawi and Douglas (2000)
for our earlier application of Phi statis-
tics in learning machine translation mod-
els from examples). Other classifiers
such as decision trees (Quinlan, 1993) or
support vector machines (Vapnik, 1995)
could be used instead.
Matcher The matcher can compute a dis-
tortion mapping and associated distance
between the output s of the speech rec-
ognizer and a variant v. Various match-
ers can be used such as those suggested
in example-based approaches to machine
translation (Sumita and Iida, 1995). So
far we have used a weighted string edit
distance matcher and experimented with
different substitution weights including
ones based on measures of statistical sim-
ilarity between words such as the one
described by Pereira et al. (1993). The
output of the matcher is a real number
(the distance) and a distortion mapping
represented as a sequence of edit opera-
tions (Wagner and Fischer, 1974).
Using these two components, the method
for mapping the user&apos;s utterance to an exe-
cutable action is as follows:
</bodyText>
<listItem confidence="0.919250533333333">
1. The language model derived from con-
text c is activated in the speech recog-
nizer.
2. The speech recognizer produces a string
s from the user&apos;s utterance.
3. The classifier for c is applied to s to pro-
duce an unadapted action a.
4. The matcher is applied pairwise to com-
pare s with each variant va, derived from
a triple (e, a, c&apos;) in the unexpanded ver-
sion of c.
5. The triple (v, a&apos;, c&apos;) for which v pro-
duces the smallest distance is selected
and passed along with e to the dialog con-
troller.
</listItem>
<bodyText confidence="0.999979888888889">
The relationship between the input s, vari-
ant v, example e, and actions a and a&apos; is
depicted in Figure 1. In the figure, f is
the mapping between examples and actions
in the unexpanded context; r is the relation
between examples and variants; and g is the
search mapping implemented by the classifier-
matcher. The role of e&apos; is related to confirma-
tions as explained in the following section.
</bodyText>
<sectionHeader confidence="0.695425" genericHeader="method">
6 Confirmation and dialog control
</sectionHeader>
<bodyText confidence="0.887756">
Dialog control is straightforward as the reader
might expect, except for two aspects de-
scribed in this section: (i) evaluation of next-
context expressions, and (ii) generation of
</bodyText>
<equation confidence="0.589512857142857">
p (prompt): say a mailreader command
s (words spoken): now show me messages from Bill
v (variant): show the message from Bill Wilson
e (example): read the message from John
a (associated action): mailAgent.getWithSender(&amp;quot;jsmith@att.com&amp;quot;)
a&apos; (adapted action): mailAgent.getWithSender(&amp;quot;wwilson@att.com&amp;quot;)
e&apos; (adapted example): read the message from Bill Wilson
</equation>
<figureCaption confidence="0.9994355">
Figure 2: Example
Figure 1: Variant Transduction mappings
</figureCaption>
<bodyText confidence="0.999832393442623">
confirmation requests based on the examples
in the context and the user&apos;s input.
As noted in section 3 the third element c
of each triple (e, a, c) in a context is an ex-
pression that evaluates to the name of the
next context (dialog state) that the system
will transition to if the triple is selected. For
simple applications, c can simply always be
an identifier for a context, i.e. the dialog state
transition network is specified explicitly in ad-
vance in the triples by the application devel-
oper.
For more complex applications, next con-
text expressions c may be calls that evalu-
ate to context identifiers. In our implemen-
tation, these calls can be Java methods ex-
ecuted on objects known to the action in-
terpreter. They may thus be calls on the
back-end application system, which is appro-
priate for cases when the back-end has state
information relevant to what should happen
next (e.g. if it is an &amp;quot;intelligent agent&amp;quot;). It
might also be a call to component that imple-
ments a dialog strategy learning method (e.g.
Levin and Pieraccini (1997)), though we have
not yet tried such methods in conjunction
with the present system.
A confirmation request of the form do you
mean e&apos; is constructed for each variant-action
pair (v, a&apos;) of an example-action pair (e, a).
The string e&apos; is constructed by first comput-
ing a submapping h&apos; of the mapping h rep-
resenting the distortion between e and v. h&apos;
is derived from h by removing those edit op-
erations which were not involved in mapping
the action a to the adapted action a&apos;. (The
matcher is used to compute h except when
the process of deriving (v, a&apos;) from (e, a) al-
ready includes an explicit representation of h
and t(h).)
The restricted mapping h&apos; is used instead of
h to construct e&apos; in order to avoid misleading
the user about the extent to which the ap-
plication action is being adapted. Thus if h
includes the substitution w —+ w&apos; but t(w) is
not a substring of a then this edit operation is
not included in h&apos;. This way, e&apos; includes w un-
changed, so that the confirmation asked of the
user does not carry the implication that the
change w —+ w&apos; is taken into account in the
action a&apos; to be executed by the system. For
instance, in the example in Figure 2, the word
&amp;quot;now&amp;quot; in the user&apos;s input does not correspond
to any part of the adapted action, and is not
included in the confirmation string. In prac-
tice, the confirmation string e&apos; is computed
at the same time that the variant-action pair
(v, a&apos;) is derived from the original example
pair (e, a).
The dialog flow of control proceeds as fol-
lows:
</bodyText>
<listItem confidence="0.998206129032258">
1. The active context c is set to a distin-
guished initial context co indicated by
the application developer.
2. A prompt associated with the current ac-
tive context c is played to the user using
a speech synthesiser or by playing an au-
dio file. For this purpose the application
developer provides a text string (or audio
file) for each context in the application.
3. The user&apos;s utterance is interpreted as ex-
plained in the previous section to pro-
duce the triple (v, a&apos;, c&apos;).
4. A match distance d is computed as the
sum of the distance computed for the
matcher between s and v and the dis-
tance computed by the matcher between
v and e (where e is the example from
which v was derived).
5. If d is smaller than a preset threshold, it
is assumed that no confirmation is neces-
sary and the next three steps are skipped.
6. The system asks the user do you mean:
e&apos;. If the user responds positively then
proceed to the next step, otherwise re-
turn to step 2.
7. The action a&apos; is executed, and any string
output it produces is read to the user
with the speech synthesizer.
8. The active context is set to the result of
evaluating the expression c&apos;.
9. Return to step 2.
</listItem>
<bodyText confidence="0.995062857142857">
Figure 2 gives an example showing the
strings involved in a dialog turn. Handling
the user&apos;s verbal response to the confirmation
is done with a built-in yes-no context.
The generation of confirmation requests
requires no work by the application de-
veloper. Our approach thus provides
an even more extreme version of auto-
matic confirmation generation than that used
by Chu-Carroll and Carpenter (1999) where
only a small effort is required by the devel-
oper. In both cases, the benefits of care-
fully crafted confirmation requests are being
traded for rapid application development.
</bodyText>
<sectionHeader confidence="0.99534" genericHeader="evaluation">
7 Experiments
</sectionHeader>
<bodyText confidence="0.999982252173914">
An important question relating to our method
is the effect of the number of examples on
system interpretation accuracy. To measure
this effect, we chose the operator services call
routing task described by Gorin et al. (1997).
We chose this task because a reasonably large
data set was available in the form of actual
recordings of thousands of real customers call-
ing AT&amp;T&apos;s operators, together with tran-
scriptions and manual labeling of the de-
sired call destination. More specifically, we
measure the call routing accuracy for uncon-
strained caller responses to the initial context
prompt ATT. How may I help you?. An-
other advantage of this task was that bench-
mark call routing accuracy figures were avail-
able for systems built with the full data set
(Gorin et al., 1997; Schapire and Singer,
2000). We have not yet measured interpreta-
tion accuracy for the structurally more com-
plex e-mail access application.
In this experiment, the responses to How
may I help you? are &amp;quot;routed&amp;quot; to fifteen des-
tinations, where routing means handing off
the call to another system or human operator,
or moving to another example-action context
that will interact further with the user to elicit
further information so that a subtask (such as
making a collect call) can be completed. Thus
the actions in the initial context are simply
the destinations, i.e. a = a&apos;, and the matcher
is only used to compute e&apos;.
The fifteen destinations include a destina-
tion &amp;quot;other&amp;quot; which is treated specially in that
it is also taken to be the destination when the
system rejects the user&apos;s input, for example
because the confidence in the output of the
speech recognizer is too low. Following previ-
ous work on this task, cited above, we present
the results for each experimental condition as
an ROC curve plotting the routing accuracy
(on non-rejected utterances) as a function of
the false rejection rate (the percentage of the
samples incorrectly rejected); a classification
by the system of &amp;quot;other&amp;quot; is considered equiv-
alent to rejection.
The dataset consists of 8,844 utterances of
which 1000 were held out for testing. We refer
to the remaining 7,884 utterances as the &amp;quot;full
training dataset&amp;quot;.
In the experiments, we vary two conditions:
Input uncertainty The input string to the
interpretation component is either a hu-
man transcription of the spoken utter-
ance or the output of a speech recog-
nizer. The acoustic model used for au-
tomatic speech recognition was a gen-
eral telephone speech HHM model in all
cases. (For the full dataset, better re-
sults can be achieved by an application-
specific acoustic model, as presented by
Gorin et al. (1997) and confirmed by our
results below.)
Size of example set We select progres-
sively larger subsets of examples from
the full training set, as well as showing
results for the full training set itself. We
wish to approximate the situation where
an application developer uses typical
examples for the initial context without
knowing the distribution of call types.
We therefore select k utterances for each
destination, with k set to 3, 5, and 10,
respectively. This selection is random,
except for the provision that utterances
appearing more than once are preferred,
to approximate the notion of a typical
utterance. The selected examples are
expanded by the addition of variants, as
described earlier. For each value of k,
the results shown are for the median of
three runs.
Figure 3 shows the routing accuracy ROC
curves for transcribed input for k = 3, 5, 10
and for the full training dataset. These re-
sults for transcribed input were obtained with
BoosTexter (Schapire and Singer, 2000) as the
classifier module in our system because we
have observed that BoosTexter generally out-
performs our Phi classifier (mentioned earlier)
for text input.
Figure 4 shows the corresponding four ROC
curves for recognition output, and an ad-
ditional fifth graph (the top one) showing
the improvement that is obtained with a do-
main specific acoustic model coupled with a
trigram language model. These results for
recognition output were obtained with the
Phi classifier module rather than BoosTex-
ter; the Phi classifier performance is generally
the same as, or slightly better than, Boos-
Texter when applied to recognition output.
The language models used in the experiments
for Figure 4 are derived from the example
sets for k = 3, 5, 10 (lower three graphs) and
for the full training set (upper two graphs),
respectively. As described earlier, the lan-
guage model for restricted numbers of exam-
ples is an unweighted one that recognizes se-
quences of substrings of the examples. For the
full training set, statistical N-gram language
models are used (N=3 for the top graph and
N=2 for the second to top) since there is suf-
ficient data in the full training set for such
language models to be effective.
</bodyText>
<figure confidence="0.997113692307692">
100
90
80
70
% Correct actions 60
50
40
30
20
10
0
0 10 20 30 40 50 60 70 80
False rejection %
</figure>
<figureCaption confidence="0.987556">
Figure 3: Routing accuracy for transcribed
utterances
</figureCaption>
<bodyText confidence="0.9991275">
Comparing the two figures, it can be seen
that the performance shortfall from using
small numbers of examples compared to the
full training set is greater when speech recog-
</bodyText>
<figure confidence="0.98404975">
full training set
10 examples/action + variants
5 examples/action + variants
3 examples/action + variants
</figure>
<figureCaption confidence="0.9581795">
Figure 4: Routing accuracy for speech recog-
nition output
</figureCaption>
<bodyText confidence="0.999814363636364">
nition errors are included. This suggests that
it might be advantageous to use the examples
to adapt a general statistical language model.
There also seem to be diminishing returns as
k is increased from 3 to 5 to 10. A likely
explanation is that expansion of examples by
variants is progressively less effective as the
size of the unexpanded set is increased. This
is to be expected since additional real exam-
ples presumably are more faithful to the task
than artificially generated variants.
</bodyText>
<sectionHeader confidence="0.958913" genericHeader="conclusions">
8 Concluding remarks
</sectionHeader>
<bodyText confidence="0.999988631578948">
We have described an approach to construct-
ing interactive spoken interfaces. The ap-
proach is aimed at shifting the burden of han-
dling linguistic variation for new applications
from the application developer (or data col-
lection lab) to the underlying spoken language
understanding technology itself. Applications
are specified in terms of a relatively small
number of examples, while the mapping be-
tween the inputs that users speak, variants
of the examples, and application actions, are
handled by the system. In this approach, we
avoid the use of intermediate semantic rep-
resentations, making it possible to develop
general approaches to linguistic variation and
dialog responses in terms of word-string to
word-string transformations. Confirmation
requests used in the dialog are computed au-
tomatically from variants in a way intended to
minimize misleading the user about the appli-
cation actions to be executed by the system.
The quantitative results we have pre-
sented indicate that a surprisingly small num-
ber of training examples can provide use-
ful performance in a call routing application.
These results suggest that, even at its cur-
rent early stage of development, the vari-
ant transduction approach is a viable option
for constructing spoken language applications
rapidly without specialized expertise. This
may be appropriate, for example, for boot-
strapping data collection, as well as for situa-
tions (e.g. small businesses) for which devel-
opment of a full-blown system would be too
costly. When a full dataset is available, the
method can provide similar performance to
current techniques while reducing the level of
skill necessary to build new applications.
</bodyText>
<sectionHeader confidence="0.998014" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999219724137931">
H. Alshawi and S. Douglas. 2000. Learning
dependency transduction models from unan-
notated examples. Philosophical Transactions
of the Royal Society (Series A: Mathematical,
Physical and Engineering Sciences), 358:1357{
1372, April.
H. Aust, M. Oerder, F. Seide, and V. Steinbiss.
1995. The Philips automatic train timetable
information system. Speech Communication,
17:249{262.
Jennifer Chu-Carroll and Bob Carpenter. 1999.
Vector-based natural language call routing.
Computational Linguistic, 25(3):361-388.
J. Dowding, J. M. Gawron, D. Appelt, J. Bear,
L. Cherny, R. Moore, and D. Moran. 1994.
Gemini: A Natural Language System For
Spoken-Language Understanding. In Proc.
ARPA Human Language Technology Workshop
&apos;99, pages 43{48, Princeton, NJ.
A.L. Gorin, G. Riccardi, and J.H. Wright. 1997.
How may I help you? Speech Communication,
23(1-2):113-127.
E. Levin and R. Pieraccini. 1997. A stochas-
tic model of computer-human interaction for
learning dialogue strategies. In Proceedings of
EUROSPEECH97, pages 1883{1886, Rhodes,
Greece.
Scott Miller, Michael Crystal, Heidi Fox, Lance
Ramshaw, Richard Schwartz, Rebecca Stone,
</reference>
<figure confidence="0.990018277777778">
90
80
70
% Correct actions
60
50
40
100
00 10 20 30 40 50 60 70 80
False rejection %
30
20
10
full training set, trigrams, domain acoustics
full training set, bigrams
10 examples/action + variants, subsequences
5 examples/action + variants, subsequences
3 examples/action + variants, subsequences
</figure>
<reference confidence="0.9988375">
Ralph Weischedel, and the Annotation Group.
1998. Algorithms that learn to extract informa-
tion — BBN: description of the SIFT system as
used for MUC-7. In Proceedings of the Seventh
Message Understanding Conference (MUC-7),
Fairfax, VA. Morgan Kaufmann.
F. Pereira, N. Tishby, and L. Lee. 1993. Distribu-
tional clustering of english words. In Proceed-
ings of the 31st meeting of the Association for
Computational Linguistics, pages 183{190.
J.R. Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann, San Mateo, CA.
Robert E. Schapire and Yoram Singer. 2000.
BoosTexter: A Boosting-based System for
Text Categorization. Machine Learning,
39(2/3):135-168.
Eiichiro Sumita and Hitoshi Iida. 1995. Het-
erogeneous computing for example-based trans-
lation of spoken language. In Proceedings of
the 6th International Conference on Theoretical
and Methodological Issues in Machine Transla-
tion, pages 273{286, Leuven, Belgium.
V.N. Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer, New York.
Robert A. Wagner and Michael J. Fischer.
1974. The String-to-String Correction Prob-
lem. Journal of the Association for Computing
Machinery, 21(1):168-173, January.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.964661">
<title confidence="0.998817">Variant Transduction: A Method for Rapid Development Interactive Spoken Interfaces</title>
<author confidence="0.992387">Alshawi Douglas</author>
<affiliation confidence="0.999185">AT&amp;T Labs</affiliation>
<address confidence="0.99524">180 Park Florham Park, NJ 07932,</address>
<abstract confidence="0.999287652173913">We describe an approach (&amp;quot;variant transduction&amp;quot;) aimed at reducing the effort and skill involved building spoken language interfaces. Applications are by specifying a relatively small set of example utterance-action pairs grouped into contexts. No intermediate semantic representations are involved in the specification, and the confirmation requests used in dialog are constructed automat- These properties of ant transduction arise from combining techniques for paraphrase generation, classification, and examplematching. We describe how a spoken dialog system is constructed with this approach and also provide some experimental results on varying the number of examples used to build a particular application.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H Alshawi</author>
<author>S Douglas</author>
</authors>
<title>Learning dependency transduction models from unannotated examples.</title>
<date>2000</date>
<journal>Philosophical Transactions of the Royal Society (Series A: Mathematical, Physical and Engineering Sciences),</journal>
<volume>358</volume>
<pages>1372</pages>
<contexts>
<context position="11884" citStr="Alshawi and Douglas (2000)" startWordPosition="1923" endWordPosition="1926">re v is a variant of an example e for which the example action pair (e, a) is a member of the unexpanded pairs in the context. Note that the classifier is not trained on pairs with adapted examples a&apos; since the set of adapted actions may be too large for accurate classification (with standard classification techniques). The classifiers typically use text features such as N-grams appearing in the training data. In our experiments, we have used different classifiers, including BoosTexter (Schapire and Singer, 2000), and a classifier based on Phi-correlation statistics for the text features (see Alshawi and Douglas (2000) for our earlier application of Phi statistics in learning machine translation models from examples). Other classifiers such as decision trees (Quinlan, 1993) or support vector machines (Vapnik, 1995) could be used instead. Matcher The matcher can compute a distortion mapping and associated distance between the output s of the speech recognizer and a variant v. Various matchers can be used such as those suggested in example-based approaches to machine translation (Sumita and Iida, 1995). So far we have used a weighted string edit distance matcher and experimented with different substitution we</context>
</contexts>
<marker>Alshawi, Douglas, 2000</marker>
<rawString>H. Alshawi and S. Douglas. 2000. Learning dependency transduction models from unannotated examples. Philosophical Transactions of the Royal Society (Series A: Mathematical, Physical and Engineering Sciences), 358:1357{ 1372, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Aust</author>
<author>M Oerder</author>
<author>F Seide</author>
<author>V Steinbiss</author>
</authors>
<title>The Philips automatic train timetable information system.</title>
<date>1995</date>
<journal>Speech Communication,</journal>
<pages>17--249</pages>
<contexts>
<context position="1834" citStr="Aust et al., 1995" startWordPosition="277" endWordPosition="280">oach to handling variation is to write a large natural language grammar manually and hope that its coverage is sufficient for multiple applications (Dowding et al., 1994). Another approach is to create a simulation of the intended system (typically with a human in the loop) and then record users interacting with the simulation. The recordings are then transcribed and annotated with semantic information relating to the domain; the transcriptions and annotations can then be used to create a statistical understanding model (Miller et al., 1998) or used as guidance for manual grammar development (Aust et al., 1995). Building mixed initiative spoken language systems currently usually involves the design of semantic representations specific to the application domain. These representations are used to pass data between the language processing components: understanding, dialog, confirmation generation, and response generation. However, such representations tend to be domain-specific, and this makes it difficult to port to new domains or to use machine learning techniques without extensive handlabeling of data with the semantic representations. Furthermore, the use of intermediate semantic representations st</context>
</contexts>
<marker>Aust, Oerder, Seide, Steinbiss, 1995</marker>
<rawString>H. Aust, M. Oerder, F. Seide, and V. Steinbiss. 1995. The Philips automatic train timetable information system. Speech Communication, 17:249{262.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Chu-Carroll</author>
<author>Bob Carpenter</author>
</authors>
<title>Vector-based natural language call routing.</title>
<date>1999</date>
<journal>Computational Linguistic,</journal>
<pages>25--3</pages>
<contexts>
<context position="18504" citStr="Chu-Carroll and Carpenter (1999)" startWordPosition="3092" endWordPosition="3095">wise return to step 2. 7. The action a&apos; is executed, and any string output it produces is read to the user with the speech synthesizer. 8. The active context is set to the result of evaluating the expression c&apos;. 9. Return to step 2. Figure 2 gives an example showing the strings involved in a dialog turn. Handling the user&apos;s verbal response to the confirmation is done with a built-in yes-no context. The generation of confirmation requests requires no work by the application developer. Our approach thus provides an even more extreme version of automatic confirmation generation than that used by Chu-Carroll and Carpenter (1999) where only a small effort is required by the developer. In both cases, the benefits of carefully crafted confirmation requests are being traded for rapid application development. 7 Experiments An important question relating to our method is the effect of the number of examples on system interpretation accuracy. To measure this effect, we chose the operator services call routing task described by Gorin et al. (1997). We chose this task because a reasonably large data set was available in the form of actual recordings of thousands of real customers calling AT&amp;T&apos;s operators, together with transc</context>
</contexts>
<marker>Chu-Carroll, Carpenter, 1999</marker>
<rawString>Jennifer Chu-Carroll and Bob Carpenter. 1999. Vector-based natural language call routing. Computational Linguistic, 25(3):361-388.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Dowding</author>
<author>J M Gawron</author>
<author>D Appelt</author>
<author>J Bear</author>
<author>L Cherny</author>
<author>R Moore</author>
<author>D Moran</author>
</authors>
<title>Gemini: A Natural Language System For Spoken-Language Understanding.</title>
<date>1994</date>
<booktitle>In Proc. ARPA Human Language Technology Workshop &apos;99,</booktitle>
<pages>43--48</pages>
<location>Princeton, NJ.</location>
<contexts>
<context position="1386" citStr="Dowding et al., 1994" startWordPosition="202" endWordPosition="205">ing. We describe how a spoken dialog system is constructed with this approach and also provide some experimental results on varying the number of examples used to build a particular application. 1 Introduction Developing non-trivial interactive spoken language applications currently requires significant effort, often several person-months. A major part of this effort is aimed at coping with variation in the spoken language input by users. One approach to handling variation is to write a large natural language grammar manually and hope that its coverage is sufficient for multiple applications (Dowding et al., 1994). Another approach is to create a simulation of the intended system (typically with a human in the loop) and then record users interacting with the simulation. The recordings are then transcribed and annotated with semantic information relating to the domain; the transcriptions and annotations can then be used to create a statistical understanding model (Miller et al., 1998) or used as guidance for manual grammar development (Aust et al., 1995). Building mixed initiative spoken language systems currently usually involves the design of semantic representations specific to the application domain</context>
</contexts>
<marker>Dowding, Gawron, Appelt, Bear, Cherny, Moore, Moran, 1994</marker>
<rawString>J. Dowding, J. M. Gawron, D. Appelt, J. Bear, L. Cherny, R. Moore, and D. Moran. 1994. Gemini: A Natural Language System For Spoken-Language Understanding. In Proc. ARPA Human Language Technology Workshop &apos;99, pages 43{48, Princeton, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Gorin</author>
<author>G Riccardi</author>
<author>J H Wright</author>
</authors>
<title>How may I help you?</title>
<date>1997</date>
<journal>Speech Communication,</journal>
<pages>23--1</pages>
<contexts>
<context position="18923" citStr="Gorin et al. (1997)" startWordPosition="3160" endWordPosition="3163"> requests requires no work by the application developer. Our approach thus provides an even more extreme version of automatic confirmation generation than that used by Chu-Carroll and Carpenter (1999) where only a small effort is required by the developer. In both cases, the benefits of carefully crafted confirmation requests are being traded for rapid application development. 7 Experiments An important question relating to our method is the effect of the number of examples on system interpretation accuracy. To measure this effect, we chose the operator services call routing task described by Gorin et al. (1997). We chose this task because a reasonably large data set was available in the form of actual recordings of thousands of real customers calling AT&amp;T&apos;s operators, together with transcriptions and manual labeling of the desired call destination. More specifically, we measure the call routing accuracy for unconstrained caller responses to the initial context prompt ATT. How may I help you?. Another advantage of this task was that benchmark call routing accuracy figures were available for systems built with the full data set (Gorin et al., 1997; Schapire and Singer, 2000). We have not yet measured </context>
<context position="21310" citStr="Gorin et al. (1997)" startWordPosition="3562" endWordPosition="3565">to rejection. The dataset consists of 8,844 utterances of which 1000 were held out for testing. We refer to the remaining 7,884 utterances as the &amp;quot;full training dataset&amp;quot;. In the experiments, we vary two conditions: Input uncertainty The input string to the interpretation component is either a human transcription of the spoken utterance or the output of a speech recognizer. The acoustic model used for automatic speech recognition was a general telephone speech HHM model in all cases. (For the full dataset, better results can be achieved by an applicationspecific acoustic model, as presented by Gorin et al. (1997) and confirmed by our results below.) Size of example set We select progressively larger subsets of examples from the full training set, as well as showing results for the full training set itself. We wish to approximate the situation where an application developer uses typical examples for the initial context without knowing the distribution of call types. We therefore select k utterances for each destination, with k set to 3, 5, and 10, respectively. This selection is random, except for the provision that utterances appearing more than once are preferred, to approximate the notion of a typic</context>
</contexts>
<marker>Gorin, Riccardi, Wright, 1997</marker>
<rawString>A.L. Gorin, G. Riccardi, and J.H. Wright. 1997. How may I help you? Speech Communication, 23(1-2):113-127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Levin</author>
<author>R Pieraccini</author>
</authors>
<title>A stochastic model of computer-human interaction for learning dialogue strategies.</title>
<date>1997</date>
<booktitle>In Proceedings of EUROSPEECH97,</booktitle>
<pages>1883--1886</pages>
<location>Rhodes, Greece.</location>
<contexts>
<context position="15440" citStr="Levin and Pieraccini (1997)" startWordPosition="2526" endWordPosition="2529">ork is specified explicitly in advance in the triples by the application developer. For more complex applications, next context expressions c may be calls that evaluate to context identifiers. In our implementation, these calls can be Java methods executed on objects known to the action interpreter. They may thus be calls on the back-end application system, which is appropriate for cases when the back-end has state information relevant to what should happen next (e.g. if it is an &amp;quot;intelligent agent&amp;quot;). It might also be a call to component that implements a dialog strategy learning method (e.g. Levin and Pieraccini (1997)), though we have not yet tried such methods in conjunction with the present system. A confirmation request of the form do you mean e&apos; is constructed for each variant-action pair (v, a&apos;) of an example-action pair (e, a). The string e&apos; is constructed by first computing a submapping h&apos; of the mapping h representing the distortion between e and v. h&apos; is derived from h by removing those edit operations which were not involved in mapping the action a to the adapted action a&apos;. (The matcher is used to compute h except when the process of deriving (v, a&apos;) from (e, a) already includes an explicit repre</context>
</contexts>
<marker>Levin, Pieraccini, 1997</marker>
<rawString>E. Levin and R. Pieraccini. 1997. A stochastic model of computer-human interaction for learning dialogue strategies. In Proceedings of EUROSPEECH97, pages 1883{1886, Rhodes, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Miller</author>
<author>Michael Crystal</author>
<author>Heidi Fox</author>
<author>Lance Ramshaw</author>
<author>Richard Schwartz</author>
<author>Rebecca Stone</author>
<author>Ralph Weischedel</author>
</authors>
<title>and the Annotation Group.</title>
<date>1998</date>
<booktitle>In Proceedings of the Seventh Message Understanding Conference (MUC-7),</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<location>Fairfax, VA.</location>
<contexts>
<context position="1763" citStr="Miller et al., 1998" startWordPosition="264" endWordPosition="267"> at coping with variation in the spoken language input by users. One approach to handling variation is to write a large natural language grammar manually and hope that its coverage is sufficient for multiple applications (Dowding et al., 1994). Another approach is to create a simulation of the intended system (typically with a human in the loop) and then record users interacting with the simulation. The recordings are then transcribed and annotated with semantic information relating to the domain; the transcriptions and annotations can then be used to create a statistical understanding model (Miller et al., 1998) or used as guidance for manual grammar development (Aust et al., 1995). Building mixed initiative spoken language systems currently usually involves the design of semantic representations specific to the application domain. These representations are used to pass data between the language processing components: understanding, dialog, confirmation generation, and response generation. However, such representations tend to be domain-specific, and this makes it difficult to port to new domains or to use machine learning techniques without extensive handlabeling of data with the semantic representa</context>
</contexts>
<marker>Miller, Crystal, Fox, Ramshaw, Schwartz, Stone, Weischedel, 1998</marker>
<rawString>Scott Miller, Michael Crystal, Heidi Fox, Lance Ramshaw, Richard Schwartz, Rebecca Stone, Ralph Weischedel, and the Annotation Group. 1998. Algorithms that learn to extract information — BBN: description of the SIFT system as used for MUC-7. In Proceedings of the Seventh Message Understanding Conference (MUC-7), Fairfax, VA. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
<author>N Tishby</author>
<author>L Lee</author>
</authors>
<title>Distributional clustering of english words.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st meeting of the Association for Computational Linguistics,</booktitle>
<pages>183--190</pages>
<contexts>
<context position="12613" citStr="Pereira et al. (1993)" startWordPosition="2040" endWordPosition="2043">classifiers such as decision trees (Quinlan, 1993) or support vector machines (Vapnik, 1995) could be used instead. Matcher The matcher can compute a distortion mapping and associated distance between the output s of the speech recognizer and a variant v. Various matchers can be used such as those suggested in example-based approaches to machine translation (Sumita and Iida, 1995). So far we have used a weighted string edit distance matcher and experimented with different substitution weights including ones based on measures of statistical similarity between words such as the one described by Pereira et al. (1993). The output of the matcher is a real number (the distance) and a distortion mapping represented as a sequence of edit operations (Wagner and Fischer, 1974). Using these two components, the method for mapping the user&apos;s utterance to an executable action is as follows: 1. The language model derived from context c is activated in the speech recognizer. 2. The speech recognizer produces a string s from the user&apos;s utterance. 3. The classifier for c is applied to s to produce an unadapted action a. 4. The matcher is applied pairwise to compare s with each variant va, derived from a triple (e, a, c&apos;</context>
</contexts>
<marker>Pereira, Tishby, Lee, 1993</marker>
<rawString>F. Pereira, N. Tishby, and L. Lee. 1993. Distributional clustering of english words. In Proceedings of the 31st meeting of the Association for Computational Linguistics, pages 183{190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Quinlan</author>
</authors>
<title>C4.5: Programs for Machine Learning.</title>
<date>1993</date>
<publisher>Morgan Kaufmann,</publisher>
<location>San Mateo, CA.</location>
<contexts>
<context position="12042" citStr="Quinlan, 1993" startWordPosition="1949" endWordPosition="1950">pairs with adapted examples a&apos; since the set of adapted actions may be too large for accurate classification (with standard classification techniques). The classifiers typically use text features such as N-grams appearing in the training data. In our experiments, we have used different classifiers, including BoosTexter (Schapire and Singer, 2000), and a classifier based on Phi-correlation statistics for the text features (see Alshawi and Douglas (2000) for our earlier application of Phi statistics in learning machine translation models from examples). Other classifiers such as decision trees (Quinlan, 1993) or support vector machines (Vapnik, 1995) could be used instead. Matcher The matcher can compute a distortion mapping and associated distance between the output s of the speech recognizer and a variant v. Various matchers can be used such as those suggested in example-based approaches to machine translation (Sumita and Iida, 1995). So far we have used a weighted string edit distance matcher and experimented with different substitution weights including ones based on measures of statistical similarity between words such as the one described by Pereira et al. (1993). The output of the matcher i</context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>J.R. Quinlan. 1993. C4.5: Programs for Machine Learning. Morgan Kaufmann, San Mateo, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert E Schapire</author>
<author>Yoram Singer</author>
</authors>
<title>BoosTexter: A Boosting-based System for Text Categorization.</title>
<date>2000</date>
<booktitle>Machine Learning,</booktitle>
<pages>39--2</pages>
<contexts>
<context position="11776" citStr="Schapire and Singer, 2000" startWordPosition="1906" endWordPosition="1909">o the appropriate (possibly adapted) action: Classifier A classifier is built with training pairs (v, a) where v is a variant of an example e for which the example action pair (e, a) is a member of the unexpanded pairs in the context. Note that the classifier is not trained on pairs with adapted examples a&apos; since the set of adapted actions may be too large for accurate classification (with standard classification techniques). The classifiers typically use text features such as N-grams appearing in the training data. In our experiments, we have used different classifiers, including BoosTexter (Schapire and Singer, 2000), and a classifier based on Phi-correlation statistics for the text features (see Alshawi and Douglas (2000) for our earlier application of Phi statistics in learning machine translation models from examples). Other classifiers such as decision trees (Quinlan, 1993) or support vector machines (Vapnik, 1995) could be used instead. Matcher The matcher can compute a distortion mapping and associated distance between the output s of the speech recognizer and a variant v. Various matchers can be used such as those suggested in example-based approaches to machine translation (Sumita and Iida, 1995).</context>
<context position="19496" citStr="Schapire and Singer, 2000" startWordPosition="3257" endWordPosition="3260">es call routing task described by Gorin et al. (1997). We chose this task because a reasonably large data set was available in the form of actual recordings of thousands of real customers calling AT&amp;T&apos;s operators, together with transcriptions and manual labeling of the desired call destination. More specifically, we measure the call routing accuracy for unconstrained caller responses to the initial context prompt ATT. How may I help you?. Another advantage of this task was that benchmark call routing accuracy figures were available for systems built with the full data set (Gorin et al., 1997; Schapire and Singer, 2000). We have not yet measured interpretation accuracy for the structurally more complex e-mail access application. In this experiment, the responses to How may I help you? are &amp;quot;routed&amp;quot; to fifteen destinations, where routing means handing off the call to another system or human operator, or moving to another example-action context that will interact further with the user to elicit further information so that a subtask (such as making a collect call) can be completed. Thus the actions in the initial context are simply the destinations, i.e. a = a&apos;, and the matcher is only used to compute e&apos;. The fi</context>
<context position="22297" citStr="Schapire and Singer, 2000" startWordPosition="3726" endWordPosition="3729">e therefore select k utterances for each destination, with k set to 3, 5, and 10, respectively. This selection is random, except for the provision that utterances appearing more than once are preferred, to approximate the notion of a typical utterance. The selected examples are expanded by the addition of variants, as described earlier. For each value of k, the results shown are for the median of three runs. Figure 3 shows the routing accuracy ROC curves for transcribed input for k = 3, 5, 10 and for the full training dataset. These results for transcribed input were obtained with BoosTexter (Schapire and Singer, 2000) as the classifier module in our system because we have observed that BoosTexter generally outperforms our Phi classifier (mentioned earlier) for text input. Figure 4 shows the corresponding four ROC curves for recognition output, and an additional fifth graph (the top one) showing the improvement that is obtained with a domain specific acoustic model coupled with a trigram language model. These results for recognition output were obtained with the Phi classifier module rather than BoosTexter; the Phi classifier performance is generally the same as, or slightly better than, BoosTexter when app</context>
</contexts>
<marker>Schapire, Singer, 2000</marker>
<rawString>Robert E. Schapire and Yoram Singer. 2000. BoosTexter: A Boosting-based System for Text Categorization. Machine Learning, 39(2/3):135-168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eiichiro Sumita</author>
<author>Hitoshi Iida</author>
</authors>
<title>Heterogeneous computing for example-based translation of spoken language.</title>
<date>1995</date>
<booktitle>In Proceedings of the 6th International Conference on Theoretical and Methodological Issues in Machine Translation,</booktitle>
<pages>273--286</pages>
<location>Leuven, Belgium.</location>
<contexts>
<context position="12375" citStr="Sumita and Iida, 1995" startWordPosition="2002" endWordPosition="2005">apire and Singer, 2000), and a classifier based on Phi-correlation statistics for the text features (see Alshawi and Douglas (2000) for our earlier application of Phi statistics in learning machine translation models from examples). Other classifiers such as decision trees (Quinlan, 1993) or support vector machines (Vapnik, 1995) could be used instead. Matcher The matcher can compute a distortion mapping and associated distance between the output s of the speech recognizer and a variant v. Various matchers can be used such as those suggested in example-based approaches to machine translation (Sumita and Iida, 1995). So far we have used a weighted string edit distance matcher and experimented with different substitution weights including ones based on measures of statistical similarity between words such as the one described by Pereira et al. (1993). The output of the matcher is a real number (the distance) and a distortion mapping represented as a sequence of edit operations (Wagner and Fischer, 1974). Using these two components, the method for mapping the user&apos;s utterance to an executable action is as follows: 1. The language model derived from context c is activated in the speech recognizer. 2. The sp</context>
</contexts>
<marker>Sumita, Iida, 1995</marker>
<rawString>Eiichiro Sumita and Hitoshi Iida. 1995. Heterogeneous computing for example-based translation of spoken language. In Proceedings of the 6th International Conference on Theoretical and Methodological Issues in Machine Translation, pages 273{286, Leuven, Belgium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V N Vapnik</author>
</authors>
<title>The Nature of Statistical Learning Theory.</title>
<date>1995</date>
<publisher>Springer,</publisher>
<location>New York.</location>
<contexts>
<context position="12084" citStr="Vapnik, 1995" startWordPosition="1955" endWordPosition="1956">t of adapted actions may be too large for accurate classification (with standard classification techniques). The classifiers typically use text features such as N-grams appearing in the training data. In our experiments, we have used different classifiers, including BoosTexter (Schapire and Singer, 2000), and a classifier based on Phi-correlation statistics for the text features (see Alshawi and Douglas (2000) for our earlier application of Phi statistics in learning machine translation models from examples). Other classifiers such as decision trees (Quinlan, 1993) or support vector machines (Vapnik, 1995) could be used instead. Matcher The matcher can compute a distortion mapping and associated distance between the output s of the speech recognizer and a variant v. Various matchers can be used such as those suggested in example-based approaches to machine translation (Sumita and Iida, 1995). So far we have used a weighted string edit distance matcher and experimented with different substitution weights including ones based on measures of statistical similarity between words such as the one described by Pereira et al. (1993). The output of the matcher is a real number (the distance) and a disto</context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>V.N. Vapnik. 1995. The Nature of Statistical Learning Theory. Springer, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert A Wagner</author>
<author>Michael J Fischer</author>
</authors>
<title>The String-to-String Correction Problem.</title>
<date>1974</date>
<journal>Journal of the Association for Computing Machinery,</journal>
<pages>21--1</pages>
<contexts>
<context position="10283" citStr="Wagner and Fischer, 1974" startWordPosition="1651" endWordPosition="1655">domains, or data or heuristics from which such a mapping can be learned automatically. Examples of the token mapping are names to email addresses as illustrated in the example above, name to identifier pairs in a database system, &amp;quot;soundex&amp;quot; phonetic string spelling in directory applications, and a bilingual dictionary in a translation application. The process proceeds as follows: 1. Compute a set of lexical mappings between the variant v and example e. This is currently performed by aligning the two string in such a way as that the alignment minimizes the (weighted) edit distance between them (Wagner and Fischer, 1974). 2. The token mapping t is used to map substitution pairs identified by the alignment ((read, show) and (John, Bill Wilson) in the example above) to corresponding substitution pairs in the action string. In general this will result in a smaller set of substitution strings since not all word strings will be present in the domain of t. (In the example, this results in the single pair (jsmith@att.com, wwilson@att.com).) 3. The action substitution pairs are applied to a to produce a&apos;. 4. The resulting action a&apos; is checked for (syntactic) well-formedness in the action string domain; the variant v </context>
<context position="12769" citStr="Wagner and Fischer, 1974" startWordPosition="2067" endWordPosition="2070">stortion mapping and associated distance between the output s of the speech recognizer and a variant v. Various matchers can be used such as those suggested in example-based approaches to machine translation (Sumita and Iida, 1995). So far we have used a weighted string edit distance matcher and experimented with different substitution weights including ones based on measures of statistical similarity between words such as the one described by Pereira et al. (1993). The output of the matcher is a real number (the distance) and a distortion mapping represented as a sequence of edit operations (Wagner and Fischer, 1974). Using these two components, the method for mapping the user&apos;s utterance to an executable action is as follows: 1. The language model derived from context c is activated in the speech recognizer. 2. The speech recognizer produces a string s from the user&apos;s utterance. 3. The classifier for c is applied to s to produce an unadapted action a. 4. The matcher is applied pairwise to compare s with each variant va, derived from a triple (e, a, c&apos;) in the unexpanded version of c. 5. The triple (v, a&apos;, c&apos;) for which v produces the smallest distance is selected and passed along with e to the dialog con</context>
</contexts>
<marker>Wagner, Fischer, 1974</marker>
<rawString>Robert A. Wagner and Michael J. Fischer. 1974. The String-to-String Correction Problem. Journal of the Association for Computing Machinery, 21(1):168-173, January.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>