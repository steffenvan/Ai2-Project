<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001573">
<title confidence="0.995302">
Experiments on Summary-based Opinion Classification
</title>
<author confidence="0.996577">
Elena Lloret
</author>
<affiliation confidence="0.8008848">
Department of Software
and Computing Systems
University of Alicante
Apdo. de Correos 99
E-03080, Alicante, Spain
</affiliation>
<email confidence="0.99632">
elloret@dlsi.ua.es
</email>
<author confidence="0.95377">
Horacio Saggion
</author>
<affiliation confidence="0.775381">
Department of Infomation and
Communication Technologies
</affiliation>
<address confidence="0.774576">
Grupo TALN
Universitat Pompeu Fabra
C/T´anger, 122-134, 2nd floor
08018 Barcelona, Spain
</address>
<email confidence="0.999399">
horacio.saggion@upf.edu
</email>
<author confidence="0.993854">
Manuel Palomar
</author>
<affiliation confidence="0.800478">
Department of Software
and Computing Systems
University of Alicante
Apdo. de Correos 99
E-03080, Alicante, Spain
</affiliation>
<email confidence="0.997765">
mpalomar@dlsi.ua.es
</email>
<sectionHeader confidence="0.993865" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999862529411765">
We investigate the effect of text summarisa-
tion in the problem of rating-inference – the
task of associating a fine-grained numerical
rating to an opinionated document. We set-up
a comparison framework to study the effect of
different summarisation algorithms of various
compression rates in this task and compare the
classification accuracy of summaries and doc-
uments for associating documents to classes.
We make use of SVM algorithms to associate
numerical ratings to opinionated documents.
The algorithms are informed by linguistic and
sentiment-based features computed from full
documents and summaries. Preliminary re-
sults show that some types of summaries could
be as effective or better as full documents in
this problem.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999870448979592">
Public opinion has a great impact on company and
government decision making. In particular, compa-
nies have to constantly monitor public perception of
their products, services, and key company represen-
tatives to ensure that good reputation is maintained.
Recent cases of public figures making headlines for
the wrong reasons have shown how companies take
into account public opinion to distance themselves
from figures which can damage their public image.
The Web has become an important source for find-
ing information, in the field of business intelligence,
business analysts are turning their eyes to the Web
in order to monitor public perception on products,
services, policies, and managers. The field of senti-
ment analysis has recently emerged (Pang and Lee,
2008) as an important area of research in Natural
Language Processing (NLP) which can provide vi-
able solutions for monitoring public perception on
a number of issues; with evaluation programs such
as the Text REtrieval Conference track on blog min-
ing 1, the Text Analysis Conference 2 track on opin-
ion summarisation, and the DEfi Fouille de Textes
program (Grouin et al., 2009) advances in the state
of the art have been produced. Although sentiment
analysis involves various different problems such as
identifying subjective sentences or identifying posi-
tive and negative opinions in text, here we concen-
trate on the opinion classification task; and more
specifically on rating-inference, the task of identify-
ing the author’s evaluation of an entity with respect
to an ordinal-scale based on the author’s textual eval-
uation of the entity (Pang and Lee, 2005). The spe-
cific problem we study in this paper is that of as-
sociating a fine-grained rating (1=worst,...5=best)
to a review. This is in general considered a dif-
ficult problem because of the fuzziness inherent of
mid-range ratings (Mukras et al., 2007). A consid-
erable body of research has recently been produced
to tackle this problem (Chakraborti et al., 2007; Fer-
rari et al., 2009) and reported figures showing accu-
racies ranging from 30% to 50% for such complex
task; most approaches derive features for the classi-
fication task from the full document. In this research
we ask whether extracting features from document
summaries could help a classification system. Since
text summaries are meant to contain the essential
content of a document (Mani, 2001), we investigate
whether filtering noise through text summarisation
is of any help in the rating-inference task. In re-
</bodyText>
<footnote confidence="0.9993415">
1http:trec.nist.gov/
2http://www.nist.gov/tac/
</footnote>
<page confidence="0.939979">
107
</page>
<note confidence="0.990674">
Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 107–115,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999726583333333">
cent years, text summarisation has been used to sup-
port both manual and automatic tasks; in the SUM-
MAC evaluation (Mani et al., 1998), text summaries
were tested in document classification and ques-
tion answering tasks where summaries were consid-
ered suitable surrogates for full documents; Bagga
and Baldwin (1998) studied summarisation in the
context of a cross-document coreference task and
found that summaries improved the performance of
a clustering-based coreference mechanism; more re-
cently Latif and McGee (2009) have proposed text
summarisation as a preprocessing step for student
essay assessment finding that summaries could be
used instead of full essays to group “similar” qual-
ity essays. Summarisation has been studied in the
field of sentiment analysis with the objective of pro-
ducing opinion summaries, however, to the best of
our knowlegde there has been little research on the
study of document summarisation as a text pro-
cessing step for opinion classification. This paper
presents a framework and extensive experiments on
text summarisation for opinion classification, and in
particular, for the rating-inference problem. We will
present results indicating that some types of sum-
maries could be as effective or better than the full
documents in this task.
The remainder of the paper is organised as fol-
lows: Section 2 will compile the existing work with
respect to the inference-rating problem; Section 3
and Section 4 will describe the corpus and the NLP
tools used for all the experimental set-up. Next, the
text summarisation approaches will be described in
Section 5, and then Section 6 will show the exper-
iments conducted and the results obtained together
with a discussion. Finally, we will draw some con-
clusions and address further work in Section 7.
</bodyText>
<sectionHeader confidence="0.999766" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999905701754386">
Most of the literature regarding sentiment analysis
addresses the problem either by detecting and clas-
sifying opinions at a sentence level (Wilson et al.,
2005; Du and Tan, 2009), or by attempting to cap-
ture the overall sentiment of a document (McDonald
et al., 2007; Hu et al., 2008). Traditional approaches
tackle the task as binary classification, where text
units (e.g. words, sentences, fragments) are classi-
fied into positive vs. negative, or subjective vs. ob-
jective, according to their polarity and subjectivity
degree, respectively. However, sentiment classifica-
tion taking into account a finer granularity has been
less considered. Rating-inference is a particular task
within sentiment analysis, which aims at inferring
the author’s numerical rating for a review. For in-
stance, given a review and 5-star-rating scale (rang-
ing from 1 -the worst- to 5 -the best), this task should
correctly predict the review’s rating, based on the
language and sentiment expressed in its content.
In (Pang and Lee, 2005), the rating-inference
problem is analysed for the movies domain. In
particular, the utility of employing label and item
similarity is shown by analysing the performance
of three different methods based on SVM (one vs.
all, regression and metric labeling), in order to infer
the author’s implied numerical rating, which ranges
from 1 up to 4 stars, depending on the degree the au-
thor of the review liked or not the film. The approach
described in (Leung et al., 2006) suggests the use of
collaborative filtering algorithms together with sen-
timent analysis techniques to obtain user preferences
expressed in textual reviews, focusing also on movie
reviews. Once the opinion words from user reviews
have been identified, the polarity of those opinion
words together with their strength need to be com-
puted and mapped to the rating scales to be further
input to the collaborative input algorithms.
Apart from these approaches, this problem is
stated from a different point of view in (Shimada
and Endo, 2008). Here it is approached from the
perspective of rating different details of a product
under the same review. Consequently, they rename
the problem as “seeing several stars” instead of only
one, corresponding to the overall sentiment of the
review. Also, in (Baccianella et al., 2009) the rating
of different features regarding hotel reviews (cleanli-
ness, location, staff, etc.) is addressed by analysing
several aspects involved in the generation of prod-
uct review’s representations, such as part-of-speech
and lexicons. Other approaches (Devitt and Ahmad,
2007), (Turney, 2002) face this problem by group-
ing documents with closer stars under the same cat-
egory, i.e. positive or negative, simplifying the task
into a binary classification problem.
Recently, due to the vast amount of on-line infor-
mation and the subjectivity appearing in documents,
the combination of sentiment analysis and summari-
</bodyText>
<page confidence="0.998552">
108
</page>
<bodyText confidence="0.999917647058824">
sation task in tandem can result in great benefits
for stand-alone applications of sentiment analysis,
as well as for the potential uses of sentiment analy-
sis as part of other NLP applications (Stoyanov and
Cardie, 2006). Whilst there is much literature com-
bining sentiment analysis and text summarisation
focusing on generating opinion-oriented summaries
for the new textual genres, such as blogs (Lloret
et al., 2009), or reviews (Zhuang et al., 2006), the
use of summaries as substitutes of full documents in
tasks such as rating-inference has been not yet ex-
plored to the best of our knowledge. In contrast to
the existing literature, this paper uses summaries in-
stead of full reviews to tackle the rating-inference
task in the financial domain, and we carry out a pre-
liminary analysis concerning the potential benefits
of text summaries for this task.
</bodyText>
<sectionHeader confidence="0.995472" genericHeader="method">
3 Dataset for the Rating-inference Task
</sectionHeader>
<bodyText confidence="0.999893263157895">
Since there is no standard dataset for carrying out
the rating-inference task, the corpus used for our ex-
periments was one associated to a current project on
business intelligence we are working on. These data
consisted of 89 reviews of several English banks
(Abbey, Barcalys, Halifax, HSBC, Lloyds TSB, and
National Westminster) gathered from the Internet. In
particular the documents were collected from Ciao3,
a Website where users can write reviews about dif-
ferent products and services, depending on their own
experience.
Table 1 lists some of the statistical properties of
the data. It is worth stressing upon the fact that
the reviews have on average 2,603 words, which
means that we are dealing with long documents
rather than short ones, making the rating-inference
task even more challenging. The shortest document
contains 1,491 words, whereas the longest document
has more than 5,000 words.
</bodyText>
<figure confidence="0.343826">
# Reviews Avg length Max length Min length
89 2,603 5,730 1,491
</figure>
<tableCaption confidence="0.992364">
Table 1: Corpus Statistics
</tableCaption>
<bodyText confidence="0.999855333333333">
Since the aim of the task we are pursuing focuses
on classifying correctly the star for a review (rang-
ing from 1 to 5 stars), it is necessary to study how
</bodyText>
<footnote confidence="0.681219">
3http://www.ciao.co.uk/
</footnote>
<bodyText confidence="0.999961">
many reviews we have for each class, in order to see
whether we have a balanced distribution or not. Ta-
ble 2 shows this numbers for each star-rating. It is
worth mentioning that one-third of the reviews be-
long to the 4-star class. In contrast, we have only 9
reviews that have been rated as 3-star, consisting of
the 10% of the corpus, which is a very low number.
</bodyText>
<equation confidence="0.947598166666667">
Star-rating # reviews %
1-star 17 19
2-star 11 12
3-star 9 10
4-star 28 32
5-star 24 27
</equation>
<tableCaption confidence="0.877453">
Table 2: Class Distribution
</tableCaption>
<sectionHeader confidence="0.980852" genericHeader="method">
4 Natural Language Processing Tools
</sectionHeader>
<bodyText confidence="0.99877345">
Linguistic analysis of textual input is carried out
using the General Architecture for Text Engineer-
ing (GATE) – a framework for the development and
deployment of language processing technology in
large scale (Cunningham et al., 2002). We make use
of typical GATE components: tokenisation, parts of
speech tagging, and morphological analysis to pro-
duce document annotations. From the annotations
we produce a number of features for document rep-
resentation. Features produced from the annotations
are: string – the original, unmodified text of each
token; root – the lemmatised, lower-case form of
the token; category – the part-of-speech (POS) tag, a
symbol that represents a grammatical category such
as determiner, present-tense verb, past-tense verb,
singular noun, etc.; orth – a code representing the to-
ken’s combination of upper- and lower-case letters.
In addition to these basic features, “sentiment” fea-
tures based on a lexical resource are computed as
explained below.
</bodyText>
<subsectionHeader confidence="0.999159">
4.1 Sentiment Features
</subsectionHeader>
<bodyText confidence="0.999911777777778">
SentiWordNet (Esuli and Sebastiani, 2006) is a lexi-
cal resource in which each synset (set of synonyms)
of WordNet (Fellbaum, 1998) is associated with
three numerical scores obj (how objective the word
is), pos (how positive the word is), and neg (how
negative the word is). Each of the scores ranges
from 0 to 1, and their sum equals 1. SentiWord-
Net word values have been semi-automatically com-
puted based on the use of weakly supervised classi-
</bodyText>
<page confidence="0.99578">
109
</page>
<bodyText confidence="0.999987842105263">
fication algorithms. In this work we compute the
“general sentiment” of a word in the following way:
given a word w we compute the number of times the
word w is more positive than negative (positive &gt;
negative), the number of times is more negative than
positive (positive &lt; negative) and the total number
of entries of word w in SentiWordNet, therefore we
can consider the overall positivity or negativity a
particular word has in SentiWordNet. We are in-
terested in words that are generally “positive”, gen-
erally “negative” or generally “neutral” (not much
variation between positive and negative). For exam-
ple a word such as “good” has many more entries
where the positive score is greater than the nega-
tivity score while a word such as “unhelpful” has
more negative occurrences than positive. We use this
aggregated scores in our classification experiments.
Note that we do not apply any word sense disam-
biguation procedure here.
</bodyText>
<subsectionHeader confidence="0.984717">
4.2 Machine Learning Tool
</subsectionHeader>
<bodyText confidence="0.999964666666667">
For the experiments reported here, we adopt a Sup-
port Vector Machine (SVM) learning paradigm not
only because it has recently been used with suc-
cess in different tasks in natural language processing
(Isozaki and Kazawa, 2002), but it has been shown
particularly suitable for text categorization (Kumar
and Gopal, 2009) where the feature space is huge, as
it is in our case. We rely on the support vector ma-
chines implementation distributed with the GATE
system (Li et al., 2009) which hides from the user
the complexities of feature extraction and conver-
sion from documents to the machine learning imple-
mentation. The tool has been applied with success
to a number of datasets for opinion classification and
rating-inference (Saggion and Funk, 2009).
</bodyText>
<sectionHeader confidence="0.994071" genericHeader="method">
5 Text Summarisation Approach
</sectionHeader>
<bodyText confidence="0.9998606">
In this Section, three approaches for carrying out the
summarisation process are explained in detail. First,
a generic approach is taken as a basis, and then, it is
adapted into a query-focused and a opinion-oriented
approach, respectively.
</bodyText>
<subsectionHeader confidence="0.902879">
5.1 Generic Summarisation
</subsectionHeader>
<bodyText confidence="0.999978925">
A generic text summarisation approach is first taken
as a core, in which three main stages can be distin-
guished: i) document preprocessing; ii) relevance
detection; and ii) summary generation. Since we
work with Web documents, an initial preprocessing
step is essential to remove all unnecessary tags and
noisy information. Therefore, in the first stage the
body of the review out of the whole Web page is
automatically delimitated by means of patterns, and
only this text is used as the input for the next sum-
marisation stages. Further on, a sentence relevance
detection process is carried out employing different
combinations of various techniques. In particular,
the techniques employed are:
Term frequency (tf): this technique has been
widely used in different summarisation approaches,
showing the the most frequent words in a document
contain relevant information and can be indicative of
the document’s topic (Nenkova et al., 2006)
Textual entailment (te): a te module (Ferr´andez
et al., 2007) is used to detect redundant information
in the document, by computing the entailment be-
tween two consecutive sentences and discarding the
entailed ones. The identification of these entailment
relations helps to avoid incorporating redundant in-
formation in summaries.
Code quantity principle (cqp): this is a linguis-
tic principle which proves the existence of a propor-
tional relation between how important the informa-
tion is, and the number of coding elements it has
(Giv´on, 1990). In this approach we assume that sen-
tences containing longer noun-phrases are more rel-
evant.
The aforementioned techniques are combined
together taking always into account the term-
frequency, leading to different summarisation strate-
gies (tf, te+tf, cqp+tf, te+cqp+tf). Finally, the re-
sulting summary is produced by extracting the high-
est scored sentences up to the desired length, accord-
ing the techniques explained.
</bodyText>
<subsectionHeader confidence="0.979716">
5.2 Query-focused Summarisation
</subsectionHeader>
<bodyText confidence="0.999816555555555">
Through adapting the generic summarisation ap-
proach into a query-focused one, we could benefit
from obtaining more specific sentences with regard
to the topic of the review. As a preliminary work, we
are going to assume that a review is about a bank,
and as a consequence, the name of the bank is con-
sidered to be the topic. It is worth mentioning that a
person can refer to a specific bank in different ways.
For example, in the case of “The National Westmin-
</bodyText>
<page confidence="0.989533">
110
</page>
<bodyText confidence="0.999977925925926">
ster Bank”, it can be referred to as “National West-
minster” or “NatWest”. Such different denomina-
tions were manually identified and they were used
to biased the content of the generated summaries,
employing the same techniques of tf, te and the cqp
combined together. One limitation of this approach
is that we do not directly deal with the coreference
problem, so for example, sentences containing pro-
nouns referring also to the bank, will not be taken
into consideration in the summarisation process. We
are aware of this limitation and for future work it
would be necessary to run a coreference algorithm
to identify all occurrences of a bank within a review.
However, since the main goal of this paper is to carry
out a preliminary analysis of the usefulness of sum-
maries in contrast to whole reviews in the rating-
inference problem, we did not take this problem into
account at this stage of the research. In addition,
when we do query-focused summarisation only we
rely on the SUMMA toolkit (Saggion, 2008) to pro-
duce a query similarity value for each sentence in the
review which in turn is used to rank sentences for an
extractive summary (qf). This similarity value is the
cosine similarity between a sentence vector (terms
and weights) and a query vector (terms and weigths)
and where the query is the name of the entity being
reviewed (e.g. National Westminster).
</bodyText>
<subsectionHeader confidence="0.880299">
5.3 Opinion-oriented Summarisation
</subsectionHeader>
<bodyText confidence="0.9999835">
Since reviews are written by people who want to
express their opinion and experience with regard
to a bank, in this particular case, either generic or
query-focused summaries can miss including some
important information concerning their sentiments
and feelings towards this particular entity. There-
fore, a sentiment classification system similar to the
one used in (Balahur-Dobrescu et al., 2009) is used
together with the summarisation approach, in order
to generate opinion-oriented summaries. First of all,
the sentences containing opinions are identified, as-
signing each of them a polarity (positive and neg-
ative) and a numerical value corresponding to the
polarity strength (the higher the negative score, the
more negative the sentence and similarly, the higher
the positive score, the more positive the sentence).
Sentences containing a polarity value of 0 are con-
sidered neutral and are not taken into account. Once
the sentences are classified into positives, negatives
and neutrals, they are grouped together according
to its type. Further on, the same combination of
techniques as for previously explained summarisa-
tion approaches are then used.
Additionally, a summary containing only the most
positive and negative sentences is also generated (we
have called this type of summaries sent) in order to
check whether the polarity strength on its own could
be a relevant feature for the summarisation process.
</bodyText>
<sectionHeader confidence="0.994702" genericHeader="evaluation">
6 Evaluation Environment
</sectionHeader>
<bodyText confidence="0.999600625">
In this Section we are going to describe in detail all
the experimental set-up. Firstly, we will explain the
corpus we used together with some figures regard-
ing some statistics computed. Secondly, we will de-
scribe in-depth all the experiments we ran and the re-
sults obtained. Finally, an extensive discussion will
be given in order to analyse all the results and draw
some conclusions.
</bodyText>
<subsectionHeader confidence="0.984012">
6.1 Experiments and Results
</subsectionHeader>
<bodyText confidence="0.999056916666667">
The main objective of the paper is to investigate the
influence of summaries in contrast to full reviews for
the rating-inference problem.
The purpose of the experiments is to analyse the
performance of the different suggested text sum-
marisation approaches and compare them to the per-
formance of the full review. Therefore, the experi-
ments conducted were the following: for each pro-
posed summarisation approach, we experimented
with five different types of compression rates for
summaries (ranging from 10% to 50%). Apart from
the full review, we dealt with 14 different sum-
marisation approaches (4 for generic, 5 for query-
focused and 5 for opinion-oriented summarisation),
as well as 2 baselines (lead and final, taking the first
or the last sentences according to a specific compres-
sion rate, respectively). Each experiment consisted
of predicting the correct star of a review, either with
the review as a whole or with one of the summari-
sation approaches. As we previously said in Sec-
tion 4, for predicting the correct star-rating, we used
machine learning techniques. In particular, differ-
ent features were used to train a SVM classifier with
10-fold cross validation4, using the whole review:
</bodyText>
<footnote confidence="0.999751">
4The classifier used was the one integrated within the GATE
framework: http://gate.ac.uk/
</footnote>
<page confidence="0.998713">
111
</page>
<bodyText confidence="0.99932875">
the root of each word, its category, and the calcu-
lated value employing the SentiWordNet lexicon, as
well as their combinations. As a baseline for the full
document we took into account a totally uninformed
approach with respect to the class with higher num-
ber of reviews, i.e. considering all documents as if
they were scored with 4 stars. The different results
according different features can be seen in Table 3.
</bodyText>
<table confidence="0.996555888888889">
Feature FQ=1
baseline 0.300
root 0.378
category 0.367
sentiWN 0.333
root+category 0.356
root+sentiWN 0.333
category+sentiWN 0.389
root+category+sentiWN 0.413
</table>
<tableCaption confidence="0.974749">
Table 3: F-measure results using the full review for clas-
sification
</tableCaption>
<bodyText confidence="0.999785636363636">
Regarding the features for training the summaries,
it is worth mentioning that the best performing fea-
ture when no sentiment-based features are taken into
account is the one using the root of the words. Con-
sequently, this feature was used to train the sum-
maries. Moreover, since the best results using the
full review were obtained using the combination of
the all the features (root+category+sentiWN), we
also selected this combination to train the SVM
classifier with our summaries. Conducting both
experiments, we could analyse to what extent the
sentiment-based feature benefit the classification
process.
The results obtained are shown in Table 4 and
Table 5, respectively. These tables show the F-
measure value obtained for the classification task,
when features extracted from summaries are used
instead from the full review. On the one hand,
results using the root feature extracted from sum-
maries can be seen in Table 4. On the other hand,
Table 5 shows the results when the combination
of all the linguistic and sentiment-based features
(root+category+sentiWN), that has been extracted
from summaries, are used for training the SVM clas-
sifier.
We also performed two statistical tests in order
to measure the significance for the results obtained.
The tests we performed were the one-way Analy-
sis of Variance (ANOVA) and the t-test (Spiegel and
Castellan, 1998). Given a group of experiments, we
first run ANOVA for analysing the difference be-
tween their means. In case some differences are
found, we run the t-test between those pairs.
</bodyText>
<subsectionHeader confidence="0.99512">
6.2 Discussion
</subsectionHeader>
<bodyText confidence="0.999875047619048">
A first analysis derived from the results obtained in
Table 3 makes us be aware of the difficulty associ-
ated to the rating-inference task. As can be seen,
a baseline without any information from the docu-
ment at all, is performing around 30%, which com-
pared to the remaining approaches is not a very bad
number. However, we assumed that dealing with
some information contained in documents, the clas-
sification algorithm will do better in finding the cor-
rect star associated to a review. This was the rea-
son why we experimented with different features
alone or in combination. From these experiments,
we obtained that the combination of linguistic and
semantic-based features leads to the best results, ob-
taining a F-measure value of 41%. If sentiment-
based features are not taken into account, the best
feature is the root of the word on its own. Further-
more, in order to analyse further combinations, we
ran some experiments with bigrams. However, the
results obtained did not improve the ones we already
had, so they are not reported in this paper.
As far as the results is concerned comparing the
use of summaries to the full document, it is worth
mentioning that when using specific summarisation
approaches, such as query-focused summaries com-
bined with term-frequency, we get better results than
using the full document with a 90% confidence in-
terval, according to a t-test. In particular, qf for 10%
is significant with respect to the full document, us-
ing only root as feature for training. For the results
regarding the combination of root, category and Sen-
tiWordNet, qf for 10% and qf+tf for 10% and 20%
are significant with respect to the full document.
Concerning the different summarisation ap-
proaches, it cannot be claimed a general tendency
about which ones may lead to the best results. We
also performed some significance tests between dif-
ferent strategies, and in most of the cases, the t-
test and the ANOVA did not report significance
over 95%. Only a few approaches were significant
at a 95% confidence level, for instance, te+cqp+tf
and sent+te+cqp+tf with respect to sent+cqp+tf
</bodyText>
<page confidence="0.99364">
112
</page>
<table confidence="0.999947555555555">
Approach Compression Rate
Summarisation method 10% 20% 30% 40% 50%
lead Fβ=1 0.411 0.378 0.367 0.311 0.322
final Fβ=1 0.322 0.389 0.300 0.467 0.456
tf Fβ=1 0.400 0.344 0.400 0.367 0.367
te+tf Fβ=1 0.367 0.422 0.411 0.389 0.322
cqp+tf Fβ=1 0.300 0.344 0.311 0.300 0.256
te+cqp+tf Fβ=1 0.422 0.356 0.333 0.300 0.322
qf Fβ=1 0.513 0.388 0.375 0.363 0.363
qf+tf Fβ=1 0.567 0.467 0.311 0.367 0.389
qf+te+tf Fβ=1 0.389 0.367 0.411 0.378 0.333
qf+cqp+tf Fβ=1 0.300 0.356 0.378 0.378 0.333
qf+te+cqp+tf Fβ=1 0.322 0.322 0.367 0.367 0.356
sent Fβ=1 0.344 0.380 0.391 0.290 0.336
sent+tf Fβ=1 0.378 0.425 0.446 0.303 0.337
sent+te+tf Fβ=1 0.278 0.424 0.313 0.369 0.347
sent+cqp+tf Fβ=1 0.333 0.300 0.358 0.358 0.324
sent+te+cqp+tf Fβ=1 0.446 0.334 0.358 0.292 0.369
</table>
<tableCaption confidence="0.956340333333333">
Table 4: Classification results (F-measure) for summaries using root (lead = first sentences; final = last sentences;
tf = term frequency; te = textual entailment; cqp = code quantity principle with noun-phrases; qf = query-focused
summaries; and sent = opinion-oriented summaries)
</tableCaption>
<bodyText confidence="0.999234666666667">
for 10%; sent+tf in comparison to sent+cqp+tf
for 20%; or sent with respect to cqp+tf for 40%
and 50% compression rates. Other examples of
the approaches that were significant at a 90%
level of confidence are qf for 10% with respect to
sent+te+cqp+tf. Due to the wide range of summari-
sation strategies tested in the experiments, the results
obtained vary a lot and, due to the space limitations,
it is not possible to report all the tables. What it
seems to be clear from the results is that the code
quantity principle (see Section 5) is not contributing
much to the summarisation process, thus obtaining
poor results when it is employed. Intuitively, this
can be due to the fact that after the first mention of
the bank, there is a predominant use of pronouns,
and as a consequence, the accuracy of the tool that
identifies noun-phrases could be affected. The same
reason could be affecting the term-frequency calcu-
lus, as it is computed based on the lemmas of the
words, not taking into account the pronouns that re-
fer also to them.
</bodyText>
<sectionHeader confidence="0.969407" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999992735294118">
This paper presented a preliminary study of
inference-rating task. We have proposed here a new
framework for comparison and extrinsic evaluation
of summaries in a text-based classification task. In
our research, text summaries generated using differ-
ent strategies were used for training a SVM classifier
instead of full reviews. The aim of this task was to
correctly predict the category of a review within a 1
to 5 star-scale. For the experiments, we gathered 89
bank reviews from the Internet and we generated 16
summaries of 5 different compression rates for each
of them (80 different summaries for each review,
having generated in total 7,120 summaries). We also
experimented with several linguistic and sentiment-
based features for the classifier. Although the re-
sults obtained are not significant enough to state
that summaries really help the rating-inference task,
we have shown that in some cases the use of sum-
maries (e.g. query/entity-focused summaries) could
offer competitive advantage over the use of full doc-
uments and we have also shown that some summari-
sation techniques do not degrade the performance of
a rating-inference algorithm when compared to the
use of full documents. We strongly believe that this
preliminary study could serve as a starting point for
future developments.
Although we have carried out extensive experi-
mentation with different summarisation techniques,
compression rates, and document/summary features,
there are many issues that we have not explored. In
the future, we plan to investigate whether the re-
sults could be affected by the class distribution of
the reviews, and in this line we would like to see the
distribution of the documents using clustering tech-
</bodyText>
<page confidence="0.996709">
113
</page>
<table confidence="0.999918944444445">
Approach Compression Rate
Summarisation method 10% 20% 30% 40% 50%
lead Fβ=1 0.275 0.422 0.422 0.378 0.322
final Fβ=1 0.275 0.378 0.333 0.344 0.400
tf Fβ=1 0.411 0.422 0.411 0.378 0.378
te+tf Fβ=1 0.411 0.344 0.344 0.344 0.378
cqp+tf Fβ=1 0.358 0.267 0.333 0.222 0.289
te+cqp+tf Fβ=1 0.444 0.411 0.411 0.311 0.322
qf Fβ=1 0.563 0.488 0.400 0.375 0.350
qf+tf Fβ=1 0.444 0.411 0.433 0.367 0.356
qf+te+tf Fβ=1 0.322 0.367 0.356 0.344 0.344
qf+cqp+tf Fβ=1 0.292 0.322 0.367 0.333 0.356
qf+te+cqp+tf Fβ=1 0.356 0.378 0.356 0.367 0.356
sent Fβ=1 0.322 0.370 0.379 0.412 0.414
sent+tf Fβ=1 0.378 0.446 0.359 0.380 0.402
sent+te+tf Fβ=1 0.333 0.414 0.404 0.380 0.381
sent+cqp+tf Fβ=1 0.300 0.333 0.347 0.358 0.296
sent+te+cqp+tf Fβ=1 0.436 0.413 0.425 0.359 0.324
</table>
<tableCaption confidence="0.989541666666667">
Table 5: Classification results (F-measure) for summaries using root, category and SentiWordNet (lead = first sen-
tences; final = last sentences; tf = term frequency; te = textual entailment; cqp = code quantity principle with
noun-phrases; qf = query-focused summaries; and sent = opinion-oriented summaries)
</tableCaption>
<bodyText confidence="0.9999292">
niques. Moreover, we would also like to investigate
what it would happen if we consider the values of the
star-rating scale as ordinal numbers, and not only as
labels for categories. We will replicate the exper-
iments presented here using as evaluation measure
the “mean square error” which has been pinpointed
as a more appropriate measure for categorisation in
an ordinal scale. Finally, in the medium to long-
term we plan to extent the experiments and analy-
sis to other available datasets in different domains,
such as movie or book reviews, in order to see if
the results could be influenced by the nature of the
corpus, allowing also further results for comparison
with other approaches and assessing the difficulty of
the task from a perspective of different domains.
</bodyText>
<sectionHeader confidence="0.997499" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998764818181818">
This research has been supported by the project PROM-
ETEO “Desarrollo de T´ecnicas Inteligentes e Interacti-
vas de Mineria de Textos” (2009/119) from the Valencian
Government. Moreover, Elena Lloret is funded by the
FPI program (BES-2007-16268) from the Spanish Min-
istry of Science and Innovation under the project TEXT-
MESS (TIN2006-15265-C06-01), and Horacio Saggion
is supported by a Ram´on y Cajal Fellowship from the
Ministry of Science and Innovation, Spain. The authors
would also like to thank Alexandra Balahur for helping to
process the dataset with her Opinion Mining approach.
</bodyText>
<sectionHeader confidence="0.998944" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999742733333333">
S. Baccianella, A. Esuli, and F. Sebastiani. 2009. Multi-
facet Rating of Product Reviews. In Proceedings of
the 31th European Conference on IR Research on Ad-
vances in Information Retrieval, pages 461–472.
A. Bagga and B. Baldwin. 1998. Entity-Based Cross-
Document Coreferencing Using the Vector Space
Model. In Proceedings of the COLING-ACL, pages
79–85.
A. Balahur-Dobrescu, M. Kabadjov, J. Steinberger,
R. Steinberger, and A. Montoyo. 2009. Summarizing
Opinions in Blog Threads. In Proceedings of the Pa-
cific Asia Conference on Language, INformation and
Computation Conference, pages 606–613.
S. Chakraborti, R. Mukras, R. Lothian, N. Wiratunga,
S. Watt, and D Harper. 2007. Supervised Latent Se-
mantic Indexing using Adaptive Sprinkling. In Pro-
ceedings ofIJCAI-07, pages 1582–1587.
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. 2002. GATE: A Framework and Graphi-
cal Development Environment for Robust NLP Tools
and Applications. In Proceedings of the ACL.
A. Devitt and K. Ahmad. 2007. Sentiment Polarity Iden-
tification in Financial News: A Cohesion-based Ap-
proach. In Proceedings of the ACL, pages 984–991.
W. Du and S. Tan. 2009. An Iterative Reinforcement
Approach for Fine-Grained Opinion Mining. In Pro-
ceedings of the NAACL, pages 486–493.
A. Esuli and F. Sebastiani. 2006. SENTIWORDNET: A
Publicly Available Lexical Resource for Opinion Min-
ing. In Proceedings ofLREC, pages 417–422.
</reference>
<page confidence="0.994817">
114
</page>
<reference confidence="0.997261345794393">
C. Fellbaum. 1998. WordNet: An Electronical Lexical
Database. The MIT Press, Cambridge, MA.
O. Ferr´andez, D. Micol, R. Mu˜noz, and M. Palomar.
2007. A Perspective-Based Approach for Solving Tex-
tual Entailment Recognition. In Proceedings of the
ACL-PASCAL Workshop on Textual Entailment and
Paraphrasing, pages 66–71, June.
S. Ferrari, T. Charnois, Y. Mathet, F. Rioult, and
D. Legallois. 2009. Analyse de Discours ´Evaluatif,
Mod`ele Linguistique et Applications. In Fouille de
donn´ees d’opinion, volume E-17, pages 71–93.
T. Giv´on, 1990. Syntax: A functional-typological intro-
duction, II. John Benjamins.
C. Grouin, M. Hurault-Plantet, P. Paroubek, and J. B.
Berthelin. 2009. DEFT’07 : Une Campagne
d’Avaluation en Fouille d’Opinion. In Fouille de
donn´ees d’opinion, volume E-17, pages 1–24.
Y. Hu, W. Li, and Q. Lu. 2008. Developing Evalua-
tion Model of Topical Term for Document-Level Sen-
timent Classification. In Proceedings of the 10th Pa-
cific Rim International Conference on Artificial Intel-
ligence, pages 175–186.
H. Isozaki and H. Kazawa. 2002. Efficient Support
Vector Classifiers for Named Entity Recognition. In
Proceedings of the 19th International Conference on
Computational Linguistics, pages 390–396.
M. A. Kumar and M. Gopal. 2009. Text Categorization
Using Fuzzy Proximal SVM and Distributional Clus-
tering of Words. In Proceedings of the 13th Pacific-
Asia Conference on Advances in Knowledge Discovery
and Data Mining, pages 52–61.
S. Latif and M. McGee Wood. 2009. A Novel Technique
for Automated Linguistic Quality Assessment of Stu-
dents’ Essays Using Automatic Summarizers. Com-
puter Science and Information Engineering, World
Congress on, 5:144–148.
C. W. K. Leung, S. C. F. Chan, and F. L. Chung.
2006. Integrating Collaborative Filtering and Sen-
timent Analysis: A Rating Inference Approach. In
Proceedings of The ECAI 2006 Workshop on Recom-
mender Systems, pages 62–66.
Y. Li, K. Bontcheva, and H. Cunningham. 2009. Adapt-
ing SVM for Data Sparseness and Imbalance: A Case
Study in Information Extraction. Natural Language
Engineering, 15(2):241–271.
E. Lloret, A. Balahur, M. Palomar, and A. Montoyo.
2009. Towards Building a Competitive Opinion Sum-
marization System: Challenges and Keys. In Proceed-
ings of the NAACL. Student Research Workshop and
Doctoral Consortium, pages 72–77.
I. Mani, D. House, G. Klein, L. Hirshman, L. Obrst,
T. Firmin, M. Chrzanowski, and B. Sundheim. 1998.
The TIPSTER SUMMAC Text Summarization Evalu-
ation. Technical report, The Mitre Corporation.
I. Mani. 2001. Automatic Text Summarization. John
Benjamins Publishing Company.
R. McDonald, K. Hannan, T. Neylon, M. Wells, and
J. Reynar. 2007. Structured Models for Fine-to-
Coarse Sentiment Analysis. In Proceedings of the
ACL, pages 432–439.
R. Mukras, N. Wiratunga, R. Lothian, S. Chakraborti, and
D. Harper. 2007. Information Gain Feature Selection
for Ordinal Text Classification using Probability Re-
distribution. In Proceedings of the Textlink workshop
at IJCAI-07.
A. Nenkova, L. Vanderwende, and K. McKeown. 2006.
A Compositional Context Sensitive Multi-document
Summarizer: Exploring the Factors that Influence
Summarization. In Proceedings of the ACM SIGIR
conference on Research and development in informa-
tion retrieval, pages 573–580.
B. Pang and L. Lee. 2005. Seeing Stars: Exploiting
Class Relationships for Sentiment Categorization with
Respect to Rating Scales. In Proceedings of the ACL,
pages 115–124.
B. Pang and L. Lee. 2008. Opinion Mining and Senti-
ment Analysis. Foundations and Trends in Informa-
tion Retrieval, 2(1-2):1–135.
H. Saggion and A. Funk. 2009. Extracting Opinions and
Facts for Business Intelligence. RNTI, E-17:119–146.
H. Saggion. 2008. SUMMA: A Robust and Adapt-
able Summarization Tool. Traitement Automatique
des Languages, 49:103–125.
K. Shimada and T. Endo. 2008. Seeing Several Stars: A
Rating Inference Task for a Document Containing Sev-
eral Evaluation Criteria. In Proceedings of the 12th
Pacific-Asia Conference on Advances in Knowledge
Discovery and Data Mining, pages 1006–1014.
S. Spiegel and N. J. Castellan, Jr. 1998. Nonparametric
Statistics for the Behavioral Sciences. McGraw-Hill
International.
V. Stoyanov and C. Cardie. 2006. Toward Opinion Sum-
marization: Linking the Sources. In Proceedings of
the Workshop on Sentiment and Subjectivity in Text,
pages 9–14.
P. D. Turney. 2002. Thumbs Up or Thumbs Down?: Se-
mantic Orientation Applied to Unsupervised Classifi-
cation of Reviews. In Proceedings of the ACL, pages
417–424.
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recog-
nizing Contextual Polarity in Phrase-level Sentiment
Analysis. In Proceedings of the EMNLP, pages 347–
354.
L. Zhuang, F. Jing, and X. Y. Zhu. 2006. Movie Re-
view Mining and Summarization. In Proceedings of
the 15th ACM international conference on Information
and knowledge management, pages 43–50.
</reference>
<page confidence="0.998511">
115
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.419285">
<title confidence="0.999824">Experiments on Summary-based Opinion Classification</title>
<author confidence="0.993886">Elena</author>
<affiliation confidence="0.996232">Department of and Computing University of Apdo. de Correos</affiliation>
<address confidence="0.9938">E-03080, Alicante,</address>
<email confidence="0.986302">elloret@dlsi.ua.es</email>
<author confidence="0.976988">Horacio</author>
<affiliation confidence="0.991715">Department of Infomation</affiliation>
<title confidence="0.551545">Communication</title>
<author confidence="0.510157">Grupo</author>
<affiliation confidence="0.999122">Universitat Pompeu</affiliation>
<address confidence="0.9929225">C/T´anger, 122-134, 2nd 08018 Barcelona,</address>
<email confidence="0.999914">horacio.saggion@upf.edu</email>
<author confidence="0.98427">Manuel</author>
<affiliation confidence="0.995731">Department of and Computing University of Apdo. de Correos</affiliation>
<address confidence="0.979392">E-03080, Alicante,</address>
<email confidence="0.995501">mpalomar@dlsi.ua.es</email>
<abstract confidence="0.9991175">We investigate the effect of text summarisain the problem of the task of associating a fine-grained numerical rating to an opinionated document. We set-up a comparison framework to study the effect of different summarisation algorithms of various compression rates in this task and compare the classification accuracy of summaries and documents for associating documents to classes. We make use of SVM algorithms to associate numerical ratings to opinionated documents. The algorithms are informed by linguistic and sentiment-based features computed from full documents and summaries. Preliminary results show that some types of summaries could be as effective or better as full documents in this problem.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Baccianella</author>
<author>A Esuli</author>
<author>F Sebastiani</author>
</authors>
<title>Multifacet Rating of Product Reviews.</title>
<date>2009</date>
<booktitle>In Proceedings of the 31th European Conference on IR Research on Advances in Information Retrieval,</booktitle>
<pages>461--472</pages>
<contexts>
<context position="8144" citStr="Baccianella et al., 2009" startWordPosition="1258" endWordPosition="1261">e opinion words from user reviews have been identified, the polarity of those opinion words together with their strength need to be computed and mapped to the rating scales to be further input to the collaborative input algorithms. Apart from these approaches, this problem is stated from a different point of view in (Shimada and Endo, 2008). Here it is approached from the perspective of rating different details of a product under the same review. Consequently, they rename the problem as “seeing several stars” instead of only one, corresponding to the overall sentiment of the review. Also, in (Baccianella et al., 2009) the rating of different features regarding hotel reviews (cleanliness, location, staff, etc.) is addressed by analysing several aspects involved in the generation of product review’s representations, such as part-of-speech and lexicons. Other approaches (Devitt and Ahmad, 2007), (Turney, 2002) face this problem by grouping documents with closer stars under the same category, i.e. positive or negative, simplifying the task into a binary classification problem. Recently, due to the vast amount of on-line information and the subjectivity appearing in documents, the combination of sentiment analy</context>
</contexts>
<marker>Baccianella, Esuli, Sebastiani, 2009</marker>
<rawString>S. Baccianella, A. Esuli, and F. Sebastiani. 2009. Multifacet Rating of Product Reviews. In Proceedings of the 31th European Conference on IR Research on Advances in Information Retrieval, pages 461–472.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bagga</author>
<author>B Baldwin</author>
</authors>
<title>Entity-Based CrossDocument Coreferencing Using the Vector Space Model.</title>
<date>1998</date>
<booktitle>In Proceedings of the COLING-ACL,</booktitle>
<pages>79--85</pages>
<contexts>
<context position="4363" citStr="Bagga and Baldwin (1998)" startWordPosition="655" endWordPosition="658"> help in the rating-inference task. In re1http:trec.nist.gov/ 2http://www.nist.gov/tac/ 107 Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 107–115, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics cent years, text summarisation has been used to support both manual and automatic tasks; in the SUMMAC evaluation (Mani et al., 1998), text summaries were tested in document classification and question answering tasks where summaries were considered suitable surrogates for full documents; Bagga and Baldwin (1998) studied summarisation in the context of a cross-document coreference task and found that summaries improved the performance of a clustering-based coreference mechanism; more recently Latif and McGee (2009) have proposed text summarisation as a preprocessing step for student essay assessment finding that summaries could be used instead of full essays to group “similar” quality essays. Summarisation has been studied in the field of sentiment analysis with the objective of producing opinion summaries, however, to the best of our knowlegde there has been little research on the study of document s</context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>A. Bagga and B. Baldwin. 1998. Entity-Based CrossDocument Coreferencing Using the Vector Space Model. In Proceedings of the COLING-ACL, pages 79–85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Balahur-Dobrescu</author>
<author>M Kabadjov</author>
<author>J Steinberger</author>
<author>R Steinberger</author>
<author>A Montoyo</author>
</authors>
<title>Summarizing Opinions in Blog Threads.</title>
<date>2009</date>
<booktitle>In Proceedings of the Pacific Asia Conference on Language, INformation and Computation Conference,</booktitle>
<pages>606--613</pages>
<contexts>
<context position="19007" citStr="Balahur-Dobrescu et al., 2009" startWordPosition="3017" endWordPosition="3020"> the cosine similarity between a sentence vector (terms and weights) and a query vector (terms and weigths) and where the query is the name of the entity being reviewed (e.g. National Westminster). 5.3 Opinion-oriented Summarisation Since reviews are written by people who want to express their opinion and experience with regard to a bank, in this particular case, either generic or query-focused summaries can miss including some important information concerning their sentiments and feelings towards this particular entity. Therefore, a sentiment classification system similar to the one used in (Balahur-Dobrescu et al., 2009) is used together with the summarisation approach, in order to generate opinion-oriented summaries. First of all, the sentences containing opinions are identified, assigning each of them a polarity (positive and negative) and a numerical value corresponding to the polarity strength (the higher the negative score, the more negative the sentence and similarly, the higher the positive score, the more positive the sentence). Sentences containing a polarity value of 0 are considered neutral and are not taken into account. Once the sentences are classified into positives, negatives and neutrals, the</context>
</contexts>
<marker>Balahur-Dobrescu, Kabadjov, Steinberger, Steinberger, Montoyo, 2009</marker>
<rawString>A. Balahur-Dobrescu, M. Kabadjov, J. Steinberger, R. Steinberger, and A. Montoyo. 2009. Summarizing Opinions in Blog Threads. In Proceedings of the Pacific Asia Conference on Language, INformation and Computation Conference, pages 606–613.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Chakraborti</author>
<author>R Mukras</author>
<author>R Lothian</author>
<author>N Wiratunga</author>
<author>S Watt</author>
<author>D Harper</author>
</authors>
<title>Supervised Latent Semantic Indexing using Adaptive Sprinkling.</title>
<date>2007</date>
<booktitle>In Proceedings ofIJCAI-07,</booktitle>
<pages>1582--1587</pages>
<contexts>
<context position="3265" citStr="Chakraborti et al., 2007" startWordPosition="490" endWordPosition="493">e we concentrate on the opinion classification task; and more specifically on rating-inference, the task of identifying the author’s evaluation of an entity with respect to an ordinal-scale based on the author’s textual evaluation of the entity (Pang and Lee, 2005). The specific problem we study in this paper is that of associating a fine-grained rating (1=worst,...5=best) to a review. This is in general considered a difficult problem because of the fuzziness inherent of mid-range ratings (Mukras et al., 2007). A considerable body of research has recently been produced to tackle this problem (Chakraborti et al., 2007; Ferrari et al., 2009) and reported figures showing accuracies ranging from 30% to 50% for such complex task; most approaches derive features for the classification task from the full document. In this research we ask whether extracting features from document summaries could help a classification system. Since text summaries are meant to contain the essential content of a document (Mani, 2001), we investigate whether filtering noise through text summarisation is of any help in the rating-inference task. In re1http:trec.nist.gov/ 2http://www.nist.gov/tac/ 107 Proceedings of the NAACL HLT 2010 </context>
</contexts>
<marker>Chakraborti, Mukras, Lothian, Wiratunga, Watt, Harper, 2007</marker>
<rawString>S. Chakraborti, R. Mukras, R. Lothian, N. Wiratunga, S. Watt, and D Harper. 2007. Supervised Latent Semantic Indexing using Adaptive Sprinkling. In Proceedings ofIJCAI-07, pages 1582–1587.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Cunningham</author>
<author>D Maynard</author>
<author>K Bontcheva</author>
<author>V Tablan</author>
</authors>
<title>GATE: A Framework and Graphical Development Environment for Robust NLP Tools and Applications.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="11583" citStr="Cunningham et al., 2002" startWordPosition="1819" endWordPosition="1822">s for each star-rating. It is worth mentioning that one-third of the reviews belong to the 4-star class. In contrast, we have only 9 reviews that have been rated as 3-star, consisting of the 10% of the corpus, which is a very low number. Star-rating # reviews % 1-star 17 19 2-star 11 12 3-star 9 10 4-star 28 32 5-star 24 27 Table 2: Class Distribution 4 Natural Language Processing Tools Linguistic analysis of textual input is carried out using the General Architecture for Text Engineering (GATE) – a framework for the development and deployment of language processing technology in large scale (Cunningham et al., 2002). We make use of typical GATE components: tokenisation, parts of speech tagging, and morphological analysis to produce document annotations. From the annotations we produce a number of features for document representation. Features produced from the annotations are: string – the original, unmodified text of each token; root – the lemmatised, lower-case form of the token; category – the part-of-speech (POS) tag, a symbol that represents a grammatical category such as determiner, present-tense verb, past-tense verb, singular noun, etc.; orth – a code representing the token’s combination of upper</context>
</contexts>
<marker>Cunningham, Maynard, Bontcheva, Tablan, 2002</marker>
<rawString>H. Cunningham, D. Maynard, K. Bontcheva, and V. Tablan. 2002. GATE: A Framework and Graphical Development Environment for Robust NLP Tools and Applications. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Devitt</author>
<author>K Ahmad</author>
</authors>
<title>Sentiment Polarity Identification in Financial News: A Cohesion-based Approach.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>984--991</pages>
<contexts>
<context position="8423" citStr="Devitt and Ahmad, 2007" startWordPosition="1297" endWordPosition="1300">ed from a different point of view in (Shimada and Endo, 2008). Here it is approached from the perspective of rating different details of a product under the same review. Consequently, they rename the problem as “seeing several stars” instead of only one, corresponding to the overall sentiment of the review. Also, in (Baccianella et al., 2009) the rating of different features regarding hotel reviews (cleanliness, location, staff, etc.) is addressed by analysing several aspects involved in the generation of product review’s representations, such as part-of-speech and lexicons. Other approaches (Devitt and Ahmad, 2007), (Turney, 2002) face this problem by grouping documents with closer stars under the same category, i.e. positive or negative, simplifying the task into a binary classification problem. Recently, due to the vast amount of on-line information and the subjectivity appearing in documents, the combination of sentiment analysis and summari108 sation task in tandem can result in great benefits for stand-alone applications of sentiment analysis, as well as for the potential uses of sentiment analysis as part of other NLP applications (Stoyanov and Cardie, 2006). Whilst there is much literature combin</context>
</contexts>
<marker>Devitt, Ahmad, 2007</marker>
<rawString>A. Devitt and K. Ahmad. 2007. Sentiment Polarity Identification in Financial News: A Cohesion-based Approach. In Proceedings of the ACL, pages 984–991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Du</author>
<author>S Tan</author>
</authors>
<title>An Iterative Reinforcement Approach for Fine-Grained Opinion Mining.</title>
<date>2009</date>
<booktitle>In Proceedings of the NAACL,</booktitle>
<pages>486--493</pages>
<contexts>
<context position="6020" citStr="Du and Tan, 2009" startWordPosition="919" endWordPosition="922"> existing work with respect to the inference-rating problem; Section 3 and Section 4 will describe the corpus and the NLP tools used for all the experimental set-up. Next, the text summarisation approaches will be described in Section 5, and then Section 6 will show the experiments conducted and the results obtained together with a discussion. Finally, we will draw some conclusions and address further work in Section 7. 2 Related Work Most of the literature regarding sentiment analysis addresses the problem either by detecting and classifying opinions at a sentence level (Wilson et al., 2005; Du and Tan, 2009), or by attempting to capture the overall sentiment of a document (McDonald et al., 2007; Hu et al., 2008). Traditional approaches tackle the task as binary classification, where text units (e.g. words, sentences, fragments) are classified into positive vs. negative, or subjective vs. objective, according to their polarity and subjectivity degree, respectively. However, sentiment classification taking into account a finer granularity has been less considered. Rating-inference is a particular task within sentiment analysis, which aims at inferring the author’s numerical rating for a review. For</context>
</contexts>
<marker>Du, Tan, 2009</marker>
<rawString>W. Du and S. Tan. 2009. An Iterative Reinforcement Approach for Fine-Grained Opinion Mining. In Proceedings of the NAACL, pages 486–493.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Esuli</author>
<author>F Sebastiani</author>
</authors>
<title>SENTIWORDNET: A Publicly Available Lexical Resource for Opinion Mining.</title>
<date>2006</date>
<booktitle>In Proceedings ofLREC,</booktitle>
<pages>417--422</pages>
<contexts>
<context position="12392" citStr="Esuli and Sebastiani, 2006" startWordPosition="1940" endWordPosition="1943">of features for document representation. Features produced from the annotations are: string – the original, unmodified text of each token; root – the lemmatised, lower-case form of the token; category – the part-of-speech (POS) tag, a symbol that represents a grammatical category such as determiner, present-tense verb, past-tense verb, singular noun, etc.; orth – a code representing the token’s combination of upper- and lower-case letters. In addition to these basic features, “sentiment” features based on a lexical resource are computed as explained below. 4.1 Sentiment Features SentiWordNet (Esuli and Sebastiani, 2006) is a lexical resource in which each synset (set of synonyms) of WordNet (Fellbaum, 1998) is associated with three numerical scores obj (how objective the word is), pos (how positive the word is), and neg (how negative the word is). Each of the scores ranges from 0 to 1, and their sum equals 1. SentiWordNet word values have been semi-automatically computed based on the use of weakly supervised classi109 fication algorithms. In this work we compute the “general sentiment” of a word in the following way: given a word w we compute the number of times the word w is more positive than negative (pos</context>
</contexts>
<marker>Esuli, Sebastiani, 2006</marker>
<rawString>A. Esuli and F. Sebastiani. 2006. SENTIWORDNET: A Publicly Available Lexical Resource for Opinion Mining. In Proceedings ofLREC, pages 417–422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet: An Electronical Lexical Database.</title>
<date>1998</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="12481" citStr="Fellbaum, 1998" startWordPosition="1958" endWordPosition="1959">ginal, unmodified text of each token; root – the lemmatised, lower-case form of the token; category – the part-of-speech (POS) tag, a symbol that represents a grammatical category such as determiner, present-tense verb, past-tense verb, singular noun, etc.; orth – a code representing the token’s combination of upper- and lower-case letters. In addition to these basic features, “sentiment” features based on a lexical resource are computed as explained below. 4.1 Sentiment Features SentiWordNet (Esuli and Sebastiani, 2006) is a lexical resource in which each synset (set of synonyms) of WordNet (Fellbaum, 1998) is associated with three numerical scores obj (how objective the word is), pos (how positive the word is), and neg (how negative the word is). Each of the scores ranges from 0 to 1, and their sum equals 1. SentiWordNet word values have been semi-automatically computed based on the use of weakly supervised classi109 fication algorithms. In this work we compute the “general sentiment” of a word in the following way: given a word w we compute the number of times the word w is more positive than negative (positive &gt; negative), the number of times is more negative than positive (positive &lt; negativ</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>C. Fellbaum. 1998. WordNet: An Electronical Lexical Database. The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Ferr´andez</author>
<author>D Micol</author>
<author>R Mu˜noz</author>
<author>M Palomar</author>
</authors>
<title>A Perspective-Based Approach for Solving Textual Entailment Recognition.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing,</booktitle>
<pages>66--71</pages>
<marker>Ferr´andez, Micol, Mu˜noz, Palomar, 2007</marker>
<rawString>O. Ferr´andez, D. Micol, R. Mu˜noz, and M. Palomar. 2007. A Perspective-Based Approach for Solving Textual Entailment Recognition. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, pages 66–71, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Ferrari</author>
<author>T Charnois</author>
<author>Y Mathet</author>
<author>F Rioult</author>
<author>D Legallois</author>
</authors>
<title>Analyse de Discours ´Evaluatif, Mod`ele Linguistique et Applications. In Fouille de donn´ees d’opinion, volume E-17,</title>
<date>2009</date>
<pages>71--93</pages>
<contexts>
<context position="3288" citStr="Ferrari et al., 2009" startWordPosition="494" endWordPosition="498">inion classification task; and more specifically on rating-inference, the task of identifying the author’s evaluation of an entity with respect to an ordinal-scale based on the author’s textual evaluation of the entity (Pang and Lee, 2005). The specific problem we study in this paper is that of associating a fine-grained rating (1=worst,...5=best) to a review. This is in general considered a difficult problem because of the fuzziness inherent of mid-range ratings (Mukras et al., 2007). A considerable body of research has recently been produced to tackle this problem (Chakraborti et al., 2007; Ferrari et al., 2009) and reported figures showing accuracies ranging from 30% to 50% for such complex task; most approaches derive features for the classification task from the full document. In this research we ask whether extracting features from document summaries could help a classification system. Since text summaries are meant to contain the essential content of a document (Mani, 2001), we investigate whether filtering noise through text summarisation is of any help in the rating-inference task. In re1http:trec.nist.gov/ 2http://www.nist.gov/tac/ 107 Proceedings of the NAACL HLT 2010 Workshop on Computation</context>
</contexts>
<marker>Ferrari, Charnois, Mathet, Rioult, Legallois, 2009</marker>
<rawString>S. Ferrari, T. Charnois, Y. Mathet, F. Rioult, and D. Legallois. 2009. Analyse de Discours ´Evaluatif, Mod`ele Linguistique et Applications. In Fouille de donn´ees d’opinion, volume E-17, pages 71–93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Giv´on</author>
</authors>
<title>Syntax: A functional-typological introduction,</title>
<date>1990</date>
<publisher>John Benjamins.</publisher>
<location>II.</location>
<marker>Giv´on, 1990</marker>
<rawString>T. Giv´on, 1990. Syntax: A functional-typological introduction, II. John Benjamins.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Grouin</author>
<author>M Hurault-Plantet</author>
<author>P Paroubek</author>
<author>J B Berthelin</author>
</authors>
<title>DEFT’07 : Une Campagne d’Avaluation en Fouille d’Opinion. In Fouille de donn´ees d’opinion, volume E-17,</title>
<date>2009</date>
<pages>1--24</pages>
<contexts>
<context position="2424" citStr="Grouin et al., 2009" startWordPosition="355" endWordPosition="358">n the field of business intelligence, business analysts are turning their eyes to the Web in order to monitor public perception on products, services, policies, and managers. The field of sentiment analysis has recently emerged (Pang and Lee, 2008) as an important area of research in Natural Language Processing (NLP) which can provide viable solutions for monitoring public perception on a number of issues; with evaluation programs such as the Text REtrieval Conference track on blog mining 1, the Text Analysis Conference 2 track on opinion summarisation, and the DEfi Fouille de Textes program (Grouin et al., 2009) advances in the state of the art have been produced. Although sentiment analysis involves various different problems such as identifying subjective sentences or identifying positive and negative opinions in text, here we concentrate on the opinion classification task; and more specifically on rating-inference, the task of identifying the author’s evaluation of an entity with respect to an ordinal-scale based on the author’s textual evaluation of the entity (Pang and Lee, 2005). The specific problem we study in this paper is that of associating a fine-grained rating (1=worst,...5=best) to a re</context>
</contexts>
<marker>Grouin, Hurault-Plantet, Paroubek, Berthelin, 2009</marker>
<rawString>C. Grouin, M. Hurault-Plantet, P. Paroubek, and J. B. Berthelin. 2009. DEFT’07 : Une Campagne d’Avaluation en Fouille d’Opinion. In Fouille de donn´ees d’opinion, volume E-17, pages 1–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Hu</author>
<author>W Li</author>
<author>Q Lu</author>
</authors>
<title>Developing Evaluation Model of Topical Term for Document-Level Sentiment Classification.</title>
<date>2008</date>
<booktitle>In Proceedings of the 10th Pacific Rim International Conference on Artificial Intelligence,</booktitle>
<pages>175--186</pages>
<contexts>
<context position="6126" citStr="Hu et al., 2008" startWordPosition="939" endWordPosition="942">us and the NLP tools used for all the experimental set-up. Next, the text summarisation approaches will be described in Section 5, and then Section 6 will show the experiments conducted and the results obtained together with a discussion. Finally, we will draw some conclusions and address further work in Section 7. 2 Related Work Most of the literature regarding sentiment analysis addresses the problem either by detecting and classifying opinions at a sentence level (Wilson et al., 2005; Du and Tan, 2009), or by attempting to capture the overall sentiment of a document (McDonald et al., 2007; Hu et al., 2008). Traditional approaches tackle the task as binary classification, where text units (e.g. words, sentences, fragments) are classified into positive vs. negative, or subjective vs. objective, according to their polarity and subjectivity degree, respectively. However, sentiment classification taking into account a finer granularity has been less considered. Rating-inference is a particular task within sentiment analysis, which aims at inferring the author’s numerical rating for a review. For instance, given a review and 5-star-rating scale (ranging from 1 -the worst- to 5 -the best), this task s</context>
</contexts>
<marker>Hu, Li, Lu, 2008</marker>
<rawString>Y. Hu, W. Li, and Q. Lu. 2008. Developing Evaluation Model of Topical Term for Document-Level Sentiment Classification. In Proceedings of the 10th Pacific Rim International Conference on Artificial Intelligence, pages 175–186.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Isozaki</author>
<author>H Kazawa</author>
</authors>
<title>Efficient Support Vector Classifiers for Named Entity Recognition.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Computational Linguistics,</booktitle>
<pages>390--396</pages>
<contexts>
<context position="13983" citStr="Isozaki and Kazawa, 2002" startWordPosition="2210" endWordPosition="2213">ariation between positive and negative). For example a word such as “good” has many more entries where the positive score is greater than the negativity score while a word such as “unhelpful” has more negative occurrences than positive. We use this aggregated scores in our classification experiments. Note that we do not apply any word sense disambiguation procedure here. 4.2 Machine Learning Tool For the experiments reported here, we adopt a Support Vector Machine (SVM) learning paradigm not only because it has recently been used with success in different tasks in natural language processing (Isozaki and Kazawa, 2002), but it has been shown particularly suitable for text categorization (Kumar and Gopal, 2009) where the feature space is huge, as it is in our case. We rely on the support vector machines implementation distributed with the GATE system (Li et al., 2009) which hides from the user the complexities of feature extraction and conversion from documents to the machine learning implementation. The tool has been applied with success to a number of datasets for opinion classification and rating-inference (Saggion and Funk, 2009). 5 Text Summarisation Approach In this Section, three approaches for carryi</context>
</contexts>
<marker>Isozaki, Kazawa, 2002</marker>
<rawString>H. Isozaki and H. Kazawa. 2002. Efficient Support Vector Classifiers for Named Entity Recognition. In Proceedings of the 19th International Conference on Computational Linguistics, pages 390–396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Kumar</author>
<author>M Gopal</author>
</authors>
<title>Text Categorization Using Fuzzy Proximal SVM and Distributional Clustering of Words.</title>
<date>2009</date>
<booktitle>In Proceedings of the 13th PacificAsia Conference on Advances in Knowledge Discovery and Data Mining,</booktitle>
<pages>52--61</pages>
<contexts>
<context position="14076" citStr="Kumar and Gopal, 2009" startWordPosition="2224" endWordPosition="2227"> where the positive score is greater than the negativity score while a word such as “unhelpful” has more negative occurrences than positive. We use this aggregated scores in our classification experiments. Note that we do not apply any word sense disambiguation procedure here. 4.2 Machine Learning Tool For the experiments reported here, we adopt a Support Vector Machine (SVM) learning paradigm not only because it has recently been used with success in different tasks in natural language processing (Isozaki and Kazawa, 2002), but it has been shown particularly suitable for text categorization (Kumar and Gopal, 2009) where the feature space is huge, as it is in our case. We rely on the support vector machines implementation distributed with the GATE system (Li et al., 2009) which hides from the user the complexities of feature extraction and conversion from documents to the machine learning implementation. The tool has been applied with success to a number of datasets for opinion classification and rating-inference (Saggion and Funk, 2009). 5 Text Summarisation Approach In this Section, three approaches for carrying out the summarisation process are explained in detail. First, a generic approach is taken </context>
</contexts>
<marker>Kumar, Gopal, 2009</marker>
<rawString>M. A. Kumar and M. Gopal. 2009. Text Categorization Using Fuzzy Proximal SVM and Distributional Clustering of Words. In Proceedings of the 13th PacificAsia Conference on Advances in Knowledge Discovery and Data Mining, pages 52–61.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Latif</author>
<author>M McGee Wood</author>
</authors>
<title>A Novel Technique for Automated Linguistic Quality Assessment of Students’ Essays Using Automatic Summarizers.</title>
<date>2009</date>
<booktitle>Computer Science and Information Engineering, World Congress on,</booktitle>
<pages>5--144</pages>
<marker>Latif, Wood, 2009</marker>
<rawString>S. Latif and M. McGee Wood. 2009. A Novel Technique for Automated Linguistic Quality Assessment of Students’ Essays Using Automatic Summarizers. Computer Science and Information Engineering, World Congress on, 5:144–148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C W K Leung</author>
<author>S C F Chan</author>
<author>F L Chung</author>
</authors>
<title>Integrating Collaborative Filtering and Sentiment Analysis: A Rating Inference Approach.</title>
<date>2006</date>
<booktitle>In Proceedings of The ECAI 2006 Workshop on Recommender Systems,</booktitle>
<pages>62--66</pages>
<contexts>
<context position="7323" citStr="Leung et al., 2006" startWordPosition="1128" endWordPosition="1131">the best), this task should correctly predict the review’s rating, based on the language and sentiment expressed in its content. In (Pang and Lee, 2005), the rating-inference problem is analysed for the movies domain. In particular, the utility of employing label and item similarity is shown by analysing the performance of three different methods based on SVM (one vs. all, regression and metric labeling), in order to infer the author’s implied numerical rating, which ranges from 1 up to 4 stars, depending on the degree the author of the review liked or not the film. The approach described in (Leung et al., 2006) suggests the use of collaborative filtering algorithms together with sentiment analysis techniques to obtain user preferences expressed in textual reviews, focusing also on movie reviews. Once the opinion words from user reviews have been identified, the polarity of those opinion words together with their strength need to be computed and mapped to the rating scales to be further input to the collaborative input algorithms. Apart from these approaches, this problem is stated from a different point of view in (Shimada and Endo, 2008). Here it is approached from the perspective of rating differe</context>
</contexts>
<marker>Leung, Chan, Chung, 2006</marker>
<rawString>C. W. K. Leung, S. C. F. Chan, and F. L. Chung. 2006. Integrating Collaborative Filtering and Sentiment Analysis: A Rating Inference Approach. In Proceedings of The ECAI 2006 Workshop on Recommender Systems, pages 62–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Li</author>
<author>K Bontcheva</author>
<author>H Cunningham</author>
</authors>
<title>Adapting SVM for Data Sparseness and Imbalance: A Case Study in Information Extraction.</title>
<date>2009</date>
<journal>Natural Language Engineering,</journal>
<volume>15</volume>
<issue>2</issue>
<contexts>
<context position="14236" citStr="Li et al., 2009" startWordPosition="2254" endWordPosition="2257">ores in our classification experiments. Note that we do not apply any word sense disambiguation procedure here. 4.2 Machine Learning Tool For the experiments reported here, we adopt a Support Vector Machine (SVM) learning paradigm not only because it has recently been used with success in different tasks in natural language processing (Isozaki and Kazawa, 2002), but it has been shown particularly suitable for text categorization (Kumar and Gopal, 2009) where the feature space is huge, as it is in our case. We rely on the support vector machines implementation distributed with the GATE system (Li et al., 2009) which hides from the user the complexities of feature extraction and conversion from documents to the machine learning implementation. The tool has been applied with success to a number of datasets for opinion classification and rating-inference (Saggion and Funk, 2009). 5 Text Summarisation Approach In this Section, three approaches for carrying out the summarisation process are explained in detail. First, a generic approach is taken as a basis, and then, it is adapted into a query-focused and a opinion-oriented approach, respectively. 5.1 Generic Summarisation A generic text summarisation a</context>
</contexts>
<marker>Li, Bontcheva, Cunningham, 2009</marker>
<rawString>Y. Li, K. Bontcheva, and H. Cunningham. 2009. Adapting SVM for Data Sparseness and Imbalance: A Case Study in Information Extraction. Natural Language Engineering, 15(2):241–271.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Lloret</author>
<author>A Balahur</author>
<author>M Palomar</author>
<author>A Montoyo</author>
</authors>
<title>Towards Building a Competitive Opinion Summarization System: Challenges and Keys.</title>
<date>2009</date>
<booktitle>In Proceedings of the NAACL. Student Research Workshop and Doctoral Consortium,</booktitle>
<pages>72--77</pages>
<contexts>
<context position="9182" citStr="Lloret et al., 2009" startWordPosition="1415" endWordPosition="1418">the task into a binary classification problem. Recently, due to the vast amount of on-line information and the subjectivity appearing in documents, the combination of sentiment analysis and summari108 sation task in tandem can result in great benefits for stand-alone applications of sentiment analysis, as well as for the potential uses of sentiment analysis as part of other NLP applications (Stoyanov and Cardie, 2006). Whilst there is much literature combining sentiment analysis and text summarisation focusing on generating opinion-oriented summaries for the new textual genres, such as blogs (Lloret et al., 2009), or reviews (Zhuang et al., 2006), the use of summaries as substitutes of full documents in tasks such as rating-inference has been not yet explored to the best of our knowledge. In contrast to the existing literature, this paper uses summaries instead of full reviews to tackle the rating-inference task in the financial domain, and we carry out a preliminary analysis concerning the potential benefits of text summaries for this task. 3 Dataset for the Rating-inference Task Since there is no standard dataset for carrying out the rating-inference task, the corpus used for our experiments was one</context>
</contexts>
<marker>Lloret, Balahur, Palomar, Montoyo, 2009</marker>
<rawString>E. Lloret, A. Balahur, M. Palomar, and A. Montoyo. 2009. Towards Building a Competitive Opinion Summarization System: Challenges and Keys. In Proceedings of the NAACL. Student Research Workshop and Doctoral Consortium, pages 72–77.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mani</author>
<author>D House</author>
<author>G Klein</author>
<author>L Hirshman</author>
<author>L Obrst</author>
<author>T Firmin</author>
<author>M Chrzanowski</author>
<author>B Sundheim</author>
</authors>
<title>The TIPSTER SUMMAC Text Summarization Evaluation.</title>
<date>1998</date>
<tech>Technical report,</tech>
<institution>The Mitre Corporation.</institution>
<contexts>
<context position="4182" citStr="Mani et al., 1998" startWordPosition="629" endWordPosition="632">system. Since text summaries are meant to contain the essential content of a document (Mani, 2001), we investigate whether filtering noise through text summarisation is of any help in the rating-inference task. In re1http:trec.nist.gov/ 2http://www.nist.gov/tac/ 107 Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 107–115, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics cent years, text summarisation has been used to support both manual and automatic tasks; in the SUMMAC evaluation (Mani et al., 1998), text summaries were tested in document classification and question answering tasks where summaries were considered suitable surrogates for full documents; Bagga and Baldwin (1998) studied summarisation in the context of a cross-document coreference task and found that summaries improved the performance of a clustering-based coreference mechanism; more recently Latif and McGee (2009) have proposed text summarisation as a preprocessing step for student essay assessment finding that summaries could be used instead of full essays to group “similar” quality essays. Summarisation has been studied </context>
</contexts>
<marker>Mani, House, Klein, Hirshman, Obrst, Firmin, Chrzanowski, Sundheim, 1998</marker>
<rawString>I. Mani, D. House, G. Klein, L. Hirshman, L. Obrst, T. Firmin, M. Chrzanowski, and B. Sundheim. 1998. The TIPSTER SUMMAC Text Summarization Evaluation. Technical report, The Mitre Corporation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mani</author>
</authors>
<title>Automatic Text Summarization.</title>
<date>2001</date>
<publisher>John Benjamins Publishing Company.</publisher>
<contexts>
<context position="3662" citStr="Mani, 2001" startWordPosition="556" endWordPosition="557">nsidered a difficult problem because of the fuzziness inherent of mid-range ratings (Mukras et al., 2007). A considerable body of research has recently been produced to tackle this problem (Chakraborti et al., 2007; Ferrari et al., 2009) and reported figures showing accuracies ranging from 30% to 50% for such complex task; most approaches derive features for the classification task from the full document. In this research we ask whether extracting features from document summaries could help a classification system. Since text summaries are meant to contain the essential content of a document (Mani, 2001), we investigate whether filtering noise through text summarisation is of any help in the rating-inference task. In re1http:trec.nist.gov/ 2http://www.nist.gov/tac/ 107 Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 107–115, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics cent years, text summarisation has been used to support both manual and automatic tasks; in the SUMMAC evaluation (Mani et al., 1998), text summaries were tested in document classification and question answering t</context>
</contexts>
<marker>Mani, 2001</marker>
<rawString>I. Mani. 2001. Automatic Text Summarization. John Benjamins Publishing Company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Hannan</author>
<author>T Neylon</author>
<author>M Wells</author>
<author>J Reynar</author>
</authors>
<title>Structured Models for Fine-toCoarse Sentiment Analysis.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>432--439</pages>
<contexts>
<context position="6108" citStr="McDonald et al., 2007" startWordPosition="935" endWordPosition="938"> will describe the corpus and the NLP tools used for all the experimental set-up. Next, the text summarisation approaches will be described in Section 5, and then Section 6 will show the experiments conducted and the results obtained together with a discussion. Finally, we will draw some conclusions and address further work in Section 7. 2 Related Work Most of the literature regarding sentiment analysis addresses the problem either by detecting and classifying opinions at a sentence level (Wilson et al., 2005; Du and Tan, 2009), or by attempting to capture the overall sentiment of a document (McDonald et al., 2007; Hu et al., 2008). Traditional approaches tackle the task as binary classification, where text units (e.g. words, sentences, fragments) are classified into positive vs. negative, or subjective vs. objective, according to their polarity and subjectivity degree, respectively. However, sentiment classification taking into account a finer granularity has been less considered. Rating-inference is a particular task within sentiment analysis, which aims at inferring the author’s numerical rating for a review. For instance, given a review and 5-star-rating scale (ranging from 1 -the worst- to 5 -the </context>
</contexts>
<marker>McDonald, Hannan, Neylon, Wells, Reynar, 2007</marker>
<rawString>R. McDonald, K. Hannan, T. Neylon, M. Wells, and J. Reynar. 2007. Structured Models for Fine-toCoarse Sentiment Analysis. In Proceedings of the ACL, pages 432–439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mukras</author>
<author>N Wiratunga</author>
<author>R Lothian</author>
<author>S Chakraborti</author>
<author>D Harper</author>
</authors>
<title>Information Gain Feature Selection for Ordinal Text Classification using Probability Redistribution.</title>
<date>2007</date>
<booktitle>In Proceedings of the Textlink workshop at IJCAI-07.</booktitle>
<contexts>
<context position="3156" citStr="Mukras et al., 2007" startWordPosition="472" endWordPosition="475">blems such as identifying subjective sentences or identifying positive and negative opinions in text, here we concentrate on the opinion classification task; and more specifically on rating-inference, the task of identifying the author’s evaluation of an entity with respect to an ordinal-scale based on the author’s textual evaluation of the entity (Pang and Lee, 2005). The specific problem we study in this paper is that of associating a fine-grained rating (1=worst,...5=best) to a review. This is in general considered a difficult problem because of the fuzziness inherent of mid-range ratings (Mukras et al., 2007). A considerable body of research has recently been produced to tackle this problem (Chakraborti et al., 2007; Ferrari et al., 2009) and reported figures showing accuracies ranging from 30% to 50% for such complex task; most approaches derive features for the classification task from the full document. In this research we ask whether extracting features from document summaries could help a classification system. Since text summaries are meant to contain the essential content of a document (Mani, 2001), we investigate whether filtering noise through text summarisation is of any help in the rati</context>
</contexts>
<marker>Mukras, Wiratunga, Lothian, Chakraborti, Harper, 2007</marker>
<rawString>R. Mukras, N. Wiratunga, R. Lothian, S. Chakraborti, and D. Harper. 2007. Information Gain Feature Selection for Ordinal Text Classification using Probability Redistribution. In Proceedings of the Textlink workshop at IJCAI-07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nenkova</author>
<author>L Vanderwende</author>
<author>K McKeown</author>
</authors>
<title>A Compositional Context Sensitive Multi-document Summarizer: Exploring the Factors that Influence Summarization.</title>
<date>2006</date>
<booktitle>In Proceedings of the ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>573--580</pages>
<contexts>
<context position="15744" citStr="Nenkova et al., 2006" startWordPosition="2487" endWordPosition="2490">ation. Therefore, in the first stage the body of the review out of the whole Web page is automatically delimitated by means of patterns, and only this text is used as the input for the next summarisation stages. Further on, a sentence relevance detection process is carried out employing different combinations of various techniques. In particular, the techniques employed are: Term frequency (tf): this technique has been widely used in different summarisation approaches, showing the the most frequent words in a document contain relevant information and can be indicative of the document’s topic (Nenkova et al., 2006) Textual entailment (te): a te module (Ferr´andez et al., 2007) is used to detect redundant information in the document, by computing the entailment between two consecutive sentences and discarding the entailed ones. The identification of these entailment relations helps to avoid incorporating redundant information in summaries. Code quantity principle (cqp): this is a linguistic principle which proves the existence of a proportional relation between how important the information is, and the number of coding elements it has (Giv´on, 1990). In this approach we assume that sentences containing l</context>
</contexts>
<marker>Nenkova, Vanderwende, McKeown, 2006</marker>
<rawString>A. Nenkova, L. Vanderwende, and K. McKeown. 2006. A Compositional Context Sensitive Multi-document Summarizer: Exploring the Factors that Influence Summarization. In Proceedings of the ACM SIGIR conference on Research and development in information retrieval, pages 573–580.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
</authors>
<title>Seeing Stars: Exploiting Class Relationships for Sentiment Categorization with Respect to Rating Scales.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>115--124</pages>
<contexts>
<context position="2906" citStr="Pang and Lee, 2005" startWordPosition="430" endWordPosition="433">log mining 1, the Text Analysis Conference 2 track on opinion summarisation, and the DEfi Fouille de Textes program (Grouin et al., 2009) advances in the state of the art have been produced. Although sentiment analysis involves various different problems such as identifying subjective sentences or identifying positive and negative opinions in text, here we concentrate on the opinion classification task; and more specifically on rating-inference, the task of identifying the author’s evaluation of an entity with respect to an ordinal-scale based on the author’s textual evaluation of the entity (Pang and Lee, 2005). The specific problem we study in this paper is that of associating a fine-grained rating (1=worst,...5=best) to a review. This is in general considered a difficult problem because of the fuzziness inherent of mid-range ratings (Mukras et al., 2007). A considerable body of research has recently been produced to tackle this problem (Chakraborti et al., 2007; Ferrari et al., 2009) and reported figures showing accuracies ranging from 30% to 50% for such complex task; most approaches derive features for the classification task from the full document. In this research we ask whether extracting fea</context>
<context position="6856" citStr="Pang and Lee, 2005" startWordPosition="1049" endWordPosition="1052">gments) are classified into positive vs. negative, or subjective vs. objective, according to their polarity and subjectivity degree, respectively. However, sentiment classification taking into account a finer granularity has been less considered. Rating-inference is a particular task within sentiment analysis, which aims at inferring the author’s numerical rating for a review. For instance, given a review and 5-star-rating scale (ranging from 1 -the worst- to 5 -the best), this task should correctly predict the review’s rating, based on the language and sentiment expressed in its content. In (Pang and Lee, 2005), the rating-inference problem is analysed for the movies domain. In particular, the utility of employing label and item similarity is shown by analysing the performance of three different methods based on SVM (one vs. all, regression and metric labeling), in order to infer the author’s implied numerical rating, which ranges from 1 up to 4 stars, depending on the degree the author of the review liked or not the film. The approach described in (Leung et al., 2006) suggests the use of collaborative filtering algorithms together with sentiment analysis techniques to obtain user preferences expres</context>
</contexts>
<marker>Pang, Lee, 2005</marker>
<rawString>B. Pang and L. Lee. 2005. Seeing Stars: Exploiting Class Relationships for Sentiment Categorization with Respect to Rating Scales. In Proceedings of the ACL, pages 115–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
</authors>
<title>Opinion Mining and Sentiment Analysis. Foundations and Trends in Information Retrieval,</title>
<date>2008</date>
<pages>2--1</pages>
<contexts>
<context position="2052" citStr="Pang and Lee, 2008" startWordPosition="293" endWordPosition="296"> their products, services, and key company representatives to ensure that good reputation is maintained. Recent cases of public figures making headlines for the wrong reasons have shown how companies take into account public opinion to distance themselves from figures which can damage their public image. The Web has become an important source for finding information, in the field of business intelligence, business analysts are turning their eyes to the Web in order to monitor public perception on products, services, policies, and managers. The field of sentiment analysis has recently emerged (Pang and Lee, 2008) as an important area of research in Natural Language Processing (NLP) which can provide viable solutions for monitoring public perception on a number of issues; with evaluation programs such as the Text REtrieval Conference track on blog mining 1, the Text Analysis Conference 2 track on opinion summarisation, and the DEfi Fouille de Textes program (Grouin et al., 2009) advances in the state of the art have been produced. Although sentiment analysis involves various different problems such as identifying subjective sentences or identifying positive and negative opinions in text, here we concen</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>B. Pang and L. Lee. 2008. Opinion Mining and Sentiment Analysis. Foundations and Trends in Information Retrieval, 2(1-2):1–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Saggion</author>
<author>A Funk</author>
</authors>
<date>2009</date>
<booktitle>Extracting Opinions and Facts for Business Intelligence. RNTI, E-17:119–146.</booktitle>
<contexts>
<context position="14507" citStr="Saggion and Funk, 2009" startWordPosition="2296" endWordPosition="2299">been used with success in different tasks in natural language processing (Isozaki and Kazawa, 2002), but it has been shown particularly suitable for text categorization (Kumar and Gopal, 2009) where the feature space is huge, as it is in our case. We rely on the support vector machines implementation distributed with the GATE system (Li et al., 2009) which hides from the user the complexities of feature extraction and conversion from documents to the machine learning implementation. The tool has been applied with success to a number of datasets for opinion classification and rating-inference (Saggion and Funk, 2009). 5 Text Summarisation Approach In this Section, three approaches for carrying out the summarisation process are explained in detail. First, a generic approach is taken as a basis, and then, it is adapted into a query-focused and a opinion-oriented approach, respectively. 5.1 Generic Summarisation A generic text summarisation approach is first taken as a core, in which three main stages can be distinguished: i) document preprocessing; ii) relevance detection; and ii) summary generation. Since we work with Web documents, an initial preprocessing step is essential to remove all unnecessary tags </context>
</contexts>
<marker>Saggion, Funk, 2009</marker>
<rawString>H. Saggion and A. Funk. 2009. Extracting Opinions and Facts for Business Intelligence. RNTI, E-17:119–146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Saggion</author>
</authors>
<title>SUMMA: A Robust and Adaptable Summarization Tool.</title>
<date>2008</date>
<booktitle>Traitement Automatique des Languages,</booktitle>
<pages>49--103</pages>
<contexts>
<context position="18212" citStr="Saggion, 2008" startWordPosition="2895" endWordPosition="2896">g pronouns referring also to the bank, will not be taken into consideration in the summarisation process. We are aware of this limitation and for future work it would be necessary to run a coreference algorithm to identify all occurrences of a bank within a review. However, since the main goal of this paper is to carry out a preliminary analysis of the usefulness of summaries in contrast to whole reviews in the ratinginference problem, we did not take this problem into account at this stage of the research. In addition, when we do query-focused summarisation only we rely on the SUMMA toolkit (Saggion, 2008) to produce a query similarity value for each sentence in the review which in turn is used to rank sentences for an extractive summary (qf). This similarity value is the cosine similarity between a sentence vector (terms and weights) and a query vector (terms and weigths) and where the query is the name of the entity being reviewed (e.g. National Westminster). 5.3 Opinion-oriented Summarisation Since reviews are written by people who want to express their opinion and experience with regard to a bank, in this particular case, either generic or query-focused summaries can miss including some imp</context>
</contexts>
<marker>Saggion, 2008</marker>
<rawString>H. Saggion. 2008. SUMMA: A Robust and Adaptable Summarization Tool. Traitement Automatique des Languages, 49:103–125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Shimada</author>
<author>T Endo</author>
</authors>
<title>Seeing Several Stars: A Rating Inference Task for a Document Containing Several Evaluation Criteria.</title>
<date>2008</date>
<booktitle>In Proceedings of the 12th Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining,</booktitle>
<pages>1006--1014</pages>
<contexts>
<context position="7861" citStr="Shimada and Endo, 2008" startWordPosition="1213" endWordPosition="1216">r of the review liked or not the film. The approach described in (Leung et al., 2006) suggests the use of collaborative filtering algorithms together with sentiment analysis techniques to obtain user preferences expressed in textual reviews, focusing also on movie reviews. Once the opinion words from user reviews have been identified, the polarity of those opinion words together with their strength need to be computed and mapped to the rating scales to be further input to the collaborative input algorithms. Apart from these approaches, this problem is stated from a different point of view in (Shimada and Endo, 2008). Here it is approached from the perspective of rating different details of a product under the same review. Consequently, they rename the problem as “seeing several stars” instead of only one, corresponding to the overall sentiment of the review. Also, in (Baccianella et al., 2009) the rating of different features regarding hotel reviews (cleanliness, location, staff, etc.) is addressed by analysing several aspects involved in the generation of product review’s representations, such as part-of-speech and lexicons. Other approaches (Devitt and Ahmad, 2007), (Turney, 2002) face this problem by </context>
</contexts>
<marker>Shimada, Endo, 2008</marker>
<rawString>K. Shimada and T. Endo. 2008. Seeing Several Stars: A Rating Inference Task for a Document Containing Several Evaluation Criteria. In Proceedings of the 12th Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining, pages 1006–1014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Spiegel</author>
<author>N J Castellan</author>
</authors>
<title>Nonparametric Statistics for the Behavioral Sciences.</title>
<date>1998</date>
<publisher>McGraw-Hill International.</publisher>
<contexts>
<context position="23772" citStr="Spiegel and Castellan, 1998" startWordPosition="3771" endWordPosition="3774">cation task, when features extracted from summaries are used instead from the full review. On the one hand, results using the root feature extracted from summaries can be seen in Table 4. On the other hand, Table 5 shows the results when the combination of all the linguistic and sentiment-based features (root+category+sentiWN), that has been extracted from summaries, are used for training the SVM classifier. We also performed two statistical tests in order to measure the significance for the results obtained. The tests we performed were the one-way Analysis of Variance (ANOVA) and the t-test (Spiegel and Castellan, 1998). Given a group of experiments, we first run ANOVA for analysing the difference between their means. In case some differences are found, we run the t-test between those pairs. 6.2 Discussion A first analysis derived from the results obtained in Table 3 makes us be aware of the difficulty associated to the rating-inference task. As can be seen, a baseline without any information from the document at all, is performing around 30%, which compared to the remaining approaches is not a very bad number. However, we assumed that dealing with some information contained in documents, the classification </context>
</contexts>
<marker>Spiegel, Castellan, 1998</marker>
<rawString>S. Spiegel and N. J. Castellan, Jr. 1998. Nonparametric Statistics for the Behavioral Sciences. McGraw-Hill International.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Stoyanov</author>
<author>C Cardie</author>
</authors>
<title>Toward Opinion Summarization: Linking the Sources.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Sentiment and Subjectivity in Text,</booktitle>
<pages>9--14</pages>
<contexts>
<context position="8983" citStr="Stoyanov and Cardie, 2006" startWordPosition="1386" endWordPosition="1389">-of-speech and lexicons. Other approaches (Devitt and Ahmad, 2007), (Turney, 2002) face this problem by grouping documents with closer stars under the same category, i.e. positive or negative, simplifying the task into a binary classification problem. Recently, due to the vast amount of on-line information and the subjectivity appearing in documents, the combination of sentiment analysis and summari108 sation task in tandem can result in great benefits for stand-alone applications of sentiment analysis, as well as for the potential uses of sentiment analysis as part of other NLP applications (Stoyanov and Cardie, 2006). Whilst there is much literature combining sentiment analysis and text summarisation focusing on generating opinion-oriented summaries for the new textual genres, such as blogs (Lloret et al., 2009), or reviews (Zhuang et al., 2006), the use of summaries as substitutes of full documents in tasks such as rating-inference has been not yet explored to the best of our knowledge. In contrast to the existing literature, this paper uses summaries instead of full reviews to tackle the rating-inference task in the financial domain, and we carry out a preliminary analysis concerning the potential benef</context>
</contexts>
<marker>Stoyanov, Cardie, 2006</marker>
<rawString>V. Stoyanov and C. Cardie. 2006. Toward Opinion Summarization: Linking the Sources. In Proceedings of the Workshop on Sentiment and Subjectivity in Text, pages 9–14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P D Turney</author>
</authors>
<title>Thumbs Up or Thumbs Down?: Semantic Orientation Applied to Unsupervised Classification of Reviews.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>417--424</pages>
<contexts>
<context position="8439" citStr="Turney, 2002" startWordPosition="1301" endWordPosition="1302">of view in (Shimada and Endo, 2008). Here it is approached from the perspective of rating different details of a product under the same review. Consequently, they rename the problem as “seeing several stars” instead of only one, corresponding to the overall sentiment of the review. Also, in (Baccianella et al., 2009) the rating of different features regarding hotel reviews (cleanliness, location, staff, etc.) is addressed by analysing several aspects involved in the generation of product review’s representations, such as part-of-speech and lexicons. Other approaches (Devitt and Ahmad, 2007), (Turney, 2002) face this problem by grouping documents with closer stars under the same category, i.e. positive or negative, simplifying the task into a binary classification problem. Recently, due to the vast amount of on-line information and the subjectivity appearing in documents, the combination of sentiment analysis and summari108 sation task in tandem can result in great benefits for stand-alone applications of sentiment analysis, as well as for the potential uses of sentiment analysis as part of other NLP applications (Stoyanov and Cardie, 2006). Whilst there is much literature combining sentiment an</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>P. D. Turney. 2002. Thumbs Up or Thumbs Down?: Semantic Orientation Applied to Unsupervised Classification of Reviews. In Proceedings of the ACL, pages 417–424.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Wilson</author>
<author>J Wiebe</author>
<author>P Hoffmann</author>
</authors>
<title>Recognizing Contextual Polarity in Phrase-level Sentiment Analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the EMNLP,</booktitle>
<pages>347--354</pages>
<contexts>
<context position="6001" citStr="Wilson et al., 2005" startWordPosition="915" endWordPosition="918">on 2 will compile the existing work with respect to the inference-rating problem; Section 3 and Section 4 will describe the corpus and the NLP tools used for all the experimental set-up. Next, the text summarisation approaches will be described in Section 5, and then Section 6 will show the experiments conducted and the results obtained together with a discussion. Finally, we will draw some conclusions and address further work in Section 7. 2 Related Work Most of the literature regarding sentiment analysis addresses the problem either by detecting and classifying opinions at a sentence level (Wilson et al., 2005; Du and Tan, 2009), or by attempting to capture the overall sentiment of a document (McDonald et al., 2007; Hu et al., 2008). Traditional approaches tackle the task as binary classification, where text units (e.g. words, sentences, fragments) are classified into positive vs. negative, or subjective vs. objective, according to their polarity and subjectivity degree, respectively. However, sentiment classification taking into account a finer granularity has been less considered. Rating-inference is a particular task within sentiment analysis, which aims at inferring the author’s numerical ratin</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recognizing Contextual Polarity in Phrase-level Sentiment Analysis. In Proceedings of the EMNLP, pages 347– 354.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Zhuang</author>
<author>F Jing</author>
<author>X Y Zhu</author>
</authors>
<title>Movie Review Mining and Summarization.</title>
<date>2006</date>
<booktitle>In Proceedings of the 15th ACM international conference on Information and knowledge management,</booktitle>
<pages>43--50</pages>
<contexts>
<context position="9216" citStr="Zhuang et al., 2006" startWordPosition="1421" endWordPosition="1424">ion problem. Recently, due to the vast amount of on-line information and the subjectivity appearing in documents, the combination of sentiment analysis and summari108 sation task in tandem can result in great benefits for stand-alone applications of sentiment analysis, as well as for the potential uses of sentiment analysis as part of other NLP applications (Stoyanov and Cardie, 2006). Whilst there is much literature combining sentiment analysis and text summarisation focusing on generating opinion-oriented summaries for the new textual genres, such as blogs (Lloret et al., 2009), or reviews (Zhuang et al., 2006), the use of summaries as substitutes of full documents in tasks such as rating-inference has been not yet explored to the best of our knowledge. In contrast to the existing literature, this paper uses summaries instead of full reviews to tackle the rating-inference task in the financial domain, and we carry out a preliminary analysis concerning the potential benefits of text summaries for this task. 3 Dataset for the Rating-inference Task Since there is no standard dataset for carrying out the rating-inference task, the corpus used for our experiments was one associated to a current project o</context>
</contexts>
<marker>Zhuang, Jing, Zhu, 2006</marker>
<rawString>L. Zhuang, F. Jing, and X. Y. Zhu. 2006. Movie Review Mining and Summarization. In Proceedings of the 15th ACM international conference on Information and knowledge management, pages 43–50.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>