<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001128">
<title confidence="0.9865585">
Compositional Distributional Semantics Models
in Chunk-based Smoothed Tree Kernels
</title>
<author confidence="0.982024">
Nghia The Pham Lorenzo Ferrone
</author>
<affiliation confidence="0.999491">
University of Trento University of Rome “Tor Vergata”
</affiliation>
<email confidence="0.978819">
thenghia.pham@unitn.it lorenzo.ferrone@gmail.com
</email>
<author confidence="0.977135">
Fabio Massimo Zanzotto
</author>
<affiliation confidence="0.994221">
University of Rome “Tor Vergata”
</affiliation>
<email confidence="0.993373">
fabio.massimo.zanzotto@uniroma2.it
</email>
<sectionHeader confidence="0.994625" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999895722222222">
The field of compositional distributional
semantics has proposed very interesting
and reliable models for accounting the
distributional meaning of simple phrases.
These models however tend to disregard
the syntactic structures when they are ap-
plied to larger sentences. In this paper we
propose the chunk-based smoothed tree
kernels (CSTKs) as a way to exploit the
syntactic structures as well as the reliabil-
ity of these compositional models for sim-
ple phrases. We experiment with the rec-
ognizing textual entailment datasets. Our
experiments show that our CSTKs per-
form better than basic compositional dis-
tributional semantic models (CDSMs) re-
cursively applied at the sentence level, and
also better than syntactic tree kernels.
</bodyText>
<sectionHeader confidence="0.998762" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999926096153846">
A clear interaction between syntactic and semantic
interpretations for sentences is important for many
high-level NLP tasks, such as question-answering,
textual entailment recognition, and semantic tex-
tual similarity. Systems and models for these tasks
often use classifiers or regressors that exploit con-
volution kernels (Haussler, 1999) to model both
interpretations.
Convolution kernels are naturally defined on
spaces where there exists a similarity function be-
tween terminal nodes. This feature has been used
to integrate distributional semantics within tree
kernels. This class of kernels is often referred to as
smoothed tree kernels (Mehdad et al., 2010; Croce
et al., 2011), yet, these models only use distribu-
tional vectors for words.
Compositional distributional semantics models
(CDSMs) on the other hand are functions map-
ping text fragments to vectors (or higher-order ten-
sors) which then provide a distributional meaning
for simple phrases or sentences. Many CDSMs
have been proposed for simple phrases like non-
recursive noun phrases or verbal phrases (Mitchell
and Lapata, 2008; Baroni and Zamparelli, 2010;
Clark et al., 2008; Grefenstette and Sadrzadeh,
2011; Zanzotto et al., 2010). Non-recursive
phrases are often referred to as chunks (Abney,
1996), and thus, CDSMs are good and reliable
models for chunks.
In this paper, we present the chunk-based
smoothed tree kernels (CSTK) as a way to merge
the two approaches: the smoothed tree kernels
and the models for compositional distributional se-
mantics. Our approach overcomes the limitation
of the smoothed tree kernels which only use vec-
tors for words by exploiting reliable CDSMs over
chunks. CSTKs are defined over a chunk-based
syntactic subtrees where terminal nodes are words
or word sequences. We experimented with CSTKs
on data from the recognizing textual entailment
challenge (Dagan et al., 2006) and we compared
our CSTKs with other standard tree kernels and
standard recursive CDSMs. Experiments show
that our CSTKs perform better than basic compo-
sitional distributional semantic models (CDSMs)
recursively applied at the sentence level and better
than syntactic tree kernels.
The rest of the paper is organized as follows.
Section 2 describes the CSTKs. Section 3 re-
ports on the experimental setting and on the re-
sults. Finally, Section 4 draws the conclusions and
sketches the future work.
</bodyText>
<sectionHeader confidence="0.96036" genericHeader="method">
2 Chunk-based Smoothed Tree Kernels
</sectionHeader>
<bodyText confidence="0.999879833333333">
This section describes the new class of kernels.
We first introduce the notion of the chunk-based
syntactic subtree. Then, we describe the recursive
formulation of the class of kernels. Finally, we in-
troduce the basic CDSMs we use and we introduce
two instances of the class of kernels.
</bodyText>
<page confidence="0.990688">
93
</page>
<note confidence="0.785636">
Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 93–98,
Dublin, Ireland, August 23-24 2014.
</note>
<subsectionHeader confidence="0.67091">
2.1 Notation and preliminaries
</subsectionHeader>
<bodyText confidence="0.47783">
✭✭ ✭✭ ✭S❤❤❤❤❤
</bodyText>
<figureCaption confidence="0.999474">
Figure 1: Sample Syntactic Tree
</figureCaption>
<bodyText confidence="0.999799117647059">
A Chunk-based Syntactic Sub-Tree is a subtree
of a syntactic tree where each non-terminal node
dominating a contiguous word sequence is col-
lapsed into a chunk and, as usual in chunks (Ab-
ney, 1996), the internal structure is disregarded.
For example, Figure 2 reports some chunk-based
syntactic subtrees of the tree in Figure 1. Chunks
are represented with a pre-terminal node dominat-
ing a triangle that covers a word sequence. The
first subtree represents the chunk covering the sec-
ond NP and the node dominates the word sequence
its:d final:n concert:n. The second subtree repre-
sents the structure of the whole sentence and one
chunk, that is the first NP dominating the word
sequence the:d rock:n band:n. The third subtree
again represents the structure of the whole sen-
tence split into two chunks without the verb.
</bodyText>
<figure confidence="0.4078204">
✥✥ ✥✥S❵❵❵❵
NP VP
✘✘ ✘ ✘ ❳❳❳❳ ✏✏ ✏ PPP
the:d rock:j band:n VBZ ✥✥ ✥✥NP ❵❵❵❵
its:p final:j concert:n
</figure>
<figureCaption confidence="0.9360795">
Figure 2: Some Chunk-based Syntactic Sub-Trees
of the tree in Figure 1
</figureCaption>
<bodyText confidence="0.958736428571428">
In the following sections, generic trees are de-
noted with the letter t and N(t) denotes the set of
non-terminal nodes of tree t. Each non-terminal
node n E N(t) has a label sn representing its syn-
tactic tag. As usual for constituency-based parse
trees, pre-terminal nodes are nodes that have a sin-
gle terminal node as child. Terminal nodes of trees
are words denoted with w:pos where w is the ac-
tual token and pos is its postag. The structure of
these trees is represented as follows. Given a tree
t, ci(n) denotes i-th child of a node n in the set of
nodes N(t). The production rule headed in node
n is prod(n), that is, given the node n with m chil-
dren, prod(n) is:
</bodyText>
<equation confidence="0.997597">
prod(n) = sn → sc,(n) ... sc.(n)
</equation>
<bodyText confidence="0.999814727272727">
Finally, for a node n in N(t), the function d(n)
generates the word sequence dominated by the
non-terminal node n in the tree t. For example,
d(VP) in Figure 1 is holds:v its:p final:j concert:n.
Chunk-based Syntactic Sub-Trees (CSSTs) are
instead denoted with the letter T. Differently
from trees t, CSSTs have terminal nodes that
can represent subsequences of words of the
original sentence. The explicit syntactic structure
of a CSST is the structure not falling in chunks
and it is represented as s(T). For example, s(T3) is:
</bodyText>
<equation confidence="0.9843756">
S
✟ ✟ ❍❍
NP VP
/\
VBZ NP
</equation>
<bodyText confidence="0.998964583333333">
where T3 is the third subtree of Figure 2.
Given a tree t, the set S(t) is defined as the set
containing all the relevant CSSTs of the tree t.
As for the tree kernels (Collins and Duffy, 2002),
the set S(t) contains all CSSTs derived from the
subtrees of t such that if a node n belongs to a
subtree ts, all the siblings of n in t belongs to ts.
In other words, productions of the initial subtrees
are complete. A CSST is obtained by collapsing
in a single terminal nodes a contiguous sequence
of words dominated by a single non-terminal
node. For example:
</bodyText>
<equation confidence="0.959700166666667">
NP
✏ ✏ ��
DT NN
✟✟ ❍❍
is collapsed into:
NP
✦ ✦ ❛❛
DT NN:X
✏✏ ✏ PPP
rock:n band:n
Finally, wn E Ism represent the distributional
→
</equation>
<bodyText confidence="0.994065666666667">
vectors for words wn and f(w1 ... wk) represents
a compositional distributional semantics model
applied to the word sequence w1 ... wk.
</bodyText>
<figure confidence="0.96926655882353">
NP
✘ ✘ ✘ ❳❳❳
VP
✘ ✘✘ ❳❳❳
NP
✘ ✘ ✘ ❳❳❳
PRP JJ NN
its:p final:j concert:n
DT
the:d
NN
rock:n
NN
band:n
VBZ
holds:v
✥✥✥ ✥NP ❵❵❵❵
its:p final:j concert:n
S
✘✘ ✘ ❳❳❳
NP
✘✘✘ ✘ ❳❳❳❳
the:d rock:n band:n
VP
/\
VBZ NP
NN
NN
rock:n
band:n
94
⎧
⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪ ⎪
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
</figure>
<subsectionHeader confidence="0.9378515">
2.2 Smoothed Tree Kernels on Chunk-based
Syntactic Trees
</subsectionHeader>
<bodyText confidence="0.99857">
As usual, a tree kernel, although written in a re-
cursive way, computes the following general equa-
tion:
</bodyText>
<equation confidence="0.9926005">
K(t1, t2) = X λ|N(Ti)|+|N(Tj)|KF(τi,τj)
ri E S(t1)
rj E S(t2)
(1)
</equation>
<bodyText confidence="0.9965878">
In our case, the basic similarity KF(ti, tj) is de-
fined to take into account the syntactic structure
and the distributional semantic part. Thus, we de-
fine it as follows in line with what done with sev-
eral other smoothed tree kernels:
</bodyText>
<equation confidence="0.910967666666667">
YKF(τi,τj) = δ(s(τi),s(τj)) (f(a), f(b))
a E PT(ri)
b E PT(rj)
</equation>
<bodyText confidence="0.980626125">
where δ(s(τi), s(τj)) is the Kroneker’s delta
function between the the structural part of two
chunk-based syntactic subtrees, PT (τ) are the
nodes in τ directly covering a chunk or a word,
and (→x, →y) is the cosine similarity between the
two vectors → x and →y. For example, given the
chunk-based subtree τ3 in Figure 2 and
where C(n1, n2) =
</bodyText>
<equation confidence="0.9841452">
(f(d(n1)), f(d(n2))) if label(n1) = label(n2)
and prod(n1) =� prod(n2)
(f(d(n1)), f(d(n2)))
+ Qnc(n1)
j=1 (1 + C(cj(n1),cj(n2)))
− Qnc(n1)
j=1 (f(d(cj(n1))), f(d(cj(n2))))
if n1, n2 are not pre-terminals and
prod(n1) = prod(n2)
0 otherwise
</equation>
<bodyText confidence="0.9982035">
where nc(n1) is the lenght of the production
prod(n1).
</bodyText>
<subsectionHeader confidence="0.95322">
2.3 Compositional Distributional Semantic
Models and two Specific CSTKs
</subsectionHeader>
<bodyText confidence="0.993938307692308">
To define specific CSTKs, we need to introduce
the basic compositional distributional semantic
models (CDSMs). We use two CDSMs: the Ba-
sic Additive model (BA) and teh Full Additive
model (FA). We thus define two specific CSTKs:
the CSTK+BA that is based on the basic additive
model and the CSTK+FA that is based on the full
additive model. We describe the two CDSMs in
the following.
The Basic Additive model (BA) (introduced in
(Mitchell and Lapata, 2008)) computes the disti-
butional semantics vector of a pair of words a =
a1a2 as:
</bodyText>
<equation confidence="0.917594636363636">
ADD(a1, a2) = α→a1 + βa2→
S
✘ ✘✘ ❳❳❳
NP
✘ ✘✘ ❳❳❳
the:d orchestra:n
✏ ✏ ��
its:p show:n
the similarity KF(τ3, τ4) is:
(f(the:d orchestra:n), f(the:d rock:n band:n)) �
(f(its:p show:n), f(its:p final:j concert:n)).
</equation>
<bodyText confidence="0.999892">
The recursive formulation of the Chunk-based
Smoothed Tree Kernel (CSTK) is a bit more com-
plex but very similar to the recursive formulation
of the syntactic tree kernels:
</bodyText>
<equation confidence="0.998387666666667">
K(t1, t2) = X C(n1, n2) (2)
n1 E N(t1)
n2 E N(t2)
</equation>
<bodyText confidence="0.9998965">
where α and β weight the first and the second
word of the pair. The basic additive model for
word sequences s = w1 ... wk is recursively de-
fined as follows:
</bodyText>
<equation confidence="0.9971865">
fBA(s) = ( →
w1 if k = 1
→
α w1 + βfBA(w2 ... wk) if k &gt; 1
</equation>
<bodyText confidence="0.9739008">
The Full Additive model (FA) (used in (Gue-
vara, 2010) for adjective-noun pairs and (Zanzotto
et al., 2010) for three different syntactic relations)
→
computes the compositional vector a of a pair us-
ing two linear tranformations AR and BR respec-
tively applied to the vectors of the first and the
second word. These matrices generally only de-
pends on the syntactic relation R that links those
two words. The operation follows:
</bodyText>
<equation confidence="0.994375333333333">
→ →
fFA(a1, a2, R) = ARa1 + BRa2
τ4 =
VP
✦ ✦ ❛❛
VBZ NP
</equation>
<page confidence="0.994657">
95
</page>
<table confidence="0.998866142857143">
RR RRTWS
RTE1 RTE2 RTE3 RTE5 Average RTE1 RTE2 RTE3 RTE5 Average
Add 0.541 0.496 0.507 0.520 0.516 0.560 0.538 0.643 0.578 0.579
FullAdd 0.512 0.516 0.507 0.569 0.526 0.571 0.608 0.643 0.643 0.616
TK 0.561 0.552 0.531 0.54 0.546 0.608 0.627 0.648 0.630 0.628
CSTK+BA 0.553 0.545 0.562 0.568 0.557† 0.626 0.616 0.648 0.628 0.629†
CSTK+FA 0.543 0.550 0.574 0.576 0.560† 0.628 0.616 0.652 0.630 0.631†
</table>
<tableCaption confidence="0.8930595">
Table 1: Task-based analysis: Accuracy on Recognizing Textual Entailment († is different from both ADD and
FullADD with a stat.sig. of p &gt; 0.1.)
</tableCaption>
<bodyText confidence="0.959887333333333">
The full additive model for word sequences s =
w1 ... wk, whose node has a production rule s →
sc1 ... scm is also defined recursively:
</bodyText>
<equation confidence="0.997288">
w1→ ifk=1
→
AvnV + BvnfFA(NP)
ifs → V NP
→
AanA + BanfFA(N)
ifs → A N
E fFA(sci) otherwise
</equation>
<bodyText confidence="0.999561333333333">
where Avn, Bvn are matrices used for verb and
noun phrase interaction, and Aan, Ban are used for
adjective, noun interaction.
</bodyText>
<sectionHeader confidence="0.997856" genericHeader="method">
3 Experimental Investigation
</sectionHeader>
<subsectionHeader confidence="0.992189">
3.1 Experimental set-up
</subsectionHeader>
<bodyText confidence="0.997849186046512">
We experimented with the Recognizing Textual
Entailment datasets (RTE) (Dagan et al., 2006).
RTE is the task of deciding whether a long text
T entails a shorter text, typically a single sen-
tence, called hypothesis H. It has been often seen
as a classification task (see (Dagan et al., 2013)).
We used four datasets: RTE1, RTE2, RTE3, and
RTE5, with the standard split between training and
testing. The dev/test distribution for RTE1-3, and
RTE5 is respectively 567/800, 800/800, 800/800,
and 600/600 T-H pairs.
Distributional vectors are derived with
DISSECT (Dinu et al., 2013) from a cor-
pus obtained by the concatenation of ukWaC
(wacky.sslmit.unibo.it), a mid-2009 dump of the
English Wikipedia (en.wikipedia.org) and the
British National Corpus (www.natcorp.ox.ac.uk),
for a total of about 2.8 billion words. We collected
a 35K-by-35K matrix by counting co-occurrence
of the 30K most frequent content lemmas in
the corpus (nouns, adjectives and verbs) and all
the content lemmas occurring in the datasets
within a 3 word window. The raw count vectors
were transformed into positive Pointwise Mutual
Information scores and reduced to 300 dimensions
by Singular Value Decomposition. This setup was
picked without tuning, as we found it effective in
previous, unrelated experiments.
We built the matrices for the full additive mod-
els using the procedure described in (Guevara,
2010). We considered only two relations: the
Adjective-Noun and Verb-Noun. The full addi-
tive model falls back to the basic additional model
when syntactic relations are different from these
two.
To build the final kernel to learn the clas-
sifier, we followed standard approaches (Dagan
et al., 2013), that is, we exploited two models:
a model with only a rewrite rule feature space
(RR) and a model with the previous space along
with a token-level similarity feature (RRTWS).
The two models use our CSTKs and the stan-
dard TKs in the following way as kernel func-
</bodyText>
<equation confidence="0.624881333333333">
tions: (1) RR(p1, p2) = κ(ta1, ta2) + κ(tb1, tb2);
(2) RRTW5(p1, p2) = κ(ta1, ta2) + κ(tb1, tb2) +
(TW5(a1, b1) · TW5(a2, b2) + 1)2 where TW5
</equation>
<bodyText confidence="0.8114175">
is a weighted token similarity (as in (Corley and
Mihalcea, 2005)).
</bodyText>
<subsectionHeader confidence="0.97783">
3.2 Results
</subsectionHeader>
<bodyText confidence="0.9999335">
Table 1 shows the results of the experiments, the
table is organised as follows: columns 2-6 re-
port the accuracy of the RTE systems based on
rewrite rules (RR) and columns 7-11 report the ac-
curacies of RR systems along with token similar-
ity (RRTS). We compare five differente models:
ADD is the Basic Additive model with parameters
α = Q = 1 (as defined in 2.3) applied to the words
of the sentence (without considering its tree struc-
ture), the same is done for the Full Additive (Ful-
lADD), defined as in 2.3. The Tree Kernel (TK) as
defined in (Collins and Duffy, 2002) are applied to
</bodyText>
<equation confidence="0.993672">
fFA(s) = {
</equation>
<page confidence="0.981715">
96
</page>
<bodyText confidence="0.999981225806452">
the constituency-based tree representation of the
tree, without the intervening collapsing step de-
scribed in 2.2. These three models are the base-
line against which we compare the CSTK models
where the collapsing procedure is done via Basic
Additive (CSTK + BA, again with α = Q = 1) and
FullAdditive (CSTK + FA), as described in sec-
tion 2.2, again, with the aforementioned restric-
tion on the relation considered. For RR models we
have that CSTK+BA and CSTK+FA both achieve
higher accuracy than ADD and FullAdd, with a
statistical significante greater than 93.7%, as com-
puted with the sign test. Specifically we have that
CSTK+BA has an average accuracy 7.94% higher
than ADD and 5.89% higher than FullADD, while
CSTK+FA improves on ADD and FullADD by
8.52% and 6.46%, respectively. The same trend is
visible for the RRTS model, again both models are
statistically better than ADD and FullADD, in this
case we have that CSTK+BA is 8.63% more ac-
curate then ADD and 2.11% more than FullADD,
CSTK+FA is respectively 8.98% and 2.43% more
accurate than ADD and FullADD. As for the TK
models we have that both CSTK models achieve
again an higher average accuracy: for RR models
CSTK+BA and CSTK+FA are respectively 2.01%
and 0.15% better than TK, while for RRTS models
the number are 2.54% and 0.47%. These results
though are not statistically significant, as is the
difference between the two CSTK models them-
selves.
</bodyText>
<sectionHeader confidence="0.998392" genericHeader="conclusions">
4 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999973307692308">
In this paper, we introduced a novel sub-class
of the convolution kernels in order exploit reli-
able compositional distributional semantic mod-
els along with the syntactic structure of sen-
tences. Experiments show that this novel sub-
class, namely, the Chunk-based Smoothed Tree
Kernels (CSTKs), are a promising solution, per-
forming significantly better than a naive recursive
application of the compositional distributional se-
mantic models. We experimented with CSTKS
equipped with the basic additive and the full addi-
tive CDSMs but these kernels are definitely open
to all the CDSMs.
</bodyText>
<sectionHeader confidence="0.99804" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.990573">
We acknowledge ERC 2011 Starting Independent
Research Grant n. 283554 (COMPOSES).
</bodyText>
<sectionHeader confidence="0.983151" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999864703703703">
Steven Abney. 1996. Part-of-speech tagging and par-
tial parsing. In G.Bloothooft K.Church, S.Young,
editor, Corpus-based methods in language and
speech. Kluwer academic publishers, Dordrecht.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages
1183–1193, Cambridge, MA, October. Association
for Computational Linguistics.
Stephen Clark, Bob Coecke, and Mehrnoosh
Sadrzadeh. 2008. A compositional distributional
model of meaning. Proceedings of the Second
Symposium on Quantum Interaction (QI-2008),
pages 133–140.
Michael Collins and Nigel Duffy. 2002. New rank-
ing algorithms for parsing and tagging: Kernels over
discrete structures, and the voted perceptron. In Pro-
ceedings of ACL02.
Courtney Corley and Rada Mihalcea. 2005. Measur-
ing the semantic similarity of texts. In Proc. of the
ACL Workshop on Empirical Modeling of Seman-
tic Equivalence and Entailment, pages 13–18. As-
sociation for Computational Linguistics, Ann Arbor,
Michigan, June.
Danilo Croce, Alessandro Moschitti, and Roberto
Basili. 2011. Structured lexical similarity via con-
volution kernels on dependency trees. In Proceed-
ings of the Conference on Empirical Methods in
Natural Language Processing, EMNLP ’11, pages
1034–1046, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The pascal recognising textual entailment
challenge. In Quionero-Candela et al., editor,
LNAI 3944: MLCW 2005, pages 177–190. Springer-
Verlag, Milan, Italy.
Ido Dagan, Dan Roth, Mark Sammons, and Fabio Mas-
simo Zanzotto. 2013. Recognizing Textual Entail-
ment: Models and Applications. Synthesis Lectures
on Human Language Technologies. Morgan &amp; Clay-
pool Publishers.
Georgiana Dinu, Nghia The Pham, and Marco Baroni.
2013. DISSECT: DIStributional SEmantics Com-
position Toolkit. In Proceedings of ACL (System
Demonstrations), pages 31–36, Sofia, Bulgaria.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical composi-
tional distributional model of meaning. In Proceed-
ings of the Conference on Empirical Methods in
Natural Language Processing, EMNLP ’11, pages
1394–1404, Stroudsburg, PA, USA. Association for
Computational Linguistics.
</reference>
<page confidence="0.992939">
97
</page>
<reference confidence="0.99966537037037">
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of the 2010 Workshop on
GEometrical Models of Natural Language Seman-
tics, pages 33–37, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
David Haussler. 1999. Convolution kernels on discrete
structures. Technical report, University of Califor-
nia at Santa Cruz.
Yashar Mehdad, Alessandro Moschitti, and Fabio Mas-
simo Zanzotto. 2010. Syntactic/semantic struc-
tures for textual entailment recognition. In Human
Language Technologies: The 2010 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, HLT ’10, pages
1020–1028, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings
of ACL-08: HLT, pages 236–244, Columbus, Ohio,
June. Association for Computational Linguistics.
Fabio Massimo Zanzotto, Ioannis Korkontzelos,
Francesca Fallucchi, and Suresh Manandhar. 2010.
Estimating linear models for compositional distribu-
tional semantics. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics
(COLING), August,.
</reference>
<page confidence="0.996217">
98
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.861866">
<title confidence="0.982806">Compositional Distributional Semantics in Chunk-based Smoothed Tree Kernels</title>
<author confidence="0.984043">Nghia The Pham Lorenzo Ferrone</author>
<affiliation confidence="0.991038">University of Trento University of Rome “Tor Vergata”</affiliation>
<email confidence="0.941372">thenghia.pham@unitn.itlorenzo.ferrone@gmail.com</email>
<author confidence="0.999914">Fabio Massimo</author>
<affiliation confidence="0.999312">University of Rome “Tor Vergata”</affiliation>
<email confidence="0.984564">fabio.massimo.zanzotto@uniroma2.it</email>
<abstract confidence="0.998913842105263">The field of compositional distributional semantics has proposed very interesting and reliable models for accounting the distributional meaning of simple phrases. These models however tend to disregard the syntactic structures when they are applied to larger sentences. In this paper we the smoothed tree as a way to exploit the syntactic structures as well as the reliability of these compositional models for simple phrases. We experiment with the recognizing textual entailment datasets. Our experiments show that our CSTKs perform better than basic compositional distributional semantic models (CDSMs) recursively applied at the sentence level, and also better than syntactic tree kernels.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Abney</author>
</authors>
<title>Part-of-speech tagging and partial parsing.</title>
<date>1996</date>
<booktitle>In G.Bloothooft K.Church, S.Young, editor, Corpus-based methods in language and speech. Kluwer academic publishers,</booktitle>
<location>Dordrecht.</location>
<contexts>
<context position="2330" citStr="Abney, 1996" startWordPosition="332" endWordPosition="333"> 2010; Croce et al., 2011), yet, these models only use distributional vectors for words. Compositional distributional semantics models (CDSMs) on the other hand are functions mapping text fragments to vectors (or higher-order tensors) which then provide a distributional meaning for simple phrases or sentences. Many CDSMs have been proposed for simple phrases like nonrecursive noun phrases or verbal phrases (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Clark et al., 2008; Grefenstette and Sadrzadeh, 2011; Zanzotto et al., 2010). Non-recursive phrases are often referred to as chunks (Abney, 1996), and thus, CDSMs are good and reliable models for chunks. In this paper, we present the chunk-based smoothed tree kernels (CSTK) as a way to merge the two approaches: the smoothed tree kernels and the models for compositional distributional semantics. Our approach overcomes the limitation of the smoothed tree kernels which only use vectors for words by exploiting reliable CDSMs over chunks. CSTKs are defined over a chunk-based syntactic subtrees where terminal nodes are words or word sequences. We experimented with CSTKs on data from the recognizing textual entailment challenge (Dagan et al.,</context>
<context position="4176" citStr="Abney, 1996" startWordPosition="626" endWordPosition="628">sed syntactic subtree. Then, we describe the recursive formulation of the class of kernels. Finally, we introduce the basic CDSMs we use and we introduce two instances of the class of kernels. 93 Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 93–98, Dublin, Ireland, August 23-24 2014. 2.1 Notation and preliminaries ✭✭ ✭✭ ✭S❤❤❤❤❤ Figure 1: Sample Syntactic Tree A Chunk-based Syntactic Sub-Tree is a subtree of a syntactic tree where each non-terminal node dominating a contiguous word sequence is collapsed into a chunk and, as usual in chunks (Abney, 1996), the internal structure is disregarded. For example, Figure 2 reports some chunk-based syntactic subtrees of the tree in Figure 1. Chunks are represented with a pre-terminal node dominating a triangle that covers a word sequence. The first subtree represents the chunk covering the second NP and the node dominates the word sequence its:d final:n concert:n. The second subtree represents the structure of the whole sentence and one chunk, that is the first NP dominating the word sequence the:d rock:n band:n. The third subtree again represents the structure of the whole sentence split into two chu</context>
</contexts>
<marker>Abney, 1996</marker>
<rawString>Steven Abney. 1996. Part-of-speech tagging and partial parsing. In G.Bloothooft K.Church, S.Young, editor, Corpus-based methods in language and speech. Kluwer academic publishers, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1183--1193</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, MA,</location>
<contexts>
<context position="2183" citStr="Baroni and Zamparelli, 2010" startWordPosition="308" endWordPosition="311">ature has been used to integrate distributional semantics within tree kernels. This class of kernels is often referred to as smoothed tree kernels (Mehdad et al., 2010; Croce et al., 2011), yet, these models only use distributional vectors for words. Compositional distributional semantics models (CDSMs) on the other hand are functions mapping text fragments to vectors (or higher-order tensors) which then provide a distributional meaning for simple phrases or sentences. Many CDSMs have been proposed for simple phrases like nonrecursive noun phrases or verbal phrases (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Clark et al., 2008; Grefenstette and Sadrzadeh, 2011; Zanzotto et al., 2010). Non-recursive phrases are often referred to as chunks (Abney, 1996), and thus, CDSMs are good and reliable models for chunks. In this paper, we present the chunk-based smoothed tree kernels (CSTK) as a way to merge the two approaches: the smoothed tree kernels and the models for compositional distributional semantics. Our approach overcomes the limitation of the smoothed tree kernels which only use vectors for words by exploiting reliable CDSMs over chunks. CSTKs are defined over a chunk-based syntactic subtrees wh</context>
</contexts>
<marker>Baroni, Zamparelli, 2010</marker>
<rawString>Marco Baroni and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1183–1193, Cambridge, MA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>Bob Coecke</author>
<author>Mehrnoosh Sadrzadeh</author>
</authors>
<title>A compositional distributional model of meaning.</title>
<date>2008</date>
<booktitle>Proceedings of the Second Symposium on Quantum Interaction (QI-2008),</booktitle>
<pages>133--140</pages>
<contexts>
<context position="2203" citStr="Clark et al., 2008" startWordPosition="312" endWordPosition="315">ate distributional semantics within tree kernels. This class of kernels is often referred to as smoothed tree kernels (Mehdad et al., 2010; Croce et al., 2011), yet, these models only use distributional vectors for words. Compositional distributional semantics models (CDSMs) on the other hand are functions mapping text fragments to vectors (or higher-order tensors) which then provide a distributional meaning for simple phrases or sentences. Many CDSMs have been proposed for simple phrases like nonrecursive noun phrases or verbal phrases (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Clark et al., 2008; Grefenstette and Sadrzadeh, 2011; Zanzotto et al., 2010). Non-recursive phrases are often referred to as chunks (Abney, 1996), and thus, CDSMs are good and reliable models for chunks. In this paper, we present the chunk-based smoothed tree kernels (CSTK) as a way to merge the two approaches: the smoothed tree kernels and the models for compositional distributional semantics. Our approach overcomes the limitation of the smoothed tree kernels which only use vectors for words by exploiting reliable CDSMs over chunks. CSTKs are defined over a chunk-based syntactic subtrees where terminal nodes a</context>
</contexts>
<marker>Clark, Coecke, Sadrzadeh, 2008</marker>
<rawString>Stephen Clark, Bob Coecke, and Mehrnoosh Sadrzadeh. 2008. A compositional distributional model of meaning. Proceedings of the Second Symposium on Quantum Interaction (QI-2008), pages 133–140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL02.</booktitle>
<contexts>
<context position="6416" citStr="Collins and Duffy, 2002" startWordPosition="1030" endWordPosition="1033">ee t. For example, d(VP) in Figure 1 is holds:v its:p final:j concert:n. Chunk-based Syntactic Sub-Trees (CSSTs) are instead denoted with the letter T. Differently from trees t, CSSTs have terminal nodes that can represent subsequences of words of the original sentence. The explicit syntactic structure of a CSST is the structure not falling in chunks and it is represented as s(T). For example, s(T3) is: S ✟ ✟ ❍❍ NP VP /\ VBZ NP where T3 is the third subtree of Figure 2. Given a tree t, the set S(t) is defined as the set containing all the relevant CSSTs of the tree t. As for the tree kernels (Collins and Duffy, 2002), the set S(t) contains all CSSTs derived from the subtrees of t such that if a node n belongs to a subtree ts, all the siblings of n in t belongs to ts. In other words, productions of the initial subtrees are complete. A CSST is obtained by collapsing in a single terminal nodes a contiguous sequence of words dominated by a single non-terminal node. For example: NP ✏ ✏ �� DT NN ✟✟ ❍❍ is collapsed into: NP ✦ ✦ ❛❛ DT NN:X ✏✏ ✏ PPP rock:n band:n Finally, wn E Ism represent the distributional → vectors for words wn and f(w1 ... wk) represents a compositional distributional semantics model applied </context>
<context position="13895" citStr="Collins and Duffy, 2002" startWordPosition="2348" endWordPosition="2351">rity (as in (Corley and Mihalcea, 2005)). 3.2 Results Table 1 shows the results of the experiments, the table is organised as follows: columns 2-6 report the accuracy of the RTE systems based on rewrite rules (RR) and columns 7-11 report the accuracies of RR systems along with token similarity (RRTS). We compare five differente models: ADD is the Basic Additive model with parameters α = Q = 1 (as defined in 2.3) applied to the words of the sentence (without considering its tree structure), the same is done for the Full Additive (FullADD), defined as in 2.3. The Tree Kernel (TK) as defined in (Collins and Duffy, 2002) are applied to fFA(s) = { 96 the constituency-based tree representation of the tree, without the intervening collapsing step described in 2.2. These three models are the baseline against which we compare the CSTK models where the collapsing procedure is done via Basic Additive (CSTK + BA, again with α = Q = 1) and FullAdditive (CSTK + FA), as described in section 2.2, again, with the aforementioned restriction on the relation considered. For RR models we have that CSTK+BA and CSTK+FA both achieve higher accuracy than ADD and FullAdd, with a statistical significante greater than 93.7%, as comp</context>
</contexts>
<marker>Collins, Duffy, 2002</marker>
<rawString>Michael Collins and Nigel Duffy. 2002. New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron. In Proceedings of ACL02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Courtney Corley</author>
<author>Rada Mihalcea</author>
</authors>
<title>Measuring the semantic similarity of texts.</title>
<date>2005</date>
<booktitle>In Proc. of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment,</booktitle>
<pages>13--18</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="13310" citStr="Corley and Mihalcea, 2005" startWordPosition="2241" endWordPosition="2244">hen syntactic relations are different from these two. To build the final kernel to learn the classifier, we followed standard approaches (Dagan et al., 2013), that is, we exploited two models: a model with only a rewrite rule feature space (RR) and a model with the previous space along with a token-level similarity feature (RRTWS). The two models use our CSTKs and the standard TKs in the following way as kernel functions: (1) RR(p1, p2) = κ(ta1, ta2) + κ(tb1, tb2); (2) RRTW5(p1, p2) = κ(ta1, ta2) + κ(tb1, tb2) + (TW5(a1, b1) · TW5(a2, b2) + 1)2 where TW5 is a weighted token similarity (as in (Corley and Mihalcea, 2005)). 3.2 Results Table 1 shows the results of the experiments, the table is organised as follows: columns 2-6 report the accuracy of the RTE systems based on rewrite rules (RR) and columns 7-11 report the accuracies of RR systems along with token similarity (RRTS). We compare five differente models: ADD is the Basic Additive model with parameters α = Q = 1 (as defined in 2.3) applied to the words of the sentence (without considering its tree structure), the same is done for the Full Additive (FullADD), defined as in 2.3. The Tree Kernel (TK) as defined in (Collins and Duffy, 2002) are applied to</context>
</contexts>
<marker>Corley, Mihalcea, 2005</marker>
<rawString>Courtney Corley and Rada Mihalcea. 2005. Measuring the semantic similarity of texts. In Proc. of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pages 13–18. Association for Computational Linguistics, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danilo Croce</author>
<author>Alessandro Moschitti</author>
<author>Roberto Basili</author>
</authors>
<title>Structured lexical similarity via convolution kernels on dependency trees.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<pages>1034--1046</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1744" citStr="Croce et al., 2011" startWordPosition="242" endWordPosition="245">ons for sentences is important for many high-level NLP tasks, such as question-answering, textual entailment recognition, and semantic textual similarity. Systems and models for these tasks often use classifiers or regressors that exploit convolution kernels (Haussler, 1999) to model both interpretations. Convolution kernels are naturally defined on spaces where there exists a similarity function between terminal nodes. This feature has been used to integrate distributional semantics within tree kernels. This class of kernels is often referred to as smoothed tree kernels (Mehdad et al., 2010; Croce et al., 2011), yet, these models only use distributional vectors for words. Compositional distributional semantics models (CDSMs) on the other hand are functions mapping text fragments to vectors (or higher-order tensors) which then provide a distributional meaning for simple phrases or sentences. Many CDSMs have been proposed for simple phrases like nonrecursive noun phrases or verbal phrases (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Clark et al., 2008; Grefenstette and Sadrzadeh, 2011; Zanzotto et al., 2010). Non-recursive phrases are often referred to as chunks (Abney, 1996), and thus, CD</context>
</contexts>
<marker>Croce, Moschitti, Basili, 2011</marker>
<rawString>Danilo Croce, Alessandro Moschitti, and Roberto Basili. 2011. Structured lexical similarity via convolution kernels on dependency trees. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 1034–1046, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The pascal recognising textual entailment challenge.</title>
<date>2006</date>
<booktitle>LNAI 3944: MLCW 2005,</booktitle>
<pages>177--190</pages>
<editor>In Quionero-Candela et al., editor,</editor>
<publisher>SpringerVerlag,</publisher>
<location>Milan, Italy.</location>
<contexts>
<context position="2936" citStr="Dagan et al., 2006" startWordPosition="426" endWordPosition="429">(Abney, 1996), and thus, CDSMs are good and reliable models for chunks. In this paper, we present the chunk-based smoothed tree kernels (CSTK) as a way to merge the two approaches: the smoothed tree kernels and the models for compositional distributional semantics. Our approach overcomes the limitation of the smoothed tree kernels which only use vectors for words by exploiting reliable CDSMs over chunks. CSTKs are defined over a chunk-based syntactic subtrees where terminal nodes are words or word sequences. We experimented with CSTKs on data from the recognizing textual entailment challenge (Dagan et al., 2006) and we compared our CSTKs with other standard tree kernels and standard recursive CDSMs. Experiments show that our CSTKs perform better than basic compositional distributional semantic models (CDSMs) recursively applied at the sentence level and better than syntactic tree kernels. The rest of the paper is organized as follows. Section 2 describes the CSTKs. Section 3 reports on the experimental setting and on the results. Finally, Section 4 draws the conclusions and sketches the future work. 2 Chunk-based Smoothed Tree Kernels This section describes the new class of kernels. We first introduc</context>
<context position="11256" citStr="Dagan et al., 2006" startWordPosition="1904" endWordPosition="1907">lysis: Accuracy on Recognizing Textual Entailment († is different from both ADD and FullADD with a stat.sig. of p &gt; 0.1.) The full additive model for word sequences s = w1 ... wk, whose node has a production rule s → sc1 ... scm is also defined recursively: w1→ ifk=1 → AvnV + BvnfFA(NP) ifs → V NP → AanA + BanfFA(N) ifs → A N E fFA(sci) otherwise where Avn, Bvn are matrices used for verb and noun phrase interaction, and Aan, Ban are used for adjective, noun interaction. 3 Experimental Investigation 3.1 Experimental set-up We experimented with the Recognizing Textual Entailment datasets (RTE) (Dagan et al., 2006). RTE is the task of deciding whether a long text T entails a shorter text, typically a single sentence, called hypothesis H. It has been often seen as a classification task (see (Dagan et al., 2013)). We used four datasets: RTE1, RTE2, RTE3, and RTE5, with the standard split between training and testing. The dev/test distribution for RTE1-3, and RTE5 is respectively 567/800, 800/800, 800/800, and 600/600 T-H pairs. Distributional vectors are derived with DISSECT (Dinu et al., 2013) from a corpus obtained by the concatenation of ukWaC (wacky.sslmit.unibo.it), a mid-2009 dump of the English Wik</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2006</marker>
<rawString>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006. The pascal recognising textual entailment challenge. In Quionero-Candela et al., editor, LNAI 3944: MLCW 2005, pages 177–190. SpringerVerlag, Milan, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Dan Roth</author>
<author>Mark Sammons</author>
<author>Fabio Massimo Zanzotto</author>
</authors>
<title>Recognizing Textual Entailment: Models and Applications. Synthesis Lectures on Human Language Technologies.</title>
<date>2013</date>
<publisher>Morgan &amp; Claypool Publishers.</publisher>
<contexts>
<context position="11455" citStr="Dagan et al., 2013" startWordPosition="1941" endWordPosition="1944">oduction rule s → sc1 ... scm is also defined recursively: w1→ ifk=1 → AvnV + BvnfFA(NP) ifs → V NP → AanA + BanfFA(N) ifs → A N E fFA(sci) otherwise where Avn, Bvn are matrices used for verb and noun phrase interaction, and Aan, Ban are used for adjective, noun interaction. 3 Experimental Investigation 3.1 Experimental set-up We experimented with the Recognizing Textual Entailment datasets (RTE) (Dagan et al., 2006). RTE is the task of deciding whether a long text T entails a shorter text, typically a single sentence, called hypothesis H. It has been often seen as a classification task (see (Dagan et al., 2013)). We used four datasets: RTE1, RTE2, RTE3, and RTE5, with the standard split between training and testing. The dev/test distribution for RTE1-3, and RTE5 is respectively 567/800, 800/800, 800/800, and 600/600 T-H pairs. Distributional vectors are derived with DISSECT (Dinu et al., 2013) from a corpus obtained by the concatenation of ukWaC (wacky.sslmit.unibo.it), a mid-2009 dump of the English Wikipedia (en.wikipedia.org) and the British National Corpus (www.natcorp.ox.ac.uk), for a total of about 2.8 billion words. We collected a 35K-by-35K matrix by counting co-occurrence of the 30K most fr</context>
<context position="12841" citStr="Dagan et al., 2013" startWordPosition="2153" endWordPosition="2156">re transformed into positive Pointwise Mutual Information scores and reduced to 300 dimensions by Singular Value Decomposition. This setup was picked without tuning, as we found it effective in previous, unrelated experiments. We built the matrices for the full additive models using the procedure described in (Guevara, 2010). We considered only two relations: the Adjective-Noun and Verb-Noun. The full additive model falls back to the basic additional model when syntactic relations are different from these two. To build the final kernel to learn the classifier, we followed standard approaches (Dagan et al., 2013), that is, we exploited two models: a model with only a rewrite rule feature space (RR) and a model with the previous space along with a token-level similarity feature (RRTWS). The two models use our CSTKs and the standard TKs in the following way as kernel functions: (1) RR(p1, p2) = κ(ta1, ta2) + κ(tb1, tb2); (2) RRTW5(p1, p2) = κ(ta1, ta2) + κ(tb1, tb2) + (TW5(a1, b1) · TW5(a2, b2) + 1)2 where TW5 is a weighted token similarity (as in (Corley and Mihalcea, 2005)). 3.2 Results Table 1 shows the results of the experiments, the table is organised as follows: columns 2-6 report the accuracy of </context>
</contexts>
<marker>Dagan, Roth, Sammons, Zanzotto, 2013</marker>
<rawString>Ido Dagan, Dan Roth, Mark Sammons, and Fabio Massimo Zanzotto. 2013. Recognizing Textual Entailment: Models and Applications. Synthesis Lectures on Human Language Technologies. Morgan &amp; Claypool Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgiana Dinu</author>
<author>Nghia The Pham</author>
<author>Marco Baroni</author>
</authors>
<title>DISSECT: DIStributional SEmantics Composition Toolkit.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL (System Demonstrations),</booktitle>
<pages>31--36</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="11743" citStr="Dinu et al., 2013" startWordPosition="1984" endWordPosition="1987">l Investigation 3.1 Experimental set-up We experimented with the Recognizing Textual Entailment datasets (RTE) (Dagan et al., 2006). RTE is the task of deciding whether a long text T entails a shorter text, typically a single sentence, called hypothesis H. It has been often seen as a classification task (see (Dagan et al., 2013)). We used four datasets: RTE1, RTE2, RTE3, and RTE5, with the standard split between training and testing. The dev/test distribution for RTE1-3, and RTE5 is respectively 567/800, 800/800, 800/800, and 600/600 T-H pairs. Distributional vectors are derived with DISSECT (Dinu et al., 2013) from a corpus obtained by the concatenation of ukWaC (wacky.sslmit.unibo.it), a mid-2009 dump of the English Wikipedia (en.wikipedia.org) and the British National Corpus (www.natcorp.ox.ac.uk), for a total of about 2.8 billion words. We collected a 35K-by-35K matrix by counting co-occurrence of the 30K most frequent content lemmas in the corpus (nouns, adjectives and verbs) and all the content lemmas occurring in the datasets within a 3 word window. The raw count vectors were transformed into positive Pointwise Mutual Information scores and reduced to 300 dimensions by Singular Value Decompos</context>
</contexts>
<marker>Dinu, Pham, Baroni, 2013</marker>
<rawString>Georgiana Dinu, Nghia The Pham, and Marco Baroni. 2013. DISSECT: DIStributional SEmantics Composition Toolkit. In Proceedings of ACL (System Demonstrations), pages 31–36, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Grefenstette</author>
<author>Mehrnoosh Sadrzadeh</author>
</authors>
<title>Experimental support for a categorical compositional distributional model of meaning.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<pages>1394--1404</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2237" citStr="Grefenstette and Sadrzadeh, 2011" startWordPosition="316" endWordPosition="319">emantics within tree kernels. This class of kernels is often referred to as smoothed tree kernels (Mehdad et al., 2010; Croce et al., 2011), yet, these models only use distributional vectors for words. Compositional distributional semantics models (CDSMs) on the other hand are functions mapping text fragments to vectors (or higher-order tensors) which then provide a distributional meaning for simple phrases or sentences. Many CDSMs have been proposed for simple phrases like nonrecursive noun phrases or verbal phrases (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Clark et al., 2008; Grefenstette and Sadrzadeh, 2011; Zanzotto et al., 2010). Non-recursive phrases are often referred to as chunks (Abney, 1996), and thus, CDSMs are good and reliable models for chunks. In this paper, we present the chunk-based smoothed tree kernels (CSTK) as a way to merge the two approaches: the smoothed tree kernels and the models for compositional distributional semantics. Our approach overcomes the limitation of the smoothed tree kernels which only use vectors for words by exploiting reliable CDSMs over chunks. CSTKs are defined over a chunk-based syntactic subtrees where terminal nodes are words or word sequences. We exp</context>
</contexts>
<marker>Grefenstette, Sadrzadeh, 2011</marker>
<rawString>Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011. Experimental support for a categorical compositional distributional model of meaning. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 1394–1404, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emiliano Guevara</author>
</authors>
<title>A regression model of adjective-noun compositionality in distributional semantics.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics,</booktitle>
<pages>33--37</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="9785" citStr="Guevara, 2010" startWordPosition="1646" endWordPosition="1648"> similarity KF(τ3, τ4) is: (f(the:d orchestra:n), f(the:d rock:n band:n)) � (f(its:p show:n), f(its:p final:j concert:n)). The recursive formulation of the Chunk-based Smoothed Tree Kernel (CSTK) is a bit more complex but very similar to the recursive formulation of the syntactic tree kernels: K(t1, t2) = X C(n1, n2) (2) n1 E N(t1) n2 E N(t2) where α and β weight the first and the second word of the pair. The basic additive model for word sequences s = w1 ... wk is recursively defined as follows: fBA(s) = ( → w1 if k = 1 → α w1 + βfBA(w2 ... wk) if k &gt; 1 The Full Additive model (FA) (used in (Guevara, 2010) for adjective-noun pairs and (Zanzotto et al., 2010) for three different syntactic relations) → computes the compositional vector a of a pair using two linear tranformations AR and BR respectively applied to the vectors of the first and the second word. These matrices generally only depends on the syntactic relation R that links those two words. The operation follows: → → fFA(a1, a2, R) = ARa1 + BRa2 τ4 = VP ✦ ✦ ❛❛ VBZ NP 95 RR RRTWS RTE1 RTE2 RTE3 RTE5 Average RTE1 RTE2 RTE3 RTE5 Average Add 0.541 0.496 0.507 0.520 0.516 0.560 0.538 0.643 0.578 0.579 FullAdd 0.512 0.516 0.507 0.569 0.526 0.5</context>
<context position="12548" citStr="Guevara, 2010" startWordPosition="2108" endWordPosition="2109">k), for a total of about 2.8 billion words. We collected a 35K-by-35K matrix by counting co-occurrence of the 30K most frequent content lemmas in the corpus (nouns, adjectives and verbs) and all the content lemmas occurring in the datasets within a 3 word window. The raw count vectors were transformed into positive Pointwise Mutual Information scores and reduced to 300 dimensions by Singular Value Decomposition. This setup was picked without tuning, as we found it effective in previous, unrelated experiments. We built the matrices for the full additive models using the procedure described in (Guevara, 2010). We considered only two relations: the Adjective-Noun and Verb-Noun. The full additive model falls back to the basic additional model when syntactic relations are different from these two. To build the final kernel to learn the classifier, we followed standard approaches (Dagan et al., 2013), that is, we exploited two models: a model with only a rewrite rule feature space (RR) and a model with the previous space along with a token-level similarity feature (RRTWS). The two models use our CSTKs and the standard TKs in the following way as kernel functions: (1) RR(p1, p2) = κ(ta1, ta2) + κ(tb1, </context>
</contexts>
<marker>Guevara, 2010</marker>
<rawString>Emiliano Guevara. 2010. A regression model of adjective-noun compositionality in distributional semantics. In Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics, pages 33–37, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Haussler</author>
</authors>
<title>Convolution kernels on discrete structures.</title>
<date>1999</date>
<tech>Technical report,</tech>
<institution>University of California at Santa Cruz.</institution>
<contexts>
<context position="1400" citStr="Haussler, 1999" startWordPosition="191" endWordPosition="192">periment with the recognizing textual entailment datasets. Our experiments show that our CSTKs perform better than basic compositional distributional semantic models (CDSMs) recursively applied at the sentence level, and also better than syntactic tree kernels. 1 Introduction A clear interaction between syntactic and semantic interpretations for sentences is important for many high-level NLP tasks, such as question-answering, textual entailment recognition, and semantic textual similarity. Systems and models for these tasks often use classifiers or regressors that exploit convolution kernels (Haussler, 1999) to model both interpretations. Convolution kernels are naturally defined on spaces where there exists a similarity function between terminal nodes. This feature has been used to integrate distributional semantics within tree kernels. This class of kernels is often referred to as smoothed tree kernels (Mehdad et al., 2010; Croce et al., 2011), yet, these models only use distributional vectors for words. Compositional distributional semantics models (CDSMs) on the other hand are functions mapping text fragments to vectors (or higher-order tensors) which then provide a distributional meaning for</context>
</contexts>
<marker>Haussler, 1999</marker>
<rawString>David Haussler. 1999. Convolution kernels on discrete structures. Technical report, University of California at Santa Cruz.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yashar Mehdad</author>
<author>Alessandro Moschitti</author>
<author>Fabio Massimo Zanzotto</author>
</authors>
<title>Syntactic/semantic structures for textual entailment recognition.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10,</booktitle>
<pages>1020--1028</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1723" citStr="Mehdad et al., 2010" startWordPosition="238" endWordPosition="241">semantic interpretations for sentences is important for many high-level NLP tasks, such as question-answering, textual entailment recognition, and semantic textual similarity. Systems and models for these tasks often use classifiers or regressors that exploit convolution kernels (Haussler, 1999) to model both interpretations. Convolution kernels are naturally defined on spaces where there exists a similarity function between terminal nodes. This feature has been used to integrate distributional semantics within tree kernels. This class of kernels is often referred to as smoothed tree kernels (Mehdad et al., 2010; Croce et al., 2011), yet, these models only use distributional vectors for words. Compositional distributional semantics models (CDSMs) on the other hand are functions mapping text fragments to vectors (or higher-order tensors) which then provide a distributional meaning for simple phrases or sentences. Many CDSMs have been proposed for simple phrases like nonrecursive noun phrases or verbal phrases (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Clark et al., 2008; Grefenstette and Sadrzadeh, 2011; Zanzotto et al., 2010). Non-recursive phrases are often referred to as chunks (Abney</context>
</contexts>
<marker>Mehdad, Moschitti, Zanzotto, 2010</marker>
<rawString>Yashar Mehdad, Alessandro Moschitti, and Fabio Massimo Zanzotto. 2010. Syntactic/semantic structures for textual entailment recognition. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10, pages 1020–1028, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Vector-based models of semantic composition.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>236--244</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="2154" citStr="Mitchell and Lapata, 2008" startWordPosition="304" endWordPosition="307">een terminal nodes. This feature has been used to integrate distributional semantics within tree kernels. This class of kernels is often referred to as smoothed tree kernels (Mehdad et al., 2010; Croce et al., 2011), yet, these models only use distributional vectors for words. Compositional distributional semantics models (CDSMs) on the other hand are functions mapping text fragments to vectors (or higher-order tensors) which then provide a distributional meaning for simple phrases or sentences. Many CDSMs have been proposed for simple phrases like nonrecursive noun phrases or verbal phrases (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Clark et al., 2008; Grefenstette and Sadrzadeh, 2011; Zanzotto et al., 2010). Non-recursive phrases are often referred to as chunks (Abney, 1996), and thus, CDSMs are good and reliable models for chunks. In this paper, we present the chunk-based smoothed tree kernels (CSTK) as a way to merge the two approaches: the smoothed tree kernels and the models for compositional distributional semantics. Our approach overcomes the limitation of the smoothed tree kernels which only use vectors for words by exploiting reliable CDSMs over chunks. CSTKs are defined over a chun</context>
<context position="9003" citStr="Mitchell and Lapata, 2008" startWordPosition="1490" endWordPosition="1493">minals and prod(n1) = prod(n2) 0 otherwise where nc(n1) is the lenght of the production prod(n1). 2.3 Compositional Distributional Semantic Models and two Specific CSTKs To define specific CSTKs, we need to introduce the basic compositional distributional semantic models (CDSMs). We use two CDSMs: the Basic Additive model (BA) and teh Full Additive model (FA). We thus define two specific CSTKs: the CSTK+BA that is based on the basic additive model and the CSTK+FA that is based on the full additive model. We describe the two CDSMs in the following. The Basic Additive model (BA) (introduced in (Mitchell and Lapata, 2008)) computes the distibutional semantics vector of a pair of words a = a1a2 as: ADD(a1, a2) = α→a1 + βa2→ S ✘ ✘✘ ❳❳❳ NP ✘ ✘✘ ❳❳❳ the:d orchestra:n ✏ ✏ �� its:p show:n the similarity KF(τ3, τ4) is: (f(the:d orchestra:n), f(the:d rock:n band:n)) � (f(its:p show:n), f(its:p final:j concert:n)). The recursive formulation of the Chunk-based Smoothed Tree Kernel (CSTK) is a bit more complex but very similar to the recursive formulation of the syntactic tree kernels: K(t1, t2) = X C(n1, n2) (2) n1 E N(t1) n2 E N(t2) where α and β weight the first and the second word of the pair. The basic additive mode</context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. In Proceedings of ACL-08: HLT, pages 236–244, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Massimo Zanzotto</author>
<author>Ioannis Korkontzelos</author>
<author>Francesca Fallucchi</author>
<author>Suresh Manandhar</author>
</authors>
<title>Estimating linear models for compositional distributional semantics.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (COLING),</booktitle>
<contexts>
<context position="2261" citStr="Zanzotto et al., 2010" startWordPosition="320" endWordPosition="323"> class of kernels is often referred to as smoothed tree kernels (Mehdad et al., 2010; Croce et al., 2011), yet, these models only use distributional vectors for words. Compositional distributional semantics models (CDSMs) on the other hand are functions mapping text fragments to vectors (or higher-order tensors) which then provide a distributional meaning for simple phrases or sentences. Many CDSMs have been proposed for simple phrases like nonrecursive noun phrases or verbal phrases (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Clark et al., 2008; Grefenstette and Sadrzadeh, 2011; Zanzotto et al., 2010). Non-recursive phrases are often referred to as chunks (Abney, 1996), and thus, CDSMs are good and reliable models for chunks. In this paper, we present the chunk-based smoothed tree kernels (CSTK) as a way to merge the two approaches: the smoothed tree kernels and the models for compositional distributional semantics. Our approach overcomes the limitation of the smoothed tree kernels which only use vectors for words by exploiting reliable CDSMs over chunks. CSTKs are defined over a chunk-based syntactic subtrees where terminal nodes are words or word sequences. We experimented with CSTKs on </context>
<context position="9838" citStr="Zanzotto et al., 2010" startWordPosition="1653" endWordPosition="1656">:n), f(the:d rock:n band:n)) � (f(its:p show:n), f(its:p final:j concert:n)). The recursive formulation of the Chunk-based Smoothed Tree Kernel (CSTK) is a bit more complex but very similar to the recursive formulation of the syntactic tree kernels: K(t1, t2) = X C(n1, n2) (2) n1 E N(t1) n2 E N(t2) where α and β weight the first and the second word of the pair. The basic additive model for word sequences s = w1 ... wk is recursively defined as follows: fBA(s) = ( → w1 if k = 1 → α w1 + βfBA(w2 ... wk) if k &gt; 1 The Full Additive model (FA) (used in (Guevara, 2010) for adjective-noun pairs and (Zanzotto et al., 2010) for three different syntactic relations) → computes the compositional vector a of a pair using two linear tranformations AR and BR respectively applied to the vectors of the first and the second word. These matrices generally only depends on the syntactic relation R that links those two words. The operation follows: → → fFA(a1, a2, R) = ARa1 + BRa2 τ4 = VP ✦ ✦ ❛❛ VBZ NP 95 RR RRTWS RTE1 RTE2 RTE3 RTE5 Average RTE1 RTE2 RTE3 RTE5 Average Add 0.541 0.496 0.507 0.520 0.516 0.560 0.538 0.643 0.578 0.579 FullAdd 0.512 0.516 0.507 0.569 0.526 0.571 0.608 0.643 0.643 0.616 TK 0.561 0.552 0.531 0.54 </context>
</contexts>
<marker>Zanzotto, Korkontzelos, Fallucchi, Manandhar, 2010</marker>
<rawString>Fabio Massimo Zanzotto, Ioannis Korkontzelos, Francesca Fallucchi, and Suresh Manandhar. 2010. Estimating linear models for compositional distributional semantics. In Proceedings of the 23rd International Conference on Computational Linguistics (COLING), August,.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>