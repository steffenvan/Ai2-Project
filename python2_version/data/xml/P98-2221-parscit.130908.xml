<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.904635">
Modeling with Structures in Statistical Machine Translation
</title>
<author confidence="0.99645">
Ye-Yi Wang and Alex Waibel
</author>
<affiliation confidence="0.988978">
School of Computer Science
Carnegie Mellon University
</affiliation>
<address confidence="0.5639585">
5000 Forbes Avenue
Pittsburgh, PA 15213, USA
</address>
<email confidence="0.919404">
{yyw,waibel}Ocs.cmu.edu
</email>
<sectionHeader confidence="0.992927" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999921666666667">
Most statistical machine translation systems
employ a word-based alignment model. In this
paper we demonstrate that word-based align-
ment is a major cause of translation errors. We
propose a new alignment model based on shal-
low phrase structures, and the structures can
be automatically acquired from parallel corpus.
This new model achieved over 10% error reduc-
tion for our spoken language translation task.
</bodyText>
<sectionHeader confidence="0.998424" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996643857142857">
Most (if not all) statistical machine translation
systems employ a word-based alignment model
(Brown et al., 1993; Vogel, Ney, and Tillman,
1996; Wang and Waibel, 1997), which treats
words in a sentence as independent entities and
ignores the structural relationship among them.
While this independence assumption works well
in speech recognition, it poses a major problem
in our experiments with spoken language trans-
lation between a language pair with very dif-
ferent word orders. In this paper we propose a
translation model that employs shallow phrase
structures. It has the following advantages over
word-based alignment:
</bodyText>
<listItem confidence="0.8053155">
• Since the translation model can directly de-
pict phrase reordering in translation, it is
more accurate for translation between lan-
guages with different word (phrase) orders.
• The decoder of the translation system can
use the phrase information and extend
hypothesis by phrases (multiple words),
therefore it can speed up decoding.
</listItem>
<bodyText confidence="0.999826727272727">
The paper is organized as follows. In sec-
tion 2, the problems of word-based alignment
models are discussed. To alienate these prob-
lems, a new alignment model based on shal-
low phrase structures is introduced in section
3. In section 4, a grammar inference algorithm
is presented that can automatically acquire the
phrase structures used in the new model. Trans-
lation performance is then evaluated in sec-
tion 5, and conclusions are presented in sec-
tion 6.
</bodyText>
<sectionHeader confidence="0.978632" genericHeader="method">
2 Word-based Alignment Model
</sectionHeader>
<bodyText confidence="0.4948622">
In a word-based alignment translation model,
the transformation from a sentence at the source
end of a communication channel to a sentence
at the target end can be described with the fol-
lowing random process:
</bodyText>
<listItem confidence="0.9987352">
1. Pick a length for the sentence at the target
end.
2. For each word position in the target sen-
tence, align it with a source word.
3. Produce a word at each target word po-
</listItem>
<bodyText confidence="0.890386625">
sition according to the source word with
which the target word position has been
aligned.
IBM Alignment Model 2 is a typical example
of word-based alignment. Assuming a sentence
s = si,• • •,s/ at the source of a channel, the
model picks a length m of the target sentence
t according to the distribution P(m I s) = f,
where is a small, fixed number. Then for each
position i (0 &lt; i &lt; m) in t, it finds its corre-
sponding position ai in s according to an align-
ment distribution P(a, m, s) = a(a, I
i,m,1). Finally, it generates a word t, at the
position i of t from the source word sa, at the
aligned position ai, according to a translation
distribution P(t I ei-1, s) = t(t, I sa,).
</bodyText>
<page confidence="0.949082">
1357
</page>
<footnote confidence="0.54330275">
waere denn Montag der sech und zwanzigste Juli moeglich
it &apos;s going to difficulty to find meeting time I think is Monday the twenty sixth of July possible
waere denn Montag der sech und zwanzigste full moeglich
it &apos;s going to difficulty to find meeting time I think is Monday the twenty sixth of July possible
</footnote>
<figureCaption confidence="0.974965">
Figure 1: Word Alignment with deletion in translation: the top alignment is the one made by IBM
Alignment Model 2, the bottom one is the &apos;ideal&apos; alignment.
</figureCaption>
<figure confidence="0.87083">
fuer der zweiten Termin im Mai koennte ich den Mittwoch den fuenf und zwanzigsten anbieten
I could offer you Wednesday the twenty fifth for the second date in May
fuer der zweiten Termin im Mai koennte ich den Mittwoch den fuenf und zwanzigsten anbieten
I could offer you Wednesday the twenty fifth for the second date in May
</figure>
<figureCaption confidence="0.977563">
Figure 2: Word Alignment of translation with different phrase order: the top alignment is the one
made by IBM Alignment Model 2, the bottom one is the &apos;ideal&apos; alignment.
</figureCaption>
<figure confidence="0.65188">
fuer der zweiten Termin im Mai koennte ich den Mittwoch den fuenf und zwanzigsten anbieten
I could offer you Wednesday the twenty fifth for the second date in May
</figure>
<figureCaption confidence="0.966566">
Figure 3: Word Alignment with Model 1 for one of the previous examples. Because no alignment
probability penalizes the long distance phrase reordering, it is much closer to the &apos;ideal&apos; alignment.
</figureCaption>
<page confidence="0.985316">
1358
</page>
<bodyText confidence="0.98125675">
Therefore, P(t I s) is the sum of the proba-
bilities of generating t from s over all possible
alignments A, in which the position i in t is
aligned with the position ai in s:
</bodyText>
<equation confidence="0.635382571428571">
P(t I s)
m
= EE—•E llt(t
a1=0 am=0 j=1
m
= crlEt(tilsi)a(il j,l,m)
j=1i=o
</equation>
<bodyText confidence="0.997400368421053">
A word-based model may have severe prob-
lems when there are deletions in translation
(this may be a result of erroneous sentence
alignment) or the two languages have different
word orders, like English and German. Figure 1
and Figure 2 show some problematic alignments
between English/German sentences made by
IBM Model 2, together with the &apos;ideal&apos; align-
ments for the sentences. Here the alignment
parameters penalize the alignment of English
words with their German translation equiva-
lents because the translation equivalents are far
away from the words.
An experiment reveals how often this kind
of &amp;quot;skewed&amp;quot; alignment happens in our En-
glish/German scheduling conversation parallel
corpus (Wang and Waibel, 1997). The ex-
periment was based on the following obser-
vation: IBM translation Model 1 (where the
alignment distribution is uniform) and Model
2 found similar Viterbi alignments when there
were no movements or deletions, and they pre-
dicted very different Viterbi alignments when
the skewness was severe in a sentence pair, since
the alignment parameters in Model 2 penalize
the long distance alignment. Figure 3 shows the
Viterbi alignment discovered by Model 1 for the
same sentences in Figure 21.
We measured the distance of a Model 1
alignment al and a Model 2 alignment a2
as El la! — an. To estimate the skew-
ness of the corpus, we collected the statistics
about the percentage of sentence pairs (with at
&apos;The better alignment on a given pair of sentences
does not mean Model 1 is a better model. Non-uniform
alignment distribution is desirable. Otherwise, language
model would be the only factor that determines the
source sentence word order in decoding.
</bodyText>
<figure confidence="0.993050375">
30
25
20
15
10
0
0 0.5 1 1.5 2 25
Alignment distance &gt; x * target sentence length
</figure>
<figureCaption confidence="0.999992">
Figure 4: Skewness of Translations
</figureCaption>
<bodyText confidence="0.999913181818182">
least five words in a sentence) with Model 1
and Model 2 alignment distance greater than
1/4,2/4,3/4, • • • , 10/4 of the target sentence
length. By checking the Viterbi alignments
made by both models, it is almost certain that
whenever the distance is greater that 3/4 of the
target sentence length, there is either a move-
ment or a deletion in the sentence pair. Fig-
ure 4 plots this statistic — around 30% of the
sentence pairs in our training data have some
degree of skewness in alignments.
</bodyText>
<sectionHeader confidence="0.994618" genericHeader="method">
3 Structure-based Alignment Model
</sectionHeader>
<bodyText confidence="0.999831111111111">
To solve the problems with the word-based
alignment models, we present a structure-based
alignment model here. The idea is to di-
rectly model the phrase movement with a rough
alignment, and then model the word alignment
within phrases with a detailed alignment.
Given an English sentence e = e1e2 • • • ei, its
German translation g = gig2 • • • gin can be gen-
erated by the following process:
</bodyText>
<listItem confidence="0.986454">
1. Parse e into a sequence of phrases, so
</listItem>
<equation confidence="0.969827">
E = (en, eu, • • • , e 22 • • • e2/2) • &apos; •
(eni en2, • • • , enin)
EIDE]. E2 • • • En,
</equation>
<bodyText confidence="0.673093428571429">
where E0 is a null phrase.
2. With the probability P(q I e,E), deter-
mine q &lt; n 1, the number of phrases in
g. Let G1 • • • Gq denote these q phrases.
Each source phrase can be aligned with at
most one target phrase. Unlike English
phrases, words in a German phrase do not
</bodyText>
<figure confidence="0.8919455">
Sample Percentage
I
</figure>
<page confidence="0.981993">
1359
</page>
<bodyText confidence="0.870382857142857">
have to form a consecutive sequence. So
g may be expressed with something like
g = gl1gi2g2013g22 • • where gij repre-
sents the j-th word in the i-th phrase.
3. For each German phrase Gi, 0 &lt; i &lt; q, with
the probability P(ri E,e), align it
with an English phrase Er,.
</bodyText>
<listItem confidence="0.704118090909091">
4. For each German phrase G2,0 &lt; i &lt; q, de-
termine its beginning position bi in g with
the distribution P(bi e, E).
5. Now it is time to generate the individual
words in the German phrases through de-
tailed alignment. It works like IBM Model
4. For each word eij in the phrase Ei,
its fertility ckij has the distribution P(Oii
4/1-1 b, rg, e, E)•
6. For each word eij in the phrase Ei, it gen-
erates a tablet r = {rim rii2, • • • Tijo,, }
</listItem>
<bodyText confidence="0.978225111111111">
by generating each of the words in
in turn with the probability P(riik
k-1Ti 3i-1 r1, , bq, ro q, e E) for the k-th
o o
word in the tablet.
7. For each element rijk in the tablet
rt3, the permutation rijk determines
its position in the target sentence ac-
cording to the distribution P(riik
</bodyText>
<equation confidence="0.734702">
,r1-1 bq „q E)
„ij1 , „ , „ , , 0, 0, 0, 0, •
</equation>
<bodyText confidence="0.9816204">
We made the following independence assump-
tions:
1. The number of target sentence phrases de-
pends only on the number of phrases in the
source sentence:
</bodyText>
<equation confidence="0.988044">
P(q I e,E) = pri(q n)
2. P(r I i,r1,E,e)
= a(r I i) X noci&lt;i - j))
</equation>
<bodyText confidence="0.959768583333333">
where 8(x , y) = 1 when x = y, and
c5(x,y) = 0 otherwise.
This assumption states that P(ri
E,e) depends on i and ri. It also
depends on r,i)-1 with the factor
b(ri, r3)) to ensure that each English- phrase
is aligned with at most one German phrase.
3. The beginning position of a target phrase
depends on its distance from the beginning
position of its preceding phrase, as well as
the length of the source phrase aligned with
the preceding phrase:
</bodyText>
<equation confidence="0.9611325">
P(bi e, E)
= a(bi-bi-i I Eri-II) = a(Ai I)
</equation>
<bodyText confidence="0.8749585">
4. The fertility and translation tablet of a
source word depend on the word only:
</bodyText>
<equation confidence="0.978537">
P(OiiI i,j,-&apos;,-&apos;,bg,rg,e,E)
n((ki.i I eii)
P(Tijk
= t(7-ijk I eii)
</equation>
<bodyText confidence="0.981032571428572">
5. The leftmost position of the translations of
a source word depends on its distance from
the beginning of the target phrase aligned
with the source phrase that contains that
source word. It also depends on the iden-
tity of the phrase, and the position of the
source word in the source phrase.
</bodyText>
<equation confidence="0.845797666666667">
P(ri 7.11, 0/0, ,
06 rg, e, E)
= di(r-iji - biI E, j)
</equation>
<bodyText confidence="0.999941">
For a target word Tijk other than the left-
most ro in the translation tablet of the
source ei3, its position depends on its dis-
tance from the position of another tablet
word ri3(k_1) closest to its left, the class of
the target word ri3k, and the fertility of the
source word eij.
</bodyText>
<equation confidence="0.826745333333333">
P(rtik I
= d2(1r23k - rti(k-i) IG(Tiik),(kii)
here G(g) is the equivalent class for g.
</equation>
<subsectionHeader confidence="0.899528">
3.1 Parameter Estimation
</subsectionHeader>
<bodyText confidence="0.999883416666667">
EM algorithm was used to estimate the seven
types of parameters: xi, a, a, 0, r, d1 and
d2. We used a subset of probable alignments
in the EM learning, since the total number of
alignments is exponential to the target sentence
length. The subset was the neighboring align-
ments (Brown et al., 1993) of the Viterbi align-
ments discovered by Model 1 and Model 2. We
chose to include the Model 1 Viterbi alignment
here because the Model 1 alignment is closer
to the &amp;quot;ideal&amp;quot; when strong skewness exists in a
sentence pair.
</bodyText>
<sectionHeader confidence="0.945945" genericHeader="method">
4 Finding the Structures
</sectionHeader>
<bodyText confidence="0.915306">
It is of little interest for the structure-based
alignment model if we have to manually find
</bodyText>
<page confidence="0.979954">
1360
</page>
<bodyText confidence="0.99996075">
the language structures and write a grammar
for them, since the primary merit of statistical
machine translation is to reduce human labor.
In this section we introduce a grammar infer-
ence technique that finds the phrases used in the
structure-based alignment model. It is based on
the work in (Ries, Bugs, and Wang, 1995), where
the following two operators are used:
</bodyText>
<listItem confidence="0.990398">
1. Clustering: Clustering words/phrases
</listItem>
<bodyText confidence="0.5246805">
with similar meanings/grammatical func-
tions into equivalent classes. The mutual
information clustering algorithm (Brown et
al., 1992) were used for this.
</bodyText>
<listItem confidence="0.9357385">
2. Phrasing: The equivalent class sequence
c1, c2, • • • ck forms a phrase if
</listItem>
<equation confidence="0.9958495">
P(ClIC21. • •Ck) &gt; 0,
p(ci,c2, • • •Ck)1°g p(cOp(c2) • • •P(Ck)
</equation>
<bodyText confidence="0.999902586206896">
where 0 is a threshold. By changing the
threshold, we obtain a different number of
phrases.
The two operators are iteratively applied to
the training corpus in alternative steps. This
results in hierarchical phrases in the form of se-
quences of equivalent classes of words/phrases.
Since the algorithm only uses a monolin-
gual corpus, it often introduces some language-
specific structures resulting from biased usages
of a specific language. In machine transla-
tion we are more interested in cross-linguistic
structures, similar to the case of using interlin-
gua to represent cross-linguistic information in
knowledge-based MT.
To obtain structures that are common in both
languages, a bilingual mutual information clus-
tering algorithm (Wang, Lafferty, and Waibel,
1996) was used as the clustering operator. It
takes constraints from parallel corpus. We also
introduced an additional constraint in cluster-
ing, which requires that words in the same class
must have at least one common potential part-
of-speech.
Bilingual constraints are also imposed on the
phrasing operator. We used bilingual heuris-
tics to filter out the sequences acquired by the
phrasing operator that may not be common in
multiple languages. The heuristics include:
</bodyText>
<listItem confidence="0.704828722222222">
1. Average Translation Span: Given a
phrase candidate, its average translation
span is the distance between the leftmost
and the rightmost target positions aligned
with the words inside the candidate, av-
eraged over all Model 1 Viterbi alignments
of sample sentences. A candidate is filtered
out if its average translation span is greater
than the length of the candidate multiplied
by a threshold. This criterion states that
the words in the translation of a phrase
have to be close enough to form a phrase
in another language.
2. Ambiguity Reduction: A word occur-
ring in a phrase should be less ambiguous
than in other random context. Therefore
a phrase should reduce the ambiguity (un-
certainty) of the words inside it. For each
</listItem>
<bodyText confidence="0.9824926">
source language word class c, its translation
entropy is defined as Eg t(g I c)log(g I c).
The average per source class entropy re-
duction induced by the introduction of a
phrase P is therefore
</bodyText>
<equation confidence="0.547060666666667">
—1 Di E[E4g c) log t(g I c)
cep g
- E t(g I c, P) log t(g I c,P)]
</equation>
<bodyText confidence="0.997996777777778">
A threshold was set up for minimum en-
tropy reduction.
By applying the clustering operator followed
with the phrasing operator, we obtained shallow
phrase structures partly shown in Figure 5.
Given a set of phrases, we can deterministi-
cally parse a sentence into a sequence of phrases
by replacing the leftmost unparsed substring
with the longest matching phrase in the set.
</bodyText>
<sectionHeader confidence="0.976007" genericHeader="evaluation">
5 Evaluation and Discussion
</sectionHeader>
<bodyText confidence="0.9801471">
We used the Janus English/German schedul-
ing corpus (Suhm et al., 1995) to train our
phrase-based alignment model. Around 30,000
parallel sentences (400,000 words altogether for
both languages) were used for training. The
same data were used to train Simplified Model
2 (Wang and Waibel, 1997) and IBM Model
3 for performance comparison. A larger En-
glish monolingual corpus with around 0.5 mil-
lion words was used for the training of a bigram
</bodyText>
<page confidence="0.982261">
1361
</page>
<table confidence="0.944864363636364">
[Sunday Monday...] [afternoon morning...]
[Sunday Monday...] [at by...] [one two...]
[Sunday Monday...] [the every each...] [first second third...]
[Sunday Monday. ..3 [the every each...] [twenty depending remaining]
[Sunday Monday...] [the every each...] [eleventh thirteenth...]
[Sunday Monday...] [in within...] [January February...]
[January February...] [first second third...] [at by...]
[January February...] [first second third...]
[January February...] [the every each...] [first second third...]
[I he she itself] [have propose remember hate...]
[eleventh thirteenth...] [after before around] [one two three...]
</table>
<figureCaption confidence="0.985253">
Figure 5: Example of Acquired Phrases. Words in a bracket form a cluster, phrases are cluster
sequences. Ellipses indicate that a cluster has more words than those shown here.
</figureCaption>
<table confidence="0.995098">
Model Correct OK Incorrect Accuracy
Model 2 284 87 176 59.9%
Model 3 98 45 57 60.3%
S. Model 303 96 148 64.2%
</table>
<tableCaption confidence="0.999507">
Table 1: Translation Accuracy: a correct trans-
</tableCaption>
<bodyText confidence="0.975814411764706">
lation gets one credit, an okay translation gets
1/2 credit, an incorrect one gets 0 credit. Since
the IBM Model 3 decoder is too slow, its per-
formance was not measured on the entire test
set.
language model. A preprocessor splited Ger-
man compound nouns. Words that occurred
only once were taken as unknown words. This
resulted in a lexicon of 1372 English and 2202
German words. The English/German lexicons
were classified into 250 classes in each language
and 560 English phrases were constructed upon
these classes with the grammar inference algo-
rithm described earlier.
We limited the maximum sentence length to
be 20 words/15 phrases long, the maximum fer-
tility for non-null words to be 3.
</bodyText>
<subsectionHeader confidence="0.992756">
5.1 Translation Accuracy
</subsectionHeader>
<bodyText confidence="0.9997425">
Table 1 shows the end-to-end translation perfor-
mance. The structure-based model achieved an
error reduction of around 12.5% over the word-
based alignment models.
</bodyText>
<subsectionHeader confidence="0.999594">
5.2 Word Order and Phrase Alignment
</subsectionHeader>
<bodyText confidence="0.999650416666667">
Table 2 shows the alignment distribution for the
first German word/phrase in Simplified Model
2 and the structure-based model. The probabil-
ity mass is more scattered in the structure-based
model, reflecting the fact that English and Ger-
man have different phrase orders. On the other
hand, the word based model tends to align a
target word with the source words at similar po-
sitions, which resulted in many incorrect align-
ments, hence made the word translation proba-
bility t distributed over many unrelated target
words, as to be shown in the next subsection.
</bodyText>
<subsectionHeader confidence="0.998873">
5.3 Model Complexity
</subsectionHeader>
<bodyText confidence="0.9999925">
The structure-based model has 3,081,617 free
parameters, an increase of about 2% over the
3,022,373 free parameters of Simplified Model 2.
This small increase does not cause over-fitting,
as the performance on the test data suggests.
On the other hand, the structure-based model
is more accurate. This can be illustrated with
an example of the translation probability distri-
bution of the English word &amp;quot;I&amp;quot;. Table 3 shows
the possible translations of &amp;quot;I&amp;quot; with probability
greater than 0.01. It is clear that the structure-
based model &amp;quot;focuses&amp;quot; better on the correct
translations. It is interesting to note that the
German translations in Simplified Model 2 of-
ten appear at the beginning of a sentence, the
position where &amp;quot;I&amp;quot; often appears in English sen-
tences. It is the biased word-based alignments
that pull the unrelated words together and in-
crease the translation uncertainty.
We define the average translation entropy as
</bodyText>
<equation confidence="0.976595333333333">
E
P(e) E —t(gi I ei) log t(gi I ei).
i.o j.1
</equation>
<page confidence="0.952386">
1362
</page>
<table confidence="0.88706">
j 0 1 2 3 4 5 6 7 8 9 ...
am2(j Ii) 0.04 0.86 0.054 0.025 0.008 0.005 0.004 0.002 3.3 x 10-4 2.9 x 10-4 ...
asm(j I 1) 0.003 0.29 0.25 0.15 0.07 0.11 0.05 0.04 0.02 0.01 ...
</table>
<tableCaption confidence="0.923122666666667">
Table 2: The alignment distribution for the first German word/phrase in Simplified Model 2 and
in the structure-based model. The second distribution reflects the higher possibility of phrase
reordering in translation.
</tableCaption>
<table confidence="0.999411625">
tm2(*I I) tsm (*I I)
ich 0.708 ich 0.988
da 0.104 mich 0.010
am 0.024
das 0.022
dann 0.022
also 0.019
es 0.011
</table>
<tableCaption confidence="0.998167">
Table 3: The translation distribution of &amp;quot;I&amp;quot;. It
</tableCaption>
<bodyText confidence="0.987971461538462">
is more uncertain in the word-based alignment
model because the biased alignment distribu-
tion forced the associations between unrelated
English/German words.
(m, n are English and German lexicon size.)
It is a direct measurement of word transla-
tion uncertainty. The average translation en-
tropy is 3.01 bits per source word in Sim-
plified Model 2, 2.68 in Model 3, and 2.50
in the structured-based model. Therefore
information-theoretically the complexity of the
word-based alignment models is higher than
that of the structure-based model.
</bodyText>
<sectionHeader confidence="0.999617" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.9998826">
The structure-based alignment directly models
the word order difference between English and
German, makes the word translation distribu-
tion focus on the correct ones, hence improves
translation performance.
</bodyText>
<sectionHeader confidence="0.999145" genericHeader="acknowledgments">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999342">
We would like to thank the anonymous COL-
ING/ACL reviewers for valuable comments.
This research was partly supported by ATR and
the Verbmobil Project. The views and conclu-
sions in this document are those of the authors.
</bodyText>
<sectionHeader confidence="0.998928" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999821853658536">
Brown, P. F., S. A. Della-Pietra, V. J Della-
Pietra, and R. L. Mercer. 1993. The Math-
ematics of Statistical Machine Translation:
Parameter Estimation. Computational Lin-
guistics, 19(2) :263-311.
Brown, P. F., V. J. Della-Pietra, P. V. deSouza,
J. C. Lai, and R. L. Mercer. 1992. Class-
Based N-gram Models of Natural Language.
Computational Linguistics, 18(4):467-479.
Ries, Klaus, Finn Dag Buo, and Ye-
Yi Wang. 1995. Improved Language
Modelling by Unsupervised Acquisi-
tion of Structure. In ICASSP &apos;95.
IEEE. corrected version available via
http://www.cs.cmu.edu/fies/icassp_95.html.
Suhm, B., P.Geutner, T. Kemp, A. Lavie,
L. Mayfield, A. McNair, I. Rogina, T. Schultz,
T. Sloboda, W. Ward, M. Woszczyna, and
A. Waibel. 1995. JANUS: Towards multilin-
gual spoken language translation. In Proceed-
ings of the ARPA Speech Spoken Language
Technology Workshop, Austin, TX, 1995.
Vogel, S., H. Ney, and C. Tillman. 1996.
HMM-Based Word Alignment in Statistical
Translation. In Proceedings of the Seven-
teenth International Conference on Compu-
tational Linguistics: COLING-96, pages 836-
841, Copenhagen, Denmark.
Wang, Y., J. Lafferty, and A. Waibel. 1996.
Word Clustering with Parallel Spoken Lan-
guage Corpora. In Proceedings of the 4th In-
ternational Conference on Spoken Language
Processing (ICSLP&apos;96), Philadelphia, USA.
Wang, Y. and A. Waibel. 1997. Decoding Al-
gorithm in Statistical Machine Translation.
In Proceedings of the 35th Annual Meeting
of the Association for Computational Lin-
guistics and 8th Conference of the European
Chapter of the Association for Computational
Linguistics (AC L/EA CL &apos;97), pages 366-372,
Madrid, Spain.
</reference>
<page confidence="0.919388">
1363
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.838168">
<title confidence="0.999988">Modeling with Structures in Statistical Machine Translation</title>
<author confidence="0.999841">Wang Waibel</author>
<affiliation confidence="0.9998035">School of Computer Science Carnegie Mellon University</affiliation>
<address confidence="0.997958">5000 Forbes Avenue Pittsburgh, PA 15213, USA</address>
<email confidence="0.999785">yywOcs.cmu.edu</email>
<email confidence="0.999785">waibelOcs.cmu.edu</email>
<abstract confidence="0.9701553">Most statistical machine translation systems employ a word-based alignment model. In this paper we demonstrate that word-based alignment is a major cause of translation errors. We propose a new alignment model based on shallow phrase structures, and the structures can be automatically acquired from parallel corpus. This new model achieved over 10% error reduction for our spoken language translation task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S A Della-Pietra</author>
<author>V J DellaPietra</author>
<author>R L Mercer</author>
</authors>
<date>1993</date>
<journal>The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>263--311</pages>
<contexts>
<context position="755" citStr="Brown et al., 1993" startWordPosition="106" endWordPosition="109">ity 5000 Forbes Avenue Pittsburgh, PA 15213, USA {yyw,waibel}Ocs.cmu.edu Abstract Most statistical machine translation systems employ a word-based alignment model. In this paper we demonstrate that word-based alignment is a major cause of translation errors. We propose a new alignment model based on shallow phrase structures, and the structures can be automatically acquired from parallel corpus. This new model achieved over 10% error reduction for our spoken language translation task. 1 Introduction Most (if not all) statistical machine translation systems employ a word-based alignment model (Brown et al., 1993; Vogel, Ney, and Tillman, 1996; Wang and Waibel, 1997), which treats words in a sentence as independent entities and ignores the structural relationship among them. While this independence assumption works well in speech recognition, it poses a major problem in our experiments with spoken language translation between a language pair with very different word orders. In this paper we propose a translation model that employs shallow phrase structures. It has the following advantages over word-based alignment: • Since the translation model can directly depict phrase reordering in translation, it </context>
<context position="10790" citStr="Brown et al., 1993" startWordPosition="1938" endWordPosition="1941">ablet of the source ei3, its position depends on its distance from the position of another tablet word ri3(k_1) closest to its left, the class of the target word ri3k, and the fertility of the source word eij. P(rtik I = d2(1r23k - rti(k-i) IG(Tiik),(kii) here G(g) is the equivalent class for g. 3.1 Parameter Estimation EM algorithm was used to estimate the seven types of parameters: xi, a, a, 0, r, d1 and d2. We used a subset of probable alignments in the EM learning, since the total number of alignments is exponential to the target sentence length. The subset was the neighboring alignments (Brown et al., 1993) of the Viterbi alignments discovered by Model 1 and Model 2. We chose to include the Model 1 Viterbi alignment here because the Model 1 alignment is closer to the &amp;quot;ideal&amp;quot; when strong skewness exists in a sentence pair. 4 Finding the Structures It is of little interest for the structure-based alignment model if we have to manually find 1360 the language structures and write a grammar for them, since the primary merit of statistical machine translation is to reduce human labor. In this section we introduce a grammar inference technique that finds the phrases used in the structure-based alignmen</context>
</contexts>
<marker>Brown, Della-Pietra, DellaPietra, Mercer, 1993</marker>
<rawString>Brown, P. F., S. A. Della-Pietra, V. J DellaPietra, and R. L. Mercer. 1993. The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics, 19(2) :263-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>V J Della-Pietra</author>
<author>P V deSouza</author>
<author>J C Lai</author>
<author>R L Mercer</author>
</authors>
<title>ClassBased N-gram Models of Natural Language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<pages>18--4</pages>
<contexts>
<context position="11673" citStr="Brown et al., 1992" startWordPosition="2081" endWordPosition="2084">erest for the structure-based alignment model if we have to manually find 1360 the language structures and write a grammar for them, since the primary merit of statistical machine translation is to reduce human labor. In this section we introduce a grammar inference technique that finds the phrases used in the structure-based alignment model. It is based on the work in (Ries, Bugs, and Wang, 1995), where the following two operators are used: 1. Clustering: Clustering words/phrases with similar meanings/grammatical functions into equivalent classes. The mutual information clustering algorithm (Brown et al., 1992) were used for this. 2. Phrasing: The equivalent class sequence c1, c2, • • • ck forms a phrase if P(ClIC21. • •Ck) &gt; 0, p(ci,c2, • • •Ck)1°g p(cOp(c2) • • •P(Ck) where 0 is a threshold. By changing the threshold, we obtain a different number of phrases. The two operators are iteratively applied to the training corpus in alternative steps. This results in hierarchical phrases in the form of sequences of equivalent classes of words/phrases. Since the algorithm only uses a monolingual corpus, it often introduces some languagespecific structures resulting from biased usages of a specific language</context>
</contexts>
<marker>Brown, Della-Pietra, deSouza, Lai, Mercer, 1992</marker>
<rawString>Brown, P. F., V. J. Della-Pietra, P. V. deSouza, J. C. Lai, and R. L. Mercer. 1992. ClassBased N-gram Models of Natural Language. Computational Linguistics, 18(4):467-479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Ries</author>
<author>Finn Dag Buo</author>
<author>YeYi Wang</author>
</authors>
<title>Improved Language Modelling by Unsupervised Acquisition of Structure.</title>
<date>1995</date>
<booktitle>In ICASSP &apos;95. IEEE.</booktitle>
<note>corrected version available via http://www.cs.cmu.edu/fies/icassp_95.html.</note>
<marker>Ries, Buo, Wang, 1995</marker>
<rawString>Ries, Klaus, Finn Dag Buo, and YeYi Wang. 1995. Improved Language Modelling by Unsupervised Acquisition of Structure. In ICASSP &apos;95. IEEE. corrected version available via http://www.cs.cmu.edu/fies/icassp_95.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Suhm</author>
<author>T Kemp P Geutner</author>
<author>A Lavie</author>
<author>L Mayfield</author>
<author>A McNair</author>
<author>I Rogina</author>
<author>T Schultz</author>
<author>T Sloboda</author>
<author>W Ward</author>
<author>M Woszczyna</author>
<author>A Waibel</author>
</authors>
<title>JANUS: Towards multilingual spoken language translation.</title>
<date>1995</date>
<booktitle>In Proceedings of the ARPA Speech Spoken Language Technology Workshop,</booktitle>
<location>Austin, TX,</location>
<contexts>
<context position="14527" citStr="Suhm et al., 1995" startWordPosition="2557" endWordPosition="2560"> class entropy reduction induced by the introduction of a phrase P is therefore —1 Di E[E4g c) log t(g I c) cep g - E t(g I c, P) log t(g I c,P)] A threshold was set up for minimum entropy reduction. By applying the clustering operator followed with the phrasing operator, we obtained shallow phrase structures partly shown in Figure 5. Given a set of phrases, we can deterministically parse a sentence into a sequence of phrases by replacing the leftmost unparsed substring with the longest matching phrase in the set. 5 Evaluation and Discussion We used the Janus English/German scheduling corpus (Suhm et al., 1995) to train our phrase-based alignment model. Around 30,000 parallel sentences (400,000 words altogether for both languages) were used for training. The same data were used to train Simplified Model 2 (Wang and Waibel, 1997) and IBM Model 3 for performance comparison. A larger English monolingual corpus with around 0.5 million words was used for the training of a bigram 1361 [Sunday Monday...] [afternoon morning...] [Sunday Monday...] [at by...] [one two...] [Sunday Monday...] [the every each...] [first second third...] [Sunday Monday. ..3 [the every each...] [twenty depending remaining] [Sunday</context>
</contexts>
<marker>Suhm, Geutner, Lavie, Mayfield, McNair, Rogina, Schultz, Sloboda, Ward, Woszczyna, Waibel, 1995</marker>
<rawString>Suhm, B., P.Geutner, T. Kemp, A. Lavie, L. Mayfield, A. McNair, I. Rogina, T. Schultz, T. Sloboda, W. Ward, M. Woszczyna, and A. Waibel. 1995. JANUS: Towards multilingual spoken language translation. In Proceedings of the ARPA Speech Spoken Language Technology Workshop, Austin, TX, 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Vogel</author>
<author>H Ney</author>
<author>C Tillman</author>
</authors>
<title>HMM-Based Word Alignment in Statistical Translation.</title>
<date>1996</date>
<booktitle>In Proceedings of the Seventeenth International Conference on Computational Linguistics: COLING-96,</booktitle>
<pages>836--841</pages>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="786" citStr="Vogel, Ney, and Tillman, 1996" startWordPosition="110" endWordPosition="114">ue Pittsburgh, PA 15213, USA {yyw,waibel}Ocs.cmu.edu Abstract Most statistical machine translation systems employ a word-based alignment model. In this paper we demonstrate that word-based alignment is a major cause of translation errors. We propose a new alignment model based on shallow phrase structures, and the structures can be automatically acquired from parallel corpus. This new model achieved over 10% error reduction for our spoken language translation task. 1 Introduction Most (if not all) statistical machine translation systems employ a word-based alignment model (Brown et al., 1993; Vogel, Ney, and Tillman, 1996; Wang and Waibel, 1997), which treats words in a sentence as independent entities and ignores the structural relationship among them. While this independence assumption works well in speech recognition, it poses a major problem in our experiments with spoken language translation between a language pair with very different word orders. In this paper we propose a translation model that employs shallow phrase structures. It has the following advantages over word-based alignment: • Since the translation model can directly depict phrase reordering in translation, it is more accurate for translatio</context>
</contexts>
<marker>Vogel, Ney, Tillman, 1996</marker>
<rawString>Vogel, S., H. Ney, and C. Tillman. 1996. HMM-Based Word Alignment in Statistical Translation. In Proceedings of the Seventeenth International Conference on Computational Linguistics: COLING-96, pages 836-841, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wang</author>
<author>J Lafferty</author>
<author>A Waibel</author>
</authors>
<title>Word Clustering with Parallel Spoken Language Corpora.</title>
<date>1996</date>
<booktitle>In Proceedings of the 4th International Conference on Spoken Language Processing (ICSLP&apos;96),</booktitle>
<location>Philadelphia, USA.</location>
<contexts>
<context position="12600" citStr="Wang, Lafferty, and Waibel, 1996" startWordPosition="2229" endWordPosition="2233">d to the training corpus in alternative steps. This results in hierarchical phrases in the form of sequences of equivalent classes of words/phrases. Since the algorithm only uses a monolingual corpus, it often introduces some languagespecific structures resulting from biased usages of a specific language. In machine translation we are more interested in cross-linguistic structures, similar to the case of using interlingua to represent cross-linguistic information in knowledge-based MT. To obtain structures that are common in both languages, a bilingual mutual information clustering algorithm (Wang, Lafferty, and Waibel, 1996) was used as the clustering operator. It takes constraints from parallel corpus. We also introduced an additional constraint in clustering, which requires that words in the same class must have at least one common potential partof-speech. Bilingual constraints are also imposed on the phrasing operator. We used bilingual heuristics to filter out the sequences acquired by the phrasing operator that may not be common in multiple languages. The heuristics include: 1. Average Translation Span: Given a phrase candidate, its average translation span is the distance between the leftmost and the right</context>
</contexts>
<marker>Wang, Lafferty, Waibel, 1996</marker>
<rawString>Wang, Y., J. Lafferty, and A. Waibel. 1996. Word Clustering with Parallel Spoken Language Corpora. In Proceedings of the 4th International Conference on Spoken Language Processing (ICSLP&apos;96), Philadelphia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wang</author>
<author>A Waibel</author>
</authors>
<title>Decoding Algorithm in Statistical Machine Translation.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the European Chapter of the Association for Computational Linguistics (AC L/EA CL &apos;97),</booktitle>
<pages>366--372</pages>
<location>Madrid,</location>
<contexts>
<context position="810" citStr="Wang and Waibel, 1997" startWordPosition="115" endWordPosition="118">yw,waibel}Ocs.cmu.edu Abstract Most statistical machine translation systems employ a word-based alignment model. In this paper we demonstrate that word-based alignment is a major cause of translation errors. We propose a new alignment model based on shallow phrase structures, and the structures can be automatically acquired from parallel corpus. This new model achieved over 10% error reduction for our spoken language translation task. 1 Introduction Most (if not all) statistical machine translation systems employ a word-based alignment model (Brown et al., 1993; Vogel, Ney, and Tillman, 1996; Wang and Waibel, 1997), which treats words in a sentence as independent entities and ignores the structural relationship among them. While this independence assumption works well in speech recognition, it poses a major problem in our experiments with spoken language translation between a language pair with very different word orders. In this paper we propose a translation model that employs shallow phrase structures. It has the following advantages over word-based alignment: • Since the translation model can directly depict phrase reordering in translation, it is more accurate for translation between languages with</context>
<context position="5438" citStr="Wang and Waibel, 1997" startWordPosition="916" endWordPosition="919">may be a result of erroneous sentence alignment) or the two languages have different word orders, like English and German. Figure 1 and Figure 2 show some problematic alignments between English/German sentences made by IBM Model 2, together with the &apos;ideal&apos; alignments for the sentences. Here the alignment parameters penalize the alignment of English words with their German translation equivalents because the translation equivalents are far away from the words. An experiment reveals how often this kind of &amp;quot;skewed&amp;quot; alignment happens in our English/German scheduling conversation parallel corpus (Wang and Waibel, 1997). The experiment was based on the following observation: IBM translation Model 1 (where the alignment distribution is uniform) and Model 2 found similar Viterbi alignments when there were no movements or deletions, and they predicted very different Viterbi alignments when the skewness was severe in a sentence pair, since the alignment parameters in Model 2 penalize the long distance alignment. Figure 3 shows the Viterbi alignment discovered by Model 1 for the same sentences in Figure 21. We measured the distance of a Model 1 alignment al and a Model 2 alignment a2 as El la! — an. To estimate t</context>
<context position="14749" citStr="Wang and Waibel, 1997" startWordPosition="2591" endWordPosition="2594">ustering operator followed with the phrasing operator, we obtained shallow phrase structures partly shown in Figure 5. Given a set of phrases, we can deterministically parse a sentence into a sequence of phrases by replacing the leftmost unparsed substring with the longest matching phrase in the set. 5 Evaluation and Discussion We used the Janus English/German scheduling corpus (Suhm et al., 1995) to train our phrase-based alignment model. Around 30,000 parallel sentences (400,000 words altogether for both languages) were used for training. The same data were used to train Simplified Model 2 (Wang and Waibel, 1997) and IBM Model 3 for performance comparison. A larger English monolingual corpus with around 0.5 million words was used for the training of a bigram 1361 [Sunday Monday...] [afternoon morning...] [Sunday Monday...] [at by...] [one two...] [Sunday Monday...] [the every each...] [first second third...] [Sunday Monday. ..3 [the every each...] [twenty depending remaining] [Sunday Monday...] [the every each...] [eleventh thirteenth...] [Sunday Monday...] [in within...] [January February...] [January February...] [first second third...] [at by...] [January February...] [first second third...] [Janua</context>
</contexts>
<marker>Wang, Waibel, 1997</marker>
<rawString>Wang, Y. and A. Waibel. 1997. Decoding Algorithm in Statistical Machine Translation. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the European Chapter of the Association for Computational Linguistics (AC L/EA CL &apos;97), pages 366-372, Madrid, Spain.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>