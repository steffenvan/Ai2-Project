<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001151">
<title confidence="0.942909">
Learning to Identify Definitions using Syntactic Features
</title>
<author confidence="0.989975">
Ismail Fahmi and Gosse Bouma
</author>
<affiliation confidence="0.9947605">
Information Science
Groningen University
</affiliation>
<email confidence="0.991827">
{i.fahmi,g.bouma}@rug.nl
</email>
<sectionHeader confidence="0.982986" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999974">
This paper describes an approach to learn-
ing concept definitions which operates on
fully parsed text. A subcorpus of the
Dutch version of Wikipedia was searched
for sentences which have the syntactic
properties of definitions. Next, we ex-
perimented with various text classifica-
tion techniques to distinguish actual defi-
nitions from other sentences. A maximum
entropy classifier which incorporates fea-
tures referring to the position of the sen-
tence in the document as well as various
syntactic features, gives the best results.
</bodyText>
<sectionHeader confidence="0.996304" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998652701754386">
Answering definition questions is a challenge for
question answering systems. Much work in QA
has focused on answering factoid questions, which
are characterized by the fact that given the ques-
tion, one can typically make strong predictions
about the type of expected answer (i.e. a date,
name of a person, amount, etc.). Definition ques-
tions require a different approach, as a definition
can be a phrase or sentence for which only very
global characteristics hold.
In the CLEF 2005 QA task, 60 out of 200 ques-
tions were asking for the definition of a named
entity (a person or organization) such as Who is
Goodwill Zwelithini? or What is IKEA? Answers
are phrases such as current king of the Zulu nation,
or Swedish home furnishings retailer. For answer-
ing definition questions restricted to named enti-
ties, it generally suffices to search for noun phrases
consisting of the named entity and a preceding or
following nominal phrase. Bouma et al. (2005) ex-
tract all such noun phrases from the Dutch CLEF
corpus off-line, and return the most frequent heads
of co-occurring nominal phrases expanded with
adjectival or prepositional modifiers as answer to
named entity definition questions. The resulting
system answers 50% of the CLEF 2005 definition
questions correctly.
For a Dutch medical QA system, which is being
developed as part of the IMIX project&apos;, several sets
of test questions were collected. Approximately
15% of the questions are definition questions, such
as What is a runner’s knee? and What is cere-
brovascular accident?. Answers to such questions
(asking for the definition of a concept) are typi-
cally found in sentences such as A runner’s knee
is a degenerative condition of the cartilage sur-
face of the back of the knee cap, or patella or A
cerebrovascular accident is a decrease in the num-
ber of circulating white blood cells (leukocytes)
in the blood. One approach to finding answers to
concept definitions simply searches the corpus for
sentences consisting of a subject, a copular verb,
and a predicative phrase. If the concept matches
the subject, the predicative phrase can be returned
as answer. A preliminary evaluation of this tech-
nique in Tjong Kim Sang et al. (2005) revealed
that only 18% of the extracted sentences (from
a corpus consisting of a mixture of encyclopedic
texts and web documents) is actually a definition.
For instance, sentences such as RSI is a major
problem in the Netherlands, every suicide attempt
is an emergency or an infection of the lungs is the
most serious complication are of the relevant syn-
tactic form, but do not constitute definitions.
In this paper, we concentrate on a method for
improving the precision of recognizing definition
sentences. In particular, we investigate to what
</bodyText>
<footnote confidence="0.96209">
1www.let.rug.nl/˜gosse/Imix
</footnote>
<page confidence="0.999129">
64
</page>
<bodyText confidence="0.9998761">
extent machine learning techniques can be used
to distinguish definitions from non-definitions in
a corpus of sentences containing a subject, copu-
lar verb, and predicative phrase. A manually an-
notated subsection of the corpus was divided into
definition and non-definition sentences. Next, we
trained various classifiers using unigram and bi-
gram features, and various syntactic features. The
best classifier achieves a 60% error reduction com-
pared to our baseline system.
</bodyText>
<sectionHeader confidence="0.975416" genericHeader="introduction">
2 Previous work
</sectionHeader>
<bodyText confidence="0.999940211538462">
Work on identifying definitions from free text ini-
tially relied on manually crafted patterns without
applying any machine learning technique. Kla-
vans and Muresan (2000) set up a pattern extractor
for their Definder system using a tagger and a fi-
nite state grammar. Joho and Sanderson (2000) re-
trieve descriptive phrases (dp) of query nouns (qn)
from text to answer definition questions like Who
is qn? Patterns such as ‘dp especially qn’, as uti-
lized by Hearst (1992), are used to extract names
and their descriptions.
Similar patterns are also applied by Liu et al.
(2003) to mine definitions of topic-specific con-
cepts on the Web. As an additional assumption,
specific documents dedicated to the concepts can
be identified if they have particular HTML and hy-
perlink structures.
Hildebrandt et al. (2004) exploit surface pat-
terns to extract as many relevant ”nuggets” of in-
formation of a concept as possible. Similar to our
work, a copular pattern NP1 be NP2 is used as
one of the extraction patterns. Nuggets which do
not begin with a determiner are discarded to fil-
ter out spurious nuggets (e.g., progressive tense).
Nuggets extracted from every article in a corpus
are then stored in a relational database. In the end,
answering definition questions becomes as simple
as looking up relevant terms from the database.
This strategy is similar to our approach for answer-
ing definition questions.
The use of machine learning techniques can be
found in Miliaraki and Androutsopoulos (2004)
and Androutsopoulos and Galanis (2005) They
use similar patterns as (Joho and Sanderson,
2000) to construct training attributes. Sager and
L’Homme (1994) note that the definition of a
term should at least always contain genus (term’s
category) and species (term’s properties). Blair-
Goldensohn et al. (2004) uses machine learn-
ing and manually crafted lexico-syntactic patterns
to match sentences containing both a genus and
species phrase for a given term.
There is an intuition that most of definition
sentences are located at the beginning of docu-
ments. This lead to the use of sentence num-
ber as a good indicator of potential definition sen-
tences. Joho and Sanderson (2000) use the posi-
tion of the sentences as one of their ranking crite-
ria, while Miliaraki and Androutsopoulos (2004),
Androutsopoulos and Galanis (2005) and Blair-
Goldensohn et al. (2004) apply it as one of their
learning attributes.
</bodyText>
<sectionHeader confidence="0.9131095" genericHeader="method">
3 Syntactic properties of potential
definition sentences
</sectionHeader>
<bodyText confidence="0.99994365625">
To answer medical definition sentences, we used
the medical pages of Dutch Wikipedia2 as source.
Medical pages were selected by selecting all pages
mentioned on the Healthcare index page, and re-
cursively including pages mentioned on retrieved
pages as well.
The corpus was parsed syntactically by Alpino,
a robust wide-coverage parser for Dutch (Malouf
and van Noord, 2004). The result of parsing (il-
lustrated in Figure 1) is a dependency graph. The
Alpino-parser comes with an integrated named en-
tity classifier which assigns distinct part-of-speech
tags to person, organization, and geographical
named entities.
Potential definition sentences are sentences con-
taining a form of the verb zijn3 (to be) with a
subject and nominal predicative phrase as sisters.
The syntactic pattern does not match sentences in
which zijn is used as a possessive pronoun (his)
and sentences where a form of zijn is used as an
auxiliary. In the latter case, no predicative phrase
complement will be found. On the other hand,
we do include sentences in which the predicative
phrase precedes the subject, as in Onderdeel van
de testis is de Leydig-cel (the Leydig cel is part of
the testis). As word order in Dutch is less strict
than in English, it becomes relevant to include
such non-canonical word orders as well.
A number of non-definition sentences that will
be extracted using this method can be filtered by
simple lexical methods. For instance, if the subject
is headed by (the Dutch equivalents of) cause, con-
</bodyText>
<footnote confidence="0.943153666666667">
2nl.wikipedia.org
3Note that the example uses ben (the first person singular
form of the verb) as root for zijn.
</footnote>
<page confidence="0.999267">
65
</page>
<figure confidence="0.626869272727273">
–
smain
hd obj1
prep conj
mets
cnj crd
np vg
ens
hd app hd app
noun name noun num
symbools N7 atoom nummerg 710
</figure>
<figureCaption confidence="0.994172">
Figure 1: Parse of (the Dutch equivalent of) Nitrogen is a chemical element with symbol N and atomic
number 7. Nodes are labelled with depedency relations and categories or part-of-speech tags, root forms,
and string positions.
</figureCaption>
<figure confidence="0.995488380952381">
mod
adj
scheikundig3
hd
noun
element¢
det
det
een2
mod
pp
hd
verb
ben1
su
noun
stikstof0
predc
np
cnj
np
</figure>
<bodyText confidence="0.974415375">
sequence, example, problem, result, feature, pos-
sibility, symptom, sign, etc., or contains the deter-
miner geen (no), the sentence will not be included
in the list of potential definitions.
However, even after applying the lexical filter,
not all extracted sentences are definitions. In the
next sections, we describe experiments aimed at
increasing the accuracy of the extraction method.
</bodyText>
<sectionHeader confidence="0.799842" genericHeader="method">
4 Annotating training examples
</sectionHeader>
<bodyText confidence="0.994726944444444">
To create evaluation and training data, 2500 ex-
tracted sentences were manually annotated as def-
inition, non-definition, or undecided. One of the
criteria for undecided sentences is that it mentions
a characteristic of a definition but is not really
a (complete) definition, for example, Benzeen is
carcinogeen (Benzene is a carcinogen). The result
of this annotation is given in Table 1. The anno-
tated data was used both to evaluate the accuracy
of the syntactic extraction method, and to training
and evaluate material for the machine learning ex-
periments as discussed in the next sections.
After discarding the undecided sentences, we
are left with 2299 sentences, 1366 of which are
definitions. This means that the accuracy of the
extraction method using only syntax was 59%.4
4This is considerably higher than the estimated accuracy
of 18% reported in Tjong Kim Sang et al. (2005). This is
probably partly due to the fact that the current corpus con-
sists of encyclopedic material only, whereas the corpus used
If we take sentence postion into account as well,
and classify all first sentences as definitions and
all other sentences as non-definitions, a baseline
accuracy of 75,9% is obtained.
It is obvious from Table 1 that the first sen-
tences of Wikipedia lemmas that match the syn-
tactic pattern are almost always definitions. It
seems that e.g. Google’s5 define query feature,
when restricted to Dutch at least, relies heavily
on this fact to answer definition queries. How-
ever it is also obvious that definition sentences can
also be found in other positions. For documents
from other sources, which are not as structured as
Wikipedia, the first position sentence is likely to
be an even weaker predictor of definition vs. non-
definition sentences.
</bodyText>
<sectionHeader confidence="0.924668" genericHeader="method">
5 Attributes of definition sentences
</sectionHeader>
<bodyText confidence="0.999910571428572">
We aim at finding the best attributes for classifying
definition sentences. We experimented with com-
binations of the following attributes:
Text properties: bag-of-words, bigrams, and
root forms. Punctuation is included as Klavans
and Muresan (2000) observe that it can be used to
recognize definitions (i.e. definitions tend to con-
</bodyText>
<footnote confidence="0.8531096">
in Tjong Kim Sang et al. (2005) contained web material
from various sources, such as patient discussion groups, as
well. The latter tends to contain more subjective and context-
dependent material.
5google.com
</footnote>
<page confidence="0.970856">
66
</page>
<table confidence="0.950024">
Sentence Def Non-def Undecided
position
first 831 18 31
other 535 915 170
Total 1366 933 201
</table>
<tableCaption confidence="0.942627333333333">
Table 1: Number of sentences in the first and
other position of documents annotated as defini-
tion, non-definition, and undecided.
</tableCaption>
<bodyText confidence="0.993848707317073">
tain parentheses more often than non-definitions).
No stopword filtering is applied as in our exper-
iments it consistently decreased accuracy. Note
that we include all bigrams in a sentence as fea-
ture. A different use of n-grams has been explored
by Androutsopoulos and Galanis (2005) who add
only n-grams (n E 11,2,31) occurring frequently
either directly before or after a target term.
Document property: the position of each sen-
tence in the document. This attribute has been fre-
quently used in previous work and is motivated by
the observation that definitions are likely to be lo-
cated in the beginning of a document.
Syntactic properties: position of each sub-
ject in the sentence (initial, e.g. X is Y; or non-
initial, e.g. Y is X), and of each subject and
predicative complement: type of determiner (def-
inite, indefinite, other). These attributes have not
been investigated in previous work. In our exper-
iments, sentence-initial subjects appear in 92% of
the definition sentences and and 76% of the non-
definition sentences. These values show that a
definition sentence with a copular pattern tends
to put its subject in the beginning. Two other
attributes are used to encode the type of deter-
miner of the subject and predicative compelement.
As shown in Table 2, the majority of subjects in
definition sentences have no determiner (62%),
e.g. Paracetamol is een pijnstillend en koortsver-
lagend middel (Paracetamol is an pain alleviat-
ing and a fever reducing medicine), while in non-
definition sentences subject determiners tend to be
definite (50%), e.g. De werkzame stof is acetyl-
salicylzuur (The operative substance is acetylsal-
icylacid). Predicative complements, as shown in
Table 3, tend to contain indefinite determiners in
definition sentences (64%), e.g. een pijnstillend
... medicijn (a pain alleviating... medicine), while
in non-definition the determiner tends to be def-
inite (33%), e.g. Een fenomeen is de Landsge-
meinde (A phenomenon is the Landsgemeinde).
</bodyText>
<table confidence="0.8692946">
Type Definition Non-def
definite 23 50
indefinite 13 12
nodeterminer 62 29
other 2 9
</table>
<tableCaption confidence="0.931895">
Table 2: Percentage of determiner types of sub-
jects in definition and non-definition sentences.
</tableCaption>
<table confidence="0.9734266">
Type Definition Non-def
definite 23 33
indefinite 64 29
nodeterminer 8 1
other 4 28
</table>
<tableCaption confidence="0.982916333333333">
Table 3: Percentage of determiner types of
predicative complements in definition and non-
definition sentences.
</tableCaption>
<bodyText confidence="0.852481130434783">
Named entity tags: named entity class (NEC)
of subjects, e.g. location, person, organization,
or no-class. A significant difference in the dis-
tribution of this feature between definition and
non-definition sentences can be observed in Table
4. More definition sentences have named entity
classes contained in their subjects (40.63%) com-
pared to non-definition sentences (11.58%). We
also experimented with named entity classes con-
tained in predicative complements but it turned
out that very few predicates contained named en-
tities, and thus no significant differences in distri-
bution between definition and non-definition sen-
tences could be observed.
Features for lexical patterns, as used in (An-
droutsopoulos and Galanis, 2005), e.g. qn which
(is|was|are|were) dp, are not added because in this
experiment we investigate only a copular pattern.
WordNet-based attributes are also excluded, given
that coverage for Dutch (using EuroWordNet)
tends to be less good than for English, and even for
English their contribution is sometimes insignifi-
cant (Miliaraki and Androutsopoulos, 2004).
</bodyText>
<table confidence="0.6870956">
Type Definition Non-def
no-nec 59 88
location 10 4
organization 8 3
person 22 4
</table>
<tableCaption confidence="0.9899255">
Table 4: Percentage of named-entity classes of
subjects in definition and non-definition sentences.
</tableCaption>
<page confidence="0.998045">
67
</page>
<figureCaption confidence="0.961310476190476">
word bigrams only bigram + synt + pos
is a first sent
a other sent
are is a
is indef pred
) is no det subj
the init subj
is DIGITS a
are the are
this is
or other det pred
is of ) is
this/these noninit subj
atomic number def subj
atomic number DIGITS the
with symbol is DIGITS
and atomic number are the
that this
chemical or
a chemical other det subj
chemical element is of
</figureCaption>
<bodyText confidence="0.993063346153846">
Table 5: 20 most informative features for the sys-
tems using word bigrams only and word bigrams
in combination with syntactic and sentence posi-
tion features (word features have been translated
into English).
We use the text classification tool Rainbow6
(McCallum, 2000) to perform most of our experi-
ments. Each sentence is represented as a string of
words, possibly followed by bigrams, root forms,
(combinations of) syntactic features, etc.
All experiments were performed by selecting
only the 2000 highest ranked features according
to information gain. In the experiments which in-
clude syntactic features, the most informative fea-
tures tend to contain a fair number of syntactic fea-
tures. This is illustrated for the configuration using
bigrams, sentence position, and syntax in table 5.
It supports our intuition that the position of sub-
jects and the type of determiner of subjects and
predicative complements are clues to recognizing
definition sentences.
To investigate the effect of each attribute, we
set up several configurations of training examples
as described in Table 6. We start with using only
bag-of-words or bigrams, and then combine them
with other attribute sets.
</bodyText>
<footnote confidence="0.589743">
6www.cs.cmu.edu/˜mccallum/bow/rainbow/
</footnote>
<figure confidence="0.9860475">
Cfg Description
1 using only bag-of-words
2 using only bigrams
3 combining bigrams &amp; bag-of-words
4 adding syntactic properties to
config. 3
5 adding syntactic properties
&amp; NEC to config. 3
6 adding sentence position to
config. 3
7 adding root forms to
config. 3
8 adding syntactic properties &amp;
sentence position to config. 3
9 adding syntactic properties, sentence
position &amp; NEC to config. 3
10 adding syntactic properties, sentence
position &amp; root forms to config. 3)
11 using all attributes (adding NEC
to configuration 10)
</figure>
<tableCaption confidence="0.9315455">
Table 6: The description of the attribute configu-
rations.
</tableCaption>
<sectionHeader confidence="0.99906" genericHeader="method">
6 Learning-based methods
</sectionHeader>
<bodyText confidence="0.999970730769231">
We apply three supervised learning methods to
each of the attribute configurations in Table 6,
namely naive Bayes, maximum entropy, and sup-
port vector machines (SVMs). Naive Bayes is a
fast and easy to use classifier based on the prob-
abilistic model of text and has often been used in
text classification tasks as a baseline. Maximum
entropy is a general estimation technique that has
been used in many fields such as information re-
trieval and machine learning. Some experiments
in text classification show that maximum entropy
often outperforms naive Bayes, e.g. on two of
three data sets in Nigam et al. (1999). SVMs are
a new learning method but have been reported by
Joachims (1998) to be well suited for learning in
text classification.
We experiment with three kernel types of
SVMs: linear, polynomial, and radial base func-
tion (RBF). Rainbow (McCallum, 2000) is used to
examine these learning methods, except the RBF
kernel for which libsvm (Chang and Lin, 2001)
is used. Miliaraki and Androutsopoulos (2004)
use a SVM with simple inner product (polyno-
mial of first degree) kernel because higher degree
polynomial kernels were reported as giving no im-
provement. However we want to experiment with
</bodyText>
<page confidence="0.997955">
68
</page>
<table confidence="0.9993245">
Cfg NB ME svm1a svm2b svm3c
1 85.75 ± 0.57 85.35 ± 0.77 77.65 ± 0.87 78.39 ± 0.67 81.95 ± 0.82
2 87.77 ± 0.51 88.65 ± 0.54 84.02 ± 0.47 84.26 ± 0.52 85.38 ± 0.77
3 89.82 ± 0.53 88.82 ± 0.66 83.93 ± 0.57 84.24 ± 0.54 87.04 ± 0.95
4 85.22 ± 0.35 89.08 ± 0.50 84.93 ± 0.57 85.57 ± 0.53 87.77 ± 0.89
5 85.44 ± 0.45 91.38 ± 0.42 86.90 ± 0.48 86.90 ± 0.53 87.60 ± 0.87
6 90.26 ± 0.71 90.70 ± 0.48 85.26 ± 0.56 86.05 ± 0.64 88.52 ± 0.92
7 88.60 ± 0.81 88.99 ± 0.51 83.38 ± 0.38 84.69 ± 0.43 87.08 ± 0.87
8 86.40 ± 0.51 92.21 ± 0.27 86.57 ± 0.42 87.29 ± 0.47 88.77 ± 0.77
9 87.12 ± 0.52 90.83 ± 0.43 87.21 ± 0.42 87.99 ± 0.53 89.04 ± 0.67
10 87.60 ± 0.38 91.16 ± 0.43 86.68 ± 0.40 86.97 ± 0.41 88.91 ± 0.68
11 86.72 ± 0.46 91.16 ± 0.35 87.47 ± 0.40 87.05 ± 0.63 89.47 ± 0.67
</table>
<tableCaption confidence="0.8539244">
aSVM with linear kernel (Rainbow)
bSVM with polynomial kernel (Rainbow)
cSVM with RBF kernel (libsvm)
Table 7: Accuracy and standard error (%) estimates for the dataset using naive Bayes (NB), maximum
entropy (ME), and three SVM settings at the different attribute configurations.
</tableCaption>
<bodyText confidence="0.999080944444445">
the RBF (gaussian) kernel by selecting model pa-
rameters C (penalty for misclassification) and γ
(function of the deviation of the Gaussian Kernel)
so that the classifier can accurately predict testing
data. This experiment is based on the argument
that if a complete model selection using the gaus-
sian kernel has been conducted, there is no need
to consider linear SVM, because the RBF kernel
with certain parameters (C, γ) has the same per-
formance as the linear kernel with a penalty pa-
rameter C˜ (Keerthi and Lin, 2003).
Given the finite dataset, we use k-fold cross-
validation (k = 20) to estimate the future perfor-
mance of each classifier induced by its learning
method and dataset. This estimation method intro-
duces lower bias compared to a bootstrap method
which has extremely large bias on some problems
(Kohavi, 1995).
</bodyText>
<sectionHeader confidence="0.987734" genericHeader="evaluation">
7 Evaluation
</sectionHeader>
<bodyText confidence="0.999911295454545">
We evaluated each configuration of Section 5 and
each learning method of Section 6 on the dataset
which consists of 1336 definitions and 963 non-
definitions sentences. Table 7 reports the accuracy
and standard error estimated from this experiment.
In all experiment runs, all of the classifiers in all
configurations outperform our baseline (75.9%).
The best accuracy of each classifier (bold) is be-
tween 11.57% to 16.31% above the baseline.
The bigram only attributes (config. 2) clearly
outperform the simplest setting (bag-of-word only
attributes) for all classifiers. The combination of
both attributes (config. 3) achieves some improve-
ment between 0.17% to 4.41% from configuration
2. It is surprising that naive Bayes shows the best
and relatively high accuracy in this base config-
uration (89.82%) and even outperforms all other
settings.
Adding syntactic properties (config. 4) or posi-
tion of sentences in documents (config. 6) to the
base configuration clearly gives some improve-
ment (in 4 and 5 classifiers respectively for each
configuration). But, adding root forms (config.
7) does not significantly contribute to an improve-
ment. These results show that in general, syntactic
properties can improve the performance of most
classifiers. The results also support the intuition
that the position of sentences in documents plays
important role in identifying definition sentences.
Moreover, this intuition is also supported by the
result that the best performance of naive Bayes is
achieved at configuration 6 (90.26%). Compared
to the syntactic features, sentence positions give
better accuracy in all classifiers.
The above results demonstrate an interesting
finding that a simple attribute set which consists of
bag-of-words, bigrams, and sentence position un-
der a fast and simple classifier (e.g. naive Bayes)
could give a relatively high accuracy. One expla-
nation that we can think of is that candidate sen-
tences have been syntactically very well extracted
with our filter. Thus, the sentences are biased by
the filter from which important words and bigrams
of definitions can be found in most of the sen-
</bodyText>
<page confidence="0.998433">
69
</page>
<bodyText confidence="0.999956555555556">
tences. For example, the word and bigrams is een
(is a), een (a), zijn (are), is (is), zijn de (are the),
and is van (is of) are good clues to definitions and
consequently have high information gain. We have
to test this result in a future work on candidate def-
inition sentences which are extracted by filters us-
ing various other syntactic patterns.
More improvement is shown when both syntac-
tic properties and sentence position are added to-
gether (config. 8). All of the classifiers in this con-
figuration obtain more error reduction compared
to the base configuration. Moreover, the best ac-
curacy of this experiment is shown by maximum
entropy at this configuration (92.21%). This may
be a sign that our proposed syntactic properties are
good indicators to identify definition sentences.
Other interesting findings can be found in the
addition of named entity classes to configuration
3 (config. 5), to configuration 8 (config. 9) and
to configuration 10 (config. 11). In these con-
figurations, adding NEC increases accuracies of
almost all classifiers. On the other hand, adding
root forms to configuration 3 (config. 7) and to
configuration 8 (config. 10) does not improve ac-
curacies. However, the best accuracies of naive
Bayes (90.26%) and maximum entropy (92.21%)
are achieved when named entity and root forms are
not included as attributes.
We now evaluate the classifiers. It is clear
from the table that SVM1 and SVM2 settings can
not achieve better accuracy compared to the naive
Bayes setting, while SVM3 setting marginally out-
performs naive Bayes (on 6 out of 11 configura-
tions). This result is contrary to the superiority of
SVMs in many text classification tasks. Huang et
al. (2003) reported that both classifiers show sim-
ilar predictive accuracy and AUC (area under the
ROC (Receiver Operating Characteristics) curve)
scores. This performance of naive Bayes supports
the motivation behind its renaisance in machine
learning (Lewis, 1998).
From the three SVM settings, SVM with RBF
kernel appears as the best classifier for our task
in which it outperforms other SVMs settings in
all configurations. This result supports the above
mentioned argument that if the best C and γ can be
selected, we do not need to consider linear SVM
(e.g. the svm1 setting).
Among all of the classifiers, maximum entropy
shows the best accuracy. It wins at 9 out of 11
configurations in all experiments. This result con-
firms previous reports e.g. in Nigam et al. (1999)
that maximum entropy performs better than naive
Bayes in some text classification tasks.
</bodyText>
<sectionHeader confidence="0.885303" genericHeader="conclusions">
8 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.999994825">
We have presented an experiment in identifying
definition sentences using syntactic properties and
learning-based methods. Our method is concen-
trated on improving the precision of recognizing
definition sentences. The first step is extracting
candidate definition sentences from a fully parsed
text using syntactic properties of definitions. To
distinguish definition from non-definition sen-
tences, we investigated several machine learning
methods, namely naive Bayes, maximum entropy,
and SVMs. We also experimented with several at-
tribute configurations. In this selection, we com-
bine text properties, document properties, and syn-
tactic properties of the sentences. We have shown
that adding syntactic properties, in particular the
position of subjects in the sentence, type of de-
terminer of each subject and predicative comple-
ment, improves the accuracy of most machine
learning techniques, and leads to the most accu-
rate result overall.
Our method has been evaluated on a subset of
manually annotated data from Wikipedia. The
combination of highly structured text material and
a syntactic filter leads to a relatively high initial
baseline.
Our results on the performance of SVMs do not
confirm the superiority of this learning method for
(text) classification tasks. Naive Bayes, which is
well known from its simplicity, appears to give
reasonably high accuracy. Moreover, it achieves
a high accuracy on simple attribute configuration
sets (containing no syntactic properties). In gen-
eral, our method will give the best result if all
properties except named entity classes and root
forms are used as attributes and maximum entropy
is applied as a classifier.
We are currently working on using more syn-
tactic patterns to extract candidate definition sen-
tences. This will increase the number of definition
sentences that we can identify from text.
</bodyText>
<sectionHeader confidence="0.996353" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9964645">
I. Androutsopoulos and D. Galanis. 2005. A prac-
tically unsupervised learning method to identify
single-snippet answers to definition questions on the
web. In Human Language Technology Conference
</reference>
<page confidence="0.967447">
70
</page>
<reference confidence="0.999480776470588">
and Conference on Empirical Methods in Natural
Language Processing (HLT-EMNLP 2005), Vancou-
ver, Canada.
S. Blair-Goldensohn, K. McKeown, and A.H. Schlaik-
jer. 2004. Answering definitional questions: A hy-
brid approach. In New Directions in Question An-
swering, pages 47–58.
Gosse Bouma, Jori Mur, Gertjan van Noord, Lonneke
van der Plas, and J¨org Tiedemann. 2005. Question
answering for Dutch using dependency relations. In
Working Notes for the CLEF 2005 Workshop, Vi-
enna.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: a library for support vector machines.
Software available at http://www.csie.ntu.
edu.tw/˜cjlin/libsvm.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proc. of the 14th
COLING, pages 539–545, Nantes, France.
W. Hildebrandt, B. Katz, and J.J. Lin. 2004. An-
swering definition questions with multiple knowl-
edge sources. In HLT-NAACL, pages 49–56.
Jin Huang, Jingjing Lu, and Charles X. Ling. 2003.
Comparing naive bayes, decision trees, and svm
with auc and accuracy. In ICDM ’03: Proceedings
of the Third IEEE International Conference on Data
Mining, Washington, DC, USA. IEEE Computer So-
ciety.
Thorsten Joachims. 1998. Text categorization with
support vector machines: learning with many rele-
vant features. In Claire N’edellec and C’eline Rou-
veirol, editors, Proceedings of ECML-98, 10th Euro-
pean Conference on Machine Learning, pages 137–
142, Chemnitz, DE. Springer Verlag, Heidelberg,
DE.
H. Joho and M. Sanderson. 2000. Retrieving descrip-
tive phrases from large amounts of free text. In
CIKM, pages 180–186.
S. Sathiya Keerthi and Chih-Jen Lin. 2003. Asymp-
totic behaviors of support vector machines with
gaussian kernel. Neural Comput., 15(7):1667–1689.
J.L. Klavans and S. Muresan. 2000. Definder: Rule-
based methods for the extraction of medical termi-
nology and their associated definitions from on-line
text. In American Medical Informatics Assoc 2000.
Ron Kohavi. 1995. A study of cross-validation and
bootstrap for accuracy estimation and model selec-
tion. In IJCAI, pages 1137–1145.
David D. Lewis. 1998. Naive (Bayes) at forty: The in-
dependence assumption in information retrieval. In
Claire N´edellec and C´eline Rouveirol, editors, Pro-
ceedings of ECML-98, 10th European Conference
on Machine Learning, pages 4–15, Chemnitz, DE.
Springer Verlag, Heidelberg, DE.
B. Liu, C.W. Chin, and H.T. Ng. 2003. Mining topic-
specific concepts and definitions on the web. In
WWW ’03: Proceedings of the 12th international
conference on World Wide Web, pages 251–260,
New York, NY, USA. ACM Press.
Robert Malouf and Gertjan van Noord. 2004. Wide
coverage parsing with stochastic attribute value
grammars. In IJCNLP-04 Workshop Beyond Shal-
low Analyses - Formalisms and statistical modeling
for deep analyses, Hainan.
A McCallum. 2000. Bow: A toolkit for statisti-
cal language modeling, text retrieval, classification
and clustering. http://www.cs.cmu.edu/
˜mccallum/bow.
S. Miliaraki and I. Androutsopoulos. 2004. Learning
to identify single-snippet answers to definition ques-
tions. In 20th International Conference on Compu-
tational Linguistics (COLING 2004), pages 1360–
1366, Geneva, Switzerland. COLING 2004.
K. Nigam, J. Lafferty, and A. McCallum. 1999. Using
maximum entropy for text classification. In IJCAI-
99 Workshop on Machine Learning for Information
Filtering, pages 61–67.
Juan C. Sager and M.C. L’Homme. 1994. A model
for definition of concepts. Terminology, pages 351–
374.
Erik Tjong Kim Sang, Gosse Bouma, and Maarten
de Rijke. 2005. Developing offline strategies for
answering medical questions. In Diego Moll´a and
Jos´e Luis Vicedo, editors, AAAI 2005 workshop on
Question Answering in Restricted Domains.
</reference>
<page confidence="0.999096">
71
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.496956">
<title confidence="0.837113666666667">Learning to Identify Definitions using Syntactic Features Fahmi Information</title>
<author confidence="0.520788">Groningen</author>
<abstract confidence="0.998404928571428">This paper describes an approach to learning concept definitions which operates on fully parsed text. A subcorpus of the Dutch version of Wikipedia was searched for sentences which have the syntactic properties of definitions. Next, we experimented with various text classification techniques to distinguish actual definitions from other sentences. A maximum entropy classifier which incorporates features referring to the position of the sentence in the document as well as various syntactic features, gives the best results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>I Androutsopoulos</author>
<author>D Galanis</author>
</authors>
<title>A practically unsupervised learning method to identify single-snippet answers to definition questions on the web.</title>
<date>2005</date>
<booktitle>In Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT-EMNLP</booktitle>
<location>Vancouver, Canada.</location>
<contexts>
<context position="5492" citStr="Androutsopoulos and Galanis (2005)" startWordPosition="876" endWordPosition="879">ept as possible. Similar to our work, a copular pattern NP1 be NP2 is used as one of the extraction patterns. Nuggets which do not begin with a determiner are discarded to filter out spurious nuggets (e.g., progressive tense). Nuggets extracted from every article in a corpus are then stored in a relational database. In the end, answering definition questions becomes as simple as looking up relevant terms from the database. This strategy is similar to our approach for answering definition questions. The use of machine learning techniques can be found in Miliaraki and Androutsopoulos (2004) and Androutsopoulos and Galanis (2005) They use similar patterns as (Joho and Sanderson, 2000) to construct training attributes. Sager and L’Homme (1994) note that the definition of a term should at least always contain genus (term’s category) and species (term’s properties). BlairGoldensohn et al. (2004) uses machine learning and manually crafted lexico-syntactic patterns to match sentences containing both a genus and species phrase for a given term. There is an intuition that most of definition sentences are located at the beginning of documents. This lead to the use of sentence number as a good indicator of potential definition</context>
<context position="11734" citStr="Androutsopoulos and Galanis (2005)" startWordPosition="1888" endWordPosition="1891">s patient discussion groups, as well. The latter tends to contain more subjective and contextdependent material. 5google.com 66 Sentence Def Non-def Undecided position first 831 18 31 other 535 915 170 Total 1366 933 201 Table 1: Number of sentences in the first and other position of documents annotated as definition, non-definition, and undecided. tain parentheses more often than non-definitions). No stopword filtering is applied as in our experiments it consistently decreased accuracy. Note that we include all bigrams in a sentence as feature. A different use of n-grams has been explored by Androutsopoulos and Galanis (2005) who add only n-grams (n E 11,2,31) occurring frequently either directly before or after a target term. Document property: the position of each sentence in the document. This attribute has been frequently used in previous work and is motivated by the observation that definitions are likely to be located in the beginning of a document. Syntactic properties: position of each subject in the sentence (initial, e.g. X is Y; or noninitial, e.g. Y is X), and of each subject and predicative complement: type of determiner (definite, indefinite, other). These attributes have not been investigated in pre</context>
<context position="14525" citStr="Androutsopoulos and Galanis, 2005" startWordPosition="2323" endWordPosition="2327">r no-class. A significant difference in the distribution of this feature between definition and non-definition sentences can be observed in Table 4. More definition sentences have named entity classes contained in their subjects (40.63%) compared to non-definition sentences (11.58%). We also experimented with named entity classes contained in predicative complements but it turned out that very few predicates contained named entities, and thus no significant differences in distribution between definition and non-definition sentences could be observed. Features for lexical patterns, as used in (Androutsopoulos and Galanis, 2005), e.g. qn which (is|was|are|were) dp, are not added because in this experiment we investigate only a copular pattern. WordNet-based attributes are also excluded, given that coverage for Dutch (using EuroWordNet) tends to be less good than for English, and even for English their contribution is sometimes insignificant (Miliaraki and Androutsopoulos, 2004). Type Definition Non-def no-nec 59 88 location 10 4 organization 8 3 person 22 4 Table 4: Percentage of named-entity classes of subjects in definition and non-definition sentences. 67 word bigrams only bigram + synt + pos is a first sent a oth</context>
</contexts>
<marker>Androutsopoulos, Galanis, 2005</marker>
<rawString>I. Androutsopoulos and D. Galanis. 2005. A practically unsupervised learning method to identify single-snippet answers to definition questions on the web. In Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT-EMNLP 2005), Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Blair-Goldensohn</author>
<author>K McKeown</author>
<author>A H Schlaikjer</author>
</authors>
<title>Answering definitional questions: A hybrid approach.</title>
<date>2004</date>
<booktitle>In New Directions in Question Answering,</booktitle>
<pages>47--58</pages>
<marker>Blair-Goldensohn, McKeown, Schlaikjer, 2004</marker>
<rawString>S. Blair-Goldensohn, K. McKeown, and A.H. Schlaikjer. 2004. Answering definitional questions: A hybrid approach. In New Directions in Question Answering, pages 47–58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gosse Bouma</author>
<author>Jori Mur</author>
<author>Gertjan van Noord</author>
<author>Lonneke van der Plas</author>
<author>J¨org Tiedemann</author>
</authors>
<title>Question answering for Dutch using dependency relations.</title>
<date>2005</date>
<booktitle>In Working Notes for the CLEF 2005 Workshop,</booktitle>
<location>Vienna.</location>
<marker>Bouma, Mur, van Noord, van der Plas, Tiedemann, 2005</marker>
<rawString>Gosse Bouma, Jori Mur, Gertjan van Noord, Lonneke van der Plas, and J¨org Tiedemann. 2005. Question answering for Dutch using dependency relations. In Working Notes for the CLEF 2005 Workshop, Vienna.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: a library for support vector machines. Software available at http://www.csie.ntu.</title>
<date>2001</date>
<note>edu.tw/˜cjlin/libsvm.</note>
<contexts>
<context position="18240" citStr="Chang and Lin, 2001" startWordPosition="2930" endWordPosition="2933">neral estimation technique that has been used in many fields such as information retrieval and machine learning. Some experiments in text classification show that maximum entropy often outperforms naive Bayes, e.g. on two of three data sets in Nigam et al. (1999). SVMs are a new learning method but have been reported by Joachims (1998) to be well suited for learning in text classification. We experiment with three kernel types of SVMs: linear, polynomial, and radial base function (RBF). Rainbow (McCallum, 2000) is used to examine these learning methods, except the RBF kernel for which libsvm (Chang and Lin, 2001) is used. Miliaraki and Androutsopoulos (2004) use a SVM with simple inner product (polynomial of first degree) kernel because higher degree polynomial kernels were reported as giving no improvement. However we want to experiment with 68 Cfg NB ME svm1a svm2b svm3c 1 85.75 ± 0.57 85.35 ± 0.77 77.65 ± 0.87 78.39 ± 0.67 81.95 ± 0.82 2 87.77 ± 0.51 88.65 ± 0.54 84.02 ± 0.47 84.26 ± 0.52 85.38 ± 0.77 3 89.82 ± 0.53 88.82 ± 0.66 83.93 ± 0.57 84.24 ± 0.54 87.04 ± 0.95 4 85.22 ± 0.35 89.08 ± 0.50 84.93 ± 0.57 85.57 ± 0.53 87.77 ± 0.89 5 85.44 ± 0.45 91.38 ± 0.42 86.90 ± 0.48 86.90 ± 0.53 87.60 ± 0.87</context>
</contexts>
<marker>Chang, Lin, 2001</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: a library for support vector machines. Software available at http://www.csie.ntu. edu.tw/˜cjlin/libsvm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proc. of the 14th COLING,</booktitle>
<pages>539--545</pages>
<location>Nantes, France.</location>
<contexts>
<context position="4432" citStr="Hearst (1992)" startWordPosition="706" endWordPosition="707">ram features, and various syntactic features. The best classifier achieves a 60% error reduction compared to our baseline system. 2 Previous work Work on identifying definitions from free text initially relied on manually crafted patterns without applying any machine learning technique. Klavans and Muresan (2000) set up a pattern extractor for their Definder system using a tagger and a finite state grammar. Joho and Sanderson (2000) retrieve descriptive phrases (dp) of query nouns (qn) from text to answer definition questions like Who is qn? Patterns such as ‘dp especially qn’, as utilized by Hearst (1992), are used to extract names and their descriptions. Similar patterns are also applied by Liu et al. (2003) to mine definitions of topic-specific concepts on the Web. As an additional assumption, specific documents dedicated to the concepts can be identified if they have particular HTML and hyperlink structures. Hildebrandt et al. (2004) exploit surface patterns to extract as many relevant ”nuggets” of information of a concept as possible. Similar to our work, a copular pattern NP1 be NP2 is used as one of the extraction patterns. Nuggets which do not begin with a determiner are discarded to fi</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti A. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proc. of the 14th COLING, pages 539–545, Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Hildebrandt</author>
<author>B Katz</author>
<author>J J Lin</author>
</authors>
<title>Answering definition questions with multiple knowledge sources.</title>
<date>2004</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>49--56</pages>
<contexts>
<context position="4770" citStr="Hildebrandt et al. (2004)" startWordPosition="758" endWordPosition="761">ttern extractor for their Definder system using a tagger and a finite state grammar. Joho and Sanderson (2000) retrieve descriptive phrases (dp) of query nouns (qn) from text to answer definition questions like Who is qn? Patterns such as ‘dp especially qn’, as utilized by Hearst (1992), are used to extract names and their descriptions. Similar patterns are also applied by Liu et al. (2003) to mine definitions of topic-specific concepts on the Web. As an additional assumption, specific documents dedicated to the concepts can be identified if they have particular HTML and hyperlink structures. Hildebrandt et al. (2004) exploit surface patterns to extract as many relevant ”nuggets” of information of a concept as possible. Similar to our work, a copular pattern NP1 be NP2 is used as one of the extraction patterns. Nuggets which do not begin with a determiner are discarded to filter out spurious nuggets (e.g., progressive tense). Nuggets extracted from every article in a corpus are then stored in a relational database. In the end, answering definition questions becomes as simple as looking up relevant terms from the database. This strategy is similar to our approach for answering definition questions. The use </context>
</contexts>
<marker>Hildebrandt, Katz, Lin, 2004</marker>
<rawString>W. Hildebrandt, B. Katz, and J.J. Lin. 2004. Answering definition questions with multiple knowledge sources. In HLT-NAACL, pages 49–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin Huang</author>
<author>Jingjing Lu</author>
<author>Charles X Ling</author>
</authors>
<title>Comparing naive bayes, decision trees, and svm with auc and accuracy.</title>
<date>2003</date>
<booktitle>In ICDM ’03: Proceedings of the Third IEEE International Conference on Data Mining,</booktitle>
<publisher>IEEE Computer Society.</publisher>
<location>Washington, DC, USA.</location>
<contexts>
<context position="24180" citStr="Huang et al. (2003)" startWordPosition="3943" endWordPosition="3946">ng root forms to configuration 3 (config. 7) and to configuration 8 (config. 10) does not improve accuracies. However, the best accuracies of naive Bayes (90.26%) and maximum entropy (92.21%) are achieved when named entity and root forms are not included as attributes. We now evaluate the classifiers. It is clear from the table that SVM1 and SVM2 settings can not achieve better accuracy compared to the naive Bayes setting, while SVM3 setting marginally outperforms naive Bayes (on 6 out of 11 configurations). This result is contrary to the superiority of SVMs in many text classification tasks. Huang et al. (2003) reported that both classifiers show similar predictive accuracy and AUC (area under the ROC (Receiver Operating Characteristics) curve) scores. This performance of naive Bayes supports the motivation behind its renaisance in machine learning (Lewis, 1998). From the three SVM settings, SVM with RBF kernel appears as the best classifier for our task in which it outperforms other SVMs settings in all configurations. This result supports the above mentioned argument that if the best C and γ can be selected, we do not need to consider linear SVM (e.g. the svm1 setting). Among all of the classifier</context>
</contexts>
<marker>Huang, Lu, Ling, 2003</marker>
<rawString>Jin Huang, Jingjing Lu, and Charles X. Ling. 2003. Comparing naive bayes, decision trees, and svm with auc and accuracy. In ICDM ’03: Proceedings of the Third IEEE International Conference on Data Mining, Washington, DC, USA. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Text categorization with support vector machines: learning with many relevant features.</title>
<date>1998</date>
<booktitle>In Claire N’edellec and C’eline Rouveirol, editors, Proceedings of ECML-98, 10th European Conference on Machine Learning,</booktitle>
<pages>137--142</pages>
<publisher>Springer Verlag,</publisher>
<location>Chemnitz, DE.</location>
<contexts>
<context position="17957" citStr="Joachims (1998)" startWordPosition="2886" endWordPosition="2887">figurations in Table 6, namely naive Bayes, maximum entropy, and support vector machines (SVMs). Naive Bayes is a fast and easy to use classifier based on the probabilistic model of text and has often been used in text classification tasks as a baseline. Maximum entropy is a general estimation technique that has been used in many fields such as information retrieval and machine learning. Some experiments in text classification show that maximum entropy often outperforms naive Bayes, e.g. on two of three data sets in Nigam et al. (1999). SVMs are a new learning method but have been reported by Joachims (1998) to be well suited for learning in text classification. We experiment with three kernel types of SVMs: linear, polynomial, and radial base function (RBF). Rainbow (McCallum, 2000) is used to examine these learning methods, except the RBF kernel for which libsvm (Chang and Lin, 2001) is used. Miliaraki and Androutsopoulos (2004) use a SVM with simple inner product (polynomial of first degree) kernel because higher degree polynomial kernels were reported as giving no improvement. However we want to experiment with 68 Cfg NB ME svm1a svm2b svm3c 1 85.75 ± 0.57 85.35 ± 0.77 77.65 ± 0.87 78.39 ± 0.</context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>Thorsten Joachims. 1998. Text categorization with support vector machines: learning with many relevant features. In Claire N’edellec and C’eline Rouveirol, editors, Proceedings of ECML-98, 10th European Conference on Machine Learning, pages 137– 142, Chemnitz, DE. Springer Verlag, Heidelberg, DE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Joho</author>
<author>M Sanderson</author>
</authors>
<title>Retrieving descriptive phrases from large amounts of free text.</title>
<date>2000</date>
<booktitle>In CIKM,</booktitle>
<pages>180--186</pages>
<contexts>
<context position="4255" citStr="Joho and Sanderson (2000)" startWordPosition="673" endWordPosition="676">b, and predicative phrase. A manually annotated subsection of the corpus was divided into definition and non-definition sentences. Next, we trained various classifiers using unigram and bigram features, and various syntactic features. The best classifier achieves a 60% error reduction compared to our baseline system. 2 Previous work Work on identifying definitions from free text initially relied on manually crafted patterns without applying any machine learning technique. Klavans and Muresan (2000) set up a pattern extractor for their Definder system using a tagger and a finite state grammar. Joho and Sanderson (2000) retrieve descriptive phrases (dp) of query nouns (qn) from text to answer definition questions like Who is qn? Patterns such as ‘dp especially qn’, as utilized by Hearst (1992), are used to extract names and their descriptions. Similar patterns are also applied by Liu et al. (2003) to mine definitions of topic-specific concepts on the Web. As an additional assumption, specific documents dedicated to the concepts can be identified if they have particular HTML and hyperlink structures. Hildebrandt et al. (2004) exploit surface patterns to extract as many relevant ”nuggets” of information of a c</context>
<context position="5548" citStr="Joho and Sanderson, 2000" startWordPosition="885" endWordPosition="888">2 is used as one of the extraction patterns. Nuggets which do not begin with a determiner are discarded to filter out spurious nuggets (e.g., progressive tense). Nuggets extracted from every article in a corpus are then stored in a relational database. In the end, answering definition questions becomes as simple as looking up relevant terms from the database. This strategy is similar to our approach for answering definition questions. The use of machine learning techniques can be found in Miliaraki and Androutsopoulos (2004) and Androutsopoulos and Galanis (2005) They use similar patterns as (Joho and Sanderson, 2000) to construct training attributes. Sager and L’Homme (1994) note that the definition of a term should at least always contain genus (term’s category) and species (term’s properties). BlairGoldensohn et al. (2004) uses machine learning and manually crafted lexico-syntactic patterns to match sentences containing both a genus and species phrase for a given term. There is an intuition that most of definition sentences are located at the beginning of documents. This lead to the use of sentence number as a good indicator of potential definition sentences. Joho and Sanderson (2000) use the position o</context>
</contexts>
<marker>Joho, Sanderson, 2000</marker>
<rawString>H. Joho and M. Sanderson. 2000. Retrieving descriptive phrases from large amounts of free text. In CIKM, pages 180–186.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sathiya Keerthi</author>
<author>Chih-Jen Lin</author>
</authors>
<title>Asymptotic behaviors of support vector machines with gaussian kernel.</title>
<date>2003</date>
<journal>Neural Comput.,</journal>
<volume>15</volume>
<issue>7</issue>
<contexts>
<context position="20047" citStr="Keerthi and Lin, 2003" startWordPosition="3281" endWordPosition="3284"> Bayes (NB), maximum entropy (ME), and three SVM settings at the different attribute configurations. the RBF (gaussian) kernel by selecting model parameters C (penalty for misclassification) and γ (function of the deviation of the Gaussian Kernel) so that the classifier can accurately predict testing data. This experiment is based on the argument that if a complete model selection using the gaussian kernel has been conducted, there is no need to consider linear SVM, because the RBF kernel with certain parameters (C, γ) has the same performance as the linear kernel with a penalty parameter C˜ (Keerthi and Lin, 2003). Given the finite dataset, we use k-fold crossvalidation (k = 20) to estimate the future performance of each classifier induced by its learning method and dataset. This estimation method introduces lower bias compared to a bootstrap method which has extremely large bias on some problems (Kohavi, 1995). 7 Evaluation We evaluated each configuration of Section 5 and each learning method of Section 6 on the dataset which consists of 1336 definitions and 963 nondefinitions sentences. Table 7 reports the accuracy and standard error estimated from this experiment. In all experiment runs, all of the </context>
</contexts>
<marker>Keerthi, Lin, 2003</marker>
<rawString>S. Sathiya Keerthi and Chih-Jen Lin. 2003. Asymptotic behaviors of support vector machines with gaussian kernel. Neural Comput., 15(7):1667–1689.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L Klavans</author>
<author>S Muresan</author>
</authors>
<title>Definder: Rulebased methods for the extraction of medical terminology and their associated definitions from on-line text.</title>
<date>2000</date>
<journal>In American Medical Informatics Assoc</journal>
<contexts>
<context position="4133" citStr="Klavans and Muresan (2000)" startWordPosition="650" endWordPosition="654">ques can be used to distinguish definitions from non-definitions in a corpus of sentences containing a subject, copular verb, and predicative phrase. A manually annotated subsection of the corpus was divided into definition and non-definition sentences. Next, we trained various classifiers using unigram and bigram features, and various syntactic features. The best classifier achieves a 60% error reduction compared to our baseline system. 2 Previous work Work on identifying definitions from free text initially relied on manually crafted patterns without applying any machine learning technique. Klavans and Muresan (2000) set up a pattern extractor for their Definder system using a tagger and a finite state grammar. Joho and Sanderson (2000) retrieve descriptive phrases (dp) of query nouns (qn) from text to answer definition questions like Who is qn? Patterns such as ‘dp especially qn’, as utilized by Hearst (1992), are used to extract names and their descriptions. Similar patterns are also applied by Liu et al. (2003) to mine definitions of topic-specific concepts on the Web. As an additional assumption, specific documents dedicated to the concepts can be identified if they have particular HTML and hyperlink </context>
<context position="10934" citStr="Klavans and Muresan (2000)" startWordPosition="1759" endWordPosition="1762">st, relies heavily on this fact to answer definition queries. However it is also obvious that definition sentences can also be found in other positions. For documents from other sources, which are not as structured as Wikipedia, the first position sentence is likely to be an even weaker predictor of definition vs. nondefinition sentences. 5 Attributes of definition sentences We aim at finding the best attributes for classifying definition sentences. We experimented with combinations of the following attributes: Text properties: bag-of-words, bigrams, and root forms. Punctuation is included as Klavans and Muresan (2000) observe that it can be used to recognize definitions (i.e. definitions tend to conin Tjong Kim Sang et al. (2005) contained web material from various sources, such as patient discussion groups, as well. The latter tends to contain more subjective and contextdependent material. 5google.com 66 Sentence Def Non-def Undecided position first 831 18 31 other 535 915 170 Total 1366 933 201 Table 1: Number of sentences in the first and other position of documents annotated as definition, non-definition, and undecided. tain parentheses more often than non-definitions). No stopword filtering is applied</context>
</contexts>
<marker>Klavans, Muresan, 2000</marker>
<rawString>J.L. Klavans and S. Muresan. 2000. Definder: Rulebased methods for the extraction of medical terminology and their associated definitions from on-line text. In American Medical Informatics Assoc 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ron Kohavi</author>
</authors>
<title>A study of cross-validation and bootstrap for accuracy estimation and model selection. In</title>
<date>1995</date>
<booktitle>IJCAI,</booktitle>
<pages>1137--1145</pages>
<contexts>
<context position="20350" citStr="Kohavi, 1995" startWordPosition="3333" endWordPosition="3334"> experiment is based on the argument that if a complete model selection using the gaussian kernel has been conducted, there is no need to consider linear SVM, because the RBF kernel with certain parameters (C, γ) has the same performance as the linear kernel with a penalty parameter C˜ (Keerthi and Lin, 2003). Given the finite dataset, we use k-fold crossvalidation (k = 20) to estimate the future performance of each classifier induced by its learning method and dataset. This estimation method introduces lower bias compared to a bootstrap method which has extremely large bias on some problems (Kohavi, 1995). 7 Evaluation We evaluated each configuration of Section 5 and each learning method of Section 6 on the dataset which consists of 1336 definitions and 963 nondefinitions sentences. Table 7 reports the accuracy and standard error estimated from this experiment. In all experiment runs, all of the classifiers in all configurations outperform our baseline (75.9%). The best accuracy of each classifier (bold) is between 11.57% to 16.31% above the baseline. The bigram only attributes (config. 2) clearly outperform the simplest setting (bag-of-word only attributes) for all classifiers. The combinatio</context>
</contexts>
<marker>Kohavi, 1995</marker>
<rawString>Ron Kohavi. 1995. A study of cross-validation and bootstrap for accuracy estimation and model selection. In IJCAI, pages 1137–1145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Lewis</author>
</authors>
<title>Naive (Bayes) at forty: The independence assumption in information retrieval.</title>
<date>1998</date>
<booktitle>In Claire N´edellec and C´eline Rouveirol, editors, Proceedings of ECML-98, 10th European Conference on Machine Learning,</booktitle>
<pages>4--15</pages>
<publisher>Springer Verlag,</publisher>
<location>Chemnitz, DE.</location>
<contexts>
<context position="24436" citStr="Lewis, 1998" startWordPosition="3981" endWordPosition="3982">butes. We now evaluate the classifiers. It is clear from the table that SVM1 and SVM2 settings can not achieve better accuracy compared to the naive Bayes setting, while SVM3 setting marginally outperforms naive Bayes (on 6 out of 11 configurations). This result is contrary to the superiority of SVMs in many text classification tasks. Huang et al. (2003) reported that both classifiers show similar predictive accuracy and AUC (area under the ROC (Receiver Operating Characteristics) curve) scores. This performance of naive Bayes supports the motivation behind its renaisance in machine learning (Lewis, 1998). From the three SVM settings, SVM with RBF kernel appears as the best classifier for our task in which it outperforms other SVMs settings in all configurations. This result supports the above mentioned argument that if the best C and γ can be selected, we do not need to consider linear SVM (e.g. the svm1 setting). Among all of the classifiers, maximum entropy shows the best accuracy. It wins at 9 out of 11 configurations in all experiments. This result confirms previous reports e.g. in Nigam et al. (1999) that maximum entropy performs better than naive Bayes in some text classification tasks.</context>
</contexts>
<marker>Lewis, 1998</marker>
<rawString>David D. Lewis. 1998. Naive (Bayes) at forty: The independence assumption in information retrieval. In Claire N´edellec and C´eline Rouveirol, editors, Proceedings of ECML-98, 10th European Conference on Machine Learning, pages 4–15, Chemnitz, DE. Springer Verlag, Heidelberg, DE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Liu</author>
<author>C W Chin</author>
<author>H T Ng</author>
</authors>
<title>Mining topicspecific concepts and definitions on the web. In</title>
<date>2003</date>
<booktitle>WWW ’03: Proceedings of the 12th international conference on World Wide Web,</booktitle>
<pages>251--260</pages>
<publisher>ACM Press.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="4538" citStr="Liu et al. (2003)" startWordPosition="722" endWordPosition="725">red to our baseline system. 2 Previous work Work on identifying definitions from free text initially relied on manually crafted patterns without applying any machine learning technique. Klavans and Muresan (2000) set up a pattern extractor for their Definder system using a tagger and a finite state grammar. Joho and Sanderson (2000) retrieve descriptive phrases (dp) of query nouns (qn) from text to answer definition questions like Who is qn? Patterns such as ‘dp especially qn’, as utilized by Hearst (1992), are used to extract names and their descriptions. Similar patterns are also applied by Liu et al. (2003) to mine definitions of topic-specific concepts on the Web. As an additional assumption, specific documents dedicated to the concepts can be identified if they have particular HTML and hyperlink structures. Hildebrandt et al. (2004) exploit surface patterns to extract as many relevant ”nuggets” of information of a concept as possible. Similar to our work, a copular pattern NP1 be NP2 is used as one of the extraction patterns. Nuggets which do not begin with a determiner are discarded to filter out spurious nuggets (e.g., progressive tense). Nuggets extracted from every article in a corpus are </context>
</contexts>
<marker>Liu, Chin, Ng, 2003</marker>
<rawString>B. Liu, C.W. Chin, and H.T. Ng. 2003. Mining topicspecific concepts and definitions on the web. In WWW ’03: Proceedings of the 12th international conference on World Wide Web, pages 251–260, New York, NY, USA. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Malouf</author>
<author>Gertjan van Noord</author>
</authors>
<title>Wide coverage parsing with stochastic attribute value grammars.</title>
<date>2004</date>
<booktitle>In IJCNLP-04 Workshop Beyond Shallow</booktitle>
<location>Hainan.</location>
<marker>Malouf, van Noord, 2004</marker>
<rawString>Robert Malouf and Gertjan van Noord. 2004. Wide coverage parsing with stochastic attribute value grammars. In IJCNLP-04 Workshop Beyond Shallow Analyses - Formalisms and statistical modeling for deep analyses, Hainan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
</authors>
<title>Bow: A toolkit for statistical language modeling, text retrieval, classification and clustering.</title>
<date>2000</date>
<note>http://www.cs.cmu.edu/ ˜mccallum/bow.</note>
<contexts>
<context position="15707" citStr="McCallum, 2000" startWordPosition="2527" endWordPosition="2528"> synt + pos is a first sent a other sent are is a is indef pred ) is no det subj the init subj is DIGITS a are the are this is or other det pred is of ) is this/these noninit subj atomic number def subj atomic number DIGITS the with symbol is DIGITS and atomic number are the that this chemical or a chemical other det subj chemical element is of Table 5: 20 most informative features for the systems using word bigrams only and word bigrams in combination with syntactic and sentence position features (word features have been translated into English). We use the text classification tool Rainbow6 (McCallum, 2000) to perform most of our experiments. Each sentence is represented as a string of words, possibly followed by bigrams, root forms, (combinations of) syntactic features, etc. All experiments were performed by selecting only the 2000 highest ranked features according to information gain. In the experiments which include syntactic features, the most informative features tend to contain a fair number of syntactic features. This is illustrated for the configuration using bigrams, sentence position, and syntax in table 5. It supports our intuition that the position of subjects and the type of determi</context>
<context position="18136" citStr="McCallum, 2000" startWordPosition="2914" endWordPosition="2915">of text and has often been used in text classification tasks as a baseline. Maximum entropy is a general estimation technique that has been used in many fields such as information retrieval and machine learning. Some experiments in text classification show that maximum entropy often outperforms naive Bayes, e.g. on two of three data sets in Nigam et al. (1999). SVMs are a new learning method but have been reported by Joachims (1998) to be well suited for learning in text classification. We experiment with three kernel types of SVMs: linear, polynomial, and radial base function (RBF). Rainbow (McCallum, 2000) is used to examine these learning methods, except the RBF kernel for which libsvm (Chang and Lin, 2001) is used. Miliaraki and Androutsopoulos (2004) use a SVM with simple inner product (polynomial of first degree) kernel because higher degree polynomial kernels were reported as giving no improvement. However we want to experiment with 68 Cfg NB ME svm1a svm2b svm3c 1 85.75 ± 0.57 85.35 ± 0.77 77.65 ± 0.87 78.39 ± 0.67 81.95 ± 0.82 2 87.77 ± 0.51 88.65 ± 0.54 84.02 ± 0.47 84.26 ± 0.52 85.38 ± 0.77 3 89.82 ± 0.53 88.82 ± 0.66 83.93 ± 0.57 84.24 ± 0.54 87.04 ± 0.95 4 85.22 ± 0.35 89.08 ± 0.50 8</context>
</contexts>
<marker>McCallum, 2000</marker>
<rawString>A McCallum. 2000. Bow: A toolkit for statistical language modeling, text retrieval, classification and clustering. http://www.cs.cmu.edu/ ˜mccallum/bow.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Miliaraki</author>
<author>I Androutsopoulos</author>
</authors>
<title>Learning to identify single-snippet answers to definition questions.</title>
<date>2004</date>
<booktitle>In 20th International Conference on Computational Linguistics (COLING 2004),</booktitle>
<pages>1360--1366</pages>
<location>Geneva, Switzerland. COLING</location>
<contexts>
<context position="5453" citStr="Miliaraki and Androutsopoulos (2004)" startWordPosition="871" endWordPosition="874">levant ”nuggets” of information of a concept as possible. Similar to our work, a copular pattern NP1 be NP2 is used as one of the extraction patterns. Nuggets which do not begin with a determiner are discarded to filter out spurious nuggets (e.g., progressive tense). Nuggets extracted from every article in a corpus are then stored in a relational database. In the end, answering definition questions becomes as simple as looking up relevant terms from the database. This strategy is similar to our approach for answering definition questions. The use of machine learning techniques can be found in Miliaraki and Androutsopoulos (2004) and Androutsopoulos and Galanis (2005) They use similar patterns as (Joho and Sanderson, 2000) to construct training attributes. Sager and L’Homme (1994) note that the definition of a term should at least always contain genus (term’s category) and species (term’s properties). BlairGoldensohn et al. (2004) uses machine learning and manually crafted lexico-syntactic patterns to match sentences containing both a genus and species phrase for a given term. There is an intuition that most of definition sentences are located at the beginning of documents. This lead to the use of sentence number as a</context>
<context position="14881" citStr="Miliaraki and Androutsopoulos, 2004" startWordPosition="2376" endWordPosition="2379">ve complements but it turned out that very few predicates contained named entities, and thus no significant differences in distribution between definition and non-definition sentences could be observed. Features for lexical patterns, as used in (Androutsopoulos and Galanis, 2005), e.g. qn which (is|was|are|were) dp, are not added because in this experiment we investigate only a copular pattern. WordNet-based attributes are also excluded, given that coverage for Dutch (using EuroWordNet) tends to be less good than for English, and even for English their contribution is sometimes insignificant (Miliaraki and Androutsopoulos, 2004). Type Definition Non-def no-nec 59 88 location 10 4 organization 8 3 person 22 4 Table 4: Percentage of named-entity classes of subjects in definition and non-definition sentences. 67 word bigrams only bigram + synt + pos is a first sent a other sent are is a is indef pred ) is no det subj the init subj is DIGITS a are the are this is or other det pred is of ) is this/these noninit subj atomic number def subj atomic number DIGITS the with symbol is DIGITS and atomic number are the that this chemical or a chemical other det subj chemical element is of Table 5: 20 most informative features for </context>
<context position="18286" citStr="Miliaraki and Androutsopoulos (2004)" startWordPosition="2936" endWordPosition="2939">t has been used in many fields such as information retrieval and machine learning. Some experiments in text classification show that maximum entropy often outperforms naive Bayes, e.g. on two of three data sets in Nigam et al. (1999). SVMs are a new learning method but have been reported by Joachims (1998) to be well suited for learning in text classification. We experiment with three kernel types of SVMs: linear, polynomial, and radial base function (RBF). Rainbow (McCallum, 2000) is used to examine these learning methods, except the RBF kernel for which libsvm (Chang and Lin, 2001) is used. Miliaraki and Androutsopoulos (2004) use a SVM with simple inner product (polynomial of first degree) kernel because higher degree polynomial kernels were reported as giving no improvement. However we want to experiment with 68 Cfg NB ME svm1a svm2b svm3c 1 85.75 ± 0.57 85.35 ± 0.77 77.65 ± 0.87 78.39 ± 0.67 81.95 ± 0.82 2 87.77 ± 0.51 88.65 ± 0.54 84.02 ± 0.47 84.26 ± 0.52 85.38 ± 0.77 3 89.82 ± 0.53 88.82 ± 0.66 83.93 ± 0.57 84.24 ± 0.54 87.04 ± 0.95 4 85.22 ± 0.35 89.08 ± 0.50 84.93 ± 0.57 85.57 ± 0.53 87.77 ± 0.89 5 85.44 ± 0.45 91.38 ± 0.42 86.90 ± 0.48 86.90 ± 0.53 87.60 ± 0.87 6 90.26 ± 0.71 90.70 ± 0.48 85.26 ± 0.56 86.0</context>
</contexts>
<marker>Miliaraki, Androutsopoulos, 2004</marker>
<rawString>S. Miliaraki and I. Androutsopoulos. 2004. Learning to identify single-snippet answers to definition questions. In 20th International Conference on Computational Linguistics (COLING 2004), pages 1360– 1366, Geneva, Switzerland. COLING 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Nigam</author>
<author>J Lafferty</author>
<author>A McCallum</author>
</authors>
<title>Using maximum entropy for text classification.</title>
<date>1999</date>
<booktitle>In IJCAI99 Workshop on Machine Learning for Information Filtering,</booktitle>
<pages>61--67</pages>
<contexts>
<context position="17883" citStr="Nigam et al. (1999)" startWordPosition="2871" endWordPosition="2874">ethods We apply three supervised learning methods to each of the attribute configurations in Table 6, namely naive Bayes, maximum entropy, and support vector machines (SVMs). Naive Bayes is a fast and easy to use classifier based on the probabilistic model of text and has often been used in text classification tasks as a baseline. Maximum entropy is a general estimation technique that has been used in many fields such as information retrieval and machine learning. Some experiments in text classification show that maximum entropy often outperforms naive Bayes, e.g. on two of three data sets in Nigam et al. (1999). SVMs are a new learning method but have been reported by Joachims (1998) to be well suited for learning in text classification. We experiment with three kernel types of SVMs: linear, polynomial, and radial base function (RBF). Rainbow (McCallum, 2000) is used to examine these learning methods, except the RBF kernel for which libsvm (Chang and Lin, 2001) is used. Miliaraki and Androutsopoulos (2004) use a SVM with simple inner product (polynomial of first degree) kernel because higher degree polynomial kernels were reported as giving no improvement. However we want to experiment with 68 Cfg N</context>
<context position="24947" citStr="Nigam et al. (1999)" startWordPosition="4069" endWordPosition="4072">s performance of naive Bayes supports the motivation behind its renaisance in machine learning (Lewis, 1998). From the three SVM settings, SVM with RBF kernel appears as the best classifier for our task in which it outperforms other SVMs settings in all configurations. This result supports the above mentioned argument that if the best C and γ can be selected, we do not need to consider linear SVM (e.g. the svm1 setting). Among all of the classifiers, maximum entropy shows the best accuracy. It wins at 9 out of 11 configurations in all experiments. This result confirms previous reports e.g. in Nigam et al. (1999) that maximum entropy performs better than naive Bayes in some text classification tasks. 8 Conclusions and future work We have presented an experiment in identifying definition sentences using syntactic properties and learning-based methods. Our method is concentrated on improving the precision of recognizing definition sentences. The first step is extracting candidate definition sentences from a fully parsed text using syntactic properties of definitions. To distinguish definition from non-definition sentences, we investigated several machine learning methods, namely naive Bayes, maximum ent</context>
</contexts>
<marker>Nigam, Lafferty, McCallum, 1999</marker>
<rawString>K. Nigam, J. Lafferty, and A. McCallum. 1999. Using maximum entropy for text classification. In IJCAI99 Workshop on Machine Learning for Information Filtering, pages 61–67.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juan C Sager</author>
<author>M C L’Homme</author>
</authors>
<title>A model for definition of concepts.</title>
<date>1994</date>
<journal>Terminology,</journal>
<pages>351--374</pages>
<marker>Sager, L’Homme, 1994</marker>
<rawString>Juan C. Sager and M.C. L’Homme. 1994. A model for definition of concepts. Terminology, pages 351– 374.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik Tjong Kim Sang</author>
<author>Gosse Bouma</author>
<author>Maarten de Rijke</author>
</authors>
<title>Developing offline strategies for answering medical questions.</title>
<date>2005</date>
<booktitle>AAAI 2005 workshop on Question Answering in Restricted Domains.</booktitle>
<editor>In Diego Moll´a and Jos´e Luis Vicedo, editors,</editor>
<marker>Sang, Bouma, de Rijke, 2005</marker>
<rawString>Erik Tjong Kim Sang, Gosse Bouma, and Maarten de Rijke. 2005. Developing offline strategies for answering medical questions. In Diego Moll´a and Jos´e Luis Vicedo, editors, AAAI 2005 workshop on Question Answering in Restricted Domains.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>