<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000180">
<title confidence="0.9978945">
Naming the Past: Named Entity and Animacy Recognition
in 19th Century Swedish Literature
</title>
<author confidence="0.961516">
Lars Borin, Dimitrios Kokkinakis, Leif-Jöran Olsson
</author>
<affiliation confidence="0.824094333333333">
Litteraturbanken and Språkdata/Språkbanken
Department of Swedish Language, Göteborg University
Sweden
</affiliation>
<email confidence="0.994592">
{first.last}@svenska.gu.se
</email>
<sectionHeader confidence="0.995516" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999867133333333">
This paper provides a description and
evaluation of a generic named-entity rec-
ognition (NER) system for Swedish applied
to electronic versions of Swedish literary
classics from the 19th century. We discuss
the challenges posed by these texts and the
necessary adaptations introduced into the
NER system in order to achieve accurate
results, useful both for metadata genera-
tion, but also for the enhancement of the
searching and browsing capabilities of Lit-
teraturbanken, the Swedish Literature
Bank, an ongoing cultural heritage project
which aims to digitize significant works of
Swedish literature.
</bodyText>
<sectionHeader confidence="0.998995" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999756956521739">
In this paper we investigate generic named entity
recognition (NER) technology and the necessary
adaptation required in order to automatically anno-
tate electronic versions of a number of Swedish
literary works of fiction from the 19th century.
Both the genre and language variety are markedly
different from the text types that our NER system
was originally developed to annotate. This presents
a challenge, posing both specific and more generic
problems that need to be dealt with.
In section 2 we present briefly the background
and motivation for the present work, and section 3
gives some information on related work. In section
4 we provide a description of the named entity rec-
ognition system used in this work, its entity taxon-
omy, including the animacy recognition compo-
nent and the labeled consistency approach that is
explored. Problems faced in the literary texts and
the kinds of adaptations performed in the recogni-
tion system as well as evaluation and error analysis
are given in section 5. Finally, section 6 summa-
rizes the work and provides some thoughts for fu-
ture work.
</bodyText>
<sectionHeader confidence="0.987658" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.99981536">
Litteraturbanken &lt;http://litteraturbanken.se/&gt; (the
Swedish Literature Bank) is a cultural heritage pro-
ject financed by the Swedish Academy1. Littera-
turbanken has as its aim to make available online
the full text of significant works of Swedish litera-
ture, old and new, in critical editions suitable for
literary research and for the teaching of literature.
There is also abundant ancillary material on the
website, such as author presentations, bibliogra-
phies, thematic essays about authorships, genres or
periods, written by experts in each field.
Similarly to many other literature digitization
initiatives, most of the works in Litteraturbanken
are such for which copyright has expired (i.e., at
least 70 years have passed since the death of the
author); at present the bulk of the texts are from the
18th, 19th and early 20th century. However, there
is also an agreement with the organizations repre-
senting authors’ intellectual property rights, allow-
ing the inclusion of modern works according to a
uniform royalty payment scheme. At present, Lit-
teraturbanken holds about 150 works – mainly
novels – by about 50 different authors. The text
collection is slated to grow by 80–100 novel-length
works (appr. 4–6 million words) annually.
</bodyText>
<footnote confidence="0.894563">
1 The present permanent version of Litteraturbanken was pre-
ceded by a two-year pilot project by the same name, funded by
the Bank of Sweden Tercentenary Foundation.
</footnote>
<page confidence="0.559605">
1
</page>
<note confidence="0.992731">
Proceedings of the Workshop on Language Technology for Cultural Heritage Data (LaTeCH 2007), pages 1–8,
Prague, 28 June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999948970588235">
Even at outset of the Litteraturbanken project, it
was decided to design the technical solutions with
language technology in mind. The rationale for this
was that we saw these literary texts not only as rep-
resenting Sweden’s literary heritage, but also as
high-grade empirical data for linguistic investiga-
tions, i.e. as corpus components. Hence, we wanted
to build an infrastructure for Litteraturbanken
which would allow this intended dual purpose of
the material to be realized to the fullest.2 However,
we soon started to think about how the kinds of
annotations that language technology could pro-
vide could be of use to others than linguists, e.g.
literary scholars, historians and researchers in other
fields in the humanities and social sciences.
Here, we will focus on one of these annotation
types, namely NER and entity annotation. Com-
bined with suitable interfaces for displaying,
searching, selecting, correlating and browsing
named entities, we believe that the recognition and
annotation of named entities in Litteraturbanken
will facilitate more advanced research on literature
(particularly in the field of literary onomastics; see
Dalen-Oskam and Zundert, 2004), but also, e.g.,
historians could find this facility useful, insofar as
these fictional narratives also contain, e.g. descrip-
tions of real locations, characterizations of real
contemporary public figures, etc. Flanders et al.
(1998: 285) argue that references to people in his-
torical sources are of intrinsic interest since they
may reveal “networks of friendship, enmity, and
collaboration; familial relationships; and political
alliances [...] class position, intellectual affilia-
tions, and literary bent of the author”.
</bodyText>
<sectionHeader confidence="0.999953" genericHeader="related work">
3 Related Work
</sectionHeader>
<bodyText confidence="0.978943016393443">
The presented work is naturally related to research
on NER, particularly as applied to dia-
chronic/historical corpora. The technology itself
has been applied to various domains and genres
over the last couple of decades such as financial
news and biomedicine, with performance rates dif-
ficult to compare since the task is usually tied to
particular domains/genres and applications. For a
concise overview of the technology see Borthwick,
2 This precluded the use of ready-made digital library or CMS
solutions, as we wanted to be compatible with emerging stan-
dards for language resources and tools, e.g. TEI/(X)CES and
ISO TC37/SC07, which to our knowledge has never been a
consideration in the design of digital library or CM systems.
(1999). Even though this technology is widely used
in a number of domains, studies dealing with his-
torical corpora are mostly comparatively recent
(see for instance the recent workshop on historical
text mining;
&lt;http://ucrel.lancs.ac.uk/events/htm06/&gt;).
Shoemaker (2005) reports on how the Old Bai-
ley Proceedings, which contain accounts of trials
that took place at the Old Bailey, the primary
criminal court in London, between 1674 and 1834,
was marked up for a number of semantic catego-
ries, including the crime date and location, the de-
fendant’s gender, the victim’s name etc. Most of
the work was done manually while support was
provided for automatic person name3 identification
(cf. Bontcheva et al., 2002). The author mentions
future plans to take advantage of the structured
nature of the Proceedings and to use the lists of
persons, locations and occupations that have al-
ready been compiled for annotating new texts.
Crane and Jones (2006) discuss the evaluation of
the extraction of 10 named entity classes (personal
names, locations, dates, products, organizations,
streets, newspapers, ships, regiments and railroads)
from a 19th century newspaper. The quality of
their results vary for different entity types, from
99.3% precision for Streets to 57.5% precision for
Products. The authors suggest the kinds of knowl-
edge that digital libraries need to assemble as part
of their machine readable reference collections in
order to support entity identification as a core ser-
vice, namely, the need for bigger authority lists,
more refined rule sets and rich knowledge sources
as training data.
At least least two projects are also relevant in
the context of NER and historical text processing,
namely NORA &lt;http://www.noraproject.org/&gt; and
ARMADILLO
&lt;http://www.hrionline.ac.uk/armadillo/&gt;. The goal
of the first is to produce text mining software for
discovering, visualizing, and exploring significant
patterns across large collections of full-text hu-
manities resources in existing digital libraries. The
goal of the latter is to evaluate the benefits of
automated mining techniques (including informa-
tion extraction) on a set of online resources in
eighteenth-century British social history.
</bodyText>
<footnote confidence="0.9021595">
3 By using the General Architecture for Text Engineering
(GATE) platform; &lt;http://gate.ac.uk&gt;.
</footnote>
<page confidence="0.995513">
2
</page>
<sectionHeader confidence="0.986767" genericHeader="method">
4 Named Entity Recognition
</sectionHeader>
<bodyText confidence="0.998279666666667">
Named entity recognition (NER) or entity identifi-
cation/extraction, is an important supporting tech-
nology with numerous applications in a number of
human language technologies. The system we use
originates from the work conducted in the Nomen
Nescio project; for details see Johannessen et al.
(2005). In brief, the Swedish system is a multi-
purpose NER system, comprised by a number of
modules applied in a pipeline fash-ion. Six major
components can be distinguished, making a clear
separation between lexical, gram-matical and proc-
essing resources. The six compo-nents are:
</bodyText>
<listItem confidence="0.976172217391305">
• lists of multiword names, taken from
various Internet sites or extracted from vari-
ous corpora, running directly over the to-
kenised text being processed;
• a rule-based, shallow parsing component
that uses finite-state grammars, one gram-
mar for each type of entity recognized;
• a module that uses the annotations pro-
duced by the previous two components,
which have a high rate in precision, in order
to make decisions regarding other un-
annotated entities. This module is further
discussed in Section 4.2;
• lists of single names (approx. 100,000);
• name similarity, this module is further
discussed in Section 4.3;
• a theory revision and refinement mod-
ule, which makes a final control of an anno-
tated document, in order to detect and re-
solve possible errors and assign new annota-
tions based on existing ones, for instance by
applying name similarity or by combining
various annotation fragments.
</listItem>
<subsectionHeader confidence="0.993492">
4.1 Named-Entity Taxonomy
</subsectionHeader>
<bodyText confidence="0.99992975">
The nature and type of named entities vary depend-
ing on the task under investigation or the target
application. In any case, personal names, location
and organization names are considered “generic”.
Since semantic annotation is not as well under-
stood as grammatical annotation, there is no con-
sensus on a standard tagset and content to be gen-
erally applicable. Recently, however, there have
been attempts to define and apply richer name hi-
erarchies for various tasks, both specific (Fleisch-
man and Hovy, 2002) and generic (Sekine, 2004).
Our current system implements a rather fine-
grained named entity taxonomy with 8 main
named entitiy types as well as 57 subtypes. Details
can be found in Johannessen et al., 2005, and Kok-
kinakis, 2004. The eight main categories are:
</bodyText>
<listItem confidence="0.9994761875">
• Person (PRS): people names (forenames,
surnames), groups of people, animal/pet
names, mythological, theonyms;
• Location (LOC): functional locations,
geographical, geo-political, astrological;
• Organization (ORG): political, athletic,
media, military, etc.;
• Artifact (OBJ): food/wine products,
prizes, communic. means (vehicles) etc.;
• Work&amp;Art (WRK): printed material,
names of films and novels, sculptures etc.;
• Event (EVN): religious, athletic, scien-
tific, cultural etc.;
• Measure/Numerical (MSR): volume, age,
index, dosage, web-related, speed etc.;
• Temporal (TME).
</listItem>
<bodyText confidence="0.997831666666667">
Time expressions are important since they allow
temporal reasoning about complex events as well
as time-line visualization of the story developed in
a text. The temporal expressions recognized in-
clude both relative (nästa vecka ‘next week’) and
absolute expressions (klockan 8 på morgonen i dag
‘8 o’clock in the morning today’), and sets or se-
quences of time points or stretches of time (varje
dag ‘every day’).
</bodyText>
<subsectionHeader confidence="0.998613">
4.2 Animacy Recognition
</subsectionHeader>
<bodyText confidence="0.999909166666667">
The rule-based component of the person-name rec-
ognition grammar is based on a large set of desig-
nator words and a group of phrases and verbal
predicates that most probably require an animate
subject (e.g. berätta ‘to tell’, fundera ‘to think’,
tröttna ‘to become tired’). These are used in con-
junction with orthographic markers in the text,
such as capitalization, for the recognition of per-
sonal names. In this work, we consider the first
group (designators) as relevant knowledge to be
extracted from the person name recognizer, which
is explored for the annotation of animate instances
</bodyText>
<page confidence="0.993547">
3
</page>
<bodyText confidence="0.999933769230769">
in the literary texts. The designators are imple-
mented as a separate module in the current pipe-
line, and constitute a piece of information which is
considered important for a wide range of tasks (cf.
Orasan and Evans, 2001).
The designators are divided into four groups:
designators that denote the nationality or the eth-
nic/racial group of a person (e.g. tysken ‘the Ger-
man [person]’); designators that denote a profes-
sion (e.g. läkaren ‘the doctor’); those that denote
family ties and relationships (e.g. svärson ‘son in
law’); and finally a group that indicates a human
individual but cannot be unambiguously catego-
rized into any of the three other groups (e.g. pa-
tienten ‘the patient’). Apart from this grouping,
inherent qualities, for at least a large group of the
designators, (internal evidence/morphological
cues) also indicate referent (natural) gender. In this
way, the animacy annotation is further specified
for male, female or unknown gender; unknown in
this context means unresolved or ambiguous, such
as barn ‘child’.
Swedish is a compounding language and com-
pound words are written as a single orthographic
unit (i.e. solid compounds). This fact makes the
recognition of animacy straightforward with mini-
mal resources and feasible by the use of a set of
suitable headwords, and by capturing modifiers by
simple regular expressions. Approximately 25 pat-
terns are enough to identify the vast majority of
animate entities in a text; patterns such as
“inna/innan/innor”, “man/mannen/män/männen”,
“log/logen/loger”, “ktör/ktören/ktörer” and
“iker/ikern/ikerna”. For instance, the pattern in (1)
consists of a reliable suffix “inna” which is a typi-
cal designator for female individuals, preceded by
a set of obligatory strings and an optional regular
expression which captures a long list of com-
pounds (2).
</bodyText>
<listItem confidence="0.870022142857143">
(1) [a-zåä6]*(kvJälskarJmanJgrevJ...)inna
(2) taleskvinna, yrkeskvinna, idrotts-
kvinna, ungkvinna, Stockholmskvin-
na, Dalakvinna, samboälskarinna,
lyxälskarinna, ex-älskarinna, sam-
largrevinna, exälskarinna, markgre-
vinna, majgrevinna, änkegrevinna,...
</listItem>
<bodyText confidence="0.8324026">
Examples of animacy annotations are given in (3).
The attribute value FAM stands for FAmily relation
and Male; PRM for PRofession and Male; FAF for
FAmily relation and Female and finally UNF for
UNknown and Female.
</bodyText>
<listItem confidence="0.572238">
(3) [...]&lt;ENAMEX TYPE=&amp;quot;FAM&amp;quot;&gt;riksgrefvin-
nans far&lt;/ENAMEX&gt;, &lt;ENAMEX TYPE=
</listItem>
<table confidence="0.9820876">
&amp;quot;PRM&amp;quot;&gt;6fveramiralen&lt;/ENAMEX&gt; [...]
hade till &lt;ENAMEX TYPE=&amp;quot;FAF&amp;quot;&gt;mor
&lt;/ENAMEX&gt; &lt;ENAMEX TYPE=&amp;quot;UNF&amp;quot;&gt;gre-
fvinnan&lt;/ENAMEX&gt; Beata Wrangel från
[...]
</table>
<tableCaption confidence="0.908">
Table (3) in Section 6.1 presents the results for the
</tableCaption>
<bodyText confidence="0.4305346">
evaluation of this type of normative information.
Note also, that in order to make the annotations
more practical we have included the person name
designators (e.g. ‘herr’ – ‘Mr’) in the markup as in
(4); here PRS stands for PeRSon:
</bodyText>
<figure confidence="0.4846758">
(4) &lt;ENAMEX TYPE=&amp;quot;UNM&amp;quot;&gt;Herr&lt;/ENAMEX&gt;
&lt;ENAMEX TYPE=&amp;quot;PRS&amp;quot; SBT=&amp;quot;HUM&amp;quot;&gt;Boman
&lt;/ENAMEX&gt; becomes &lt;ENAMEX TYPE=
&amp;quot;PRS-UNM&amp;quot; SBT=&amp;quot;HUM&amp;quot;&gt;Herr Boman
&lt;/ENAMEX&gt;
</figure>
<subsectionHeader confidence="0.974976">
4.3 Name Similarity
</subsectionHeader>
<bodyText confidence="0.999964580645161">
We can safely assume that the various system re-
sources will not be able to identify all possible en-
tities in the texts, particularly personal and location
names. Although there is a large overlap between
the names in the texts and the gazetteer lists, there
were cases that could be considered as entity can-
didates but were left unmarked. This is because
exhaustive lists of names even for limited domains
are hard to obtain, and, in some domains even dif-
ficult to manage. Therefore, we also calculated the
orthographic similarity between such words and
the gazetteer content, according to the following
criteria: a potential entity starts with a capital let-
ter; it is ≥ 5 characters long; it is not part of any
other annotation and it does not stand in the begin-
ning of a sentence. We have empirically observed
that the length of 5 characters is a reliable thresh-
old, unlikely to exclude many NEs. As a matter of
fact, only two such cases could be found in the
evaluation sample, namely ätten Puff ‘the family
Puff’ and “Yen-” in the context “Yen- kenberg”
As measure of orthographic similarity (or rather,
difference) we used the Levenshtein distance (LD;
also known as edit distance) between two strings.
The LD is the number of deletions, insertions or
substitutions required to transform a string into
another string. The greater the distance, the more
different the strings are. We chose to regard 1 and
2 as trustworthy values and disregarded the rest.
We chose these two values since empirical obser-
vations suggest that contemporary Swedish and
</bodyText>
<page confidence="0.99501">
4
</page>
<bodyText confidence="0.999298">
19th century Swedish entities usually differ in one
or two characters. In case of more than one match,
we choose the most frequent alternative, as in the
case of Wenern below. Table 1 illustrates various
cases and the obtained results.
</bodyText>
<table confidence="0.999157777777778">
text word # gazeteer LD ann. ??
Dalarne 6 Dalarna 1 loc yes
Asptomten 1 --- --- --- -
Härnevi* 1 Arnevi 2 prs no
Sabbathsberg 1 Sabbatsberg 1 loc yes
Wenern* 7 Werner,Waern 2 prs no
Vänern 2 loc
Kaknäs 1 Valnäs,Ramnäs 2 loc yes
Kallmar 1 Kalmar 1 loc yes
</table>
<tableCaption confidence="0.9923695">
Table 1. LD between potential NEs and the ga-
zeteers; ‘*’: both are locations;‘??’: correct annot.?
</tableCaption>
<sectionHeader confidence="0.991316" genericHeader="method">
5 The Document Centered Approach
</sectionHeader>
<bodyText confidence="0.999688976744186">
There is a known tradeoff between rule-based and
statistical systems. Handcrafted grammar-based
systems typically obtain better results, but at the
cost of considerable manual effort by domain ex-
perts. Statistical NER systems typically require a
large amount of manually annotated training data,
but can be ported to other domains or genres more
rapidly and require less manual work. Although the
Swedish system is mainly rule-based, using a
handcrafted grammar for each entity group, it can
also be considered a hybrid system in the sense
that it applies a document-centered approach
(DCA) to entity annotation, which is a different
paradigm compared to the local context approach,
called external evidence by McDonald (1996).
With DCA, information for the disambiguation of
a name is derived from the entire document.
DCA as a term originates from the work by
Mikheev (2000: 138), who claims that:
important words are typically used in a
document more than once and in different
contexts. Some of these contexts create
very ambiguous situations but some don’t.
Furthermore, ambiguous words and
phrases are usually unambiguously intro-
duced at least once in the text unless they
are part of common knowledge presup-
posed to be known by the readers.
This implies a form of online learning from the
document being processed where unambiguous
usages are used for assigning annotations to am-
biguous words, and information for disambiguation
is derived from the entire document.
Similarly, label consistency, the preference of
the same annotation for the same word sequence
everywhere in a particular discourse, is a compara-
ble approach for achieving qualitatively higher re-
call rates with minimal resource overhead (cf.
Krishnan and Manning, 2006). Such an approach
has been used, e.g., by Aramaki et al. (2006), for
the identification of personal health information
(age, id, date, phone, location and doctor´s and pa-
tient´s names).
</bodyText>
<figureCaption confidence="0.997221">
Figure 1. Example of label consistency
</figureCaption>
<bodyText confidence="0.9873826">
Figure 1 illustrates this approach with an example
taken from Almqvist’s Collected Works, Vol. 30. In
this example, the first occurrence of the female
person name Micmac, which is not in the gazetteer
lists, is introduced by the author with the unambi-
guous designator faster ‘aunt’. Many of the subse-
quent mentions of the same name are given with-
out any reliable clue for appropriate labelling.
However, as already discussed, there is strong evi-
dence that subsequent mentions of the same name
should be annotated with the same label, and since
the same entity usually appears more than once in
the same discourse, in our case a book, labelling
consistency should guarantee better performance.
There are exceptions for certain NE categories
which may consist of words that are not proper
nouns such as in the Work&amp;Art category, and of
course the temporal and measure groups which are
blocked from this type of processing; cf. section
6.2.
</bodyText>
<sectionHeader confidence="0.981488" genericHeader="evaluation">
6 Evaluation and Error Analysis
</sectionHeader>
<bodyText confidence="0.999788">
The system was evaluated twice, while no nor-
malization or other preprocessing was applied to
the original documents. Problems identified during
the first evaluation round were taken under consid-
eration and specific changes were suggested to the
system by incorporating appropriate modifications.
</bodyText>
<page confidence="0.985999">
5
</page>
<bodyText confidence="0.998176285714286">
During the first run, no adaptations or enhance-
ments were made to the original NER system. Af-
ter the first evaluation round, four major areas
were identified in which the system either failed to
produce an annotation or produced only partial or
erroneous annotations. These failures were caused
by:
</bodyText>
<listItem confidence="0.667650243902439">
• Spelling variation: particularly the use of
&lt;f/w/e/q&gt; instead of &lt;v/v/ä/k&gt; as in modern
Swedish. Most of the cases could be easily
solved while other required different means
such as calculating the LD between the
name lists and possible name mentions in
the texts (Section 4.3). One case that could
be easily tackled was the addition of alter-
nate spelling forms for a handful of key-
words and designators, especially the prepo-
sition av/af common in temporal contexts,
such as i början af/av 1790-talet ‘in the be-
ginning of the 1790s’; or words such
begge/bägge ‘both’ and qväll/kväll ‘eve-
ning’;
• A number of definite plural forms of
nouns, often designating a group of persons,
with the suffix “erne” instead the “erna” as
in modern Swedish, such as Kine-
serne/Kineserna ‘the Chinese [people]’ and
Svenskarne/Svenskarna ‘the Swedes’;
• Unknown names: mentioned once with
unreliable context;
• Structure preservation: the document
structure of the texts in Litteraturbankens is
designed to create a faithful rendering of the
visual appearance of the original printed
books. In extracting the texts from the XML
format used in Litteraturbanken, we did not
want to apply any kind of normalization or
other processing. Such an approach would
have altered the document structure. This
implies that for a handful of the entities, for
which the hyphenation in the original paper
version has divided a name into two parts, as
in (5), correct identification cannot be ac-
complished, while in some cases only a par-
tial identification was possible, as in (6).
(5) [...] Stock- holm
(6) &lt;ENAMEX TYPE=”PRS” SBT=”HUM”&gt;Bertha
von Lichten-&lt;/ENAMEX&gt; ried
</listItem>
<subsectionHeader confidence="0.895634">
6.1 Results
</subsectionHeader>
<bodyText confidence="0.999995607142857">
As a baseline for the evaluation we use the result
of simple dictionary lookup in the single name
gazetteer. This process is very accurate (w.r.t. pre-
cision). We could identify a number of cases with
erroneous annotations, due to various circum-
stances: Names in the gazetteer lists may have
multiple entity tags associated with them, and thus
an entity may belong to more than one group that
could not be disambiguated by the surrounding
context, such as Ekhammar as a city and surname;
many names are ambiguous with common nouns
or verbs, such as Stig as a first name and as the
verb ‘step/walk’; the gazetteers contained a num-
ber of words that should not have been in the list in
the first place, such as Hvem ‘Who’, styrman ‘first
mate’ and fänrik ‘lieutenant’. A probable cause of
the latter problem is the fact that the name lists
have been semi-automatically compiled from vari-
ous sources including corpora and the Internet.
We performed two evaluations, based on two
different random samples consisting of 500 seg-
ments (roughly 30,000 tokens) each. A segment
consists of an integral number of sentences (up to
10–20). The overall results for all tests are shown
in table 2. Results for individual entities using the
whole system during both runs are found in table 3.
The samples were evaluated according to preci-
sion, recall and f-score using the formulas:
</bodyText>
<construct confidence="0.5872728">
Precision = (Total Correct + Partially Corrtect) /
All Produced
Recall = (Total Correct + Partially Correct) /
All Possible
F-score =2*P*R/P+R.
</construct>
<tableCaption confidence="0.99716">
Table 2. Overall performance of the NER
</tableCaption>
<page confidence="0.998833">
6
</page>
<tableCaption confidence="0.911005">
Table 3. Performance of the NER on the individual
named entities including animacy
</tableCaption>
<bodyText confidence="0.991069833333333">
Partially correct means that an annotation gets par-
tial credit. For instance, if the system produces an
annotation for the functional location Nya Elemen-
tarskolan as in (7) instead of the correct (8), then
such annotations are given half a point, instead of a
perfect score.
</bodyText>
<figure confidence="0.882223">
(7) Nya &lt;ENAMEX TYPE=”LOC” SBT=”FNC”&gt;
Elementarskolan&lt;/ENAMEX&gt;
(8) &lt;ENAMEX TYPE=”LOC” SBT=”FNC”&gt;Nya
Elementarskolan&lt;/ENAMEX&gt;
</figure>
<bodyText confidence="0.99964">
If, on the other hand, the type is correct but the
subtype is wrong, then the annotation is given a
score of 0.75 points (e.g. a functional location in-
stead of a geopolitical location).
</bodyText>
<subsectionHeader confidence="0.999877">
6.2 Limitations of the Centering Approach
</subsectionHeader>
<bodyText confidence="0.969916741935484">
Labeling consistency and the DCA approach relies
on the assumption that usage is consistent within
the same document by the same author. However,
we have observed that there are problems with en-
tities composed of more than a single word, par-
ticularly within the group Work&amp;Art, which can
produce conflicting information, if we allow the
individual words in such content (often nouns or
adjectives) to be re-applied in the text.
For instance, the name of the novel Syster och
bror occurred 32 times in one of the evaluation
texts (Almqvist’s Collected Works Volume 29). If
we allow the individual words that constitute the
title, Syster, och and bror to be re-applied in the
text as individual words (2 common nouns and a
conjunction), then we would have degraded the
precision considerably since we would have al-
lowed Work&amp;Art annotations for irrelevant words.
However, such cases can be resolved by simply
letting the system ignore multiword Work&amp;Art
annotations during the DCA processing.
Figure 2. Occurrences of the multi-word entity Sys-
ter och bror; the rule-based system could reliably
identify and annotate 2/32 occurrences.
Generally speaking, the experimental results have
shown that any breaking of a multiword entity,
except personal names, into its individual words
often has a negative effect on performance. The
best results are achieved when the DCA approach
deals with single or bigram entities, particularly
personal names.
</bodyText>
<page confidence="0.99931">
7
</page>
<sectionHeader confidence="0.993251" genericHeader="conclusions">
7 Conclusions and Future Prospects
</sectionHeader>
<bodyText confidence="0.999797625">
In this paper we have described the application of a
generic Swedish named entity recognition system
to a number of literary texts, novels from the 19th
century, part of Litteraturbanken, the Swedish Lit-
erature Bank. We evaluated the results of the
named entity recognition and identified a number
of error sources which we tried to resolve and then
introduce changes that would cover for such cases
in the rule-based component of the system, in order
to increase its performance (precision and recall)
during a second evaluation round.
Entity annotations open up a whole new re-
search spectrum for new kinds of qualitative and
quantitative exploitations of literary and historical
texts, allowing more semantically-oriented explo-
ration of the textual content. In the near future, we
will annotate and evaluate a larger sample and pos-
sibly integrate machine learning techniques in or-
der to improve the results even more. We are also
working to integrate the handling of named entity
annotations into Litteraturbanken’s search and
browsing interfaces and hope to be able to conduct
our first demonstrations and tests with users later
this year.
</bodyText>
<sectionHeader confidence="0.999111" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999866701492537">
Eiji Aramaki, Takeshi Imai, Kengo Miyo and Kazuhiko
Ohe. 2006. Automatic Deidentification by using Sen-
tence Features and Label Consistency. Challenges in
NLP for Clinical Data Workshop. Washington DC.
Kalina Bontcheva, Diana Maynard, Hamish Cunning-
ham and Horacio Saggion. 2002. Using Human Lan-
guage Technology for Automatic Annotation and In-
dexing of Digital Library Content. Proceedings of the
6th European Conference on Research and Advanced
Technology for Digital Libraries.
Andrew Borthwick. 1999. A Maximum Entropy Ap-
proach to Named Entity Recognition. PhD Thesis.
New York University.
Gregory Crane and Alison Jones. 2006. The Challenge
of Virginia Banks: an Evaluation of Named Entity
Analysis in a 19th-century Newspaper Collection.
ACM/IEEE Joint Conference on Digital Libraries,
JCDL. Chapel Hill, NC, USA. 31–40.
Karina van Dalen-Oskam and Joris van Zundert. 2004.
Modelling Features of Characters: Some Digital
Ways to Look at Names in Literary Texts. Literary
and Linguistic Computing 19(3): 289–301.
Julia Flanders, Syd Bauman, Paul Caton and Mavis
Cournane. 1998. Names Proper and Improper: Ap-
plying the TEI to the Classification of Proper Nouns.
Computers and the Humanities 31(4): 285–300.
Michael Fleischman and Eduard Hovy. 2002. Fine
Grained Classification of Named Entities. Proceed-
ings of the 19th International Conference on Compu-
tational Linguistics. Taipei, Taiwan. 1–7.
Janne Bondi Johannessen, Kristin Hagen, Åsne
Haaland, Andra Björk Jónsdottir, Anders Nøklestad,
Dimitrios Kokkinakis, Paul Meurer, Eckhard Bick
and Dorte Haltrup. 2005. Named Entity Recognition
for the Mainland Scandinavian Languages. Literary
and Linguistic Computing. 20(1): 91–102.
Dimitrios Kokkinakis. 2004. Reducing the Effect of
Name Explosion. Proceedings of the LREC-
Workshop: Beyond Named Entity Recognition - Se-
mantic Labeling for NLP. Lisbon, Portugal.
Vijay Krishnan and Christopher D. Manning. 2006. An
Efficient Two-Stage Model for Exploiting Non-Local
Dependencies in Named Entity Recognition. Pro-
ceedings of COLING/ ACL 2006. Sydney, Australia.
1121–1128.
David D. McDonald. 1996. Internal and External Evi-
dence in the Identification and Semantic Categorisa-
tion of Proper Nouns. Corpus-Processing for Lexical
Acquisition. James Pustejovsky and Bran Boguraev
(eds). MIT Press. 21–39.
Andrei Mikheev. 2000. Document Centered Approach
to Text Normalization. Proceedings of the 23rd ACM
SIGIR Conference on Research and Development in
Information Retrieval. Athens, Greece. 136–143.
Satoshi Sekine. 2004. Definition, Dictionaries and Tag-
ger for Extended Named Entity Hierarchy. Proceed-
ings of the Language Resources and Evaluation Con-
ference (LREC). Lisbon, Portugal.
Constantin Orasan and Roger Evans. 2001. Learning to
Identify Animate References. Proceedings of the
Workshop on Computational Natural Language
Learning (CoNLL-2001). ACL-2001. Toulouse,
France.
Robert Shoemaker. 2005. Digital London. Creating a
Searchable Web of Interlinked Sources on Eighteenth
Century London. Program: Electronic Library &amp; In-
formation Systems 39(4): 297–311.
</reference>
<page confidence="0.998486">
8
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.412592">
<title confidence="0.9898875">Naming the Past: Named Entity and Animacy in 19th Century Swedish Literature</title>
<author confidence="0.8921805">Lars Borin</author>
<author confidence="0.8921805">Dimitrios Kokkinakis</author>
<author confidence="0.8921805">Leif-Jöran Litteraturbanken</author>
<affiliation confidence="0.96036">Department of Swedish Language, Göteborg</affiliation>
<address confidence="0.60597">Sweden</address>
<email confidence="0.966807">{first.last}@svenska.gu.se</email>
<abstract confidence="0.9918569375">This paper provides a description and evaluation of a generic named-entity recognition (NER) system for Swedish applied to electronic versions of Swedish literary classics from the 19th century. We discuss the challenges posed by these texts and the necessary adaptations introduced into the NER system in order to achieve accurate results, useful both for metadata generation, but also for the enhancement of the and browsing capabilities of Litthe Swedish Literature Bank, an ongoing cultural heritage project which aims to digitize significant works of Swedish literature.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eiji Aramaki</author>
</authors>
<title>Takeshi Imai, Kengo Miyo and Kazuhiko Ohe.</title>
<date>2006</date>
<location>Washington DC.</location>
<marker>Aramaki, 2006</marker>
<rawString>Eiji Aramaki, Takeshi Imai, Kengo Miyo and Kazuhiko Ohe. 2006. Automatic Deidentification by using Sentence Features and Label Consistency. Challenges in NLP for Clinical Data Workshop. Washington DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kalina Bontcheva</author>
<author>Diana Maynard</author>
<author>Hamish Cunningham</author>
<author>Horacio Saggion</author>
</authors>
<title>Using Human Language Technology for Automatic Annotation and Indexing</title>
<date>2002</date>
<booktitle>of Digital Library Content. Proceedings of the 6th European Conference on Research and Advanced Technology for Digital Libraries.</booktitle>
<contexts>
<context position="6742" citStr="Bontcheva et al., 2002" startWordPosition="1028" endWordPosition="1031"> with historical corpora are mostly comparatively recent (see for instance the recent workshop on historical text mining; &lt;http://ucrel.lancs.ac.uk/events/htm06/&gt;). Shoemaker (2005) reports on how the Old Bailey Proceedings, which contain accounts of trials that took place at the Old Bailey, the primary criminal court in London, between 1674 and 1834, was marked up for a number of semantic categories, including the crime date and location, the defendant’s gender, the victim’s name etc. Most of the work was done manually while support was provided for automatic person name3 identification (cf. Bontcheva et al., 2002). The author mentions future plans to take advantage of the structured nature of the Proceedings and to use the lists of persons, locations and occupations that have already been compiled for annotating new texts. Crane and Jones (2006) discuss the evaluation of the extraction of 10 named entity classes (personal names, locations, dates, products, organizations, streets, newspapers, ships, regiments and railroads) from a 19th century newspaper. The quality of their results vary for different entity types, from 99.3% precision for Streets to 57.5% precision for Products. The authors suggest the</context>
</contexts>
<marker>Bontcheva, Maynard, Cunningham, Saggion, 2002</marker>
<rawString>Kalina Bontcheva, Diana Maynard, Hamish Cunningham and Horacio Saggion. 2002. Using Human Language Technology for Automatic Annotation and Indexing of Digital Library Content. Proceedings of the 6th European Conference on Research and Advanced Technology for Digital Libraries.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Borthwick</author>
</authors>
<title>A Maximum Entropy Approach to Named Entity Recognition. PhD Thesis.</title>
<date>1999</date>
<location>New York University.</location>
<marker>Borthwick, 1999</marker>
<rawString>Andrew Borthwick. 1999. A Maximum Entropy Approach to Named Entity Recognition. PhD Thesis. New York University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Crane</author>
<author>Alison Jones</author>
</authors>
<title>The Challenge of Virginia Banks: an Evaluation of Named Entity Analysis in a 19th-century Newspaper Collection.</title>
<date>2006</date>
<booktitle>ACM/IEEE Joint Conference on Digital Libraries, JCDL. Chapel Hill,</booktitle>
<pages>31--40</pages>
<location>NC, USA.</location>
<contexts>
<context position="6978" citStr="Crane and Jones (2006)" startWordPosition="1067" endWordPosition="1070">n accounts of trials that took place at the Old Bailey, the primary criminal court in London, between 1674 and 1834, was marked up for a number of semantic categories, including the crime date and location, the defendant’s gender, the victim’s name etc. Most of the work was done manually while support was provided for automatic person name3 identification (cf. Bontcheva et al., 2002). The author mentions future plans to take advantage of the structured nature of the Proceedings and to use the lists of persons, locations and occupations that have already been compiled for annotating new texts. Crane and Jones (2006) discuss the evaluation of the extraction of 10 named entity classes (personal names, locations, dates, products, organizations, streets, newspapers, ships, regiments and railroads) from a 19th century newspaper. The quality of their results vary for different entity types, from 99.3% precision for Streets to 57.5% precision for Products. The authors suggest the kinds of knowledge that digital libraries need to assemble as part of their machine readable reference collections in order to support entity identification as a core service, namely, the need for bigger authority lists, more refined r</context>
</contexts>
<marker>Crane, Jones, 2006</marker>
<rawString>Gregory Crane and Alison Jones. 2006. The Challenge of Virginia Banks: an Evaluation of Named Entity Analysis in a 19th-century Newspaper Collection. ACM/IEEE Joint Conference on Digital Libraries, JCDL. Chapel Hill, NC, USA. 31–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karina van Dalen-Oskam</author>
<author>Joris van Zundert</author>
</authors>
<title>Modelling Features of Characters: Some Digital Ways to Look at Names in Literary Texts.</title>
<date>2004</date>
<journal>Literary and Linguistic Computing</journal>
<volume>19</volume>
<issue>3</issue>
<pages>289--301</pages>
<marker>van Dalen-Oskam, van Zundert, 2004</marker>
<rawString>Karina van Dalen-Oskam and Joris van Zundert. 2004. Modelling Features of Characters: Some Digital Ways to Look at Names in Literary Texts. Literary and Linguistic Computing 19(3): 289–301.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Flanders</author>
<author>Syd Bauman</author>
<author>Paul Caton</author>
<author>Mavis Cournane</author>
</authors>
<date>1998</date>
<booktitle>Names Proper and Improper: Applying the TEI to the Classification of Proper Nouns. Computers and the Humanities</booktitle>
<volume>31</volume>
<issue>4</issue>
<pages>285--300</pages>
<contexts>
<context position="4987" citStr="Flanders et al. (1998" startWordPosition="756" endWordPosition="759"> annotation types, namely NER and entity annotation. Combined with suitable interfaces for displaying, searching, selecting, correlating and browsing named entities, we believe that the recognition and annotation of named entities in Litteraturbanken will facilitate more advanced research on literature (particularly in the field of literary onomastics; see Dalen-Oskam and Zundert, 2004), but also, e.g., historians could find this facility useful, insofar as these fictional narratives also contain, e.g. descriptions of real locations, characterizations of real contemporary public figures, etc. Flanders et al. (1998: 285) argue that references to people in historical sources are of intrinsic interest since they may reveal “networks of friendship, enmity, and collaboration; familial relationships; and political alliances [...] class position, intellectual affiliations, and literary bent of the author”. 3 Related Work The presented work is naturally related to research on NER, particularly as applied to diachronic/historical corpora. The technology itself has been applied to various domains and genres over the last couple of decades such as financial news and biomedicine, with performance rates difficult t</context>
</contexts>
<marker>Flanders, Bauman, Caton, Cournane, 1998</marker>
<rawString>Julia Flanders, Syd Bauman, Paul Caton and Mavis Cournane. 1998. Names Proper and Improper: Applying the TEI to the Classification of Proper Nouns. Computers and the Humanities 31(4): 285–300.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Fleischman</author>
<author>Eduard Hovy</author>
</authors>
<title>Fine Grained Classification of Named Entities.</title>
<date>2002</date>
<booktitle>Proceedings of the 19th International Conference on Computational Linguistics.</booktitle>
<location>Taipei,</location>
<contexts>
<context position="10368" citStr="Fleischman and Hovy, 2002" startWordPosition="1587" endWordPosition="1591">es, for instance by applying name similarity or by combining various annotation fragments. 4.1 Named-Entity Taxonomy The nature and type of named entities vary depending on the task under investigation or the target application. In any case, personal names, location and organization names are considered “generic”. Since semantic annotation is not as well understood as grammatical annotation, there is no consensus on a standard tagset and content to be generally applicable. Recently, however, there have been attempts to define and apply richer name hierarchies for various tasks, both specific (Fleischman and Hovy, 2002) and generic (Sekine, 2004). Our current system implements a rather finegrained named entity taxonomy with 8 main named entitiy types as well as 57 subtypes. Details can be found in Johannessen et al., 2005, and Kokkinakis, 2004. The eight main categories are: • Person (PRS): people names (forenames, surnames), groups of people, animal/pet names, mythological, theonyms; • Location (LOC): functional locations, geographical, geo-political, astrological; • Organization (ORG): political, athletic, media, military, etc.; • Artifact (OBJ): food/wine products, prizes, communic. means (vehicles) etc.;</context>
</contexts>
<marker>Fleischman, Hovy, 2002</marker>
<rawString>Michael Fleischman and Eduard Hovy. 2002. Fine Grained Classification of Named Entities. Proceedings of the 19th International Conference on Computational Linguistics. Taipei, Taiwan. 1–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janne Bondi Johannessen</author>
<author>Kristin Hagen</author>
</authors>
<title>Åsne Haaland, Andra Björk Jónsdottir, Anders Nøklestad, Dimitrios Kokkinakis, Paul Meurer, Eckhard Bick and Dorte Haltrup.</title>
<date>2005</date>
<volume>20</volume>
<issue>1</issue>
<pages>91--102</pages>
<marker>Johannessen, Hagen, 2005</marker>
<rawString>Janne Bondi Johannessen, Kristin Hagen, Åsne Haaland, Andra Björk Jónsdottir, Anders Nøklestad, Dimitrios Kokkinakis, Paul Meurer, Eckhard Bick and Dorte Haltrup. 2005. Named Entity Recognition for the Mainland Scandinavian Languages. Literary and Linguistic Computing. 20(1): 91–102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dimitrios Kokkinakis</author>
</authors>
<title>Reducing the Effect of Name Explosion. Proceedings of the LRECWorkshop: Beyond Named Entity Recognition - Semantic Labeling for NLP.</title>
<date>2004</date>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="10596" citStr="Kokkinakis, 2004" startWordPosition="1628" endWordPosition="1630">se, personal names, location and organization names are considered “generic”. Since semantic annotation is not as well understood as grammatical annotation, there is no consensus on a standard tagset and content to be generally applicable. Recently, however, there have been attempts to define and apply richer name hierarchies for various tasks, both specific (Fleischman and Hovy, 2002) and generic (Sekine, 2004). Our current system implements a rather finegrained named entity taxonomy with 8 main named entitiy types as well as 57 subtypes. Details can be found in Johannessen et al., 2005, and Kokkinakis, 2004. The eight main categories are: • Person (PRS): people names (forenames, surnames), groups of people, animal/pet names, mythological, theonyms; • Location (LOC): functional locations, geographical, geo-political, astrological; • Organization (ORG): political, athletic, media, military, etc.; • Artifact (OBJ): food/wine products, prizes, communic. means (vehicles) etc.; • Work&amp;Art (WRK): printed material, names of films and novels, sculptures etc.; • Event (EVN): religious, athletic, scientific, cultural etc.; • Measure/Numerical (MSR): volume, age, index, dosage, web-related, speed etc.; • Te</context>
</contexts>
<marker>Kokkinakis, 2004</marker>
<rawString>Dimitrios Kokkinakis. 2004. Reducing the Effect of Name Explosion. Proceedings of the LRECWorkshop: Beyond Named Entity Recognition - Semantic Labeling for NLP. Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vijay Krishnan</author>
<author>Christopher D Manning</author>
</authors>
<title>An Efficient Two-Stage Model for Exploiting Non-Local Dependencies in Named Entity Recognition.</title>
<date>2006</date>
<booktitle>Proceedings of COLING/ ACL 2006.</booktitle>
<location>Sydney,</location>
<contexts>
<context position="19106" citStr="Krishnan and Manning, 2006" startWordPosition="2950" endWordPosition="2953"> unambiguously introduced at least once in the text unless they are part of common knowledge presupposed to be known by the readers. This implies a form of online learning from the document being processed where unambiguous usages are used for assigning annotations to ambiguous words, and information for disambiguation is derived from the entire document. Similarly, label consistency, the preference of the same annotation for the same word sequence everywhere in a particular discourse, is a comparable approach for achieving qualitatively higher recall rates with minimal resource overhead (cf. Krishnan and Manning, 2006). Such an approach has been used, e.g., by Aramaki et al. (2006), for the identification of personal health information (age, id, date, phone, location and doctor´s and patient´s names). Figure 1. Example of label consistency Figure 1 illustrates this approach with an example taken from Almqvist’s Collected Works, Vol. 30. In this example, the first occurrence of the female person name Micmac, which is not in the gazetteer lists, is introduced by the author with the unambiguous designator faster ‘aunt’. Many of the subsequent mentions of the same name are given without any reliable clue for ap</context>
</contexts>
<marker>Krishnan, Manning, 2006</marker>
<rawString>Vijay Krishnan and Christopher D. Manning. 2006. An Efficient Two-Stage Model for Exploiting Non-Local Dependencies in Named Entity Recognition. Proceedings of COLING/ ACL 2006. Sydney, Australia. 1121–1128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D McDonald</author>
</authors>
<title>Internal and External Evidence in the Identification and Semantic Categorisation of Proper Nouns. Corpus-Processing for Lexical Acquisition. James Pustejovsky and Bran Boguraev (eds).</title>
<date>1996</date>
<pages>21--39</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="18090" citStr="McDonald (1996)" startWordPosition="2791" endWordPosition="2792">ly obtain better results, but at the cost of considerable manual effort by domain experts. Statistical NER systems typically require a large amount of manually annotated training data, but can be ported to other domains or genres more rapidly and require less manual work. Although the Swedish system is mainly rule-based, using a handcrafted grammar for each entity group, it can also be considered a hybrid system in the sense that it applies a document-centered approach (DCA) to entity annotation, which is a different paradigm compared to the local context approach, called external evidence by McDonald (1996). With DCA, information for the disambiguation of a name is derived from the entire document. DCA as a term originates from the work by Mikheev (2000: 138), who claims that: important words are typically used in a document more than once and in different contexts. Some of these contexts create very ambiguous situations but some don’t. Furthermore, ambiguous words and phrases are usually unambiguously introduced at least once in the text unless they are part of common knowledge presupposed to be known by the readers. This implies a form of online learning from the document being processed where</context>
</contexts>
<marker>McDonald, 1996</marker>
<rawString>David D. McDonald. 1996. Internal and External Evidence in the Identification and Semantic Categorisation of Proper Nouns. Corpus-Processing for Lexical Acquisition. James Pustejovsky and Bran Boguraev (eds). MIT Press. 21–39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrei Mikheev</author>
</authors>
<title>Document Centered Approach to Text Normalization.</title>
<date>2000</date>
<booktitle>Proceedings of the 23rd ACM SIGIR Conference on Research and Development in Information Retrieval.</booktitle>
<pages>136--143</pages>
<location>Athens,</location>
<contexts>
<context position="18239" citStr="Mikheev (2000" startWordPosition="2817" endWordPosition="2818">manually annotated training data, but can be ported to other domains or genres more rapidly and require less manual work. Although the Swedish system is mainly rule-based, using a handcrafted grammar for each entity group, it can also be considered a hybrid system in the sense that it applies a document-centered approach (DCA) to entity annotation, which is a different paradigm compared to the local context approach, called external evidence by McDonald (1996). With DCA, information for the disambiguation of a name is derived from the entire document. DCA as a term originates from the work by Mikheev (2000: 138), who claims that: important words are typically used in a document more than once and in different contexts. Some of these contexts create very ambiguous situations but some don’t. Furthermore, ambiguous words and phrases are usually unambiguously introduced at least once in the text unless they are part of common knowledge presupposed to be known by the readers. This implies a form of online learning from the document being processed where unambiguous usages are used for assigning annotations to ambiguous words, and information for disambiguation is derived from the entire document. Si</context>
</contexts>
<marker>Mikheev, 2000</marker>
<rawString>Andrei Mikheev. 2000. Document Centered Approach to Text Normalization. Proceedings of the 23rd ACM SIGIR Conference on Research and Development in Information Retrieval. Athens, Greece. 136–143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Sekine</author>
</authors>
<title>Definition, Dictionaries and Tagger for Extended Named Entity Hierarchy.</title>
<date>2004</date>
<booktitle>Proceedings of the Language Resources and Evaluation Conference (LREC).</booktitle>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="10395" citStr="Sekine, 2004" startWordPosition="1594" endWordPosition="1595">rity or by combining various annotation fragments. 4.1 Named-Entity Taxonomy The nature and type of named entities vary depending on the task under investigation or the target application. In any case, personal names, location and organization names are considered “generic”. Since semantic annotation is not as well understood as grammatical annotation, there is no consensus on a standard tagset and content to be generally applicable. Recently, however, there have been attempts to define and apply richer name hierarchies for various tasks, both specific (Fleischman and Hovy, 2002) and generic (Sekine, 2004). Our current system implements a rather finegrained named entity taxonomy with 8 main named entitiy types as well as 57 subtypes. Details can be found in Johannessen et al., 2005, and Kokkinakis, 2004. The eight main categories are: • Person (PRS): people names (forenames, surnames), groups of people, animal/pet names, mythological, theonyms; • Location (LOC): functional locations, geographical, geo-political, astrological; • Organization (ORG): political, athletic, media, military, etc.; • Artifact (OBJ): food/wine products, prizes, communic. means (vehicles) etc.; • Work&amp;Art (WRK): printed </context>
</contexts>
<marker>Sekine, 2004</marker>
<rawString>Satoshi Sekine. 2004. Definition, Dictionaries and Tagger for Extended Named Entity Hierarchy. Proceedings of the Language Resources and Evaluation Conference (LREC). Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Constantin Orasan</author>
<author>Roger Evans</author>
</authors>
<title>Learning to Identify Animate References.</title>
<date>2001</date>
<booktitle>Proceedings of the Workshop on Computational Natural Language Learning (CoNLL-2001). ACL-2001.</booktitle>
<location>Toulouse, France.</location>
<contexts>
<context position="12461" citStr="Orasan and Evans, 2001" startWordPosition="1912" endWordPosition="1915">subject (e.g. berätta ‘to tell’, fundera ‘to think’, tröttna ‘to become tired’). These are used in conjunction with orthographic markers in the text, such as capitalization, for the recognition of personal names. In this work, we consider the first group (designators) as relevant knowledge to be extracted from the person name recognizer, which is explored for the annotation of animate instances 3 in the literary texts. The designators are implemented as a separate module in the current pipeline, and constitute a piece of information which is considered important for a wide range of tasks (cf. Orasan and Evans, 2001). The designators are divided into four groups: designators that denote the nationality or the ethnic/racial group of a person (e.g. tysken ‘the German [person]’); designators that denote a profession (e.g. läkaren ‘the doctor’); those that denote family ties and relationships (e.g. svärson ‘son in law’); and finally a group that indicates a human individual but cannot be unambiguously categorized into any of the three other groups (e.g. patienten ‘the patient’). Apart from this grouping, inherent qualities, for at least a large group of the designators, (internal evidence/morphological cues) </context>
</contexts>
<marker>Orasan, Evans, 2001</marker>
<rawString>Constantin Orasan and Roger Evans. 2001. Learning to Identify Animate References. Proceedings of the Workshop on Computational Natural Language Learning (CoNLL-2001). ACL-2001. Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Shoemaker</author>
</authors>
<title>Digital London. Creating a Searchable Web of Interlinked Sources</title>
<date>2005</date>
<booktitle>on Eighteenth Century London. Program: Electronic Library &amp; Information Systems</booktitle>
<volume>39</volume>
<issue>4</issue>
<pages>297--311</pages>
<contexts>
<context position="6300" citStr="Shoemaker (2005)" startWordPosition="955" endWordPosition="956">cise overview of the technology see Borthwick, 2 This precluded the use of ready-made digital library or CMS solutions, as we wanted to be compatible with emerging standards for language resources and tools, e.g. TEI/(X)CES and ISO TC37/SC07, which to our knowledge has never been a consideration in the design of digital library or CM systems. (1999). Even though this technology is widely used in a number of domains, studies dealing with historical corpora are mostly comparatively recent (see for instance the recent workshop on historical text mining; &lt;http://ucrel.lancs.ac.uk/events/htm06/&gt;). Shoemaker (2005) reports on how the Old Bailey Proceedings, which contain accounts of trials that took place at the Old Bailey, the primary criminal court in London, between 1674 and 1834, was marked up for a number of semantic categories, including the crime date and location, the defendant’s gender, the victim’s name etc. Most of the work was done manually while support was provided for automatic person name3 identification (cf. Bontcheva et al., 2002). The author mentions future plans to take advantage of the structured nature of the Proceedings and to use the lists of persons, locations and occupations th</context>
</contexts>
<marker>Shoemaker, 2005</marker>
<rawString>Robert Shoemaker. 2005. Digital London. Creating a Searchable Web of Interlinked Sources on Eighteenth Century London. Program: Electronic Library &amp; Information Systems 39(4): 297–311.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>