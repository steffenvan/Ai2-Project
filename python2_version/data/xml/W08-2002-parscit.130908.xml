<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000133">
<title confidence="0.99431">
Learning to Map Text to Graph-based Meaning Representations via
Grammar Induction
</title>
<author confidence="0.994027">
Smaranda Muresan
</author>
<affiliation confidence="0.99505">
Laboratory for Computational Linguistics and Information Processing
Institute for Advanced Computer Studies
University of Maryland
</affiliation>
<address confidence="0.932121">
College Park, MD 20742, USA
</address>
<email confidence="0.999677">
smara@umiacs.umd.edu
</email>
<sectionHeader confidence="0.993919" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999933944444445">
We argue in favor of using a graph-based
representation for language meaning and
propose a novel learning method to map
natural language text to its graph-based
meaning representation. We present a
grammar formalism, which combines syn-
tax and semantics, and has ontology con-
straints at the rule level. These constraints
establish links between language expres-
sions and the entities they refer to in the
real world. We present a relational learning
algorithm that learns these grammars from
a small representative set of annotated ex-
amples, and show how this grammar in-
duction framework and the ontology-based
semantic representation allow us to di-
rectly map text to graph-based meaning
representations.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9995110625">
Recent work (Wong and Mooney, 2007; Zettle-
moyer and Collins, 2005; He and Young, 2006)
has developed learning algorithms for the problem
of mapping sentences to their underlying semantic
representations. These semantic representations
vary from λ-expressions (Bos et al., 2004; Zettle-
moyer and Collins, 2005; Wong and Mooney,
2007) to DB query languages and command-like
languages (RoboCup Coach Language, CLang)
(Ge and Mooney, 2005).
In this paper we focus on an ontology-based
semantic representation which allows us to en-
code the meaning of a text as a direct acyclic
graph. Recently, there is a growing interest
on ontology-based NLP, starting from efforts in
defining ontology-based semantic representations
</bodyText>
<footnote confidence="0.72979875">
© 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
</footnote>
<bodyText confidence="0.978416666666667">
(Nirenburg and Raskin, 2004), to using ontologi-
cal resources in NLP applications, such as ques-
tion answering (Basili et al., 2004; Beale et al.,
2004), and building annotated corpora, such as the
OntoNotes project (Hovy et al., 2006).
There are three novel properties to ontology-
based semantics that we propose in this paper:
• There is a direct link between the ontology
and the grammar through constraints at the
grammar rule level. These ontology con-
straints enable access to meaning during lan-
guage processing (parsing and generation).
</bodyText>
<listItem confidence="0.607771">
• Our ontology-based semantic representation
</listItem>
<bodyText confidence="0.966586961538461">
is expressive enough to capture various phe-
nomena of natural language, yet restric-
tive enough to facilitate grammar learning.
The representation encodes both ontological
meaning (concepts and relations among con-
cepts) and extra-ontological meaning, such as
voice, tense, aspect, modality.
• Our representation and grammar learning
framework allow a direct mapping of text to
its meaning, encoded as a direct acyclic graph
(DAG). We consider that “understanding” a
text is the ability to correctly answer, at the
conceptual level, all the questions asked w.r.t
to that text, and thus Meaning = Text + all
Questions/Answers w.r.t that Text. Under this
assumption, obtaining the meaning of a text
is reduced to a question answering process,
which in our framework is a DAG matching
problem.
First, we review our grammar formalism intro-
duced in (Muresan, 2006; Muresan and Rambow,
2007), called Lexicalized Well-Founded Gram-
mars. Second, we present a relational learning al-
gorithm for inducing these grammars from a rep-
resentative sample of strings annotated with their
semantics, along with minimal assumptions about
</bodyText>
<page confidence="0.954451">
9
</page>
<note confidence="0.91929">
Coling 2008: Proceedings of 3rd Textgraphs workshop on Graph-Based Algorithms in Natural Language Processing, pages 9–16
</note>
<figure confidence="0.945404783783784">
Manchester, August 2008
I. Semantic Molecules
D
b
1
C C C C C C
D E A C
X1.isa = major, X2.Y=X1
b. (damage/noun)&apos;= 0
B B B B B B B @
2 3
cat noun
6 7
4nr sg 5
head X3
h2
b2DX3.isa = damageE
c. (major damage)&apos;= 0
B B B B B B @
1
C C C C C
E A C
X1.isa = major, X.Y=X1, X.isa=damage
a. (major/adj)&apos;= 0 h1
B B B B B B B @ b1
2 cat adj
6 4head X1
mod X2
3
57
2 3
cat n
6 7
4nr sg 5
head X
h
II. Constraint Grammar Rule
</figure>
<equation confidence="0.911286666666667">
N(w, “b”) → Adj(w1, “bl”), N(w2, “b2”): Φc(h, h1, h2), Φo(b)
Φc(h, h1, h2) = {h.cat = n, h.head = h1.mod, h.head = h2.head, h.nr = h2.nr, h1.cat = adj, h2.cat = n}
Φo(b) returns (X1.isa = major, X.degree = X1, X.isa = damage)
</equation>
<figureCaption confidence="0.968485">
Figure 1: Examples of three semantic molecules (I), and a constraint grammar rule together with the
semantic composition and ontology-based interpretation constraints, Φc and Φo (II)
</figureCaption>
<bodyText confidence="0.999747111111111">
syntax. Then, we describe the levels of represen-
tation we use to go from utterances to their graph-
based meaning representations, and show how our
representation is suitable to define the meaning of
an utterance/text through answers to questions. As
a proof of concept we discuss how our framework
can be used to acquire terminological knowledge
from natural language definitions and to query this
knowledge using wh-questions.
</bodyText>
<sectionHeader confidence="0.967751" genericHeader="method">
2 Grammar Formalism
</sectionHeader>
<bodyText confidence="0.991700261538462">
Lexicalized Well-Founded Grammars (LWFGs)
introduced in (Muresan, 2006; Muresan and Ram-
bow, 2007) are a type of Definite Clause Gram-
mars (Pereira and Warren, 1980) where: (1) the
context-free backbone is extended by introducing
a partial ordering relation among nonterminals (the
basis for “well-founded”); (2) each string is as-
sociated with a syntactic-semantic representation
called a semantic molecule; and (3) grammar rules
have two types of constraints: one for semantic
composition and one for ontology-based semantic
interpretation. The last two properties allow us to
have a syntactic-semantic grammar. The ontology
constraints provide access to meaning during lan-
guage learning, parsing and generation. The first
property allows us to learn these grammars from a
small set of annotated examples.
The semantic molecule is a syntactic-semantic
representation of natural language strings w&apos; =
(b), where h (head) encodes the information re-
quired for semantic composition, and b (body) is
the actual semantic representation of the string.
Figure 1 gives examples of semantic molecules for
an adjective, a noun and a noun phrase, as pre-
sented in (Muresan and Rambow, 2007).
The head h of the semantic molecule is a flat
feature structure (i.e., feature values are atomic),
having at least two attributes that encode the syn-
tactic category of the associated string, cat, and
the head of the string, head. In addition, attributes
for agreement and other grammatical features can
be present (e.g., nr, pers for number and person).
The set of attributes is finite and known a-priori for
each syntactic category. Being a one-level feature
structure, no recursive or embedded structures are
allowed (unlike other grammar formalisms such as
HPSG, LFG), which makes this representation ap-
pealing for a learning framework. Recursion in the
grammar is obtained through the recursive gram-
mar rules and the composition constraint.
The body, b, of a semantic molecule is a flat rep-
resentation, called OntoSeR (Ontology-based Se-
mantic Representation). No embedding of pred-
icates is allowed, as in Minimal Recursion Se-
mantics (MRS) (Copestake et al., 1999). Unlike
MRS, OntoSeR is a logical form built as a con-
junction of atomic predicates (concept).(attr) =
(concept), where variables are either concept or
slot (attr) identifiers in an ontology. For example,
the adjective major is represented as (X1.isa =
major, X2.Y = X1), which says that the meaning
of an adjective is a concept X1 (X1.isa = major)
that is the value of a property of another concept
X2 (X2.Y = X1) in the ontology.
A LWFG specifies one or more semantic
molecules for each string that can be parsed by
the grammar. The lexicon of a LWFG consists of
words paired with their semantic molecules shown
in Figure 1(Ia and Ib). In addition to the lexicon, a
LWFG has a set of constraint grammar rules. An
example of a LWFG rule is given in Figure 1(II).
Grammar nonterminals are augmented with pairs
of strings and their semantic molecules. These
pairs are called syntagmas, and are denoted by
Q = (w, w&apos;) = (w, (h )). This rule generates the
</bodyText>
<subsectionHeader confidence="0.333378">
b
</subsectionHeader>
<bodyText confidence="0.99979525">
syntagma corresponding to major damage whose
semantic molecule is given in Figure 1(Ic). There
are two types of constraints at the grammar rule
level — one for semantic composition (how the
</bodyText>
<page confidence="0.99695">
10
</page>
<bodyText confidence="0.999736081081081">
meaning of a natural language expression is com-
posed from the meaning of its parts) and one for
ontology-based semantic interpretation. The com-
position constraints Φ, are applied to the heads of
the semantic molecules, the bodies being just con-
catenated. Figure 1 shows that the body of the se-
mantic molecule for major damage is a concate-
nation of the bodies of the adjective major and
noun damage, together with a variable substitu-
tion. This variable substitution {X2/X, X3/XI is
a result of Φ,, which is a system of equations —
a simplified version of “path equations” (Shieber
et al., 1983), because the heads are flat feature
structures. These constraints are learned together
with the grammar rules. The ontology-based con-
straints Φ� represent the validation on the ontol-
ogy, and are applied to the body of the semantic
molecule associated with the left-hand side non-
terminal. The ontology-based interpretation is not
done during the composition operation, but after-
words. Thus, for example, the head of the noun
phrase major damage does not need to store the
slot Y , a fact that allows us to use flat feature
structures to represent the head of the semantic
molecules. The ontology-based constraints are not
learned; rather, Φ� is a general predicate applied
to the logical form semantic representation which
fully contains all the required information needed
for validation on the ontology. Thus, it is indepen-
dent of grammatical categories. This predicate can
succeed or fail as a result of querying the ontology
— when it succeeds, it instantiates the variables of
the semantic representation with concepts/slots in
the ontology (Y = degree). For example, given
the phrase major damage, Φ� succeeds and returns
(X1.isa = major, X.degree = X1, X.isa =
damage), while given major birth it fails.
</bodyText>
<sectionHeader confidence="0.98035" genericHeader="method">
3 Grammar Learning Algorithm
</sectionHeader>
<bodyText confidence="0.999166888888889">
Unlike stochastic grammar learning for syntac-
tic parsing (e.g., (Collins, 1999)), LWFG is well
suited to learning from reduced-size training data.
Furthermore, unlike previous formalisms used for
deeper representations (e.g, HPSG, LFG), our
LWFG formalism is characterized by a formal
guarantee of polynomial learnability (Muresan,
2006).
A key to these properties is the partial order-
ing among grammar nonterminals, i.e., the set of
nonterminals is well-founded. This partial order-
ing among nonterminals allows us to define the
representative examples of a LWFG, and to learn
LWFGs from this small set of examples. The rep-
resentative examples ER of a LWFG, G, are the
simplest syntagmas ground-derived by the gram-
mar G — i.e., for each grammar rule, there ex-
ists a syntagma which is ground-derived from it in
the minimum number of steps. Informally, repre-
sentative examples are building blocks from which
larger structures can be inferred via reference to a
larger corpus EQ which can be only weakly anno-
tated (i.e., bracketed), or unannotated. This larger
corpus, EQ, is used for generalization during learn-
ing (Figure 2).
The theoretical learning model is Grammar
Approximation by Representative Sublanguage
(GARS) introduced in (Muresan, 2006; Muresan
and Rambow, 2007). We proved that the search
space for grammar induction is a complete gram-
mar lattice, and we gave a learnability theorem for
LWFG induction. The GARS model uses a poly-
nomial algorithm for LWFG learning that takes
advantage of the building blocks nature of repre-
sentative examples. The learning algorithm be-
longs to the class of Inductive Logic Programming
methods (ILP), based on entailment (Muggleton,
1995; Dzeroski, 2007). Unlike existing ILP meth-
ods that use randomly-selected examples, our al-
gorithm learns from a set of representative exam-
ples allowing a polynomial efficiency for learn-
ing a syntactico-semantic constraint-based gram-
mar, suitable to capture large fragments of natural
language (Muresan, 2006).
The LWFG induction algorithm is a cover set al-
gorithm, where at each step a new constraint gram-
mar rule is learned from the current representative
example, Q E ER. Then this rule is added to the
grammar rule set. The process continues until all
the representative examples are covered. We de-
scribe below the process of learning a grammar
rule from the current representative example, illus-
trated as well in Figure 2.
Step 1. In the first step, the most specific gram-
mar rule is generated from the current represen-
tative example. The category name annotated
in the representative example gives the name of
the left-hand-side nonterminal (“predicate inven-
tion”, in ILP terminology), while a robust parser
returns the minimum number of chunks cover-
ing the representative example. The categories
of the chunks give the nonterminals of the right-
hand side of the most specific rule. For ex-
</bodyText>
<page confidence="0.997766">
11
</page>
<figureCaption confidence="0.939714">
Figure 2: An iteration step of the learning algorithm
</figureCaption>
<figure confidence="0.92871965625">
Adj (major,
Noun
(damage,
CURRENT REPRESENTATIVE EXAMPLE
cat n
nr sg
(major damage,
head X
)
&lt;X1.isa=major, X.Y=X1, X.isa=X1&gt;
STEP 1 (ROBUST PARSING)
MOST SPECIFIC CONSTRAINT GRAMMAR RULE
chunks={[Adj(major), A(major)],[Noun(damage), N(damage)]}
N Adj Noun:
Φc4
Φc4=
{h.cat=n, h.head=h1.mod, h.head=h2.head,
h.nr=h2.nr, h1.cat=adj, h2.cat=noun }
STEP 2 (RULE GENERALIZATION)
CANDIDATE GRAMMAR RULES
r1
r2
r3
N
N
N
A N:
Adj Noun:
A Noun:
Φc6
Φc5
Φc4(score=1)
(score=2)
(score=3)
BEST RULE
N A N:
Φc6
Performance Criteria
REPRESENTATIVE SUBLANGUAGE
Eσ
major damage
very beautiful painting
loud clear noise
BACKGROUND KNOWLEDGE
cat adj
head X1
mod X2
)
&lt;X1.isa=major, X2.Y=X1&gt;
cat noun
nr sg
head X3
&lt;X3.isa=damage&gt;
)
A
A
N
Adj:
Φc1
Adv A:
Noun:
Φc2
Φc3
N
</figure>
<bodyText confidence="0.999048848484849">
ample, in Figure 2, given the representative ex-
ample major damage annotated with its seman-
tic molecule, and the background knowledge con-
taining the already learned rules A —* Adj
and N —* Noun,&apos; the robust parser generates
the chunks corresponding to the adjective major
and the noun damage: [Adj(major),A(major)] and
[Noun(damage),N(damage)], respectively. The
most specific rule generated is thus N —*
Adj Noun: Φc4, where the left hand side nonter-
minal is given by the category of the representative
example, in this case n. The compositional con-
straints Φc4 are learned as well. It can be seen that
the annotation of the representative example does
not require us to provide ontology-specific roles or
concepts. Thus, grammar learning is general, and
can be done using a small, generic lexicon.
Step 2. In the second step, this most specific rule is
generalized, obtaining a set of candidate grammar
rules. The performance criterion in choosing the
best grammar rule among these candidate hypothe-
ses is the number of the examples in the represen-
tative sublanguage EQ (generalization corpus) that
can be parsed using the candidate grammar rule to-
gether with the previous learned rules. In Figure
2 given the representative sublanguage EQ={ ma-
jor damage, loud clear noise, very beautiful paint-
ing} the learner will generalize to the recursive
rule N —* A N : Φ6, since only this rule can parse
&apos;For readability, we only show the context-free backbone
of the grammar rules, and Φo are not discussed since they are
not learned.
all the examples in EQ.
</bodyText>
<sectionHeader confidence="0.894577" genericHeader="method">
4 Levels of Representation
</sectionHeader>
<bodyText confidence="0.999092714285714">
In order to transform natural language utterances
to knowledge, we consider three levels of repre-
sentation: the utterance level, the text level and the
ontology level. In Section 4.4 we show that these
levels of representation allow us to define meaning
as Meaning=Text+all Questions/Answers w.r.t that
Text, using a DAG matching approach.
</bodyText>
<subsectionHeader confidence="0.983234">
4.1 Utterance-level Representation
</subsectionHeader>
<bodyText confidence="0.9998769">
At the utterance level, the semantic representation
corresponds directly to a syntagma Q after the on-
tology constraint Φo is applied. This representa-
tion is called Ontology-based Semantic Represen-
tation OntoSeR. At this level, the attrIDs are in-
stantiated with values of the slots from the ontol-
ogy, while the conceptIDs remain variables to al-
low further composition to take place. At OntoSeR
level we can exploit the reversibility of the gram-
mar, since this representation is used during pars-
ing/generation.
In Figure 3 we show the semantic represen-
tation OntoSeR for the utterance Hepatitis B is
an acute viral hepatitis caused by a virus that
tends to persist in the blood serum, obtained using
our parser in conjunction with our learned gram-
mar. The composition constraints bind the con-
ceptID variables, while the ontology constraint in-
stantiates the attrID variables with values of slots
in the ontology. The ontology constraint can be
</bodyText>
<page confidence="0.931813">
12
</page>
<equation confidence="0.59323">
Hepatitis B is an acute viral hepatitis caused by a virus that tends to persist in the blood serum.
OntoSeR = ((A.name=hepatitisB)HepatitisB, (A.tense=pr)is, (A.det=an)an, (B.is a=acute, A.duration=B)acute,
</equation>
<table confidence="0.969000166666667">
(C.is a=viral, A.kind of=C)viral, (A.is a=hepatitis)hepatitis, (D.vft=ed, D.voice=pas, D.is a=cause, D.ag=E,
D.th=A)caused, (ag.is a=by, D.ag=E)by, (E.det=a)a, (E.is a=virus)virus, (E.is a=that)that, (F.tense=pr, F.is a=tend,
F.no ag=E, F.prop=G)tends, (G.vft=to, G.is a=persist, G.th=E)to persist, (loc.is a=in, G.loc=H)in, (H.det=the)the,
(I.is a=blood, H.of=I)blood, (H.is a=serum)serum )
OKR
TKR
#tend34
˜29.name= hepatitisB ˜33.det= virus
˜29.tense= pr ˜33.is_a= that #hepatitis prop
˜20.det= an ˜34.tense= pr
˜30.isacute ˜34.is_a= tend
_a=
#cause32 #persist35
˜29.duration=˜2 ˜34.no_role=˜33 sub
˜31.is_a= viral ˜34.prop=˜35
th ag th loc
˜29.kind_of=˜3 ˜35.vft= to
˜29.is_a= hepatitis ˜35.is_a= persist
˜32.vft= ˜35.th #hepatitisB #virus33 #serum36
ed =˜33
˜32.voice= pas loc.is_a= in
˜32.iscause ˜35.loc=˜36
_a=
duration of
˜32.ag=˜5 ˜36.det= the kind_of
˜32.th=˜1 ˜37.is_a= blood
ag.is_a= by ˜36.of=˜37 #acute #viral #blood
˜32.ag=˜33 ˜36.isserum
_a=
˜33.det= a
</table>
<figureCaption confidence="0.998804">
Figure 3: Example of an utterance and its levels of representation
</figureCaption>
<bodyText confidence="0.998697413793104">
seen as a local semantic interpretation at the ut-
terance/grammar rule level, providing access to
meaning during parsing/generation. In this pa-
per, this semantic interpretation is based only on
a weak “ontological model”. For the verb the-
matic roles we considered the thematic roles de-
rived from Dorr’s LCS Database (e.g., ag=agent,
th=theme, prop=proposition) (Dorr, 1997). For
adjectives and adverbs we took the roles (prop-
erties) from WordNet (Miller, 1990). For prepo-
sitions we considered the LCS Database. We
also have manually added specific/dummy seman-
tic roles when they were not present in these re-
sources (e.g., of between blood and serum).
The example in Figure 3 shows the output of
our parser in conjunction with the learned gram-
mar for a definitional sentence that contains several
linguistic phenomena such as copula to-be predica-
tive, reduced relative clauses (caused by ...), rel-
ative clauses (virus that ...), raising construction
(tends to persist, where virus is not the argument
of tends but the argument of persist), and noun
compounds (blood serum). For readability, we in-
dicate what part of OntoSeR corresponds to each
lexical item. It can be noticed that OntoSeR con-
tains representations of both ontological meaning
(concepts and relations among concepts) as well as
extra-ontological meaning such as tense and voice
(D.voice = pas; F.tense = pr).
</bodyText>
<subsectionHeader confidence="0.978673">
4.2 Text-level Representation
</subsectionHeader>
<bodyText confidence="0.999943555555556">
The text-level representation TKR, or discourse
level representation, represents asserted represen-
tations. ConceptIDs become constants, and no
composition can happen at this level. However, we
still have (indirect) reversibility, since TKR repre-
sents all the asserted OntoSeRs. Therefore, all the
information needed for reversibility is still present.
Figure 3 shows an example of the TKR for the
above utterance.
</bodyText>
<subsectionHeader confidence="0.995185">
4.3 Ontology-level Representation
</subsectionHeader>
<bodyText confidence="0.999964722222222">
Ontology-level knowledge representation OKR is
obtained after task-specific interpretation, which
can be seen as a global semantic interpretation.
OKR is a directed acyclic graph (DAG) G =
(V, E). Edges, E, are either semantic roles given
by verbs, prepositions, adjectives and adverbs,
or extra-ontological meaning properties, such as
tense, aspect, modality, negation. Vertices, V are
either concepts (corresponding to nouns, verbs,
adjectives, adverbs, pronouns, cf. Quine’s crite-
rion (Sowa, 1999, page 496)), or values of the
extra-ontological properties such as present cor-
responding to tense property. In this paper, the
task-specific interpretation is geared mainly to-
wards terminological interpretation. We filter from
OntoSeR determiners and some verb forms, such
as tense, aspect, since temporal relations appear
less in terminological knowledge than in factual
</bodyText>
<page confidence="0.997953">
13
</page>
<bodyText confidence="0.999589185185185">
knowledge. However, we treat modals and nega-
tion, as they are relevant for terminological knowl-
edge. An example of OKR for the above utterance
is given in Figure 3.
We consider both concepts (e.g., #acute,
#blood), and instances of concepts (e.g., #virus33,
#cause32). Concepts are denoted in OKR by
#name concept, and they form a hierarchy of con-
cepts based on the subsume relation (sub), which
is the inverse of the is a relation. An instance of
a concept is denoted by the name of a concept fol-
lowed by the instance number (e.g., #virus33). A
concept and an instance of this concept are two dif-
ferent vertices in OKR, having the same name. At
the OKR level we assume the principle of concept
identity which means that there is a bijection be-
tween a vertex in OKR and a referent. For exam-
ple, if we do not have pronoun resolution, the pro-
noun and the noun it refers to will be represented
as two separate vertices in the graph. Currently,
our semantic interpreter implements only a weak
concept identity principle which facilitates struc-
ture sharing and inheritance.
To give these two properties we first introduce
some notations. A DAG is called rooted at a vertex
u ∈ V , if there exists a path from u to each vertex
of the DAG. We have the following definition:
</bodyText>
<construct confidence="0.875677444444445">
Definition 1. Two subDAGs rooted at two vertices
u, u&apos; are equal if the set of the adjacent vertices to
u and u&apos; respectively, are equal and if the edges in-
cident from u and u&apos; have the same semantic roles
as labels.
Property 1 (Structure Sharing). In an OKR, all
vertices u, u&apos; ∈ V with the same name, and whose
subDAGs are equal are identical (i.e., the same
vertex in OKR).
</construct>
<bodyText confidence="0.989720379310345">
Using a hash table, there is a linear algorithm
O(|V  |+ |E|) which transforms an OKR to an
equivalent OKR which satisfies Property 1. In Fig-
ure 4 it can be seen that the OKRs of Hepatitis
A and Hepatitis B share the representation corre-
sponding to blood serum (i.e., blood serum is the
same concept instance and due to Property 1 we
have that #serum36=#serum27 and thus they have
the same vertex in the OKR).
Property 2 (Inheritance). A concept in a hierarchy
of concepts can be linked by the sub relation only
to its parent(s), and not to any other ancestors. A
subDAG defining a property of a concept from the
hierarchy of concepts can be found only once in
the OKR at the level of the most general concept
that has this property.
For terminological knowledge we have that any
instance of a concept is a concept, and the defi-
nition is the naming of a concept instance. For
example, the definition of Hepatitis B, is an in-
stance of a concept #hepatitis which has additional
attributes acute, viral and caused by a virus that
tends to persist in the blood serum. Thus, an
additional instance of concept #hepatitis is cre-
ated, which is named #hepatitisB. The fact that
we can have the definition as a naming of a con-
cept instance is facilitated also by our treatment
of copula to-be at the OntoSeR level (A.name =
hepatitisB, ... , A.is a = hepatitis in Figure 3)
</bodyText>
<subsectionHeader confidence="0.999866">
4.4 Meaning as Answers to Questions
</subsectionHeader>
<bodyText confidence="0.996257153846154">
We consider that “understanding” a text is the abil-
ity to correctly answer, at the conceptual level,
all the questions asked w.r.t to that text, and thus
Meaning = Text + all Questions/Answers w.r.t that
Text. In our framework we consider the principle
of natural language as problem formulation, and
not problem solving. Thus, we can represent at
OKR level a paradox formulation in natural lan-
guage, even if the reasoning about its solution can-
not be emphasized. Our levels of representations
allow us to define the meaning of questions, an-
swers and utterances using a DAG matching ap-
proach.
</bodyText>
<construct confidence="0.999228714285714">
Definition 2. The meaning of a question, q, with
respect to an utterance/discourse, is the set of all
answers that can be directly obtained from that ut-
terance/discourse. The semantic representation of
a question is a subgraph of the utterance graph
where the wh-word substitutes the answer con-
cept(s).
Definition 3. The answer to a question is the con-
cept that matches the wh-word through the DAG
matching algorithm between the question’s sub-
DAG and the utterance/discourse DAG.
Definition 4. The meaning of an utterance u is the
set of all questions that can be asked w.r.t that ut-
terance, together with their answers.
</construct>
<bodyText confidence="0.996823666666667">
Unlike meaning as truth conditions, where the
problem of meaning equivalence is reduced to
logical form equivalence, in our case meaning
equivalence is reduced to semantic equivalence of
DAGs/subDAGs which obey the concept identity
principle (weak, or strong). The matching algo-
</bodyText>
<page confidence="0.997767">
14
</page>
<bodyText confidence="0.9978252">
rithm obtains the same answers to questions, rela-
tive to semantic equivalent DAGs. If we consider
only the weak concept identity principle given by
Properties 1 and 2, the problem is reduced to
DAG/subDAG identity.
</bodyText>
<sectionHeader confidence="0.996466" genericHeader="evaluation">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999304634146342">
The grammar formalism, learning model and our
ontology-based representation allow us to directly
map text to graph-based meaning representations.
Our method relies on a general grammar learn-
ing framework and a task-specific semantic inter-
preter. Learning is done based on annotated ex-
amples that do not contain ontology-specific roles
or concepts as we saw in Section 3, and thus our
learning framework is general. We can use any
ontology, depending on the application. The task-
specific semantic interpreter we are currently using
is targeted for terminological knowledge, and uses
a weak “ontological model” based on admissibility
relations we can find at the level of lexical entries
and a weak concept identity principle.
In (Muresan, 2006) we showed that our gram-
mar formalism and induction model allow us to
learn diverse and complex linguistic phenomena:
complex noun phrases (e.g., noun compounds,
nominalization), prepositional phrases, reduced
relative clauses, finite and non-finite verbal con-
structions (including, tense, aspect, negation), co-
ordination, copula to be, raising and control con-
structions, and rules for wh-questions (including
long-distance dependencies).
In this section we discuss the processes
of knowledge acquisition and natural language
querying, by presenting an example of construct-
ing terminological knowledge from definitions of
hepatitis, Hepatitis A and Hepatitis B. The defi-
nitional text and OKRs are presented in Figure 4,
OKR being shown only for the last two definitions
for readability reasons. A question and answer re-
lated to the resulting OKR are also given.
The definiendum is always a concept, and it is
part of the sub hierarchy. The concepts in the sub
hierarchy are presented in bold in Figure 4. In ad-
dition to the concepts that are defined, we can also
have concepts that are referred (i.e., they are part
of the definiens), if they do not have any modifi-
cation (e.g., #blood in definition of Hepatitis A,
and Hepatitis B). If a referred concept has modi-
fications, it is represented as an instance of a con-
cept in OKR. As a consequence, various verbal-
izations of concept properties can be differentiated
in OKR, allowing us to obtain direct answers that
are specific to each verbalization. For example, the
term virus appears in the definition of both Hepati-
tis A and Hepatitis B. In OKR, they are two differ-
ent instances of a concept, #virus25 and #virus33,
since they have different modifications: persists
in the blood serum, does not persists in the blood
serum, respectively. These modifications are an es-
sential part of the differentia of the two concepts
#hepatitisA and #hepatitisB, causing the distinc-
tion between the two. When we ask the question
What is caused by a virus that persists in the blood
serum? we obtain only the correct answer #hepati-
tisB (Figure 4).
Another important aspect that shows the ade-
quacy of our representation for direct acquisition
and query is the OKR-equivalences that we ob-
tain for different syntactic forms. They are related
mainly to verbal constructions. Among OKR-
equivalences we have: 1) active and passive con-
structions; 2) -ed and -ing verb forms in reduced
relative clauses are equivalent to passive/active
verbal constructions; 3) constructions involving
raising verbs, where we can take advantage of the
fact that the controller is not the semantic argument
of the raising verb (e.g., in the definition of Hep-
atitis B we have ... caused by a virus that tends to
persist in the blood serum, while the question can
be asked without the raising verb What is caused
by a virus that persists in the blood serum?; see
Figure 4).
Besides acquisition of terminological knowl-
edge, our grammar and semantic interpreter facil-
itates natural language querying of the acquired
knowledge base, by treatment of wh-questions.
Querying is a DAG matching problem, where the
wh-word is matched to the answer concept.
</bodyText>
<sectionHeader confidence="0.999499" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999811272727273">
This paper has presented a learning framework
to automatically map natural language to graph-
based meaning representations via grammar in-
duction. We presented an ontology-based seman-
tic representation that allows us to define meaning
as Meaning=Text+all Questions/Answers w.r.t that
Text, using a DAG matching approach.
In the future, we plan to extend this work in two
main directions. First, we plan to use a stronger
semantic context with hierarchies of concepts and
semantic roles, selectional restrictions, as well as
</bodyText>
<page confidence="0.934485">
15
</page>
<figure confidence="0.990852192982456">
Q1: What is caused by a virus that persists in the
blood serum?
#persist
loc
#cause
#serum #virus #what
of
#blood
A1: #hepatitisB
#cause32
#persist35
th ag th loc
sub
#hepatitis
#hepatitisB
#virus33
#serum27
duration kind_of
of
#acute #viral #blood
th ag th
1. Hepatitis is a disease caused by infectious or toxic agents and
characterized by jaundice, fever and liver enlargement.
2. Hepatitis A is an acute but benign viral hepatitis caused by a virus
that does not persist in the blood serum.
3. Hepatitis B is an acute viral hepatitis caused by a virus that tends
to persist in the blood serum.
loc
th
neg ag
y
#persist26
#virus25
benignity kind_of
#benign
#cause24
#hepatitisA
th
sub sub
#acute
#disease
#hepatitis
sub
#hepatitisB
#viral
#serum27
#blood
of
loc
th
#tend34
#persist35
#virus33
prop
th
ag
#cause32
</figure>
<figureCaption confidence="0.999929">
Figure 4: Acquisition/Query of terminological knowledge
</figureCaption>
<bodyText confidence="0.812859666666667">
semantic equivalences based on synonymy and
anaphora. The second direction is to enhance the
ontology with probabilities.
</bodyText>
<sectionHeader confidence="0.989316" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9924413">
Basili, Roberto, Dorte H. Hansen, Patrizia Paggio,
Maria Teresa Pazienza, and Fabio Zanzotto. 2004. On-
tological resources and question answering. In Workshop
on Pragmatics of Question Answering, held jointly with
NAACL 2004.
Beale, Stephen, Benoit Lavoie, Marjorie McShane, Sergei
Nirenburg, and Tanya Korelsky. 2004. Question answer-
ing using ontological semantics. In ACL 2004: Second
Workshop on Text Meaning and Interpretation.
Bos, Johan, Stephen Clark, Mark Steedman, James R. Cur-
ran, and Julia Hockenmaier. 2004. Wide-coverage seman-
tic representations from a CCG parser. In Proceedings of
COLING-04.
Collins, Michael. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University of
Pennsylvania.
Copestake, Ann, Dan Flickinger, Ivan A. Sag, and Carl Pol-
lard. 1999. Minimal Recursion Semantics: An introduc-
tion.
Dorr, Bonnie J. 1997. Large-scale dictionary construction for
foreign language tutoring and interlingual machine trans-
lation. Machine Translation, 12(4):271–322.
Dzeroski, Saso. 2007. Inductive logic programming in a nut-
shell. In Getoor, Lise and Ben Taskar, editors, Introduction
to Statistical Relational Learning. The MIT Press.
Ge, Ruifang and Raymond J. Mooney. 2005. A statistical
semantic parser that integrates syntax and semantics. In
Proceedings of CoNLL-2005.
He, Yulan and Steve Young. 2006. Spoken language un-
derstanding using the hidden vector state model. Speech
Communication Special Issue on Spoken Language Under-
standing in Conversational Systems, 48(3-4).
Hovy, Eduard, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes: The
90% solution. In Proceedings ofHLT-NAACL 2006.
Miller, George. 1990. WordNet: An on-line lexical database.
Journal ofLexicography, 3(4):235–312.
Muggleton, Stephen. 1995. Inverse Entailment and Progol.
New Generation Computing, Special Issue on Inductive
Logic Programming, 13(3-4):245–286.
Muresan, Smaranda and Owen Rambow. 2007. Grammar ap-
proximation by representative sublanguage: A new model
for language learning. In Proceedings of the 45th Annual
Meeting of the Association for Computational Linguistics
(ACL).
Muresan, Smaranda. 2006. Learning constraint-based gram-
mars from representative examples: Theory and applica-
tions. Technical report, PhD Thesis, Columbia University.
Nirenburg, Sergei and Victor Raskin. 2004. Ontological Se-
mantics. MIT Press.
Pereira, Fernando C. and David H.D Warren. 1980. Definite
Clause Grammars for language analysis. Artificial Intelli-
gence, 13:231–278.
Shieber, Stuart, Hans Uszkoreit, Fernando Pereira, Jane
Robinson, and Mabry Tyson. 1983. The formalism and
implementation of PATR-II. In Grosz, Barbara J. and
Mark Stickel, editors, Research on Interactive Acquisition
and Use of Knowledge, pages 39–79. SRI International,
Menlo Park, CA, November.
Sowa, John F. 1999. Knowledge Representation: Logical,
Philosophical, and Computational Foundations. Brooks
Cole Publishing Co., Pacific Grove, CA.
Wong, Yuk Wah and Raymond Mooney. 2007. Learning syn-
chronous grammars for semantic parsing with lambda cal-
culus. In Proceedings of the 45th Annual Meeting of the
Association for Computational Linguistics (ACL-2007).
Zettlemoyer, Luke S. and Michael Collins. 2005. Learning
to map sentences to logical form: Structured classification
with probabilistic categorial grammars. In Proceedings of
UAI-05.
</reference>
<page confidence="0.998498">
16
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.894130">
<title confidence="0.996955">Learning to Map Text to Graph-based Meaning Representations Grammar Induction</title>
<author confidence="0.933842">Smaranda</author>
<affiliation confidence="0.993826333333333">Laboratory for Computational Linguistics and Information Institute for Advanced Computer University of</affiliation>
<address confidence="0.996613">College Park, MD 20742,</address>
<email confidence="0.999867">smara@umiacs.umd.edu</email>
<abstract confidence="0.998197736842105">We argue in favor of using a graph-based representation for language meaning and propose a novel learning method to map natural language text to its graph-based meaning representation. We present a grammar formalism, which combines syntax and semantics, and has ontology constraints at the rule level. These constraints establish links between language expressions and the entities they refer to in the real world. We present a relational learning algorithm that learns these grammars from a small representative set of annotated examples, and show how this grammar induction framework and the ontology-based semantic representation allow us to directly map text to graph-based meaning representations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Roberto Basili</author>
<author>Dorte H Hansen</author>
<author>Patrizia Paggio</author>
<author>Maria Teresa Pazienza</author>
<author>Fabio Zanzotto</author>
</authors>
<title>Ontological resources and question answering.</title>
<date>2004</date>
<booktitle>In Workshop on Pragmatics of Question Answering, held jointly with NAACL</booktitle>
<contexts>
<context position="2027" citStr="Basili et al., 2004" startWordPosition="287" endWordPosition="290">ge, CLang) (Ge and Mooney, 2005). In this paper we focus on an ontology-based semantic representation which allows us to encode the meaning of a text as a direct acyclic graph. Recently, there is a growing interest on ontology-based NLP, starting from efforts in defining ontology-based semantic representations © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. (Nirenburg and Raskin, 2004), to using ontological resources in NLP applications, such as question answering (Basili et al., 2004; Beale et al., 2004), and building annotated corpora, such as the OntoNotes project (Hovy et al., 2006). There are three novel properties to ontologybased semantics that we propose in this paper: • There is a direct link between the ontology and the grammar through constraints at the grammar rule level. These ontology constraints enable access to meaning during language processing (parsing and generation). • Our ontology-based semantic representation is expressive enough to capture various phenomena of natural language, yet restrictive enough to facilitate grammar learning. The representation</context>
</contexts>
<marker>Basili, Hansen, Paggio, Pazienza, Zanzotto, 2004</marker>
<rawString>Basili, Roberto, Dorte H. Hansen, Patrizia Paggio, Maria Teresa Pazienza, and Fabio Zanzotto. 2004. Ontological resources and question answering. In Workshop on Pragmatics of Question Answering, held jointly with NAACL 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Beale</author>
<author>Benoit Lavoie</author>
<author>Marjorie McShane</author>
<author>Sergei Nirenburg</author>
<author>Tanya Korelsky</author>
</authors>
<title>Question answering using ontological semantics.</title>
<date>2004</date>
<booktitle>In ACL 2004: Second Workshop on Text Meaning and Interpretation.</booktitle>
<contexts>
<context position="2048" citStr="Beale et al., 2004" startWordPosition="291" endWordPosition="294">oney, 2005). In this paper we focus on an ontology-based semantic representation which allows us to encode the meaning of a text as a direct acyclic graph. Recently, there is a growing interest on ontology-based NLP, starting from efforts in defining ontology-based semantic representations © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. (Nirenburg and Raskin, 2004), to using ontological resources in NLP applications, such as question answering (Basili et al., 2004; Beale et al., 2004), and building annotated corpora, such as the OntoNotes project (Hovy et al., 2006). There are three novel properties to ontologybased semantics that we propose in this paper: • There is a direct link between the ontology and the grammar through constraints at the grammar rule level. These ontology constraints enable access to meaning during language processing (parsing and generation). • Our ontology-based semantic representation is expressive enough to capture various phenomena of natural language, yet restrictive enough to facilitate grammar learning. The representation encodes both ontolog</context>
</contexts>
<marker>Beale, Lavoie, McShane, Nirenburg, Korelsky, 2004</marker>
<rawString>Beale, Stephen, Benoit Lavoie, Marjorie McShane, Sergei Nirenburg, and Tanya Korelsky. 2004. Question answering using ontological semantics. In ACL 2004: Second Workshop on Text Meaning and Interpretation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bos</author>
<author>Stephen Clark</author>
<author>Mark Steedman</author>
<author>James R Curran</author>
<author>Julia Hockenmaier</author>
</authors>
<title>Wide-coverage semantic representations from a CCG parser.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING-04.</booktitle>
<contexts>
<context position="1282" citStr="Bos et al., 2004" startWordPosition="182" endWordPosition="185">ies they refer to in the real world. We present a relational learning algorithm that learns these grammars from a small representative set of annotated examples, and show how this grammar induction framework and the ontology-based semantic representation allow us to directly map text to graph-based meaning representations. 1 Introduction Recent work (Wong and Mooney, 2007; Zettlemoyer and Collins, 2005; He and Young, 2006) has developed learning algorithms for the problem of mapping sentences to their underlying semantic representations. These semantic representations vary from λ-expressions (Bos et al., 2004; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007) to DB query languages and command-like languages (RoboCup Coach Language, CLang) (Ge and Mooney, 2005). In this paper we focus on an ontology-based semantic representation which allows us to encode the meaning of a text as a direct acyclic graph. Recently, there is a growing interest on ontology-based NLP, starting from efforts in defining ontology-based semantic representations © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some r</context>
</contexts>
<marker>Bos, Clark, Steedman, Curran, Hockenmaier, 2004</marker>
<rawString>Bos, Johan, Stephen Clark, Mark Steedman, James R. Curran, and Julia Hockenmaier. 2004. Wide-coverage semantic representations from a CCG parser. In Proceedings of COLING-04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="10196" citStr="Collins, 1999" startWordPosition="1649" endWordPosition="1650">resentation which fully contains all the required information needed for validation on the ontology. Thus, it is independent of grammatical categories. This predicate can succeed or fail as a result of querying the ontology — when it succeeds, it instantiates the variables of the semantic representation with concepts/slots in the ontology (Y = degree). For example, given the phrase major damage, Φ� succeeds and returns (X1.isa = major, X.degree = X1, X.isa = damage), while given major birth it fails. 3 Grammar Learning Algorithm Unlike stochastic grammar learning for syntactic parsing (e.g., (Collins, 1999)), LWFG is well suited to learning from reduced-size training data. Furthermore, unlike previous formalisms used for deeper representations (e.g, HPSG, LFG), our LWFG formalism is characterized by a formal guarantee of polynomial learnability (Muresan, 2006). A key to these properties is the partial ordering among grammar nonterminals, i.e., the set of nonterminals is well-founded. This partial ordering among nonterminals allows us to define the representative examples of a LWFG, and to learn LWFGs from this small set of examples. The representative examples ER of a LWFG, G, are the simplest s</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Collins, Michael. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Copestake</author>
<author>Dan Flickinger</author>
<author>Ivan A Sag</author>
<author>Carl Pollard</author>
</authors>
<title>Minimal Recursion Semantics: An introduction.</title>
<date>1999</date>
<contexts>
<context position="7136" citStr="Copestake et al., 1999" startWordPosition="1131" endWordPosition="1134"> person). The set of attributes is finite and known a-priori for each syntactic category. Being a one-level feature structure, no recursive or embedded structures are allowed (unlike other grammar formalisms such as HPSG, LFG), which makes this representation appealing for a learning framework. Recursion in the grammar is obtained through the recursive grammar rules and the composition constraint. The body, b, of a semantic molecule is a flat representation, called OntoSeR (Ontology-based Semantic Representation). No embedding of predicates is allowed, as in Minimal Recursion Semantics (MRS) (Copestake et al., 1999). Unlike MRS, OntoSeR is a logical form built as a conjunction of atomic predicates (concept).(attr) = (concept), where variables are either concept or slot (attr) identifiers in an ontology. For example, the adjective major is represented as (X1.isa = major, X2.Y = X1), which says that the meaning of an adjective is a concept X1 (X1.isa = major) that is the value of a property of another concept X2 (X2.Y = X1) in the ontology. A LWFG specifies one or more semantic molecules for each string that can be parsed by the grammar. The lexicon of a LWFG consists of words paired with their semantic mo</context>
</contexts>
<marker>Copestake, Flickinger, Sag, Pollard, 1999</marker>
<rawString>Copestake, Ann, Dan Flickinger, Ivan A. Sag, and Carl Pollard. 1999. Minimal Recursion Semantics: An introduction.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie J Dorr</author>
</authors>
<title>Large-scale dictionary construction for foreign language tutoring and interlingual machine translation.</title>
<date>1997</date>
<journal>Machine Translation,</journal>
<volume>12</volume>
<issue>4</issue>
<contexts>
<context position="18315" citStr="Dorr, 1997" startWordPosition="2899" endWordPosition="2900">ause ˜35.loc=˜36 _a= duration of ˜32.ag=˜5 ˜36.det= the kind_of ˜32.th=˜1 ˜37.is_a= blood ag.is_a= by ˜36.of=˜37 #acute #viral #blood ˜32.ag=˜33 ˜36.isserum _a= ˜33.det= a Figure 3: Example of an utterance and its levels of representation seen as a local semantic interpretation at the utterance/grammar rule level, providing access to meaning during parsing/generation. In this paper, this semantic interpretation is based only on a weak “ontological model”. For the verb thematic roles we considered the thematic roles derived from Dorr’s LCS Database (e.g., ag=agent, th=theme, prop=proposition) (Dorr, 1997). For adjectives and adverbs we took the roles (properties) from WordNet (Miller, 1990). For prepositions we considered the LCS Database. We also have manually added specific/dummy semantic roles when they were not present in these resources (e.g., of between blood and serum). The example in Figure 3 shows the output of our parser in conjunction with the learned grammar for a definitional sentence that contains several linguistic phenomena such as copula to-be predicative, reduced relative clauses (caused by ...), relative clauses (virus that ...), raising construction (tends to persist, where</context>
</contexts>
<marker>Dorr, 1997</marker>
<rawString>Dorr, Bonnie J. 1997. Large-scale dictionary construction for foreign language tutoring and interlingual machine translation. Machine Translation, 12(4):271–322.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saso Dzeroski</author>
</authors>
<title>Inductive logic programming in a nutshell.</title>
<date>2007</date>
<booktitle>Introduction to Statistical Relational Learning.</booktitle>
<editor>In Getoor, Lise and Ben Taskar, editors,</editor>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="11815" citStr="Dzeroski, 2007" startWordPosition="1903" endWordPosition="1904">eneralization during learning (Figure 2). The theoretical learning model is Grammar Approximation by Representative Sublanguage (GARS) introduced in (Muresan, 2006; Muresan and Rambow, 2007). We proved that the search space for grammar induction is a complete grammar lattice, and we gave a learnability theorem for LWFG induction. The GARS model uses a polynomial algorithm for LWFG learning that takes advantage of the building blocks nature of representative examples. The learning algorithm belongs to the class of Inductive Logic Programming methods (ILP), based on entailment (Muggleton, 1995; Dzeroski, 2007). Unlike existing ILP methods that use randomly-selected examples, our algorithm learns from a set of representative examples allowing a polynomial efficiency for learning a syntactico-semantic constraint-based grammar, suitable to capture large fragments of natural language (Muresan, 2006). The LWFG induction algorithm is a cover set algorithm, where at each step a new constraint grammar rule is learned from the current representative example, Q E ER. Then this rule is added to the grammar rule set. The process continues until all the representative examples are covered. We describe below the</context>
</contexts>
<marker>Dzeroski, 2007</marker>
<rawString>Dzeroski, Saso. 2007. Inductive logic programming in a nutshell. In Getoor, Lise and Ben Taskar, editors, Introduction to Statistical Relational Learning. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruifang Ge</author>
<author>Raymond J Mooney</author>
</authors>
<title>A statistical semantic parser that integrates syntax and semantics.</title>
<date>2005</date>
<booktitle>In Proceedings of CoNLL-2005.</booktitle>
<contexts>
<context position="1440" citStr="Ge and Mooney, 2005" startWordPosition="206" endWordPosition="209">amples, and show how this grammar induction framework and the ontology-based semantic representation allow us to directly map text to graph-based meaning representations. 1 Introduction Recent work (Wong and Mooney, 2007; Zettlemoyer and Collins, 2005; He and Young, 2006) has developed learning algorithms for the problem of mapping sentences to their underlying semantic representations. These semantic representations vary from λ-expressions (Bos et al., 2004; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007) to DB query languages and command-like languages (RoboCup Coach Language, CLang) (Ge and Mooney, 2005). In this paper we focus on an ontology-based semantic representation which allows us to encode the meaning of a text as a direct acyclic graph. Recently, there is a growing interest on ontology-based NLP, starting from efforts in defining ontology-based semantic representations © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. (Nirenburg and Raskin, 2004), to using ontological resources in NLP applications, such as question answering (Basili et al., 2004; Beale et al</context>
</contexts>
<marker>Ge, Mooney, 2005</marker>
<rawString>Ge, Ruifang and Raymond J. Mooney. 2005. A statistical semantic parser that integrates syntax and semantics. In Proceedings of CoNLL-2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yulan He</author>
<author>Steve Young</author>
</authors>
<title>Spoken language understanding using the hidden vector state model.</title>
<date>2006</date>
<journal>Speech Communication Special Issue on Spoken Language Understanding in Conversational Systems,</journal>
<pages>48--3</pages>
<contexts>
<context position="1092" citStr="He and Young, 2006" startWordPosition="157" endWordPosition="160">We present a grammar formalism, which combines syntax and semantics, and has ontology constraints at the rule level. These constraints establish links between language expressions and the entities they refer to in the real world. We present a relational learning algorithm that learns these grammars from a small representative set of annotated examples, and show how this grammar induction framework and the ontology-based semantic representation allow us to directly map text to graph-based meaning representations. 1 Introduction Recent work (Wong and Mooney, 2007; Zettlemoyer and Collins, 2005; He and Young, 2006) has developed learning algorithms for the problem of mapping sentences to their underlying semantic representations. These semantic representations vary from λ-expressions (Bos et al., 2004; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007) to DB query languages and command-like languages (RoboCup Coach Language, CLang) (Ge and Mooney, 2005). In this paper we focus on an ontology-based semantic representation which allows us to encode the meaning of a text as a direct acyclic graph. Recently, there is a growing interest on ontology-based NLP, starting from efforts in defining ontology-bas</context>
</contexts>
<marker>He, Young, 2006</marker>
<rawString>He, Yulan and Steve Young. 2006. Spoken language understanding using the hidden vector state model. Speech Communication Special Issue on Spoken Language Understanding in Conversational Systems, 48(3-4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Mitchell Marcus</author>
<author>Martha Palmer</author>
<author>Lance Ramshaw</author>
<author>Ralph Weischedel</author>
</authors>
<title>Ontonotes: The 90% solution.</title>
<date>2006</date>
<booktitle>In Proceedings ofHLT-NAACL</booktitle>
<contexts>
<context position="2131" citStr="Hovy et al., 2006" startWordPosition="304" endWordPosition="307">ch allows us to encode the meaning of a text as a direct acyclic graph. Recently, there is a growing interest on ontology-based NLP, starting from efforts in defining ontology-based semantic representations © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. (Nirenburg and Raskin, 2004), to using ontological resources in NLP applications, such as question answering (Basili et al., 2004; Beale et al., 2004), and building annotated corpora, such as the OntoNotes project (Hovy et al., 2006). There are three novel properties to ontologybased semantics that we propose in this paper: • There is a direct link between the ontology and the grammar through constraints at the grammar rule level. These ontology constraints enable access to meaning during language processing (parsing and generation). • Our ontology-based semantic representation is expressive enough to capture various phenomena of natural language, yet restrictive enough to facilitate grammar learning. The representation encodes both ontological meaning (concepts and relations among concepts) and extra-ontological meaning,</context>
</contexts>
<marker>Hovy, Marcus, Palmer, Ramshaw, Weischedel, 2006</marker>
<rawString>Hovy, Eduard, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2006. Ontonotes: The 90% solution. In Proceedings ofHLT-NAACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Miller</author>
</authors>
<title>WordNet: An on-line lexical database.</title>
<date>1990</date>
<journal>Journal ofLexicography,</journal>
<pages>3--4</pages>
<contexts>
<context position="18402" citStr="Miller, 1990" startWordPosition="2913" endWordPosition="2914">lood ag.is_a= by ˜36.of=˜37 #acute #viral #blood ˜32.ag=˜33 ˜36.isserum _a= ˜33.det= a Figure 3: Example of an utterance and its levels of representation seen as a local semantic interpretation at the utterance/grammar rule level, providing access to meaning during parsing/generation. In this paper, this semantic interpretation is based only on a weak “ontological model”. For the verb thematic roles we considered the thematic roles derived from Dorr’s LCS Database (e.g., ag=agent, th=theme, prop=proposition) (Dorr, 1997). For adjectives and adverbs we took the roles (properties) from WordNet (Miller, 1990). For prepositions we considered the LCS Database. We also have manually added specific/dummy semantic roles when they were not present in these resources (e.g., of between blood and serum). The example in Figure 3 shows the output of our parser in conjunction with the learned grammar for a definitional sentence that contains several linguistic phenomena such as copula to-be predicative, reduced relative clauses (caused by ...), relative clauses (virus that ...), raising construction (tends to persist, where virus is not the argument of tends but the argument of persist), and noun compounds (b</context>
</contexts>
<marker>Miller, 1990</marker>
<rawString>Miller, George. 1990. WordNet: An on-line lexical database. Journal ofLexicography, 3(4):235–312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Muggleton</author>
</authors>
<date>1995</date>
<booktitle>Inverse Entailment and Progol. New Generation Computing, Special Issue on Inductive Logic Programming,</booktitle>
<pages>13--3</pages>
<contexts>
<context position="11798" citStr="Muggleton, 1995" startWordPosition="1901" endWordPosition="1902">EQ, is used for generalization during learning (Figure 2). The theoretical learning model is Grammar Approximation by Representative Sublanguage (GARS) introduced in (Muresan, 2006; Muresan and Rambow, 2007). We proved that the search space for grammar induction is a complete grammar lattice, and we gave a learnability theorem for LWFG induction. The GARS model uses a polynomial algorithm for LWFG learning that takes advantage of the building blocks nature of representative examples. The learning algorithm belongs to the class of Inductive Logic Programming methods (ILP), based on entailment (Muggleton, 1995; Dzeroski, 2007). Unlike existing ILP methods that use randomly-selected examples, our algorithm learns from a set of representative examples allowing a polynomial efficiency for learning a syntactico-semantic constraint-based grammar, suitable to capture large fragments of natural language (Muresan, 2006). The LWFG induction algorithm is a cover set algorithm, where at each step a new constraint grammar rule is learned from the current representative example, Q E ER. Then this rule is added to the grammar rule set. The process continues until all the representative examples are covered. We d</context>
</contexts>
<marker>Muggleton, 1995</marker>
<rawString>Muggleton, Stephen. 1995. Inverse Entailment and Progol. New Generation Computing, Special Issue on Inductive Logic Programming, 13(3-4):245–286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Smaranda Muresan</author>
<author>Owen Rambow</author>
</authors>
<title>Grammar approximation by representative sublanguage: A new model for language learning.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="3365" citStr="Muresan and Rambow, 2007" startWordPosition="496" endWordPosition="499">voice, tense, aspect, modality. • Our representation and grammar learning framework allow a direct mapping of text to its meaning, encoded as a direct acyclic graph (DAG). We consider that “understanding” a text is the ability to correctly answer, at the conceptual level, all the questions asked w.r.t to that text, and thus Meaning = Text + all Questions/Answers w.r.t that Text. Under this assumption, obtaining the meaning of a text is reduced to a question answering process, which in our framework is a DAG matching problem. First, we review our grammar formalism introduced in (Muresan, 2006; Muresan and Rambow, 2007), called Lexicalized Well-Founded Grammars. Second, we present a relational learning algorithm for inducing these grammars from a representative sample of strings annotated with their semantics, along with minimal assumptions about 9 Coling 2008: Proceedings of 3rd Textgraphs workshop on Graph-Based Algorithms in Natural Language Processing, pages 9–16 Manchester, August 2008 I. Semantic Molecules D b 1 C C C C C C D E A C X1.isa = major, X2.Y=X1 b. (damage/noun)&apos;= 0 B B B B B B B @ 2 3 cat noun 6 7 4nr sg 5 head X3 h2 b2DX3.isa = damageE c. (major damage)&apos;= 0 B B B B B B @ 1 C C C C C E A C X</context>
<context position="5088" citStr="Muresan and Rambow, 2007" startWordPosition="811" endWordPosition="815">mantic composition and ontology-based interpretation constraints, Φc and Φo (II) syntax. Then, we describe the levels of representation we use to go from utterances to their graphbased meaning representations, and show how our representation is suitable to define the meaning of an utterance/text through answers to questions. As a proof of concept we discuss how our framework can be used to acquire terminological knowledge from natural language definitions and to query this knowledge using wh-questions. 2 Grammar Formalism Lexicalized Well-Founded Grammars (LWFGs) introduced in (Muresan, 2006; Muresan and Rambow, 2007) are a type of Definite Clause Grammars (Pereira and Warren, 1980) where: (1) the context-free backbone is extended by introducing a partial ordering relation among nonterminals (the basis for “well-founded”); (2) each string is associated with a syntactic-semantic representation called a semantic molecule; and (3) grammar rules have two types of constraints: one for semantic composition and one for ontology-based semantic interpretation. The last two properties allow us to have a syntactic-semantic grammar. The ontology constraints provide access to meaning during language learning, parsing a</context>
<context position="11390" citStr="Muresan and Rambow, 2007" startWordPosition="1833" endWordPosition="1836">f a LWFG, G, are the simplest syntagmas ground-derived by the grammar G — i.e., for each grammar rule, there exists a syntagma which is ground-derived from it in the minimum number of steps. Informally, representative examples are building blocks from which larger structures can be inferred via reference to a larger corpus EQ which can be only weakly annotated (i.e., bracketed), or unannotated. This larger corpus, EQ, is used for generalization during learning (Figure 2). The theoretical learning model is Grammar Approximation by Representative Sublanguage (GARS) introduced in (Muresan, 2006; Muresan and Rambow, 2007). We proved that the search space for grammar induction is a complete grammar lattice, and we gave a learnability theorem for LWFG induction. The GARS model uses a polynomial algorithm for LWFG learning that takes advantage of the building blocks nature of representative examples. The learning algorithm belongs to the class of Inductive Logic Programming methods (ILP), based on entailment (Muggleton, 1995; Dzeroski, 2007). Unlike existing ILP methods that use randomly-selected examples, our algorithm learns from a set of representative examples allowing a polynomial efficiency for learning a s</context>
</contexts>
<marker>Muresan, Rambow, 2007</marker>
<rawString>Muresan, Smaranda and Owen Rambow. 2007. Grammar approximation by representative sublanguage: A new model for language learning. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Smaranda Muresan</author>
</authors>
<title>Learning constraint-based grammars from representative examples: Theory and applications.</title>
<date>2006</date>
<tech>Technical report, PhD Thesis,</tech>
<institution>Columbia University.</institution>
<contexts>
<context position="3338" citStr="Muresan, 2006" startWordPosition="494" endWordPosition="495">aning, such as voice, tense, aspect, modality. • Our representation and grammar learning framework allow a direct mapping of text to its meaning, encoded as a direct acyclic graph (DAG). We consider that “understanding” a text is the ability to correctly answer, at the conceptual level, all the questions asked w.r.t to that text, and thus Meaning = Text + all Questions/Answers w.r.t that Text. Under this assumption, obtaining the meaning of a text is reduced to a question answering process, which in our framework is a DAG matching problem. First, we review our grammar formalism introduced in (Muresan, 2006; Muresan and Rambow, 2007), called Lexicalized Well-Founded Grammars. Second, we present a relational learning algorithm for inducing these grammars from a representative sample of strings annotated with their semantics, along with minimal assumptions about 9 Coling 2008: Proceedings of 3rd Textgraphs workshop on Graph-Based Algorithms in Natural Language Processing, pages 9–16 Manchester, August 2008 I. Semantic Molecules D b 1 C C C C C C D E A C X1.isa = major, X2.Y=X1 b. (damage/noun)&apos;= 0 B B B B B B B @ 2 3 cat noun 6 7 4nr sg 5 head X3 h2 b2DX3.isa = damageE c. (major damage)&apos;= 0 B B B </context>
<context position="5061" citStr="Muresan, 2006" startWordPosition="809" endWordPosition="810">her with the semantic composition and ontology-based interpretation constraints, Φc and Φo (II) syntax. Then, we describe the levels of representation we use to go from utterances to their graphbased meaning representations, and show how our representation is suitable to define the meaning of an utterance/text through answers to questions. As a proof of concept we discuss how our framework can be used to acquire terminological knowledge from natural language definitions and to query this knowledge using wh-questions. 2 Grammar Formalism Lexicalized Well-Founded Grammars (LWFGs) introduced in (Muresan, 2006; Muresan and Rambow, 2007) are a type of Definite Clause Grammars (Pereira and Warren, 1980) where: (1) the context-free backbone is extended by introducing a partial ordering relation among nonterminals (the basis for “well-founded”); (2) each string is associated with a syntactic-semantic representation called a semantic molecule; and (3) grammar rules have two types of constraints: one for semantic composition and one for ontology-based semantic interpretation. The last two properties allow us to have a syntactic-semantic grammar. The ontology constraints provide access to meaning during l</context>
<context position="10454" citStr="Muresan, 2006" startWordPosition="1684" endWordPosition="1685">s the variables of the semantic representation with concepts/slots in the ontology (Y = degree). For example, given the phrase major damage, Φ� succeeds and returns (X1.isa = major, X.degree = X1, X.isa = damage), while given major birth it fails. 3 Grammar Learning Algorithm Unlike stochastic grammar learning for syntactic parsing (e.g., (Collins, 1999)), LWFG is well suited to learning from reduced-size training data. Furthermore, unlike previous formalisms used for deeper representations (e.g, HPSG, LFG), our LWFG formalism is characterized by a formal guarantee of polynomial learnability (Muresan, 2006). A key to these properties is the partial ordering among grammar nonterminals, i.e., the set of nonterminals is well-founded. This partial ordering among nonterminals allows us to define the representative examples of a LWFG, and to learn LWFGs from this small set of examples. The representative examples ER of a LWFG, G, are the simplest syntagmas ground-derived by the grammar G — i.e., for each grammar rule, there exists a syntagma which is ground-derived from it in the minimum number of steps. Informally, representative examples are building blocks from which larger structures can be inferr</context>
<context position="12106" citStr="Muresan, 2006" startWordPosition="1945" endWordPosition="1946">earnability theorem for LWFG induction. The GARS model uses a polynomial algorithm for LWFG learning that takes advantage of the building blocks nature of representative examples. The learning algorithm belongs to the class of Inductive Logic Programming methods (ILP), based on entailment (Muggleton, 1995; Dzeroski, 2007). Unlike existing ILP methods that use randomly-selected examples, our algorithm learns from a set of representative examples allowing a polynomial efficiency for learning a syntactico-semantic constraint-based grammar, suitable to capture large fragments of natural language (Muresan, 2006). The LWFG induction algorithm is a cover set algorithm, where at each step a new constraint grammar rule is learned from the current representative example, Q E ER. Then this rule is added to the grammar rule set. The process continues until all the representative examples are covered. We describe below the process of learning a grammar rule from the current representative example, illustrated as well in Figure 2. Step 1. In the first step, the most specific grammar rule is generated from the current representative example. The category name annotated in the representative example gives the n</context>
<context position="26177" citStr="Muresan, 2006" startWordPosition="4205" endWordPosition="4206">representations. Our method relies on a general grammar learning framework and a task-specific semantic interpreter. Learning is done based on annotated examples that do not contain ontology-specific roles or concepts as we saw in Section 3, and thus our learning framework is general. We can use any ontology, depending on the application. The taskspecific semantic interpreter we are currently using is targeted for terminological knowledge, and uses a weak “ontological model” based on admissibility relations we can find at the level of lexical entries and a weak concept identity principle. In (Muresan, 2006) we showed that our grammar formalism and induction model allow us to learn diverse and complex linguistic phenomena: complex noun phrases (e.g., noun compounds, nominalization), prepositional phrases, reduced relative clauses, finite and non-finite verbal constructions (including, tense, aspect, negation), coordination, copula to be, raising and control constructions, and rules for wh-questions (including long-distance dependencies). In this section we discuss the processes of knowledge acquisition and natural language querying, by presenting an example of constructing terminological knowledg</context>
</contexts>
<marker>Muresan, 2006</marker>
<rawString>Muresan, Smaranda. 2006. Learning constraint-based grammars from representative examples: Theory and applications. Technical report, PhD Thesis, Columbia University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergei Nirenburg</author>
<author>Victor Raskin</author>
</authors>
<title>Ontological Semantics.</title>
<date>2004</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="1926" citStr="Nirenburg and Raskin, 2004" startWordPosition="270" endWordPosition="273"> Collins, 2005; Wong and Mooney, 2007) to DB query languages and command-like languages (RoboCup Coach Language, CLang) (Ge and Mooney, 2005). In this paper we focus on an ontology-based semantic representation which allows us to encode the meaning of a text as a direct acyclic graph. Recently, there is a growing interest on ontology-based NLP, starting from efforts in defining ontology-based semantic representations © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. (Nirenburg and Raskin, 2004), to using ontological resources in NLP applications, such as question answering (Basili et al., 2004; Beale et al., 2004), and building annotated corpora, such as the OntoNotes project (Hovy et al., 2006). There are three novel properties to ontologybased semantics that we propose in this paper: • There is a direct link between the ontology and the grammar through constraints at the grammar rule level. These ontology constraints enable access to meaning during language processing (parsing and generation). • Our ontology-based semantic representation is expressive enough to capture various phe</context>
</contexts>
<marker>Nirenburg, Raskin, 2004</marker>
<rawString>Nirenburg, Sergei and Victor Raskin. 2004. Ontological Semantics. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando C Pereira</author>
<author>David H D Warren</author>
</authors>
<title>Definite Clause Grammars for language analysis.</title>
<date>1980</date>
<journal>Artificial Intelligence,</journal>
<pages>13--231</pages>
<contexts>
<context position="5154" citStr="Pereira and Warren, 1980" startWordPosition="824" endWordPosition="827">Φc and Φo (II) syntax. Then, we describe the levels of representation we use to go from utterances to their graphbased meaning representations, and show how our representation is suitable to define the meaning of an utterance/text through answers to questions. As a proof of concept we discuss how our framework can be used to acquire terminological knowledge from natural language definitions and to query this knowledge using wh-questions. 2 Grammar Formalism Lexicalized Well-Founded Grammars (LWFGs) introduced in (Muresan, 2006; Muresan and Rambow, 2007) are a type of Definite Clause Grammars (Pereira and Warren, 1980) where: (1) the context-free backbone is extended by introducing a partial ordering relation among nonterminals (the basis for “well-founded”); (2) each string is associated with a syntactic-semantic representation called a semantic molecule; and (3) grammar rules have two types of constraints: one for semantic composition and one for ontology-based semantic interpretation. The last two properties allow us to have a syntactic-semantic grammar. The ontology constraints provide access to meaning during language learning, parsing and generation. The first property allows us to learn these grammar</context>
</contexts>
<marker>Pereira, Warren, 1980</marker>
<rawString>Pereira, Fernando C. and David H.D Warren. 1980. Definite Clause Grammars for language analysis. Artificial Intelligence, 13:231–278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Shieber</author>
<author>Hans Uszkoreit</author>
<author>Fernando Pereira</author>
<author>Jane Robinson</author>
<author>Mabry Tyson</author>
</authors>
<title>The formalism and implementation of PATR-II.</title>
<date>1983</date>
<booktitle>Research on Interactive Acquisition and Use of Knowledge,</booktitle>
<pages>39--79</pages>
<editor>In Grosz, Barbara J. and Mark Stickel, editors,</editor>
<publisher>SRI International,</publisher>
<location>Menlo Park, CA,</location>
<contexts>
<context position="8877" citStr="Shieber et al., 1983" startWordPosition="1437" endWordPosition="1440">tic composition (how the 10 meaning of a natural language expression is composed from the meaning of its parts) and one for ontology-based semantic interpretation. The composition constraints Φ, are applied to the heads of the semantic molecules, the bodies being just concatenated. Figure 1 shows that the body of the semantic molecule for major damage is a concatenation of the bodies of the adjective major and noun damage, together with a variable substitution. This variable substitution {X2/X, X3/XI is a result of Φ,, which is a system of equations — a simplified version of “path equations” (Shieber et al., 1983), because the heads are flat feature structures. These constraints are learned together with the grammar rules. The ontology-based constraints Φ� represent the validation on the ontology, and are applied to the body of the semantic molecule associated with the left-hand side nonterminal. The ontology-based interpretation is not done during the composition operation, but afterwords. Thus, for example, the head of the noun phrase major damage does not need to store the slot Y , a fact that allows us to use flat feature structures to represent the head of the semantic molecules. The ontology-base</context>
</contexts>
<marker>Shieber, Uszkoreit, Pereira, Robinson, Tyson, 1983</marker>
<rawString>Shieber, Stuart, Hans Uszkoreit, Fernando Pereira, Jane Robinson, and Mabry Tyson. 1983. The formalism and implementation of PATR-II. In Grosz, Barbara J. and Mark Stickel, editors, Research on Interactive Acquisition and Use of Knowledge, pages 39–79. SRI International, Menlo Park, CA, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John F Sowa</author>
</authors>
<title>Knowledge Representation: Logical, Philosophical, and Computational Foundations.</title>
<date>1999</date>
<publisher>Brooks Cole Publishing Co.,</publisher>
<location>Pacific Grove, CA.</location>
<contexts>
<context position="20292" citStr="Sowa, 1999" startWordPosition="3196" endWordPosition="3197">still present. Figure 3 shows an example of the TKR for the above utterance. 4.3 Ontology-level Representation Ontology-level knowledge representation OKR is obtained after task-specific interpretation, which can be seen as a global semantic interpretation. OKR is a directed acyclic graph (DAG) G = (V, E). Edges, E, are either semantic roles given by verbs, prepositions, adjectives and adverbs, or extra-ontological meaning properties, such as tense, aspect, modality, negation. Vertices, V are either concepts (corresponding to nouns, verbs, adjectives, adverbs, pronouns, cf. Quine’s criterion (Sowa, 1999, page 496)), or values of the extra-ontological properties such as present corresponding to tense property. In this paper, the task-specific interpretation is geared mainly towards terminological interpretation. We filter from OntoSeR determiners and some verb forms, such as tense, aspect, since temporal relations appear less in terminological knowledge than in factual 13 knowledge. However, we treat modals and negation, as they are relevant for terminological knowledge. An example of OKR for the above utterance is given in Figure 3. We consider both concepts (e.g., #acute, #blood), and insta</context>
</contexts>
<marker>Sowa, 1999</marker>
<rawString>Sowa, John F. 1999. Knowledge Representation: Logical, Philosophical, and Computational Foundations. Brooks Cole Publishing Co., Pacific Grove, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuk Wah Wong</author>
<author>Raymond Mooney</author>
</authors>
<title>Learning synchronous grammars for semantic parsing with lambda calculus.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL-2007).</booktitle>
<contexts>
<context position="1040" citStr="Wong and Mooney, 2007" startWordPosition="148" endWordPosition="151">guage text to its graph-based meaning representation. We present a grammar formalism, which combines syntax and semantics, and has ontology constraints at the rule level. These constraints establish links between language expressions and the entities they refer to in the real world. We present a relational learning algorithm that learns these grammars from a small representative set of annotated examples, and show how this grammar induction framework and the ontology-based semantic representation allow us to directly map text to graph-based meaning representations. 1 Introduction Recent work (Wong and Mooney, 2007; Zettlemoyer and Collins, 2005; He and Young, 2006) has developed learning algorithms for the problem of mapping sentences to their underlying semantic representations. These semantic representations vary from λ-expressions (Bos et al., 2004; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007) to DB query languages and command-like languages (RoboCup Coach Language, CLang) (Ge and Mooney, 2005). In this paper we focus on an ontology-based semantic representation which allows us to encode the meaning of a text as a direct acyclic graph. Recently, there is a growing interest on ontology-based</context>
</contexts>
<marker>Wong, Mooney, 2007</marker>
<rawString>Wong, Yuk Wah and Raymond Mooney. 2007. Learning synchronous grammars for semantic parsing with lambda calculus. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL-2007).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke S Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars.</title>
<date>2005</date>
<booktitle>In Proceedings of UAI-05.</booktitle>
<contexts>
<context position="1071" citStr="Zettlemoyer and Collins, 2005" startWordPosition="152" endWordPosition="156">-based meaning representation. We present a grammar formalism, which combines syntax and semantics, and has ontology constraints at the rule level. These constraints establish links between language expressions and the entities they refer to in the real world. We present a relational learning algorithm that learns these grammars from a small representative set of annotated examples, and show how this grammar induction framework and the ontology-based semantic representation allow us to directly map text to graph-based meaning representations. 1 Introduction Recent work (Wong and Mooney, 2007; Zettlemoyer and Collins, 2005; He and Young, 2006) has developed learning algorithms for the problem of mapping sentences to their underlying semantic representations. These semantic representations vary from λ-expressions (Bos et al., 2004; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007) to DB query languages and command-like languages (RoboCup Coach Language, CLang) (Ge and Mooney, 2005). In this paper we focus on an ontology-based semantic representation which allows us to encode the meaning of a text as a direct acyclic graph. Recently, there is a growing interest on ontology-based NLP, starting from efforts in </context>
</contexts>
<marker>Zettlemoyer, Collins, 2005</marker>
<rawString>Zettlemoyer, Luke S. and Michael Collins. 2005. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In Proceedings of UAI-05.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>