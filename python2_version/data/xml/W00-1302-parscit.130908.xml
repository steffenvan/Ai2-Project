<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000022">
<title confidence="0.9978535">
What&apos;s yours and what&apos;s mine: Determining Intellectual
Attribution in Scientific Text
</title>
<author confidence="0.988999">
Simone Teufelt Marc Moens
</author>
<affiliation confidence="0.9863165">
Computer Science Department HCRC Language Technology Group
Columbia University University of Edinburgh
</affiliation>
<email confidence="0.83618">
teufelOcs.columbia.edu Marc.MoensOed.ac.uk
</email>
<sectionHeader confidence="0.994383" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999796933333334">
We believe that identifying the structure of scien-
tific argumentation in articles can help in tasks
such as automatic summarization or the auto-
mated construction of citation indexes. One par-
ticularly important aspect of this structure is the
question of who a given scientific statement is at-
tributed to: other researchers, the field in general,
or the authors themselves.
We present the algorithm and a systematic eval-
uation of a system which can recognize the most
salient textual properties that contribute to the
global argumentative structure of a text. In this
paper we concentrate on two particular features,
namely the occurrences of prototypical agents and
their actions in scientific text.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999612142857143">
When writing an article, one does not normally
go straight to presenting the innovative scien-
tific claim. Instead, one establishes other, well-
known scientific facts first, which are contributed
by other researchers. Attribution of ownership of-
ten happens explicitly, by phrases such as &amp;quot;Chom-
sky (1965) claims that&amp;quot;. The question of intel-
lectual attribution is important for researchers:
not understanding the argumentative status of
part of the text is a common problem for non-
experts reading highly specific texts aimed at ex-
perts (Rowley, 1982). In particular, after reading
an article, researchers need to know who holds the
&amp;quot;knowledge claim&amp;quot; for a certain fact that interests
them.
We propose that segmentation according to in-
tellectual ownership can be done automatically,
and that such a segmentation has advantages for
various shallow text understanding tasks. At the
heart of our classification scheme is the following
trisection:
</bodyText>
<listItem confidence="0.999884">
• BACKGROUND (generally known work)
• OWN, new work and
• specific OTHER work.
</listItem>
<bodyText confidence="0.854718454545454">
The advantages of a segmentation at a rhetori-
cal level is that rhetorics is conveniently constant
tThis work was done while the first author was at the
HCRC Language Technology Group, Edinburgh.
BACKGROUND:
Researchers in knowledge representa-
tion agree that one of the hard problems of
understanding narrative is the representation
of temporal information. Certain facts of nat-
ural language make it hard to capture tempo-
ral information [...]
</bodyText>
<sectionHeader confidence="0.415024" genericHeader="introduction">
OTHER WORK:
</sectionHeader>
<bodyText confidence="0.699506333333333">
Recently, Researcher-4 has suggested the
following solution to this problem [...].
WEAKNESS/CONTRAST:
But this solution cannot be used to inter-
pret the following Japanese examples: [...]
OWN CONTRIBUTION:
We propose a solution which circumvents
this problem while retaining the explanatory
power of Researcher-4&apos;s approach.
</bodyText>
<figureCaption confidence="0.999155">
Figure 1: Fictional introduction section
</figureCaption>
<bodyText confidence="0.998409307692308">
across different articles. Subject matter, on the
contrary, is not constant, nor are writing style and
other factors.
We work with a corpus of scientific pa-
pers (80 computational linguistics conference ar-
ticles (ACL, EACL, COLING or ANLP), de-
posited on the CMP_LG archive between 1994
and 1996). This is a difficult test bed due to
the large variation with respect to different fac-
tors: subdomain (theoretical linguistics, statisti-
cal NLP, logic programming, computational psy-
cholinguistics), types of research (implementa-
tion, review, evaluation, empirical vs. theoreti-
cal research), writing style (formal vs. informal)
and presentational styles (fixed section structure
of type Introduction—Method—Results—Conclusion
vs. more idiosyncratic, problem-structured presen-
tation).
One thing, however, is constant across all arti-
cles: the argumentative aim of every single article
is to show that the given work is a contribution to
science (Swales, 1990; Myers, 1992; Hyland, 1998).
Theories of scientific argumentation in research ar-
ticles stress that authors follow well-predictable
stages of argumentation, as in the fictional intro-
duction in figure 1.
</bodyText>
<page confidence="0.995469">
9
</page>
<bodyText confidence="0.741810777777778">
Are the scientific statements expressed
in this sentence attributed to the
authors, the general field, or specific other
researchers?
Own work Other Work
BACKGROUND
Does this sentence contain material Does it describe a negative aspect
that describes the specific aim of the other work, or a contrast
of the paper? or comparison of the own work to it?
</bodyText>
<figure confidence="0.928717375">
NO NO
Does this sentence make Does this sentence mention
reference to the external the other work as basis of
structure of the paper? or support for own work?
AIM
NO
TEXTUAL
OWN
</figure>
<figureCaption confidence="0.999903">
Figure 2: Annotation Scheme for Argumentative Zones
</figureCaption>
<bodyText confidence="0.999035555555556">
Our hypothesis is that a segmentation based on
regularities of scientific argumentation and on at-
tribution of intellectual ownership is one of the
most stable and generalizable dimensions which
contribute to the structure of scientific texts. In
the next section we will describe an annotation
scheme which we designed for capturing these ef-
fects. Its categories are based on Swales&apos; (1990)
CARS model.
</bodyText>
<subsectionHeader confidence="0.999722">
1.1 The scheme
</subsectionHeader>
<bodyText confidence="0.999820785714286">
As our corpus contains many statements talking
about relations between own and other work, we
decided to add two classes (&amp;quot;zones&amp;quot;) for express-
ing relations to the core set of OWN, OTHER
and BACKGROUND, namely contrastive statements
(CONTRAST; comparable to Swales&apos; (1990) move
2A/B) and statements of intellectual ancestry
(BAsis; Swales&apos; move 2D). The label OTHER is
thus reserved for neutral descriptions of other
work. OWN segments are further subdivided to
mark explicit aim statements (ADA; Swales&apos; move
3.1A/B), and explicit section previews (TEXTUAL;
Swales&apos; move 3.3). All other statements about the
own work are classified as OWN. Each of the seven
category covers one sentence.
Our classification, which is a further develop-
ment of the scheme in Teufel and Moens (1999),
can be described procedurally as a decision tree
(Figure 2), where five questions are asked about
each sentence, concerning intellectual attribution,
author stance and continuation vs. contrast. Fig-
ure 3 gives typical example sentences for each zone.
The intellectual-attribution distinction we make
is comparable with Wiebe&apos;s (1994) distinction into
subjective and objective statements. Subjectivity
is a property which is related to the attribution of
authorship as well as to author stance, but it is
just one of the dimensions we consider.
</bodyText>
<subsectionHeader confidence="0.999503">
1.2 Use of Argumentative Zones
</subsectionHeader>
<bodyText confidence="0.999941515151515">
Which practical use would segmenting a paper into
argumentative zones have?
Firstly, rhetorical information as encoded in
these zones should prove useful for summariza-
tion. Sentence extracts, still the main type of
summarization around, are notoriously context-
insensitive. Context in the form of argumentative
relations of segments to the overall paper could
provide a skeleton by which to tailor sentence ex-
tracts to user expertise (as certain users or certain
tasks do not require certain types of information).
A system which uses such rhetorical zones to pro-
duce task—tailored extracts for medical articles, al-
beit on the basis of manually-segmented texts, is
given by Welions and Purcell (1999).
Another hard task is sentence extraction from
long texts, e.g. scientific journal articles of 20
pages of length, with a high compression. This
task is hard because one has to make decisions
about how the extracted sentences relate to each
other and how they relate to the overall message
of the text, before one can further compress them.
Rhetorical context of the kind described above is
very likely to make these decisions easier.
Secondly, it should also help improve citation
indexes, e.g. automatically derived ones like
Lawrence et al.&apos;s (1999) and Nanba and Oku-
mura&apos;s (1999). Citation indexes help organize sci-
entific online literature by linking cited (outgoing)
and citing (incoming) articles with a given text.
But these indexes are mainly &amp;quot;quantitative&amp;quot;, list-
ing other works without further qualifying whether
a reference to another work is there to extend the
</bodyText>
<page confidence="0.991531">
10
</page>
<table confidence="0.986417333333333">
AIM &amp;quot;We have proposed a method of clustering words based on large corpus data.&amp;quot;
TEXTUAL &amp;quot;Section 2 describes three unification-based parsers which are...&amp;quot;
OWN &amp;quot;We also compare with the English language and draw some conclusions on the benefits
of our approach.&amp;quot;
BACKGROUND &amp;quot;Part-of-speech tagging is the process of assigning grammatical categories to individual
words in a corpus.&amp;quot;
CONTRAST &amp;quot;However, no method for extracting the relationships from superficial linguistic ex-
pressions was described in their paper.&amp;quot;
BASIS &amp;quot;Our disambiguation method is based on the similarity of context vectors, which was
originated by Wilks et al. 1990.&amp;quot;
OTHER &amp;quot;Strzalkowski&apos;s Essential Arguments Approach (EAA) is a top-down approach to gen-
eration...&amp;quot;
</table>
<figureCaption confidence="0.990962">
Figure 3: Examples for Argumentative Zones
</figureCaption>
<bodyText confidence="0.999772">
earlier work, correct it, point out a weakness in
it, or just provide it as general background. This
&amp;quot;qualitative&amp;quot; information could be directly con-
tributed by our argumentative zones.
In this paper, we will describe the algorithm of
an argumentative zoner. The main focus of the
paper is the description of two features which are
particularly useful for attribution determination:
prototypical agents and actions.
</bodyText>
<sectionHeader confidence="0.872377" genericHeader="method">
2 Human Annotation of
Argumentative Zones
</sectionHeader>
<bodyText confidence="0.994693">
We have previously evaluated the scheme empiri-
cally by extensive experiments with three subjects,
over a range of 48 articles (Teufel et al., 1999).
</bodyText>
<listItem confidence="0.781445428571429">
• We measured stability (the degree to which the
same annotator will produce an annotation after
6 weeks) and reproducibility (the degree to which
two unrelated annotators will produce the same
annotation), using the Kappa coefficient K (Siegel
and Castellan, 1988; Carletta, 1996), which con-
trols agreement P(A) for chance agreement P(E):
</listItem>
<equation confidence="0.911405">
K P(A)—P(E)
Es
</equation>
<bodyText confidence="0.999885888888889">
Kappa is 0 for if agreement is only as would be
expected by chance annotation following the same
distribution as the observed distribution, and 1 for
perfect agreement. Values of Kappa surpassing
.8 are typically accepted as showing a very high
level of agreement (Krippendorff, 1980; Landis and
Koch, 1977).
Our experiments show that humans can distin-
guish own, other specific and other general work
with high stability (K=.83, .79, .81; N=1248; k=2,
where K stands for the Kappa coefficient, N for
the number of items (sentences) annotated and k
for the number of annotators) and reproducibil-
ity (K=.78, N=4031, k=3), corresponding to 94%,
93%, 93% (stability) and 93% (reproducibility)
agreement.
The full distinction into all seven categories of
the annotation scheme is slightly less stable and
reproducible (stability: K=.82, .81, .76; N=1220;
k=2 (equiv. to 93%, 92%, 90% agreement); repro-
ducibility: K=.71, N=4261, k=3 (equiv. to 87%
agreement)), but still in the range of what is gener-
ally accepted as reliable annotation. We conclude
from this that humans can distinguish attribution
and full argumentative zones, if trained. Human
annotation is used as training material in our sta-
tistical classifier.
</bodyText>
<sectionHeader confidence="0.931823" genericHeader="method">
3 Automatic Argumentative
Zoning
</sectionHeader>
<bodyText confidence="0.994754272727273">
As our task is not defined by topic coherence
like the related tasks of Morris and Hirst (1991),
Hearst (1997), Kan et al. (1998) and Reynar
(1999), we predict that keyword-based techniques
for automatic argumentative zoning will not work
well (cf. the results using text categorization as
described later). We decided to perform machine
learning, based on sentential features like the ones
used by sentence extraction. Argumentative zones
have properties which help us determine them on
the surface:
</bodyText>
<listItem confidence="0.914274434782609">
• Zones appear in typical positions in the article
(Myers, 1992); we model this with a set of
location features.
• Linguistic features like tense and voice cor-
relate with zones (Biber (1995) and Riley
(1991) show correlation for similar zones like
&amp;quot;method&amp;quot; and &amp;quot;introduction&amp;quot;). We model
this with syntactic features.
• Zones tend to follow particular other zones
(Swales, 1990); we model this with an ngram
model operating over sentences.
• Beginnings of attribution zones are linguisti-
cally marked by meta-discourse like &amp;quot;Other
researchers claim that&amp;quot; (Swales, 1990; Hy-
land, 1998); we model this with a specialized
agents and actions recognizer, and by recog-
nizing formal citations.
• Statements without explicit attribution are
interpreted as being of the same attribution
as previous sentences in the same segment of
attribution; we model this with a modified
agent feature which keeps track of previously
recognized agents.
</listItem>
<page confidence="0.994364">
11
</page>
<subsectionHeader confidence="0.999958">
3.1 Recognizing Agents and Actions
</subsectionHeader>
<bodyText confidence="0.999125083333333">
Paice (1981) introduces grammars for pattern
matching of indicator phrases, e.g. &amp;quot;the
aim/purpose of this paper/article/study&amp;quot; and &amp;quot;we
conclude/propose&amp;quot;. Such phrases can be useful
indicators of overall importance. However, for
our task, more flexible meta-discourse expressions
need to be determined. The description of a re-
search tradition, or the statement that the work
described in the paper is the continuation of some
other work, cover a wide range of syntactic and
lexical expressions and are too hard to find for a
mechanism like simple pattern matching.
</bodyText>
<figureCaption confidence="0.994056">
Figure 4: Agent Lexicon: 168 Patterns, 13 Classes
</figureCaption>
<bodyText confidence="0.995752630136986">
We suggest that the robust recognition of pro-
totypical agents and actions is one way out of this
dilemma. The agents we propose to recognize de-
scribe fixed role-players in the argumentation. In
Figure 1, prototypical agents are given in bold-
face (&amp;quot;Researchers in knowledge representation,
&amp;quot;Researcher-4&amp;quot; and &amp;quot;we&amp;quot;). We also propose pro-
totypical actions frequently occurring in scientific
discourse (shown underlined in Figure 1): the re-
searchers &amp;quot;agree&amp;quot;, Researcher-4 &amp;quot;suggested&amp;quot; some-
thing, the solution &amp;quot;cannot be used&amp;quot;.
We will now describe an algorithm which rec-
ognizes and classifies agents and actions. We
use a manually created lexicon for patterns for
agents, and a manually clustered verb lexicon for
the verbs. Figure 4 lists the agent types we dis-
tinguish. The main three types are US_AGENT,
THEM_AGENT and GENERAL-AGENT. A fourth
type is US_PREVIOUS-AGENT (the authors, but in
a previous paper).
Additional agent types include non-personal
agents like aims, problems, solutions, absence of
solution, or textual segments. There are four
equivalence classes of agents with ambiguous
reference ( &amp;quot;this system&amp;quot;), namely REF_US_AGENT,
THEM_PRONOUN_AGENT, AIM_REF_AGENT,
REF_AGENT. The total of 168 patterns in the
lexicon expands to many more as we use a replace
mechanism (@WORK_NOUN is expanded to
&amp;quot;paper, article, study, chapter&amp;quot; etc).
For verbs, we use a manually created the ac-
tion lexicon summarized in Figure 6. The verb
classes are based on semantic concepts such as
similarity, contrast, competition, presentation, ar-
gumentation and textual structure. For ex-
ample, PRESENTATION_ACTIONS include commu-
nication verbs like &amp;quot;present&amp;quot;, &amp;quot;report&amp;quot;, &amp;quot;state&amp;quot;
(Myers, 1992; Thompson and Yiyun, 1991), RE-
SEARCH_ACTIONS include &amp;quot;analyze&amp;quot;, &amp;quot;conduct&amp;quot;
and &amp;quot;observe&amp;quot;, and ARGUMENTATION_ACTIONS
&amp;quot;argue&amp;quot;, &amp;quot;disagree&amp;quot;, &amp;quot;object to&amp;quot;. Domain-specific
actions are contained in the classes indicating
a problem ( &amp;quot;degrade&amp;quot;, &amp;quot;overestimate&amp;quot;),
and solution-contributing actions (&amp;quot;&amp;quot;circumvent&amp;quot;,
solve&amp;quot;, &amp;quot;mitigate&amp;quot;).
The main reason for using a hand-crafted, genre-
specific lexicon instead of a general resource such
as WordNet or Levin&apos;s (1993) classes (as used in
Klavans and Kan (1998)), was to avoid polysemy
problems without having to perform word sense
disambiguation. Verbs in our texts often have a
specialized meaning in the domain of scientific ar-
gumentation, which our lexicon readily encodes.
We did notice some ambiguity problems (e.g. &amp;quot;fol-
low&amp;quot; can mean following another approach, or it
can mean follow in a sense having nothing to do
with presentation of research, e.g. following an
arc in an algorithm). In a wider domain, however,
ambiguity would be a much bigger problem.
Processing of the articles includes transforma-
tion from 14TEX into XML format, recognition
of formal citations and author names in running
text, tokenization, sentence separation and POS-
tagging. The pipeline uses the TTT software pro-
vided by the HCRC Language Technology Group
(Grover et al., 1999). The algorithm for deter-
mining agents in subject positions (or By-PPs in
passive sentences) is based on a finite automaton
which uses POS-input; cf. Figure 5.
In the case that more than one finite verb is
found in a sentence, the first finite verb which has
agents and/or actions in the sentences is used as
a value for that sentence.
</bodyText>
<sectionHeader confidence="0.999186" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999967714285714">
We carried out two evaluations. Evaluation A
tests whether all patterns were recognized as in-
tended by the algorithm, and whether patterns
were found that should not have been recognized.
Evaluation B tests how well agent and action
recognition helps us perform argumentative zon-
ing automatically.
</bodyText>
<subsectionHeader confidence="0.98926">
4.1 Evaluation A: Correctness
</subsectionHeader>
<bodyText confidence="0.9999694">
We first manually evaluated the error level of the
POS-Tagging of finite verbs, as our algorithm cru-
cially relies on finite verbs. In a random sample of
100 sentences from our corpus (containing a total
of 184 finite verbs), the tagger showed a recall of
</bodyText>
<figure confidence="0.999333066666667">
Agent Type Example
US_AGENT
THEM_AGENT
GENERAL_AGENT
US_PREVIOUS_AGENT
OUR_AIM_AGENT
REF_US_AGENT
REF_AGENT
THEM_PRONOUN_AGENT
AIM_FLEF_AGENT
GAP_AGENT
PROBLEM_AGENT
SOLUTION_AGENT
TEXTSTRUCTUFtE_AGENT
we
his approach
traditional methods
the approach given in
X (99)
the point of this study
this paper
the paper
they
its goal
none of these papers
these drawbacks
a way out of this
dilemma
the concluding chap-
ter
</figure>
<page confidence="0.88667">
12
</page>
<listItem confidence="0.999282066666667">
1. Start from the first finite verb in the sentence.
2. Check right context of the finite verb for verbal forms of interest which might make up more
complex tenses. Remain within the assumed clause boundaries; do not cross commas or other
finite verbs. Once the main verb of that construction (the &amp;quot;semantic&amp;quot; verb) has been found,
a simple morphological analysis determines its lemma; the tense and voice of the construction
follow from the succession of auxiliary verbs encountered.
3. Look up the lemma of semantic verb in Action Lexicon; return the associated Action Class if
successful. Else return Action 0.
4. Determine if one of the 32 fixed negation words contained in the lexicon (e.g. &amp;quot;not, don&apos;t,
neither&amp;quot;) is present within a fixed window of 6 to the right of the finite verb.
5. Search for the agent either as a by-PP to the right, or as a subject-NP to the left, depending on
the voice of the construction as determined in step 2. Remain within assumed clause boundaries.
6. If one of the Agent Patterns matches within that area in the sentence, return the Agent Type.
Else return Agent 0.
7. Repeat Steps 1-6 until there are no more finite verbs left.
</listItem>
<figureCaption confidence="0.815209928571429">
Figure 5: Algorithm for Agent and Action Detection
Action Type Example Action Type Example
AFFECT we hope to improve our results NEED this approach, however, lacks...
ARGUMENTATION we argue against a model of PRESENTATION we present here a method for...
AWARENESS we are not aware of attempts PROBLEM this approach fails...
BETTER_SOLUTION our system outperforms... RESEARCH we collected our data from...
CHANGE we extend &lt;CITE/&gt; &apos;s algo- SIMILAR our approach resembles that of
rithm
COMPARISON we tested our system against... SOLUTION we solve this problem by...
CONTINUATION we follow &lt;REF/&gt; ... TEXTSTRUCTURE the paper is organized...
CONTRAST our approach differs from .. . USE we employ &lt;REF/&gt; &apos;s method...
FUTURE_INTEREST we intend to improve ... COPULA our goal is to...
INTEREST we are concerned with . . . POSSESSION we have three goals...
Figure 6: Action Lexicon: 366 Verbs, 20 Classes
</figureCaption>
<bodyText confidence="0.997561148148148">
95% and a precision of 93%.
We found that for the 174 correctly determined
finite verbs (out of the total 184), the heuristics for
negation worked without any errors (100% accu-
racy). The correct semantic verb was determined
in 96% percent of all cases; errors are mostly due
to misrecognition of clause boundaries. Action
Type lookup was fully correct, even in the case
of phrasal verbs and longer idiomatic expressions
(&amp;quot;have to&amp;quot; is a NEED_ACTION; &amp;quot;be inspired by&amp;quot; is
a CONTINUE_ACTION). There were 7 voice errors,
2 of which were due to POS-tagging errors (past
participle misrecognized). The remaining 5 voice
errors correspond to a 98% accuracy. Figure 7
gives an example for a voice error (underlined) in
the output of the action/agent determination.
Correctness of Agent Type determination was
tested on a random sample of 100 sentences con-
taining at least one agent, resulting in 111 agents.
No agent pattern that should have been identi-
fied was missed (100% recall). Of the 111 agents,
105 cases were completely correct: the agent pat-
tern covered the complete grammatical subject or
by-PP intended (precision of 95%). There was one
complete error, caused by a POS-tagging error. In
5 of the 111 agents, the pattern covered only part
At the point where John &lt;ACTION
</bodyText>
<figure confidence="0.9581818">
TENSE=PRESENT VOICE=ACTIVE
MODAL=NOMODAL NEGATION=0
ACTIONTYPE=0&gt; knows &lt;/ACTION&gt; the truth
has been &lt;FINITE TENSE=PRESENT_PERFECT
VOICE=PASSIVE MODAL=NOMODAL NEGA-
TION=0 ACTIONTYPE=0&gt; processed
&lt;/ACTION&gt; , a complete clause will have
been &lt;ACTION TENSE=FUTURE_PERFECT
VOICE=ACTIVE MODAL=NOMODAL NEGA-
TION=0 ACTIONTYPE=0&gt; built &lt;/ACTION&gt;
</figure>
<figureCaption confidence="0.999935">
Figure 7: Sample Output of Action Detection
</figureCaption>
<bodyText confidence="0.999672714285714">
of a subject NP (typically the NP in a postmodify-
ing PP), as in the phrase &amp;quot;the problem with these
approaches&amp;quot; which was classified as REF_AGENT.
These cases (counted as errors) indeed constitute
no grave errors, as they still give an indication
which type of agents the nominal phrase is associ-
ated with.
</bodyText>
<page confidence="0.997685">
13
</page>
<subsectionHeader confidence="0.947188">
4.2 Evaluation B: Usefulness for
Argumentative Zoning
</subsectionHeader>
<bodyText confidence="0.99959675">
We evaluated the usefulness of the Agent and Ac-
tion features by measuring if they improve the
classification results of our stochastic classifier for
argumentative zones.
We use 14 features given in figure 8, some of
which are adapted from sentence extraction tech-
niques (Paice, 1990; Kupiec et al., 1995; Teufel and
Moens, 1999).
</bodyText>
<listItem confidence="0.997018642857143">
1. Absolute location of sentence in document
2. Relative location of sentence in section
3. Location of a sentence in paragraph
4. Presence of citations
5. Location of citations
6. Type of citations (self citation or not)
7. Type of headline
8. Presence of tf/idf key words
9. Presence of title words
10. Sentence length
11. Presence of modal auxiliaries
12. Tense of the finite verb
13. Voice of the finite verb
14. Presence of Formulaic Expressions
</listItem>
<figureCaption confidence="0.977783">
Figure 8: Other features used
</figureCaption>
<bodyText confidence="0.996993430107527">
All features except Citation Location and
Citation Type proved helpful for classification.
Two different statistical models were used: a Naive
Bayesian model as in Kupiec et al.&apos;s (1995) exper-
iment, cf. Figure 9, and an ngram model over sen-
tences, cf. Figure 10. Learning is supervised and
training examples are provided by our previous hu-
man annotation. Classification preceeds sentence
by sentence. The ngram model combines evidence
from the context (Cm-2, Cm-2) and from I senten-
tial features (Fm,o F171,1- 1), assuming that those
two factors are independent of each other. It uses
the same likelihood estimation as the Naive Bayes,
but maximises a context-sensitive prior using the
Viterbi algorithm. We received best results for
n=2, i.e. a bigram model.
The results of stochastic classification (pre-
sented in figure 11) were compiled with a 10-fold
cross-validation on our 80-paper corpus, contain-
ing a total of 12422 sentences (classified items).
As the first baseline, we use a standard text cat-
egorization method for classification (where each
sentence is considered as a document*) Baseline 1
has an accuracy of 69%, which is low considering
that the most frequent category (OwN) also cov-
ers 69% of all sentences. Worse still, the classifier
classifies almost all sentences as OWN and OTHER
segments (the most frequent categories). Recall on
the rare categories but important categories AIM,
TEXTUAL, CONTRAST and BASIS is zero Or very
low. Text classification is therefore not a solution.
*We used the Rainbow implementation of a Naive Bayes
tf/idf method, 10-fold cross-validation.
Baseline 2, the most frequent category (OWN),
is a particularly bad baseline: its recall on all cate-
gories except OWN is zero. We cannot see this bad
performance in the percentage accuracy values,
but only in the Kappa values (measured against
one human annotator, i.e. k=2). As Kappa takes
performance on rare categories into account more,
it is a more intuitive measure for our task.
In figure 11, NB refers to the Naive Bayes model,
and NB+ to the Naive Bayes model augmented
with the ngram model. We can see that the
stochastic models obtain substantial improvement
over the baselines, particularly with respect to pre-
cision and recall of the rare categories, raising re-
call considerably in all cases, while keeping preci-
sion at the same level as Baseline 1 or improving
it (exception: precision for BASIS drops; precision
for AIM is insignificantly lower).
If we look at the contribution of single features
(reported for the Naive Bayes system in figure 12),
we see that Agent and Action features improve
the overall performance of the system by .02 and
.04 Kappa points respectively (.36 to .38/.40).
This is a good performance for single features.
Agent is a strong feature beating both baselines.
Taken by itself, its performance at K=.08 is still
weaker than some other features in the pool, e.g.
the Headline feature (K=.19), the Citation fea-
ture (K=.18) and the Absolute Location Fea-
ture (K=.17). (Figure 12 reports classification re-
sults only for the stronger features, i.e. those who
are better than Baseline 2). The Action feature,
if considered on its own, is rather weak: it shows
a slightly better Kappa value than Baseline 2, but
does not even reach the level of random agreement
(K=0). Nevertheless, if taken together with the
other features, it still improves results.
Building on the idea that intellectual attribu-
tion is a segment-based phenomena, we improved
the Agent feature by including history (feature
SAgent). The assumption is that in unmarked sen-
tences the agent of the previous attribution is still
active. Wiebe (1994) also reports segment-based
agenthood as one of the most successful features.
SAgent alone achieved a classification success of
K=.21, which makes SAgent the best single fea-
tures available in the entire feature pool. Inclusion
of SAgent to the final model improved results to
K=.43 (bigram model).
Figure 12 also shows that different features are
better at disambiguating certain categories. The
Formulaic feature, which is not very strong on
its own, is the most diverse, as it contributes to
the disambiguation of six categories directly. Both
Agent and Action features disambiguate cate-
gories which many of the other 12 features cannot
disambiguate (e.g. CONTRAST), and SAgent addi-
tionally contributes towards the determination of
BACKGROUND zones (along with the Formulaic
and the Absolute Location feature).
</bodyText>
<page confidence="0.980232">
14
</page>
<table confidence="0.853008636363636">
P(Fir)
Fn -1) Ps-,&apos; P(C) 71-1
rij=0 P (Fi)
P(CIFo, . • , Probability that a sentence has target category C, given its feature values Fo, • • • ,
P(C): Fn—i;
P(FilC): (Overall) probability of category C);
Probability of feature-value pair Fj, given that the sentence is of target category C;
Probability of feature value Fi;
Figure 9: Naive Bayesian Classifier
fljgj P(Fm,j1Cm)
P(Cm1Pm,o, ,Cm—i) P(Cm1Cm C171-2)
F111110 P (Fm,i
m: index of sentence (mth sentence in text)
Cm: number of features considered
P(Cm1Pm,o, • • • , , , target category associated with sentence at index m
P(Cm1Cm-1 , Cm-2): Probability that sentence m has target category Cm, given its
P(Fm,j1Cm): feature values Fm,o, and given its context Co,
Probability that sentence in has target category C, given the cat-
egories of the two previous sentences;
Probability of feature-value pair Fi occurring within target cate-
gory C at position m;
Probability of feature value Fmj;
</table>
<figureCaption confidence="0.59859">
Figure 10: Bigram Model
</figureCaption>
<sectionHeader confidence="0.997881" genericHeader="discussions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999786680851064">
The result for automatic classification is in agree-
ment with our previous experimental results for
human classification: humans, too, recognize the
categories AIM and TEXTUAL most robustly (cf.
Figure 11). AIM and TEXTUAL sentences, stating
knowledge claims and organizing the text respec-
tively, are conventionalized to a high degree. The
system&apos;s results for AIM sentences, for instance,
compares favourably to similar sentence extraction
experiments (cf. Kupiec et al.&apos;s (1995) results of
42%/42% recall and precision for extracting &amp;quot;rel-
evant&amp;quot; sentences from scientific articles). BASIS
and CONTRAST sentences have a less prototypical
syntactic realization, and they also occur at less
predictable places in the document. Therefore, it
is far more difficult for both machine and human
to recognize such sentences.
While the system does well for ADA and TEX-
TUAL sentences, and provides substantial improve-
ment over both baselines, the difference to human
performance is still quite large (cf. figure 11). We
attribute most of this difference to the modest size
of our training corpus: 80 papers are not much for
machine learning of such high-level features. It is
possible that a more sophisticated model, in com-
bination with more training material, would im-
prove results significantly. However, when we ran
them on our data as it is now, different other sta-
tistical models, e.g. Ripper (Cohen, 1996) and a
Maximum Entropy model, all showed similar nu-
merical results.
Another factor which decreases results are in-
consistencies in the training data: we discovered
that 4% of the sentences with the same features
were classified differently by the human annota-
tion. This points to the fact that our set of fea-
tures could be made more distinctive. In most
of these cases, there were linguistic expressions
present, such as subtle signs of criticism, which
humans correctly identified, but for which the fea-
tures are too coarse. Therefore, the addition of
&amp;quot;deeper&amp;quot; features to the pool, which model the se-
mantics of the meta-discourse shallowly, seemed
a promising avenue. We consider the automatic
and robust recognition of agents and actions, as
presented here, to be the first incarnations of such
features.
</bodyText>
<sectionHeader confidence="0.999551" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.99997375">
Argumentative zoning is the task of breaking a
text containing a scientific argument into linear
zones of the same argumentative status, or zones
of the same intellectual attribution. We plan to
use argumentative zoning as a first step for IR and
shallow document understanding tasks like sum-
marization. In contrast to hierarchical segmenta-
tion (e.g. Marcu&apos;s (1997) work, which is based on
RST (Mann and Thompson, 1987)), this type of
segmentation aims at capturing the argumentative
status of a piece of text in respect to the overall
argumentative act of the paper. It does not deter-
</bodyText>
<page confidence="0.988945">
15
</page>
<table confidence="0.8322392">
Method Acc. K Precision/recall per category (in %)
(%) AIM CONTR. TXT. OWN BACKG. BASIS OTHER
Human Performance 87 F .i1J 72/56
50/55 79/79 94/92
68/75 82/34 74/
</table>
<figure confidence="0.79098625">
71
72
NB+ (best results)
NB (best results)
40/53 33/20 62/57 85/85 30/58 28/31 50/38
.41 42/60 34/22 61/60 82/90
.43
40/43 27/41 53/29
</figure>
<figureCaption confidence="0.92277575">
Basel. 1: Text categ. 69 .13 44/9 32/42 58/14 77/90 20/5 47/12 31/16
Basel, 2: Most freq. cat. 69 -.12 0/0 0/0 0/0 69/100 0/0 0/0 0/0
Figure 11: Accuracy, Kappa, Precision and Recall of Human and Automatic Processing, in comparison
to baselines
</figureCaption>
<table confidence="0.995742470588235">
Features used Acc. K Aim Precision/recall per category (in %) OTHER
(Naive Bayes System) (%) CONTR. TXT. OWN BACKG. BASIS
Action alone 68 -.11 0/0 43/1 0/0 68/99 0/0 0/0 0/0
Agent alone 67 .08 0/0 0/0 0/0 71/93 0/0 0/0 36/23
SAgent alone 70 .21 0/0 17/0 0/0 74/94 53/16 0/0 46/33
Abs. Location alone 70 .17 0/0 0/0 0/0 74/97 40/36 0/0 28/9
Headlines alone 69 .19 0/0 0/0 0/0 75/95 0/0 0/0 29/25
Citation alone 70 .18 0/0 0/0 0/0 73/96 0/0 0/0 43/30
Citation Type alone 70 .13 0/0 0/0 0/0 72/98 0/0 0/0 43/24
Citation Locat. alone 70 .13 0/0 0/0 0/0 72/97 0/0 0/0 43/24
Formulaic alone 70 .07 40/2 45/2 75/39 71/98 0/0 40/1 47/13
12 other features 71 .36 37/53 32/17 54/47 81/91 39/41 22/32 45/22
12 fea.+Act ion 71 .38 38/57 34/22 58/59 81/91 39/40 25/38 48/22
12 fea.+Agent 72 .40 40/57 35/18 59/51 82/91 39/43 25/34 52/29
12 fea.+SAgent 73 .40 39/57 33/19 61/51 81/91 42/43 25/33 52/29
12 fea.+Act ion+Agent 71 .43 40/53 33/20 62/57 85/85 30/58 28/31 50/38
12 fea.+Action+SAgent 73 .41 41/59 34/22 62/61 82/91 41/42 27/39 51/29
</table>
<figureCaption confidence="0.9120525">
Figure 12: Accuracy, Kappa, Precision and Recall of Automatic Processing (Naive Bayes system), per
individual features
</figureCaption>
<bodyText confidence="0.999675736842105">
mine the rhetorical structure within zones. Sub-
zone structure is most likely related to domain-
specific rhetorical relations which are not directly
relevant to the discourse-level relations we wish to
recognize.
We have presented a fully implemented proto-
type for argumentative zoning. Its main inno-
vation are two new features: prototypical agents
and actions - semi-shallow representations of the
overall scientific argumentation of the article. For
agent and action recognition, we use syntactic
heuristics and two extensive libraries of patterns.
Processing is robust and very low in error. We
evaluated the system without and with the agent
and action features and found that the features im-
prove results for automatic argumentative zoning
considerably. History-aware agents are the best
single feature in a large, extensively tested feature
pool.
</bodyText>
<sectionHeader confidence="0.999286" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9996818">
Biber, Douglas. 1995. Dimensions of Register Varia-
tion: A Cross-linguistic Comparison. Cambridge,
England: Cambridge University Press.
Carletta, Jean. 1996. Assessing agreement on classi-
fication tasks: The kappa statistic. Computational
Linguistics 22(2): 249-254. .
Cohen, William W. 1996. Learning trees and rules
with set-valued features. In Proceedings of AAAI-
96.
Grover, Claire, Andrei Milcheev, and Colin Mathe-
son. 1999. LT TTT Version 1.0: Text Tokenisa-
tion Software. Technical report, Human Commu-
nication Research Centre, University of Edinburgh.
http://www.ltg. ed. ac . uk/software/ttt/.
Hearst, Marti A. 1997. TextTiling: Segmenting text
into multi-paragraph subtopic passages. Computa-
tional Linguistics 23(1): 33-64.
Hyland, Ken. 1998. Persuasion and context: The prag-
matics of academic metadiscourse. Journal of Prag-
matics 30(4): 437-455.
Kan, Min-Yen, Judith L. Klavans, and Kathleen R.
McKeown. 1998. Linear Segmentation and Segment
Significance. In Proceedings of the Sixth Workshop
on Very Large Corpora (COLN C/A CL-98), 197-
205.
Klavans, Judith L., and Min-Yen Kan. 1998. Role
of verbs in document analysis. In Proceedings
of 36th Annual Meeting of the Association for
Computational Linguistics and the 17th Interna-
tional Conference on Computational Linguistics
(ACL/COLING-98), 680-686.
Krippendorff, Klaus. 1980. Content Analysis: An In-
troduction to its Methodology. Beverly Rills, CA:
Sage Publications.
Kupiec, Julian, Jan 0. Pedersen, and Francine Chen.
</reference>
<page confidence="0.971125">
16
</page>
<reference confidence="0.999825716049383">
1995. A trainable document summarizer. In Pro-
ceedings of the 18th Annual International Confer-
ence on Research and Development in Information
Retrieval (SIGIR-95), 68-73.
Landis, J.R., and G.G. Koch. 1977. The Measurement
of Observer Agreement for Categorical Data. Bio-
metrics 33: 159-174.
Lawrence, Steve, C. Lee Giles, and Kurt Bollacker.
1999. Digital libraries and autonomous citation in-
dexing. IEEE Computer 32(6): 67-71.
Levin, Beth. 1993. English Verb Classes and Alterna-
tions. Chicago, IL: University of Chicago Press.
Mann, William C., and Sandra A. Thompson. 1987.
Rhetorical Structure Theory: Description and Con-
struction of text structures. In Gerard Kempen,
ed., Natural Language Generation: New Results in
Artificial Intelligence, Psychology, and Linguistics,
85-95. Dordrecht, NL: Marinus Nijhoff Publishers.
Marcu, Daniel. 1997. From Discourse Structures to
Text Summaries. In Inderjeet Mani and Mark T.
Maybury, eds., Proceedings of the ACL/EACL-97
Workshop on Intelligent Scalable Text Summariza-
tion, 82-88.
Morris, Jane, and Graeme Hirst. 1991. Lexical cohe-
sion computed by thesaural relations as an indicator
of the structure of text. Computational Linguistics
17: 21-48.
Myers, Greg. 1992. In this paper we report...-speech
acts and scientific facts. Journal of Pragmatics
17(4): 295-313.
Nanba, Hidetsugu, and Manabu Okumura. 1999. To-
wards multi-paper summarization using reference
information. In Proceedings of IJCAI-99, 926-
931. http: //galaga. jaist .ac . jp: 8000/-nanba/
study/papers .html.
Paice, Chris D. 1981. The automatic generation of
literary abstracts: an approach based on the iden-
tification of self-indicating phrases. In Robert Nor-
man Oddy, Stephen E. Robertson, Cornelis Joost
van Rijsbergen, and P. W. Williams, eds., Infor-
mation Retrieval Research, 172-191. London, UK:
Butterworth.
Paice, Chris D. 1990. Constructing literature abstracts
by computer: techniques and prospects. Informa-
tion Processing and Management 26: 171-186.
Reynar, Jeffrey C. 1999. Statistical models for topic
segmentation. In Proceedings of the 37th Annual
Meeting of the Association for Computational Lin-
guistics (ACL-99), 357-364.
Riley, Kathryn. 1991. Passive voice and rhetorical role
in scientific writing. Journal of Technical Writing
and Communication 21(3): 239-257.
Rowley, Jennifer. 1982. Abstracting and Indexing.
London, UK: Bingley.
Siegel, Sidney, and N. John Jr. Castellan. 1988. Non-
parametric Statistics for the Behavioral Sciences.
Berkeley, CA: McGraw-Hill, 2nd edn.
Swales, John. 1990. Genre Analysis: English in Aca-
demic and Research Settings. Chapter 7: Research
articles in English, 110-176. Cambridge, UK: Cam-
bridge University Press.
Teufel, Simone, Jean Carletta, and Marc Moens. 1999.
An annotation scheme for discourse-level argumen-
tation in research articles. In Proceedings of the 8th
Meeting of the European Chapter of the Association
for Computational Linguistics (EA CL-99), 110-117.
Teufel, Simone, and Marc Moens. 1999. Argumenta-
tive classification of extracted sentences as a first
step towards flexible abstracting. In Inderjeet Mani
and Mark T. Maybury, eds., Advances in Auto-
matic Text Summarization, 155-171. Cambridge,
MA: MIT Press.
Thompson, Geoff, and Ye Yiyun. 1991. Evaluation in
the reporting verbs used in academic papers. Ap-
plied Linguistics 12(4): 365-382.
Wellons, M. E., and G. P. Purcell. 1999. Task-specific
extracts for using the medical literature. In Pro-
ceedings of the American Medical Informatics Sym-
posium, 1004-1008.
Wiebe, Janyce. 1994. Tracking point of view in narra-
tive. Computational Linguistics 20(2): 223-287.
</reference>
<page confidence="0.999409">
17
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.879135">
<title confidence="0.9674235">What&apos;s yours and what&apos;s mine: Determining Attribution in Scientific Text</title>
<author confidence="0.996916">Simone Teufelt Marc Moens</author>
<affiliation confidence="0.999922">Computer Science Department HCRC Language Technology Group Columbia University University of Edinburgh</affiliation>
<email confidence="0.958132">teufelOcs.columbia.eduMarc.MoensOed.ac.uk</email>
<abstract confidence="0.99896075">We believe that identifying the structure of scientific argumentation in articles can help in tasks such as automatic summarization or the automated construction of citation indexes. One particularly important aspect of this structure is the question of who a given scientific statement is attributed to: other researchers, the field in general, or the authors themselves. We present the algorithm and a systematic evaluation of a system which can recognize the most salient textual properties that contribute to the global argumentative structure of a text. In this paper we concentrate on two particular features, namely the occurrences of prototypical agents and their actions in scientific text.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Douglas Biber</author>
</authors>
<title>Dimensions of Register Variation: A Cross-linguistic Comparison.</title>
<date>1995</date>
<publisher>Cambridge University Press.</publisher>
<location>Cambridge, England:</location>
<contexts>
<context position="11584" citStr="Biber (1995)" startWordPosition="1775" endWordPosition="1776">and Hirst (1991), Hearst (1997), Kan et al. (1998) and Reynar (1999), we predict that keyword-based techniques for automatic argumentative zoning will not work well (cf. the results using text categorization as described later). We decided to perform machine learning, based on sentential features like the ones used by sentence extraction. Argumentative zones have properties which help us determine them on the surface: • Zones appear in typical positions in the article (Myers, 1992); we model this with a set of location features. • Linguistic features like tense and voice correlate with zones (Biber (1995) and Riley (1991) show correlation for similar zones like &amp;quot;method&amp;quot; and &amp;quot;introduction&amp;quot;). We model this with syntactic features. • Zones tend to follow particular other zones (Swales, 1990); we model this with an ngram model operating over sentences. • Beginnings of attribution zones are linguistically marked by meta-discourse like &amp;quot;Other researchers claim that&amp;quot; (Swales, 1990; Hyland, 1998); we model this with a specialized agents and actions recognizer, and by recognizing formal citations. • Statements without explicit attribution are interpreted as being of the same attribution as previous sen</context>
</contexts>
<marker>Biber, 1995</marker>
<rawString>Biber, Douglas. 1995. Dimensions of Register Variation: A Cross-linguistic Comparison. Cambridge, England: Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Carletta</author>
</authors>
<title>Assessing agreement on classification tasks: The kappa statistic.</title>
<date>1996</date>
<journal>Computational Linguistics</journal>
<volume>22</volume>
<issue>2</issue>
<pages>249--254</pages>
<contexts>
<context position="9573" citStr="Carletta, 1996" startWordPosition="1458" endWordPosition="1459"> of the paper is the description of two features which are particularly useful for attribution determination: prototypical agents and actions. 2 Human Annotation of Argumentative Zones We have previously evaluated the scheme empirically by extensive experiments with three subjects, over a range of 48 articles (Teufel et al., 1999). • We measured stability (the degree to which the same annotator will produce an annotation after 6 weeks) and reproducibility (the degree to which two unrelated annotators will produce the same annotation), using the Kappa coefficient K (Siegel and Castellan, 1988; Carletta, 1996), which controls agreement P(A) for chance agreement P(E): K P(A)—P(E) Es Kappa is 0 for if agreement is only as would be expected by chance annotation following the same distribution as the observed distribution, and 1 for perfect agreement. Values of Kappa surpassing .8 are typically accepted as showing a very high level of agreement (Krippendorff, 1980; Landis and Koch, 1977). Our experiments show that humans can distinguish own, other specific and other general work with high stability (K=.83, .79, .81; N=1248; k=2, where K stands for the Kappa coefficient, N for the number of items (sente</context>
</contexts>
<marker>Carletta, 1996</marker>
<rawString>Carletta, Jean. 1996. Assessing agreement on classification tasks: The kappa statistic. Computational Linguistics 22(2): 249-254. .</rawString>
</citation>
<citation valid="true">
<authors>
<author>William W Cohen</author>
</authors>
<title>Learning trees and rules with set-valued features.</title>
<date>1996</date>
<booktitle>In Proceedings of AAAI96.</booktitle>
<contexts>
<context position="29026" citStr="Cohen, 1996" startWordPosition="4534" endWordPosition="4535"> recognize such sentences. While the system does well for ADA and TEXTUAL sentences, and provides substantial improvement over both baselines, the difference to human performance is still quite large (cf. figure 11). We attribute most of this difference to the modest size of our training corpus: 80 papers are not much for machine learning of such high-level features. It is possible that a more sophisticated model, in combination with more training material, would improve results significantly. However, when we ran them on our data as it is now, different other statistical models, e.g. Ripper (Cohen, 1996) and a Maximum Entropy model, all showed similar numerical results. Another factor which decreases results are inconsistencies in the training data: we discovered that 4% of the sentences with the same features were classified differently by the human annotation. This points to the fact that our set of features could be made more distinctive. In most of these cases, there were linguistic expressions present, such as subtle signs of criticism, which humans correctly identified, but for which the features are too coarse. Therefore, the addition of &amp;quot;deeper&amp;quot; features to the pool, which model the s</context>
</contexts>
<marker>Cohen, 1996</marker>
<rawString>Cohen, William W. 1996. Learning trees and rules with set-valued features. In Proceedings of AAAI96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claire Grover</author>
<author>Andrei Milcheev</author>
<author>Colin Matheson</author>
</authors>
<title>LT TTT Version 1.0: Text Tokenisation Software.</title>
<date>1999</date>
<tech>Technical report,</tech>
<editor>http://www.ltg. ed. ac . uk/software/ttt/.</editor>
<institution>Human Communication Research Centre, University of Edinburgh.</institution>
<contexts>
<context position="15943" citStr="Grover et al., 1999" startWordPosition="2428" endWordPosition="2431">, which our lexicon readily encodes. We did notice some ambiguity problems (e.g. &amp;quot;follow&amp;quot; can mean following another approach, or it can mean follow in a sense having nothing to do with presentation of research, e.g. following an arc in an algorithm). In a wider domain, however, ambiguity would be a much bigger problem. Processing of the articles includes transformation from 14TEX into XML format, recognition of formal citations and author names in running text, tokenization, sentence separation and POStagging. The pipeline uses the TTT software provided by the HCRC Language Technology Group (Grover et al., 1999). The algorithm for determining agents in subject positions (or By-PPs in passive sentences) is based on a finite automaton which uses POS-input; cf. Figure 5. In the case that more than one finite verb is found in a sentence, the first finite verb which has agents and/or actions in the sentences is used as a value for that sentence. 4 Evaluation We carried out two evaluations. Evaluation A tests whether all patterns were recognized as intended by the algorithm, and whether patterns were found that should not have been recognized. Evaluation B tests how well agent and action recognition helps </context>
</contexts>
<marker>Grover, Milcheev, Matheson, 1999</marker>
<rawString>Grover, Claire, Andrei Milcheev, and Colin Matheson. 1999. LT TTT Version 1.0: Text Tokenisation Software. Technical report, Human Communication Research Centre, University of Edinburgh. http://www.ltg. ed. ac . uk/software/ttt/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>TextTiling: Segmenting text into multi-paragraph subtopic passages.</title>
<date>1997</date>
<journal>Computational Linguistics</journal>
<volume>23</volume>
<issue>1</issue>
<pages>33--64</pages>
<contexts>
<context position="11003" citStr="Hearst (1997)" startWordPosition="1684" endWordPosition="1685">ies of the annotation scheme is slightly less stable and reproducible (stability: K=.82, .81, .76; N=1220; k=2 (equiv. to 93%, 92%, 90% agreement); reproducibility: K=.71, N=4261, k=3 (equiv. to 87% agreement)), but still in the range of what is generally accepted as reliable annotation. We conclude from this that humans can distinguish attribution and full argumentative zones, if trained. Human annotation is used as training material in our statistical classifier. 3 Automatic Argumentative Zoning As our task is not defined by topic coherence like the related tasks of Morris and Hirst (1991), Hearst (1997), Kan et al. (1998) and Reynar (1999), we predict that keyword-based techniques for automatic argumentative zoning will not work well (cf. the results using text categorization as described later). We decided to perform machine learning, based on sentential features like the ones used by sentence extraction. Argumentative zones have properties which help us determine them on the surface: • Zones appear in typical positions in the article (Myers, 1992); we model this with a set of location features. • Linguistic features like tense and voice correlate with zones (Biber (1995) and Riley (1991) s</context>
</contexts>
<marker>Hearst, 1997</marker>
<rawString>Hearst, Marti A. 1997. TextTiling: Segmenting text into multi-paragraph subtopic passages. Computational Linguistics 23(1): 33-64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken Hyland</author>
</authors>
<title>Persuasion and context: The pragmatics of academic metadiscourse.</title>
<date>1998</date>
<journal>Journal of Pragmatics</journal>
<volume>30</volume>
<issue>4</issue>
<pages>437--455</pages>
<contexts>
<context position="3796" citStr="Hyland, 1998" startWordPosition="560" endWordPosition="561"> to different factors: subdomain (theoretical linguistics, statistical NLP, logic programming, computational psycholinguistics), types of research (implementation, review, evaluation, empirical vs. theoretical research), writing style (formal vs. informal) and presentational styles (fixed section structure of type Introduction—Method—Results—Conclusion vs. more idiosyncratic, problem-structured presentation). One thing, however, is constant across all articles: the argumentative aim of every single article is to show that the given work is a contribution to science (Swales, 1990; Myers, 1992; Hyland, 1998). Theories of scientific argumentation in research articles stress that authors follow well-predictable stages of argumentation, as in the fictional introduction in figure 1. 9 Are the scientific statements expressed in this sentence attributed to the authors, the general field, or specific other researchers? Own work Other Work BACKGROUND Does this sentence contain material Does it describe a negative aspect that describes the specific aim of the other work, or a contrast of the paper? or comparison of the own work to it? NO NO Does this sentence make Does this sentence mention reference to t</context>
<context position="11975" citStr="Hyland, 1998" startWordPosition="1833" endWordPosition="1835"> determine them on the surface: • Zones appear in typical positions in the article (Myers, 1992); we model this with a set of location features. • Linguistic features like tense and voice correlate with zones (Biber (1995) and Riley (1991) show correlation for similar zones like &amp;quot;method&amp;quot; and &amp;quot;introduction&amp;quot;). We model this with syntactic features. • Zones tend to follow particular other zones (Swales, 1990); we model this with an ngram model operating over sentences. • Beginnings of attribution zones are linguistically marked by meta-discourse like &amp;quot;Other researchers claim that&amp;quot; (Swales, 1990; Hyland, 1998); we model this with a specialized agents and actions recognizer, and by recognizing formal citations. • Statements without explicit attribution are interpreted as being of the same attribution as previous sentences in the same segment of attribution; we model this with a modified agent feature which keeps track of previously recognized agents. 11 3.1 Recognizing Agents and Actions Paice (1981) introduces grammars for pattern matching of indicator phrases, e.g. &amp;quot;the aim/purpose of this paper/article/study&amp;quot; and &amp;quot;we conclude/propose&amp;quot;. Such phrases can be useful indicators of overall importance. </context>
</contexts>
<marker>Hyland, 1998</marker>
<rawString>Hyland, Ken. 1998. Persuasion and context: The pragmatics of academic metadiscourse. Journal of Pragmatics 30(4): 437-455.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min-Yen Kan</author>
<author>Judith L Klavans</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Linear Segmentation and Segment Significance.</title>
<date>1998</date>
<booktitle>In Proceedings of the Sixth Workshop on Very Large Corpora (COLN C/A CL-98),</booktitle>
<pages>197--205</pages>
<contexts>
<context position="11022" citStr="Kan et al. (1998)" startWordPosition="1686" endWordPosition="1689">tation scheme is slightly less stable and reproducible (stability: K=.82, .81, .76; N=1220; k=2 (equiv. to 93%, 92%, 90% agreement); reproducibility: K=.71, N=4261, k=3 (equiv. to 87% agreement)), but still in the range of what is generally accepted as reliable annotation. We conclude from this that humans can distinguish attribution and full argumentative zones, if trained. Human annotation is used as training material in our statistical classifier. 3 Automatic Argumentative Zoning As our task is not defined by topic coherence like the related tasks of Morris and Hirst (1991), Hearst (1997), Kan et al. (1998) and Reynar (1999), we predict that keyword-based techniques for automatic argumentative zoning will not work well (cf. the results using text categorization as described later). We decided to perform machine learning, based on sentential features like the ones used by sentence extraction. Argumentative zones have properties which help us determine them on the surface: • Zones appear in typical positions in the article (Myers, 1992); we model this with a set of location features. • Linguistic features like tense and voice correlate with zones (Biber (1995) and Riley (1991) show correlation for</context>
</contexts>
<marker>Kan, Klavans, McKeown, 1998</marker>
<rawString>Kan, Min-Yen, Judith L. Klavans, and Kathleen R. McKeown. 1998. Linear Segmentation and Segment Significance. In Proceedings of the Sixth Workshop on Very Large Corpora (COLN C/A CL-98), 197-205.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judith L Klavans</author>
<author>Min-Yen Kan</author>
</authors>
<title>Role of verbs in document analysis.</title>
<date>1998</date>
<booktitle>In Proceedings of 36th Annual Meeting of the Association for Computational Linguistics and the 17th International Conference on Computational Linguistics (ACL/COLING-98),</booktitle>
<pages>680--686</pages>
<contexts>
<context position="15143" citStr="Klavans and Kan (1998)" startWordPosition="2300" endWordPosition="2303">textual structure. For example, PRESENTATION_ACTIONS include communication verbs like &amp;quot;present&amp;quot;, &amp;quot;report&amp;quot;, &amp;quot;state&amp;quot; (Myers, 1992; Thompson and Yiyun, 1991), RESEARCH_ACTIONS include &amp;quot;analyze&amp;quot;, &amp;quot;conduct&amp;quot; and &amp;quot;observe&amp;quot;, and ARGUMENTATION_ACTIONS &amp;quot;argue&amp;quot;, &amp;quot;disagree&amp;quot;, &amp;quot;object to&amp;quot;. Domain-specific actions are contained in the classes indicating a problem ( &amp;quot;degrade&amp;quot;, &amp;quot;overestimate&amp;quot;), and solution-contributing actions (&amp;quot;&amp;quot;circumvent&amp;quot;, solve&amp;quot;, &amp;quot;mitigate&amp;quot;). The main reason for using a hand-crafted, genrespecific lexicon instead of a general resource such as WordNet or Levin&apos;s (1993) classes (as used in Klavans and Kan (1998)), was to avoid polysemy problems without having to perform word sense disambiguation. Verbs in our texts often have a specialized meaning in the domain of scientific argumentation, which our lexicon readily encodes. We did notice some ambiguity problems (e.g. &amp;quot;follow&amp;quot; can mean following another approach, or it can mean follow in a sense having nothing to do with presentation of research, e.g. following an arc in an algorithm). In a wider domain, however, ambiguity would be a much bigger problem. Processing of the articles includes transformation from 14TEX into XML format, recognition of form</context>
</contexts>
<marker>Klavans, Kan, 1998</marker>
<rawString>Klavans, Judith L., and Min-Yen Kan. 1998. Role of verbs in document analysis. In Proceedings of 36th Annual Meeting of the Association for Computational Linguistics and the 17th International Conference on Computational Linguistics (ACL/COLING-98), 680-686.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Krippendorff</author>
</authors>
<title>Content Analysis: An Introduction to its Methodology. Beverly Rills,</title>
<date>1980</date>
<publisher>Sage Publications.</publisher>
<location>CA:</location>
<contexts>
<context position="9930" citStr="Krippendorff, 1980" startWordPosition="1516" endWordPosition="1517">ity (the degree to which the same annotator will produce an annotation after 6 weeks) and reproducibility (the degree to which two unrelated annotators will produce the same annotation), using the Kappa coefficient K (Siegel and Castellan, 1988; Carletta, 1996), which controls agreement P(A) for chance agreement P(E): K P(A)—P(E) Es Kappa is 0 for if agreement is only as would be expected by chance annotation following the same distribution as the observed distribution, and 1 for perfect agreement. Values of Kappa surpassing .8 are typically accepted as showing a very high level of agreement (Krippendorff, 1980; Landis and Koch, 1977). Our experiments show that humans can distinguish own, other specific and other general work with high stability (K=.83, .79, .81; N=1248; k=2, where K stands for the Kappa coefficient, N for the number of items (sentences) annotated and k for the number of annotators) and reproducibility (K=.78, N=4031, k=3), corresponding to 94%, 93%, 93% (stability) and 93% (reproducibility) agreement. The full distinction into all seven categories of the annotation scheme is slightly less stable and reproducible (stability: K=.82, .81, .76; N=1220; k=2 (equiv. to 93%, 92%, 90% agre</context>
</contexts>
<marker>Krippendorff, 1980</marker>
<rawString>Krippendorff, Klaus. 1980. Content Analysis: An Introduction to its Methodology. Beverly Rills, CA: Sage Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pedersen</author>
<author>Francine Chen</author>
</authors>
<title>A trainable document summarizer.</title>
<date>1995</date>
<booktitle>In Proceedings of the 18th Annual International Conference on Research and Development in Information Retrieval (SIGIR-95),</booktitle>
<pages>68--73</pages>
<marker>Pedersen, Chen, 1995</marker>
<rawString>Kupiec, Julian, Jan 0. Pedersen, and Francine Chen. 1995. A trainable document summarizer. In Proceedings of the 18th Annual International Conference on Research and Development in Information Retrieval (SIGIR-95), 68-73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Landis</author>
<author>G G Koch</author>
</authors>
<title>The Measurement of Observer Agreement for Categorical Data.</title>
<date>1977</date>
<journal>Biometrics</journal>
<volume>33</volume>
<pages>159--174</pages>
<contexts>
<context position="9954" citStr="Landis and Koch, 1977" startWordPosition="1518" endWordPosition="1521">hich the same annotator will produce an annotation after 6 weeks) and reproducibility (the degree to which two unrelated annotators will produce the same annotation), using the Kappa coefficient K (Siegel and Castellan, 1988; Carletta, 1996), which controls agreement P(A) for chance agreement P(E): K P(A)—P(E) Es Kappa is 0 for if agreement is only as would be expected by chance annotation following the same distribution as the observed distribution, and 1 for perfect agreement. Values of Kappa surpassing .8 are typically accepted as showing a very high level of agreement (Krippendorff, 1980; Landis and Koch, 1977). Our experiments show that humans can distinguish own, other specific and other general work with high stability (K=.83, .79, .81; N=1248; k=2, where K stands for the Kappa coefficient, N for the number of items (sentences) annotated and k for the number of annotators) and reproducibility (K=.78, N=4031, k=3), corresponding to 94%, 93%, 93% (stability) and 93% (reproducibility) agreement. The full distinction into all seven categories of the annotation scheme is slightly less stable and reproducible (stability: K=.82, .81, .76; N=1220; k=2 (equiv. to 93%, 92%, 90% agreement); reproducibility:</context>
</contexts>
<marker>Landis, Koch, 1977</marker>
<rawString>Landis, J.R., and G.G. Koch. 1977. The Measurement of Observer Agreement for Categorical Data. Biometrics 33: 159-174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve Lawrence</author>
<author>C Lee Giles</author>
<author>Kurt Bollacker</author>
</authors>
<title>Digital libraries and autonomous citation indexing.</title>
<date>1999</date>
<journal>IEEE Computer</journal>
<volume>32</volume>
<issue>6</issue>
<pages>67--71</pages>
<marker>Lawrence, Giles, Bollacker, 1999</marker>
<rawString>Lawrence, Steve, C. Lee Giles, and Kurt Bollacker. 1999. Digital libraries and autonomous citation indexing. IEEE Computer 32(6): 67-71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Levin</author>
</authors>
<title>English Verb Classes and Alternations.</title>
<date>1993</date>
<publisher>University of Chicago Press.</publisher>
<location>Chicago, IL:</location>
<marker>Levin, 1993</marker>
<rawString>Levin, Beth. 1993. English Verb Classes and Alternations. Chicago, IL: University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Mann</author>
<author>Sandra A Thompson</author>
</authors>
<title>Rhetorical Structure Theory: Description and Construction of text structures.</title>
<date>1987</date>
<booktitle>Natural Language Generation: New Results in Artificial Intelligence, Psychology, and Linguistics,</booktitle>
<pages>85--95</pages>
<editor>In Gerard Kempen, ed.,</editor>
<publisher>Marinus Nijhoff Publishers.</publisher>
<location>Dordrecht, NL:</location>
<contexts>
<context position="30266" citStr="Mann and Thompson, 1987" startWordPosition="4732" endWordPosition="4735">meta-discourse shallowly, seemed a promising avenue. We consider the automatic and robust recognition of agents and actions, as presented here, to be the first incarnations of such features. 6 Conclusions Argumentative zoning is the task of breaking a text containing a scientific argument into linear zones of the same argumentative status, or zones of the same intellectual attribution. We plan to use argumentative zoning as a first step for IR and shallow document understanding tasks like summarization. In contrast to hierarchical segmentation (e.g. Marcu&apos;s (1997) work, which is based on RST (Mann and Thompson, 1987)), this type of segmentation aims at capturing the argumentative status of a piece of text in respect to the overall argumentative act of the paper. It does not deter15 Method Acc. K Precision/recall per category (in %) (%) AIM CONTR. TXT. OWN BACKG. BASIS OTHER Human Performance 87 F .i1J 72/56 50/55 79/79 94/92 68/75 82/34 74/ 71 72 NB+ (best results) NB (best results) 40/53 33/20 62/57 85/85 30/58 28/31 50/38 .41 42/60 34/22 61/60 82/90 .43 40/43 27/41 53/29 Basel. 1: Text categ. 69 .13 44/9 32/42 58/14 77/90 20/5 47/12 31/16 Basel, 2: Most freq. cat. 69 -.12 0/0 0/0 0/0 69/100 0/0 0/0 0/0 </context>
</contexts>
<marker>Mann, Thompson, 1987</marker>
<rawString>Mann, William C., and Sandra A. Thompson. 1987. Rhetorical Structure Theory: Description and Construction of text structures. In Gerard Kempen, ed., Natural Language Generation: New Results in Artificial Intelligence, Psychology, and Linguistics, 85-95. Dordrecht, NL: Marinus Nijhoff Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
</authors>
<title>From Discourse Structures to Text Summaries.</title>
<date>1997</date>
<booktitle>Proceedings of the ACL/EACL-97 Workshop on Intelligent Scalable Text Summarization,</booktitle>
<pages>82--88</pages>
<editor>In Inderjeet Mani and Mark T. Maybury, eds.,</editor>
<marker>Marcu, 1997</marker>
<rawString>Marcu, Daniel. 1997. From Discourse Structures to Text Summaries. In Inderjeet Mani and Mark T. Maybury, eds., Proceedings of the ACL/EACL-97 Workshop on Intelligent Scalable Text Summarization, 82-88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane Morris</author>
<author>Graeme Hirst</author>
</authors>
<title>Lexical cohesion computed by thesaural relations as an indicator of the structure of text.</title>
<date>1991</date>
<journal>Computational Linguistics</journal>
<volume>17</volume>
<pages>21--48</pages>
<contexts>
<context position="10988" citStr="Morris and Hirst (1991)" startWordPosition="1680" endWordPosition="1683">on into all seven categories of the annotation scheme is slightly less stable and reproducible (stability: K=.82, .81, .76; N=1220; k=2 (equiv. to 93%, 92%, 90% agreement); reproducibility: K=.71, N=4261, k=3 (equiv. to 87% agreement)), but still in the range of what is generally accepted as reliable annotation. We conclude from this that humans can distinguish attribution and full argumentative zones, if trained. Human annotation is used as training material in our statistical classifier. 3 Automatic Argumentative Zoning As our task is not defined by topic coherence like the related tasks of Morris and Hirst (1991), Hearst (1997), Kan et al. (1998) and Reynar (1999), we predict that keyword-based techniques for automatic argumentative zoning will not work well (cf. the results using text categorization as described later). We decided to perform machine learning, based on sentential features like the ones used by sentence extraction. Argumentative zones have properties which help us determine them on the surface: • Zones appear in typical positions in the article (Myers, 1992); we model this with a set of location features. • Linguistic features like tense and voice correlate with zones (Biber (1995) and</context>
</contexts>
<marker>Morris, Hirst, 1991</marker>
<rawString>Morris, Jane, and Graeme Hirst. 1991. Lexical cohesion computed by thesaural relations as an indicator of the structure of text. Computational Linguistics 17: 21-48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Myers</author>
</authors>
<title>In this paper we report...-speech acts and scientific facts.</title>
<date>1992</date>
<journal>Journal of Pragmatics</journal>
<volume>17</volume>
<issue>4</issue>
<pages>295--313</pages>
<contexts>
<context position="3781" citStr="Myers, 1992" startWordPosition="558" endWordPosition="559"> with respect to different factors: subdomain (theoretical linguistics, statistical NLP, logic programming, computational psycholinguistics), types of research (implementation, review, evaluation, empirical vs. theoretical research), writing style (formal vs. informal) and presentational styles (fixed section structure of type Introduction—Method—Results—Conclusion vs. more idiosyncratic, problem-structured presentation). One thing, however, is constant across all articles: the argumentative aim of every single article is to show that the given work is a contribution to science (Swales, 1990; Myers, 1992; Hyland, 1998). Theories of scientific argumentation in research articles stress that authors follow well-predictable stages of argumentation, as in the fictional introduction in figure 1. 9 Are the scientific statements expressed in this sentence attributed to the authors, the general field, or specific other researchers? Own work Other Work BACKGROUND Does this sentence contain material Does it describe a negative aspect that describes the specific aim of the other work, or a contrast of the paper? or comparison of the own work to it? NO NO Does this sentence make Does this sentence mention</context>
<context position="11458" citStr="Myers, 1992" startWordPosition="1753" endWordPosition="1754">l classifier. 3 Automatic Argumentative Zoning As our task is not defined by topic coherence like the related tasks of Morris and Hirst (1991), Hearst (1997), Kan et al. (1998) and Reynar (1999), we predict that keyword-based techniques for automatic argumentative zoning will not work well (cf. the results using text categorization as described later). We decided to perform machine learning, based on sentential features like the ones used by sentence extraction. Argumentative zones have properties which help us determine them on the surface: • Zones appear in typical positions in the article (Myers, 1992); we model this with a set of location features. • Linguistic features like tense and voice correlate with zones (Biber (1995) and Riley (1991) show correlation for similar zones like &amp;quot;method&amp;quot; and &amp;quot;introduction&amp;quot;). We model this with syntactic features. • Zones tend to follow particular other zones (Swales, 1990); we model this with an ngram model operating over sentences. • Beginnings of attribution zones are linguistically marked by meta-discourse like &amp;quot;Other researchers claim that&amp;quot; (Swales, 1990; Hyland, 1998); we model this with a specialized agents and actions recognizer, and by recognizin</context>
<context position="14648" citStr="Myers, 1992" startWordPosition="2237" endWordPosition="2238">es of agents with ambiguous reference ( &amp;quot;this system&amp;quot;), namely REF_US_AGENT, THEM_PRONOUN_AGENT, AIM_REF_AGENT, REF_AGENT. The total of 168 patterns in the lexicon expands to many more as we use a replace mechanism (@WORK_NOUN is expanded to &amp;quot;paper, article, study, chapter&amp;quot; etc). For verbs, we use a manually created the action lexicon summarized in Figure 6. The verb classes are based on semantic concepts such as similarity, contrast, competition, presentation, argumentation and textual structure. For example, PRESENTATION_ACTIONS include communication verbs like &amp;quot;present&amp;quot;, &amp;quot;report&amp;quot;, &amp;quot;state&amp;quot; (Myers, 1992; Thompson and Yiyun, 1991), RESEARCH_ACTIONS include &amp;quot;analyze&amp;quot;, &amp;quot;conduct&amp;quot; and &amp;quot;observe&amp;quot;, and ARGUMENTATION_ACTIONS &amp;quot;argue&amp;quot;, &amp;quot;disagree&amp;quot;, &amp;quot;object to&amp;quot;. Domain-specific actions are contained in the classes indicating a problem ( &amp;quot;degrade&amp;quot;, &amp;quot;overestimate&amp;quot;), and solution-contributing actions (&amp;quot;&amp;quot;circumvent&amp;quot;, solve&amp;quot;, &amp;quot;mitigate&amp;quot;). The main reason for using a hand-crafted, genrespecific lexicon instead of a general resource such as WordNet or Levin&apos;s (1993) classes (as used in Klavans and Kan (1998)), was to avoid polysemy problems without having to perform word sense disambiguation. Verbs in our texts</context>
</contexts>
<marker>Myers, 1992</marker>
<rawString>Myers, Greg. 1992. In this paper we report...-speech acts and scientific facts. Journal of Pragmatics 17(4): 295-313.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hidetsugu Nanba</author>
<author>Manabu Okumura</author>
</authors>
<title>Towards multi-paper summarization using reference information.</title>
<date>1999</date>
<booktitle>In Proceedings of IJCAI-99,</booktitle>
<pages>926--931</pages>
<note>http: //galaga. jaist .ac . jp: 8000/-nanba/ study/papers .html.</note>
<marker>Nanba, Okumura, 1999</marker>
<rawString>Nanba, Hidetsugu, and Manabu Okumura. 1999. Towards multi-paper summarization using reference information. In Proceedings of IJCAI-99, 926-931. http: //galaga. jaist .ac . jp: 8000/-nanba/ study/papers .html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris D Paice</author>
</authors>
<title>The automatic generation of literary abstracts: an approach based on the identification of self-indicating phrases.</title>
<date>1981</date>
<booktitle>Information Retrieval Research,</booktitle>
<pages>172--191</pages>
<editor>In Robert Norman Oddy, Stephen E. Robertson, Cornelis Joost van Rijsbergen, and P. W. Williams, eds.,</editor>
<location>London, UK: Butterworth.</location>
<contexts>
<context position="12372" citStr="Paice (1981)" startWordPosition="1895" endWordPosition="1896">ales, 1990); we model this with an ngram model operating over sentences. • Beginnings of attribution zones are linguistically marked by meta-discourse like &amp;quot;Other researchers claim that&amp;quot; (Swales, 1990; Hyland, 1998); we model this with a specialized agents and actions recognizer, and by recognizing formal citations. • Statements without explicit attribution are interpreted as being of the same attribution as previous sentences in the same segment of attribution; we model this with a modified agent feature which keeps track of previously recognized agents. 11 3.1 Recognizing Agents and Actions Paice (1981) introduces grammars for pattern matching of indicator phrases, e.g. &amp;quot;the aim/purpose of this paper/article/study&amp;quot; and &amp;quot;we conclude/propose&amp;quot;. Such phrases can be useful indicators of overall importance. However, for our task, more flexible meta-discourse expressions need to be determined. The description of a research tradition, or the statement that the work described in the paper is the continuation of some other work, cover a wide range of syntactic and lexical expressions and are too hard to find for a mechanism like simple pattern matching. Figure 4: Agent Lexicon: 168 Patterns, 13 Classe</context>
</contexts>
<marker>Paice, 1981</marker>
<rawString>Paice, Chris D. 1981. The automatic generation of literary abstracts: an approach based on the identification of self-indicating phrases. In Robert Norman Oddy, Stephen E. Robertson, Cornelis Joost van Rijsbergen, and P. W. Williams, eds., Information Retrieval Research, 172-191. London, UK: Butterworth.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris D Paice</author>
</authors>
<title>Constructing literature abstracts by computer: techniques and prospects.</title>
<date>1990</date>
<journal>Information Processing and Management</journal>
<volume>26</volume>
<pages>171--186</pages>
<contexts>
<context position="21653" citStr="Paice, 1990" startWordPosition="3350" endWordPosition="3351">n a postmodifying PP), as in the phrase &amp;quot;the problem with these approaches&amp;quot; which was classified as REF_AGENT. These cases (counted as errors) indeed constitute no grave errors, as they still give an indication which type of agents the nominal phrase is associated with. 13 4.2 Evaluation B: Usefulness for Argumentative Zoning We evaluated the usefulness of the Agent and Action features by measuring if they improve the classification results of our stochastic classifier for argumentative zones. We use 14 features given in figure 8, some of which are adapted from sentence extraction techniques (Paice, 1990; Kupiec et al., 1995; Teufel and Moens, 1999). 1. Absolute location of sentence in document 2. Relative location of sentence in section 3. Location of a sentence in paragraph 4. Presence of citations 5. Location of citations 6. Type of citations (self citation or not) 7. Type of headline 8. Presence of tf/idf key words 9. Presence of title words 10. Sentence length 11. Presence of modal auxiliaries 12. Tense of the finite verb 13. Voice of the finite verb 14. Presence of Formulaic Expressions Figure 8: Other features used All features except Citation Location and Citation Type proved helpful </context>
</contexts>
<marker>Paice, 1990</marker>
<rawString>Paice, Chris D. 1990. Constructing literature abstracts by computer: techniques and prospects. Information Processing and Management 26: 171-186.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey C Reynar</author>
</authors>
<title>Statistical models for topic segmentation.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL-99),</booktitle>
<pages>357--364</pages>
<contexts>
<context position="11040" citStr="Reynar (1999)" startWordPosition="1691" endWordPosition="1692">tly less stable and reproducible (stability: K=.82, .81, .76; N=1220; k=2 (equiv. to 93%, 92%, 90% agreement); reproducibility: K=.71, N=4261, k=3 (equiv. to 87% agreement)), but still in the range of what is generally accepted as reliable annotation. We conclude from this that humans can distinguish attribution and full argumentative zones, if trained. Human annotation is used as training material in our statistical classifier. 3 Automatic Argumentative Zoning As our task is not defined by topic coherence like the related tasks of Morris and Hirst (1991), Hearst (1997), Kan et al. (1998) and Reynar (1999), we predict that keyword-based techniques for automatic argumentative zoning will not work well (cf. the results using text categorization as described later). We decided to perform machine learning, based on sentential features like the ones used by sentence extraction. Argumentative zones have properties which help us determine them on the surface: • Zones appear in typical positions in the article (Myers, 1992); we model this with a set of location features. • Linguistic features like tense and voice correlate with zones (Biber (1995) and Riley (1991) show correlation for similar zones lik</context>
</contexts>
<marker>Reynar, 1999</marker>
<rawString>Reynar, Jeffrey C. 1999. Statistical models for topic segmentation. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL-99), 357-364.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathryn Riley</author>
</authors>
<title>Passive voice and rhetorical role in scientific writing.</title>
<date>1991</date>
<journal>Journal of Technical Writing and Communication</journal>
<volume>21</volume>
<issue>3</issue>
<pages>239--257</pages>
<contexts>
<context position="11601" citStr="Riley (1991)" startWordPosition="1778" endWordPosition="1779"> Hearst (1997), Kan et al. (1998) and Reynar (1999), we predict that keyword-based techniques for automatic argumentative zoning will not work well (cf. the results using text categorization as described later). We decided to perform machine learning, based on sentential features like the ones used by sentence extraction. Argumentative zones have properties which help us determine them on the surface: • Zones appear in typical positions in the article (Myers, 1992); we model this with a set of location features. • Linguistic features like tense and voice correlate with zones (Biber (1995) and Riley (1991) show correlation for similar zones like &amp;quot;method&amp;quot; and &amp;quot;introduction&amp;quot;). We model this with syntactic features. • Zones tend to follow particular other zones (Swales, 1990); we model this with an ngram model operating over sentences. • Beginnings of attribution zones are linguistically marked by meta-discourse like &amp;quot;Other researchers claim that&amp;quot; (Swales, 1990; Hyland, 1998); we model this with a specialized agents and actions recognizer, and by recognizing formal citations. • Statements without explicit attribution are interpreted as being of the same attribution as previous sentences in the sam</context>
</contexts>
<marker>Riley, 1991</marker>
<rawString>Riley, Kathryn. 1991. Passive voice and rhetorical role in scientific writing. Journal of Technical Writing and Communication 21(3): 239-257.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Rowley</author>
</authors>
<title>Abstracting and Indexing.</title>
<date>1982</date>
<location>London, UK: Bingley.</location>
<contexts>
<context position="1528" citStr="Rowley, 1982" startWordPosition="225" endWordPosition="226">ypical agents and their actions in scientific text. 1 Introduction When writing an article, one does not normally go straight to presenting the innovative scientific claim. Instead, one establishes other, wellknown scientific facts first, which are contributed by other researchers. Attribution of ownership often happens explicitly, by phrases such as &amp;quot;Chomsky (1965) claims that&amp;quot;. The question of intellectual attribution is important for researchers: not understanding the argumentative status of part of the text is a common problem for nonexperts reading highly specific texts aimed at experts (Rowley, 1982). In particular, after reading an article, researchers need to know who holds the &amp;quot;knowledge claim&amp;quot; for a certain fact that interests them. We propose that segmentation according to intellectual ownership can be done automatically, and that such a segmentation has advantages for various shallow text understanding tasks. At the heart of our classification scheme is the following trisection: • BACKGROUND (generally known work) • OWN, new work and • specific OTHER work. The advantages of a segmentation at a rhetorical level is that rhetorics is conveniently constant tThis work was done while the </context>
</contexts>
<marker>Rowley, 1982</marker>
<rawString>Rowley, Jennifer. 1982. Abstracting and Indexing. London, UK: Bingley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Castellan</author>
</authors>
<title>Nonparametric Statistics for the Behavioral Sciences.</title>
<date>1988</date>
<location>Berkeley, CA:</location>
<note>McGraw-Hill, 2nd edn.</note>
<contexts>
<context position="9556" citStr="Castellan, 1988" startWordPosition="1456" endWordPosition="1457">r. The main focus of the paper is the description of two features which are particularly useful for attribution determination: prototypical agents and actions. 2 Human Annotation of Argumentative Zones We have previously evaluated the scheme empirically by extensive experiments with three subjects, over a range of 48 articles (Teufel et al., 1999). • We measured stability (the degree to which the same annotator will produce an annotation after 6 weeks) and reproducibility (the degree to which two unrelated annotators will produce the same annotation), using the Kappa coefficient K (Siegel and Castellan, 1988; Carletta, 1996), which controls agreement P(A) for chance agreement P(E): K P(A)—P(E) Es Kappa is 0 for if agreement is only as would be expected by chance annotation following the same distribution as the observed distribution, and 1 for perfect agreement. Values of Kappa surpassing .8 are typically accepted as showing a very high level of agreement (Krippendorff, 1980; Landis and Koch, 1977). Our experiments show that humans can distinguish own, other specific and other general work with high stability (K=.83, .79, .81; N=1248; k=2, where K stands for the Kappa coefficient, N for the numbe</context>
</contexts>
<marker>Castellan, 1988</marker>
<rawString>Siegel, Sidney, and N. John Jr. Castellan. 1988. Nonparametric Statistics for the Behavioral Sciences. Berkeley, CA: McGraw-Hill, 2nd edn.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Swales</author>
</authors>
<title>Genre Analysis: English in Academic and Research Settings. Chapter 7: Research articles in English,</title>
<date>1990</date>
<pages>110--176</pages>
<publisher>Cambridge University Press.</publisher>
<location>Cambridge, UK:</location>
<contexts>
<context position="3768" citStr="Swales, 1990" startWordPosition="556" endWordPosition="557">arge variation with respect to different factors: subdomain (theoretical linguistics, statistical NLP, logic programming, computational psycholinguistics), types of research (implementation, review, evaluation, empirical vs. theoretical research), writing style (formal vs. informal) and presentational styles (fixed section structure of type Introduction—Method—Results—Conclusion vs. more idiosyncratic, problem-structured presentation). One thing, however, is constant across all articles: the argumentative aim of every single article is to show that the given work is a contribution to science (Swales, 1990; Myers, 1992; Hyland, 1998). Theories of scientific argumentation in research articles stress that authors follow well-predictable stages of argumentation, as in the fictional introduction in figure 1. 9 Are the scientific statements expressed in this sentence attributed to the authors, the general field, or specific other researchers? Own work Other Work BACKGROUND Does this sentence contain material Does it describe a negative aspect that describes the specific aim of the other work, or a contrast of the paper? or comparison of the own work to it? NO NO Does this sentence make Does this sen</context>
<context position="11771" citStr="Swales, 1990" startWordPosition="1803" endWordPosition="1804">g text categorization as described later). We decided to perform machine learning, based on sentential features like the ones used by sentence extraction. Argumentative zones have properties which help us determine them on the surface: • Zones appear in typical positions in the article (Myers, 1992); we model this with a set of location features. • Linguistic features like tense and voice correlate with zones (Biber (1995) and Riley (1991) show correlation for similar zones like &amp;quot;method&amp;quot; and &amp;quot;introduction&amp;quot;). We model this with syntactic features. • Zones tend to follow particular other zones (Swales, 1990); we model this with an ngram model operating over sentences. • Beginnings of attribution zones are linguistically marked by meta-discourse like &amp;quot;Other researchers claim that&amp;quot; (Swales, 1990; Hyland, 1998); we model this with a specialized agents and actions recognizer, and by recognizing formal citations. • Statements without explicit attribution are interpreted as being of the same attribution as previous sentences in the same segment of attribution; we model this with a modified agent feature which keeps track of previously recognized agents. 11 3.1 Recognizing Agents and Actions Paice (1981</context>
</contexts>
<marker>Swales, 1990</marker>
<rawString>Swales, John. 1990. Genre Analysis: English in Academic and Research Settings. Chapter 7: Research articles in English, 110-176. Cambridge, UK: Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Teufel</author>
<author>Jean Carletta</author>
<author>Marc Moens</author>
</authors>
<title>An annotation scheme for discourse-level argumentation in research articles.</title>
<date>1999</date>
<booktitle>In Proceedings of the 8th Meeting of the European Chapter of the Association for Computational Linguistics (EA CL-99),</booktitle>
<pages>110--117</pages>
<contexts>
<context position="9290" citStr="Teufel et al., 1999" startWordPosition="1413" endWordPosition="1416">mentative Zones earlier work, correct it, point out a weakness in it, or just provide it as general background. This &amp;quot;qualitative&amp;quot; information could be directly contributed by our argumentative zones. In this paper, we will describe the algorithm of an argumentative zoner. The main focus of the paper is the description of two features which are particularly useful for attribution determination: prototypical agents and actions. 2 Human Annotation of Argumentative Zones We have previously evaluated the scheme empirically by extensive experiments with three subjects, over a range of 48 articles (Teufel et al., 1999). • We measured stability (the degree to which the same annotator will produce an annotation after 6 weeks) and reproducibility (the degree to which two unrelated annotators will produce the same annotation), using the Kappa coefficient K (Siegel and Castellan, 1988; Carletta, 1996), which controls agreement P(A) for chance agreement P(E): K P(A)—P(E) Es Kappa is 0 for if agreement is only as would be expected by chance annotation following the same distribution as the observed distribution, and 1 for perfect agreement. Values of Kappa surpassing .8 are typically accepted as showing a very hig</context>
</contexts>
<marker>Teufel, Carletta, Moens, 1999</marker>
<rawString>Teufel, Simone, Jean Carletta, and Marc Moens. 1999. An annotation scheme for discourse-level argumentation in research articles. In Proceedings of the 8th Meeting of the European Chapter of the Association for Computational Linguistics (EA CL-99), 110-117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Teufel</author>
<author>Marc Moens</author>
</authors>
<title>Argumentative classification of extracted sentences as a first step towards flexible abstracting.</title>
<date>1999</date>
<booktitle>In Inderjeet Mani and</booktitle>
<pages>155--171</pages>
<editor>Mark T. Maybury, eds.,</editor>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA:</location>
<contexts>
<context position="5752" citStr="Teufel and Moens (1999)" startWordPosition="870" endWordPosition="873">to the core set of OWN, OTHER and BACKGROUND, namely contrastive statements (CONTRAST; comparable to Swales&apos; (1990) move 2A/B) and statements of intellectual ancestry (BAsis; Swales&apos; move 2D). The label OTHER is thus reserved for neutral descriptions of other work. OWN segments are further subdivided to mark explicit aim statements (ADA; Swales&apos; move 3.1A/B), and explicit section previews (TEXTUAL; Swales&apos; move 3.3). All other statements about the own work are classified as OWN. Each of the seven category covers one sentence. Our classification, which is a further development of the scheme in Teufel and Moens (1999), can be described procedurally as a decision tree (Figure 2), where five questions are asked about each sentence, concerning intellectual attribution, author stance and continuation vs. contrast. Figure 3 gives typical example sentences for each zone. The intellectual-attribution distinction we make is comparable with Wiebe&apos;s (1994) distinction into subjective and objective statements. Subjectivity is a property which is related to the attribution of authorship as well as to author stance, but it is just one of the dimensions we consider. 1.2 Use of Argumentative Zones Which practical use wou</context>
<context position="21699" citStr="Teufel and Moens, 1999" startWordPosition="3356" endWordPosition="3359">hrase &amp;quot;the problem with these approaches&amp;quot; which was classified as REF_AGENT. These cases (counted as errors) indeed constitute no grave errors, as they still give an indication which type of agents the nominal phrase is associated with. 13 4.2 Evaluation B: Usefulness for Argumentative Zoning We evaluated the usefulness of the Agent and Action features by measuring if they improve the classification results of our stochastic classifier for argumentative zones. We use 14 features given in figure 8, some of which are adapted from sentence extraction techniques (Paice, 1990; Kupiec et al., 1995; Teufel and Moens, 1999). 1. Absolute location of sentence in document 2. Relative location of sentence in section 3. Location of a sentence in paragraph 4. Presence of citations 5. Location of citations 6. Type of citations (self citation or not) 7. Type of headline 8. Presence of tf/idf key words 9. Presence of title words 10. Sentence length 11. Presence of modal auxiliaries 12. Tense of the finite verb 13. Voice of the finite verb 14. Presence of Formulaic Expressions Figure 8: Other features used All features except Citation Location and Citation Type proved helpful for classification. Two different statistical </context>
</contexts>
<marker>Teufel, Moens, 1999</marker>
<rawString>Teufel, Simone, and Marc Moens. 1999. Argumentative classification of extracted sentences as a first step towards flexible abstracting. In Inderjeet Mani and Mark T. Maybury, eds., Advances in Automatic Text Summarization, 155-171. Cambridge, MA: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoff Thompson</author>
<author>Ye Yiyun</author>
</authors>
<title>Evaluation in the reporting verbs used in academic papers.</title>
<date>1991</date>
<journal>Applied Linguistics</journal>
<volume>12</volume>
<issue>4</issue>
<pages>365--382</pages>
<contexts>
<context position="14675" citStr="Thompson and Yiyun, 1991" startWordPosition="2239" endWordPosition="2242">with ambiguous reference ( &amp;quot;this system&amp;quot;), namely REF_US_AGENT, THEM_PRONOUN_AGENT, AIM_REF_AGENT, REF_AGENT. The total of 168 patterns in the lexicon expands to many more as we use a replace mechanism (@WORK_NOUN is expanded to &amp;quot;paper, article, study, chapter&amp;quot; etc). For verbs, we use a manually created the action lexicon summarized in Figure 6. The verb classes are based on semantic concepts such as similarity, contrast, competition, presentation, argumentation and textual structure. For example, PRESENTATION_ACTIONS include communication verbs like &amp;quot;present&amp;quot;, &amp;quot;report&amp;quot;, &amp;quot;state&amp;quot; (Myers, 1992; Thompson and Yiyun, 1991), RESEARCH_ACTIONS include &amp;quot;analyze&amp;quot;, &amp;quot;conduct&amp;quot; and &amp;quot;observe&amp;quot;, and ARGUMENTATION_ACTIONS &amp;quot;argue&amp;quot;, &amp;quot;disagree&amp;quot;, &amp;quot;object to&amp;quot;. Domain-specific actions are contained in the classes indicating a problem ( &amp;quot;degrade&amp;quot;, &amp;quot;overestimate&amp;quot;), and solution-contributing actions (&amp;quot;&amp;quot;circumvent&amp;quot;, solve&amp;quot;, &amp;quot;mitigate&amp;quot;). The main reason for using a hand-crafted, genrespecific lexicon instead of a general resource such as WordNet or Levin&apos;s (1993) classes (as used in Klavans and Kan (1998)), was to avoid polysemy problems without having to perform word sense disambiguation. Verbs in our texts often have a specialized m</context>
</contexts>
<marker>Thompson, Yiyun, 1991</marker>
<rawString>Thompson, Geoff, and Ye Yiyun. 1991. Evaluation in the reporting verbs used in academic papers. Applied Linguistics 12(4): 365-382.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M E Wellons</author>
<author>G P Purcell</author>
</authors>
<title>Task-specific extracts for using the medical literature.</title>
<date>1999</date>
<booktitle>In Proceedings of the American Medical Informatics Symposium,</booktitle>
<pages>1004--1008</pages>
<marker>Wellons, Purcell, 1999</marker>
<rawString>Wellons, M. E., and G. P. Purcell. 1999. Task-specific extracts for using the medical literature. In Proceedings of the American Medical Informatics Symposium, 1004-1008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
</authors>
<title>Tracking point of view in narrative.</title>
<date>1994</date>
<journal>Computational Linguistics</journal>
<volume>20</volume>
<issue>2</issue>
<pages>223--287</pages>
<contexts>
<context position="25827" citStr="Wiebe (1994)" startWordPosition="4030" endWordPosition="4031">on results only for the stronger features, i.e. those who are better than Baseline 2). The Action feature, if considered on its own, is rather weak: it shows a slightly better Kappa value than Baseline 2, but does not even reach the level of random agreement (K=0). Nevertheless, if taken together with the other features, it still improves results. Building on the idea that intellectual attribution is a segment-based phenomena, we improved the Agent feature by including history (feature SAgent). The assumption is that in unmarked sentences the agent of the previous attribution is still active. Wiebe (1994) also reports segment-based agenthood as one of the most successful features. SAgent alone achieved a classification success of K=.21, which makes SAgent the best single features available in the entire feature pool. Inclusion of SAgent to the final model improved results to K=.43 (bigram model). Figure 12 also shows that different features are better at disambiguating certain categories. The Formulaic feature, which is not very strong on its own, is the most diverse, as it contributes to the disambiguation of six categories directly. Both Agent and Action features disambiguate categories whic</context>
</contexts>
<marker>Wiebe, 1994</marker>
<rawString>Wiebe, Janyce. 1994. Tracking point of view in narrative. Computational Linguistics 20(2): 223-287.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>