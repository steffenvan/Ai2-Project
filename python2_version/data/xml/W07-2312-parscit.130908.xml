<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000617">
<title confidence="0.994589">
Measuring Variability in Sentence Ordering for News Summarization
</title>
<author confidence="0.898296">
Nitin Madnania,b Rebecca Passonneauc Necip Fazil Ayana,b John M. Conroyd
Bonnie J. Dorra,b Judith L. Klavans&apos; Dianne P. O’Learya,b Judith D. Schlesingerd
</author>
<affiliation confidence="0.999591333333333">
aDepartment of Computer Science
bInstitute for Advanced Computer Studies
University of Maryland, College Park
</affiliation>
<email confidence="0.990424">
{nmadnani,nfa,bonnie,oleary}@cs.umd.edu
</email>
<affiliation confidence="0.833522">
cCenter for Computational Learning Systems, Columbia University
</affiliation>
<email confidence="0.9772">
becky@cs.columbia.edu
</email>
<affiliation confidence="0.579382">
dIDA/Center for Computing Sciences
</affiliation>
<email confidence="0.947345">
{conroy,judith}@super.org
</email>
<affiliation confidence="0.971702">
&apos;College of Information Studies, University of Maryland, College Park
</affiliation>
<email confidence="0.987872">
jklavans@umd.edu
</email>
<sectionHeader confidence="0.99853" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99970280952381">
The issue of sentence ordering is an important one
for natural language tasks such as multi-document
summarization, yet there has not been a quantita-
tive exploration of the range of acceptable sentence
orderings for short texts. We present results of a
sentence reordering experiment with three experi-
mental conditions. Our findings indicate a very high
degree of variability in the orderings that the eigh-
teen subjects produce. In addition, the variability
of reorderings is significantly greater when the ini-
tial ordering seen by subjects is different from the
original summary. We conclude that evaluation of
sentence ordering should use multiple reference or-
derings. Our evaluation presents several metrics that
might prove useful in assessing against multiple ref-
erences. We conclude with a deeper set of ques-
tions: (a) what sorts of independent assessments of
quality of the different reference orderings could
be made and (b) whether a large enough test set
would obviate the need for such independent means
of quality assessment.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99997765">
The issue of ordering content in a multi-document
extractive summary is an important problem that
has received little attention until recently. Sen-
tence ordering, along with other factors that affect
coherence and readability, is of particular concern
for multi-document summarization, where differ-
ent source articles contribute sentences to a sum-
mary. We conducted an exploratory study to deter-
mine how much variation humans would produce in
a reordering task under different experimental con-
ditions, in order to assess the issues for evaluating
automated reordering.
While a good ordering is essential for summary
comprehension (Barzilay et al., 2002), and recent
work on sentence ordering (Bollegala et al., 2006)
does show promise, it is important to note that de-
termining an optimal sentence ordering for a given
summary may not be feasible. The question for
evaluation of ordering is whether there is a single
best ordering that humans will converge on, or that
would lead to maximum reading comprehension,
or that would maximize another extrinsic summary
evaluation measure. On texts of approximately the
same length as summaries we look at here, Kara-
manis et al. (2005) found that experts produce dif-
ferent sentence orderings for expressing database
facts about archaeology. We find that summaries
of newswire have a relatively larger set of coherent
orderings.
We conducted an experiment where human sub-
jects were asked to reorder multi-document sum-
maries in order to maximize their coherence. The
summaries used in this experiment were originally
produced by a different set of human summarizers
as part of a multi-document summarization task that
was conducted by NIST in 2004. We present quan-
titative results that show that there is a large amount
of variability among the reorderings considered co-
herent. On this basis, we suggest that evaluation of
sentence ordering should use multiple references.
</bodyText>
<listItem confidence="0.76181575">
For each such summary in the experiment, we
create three initial sentence orderings: (a) original
order (b) random order, and (c) the output of an au-
tomated ordering algorithm. We show that:
• The initial orderings presented to the human
subjects have a statistically significant impact
on the reorderings that they create.
• The set of individual human reorderings ex-
</listItem>
<page confidence="0.998186">
81
</page>
<bodyText confidence="0.993962">
hibits a significant amount of variability.
The next section provides some background for
the sentence ordering task and presents the auto-
mated sentence ordering algorithm used in our ex-
periments. Section 3 describes the experimental de-
sign. Sections 4 and 5 present quantitative analyses
of the results of the experiment. Section 6 discusses
related work. We discuss our results in Section 7
and conclude in Section 8.
</bodyText>
<sectionHeader confidence="0.980564" genericHeader="introduction">
2 Sentence Ordering Algorithms
</sectionHeader>
<bodyText confidence="0.999668172413793">
A number of approaches have been applied to
sentence ordering for multi-document summariza-
tion (Radev and McKeown, 1999). The first tech-
niques exploited chronological information in the
documents (McKeown et al., 1999; Lin and Hovy,
2002). Barzilay et al. (2002) were the first to dis-
cuss the impact of sentence ordering in the con-
text of multi-document summarization in the news
genre. They used an augmented chronological or-
dering algorithm that first identified and clustered
related sentences, then imposed an ordering as di-
rected by the chronology. Okazaki et al. (2004) fur-
ther improved the chronological ordering algorithm
by first arranging sentences in simple chronological
order, then performing local reorderings.
More recent work includes probabilistic ap-
proaches that try to model the structure of text (La-
pata, 2003) and algorithms that use large corpora to
learn an ordering and then apply it to the summary
under consideration (Bollegala et al., 2005).
Conroy et al. (2006) treat sentence ordering as
a Traveling Salesperson Problem (TSP), similar to
Althaus et al. (2004). Starting from a designated
first sentence, they reorder the other sentences so
that the sum of the distances between adjacent sen-
tences is minimized. The distance (cjk) between
any pair of sentences j and k is computed by first
obtaining a similarity score (bjk) for the pair, and
then normalizing this score:
</bodyText>
<equation confidence="0.886434666666667">
bjk
cjk = 1 − (cjj = 0) (1)
Vbjj ,lbkk
</equation>
<bodyText confidence="0.999627">
Because a typical multi-document extractive
summary usually contains a small number of sen-
tences, a near-optimal solution to this TSP can be
found either by exhaustive search or by random
sampling. In this paper, we use this TSP ordering
algorithm to construct one of the three experimental
conditions.
</bodyText>
<sectionHeader confidence="0.997712" genericHeader="method">
3 Experimental Design
</sectionHeader>
<bodyText confidence="0.999773869565217">
We designed an experiment to test two hypotheses:
(1) that the initial orderings presented to the human
subjects have a statistically significant impact on the
reorderings that they create, and (2) that the set of
individual human reorderings exhibits a significant
amount of variability.
For our experiment, we randomly chose nine 100-
word human-written summaries† out of 200 hu-
man written summaries produced by NIST; they
were used as references to evaluate extractive multi-
document summaries in 2004 (Harman, 2004). We
later retrieved the quality judgments performed by
NIST assessors on seven of the summaries; the re-
maining two were used as a reference model for as-
sessors and had no quality judgments. The seven
summaries for which we had judgments were all
given high ratings of 1 or 2 (out of 5) on seven ques-
tions such as, Does the summary build from sen-
tence to sentence to a coherent body of information
about the topic?
The nine summaries were evenly divided into
three different groups: 51_3, 54_6 and 57_9. For
each summary, we used three orderings:
</bodyText>
<listItem confidence="0.945960571428571">
• O: the original ordering of sentences in the
summary, as written by the author of the sum-
mary.
• R: a random ordering of the sentences
• T: an ordering created by applying the TSP or-
dering algorithm described in the previous sec-
tion.
</listItem>
<bodyText confidence="0.9955936">
We constrained the random and the TSP order-
ings so that the first sentence of the human summary
appeared first.
Eighteen human subjects were divided into three
groups (I, II, and III), 6 subjects per group. We
presented each subject with each of the nine sum-
maries, in either its original ordering (condition
CO), random ordering (condition CR), or TSP or-
dering (condition CT), as described in the Latin
square design of Figure 1. For example, the six
subjects in group II were presented with summaries
1 − 3 in random order, 4 − 6 in original order, and
7 − 9 in TSP order. Thus the experiment produced
18 reorderings for each of the nine summaries, six
per initial order.
</bodyText>
<footnote confidence="0.957488">
†D30024-C (Document set D30024, NIST author ID C),
D31022-F, D31008-E, D30048-C, D30037-A, D30001-A,
D30051-D, D30015-E, D31001-C
</footnote>
<page confidence="0.99729">
82
</page>
<table confidence="0.574112">
S1−3 S4−6 S7−9
CO I II III
CR II III I
CT III I II
</table>
<figureCaption confidence="0.997274">
Figure 1: Latin Square Design
</figureCaption>
<bodyText confidence="0.999963904761905">
The human subjects chosen for the experiment
were all native English speakers. Subjects accessed
the task on a website, including the instructions,
which explained that they would be reading a docu-
ment on the screen and could reorder the sentences
in that document so as to make the document more
coherent. In order to prevent the introduction of
any bias, the order of presentation of summaries
was randomized for every subject. The instruc-
tions clearly specified the possibility that summaries
might need little or no reordering. It would be diffi-
cult to measure whether the instructions led subjects
to believe that all summaries could be improved by
reordering. We do not have objective criteria to
identify a control set of summaries that cannot be
improved by reordering; in fact, this is a subjective
judgement that is likely to vary between individu-
als. Because the three experimental conditions had
the same instructions, we believe the significant dif-
ferences in amount of reordering across conditions
is a real effect rather than an artifact.
</bodyText>
<sectionHeader confidence="0.979531" genericHeader="method">
4 Variability across Experimental
Conditions
</sectionHeader>
<bodyText confidence="0.99997975">
To measure the variability across the experimental
conditions, we developed two methods that assign a
global score to each set of reorderings by comparing
them to a particular reference point.
</bodyText>
<subsectionHeader confidence="0.999606">
4.1 Method 1: Confusion Matrices and κ
</subsectionHeader>
<bodyText confidence="0.998701466666667">
In NLP evaluation, confusion matrices have typ-
ically been used in annotation tasks (Bruce and
Wiebe (1998), Tomuro (2001)) where the matrix
represents the comparison of two judges, and the
κ inter-annotator agreement metric value (Cohen,
1960) gives a measure of the amount of agreement
between the two judges, after factoring out chance.
However, κ has been used to quantify the observed
distribution in confusion matrices of other types in
a range of other fields and applications (e.g., assess-
ing map accuracy (Hardin, 1999), or optical recog-
nition (Ross et al., 2002)). Here we use it to quantify
variability within sets of reorderings for a summary.
Given a representation of each summary as a set
of sequentially indexed sentences (e.g., 1, 2, 3, 4,
</bodyText>
<table confidence="0.636317333333333">
1 2 3 4 5
1 5 0 0 1 0
2 1 3 1 0 1
3 0 2 2 1 1
4 0 0 2 2 2
5 0 1 1 2 2
</table>
<figureCaption confidence="0.9960105">
Figure 2: Confusion matrix for a set of reorderings
(summary 1, condition=CO, reference=O; κ=0.33)
</figureCaption>
<bodyText confidence="0.999957">
5), and of each reordering as a corresponding se-
quence of positional indices, we can create confu-
sion matrices as in Figure 2. The rows represent
the sequential positions of the summary sentences
in one of the three initial orders that subjects were
presented with, the columns represent the sentence
indices, and each cell value mij indicates how often
sentence j occurred in position i. Figure 2 shows
the confusion matrix obtained by comparing the 6
reorderings for summary 1, obtained under the CO
experimental condition, to the original order (O) for
that summary. The first column of the figure indi-
cates that among the reorderings under considera-
tion, five have sentence 1 in the first position, and
one has sentence 1 in the second position. If all re-
orderings reproduced the original order O, the five
cells of the matrix on the main diagonal would all
have the value 6, and all other cells would have the
value 0. The column headings of the corresponding
confusion matrix for the random order (R) corre-
spond to the sequence R, and similarly for (T).
In the general case (any agreement matrix), κ
measures whether the distribution of values within
a matrix differs from the distribution that would be
predicted by chance; the ratios of column and row
marginals to the matrix total provide estimates of
the expected values within each cell. Given a con-
fusion matrix where the cells on the matrix diagonal
are denoted as nii, the row marginals as ni+, the col-
umn marginals as n+i and the matrix total as n++,
the formula for κ is:
</bodyText>
<equation confidence="0.973976666666667">
κ = Ei Pii − Ei Pi+P+i
1 − � (2)
i Pi+P+i
</equation>
<bodyText confidence="0.987585">
where Pii = nii
n++ (the proportion of cases where a
sentence ended up in its original slot), P+i = n+i
n++,
and Pi+ =ni+
n++ . If all cells on the diagonal are 6, κ
is equal to 1. The question of interest to us is how
closely a given matrix approximates this degree of
agreement with the initial order.
For each summary 1-9 and for each condition,
</bodyText>
<page confidence="0.992761">
83
</page>
<bodyText confidence="0.999873769230769">
we construct three confusion matrices: one with
each initial order O, R, and T as the target of com-
parison. We denote the corresponding κ values
as κO, κR, and κT. In principle, κ values range
from 1 to values approaching -1, with 1 indicat-
ing perfect agreement, 0 indicating no difference
from chance, and negative values indicating dis-
agreements greater than expected by chance. Here,
because all row and column marginals necessarily
sum to 6, κ ranges from 1 to 0, with 1 indicating
that the set of reorderings all reproduce the initial
ordering, and 0 indicating that the set of reorderings
conforms to chance.
</bodyText>
<subsectionHeader confidence="0.992859">
4.2 Method 2: Means Vectors and Three
Correlation Metrics
</subsectionHeader>
<bodyText confidence="0.999816444444444">
Our second method for measuring the amount of
variability in a set of reorderings is based on the
observation that each reordering is the same length
as the initial ordering, and that each sentence index
must occur exactly once per reordering. Each set of
reorderings can be represented by a means vector,
where each element of the vector is the mean sen-
tence index for all reorderings in that set. We use
three correlation metrics to give different measures
of how well a means vector correlates with the ini-
tial orderings O, R and T.
The mean of the indices in each sentence posi-
tion will more nearly approximate the original sen-
tence index in that position when there are fewer
instances of substituting a different sentence, and
when substitutions involve sentences that were orig-
inally closer to the given slot. Figure 3 gives a hy-
pothetical example in which each of the 6 subjects
used the same ordering shifted by one: half the sub-
jects shifted the summary by starting with the last
sentence, then continuing from sentence 1 through
5 in sequence; the other half shifted the summary by
starting with sentence 2 and continuing in sequence,
with sentence 1 in the last position. Comparison of
the means vector to the original order (O) indicates
that this set of reorderings is quite similar to the
original for the second through fifth positions and
different in the first and last positions.
There are many distributions of sentences within
a set of reorderings that can lead to the same means
vector, thus we lose the power to identify some
of the differences between individual reorderings
within an experimental condition. However, the
question we want to assess is whether the pattern
given by a set of reorderings taken as a whole cor-
relates well with the initial presentation order. We
</bodyText>
<table confidence="0.99912825">
O 1 2 3 4 5 6
S1 2 3 4 5 6 1
S2 6 1 2 3 4 5
S4 6 1 2 3 4 5
S3 2 3 4 5 6 1
S5 2 3 4 5 6 1
Ss 6 1 2 3 4 5
Mean 4 2 3 4 5 3
</table>
<figureCaption confidence="0.9551105">
Figure 3: A hypothetical example illustrating
Means Vectors
</figureCaption>
<bodyText confidence="0.997859192307692">
compute means vectors for each condition for each
summary, giving 27 such vectors. We compare each
means vector representing a set of reorderings to
each initial ordering O, R and T using three corre-
lation coefficients: Pearson’s r, Spearman’s ρ, and
Kendall’s τ (Lapata, 2006).
The three correlation coefficients test the close-
ness of two series of numbers, or two variables x
and y, in different ways. Pearson’s r is a para-
metric test of whether there is a perfect linear re-
lation between the two variables. Spearman’s ρ and
Kendall’s τ are non-parametric tests. Spearman’s
ρ is computed by replacing the variable values by
their rank and computing the correlation. Kendall’s
τ is based on counting the number of pairs xi, xi+1
and yi, yi+1 where the deltas of both pairs have the
same sign. In sum, the three metrics test whether
x and y are in a linear relation, a rank-preserving
relation, or an order-preserving relation. Since we
are comparing a set of reorderings to an initial or-
der, rather than two sequences, it is unclear to us
what grounds there would be for preferring one cor-
relation over another. Given the exploratory nature
of this method, we chose to compare results across
metrics in order to determine empirically whether
they support the same conclusions.
</bodyText>
<subsectionHeader confidence="0.778967">
4.3 Results
</subsectionHeader>
<bodyText confidence="0.9999832">
The two scoring methods yield 12 global scores‡
per summary per experimental condition, or twenty
seven observations per score. We computed
ANOVAs (analysis of variance) for each of the
twelve scores in turn, with the score as the depen-
dent variable and with summary number and condi-
tion as factors. Condition had a significant effect for
all three κ metrics, and for rO, ρO, τO and τT. This
indicates that the mean score for the nine summaries
differs, depending on the condition, for these seven
</bodyText>
<footnote confidence="0.567594">
‡4 metrics (κ, r, p, T) and three initial orders (O, R and T)
as targets of comparison
</footnote>
<page confidence="0.989538">
84
</page>
<table confidence="0.999839">
Metric p-value HSD D1 δ1 D2 δ2
κO 0.004027 0.1370 CO &gt; CR 0.2768 CO &gt; CT 0.2143
κR 0.0001682 0.0858 CR &gt; CO 0.2279 CR &gt; CT 0.1929
κT 0.002455 0.1356 CT &gt; CO 0.2848 CT &gt; CR 0.2325
rO 0.00004985 0.1556 CO &gt; CR 0.4646 CT &gt; CR 0.3643
rR 0.7604 - - - - -
rT 0.1135 - - - - -
ρO 0.0003774 0.2006 CO &gt; CR 0.5103 CT &gt; CR 0.3983
ρR 0.931 - - - - -
ρT 0.09643 - - - - -
τO 0.0004306 0.2129 CO &gt; CR 0.5338 CT &gt; CR 0.4209
τR 0.8394 - - - - -
τT 0.03532 0.2482 CO &gt; CR 0.3685 CT &gt; CR 0.2957
</table>
<tableCaption confidence="0.999872">
Table 1: Analysis of variance (ANOVA) using Tukey’s HSD for twelve global scores
</tableCaption>
<bodyText confidence="0.999936347222222">
of the twelve scores we computed. In all cases, sum-
mary was a non-significant factor, meaning that the
means of the specified metric (e.g., κO) do not vary
depending on the summary.
Table 1 presents the p-values for the analysis of
variance of each metric with condition as a factor.
A significant p-value indicates that there is a sig-
nificant effect of condition on the mean, but does
not indicate whether the means for each condition
were significantly different from all others. We
use Tukey’s Honest Significant Difference (HSD)
method to examine the significant differences in
more detail. For each score, Table 1 shows Tukey’s
HSD (the delta at which two means become signif-
icantly different), the pairs of conditions whose dif-
ference in means was greater than the HSD, and the
actual deltas. For example, row 1 of the table indi-
cates that the mean κ0 is higher in condition O (CO)
than in condition R (CR) by 0.2768, which is ap-
proximately twice the HSD of 0.1370. In all cases
where condition was significant, two out of the three
possible differences were statistically significant.
For each κ row, the initial order that is used as the
target is also the order defining the condition whose
mean κ scores are significantly greater than both
other conditions; thus for κO (comparison to the
original order O), the CO condition (where subjects
reordered O) is the one that has statistically signifi-
cant differences from the other two. In other words,
no matter what the target is, analysis of variance of
the mean κ scores shows that the reorderings created
under condition CO have a non-chance similarity to
the O ordering that is significantly greater than the
other two reordering conditions; the reorderings un-
der condition CR have a non-chance similarity to
the R ordering that is significantly greater than the
other two reordering conditions; the reorderings un-
der condition CT have a non-chance similarity to
the T ordering that is significantly greater than the
other two orderings. The sizes of the δs are roughly
the same. There are no other significant differences.
The κ scores provide one type of evidence show-
ing that taken as a set, the initial order that a set
of subjects are presented with has a statistically sig-
nificant effect on the reorderings that they produce;
the reorderings they produce will always be signif-
icantly closer to the initial order they are presented
with, with chance similarity factored out, than to
any of the other two initial orders.
For the three correlation coefficients that compare
the means vectors to one of the three possible tar-
gets, the experimental condition has a significant
effect on the means of the coefficient only for the
cases where the target is the original order (O). In
other words, there is no significant difference, de-
pending on experimental condition, in the mean r,
ρ, or τ scores when the sets of reorderings are com-
pared with the random (R) or TSP (T) order, with
one exception. There is also a significant difference
in the means of τ when the sets of reorderings are
compared with the TSP ordering. However, the ef-
fect is less significant than when compared with the
O ordering, and the HSD is greater.
The results for r scores indicate that sets of re-
orderings created under conditions CO and CT have
significantly higher mean correlations with the orig-
inal order (O) than those under the CR condition.
The other correlation analyses have parallel results:
both the CO and CT conditions have mean correla-
tions with the O ordering (and with the T ordering,
for τ) that are significantly higher than the CR con-
dition. These results suggest that not only is there
</bodyText>
<page confidence="0.999453">
85
</page>
<bodyText confidence="0.999906714285714">
a significant effect of the initial order on the range
of reorderings produced, but also that under the CR
condition (subjects see a random order), the order-
ings produced are far more variable (less correlated
with anything) than under the CO and CT condi-
tions. r seems more sensitive than p or T in that the
HSDs are smaller.
</bodyText>
<sectionHeader confidence="0.979632" genericHeader="method">
5 Variability among Individual Subjects
</sectionHeader>
<bodyText confidence="0.999865272727273">
The second analysis we performed was to measure
the amount of variability among individual subjects.
For this analysis, we use a variant of a method
used by Karamanis et al. (2005) (based on (Lapata,
2003)). They first computed the average distance
between each pair of expert pairs among 4 experts (6
pairs). Experts were then compared based on their
average T value, T. When the T for a pair of experts
is high, the experts are quite similar on all reorder-
ings. For three of the experts, the T scores for all
pairwise combinations were found to be rather high,
and not significantly different, while the fourth ex-
pert was different from the other three.
Where they had 16 observations for which they
computed T scores (each expert performed 16 order-
ings of the same items), we have more observations
overall, but far fewer on which to make a compari-
son among pairs of subjects. Within a given condi-
tion, we have 6 subjects (15 pairs) but only 3 sum-
maries, which is not enough to justify comparing
the mean T scores. Thus, we measure the similarity
of two subjects’ orderings using Kendall’s T.
</bodyText>
<subsectionHeader confidence="0.770708">
5.1 Results
</subsectionHeader>
<bodyText confidence="0.999769684210526">
For 18 subjects, there are C(18,2) = 153 unique
pairwise comparisons among subjects. We com-
puted Kendall’s T for every pair of subjects and then
applied analysis of variance to the T scores. With T
score as the dependent variable, and summary num-
ber, pair id and condition as factors, all factors were
highly significant (p ^_ 0).
From the fact that summary number is a signifi-
cant factor in predicting mean T scores, we can con-
clude that the 9 summaries differ from each other
in terms of the variability among individuals. As
in the earlier ANOVA presented in Table 1, we use
Tukey’s HSD to determine the magnitude of the dif-
ference in means that is necessary for statistical sig-
nificance, and use this to identify which summaries
have significant differences in the amount of simi-
larity among subjects’ reorderings.
Applying Tukey’s method to summary number
as a factor yields the differences shown in Table 2.
</bodyText>
<table confidence="0.989799">
S+ S−
8, 9, 6, 5, 3, 1, 2, 4 &gt; 7
8, 9, 6 &gt; 4, 2
8, 9 &gt; 1, 3, 5
</table>
<tableCaption confidence="0.988115">
Table 2: Tukey analysis of summary number as a
</tableCaption>
<bodyText confidence="0.968229862068965">
factor on Kendall’s T scores between individual sub-
jects’ reorderings
Among the 36 pairs of comparisons, twenty were
significantly different. Here we present only the
significant comparisons, not the size of the HSD
nor the deltas for each comparison. The column
on the left (S+) shows the summary numbers where
the mean T values for pairs of individuals were
significantly greater than for the summary num-
bers in the right column (S_). Each row summa-
rizes |5+ |x |5_ |comparisons. With summary 7,
there were lower T values than for all other sum-
maries, meaning individuals’ orderings were least
alike. There were two other sets of comparisons
with significant differences: summaries 8, 9 and 6
had significantly higher T values than 4 and 2, and
summaries 8 and 9 had significantly higher T val-
ues than 1, 3 and 5. No other comparisons among
summaries had significantly distinct mean T values.
If we were to apply Tukey’s HSD to the pair id
factor, which was also highly significant as a pre-
dictor of T values, it becomes difficult to summarize
the significant differences. There are C(153, 2) =
11, 628 pairwise comparisons of pairs of subjects;
of these, 4, 225 were found to be statistically signif-
icant, using Tukey’s method, and an analogous table
to Table 2 would have 210 lines. This demonstrates
that, overall, there is a large amount of variability
among the individuals’ reorderings.
</bodyText>
<sectionHeader confidence="0.8833635" genericHeader="method">
6 Related Work on Evaluating Sentence
Ordering
</sectionHeader>
<bodyText confidence="0.999937230769231">
Karamanis and Mellish (2005) also measure the
amount of variability between human subjects.
However, there are several dimensions of contrast
between our experiment and theirs: Their exper-
iment operates in a very distinct domain (archae-
ology) and genre (descriptions of museum arti-
facts) whereas we use domain-independent multi-
document summaries derived from news articles.
We use ordinary, English-speaking volunteers as
compared to the domain and genre experts that they
employ (archaeologists trained in museum label-
ing). In terms of the experimental design, we use a
Latin square design with three experimental condi-
</bodyText>
<page confidence="0.992236">
86
</page>
<bodyText confidence="0.9999835">
tions whereas they have a single experimental con-
dition. Another important difference is the nature
of the ordering task itself—the task we chose was a
simple text-to-text ordering task whereas their task
was a modified fact-to-text ordering task, i.e., al-
though their subjects saw sentences, it is not clear
whether they were simply sentences corresponding
to database facts and devoid of connectives, pro-
nouns etc. We applied analysis of variance to all
pairs of subjects’ T scores directly, rather than to
the specialized scores that they compute, so we can-
not directly compare results. However, the amount
of variation we find seems far greater.
Barzilay et al. (2002) also conducted experiments
that asked human subjects to create alternative or-
derings and showed that subjects rarely agreed on
a single ordering for the given text. However, they
did not conduct a detailed quantitative analysis of
the amount of variability found in the set of human
reorderings.
Okazaki et al. (2004) do ask the human judges to
provide a corrected ordering for each ordering that
they grade during evaluation. However, only one
corrected ordering per summary is created. In ad-
dition, the number of humans subjects used for the
evaluation task and the measures taken for circum-
venting bias, if any, are not reported. By contrast,
our experiment uses a Latin Square with fully ran-
domized presentation order to circumvent the intro-
duction of any bias. Moreover, we create 18 cor-
rected orderings for each summary and are, there-
fore, in a much better position to draw general con-
clusions about variability in sentence orderings for
extractive news summaries.
Lapata (2003) required the human subjects to
create multiple orderings so as to produce a co-
herent text but used all the human orderings solely
for the purpose of comparing the proposed ordering
technique and not for any form of variability analy-
sis.
</bodyText>
<sectionHeader confidence="0.99978" genericHeader="method">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999960677966102">
Our most noteworthy finding is that for summaries
of clusters of news articles, the degree of similar-
ity of the reorderings to the original text is inversely
related to the degree of randomness in the ordering
that humans see. This gives us new insight into the
sentence ordering task for humans. Our results sug-
gest that humans are better at creating coherence
from coherence than from incoherence. Even un-
der the experimental condition with the lowest dis-
order (CO), there is a significant amount of varia-
tion. While there is no single best ordering, there
are better and worse orderings, and TSP generally
seems better than a random set of orderings. This
is clearly apparent from Table 1— if we look at the
cases where we use the original order O as the tar-
get of comparison (first, fourth, seventh and tenth
rows), column five shows that in 3 out of 4 cases, the
CT condition (where subjects were presented with
the TSP ordered sentences) has significantly higher
scores than the CR condition (where they were pre-
sented with randomly ordered sentences). We con-
clude from this that evaluation of sentence ordering
should use multiple references.
To evaluate against multiple references, we sug-
gest that a variation of the metrics we present here
can be used in which test orderings are each com-
pared to a set of target orderings comprised of the
multiple references for a given text, in contrast to
our method of comparing a set to a single target
ordering. In confusion matrices for each text, the
columns would represent the sentence indices of the
multiple references, the rows would represent the
sequential positions of the algorithm output for that
text, and the cell values mij would represent how of-
ten the ith sentence of the output ordering occurred
in the jth position across the multiple references.
In using multiple references, we also need fur-
ther research on how to assess the relative quality of
each reference order, and how to assess whether dif-
ferences in quality among the references affect the
evaluation. We believe that the relative coherence
of summaries depends on many factors besides sen-
tence order. A raw comparison of the nO values for
summary 8$$, for example, gives unusual results:
the CR condition reorderings most closely repro-
duce the original summary, followed by the CT con-
dition. We had the impression on reading this sum-
mary that there is a relative lack of use of devices
linking the sentences to one another, such as dis-
course connectives, lexical cohesion, or anaphora.
This raises the question of whether such devices are
less necessary to create readability in short texts.
While the seven summaries for which we found
quality ratings were of roughly equal quality (sum-
mary 8 had the highest possible quality ratings), the
DUC quality assessments have been shown to differ
between assessors (Passonneau et al., 2005).
In the DUC summarization evaluations, perfor-
mance of systems is assessed by averaging over
</bodyText>
<note confidence="0.266468">
**Document Set: D30015, NIST Author ID: E
</note>
<page confidence="0.997534">
87
</page>
<bodyText confidence="0.999931833333333">
large numbers of conditions, e.g., different docu-
ment sets with different characteristics. We believe
our lack of knowledge about the range of factors af-
fecting the ordering task, and the way they interact,
can be partly compensated for by evaluating order-
ing algorithms over a wide range of inputs.
</bodyText>
<sectionHeader confidence="0.997386" genericHeader="conclusions">
8 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999993791666667">
We conducted a reordering experiment that aims to
gauge the difficulty of the sentence ordering task
in the context of short, domain-independent multi-
document summaries. Our results indicate that the
sets of reorderings produced by human subjects de-
pend, in a statistically significant manner, on the ini-
tial orders that the subjects are shown. In addition,
we also show the existence of significant variability
among subjects’ reorderings. Both facts support our
claim that there are multiple coherent orderings for
a given summary. We believe that this has a signif-
icant impact on the evaluation of automatic order-
ing algorithms—all such algorithms should be eval-
uated against multiple reference orderings.
Our experiment has quantified the range of vari-
ability in human generated orderings under three
conditions. In our view, a second, extrinsic assess-
ment of the quality of the various reorderings would
be necessary in order to determine whether there are
grounds for ranking different orderings of the same
summary. An example of an extrinsic assessment
would be reading comprehension under time con-
straints. In future work, we would like to extend
our investigations to include extrinsic assessment.
</bodyText>
<sectionHeader confidence="0.998738" genericHeader="acknowledgments">
9 Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999825">
We are indebted to Mirella Lapata for sharing re-
sources useful for this work. This work has been
supported under the GALE program of the Defense
Advaned Research Projects Agency, Contract Nos.
HR0011-06-2-001 and HR0011-06-C-0023. Any
opinions, findings, conclusions or recommendations
expressed in this paper are those of the authors and
do not necessarily reflect the views of DARPA.
</bodyText>
<sectionHeader confidence="0.999645" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999670725806452">
E. Althaus, N. Karamanis and A. Koller. 2004. Com-
puting locally coherent discourses. In Proceedings of
ACL.
R. Barzilay, N. Elhadad, and K. McKeown. 2002. Infer-
ring strategies for sentence ordering in multidocument
news summarization. JAIR, 17:35–55.
D. Bollegala, N. Okazaki, and M. Ishizuka. 2005. A
machine learning approach to sentence ordering for
multi-document summarization and its evaluation. In
Proceedings of IJCNLP.
D. Bollegala, N. Okazaki, and M. Ishizuka. 2006. A
bottom-up approach to sentence ordering for multi-
document summarization. In Proceedings of COL-
ING/ACL.
R. Bruce and J. Wiebe. 1998. Word-sense distinguisha-
bility and inter-coder agreement. In Proceedings of
COLING/ACL.
J. Cohen. 1960. A coefficient of agreement for nominal
scales. Educational and Psychological Measurement,
20:37–46.
J. M. Conroy, J. D. Schlesinger, D. P. O’Leary, and
J. Goldstein. 2006. Back to basics: Classy 2006. In
Proceedings of DUC’06.
P. Hardin. 1999. Comparing main diagonal entries in
normalized confusion matrices: A bootstrapping ap-
proach. In IEEE International GRS Symposium.
D. Harman. 2004. Proceedings of DUC’04. Boston,
MA.
N. Karamanis and C. Mellish. 2005. Using a corpus of
sentence orderings defined by many experts to evalute
metrics of coherence for text structuring. In Proceed-
ings of ENLG.
M. Lapata. 2003. Probabilistic text structuring: Experi-
ments with sentence ordering. In Proceedings ofACL.
M. Lapata. 2006. Automatic evaluation of information
ordering: Kendall’s tau. Computational Linguistics,
32(4):471–484.
C-Y. Lin and E. Hovy. 2002. Automated multi-
document summarization in neats. In Proceedings of
HLT.
K. McKeown, J. Klavans, V. Hatzivassiloglou, R. Barzi-
lay, and E. Eskin. 1999. Towards multidocu-
ment summarization by reformulation: Progress and
prospects. In Proceedings of AAAI/IAAI.
N. Okazaki, Y. Matsuo, and M. Ishizuka. 2004. Improv-
ing chronological sentence ordering by precedence re-
lation. In Proceedings of COLING.
R. Passonneau, A. Nenkova, K. McKeown, and S. Sigel-
man. 2005. Applying the pyramid method in DUC
2005. In Proceedings of DUC’05.
D. R. Radev and K. McKeown. 1999. Generating
natural language summaries from multiple on-line
sources. Computational Linguistics, 24:469–500.
T. D. Ross, L. A. Westerkamp; R. L. Dilsavor,
J. C. Mossing, and E. G. Zelnio. 2002. Performance
measures for summarizing confusion matrices: The
AFRL COMPASE approach. In SPIE International
Society for Optical Engineering Proceedings Series.
N. Tomuro. 2001. Systematic polysemy and inter-
annotator disagreement: Empirical examinations. In
Proceedings of the First International Workshop on
Generative Approaches to Lexicon.
</reference>
<page confidence="0.999403">
88
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.475714">
<title confidence="0.999922">Measuring Variability in Sentence Ordering for News Summarization</title>
<author confidence="0.910806">Rebecca Necip Fazil John M J Judith L P Judith D</author>
<affiliation confidence="0.867318166666667">of Computer for Advanced Computer University of Maryland, College for Computational Learning Systems, Columbia for Computing of Information Studies, University of Maryland, College</affiliation>
<email confidence="0.999688">jklavans@umd.edu</email>
<abstract confidence="0.999071727272727">The issue of sentence ordering is an important one for natural language tasks such as multi-document summarization, yet there has not been a quantitative exploration of the range of acceptable sentence orderings for short texts. We present results of a sentence reordering experiment with three experimental conditions. Our findings indicate a very high degree of variability in the orderings that the eighteen subjects produce. In addition, the variability of reorderings is significantly greater when the initial ordering seen by subjects is different from the original summary. We conclude that evaluation of sentence ordering should use multiple reference orderings. Our evaluation presents several metrics that might prove useful in assessing against multiple references. We conclude with a deeper set of questions: (a) what sorts of independent assessments of quality of the different reference orderings could be made and (b) whether a large enough test set would obviate the need for such independent means of quality assessment.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Althaus</author>
<author>N Karamanis</author>
<author>A Koller</author>
</authors>
<title>Computing locally coherent discourses.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="5489" citStr="Althaus et al. (2004)" startWordPosition="829" endWordPosition="832">d clustered related sentences, then imposed an ordering as directed by the chronology. Okazaki et al. (2004) further improved the chronological ordering algorithm by first arranging sentences in simple chronological order, then performing local reorderings. More recent work includes probabilistic approaches that try to model the structure of text (Lapata, 2003) and algorithms that use large corpora to learn an ordering and then apply it to the summary under consideration (Bollegala et al., 2005). Conroy et al. (2006) treat sentence ordering as a Traveling Salesperson Problem (TSP), similar to Althaus et al. (2004). Starting from a designated first sentence, they reorder the other sentences so that the sum of the distances between adjacent sentences is minimized. The distance (cjk) between any pair of sentences j and k is computed by first obtaining a similarity score (bjk) for the pair, and then normalizing this score: bjk cjk = 1 − (cjj = 0) (1) Vbjj ,lbkk Because a typical multi-document extractive summary usually contains a small number of sentences, a near-optimal solution to this TSP can be found either by exhaustive search or by random sampling. In this paper, we use this TSP ordering algorithm t</context>
</contexts>
<marker>Althaus, Karamanis, Koller, 2004</marker>
<rawString>E. Althaus, N. Karamanis and A. Koller. 2004. Computing locally coherent discourses. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>N Elhadad</author>
<author>K McKeown</author>
</authors>
<title>Inferring strategies for sentence ordering in multidocument news summarization.</title>
<date>2002</date>
<journal>JAIR,</journal>
<pages>17--35</pages>
<contexts>
<context position="2319" citStr="Barzilay et al., 2002" startWordPosition="326" endWordPosition="329">n a multi-document extractive summary is an important problem that has received little attention until recently. Sentence ordering, along with other factors that affect coherence and readability, is of particular concern for multi-document summarization, where different source articles contribute sentences to a summary. We conducted an exploratory study to determine how much variation humans would produce in a reordering task under different experimental conditions, in order to assess the issues for evaluating automated reordering. While a good ordering is essential for summary comprehension (Barzilay et al., 2002), and recent work on sentence ordering (Bollegala et al., 2006) does show promise, it is important to note that determining an optimal sentence ordering for a given summary may not be feasible. The question for evaluation of ordering is whether there is a single best ordering that humans will converge on, or that would lead to maximum reading comprehension, or that would maximize another extrinsic summary evaluation measure. On texts of approximately the same length as summaries we look at here, Karamanis et al. (2005) found that experts produce different sentence orderings for expressing data</context>
<context position="4663" citStr="Barzilay et al. (2002)" startWordPosition="699" endWordPosition="702">he sentence ordering task and presents the automated sentence ordering algorithm used in our experiments. Section 3 describes the experimental design. Sections 4 and 5 present quantitative analyses of the results of the experiment. Section 6 discusses related work. We discuss our results in Section 7 and conclude in Section 8. 2 Sentence Ordering Algorithms A number of approaches have been applied to sentence ordering for multi-document summarization (Radev and McKeown, 1999). The first techniques exploited chronological information in the documents (McKeown et al., 1999; Lin and Hovy, 2002). Barzilay et al. (2002) were the first to discuss the impact of sentence ordering in the context of multi-document summarization in the news genre. They used an augmented chronological ordering algorithm that first identified and clustered related sentences, then imposed an ordering as directed by the chronology. Okazaki et al. (2004) further improved the chronological ordering algorithm by first arranging sentences in simple chronological order, then performing local reorderings. More recent work includes probabilistic approaches that try to model the structure of text (Lapata, 2003) and algorithms that use large c</context>
<context position="26516" citStr="Barzilay et al. (2002)" startWordPosition="4524" endWordPosition="4527">ition. Another important difference is the nature of the ordering task itself—the task we chose was a simple text-to-text ordering task whereas their task was a modified fact-to-text ordering task, i.e., although their subjects saw sentences, it is not clear whether they were simply sentences corresponding to database facts and devoid of connectives, pronouns etc. We applied analysis of variance to all pairs of subjects’ T scores directly, rather than to the specialized scores that they compute, so we cannot directly compare results. However, the amount of variation we find seems far greater. Barzilay et al. (2002) also conducted experiments that asked human subjects to create alternative orderings and showed that subjects rarely agreed on a single ordering for the given text. However, they did not conduct a detailed quantitative analysis of the amount of variability found in the set of human reorderings. Okazaki et al. (2004) do ask the human judges to provide a corrected ordering for each ordering that they grade during evaluation. However, only one corrected ordering per summary is created. In addition, the number of humans subjects used for the evaluation task and the measures taken for circumventin</context>
</contexts>
<marker>Barzilay, Elhadad, McKeown, 2002</marker>
<rawString>R. Barzilay, N. Elhadad, and K. McKeown. 2002. Inferring strategies for sentence ordering in multidocument news summarization. JAIR, 17:35–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bollegala</author>
<author>N Okazaki</author>
<author>M Ishizuka</author>
</authors>
<title>A machine learning approach to sentence ordering for multi-document summarization and its evaluation.</title>
<date>2005</date>
<booktitle>In Proceedings of IJCNLP.</booktitle>
<contexts>
<context position="5368" citStr="Bollegala et al., 2005" startWordPosition="810" endWordPosition="813">-document summarization in the news genre. They used an augmented chronological ordering algorithm that first identified and clustered related sentences, then imposed an ordering as directed by the chronology. Okazaki et al. (2004) further improved the chronological ordering algorithm by first arranging sentences in simple chronological order, then performing local reorderings. More recent work includes probabilistic approaches that try to model the structure of text (Lapata, 2003) and algorithms that use large corpora to learn an ordering and then apply it to the summary under consideration (Bollegala et al., 2005). Conroy et al. (2006) treat sentence ordering as a Traveling Salesperson Problem (TSP), similar to Althaus et al. (2004). Starting from a designated first sentence, they reorder the other sentences so that the sum of the distances between adjacent sentences is minimized. The distance (cjk) between any pair of sentences j and k is computed by first obtaining a similarity score (bjk) for the pair, and then normalizing this score: bjk cjk = 1 − (cjj = 0) (1) Vbjj ,lbkk Because a typical multi-document extractive summary usually contains a small number of sentences, a near-optimal solution to thi</context>
</contexts>
<marker>Bollegala, Okazaki, Ishizuka, 2005</marker>
<rawString>D. Bollegala, N. Okazaki, and M. Ishizuka. 2005. A machine learning approach to sentence ordering for multi-document summarization and its evaluation. In Proceedings of IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bollegala</author>
<author>N Okazaki</author>
<author>M Ishizuka</author>
</authors>
<title>A bottom-up approach to sentence ordering for multidocument summarization.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING/ACL.</booktitle>
<contexts>
<context position="2382" citStr="Bollegala et al., 2006" startWordPosition="336" endWordPosition="339">that has received little attention until recently. Sentence ordering, along with other factors that affect coherence and readability, is of particular concern for multi-document summarization, where different source articles contribute sentences to a summary. We conducted an exploratory study to determine how much variation humans would produce in a reordering task under different experimental conditions, in order to assess the issues for evaluating automated reordering. While a good ordering is essential for summary comprehension (Barzilay et al., 2002), and recent work on sentence ordering (Bollegala et al., 2006) does show promise, it is important to note that determining an optimal sentence ordering for a given summary may not be feasible. The question for evaluation of ordering is whether there is a single best ordering that humans will converge on, or that would lead to maximum reading comprehension, or that would maximize another extrinsic summary evaluation measure. On texts of approximately the same length as summaries we look at here, Karamanis et al. (2005) found that experts produce different sentence orderings for expressing database facts about archaeology. We find that summaries of newswir</context>
</contexts>
<marker>Bollegala, Okazaki, Ishizuka, 2006</marker>
<rawString>D. Bollegala, N. Okazaki, and M. Ishizuka. 2006. A bottom-up approach to sentence ordering for multidocument summarization. In Proceedings of COLING/ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bruce</author>
<author>J Wiebe</author>
</authors>
<title>Word-sense distinguishability and inter-coder agreement.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING/ACL.</booktitle>
<contexts>
<context position="9791" citStr="Bruce and Wiebe (1998)" startWordPosition="1554" endWordPosition="1557">nt that is likely to vary between individuals. Because the three experimental conditions had the same instructions, we believe the significant differences in amount of reordering across conditions is a real effect rather than an artifact. 4 Variability across Experimental Conditions To measure the variability across the experimental conditions, we developed two methods that assign a global score to each set of reorderings by comparing them to a particular reference point. 4.1 Method 1: Confusion Matrices and κ In NLP evaluation, confusion matrices have typically been used in annotation tasks (Bruce and Wiebe (1998), Tomuro (2001)) where the matrix represents the comparison of two judges, and the κ inter-annotator agreement metric value (Cohen, 1960) gives a measure of the amount of agreement between the two judges, after factoring out chance. However, κ has been used to quantify the observed distribution in confusion matrices of other types in a range of other fields and applications (e.g., assessing map accuracy (Hardin, 1999), or optical recognition (Ross et al., 2002)). Here we use it to quantify variability within sets of reorderings for a summary. Given a representation of each summary as a set of </context>
</contexts>
<marker>Bruce, Wiebe, 1998</marker>
<rawString>R. Bruce and J. Wiebe. 1998. Word-sense distinguishability and inter-coder agreement. In Proceedings of COLING/ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cohen</author>
</authors>
<title>A coefficient of agreement for nominal scales.</title>
<date>1960</date>
<booktitle>Educational and Psychological Measurement,</booktitle>
<pages>20--37</pages>
<contexts>
<context position="9928" citStr="Cohen, 1960" startWordPosition="1576" endWordPosition="1577">erences in amount of reordering across conditions is a real effect rather than an artifact. 4 Variability across Experimental Conditions To measure the variability across the experimental conditions, we developed two methods that assign a global score to each set of reorderings by comparing them to a particular reference point. 4.1 Method 1: Confusion Matrices and κ In NLP evaluation, confusion matrices have typically been used in annotation tasks (Bruce and Wiebe (1998), Tomuro (2001)) where the matrix represents the comparison of two judges, and the κ inter-annotator agreement metric value (Cohen, 1960) gives a measure of the amount of agreement between the two judges, after factoring out chance. However, κ has been used to quantify the observed distribution in confusion matrices of other types in a range of other fields and applications (e.g., assessing map accuracy (Hardin, 1999), or optical recognition (Ross et al., 2002)). Here we use it to quantify variability within sets of reorderings for a summary. Given a representation of each summary as a set of sequentially indexed sentences (e.g., 1, 2, 3, 4, 1 2 3 4 5 1 5 0 0 1 0 2 1 3 1 0 1 3 0 2 2 1 1 4 0 0 2 2 2 5 0 1 1 2 2 Figure 2: Confusi</context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>J. Cohen. 1960. A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20:37–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Conroy</author>
<author>J D Schlesinger</author>
<author>D P O’Leary</author>
<author>J Goldstein</author>
</authors>
<title>Back to basics: Classy</title>
<date>2006</date>
<booktitle>In Proceedings of DUC’06.</booktitle>
<marker>Conroy, Schlesinger, O’Leary, Goldstein, 2006</marker>
<rawString>J. M. Conroy, J. D. Schlesinger, D. P. O’Leary, and J. Goldstein. 2006. Back to basics: Classy 2006. In Proceedings of DUC’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Hardin</author>
</authors>
<title>Comparing main diagonal entries in normalized confusion matrices: A bootstrapping approach.</title>
<date>1999</date>
<booktitle>In IEEE International GRS Symposium.</booktitle>
<contexts>
<context position="10212" citStr="Hardin, 1999" startWordPosition="1623" endWordPosition="1624">omparing them to a particular reference point. 4.1 Method 1: Confusion Matrices and κ In NLP evaluation, confusion matrices have typically been used in annotation tasks (Bruce and Wiebe (1998), Tomuro (2001)) where the matrix represents the comparison of two judges, and the κ inter-annotator agreement metric value (Cohen, 1960) gives a measure of the amount of agreement between the two judges, after factoring out chance. However, κ has been used to quantify the observed distribution in confusion matrices of other types in a range of other fields and applications (e.g., assessing map accuracy (Hardin, 1999), or optical recognition (Ross et al., 2002)). Here we use it to quantify variability within sets of reorderings for a summary. Given a representation of each summary as a set of sequentially indexed sentences (e.g., 1, 2, 3, 4, 1 2 3 4 5 1 5 0 0 1 0 2 1 3 1 0 1 3 0 2 2 1 1 4 0 0 2 2 2 5 0 1 1 2 2 Figure 2: Confusion matrix for a set of reorderings (summary 1, condition=CO, reference=O; κ=0.33) 5), and of each reordering as a corresponding sequence of positional indices, we can create confusion matrices as in Figure 2. The rows represent the sequential positions of the summary sentences in one</context>
</contexts>
<marker>Hardin, 1999</marker>
<rawString>P. Hardin. 1999. Comparing main diagonal entries in normalized confusion matrices: A bootstrapping approach. In IEEE International GRS Symposium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Harman</author>
</authors>
<date>2004</date>
<booktitle>Proceedings of DUC’04.</booktitle>
<location>Boston, MA.</location>
<contexts>
<context position="6683" citStr="Harman, 2004" startWordPosition="1025" endWordPosition="1026">rdering algorithm to construct one of the three experimental conditions. 3 Experimental Design We designed an experiment to test two hypotheses: (1) that the initial orderings presented to the human subjects have a statistically significant impact on the reorderings that they create, and (2) that the set of individual human reorderings exhibits a significant amount of variability. For our experiment, we randomly chose nine 100- word human-written summaries† out of 200 human written summaries produced by NIST; they were used as references to evaluate extractive multidocument summaries in 2004 (Harman, 2004). We later retrieved the quality judgments performed by NIST assessors on seven of the summaries; the remaining two were used as a reference model for assessors and had no quality judgments. The seven summaries for which we had judgments were all given high ratings of 1 or 2 (out of 5) on seven questions such as, Does the summary build from sentence to sentence to a coherent body of information about the topic? The nine summaries were evenly divided into three different groups: 51_3, 54_6 and 57_9. For each summary, we used three orderings: • O: the original ordering of sentences in the summar</context>
</contexts>
<marker>Harman, 2004</marker>
<rawString>D. Harman. 2004. Proceedings of DUC’04. Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Karamanis</author>
<author>C Mellish</author>
</authors>
<title>Using a corpus of sentence orderings defined by many experts to evalute metrics of coherence for text structuring.</title>
<date>2005</date>
<booktitle>In Proceedings of ENLG. M. Lapata.</booktitle>
<contexts>
<context position="25249" citStr="Karamanis and Mellish (2005)" startWordPosition="4327" endWordPosition="4330"> summaries had significantly distinct mean T values. If we were to apply Tukey’s HSD to the pair id factor, which was also highly significant as a predictor of T values, it becomes difficult to summarize the significant differences. There are C(153, 2) = 11, 628 pairwise comparisons of pairs of subjects; of these, 4, 225 were found to be statistically significant, using Tukey’s method, and an analogous table to Table 2 would have 210 lines. This demonstrates that, overall, there is a large amount of variability among the individuals’ reorderings. 6 Related Work on Evaluating Sentence Ordering Karamanis and Mellish (2005) also measure the amount of variability between human subjects. However, there are several dimensions of contrast between our experiment and theirs: Their experiment operates in a very distinct domain (archaeology) and genre (descriptions of museum artifacts) whereas we use domain-independent multidocument summaries derived from news articles. We use ordinary, English-speaking volunteers as compared to the domain and genre experts that they employ (archaeologists trained in museum labeling). In terms of the experimental design, we use a Latin square design with three experimental condi86 tions</context>
</contexts>
<marker>Karamanis, Mellish, 2005</marker>
<rawString>N. Karamanis and C. Mellish. 2005. Using a corpus of sentence orderings defined by many experts to evalute metrics of coherence for text structuring. In Proceedings of ENLG. M. Lapata. 2003. Probabilistic text structuring: Experiments with sentence ordering. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lapata</author>
</authors>
<title>Automatic evaluation of information ordering: Kendall’s tau.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>4</issue>
<contexts>
<context position="15472" citStr="Lapata, 2006" startWordPosition="2596" endWordPosition="2597">e question we want to assess is whether the pattern given by a set of reorderings taken as a whole correlates well with the initial presentation order. We O 1 2 3 4 5 6 S1 2 3 4 5 6 1 S2 6 1 2 3 4 5 S4 6 1 2 3 4 5 S3 2 3 4 5 6 1 S5 2 3 4 5 6 1 Ss 6 1 2 3 4 5 Mean 4 2 3 4 5 3 Figure 3: A hypothetical example illustrating Means Vectors compute means vectors for each condition for each summary, giving 27 such vectors. We compare each means vector representing a set of reorderings to each initial ordering O, R and T using three correlation coefficients: Pearson’s r, Spearman’s ρ, and Kendall’s τ (Lapata, 2006). The three correlation coefficients test the closeness of two series of numbers, or two variables x and y, in different ways. Pearson’s r is a parametric test of whether there is a perfect linear relation between the two variables. Spearman’s ρ and Kendall’s τ are non-parametric tests. Spearman’s ρ is computed by replacing the variable values by their rank and computing the correlation. Kendall’s τ is based on counting the number of pairs xi, xi+1 and yi, yi+1 where the deltas of both pairs have the same sign. In sum, the three metrics test whether x and y are in a linear relation, a rank-pre</context>
</contexts>
<marker>Lapata, 2006</marker>
<rawString>M. Lapata. 2006. Automatic evaluation of information ordering: Kendall’s tau. Computational Linguistics, 32(4):471–484.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-Y Lin</author>
<author>E Hovy</author>
</authors>
<title>Automated multidocument summarization in neats.</title>
<date>2002</date>
<booktitle>In Proceedings of HLT.</booktitle>
<contexts>
<context position="4639" citStr="Lin and Hovy, 2002" startWordPosition="695" endWordPosition="698">some background for the sentence ordering task and presents the automated sentence ordering algorithm used in our experiments. Section 3 describes the experimental design. Sections 4 and 5 present quantitative analyses of the results of the experiment. Section 6 discusses related work. We discuss our results in Section 7 and conclude in Section 8. 2 Sentence Ordering Algorithms A number of approaches have been applied to sentence ordering for multi-document summarization (Radev and McKeown, 1999). The first techniques exploited chronological information in the documents (McKeown et al., 1999; Lin and Hovy, 2002). Barzilay et al. (2002) were the first to discuss the impact of sentence ordering in the context of multi-document summarization in the news genre. They used an augmented chronological ordering algorithm that first identified and clustered related sentences, then imposed an ordering as directed by the chronology. Okazaki et al. (2004) further improved the chronological ordering algorithm by first arranging sentences in simple chronological order, then performing local reorderings. More recent work includes probabilistic approaches that try to model the structure of text (Lapata, 2003) and alg</context>
</contexts>
<marker>Lin, Hovy, 2002</marker>
<rawString>C-Y. Lin and E. Hovy. 2002. Automated multidocument summarization in neats. In Proceedings of HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K McKeown</author>
<author>J Klavans</author>
<author>V Hatzivassiloglou</author>
<author>R Barzilay</author>
<author>E Eskin</author>
</authors>
<title>Towards multidocument summarization by reformulation: Progress and prospects.</title>
<date>1999</date>
<booktitle>In Proceedings of AAAI/IAAI.</booktitle>
<contexts>
<context position="4618" citStr="McKeown et al., 1999" startWordPosition="691" endWordPosition="694">next section provides some background for the sentence ordering task and presents the automated sentence ordering algorithm used in our experiments. Section 3 describes the experimental design. Sections 4 and 5 present quantitative analyses of the results of the experiment. Section 6 discusses related work. We discuss our results in Section 7 and conclude in Section 8. 2 Sentence Ordering Algorithms A number of approaches have been applied to sentence ordering for multi-document summarization (Radev and McKeown, 1999). The first techniques exploited chronological information in the documents (McKeown et al., 1999; Lin and Hovy, 2002). Barzilay et al. (2002) were the first to discuss the impact of sentence ordering in the context of multi-document summarization in the news genre. They used an augmented chronological ordering algorithm that first identified and clustered related sentences, then imposed an ordering as directed by the chronology. Okazaki et al. (2004) further improved the chronological ordering algorithm by first arranging sentences in simple chronological order, then performing local reorderings. More recent work includes probabilistic approaches that try to model the structure of text (</context>
</contexts>
<marker>McKeown, Klavans, Hatzivassiloglou, Barzilay, Eskin, 1999</marker>
<rawString>K. McKeown, J. Klavans, V. Hatzivassiloglou, R. Barzilay, and E. Eskin. 1999. Towards multidocument summarization by reformulation: Progress and prospects. In Proceedings of AAAI/IAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Okazaki</author>
<author>Y Matsuo</author>
<author>M Ishizuka</author>
</authors>
<title>Improving chronological sentence ordering by precedence relation.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="4976" citStr="Okazaki et al. (2004)" startWordPosition="750" endWordPosition="753"> in Section 8. 2 Sentence Ordering Algorithms A number of approaches have been applied to sentence ordering for multi-document summarization (Radev and McKeown, 1999). The first techniques exploited chronological information in the documents (McKeown et al., 1999; Lin and Hovy, 2002). Barzilay et al. (2002) were the first to discuss the impact of sentence ordering in the context of multi-document summarization in the news genre. They used an augmented chronological ordering algorithm that first identified and clustered related sentences, then imposed an ordering as directed by the chronology. Okazaki et al. (2004) further improved the chronological ordering algorithm by first arranging sentences in simple chronological order, then performing local reorderings. More recent work includes probabilistic approaches that try to model the structure of text (Lapata, 2003) and algorithms that use large corpora to learn an ordering and then apply it to the summary under consideration (Bollegala et al., 2005). Conroy et al. (2006) treat sentence ordering as a Traveling Salesperson Problem (TSP), similar to Althaus et al. (2004). Starting from a designated first sentence, they reorder the other sentences so that t</context>
<context position="26834" citStr="Okazaki et al. (2004)" startWordPosition="4575" endWordPosition="4578"> facts and devoid of connectives, pronouns etc. We applied analysis of variance to all pairs of subjects’ T scores directly, rather than to the specialized scores that they compute, so we cannot directly compare results. However, the amount of variation we find seems far greater. Barzilay et al. (2002) also conducted experiments that asked human subjects to create alternative orderings and showed that subjects rarely agreed on a single ordering for the given text. However, they did not conduct a detailed quantitative analysis of the amount of variability found in the set of human reorderings. Okazaki et al. (2004) do ask the human judges to provide a corrected ordering for each ordering that they grade during evaluation. However, only one corrected ordering per summary is created. In addition, the number of humans subjects used for the evaluation task and the measures taken for circumventing bias, if any, are not reported. By contrast, our experiment uses a Latin Square with fully randomized presentation order to circumvent the introduction of any bias. Moreover, we create 18 corrected orderings for each summary and are, therefore, in a much better position to draw general conclusions about variability</context>
</contexts>
<marker>Okazaki, Matsuo, Ishizuka, 2004</marker>
<rawString>N. Okazaki, Y. Matsuo, and M. Ishizuka. 2004. Improving chronological sentence ordering by precedence relation. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Passonneau</author>
<author>A Nenkova</author>
<author>K McKeown</author>
<author>S Sigelman</author>
</authors>
<title>Applying the pyramid method in DUC</title>
<date>2005</date>
<booktitle>In Proceedings of DUC’05.</booktitle>
<contexts>
<context position="30617" citStr="Passonneau et al., 2005" startWordPosition="5212" endWordPosition="5215">erings most closely reproduce the original summary, followed by the CT condition. We had the impression on reading this summary that there is a relative lack of use of devices linking the sentences to one another, such as discourse connectives, lexical cohesion, or anaphora. This raises the question of whether such devices are less necessary to create readability in short texts. While the seven summaries for which we found quality ratings were of roughly equal quality (summary 8 had the highest possible quality ratings), the DUC quality assessments have been shown to differ between assessors (Passonneau et al., 2005). In the DUC summarization evaluations, performance of systems is assessed by averaging over **Document Set: D30015, NIST Author ID: E 87 large numbers of conditions, e.g., different document sets with different characteristics. We believe our lack of knowledge about the range of factors affecting the ordering task, and the way they interact, can be partly compensated for by evaluating ordering algorithms over a wide range of inputs. 8 Conclusions and Future Work We conducted a reordering experiment that aims to gauge the difficulty of the sentence ordering task in the context of short, domain</context>
</contexts>
<marker>Passonneau, Nenkova, McKeown, Sigelman, 2005</marker>
<rawString>R. Passonneau, A. Nenkova, K. McKeown, and S. Sigelman. 2005. Applying the pyramid method in DUC 2005. In Proceedings of DUC’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D R Radev</author>
<author>K McKeown</author>
</authors>
<title>Generating natural language summaries from multiple on-line sources.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<pages>24--469</pages>
<contexts>
<context position="4521" citStr="Radev and McKeown, 1999" startWordPosition="677" endWordPosition="680">eate. • The set of individual human reorderings ex81 hibits a significant amount of variability. The next section provides some background for the sentence ordering task and presents the automated sentence ordering algorithm used in our experiments. Section 3 describes the experimental design. Sections 4 and 5 present quantitative analyses of the results of the experiment. Section 6 discusses related work. We discuss our results in Section 7 and conclude in Section 8. 2 Sentence Ordering Algorithms A number of approaches have been applied to sentence ordering for multi-document summarization (Radev and McKeown, 1999). The first techniques exploited chronological information in the documents (McKeown et al., 1999; Lin and Hovy, 2002). Barzilay et al. (2002) were the first to discuss the impact of sentence ordering in the context of multi-document summarization in the news genre. They used an augmented chronological ordering algorithm that first identified and clustered related sentences, then imposed an ordering as directed by the chronology. Okazaki et al. (2004) further improved the chronological ordering algorithm by first arranging sentences in simple chronological order, then performing local reorderi</context>
</contexts>
<marker>Radev, McKeown, 1999</marker>
<rawString>D. R. Radev and K. McKeown. 1999. Generating natural language summaries from multiple on-line sources. Computational Linguistics, 24:469–500.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T D Ross</author>
<author>L A Westerkamp</author>
<author>R L Dilsavor</author>
<author>J C Mossing</author>
<author>E G Zelnio</author>
</authors>
<title>Performance measures for summarizing confusion matrices: The AFRL COMPASE approach.</title>
<date>2002</date>
<booktitle>In SPIE International Society for Optical Engineering Proceedings Series.</booktitle>
<contexts>
<context position="10256" citStr="Ross et al., 2002" startWordPosition="1629" endWordPosition="1632"> point. 4.1 Method 1: Confusion Matrices and κ In NLP evaluation, confusion matrices have typically been used in annotation tasks (Bruce and Wiebe (1998), Tomuro (2001)) where the matrix represents the comparison of two judges, and the κ inter-annotator agreement metric value (Cohen, 1960) gives a measure of the amount of agreement between the two judges, after factoring out chance. However, κ has been used to quantify the observed distribution in confusion matrices of other types in a range of other fields and applications (e.g., assessing map accuracy (Hardin, 1999), or optical recognition (Ross et al., 2002)). Here we use it to quantify variability within sets of reorderings for a summary. Given a representation of each summary as a set of sequentially indexed sentences (e.g., 1, 2, 3, 4, 1 2 3 4 5 1 5 0 0 1 0 2 1 3 1 0 1 3 0 2 2 1 1 4 0 0 2 2 2 5 0 1 1 2 2 Figure 2: Confusion matrix for a set of reorderings (summary 1, condition=CO, reference=O; κ=0.33) 5), and of each reordering as a corresponding sequence of positional indices, we can create confusion matrices as in Figure 2. The rows represent the sequential positions of the summary sentences in one of the three initial orders that subjects w</context>
</contexts>
<marker>Ross, Westerkamp, Dilsavor, Mossing, Zelnio, 2002</marker>
<rawString>T. D. Ross, L. A. Westerkamp; R. L. Dilsavor, J. C. Mossing, and E. G. Zelnio. 2002. Performance measures for summarizing confusion matrices: The AFRL COMPASE approach. In SPIE International Society for Optical Engineering Proceedings Series.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Tomuro</author>
</authors>
<title>Systematic polysemy and interannotator disagreement: Empirical examinations.</title>
<date>2001</date>
<booktitle>In Proceedings of the First International Workshop on Generative Approaches to Lexicon.</booktitle>
<contexts>
<context position="9806" citStr="Tomuro (2001)" startWordPosition="1558" endWordPosition="1559">y between individuals. Because the three experimental conditions had the same instructions, we believe the significant differences in amount of reordering across conditions is a real effect rather than an artifact. 4 Variability across Experimental Conditions To measure the variability across the experimental conditions, we developed two methods that assign a global score to each set of reorderings by comparing them to a particular reference point. 4.1 Method 1: Confusion Matrices and κ In NLP evaluation, confusion matrices have typically been used in annotation tasks (Bruce and Wiebe (1998), Tomuro (2001)) where the matrix represents the comparison of two judges, and the κ inter-annotator agreement metric value (Cohen, 1960) gives a measure of the amount of agreement between the two judges, after factoring out chance. However, κ has been used to quantify the observed distribution in confusion matrices of other types in a range of other fields and applications (e.g., assessing map accuracy (Hardin, 1999), or optical recognition (Ross et al., 2002)). Here we use it to quantify variability within sets of reorderings for a summary. Given a representation of each summary as a set of sequentially in</context>
</contexts>
<marker>Tomuro, 2001</marker>
<rawString>N. Tomuro. 2001. Systematic polysemy and interannotator disagreement: Empirical examinations. In Proceedings of the First International Workshop on Generative Approaches to Lexicon.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>