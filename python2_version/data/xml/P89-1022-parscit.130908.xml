<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000481">
<title confidence="0.729825">
AUTOMATIC ACQUISITION OF THE LEXICAL SEMANTICS OF VERBS
FROM SENTENCE FRAMES*
</title>
<author confidence="0.941441">
Mort Webster and Mitch Marcus
</author>
<affiliation confidence="0.9977005">
Department of Computer and Information Science
University of Pennsylvania
</affiliation>
<address confidence="0.885892">
200 S. 33rd Street
Philadelphia, PA 19104
</address>
<sectionHeader confidence="0.564651" genericHeader="abstract">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.9999233125">
This paper presents a computational model of verb
acquisition which uses what we will call the princi-
ple of structured overcommitment to eliminate the
need for negative evidence. The learner escapes
from the need to be told that certain possibili-
ties cannot occur (i.e., are &amp;quot;ungrammatical&amp;quot;) by
one simple expedient: It assumes that all proper-
ties it has observed are either obligatory or for-
bidden until it sees otherwise, at which point it
decides that what it thought was either obliga-
tory or forbidden is merely optional. This model
is built upon a classification of verbs based upon
a simple three-valued set of features which repre-
sents key aspects of a verb&apos;s syntactic structure,
its predicate/argument structure, and the map-
ping between them.
</bodyText>
<sectionHeader confidence="0.99972" genericHeader="keywords">
1 INTRODUCTION
</sectionHeader>
<bodyText confidence="0.998989575757576">
The problem of how language is learned is per-
haps the most difficult puzzle in language under-
standing. It is necessary to understand learning in
order to understand how people use and organize
language. To build truly robust natural language
systems, we must ultimately understand how to
enable our systems to learn new forms themselves.
Consider the problem of learning new lexical
items in context. To take a specific example, how
is it that a child can learn the difference between
the verbs look and see (inspired by Landau and
Gleitman(1985) )? They clearly have similar core
meanings, namely &amp;quot;perceive by sight&amp;quot;. One ini-
tially attractive and widely-held hypothesis is that
This work was partially supported by the DARPA
grant N00014-85-K0018, and ARO grant DAA29-84-9-
0027. The authors also wish to thank Beth Levin and the
anonymous reviewers of this paper for many helpful com-
ments. We also benefited greatly from discussion of issues
of verb acquisition in children with Lila Gleitman.
word meaning is learned directly by observation
of the surrounding non-linguistic context. While
this hypothesis ultimately only begs the question,
it also runs into immediate substantive difficulties
here, since there is usually looking going on at the
same time as seeing and vice versa. But how can
one learn that these verbs differ in that look is
an active verb and see is stative? This difference,
although difficult to observe in the environment,
is clearly marked in the different syntactic frames
the two verbs are found in. For example, see, be-
ing a stative perception verb, can take a sentence
complement:
</bodyText>
<listItem confidence="0.97956125">
(1) John saw that Mary was reading.
while look cannot:
(2)* John looked that Mary was reading.
Also look can be used in an imperative,
(3) Look at the ball!
while it sounds a bit strange to command someone
to see,
(4) ? See the ball!
</listItem>
<bodyText confidence="0.9996774">
(Examples like &amp;quot;look Jane, see Spot run!&amp;quot;
notwithstanding.) This difference reflects the fact
that one can command someone to direct their
eyes (look) but not to mentally perceive what
someone else perceives (see). As this example
shows, there are clear semantic differences between
verbs that are reflected in the syntax, but not ob-
vious by observation alone. The fact that children
are able to correctly learn the meanings of look and
see, as well as hundreds of other verbs, with mini-
mal exposure suggests that there is some correla-
tion between syntax and semantics that facilitates
the learning of word meaning.
Still, this and similar arguments ignore the fact
that children do not have access to the negative
</bodyText>
<page confidence="0.995562">
177
</page>
<bodyText confidence="0.999685444444444">
evidence crucial to establishing the active/stative
distinction of the look/see pair. Children cannot
know that sentences like (2) and (4) do not oc-
cur, and it is well established that children are
not corrected for syntactic errors. Such evidence
renders highly implausible models like that of
Pinker(1987), which depend crucially on negative
examples. How then can this semantic/syntactic
correlation be exploited?
</bodyText>
<sectionHeader confidence="0.999722333333333" genericHeader="introduction">
2 STRUCTURED OVERCOM-
MITMENT AND A LEARNING
ALGORITHM
</sectionHeader>
<bodyText confidence="0.999894710843374">
In this paper, we will present a computational
model of verb acquisition which uses what we will
call the principle of structured overcommitment to
eliminate the need for such negative evidence. In
essence, our learner learns by initially jumping to
the strongest conclusions it can, simply assum-
ing that everything within its descriptive system
that it hasn&apos;t seen will never occur, and then later
weakening its hypotheses when faced with contra-
dictory evidence. Thus, the learner escapes from
the need to be told that certain possibilities can-
not occur (i.e. are&amp;quot;ungrammaticar) by the simple
expedient of assuming that all properties it has ob-
served are either always obligatory or always for-
bidden. If and when the learner discovers that
it was wrong about such a strong assumption, it
reclassifies the property from either obligatory or
forbidden to merely optional.
Note that this learning principal requires that
no intermediate analysis is ever abandoned; anal-
yses are only further refined by the weakening of
universals (X ALWAYS has property P) to existen-
tials (X SOMETIMES has property P). It is in this
sense that the overcommitment iestructured.&amp;quot;
For such a learning strategy to work, it must be
the case that the set of features which underlies the
learning process are surface observable; the learner
must be able to determine of a particular instance
of (in this case) a verb structure whether some
property is true or false of it. This would seem to
imply, as far as we can tell, a commitment to the
notion of em learning as selection widely presup-
posed in the linguistic study of generative gram-
mar (as surveyed, for example, in Berwick(1985).
Thus, we propose that the problem of learning the
category of a verb does not require that a natu-
ral language understanding system synthesize em
de novo a new structure to represent its seman-
tic class, but rather that it determine to which of
a predefined, presumably innate set of verb cate-
gories a given verb belongs. In what follows below,
we argue that a relevant classification of verb cat-
egories can be represented by simple conjunctions
of a finite number of predefined quasi-independent
features with no need for disjunction or complex
boolean combinations of features.
Given such a feature set, the Principal of Struc-
tured Overcommitment defines a partial ordering
(or, if one prefers, a tangled hierarchy) of verbs as
follows: At the highest level of the hierarchy is a
set of verb classes where all the primary four fea-
tures, where defined, are either obligatory or for-
bidden. Under each of these&amp;quot;primary&amp;quot; categories
there are those categories which differ from it only
in that some category which is obligatory or for-
bidden in the higher class is optional in the lower
class. Note that both obligatory and forbidden
categories at one level lead to the same optional
category at the next level down.
The learning system, upon encountering a verb
for the first time, will necessarily classify that verb
into one of the ten top-level categories. This is be-
cause the learner assumes, for example, that if a
verb is used with an object upon first encounter.
that it always has an object; if it has no object,
that it never has an object, etc. The learner will
leave each verb classification unchanged upon en-
countering new verb instances until a usage occurs
that falsifies at least one of the current feature val-
ues. When encountering such a usage i.e. a verb
frame in which a property that is marked obliga-
tory is missing, or a property that is marked for-
bidden is present (there are no other possibilities)
- then the learner reclassifies the verb by mov-
ing down the hierarchy at least one level replacing
the OBLIGATORY or FORBIDDEN value of that
feature with OPTIONAL.
Note that, for each verb, the learner&apos;s classifica-
tion moves monotonically lower on this hierarchy,
until it eventually remains unchanged because the
learner has arrived at the correct value. (Thus
this learner embodies a kind of em learning in the
limit.
</bodyText>
<sectionHeader confidence="0.9981685" genericHeader="method">
3 THE FEATURE SET AND THE
VERB HIERARCHY
</sectionHeader>
<bodyText confidence="0.999947333333333">
As discussed above, our learner describes each
verb by means of a vector of features. Some
of these features describe syntactic properties
of the verb (e.g.&amp;quot;Takes an Object&amp;quot;), others de-
scribe aspects of the theta-structure (the predi-
cate/argument structure) of the verb (e.g. &amp;quot;Takes
</bodyText>
<page confidence="0.991307">
178
</page>
<bodyText confidence="0.999920289156627">
an Agent&amp;quot; ,&amp;quot;Takes a Theme&amp;quot;), while others de-
scribe some key properties of the mapping be-
tween theta-structure and syntactic structure
(e.g. &amp;quot;Theme Appears As Surface Object&amp;quot;). Most
of these features are three-valued; they de-
scribe properties that are either always true (e.g.
that&amp;quot;devour&amp;quot; always Takes An Object), always
false (e.g. that &amp;quot;fall&amp;quot; never Takes An Object) or
properties that are optionally true (e.g. that&amp;quot;eat&amp;quot;
optionally Takes An Object). Always true values
will be indicated as&amp;quot;+&amp;quot; below, always false values
as&amp;quot;—&amp;quot; and optional values as &amp;quot;0&amp;quot;.
All verbs are specified for the first three
features mentioned above: &amp;quot;Takes an Object&amp;quot;
(OBJ),&amp;quot;Takes an Agent&amp;quot; (AGT), and&amp;quot;Takes a
Theme&amp;quot; (THEME). All verbs that allow OBJ and
THEME are specified for&amp;quot;Theme Appears As Ob-
ject&amp;quot; (TAO), otherwise TAO is undefined. At the
highest level of the hierarchy is a set of verb classes
where all these primary features, where defined,
are either obligatory or forbidden. Thus there are
at most 10 primary verb types; of the eight for the
first three features, only two (+—+, and +++)
split for TAO.
The full set of features we assume include the
primary set of features (OBJ, AGT, THEME, and
TAO), as described above,,and a secondary set of
features which play a secondary role in the learn-
ing algorithm, as will be discussed below. These
secondary features are either thematic properties,
or correlations between thematic and syntactic
roles. The thematic properties are: LOC - takes a
locative; INST - takes an instrument; and DAT -
takes a dative. The first thematic-syntactic map-
ping feature &amp;quot;Instrument as Subject&amp;quot; is false if
no instrument can appear in subject position (or,
true if the subject is always an instrument, al-
though this is never the case.) The second such
feature &amp;quot;Theme as Chomeur&amp;quot; (TAC) is the only
non-trinary-valued feature in our learner; it spec-
ifies what preposition marks the theme when it is
not realized as subject or object. This feature, if
not —, either takes a lexical item (a preposition,
actually, as its value, or else the null string. We
treat verbs with double objects (e.g. &amp;quot;John gave
Mary the ball.&amp;quot;) as having a Dative as object, and
the theme as either marked by a null preposition
or, somewhat alternatively, as a bare NP chomeur.
(The facts we deal with here don&apos;t decide between
these two analyses.)
Note that this analysis does not make explict
what can appear as object; it is a claim of the
analysis that if the verb is OBJ:+ or OBJ:0 and is
TAO:— or TAO:0, then whatever other thematic
roles may occur can be realized as the object. This
may well be too strong, but we are still seeking a
counterexample.
Figure 1 shows our classification of some verb
classes of English, given this feature set. (This
classification owes much to Levin(1985), as well as
to Grimshaw(1983) and Jackendoff(1983).) This is
only the beginning of such a classification, clearly;
for example, we have concentrated our efforts
solely on verbs that take simple NPs as comple-
ments. Our intention is merely to provide a rich
enough set of verb classes to show that our clas-
sification scheme has merit, and that the learning
algorithm works. We believe that this set of fea-
tures is rich enough to describe not only the verb
classes covered here but other similar classes. It is
also our hope that an analysis of verbs with richer
complement structures will extend the set of fea-
tures without changing the analysis of the classes
currently handled.
It is interesting to note that although the partial
ordering of verb classes is defined in terms of fea-
tures defined over syntactic and theta structures,
that there appears to be at least a very strong se-
mantic reflex to the network. Due to lack of space,
we label verb classes in Figure 1 only with exem-
plars; here we give a list of either typical verbs in
the class, and/or a brief description of the class,
in semantic terms:
</bodyText>
<listItem confidence="0.990370083333333">
• Spray, load, inscribe, sow: Verbs of physical
contact that show the completive/noncomple-
tivel alternation. If completive, like &amp;quot;fill&amp;quot;.
• Clear, empty: Similar to spray/load, but if
completive, like &amp;quot;empty&amp;quot;.
• Wipe: Like clear, but no completive pattern.
• Throw: The following four verb classes all in-
volve an object and a trajectory. &amp;quot;Throw&amp;quot;
verbs don&apos;t require a terminus of the trajec-
tory.
• Present: Like &amp;quot;throw&amp;quot;, as far as we can tell.
• Give: Requires a terminus.
</listItem>
<bodyText confidence="0.530921">
1This is the difference between:
I loaded the hay on the truck.
and
I loaded the truck with hay.
In the second case, but not the first, there is a implication
that the truck is completely full.
</bodyText>
<page confidence="0.981469">
179
</page>
<table confidence="0.991247388059702">
OBJ AGT THEME TAO LOC INST DAT TAC INST
AS
SUBJ
SPRAY, + + 0 0 0 0 — with 0
LOAD .
CLEAR, + + 0 0 + o — of —
EMPTY .
WIPE -I- 4- 0 0 + 0 — — —
PRESENT + + + 0 — 0 0 with 0
.
SEARCH 0 + 0 — 0 0 — for —
MELT, 0 0 + 0 — 0 — — 0
BREAK .
PIERCE, + 0 -1- -I- — 0 — — 0
DESTROY
POKE, + + + o o o — — o
TOUCH
PUT + + + + + 0 — — 0
.
DEVOUR + + + + _ — — _ —
DYNAMITE
HUG + + + + 0 0 — — —
EAT 0 + 0 0 — 0 — — —
,
SWIM, — + — — 0 — — — —
FLY .
BREATHE 0 + 0 0 0 — — — —
FILL + + 0 — + 0 — with 0
THROW + + + 0 — — 0 &lt;null&gt; —
PUSH 0 + + 0 o o — at 0
, against
GIVE + + + 0 — — + &lt;null&gt; —
STAND + 0 + + 0 0 — — —
.
DIE, — — + —
FLOWER
RAIN — — 0 —
Figure 1: Some verb feature descriptions.
DE
( - - • - ) (••++) ( + - + • )
lialwastly Native) PUT DEVOUR (ONLY Was
+LOC -LOC I at spot)
1 •4; 4e
HUD
OLOC
N
(•••0)
4s, 11. (•0.4.)
POILE / TOUCH MUM
GIVE
- DAT
•DAT
[ 4111&amp;quot; sti■
PRESENT i
COAT THRCIWODAT
[ grotocAND 1
- TAC . with TAC &lt;ma&gt;
/ 1 I
(0++0)
PUSH (&amp;quot;00)
WIPE
.LCC, -TAC
40&amp;quot;.49
(00+0) (0•00)
NMENMS &amp;quot;4.
[MT BREATHE I
LOC, OINST OLOC, -INST
</table>
<figureCaption confidence="0.996689">
Figure 2: The verb hierarchy.
</figureCaption>
<figure confidence="0.997678714285714">
C....) ( • - )
(ALWAYS latwopomost (Solf-mootortti
Oral
/
( 0- ) (••0-)
RAIN PILL
(•••-)
(ALWAYS omplotivol
II
(0+0-)
SEARCH
[
MCC, TAC.with TAC-ol ]
SPRAY/LOAD CLEAR
</figure>
<page confidence="0.951995">
180
</page>
<listItem confidence="0.998941518518519">
• Poke, jab, stick, touch: Some object follows a
trajectory, resulting in surface contact.
• Bug: Surface contact, no trajectory.
• Fill: Inherently completive verbs.
• Search: Verbs that show a completive/non-
completive alternation that doesn&apos;t involve
physical contact.
• Die, flower: Change of state. Inherently non-
agentive.
• Break: Change of state, undergoing causitive
alternation.
• Destroy: Verbs of destruction.
• Pierce: Verbs of destruction involving a tra-
jectory.
• Devour, dynamite: Verbs of destruction with
incorporated instruments
• Put: Simple change of location.
• Eat: Verbs of ingesting allowing instruments
• Breathe: Verbs of ingesting that incorporate
instrument
• Fall, swim: Verbs of movement with incorpo-
rated theme and incorporated manner.
• Push: Exerting force; maybe something
moves, maybe not.
• Stand: Like &amp;quot;break&amp;quot;, but at a location.
• Rain: Verbs which have no agent, and incor-
porate their patient.
</listItem>
<bodyText confidence="0.999965365853659">
The set of verb classes that we have investigated
interacts with our learning algorithm to define the
partial order of verb classes illustrated schemati-
cally in Figure 2.
For simplicity, this diagram is organized by the
values of the four principle features of our system.
Each subsystem shown in brackets shares the same
principle features; the individual verbs within each
subsystem differ in secondary features as shown.
If one of the primary features is made optional,
the learning algorithm will map all verbs in each
subsystem into the same subordinate subsystem
as shown; of course, secondary feature values are
maintained as well. In some cases, a sub-hierarchy
within a subsystem shows the learning of a sec-
ondary feature.
We should note that several of the primary verb
classes in Figure 2 are unlabelled because they cor-
respond to no English verbs: The class &amp;quot;-- --&amp;quot;
would be the class of rain if it didn&apos;t allow forms
like &amp;quot;hail stones rained from the sky&amp;quot;, while the
class &amp;quot;4--++&amp;quot; would be the class of verbs like &amp;quot;de-
stroy&amp;quot; if they only took instruments as subjects.
Such classes may be artifacts of our analysis, or
they may be somewhat unlikely classes that are
filled in languages other than English.
Note that sub—patterns in the primary feature
subvector seem to signal semantic properties in a
straightforward way. So, for example, it appears
that verbs have the pattern {OBJ:+, THEME:+,
TAO:—} only if they are inherently completive;
consider &amp;quot;search&amp;quot; and &apos;fill&amp;quot;. Similarly, the rare
verbs that have the pattern {OBJ:—, THEME:—},
i.e those that are truly intransitive, appear to in-
corporate their theme into their meaning; a typi-
cal case here is &amp;quot;swim&amp;quot;. Verbs that are {OBJ:—,
AGT:—} (e.g. &amp;quot;die&amp;quot;) are inherently stative; they
allow no agency. Those verbs that are {AGT:+}
incorporate the instrument of the operation into
their meaning. We will have to say about this be-
low.
</bodyText>
<sectionHeader confidence="0.9999645" genericHeader="method">
4 THE LEARNING ALGORITHM
AT WORK
</sectionHeader>
<bodyText confidence="0.99995444">
Let us now see how the learning algorithm works
for a few verbs.
Our model presupposes that the learner receives
as input a parse of the sentence from which to de-
rive the subject and object grammatical relations,
and a representation of what NPs serve as agent,
patient, instrument and location. This may be
seen as begging the question of verb acquisition,
because, it may be asked, how could an intelligent
learner know what entities function as agent, pa-
tient, etc. without understanding the meaning of
the verb? Our model in fact presupposes that a
learner can distinguish between such general cat-
egories as animate, inanimate, instrument, and
locative from direct observation of the environ-
ment, without explicit support from verb meaning;
i.e. that it will be clear from observation em who is
acting on em what em where. This assumption is
not unreasonable; there is strong experimental ev-
idence that children do in fact perceive even some-
thing as subtle as the difference between animate
and inanimate motion well before the two word
stage (see Golinkoff et al, 1984). This notion that
agent, patient and the like can be derived from
direct observation (perhaps focussed by what NPs
</bodyText>
<page confidence="0.996767">
181
</page>
<bodyText confidence="0.992159766666667">
appear in the sentence) is a weak form of what
is sometimes called the em semantic bootstrap-
ping hypothesis (Pinker(1984)). The theory that
we present here is actually a combination of this
weak form of semantic bootstrapping with what is
called em syntactic bootstrapping, the notion that
syntactic frames alone offer enough information to
classify verbs (see Naigles, Gleitman, and Gleit-
man (in press) and Fisher, Gleitman and Gleit-
man(1988).)
With this preliminary out of the way, let&apos;s turn
to a simple example. Suppose the learner encoun-
ters the verb &amp;quot;break&amp;quot;, never seen before, in the
context
(6) The window broke.
The learner sees that the referent of &amp;quot;the window&amp;quot;
is inanimate, and thus is the theme. Given this
and the syntactic frame of (6), the learner can see
that em break (a) does not take an object, in this
case, (b) does not take an agent, and (c) takes
a patient. By Structured Overcommitment, the
learner therefore assumes that em break em never
takes an object, em never takes a subject, and em
always takes a patient. Thus, it classifies em break
as {OBJ:—, AGT:—, THEME:+, TAO:—} (if TAO
is undefined, it is assigned &amp;quot;—&amp;quot;). It also assumes
that em break is {DAT:—, LOC:—, INST:—, }
for similar reasons. This is the class of DIE, one
of the toplevel verb classes.
Next, suppose it sees
</bodyText>
<listItem confidence="0.700984">
(7) John broke the window.
</listItem>
<bodyText confidence="0.999846823529412">
and sees from observation that the referent of
&amp;quot;John&amp;quot; is an agent, the referent of &amp;quot;the window&amp;quot;
a patient, and from syntax that &amp;quot;John&amp;quot; is sub-
ject, and &amp;quot;the window&amp;quot; object. That em break
takes an object conflicts with the current view that
em break NEVER takes an object, and therefore
this strong assumption is weakened to say that
em break SOMETIMES takes an object. Simi-
larly, the learner must fall back to the position
that em break SOMETIMES can have the theme
serve as object, and can SOMETIMES have an
agent. This takes {OBJ:—, AGT:—, THEME:+,
TAO:—} to {OBJ:0, AGT:0, THEME:+, TA0:0},
which is the class of both em break and em stand.
However, since it has never seen a locative for em
break, it assumes that em break falls into exactly
the category we have labelled as &amp;quot;break&amp;quot; •2
</bodyText>
<note confidence="0.43247">
2And how would it distinguish between
The vase stood on the table.
and
</note>
<bodyText confidence="0.99965725">
There are, of course, many other possible orders
in which the learner might encounter the verb em
break. Suppose the learner first encounters the
pattern
</bodyText>
<listItem confidence="0.636718">
(8) John broke the window.
</listItem>
<bodyText confidence="0.979713622222222">
before any other occurrences of this verb. Given
only (8), it will assume that em break always takes
an object, always takes an agent, always has a pa-
tient, and always has the patient serving as ob-
ject. The learner will also assume that em break
never takes a location, a dative, etc. This will
give it the initial description of {OBJ:+, AGT:+,
THEME:+, TA0:+, , LOC:—}, which causes
the learner to classify em break as falling into
the toplevel verb class of DEVOUR, verbs of de-
struction with the instrument incorporated into
the verb meaning.
Next, suppose the learner sees
(9) The hammer broke the window.
where the learner observes that &amp;quot;hammer&amp;quot; is an
inanimate object, and therefore must serve as in-
strument, not agent. This means that the earlier
assumption that agent is necessary was an over-
commitment (as was the unmentioned assump-
tion that an instrument was forbidden). The
learner therefore weakens the description of em
break to {OBJ:+, AGT:0, THEME:+, TA0:+,
, LOC:—, INST:0}, which moves em break into
the verb class of DESTROY, destruction without
incorporated instrument.
Finally (as it turns out), suppose the learner
sees
(10) The window broke.
Now it discovers that the object is not obliga-
tory, and also that the theme can appear as sub-
ject, not object, which means that TAO is op-
tional, not obligatory. This now takes em break to
{OBJ:0, AGT:0, THEME:+, TAO:0, }, which
is the verb class of break.
We interposed (9) between (8) and (10) in this
sequence just to exercise the learner. If (10) fol-
lowed (8) directly, the learner would have taken em
break to verb class BREAK all the more quickly.
Although we will not explicitly go through the ex-
ercise here, it is important to our claims that any
permutation of the potential sentence frames of em
break will take the learner to BREAK, although
some combinations require verb classes not shown
The base broke on the table?
This is a problem we discuss at the end of this paper.
</bodyText>
<page confidence="0.992976">
182
</page>
<bodyText confidence="0.99991637037037">
on our chart for the sake of simplicity (e.g. the
class {OBJ:0, AGT:—, THEME:+, TAO:0} if it
hasn&apos;t yet seen an agent as subject.).
We were somewhat surprised to note that the
trajectory of em break takes the learner through a
sequence of states whose semantics are useful ap-
proximations of the meaning of this verb. In the
first case above, the learner goes through the class
of &amp;quot;change of state without agency&amp;quot;, into the class
of BREAK, i.e. &amp;quot;change of state involving no lo-
cation&amp;quot;. In the second case, the trajectory takes
the learner through &amp;quot;destroy with an incorporated
instrument&amp;quot;, and then DESTROY into BREAK.
In both of these cases, it happens that the trajec-
tory of em break through our hierarchy causes it
to have a meaning consistent with its final mean-
ing at each point of the way. While this will not
always be true, it seems that it is quite often the
case. We find this property of our verb classifica-
tion very encouraging, particularly given its gene-
sis in our simple learning principle.
We now consider a similar example for a dif-
ferent verb, the verb em load, in somewhat terser
form. And again, we have chosen a somewhat indi-
rect route to the final derived verb class to demon-
strate complex trajectories through the space of
verb classes. Assume the learner first encounters
</bodyText>
<listItem confidence="0.673209">
(11) John loads the hay onto the truck.
</listItem>
<bodyText confidence="0.9928205">
From (11), the learner builds the representa-
tion {OBJ:+, AGT:+, THEME:+, TAO:-i-, • • • ,
LOC:+, , DAT:—), which lands the learner into
the class of PUT, i.e. &amp;quot;simple change of location&amp;quot;.
We assume that the learner can derive that &amp;quot;the
truck&amp;quot; is a locative both from the prepositional
marking, and from direct observation.
Next the learner encounters
</bodyText>
<listItem confidence="0.70554">
(12) John loads the hay.
</listItem>
<bodyText confidence="0.990479678571429">
From this, the learner discovers that the location
is not obligatory, but merely optional, shifting
it to {OBJ:-i-, AGT:+, THEME:+, TA0:+, ,
LOC:0 ... , DAT:—), the verb class of HUG, with
the general meaning of &amp;quot;surface contact with no
trajectory.&amp;quot;
The next sentence encountered is
(13) John loads the truck with hay.
This sentence tells the learner that the theme need
only optionally serve as object, that it can be •
shifted to a non-argument position marked with
the preposition em with. This gives em load
the description of {OBJ:+, AGT:+, THEME:+,
TAO:0, TAC:with, , LOC:0 , DAT:—). This
new description takes em load now into the verb
class of POKE/TOUCH, surface contact by an
object that has followed some trajectory. (We
have explicitly indicated in our description here
that {DAT:—} was part of the verb description,
rather than leaving this fact implicit, because we
knew, of course, that this feature would be needed
to distinguish between the verb classes of GIVE
and POKE/TOUCH. We should stress that this
and many other features are encoded as &amp;quot;—&amp;quot; until
encountered by the learner; we have simply sup-
pressed explicitly representing such features in our
account here unless needed.)
Finally, the learner encounters the sentence
</bodyText>
<listItem confidence="0.716375">
(14) John loads the truck.
</listItem>
<bodyText confidence="0.999526133333334">
which makes it only optional that the theme
must occur, shifting the verb representation to
{OBJ:+, AGT:+, THEME:0, TAO:0, TAC:with,
, LOC:0 , DAT:—). The principle four few-
tures of this description put the verb into the gen-
eral area of WIPE, CLEAR and SPRAY/LOAD,
but the optional locative, and the fact that the
theme can be marked with em with select for the
class of SPRAY/LOAD, verbs of physical contact
that show the completive/noncompletive alterna-
tion:
Note that in this case again, the semantics of the
verb classes along the learning trajectory are rea-
sonable successive approximations to the meaning
of the verb.
</bodyText>
<sectionHeader confidence="0.9998295" genericHeader="method">
5 FURTHER RESEARCH AND
SOME PROBLEMS
</sectionHeader>
<bodyText confidence="0.999668">
One difficulty with this approach which we have
not yet confronted is that real data is somewhat
noisy. For example, although it is often claimed
that Motherese is extremely clean, one researcher
has observed that the verb &amp;quot;put&amp;quot;, which requires
both a location and an object to be fully grammat-
ical, has been observed in Motherese (although
extremely infrequently) without a location. We
strongly suspect, of course, that the assumption
that one instance suffices to change the learner&apos;s
model is too strong. It would be relatively easy
to extend the model we give here with a couple
of bits to count the number of counterexamples
seen for each obligatory or forbidden feature, with
two or three examples needed within some limited
time period to shift the feature to optional.
Can the model we describe here be taken as a
psychological model? At first glance, clearly not,
</bodyText>
<page confidence="0.996861">
183
</page>
<bodyText confidence="0.999987727272727">
because this model appears to be deeply conser-
vative, and as Pinker(1987) demonstrates, chil-
dren freely use verbs in patterns that they have
not seen. In our terms, they use verbs as if they
had moved them down the hierarchy without ev-
idence. The facts as currently understood can be
accounted for by our model given one simple as-
sumption: While children summarize their expo-
sure to verb usages as discussed above, they will
use those verbs in highly productive alternations
(as if they were in lower categories) for some pe-
riod after exposure to the verb. The claim is that
their em usage might be non-conservative, even
if their representations of verb class are. By this
model, the child would restrict the usage of a given
verb to the represented usages only after some pe-
riod of time. The mechanisms for deriving criteria
for productive usage of verb patterns described by
Pinker(1987) could also be added to our model
without difficulty. In essence, one would then
have a non-conservative learner with a conserva-
tive core.
</bodyText>
<sectionHeader confidence="0.999706" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.999960243902439">
[1] Berwick, R. (1985) The Acquisition of Syntac-
tic Knowledge. Cambridge, MA: MIT Press.
[2] Fisher, C.; Gleitman, H.; and Gleitman, L.
(1988) Relations between verb syntax and
verb semantics: On the semantic content of
subcategorization frames. Submitted for pub-
lication.
[3] Golinkoff, R.M.; Harding, C.G.; Carson, V.;
and Sexton, M.E. (1984) The infant&apos;s percep-
tion of causal events: the distinction between
animate and inanimate object. In L.P. Lip-
sitt and C. Rovee-Collier (Eds.) Advances in
Infancy Research 3: 145-65.
[4] Grirnshaw, J. (1983) Subcategorization and
grammatical relations. In A. Zaenen (Ed.),
Subjects and other subjects. Evanston: Indi-
ana University Linguistics Club.
[5] Jackendoff, R. (1983) Semantics and cogni-
tion. Cambridge, MA: The MIT Press.
[6] Landau, B. and Gleitman, L.R. (1985) Lan-
guage and experience: Evidence from the
blind child. Cambridge, MA: Harvard Univer-
sity Press.
[7] Levin, B. (1985) Lexical semantics in review:
An introduction. In B. Levin (Ed.), Lexical
semantics in review. Lexicon Project Working
Papers, 1. Cambridge, MA: MIT Center for
Cognitive Science.
[8] Naigles, L.; Gleitman, H.; and Gleitman,
L.R. (in press) Children acquire word mean-
ing components from syntactic evidence. In
E. Dromi (Ed.) Linguistic and conceptual de-
velopment. Ablex.
[9] Pinker, S. (1984) Language Learnability and
Language Development. Cambridge, MA:
Harvard University Press.
[10] Pinker, S. (1987) Resolving a learnability
paradox in the acquisition of the verb lexi-
con. Lexicon project working papers 17. Cam-
bridge, MA: MIT Center for Cognitive Sci-
ence.
</reference>
<page confidence="0.998706">
184
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.679418">
<title confidence="0.8470875">AUTOMATIC ACQUISITION OF THE LEXICAL SEMANTICS OF VERBS FROM SENTENCE FRAMES*</title>
<author confidence="0.999665">Mort Webster</author>
<author confidence="0.999665">Mitch Marcus</author>
<affiliation confidence="0.9996305">Department of Computer and Information Science University of Pennsylvania</affiliation>
<address confidence="0.999352">200 S. 33rd Street Philadelphia, PA 19104</address>
<abstract confidence="0.998823882352941">This paper presents a computational model of verb acquisition which uses what we will call the princiof overcommitment eliminate the need for negative evidence. The learner escapes from the need to be told that certain possibilities cannot occur (i.e., are &amp;quot;ungrammatical&amp;quot;) by one simple expedient: It assumes that all properties it has observed are either obligatory or foruntil it at which point it decides that what it thought was either obligatory or forbidden is merely optional. This model is built upon a classification of verbs based upon a simple three-valued set of features which represents key aspects of a verb&apos;s syntactic structure, its predicate/argument structure, and the mapping between them.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Berwick</author>
</authors>
<title>The Acquisition of Syntactic Knowledge.</title>
<date>1985</date>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA:</location>
<marker>[1]</marker>
<rawString>Berwick, R. (1985) The Acquisition of Syntactic Knowledge. Cambridge, MA: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fisher</author>
<author>H Gleitman</author>
<author>L Gleitman</author>
</authors>
<title>Relations between verb syntax and verb semantics: On the semantic content of subcategorization frames.</title>
<date>1988</date>
<note>Submitted for publication.</note>
<marker>[2]</marker>
<rawString>Fisher, C.; Gleitman, H.; and Gleitman, L. (1988) Relations between verb syntax and verb semantics: On the semantic content of subcategorization frames. Submitted for publication.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M Golinkoff</author>
<author>C G Harding</author>
<author>V Carson</author>
<author>M E Sexton</author>
</authors>
<title>The infant&apos;s perception of causal events: the distinction between animate and inanimate object.</title>
<date>1984</date>
<booktitle>In L.P. Lipsitt and C. Rovee-Collier (Eds.) Advances in Infancy Research</booktitle>
<volume>3</volume>
<pages>145--65</pages>
<marker>[3]</marker>
<rawString>Golinkoff, R.M.; Harding, C.G.; Carson, V.; and Sexton, M.E. (1984) The infant&apos;s perception of causal events: the distinction between animate and inanimate object. In L.P. Lipsitt and C. Rovee-Collier (Eds.) Advances in Infancy Research 3: 145-65.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Grirnshaw</author>
</authors>
<title>Subcategorization and grammatical relations. In A. Zaenen (Ed.), Subjects and other subjects.</title>
<date>1983</date>
<institution>Indiana University Linguistics Club.</institution>
<location>Evanston:</location>
<marker>[4]</marker>
<rawString>Grirnshaw, J. (1983) Subcategorization and grammatical relations. In A. Zaenen (Ed.), Subjects and other subjects. Evanston: Indiana University Linguistics Club.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Jackendoff</author>
</authors>
<title>Semantics and cognition.</title>
<date>1983</date>
<publisher>The MIT Press.</publisher>
<location>Cambridge, MA:</location>
<marker>[5]</marker>
<rawString>Jackendoff, R. (1983) Semantics and cognition. Cambridge, MA: The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Landau</author>
<author>L R Gleitman</author>
</authors>
<title>Language and experience: Evidence from the blind child.</title>
<date>1985</date>
<publisher>Harvard University Press.</publisher>
<location>Cambridge, MA:</location>
<marker>[6]</marker>
<rawString>Landau, B. and Gleitman, L.R. (1985) Language and experience: Evidence from the blind child. Cambridge, MA: Harvard University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Levin</author>
</authors>
<title>Lexical semantics in review: An introduction. In</title>
<date>1985</date>
<institution>MIT Center for Cognitive Science.</institution>
<location>Cambridge, MA:</location>
<marker>[7]</marker>
<rawString>Levin, B. (1985) Lexical semantics in review: An introduction. In B. Levin (Ed.), Lexical semantics in review. Lexicon Project Working Papers, 1. Cambridge, MA: MIT Center for Cognitive Science.</rawString>
</citation>
<citation valid="false">
<authors>
<author>L Naigles</author>
<author>H Gleitman</author>
<author>L R Gleitman</author>
</authors>
<title>(in press) Children acquire word meaning components from syntactic evidence.</title>
<booktitle>In E. Dromi (Ed.) Linguistic and conceptual development. Ablex.</booktitle>
<marker>[8]</marker>
<rawString>Naigles, L.; Gleitman, H.; and Gleitman, L.R. (in press) Children acquire word meaning components from syntactic evidence. In E. Dromi (Ed.) Linguistic and conceptual development. Ablex.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pinker</author>
</authors>
<title>Language Learnability and Language Development.</title>
<date>1984</date>
<publisher>Harvard University Press.</publisher>
<location>Cambridge, MA:</location>
<marker>[9]</marker>
<rawString>Pinker, S. (1984) Language Learnability and Language Development. Cambridge, MA: Harvard University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pinker</author>
</authors>
<title>Resolving a learnability paradox in the acquisition of the verb lexicon. Lexicon project working papers 17.</title>
<date>1987</date>
<institution>MIT Center for Cognitive Science.</institution>
<location>Cambridge, MA:</location>
<marker>[10]</marker>
<rawString>Pinker, S. (1987) Resolving a learnability paradox in the acquisition of the verb lexicon. Lexicon project working papers 17. Cambridge, MA: MIT Center for Cognitive Science.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>