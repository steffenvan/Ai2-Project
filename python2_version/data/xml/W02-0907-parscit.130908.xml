<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000017">
<note confidence="0.988397">
Unsupervised Lexical Acquisition: Proceedings of the Workshop of the
ACL Special Interest Group on the Lexicon (SIGLEX), Philadelphia,
July 2002, pp. 51-58. Association for Computational Linguistics.
</note>
<bodyText confidence="0.999736794871795">
with low frequency scFs. According to one ac-
count (Briscoe and Carroll, 1997) the majority
of errors arise in SCF acquisition because of the
statistical filtering process.
Korhonen et al. (2000) investigated reasons
for the poor performance of the statistical filters
and reported better accuracy with a simple filter
which sets a threshold on the relative frequency
of SCF entries, selecting only those scFs which
are high in frequency. Korhonen (2000) pro-
posed combining this filter with a method which
deals better with sparse data. The method
exploits the knowledge that semantically sim-
ilar verbs show similar subcategorization be-
haviour. It involves identifying the semantic
class of a predicate, using a hypothesis generator
to acquire a SCF distribution for the predicate,
smoothing this distribution with the back-off es-
timates of the respective semantic verb class,
and setting an empirically defined threshold on
the probability estimates from smoothing to fil-
ter out unreliable scFs.
The small scale experiment reported in
Korhonen (2000) demonstrates that this
semantically-driven approach can significantly
improve the accuracy of hypothesis selection,
for both high and low frequency scFs. In
this paper, we describe refining this approach
further so that it is suitable for larger scale SCF
acquisition. Essentially, we propose methods for
construction of semantic classes and automatic
semantic classification of verbs. We report an
experiment which shows that the method can
be used to improve large-scale SCF acquisition.
We introduce the baseline SCF acquisition
framework in section 2 and describe our exten-
sions to it in section 3. Section 4 gives details of
the experimental evaluation and section 5 con-
cludes with directions for future work.
</bodyText>
<sectionHeader confidence="0.971872" genericHeader="method">
2 Framework for SCF Acquisition
</sectionHeader>
<subsectionHeader confidence="0.846202">
2.1 Hypothesis Generation
</subsectionHeader>
<bodyText confidence="0.999760173913044">
We employ for hypothesis generation the SCF ac-
quisition system of Briscoe and Carroll (1997).
This system is capable of distinguishing 163 ver-
bal scFs a superset of those found in the ANLT
(Boguraev et al., 1987) and COMLEX Syntax dic-
tionaries (Grishman et al., 1994) and return-
ing relative frequencies for each SCF found for a
verb.
It works by first tagging, lemmatizing and
parsing corpus data using a robust statistical
parser (Carroll and Briscoe, 1996) which em-
ploys a grammar written in a feature-based uni-
fication grammar formalism. This yields com-
plete though intermediate parses. Local syntac-
tic frames including the syntactic categories and
head lemmas of constituents are then extracted
from parses, from sentence subanalyses which
begin/end at the boudaries of predicates.
The resulting patterns are assigned to scFs on
the basis of the feature values of syntactic cate-
gories and head lemmas in each pattern. Finally,
sets of scFs are gathered for verbs and putative
lexical entries are constructed.
</bodyText>
<subsectionHeader confidence="0.991607">
2.2 Hypothesis Selection
</subsectionHeader>
<bodyText confidence="0.999992615384615">
The method for hypothesis selection exploits the
knowledge that semantically similar verbs are
frequently similar also in terms of subcatego-
rization. The motion verbs fly and move, for
example, take similar SCF distributions, which
differ essentially from the ones taken e.g. by
communication verbs tell and say.
Although no perfect correspondence exists
between the semantic and syntactic properties
of verbs, useful generalizations can be made:
Levin (1993) has demonstrated that verb senses
can be divided into semantic classes distinctive
in terms of subcategorization. Korhonen (2000)
shows that verb forms can also be divided into
such classes, according to their predominant (i.e.
the most frequent) sense.
For instance, the verb form specific SCF dis-
tributions for the highly polysemic verbs fly and
move correlate quite well because the predom-
inant senses of these verbs (according to the
WordNet (Miller, 1990) frequency data2) are
similar. They both belong to the Levin &amp;quot;Mo-
tion verbs&amp;quot;. Good correlation is observed be-
cause the distribution of verb senses tends to be
zipfian: the predominant sense typically covers
most of the total frequency mass (Preiss et al.,
</bodyText>
<subsectionHeader confidence="0.402824">
2111 this paper, we refer to the WordNet version 1.6.
</subsectionHeader>
<bodyText confidence="0.999436272727273">
2002).
The method for hypothesis selection involves
first (manually) identifying, for a single verb,
the broad Levin class3 corresponding to the pre-
dominant sense of this verb in WordNet. A
SCF distribution is then acquired for the verb
from corpus data, using the hypothesis genera-
tor described in the previous section. This dis-
tribution is smoothed - using linear interpolation
(Chen and Goodman, 1996). - with the &amp;quot;back-
off&amp;quot; estimates of the Levin class.
Back-off estimates are obtained by (i) choos-
ing 4-5 representative Levin verbs from a class,
(ii) building SCF distributions for these verbs by
manually analysing c. 300 occurrences of each
verb in the BNC corpus (Leech, 1992) and (iii)
merging the resulting set of SCF distributions.
For example, the back-off estimates for the &amp;quot;Mo-
tion verbs&amp;quot; are constructed by merging the SCF
distributions for fly, walk, march and travel.
A simple method is finally used for filtering,
which sets an empirically defined threshold on
the probability estimates from smoothing.
Essentially, this method involves using a pri-
ori knowledge about generalizations of verb se-
mantics to guide subcategorization acquisition.
Back-off estimates are used to correct the ac-
quired SCF distribution and deal with sparse
data. However, the parameters used in smooth-
ing are obtained by optimising SCF acquisition
performance on held-out training data so that
most of the smoothed probability is determined
by the maximum likelihood estimate (mLE) from
the hypothesis generator.
Korhonen (2000) evaluated this method with
60 test verbs from 10 Levin classes. It yielded
88% type precision (the percentage of SCF types
that the method proposes which are correct) and
69% type recall (the percentage of SCF types in
the gold standard that the method proposes).
The method was compared against a baseline
method which sets threshold on MLES of SCFS
from the hypothesis generator. F measure4 was
77.1 for the novel method and 70.1 for the base-
</bodyText>
<footnote confidence="0.8899752">
3Korhonen (2000) employs broad Levin classes only
(e.g. 51. &amp;quot;Motion verbs&amp;quot;), not subclasses these may di-
vide into (e.g. 51.2 &amp;quot;Leave verbs&amp;quot;).
4F = 2.precision•recall
precision±recall
</footnote>
<bodyText confidence="0.9737332">
line.
This preliminary experiment shows that the
proposed method provides an effective way of
dealing with low frequency associations and a
means of predicting unseen associations in cor-
pus data.
In this paper, we describe how this
semantically-driven method for hypothesis selec-
tion was refined further and integrated as part
of large-scale SCF acquisition.
</bodyText>
<sectionHeader confidence="0.847712" genericHeader="method">
3 Extensions to the Framework
</sectionHeader>
<bodyText confidence="0.967856">
Applying the method on a large scale requires
us to (a) define a comprehensive set of seman-
tic verb classes, (b) to obtain back-off estimates
for each class, and (c) to implement a method
capable of automatically assigning verbs to se-
mantic classes. We propose methods for (a) and
(c) in sections 3.1 and 3.2, respectively. Section
3.3 discusses the work completed on (a), (b) and
(c).
</bodyText>
<subsectionHeader confidence="0.997442">
3.1 Semantic Classes
</subsectionHeader>
<bodyText confidence="0.999974473684211">
Korhonen (2000) classified verbs into Levin
classes. These provide us with a good start-
ing point for large-scale SCF acquisition as well.
Although not comprehensive, they cover a sub-
stantial number of diathesis alternations occur-
ring in English and work on refining and extend-
ing this classification is under way (Dang et al.,
1998; Dorr, 1997)5.
As it is important to minimise the cost in-
volved in constructing back-off estimates, we do
not adopt the 191 Levin classes as they stand, al-
though this would allow maximal accuracy. We
also do not adopt broad Levin classes as they
stand, as some proved inaccurate in the prelim-
inary experiment. Rather, a method was de-
vised for determining the specificity of the Levin
class(es) required for adequate distinctiveness in
terms of subcategorization. The method pro-
ceeds in two steps, by examining the:
</bodyText>
<footnote confidence="0.5491355">
Step 1: Syntactic similarity between Levin
classes. In this, we use Dorr&apos;s (1997) source
of LDOCE grammatical codes (Procter, 1978) for
5In the work reported in this paper, we concentrate
on Levin classes only, leaving the construction of novel
verb classes for future work.
</footnote>
<figureCaption confidence="0.713864647058824">
Levin classes°. Dorr constructed this source by
extracting basic syntactic patterns from all the
sentences in Levin (1993) and mapping these onto
LDOCE codes. To determine syntactic similarity
between a set of Levin (sub)classes, we consider
the degree of intersection between the LDOCE codes
for these classes.
Step 2: Subcategorization similarity between
verbs in Levin classes. We choose a few
individual verbs whose predominant sense belongs
to the Levin (sub)classes under investigation and
examine (1) the intersection of SCFS between the
verbs, (2) the dissimilarity of SCF distributions for
these verbs according to Kullback-Leibler distance
(KL) and (3) the similarity in ranking of SCFS in
the distributions according to the Spearman rank
correlation coefficient (Ftc)7.
</figureCaption>
<bodyText confidence="0.999858625">
Step 2 complements Step 1, as the syntac-
tic information included in Levin (1993) is not
conclusive and does not provide any informa-
tion about the relative frequency of scFs. In
addition, it allows us to examine the degree of
SCF correlation between all the verb form spe-
cific SCF distributions we are actually concerned
with.
For example, this method can be used to
decide whether the Levin class of &amp;quot;Verbs of
Sending and Carrying&amp;quot; is distinctive enough
in terms of subcategorization, or whether it
should be divided into subclasses, i.e. &amp;quot;Send&amp;quot;,
&amp;quot;Slide&amp;quot;, &amp;quot;Bring and Take&amp;quot;, &amp;quot;Carry&amp;quot; and &amp;quot;Drive&amp;quot;
verbs. Step 1 determines that the intersection of
LDOCE codes between the five subclasses is fairly
large. All classes share 2 LDOCE codes, four
share 5, and three share 1. Step 2 concludes that
the SCF distributions for five individual verbs
(one from each subclass: send, float, bring, carry
and ferry) are fairly similar in terms of KL, RC
and the intersection of scFs. The broad class
seems thus syntactically coherent enough to pro-
vide an adequate basis for back-off estimates8.
</bodyText>
<subsectionHeader confidence="0.999627">
3.2 Automatic Verb Classification
</subsectionHeader>
<bodyText confidence="0.99987">
In Korhonen (2000), verbs were manually as-
signed to semantic classes. For large-scale SCF
acquisition, a method is needed for automatic
</bodyText>
<footnote confidence="0.904781666666667">
6We are indebted to Bonnie Dorr for the use of these
codes.
7We use SCF distributions obtained via manual analy-
sis of corpus data. See e.g. (Manning and Schfitze, 1999)
for details of Kt, and RC.
8See Korhonen (2002) for details of this method.
</footnote>
<bodyText confidence="0.9997769375">
classification of verbs. We propose one which
involves assigning verbs to semantic classes via
WordNet. Although WordNet&apos;s semantic or-
ganization does not always go hand in hand
with syntactic information, synonymous verbs
in WordNet exhibit syntactic behaviour similar
to that characterised in the classification system
of Levin (Dorr, 1997).
Dorr (1997) has proposed a fully automatic
verb classification algorithm which classifies
verbs semantically on the basis of their Word-
Net synonyms. This algorithm relies solely on
lexical resources but is not accurate enough for
our purpose. As inaccurate assignments can
actually degrade SCF acquisition performance,
some allowance for manual intervention is nec-
essary. We therefore propose a semi-automatic
algorithm. This assigns entire WordNet synsets
to semantic classes9, making use of Levin&apos;s verb
index, the LDOCE dictionary and Dorr&apos;s LDOCE
codes for Levin classes.
The algorithm classifies synsets subhierarchy
by subhierarchy, starting from the top level
synsets, and going further down in the taxon-
omy when required. Each synset is classified by
first assigning the majority of its member verbs
to a semantic class and then choosing the Levin
class supported by the highest number of verbs.
&apos;Member verbs&apos; refer here to those which, ac-
cording to their predominant sense, are mem-
bers of the synset in question and of its hyponym
synsets. The algorithm proceeds as follows:
</bodyText>
<tableCaption confidence="0.506915692307692">
Step 1: If the majority of member verbs of a given
synset S are Levin verbs19 from the same class, clas-
sify S directly.
Step 2: Otherwise, classify more member verbs (accord-
ing to Step 4a-d) until the majority are classified,
and then go back to Step 1.
Step 3: Otherwise, if the classified verbs point to dif-
ferent Levin classes, examine whether S consists of
hyponym synsets. If not, assign S to the Levin class
supported by the highest number of classified verbs.
If yes, go one level down in the hierarchy and clas-
sify the hyponym synsets separately, starting again
from Step 1.
</tableCaption>
<footnote confidence="0.9948085">
9Our objective is to build a static source where Word-
Net synsets are associated with different Levin classes.
Although static, the source will allow for updating and
adding new verbs to WordNet.
&apos;9`Levin verbs&apos; refer to verbs whose predominant sense
belongs to the Levin class in question.
</footnote>
<note confidence="0.7124315">
Step 4: If S includes no Levin verbs, proceed as follows
to classify the majority of member verbs of S:
</note>
<figure confidence="0.95598425">
(a) Extract the predominant sense of a given verb
V from WordNet
(b) Extract the syntactic codes from LDOCE rele-
vant to this sense
</figure>
<figureCaption confidence="0.894097333333333">
(c) Examine whether V could be assigned to a
Levin class already associated with the other
verbs in the (i) same synset, (i) possible hyper-
nym synset or (iii) possible sister synsets by
comparing the LDOCE codes of the sense and
Dorr&apos;s LDOCE codes of the respective Levin
</figureCaption>
<bodyText confidence="0.879951138888889">
class(es). Given the hypothesised classes,
make the final class assignment manually.
(d) If no suitable class is found, re-examine the
case after more verbs have been analysed.
If the classification remains unsolved, set V
aside for later examination.
The above algorithm is for the most part au-
tomatic, however, Step 4b and part of Step 4c
(the final class assignment) are done manually
to ensure accuracy of classification.
For example, the synset no. 00994853 includes
13 member verbs, 4 of which are Levin &amp;quot;Verbs
of Sending and Carrying&amp;quot;. We need to classify
more verbs to determine class assignment. We
choose whisk and extract its predominant sense
from WordNet:
whisk -- (move somewhere quickly; &amp;quot;The president
was whisked away in his limo&amp;quot;)
In LDOCE the verb has three senses. That cor-
responding to the predominant WordNet sense
is identified as:
2. [X9 esp. OFF, AWAY] to remove
b. by taking suddenly:
&amp;quot;She whisked the cups away
/ whisked him (off) home&amp;quot;
11 Levin classes are already matched with the
verbs in the same, hypernym and sister synsets.
Those whose syntactic description includes the
LDOCE code X9 are:
Verbs of putting
Verbs of removing
Verbs of sending and carrying
Verbs of exerting force
Verbs of motion
After verifying these options manually, whisk
is assigned to &amp;quot;Verbs of Sending and Carrying&amp;quot;.
</bodyText>
<subsectionHeader confidence="0.94121">
3.3 Completed Work
</subsectionHeader>
<bodyText confidence="0.998937388888889">
The verb classification algorithm was applied to
3 WordNet verb files (contact, possession and
motion verbs). 1581 synsets in these files were
assigned to semantic classes and 148 were left
unclassified. A small number of synsets (35)
from other verb files were classified as well.
From the total of 32 broad Levin classes ex-
emplified among the classified WordNet synsets,
22 of the most frequent were chosen for fur-
ther work. These were re-grouped into semantic
classes by using the method described in sec-
tion 3.1. This led to the combination of 5 pairs
of broad Levin classes and the division of 3 into
subclasses. The resulting 20 semantic classes are
shown in table 1, labelled by class codes shown
in the first column. Back-off estimates for these
classes were built using the method described in
section 2.2.
</bodyText>
<table confidence="0.997877838709677">
Levin Verbs of
A 9. Putting
B 10. Removing: 10.1-3, 10.5-9
C 10. Removing: 10.4
D 11. Sending and Carrying
12. Exerting Force
E 13. Change of Possession
F 15. Hold and Keep
16. Concealment
G 17. Throwing
H 18. Contact by Impact
19. Poke Verbs
I 20. Contact
J 21. Cutting
K 22. Combining and Attaching:
22.1-4
L 22. Combining and Attaching:
22.5
M 23. Separating and Disassembling:
21.1-3
N 23. Separating and Disassembling:
23.4
0 34. Assessment
35. Searching
P 36. Social Interaction
Q 42. Killing
R 44. Destroy
S 47. Existence: Verbs of Spatial
Configuration
50. Assuming Position
T 51. Motion
</table>
<tableCaption confidence="0.999668">
Table 1: Semantic verb classes
</tableCaption>
<sectionHeader confidence="0.964351" genericHeader="evaluation">
4 Experimental Evaluation
</sectionHeader>
<subsectionHeader confidence="0.961662">
4.1 Classification Algorithm
</subsectionHeader>
<bodyText confidence="0.999960473684211">
To evaluate the classification algorithm, we
chose 30 synsets from among the 1616 classi-
fied. The synsets were chosen at random so
that 10 were taken from each of the three Word-
Net verb files (contact, possession and motion
verbs). From the total of 378 verbs in these
synsets, we chose for evaluation those 151 which
were neither Levin verbs nor classified manu-
ally when linking synsets with Levin classes. For
these verbs, the semantic classification was com-
pared against manually obtained gold standard
classification. The algorithm classified correctly
140 verbs. The accuracy of the class assignment
was thus 93%.
This result is encouraging. However, the cor-
respondence between synsets and Levin classes
varies largely across WordNet. Further evalu-
ation is therefore needed in the future with a
larger set of WordNet files.
</bodyText>
<subsectionHeader confidence="0.98921">
4.2 Subcategorization Acquisition
4.2.1 Test Data and Method
</subsectionHeader>
<bodyText confidence="0.9999935625">
To evaluate the revised approach to hypothe-
sis selection, we took a sample of 20M words of
BNC. We extracted all sentences containing an
occurrence of one of 91 verbs. The verbs were
chosen at random, subject to the constraint that
they were classified by the algorithm as members
of one of the 20 semantic classes constructed (ta-
ble 1). After the extraction process, we retained
c. 1000 citations for each verb.
The sentences containing these verbs were
processed by the SCF acquisition system. The
hypothesis generator was held constant, the ex-
ception being that the data for these experi-
ments were parsed using a probabilistic chart
parser (Chitrao and Grishman, 1990). For hy-
pothesis selection, we employed the method of
Korhonen (2000) with the extensions described.
We also obtained results for the baseline MLE
thresholding method without any smoothing.
The results were evaluated against a manual
analysis of the corpus data. This was obtained
by analysing a maximum of 300 occurrences for
each test verb in the BNC corpora. We calculated
type precision, type recall and F measure. In
addition to the system results, we calculated KL
and RC between the acquired unfiltered SCF dis-
tributions and the gold standard distributions.
We also recorded the total number of unseen
scFs in the acquired unfiltered SCF distributions
which occurred in the gold standard distribu-
tions. This was to investigate how well the ap-
proach deals with sparse data.
</bodyText>
<subsubsectionHeader confidence="0.589522">
4.2.2 Results
</subsubsectionHeader>
<bodyText confidence="0.965507054054054">
Table 2 gives average results for all the 91 test
verbs. The semantically-driven method outper-
forms the baseline on most measures. The im-
proved Tun indicates that the method improves
the overall accuracy of SCF distributions. The
results with RC show that it helps to correct the
ranking of scFs. Precision worsens slighly from
the baseline (1.4%), however, recall improves
significantly (24%). That both precision and
recall are high demonstrates that the method
deals well with both highly ranked scFs and
those low in frequency. The baseline method
simply ignores low frequency data and therefore
yields poor recall. While a total of 114 gold
standard scFs were unseen in the data from the
hypothesis generator, only 24 were unseen after
smoothing with back-off estimates. This shows
further that the method deals effectively with
sparse data.
Verb class specific results in table 3 allow us to
examine the accuracy of the back-off estimates.
The first column shows a semantic verb class and
the second indicates the number of verbs tested
for the class.
According to the F measure, the semantically-
driven method (sEm) outperforms the baseline
method (BL) in all 16 verb classes. KL and RC
show improvement in 12 classes. Four classes
show worse performance: B, E, F, and 0. Several
reasons were identified for the poor performance
(or small improvement) with some verbs/classes:
Firstly, the class specific back-off estimates are
not accurate enough for highly polysemic verbs.
&amp;quot;Note that KT, &gt; 0, with KT, near to 0 denoting strong
association, and —1 &lt; RC &lt;1, with RC near to 0 denot-
ing a low degree of association and RC near to -1 and 1
denoting strong association.
</bodyText>
<table confidence="0.9993675">
System results Unseen
Method KL RC Precision (%) Recall (%) F SCES
Baseline 0.42 0.62 82.2 48.5 61.0 114
Semantic 0.26 0.74 80.8 72.5 76.4 24
</table>
<tableCaption confidence="0.919281">
Table 2: Average results for 91 verbs
</tableCaption>
<table confidence="0.99972652631579">
Sem. Verbs KL RC F Measure Unseen
Class Tested SCES
BL SEM BL SEM BL SEM BL SEM
A 6 0.39 0.23 0.63 0.83 62.9 83.3 10 2
B 7 0.11 0.21 0.90 0.87 65.7 73.7 8 1
C 3 0.54 0.16 0.33 0.72 54.6 84.0 5 0
D 10 0.63 0.39 0.46 0.62 55.6 74.5 20 2
E 9 0.13 0.16 0.87 0.87 58.6 60.4 17 3
F 5 0.21 0.26 0.78 0.88 50.7 60.2 11 6
G 3 0.52 0.27 0.48 0.88 71.4 89.8 6 1
H 6 0.45 0.27 0.64 0.64 64.9 84.2 4 1
I 3 0.15 0.14 0.59 0.66 57.9 73.7 7 1
J 5 0.40 0.11 0.66 0.84 63.9 84.8 1 1
K 5 0.52 0.45 0.66 0.77 63.3 70.3 3 0
0 4 0.35 0.44 0.59 0.59 54.6 57.7 3 2
P 7 0.55 0.39 0.62 0.63 61.9 65.3 10 3
R 3 0.10 0.00 1.00 1.00 63.2 66.7 0 0
S 4 0.71 0.22 0.37 0.65 66.7 87.9 2 0
T 11 0.59 0.28 0.55 0.71 64.2 89.1 7 1
</table>
<tableCaption confidence="0.998624">
Table 3: Results for semantic classes
</tableCaption>
<bodyText confidence="0.99921656">
For example, 6 scFs are reported unseen with
class F and these all involve senses not taken
by the verbs used for constructing the back-
off estimates. Secondly, the back-off estimates
are not accurate for verbs whose distribution
of senses is not particularly zipfian, i.e. verbs
whose predominant sense is not very frequent
(e.g. keep in class F). Thirdly, the empiri-
cally set (verb class specific) filtering thresh-
olds appeared too high/low for some individual
verbs. Finally, although most semantic classes
proved fine-grained enough, one class appeared
too broad: class F, which was obtained by merg-
ing two broad Levin classes (&amp;quot;Hold and Keep&amp;quot;
and &amp;quot;Concealment&amp;quot; verbs).
In sum, the method proposed for construc-
tion of semantic classes proved fairly accurate.
Semantic classes can easily be evaluated in the
context of SCF acquisition and refined further
where required. Korhonen (2000) predicted that
the total number of semantic classes across the
whole lexicon is unlikely to exceed 50. This
seems still a valid prediction, as promising re-
sults were generally obtained assuming a broad
notion of a semantic class.
</bodyText>
<sectionHeader confidence="0.995862" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999967190476191">
We adopted the semantically motivated ap-
proach to hypothesis selection proposed by
Korhonen (2000) and modified it to be suitable
for large-scale SCF acquisition. A method was
proposed for construction of semantic classes
and a classification algorithm was designed
which automatically assigns verbs to semantic
classes via WordNet. Experimental evaluation
was provided which showed that the classifica-
tion algorithm is highly accurate and that the
revised method for semantically-driven hypoth-
esis selection can be used to succesfully guide,
structure and improve the large-scale acquisition
of scFs from corpus data.
Future work will include: investigating re-
ducing manual effort in the classification algo-
rithm and construction of back-off estimates,
constructing semantic classes and back-off es-
timates across the entire lexicon, applying the
classification algorithm to the remaining Word-
Net synsets and verb files, addressing the prob-
</bodyText>
<reference confidence="0.998828580246914">
M. Brent. 1993. From grammar to lexicon: unsu-
pervised learning of lexical syntax. Computational
Linguistics, 19(3):243-262.
E. J. Briscoe and J. Carroll. 1997. Automatic ex-
traction of subcategorization from corpora. In
5th ACL Conference on Applied Natural Language
Processing, pages 356-363.
J. Carroll and E. J. Briscoe. 1996. Apportioning
development effort in a probabilistic lr parsing
system through evaluation. In 1st ACL/SIGDAT
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 92-100.
G. Carroll and M. Rooth. 1998. Valence induction
with a head-lexicalized pcfg. In 3rd Conference on
Empirical Methods in Natural Language Process-
ing.
Stanley F. Chen and Joshua Goodman. 1996. An
empirical study of smoothing techniques for lan-
guage modeling. In ACL-96, pages 310-318. ACL.
M. Chitrao and R. Grishman. 1990. Statistical pars-
ing of messages. In Darpa Speech and Natural Lan-
guage Workshop, pages 263-266.
H. T. Dang, K. Kipper, M. Palmer, and
J. Rosensweig. 1998. Investigating regular sense
extensions based on intersective levin classes.
In 36th Annual Meeting of the Association for
Computational Linguistics and 17th International
Conference on Computational Linguistics, pages
293-299.
B. Dorr. 1997. Large-scale dictionary construc-
tion for foreign language tutoring and interlin-
gual machine translation. Machine Translation,
12(4):271-325.
R. Grishman, C. Macleod, and A. Meyers. 1994.
Comlex syntax: building a computational lexi-
con. In International Conference on Computa-
tional Linguistics, pages 268-272.
Korhonen, G. Gorrell, and D. McCarthy. 2000.
Statistical filtering and subcategorization frame
acquisition. In Joint SIGDAT Conference on Em-
pirical Methods in Natural Language Processing
and Very Large Corpora, pages 199-205.
Korhonen. 2000. Using semantically motivated
estimates to help subcategorization acquisition. In
Joint SIGDAT Conference on Empirical Methods
in Natural Language Processing and Very Large
Corpora, pages 216-223.
Korhonen. 2002. Subcategorization Acquisition.
Ph.D. thesis, University of Cambridge, UK.
G. Leech. 1992. 100 million words of english:
the british national corpus. Language Research,
28(1):1-13.
B. Levin. 1993. English Verb Classes and Alterna-
tions. Chicago University Press, Chicago.
C. Manning and H. Schiitze. 1999. Foundations
of Statistical Natural Language Processing. MIT
Press, Cambridge, Massachusetts.
C. Manning. 1993. Automatic acquisition of a large
subcategorization dictionary from corpora. In 31st
Annual Meeting of the Association for Computa-
tional Linguistics, pages 235-242.
G. A. Miller. 1990. Wordnet: An on-line lexical
database. International Journal of Lexicography,
3(4):235-312.
J. Preiss, A. Korhonen, and E. J. Briscoe. 2002. Sub-
categorization acquisition as an evaluation method
for wsd. In Language Resources and Evaluation
Conference. To appear.
P. Procter. 1978. Longman Dictionary of Contem-
porary English. Longman, England.
A. Sarkar and D. Zeman. 2000. Automatic extrac-
tion of subcategorization frames for czech. In 19th
International Conference on Computational Lin-
guistics, pages 691-697.
A. Ushioda, D. Evans, T. Gibson, and A. Waibel.
1993. The automatic acquisition of frequencies
of verb subcategorization frames from tagged cor-
pora. In B. Boguraev and J. Pustejovsky, edi-
tors, SIGLEX ACL Workshop on the Acquisition
of Lexical Knowledge from Text, pages 95-106.
Columbus, Ohio.
</reference>
<bodyText confidence="0.9878675">
lem of polysemy, and working on improving the A.
accuracy of the filtering method.
</bodyText>
<figure confidence="0.818942666666667">
References
A.
B. Boguraev, E. J. Briscoe, J. Carroll, D. Carter,
</figure>
<figureCaption confidence="0.7270925">
and C. Grover. 1987. The derivation of a
grammatically-indexed lexicon from the longman
dictionary of contemporary english. In 25th An-
nual Meeting of the Association for Computational
</figureCaption>
<bodyText confidence="0.804758">
Linguistics, pages 193-200. A.
</bodyText>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.7636445">Unsupervised Lexical Acquisition: Proceedings of the Workshop of the ACL Special Interest Group on the Lexicon (SIGLEX), Philadelphia,</note>
<abstract confidence="0.936483392294221">July 2002, pp. 51-58. Association for Computational Linguistics. with low frequency scFs. According to one account (Briscoe and Carroll, 1997) the majority errors arise in because of the statistical filtering process. Korhonen et al. (2000) investigated reasons for the poor performance of the statistical filters and reported better accuracy with a simple filter which sets a threshold on the relative frequency selecting only those scFs which are high in frequency. Korhonen (2000) proposed combining this filter with a method which deals better with sparse data. The method exploits the knowledge that semantically similar verbs show similar subcategorization behaviour. It involves identifying the semantic class of a predicate, using a hypothesis generator acquire a for the predicate, smoothing this distribution with the back-off estimates of the respective semantic verb class, and setting an empirically defined threshold on the probability estimates from smoothing to filter out unreliable scFs. The small scale experiment reported in Korhonen (2000) demonstrates that this semantically-driven approach can significantly improve the accuracy of hypothesis selection, for both high and low frequency scFs. In this paper, we describe refining this approach so that it is suitable for larger scale acquisition. Essentially, we propose methods for construction of semantic classes and automatic semantic classification of verbs. We report an experiment which shows that the method can used to improve large-scale introduce the baseline framework in section 2 and describe our extensions to it in section 3. Section 4 gives details of the experimental evaluation and section 5 concludes with directions for future work. 2 Framework for SCF Acquisition 2.1 Hypothesis Generation employ for hypothesis generation the acquisition system of Briscoe and Carroll (1997). system is capable of distinguishing 163 verscFs a superset of those found in the et al., 1987) and dic- (Grishman et al., 1994) and returnrelative frequencies for each for a verb. It works by first tagging, lemmatizing and parsing corpus data using a robust statistical parser (Carroll and Briscoe, 1996) which employs a grammar written in a feature-based unification grammar formalism. This yields complete though intermediate parses. Local syntactic frames including the syntactic categories and head lemmas of constituents are then extracted from parses, from sentence subanalyses which begin/end at the boudaries of predicates. The resulting patterns are assigned to scFs on the basis of the feature values of syntactic categories and head lemmas in each pattern. Finally, sets of scFs are gathered for verbs and putative lexical entries are constructed. 2.2 Hypothesis Selection The method for hypothesis selection exploits the knowledge that semantically similar verbs are frequently similar also in terms of subcategorization. The motion verbs fly and move, for take similar which differ essentially from the ones taken e.g. by verbs Although no perfect correspondence exists between the semantic and syntactic properties of verbs, useful generalizations can be made: (1993) has demonstrated that verb can be divided into semantic classes distinctive in terms of subcategorization. Korhonen (2000) that verb also be divided into such classes, according to their predominant (i.e. the most frequent) sense. instance, the verb form specific distributions for the highly polysemic verbs fly and move correlate quite well because the predominant senses of these verbs (according to the (Miller, 1990) frequency are similar. They both belong to the Levin &amp;quot;Motion verbs&amp;quot;. Good correlation is observed because the distribution of verb senses tends to be zipfian: the predominant sense typically covers most of the total frequency mass (Preiss et al., paper, we refer to the WordNet version 1.6. 2002). The method for hypothesis selection involves first (manually) identifying, for a single verb, broad Levin corresponding to the predominant sense of this verb in WordNet. A is then acquired for the verb from corpus data, using the hypothesis generator described in the previous section. This distribution is smoothed using linear interpolation (Chen and Goodman, 1996). with the &amp;quot;backoff&amp;quot; estimates of the Levin class. Back-off estimates are obtained by (i) choosing 4-5 representative Levin verbs from a class, building for these verbs by manually analysing c. 300 occurrences of each in the (Leech, 1992) and (iii) the resulting set of For example, the back-off estimates for the &amp;quot;Moverbs&amp;quot; are constructed by merging the for walk, march A simple method is finally used for filtering, which sets an empirically defined threshold on the probability estimates from smoothing. Essentially, this method involves using a priori knowledge about generalizations of verb semantics to guide subcategorization acquisition. Back-off estimates are used to correct the acand deal with sparse data. However, the parameters used in smoothare obtained by optimising performance on held-out training data so that most of the smoothed probability is determined by the maximum likelihood estimate (mLE) from the hypothesis generator. Korhonen (2000) evaluated this method with 60 test verbs from 10 Levin classes. It yielded type precision (the percentage of that the method proposes which are correct) and type recall (the percentage of in the gold standard that the method proposes). The method was compared against a baseline which sets threshold on the hypothesis generator. F was for the novel method and 70.1 for the base- (2000) employs broad Levin classes only 51. &amp;quot;Motion verbs&amp;quot;), not subclasses these may diinto (e.g. 51.2 = precision±recall line. This preliminary experiment shows that the proposed method provides an effective way of dealing with low frequency associations and a means of predicting unseen associations in corpus data. In this paper, we describe how this semantically-driven method for hypothesis selection was refined further and integrated as part large-scale 3 Extensions to the Framework Applying the method on a large scale requires us to (a) define a comprehensive set of semantic verb classes, (b) to obtain back-off estimates for each class, and (c) to implement a method capable of automatically assigning verbs to semantic classes. We propose methods for (a) and (c) in sections 3.1 and 3.2, respectively. Section 3.3 discusses the work completed on (a), (b) and (c). Classes Korhonen (2000) classified verbs into Levin classes. These provide us with a good startpoint for large-scale as well. Although not comprehensive, they cover a substantial number of diathesis alternations occurring in English and work on refining and extending this classification is under way (Dang et al., Dorr, As it is important to minimise the cost involved in constructing back-off estimates, we do not adopt the 191 Levin classes as they stand, although this would allow maximal accuracy. We also do not adopt broad Levin classes as they stand, as some proved inaccurate in the preliminary experiment. Rather, a method was devised for determining the specificity of the Levin class(es) required for adequate distinctiveness in terms of subcategorization. The method proceeds in two steps, by examining the: Step 1: Syntactic similarity between Levin this, we use Dorr&apos;s (1997) source codes (Procter, 1978) for the work reported in this paper, we concentrate on Levin classes only, leaving the construction of novel verb classes for future work. Dorr constructed this source by extracting basic syntactic patterns from all the sentences in Levin (1993) and mapping these onto To determine syntactic similarity between a set of Levin (sub)classes, we consider degree of intersection between the for these classes. Step 2: Subcategorization similarity between in Levin classes. choose a few individual verbs whose predominant sense belongs to the Levin (sub)classes under investigation and (1) the intersection of the (2) the dissimilarity of for these verbs according to Kullback-Leibler distance and (3) the similarity in ranking of the distributions according to the Spearman rank coefficient Step 2 complements Step 1, as the syntactic information included in Levin (1993) is not conclusive and does not provide any information about the relative frequency of scFs. In addition, it allows us to examine the degree of between all the verb form spewe are actually concerned with. For example, this method can be used to decide whether the Levin class of &amp;quot;Verbs of Sending and Carrying&amp;quot; is distinctive enough in terms of subcategorization, or whether it be divided into subclasses, i.e. &amp;quot;Bring &amp;quot;Carry&amp;quot; verbs. Step 1 determines that the intersection of between the five subclasses is fairly All classes share 2 four share 5, and three share 1. Step 2 concludes that for five individual verbs from each subclass: float, bring, carry fairly similar in terms of RC and the intersection of scFs. The broad class seems thus syntactically coherent enough to proan adequate basis for back-off 3.2 Automatic Verb Classification In Korhonen (2000), verbs were manually asto semantic classes. For large-scale acquisition, a method is needed for automatic are indebted to Bonnie Dorr for the use of these codes. use obtained via manual analysis of corpus data. See e.g. (Manning and Schfitze, 1999) details of Kt, and Korhonen (2002) for details of this method. classification of verbs. We propose one which involves assigning verbs to semantic classes via WordNet. Although WordNet&apos;s semantic organization does not always go hand in hand with syntactic information, synonymous verbs in WordNet exhibit syntactic behaviour similar to that characterised in the classification system of Levin (Dorr, 1997). Dorr (1997) has proposed a fully automatic verb classification algorithm which classifies verbs semantically on the basis of their Word- Net synonyms. This algorithm relies solely on lexical resources but is not accurate enough for our purpose. As inaccurate assignments can degrade performance, some allowance for manual intervention is necessary. We therefore propose a semi-automatic algorithm. This assigns entire WordNet synsets semantic making use of Levin&apos;s verb the and Dorr&apos;s codes for Levin classes. The algorithm classifies synsets subhierarchy by subhierarchy, starting from the top level synsets, and going further down in the taxonomy when required. Each synset is classified by first assigning the majority of its member verbs to a semantic class and then choosing the Levin class supported by the highest number of verbs. &apos;Member verbs&apos; refer here to those which, according to their predominant sense, are members of the synset in question and of its hyponym synsets. The algorithm proceeds as follows: 1: the majority of member verbs of a given Levin from the same class, clas- 2: classify more member verbs (according to Step 4a-d) until the majority are classified, and then go back to Step 1. Step 3: Otherwise, if the classified verbs point to dif- Levin classes, examine whether of synsets. If not, assign the Levin class supported by the highest number of classified verbs. If yes, go one level down in the hierarchy and classify the hyponym synsets separately, starting again from Step 1. objective is to build a static source where Word- Net synsets are associated with different Levin classes. Although static, the source will allow for updating and adding new verbs to WordNet. verbs&apos; refer to verbs whose predominant sense belongs to the Levin class in question. 4: If no Levin verbs, proceed as follows classify the majority of member verbs of (a) Extract the predominant sense of a given verb V from WordNet Extract the syntactic codes from relevant to this sense (c) Examine whether V could be assigned to a Levin class already associated with the other verbs in the (i) same synset, (i) possible hypernym synset or (iii) possible sister synsets by the of the sense and of the respective Levin class(es). Given the hypothesised make the final class assignment manually. (d) If no suitable class is found, re-examine the case after more verbs have been analysed. If the classification remains unsolved, set V aside for later examination. The above algorithm is for the most part automatic, however, Step 4b and part of Step 4c (the final class assignment) are done manually to ensure accuracy of classification. For example, the synset no. 00994853 includes 13 member verbs, 4 of which are Levin &amp;quot;Verbs of Sending and Carrying&amp;quot;. We need to classify more verbs to determine class assignment. We extract its predominant sense from WordNet: whisk -- (move somewhere quickly; &amp;quot;The president was whisked away in his limo&amp;quot;) verb has three senses. That corresponding to the predominant WordNet sense is identified as: 2. [X9 esp. OFF, AWAY] to remove b. by taking suddenly: &amp;quot;She whisked the cups away / whisked him (off) home&amp;quot; 11 Levin classes are already matched with the verbs in the same, hypernym and sister synsets. Those whose syntactic description includes the Verbs of putting Verbs of removing Verbs of sending and carrying Verbs of exerting force Verbs of motion verifying these options manually, is assigned to &amp;quot;Verbs of Sending and Carrying&amp;quot;. 3.3 Completed Work The verb classification algorithm was applied to 3 WordNet verb files (contact, possession and motion verbs). 1581 synsets in these files were assigned to semantic classes and 148 were left unclassified. A small number of synsets (35) from other verb files were classified as well. From the total of 32 broad Levin classes exemplified among the classified WordNet synsets, 22 of the most frequent were chosen for further work. These were re-grouped into semantic classes by using the method described in section 3.1. This led to the combination of 5 pairs of broad Levin classes and the division of 3 into subclasses. The resulting 20 semantic classes are shown in table 1, labelled by class codes shown in the first column. Back-off estimates for these classes were built using the method described in section 2.2. Verbs A 9. Putting B 10. Removing: 10.1-3, 10.5-9 C 10. Removing: 10.4 D 11. Sending and Carrying 12. Exerting Force E 13. Change of Possession F 15. Hold and Keep 16. Concealment G 17. Throwing H 18. Contact by Impact 19. Poke Verbs I 20. Contact J 21. Cutting K 22. Combining and Attaching: 22.1-4 L 22. Combining and Attaching: 22.5 M 23. Separating and Disassembling: 21.1-3 N 23. Separating and Disassembling: 23.4 0 34. Assessment 35. Searching P 36. Social Interaction Q 42. Killing R 44. Destroy S 47. Existence: Verbs of Spatial Configuration 50. Assuming Position T 51. Motion Table 1: Semantic verb classes 4 Experimental Evaluation 4.1 Classification Algorithm To evaluate the classification algorithm, we chose 30 synsets from among the 1616 classified. The synsets were chosen at random so that 10 were taken from each of the three Word- Net verb files (contact, possession and motion verbs). From the total of 378 verbs in these synsets, we chose for evaluation those 151 which were neither Levin verbs nor classified manually when linking synsets with Levin classes. For these verbs, the semantic classification was compared against manually obtained gold standard classification. The algorithm classified correctly 140 verbs. The accuracy of the class assignment was thus 93%. This result is encouraging. However, the correspondence between synsets and Levin classes varies largely across WordNet. Further evaluation is therefore needed in the future with a larger set of WordNet files. 4.2 Subcategorization Acquisition 4.2.1 Test Data and Method To evaluate the revised approach to hypothesis selection, we took a sample of 20M words of extracted all sentences containing an occurrence of one of 91 verbs. The verbs were chosen at random, subject to the constraint that they were classified by the algorithm as members of one of the 20 semantic classes constructed (table 1). After the extraction process, we retained c. 1000 citations for each verb. The sentences containing these verbs were by the system. The hypothesis generator was held constant, the exception being that the data for these experiments were parsed using a probabilistic chart parser (Chitrao and Grishman, 1990). For hypothesis selection, we employed the method of Korhonen (2000) with the extensions described. also obtained results for the baseline thresholding method without any smoothing. The results were evaluated against a manual analysis of the corpus data. This was obtained by analysing a maximum of 300 occurrences for test verb in the We calculated type precision, type recall and F measure. In to the system results, we calculated the acquired unfiltered distributions and the gold standard distributions. We also recorded the total number of unseen in the acquired unfiltered which occurred in the gold standard distributions. This was to investigate how well the approach deals with sparse data. 4.2.2 Results Table 2 gives average results for all the 91 test verbs. The semantically-driven method outperforms the baseline on most measures. The imindicates that the method improves overall accuracy of The with that it helps to correct the ranking of scFs. Precision worsens slighly from the baseline (1.4%), however, recall improves significantly (24%). That both precision and recall are high demonstrates that the method deals well with both highly ranked scFs and those low in frequency. The baseline method simply ignores low frequency data and therefore yields poor recall. While a total of 114 gold standard scFs were unseen in the data from the hypothesis generator, only 24 were unseen after smoothing with back-off estimates. This shows further that the method deals effectively with sparse data. Verb class specific results in table 3 allow us to examine the accuracy of the back-off estimates. The first column shows a semantic verb class and the second indicates the number of verbs tested for the class. According to the F measure, the semanticallydriven method (sEm) outperforms the baseline (BL) in all 16 verb classes. show improvement in 12 classes. Four classes worse performance: E, F, 0. Several reasons were identified for the poor performance (or small improvement) with some verbs/classes: Firstly, the class specific back-off estimates are not accurate enough for highly polysemic verbs. &amp;quot;Note that KT, &gt; 0, with KT, near to 0 denoting strong and —1 &lt; with to 0 denota low degree of association and to -1 and 1 denoting strong association. System results Unseen Method KL RC Precision (%) Recall (%) F SCES Baseline 0.42 0.62 82.2 48.5 61.0 114 Semantic 0.26 0.74 80.8 72.5 76.4 24 Table 2: Average results for 91 verbs Class Tested KL RC F Measure Unseen SCES BL SEM BL SEM BL SEM BL SEM A 6 0.39 0.23 0.63 0.83 62.9 83.3 10 2 B 7 0.11 0.21 0.90 0.87 65.7 73.7 8 1 C 3 0.54 0.16 0.33 0.72 54.6 84.0 5 0 D 10 0.63 0.39 0.46 0.62 55.6 74.5 20 2 E 9 0.13 0.16 0.87 0.87 58.6 60.4 17 3 F 5 0.21 0.26 0.78 0.88 50.7 60.2 11 6 G 3 0.52 0.27 0.48 0.88 71.4 89.8 6 1 H 6 0.45 0.27 0.64 0.64 64.9 84.2 4 1 I 3 0.15 0.14 0.59 0.66 57.9 73.7 7 1 J 5 0.40 0.11 0.66 0.84 63.9 84.8 1 1 K 5 0.52 0.45 0.66 0.77 63.3 70.3 3 0 0 4 0.35 0.44 0.59 0.59 54.6 57.7 3 2 P 7 0.55 0.39 0.62 0.63 61.9 65.3 10 3 R 3 0.10 0.00 1.00 1.00 63.2 66.7 0 0 S 4 0.71 0.22 0.37 0.65 66.7 87.9 2 0 T 11 0.59 0.28 0.55 0.71 64.2 89.1 7 1 Table 3: Results for semantic classes For example, 6 scFs are reported unseen with these all involve senses not taken by the verbs used for constructing the backoff estimates. Secondly, the back-off estimates are not accurate for verbs whose distribution of senses is not particularly zipfian, i.e. verbs whose predominant sense is not very frequent class F). Thirdly, the empirically set (verb class specific) filtering thresholds appeared too high/low for some individual verbs. Finally, although most semantic classes proved fine-grained enough, one class appeared broad: class was obtained by mergtwo broad Levin classes and &amp;quot;Concealment&amp;quot; verbs). In sum, the method proposed for construction of semantic classes proved fairly accurate. Semantic classes can easily be evaluated in the of and refined further where required. Korhonen (2000) predicted that the total number of semantic classes across the whole lexicon is unlikely to exceed 50. This seems still a valid prediction, as promising results were generally obtained assuming a broad notion of a semantic class. 5 Conclusion We adopted the semantically motivated approach to hypothesis selection proposed by Korhonen (2000) and modified it to be suitable large-scale A method was proposed for construction of semantic classes and a classification algorithm was designed which automatically assigns verbs to semantic classes via WordNet. Experimental evaluation was provided which showed that the classification algorithm is highly accurate and that the revised method for semantically-driven hypothesis selection can be used to succesfully guide, structure and improve the large-scale acquisition of scFs from corpus data. Future work will include: investigating reducing manual effort in the classification algorithm and construction of back-off estimates, constructing semantic classes and back-off estimates across the entire lexicon, applying the classification algorithm to the remaining Wordsynsets and verb files, addressing the prob- M. Brent. 1993. From grammar to lexicon: unsulearning of lexical syntax. E. J. Briscoe and J. Carroll. 1997. Automatic extraction of subcategorization from corpora. In 5th ACL Conference on Applied Natural Language 356-363. J. Carroll and E. J. Briscoe. 1996. Apportioning development effort in a probabilistic lr parsing through evaluation. In ACL/SIGDAT Conference on Empirical Methods in Natural Lan- Processing, G. Carroll and M. Rooth. 1998. Valence induction a head-lexicalized pcfg. In Conference on Empirical Methods in Natural Language Processing. Stanley F. Chen and Joshua Goodman. 1996. An empirical study of smoothing techniques for lanmodeling. In 310-318. ACL. M. Chitrao and R. Grishman. 1990. Statistical parsof messages. In Speech and Natural Lan- Workshop, H. T. Dang, K. Kipper, M. Palmer, and J. Rosensweig. 1998. Investigating regular sense extensions based on intersective levin classes.</abstract>
<note confidence="0.832197846153846">Annual Meeting of the Association for Computational Linguistics and 17th International on Computational Linguistics, 293-299. B. Dorr. 1997. Large-scale dictionary construction for foreign language tutoring and interlinmachine translation. Translation, 12(4):271-325. R. Grishman, C. Macleod, and A. Meyers. 1994. Comlex syntax: building a computational lexi- In Conference on Computa- Linguistics, Korhonen, G. Gorrell, and D. McCarthy. 2000. Statistical filtering and subcategorization frame In SIGDAT Conference on Empirical Methods in Natural Language Processing Very Large Corpora, Korhonen. 2000. Using semantically motivated estimates to help subcategorization acquisition. In Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large 2002. Acquisition. Ph.D. thesis, University of Cambridge, UK. G. Leech. 1992. 100 million words of english: british national corpus. Research, 28(1):1-13. Levin. 1993. Verb Classes and Alterna- University Press, Chicago. Manning and H. Schiitze. 1999. Statistical Natural Language Processing. Press, Cambridge, Massachusetts. C. Manning. 1993. Automatic acquisition of a large dictionary from corpora. In Annual Meeting of the Association for Computa- Linguistics, A. Miller. 1990. Wordnet: An lexical Journal of Lexicography, 3(4):235-312. J. Preiss, A. Korhonen, and E. J. Briscoe. 2002. Subcategorization acquisition as an evaluation method wsd. In Resources and Evaluation appear. Procter. 1978. Dictionary of Contem- English. England. A. Sarkar and D. Zeman. 2000. Automatic extracof subcategorization frames for czech. In International Conference on Computational Lin- 691-697. A. Ushioda, D. Evans, T. Gibson, and A. Waibel. 1993. The automatic acquisition of frequencies of verb subcategorization frames from tagged corpora. In B. Boguraev and J. Pustejovsky, edi-</note>
<title confidence="0.508">ACL Workshop on the Acquisition</title>
<affiliation confidence="0.610272">Lexical Knowledge from Text,</affiliation>
<address confidence="0.98989">Columbus, Ohio.</address>
<abstract confidence="0.920806">lem of polysemy, and working on improving the A. accuracy of the filtering method.</abstract>
<note confidence="0.784705375">References A. B. Boguraev, E. J. Briscoe, J. Carroll, D. Carter, and C. Grover. 1987. The derivation of a grammatically-indexed lexicon from the longman of contemporary english. In Annual Meeting of the Association for Computational 193-200. A.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Brent</author>
</authors>
<title>From grammar to lexicon: unsupervised learning of lexical syntax.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--3</pages>
<marker>Brent, 1993</marker>
<rawString>M. Brent. 1993. From grammar to lexicon: unsupervised learning of lexical syntax. Computational Linguistics, 19(3):243-262.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E J Briscoe</author>
<author>J Carroll</author>
</authors>
<title>Automatic extraction of subcategorization from corpora.</title>
<date>1997</date>
<booktitle>In 5th ACL Conference on Applied Natural Language Processing,</booktitle>
<pages>356--363</pages>
<contexts>
<context position="2116" citStr="Briscoe and Carroll (1997)" startWordPosition="315" endWordPosition="318">larger scale SCF acquisition. Essentially, we propose methods for construction of semantic classes and automatic semantic classification of verbs. We report an experiment which shows that the method can be used to improve large-scale SCF acquisition. We introduce the baseline SCF acquisition framework in section 2 and describe our extensions to it in section 3. Section 4 gives details of the experimental evaluation and section 5 concludes with directions for future work. 2 Framework for SCF Acquisition 2.1 Hypothesis Generation We employ for hypothesis generation the SCF acquisition system of Briscoe and Carroll (1997). This system is capable of distinguishing 163 verbal scFs a superset of those found in the ANLT (Boguraev et al., 1987) and COMLEX Syntax dictionaries (Grishman et al., 1994) and returning relative frequencies for each SCF found for a verb. It works by first tagging, lemmatizing and parsing corpus data using a robust statistical parser (Carroll and Briscoe, 1996) which employs a grammar written in a feature-based unification grammar formalism. This yields complete though intermediate parses. Local syntactic frames including the syntactic categories and head lemmas of constituents are then ext</context>
</contexts>
<marker>Briscoe, Carroll, 1997</marker>
<rawString>E. J. Briscoe and J. Carroll. 1997. Automatic extraction of subcategorization from corpora. In 5th ACL Conference on Applied Natural Language Processing, pages 356-363.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carroll</author>
<author>E J Briscoe</author>
</authors>
<title>Apportioning development effort in a probabilistic lr parsing system through evaluation.</title>
<date>1996</date>
<booktitle>In 1st ACL/SIGDAT Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>92--100</pages>
<contexts>
<context position="2482" citStr="Carroll and Briscoe, 1996" startWordPosition="377" endWordPosition="380">ction 4 gives details of the experimental evaluation and section 5 concludes with directions for future work. 2 Framework for SCF Acquisition 2.1 Hypothesis Generation We employ for hypothesis generation the SCF acquisition system of Briscoe and Carroll (1997). This system is capable of distinguishing 163 verbal scFs a superset of those found in the ANLT (Boguraev et al., 1987) and COMLEX Syntax dictionaries (Grishman et al., 1994) and returning relative frequencies for each SCF found for a verb. It works by first tagging, lemmatizing and parsing corpus data using a robust statistical parser (Carroll and Briscoe, 1996) which employs a grammar written in a feature-based unification grammar formalism. This yields complete though intermediate parses. Local syntactic frames including the syntactic categories and head lemmas of constituents are then extracted from parses, from sentence subanalyses which begin/end at the boudaries of predicates. The resulting patterns are assigned to scFs on the basis of the feature values of syntactic categories and head lemmas in each pattern. Finally, sets of scFs are gathered for verbs and putative lexical entries are constructed. 2.2 Hypothesis Selection The method for hypot</context>
</contexts>
<marker>Carroll, Briscoe, 1996</marker>
<rawString>J. Carroll and E. J. Briscoe. 1996. Apportioning development effort in a probabilistic lr parsing system through evaluation. In 1st ACL/SIGDAT Conference on Empirical Methods in Natural Language Processing, pages 92-100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Carroll</author>
<author>M Rooth</author>
</authors>
<title>Valence induction with a head-lexicalized pcfg.</title>
<date>1998</date>
<booktitle>In 3rd Conference on Empirical Methods in Natural Language Processing.</booktitle>
<marker>Carroll, Rooth, 1998</marker>
<rawString>G. Carroll and M. Rooth. 1998. Valence induction with a head-lexicalized pcfg. In 3rd Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1996</date>
<booktitle>In ACL-96,</booktitle>
<pages>310--318</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="4695" citStr="Chen and Goodman, 1996" startWordPosition="721" endWordPosition="724">ion is observed because the distribution of verb senses tends to be zipfian: the predominant sense typically covers most of the total frequency mass (Preiss et al., 2111 this paper, we refer to the WordNet version 1.6. 2002). The method for hypothesis selection involves first (manually) identifying, for a single verb, the broad Levin class3 corresponding to the predominant sense of this verb in WordNet. A SCF distribution is then acquired for the verb from corpus data, using the hypothesis generator described in the previous section. This distribution is smoothed - using linear interpolation (Chen and Goodman, 1996). - with the &amp;quot;backoff&amp;quot; estimates of the Levin class. Back-off estimates are obtained by (i) choosing 4-5 representative Levin verbs from a class, (ii) building SCF distributions for these verbs by manually analysing c. 300 occurrences of each verb in the BNC corpus (Leech, 1992) and (iii) merging the resulting set of SCF distributions. For example, the back-off estimates for the &amp;quot;Motion verbs&amp;quot; are constructed by merging the SCF distributions for fly, walk, march and travel. A simple method is finally used for filtering, which sets an empirically defined threshold on the probability estimates f</context>
</contexts>
<marker>Chen, Goodman, 1996</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1996. An empirical study of smoothing techniques for language modeling. In ACL-96, pages 310-318. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Chitrao</author>
<author>R Grishman</author>
</authors>
<title>Statistical parsing of messages.</title>
<date>1990</date>
<booktitle>In Darpa Speech and Natural Language Workshop,</booktitle>
<pages>263--266</pages>
<contexts>
<context position="17838" citStr="Chitrao and Grishman, 1990" startWordPosition="2862" endWordPosition="2865">is selection, we took a sample of 20M words of BNC. We extracted all sentences containing an occurrence of one of 91 verbs. The verbs were chosen at random, subject to the constraint that they were classified by the algorithm as members of one of the 20 semantic classes constructed (table 1). After the extraction process, we retained c. 1000 citations for each verb. The sentences containing these verbs were processed by the SCF acquisition system. The hypothesis generator was held constant, the exception being that the data for these experiments were parsed using a probabilistic chart parser (Chitrao and Grishman, 1990). For hypothesis selection, we employed the method of Korhonen (2000) with the extensions described. We also obtained results for the baseline MLE thresholding method without any smoothing. The results were evaluated against a manual analysis of the corpus data. This was obtained by analysing a maximum of 300 occurrences for each test verb in the BNC corpora. We calculated type precision, type recall and F measure. In addition to the system results, we calculated KL and RC between the acquired unfiltered SCF distributions and the gold standard distributions. We also recorded the total number o</context>
</contexts>
<marker>Chitrao, Grishman, 1990</marker>
<rawString>M. Chitrao and R. Grishman. 1990. Statistical parsing of messages. In Darpa Speech and Natural Language Workshop, pages 263-266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H T Dang</author>
<author>K Kipper</author>
<author>M Palmer</author>
<author>J Rosensweig</author>
</authors>
<title>Investigating regular sense extensions based on intersective levin classes.</title>
<date>1998</date>
<booktitle>In 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics,</booktitle>
<pages>293--299</pages>
<contexts>
<context position="7564" citStr="Dang et al., 1998" startWordPosition="1180" endWordPosition="1183">n back-off estimates for each class, and (c) to implement a method capable of automatically assigning verbs to semantic classes. We propose methods for (a) and (c) in sections 3.1 and 3.2, respectively. Section 3.3 discusses the work completed on (a), (b) and (c). 3.1 Semantic Classes Korhonen (2000) classified verbs into Levin classes. These provide us with a good starting point for large-scale SCF acquisition as well. Although not comprehensive, they cover a substantial number of diathesis alternations occurring in English and work on refining and extending this classification is under way (Dang et al., 1998; Dorr, 1997)5. As it is important to minimise the cost involved in constructing back-off estimates, we do not adopt the 191 Levin classes as they stand, although this would allow maximal accuracy. We also do not adopt broad Levin classes as they stand, as some proved inaccurate in the preliminary experiment. Rather, a method was devised for determining the specificity of the Levin class(es) required for adequate distinctiveness in terms of subcategorization. The method proceeds in two steps, by examining the: Step 1: Syntactic similarity between Levin classes. In this, we use Dorr&apos;s (1997) so</context>
</contexts>
<marker>Dang, Kipper, Palmer, Rosensweig, 1998</marker>
<rawString>H. T. Dang, K. Kipper, M. Palmer, and J. Rosensweig. 1998. Investigating regular sense extensions based on intersective levin classes. In 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, pages 293-299.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Dorr</author>
</authors>
<title>Large-scale dictionary construction for foreign language tutoring and interlingual machine translation.</title>
<date>1997</date>
<journal>Machine Translation,</journal>
<pages>12--4</pages>
<contexts>
<context position="7577" citStr="Dorr, 1997" startWordPosition="1184" endWordPosition="1185">s for each class, and (c) to implement a method capable of automatically assigning verbs to semantic classes. We propose methods for (a) and (c) in sections 3.1 and 3.2, respectively. Section 3.3 discusses the work completed on (a), (b) and (c). 3.1 Semantic Classes Korhonen (2000) classified verbs into Levin classes. These provide us with a good starting point for large-scale SCF acquisition as well. Although not comprehensive, they cover a substantial number of diathesis alternations occurring in English and work on refining and extending this classification is under way (Dang et al., 1998; Dorr, 1997)5. As it is important to minimise the cost involved in constructing back-off estimates, we do not adopt the 191 Levin classes as they stand, although this would allow maximal accuracy. We also do not adopt broad Levin classes as they stand, as some proved inaccurate in the preliminary experiment. Rather, a method was devised for determining the specificity of the Levin class(es) required for adequate distinctiveness in terms of subcategorization. The method proceeds in two steps, by examining the: Step 1: Syntactic similarity between Levin classes. In this, we use Dorr&apos;s (1997) source of LDOCE</context>
<context position="10970" citStr="Dorr, 1997" startWordPosition="1725" endWordPosition="1726"> is needed for automatic 6We are indebted to Bonnie Dorr for the use of these codes. 7We use SCF distributions obtained via manual analysis of corpus data. See e.g. (Manning and Schfitze, 1999) for details of Kt, and RC. 8See Korhonen (2002) for details of this method. classification of verbs. We propose one which involves assigning verbs to semantic classes via WordNet. Although WordNet&apos;s semantic organization does not always go hand in hand with syntactic information, synonymous verbs in WordNet exhibit syntactic behaviour similar to that characterised in the classification system of Levin (Dorr, 1997). Dorr (1997) has proposed a fully automatic verb classification algorithm which classifies verbs semantically on the basis of their WordNet synonyms. This algorithm relies solely on lexical resources but is not accurate enough for our purpose. As inaccurate assignments can actually degrade SCF acquisition performance, some allowance for manual intervention is necessary. We therefore propose a semi-automatic algorithm. This assigns entire WordNet synsets to semantic classes9, making use of Levin&apos;s verb index, the LDOCE dictionary and Dorr&apos;s LDOCE codes for Levin classes. The algorithm classifi</context>
</contexts>
<marker>Dorr, 1997</marker>
<rawString>B. Dorr. 1997. Large-scale dictionary construction for foreign language tutoring and interlingual machine translation. Machine Translation, 12(4):271-325.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Grishman</author>
<author>C Macleod</author>
<author>A Meyers</author>
</authors>
<title>Comlex syntax: building a computational lexicon.</title>
<date>1994</date>
<booktitle>In International Conference on Computational Linguistics,</booktitle>
<pages>268--272</pages>
<contexts>
<context position="2291" citStr="Grishman et al., 1994" startWordPosition="346" endWordPosition="349">ows that the method can be used to improve large-scale SCF acquisition. We introduce the baseline SCF acquisition framework in section 2 and describe our extensions to it in section 3. Section 4 gives details of the experimental evaluation and section 5 concludes with directions for future work. 2 Framework for SCF Acquisition 2.1 Hypothesis Generation We employ for hypothesis generation the SCF acquisition system of Briscoe and Carroll (1997). This system is capable of distinguishing 163 verbal scFs a superset of those found in the ANLT (Boguraev et al., 1987) and COMLEX Syntax dictionaries (Grishman et al., 1994) and returning relative frequencies for each SCF found for a verb. It works by first tagging, lemmatizing and parsing corpus data using a robust statistical parser (Carroll and Briscoe, 1996) which employs a grammar written in a feature-based unification grammar formalism. This yields complete though intermediate parses. Local syntactic frames including the syntactic categories and head lemmas of constituents are then extracted from parses, from sentence subanalyses which begin/end at the boudaries of predicates. The resulting patterns are assigned to scFs on the basis of the feature values of</context>
</contexts>
<marker>Grishman, Macleod, Meyers, 1994</marker>
<rawString>R. Grishman, C. Macleod, and A. Meyers. 1994. Comlex syntax: building a computational lexicon. In International Conference on Computational Linguistics, pages 268-272.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gorrell Korhonen</author>
<author>D McCarthy</author>
</authors>
<title>Statistical filtering and subcategorization frame acquisition.</title>
<date>2000</date>
<booktitle>In Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,</booktitle>
<pages>199--205</pages>
<marker>Korhonen, McCarthy, 2000</marker>
<rawString>Korhonen, G. Gorrell, and D. McCarthy. 2000. Statistical filtering and subcategorization frame acquisition. In Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, pages 199-205.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Korhonen</author>
</authors>
<title>Using semantically motivated estimates to help subcategorization acquisition.</title>
<date>2000</date>
<booktitle>In Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,</booktitle>
<pages>216--223</pages>
<contexts>
<context position="653" citStr="Korhonen (2000)" startWordPosition="95" endWordPosition="96">eedings of the Workshop of the ACL Special Interest Group on the Lexicon (SIGLEX), Philadelphia, July 2002, pp. 51-58. Association for Computational Linguistics. with low frequency scFs. According to one account (Briscoe and Carroll, 1997) the majority of errors arise in SCF acquisition because of the statistical filtering process. Korhonen et al. (2000) investigated reasons for the poor performance of the statistical filters and reported better accuracy with a simple filter which sets a threshold on the relative frequency of SCF entries, selecting only those scFs which are high in frequency. Korhonen (2000) proposed combining this filter with a method which deals better with sparse data. The method exploits the knowledge that semantically similar verbs show similar subcategorization behaviour. It involves identifying the semantic class of a predicate, using a hypothesis generator to acquire a SCF distribution for the predicate, smoothing this distribution with the back-off estimates of the respective semantic verb class, and setting an empirically defined threshold on the probability estimates from smoothing to filter out unreliable scFs. The small scale experiment reported in Korhonen (2000) de</context>
<context position="3651" citStr="Korhonen (2000)" startWordPosition="553" endWordPosition="554"> 2.2 Hypothesis Selection The method for hypothesis selection exploits the knowledge that semantically similar verbs are frequently similar also in terms of subcategorization. The motion verbs fly and move, for example, take similar SCF distributions, which differ essentially from the ones taken e.g. by communication verbs tell and say. Although no perfect correspondence exists between the semantic and syntactic properties of verbs, useful generalizations can be made: Levin (1993) has demonstrated that verb senses can be divided into semantic classes distinctive in terms of subcategorization. Korhonen (2000) shows that verb forms can also be divided into such classes, according to their predominant (i.e. the most frequent) sense. For instance, the verb form specific SCF distributions for the highly polysemic verbs fly and move correlate quite well because the predominant senses of these verbs (according to the WordNet (Miller, 1990) frequency data2) are similar. They both belong to the Levin &amp;quot;Motion verbs&amp;quot;. Good correlation is observed because the distribution of verb senses tends to be zipfian: the predominant sense typically covers most of the total frequency mass (Preiss et al., 2111 this pape</context>
<context position="5810" citStr="Korhonen (2000)" startWordPosition="896" endWordPosition="897">nally used for filtering, which sets an empirically defined threshold on the probability estimates from smoothing. Essentially, this method involves using a priori knowledge about generalizations of verb semantics to guide subcategorization acquisition. Back-off estimates are used to correct the acquired SCF distribution and deal with sparse data. However, the parameters used in smoothing are obtained by optimising SCF acquisition performance on held-out training data so that most of the smoothed probability is determined by the maximum likelihood estimate (mLE) from the hypothesis generator. Korhonen (2000) evaluated this method with 60 test verbs from 10 Levin classes. It yielded 88% type precision (the percentage of SCF types that the method proposes which are correct) and 69% type recall (the percentage of SCF types in the gold standard that the method proposes). The method was compared against a baseline method which sets threshold on MLES of SCFS from the hypothesis generator. F measure4 was 77.1 for the novel method and 70.1 for the base3Korhonen (2000) employs broad Levin classes only (e.g. 51. &amp;quot;Motion verbs&amp;quot;), not subclasses these may divide into (e.g. 51.2 &amp;quot;Leave verbs&amp;quot;). 4F = 2.precisi</context>
<context position="7248" citStr="Korhonen (2000)" startWordPosition="1130" endWordPosition="1131">In this paper, we describe how this semantically-driven method for hypothesis selection was refined further and integrated as part of large-scale SCF acquisition. 3 Extensions to the Framework Applying the method on a large scale requires us to (a) define a comprehensive set of semantic verb classes, (b) to obtain back-off estimates for each class, and (c) to implement a method capable of automatically assigning verbs to semantic classes. We propose methods for (a) and (c) in sections 3.1 and 3.2, respectively. Section 3.3 discusses the work completed on (a), (b) and (c). 3.1 Semantic Classes Korhonen (2000) classified verbs into Levin classes. These provide us with a good starting point for large-scale SCF acquisition as well. Although not comprehensive, they cover a substantial number of diathesis alternations occurring in English and work on refining and extending this classification is under way (Dang et al., 1998; Dorr, 1997)5. As it is important to minimise the cost involved in constructing back-off estimates, we do not adopt the 191 Levin classes as they stand, although this would allow maximal accuracy. We also do not adopt broad Levin classes as they stand, as some proved inaccurate in t</context>
<context position="10266" citStr="Korhonen (2000)" startWordPosition="1614" endWordPosition="1615">d into subclasses, i.e. &amp;quot;Send&amp;quot;, &amp;quot;Slide&amp;quot;, &amp;quot;Bring and Take&amp;quot;, &amp;quot;Carry&amp;quot; and &amp;quot;Drive&amp;quot; verbs. Step 1 determines that the intersection of LDOCE codes between the five subclasses is fairly large. All classes share 2 LDOCE codes, four share 5, and three share 1. Step 2 concludes that the SCF distributions for five individual verbs (one from each subclass: send, float, bring, carry and ferry) are fairly similar in terms of KL, RC and the intersection of scFs. The broad class seems thus syntactically coherent enough to provide an adequate basis for back-off estimates8. 3.2 Automatic Verb Classification In Korhonen (2000), verbs were manually assigned to semantic classes. For large-scale SCF acquisition, a method is needed for automatic 6We are indebted to Bonnie Dorr for the use of these codes. 7We use SCF distributions obtained via manual analysis of corpus data. See e.g. (Manning and Schfitze, 1999) for details of Kt, and RC. 8See Korhonen (2002) for details of this method. classification of verbs. We propose one which involves assigning verbs to semantic classes via WordNet. Although WordNet&apos;s semantic organization does not always go hand in hand with syntactic information, synonymous verbs in WordNet exhi</context>
<context position="17907" citStr="Korhonen (2000)" startWordPosition="2875" endWordPosition="2876">ntaining an occurrence of one of 91 verbs. The verbs were chosen at random, subject to the constraint that they were classified by the algorithm as members of one of the 20 semantic classes constructed (table 1). After the extraction process, we retained c. 1000 citations for each verb. The sentences containing these verbs were processed by the SCF acquisition system. The hypothesis generator was held constant, the exception being that the data for these experiments were parsed using a probabilistic chart parser (Chitrao and Grishman, 1990). For hypothesis selection, we employed the method of Korhonen (2000) with the extensions described. We also obtained results for the baseline MLE thresholding method without any smoothing. The results were evaluated against a manual analysis of the corpus data. This was obtained by analysing a maximum of 300 occurrences for each test verb in the BNC corpora. We calculated type precision, type recall and F measure. In addition to the system results, we calculated KL and RC between the acquired unfiltered SCF distributions and the gold standard distributions. We also recorded the total number of unseen scFs in the acquired unfiltered SCF distributions which occu</context>
<context position="22107" citStr="Korhonen (2000)" startWordPosition="3627" endWordPosition="3628">verbs whose predominant sense is not very frequent (e.g. keep in class F). Thirdly, the empirically set (verb class specific) filtering thresholds appeared too high/low for some individual verbs. Finally, although most semantic classes proved fine-grained enough, one class appeared too broad: class F, which was obtained by merging two broad Levin classes (&amp;quot;Hold and Keep&amp;quot; and &amp;quot;Concealment&amp;quot; verbs). In sum, the method proposed for construction of semantic classes proved fairly accurate. Semantic classes can easily be evaluated in the context of SCF acquisition and refined further where required. Korhonen (2000) predicted that the total number of semantic classes across the whole lexicon is unlikely to exceed 50. This seems still a valid prediction, as promising results were generally obtained assuming a broad notion of a semantic class. 5 Conclusion We adopted the semantically motivated approach to hypothesis selection proposed by Korhonen (2000) and modified it to be suitable for large-scale SCF acquisition. A method was proposed for construction of semantic classes and a classification algorithm was designed which automatically assigns verbs to semantic classes via WordNet. Experimental evaluation</context>
</contexts>
<marker>Korhonen, 2000</marker>
<rawString>Korhonen. 2000. Using semantically motivated estimates to help subcategorization acquisition. In Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, pages 216-223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Korhonen</author>
</authors>
<title>Subcategorization Acquisition.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Cambridge, UK.</institution>
<contexts>
<context position="10600" citStr="Korhonen (2002)" startWordPosition="1671" endWordPosition="1672">ch subclass: send, float, bring, carry and ferry) are fairly similar in terms of KL, RC and the intersection of scFs. The broad class seems thus syntactically coherent enough to provide an adequate basis for back-off estimates8. 3.2 Automatic Verb Classification In Korhonen (2000), verbs were manually assigned to semantic classes. For large-scale SCF acquisition, a method is needed for automatic 6We are indebted to Bonnie Dorr for the use of these codes. 7We use SCF distributions obtained via manual analysis of corpus data. See e.g. (Manning and Schfitze, 1999) for details of Kt, and RC. 8See Korhonen (2002) for details of this method. classification of verbs. We propose one which involves assigning verbs to semantic classes via WordNet. Although WordNet&apos;s semantic organization does not always go hand in hand with syntactic information, synonymous verbs in WordNet exhibit syntactic behaviour similar to that characterised in the classification system of Levin (Dorr, 1997). Dorr (1997) has proposed a fully automatic verb classification algorithm which classifies verbs semantically on the basis of their WordNet synonyms. This algorithm relies solely on lexical resources but is not accurate enough fo</context>
</contexts>
<marker>Korhonen, 2002</marker>
<rawString>Korhonen. 2002. Subcategorization Acquisition. Ph.D. thesis, University of Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Leech</author>
</authors>
<title>100 million words of english: the british national corpus.</title>
<date>1992</date>
<journal>Language Research,</journal>
<pages>28--1</pages>
<contexts>
<context position="4974" citStr="Leech, 1992" startWordPosition="770" endWordPosition="771">entifying, for a single verb, the broad Levin class3 corresponding to the predominant sense of this verb in WordNet. A SCF distribution is then acquired for the verb from corpus data, using the hypothesis generator described in the previous section. This distribution is smoothed - using linear interpolation (Chen and Goodman, 1996). - with the &amp;quot;backoff&amp;quot; estimates of the Levin class. Back-off estimates are obtained by (i) choosing 4-5 representative Levin verbs from a class, (ii) building SCF distributions for these verbs by manually analysing c. 300 occurrences of each verb in the BNC corpus (Leech, 1992) and (iii) merging the resulting set of SCF distributions. For example, the back-off estimates for the &amp;quot;Motion verbs&amp;quot; are constructed by merging the SCF distributions for fly, walk, march and travel. A simple method is finally used for filtering, which sets an empirically defined threshold on the probability estimates from smoothing. Essentially, this method involves using a priori knowledge about generalizations of verb semantics to guide subcategorization acquisition. Back-off estimates are used to correct the acquired SCF distribution and deal with sparse data. However, the parameters used </context>
</contexts>
<marker>Leech, 1992</marker>
<rawString>G. Leech. 1992. 100 million words of english: the british national corpus. Language Research, 28(1):1-13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Levin</author>
</authors>
<title>English Verb Classes and Alternations.</title>
<date>1993</date>
<publisher>Chicago University Press,</publisher>
<location>Chicago.</location>
<contexts>
<context position="3521" citStr="Levin (1993)" startWordPosition="535" endWordPosition="536">ies and head lemmas in each pattern. Finally, sets of scFs are gathered for verbs and putative lexical entries are constructed. 2.2 Hypothesis Selection The method for hypothesis selection exploits the knowledge that semantically similar verbs are frequently similar also in terms of subcategorization. The motion verbs fly and move, for example, take similar SCF distributions, which differ essentially from the ones taken e.g. by communication verbs tell and say. Although no perfect correspondence exists between the semantic and syntactic properties of verbs, useful generalizations can be made: Levin (1993) has demonstrated that verb senses can be divided into semantic classes distinctive in terms of subcategorization. Korhonen (2000) shows that verb forms can also be divided into such classes, according to their predominant (i.e. the most frequent) sense. For instance, the verb form specific SCF distributions for the highly polysemic verbs fly and move correlate quite well because the predominant senses of these verbs (according to the WordNet (Miller, 1990) frequency data2) are similar. They both belong to the Levin &amp;quot;Motion verbs&amp;quot;. Good correlation is observed because the distribution of verb </context>
<context position="8477" citStr="Levin (1993)" startWordPosition="1330" endWordPosition="1331">periment. Rather, a method was devised for determining the specificity of the Levin class(es) required for adequate distinctiveness in terms of subcategorization. The method proceeds in two steps, by examining the: Step 1: Syntactic similarity between Levin classes. In this, we use Dorr&apos;s (1997) source of LDOCE grammatical codes (Procter, 1978) for 5In the work reported in this paper, we concentrate on Levin classes only, leaving the construction of novel verb classes for future work. Levin classes°. Dorr constructed this source by extracting basic syntactic patterns from all the sentences in Levin (1993) and mapping these onto LDOCE codes. To determine syntactic similarity between a set of Levin (sub)classes, we consider the degree of intersection between the LDOCE codes for these classes. Step 2: Subcategorization similarity between verbs in Levin classes. We choose a few individual verbs whose predominant sense belongs to the Levin (sub)classes under investigation and examine (1) the intersection of SCFS between the verbs, (2) the dissimilarity of SCF distributions for these verbs according to Kullback-Leibler distance (KL) and (3) the similarity in ranking of SCFS in the distributions acco</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>B. Levin. 1993. English Verb Classes and Alternations. Chicago University Press, Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Manning</author>
<author>H Schiitze</author>
</authors>
<date>1999</date>
<booktitle>Foundations of Statistical Natural Language Processing.</booktitle>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<marker>Manning, Schiitze, 1999</marker>
<rawString>C. Manning and H. Schiitze. 1999. Foundations of Statistical Natural Language Processing. MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Manning</author>
</authors>
<title>Automatic acquisition of a large subcategorization dictionary from corpora.</title>
<date>1993</date>
<booktitle>In 31st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>235--242</pages>
<marker>Manning, 1993</marker>
<rawString>C. Manning. 1993. Automatic acquisition of a large subcategorization dictionary from corpora. In 31st Annual Meeting of the Association for Computational Linguistics, pages 235-242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
</authors>
<title>Wordnet: An on-line lexical database.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<pages>3--4</pages>
<contexts>
<context position="3982" citStr="Miller, 1990" startWordPosition="607" endWordPosition="608"> say. Although no perfect correspondence exists between the semantic and syntactic properties of verbs, useful generalizations can be made: Levin (1993) has demonstrated that verb senses can be divided into semantic classes distinctive in terms of subcategorization. Korhonen (2000) shows that verb forms can also be divided into such classes, according to their predominant (i.e. the most frequent) sense. For instance, the verb form specific SCF distributions for the highly polysemic verbs fly and move correlate quite well because the predominant senses of these verbs (according to the WordNet (Miller, 1990) frequency data2) are similar. They both belong to the Levin &amp;quot;Motion verbs&amp;quot;. Good correlation is observed because the distribution of verb senses tends to be zipfian: the predominant sense typically covers most of the total frequency mass (Preiss et al., 2111 this paper, we refer to the WordNet version 1.6. 2002). The method for hypothesis selection involves first (manually) identifying, for a single verb, the broad Levin class3 corresponding to the predominant sense of this verb in WordNet. A SCF distribution is then acquired for the verb from corpus data, using the hypothesis generator descr</context>
</contexts>
<marker>Miller, 1990</marker>
<rawString>G. A. Miller. 1990. Wordnet: An on-line lexical database. International Journal of Lexicography, 3(4):235-312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Preiss</author>
<author>A Korhonen</author>
<author>E J Briscoe</author>
</authors>
<title>Subcategorization acquisition as an evaluation method for wsd.</title>
<date>2002</date>
<booktitle>In Language Resources and Evaluation Conference.</booktitle>
<note>To appear.</note>
<marker>Preiss, Korhonen, Briscoe, 2002</marker>
<rawString>J. Preiss, A. Korhonen, and E. J. Briscoe. 2002. Subcategorization acquisition as an evaluation method for wsd. In Language Resources and Evaluation Conference. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Procter</author>
</authors>
<title>Longman Dictionary of Contemporary English.</title>
<date>1978</date>
<publisher>Longman,</publisher>
<location>England.</location>
<contexts>
<context position="8211" citStr="Procter, 1978" startWordPosition="1288" endWordPosition="1289">t to minimise the cost involved in constructing back-off estimates, we do not adopt the 191 Levin classes as they stand, although this would allow maximal accuracy. We also do not adopt broad Levin classes as they stand, as some proved inaccurate in the preliminary experiment. Rather, a method was devised for determining the specificity of the Levin class(es) required for adequate distinctiveness in terms of subcategorization. The method proceeds in two steps, by examining the: Step 1: Syntactic similarity between Levin classes. In this, we use Dorr&apos;s (1997) source of LDOCE grammatical codes (Procter, 1978) for 5In the work reported in this paper, we concentrate on Levin classes only, leaving the construction of novel verb classes for future work. Levin classes°. Dorr constructed this source by extracting basic syntactic patterns from all the sentences in Levin (1993) and mapping these onto LDOCE codes. To determine syntactic similarity between a set of Levin (sub)classes, we consider the degree of intersection between the LDOCE codes for these classes. Step 2: Subcategorization similarity between verbs in Levin classes. We choose a few individual verbs whose predominant sense belongs to the Lev</context>
</contexts>
<marker>Procter, 1978</marker>
<rawString>P. Procter. 1978. Longman Dictionary of Contemporary English. Longman, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Sarkar</author>
<author>D Zeman</author>
</authors>
<title>Automatic extraction of subcategorization frames for czech.</title>
<date>2000</date>
<booktitle>In 19th International Conference on Computational Linguistics,</booktitle>
<pages>691--697</pages>
<marker>Sarkar, Zeman, 2000</marker>
<rawString>A. Sarkar and D. Zeman. 2000. Automatic extraction of subcategorization frames for czech. In 19th International Conference on Computational Linguistics, pages 691-697.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ushioda</author>
<author>D Evans</author>
<author>T Gibson</author>
<author>A Waibel</author>
</authors>
<title>The automatic acquisition of frequencies of verb subcategorization frames from tagged corpora.</title>
<date>1993</date>
<booktitle>SIGLEX ACL Workshop on the Acquisition of Lexical Knowledge from Text,</booktitle>
<pages>95--106</pages>
<editor>In B. Boguraev and J. Pustejovsky, editors,</editor>
<location>Columbus, Ohio.</location>
<marker>Ushioda, Evans, Gibson, Waibel, 1993</marker>
<rawString>A. Ushioda, D. Evans, T. Gibson, and A. Waibel. 1993. The automatic acquisition of frequencies of verb subcategorization frames from tagged corpora. In B. Boguraev and J. Pustejovsky, editors, SIGLEX ACL Workshop on the Acquisition of Lexical Knowledge from Text, pages 95-106. Columbus, Ohio.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>