<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.027070">
<title confidence="0.9150415">
• Using decision trees to select
the grammatical relation of a noun phrase
</title>
<author confidence="0.95785">
Simon CORSTON-OLIVER
</author>
<affiliation confidence="0.935312">
Microsoft Research
</affiliation>
<address confidence="0.865793">
One Microsoft Way
Redmond WA 98052, USA
</address>
<email confidence="0.991482">
simonco@microsoft.com
</email>
<sectionHeader confidence="0.997259" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999800076923077">
We present a machine-learning approach to
modeling the distribution of noun phrases
(NPs) within clauses with respect to a fine-
grained taxonomy of grammatical relations. We
demonstrate that a cluster of superficial
linguistic features can function as a proxy for
more abstract discourse features that are not
observable using state-of-the-art natural
language processing. The models constructed
for actual texts can be used to select among
alternative linguistic expressions of the same
propositional content when generating
discourse.
</bodyText>
<sectionHeader confidence="0.978058" genericHeader="keywords">
I. Introduction
</sectionHeader>
<bodyText confidence="0.999924808510639">
Natural language generation involves a number of
processes ranging from planning the content to be
expressed through making encoding decisions
involving syntax, the lexicon and morphology. The
present study concerns decisions made about the form
and distribution of each &amp;quot;mention&amp;quot; of a discourse
entity: should reference be made with a lexical NP, a
pronominal NP or a zero anaphor (i.e. an elided
mention)? Should a given mention be expressed as
the subject of its clause or in some other grammatical
relation?
If all works well, a natural language generation
system may end up proposing a number of possible
well-formed expressions of the same propositional
content. Although these possible formulations would
all be judged to be valid sentences of the target
language, it is not the case that they are all equally
likely to occur.
Research in the area of Preferred Argument
Structure (Corston 1996, Du Bois 1987) has
established that in discourse in many languages,
including English, NPs are distributed across
grammatical relations in statistically significant ways.
For example, transitive clauses tend not to contain
lexical NPs in both subject and object positions and
subjects of transitives tend not to be lexical NPs nor
to be discourse-new.
Unfortunately, the models used in PAS have
involved only simple chi-squared tests to identify
statistically significant patterns in the distribution of
NPs with respect to pairs of features (e.g. part of
speech and grammatical relation). A further problem
from the point of view of computational discourse
analysis is that many of the features used in empirical
studies are not observable in texts using state-of-the
art natural language processing. Such non-observable
features include animacy, the information status of a
referent, and the identification of the gender of a
referent based on world knowledge.
hi the present study, we treat the task of
determining the appropriate distribution of mentions
in text as a machine learning classification problem:
what is the probability that a mention will have a
certain grammatical relation given a rich set of
linguistic features? In particular, how accurately can
we select appropriate grammatical relations using
only superficial linguistic features?
</bodyText>
<sectionHeader confidence="0.997545" genericHeader="introduction">
2. Data
</sectionHeader>
<bodyText confidence="0.999977166666667">
A total of 5,252 mentions were annotated from the
Encarta electronic encyclopedia and 4,937 mentions
from the Wall Street Journal (WSJ). Sentences were
parsed using the Microsoft English Grammar
(Heidom 1999) to extract mentions and linguistic
features. These analyses were then hand-corrected to
eliminate noise in the training data caused by
inaccurate parses, allowing us to determine the upper
bound on accuracy for the classification task if the
computational analysis were perfect. Zero anaphors
were annotated only when they occurred as subjects
of coordinated clauses. They have been excluded
</bodyText>
<page confidence="0.982341">
66
</page>
<bodyText confidence="0.9962645">
from the present study since they are invariably
discourse-given subjects.
</bodyText>
<sectionHeader confidence="0.997323" genericHeader="method">
3. Features
</sectionHeader>
<bodyText confidence="0.9980219375">
Nineteen linguistic features were annotated, along
with information about the referent of each mention.
On the basis of the reference information we
extracted the feature [InformationStatus],
distinguishing &amp;quot;discourse-new&amp;quot; versus &amp;quot;discourse-
old&amp;quot;. All mentions without a prior coreferential
mention in the text were classified as discourse-new,
even if they would not traditionally be considered
referential. [InformationStatus] is not directly
observable since it requires the analyst to make
decisions about the referent of a mention.
In addition to the feature [InformationStatus], the
following eighteen observable features were
annotated. These are all features that we can
reasonably expect syntactic parsers to extract with
sufficient accuracy today or in the near future.
</bodyText>
<listItem confidence="0.99109835">
• [ClausalStatus]: Does the mention occur in a
main clause (&amp;quot;M&amp;quot;), complement clause (&amp;quot;C&amp;quot;),
or subordinate clause (&amp;quot;S&amp;quot;)?
• [Coordinated] The mention is coordinated
with at least one sibling.
• [Definite] The mention is marked with the
definite article or a demonstrative pronoun.
[Fem] The mention is unambiguously
feminine.
• [GrRel] The grammatical relation of the
mention (see below, this section).
• [HasPossessive] Modified by a possessive
pronoun or a possessive NP with the clitic &apos;s
or s
• [HasPP] Contains a postmodifying pre-
positional phrase.
• [HasRe1C1] Contains a postmodifying relative
clause.
• [InQuotes] The mention occurs in quoted
material.
• [Lex] The specific inflected form of a
pronoun, e.g. he, him.
• [Masc] The mention is unambiguously
masculine.
• [NounClass] We distinguish common nouns
versus proper names. Within proper names,
we distinguish the name of a place (&amp;quot;Geo&amp;quot;)
versus other proper names (&amp;quot;ProperName&amp;quot;).
• [Plural] The head of the mention is
morphologically marked as plural.
• [POS] The part of speech of the head of the
mention.
• [Prep] The governing preposition, if any.
• [Re1C1] The mention is a child of a relative
clause.
• [TopLevel] The mention is not embedded
within another mention.
• [Words] The total number of words in the
mention, discretized to the following values:
{0, 1, 2, 3, 4, 5, 6to10, 1 lto15, abovel5).
</listItem>
<bodyText confidence="0.98558537037037">
Gender ([Fern], [Masc]) was only annotated for
common nouns whose default word sense is gendered
(e.g. &amp;quot;mother&amp;quot;, &amp;quot;father&amp;quot;), for common nouns with
specific morphology (e.g. with the —ess suffix) and
for gender-marked proper names (e.g. &amp;quot;John&amp;quot;,
&amp;quot;Mary&amp;quot;). Gender was not marked for pronouns, to
avoid difficult encoding decisions such as the use of
generic &amp;quot;he&amp;quot;. Gender was also not marked for cases
that would require world knowledge.
The feature [GrRel] was given a much finer-
grained analysis than is usual in computational
linguistics. Studies in PAS have demonstrated the
need to distinguish finer-grained categories than the
traditional grammatical relations of English grammar
(&amp;quot;subject&amp;quot;, &amp;quot;object&amp;quot; etc) in order to account for
distributional phenomena in discourse. For example,
subjects of intransitive verbs pattern with the direct
objects of transitive verbs as being the preferred locus
for introducing new mentions. Subjects of transitives,
however, are strongly dispreferred slots for the
expression of new information. The use of fine-
grained grammatical relations enables us to make
rather specific claims about the distribution of
mentions. The taxonomy of fine-grained grammatical
relations is given below in Figure 1.
&apos;The feature [Lex] was sufficient for the decision tree tools
to learn idiosyncratic uses of gendered pronouns.
</bodyText>
<page confidence="0.998722">
67
</page>
<figureCaption confidence="0.998264">
Figure 1 The taxonomy of grammatical relations
</figureCaption>
<figure confidence="0.999715826086957">
Subject of
intransitive
Preposed PP (Pre)
Grammatical
Relation
PP complement of
verb (PP)
PP complement of
NP (PP.)
PP complement of
adjective (PPA)
Object of transitive
(Or)
Predicate nominal
(PN)
Subject of
intransitive
(non-copula) (S,)
Subject
Subject of
transitive (ar)
Subject of copula
(Se)
</figure>
<sectionHeader confidence="0.839659" genericHeader="method">
4. Decision trees
</sectionHeader>
<bodyText confidence="0.975450857142857">
For a set of annotated examples, we used decision-
tree tools to construct the conditional probability of a
specific grammatical relation, given other features in
the domain.2 The decision trees are constructed using
a Bayesian learning approach that identifies tree
structures with high posterior probability (Chickering
et al. 1997). In particular, a candidate tree structure
(S) is evaluated against data (D) using Bayes&apos; rule as
follows:
p(SID) = constant p(DIS) p(S)
For simplicity, we specify a prior distribution
over tree structures using a single parameter kappa
(k). Assuming that N(S) probabilities are needed to
parameterize a tree with structure S, we use:
</bodyText>
<equation confidence="0.991749">
p(S) = c • kN(s)
</equation>
<bodyText confidence="0.991369166666667">
2 Comparison experiments were also done with Support
Vector Machines (Platt 2000, Vapniic 1998) using a
where 0 &lt;k 1, and c is a constant such that p(S)
sums to one. Note that smaller values of kappa cause
simpler structures to be favored. As kappa grows
closer to one (k = 1 corresponds to a uniform prior
over all possible tree structures), the learned decision
trees become more elaborate. Decision trees were
built for Ice {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9,
0.95, 0.99, 0.999}.
Having selected a decision tree, we use the
posterior means of the parameters to specify a
probability distribution over the grammatical
relations. To avoid overfifting, nodes containing
fewer than fifty examples were not split during the
learning process. In building decision trees, 70% of
the data was used for training and 30% for held-out
evaluation.
The decision trees constructed can be rather complex,
making them difficult to present visually. Figure 2
gives a simpler decision tree that predicts the
grammatical relation of a mention for Encarta at
variety of kernel functions. The results obtained were
indistinguishable from those reported here.
</bodyText>
<page confidence="0.990803">
68
</page>
<figure confidence="0.988963222222222">
I Prep = 1
No
No
POS = zerol
Yes&apos; No
I Words &gt; 151
Yes No
NounClass= I
Yes No
</figure>
<equation confidence="0.921015027777778">
p(oth)=0.03
P(08w3.18
P(Poss).:).0)
p(sc)=0.13
p(st)=0.36
p(s040.27
p(oth)=0.11
p(no)=O.03
p(et)=0.43
p(ppv)=0.02
P(sO).0.09
p(s)=0.11
p(st)=0.18
p(oth)g).01
p(na)=0.04
P(or)M:1-42
P(Pn).0.28
P(5e0)w9.02
P(Ore)o3.01
P(so)=0.04
p(s1)=0.07
p(st)=0.08
p(oth)w102
p(ns)=0.08
9(004).43
P(PPa):).04
p(pp)o0.1 5
p(se)O.13
p(si)w).08
P(58=0.08
p(oth).3.04
p(na)w3.02
p(ot)=0.33
P(On).09
p(ppv)w0.01
p(Sc)=0.13
p(sI)-0
p(arn)=0.18
p(oth).0.08
P(na)wD.28
p(o1)=0.27
p(pn)=0.11
P(PPn)=988
p(ppv).:).03
p(pre)=0.02
IXso)=0.04
p(s)=O.09
p(s8=0.09
No
p(oth)w3.02
p(et)=O.48
pipnW).08
ta(opa).0.02
p(pw).:).02
olse&gt;=0.07
p(sI)=0.15
p(s1):).09
P(olt)3.03
000=0.07
p(et)O.48
P(PosS).5.10
5(PPa)=0.09
P19118=0.03
P(s8=0.09
p(sI)0O.03
p(o81):1.17
IS(ns)=029
p(c4):1-29
p(pn).03
P(PPe)=0.05
P(Se).-103
P(st)=0.08
</equation>
<figure confidence="0.995679853333333">
No
I Prep = of I
Yes No
I ReICI
Yes&apos; No
I PiPP4&amp;quot;6 p1995)0.01
p(pPn)=0.58
5(995)00.38
p(pre)=0.01
Yes
Yes
IPOS Datel
Yes-- No
0(nP0.020 c4pou)=0.88
p(pre)=9.59 p(pre)=0.18
P&amp;G!
P(Oth)=0.08
p(na).3.11
p(05)=0.17
p(pn)=0.17
P(Oo8s)=0.08
P(XIX.)=9.11
Yes
p(oth)=0.03
p(na)=0.33
p(ot)=0.25
P(Ph)=3.03
p(poss)w:/.27
rt(1:44).0.03
P(P0h)).0i
p(ppv)=0.01
p(sm)).0i
0(.0).102
I Worcll
Yes
P(Se)=0.11
n(Si)=0.30
p(sm)=0.47
Yes
Plural
I Coordinated I
__Yes-- No
NI-c7-.InClas Proper-Geo I
Yes&apos;-&apos;No
Yes
ILex = where I
Yes-- No
p(oth)=027 1 goth--;y3.02
p(na0.01
p(ot).0.11
P(PoSs).102
p(s51).013
P(8x)=0.02
No p(s1)=0.37
p(st).37
p(ppa)w3.01
98,480=0-89
P(DPx).:. 38
p(pre)=3.01
0
p(s()=0.13
p(st)00.20
I Lex = he I
YesNo
I Lex u she
Yes No
P(14)=021
p10151=0.03
P(na)03.02
p(ot)=0.05
p(pn)).01
p(pose)=0.83
1:(0Pa)a3.01
p(s)00.02
p(38.0.01
</figure>
<figureCaption confidence="0.999724">
Figure 2 Decision tree for Encarta, at k=0.7
</figureCaption>
<bodyText confidence="0.999983130434783">
k=0.7. The tree was constructed using a subset of the
morphological and syntactic features: [Coordinated],
[HasPP], [Lex], [NounClass], [Plural], [POS], [Prep],
[Re1C1], [TopLevel], [Words]. Grammatical relations
with only a residual probability are omitted for the
sake of clarity. The top-ranked grammatical relation
at each leaf node appears in bold type. Selecting the
top-ranked grammatical relation at each node results
in a correct decision 58.82% of the time in the held-
out test data. By way of comparison, the best decision
tree for Encarta computed using all morphological
and syntactic features yields 66.05% accuracy at k =
0.999.
The distributional facts about the pronoun &amp;quot;he&amp;quot;
represented in Figure 2 illustrate the utility of the
fme-grained taxonomy of grammatical relations. The
pronoun &amp;quot;he&amp;quot; in embedded NPs ([Prep]
[TopLevel] = No) and when not in a relative clause
([Reid] = No) favors ST and SI. Other grammatical
relations have only residual probabilities. The use of
the traditional notion of subject would fail to capture
the fact that, in this syntactic context, the pronoun
&amp;quot;he&amp;quot; tends not to occur as Sc, the subject of a copula.
</bodyText>
<sectionHeader confidence="0.948043" genericHeader="method">
5. Evaluating decision trees
</sectionHeader>
<bodyText confidence="0.9997594">
Decision trees were constructed and evaluated for
each corpus. We were particularly interested in the
accuracy of models built using only observable
features. If accurate modeling were to require more
abstract discourse features such as
[InformationStatus], a feature that is not directly
observable, then a machine-learning approach to
modeling the distribution of mentions would not be
computationally feasible. Also of interest was the
generality of the models.
</bodyText>
<subsectionHeader confidence="0.998916">
5.1 Using Observable Features Only
</subsectionHeader>
<bodyText confidence="0.999992777777778">
Decision trees were built for Encarta and the Wall
Street Journal using all features except the non-
observable discourse feature [InformationStatus]. The
best accuracy when evaluated against held-out test
data and selecting the top-ranked grammatical
relation at each leaf node was 66.05% for Encarta at
k-41.999 and 65.18% for Wall Street Journal at
IM).99. Previous studies in Preferred Argument
Structure (Corston 1996, Du Bois 1987) have
</bodyText>
<page confidence="0.99974">
69
</page>
<tableCaption confidence="0.999588">
Table 1 Accuracy using only morphological and syntactic features
</tableCaption>
<table confidence="0.9966868">
Grammatical relations in Accuracy in held-out test data
training data (decision tree accuracy in parentheses)
Corpus Top-ranked Top two Using top-ranked Using top two
Encarta PPN PPN, PP v 20.88% (66.05%) 41.37% (81.92%)
WSJ OT Of, PPN 19.91% (66.16%) 35.56% (80.70%)
</table>
<bodyText confidence="0.999079754098361">
established pairings of fine-grained grammatical
relations with respect to abstract discourse factors.
New mentions in discourse, for example, tend to be
introduced as the subjects of intransitive verbs or as
direct objects, and are extremely unlikely to occur as
the subjects of transitive verbs. Some languages even
give the same morphological and syntactic treatment
to subjects of intransitives and direct objects, marking
them (so called &amp;quot;absolutive&amp;quot; case marking) in
opposition to subjects of transitives (so called
&amp;quot;ergative&amp;quot; marking). Human referents, on the other
hand, tend to occur as the subjects of transitive verbs
and as the subjects of intransitive verbs, rather than as
objects. Such discourse tendencies perhaps motivate
the use of one set of pronouns (the so called
&amp;quot;nominative&amp;quot; pronouns {&amp;quot;he&amp;quot;, &amp;quot;she&amp;quot;, &amp;quot;we&amp;quot;, &amp;quot;1&amp;quot;,
&amp;quot;they&amp;quot;}) in a language like English for subjects and a
different set of pronouns for objects (the so called
&amp;quot;accusative&amp;quot; set {&amp;quot;him&amp;quot;, &amp;quot;her&amp;quot;, &amp;quot;us&amp;quot;, &amp;quot;me&amp;quot;, &amp;quot;them&amp;quot;}).
Thus, we can see that distributional facts about
mentions in discourse sometimes cross-cut the
morphological and syntactic encoding strategies of a
language. With a fine-grained set of grammatical
relations, we can allow the decision trees to discover
such groupings of relations, rather than attempting to
specify the groupings in advance.
We evaluated the accuracy of the decision trees
by counting as a correct decision a grammatical
relation that matched the top-ranked grammatical
relation for a leaf node or the second ranked
grammatical relation for that leaf node. With this
evaluation criterion, the accuracy for Encarta is
81.92% at k=0.999 and for Wall Street Journal,
80.70% at
It is clearly naive to assume a baseline for
comparison in which all grammatical relations have
an equal probability of occurrence, i.e. 1/12 or 0.083.
Rather, in Table 1 we compare the accuracy to that
obtained by predicting the most frequent grammatical
relations observed in the training data. The decision
trees perform substantially above this baseline. The
top two grammatical relations in the two corpora do
not form a natural class. In the Wall Street Journal
texts, for example, the top two grammatical relations
are OT (object of transitive verb) and PPN
(prepositional phrase complement of a NP). It is
difficult to see how mentions in these two
grammatical relations might be related. Objects of
transitive verbs, for example, are typically entities
affected by the action of the verb. Prepositional
phrase complements of NPs, however, are
prototypically used to express attributes of the NP,
e.g. &amp;quot;the man with the red hat&amp;quot;. The grammatical
relations paired by taking the top two predictions at
each leaf node in the decision trees constructed for
the Wall Street Journal and Encarta, however,
frequently correspond to classes that have been
previously observed in the literature on Preferred
Argument Structure. The groupings {0T, SI}, Pr,
Sc) and {S1, ST}, for example, occur on multiple leaf
nodes in the decision trees for both corpora.
</bodyText>
<subsectionHeader confidence="0.999761">
5.2 Using All Features
</subsectionHeader>
<bodyText confidence="0.999916666666667">
Decision trees were built for Encarta and the Wall
Street Journal using all features including the
discourse feature [InformationStatus]. As it turned
out, the feature [InformationStatus] was not selected
during the automatic construction of the decision tree
for the Wall Street Journal. The performance of the
</bodyText>
<page confidence="0.995116">
70
</page>
<bodyText confidence="0.999927090909091">
decision trees on held-out test data from the Wall
Street Journal therefore remained the same as that
given in section 5.1. For Encarta, the addition of
[InformationStatus] yielded only a modest
improvement in accuracy. Selecting the top-ranked
grammatical relation rose from 66.05% at k=0.999 to
67.32% at k = 0.999. Applying a paired t-test, this is
statistically significant at the 0.01 level. Selecting the
top two grammatical relations caused accuracy to rise
from 81.92% at k=0.999 to 82.23% at k=0.999, not a
statistically significant improvement.
The fact that the discourse feature
[InformationStatus] does not make a marked impact
on accuracy is not surprising. The information status
of an NP is an important factor in determining
elements of form, such as the decision to use a
pronoun versus a lexical NP, or the degree of
elaboration (e.g. by means of adjectives, post-
modifying PPs and relative clauses). Those elements
of form can be viewed as proxies for the feature
[InformationStatus]. Pronouns and definite NPs, for
example, typically refer to given entities, and
therefore are compatible with the grammatical
relation ST. Similarly, long indefinite lexical NPs are
likely to be new mentions.
In a separate set of experiments conducted on the
same data, we built decision trees to predict the
information status of the referent of a noun phrase
using the other linguistic features (grammatical
relation, clausal status, definiteness and so on.) Zero
anaphors were excluded, yielding 4,996 noun phrases
for Encarta and 4,758 noun phrases for the Wall
Street Journal. The accuracy of the decision trees was
80.45% for Encarta and 78.36% for the Wall Street
Journal. To exclude the strong associations between
personal pronouns and information status, we also
built decision trees for only the lexical noun phrases
in the two corpora, a total of 4,542 noun phrases for
Encarta and 4,153 noun phrases for the Wall Street
Journal. The accuracy of the decision trees was
78.14% for Encarta and 77.45% for the Wall Street
Journal. The feature [InformationStatus] can thus be
seen to be highly inferrable given the other features
used.
</bodyText>
<subsectionHeader confidence="0.941876">
5.3 Domain-specificity of the Decision Trees
</subsectionHeader>
<bodyText confidence="0.999973290322581">
The decision trees built for the Encarta and Wall
Street Journal corpora differ considerably, as is to be
expected for such distinct genres. To measure the
specificity of the decision trees, we built models
using all the data for one corpus and evaluated on all
the data in the other corpus, using all features except
[InformationStatus]. Table 2 gives the baseline
figures for this cross-domain evaluation, selecting the
most frequent grammatical relations in the training
data. The peak accuracy from the decision trees is
given in parentheses for comparison. The decision
trees perform well above the baseline.
Table 3 provides a comparison of the accuracy of
decision trees applied across domains compared to
those constructed and evaluated within a given
domain. The extremely specialized sublanguage of
Encarta does not generalize well to the Wall Street
Journal. In particular, when selecting the top-ranked
grammatical relation, the most severe evaluation of
the accuracy of the decision trees, training on Encarta
and evaluating on the Wall Street Journal results in a
drop in accuracy of 7.54% compared to the Wall
Street Journal within-corpus model. By way of
contrast, decision trees built from the Wall Street
Journal data do generalize well to Encarta, even
yielding a modest 0.41% improvement in accuracy
over the model built for Encarta. Since the Encarta
data contains more mentions (5,252 mentions) than
the Wall Street Journal data (4,937 mentions), this
effect is not simply due to differences in the size of
the training set.
</bodyText>
<page confidence="0.999134">
71
</page>
<tableCaption confidence="0.996951">
Table 2 Cross-domain evaluation of the decision trees
</tableCaption>
<table confidence="0.99982825">
Grammatical relations in Accuracy in held-out test data
training data (decision tree accuracy in parentheses)
Train- Top-ranked Top two Using top-ranked Using top two
Test
WSJ- Or OT, PPN 15.90% (66.32%) 36.58% (79.51%)
Encarta
Encarta- PPN PPN, PP, 15.98% (61.17%) 31.90% (77.64%)
WSJ
</table>
<tableCaption confidence="0.9913805">
Table 3 Comparison of cross-domain accuracy to
within-domain accuracy
</tableCaption>
<table confidence="0.999794285714286">
Top-ranked
Train on Encarta, evaluate on WSJ 61.17%
Train on WSJ, evaluate on WSJ 66.16%
Relative difference in accuracy -7.54%
Train on WSJ, evaluate on Encarta 66.32%
Train on Encarta, evaluate on Encarta 66.05%
Relative difference in accuracy +0.41%
Top two
Train on Encarta, evaluate on WSJ 77.64%
Train on WSJ, evaluate on WSJ 80.70%
Relative difference in accuracy -3.74%
Train on WSJ, evaluate on Encarta 79.51%
Train on Encarta, evaluate on Encarta 81.92%
Relative difference in accuracy -2.94%
</table>
<subsectionHeader confidence="0.996084">
5.4 Combining the Data
</subsectionHeader>
<bodyText confidence="0.99997036">
Combining the Wall Street Journal and Encarta data
into one dataset and using 70% of the data for
training and 30% for testing yielded mixed results.
Selecting the top-ranked grammatical relation for the
combined data yielded 66.01% at k=0.99, compared
to the Encarta-specific accuracy of 66.05% and the
Wall Street Journal-specific peak accuracy of
66.16%. Selecting the top two grammatical relations,
the peak accuracy for the combined data was 81.39%
at k=0.99, a result approximately midway between the
corpus-specific results obtained in section 5.1,
namely 81.92% for Encarta and 80.70% for Wall
Street Journal.
The Wall Street Journal corpus contains a diverse
range of articles, including op-ed pieces, mundane
financial reporting, and world news. The addition of
the relatively homogeneous Encarta articles appears
to result in models that are even more robust than
those constructed solely on the basis of the Wall
Street Journal data. The addition of the heterogeneous
Wall Street Journal articles, however, dilutes the
focus of the model constructed for Encarta. This
perhaps explains the fact that the peak accuracy of the
combined model lies above that for the Wall Street
Journal but below that of Encarta.
</bodyText>
<sectionHeader confidence="0.999023" genericHeader="conclusions">
6. Conclusion
</sectionHeader>
<bodyText confidence="0.999989166666667">
Natural language generation is typically done under
one of two scenarios. In the first scenario, language is
generated ex nihilo: a planning component formulates
propositions on the basis of a database query, a
system event, or some other non-linguistic stimulus.
Under such a scenario, the discourse status of
referents is known, since the planning component has
selected the discourse entities to be expressed. More
abstract discourse features like [InformationStatus]
can therefore be used to guide the linguistic encoding
decisions.
In the second, more typical scenario, natural
language generation involves reformulating existing
text, e.g. for summarization or machine translation. In
this scenario, analysis of the linguistic stimulus will
most likely have resulted in only a partial
understanding of the source text. Coreference
relations (e.g. between a pronoun and its antecedent)
</bodyText>
<page confidence="0.993087">
72
</page>
<bodyText confidence="0.999996378378379">
may not be fully resolved, discourse relations may be
unspecified, and the information status of mentions is
unlikely to have been determined. As was shown in
section 5.2, the accuracy of the decision trees
constructed without the feature [InformationStatus] is
comparable to the accuracy that results from using
this feature, since superficial elements of the
linguistic form of a mention are motivated by the
information status of the mention.
The decision trees that were constructed to model
the distribution of NPs in real texts can be used to
guide the generation of natural language, especially to
guide the selection among alternative grammatical
ways of expressing the same propositional content.
Sentences in which mentions occur in positions that
are unlikely given a set of linguistic features should
be avoided.
One interesting problem remains for future
research: why do writers occasionally place mentions
in statistically unlikely positions? One possibility is
that writers do so for stylistic variation. Another
intriguing possibility is that statistically unusual
occurrences reflect pragmatic markedness, i.e. that
writers place NPs in certain positions in order to
signal discourse information. Fox (1987), for
example, demonstrates that lexical NPs may be used
for previously mentioned discourse entities where a
pronoun might be expected instead if there is an
episode boundary in the discourse. For example, a
protagonist in a novel may be reintroduced by name
at the beginning of a chapter. In future research we
propose to examine the mentions that occur in places
not predicted by the models. It may be that this
approach to modeling the distribution of mentions,
essentially a machine-learning approach that seeks to
mine an abstract property of texts, will provide useful
insights into issues of discourse structure.
</bodyText>
<sectionHeader confidence="0.999193" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999902681818182">
Chickering, D. M., D. Heckerman, and C. Meek, 1997, &amp;quot;A
Bayesian approach to learning Bayesian networks with
local structure,&amp;quot; In Geiger, D. and P. Punadlik Shenoy
(eds.), Uncertainty in Artificial Intelligence: Proceedings
of the Thirteenth Conference, 80-89.
Corston, S. H., 1996, Ergativity in Roviana, Solomon
Islands, Pacific Linguistics, Series B-113, Australia
National University Press: Canberra.
Du Bois, J. W., 1987, &amp;quot;The discourse basis of ergativity,&amp;quot;
Language 63:805-855.
Fox, B.A., 1987, Discourse structure and anaphora,
Cambridge Studies in Linguistics 48, Cambridge
University Press, Cambridge.
Heidom, G., 1999, &amp;quot;Intelligent writing assistance,&amp;quot; To
appear in Dale, R., H. Moisl and H. Somers (eds.), A
Handbook of Natural Language Processing Techniques,
Marcel Dekker.
Platt, J., N. Cristianini, J. Shawe-Taylor, 2000, &amp;quot;Large
margin DAGs for multiclass classification,&amp;quot; In Advances
in Neural Information Processing Systems 12, MIT Press.
Vapnilc, V., 1998, Statistical Learning Theory, Wiley-
Interscience, New York.
</reference>
<page confidence="0.999298">
73
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.615628">
<title confidence="0.9536965">Using decision trees to select the grammatical relation of a noun phrase</title>
<author confidence="0.963964">Simon</author>
<affiliation confidence="0.941048">Microsoft</affiliation>
<address confidence="0.8557975">One Microsoft Redmond WA 98052,</address>
<email confidence="0.999731">simonco@microsoft.com</email>
<abstract confidence="0.996180857142857">We present a machine-learning approach to modeling the distribution of noun phrases (NPs) within clauses with respect to a finegrained taxonomy of grammatical relations. We demonstrate that a cluster of superficial linguistic features can function as a proxy for more abstract discourse features that are not observable using state-of-the-art natural language processing. The models constructed for actual texts can be used to select among alternative linguistic expressions of the same propositional content when generating discourse.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D M Chickering</author>
<author>D Heckerman</author>
<author>C Meek</author>
</authors>
<title>A Bayesian approach to learning Bayesian networks with local structure,&amp;quot;</title>
<date>1997</date>
<booktitle>Uncertainty in Artificial Intelligence: Proceedings of the Thirteenth Conference,</booktitle>
<pages>80--89</pages>
<editor>In Geiger, D. and P. Punadlik Shenoy (eds.),</editor>
<contexts>
<context position="7878" citStr="Chickering et al. 1997" startWordPosition="1184" endWordPosition="1187">reposed PP (Pre) Grammatical Relation PP complement of verb (PP) PP complement of NP (PP.) PP complement of adjective (PPA) Object of transitive (Or) Predicate nominal (PN) Subject of intransitive (non-copula) (S,) Subject Subject of transitive (ar) Subject of copula (Se) 4. Decision trees For a set of annotated examples, we used decisiontree tools to construct the conditional probability of a specific grammatical relation, given other features in the domain.2 The decision trees are constructed using a Bayesian learning approach that identifies tree structures with high posterior probability (Chickering et al. 1997). In particular, a candidate tree structure (S) is evaluated against data (D) using Bayes&apos; rule as follows: p(SID) = constant p(DIS) p(S) For simplicity, we specify a prior distribution over tree structures using a single parameter kappa (k). Assuming that N(S) probabilities are needed to parameterize a tree with structure S, we use: p(S) = c • kN(s) 2 Comparison experiments were also done with Support Vector Machines (Platt 2000, Vapniic 1998) using a where 0 &lt;k 1, and c is a constant such that p(S) sums to one. Note that smaller values of kappa cause simpler structures to be favored. As kapp</context>
</contexts>
<marker>Chickering, Heckerman, Meek, 1997</marker>
<rawString>Chickering, D. M., D. Heckerman, and C. Meek, 1997, &amp;quot;A Bayesian approach to learning Bayesian networks with local structure,&amp;quot; In Geiger, D. and P. Punadlik Shenoy (eds.), Uncertainty in Artificial Intelligence: Proceedings of the Thirteenth Conference, 80-89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S H Corston</author>
</authors>
<title>Ergativity in Roviana, Solomon Islands,</title>
<date>1996</date>
<booktitle>Linguistics, Series B-113, Australia National</booktitle>
<publisher>University Press: Canberra.</publisher>
<location>Pacific</location>
<contexts>
<context position="1638" citStr="Corston 1996" startWordPosition="246" endWordPosition="247">scourse entity: should reference be made with a lexical NP, a pronominal NP or a zero anaphor (i.e. an elided mention)? Should a given mention be expressed as the subject of its clause or in some other grammatical relation? If all works well, a natural language generation system may end up proposing a number of possible well-formed expressions of the same propositional content. Although these possible formulations would all be judged to be valid sentences of the target language, it is not the case that they are all equally likely to occur. Research in the area of Preferred Argument Structure (Corston 1996, Du Bois 1987) has established that in discourse in many languages, including English, NPs are distributed across grammatical relations in statistically significant ways. For example, transitive clauses tend not to contain lexical NPs in both subject and object positions and subjects of transitives tend not to be lexical NPs nor to be discourse-new. Unfortunately, the models used in PAS have involved only simple chi-squared tests to identify statistically significant patterns in the distribution of NPs with respect to pairs of features (e.g. part of speech and grammatical relation). A further</context>
<context position="13234" citStr="Corston 1996" startWordPosition="1964" endWordPosition="1965">machine-learning approach to modeling the distribution of mentions would not be computationally feasible. Also of interest was the generality of the models. 5.1 Using Observable Features Only Decision trees were built for Encarta and the Wall Street Journal using all features except the nonobservable discourse feature [InformationStatus]. The best accuracy when evaluated against held-out test data and selecting the top-ranked grammatical relation at each leaf node was 66.05% for Encarta at k-41.999 and 65.18% for Wall Street Journal at IM).99. Previous studies in Preferred Argument Structure (Corston 1996, Du Bois 1987) have 69 Table 1 Accuracy using only morphological and syntactic features Grammatical relations in Accuracy in held-out test data training data (decision tree accuracy in parentheses) Corpus Top-ranked Top two Using top-ranked Using top two Encarta PPN PPN, PP v 20.88% (66.05%) 41.37% (81.92%) WSJ OT Of, PPN 19.91% (66.16%) 35.56% (80.70%) established pairings of fine-grained grammatical relations with respect to abstract discourse factors. New mentions in discourse, for example, tend to be introduced as the subjects of intransitive verbs or as direct objects, and are extremely </context>
</contexts>
<marker>Corston, 1996</marker>
<rawString>Corston, S. H., 1996, Ergativity in Roviana, Solomon Islands, Pacific Linguistics, Series B-113, Australia National University Press: Canberra.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Du Bois</author>
<author>J W</author>
</authors>
<title>The discourse basis of ergativity,&amp;quot;</title>
<date>1987</date>
<journal>Language</journal>
<pages>63--805</pages>
<marker>Bois, W, 1987</marker>
<rawString>Du Bois, J. W., 1987, &amp;quot;The discourse basis of ergativity,&amp;quot; Language 63:805-855.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B A Fox</author>
</authors>
<title>Discourse structure and anaphora, Cambridge Studies in Linguistics 48,</title>
<date>1987</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="25000" citStr="Fox (1987)" startWordPosition="3785" endWordPosition="3786">selection among alternative grammatical ways of expressing the same propositional content. Sentences in which mentions occur in positions that are unlikely given a set of linguistic features should be avoided. One interesting problem remains for future research: why do writers occasionally place mentions in statistically unlikely positions? One possibility is that writers do so for stylistic variation. Another intriguing possibility is that statistically unusual occurrences reflect pragmatic markedness, i.e. that writers place NPs in certain positions in order to signal discourse information. Fox (1987), for example, demonstrates that lexical NPs may be used for previously mentioned discourse entities where a pronoun might be expected instead if there is an episode boundary in the discourse. For example, a protagonist in a novel may be reintroduced by name at the beginning of a chapter. In future research we propose to examine the mentions that occur in places not predicted by the models. It may be that this approach to modeling the distribution of mentions, essentially a machine-learning approach that seeks to mine an abstract property of texts, will provide useful insights into issues of d</context>
</contexts>
<marker>Fox, 1987</marker>
<rawString>Fox, B.A., 1987, Discourse structure and anaphora, Cambridge Studies in Linguistics 48, Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Heidom</author>
</authors>
<title>Intelligent writing assistance,&amp;quot; To appear</title>
<date>1999</date>
<booktitle>A Handbook of Natural Language Processing Techniques,</booktitle>
<editor>in Dale, R., H. Moisl and H. Somers (eds.),</editor>
<publisher>Marcel Dekker.</publisher>
<contexts>
<context position="3211" citStr="Heidom 1999" startWordPosition="482" endWordPosition="483">e. hi the present study, we treat the task of determining the appropriate distribution of mentions in text as a machine learning classification problem: what is the probability that a mention will have a certain grammatical relation given a rich set of linguistic features? In particular, how accurately can we select appropriate grammatical relations using only superficial linguistic features? 2. Data A total of 5,252 mentions were annotated from the Encarta electronic encyclopedia and 4,937 mentions from the Wall Street Journal (WSJ). Sentences were parsed using the Microsoft English Grammar (Heidom 1999) to extract mentions and linguistic features. These analyses were then hand-corrected to eliminate noise in the training data caused by inaccurate parses, allowing us to determine the upper bound on accuracy for the classification task if the computational analysis were perfect. Zero anaphors were annotated only when they occurred as subjects of coordinated clauses. They have been excluded 66 from the present study since they are invariably discourse-given subjects. 3. Features Nineteen linguistic features were annotated, along with information about the referent of each mention. On the basis </context>
</contexts>
<marker>Heidom, 1999</marker>
<rawString>Heidom, G., 1999, &amp;quot;Intelligent writing assistance,&amp;quot; To appear in Dale, R., H. Moisl and H. Somers (eds.), A Handbook of Natural Language Processing Techniques, Marcel Dekker.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Platt</author>
<author>N Cristianini</author>
<author>J Shawe-Taylor</author>
</authors>
<title>Large margin DAGs for multiclass classification,&amp;quot;</title>
<date>2000</date>
<booktitle>In Advances in Neural Information Processing Systems 12,</booktitle>
<publisher>MIT Press.</publisher>
<marker>Platt, Cristianini, Shawe-Taylor, 2000</marker>
<rawString>Platt, J., N. Cristianini, J. Shawe-Taylor, 2000, &amp;quot;Large margin DAGs for multiclass classification,&amp;quot; In Advances in Neural Information Processing Systems 12, MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Vapnilc</author>
</authors>
<date>1998</date>
<booktitle>Statistical Learning Theory,</booktitle>
<location>WileyInterscience, New York.</location>
<marker>Vapnilc, 1998</marker>
<rawString>Vapnilc, V., 1998, Statistical Learning Theory, WileyInterscience, New York.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>