<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000212">
<title confidence="0.391598">
Estimators for Stochastic &amp;quot;Unification-Based&amp;quot; Grammars*
</title>
<author confidence="0.583436">
Mark Johnson Stuart Geman Stephen Canon
</author>
<affiliation confidence="0.6180545">
Cognitive and Linguistic Sciences Applied Mathematics Cognitive and Linguistic Sciences
Brown University Brown University Brown University
</affiliation>
<author confidence="0.996138">
Zhiyi Chi Stefan Riezler
</author>
<affiliation confidence="0.9990125">
Dept. of Statistics Institut fiir Maschinelle Sprachverarbeitung
The University of Chicago Universitat Stuttgart
</affiliation>
<sectionHeader confidence="0.980114" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999532">
Log-linear models provide a statistically sound
framework for Stochastic &amp;quot;Unification-Based&amp;quot;
Grammars (SUBGs) and stochastic versions of
other kinds of grammars. We describe two
computationally-tractable ways of estimating
the parameters of such grammars from a train-
ing corpus of syntactic analyses, and apply
these to estimate a stochastic version of Lexical-
Functional Grammar.
</bodyText>
<sectionHeader confidence="0.998423" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999398347826087">
Probabilistic methods have revolutionized com-
putational linguistics. They can provide a
systematic treatment of preferences in pars-
ing. Given a suitable estimation procedure,
stochastic models can be &amp;quot;tuned&amp;quot; to reflect the
properties of a corpus. On the other hand,
&amp;quot;Unification-Based&amp;quot; Grammars (UBGs) can ex-
press a variety of linguistically-important syn-
tactic and semantic constraints. However, de-
veloping Stochastic &amp;quot;Unification-based&amp;quot; Gram-
mars (SUBGs) has not proved as straight-
forward as might be hoped.
The simple &amp;quot;relative frequency&amp;quot; estimator
for PCFGs yields the maximum likelihood pa-
rameter estimate, which is to say that it
minimizes the Kulback-Liebler divergence be-
tween the training and estimated distributions.
On the other hand, as Abney (1997) points
out, the context-sensitive dependencies that
&amp;quot;unification-based&amp;quot; constraints introduce ren-
der the relative frequency estimator suboptimal:
in general it does not maximize the likelihood
and it is inconsistent.
</bodyText>
<footnote confidence="0.57389875">
* This research was supported by the National Science
Foundation (SBR-9720368), the US Army Research Of-
fice (DAAH04-96-BAA5), and Office of Naval Research
(N00014-97-1-0249).
</footnote>
<bodyText confidence="0.999913733333333">
Abney (1997) proposes a Markov Random
Field or log linear model for SUBGs, and the
models described here are instances of Abney&apos;s
general framework. However, the Monte-Carlo
parameter estimation procedure that Abney
proposes seems to be computationally imprac-
tical for reasonable-sized grammars. Sections 3
and 4 describe two new estimation procedures
which are computationally tractable. Section 5
describes an experiment with a small LFG cor-
pus provided to us by Xerox PARC. The log
linear framework and the estimation procedures
are extremely general, and they apply directly
to stochastic versions of HPSG and other theo-
ries of grammar.
</bodyText>
<sectionHeader confidence="0.982175" genericHeader="method">
2 Features in SUBGs
</sectionHeader>
<bodyText confidence="0.946158217391304">
We follow the statistical literature in using the
term feature to refer to the properties that pa-
rameters are associated with (we use the word
&amp;quot;attribute&amp;quot; to refer to the attributes or features
of a UBG&apos;s feature structure). Let SZ be the
set of all possible grammatical or well-formed
analyses. Each feature f maps a syntactic anal-
ysis to E S/ to a real value f (w). The form of
a syntactic analysis depends on the underlying
linguistic theory. For example, for a PCFG co
would be parse tree, for a LFG co would be a
tuple consisting of (at least) a c-structure, an f-
structure and a mapping from c-structure nodes
to f-structure elements, and for a Chomskyian
transformational grammar co would be a deriva-
tion.
Log-linear models are models in which the
log probability is a linear combination of fea-
ture values (plus a constant). PCFGs, Gibbs
distributions, Maximum-Entropy distributions
and Markov Random Fields are all examples of
log-linear models. A log-linear model associates
each feature f3 with a real-valued parameter 03.
</bodyText>
<page confidence="0.997313">
535
</page>
<bodyText confidence="0.994154">
A log-linear model with m features is one in
which the likelihood P(w) of an analysis w is:
</bodyText>
<equation confidence="0.9836345">
Pe (co = (w)
zo
ze E e fj
u/Egi
</equation>
<bodyText confidence="0.999955018518518">
While the estimators described below make
no assumptions about the range of the f, in
the models considered here the value of each
feature f2 (w) is the number of times a particu-
lar structural arrangement or configuration oc-
curs in the analysis (..v, so fi(co) ranges over the
natural numbers.
For example, the features of a PCFG are
indexed by productions, i.e., the value f(w)
of feature f, is the number of times the
ith production is used in the derivation w.
This set of features induces a tree-structured
dependency graph on the productions which
is characteristic of Markov Branching Pro-
cesses (Pearl, 1988; Frey, 1998). This tree
structure has the important consequence that
simple &amp;quot;relative-frequencies&amp;quot; yield maximum-
likelihood estimates for the 02.
Extending a PCFG model by adding addi-
tional features not associated with productions
will in general add additional dependencies, de-
stroy the tree structure, and substantially com-
plicate maximum likelihood estimation.
This is the situation for a SUBG, even if the
features are production occurences. The uni-
fication constraints create non-local dependen-
cies among the productions and the dependency
graph of a SUBG is usually not a tree. Conse-
quently, maximum likelihood estimation is no
longer a simple matter of computing relative
frequencies. But the resulting estimation proce-
dures (discussed in detail, shortly), albeit more
complicated, have the virtue of applying to es-
sentially arbitrary features—of the production
or non-production type. That is, since estima-
tors capable of finding maximum-likelihood pa-
rameter estimates for production features in a
SUBG will also find maximum-likelihood esti-
mates for non-production features, there is no
motivation for restricting features to be of the
production type.
Linguistically there is no particular reason
for assuming that productions are the best fea-
tures to use in a stochastic language model.
For example, the adjunct attachment ambigu-
ity in (1) results in alternative syntactic struc-
tures which use the same productions the same
number of times in each derivation, so a model
with only production features would necessarily
assign them the same likelihood. Thus models
that use production features alone predict that
there should not be a systematic preference for
one of these analyses over the other, contrary to
standard psycholinguistic results.
</bodyText>
<subsectionHeader confidence="0.5899235">
1.a Bill thought Hillary {v [v left] yesterday]
1.b Bill [vp[vp thought Hillary left] yesterday]
</subsectionHeader>
<bodyText confidence="0.999989315789474">
There are many different ways of choosing
features for a SUBG, and each of these choices
makes an empirical claim about possible distri-
butions of sentences. Specifying the features of
a SUBG is as much an empirical matter as spec-
ifying the grammar itself. For any given UBG
there are a large (usually infinite) number of
SUBGs that can be constructed from it, differ-
ing only in the features that each SUBG uses.
In addition to production features, the
stochastic LFG models evaluated below used
the following kinds of features, guided by the
principles proposed by Hobbs and Bear (1995).
Adjunct and argument features indicate adjunct
and argument attachment respectively, and per-
mit the model to capture a general argument
attachment preference. In addition, there are
specialized adjunct and argument features cor-
responding to each grammatical function used
in LFG (e.g., SUBJ, OBJ, COMP, XCOMP,
ADJUNCT, etc.). There are features indi-
cating both high and low attachment (deter-
mined by the complexity of the phrase being
attached to). Another feature indicates non-
right-branching nonterminal nodes. There is
a feature for non-parallel coordinate structures
(where parallelism is measured in constituent
structure terms). Each f-structure attribute-
atomic value pair which appears in any feature
structure is also used as a feature. We also use
a number of features identifying syntactic struc-
tures that seem particularly important in these
corpora, such as a feature identifying NPs that
are dates (it seems that date interpretations of
NPs are preferred). We would have liked to
have included features concerning specific lex-
ical items (to capture head-to-head dependen-
cies), but we felt that our corpora were so small
</bodyText>
<page confidence="0.990091">
536
</page>
<bodyText confidence="0.980133">
that the associated parameters could not be ac-
curately estimated.
</bodyText>
<sectionHeader confidence="0.7617845" genericHeader="method">
3 A pseudo-likelihood estimator for
log linear models
</sectionHeader>
<bodyText confidence="0.77052575">
Suppose = wi, , wr, is a training cor-
pus of n syntactic analyses. Letting f3(c75) =
f3 POI the log likelihood of the corpus
CZ) and its derivatives are:
</bodyText>
<equation confidence="0.987452666666667">
log Lo(3) = E ejf3(L3) n log Zo (2)
a log Lo (c7i)
= fi((;) — nEo(f3) (3)
</equation>
<bodyText confidence="0.998991547169811">
where E0 (f&apos;) is the expected value of h under
the distribution determined by the parameters
0. The maximum-likelihood estimates are the 0
which maximize log 14 (iv). The chief difficulty
in finding the maximum-likelihood estimates is
calculating E9 (f), which involves summing over
the space of well-formed syntactic structures a
There seems to be no analytic or efficient nu-
merical way of doing this for a realistic SUBG.
Abney (1997) proposes a gradient ascent,
based upon a Monte Carlo, procedure for esti-
mating Eo(f3). The idea is to generate random
samples of feature structures from the distribu-
tion PO(w), where 0 is the current parameter
estimate, and to use these to estimate Eo(f3),
and hence the gradient of the likelihood. Sam-
ples are generated as follows: Given a SUBG,
Abney constructs a covering PCFG based upon
the SUBG and 0, the current estimate of 0. The
derivation trees of the PCFG can be mapped
onto a set containing all of the SUBG&apos;s syn-
tactic analyses. Monte Carlo samples from the
PCFG are comparatively easy to generate, and
sample syntactic analyses that do not map to
well-formed SUBG syntactic structures are then
simply discarded. This generates a stream of
syntactic structures, but not distributed accord-
ing to P(w) (distributed instead according to
the restriction of the PCFG to the SUBG). Ab-
ney proposes using a Metropolis acceptance-
rejection method to adjust the distribution of
this stream of feature structures to achieve de-
tailed balance, which then produces a stream
of feature structures distributed according to
Pei (w).
While this scheme is theoretically sound, it
would appear to be computationally impracti-
cal for realistic SUBGs. Every step of the pro-
posed procedure (corresponding to a single step
of gradient ascent) requires a very large number
of PCFG samples: samples must be found that
correspond to well-formed SUBGs; many such
samples are required to bring the Metropolis al-
gorithm to (near) equilibrium; many samples
are needed at equilibrium to properly estimate
E(f3).
The idea of a gradient ascent of the likelihood
(2) is appealing—a simple calculation reveals
that the likelihood is concave and therefore free
of local maxima. But the gradient (in partic-
ular, E9(f3)) is intractable. This motivates an
alternative strategy involving a data-based esti-
mate of E9(f3):
</bodyText>
<equation confidence="0.9997515">
Egli) = Ei0(E0(f3(w)Iy(w))) (4)
Eoch(w)iy(w)=Yi)(5)
</equation>
<bodyText confidence="0.999644222222222">
where y(w) is the yield belonging to the syn-
tactic analysis w, and yi = y(wi) is the yield
belonging to the i&apos;th sample in the training cor-
pus.
The point is that Eo(f3(w)ly(w) yi) is gen-
erally computable. In fact, if SZ(y) is the set of
well-formed syntactic structures that have yield
y (i.e., the set of possible parses of the string y),
then
</bodyText>
<equation confidence="0.959534">
Ee(fi (w)ly(w) = yi) =
E.,En(y) „rn ekfk ((I)
Hence the cacw:aut efoE. the=1&amp;quot;COninedkfk
itiOn)
</equation>
<bodyText confidence="0.995459545454545">
a l expec-
tations only involves summing over the possible
syntactic analyses or parses 12(yi) of the strings
in the training corpus. While it is possible to
construct UBGs for which the number of pos-
sible parses is unmanageably high, for many
grammars it is quite manageable to enumerate
the set of possible parses and thereby directly
evaluate Eo (f3(w)iy (w ) = yi ) .
Therefore, we propose replacing the gradient,
(3), by
</bodyText>
<equation confidence="0.989476">
.13(63) E Egh(w)iy(w) = yi) (6)
</equation>
<bodyText confidence="0.796332">
and performing a gradient ascent. Of course (6)
is no longer the gradient of the likelihood func-
</bodyText>
<page confidence="0.986534">
537
</page>
<bodyText confidence="0.9946765">
tion, but fortunately it is (exactly) the gradient
of (the log of) another criterion:
</bodyText>
<equation confidence="0.954027">
pLem =H Po(w = wilY(w) = yi) (7)
i=1,...,n
</equation>
<bodyText confidence="0.999958175">
Instead of maximizing the likelihood of the syn-
tactic analyses over the training corpus, we
maximize the conditional likelihood of these
analyses given the observed yields. In our exper-
iments, we have used a conjugate-gradient op-
timization program adapted from the one pre-
sented in Press et al. (1992).
Regardless of the pragmatic (computational)
motivation, one could perhaps argue that the
conditional probabilities Po(wly) are as use-
ful (if not more useful) as the full probabili-
ties Po(w), at least in those cases for which
the ultimate goal is syntactic analysis. Berger
et al. (1996) and Jelinek (1997) make this same
point and arrive at the same estimator, albeit
through a maximum entropy argument.
The problem of estimating parameters for
log-linear models is not new. It is especially dif-
ficult in cases, such as ours, where a large sam-
ple space makes the direct computation of ex-
pectations infeasible. Many applications in spa-
tial statistics, involving Markov random fields
(MRF), are of this nature as well. In his
seminal development of the MRF approach to
spatial statistics, Besag introduced a &amp;quot;pseudo-
likelihood&amp;quot; estimator to address these difficul-
ties (Besag, 1974; Besag, 1975), and in fact our
proposal here is an instance of his method. In
general, the likelihood function is replaced by a
more manageable product of conditional likeli-
hoods (a pseudo-likelihood—hence the designa-
tion PLO, which is then optimized over the pa-
rameter vector, instead of the likelihood itself.
In many cases, as in our case here, this sub-
stitution side steps much of the computational
burden without sacrificing consistency (more on
this shortly).
What are the asymptotics of optimizing a
pseudo-likelihood function? Look first at the
likelihood itself. For large n:
</bodyText>
<equation confidence="0.99695">
—1 log II Pe (wi)
n
z
_1 N- log Pe (wi)
n .
=1,...,n
fPoo (w) log Po (w)dco (8)
</equation>
<bodyText confidence="0.999959285714286">
where 00 is the true (and unknown) parame-
ter vector. Up to a constant, (8) is the nega-
tive of the Kullback-Leibler divergence between
the true and estimated distributions of syntac-
tic analyses. As sample size grows, maximizing
likelihood amounts to minimizing divergence.
As for pseudo-likelihood:
</bodyText>
<equation confidence="0.9968884">
1
—n log . Po (u., = wjIy(w) = yi)
_&gt;1 log Po(co = wziY(w) = Y2)
n
Eco[f 61,,(cvly) log Pe (wly)dw]
</equation>
<bodyText confidence="0.999740424242424">
So that maximizing pseudo-likelihood (at large
samples) amounts to minimizing the average
(over yields) divergence between the true and
estimated conditional distributions of analyses
given yields.
Maximum likelihood estimation is consistent:
under broad conditions the sequence of dis-
tributions Po , associated with the maximum
likelihood estimator for 00 given the samples
con, converges to Poo. Pseudo-likelihood
is also consistent, but in the present implemen-
tation it is consistent for the conditional dis-
tributions Poo (wly(w)) and not necessarily for
the full distribution Poo (see Chi (1998)). It is
not hard to see that pseudo-likelihood will not
always correctly estimate Poo. Suppose there
is a feature L which depends only on yields:
f2(w) = fz(y(w)). (Later we will refer to such
features as pseudo-constant.) In this case, the
derivative of PLo (1.0 with respect to 0i is zero;
PL0(65) contains no information about 0. In
fact, in this case any value of 0i gives the same
conditional distribution Po (w ) y(w)); 0i is irrele-
vant to the problem of choosing good parses.
Despite the assurance of consistency, pseudo-
likelihood estimation is prone to over fitting
when a large number of features is matched
against a modest-sized training corpus. One
particularly troublesome manifestation of over
fitting results from the existence of features
which, relative to the training set, we might
term &amp;quot;pseudo-maximal&amp;quot;: Let us say that a
feature f is pseudo-maximal for a yield y if
</bodyText>
<equation confidence="0.98761">
—I log WM
—1 log PLo()
e•■•
r••■■•
</equation>
<page confidence="0.988955">
538
</page>
<bodyText confidence="0.999891272727273">
Vwf E 1/(y) f(w) &gt; f (co&apos;) where w is any cor-
rect parse of y, i.e., the feature&apos;s value on every
correct parse (.4) of y is greater than or equal
to its value on any other parse of y. Pseudo-
minimal features are defined similarly. It is easy
to see that if h is pseudo-maximal on each sen-
tence of the training corpus then the param-
eter assignment 03 = oo maximizes the cor-
pus pseudo-likelihood. (Similarly, the assign-
ment 03 = —oo maximizes pseudo-likelihood if
fy is pseudo-minimal over the training corpus).
Such infinite parameter values indicate that the
model treats pseudo-maximal features categori-
cally; i.e., any parse with a non-maximal feature
value is assigned a zero conditional probability.
Of course, a feature which is pseudo-maximal
over the training corpus is not necessarily
pseudo-maximal for all yields. This is an in-
stance of over fitting, and it can be addressed,
as is customary, by adding a regularization term
that promotes small values of 0 to the objec-
tive function. A common choice is to add a
quadratic to the log-likelihood, which corre-
sponds to multiplying the likelihood itself by
a normal distribution. In our experiments, we
multiplied the pseudo-likelihood by a zero-mean
normal in 01, 0,„ with diagonal covariance,
and with standard deviation ay for 03 equal to
7 times the maximum value of fi found in any
parse in the training corpus. (We experimented
with other values for cry, but the choice seems to
have little effect). Thus instead of maximizing
the log pseudo-likelihood, we choose 0 to maxi-
</bodyText>
<equation confidence="0.870494">
mize 0?
log PL9() — E 3 (9)
20&amp;quot;?
j=1,...,m 3
</equation>
<sectionHeader confidence="0.779474" genericHeader="method">
4 A maximum correct estimator for
log linear models
</sectionHeader>
<bodyText confidence="0.999959782608696">
The pseudo-likelihood estimator described in
the last section finds parameter values which
maximize the conditional probabilities of the
observed parses (syntactic analyses) given the
observed sentences (yields) in the training cor-
pus. One of the empirical evaluation measures
we use in the next section measures the num-
ber of correct parses selected from the set of
all possible parses. This suggests another pos-
sible objective function: choose 0 to maximize
the number Co (c3) of times the maximum likeli-
hood parse (under 0) is in fact the correct parse,
in the training corpus.
C9(c73) is a highly discontinuous function of 0,
and most conventional optimization algorithms
perform poorly on it. We had the most suc-
cess with a slightly modified version of the sim-
ulated annealing optimizer described in Press
et al. (1992). This procedure is much more com-
putationally intensive than the gradient-based
pseudo-likelihood procedure. Its computational
difficulty grows (and the quality of solutions de-
grade) rapidly with the number of features.
</bodyText>
<sectionHeader confidence="0.959644" genericHeader="method">
5 Empirical evaluation
</sectionHeader>
<bodyText confidence="0.989376743589744">
Ron Kaplan and Hadar Shemtov at Xerox PARC
provided us with two LFG parsed corpora. The
Verbmobil corpus contains appointment plan-
ning dialogs, while the Homecentre corpus is
drawn from Xerox printer documentation. Ta-
ble 1 summarizes the basic properties of these
corpora. These corpora contain packed c/f-
structure representations (Maxwell III and Ka-
plan, 1995) of the grammatical parses of each
sentence with respect to Lexical-Functional
grammars. The corpora also indicate which of
these parses is in fact the correct parse (this
information was manually entered). Because
slightly different grammars were used for each
corpus we chose not to combine the two corpora,
although we used the set of features described in
section 2 for both in the experiments described
below. Table 2 describes the properties of the
features used for each corpus.
In addition to the two estimators described
above we also present results from a baseline es-
timator in which all parses are treated as equally
likely (this corresponds to setting all the param-
eters 01 to zero).
We evaluated our estimators using held-out
test corpus (7)test . We used two evaluation
measures. In an actual parsing application a
SUBG might be used to identify the correct
parse from the set of grammatical parses, so
ouorfitrestevaluation measure counts the number
c(.5st)
of sentences in the test corpus CA-hest
whose maximum likelihood parse under the es-
timated model 0 is actually the correct parse.
If a sentence has 1 most likely parses (i.e., all
1 parses have the same conditional probability)
and one of these parses is the correct parse, then
we score 1// for this sentence.
The second evaluation measure is the pseudo-
</bodyText>
<page confidence="0.993439">
539
</page>
<table confidence="0.997754">
Verbmobil corpus Homecentre corpus
Number of sentences 540 980
Number of ambiguous sentences 314 481
Number of parses of ambiguous sentences 3245 3169
</table>
<tableCaption confidence="0.995387">
Table 1: Properties of the two corpora used to evaluate the estimators.
</tableCaption>
<table confidence="0.999334166666667">
Verbmobil corpus Homecentre corpus
Number of features 191 227
Number of rule features 59 57
Number of pseudo-constant features 19 41
Number of pseudo-maximal features 12 4
Number of pseudo-minimal features 8 5
</table>
<tableCaption confidence="0.690246">
Table 2: Properties of the features used in the stochastic LFG models. The numbers of pseudo-
maximal and pseudo-minimal features do not include pseudo-constant features.
likelihood itself, Pk) (itest )- The pseudo-
</tableCaption>
<bodyText confidence="0.999716071428571">
likelihood of the test corpus is the likelihood of
the correct parses given their yields, so pseudo-
likelihood measures how much of the probabil-
ity mass the model puts onto the correct anal-
yses. This metric seems more relevant to ap-
plications where the system needs to estimate
how likely it is that the correct analysis lies in
a certain set of possible parses; e.g., ambiguity-
preserving translation and human-assisted dis-
ambiguation. To make the numbers more man-
ageable, we actually present the negative loga-
rithm of the pseudo-likelihood rather than the
pseudo-likelihood itself—so smaller is better.
Because of the small size of our corpora we
evaluated our estimators using a 10-way cross-
validation paradigm. We randomly assigned
sentences of each corpus into 10 approximately
equal-sized subcorpora, each of which was used
in turn as the test corpus. We evaluated on each
subcorpus the parameters that were estimated
from the 9 remaining subcorpora that served as
the training corpus for this run. The evalua-
tion scores from each subcorpus were summed
in order to provide the scores presented here.
Table 3 presents the results of the empiri-
cal evaluation. The superior performance of
both estimators on the Verbmobil corpus prob-
ably reflects the fact that the non-rule fea-
tures were designed to match both the gram-
mar and content of that corpus. The pseudo-
likelihood estimator performed better than the
correct-parses estimator on both corpora un-
der both evaluation metrics. There seems to
be substantial over learning in all these mod-
els; we routinely improved performance by dis-
carding features. With a small number of
features the correct-parses estimator typically
scores better than the pseudo-likelihood estima-
tor on the correct-parses evaluation metric, but
the pseudo-likelihood estimator always scores
better on the pseudo-likelihood evaluation met-
ric.
</bodyText>
<sectionHeader confidence="0.999198" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.919931095238095">
This paper described a log-linear model for
SUBGs and evaluated two estimators for such
models. Because estimators that can estimate
rule features for SUBGs can also estimate other
kinds of features, there is no particular reason to
limit attention to rule features in a SUBG. In-
deed, the number and choice of features strongly
influences the performance of the model. The
estimated models are able to identify the cor-
rect parse from the set of all possible parses ap-
proximately 50% of the time.
We would have liked to introduce features
corresponding to dependencies between lexical
items. Log-linear models are well-suited for lex-
ical dependencies, but because of the large num-
ber of such dependencies substantially larger
corpora will probably be needed to estimate
such mo dels .1
&apos;Alternatively, it may be possible to use a simpler
non-SUBG model of lexical dependencies estimated from
a much larger corpus as the reference distribution with
</bodyText>
<page confidence="0.986703">
540
</page>
<table confidence="0.9233984">
Verbmobil corpus Homecent re corpus
C(ii&apos;test) — log PL(test) C(test) — log PL(test)
Baseline estimator 9.7% 533 15.2% 655
Pseudo-likelihood estimator 58.7% 396 58.8% 583
Correct-parses estimator 53.7% 469 53.2% 604
</table>
<tableCaption confidence="0.999597">
Table 3: An empirical evaluation of the estimators. C(test) is the number of maximum likelihood
</tableCaption>
<bodyText confidence="0.951095416666667">
parses of the test corpus that were the correct parses, and — log PL(E5test) is the negative logarithm
of the pseudo-likelihood of the test corpus.
However, there may be applications which
can benefit from a model that performs even at
this level. For example, in a machine-assisted
translation system a model like ours could
be used to order possible translations so that
more likely alternatives are presented before less
likely ones. In the ambiguity-preserving trans-
lation framework, a model like this one could be
used to choose between sets of analyses whose
ambiguities cannot be preserved in translation.
</bodyText>
<sectionHeader confidence="0.997406" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999719632653061">
Steven P. Abney. 1997. Stochastic Attribute-
Value Grammars. Computational Linguis-
tics, 23(4):597-617.
Adam L. Berger, Vincent J. Della Pietra,
and Stephen A. Della Pietra. 1996. A
maximum entropy approach to natural lan-
guage processing. Computational Linguistics,
22(1):39-71.
J. Besag. 1974. Spatial interaction and the sta-
tistical analysis of lattice systems (with dis-
cussion). Journal of the Royal Statistical So-
ciety, Series D, 36:192-236.
J. Besag. 1975. Statistical analysis of non-
lattice data. The Statistician, 24:179-195.
Zhiyi Chi. 1998. Probability Models for Com-
plex Systems. Ph.D. thesis, Brown University.
Brendan J. Frey. 1998. Graphical Models for
Machine Learning and Digital Communica-
tion. The MIT Press, Cambridge, Mas-
sachusetts.
Jerry R. Hobbs and John Bear. 1995. Two
principles of parse preference. In Antonio
Zampolli, Nicoletta Calzolari, and Martha
Palmer, editors, Linguistica Computazionale:
Current Issues in Computational Linguistics:
respect to which the SUBG model is defined, as described
in Jelinek (1997).
In Honour of Don Walker, pages 503-512.
Kluwer.
Frederick Jelinek. 1997. Statistical Methods for
Speech Recognition. The MIT Press, Cam-
bridge, Massachusetts.
John T. Maxwell III and Ronald M. Kaplan.
1995. A method for disjunctive constraint
satisfaction. In Mary Dalrymple, Ronald M.
Kaplan, John T. Maxwell III, and Annie
Zaenen, editors, Formal Issues in Lexical-
Functional Grammar, number 47 in CSLI
Lecture Notes Series, chapter 14, pages 381-
481. CSLI Publications.
Judea Pearl. 1988. Pro babalistic Reasoning in
Intelligent Systems: Networks of Plausible
Inference. Morgan Kaufmann, San Mateo,
California.
William H. Press, Saul A. Teukolsky,
William T. Vetterling, and Brian P. Flannery.
1992. Numerical Recipies in C: The Art of
Scientific Computing. Cambridge University
Press, Cambridge, England, 2nd edition.
</reference>
<page confidence="0.997849">
541
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.657167">
<title confidence="0.995566">Estimators for Stochastic &amp;quot;Unification-Based&amp;quot; Grammars*</title>
<author confidence="0.998448">Mark Johnson Stuart Geman Stephen Canon</author>
<affiliation confidence="0.9572905">Cognitive and Linguistic Sciences Applied Mathematics Cognitive and Linguistic Sciences Brown University Brown University Brown University</affiliation>
<author confidence="0.997494">Zhiyi Chi Stefan Riezler</author>
<affiliation confidence="0.997936">Dept. of Statistics Institut fiir Maschinelle Sprachverarbeitung The University of Chicago Universitat Stuttgart</affiliation>
<abstract confidence="0.992507">Log-linear models provide a statistically sound framework for Stochastic &amp;quot;Unification-Based&amp;quot; Grammars (SUBGs) and stochastic versions of other kinds of grammars. We describe two computationally-tractable ways of estimating the parameters of such grammars from a training corpus of syntactic analyses, and apply these to estimate a stochastic version of Lexical-</abstract>
<intro confidence="0.736635">Functional Grammar.</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven P Abney</author>
</authors>
<title>Stochastic AttributeValue Grammars.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<pages>23--4</pages>
<contexts>
<context position="1537" citStr="Abney (1997)" startWordPosition="207" endWordPosition="208">able estimation procedure, stochastic models can be &amp;quot;tuned&amp;quot; to reflect the properties of a corpus. On the other hand, &amp;quot;Unification-Based&amp;quot; Grammars (UBGs) can express a variety of linguistically-important syntactic and semantic constraints. However, developing Stochastic &amp;quot;Unification-based&amp;quot; Grammars (SUBGs) has not proved as straightforward as might be hoped. The simple &amp;quot;relative frequency&amp;quot; estimator for PCFGs yields the maximum likelihood parameter estimate, which is to say that it minimizes the Kulback-Liebler divergence between the training and estimated distributions. On the other hand, as Abney (1997) points out, the context-sensitive dependencies that &amp;quot;unification-based&amp;quot; constraints introduce render the relative frequency estimator suboptimal: in general it does not maximize the likelihood and it is inconsistent. * This research was supported by the National Science Foundation (SBR-9720368), the US Army Research Office (DAAH04-96-BAA5), and Office of Naval Research (N00014-97-1-0249). Abney (1997) proposes a Markov Random Field or log linear model for SUBGs, and the models described here are instances of Abney&apos;s general framework. However, the Monte-Carlo parameter estimation procedure th</context>
<context position="8725" citStr="Abney (1997)" startWordPosition="1357" endWordPosition="1358">ctic analyses. Letting f3(c75) = f3 POI the log likelihood of the corpus CZ) and its derivatives are: log Lo(3) = E ejf3(L3) n log Zo (2) a log Lo (c7i) = fi((;) — nEo(f3) (3) where E0 (f&apos;) is the expected value of h under the distribution determined by the parameters 0. The maximum-likelihood estimates are the 0 which maximize log 14 (iv). The chief difficulty in finding the maximum-likelihood estimates is calculating E9 (f), which involves summing over the space of well-formed syntactic structures a There seems to be no analytic or efficient numerical way of doing this for a realistic SUBG. Abney (1997) proposes a gradient ascent, based upon a Monte Carlo, procedure for estimating Eo(f3). The idea is to generate random samples of feature structures from the distribution PO(w), where 0 is the current parameter estimate, and to use these to estimate Eo(f3), and hence the gradient of the likelihood. Samples are generated as follows: Given a SUBG, Abney constructs a covering PCFG based upon the SUBG and 0, the current estimate of 0. The derivation trees of the PCFG can be mapped onto a set containing all of the SUBG&apos;s syntactic analyses. Monte Carlo samples from the PCFG are comparatively easy t</context>
</contexts>
<marker>Abney, 1997</marker>
<rawString>Steven P. Abney. 1997. Stochastic AttributeValue Grammars. Computational Linguistics, 23(4):597-617.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<pages>22--1</pages>
<contexts>
<context position="12382" citStr="Berger et al. (1996)" startWordPosition="1962" endWordPosition="1965"> = wilY(w) = yi) (7) i=1,...,n Instead of maximizing the likelihood of the syntactic analyses over the training corpus, we maximize the conditional likelihood of these analyses given the observed yields. In our experiments, we have used a conjugate-gradient optimization program adapted from the one presented in Press et al. (1992). Regardless of the pragmatic (computational) motivation, one could perhaps argue that the conditional probabilities Po(wly) are as useful (if not more useful) as the full probabilities Po(w), at least in those cases for which the ultimate goal is syntactic analysis. Berger et al. (1996) and Jelinek (1997) make this same point and arrive at the same estimator, albeit through a maximum entropy argument. The problem of estimating parameters for log-linear models is not new. It is especially difficult in cases, such as ours, where a large sample space makes the direct computation of expectations infeasible. Many applications in spatial statistics, involving Markov random fields (MRF), are of this nature as well. In his seminal development of the MRF approach to spatial statistics, Besag introduced a &amp;quot;pseudolikelihood&amp;quot; estimator to address these difficulties (Besag, 1974; Besag, </context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam L. Berger, Vincent J. Della Pietra, and Stephen A. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39-71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Besag</author>
</authors>
<title>Spatial interaction and the statistical analysis of lattice systems (with discussion).</title>
<date>1974</date>
<journal>Journal of the Royal Statistical Society, Series D,</journal>
<pages>36--192</pages>
<contexts>
<context position="12973" citStr="Besag, 1974" startWordPosition="2059" endWordPosition="2060">Berger et al. (1996) and Jelinek (1997) make this same point and arrive at the same estimator, albeit through a maximum entropy argument. The problem of estimating parameters for log-linear models is not new. It is especially difficult in cases, such as ours, where a large sample space makes the direct computation of expectations infeasible. Many applications in spatial statistics, involving Markov random fields (MRF), are of this nature as well. In his seminal development of the MRF approach to spatial statistics, Besag introduced a &amp;quot;pseudolikelihood&amp;quot; estimator to address these difficulties (Besag, 1974; Besag, 1975), and in fact our proposal here is an instance of his method. In general, the likelihood function is replaced by a more manageable product of conditional likelihoods (a pseudo-likelihood—hence the designation PLO, which is then optimized over the parameter vector, instead of the likelihood itself. In many cases, as in our case here, this substitution side steps much of the computational burden without sacrificing consistency (more on this shortly). What are the asymptotics of optimizing a pseudo-likelihood function? Look first at the likelihood itself. For large n: —1 log II Pe (</context>
</contexts>
<marker>Besag, 1974</marker>
<rawString>J. Besag. 1974. Spatial interaction and the statistical analysis of lattice systems (with discussion). Journal of the Royal Statistical Society, Series D, 36:192-236.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Besag</author>
</authors>
<title>Statistical analysis of nonlattice data.</title>
<date>1975</date>
<journal>The Statistician,</journal>
<pages>24--179</pages>
<contexts>
<context position="12987" citStr="Besag, 1975" startWordPosition="2061" endWordPosition="2062"> (1996) and Jelinek (1997) make this same point and arrive at the same estimator, albeit through a maximum entropy argument. The problem of estimating parameters for log-linear models is not new. It is especially difficult in cases, such as ours, where a large sample space makes the direct computation of expectations infeasible. Many applications in spatial statistics, involving Markov random fields (MRF), are of this nature as well. In his seminal development of the MRF approach to spatial statistics, Besag introduced a &amp;quot;pseudolikelihood&amp;quot; estimator to address these difficulties (Besag, 1974; Besag, 1975), and in fact our proposal here is an instance of his method. In general, the likelihood function is replaced by a more manageable product of conditional likelihoods (a pseudo-likelihood—hence the designation PLO, which is then optimized over the parameter vector, instead of the likelihood itself. In many cases, as in our case here, this substitution side steps much of the computational burden without sacrificing consistency (more on this shortly). What are the asymptotics of optimizing a pseudo-likelihood function? Look first at the likelihood itself. For large n: —1 log II Pe (wi) n z _1 N- </context>
</contexts>
<marker>Besag, 1975</marker>
<rawString>J. Besag. 1975. Statistical analysis of nonlattice data. The Statistician, 24:179-195.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhiyi Chi</author>
</authors>
<title>Probability Models for Complex Systems.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>Brown University.</institution>
<contexts>
<context position="14636" citStr="Chi (1998)" startWordPosition="2329" endWordPosition="2330">e (wly)dw] So that maximizing pseudo-likelihood (at large samples) amounts to minimizing the average (over yields) divergence between the true and estimated conditional distributions of analyses given yields. Maximum likelihood estimation is consistent: under broad conditions the sequence of distributions Po , associated with the maximum likelihood estimator for 00 given the samples con, converges to Poo. Pseudo-likelihood is also consistent, but in the present implementation it is consistent for the conditional distributions Poo (wly(w)) and not necessarily for the full distribution Poo (see Chi (1998)). It is not hard to see that pseudo-likelihood will not always correctly estimate Poo. Suppose there is a feature L which depends only on yields: f2(w) = fz(y(w)). (Later we will refer to such features as pseudo-constant.) In this case, the derivative of PLo (1.0 with respect to 0i is zero; PL0(65) contains no information about 0. In fact, in this case any value of 0i gives the same conditional distribution Po (w ) y(w)); 0i is irrelevant to the problem of choosing good parses. Despite the assurance of consistency, pseudolikelihood estimation is prone to over fitting when a large number of fe</context>
</contexts>
<marker>Chi, 1998</marker>
<rawString>Zhiyi Chi. 1998. Probability Models for Complex Systems. Ph.D. thesis, Brown University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brendan J Frey</author>
</authors>
<title>Graphical Models for Machine Learning and Digital Communication.</title>
<date>1998</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="4377" citStr="Frey, 1998" startWordPosition="671" endWordPosition="672">ators described below make no assumptions about the range of the f, in the models considered here the value of each feature f2 (w) is the number of times a particular structural arrangement or configuration occurs in the analysis (..v, so fi(co) ranges over the natural numbers. For example, the features of a PCFG are indexed by productions, i.e., the value f(w) of feature f, is the number of times the ith production is used in the derivation w. This set of features induces a tree-structured dependency graph on the productions which is characteristic of Markov Branching Processes (Pearl, 1988; Frey, 1998). This tree structure has the important consequence that simple &amp;quot;relative-frequencies&amp;quot; yield maximumlikelihood estimates for the 02. Extending a PCFG model by adding additional features not associated with productions will in general add additional dependencies, destroy the tree structure, and substantially complicate maximum likelihood estimation. This is the situation for a SUBG, even if the features are production occurences. The unification constraints create non-local dependencies among the productions and the dependency graph of a SUBG is usually not a tree. Consequently, maximum likelih</context>
</contexts>
<marker>Frey, 1998</marker>
<rawString>Brendan J. Frey. 1998. Graphical Models for Machine Learning and Digital Communication. The MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry R Hobbs</author>
<author>John Bear</author>
</authors>
<title>Two principles of parse preference.</title>
<date>1995</date>
<booktitle>Linguistica Computazionale: Current Issues in Computational Linguistics: respect to which the SUBG model is defined, as described in Jelinek</booktitle>
<editor>In Antonio Zampolli, Nicoletta Calzolari, and Martha Palmer, editors,</editor>
<contexts>
<context position="6803" citStr="Hobbs and Bear (1995)" startWordPosition="1045" endWordPosition="1048">p thought Hillary left] yesterday] There are many different ways of choosing features for a SUBG, and each of these choices makes an empirical claim about possible distributions of sentences. Specifying the features of a SUBG is as much an empirical matter as specifying the grammar itself. For any given UBG there are a large (usually infinite) number of SUBGs that can be constructed from it, differing only in the features that each SUBG uses. In addition to production features, the stochastic LFG models evaluated below used the following kinds of features, guided by the principles proposed by Hobbs and Bear (1995). Adjunct and argument features indicate adjunct and argument attachment respectively, and permit the model to capture a general argument attachment preference. In addition, there are specialized adjunct and argument features corresponding to each grammatical function used in LFG (e.g., SUBJ, OBJ, COMP, XCOMP, ADJUNCT, etc.). There are features indicating both high and low attachment (determined by the complexity of the phrase being attached to). Another feature indicates nonright-branching nonterminal nodes. There is a feature for non-parallel coordinate structures (where parallelism is measu</context>
</contexts>
<marker>Hobbs, Bear, 1995</marker>
<rawString>Jerry R. Hobbs and John Bear. 1995. Two principles of parse preference. In Antonio Zampolli, Nicoletta Calzolari, and Martha Palmer, editors, Linguistica Computazionale: Current Issues in Computational Linguistics: respect to which the SUBG model is defined, as described in Jelinek (1997).</rawString>
</citation>
<citation valid="false">
<booktitle>In Honour of Don Walker,</booktitle>
<pages>503--512</pages>
<publisher>Kluwer.</publisher>
<marker></marker>
<rawString>In Honour of Don Walker, pages 503-512. Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
</authors>
<title>Statistical Methods for Speech Recognition.</title>
<date>1997</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="12401" citStr="Jelinek (1997)" startWordPosition="1967" endWordPosition="1968">...,n Instead of maximizing the likelihood of the syntactic analyses over the training corpus, we maximize the conditional likelihood of these analyses given the observed yields. In our experiments, we have used a conjugate-gradient optimization program adapted from the one presented in Press et al. (1992). Regardless of the pragmatic (computational) motivation, one could perhaps argue that the conditional probabilities Po(wly) are as useful (if not more useful) as the full probabilities Po(w), at least in those cases for which the ultimate goal is syntactic analysis. Berger et al. (1996) and Jelinek (1997) make this same point and arrive at the same estimator, albeit through a maximum entropy argument. The problem of estimating parameters for log-linear models is not new. It is especially difficult in cases, such as ours, where a large sample space makes the direct computation of expectations infeasible. Many applications in spatial statistics, involving Markov random fields (MRF), are of this nature as well. In his seminal development of the MRF approach to spatial statistics, Besag introduced a &amp;quot;pseudolikelihood&amp;quot; estimator to address these difficulties (Besag, 1974; Besag, 1975), and in fact </context>
</contexts>
<marker>Jelinek, 1997</marker>
<rawString>Frederick Jelinek. 1997. Statistical Methods for Speech Recognition. The MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John T Maxwell</author>
<author>Ronald M Kaplan</author>
</authors>
<title>A method for disjunctive constraint satisfaction.</title>
<date>1995</date>
<booktitle>Formal Issues in LexicalFunctional Grammar, number 47 in CSLI Lecture Notes Series, chapter 14,</booktitle>
<pages>381--481</pages>
<editor>In Mary Dalrymple, Ronald M. Kaplan, John T. Maxwell III, and Annie Zaenen, editors,</editor>
<publisher>CSLI Publications.</publisher>
<marker>Maxwell, Kaplan, 1995</marker>
<rawString>John T. Maxwell III and Ronald M. Kaplan. 1995. A method for disjunctive constraint satisfaction. In Mary Dalrymple, Ronald M. Kaplan, John T. Maxwell III, and Annie Zaenen, editors, Formal Issues in LexicalFunctional Grammar, number 47 in CSLI Lecture Notes Series, chapter 14, pages 381-481. CSLI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judea Pearl</author>
</authors>
<title>Pro babalistic Reasoning in Intelligent Systems: Networks of Plausible Inference.</title>
<date>1988</date>
<publisher>Morgan Kaufmann,</publisher>
<location>San Mateo, California.</location>
<contexts>
<context position="4364" citStr="Pearl, 1988" startWordPosition="669" endWordPosition="670">ile the estimators described below make no assumptions about the range of the f, in the models considered here the value of each feature f2 (w) is the number of times a particular structural arrangement or configuration occurs in the analysis (..v, so fi(co) ranges over the natural numbers. For example, the features of a PCFG are indexed by productions, i.e., the value f(w) of feature f, is the number of times the ith production is used in the derivation w. This set of features induces a tree-structured dependency graph on the productions which is characteristic of Markov Branching Processes (Pearl, 1988; Frey, 1998). This tree structure has the important consequence that simple &amp;quot;relative-frequencies&amp;quot; yield maximumlikelihood estimates for the 02. Extending a PCFG model by adding additional features not associated with productions will in general add additional dependencies, destroy the tree structure, and substantially complicate maximum likelihood estimation. This is the situation for a SUBG, even if the features are production occurences. The unification constraints create non-local dependencies among the productions and the dependency graph of a SUBG is usually not a tree. Consequently, ma</context>
</contexts>
<marker>Pearl, 1988</marker>
<rawString>Judea Pearl. 1988. Pro babalistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann, San Mateo, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William H Press</author>
<author>Saul A Teukolsky</author>
<author>William T Vetterling</author>
<author>Brian P Flannery</author>
</authors>
<title>Numerical Recipies in C: The Art of Scientific Computing.</title>
<date>1992</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, England,</location>
<contexts>
<context position="12094" citStr="Press et al. (1992)" startWordPosition="1916" endWordPosition="1919"> . Therefore, we propose replacing the gradient, (3), by .13(63) E Egh(w)iy(w) = yi) (6) and performing a gradient ascent. Of course (6) is no longer the gradient of the likelihood func537 tion, but fortunately it is (exactly) the gradient of (the log of) another criterion: pLem =H Po(w = wilY(w) = yi) (7) i=1,...,n Instead of maximizing the likelihood of the syntactic analyses over the training corpus, we maximize the conditional likelihood of these analyses given the observed yields. In our experiments, we have used a conjugate-gradient optimization program adapted from the one presented in Press et al. (1992). Regardless of the pragmatic (computational) motivation, one could perhaps argue that the conditional probabilities Po(wly) are as useful (if not more useful) as the full probabilities Po(w), at least in those cases for which the ultimate goal is syntactic analysis. Berger et al. (1996) and Jelinek (1997) make this same point and arrive at the same estimator, albeit through a maximum entropy argument. The problem of estimating parameters for log-linear models is not new. It is especially difficult in cases, such as ours, where a large sample space makes the direct computation of expectations </context>
<context position="18018" citStr="Press et al. (1992)" startWordPosition="2901" endWordPosition="2904">ields) in the training corpus. One of the empirical evaluation measures we use in the next section measures the number of correct parses selected from the set of all possible parses. This suggests another possible objective function: choose 0 to maximize the number Co (c3) of times the maximum likelihood parse (under 0) is in fact the correct parse, in the training corpus. C9(c73) is a highly discontinuous function of 0, and most conventional optimization algorithms perform poorly on it. We had the most success with a slightly modified version of the simulated annealing optimizer described in Press et al. (1992). This procedure is much more computationally intensive than the gradient-based pseudo-likelihood procedure. Its computational difficulty grows (and the quality of solutions degrade) rapidly with the number of features. 5 Empirical evaluation Ron Kaplan and Hadar Shemtov at Xerox PARC provided us with two LFG parsed corpora. The Verbmobil corpus contains appointment planning dialogs, while the Homecentre corpus is drawn from Xerox printer documentation. Table 1 summarizes the basic properties of these corpora. These corpora contain packed c/fstructure representations (Maxwell III and Kaplan, 1</context>
</contexts>
<marker>Press, Teukolsky, Vetterling, Flannery, 1992</marker>
<rawString>William H. Press, Saul A. Teukolsky, William T. Vetterling, and Brian P. Flannery. 1992. Numerical Recipies in C: The Art of Scientific Computing. Cambridge University Press, Cambridge, England, 2nd edition.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>