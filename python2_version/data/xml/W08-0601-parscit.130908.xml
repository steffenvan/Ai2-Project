<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.026602">
<title confidence="0.99906">
A Graph Kernel for Protein-Protein Interaction Extraction
</title>
<author confidence="0.996956">
Antti Airola, Sampo Pyysalo, Jari Bj¨orne, Tapio Pahikkala, Filip Ginter and Tapio Salakoski
</author>
<affiliation confidence="0.912206">
Turku Centre for Computer Science
and Department of IT, University of Turku
</affiliation>
<address confidence="0.95303">
Joukahaisenkatu 3-5
20520 Turku, Finland
</address>
<email confidence="0.998816">
firstname.lastname@utu.fi
</email>
<sectionHeader confidence="0.99564" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999650619047619">
In this paper, we propose a graph kernel
based approach for the automated extraction
of protein-protein interactions (PPI) from sci-
entific literature. In contrast to earlier ap-
proaches to PPI extraction, the introduced all-
dependency-paths kernel has the capability
to consider full, general dependency graphs.
We evaluate the proposed method across five
publicly available PPI corpora providing the
most comprehensive evaluation done for a ma-
chine learning based PPI-extraction system.
Our method is shown to achieve state-of-the-
art performance with respect to comparable
evaluations, achieving 56.4 F-score and 84.8
AUC on the AImed corpus. Further, we iden-
tify several pitfalls that can make evaluations
of PPI-extraction systems incomparable, or
even invalid. These include incorrect cross-
validation strategies and problems related to
comparing F-score results achieved on differ-
ent evaluation resources.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996109222222222">
Automated protein-protein interaction (PPI) extrac-
tion from scientific literature is a task of significant
interest in the BioNLP field. The most commonly
addressed problem has been the extraction of binary
interactions, where the system identifies which pro-
tein pairs in a sentence have a biologically relevant
relationship between them. Proposed solutions in-
clude both hand-crafted rule-based systems and ma-
chine learning approaches (see e.g. (Bunescu et al.,
2005)). A wide range of results have been reported
for the systems, but as we will show, differences in
evaluation resources, metrics and strategies make di-
rect comparison of these numbers problematic. Fur-
ther, the results gained from the BioCreative II eval-
uation, where the best performing system achieved
a 29% F-score (Hunter et al., 2008), suggest that the
problem of extracting binary protein protein interac-
tions is far from solved.
The public availability of large annotated PPI-
corpora such as AImed (Bunescu et al., 2005),
BioInfer (Pyysalo et al., 2007a) and GENIA (Kim
et al., 2008), provides an opportunity for building
PPI extraction systems automatically using machine
learning. A major challenge is how to supply the
learner with the contextual and syntactic informa-
tion needed to distinguish between interactions and
non-interactions. To address the ambiguity and vari-
ability of the natural language expressions used to
state PPI, several recent studies have focused on
the development, adaptation and application of NLP
tools for the biomedical domain. Many high-quality
domain-specific tools are now freely available, in-
cluding full parsers such as that introduced by Char-
niak and Lease (2005). Additionally, a number
of conversions from phrase structure parses to de-
pendency structures that make the relationships be-
tween words more directly accessible have been in-
troduced. These include conversions into represen-
tations such as the Stanford dependency scheme (de
Marneffe et al., 2006) that are explicitly designed for
information extraction purposes. However, special-
ized feature representations and kernels are required
to make learning from such structures possible.
Approaches such as subsequence kernels
(Bunescu and Mooney, 2006), tree kernels (Zelenko
</bodyText>
<page confidence="0.808065">
1
</page>
<note confidence="0.7603965">
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 1–9,
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</note>
<figure confidence="0.9635724">
prep_of&gt;
prep_of&gt; conj_and&gt;
interaction of P1 and P2
P1 is a P2 binding protein
P1 fails to bind P2
</figure>
<figureCaption confidence="0.998369">
Figure 1: Stanford dependency parses (“collapsed” rep-
resentation) where the shortest path, shown in bold, ex-
cludes important words.
</figureCaption>
<bodyText confidence="0.999985384615385">
et al., 2003) and shortest path kernels (Bunescu
and Mooney, 2005) have been proposed and suc-
cessfully used for relation extraction. However,
these methods lack the expressive power to consider
representations derived from general, possibly
cyclic, dependency graph structures, such as those
generated by the Stanford tools. The subsequence
kernel approach does not consider parses at all, and
the shortest path approach is limited to representing
only a single path in the full dependency graph,
which excludes relevant words even in many simple
cases (Figure 1). Tree kernels can represent more
complex structures, but are still restricted to tree
representations.
Lately, in the framework of kernel-based machine
learning methods there has been an increased in-
terest in designing kernel functions for graph data.
Building on the work of G¨artner et al. (2003),
graph representations tailored for the task of depen-
dency parse ranking were proposed by Pahikkala et
al. (2006b). Though the proposed representations
are not directly applicable to the task of PPI extrac-
tion, they offer insight in how to learn from depen-
dency graphs. We develop a graph kernel approach
for PPI extraction based on these ideas.
We next define a graph representation suitable for
describing potential interactions and introduce a ker-
nel which makes efficient learning from a general,
unrestricted graph representation possible. Then we
provide a short description of the sparse regular-
ized least squares (sparse RLS) kernel-based ma-
chine learning method we use for PPI-extraction.
Further, we rigorously assess our method on five
publicly available PPI corpora, providing the first
broad cross-corpus evaluation with a machine learn-
ing approach to PPI extraction. Finally, we discuss
the effects that different evaluation strategies, choice
of corpus and applied metrics have on measured per-
formance, and conclude.
</bodyText>
<sectionHeader confidence="0.942789" genericHeader="introduction">
2 Method
</sectionHeader>
<bodyText confidence="0.999901">
We next present our graph representation, formalize
the notion of graph kernels, and present our learning
method of choice, the sparse RLS.
</bodyText>
<subsectionHeader confidence="0.991568">
2.1 Graph encoding of sentence structure
</subsectionHeader>
<bodyText confidence="0.999975090909091">
As in most recent work on machine learning for PPI
extraction, we cast the task as learning a decision
function that determines for each unordered candi-
date pair of protein names occurring together in a
sentence whether the two proteins interact. In the
following, we first define the graph representation
used to represent an interaction candidate pair. We
then proceed to derive the kernel used to measure
the similarities of these graphs.
We assume that the input of our learning method
is a dependency parse of a sentence where a pair of
protein names is marked as the candidate interac-
tion for which an extraction decision must be made.
Based on this, we form a weighted, directed graph
that consists of two unconnected subgraphs. One
represents the dependency structure of the sentence,
and the other the linear order of the words (see Fig-
ure 2).
The first subgraph is built from the dependency
analysis. One vertex and an associated set of labels
is created in the graph for each token and for each
dependency. The vertices that represent tokens have
as labels the text and part-of-speech (POS) of the
token. To ensure generalization of the learned ex-
traction model, the labels of vertices that correspond
to protein names are replaced with PROT1, PROT2
or PROT, where PROT1 and PROT2 are the pair of
interest. The vertices that represent dependencies
are labeled with the type of the dependency. The
edges in the subgraph are defined so that each de-
pendency vertex is connected by an incoming edge
from the vertex representing its governor token, and
by an outgoing edge to the vertex representing its de-
</bodyText>
<figure confidence="0.993259125">
&lt;nsubj
&lt;cop
&lt;det
&lt;nn
&lt;nn
xcomp&gt;
&lt;nsubj &lt;aux dobj&gt;
&lt;xsubj
</figure>
<page confidence="0.96329">
2
</page>
<figureCaption confidence="0.9965148">
Figure 2: Graph representation generated from an example sentence. The candidate interaction pair is marked as
PROT1 and PROT2, the third protein is marked as PROT. The shortest path between the proteins is shown in bold. In
the dependency based subgraph all nodes in a shortest path are specialized using a post-tag (IP). In the linear order
subgraph possible tags are (B)efore, (M)iddle, and (A)fter. For the other two candidate pairs in the sentence, graphs
with the same structure but different weights and labels would be generated.
</figureCaption>
<bodyText confidence="0.9832406">
pendent token. The graph thus represents the entire
sentence structure.
It is widely acknowledged that the words between
the candidate entities or connecting them in a syn-
tactic representation are particularly likely to carry
information regarding their relationship; (Bunescu
and Mooney, 2005) formalize this intuition for de-
pendency graphs as the shortest path hypothesis. We
apply this insight in two ways in the graph repre-
sentation: the labels of the nodes on the shortest
undirected paths connecting PROT1 and PROT2 are
differentiated from the labels outside the paths us-
ing a special tag. Further, the edges are assigned
weights; after limited preliminary experiments, we
chose a simple weighting scheme where all edges
on the shortest paths receive a weight of 0.9 and
other edges receive a weight of 0.3. The represen-
tation thus allows us to emphasize the shortest path
without completely disregarding potentially relevant
words outside of the path.
The second subgraph is built from the linear struc-
ture of the sentence. For each token, a second ver-
tex is created and the labels for the vertices are de-
rived from the texts, POS-tags and named entity tag-
ging as above. The labels of each word are special-
ized to denote whether the word appears before, in-
between, or after the protein pair of interest. Each
word node is connected by an edge to its succeed-
ing word, as determined by sentence order the of the
words. Each edge is given the weight 0.9.
</bodyText>
<subsectionHeader confidence="0.996745">
2.2 The all-dependency-paths graph kernel
</subsectionHeader>
<bodyText confidence="0.999608344827586">
We next formalize the graph representation and
present the all-dependency-paths kernel. This ker-
nel can be considered as a practical instantiation of
the theoretical graph kernel framework introduced
by G¨artner et al. (2003). Let V be the set of ver-
tices in the graph and L be the set of possible labels
vertices can have. We represent the graph with an
adjacency matrix A E R|V |×|V |, whose rows and
columns are indexed by the vertices, and [A]i,j con-
tains the weight of the edge connecting vi E V and
vj E V if such an edge exists, and zero otherwise.
Further, we represent the labels as a label allocation
matrix L E R|L|×|V  |so that Li,j = 1 if the j-th
vertex has the i-th label and Li,j = 0 otherwise. Be-
cause only a very small fraction of all the possible
labels are ever assigned to any single node, this ma-
trix is extremely sparse.
It is well known that when an adjacency matrix is
multiplied with itself, each element [A2]i,j contains
the summed weight of paths from vertex vi to vertex
vj through one intervening vertex, that is, paths of
length two. Similarly, for any length n, the summed
weights from vi to vj can be determined by calculat-
ing [An]i,j.
Since we are interested not only in paths of one
specific length, it is natural to combine the effect of
paths of different lengths by summing the powers
of the adjacency matrices. We calculate the infinite
sum of the weights of all possible paths connecting
</bodyText>
<page confidence="0.986529">
3
</page>
<bodyText confidence="0.9079152">
the vertices using the Neumann Series, defined as
(I − A)−1 = I + A + A2 + ... =
if |A |&lt; 1 where |A |is the spectral radius of A
(Meyer, 2000). From this sum we can form a new
adjacency matrix
</bodyText>
<equation confidence="0.981526">
W = (I − A)−1 − I .
</equation>
<bodyText confidence="0.984006708333333">
The final adjacency matrix contains the summed
weights of all possible paths connecting the ver-
tices. The identity matrix is subtracted to remove
the paths of length zero, which would correspond to
self-loops.
Next, we present the graph kernel that utilizes the
graph representation defined previously. We define
an instance G representing a candidate interaction
as G = LW LT, where L and W are the label al-
location matrix and the final adjacency matrix cor-
responding to the graph representation of the candi-
date interaction.
Following G¨artner et al. (2003) the graph kernel
is defined as
k(G&apos;, G&apos;&apos;) =
where G&apos; and G&apos;&apos; are two instances formed as de-
fined previously. The features can be thought as
combinations of labels from connected pairs of ver-
tices, with a value that represents the strength of
their connection. In practical implementations, the
full G matrices, which consist mostly of zeroes, are
never explicitly formed. Rather, only the non-zero
elements are stored in memory and used when cal-
culating the kernels.
</bodyText>
<subsectionHeader confidence="0.992513">
2.3 Scalable learning with Sparse RLS
</subsectionHeader>
<bodyText confidence="0.999955380952381">
RLS is a state-of-the-art kernel-based machine
learning method which has been shown to have
comparable performance to support vector machines
(Rifkin et al., 2003). We choose the sparse version
of the algorithm, also known as subset of regressors,
as it allows us to scale up the method to very large
training set sizes. Sparse RLS also has the property
that it is possible to perform cross-validation and
regularization parameter selection so that their time
complexities are negligible compared to the training
complexity. These efficient methods are analogous
to the ones proposed by Pahikkala et al. (2006a) for
the basic RLS regression.
We now briefly present the basic sparse RLS al-
gorithm. Let m denote the training set size and
M = 11, ... , m} an index set in which the indices
refer to the examples in the training set. Instead of
allowing functions that can be expressed as a linear
combination over the whole training set, as in the
case of basic RLS regression, we only allow func-
tions of the following restricted type:
</bodyText>
<equation confidence="0.9920765">
f(·) = X aik(·, xi), (1)
iEB
</equation>
<bodyText confidence="0.999787">
where k is the kernel function, xi are training data
points, ai E R are weights, and the set indexing the
basis vectors B C M is selected in advance. The co-
efficients ai that determine (1) are obtained by min-
imizing
</bodyText>
<equation confidence="0.82401">
Xm X(yi − ajk(xi, xj))2 +A X aiajk(xi, xj),
i=1 jEB i,jEB
</equation>
<bodyText confidence="0.999249466666667">
where the first term is the squared loss function, the
second term is the regularizer, and A E R+ is a reg-
ularization parameter. Note that all the training in-
stances are used for determining the coefficient vec-
tor. The minimizer is obtained by solving the corre-
sponding system of linear equations, which can be
performed in O(m|B|2) time.
We set the maximum number of basis vectors to
4000 in all experiments in this study. The subset
is selected randomly when the training set size ex-
ceeds this number. Other methods for the selection
of the basis vectors were considered by Rifkin et
al. (2003), who however reported that the random
selection worked as well as the more sophisticated
approaches.
</bodyText>
<sectionHeader confidence="0.998311" genericHeader="method">
3 Experimental evaluation
</sectionHeader>
<bodyText confidence="0.999919333333333">
We next describe the evaluation resources and met-
rics used, provide a comprehensive evaluation of our
method across five PPI corpora, and compare our re-
sults to earlier work. Further, we discuss the chal-
lenges inherent in providing a valid method evalua-
tion and propose solutions.
</bodyText>
<equation confidence="0.9137581">
Ak
X00
k=0
X |L|
i=1
G&apos; i,jG&apos;&apos;
i,j,
|L|
X
j=1
</equation>
<page confidence="0.96781">
4
</page>
<table confidence="0.99925">
Corpus Statistics P R Graph Kernel AUC UAUC Co-occ.
#POS. #NEG. F OF P F
AIMed 1000 4834 0.529 0.618 0.564 0.050 0.848 0.023 0.178 0.301
BioInfer 1370 8924 0.477 0.599 0.529 0.053 0.849 0.065 0.135 0.237
HPRD50 163 270 0.643 0.658 0.634 0.114 0.797 0.063 0.389 0.554
IEPA 335 482 0.696 0.827 0.751 0.070 0.851 0.051 0.408 0.576
LLL 164 166 0.725 0.872 0.768 0.178 0.834 0.122 0.559 0.703
</table>
<tableCaption confidence="0.986837">
Table 1: Counts of positive and negative examples in the corpora and (P)recision, (R)ecall (F)-score and AUC for the
graph kernel, with standard deviations provided for F and AUC.
</tableCaption>
<subsectionHeader confidence="0.995971">
3.1 Corpora and evaluation criteria
</subsectionHeader>
<bodyText confidence="0.99945655882353">
We evaluate our method using five publicly avail-
able corpora that contain PPI interaction annotation:
AImed (Bunescu et al., 2005), BioInfer (Pyysalo et
al., 2007a), HPRD50 (Fundel et al., 2007), IEPA
(Ding et al., 2002) and LLL (N´edellec, 2005). All
the corpora were processed to a common format us-
ing transformations1 that we have introduced ear-
lier (Pyysalo et al., 2008). We parse these cor-
pora with the Charniak-Lease parser (Charniak and
Lease, 2005), which has been found to perform best
among a number of parsers tested in recent domain
evaluations (Clegg and Shepherd, 2007; Pyysalo et
al., 2007b). The Charniak-Lease phrase structure
parses are transformed into the collapsed Stanford
dependency scheme using the Stanford tools (de
Marneffe et al., 2006). We cast the PPI extraction
task as binary classification, where protein pairs that
are stated to interact are positive examples and other
co-occuring pairs negative. Thus, from each sen-
tence, (2) examples are generated, where n is the
number of occurrences of protein names in the sen-
tence. Finally, we form the graph representation de-
scribed earlier for each candidate interaction.
We evaluate the method with 10-fold document-
level cross-validation on all of the corpora. This
guarantees the maximal use of the available data,
and also allows comparison to relevant earlier work.
In particular, on the AImed corpus we apply the ex-
act same 10-fold split that was used by Bunescu et
al. (2006) and Giuliano et al. (2006). Performance
is measured according to the following criteria: in-
teractions are considered untyped, undirected pair-
wise relations between specific protein mentions,
that is, if the same protein name occurs multiple
</bodyText>
<footnote confidence="0.990583">
1Available at http://mars.cs.utu.fi/PPICorpora.
</footnote>
<bodyText confidence="0.99978456">
times in a sentence, the correct interactions must be
extracted for each occurrence. Further, we do not
consider self-interactions as candidates and remove
them from the corpora prior to evaluation.
The majority of PPI extraction system evaluations
use the balanced F-score measure for quantifying the
performance of the systems. This metric is defined
as F = 2��
�+�, where p is precision and r recall. Like-
wise, we provide F-score, precision, and recall val-
ues in our evaluation. It should be noted that F-score
is very sensitive to the underlying positive/negative
pair distribution of the corpus — a property whose
impact on evaluation is discussed in detail below. As
an alternative to F-score, we also evaluate the per-
formance of our system using the area under the re-
ceiver operating characteristics curve (AUC) mea-
sure (Hanley and McNeil, 1982). AUC has the im-
portant property that it is invariant to the class dis-
tribution of the used dataset. Due to this and other
beneficial properties for comparative evaluation, the
usage of AUC for performance evaluation has been
recently advocated in the machine learning commu-
nity (see e.g. (Bradley, 1997)). Formally, AUC can
be defined as
</bodyText>
<equation confidence="0.99900575">
AUC = m+
Em�
1 H(xz − yZ)
m+m−
</equation>
<bodyText confidence="0.99997525">
where m+ and m− are the numbers of positive
and negative examples, respectively, and x1,...,xm+
are the outputs of the system for the positive, and
y1,...,ym_ for the negative examples, and
</bodyText>
<equation confidence="0.776963">
{ 1, if r &gt; 0
0.5, if r = 0
0, otherwise.
</equation>
<bodyText confidence="0.9986485">
The measure corresponds to the probability that
given a randomly chosen positive and negative ex-
</bodyText>
<equation confidence="0.9627075">
,
H(r) =
</equation>
<page confidence="0.925629">
5
</page>
<bodyText confidence="0.999892">
ample, the system will be able to correctly disin-
guish which one is which.
</bodyText>
<subsectionHeader confidence="0.999422">
3.2 Performance across corpora
</subsectionHeader>
<bodyText confidence="0.999989106666667">
The performance of our method on the five corpora
for the various metrics is presented in Table 1. For
reference, we show also the performance of the co-
occurrence (or all-true) baseline, which simply as-
signs each candidate into the interaction class. The
recall of the co-occurrence method is trivially 100%,
and in terms of AUC it has a score of 0.5, the ran-
dom baseline. All the numbers in Table 1 are aver-
ages taken over the ten folds. One should note that
because of the non-linearity of the F-score measure,
the average precision and recall will not produce ex-
actly the average F.
The results hold several interesting findings. First,
we briefly observe that on the AImed corpus, which
has recently been applied in numerous evaluations
(Sartre et al., 2008) and can be seen as an emerging
de facto standard for PPI extraction method evalua-
tion, the method achieves an F-score performance of
56.4%. As we argue in more detail below, this level
of performance is comparable to the state-of-the-art
in machine learning based PPI extraction. For the
other large corpus, BioInfer, F-score performance is
slightly lower.
Second, we observe that the F-score performance
of the method varies strikingly between the differ-
ent corpora, with results on IEPA and LLL approx-
imately 20 percentage units higher than on AImed
and BioInfer, despite the larger size of the latter two.
In our previous work we have observed similar re-
sults with a rule-based extraction method (Pyysalo et
al., 2008). As the first broad cross-corpus evaluation
using a state-of-the-art machine learning method for
PPI extraction, our results support and extend the
key finding that F-score performance results mea-
sured on different corpora cannot, in general, be
meaningfully compared.
The co-occurrence baseline numbers indicate one
reason for the high F-score variance between the
corpora. The F-score metric is not invariant to the
distribution of positive and negative examples: for
example, halving the number of negative test exam-
ples is expected to approximately halve the number
of false positives at a given recall point. Thus, the
greater the fraction of true interactions in a corpus
is, the easier it is to reach high performance in terms
of F-score. This is reflected in co-occurrence re-
sults, which range from 24% to 70% depending on
the class distribution of the corpus.
This is a critical weakness of the F-score metric in
cross-corpus comparisons as, for example, the frac-
tion of true interactions out of all candidates is 50%
on the LLL corpus but only 17% on AImed. By
contrast to the large differences in performance mea-
sured using F-score, we find that for the distribution-
invariant AUC measure the performance for all of
the AImed, BioInfer, IEPA, and LLL corpora falls in
the narrow range of 83-85%. In terms of AUC, per-
formance on the HPRD50 corpus is an outlier, being
approximately three percentage units lower than for
any other corpus. Nevertheless, the results provide a
strong argument in favor of applying the AUC met-
ric instead of, or in addition to, F-score. AUC is also
more stable in terms of variance.
Finally, we note that the similar performance in
terms of AUC for corpora with as widely differing
sizes as LLL and BioInfer indicates that past a rel-
atively modest number of examples, increasing cor-
pus size has a surprisingly small effect on the perfor-
mance of the method. A similar finding can be seen,
for example, in the relatively flat learning curve of
Giuliano et al. (2006). While the issue requires fur-
ther investigation, these results suggest that there
may be more value in investing effort in develop-
ing better learning methods as opposed to larger cor-
pora.
</bodyText>
<subsectionHeader confidence="0.99887">
3.3 Performance compared to other methods
</subsectionHeader>
<bodyText confidence="0.9999288">
We next discuss the performance of our method
compared to other methods introduced in the liter-
ature and the challenges of meaningful comparison,
where we identify three major issues.
First, as indicated by the results above, differ-
ences in the makeup of different corpora render
cross-corpus comparisons in terms of F-score es-
sentially meaningless. As F-score is typically the
only metric for which results are reported in the PPI
extraction literature, we are limited to comparing
against results on single corpora. We consider the
AImed and BioInfer evaluations to be the most rele-
vant ones, as these corpora are sufficiently large for
training and reliably testing machine learning meth-
ods. As the present study is, to the best of our knowl-
</bodyText>
<page confidence="0.999411">
6
</page>
<table confidence="0.997790285714286">
P R F
(Giuliano et al., 2006) 60.9% 57.2% 59.0%
All-dependency-paths graph kernel 52.9% 61.8% 56.4%
(Bunescu and Mooney, 2006) 65.0% 46.4% 54.2%
(Sætre et al., 2008) 64.3% 44.1% 52.0%
(Mitsumori et al., 2006) 54.2% 42.6% 47.7%
(Yakushiji et al., 2005) 33.7% 33.1% 33.4%
</table>
<tableCaption confidence="0.992433">
Table 2: (P)recision, (R)ecall and (F)-score results for methods evaluated on AImed with the correct cross-validation
methodology.
</tableCaption>
<bodyText confidence="0.999969944444445">
edge, the first to report machine learning method
performance on BioInfer, we will focus on AImed
in the following comparison.
Second, the cross-validation strategy used in eval-
uation has a large impact on measured performance.
In earlier system evaluations, two major strategies
for defining the splits used in cross-validation can
be observed. The approach used by Bunescu and
Mooney (2006), which we consider the correct one,
is to split the data into folds on level of docu-
ments. This guarantees that all pairs generated from
the same document are always either in the train-
ing set or in the test set. Another approach is to
pool all the generated pairs together, and then ran-
domly split them to folds. To illustrate the signifi-
cance of this choice, consider two interaction candi-
dates extracted from the same sentence, e.g. from
a statement of the form “P1 and P2 [... ] P3”,
where “[... ]” is any statement of interaction or non-
interaction. Due to the near-identity of contexts, a
machine learning method will easily learn to predict
that the label of the pair (P1, P2) should match that
of (P1, P3). However, such “learning” will clearly
not generalize. This approach must thus be consid-
ered invalid, because allowing pairs generated from
same sentences to appear in different folds leads to
an information leak between the training and test
sets. Sætre et al. (2008) observed that adopting the
latter cross-validation strategy on AImed could lead
up to 18 F-score percentage unit overestimation of
performance. For this reason, we will not consider
results listed in the “False 10-fold cross-validation”
table (2b) of Sætre et al. (2008).
With these restrictions in place, we now turn to
comparison with relevant results reported in related
research, summarized in Table 2. We note that
Bunescu and Mooney (2006) only applied evalua-
tion criteria where it is enough to extract only one
occurrence of each mention of an interaction from
each abstract, while the other results shown were
evaluated using the same criteria as applied here.
The former approach can produce higher perfor-
mance: the evaluation of Giuliano et al. (2006) in-
cludes both alternatives, and their method achieves
an F-score of 63.9% under the former criterion,
which they term One Answer per Relation in a
given Document (OARD). Our method outperforms
most studies using similar evaluation methodology,
with the exception being the approach of Giuliano
et al. (2006). This result is somewhat surprising,
as the method proposed by Giuliano does not ap-
ply any form of parsing but relies instead only on
the sequential order of the words. This brings us
to our third point regarding comparability of meth-
ods. As pointed out by Sætre et al. (2008), the
AImed corpus allows remarkably different “inter-
pretations” regarding the number of interacting and
non-interacting pairs. For example, where we have
identified 1000 interacting and 4834 non-interacting
protein pairs in AImed, in the data used by Giuliano
there are eight more interacting and 200 fewer non-
interacting pairs. The corpus can also be prepro-
cessed in a number of ways. In particular we noticed
that whereas protein names are always blinded in our
data, in the data used by Giuliano protein names are
sometimes partly left visible. As Giuliano has gen-
erously made his method implementation available2,
we were able to test the performance of his system
on the data we used in our experiments. This re-
sulted in an F-score of 52.4%.
Finally, there remains an issue of parameter se-
lection. For sparse RLS the values of the regular-
</bodyText>
<footnote confidence="0.993897">
2Available at http://tcc.itc.it/research/
textec/tools-resources/jsre.html.
</footnote>
<page confidence="0.999273">
7
</page>
<bodyText confidence="0.99985376">
ization parameter A and the decision threshold sep-
arating the positive and negative classes must be
chosen, which can be problematic when no sepa-
rate data for choosing them is available. Choos-
ing from several parameter values the ones that give
best results in testing, or picking the best point
from a precision/recall curve when evaluating in
terms of F-score, will lead to an overoptimistic eval-
uation of performance. This issue has often not
been addressed in earlier evaluations that do cross-
validation on a whole corpus. We choose the pa-
rameters by doing further leave-one-document-out
cross-validation within each round of 10-fold-cross-
validation, on the nine folds that constitute the train-
ing set.
As a conclusion, we observe the results achieved
with the all-dependency-paths kernel to be state-of-
the-art level. However, differences in evaluation
strategies and the large variance exhibited in the re-
sults make it impossible to state which of the sys-
tems considered can be expected in general to per-
form best. We encourage future PPI-system evalua-
tions to report AUC and F-score results over mul-
tiple corpora, following clearly defined evaluation
strategies, to bring further clarity to this issue.
</bodyText>
<sectionHeader confidence="0.996327" genericHeader="conclusions">
4 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.999877387755102">
In this paper we have proposed a graph kernel
approach to extracting protein-protein interactions,
which captures the information in unrestricted de-
pendency graphs to a format that kernel based learn-
ing algorithms can process. The method combines
syntactic analysis with a representation of the lin-
ear order of the sentence, and considers all possi-
ble paths connecting any two vertices in the result-
ing graph. We demonstrate state-of-the art perfor-
mance for the approach. All software developed in
the course of this study is made publicly available at
http://mars.cs.utu.fi/PPICorpora.
We identify a number of issues which make re-
sults achieved with different evaluation strategies
and resources incomparable, or even incorrect. In
our experimental design we consider the problems
related to differences across corpora, the effects dif-
ferent cross-validation strategies have, and how pa-
rameter selection can be done. Our recommendation
is to provide evaluations over different corpora, to
use document-level cross-validation and to always
selected parameters on the training set.
We draw attention to the behaviour of the F-score
metric over corpora with differing pair distributions.
The higher the relative frequency of interacting pairs
is, the higher the performance can be expected to
be. This is noticed both for the graph kernel method
and for the naive co-occurrence baseline. Indeed,
the strategy of just stating that all pairs interact leads
to as high result as 70% F-score on one of the cor-
pora. We consider AUC as an alternative measure
that does not exhibit such behaviour, as it is invari-
ant to the distribution of pairs. The AUC metric is
much more stable across all the corpora, and never
gives better results than random for approaches such
as the naive co-occurrence.
Though we only consider binary interactions in
this work, the graph representations have the prop-
erty that they could be used to represent more com-
plex structures than pairs. The availability of cor-
pora that annotate complex interactions, such as the
full BioInfer and GENIA, makes training a PPI ex-
traction system for extracting complex interactions
an important avenue of future research. However,
how to avoid the combinatorial explosion following
from considering triplets, quartets etc. remains an
open question. Also, the performance of the cur-
rent approaches may need to be yet improved before
extending them to recognize complex interactions.
</bodyText>
<sectionHeader confidence="0.994948" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999726333333333">
We would like to thank Razvan Bunescu, Claudio
Giuliano and Rune Sætre for their generous assis-
tance in providing us with data, software and infor-
mation about their work on PPI extraction. Further,
we thank CSC, the Finnish IT center for science,
for providing us extensive computational resources.
This work has been supported by the Academy of
Finland and the Finnish Funding Agency for Tech-
nology and Innovation, Tekes.
</bodyText>
<sectionHeader confidence="0.998831" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.987188666666667">
Andrew P. Bradley. 1997. The use of the area under
the ROC curve in the evaluation of machine learning
algorithms. Pattern Recognition, 30(7):1145–1159.
Razvan Bunescu and Raymond Mooney. 2005. A short-
est path dependency kernel for relation extraction. In
Proceedings ofHLT/EMNLP’05, pages 724–731.
</reference>
<page confidence="0.990064">
8
</page>
<reference confidence="0.999903453608247">
Razvan Bunescu and Raymond Mooney. 2006. Subse-
quence kernels for relation extraction. In Proceedings
ofNIPS’05, pages 171–178. MIT Press.
Razvan C. Bunescu, Ruifang Ge, Rohit J. Kate, Ed-
ward M. Marcotte, Raymond J. Mooney, Arun Ku-
mar Ramani, and Yuk Wah Wong. 2005. Compar-
ative experiments on learning information extractors
for proteins and their interactions. Artif Intell Med,
33(2):139–155.
Eugene Charniak and Matthew Lease. 2005. Parsing
biomedical literature. In Proceedings of IJCNLP’05,
pages 58–69.
Andrew Brian Clegg and Adrian Shepherd. 2007.
Benchmarking natural-language parsers for biological
applications using dependency graphs. BMC Bioinfor-
matics, 8(1):24.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings ofLREC’06, pages 449–454.
J. Ding, D. Berleant, D. Nettleton, and E. Wurtele. 2002.
Mining MEDLINE: abstracts, sentences, or phrases?
In Proceedings of PSB’02, pages 326–337.
Katrin Fundel, Robert Kuffner, and Ralf Zimmer. 2007.
RelEx–Relation extraction using dependency parse
trees. Bioinformatics, 23(3):365–371.
Thomas G¨artner, Peter A. Flach, and Stefan Wrobel.
2003. On graph kernels: Hardness results and efficient
alternatives. In COLT’03, pages 129–143. Springer.
Claudio Giuliano, Alberto Lavelli, and Lorenza Romano.
2006. Exploiting shallow linguistic information for re-
lation extraction from biomedical literature. In Pro-
ceedings of EACL’06.
James A. Hanley and B. J. McNeil. 1982. The meaning
and use of the area under a receiver operating charac-
teristic (roc) curve. Radiology, 143(1):29–36.
Lawrence Hunter, Zhiyong Lu, James Firby, William A.
Baumgartner, Helen L Johnson, Philip V. Ogren, and
K. Bretonnel Cohen. 2008. OpenDMAP: An open-
source, ontology-driven concept analysis engine, with
applications to capturing knowledge regarding protein
transport, protein interactions and cell-specific gene
expression. BMC Bioinformatics, 9(78).
Jin-Dong Kim, Tomoko Ohta, and Jun’ichi Tsujii. 2008.
Corpus annotation for mining biomedical events from
literature. BMC Bioinformatics, 9(10).
Carl D. Meyer. 2000. Matrix analysis and applied linear
algebra. Society for Industrial and Applied Mathe-
matics.
Tomohiro Mitsumori, Masaki Murata, Yasushi Fukuda,
Kouichi Doi, and Hirohumi Doi. 2006. Extracting
protein-protein interaction information from biomed-
ical text with svm. IEICE - Trans. Inf. Syst., E89-
D(8):2464–2466.
Claire N´edellec. 2005. Learning language in logic -
genic interaction extraction challenge. In Proceedings
ofLLL’05.
Tapio Pahikkala, Jorma Boberg, and Tapio Salakoski.
2006a. Fast n-fold cross-validation for regularized
least-squares. In Proceedings of SCAI’06, pages 83–
90.
Tapio Pahikkala, Evgeni Tsivtsivadze, Jorma Boberg, and
Tapio Salakoski. 2006b. Graph kernels versus graph
representations: a case study in parse ranking. In Pro-
ceedings of the ECML/PKDD’06 workshop on Mining
and Learning with Graphs.
Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari
Bj¨orne, Jorma Boberg, Jouni J¨arvinen, and Tapio
Salakoski. 2007a. BioInfer: A corpus for information
extraction in the biomedical domain. BMC Bioinfor-
matics, 8(50).
Sampo Pyysalo, Filip Ginter, Veronika Laippala, Ka-
tri Haverinen, Juho Heimonen, and Tapio Salakoski.
2007b. On the unification of syntactic annotations un-
der the stanford dependency scheme: A case study on
BioInfer and GENIA. In Proceedings of BioNLP’07,
pages 25–32.
Sampo Pyysalo, Antti Airola, Juho Heimonen, Jari
Bj¨orne, Filip Ginter, and Tapio Salakoski. 2008.
Comparative analysis of five protein-protein interac-
tion corpora. BMC Bioinformatics, special issue,
9(Suppl 3):S6.
Ryan Rifkin, Gene Yeo, and Tomaso Poggio, 2003. Reg-
ularized Least-squares Classi�cation, volume 190 of
NATO Science Series III: Computer and System Sci-
ences, chapter 7, pages 131–154. IOS Press.
Rune Sætre, Kenji Sagae, and Jun’ichi Tsujii. 2008. Syn-
tactic features for protein-protein interaction extrac-
tion. In Proceedings of LBM’07, volume 319, pages
6.1–6.14.
Akane Yakushiji, Yusuke Miyao, Yuka Tateisi, and
Jun’ichi Tsujii. 2005. Biomedical information ex-
traction with predicate-argument structure patterns. In
Proceedings of SMBM’05, pages 60–69.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation
extraction. J. Mach. Learn. Res., 3:1083–1106.
</reference>
<page confidence="0.997114">
9
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.577843">
<title confidence="0.999839">A Graph Kernel for Protein-Protein Interaction Extraction</title>
<author confidence="0.969526">Antti Airola</author>
<author confidence="0.969526">Sampo Pyysalo</author>
<author confidence="0.969526">Jari Bj¨orne</author>
<author confidence="0.969526">Tapio Pahikkala</author>
<author confidence="0.969526">Filip Ginter</author>
<author confidence="0.969526">Tapio</author>
<affiliation confidence="0.856916333333333">Turku Centre for Computer and Department of IT, University of Joukahaisenkatu</affiliation>
<address confidence="0.99859">20520 Turku,</address>
<email confidence="0.998857">firstname.lastname@utu.fi</email>
<abstract confidence="0.999041363636364">In this paper, we propose a graph kernel based approach for the automated extraction of protein-protein interactions (PPI) from scientific literature. In contrast to earlier approaches to PPI extraction, the introduced alldependency-paths kernel has the capability to consider full, general dependency graphs. We evaluate the proposed method across five publicly available PPI corpora providing the most comprehensive evaluation done for a machine learning based PPI-extraction system. Our method is shown to achieve state-of-theart performance with respect to comparable evaluations, achieving 56.4 F-score and 84.8 AUC on the AImed corpus. Further, we identify several pitfalls that can make evaluations of PPI-extraction systems incomparable, or even invalid. These include incorrect crossvalidation strategies and problems related to comparing F-score results achieved on different evaluation resources.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Andrew P Bradley</author>
</authors>
<title>The use of the area under the ROC curve in the evaluation of machine learning algorithms.</title>
<date>1997</date>
<journal>Pattern Recognition,</journal>
<volume>30</volume>
<issue>7</issue>
<contexts>
<context position="18266" citStr="Bradley, 1997" startWordPosition="2977" endWordPosition="2978">erlying positive/negative pair distribution of the corpus — a property whose impact on evaluation is discussed in detail below. As an alternative to F-score, we also evaluate the performance of our system using the area under the receiver operating characteristics curve (AUC) measure (Hanley and McNeil, 1982). AUC has the important property that it is invariant to the class distribution of the used dataset. Due to this and other beneficial properties for comparative evaluation, the usage of AUC for performance evaluation has been recently advocated in the machine learning community (see e.g. (Bradley, 1997)). Formally, AUC can be defined as AUC = m+ Em� 1 H(xz − yZ) m+m− where m+ and m− are the numbers of positive and negative examples, respectively, and x1,...,xm+ are the outputs of the system for the positive, and y1,...,ym_ for the negative examples, and { 1, if r &gt; 0 0.5, if r = 0 0, otherwise. The measure corresponds to the probability that given a randomly chosen positive and negative ex, H(r) = 5 ample, the system will be able to correctly disinguish which one is which. 3.2 Performance across corpora The performance of our method on the five corpora for the various metrics is presented in</context>
</contexts>
<marker>Bradley, 1997</marker>
<rawString>Andrew P. Bradley. 1997. The use of the area under the ROC curve in the evaluation of machine learning algorithms. Pattern Recognition, 30(7):1145–1159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan Bunescu</author>
<author>Raymond Mooney</author>
</authors>
<title>A shortest path dependency kernel for relation extraction.</title>
<date>2005</date>
<booktitle>In Proceedings ofHLT/EMNLP’05,</booktitle>
<pages>724--731</pages>
<contexts>
<context position="3931" citStr="Bunescu and Mooney, 2005" startWordPosition="573" endWordPosition="576">and kernels are required to make learning from such structures possible. Approaches such as subsequence kernels (Bunescu and Mooney, 2006), tree kernels (Zelenko 1 BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 1–9, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics prep_of&gt; prep_of&gt; conj_and&gt; interaction of P1 and P2 P1 is a P2 binding protein P1 fails to bind P2 Figure 1: Stanford dependency parses (“collapsed” representation) where the shortest path, shown in bold, excludes important words. et al., 2003) and shortest path kernels (Bunescu and Mooney, 2005) have been proposed and successfully used for relation extraction. However, these methods lack the expressive power to consider representations derived from general, possibly cyclic, dependency graph structures, such as those generated by the Stanford tools. The subsequence kernel approach does not consider parses at all, and the shortest path approach is limited to representing only a single path in the full dependency graph, which excludes relevant words even in many simple cases (Figure 1). Tree kernels can represent more complex structures, but are still restricted to tree representations.</context>
<context position="8453" citStr="Bunescu and Mooney, 2005" startWordPosition="1295" endWordPosition="1298">shown in bold. In the dependency based subgraph all nodes in a shortest path are specialized using a post-tag (IP). In the linear order subgraph possible tags are (B)efore, (M)iddle, and (A)fter. For the other two candidate pairs in the sentence, graphs with the same structure but different weights and labels would be generated. pendent token. The graph thus represents the entire sentence structure. It is widely acknowledged that the words between the candidate entities or connecting them in a syntactic representation are particularly likely to carry information regarding their relationship; (Bunescu and Mooney, 2005) formalize this intuition for dependency graphs as the shortest path hypothesis. We apply this insight in two ways in the graph representation: the labels of the nodes on the shortest undirected paths connecting PROT1 and PROT2 are differentiated from the labels outside the paths using a special tag. Further, the edges are assigned weights; after limited preliminary experiments, we chose a simple weighting scheme where all edges on the shortest paths receive a weight of 0.9 and other edges receive a weight of 0.3. The representation thus allows us to emphasize the shortest path without complet</context>
</contexts>
<marker>Bunescu, Mooney, 2005</marker>
<rawString>Razvan Bunescu and Raymond Mooney. 2005. A shortest path dependency kernel for relation extraction. In Proceedings ofHLT/EMNLP’05, pages 724–731.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan Bunescu</author>
<author>Raymond Mooney</author>
</authors>
<title>Subsequence kernels for relation extraction.</title>
<date>2006</date>
<booktitle>In Proceedings ofNIPS’05,</booktitle>
<pages>171--178</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="3444" citStr="Bunescu and Mooney, 2006" startWordPosition="499" endWordPosition="502">ble, including full parsers such as that introduced by Charniak and Lease (2005). Additionally, a number of conversions from phrase structure parses to dependency structures that make the relationships between words more directly accessible have been introduced. These include conversions into representations such as the Stanford dependency scheme (de Marneffe et al., 2006) that are explicitly designed for information extraction purposes. However, specialized feature representations and kernels are required to make learning from such structures possible. Approaches such as subsequence kernels (Bunescu and Mooney, 2006), tree kernels (Zelenko 1 BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 1–9, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics prep_of&gt; prep_of&gt; conj_and&gt; interaction of P1 and P2 P1 is a P2 binding protein P1 fails to bind P2 Figure 1: Stanford dependency parses (“collapsed” representation) where the shortest path, shown in bold, excludes important words. et al., 2003) and shortest path kernels (Bunescu and Mooney, 2005) have been proposed and successfully used for relation extraction. However, these methods lack the expressive pow</context>
<context position="23367" citStr="Bunescu and Mooney, 2006" startWordPosition="3835" endWordPosition="3838">keup of different corpora render cross-corpus comparisons in terms of F-score essentially meaningless. As F-score is typically the only metric for which results are reported in the PPI extraction literature, we are limited to comparing against results on single corpora. We consider the AImed and BioInfer evaluations to be the most relevant ones, as these corpora are sufficiently large for training and reliably testing machine learning methods. As the present study is, to the best of our knowl6 P R F (Giuliano et al., 2006) 60.9% 57.2% 59.0% All-dependency-paths graph kernel 52.9% 61.8% 56.4% (Bunescu and Mooney, 2006) 65.0% 46.4% 54.2% (Sætre et al., 2008) 64.3% 44.1% 52.0% (Mitsumori et al., 2006) 54.2% 42.6% 47.7% (Yakushiji et al., 2005) 33.7% 33.1% 33.4% Table 2: (P)recision, (R)ecall and (F)-score results for methods evaluated on AImed with the correct cross-validation methodology. edge, the first to report machine learning method performance on BioInfer, we will focus on AImed in the following comparison. Second, the cross-validation strategy used in evaluation has a large impact on measured performance. In earlier system evaluations, two major strategies for defining the splits used in cross-validat</context>
<context position="25462" citStr="Bunescu and Mooney (2006)" startWordPosition="4177" endWordPosition="4180">red invalid, because allowing pairs generated from same sentences to appear in different folds leads to an information leak between the training and test sets. Sætre et al. (2008) observed that adopting the latter cross-validation strategy on AImed could lead up to 18 F-score percentage unit overestimation of performance. For this reason, we will not consider results listed in the “False 10-fold cross-validation” table (2b) of Sætre et al. (2008). With these restrictions in place, we now turn to comparison with relevant results reported in related research, summarized in Table 2. We note that Bunescu and Mooney (2006) only applied evaluation criteria where it is enough to extract only one occurrence of each mention of an interaction from each abstract, while the other results shown were evaluated using the same criteria as applied here. The former approach can produce higher performance: the evaluation of Giuliano et al. (2006) includes both alternatives, and their method achieves an F-score of 63.9% under the former criterion, which they term One Answer per Relation in a given Document (OARD). Our method outperforms most studies using similar evaluation methodology, with the exception being the approach o</context>
</contexts>
<marker>Bunescu, Mooney, 2006</marker>
<rawString>Razvan Bunescu and Raymond Mooney. 2006. Subsequence kernels for relation extraction. In Proceedings ofNIPS’05, pages 171–178. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan C Bunescu</author>
<author>Ruifang Ge</author>
<author>Rohit J Kate</author>
<author>Edward M Marcotte</author>
<author>Raymond J Mooney</author>
<author>Arun Kumar Ramani</author>
<author>Yuk Wah Wong</author>
</authors>
<title>Comparative experiments on learning information extractors for proteins and their interactions.</title>
<date>2005</date>
<journal>Artif Intell Med,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="1693" citStr="Bunescu et al., 2005" startWordPosition="235" endWordPosition="238">include incorrect crossvalidation strategies and problems related to comparing F-score results achieved on different evaluation resources. 1 Introduction Automated protein-protein interaction (PPI) extraction from scientific literature is a task of significant interest in the BioNLP field. The most commonly addressed problem has been the extraction of binary interactions, where the system identifies which protein pairs in a sentence have a biologically relevant relationship between them. Proposed solutions include both hand-crafted rule-based systems and machine learning approaches (see e.g. (Bunescu et al., 2005)). A wide range of results have been reported for the systems, but as we will show, differences in evaluation resources, metrics and strategies make direct comparison of these numbers problematic. Further, the results gained from the BioCreative II evaluation, where the best performing system achieved a 29% F-score (Hunter et al., 2008), suggest that the problem of extracting binary protein protein interactions is far from solved. The public availability of large annotated PPIcorpora such as AImed (Bunescu et al., 2005), BioInfer (Pyysalo et al., 2007a) and GENIA (Kim et al., 2008), provides a</context>
<context position="15491" citStr="Bunescu et al., 2005" startWordPosition="2533" endWordPosition="2536">0.848 0.023 0.178 0.301 BioInfer 1370 8924 0.477 0.599 0.529 0.053 0.849 0.065 0.135 0.237 HPRD50 163 270 0.643 0.658 0.634 0.114 0.797 0.063 0.389 0.554 IEPA 335 482 0.696 0.827 0.751 0.070 0.851 0.051 0.408 0.576 LLL 164 166 0.725 0.872 0.768 0.178 0.834 0.122 0.559 0.703 Table 1: Counts of positive and negative examples in the corpora and (P)recision, (R)ecall (F)-score and AUC for the graph kernel, with standard deviations provided for F and AUC. 3.1 Corpora and evaluation criteria We evaluate our method using five publicly available corpora that contain PPI interaction annotation: AImed (Bunescu et al., 2005), BioInfer (Pyysalo et al., 2007a), HPRD50 (Fundel et al., 2007), IEPA (Ding et al., 2002) and LLL (N´edellec, 2005). All the corpora were processed to a common format using transformations1 that we have introduced earlier (Pyysalo et al., 2008). We parse these corpora with the Charniak-Lease parser (Charniak and Lease, 2005), which has been found to perform best among a number of parsers tested in recent domain evaluations (Clegg and Shepherd, 2007; Pyysalo et al., 2007b). The Charniak-Lease phrase structure parses are transformed into the collapsed Stanford dependency scheme using the Stanfo</context>
</contexts>
<marker>Bunescu, Ge, Kate, Marcotte, Mooney, Ramani, Wong, 2005</marker>
<rawString>Razvan C. Bunescu, Ruifang Ge, Rohit J. Kate, Edward M. Marcotte, Raymond J. Mooney, Arun Kumar Ramani, and Yuk Wah Wong. 2005. Comparative experiments on learning information extractors for proteins and their interactions. Artif Intell Med, 33(2):139–155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Matthew Lease</author>
</authors>
<title>Parsing biomedical literature.</title>
<date>2005</date>
<booktitle>In Proceedings of IJCNLP’05,</booktitle>
<pages>58--69</pages>
<contexts>
<context position="2899" citStr="Charniak and Lease (2005)" startWordPosition="421" endWordPosition="425">, 2008), provides an opportunity for building PPI extraction systems automatically using machine learning. A major challenge is how to supply the learner with the contextual and syntactic information needed to distinguish between interactions and non-interactions. To address the ambiguity and variability of the natural language expressions used to state PPI, several recent studies have focused on the development, adaptation and application of NLP tools for the biomedical domain. Many high-quality domain-specific tools are now freely available, including full parsers such as that introduced by Charniak and Lease (2005). Additionally, a number of conversions from phrase structure parses to dependency structures that make the relationships between words more directly accessible have been introduced. These include conversions into representations such as the Stanford dependency scheme (de Marneffe et al., 2006) that are explicitly designed for information extraction purposes. However, specialized feature representations and kernels are required to make learning from such structures possible. Approaches such as subsequence kernels (Bunescu and Mooney, 2006), tree kernels (Zelenko 1 BioNLP 2008: Current Trends i</context>
<context position="15818" citStr="Charniak and Lease, 2005" startWordPosition="2587" endWordPosition="2590">s in the corpora and (P)recision, (R)ecall (F)-score and AUC for the graph kernel, with standard deviations provided for F and AUC. 3.1 Corpora and evaluation criteria We evaluate our method using five publicly available corpora that contain PPI interaction annotation: AImed (Bunescu et al., 2005), BioInfer (Pyysalo et al., 2007a), HPRD50 (Fundel et al., 2007), IEPA (Ding et al., 2002) and LLL (N´edellec, 2005). All the corpora were processed to a common format using transformations1 that we have introduced earlier (Pyysalo et al., 2008). We parse these corpora with the Charniak-Lease parser (Charniak and Lease, 2005), which has been found to perform best among a number of parsers tested in recent domain evaluations (Clegg and Shepherd, 2007; Pyysalo et al., 2007b). The Charniak-Lease phrase structure parses are transformed into the collapsed Stanford dependency scheme using the Stanford tools (de Marneffe et al., 2006). We cast the PPI extraction task as binary classification, where protein pairs that are stated to interact are positive examples and other co-occuring pairs negative. Thus, from each sentence, (2) examples are generated, where n is the number of occurrences of protein names in the sentence.</context>
</contexts>
<marker>Charniak, Lease, 2005</marker>
<rawString>Eugene Charniak and Matthew Lease. 2005. Parsing biomedical literature. In Proceedings of IJCNLP’05, pages 58–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Brian Clegg</author>
<author>Adrian Shepherd</author>
</authors>
<title>Benchmarking natural-language parsers for biological applications using dependency graphs.</title>
<date>2007</date>
<journal>BMC Bioinformatics,</journal>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="15944" citStr="Clegg and Shepherd, 2007" startWordPosition="2608" endWordPosition="2611">AUC. 3.1 Corpora and evaluation criteria We evaluate our method using five publicly available corpora that contain PPI interaction annotation: AImed (Bunescu et al., 2005), BioInfer (Pyysalo et al., 2007a), HPRD50 (Fundel et al., 2007), IEPA (Ding et al., 2002) and LLL (N´edellec, 2005). All the corpora were processed to a common format using transformations1 that we have introduced earlier (Pyysalo et al., 2008). We parse these corpora with the Charniak-Lease parser (Charniak and Lease, 2005), which has been found to perform best among a number of parsers tested in recent domain evaluations (Clegg and Shepherd, 2007; Pyysalo et al., 2007b). The Charniak-Lease phrase structure parses are transformed into the collapsed Stanford dependency scheme using the Stanford tools (de Marneffe et al., 2006). We cast the PPI extraction task as binary classification, where protein pairs that are stated to interact are positive examples and other co-occuring pairs negative. Thus, from each sentence, (2) examples are generated, where n is the number of occurrences of protein names in the sentence. Finally, we form the graph representation described earlier for each candidate interaction. We evaluate the method with 10-fo</context>
</contexts>
<marker>Clegg, Shepherd, 2007</marker>
<rawString>Andrew Brian Clegg and Adrian Shepherd. 2007. Benchmarking natural-language parsers for biological applications using dependency graphs. BMC Bioinformatics, 8(1):24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings ofLREC’06,</booktitle>
<pages>449--454</pages>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings ofLREC’06, pages 449–454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ding</author>
<author>D Berleant</author>
<author>D Nettleton</author>
<author>E Wurtele</author>
</authors>
<title>Mining MEDLINE: abstracts, sentences, or phrases?</title>
<date>2002</date>
<booktitle>In Proceedings of PSB’02,</booktitle>
<pages>326--337</pages>
<contexts>
<context position="15581" citStr="Ding et al., 2002" startWordPosition="2548" endWordPosition="2551">RD50 163 270 0.643 0.658 0.634 0.114 0.797 0.063 0.389 0.554 IEPA 335 482 0.696 0.827 0.751 0.070 0.851 0.051 0.408 0.576 LLL 164 166 0.725 0.872 0.768 0.178 0.834 0.122 0.559 0.703 Table 1: Counts of positive and negative examples in the corpora and (P)recision, (R)ecall (F)-score and AUC for the graph kernel, with standard deviations provided for F and AUC. 3.1 Corpora and evaluation criteria We evaluate our method using five publicly available corpora that contain PPI interaction annotation: AImed (Bunescu et al., 2005), BioInfer (Pyysalo et al., 2007a), HPRD50 (Fundel et al., 2007), IEPA (Ding et al., 2002) and LLL (N´edellec, 2005). All the corpora were processed to a common format using transformations1 that we have introduced earlier (Pyysalo et al., 2008). We parse these corpora with the Charniak-Lease parser (Charniak and Lease, 2005), which has been found to perform best among a number of parsers tested in recent domain evaluations (Clegg and Shepherd, 2007; Pyysalo et al., 2007b). The Charniak-Lease phrase structure parses are transformed into the collapsed Stanford dependency scheme using the Stanford tools (de Marneffe et al., 2006). We cast the PPI extraction task as binary classificat</context>
</contexts>
<marker>Ding, Berleant, Nettleton, Wurtele, 2002</marker>
<rawString>J. Ding, D. Berleant, D. Nettleton, and E. Wurtele. 2002. Mining MEDLINE: abstracts, sentences, or phrases? In Proceedings of PSB’02, pages 326–337.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Fundel</author>
<author>Robert Kuffner</author>
<author>Ralf Zimmer</author>
</authors>
<title>RelEx–Relation extraction using dependency parse trees.</title>
<date>2007</date>
<journal>Bioinformatics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="15555" citStr="Fundel et al., 2007" startWordPosition="2543" endWordPosition="2546">3 0.849 0.065 0.135 0.237 HPRD50 163 270 0.643 0.658 0.634 0.114 0.797 0.063 0.389 0.554 IEPA 335 482 0.696 0.827 0.751 0.070 0.851 0.051 0.408 0.576 LLL 164 166 0.725 0.872 0.768 0.178 0.834 0.122 0.559 0.703 Table 1: Counts of positive and negative examples in the corpora and (P)recision, (R)ecall (F)-score and AUC for the graph kernel, with standard deviations provided for F and AUC. 3.1 Corpora and evaluation criteria We evaluate our method using five publicly available corpora that contain PPI interaction annotation: AImed (Bunescu et al., 2005), BioInfer (Pyysalo et al., 2007a), HPRD50 (Fundel et al., 2007), IEPA (Ding et al., 2002) and LLL (N´edellec, 2005). All the corpora were processed to a common format using transformations1 that we have introduced earlier (Pyysalo et al., 2008). We parse these corpora with the Charniak-Lease parser (Charniak and Lease, 2005), which has been found to perform best among a number of parsers tested in recent domain evaluations (Clegg and Shepherd, 2007; Pyysalo et al., 2007b). The Charniak-Lease phrase structure parses are transformed into the collapsed Stanford dependency scheme using the Stanford tools (de Marneffe et al., 2006). We cast the PPI extraction </context>
</contexts>
<marker>Fundel, Kuffner, Zimmer, 2007</marker>
<rawString>Katrin Fundel, Robert Kuffner, and Ralf Zimmer. 2007. RelEx–Relation extraction using dependency parse trees. Bioinformatics, 23(3):365–371.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas G¨artner</author>
<author>Peter A Flach</author>
<author>Stefan Wrobel</author>
</authors>
<title>On graph kernels: Hardness results and efficient alternatives.</title>
<date>2003</date>
<booktitle>In COLT’03,</booktitle>
<pages>129--143</pages>
<publisher>Springer.</publisher>
<marker>G¨artner, Flach, Wrobel, 2003</marker>
<rawString>Thomas G¨artner, Peter A. Flach, and Stefan Wrobel. 2003. On graph kernels: Hardness results and efficient alternatives. In COLT’03, pages 129–143. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudio Giuliano</author>
<author>Alberto Lavelli</author>
<author>Lorenza Romano</author>
</authors>
<title>Exploiting shallow linguistic information for relation extraction from biomedical literature.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL’06.</booktitle>
<contexts>
<context position="16847" citStr="Giuliano et al. (2006)" startWordPosition="2753" endWordPosition="2756">interact are positive examples and other co-occuring pairs negative. Thus, from each sentence, (2) examples are generated, where n is the number of occurrences of protein names in the sentence. Finally, we form the graph representation described earlier for each candidate interaction. We evaluate the method with 10-fold documentlevel cross-validation on all of the corpora. This guarantees the maximal use of the available data, and also allows comparison to relevant earlier work. In particular, on the AImed corpus we apply the exact same 10-fold split that was used by Bunescu et al. (2006) and Giuliano et al. (2006). Performance is measured according to the following criteria: interactions are considered untyped, undirected pairwise relations between specific protein mentions, that is, if the same protein name occurs multiple 1Available at http://mars.cs.utu.fi/PPICorpora. times in a sentence, the correct interactions must be extracted for each occurrence. Further, we do not consider self-interactions as candidates and remove them from the corpora prior to evaluation. The majority of PPI extraction system evaluations use the balanced F-score measure for quantifying the performance of the systems. This me</context>
<context position="22264" citStr="Giuliano et al. (2006)" startWordPosition="3656" endWordPosition="3659">roximately three percentage units lower than for any other corpus. Nevertheless, the results provide a strong argument in favor of applying the AUC metric instead of, or in addition to, F-score. AUC is also more stable in terms of variance. Finally, we note that the similar performance in terms of AUC for corpora with as widely differing sizes as LLL and BioInfer indicates that past a relatively modest number of examples, increasing corpus size has a surprisingly small effect on the performance of the method. A similar finding can be seen, for example, in the relatively flat learning curve of Giuliano et al. (2006). While the issue requires further investigation, these results suggest that there may be more value in investing effort in developing better learning methods as opposed to larger corpora. 3.3 Performance compared to other methods We next discuss the performance of our method compared to other methods introduced in the literature and the challenges of meaningful comparison, where we identify three major issues. First, as indicated by the results above, differences in the makeup of different corpora render cross-corpus comparisons in terms of F-score essentially meaningless. As F-score is typic</context>
<context position="25778" citStr="Giuliano et al. (2006)" startWordPosition="4229" endWordPosition="4232">nce. For this reason, we will not consider results listed in the “False 10-fold cross-validation” table (2b) of Sætre et al. (2008). With these restrictions in place, we now turn to comparison with relevant results reported in related research, summarized in Table 2. We note that Bunescu and Mooney (2006) only applied evaluation criteria where it is enough to extract only one occurrence of each mention of an interaction from each abstract, while the other results shown were evaluated using the same criteria as applied here. The former approach can produce higher performance: the evaluation of Giuliano et al. (2006) includes both alternatives, and their method achieves an F-score of 63.9% under the former criterion, which they term One Answer per Relation in a given Document (OARD). Our method outperforms most studies using similar evaluation methodology, with the exception being the approach of Giuliano et al. (2006). This result is somewhat surprising, as the method proposed by Giuliano does not apply any form of parsing but relies instead only on the sequential order of the words. This brings us to our third point regarding comparability of methods. As pointed out by Sætre et al. (2008), the AImed cor</context>
</contexts>
<marker>Giuliano, Lavelli, Romano, 2006</marker>
<rawString>Claudio Giuliano, Alberto Lavelli, and Lorenza Romano. 2006. Exploiting shallow linguistic information for relation extraction from biomedical literature. In Proceedings of EACL’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James A Hanley</author>
<author>B J McNeil</author>
</authors>
<title>The meaning and use of the area under a receiver operating characteristic (roc) curve.</title>
<date>1982</date>
<journal>Radiology,</journal>
<volume>143</volume>
<issue>1</issue>
<contexts>
<context position="17962" citStr="Hanley and McNeil, 1982" startWordPosition="2925" endWordPosition="2928">n system evaluations use the balanced F-score measure for quantifying the performance of the systems. This metric is defined as F = 2�� �+�, where p is precision and r recall. Likewise, we provide F-score, precision, and recall values in our evaluation. It should be noted that F-score is very sensitive to the underlying positive/negative pair distribution of the corpus — a property whose impact on evaluation is discussed in detail below. As an alternative to F-score, we also evaluate the performance of our system using the area under the receiver operating characteristics curve (AUC) measure (Hanley and McNeil, 1982). AUC has the important property that it is invariant to the class distribution of the used dataset. Due to this and other beneficial properties for comparative evaluation, the usage of AUC for performance evaluation has been recently advocated in the machine learning community (see e.g. (Bradley, 1997)). Formally, AUC can be defined as AUC = m+ Em� 1 H(xz − yZ) m+m− where m+ and m− are the numbers of positive and negative examples, respectively, and x1,...,xm+ are the outputs of the system for the positive, and y1,...,ym_ for the negative examples, and { 1, if r &gt; 0 0.5, if r = 0 0, otherwise</context>
</contexts>
<marker>Hanley, McNeil, 1982</marker>
<rawString>James A. Hanley and B. J. McNeil. 1982. The meaning and use of the area under a receiver operating characteristic (roc) curve. Radiology, 143(1):29–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence Hunter</author>
<author>Zhiyong Lu</author>
<author>James Firby</author>
<author>William A Baumgartner</author>
<author>Helen L Johnson</author>
<author>Philip V Ogren</author>
<author>K Bretonnel Cohen</author>
</authors>
<title>OpenDMAP: An opensource, ontology-driven concept analysis engine, with applications to capturing knowledge regarding protein transport, protein interactions and cell-specific gene expression.</title>
<date>2008</date>
<journal>BMC Bioinformatics,</journal>
<volume>9</volume>
<issue>78</issue>
<contexts>
<context position="2031" citStr="Hunter et al., 2008" startWordPosition="290" endWordPosition="293"> extraction of binary interactions, where the system identifies which protein pairs in a sentence have a biologically relevant relationship between them. Proposed solutions include both hand-crafted rule-based systems and machine learning approaches (see e.g. (Bunescu et al., 2005)). A wide range of results have been reported for the systems, but as we will show, differences in evaluation resources, metrics and strategies make direct comparison of these numbers problematic. Further, the results gained from the BioCreative II evaluation, where the best performing system achieved a 29% F-score (Hunter et al., 2008), suggest that the problem of extracting binary protein protein interactions is far from solved. The public availability of large annotated PPIcorpora such as AImed (Bunescu et al., 2005), BioInfer (Pyysalo et al., 2007a) and GENIA (Kim et al., 2008), provides an opportunity for building PPI extraction systems automatically using machine learning. A major challenge is how to supply the learner with the contextual and syntactic information needed to distinguish between interactions and non-interactions. To address the ambiguity and variability of the natural language expressions used to state P</context>
</contexts>
<marker>Hunter, Lu, Firby, Baumgartner, Johnson, Ogren, Cohen, 2008</marker>
<rawString>Lawrence Hunter, Zhiyong Lu, James Firby, William A. Baumgartner, Helen L Johnson, Philip V. Ogren, and K. Bretonnel Cohen. 2008. OpenDMAP: An opensource, ontology-driven concept analysis engine, with applications to capturing knowledge regarding protein transport, protein interactions and cell-specific gene expression. BMC Bioinformatics, 9(78).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin-Dong Kim</author>
<author>Tomoko Ohta</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Corpus annotation for mining biomedical events from literature.</title>
<date>2008</date>
<journal>BMC Bioinformatics,</journal>
<volume>9</volume>
<issue>10</issue>
<contexts>
<context position="2281" citStr="Kim et al., 2008" startWordPosition="331" endWordPosition="334">e e.g. (Bunescu et al., 2005)). A wide range of results have been reported for the systems, but as we will show, differences in evaluation resources, metrics and strategies make direct comparison of these numbers problematic. Further, the results gained from the BioCreative II evaluation, where the best performing system achieved a 29% F-score (Hunter et al., 2008), suggest that the problem of extracting binary protein protein interactions is far from solved. The public availability of large annotated PPIcorpora such as AImed (Bunescu et al., 2005), BioInfer (Pyysalo et al., 2007a) and GENIA (Kim et al., 2008), provides an opportunity for building PPI extraction systems automatically using machine learning. A major challenge is how to supply the learner with the contextual and syntactic information needed to distinguish between interactions and non-interactions. To address the ambiguity and variability of the natural language expressions used to state PPI, several recent studies have focused on the development, adaptation and application of NLP tools for the biomedical domain. Many high-quality domain-specific tools are now freely available, including full parsers such as that introduced by Charnia</context>
</contexts>
<marker>Kim, Ohta, Tsujii, 2008</marker>
<rawString>Jin-Dong Kim, Tomoko Ohta, and Jun’ichi Tsujii. 2008. Corpus annotation for mining biomedical events from literature. BMC Bioinformatics, 9(10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl D Meyer</author>
</authors>
<title>Matrix analysis and applied linear algebra.</title>
<date>2000</date>
<journal>Society for Industrial and Applied Mathematics.</journal>
<contexts>
<context position="11232" citStr="Meyer, 2000" startWordPosition="1800" endWordPosition="1801">ight of paths from vertex vi to vertex vj through one intervening vertex, that is, paths of length two. Similarly, for any length n, the summed weights from vi to vj can be determined by calculating [An]i,j. Since we are interested not only in paths of one specific length, it is natural to combine the effect of paths of different lengths by summing the powers of the adjacency matrices. We calculate the infinite sum of the weights of all possible paths connecting 3 the vertices using the Neumann Series, defined as (I − A)−1 = I + A + A2 + ... = if |A |&lt; 1 where |A |is the spectral radius of A (Meyer, 2000). From this sum we can form a new adjacency matrix W = (I − A)−1 − I . The final adjacency matrix contains the summed weights of all possible paths connecting the vertices. The identity matrix is subtracted to remove the paths of length zero, which would correspond to self-loops. Next, we present the graph kernel that utilizes the graph representation defined previously. We define an instance G representing a candidate interaction as G = LW LT, where L and W are the label allocation matrix and the final adjacency matrix corresponding to the graph representation of the candidate interaction. Fo</context>
</contexts>
<marker>Meyer, 2000</marker>
<rawString>Carl D. Meyer. 2000. Matrix analysis and applied linear algebra. Society for Industrial and Applied Mathematics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomohiro Mitsumori</author>
<author>Masaki Murata</author>
<author>Yasushi Fukuda</author>
<author>Kouichi Doi</author>
<author>Hirohumi Doi</author>
</authors>
<title>Extracting protein-protein interaction information from biomedical text with svm.</title>
<date>2006</date>
<journal>IEICE - Trans. Inf. Syst.,</journal>
<pages>89--8</pages>
<contexts>
<context position="23449" citStr="Mitsumori et al., 2006" startWordPosition="3849" endWordPosition="3852">ially meaningless. As F-score is typically the only metric for which results are reported in the PPI extraction literature, we are limited to comparing against results on single corpora. We consider the AImed and BioInfer evaluations to be the most relevant ones, as these corpora are sufficiently large for training and reliably testing machine learning methods. As the present study is, to the best of our knowl6 P R F (Giuliano et al., 2006) 60.9% 57.2% 59.0% All-dependency-paths graph kernel 52.9% 61.8% 56.4% (Bunescu and Mooney, 2006) 65.0% 46.4% 54.2% (Sætre et al., 2008) 64.3% 44.1% 52.0% (Mitsumori et al., 2006) 54.2% 42.6% 47.7% (Yakushiji et al., 2005) 33.7% 33.1% 33.4% Table 2: (P)recision, (R)ecall and (F)-score results for methods evaluated on AImed with the correct cross-validation methodology. edge, the first to report machine learning method performance on BioInfer, we will focus on AImed in the following comparison. Second, the cross-validation strategy used in evaluation has a large impact on measured performance. In earlier system evaluations, two major strategies for defining the splits used in cross-validation can be observed. The approach used by Bunescu and Mooney (2006), which we cons</context>
</contexts>
<marker>Mitsumori, Murata, Fukuda, Doi, Doi, 2006</marker>
<rawString>Tomohiro Mitsumori, Masaki Murata, Yasushi Fukuda, Kouichi Doi, and Hirohumi Doi. 2006. Extracting protein-protein interaction information from biomedical text with svm. IEICE - Trans. Inf. Syst., E89-D(8):2464–2466.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claire N´edellec</author>
</authors>
<title>Learning language in logic -genic interaction extraction challenge.</title>
<date>2005</date>
<booktitle>In Proceedings ofLLL’05.</booktitle>
<marker>N´edellec, 2005</marker>
<rawString>Claire N´edellec. 2005. Learning language in logic -genic interaction extraction challenge. In Proceedings ofLLL’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tapio Pahikkala</author>
<author>Jorma Boberg</author>
<author>Tapio Salakoski</author>
</authors>
<title>Fast n-fold cross-validation for regularized least-squares.</title>
<date>2006</date>
<booktitle>In Proceedings of SCAI’06,</booktitle>
<pages>83--90</pages>
<contexts>
<context position="4840" citStr="Pahikkala et al. (2006" startWordPosition="712" endWordPosition="715">ach does not consider parses at all, and the shortest path approach is limited to representing only a single path in the full dependency graph, which excludes relevant words even in many simple cases (Figure 1). Tree kernels can represent more complex structures, but are still restricted to tree representations. Lately, in the framework of kernel-based machine learning methods there has been an increased interest in designing kernel functions for graph data. Building on the work of G¨artner et al. (2003), graph representations tailored for the task of dependency parse ranking were proposed by Pahikkala et al. (2006b). Though the proposed representations are not directly applicable to the task of PPI extraction, they offer insight in how to learn from dependency graphs. We develop a graph kernel approach for PPI extraction based on these ideas. We next define a graph representation suitable for describing potential interactions and introduce a kernel which makes efficient learning from a general, unrestricted graph representation possible. Then we provide a short description of the sparse regularized least squares (sparse RLS) kernel-based machine learning method we use for PPI-extraction. Further, we ri</context>
<context position="12977" citStr="Pahikkala et al. (2006" startWordPosition="2087" endWordPosition="2090">h Sparse RLS RLS is a state-of-the-art kernel-based machine learning method which has been shown to have comparable performance to support vector machines (Rifkin et al., 2003). We choose the sparse version of the algorithm, also known as subset of regressors, as it allows us to scale up the method to very large training set sizes. Sparse RLS also has the property that it is possible to perform cross-validation and regularization parameter selection so that their time complexities are negligible compared to the training complexity. These efficient methods are analogous to the ones proposed by Pahikkala et al. (2006a) for the basic RLS regression. We now briefly present the basic sparse RLS algorithm. Let m denote the training set size and M = 11, ... , m} an index set in which the indices refer to the examples in the training set. Instead of allowing functions that can be expressed as a linear combination over the whole training set, as in the case of basic RLS regression, we only allow functions of the following restricted type: f(·) = X aik(·, xi), (1) iEB where k is the kernel function, xi are training data points, ai E R are weights, and the set indexing the basis vectors B C M is selected in advanc</context>
</contexts>
<marker>Pahikkala, Boberg, Salakoski, 2006</marker>
<rawString>Tapio Pahikkala, Jorma Boberg, and Tapio Salakoski. 2006a. Fast n-fold cross-validation for regularized least-squares. In Proceedings of SCAI’06, pages 83– 90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tapio Pahikkala</author>
<author>Evgeni Tsivtsivadze</author>
<author>Jorma Boberg</author>
<author>Tapio Salakoski</author>
</authors>
<title>Graph kernels versus graph representations: a case study in parse ranking.</title>
<date>2006</date>
<booktitle>In Proceedings of the ECML/PKDD’06 workshop on Mining and Learning with Graphs.</booktitle>
<contexts>
<context position="4840" citStr="Pahikkala et al. (2006" startWordPosition="712" endWordPosition="715">ach does not consider parses at all, and the shortest path approach is limited to representing only a single path in the full dependency graph, which excludes relevant words even in many simple cases (Figure 1). Tree kernels can represent more complex structures, but are still restricted to tree representations. Lately, in the framework of kernel-based machine learning methods there has been an increased interest in designing kernel functions for graph data. Building on the work of G¨artner et al. (2003), graph representations tailored for the task of dependency parse ranking were proposed by Pahikkala et al. (2006b). Though the proposed representations are not directly applicable to the task of PPI extraction, they offer insight in how to learn from dependency graphs. We develop a graph kernel approach for PPI extraction based on these ideas. We next define a graph representation suitable for describing potential interactions and introduce a kernel which makes efficient learning from a general, unrestricted graph representation possible. Then we provide a short description of the sparse regularized least squares (sparse RLS) kernel-based machine learning method we use for PPI-extraction. Further, we ri</context>
<context position="12977" citStr="Pahikkala et al. (2006" startWordPosition="2087" endWordPosition="2090">h Sparse RLS RLS is a state-of-the-art kernel-based machine learning method which has been shown to have comparable performance to support vector machines (Rifkin et al., 2003). We choose the sparse version of the algorithm, also known as subset of regressors, as it allows us to scale up the method to very large training set sizes. Sparse RLS also has the property that it is possible to perform cross-validation and regularization parameter selection so that their time complexities are negligible compared to the training complexity. These efficient methods are analogous to the ones proposed by Pahikkala et al. (2006a) for the basic RLS regression. We now briefly present the basic sparse RLS algorithm. Let m denote the training set size and M = 11, ... , m} an index set in which the indices refer to the examples in the training set. Instead of allowing functions that can be expressed as a linear combination over the whole training set, as in the case of basic RLS regression, we only allow functions of the following restricted type: f(·) = X aik(·, xi), (1) iEB where k is the kernel function, xi are training data points, ai E R are weights, and the set indexing the basis vectors B C M is selected in advanc</context>
</contexts>
<marker>Pahikkala, Tsivtsivadze, Boberg, Salakoski, 2006</marker>
<rawString>Tapio Pahikkala, Evgeni Tsivtsivadze, Jorma Boberg, and Tapio Salakoski. 2006b. Graph kernels versus graph representations: a case study in parse ranking. In Proceedings of the ECML/PKDD’06 workshop on Mining and Learning with Graphs.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sampo Pyysalo</author>
<author>Filip Ginter</author>
<author>Juho Heimonen</author>
<author>Jari Bj¨orne</author>
<author>Jorma Boberg</author>
<author>Jouni J¨arvinen</author>
<author>Tapio Salakoski</author>
</authors>
<title>BioInfer: A corpus for information extraction in the biomedical domain.</title>
<date>2007</date>
<journal>BMC Bioinformatics,</journal>
<volume>8</volume>
<issue>50</issue>
<marker>Pyysalo, Ginter, Heimonen, Bj¨orne, Boberg, J¨arvinen, Salakoski, 2007</marker>
<rawString>Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari Bj¨orne, Jorma Boberg, Jouni J¨arvinen, and Tapio Salakoski. 2007a. BioInfer: A corpus for information extraction in the biomedical domain. BMC Bioinformatics, 8(50).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sampo Pyysalo</author>
<author>Filip Ginter</author>
<author>Veronika Laippala</author>
<author>Katri Haverinen</author>
<author>Juho Heimonen</author>
<author>Tapio Salakoski</author>
</authors>
<title>On the unification of syntactic annotations under the stanford dependency scheme: A case study on BioInfer and GENIA.</title>
<date>2007</date>
<booktitle>In Proceedings of BioNLP’07,</booktitle>
<pages>25--32</pages>
<contexts>
<context position="2250" citStr="Pyysalo et al., 2007" startWordPosition="325" endWordPosition="328">nd machine learning approaches (see e.g. (Bunescu et al., 2005)). A wide range of results have been reported for the systems, but as we will show, differences in evaluation resources, metrics and strategies make direct comparison of these numbers problematic. Further, the results gained from the BioCreative II evaluation, where the best performing system achieved a 29% F-score (Hunter et al., 2008), suggest that the problem of extracting binary protein protein interactions is far from solved. The public availability of large annotated PPIcorpora such as AImed (Bunescu et al., 2005), BioInfer (Pyysalo et al., 2007a) and GENIA (Kim et al., 2008), provides an opportunity for building PPI extraction systems automatically using machine learning. A major challenge is how to supply the learner with the contextual and syntactic information needed to distinguish between interactions and non-interactions. To address the ambiguity and variability of the natural language expressions used to state PPI, several recent studies have focused on the development, adaptation and application of NLP tools for the biomedical domain. Many high-quality domain-specific tools are now freely available, including full parsers suc</context>
<context position="15523" citStr="Pyysalo et al., 2007" startWordPosition="2538" endWordPosition="2541">1370 8924 0.477 0.599 0.529 0.053 0.849 0.065 0.135 0.237 HPRD50 163 270 0.643 0.658 0.634 0.114 0.797 0.063 0.389 0.554 IEPA 335 482 0.696 0.827 0.751 0.070 0.851 0.051 0.408 0.576 LLL 164 166 0.725 0.872 0.768 0.178 0.834 0.122 0.559 0.703 Table 1: Counts of positive and negative examples in the corpora and (P)recision, (R)ecall (F)-score and AUC for the graph kernel, with standard deviations provided for F and AUC. 3.1 Corpora and evaluation criteria We evaluate our method using five publicly available corpora that contain PPI interaction annotation: AImed (Bunescu et al., 2005), BioInfer (Pyysalo et al., 2007a), HPRD50 (Fundel et al., 2007), IEPA (Ding et al., 2002) and LLL (N´edellec, 2005). All the corpora were processed to a common format using transformations1 that we have introduced earlier (Pyysalo et al., 2008). We parse these corpora with the Charniak-Lease parser (Charniak and Lease, 2005), which has been found to perform best among a number of parsers tested in recent domain evaluations (Clegg and Shepherd, 2007; Pyysalo et al., 2007b). The Charniak-Lease phrase structure parses are transformed into the collapsed Stanford dependency scheme using the Stanford tools (de Marneffe et al., 20</context>
</contexts>
<marker>Pyysalo, Ginter, Laippala, Haverinen, Heimonen, Salakoski, 2007</marker>
<rawString>Sampo Pyysalo, Filip Ginter, Veronika Laippala, Katri Haverinen, Juho Heimonen, and Tapio Salakoski. 2007b. On the unification of syntactic annotations under the stanford dependency scheme: A case study on BioInfer and GENIA. In Proceedings of BioNLP’07, pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sampo Pyysalo</author>
<author>Antti Airola</author>
<author>Juho Heimonen</author>
<author>Jari Bj¨orne</author>
<author>Filip Ginter</author>
<author>Tapio Salakoski</author>
</authors>
<title>Comparative analysis of five protein-protein interaction corpora. BMC Bioinformatics, special issue, 9(Suppl 3):S6.</title>
<date>2008</date>
<marker>Pyysalo, Airola, Heimonen, Bj¨orne, Ginter, Salakoski, 2008</marker>
<rawString>Sampo Pyysalo, Antti Airola, Juho Heimonen, Jari Bj¨orne, Filip Ginter, and Tapio Salakoski. 2008. Comparative analysis of five protein-protein interaction corpora. BMC Bioinformatics, special issue, 9(Suppl 3):S6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan Rifkin</author>
<author>Gene Yeo</author>
<author>Tomaso Poggio</author>
</authors>
<title>Regularized Least-squares Classi�cation,</title>
<date>2003</date>
<booktitle>of NATO Science Series III: Computer and System Sciences, chapter 7,</booktitle>
<volume>190</volume>
<pages>131--154</pages>
<publisher>IOS Press.</publisher>
<contexts>
<context position="12531" citStr="Rifkin et al., 2003" startWordPosition="2015" endWordPosition="2018">e G&apos; and G&apos;&apos; are two instances formed as defined previously. The features can be thought as combinations of labels from connected pairs of vertices, with a value that represents the strength of their connection. In practical implementations, the full G matrices, which consist mostly of zeroes, are never explicitly formed. Rather, only the non-zero elements are stored in memory and used when calculating the kernels. 2.3 Scalable learning with Sparse RLS RLS is a state-of-the-art kernel-based machine learning method which has been shown to have comparable performance to support vector machines (Rifkin et al., 2003). We choose the sparse version of the algorithm, also known as subset of regressors, as it allows us to scale up the method to very large training set sizes. Sparse RLS also has the property that it is possible to perform cross-validation and regularization parameter selection so that their time complexities are negligible compared to the training complexity. These efficient methods are analogous to the ones proposed by Pahikkala et al. (2006a) for the basic RLS regression. We now briefly present the basic sparse RLS algorithm. Let m denote the training set size and M = 11, ... , m} an index s</context>
<context position="14300" citStr="Rifkin et al. (2003)" startWordPosition="2332" endWordPosition="2335">(xi, xj), i=1 jEB i,jEB where the first term is the squared loss function, the second term is the regularizer, and A E R+ is a regularization parameter. Note that all the training instances are used for determining the coefficient vector. The minimizer is obtained by solving the corresponding system of linear equations, which can be performed in O(m|B|2) time. We set the maximum number of basis vectors to 4000 in all experiments in this study. The subset is selected randomly when the training set size exceeds this number. Other methods for the selection of the basis vectors were considered by Rifkin et al. (2003), who however reported that the random selection worked as well as the more sophisticated approaches. 3 Experimental evaluation We next describe the evaluation resources and metrics used, provide a comprehensive evaluation of our method across five PPI corpora, and compare our results to earlier work. Further, we discuss the challenges inherent in providing a valid method evaluation and propose solutions. Ak X00 k=0 X |L| i=1 G&apos; i,jG&apos;&apos; i,j, |L| X j=1 4 Corpus Statistics P R Graph Kernel AUC UAUC Co-occ. #POS. #NEG. F OF P F AIMed 1000 4834 0.529 0.618 0.564 0.050 0.848 0.023 0.178 0.301 BioInf</context>
</contexts>
<marker>Rifkin, Yeo, Poggio, 2003</marker>
<rawString>Ryan Rifkin, Gene Yeo, and Tomaso Poggio, 2003. Regularized Least-squares Classi�cation, volume 190 of NATO Science Series III: Computer and System Sciences, chapter 7, pages 131–154. IOS Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rune Sætre</author>
<author>Kenji Sagae</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Syntactic features for protein-protein interaction extraction.</title>
<date>2008</date>
<booktitle>In Proceedings of LBM’07,</booktitle>
<volume>319</volume>
<pages>6--1</pages>
<contexts>
<context position="23406" citStr="Sætre et al., 2008" startWordPosition="3842" endWordPosition="3845"> comparisons in terms of F-score essentially meaningless. As F-score is typically the only metric for which results are reported in the PPI extraction literature, we are limited to comparing against results on single corpora. We consider the AImed and BioInfer evaluations to be the most relevant ones, as these corpora are sufficiently large for training and reliably testing machine learning methods. As the present study is, to the best of our knowl6 P R F (Giuliano et al., 2006) 60.9% 57.2% 59.0% All-dependency-paths graph kernel 52.9% 61.8% 56.4% (Bunescu and Mooney, 2006) 65.0% 46.4% 54.2% (Sætre et al., 2008) 64.3% 44.1% 52.0% (Mitsumori et al., 2006) 54.2% 42.6% 47.7% (Yakushiji et al., 2005) 33.7% 33.1% 33.4% Table 2: (P)recision, (R)ecall and (F)-score results for methods evaluated on AImed with the correct cross-validation methodology. edge, the first to report machine learning method performance on BioInfer, we will focus on AImed in the following comparison. Second, the cross-validation strategy used in evaluation has a large impact on measured performance. In earlier system evaluations, two major strategies for defining the splits used in cross-validation can be observed. The approach used </context>
<context position="25016" citStr="Sætre et al. (2008)" startWordPosition="4108" endWordPosition="4111">onsider two interaction candidates extracted from the same sentence, e.g. from a statement of the form “P1 and P2 [... ] P3”, where “[... ]” is any statement of interaction or noninteraction. Due to the near-identity of contexts, a machine learning method will easily learn to predict that the label of the pair (P1, P2) should match that of (P1, P3). However, such “learning” will clearly not generalize. This approach must thus be considered invalid, because allowing pairs generated from same sentences to appear in different folds leads to an information leak between the training and test sets. Sætre et al. (2008) observed that adopting the latter cross-validation strategy on AImed could lead up to 18 F-score percentage unit overestimation of performance. For this reason, we will not consider results listed in the “False 10-fold cross-validation” table (2b) of Sætre et al. (2008). With these restrictions in place, we now turn to comparison with relevant results reported in related research, summarized in Table 2. We note that Bunescu and Mooney (2006) only applied evaluation criteria where it is enough to extract only one occurrence of each mention of an interaction from each abstract, while the other </context>
<context position="26363" citStr="Sætre et al. (2008)" startWordPosition="4327" endWordPosition="4330">aluation of Giuliano et al. (2006) includes both alternatives, and their method achieves an F-score of 63.9% under the former criterion, which they term One Answer per Relation in a given Document (OARD). Our method outperforms most studies using similar evaluation methodology, with the exception being the approach of Giuliano et al. (2006). This result is somewhat surprising, as the method proposed by Giuliano does not apply any form of parsing but relies instead only on the sequential order of the words. This brings us to our third point regarding comparability of methods. As pointed out by Sætre et al. (2008), the AImed corpus allows remarkably different “interpretations” regarding the number of interacting and non-interacting pairs. For example, where we have identified 1000 interacting and 4834 non-interacting protein pairs in AImed, in the data used by Giuliano there are eight more interacting and 200 fewer noninteracting pairs. The corpus can also be preprocessed in a number of ways. In particular we noticed that whereas protein names are always blinded in our data, in the data used by Giuliano protein names are sometimes partly left visible. As Giuliano has generously made his method implemen</context>
</contexts>
<marker>Sætre, Sagae, Tsujii, 2008</marker>
<rawString>Rune Sætre, Kenji Sagae, and Jun’ichi Tsujii. 2008. Syntactic features for protein-protein interaction extraction. In Proceedings of LBM’07, volume 319, pages 6.1–6.14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akane Yakushiji</author>
<author>Yusuke Miyao</author>
<author>Yuka Tateisi</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Biomedical information extraction with predicate-argument structure patterns.</title>
<date>2005</date>
<booktitle>In Proceedings of SMBM’05,</booktitle>
<pages>60--69</pages>
<contexts>
<context position="23492" citStr="Yakushiji et al., 2005" startWordPosition="3856" endWordPosition="3859">the only metric for which results are reported in the PPI extraction literature, we are limited to comparing against results on single corpora. We consider the AImed and BioInfer evaluations to be the most relevant ones, as these corpora are sufficiently large for training and reliably testing machine learning methods. As the present study is, to the best of our knowl6 P R F (Giuliano et al., 2006) 60.9% 57.2% 59.0% All-dependency-paths graph kernel 52.9% 61.8% 56.4% (Bunescu and Mooney, 2006) 65.0% 46.4% 54.2% (Sætre et al., 2008) 64.3% 44.1% 52.0% (Mitsumori et al., 2006) 54.2% 42.6% 47.7% (Yakushiji et al., 2005) 33.7% 33.1% 33.4% Table 2: (P)recision, (R)ecall and (F)-score results for methods evaluated on AImed with the correct cross-validation methodology. edge, the first to report machine learning method performance on BioInfer, we will focus on AImed in the following comparison. Second, the cross-validation strategy used in evaluation has a large impact on measured performance. In earlier system evaluations, two major strategies for defining the splits used in cross-validation can be observed. The approach used by Bunescu and Mooney (2006), which we consider the correct one, is to split the data </context>
</contexts>
<marker>Yakushiji, Miyao, Tateisi, Tsujii, 2005</marker>
<rawString>Akane Yakushiji, Yusuke Miyao, Yuka Tateisi, and Jun’ichi Tsujii. 2005. Biomedical information extraction with predicate-argument structure patterns. In Proceedings of SMBM’05, pages 60–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Zelenko</author>
<author>Chinatsu Aone</author>
<author>Anthony Richardella</author>
</authors>
<title>Kernel methods for relation extraction.</title>
<date>2003</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>3--1083</pages>
<marker>Zelenko, Aone, Richardella, 2003</marker>
<rawString>Dmitry Zelenko, Chinatsu Aone, and Anthony Richardella. 2003. Kernel methods for relation extraction. J. Mach. Learn. Res., 3:1083–1106.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>