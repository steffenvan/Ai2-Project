<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.024372">
<title confidence="0.9968645">
Designing a Task-Based Evaluation Methodology
for a Spoken Machine Translation System
</title>
<author confidence="0.967299">
Kavita Thomas
</author>
<affiliation confidence="0.977089">
Language Technologies Institute
Carnegie Mellon University
</affiliation>
<address confidence="0.7663035">
5000 Forbes Avenue
Pittsburgh, PA 15213, USA
</address>
<email confidence="0.998704">
kavita@cs.cmu.edu
</email>
<sectionHeader confidence="0.997389" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999959">
In this paper, I discuss issues pertinent to the
design of a task-based evaluation methodology
for a spoken machine translation (MT) sys-
tem processing human to human communica-
tion rather than human to machine communi-
cation. I claim that system mediated human to
human communication requires new evaluation
criteria and metrics based on goal complexity
and the speaker&apos;s prioritization of goals.
</bodyText>
<sectionHeader confidence="0.999391" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999933029850746">
Task-based evaluations for spoken language sys-
tems focus on evaluating whether the speaker&apos;s
task is achieved, rather than evaluating utter-
ance translation accuracy or other aspects of
system performance. Our MT project focuses
on the travel reservation domain and facilitates
on-line translation of speech between clients and
travel agents arranging travel plans. Our prior
evaluations (Gates et al., 1996) have focused
on end-to-end translation accuracy at the ut-
terance level (i.e., fraction of utterances trans-
lated perfectly, acceptably, and unacceptably).
While this method of evaluation conveys trans-
lation accuracy, it does not give any information
about how many of the client&apos;s travel arrange-
ment goals have been conveyed, nor does it take
into account the complexity of the speaker&apos;s
goals and task, or the priority that they assign
to their goals; for example, the same end-to-end
score for two dialogues may hide the fact that
in one dialogue the speakers were able to com-
municate their most important goals while in
the other they were only able to communicate
successfully the less important goals.
One common approach to evaluating spoken
language systems focusing on human-machine
dialogue is to compare system responses to cor-
rect reference answers; however, as discussed
by (Walker et al., 1997), the set of reference
answers for any particular user query is tied
to the system&apos;s dialogue strategy. Evaluation
methods independent of dialogue strategy have
focused on measuring the extent to which sys-
tems for interactive problem solving aid users
via log-file evaluations (Polifroni et al., 1992),
quantifying repair attempts via turn correction
ratio, tracking user detection and correction of
system errors (Hirschman and Pao, 1993), and
considering transaction success (Shriberg et al.,
1992). (Danieli and Gerbino, 1995) measure
the dialogue module&apos;s ability to recover from
partial failures of recognition or understanding
(i.e., implicit recovery) and inappropriate utter-
ance ratio; (Simpson and Fraser, 1993) discuss
applying turn correction ratio, transaction suc-
cess, and contextual appropriateness to dialogue
evaluations, and (Hirschman et al., 1990) dis-
cuss using task completion time as a black box
evaluation metric.
Current literature on task-based evaluation
methodologies for spoken language systems pri-
marily focuses on human-computer interactions
rather than system-mediated human-human in-
teractions. For a multilingual MT system,
speakers communicate via the system, which
translates their responses and generates the out-
put in the target language via speech synthesis.
Measuring solution quality (Sikorski and Allen,
1995), transaction success, or contextual appro-
priateness is meaningless, since we are not in-
terested in measuring how efficient travel agents
are in responding to clients&apos; queries, but rather,
how well the system conveys the speakers&apos; goals.
Likewise, task completion time will not cap-
ture task success for MT dialogues since it is
dependent on dialogue strategies and speaker
styles. Task-based evaluation methodologies for
</bodyText>
<page confidence="0.997137">
569
</page>
<bodyText confidence="0.9829993125">
MT systems must focus on whether goals are
communicated, rather than whether they are
achieved.
2 Goals of a Task-Based Evaluation
Methodology for an MT System
The goal of a task-based evaluation for an MT
system is to convey whether speakers&apos; goals
were translated correctly. An advantage of fo-
cusing on goal translation is that it allows us to
compare dialogues where the speakers employ
different dialogue strategies. In our project, we
focus on three issues in goal communication:
(1) distinction of goals based on subgoal com-
plexity, (2) distinction of goals based on the
speaker&apos;s prioritization, and (3) distinction of
goals based on domain.
</bodyText>
<sectionHeader confidence="0.864408" genericHeader="method">
3 Prioritization of Goals
</sectionHeader>
<bodyText confidence="0.999596833333333">
While we want to evaluate whether speakers&apos;
important goals are translated correctly, this is
sometimes difficult to ascertain, since not only
must the speaker&apos;s goals be concisely describ-
able and circumscribable, but also they must
not change while she is attempting to achieve
her task. Speakers usually have a prioritization
of goals that cannot be predicted in advance and
which differs between speakers; for example, if
one client wants to book a trip to Tokyo, it may
be imperative for him to book the flight tickets
at the least, while reserving rooms in a hotel
might be of secondary importance, and finding
out about sights in Tokyo might be of lowest
priority. However, his goals could be prioritized
in the opposite order, or could change if he finds
one goal too difficult to communicate and aban-
dons it in frustration.
If we insist on eschewing unreliability issues
inherent in asking the client about the priority
of his goals after the dialogue has terminated
(and he has perhaps forgotten his earlier prior-
ity assignment), we cannot rely on an invariant
prioritization of goals across speakers or across
a dialogue. The only way we can predict the
speaker&apos;s goals at the time he is trying to com-
municate them is in cases where his goals are not
communicated and he attempts to repair them.
We can distinguish between cases in which, goal
communication succeeds or fails, and we can
count the number of repair attempts in both
cases. The insight is that speakers will attempt
to repair higher priority goals more than lower
priority goals, which they will abandon sooner.
The number of repair attempts per goal quan-
tifies the speaker&apos;s priority per goal to some de-
gree.
We can capture this information in a sim-
ple metric that distinguishes between goals that
eventually succeed or fail with at least one re-
pair attempt. Goals that eventually succeed
with tg repair attempts can be given a score
of 1/t9, which has a maximum score of 1 when
there is only one repair attempt, and decays to
0 as the number of repair attempts goes to infin-
ity. Similarly, we can give a score of —(1 — 1/tg)
to goals that are eventually abandoned with tg
repair attempts; this has a maximum of 0 when
there is only a single repair attempt and goes
to —1 as tg goes to infinity. So the overall dia-
logue score becomes the average over all goals of
the difference between these two metrics, with
a maximum score of 1 and a minimum score of
—1.
</bodyText>
<equation confidence="0.965456">
{score(goal)
—(1 —
score(dialogue)
</equation>
<sectionHeader confidence="0.899826" genericHeader="method">
4 Complexity of Goals
</sectionHeader>
<bodyText confidence="0.999953285714286">
Another factor to be considered is goal com-
plexity; clearly we want to distinguish between
dialogues with the same main goals but in which
some have many subgoals while others have few
subgoals with little elaboration. For instance,
one traveller going to Tokyo may be satisfied
with simply specifying his departure and arrival
times for the outgoing and return laps of his
flight, while another may have the additional
subgoals of wanting a two-day stopover in Lon-
don, vegetarian meals, and aisle seating in the
non-smoking section. In the metric above, both
goals and subgoals are treated in the same way
(i.e., the sum over goals includes subgoals), and
we are not weighting their scores any differently.
While many subgoals require that the main
goal they fall under be communicated for them
to be communicated, it is also true that for some
speakers, communicating just the main goal and
not the subgoal may be a communication fail-
ure. For example, if it is crucial for a speaker
</bodyText>
<figure confidence="0.978462428571429">
Escore(goal) (2)
numgoals
goals
for successful goal
for unsuccessful goal
1
(1)
</figure>
<page confidence="0.978185">
570
</page>
<bodyText confidence="0.999956289473684">
to get a stopover in London, even if his main
goal (requesting a return flight from New York
to Tokyo) is successfully communicated, he will
view the communication attempt a failure un-
less the system communicates the stopover suc-
cessfully also. On the other hand, communi-
cating the subgoal (e.g., a stopover in London),
without communicating the main goal is non-
sensical — the travel agent will not know what
to make of &amp;quot;a stopover in London&amp;quot; without the
accompanying main goal requesting the flight to
Tokyo.
However, even if two dialogues have the same
goals and subgoals, the complexity of the trans-
lation task may differ; for example, if in one
dialogue (A) the speaker communicates a single
goal or subgoal per speaker turn, while in the
other (B) the speaker communicates the goal
and all its subgoals in the same speaker turn,
it is clear that the dialogue in which the entire
goal structure is conveyed in the same speaker
turn will be the more difficult translation task.
We need to be able to account for the average
goal complexity per speaker turn in a dialogue
and scale the above metric accordingly; if dia-
logues A and B have the same score according
to the given metric, we should boost the score
of B to reflect that it has required a more rigor-
ous translation effort. A first attempt would be
to simply multiply the score of the dialogue by
the average subgoal complexity per main goal
per speaker turn in the dialogue, where Nmg is
the number of main goals in a speaker turn and
Nsg is the number of subgoals. In the metric
below, the average subgoal complexity is 1 for
speaker turns in which there are no subgoals,
and increases as the number of subgoals in the
speaker turn increases.
</bodyText>
<subsubsectionHeader confidence="0.531081">
spkturns Nmg
</subsubsectionHeader>
<sectionHeader confidence="0.9750255" genericHeader="method">
5 Our Task-Based Evaluation
Methodology
</sectionHeader>
<bodyText confidence="0.999852410714286">
Scoring a dialogue is a coding task; scorers will
need to be able to distinguish goals and subgoals
in the domain. We want to minimize train-
ing for scorers while maximizing agreement be-
tween them. To do so, we list a predefined set
of main goals (e.g., making flight arrangements
or hotel bookings) and group together all sub-
goals that pertain to these main goals in a two-
level tree. Although this formalization sacrifices
subgoal complexity, we are unable to determine
this without predefining a subgoal hierarchy and
we want to avoid predefining subgoal priority,
which is set by assigning a subgoal hierarchy.
After familiarizing themselves with the set of
main goals and their accompanying subgoals,
scorers code a dialogue by distinguishing in a
speaker turn between the main goals and sub-
goals, whether they are successfully communi-
cated or not, and the number of repair attempts
in successive speaker turns. Scorers must also
indicate which domain each goal falls under; we
distinguish goals as in-domain (i.e., referring to
the travel-reservation domain), out-of-domain
(i.e., unrelated to the task in any way), and
cross-domain (i.e., discussing the weather, com-
mon polite phrases, accepting, negating, open-
ing or closing the dialogue, or asking for re-
peats).
The distinction between domains is impor-
tant in that we can separate in-domain goals
from cross-domain goals; cross-domain goals of-
ten serve a meta-level purpose in the dialogue.
We can thus evaluate performance over all goals
while maintaining a clear performance measure
for in-domain goals. Scores should be calculated
separately based on domain, since this will indi-
cate system performance more specifically, and
provide a useful metric for grammar develop-
ers to compare subsequent and current domain
scores for dialogues from a given scenario.
In a large scale evaluation, multiple pairs of
speakers will be given the same scenario (i.e., a
specific task to try and accomplish; e.g., flying
to Frankfurt, arranging a stay there for 2 nights,
sightseeing to the museums, then flying on to
Tokyo); domain scores will then be calculated
and averaged over all speakers.
Actual evaluation is performed on transcripts
of dialogues labelled with information from sys-
tem logs; this enables us to see the original ut-
terance (human transcription) and evaluate the
correctness of the target output. If we wish
to, log-file evaluations also permit us to eval-
uate the system in a glass-box approach, evalu-
ating individual system components separately
(Simpson and Fraser, 1993).
</bodyText>
<equation confidence="0.7435575">
score&apos; (dialogue) = score(dialogue) *
1 Nsg + Nmg
E [ (3)
numspkturns
</equation>
<page confidence="0.995705">
571
</page>
<sectionHeader confidence="0.994513" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.99999795">
This work describes an initial attempt to ac-
count for some of the significant issues in a task-
based evaluation methodology for an MT sys-
tem. Our choice of metric reflects separate do-
main scores, factors in subgoal complexity and
normalizes all counts to allow for comparison
among dialogues that differ in dialogue strat-
egy, subgoal complexity, number of goals and
speaker-prioritization of goals. The proposed
metric is a first attempt, and describes work in
progress; we have attempted to present the sim-
plest possible metric as an initial approach.
There are many issues that need to be ad-
dressed; for instance, we do not take into ac-
count optimality of translations. Although we
are interested in goal communication and not
utterance translation quality, the disadvantage
to the current approach is that our optimality
measure is binary, and does not give any infor-
mation about how well-phrased the translated
text is. More significantly, we have not resolved
whether to use metric (1) for both subgoals and
goals together, or to score them separately. The
proposed metric does not reflect that commu-
nicating main goals may be essential to com-
municating their subgoals. It also does not ac-
count for the possible complexity introduced by
multiple main goals per speaker turn. We also
do not account for the possibility that in an
unsuccessful dialogue, a speaker may become
more frustrated as the dialogue proceeds, and
her relative goal priorities may no longer be re-
flected in the number of repair attempts. We
may also want to further distinguish in-domain
scores based on sub-domain (e.g., flights, ho-
tels, events). Perhaps most importantly, we still
need to conduct a full-scale evaluation with the
above metric with several scorers and speaker
pairs across different versions of the system to
be able to provide actual results.
</bodyText>
<sectionHeader confidence="0.998853" genericHeader="acknowledgments">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.99723175">
I would like to thank my advisor Lori Levin,
Alon Lavie, Monika Woszczyna, and Aleksan-
dra Slavkovic for their help and suggestions with
this work.
</bodyText>
<sectionHeader confidence="0.996718" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.77738612244898">
M.Danieli and E.Gerbino. 1995. Metrics for
evaluating dialogue strategies in a spoken Ian-
guage system. In Proceeedings of the 1995
AAAI Spring Symposium on Empirical Meth-
ods in Discourse Interpretation and Genera-
tion, pages 34-39.
L.Hirschman, D.Dahl, D.P.McKay,
L.M.Norton, M.C.Linebarger. 1990. Be-
yond class A: A proposal for automatic
evaluation of discourse. In Proceedings of
the Speech and Natural Language Workshop,
pages 109-113.
L.Hirschman and C.Pao. 1993. The cost of er-
rors in a spoken language system. In Pro-
ceedings of the Third European Conference
on Speech Communication and Technology,
pages 1419-1422.
J.Polifroni, L.Hirschman, S.Seneff, and V.Zue.
1992. Experiments in evaluating interactive
spoken language systems. In Proceedings of
the DARPA Speech and NL Workshop, pages
28-31.
E.Shriberg, E.Wade, and P.Price. 1992.
Human-machine problem solving using spo-
ken language systems (sls): Factors affecting
performance and user satisfaction. In Pro-
ceedings of the DARPA Speech and NL Work-
shop, pages 49-54.
T.Sikorski and J.Allen. 1995. A task-
based evaluation of the TRAINS-95 dia-
logue system. Technical report, University of
Rochester.
A.Simpson, and N.A.Fraser. 1993. Black box
and glass box evaluation of the SUNDIAL
system. In Proceedings of the Third Euro-
pean Conference on Speech Communication
and Technology, pages 1423-1426.
M.Walker, D.J.Litman, C.A.Kamm, and
A.Abella. 1997. PARADISE: A framework
for evaluating spoken dialogue agents. Tech-
nical Report TR 97.26.1, AT and T Technical
Reports.
D.Gates, A.Lavie, L.Levin, A.Waibel,
M.Gavalda, L.Mayfield, M.Woszczyna,
P.Zhan. 1996. End-to-end Evaluation in
JANUS: a Speech-to-speech Translation
System. In Proceedings of the 12th Euro-
pean Conference on Artificial Intelligence,
Workshop on Dialogue, Budapest, Hungary.
</reference>
<page confidence="0.997413">
572
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.939730">
<title confidence="0.99965">Designing a Task-Based Evaluation Methodology for a Spoken Machine Translation System</title>
<author confidence="0.95212">Kavita Thomas</author>
<affiliation confidence="0.9969485">Language Technologies Institute Carnegie Mellon University</affiliation>
<address confidence="0.9980915">5000 Forbes Avenue Pittsburgh, PA 15213, USA</address>
<email confidence="0.999847">kavita@cs.cmu.edu</email>
<abstract confidence="0.9995114">In this paper, I discuss issues pertinent to the design of a task-based evaluation methodology for a spoken machine translation (MT) system processing human to human communication rather than human to machine communication. I claim that system mediated human to human communication requires new evaluation criteria and metrics based on goal complexity and the speaker&apos;s prioritization of goals.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Danieli</author>
<author>E Gerbino</author>
</authors>
<title>Metrics for evaluating dialogue strategies in a spoken Ianguage system.</title>
<date>1995</date>
<booktitle>In Proceeedings of the 1995 AAAI Spring Symposium on Empirical Methods in Discourse Interpretation and Generation,</booktitle>
<pages>34--39</pages>
<contexts>
<context position="2484" citStr="Danieli and Gerbino, 1995" startWordPosition="368" endWordPosition="371">are system responses to correct reference answers; however, as discussed by (Walker et al., 1997), the set of reference answers for any particular user query is tied to the system&apos;s dialogue strategy. Evaluation methods independent of dialogue strategy have focused on measuring the extent to which systems for interactive problem solving aid users via log-file evaluations (Polifroni et al., 1992), quantifying repair attempts via turn correction ratio, tracking user detection and correction of system errors (Hirschman and Pao, 1993), and considering transaction success (Shriberg et al., 1992). (Danieli and Gerbino, 1995) measure the dialogue module&apos;s ability to recover from partial failures of recognition or understanding (i.e., implicit recovery) and inappropriate utterance ratio; (Simpson and Fraser, 1993) discuss applying turn correction ratio, transaction success, and contextual appropriateness to dialogue evaluations, and (Hirschman et al., 1990) discuss using task completion time as a black box evaluation metric. Current literature on task-based evaluation methodologies for spoken language systems primarily focuses on human-computer interactions rather than system-mediated human-human interactions. For </context>
</contexts>
<marker>Danieli, Gerbino, 1995</marker>
<rawString>M.Danieli and E.Gerbino. 1995. Metrics for evaluating dialogue strategies in a spoken Ianguage system. In Proceeedings of the 1995 AAAI Spring Symposium on Empirical Methods in Discourse Interpretation and Generation, pages 34-39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Dahl L Hirschman</author>
<author>L M Norton D P McKay</author>
<author>M C Linebarger</author>
</authors>
<title>Beyond class A: A proposal for automatic evaluation of discourse.</title>
<date>1990</date>
<booktitle>In Proceedings of the Speech and Natural Language Workshop,</booktitle>
<pages>109--113</pages>
<contexts>
<context position="2821" citStr="Hirschman et al., 1990" startWordPosition="413" endWordPosition="416">id users via log-file evaluations (Polifroni et al., 1992), quantifying repair attempts via turn correction ratio, tracking user detection and correction of system errors (Hirschman and Pao, 1993), and considering transaction success (Shriberg et al., 1992). (Danieli and Gerbino, 1995) measure the dialogue module&apos;s ability to recover from partial failures of recognition or understanding (i.e., implicit recovery) and inappropriate utterance ratio; (Simpson and Fraser, 1993) discuss applying turn correction ratio, transaction success, and contextual appropriateness to dialogue evaluations, and (Hirschman et al., 1990) discuss using task completion time as a black box evaluation metric. Current literature on task-based evaluation methodologies for spoken language systems primarily focuses on human-computer interactions rather than system-mediated human-human interactions. For a multilingual MT system, speakers communicate via the system, which translates their responses and generates the output in the target language via speech synthesis. Measuring solution quality (Sikorski and Allen, 1995), transaction success, or contextual appropriateness is meaningless, since we are not interested in measuring how effi</context>
</contexts>
<marker>Hirschman, McKay, Linebarger, 1990</marker>
<rawString>L.Hirschman, D.Dahl, D.P.McKay, L.M.Norton, M.C.Linebarger. 1990. Beyond class A: A proposal for automatic evaluation of discourse. In Proceedings of the Speech and Natural Language Workshop, pages 109-113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Hirschman</author>
<author>C Pao</author>
</authors>
<title>The cost of errors in a spoken language system.</title>
<date>1993</date>
<booktitle>In Proceedings of the Third European Conference on Speech Communication and Technology,</booktitle>
<pages>1419--1422</pages>
<contexts>
<context position="2394" citStr="Hirschman and Pao, 1993" startWordPosition="356" endWordPosition="359">oach to evaluating spoken language systems focusing on human-machine dialogue is to compare system responses to correct reference answers; however, as discussed by (Walker et al., 1997), the set of reference answers for any particular user query is tied to the system&apos;s dialogue strategy. Evaluation methods independent of dialogue strategy have focused on measuring the extent to which systems for interactive problem solving aid users via log-file evaluations (Polifroni et al., 1992), quantifying repair attempts via turn correction ratio, tracking user detection and correction of system errors (Hirschman and Pao, 1993), and considering transaction success (Shriberg et al., 1992). (Danieli and Gerbino, 1995) measure the dialogue module&apos;s ability to recover from partial failures of recognition or understanding (i.e., implicit recovery) and inappropriate utterance ratio; (Simpson and Fraser, 1993) discuss applying turn correction ratio, transaction success, and contextual appropriateness to dialogue evaluations, and (Hirschman et al., 1990) discuss using task completion time as a black box evaluation metric. Current literature on task-based evaluation methodologies for spoken language systems primarily focuses</context>
</contexts>
<marker>Hirschman, Pao, 1993</marker>
<rawString>L.Hirschman and C.Pao. 1993. The cost of errors in a spoken language system. In Proceedings of the Third European Conference on Speech Communication and Technology, pages 1419-1422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Hirschman J Polifroni</author>
<author>S Seneff</author>
<author>V Zue</author>
</authors>
<title>Experiments in evaluating interactive spoken language systems.</title>
<date>1992</date>
<booktitle>In Proceedings of the DARPA Speech and NL Workshop,</booktitle>
<pages>28--31</pages>
<contexts>
<context position="2256" citStr="Polifroni et al., 1992" startWordPosition="337" endWordPosition="340">e their most important goals while in the other they were only able to communicate successfully the less important goals. One common approach to evaluating spoken language systems focusing on human-machine dialogue is to compare system responses to correct reference answers; however, as discussed by (Walker et al., 1997), the set of reference answers for any particular user query is tied to the system&apos;s dialogue strategy. Evaluation methods independent of dialogue strategy have focused on measuring the extent to which systems for interactive problem solving aid users via log-file evaluations (Polifroni et al., 1992), quantifying repair attempts via turn correction ratio, tracking user detection and correction of system errors (Hirschman and Pao, 1993), and considering transaction success (Shriberg et al., 1992). (Danieli and Gerbino, 1995) measure the dialogue module&apos;s ability to recover from partial failures of recognition or understanding (i.e., implicit recovery) and inappropriate utterance ratio; (Simpson and Fraser, 1993) discuss applying turn correction ratio, transaction success, and contextual appropriateness to dialogue evaluations, and (Hirschman et al., 1990) discuss using task completion time</context>
</contexts>
<marker>Polifroni, Seneff, Zue, 1992</marker>
<rawString>J.Polifroni, L.Hirschman, S.Seneff, and V.Zue. 1992. Experiments in evaluating interactive spoken language systems. In Proceedings of the DARPA Speech and NL Workshop, pages 28-31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Wade E Shriberg</author>
<author>P Price</author>
</authors>
<title>Human-machine problem solving using spoken language systems (sls): Factors affecting performance and user satisfaction.</title>
<date>1992</date>
<booktitle>In Proceedings of the DARPA Speech and NL Workshop,</booktitle>
<pages>49--54</pages>
<marker>Shriberg, Price, 1992</marker>
<rawString>E.Shriberg, E.Wade, and P.Price. 1992. Human-machine problem solving using spoken language systems (sls): Factors affecting performance and user satisfaction. In Proceedings of the DARPA Speech and NL Workshop, pages 49-54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Sikorski</author>
<author>J Allen</author>
</authors>
<title>A taskbased evaluation of the TRAINS-95 dialogue system.</title>
<date>1995</date>
<tech>Technical report,</tech>
<institution>University of Rochester.</institution>
<contexts>
<context position="3303" citStr="Sikorski and Allen, 1995" startWordPosition="480" endWordPosition="483">scuss applying turn correction ratio, transaction success, and contextual appropriateness to dialogue evaluations, and (Hirschman et al., 1990) discuss using task completion time as a black box evaluation metric. Current literature on task-based evaluation methodologies for spoken language systems primarily focuses on human-computer interactions rather than system-mediated human-human interactions. For a multilingual MT system, speakers communicate via the system, which translates their responses and generates the output in the target language via speech synthesis. Measuring solution quality (Sikorski and Allen, 1995), transaction success, or contextual appropriateness is meaningless, since we are not interested in measuring how efficient travel agents are in responding to clients&apos; queries, but rather, how well the system conveys the speakers&apos; goals. Likewise, task completion time will not capture task success for MT dialogues since it is dependent on dialogue strategies and speaker styles. Task-based evaluation methodologies for 569 MT systems must focus on whether goals are communicated, rather than whether they are achieved. 2 Goals of a Task-Based Evaluation Methodology for an MT System The goal of a t</context>
</contexts>
<marker>Sikorski, Allen, 1995</marker>
<rawString>T.Sikorski and J.Allen. 1995. A taskbased evaluation of the TRAINS-95 dialogue system. Technical report, University of Rochester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Simpson</author>
<author>N A Fraser</author>
</authors>
<title>Black box and glass box evaluation of the SUNDIAL system.</title>
<date>1993</date>
<booktitle>In Proceedings of the Third European Conference on Speech Communication and Technology,</booktitle>
<pages>1423--1426</pages>
<contexts>
<context position="2675" citStr="Simpson and Fraser, 1993" startWordPosition="394" endWordPosition="397">trategy. Evaluation methods independent of dialogue strategy have focused on measuring the extent to which systems for interactive problem solving aid users via log-file evaluations (Polifroni et al., 1992), quantifying repair attempts via turn correction ratio, tracking user detection and correction of system errors (Hirschman and Pao, 1993), and considering transaction success (Shriberg et al., 1992). (Danieli and Gerbino, 1995) measure the dialogue module&apos;s ability to recover from partial failures of recognition or understanding (i.e., implicit recovery) and inappropriate utterance ratio; (Simpson and Fraser, 1993) discuss applying turn correction ratio, transaction success, and contextual appropriateness to dialogue evaluations, and (Hirschman et al., 1990) discuss using task completion time as a black box evaluation metric. Current literature on task-based evaluation methodologies for spoken language systems primarily focuses on human-computer interactions rather than system-mediated human-human interactions. For a multilingual MT system, speakers communicate via the system, which translates their responses and generates the output in the target language via speech synthesis. Measuring solution qualit</context>
<context position="12261" citStr="Simpson and Fraser, 1993" startWordPosition="1986" endWordPosition="1989">a specific task to try and accomplish; e.g., flying to Frankfurt, arranging a stay there for 2 nights, sightseeing to the museums, then flying on to Tokyo); domain scores will then be calculated and averaged over all speakers. Actual evaluation is performed on transcripts of dialogues labelled with information from system logs; this enables us to see the original utterance (human transcription) and evaluate the correctness of the target output. If we wish to, log-file evaluations also permit us to evaluate the system in a glass-box approach, evaluating individual system components separately (Simpson and Fraser, 1993). score&apos; (dialogue) = score(dialogue) * 1 Nsg + Nmg E [ (3) numspkturns 571 6 Conclusions and Future Work This work describes an initial attempt to account for some of the significant issues in a taskbased evaluation methodology for an MT system. Our choice of metric reflects separate domain scores, factors in subgoal complexity and normalizes all counts to allow for comparison among dialogues that differ in dialogue strategy, subgoal complexity, number of goals and speaker-prioritization of goals. The proposed metric is a first attempt, and describes work in progress; we have attempted to pre</context>
</contexts>
<marker>Simpson, Fraser, 1993</marker>
<rawString>A.Simpson, and N.A.Fraser. 1993. Black box and glass box evaluation of the SUNDIAL system. In Proceedings of the Third European Conference on Speech Communication and Technology, pages 1423-1426.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D J Litman M Walker</author>
<author>C A Kamm</author>
<author>A Abella</author>
</authors>
<title>PARADISE: A framework for evaluating spoken dialogue agents.</title>
<date>1997</date>
<tech>Technical Report TR 97.26.1, AT and T Technical Reports.</tech>
<contexts>
<context position="1955" citStr="Walker et al., 1997" startWordPosition="291" endWordPosition="294"> arrangement goals have been conveyed, nor does it take into account the complexity of the speaker&apos;s goals and task, or the priority that they assign to their goals; for example, the same end-to-end score for two dialogues may hide the fact that in one dialogue the speakers were able to communicate their most important goals while in the other they were only able to communicate successfully the less important goals. One common approach to evaluating spoken language systems focusing on human-machine dialogue is to compare system responses to correct reference answers; however, as discussed by (Walker et al., 1997), the set of reference answers for any particular user query is tied to the system&apos;s dialogue strategy. Evaluation methods independent of dialogue strategy have focused on measuring the extent to which systems for interactive problem solving aid users via log-file evaluations (Polifroni et al., 1992), quantifying repair attempts via turn correction ratio, tracking user detection and correction of system errors (Hirschman and Pao, 1993), and considering transaction success (Shriberg et al., 1992). (Danieli and Gerbino, 1995) measure the dialogue module&apos;s ability to recover from partial failures</context>
</contexts>
<marker>Walker, Kamm, Abella, 1997</marker>
<rawString>M.Walker, D.J.Litman, C.A.Kamm, and A.Abella. 1997. PARADISE: A framework for evaluating spoken dialogue agents. Technical Report TR 97.26.1, AT and T Technical Reports.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lavie D Gates</author>
<author>A Waibel L Levin</author>
<author>L Mayfield M Gavalda</author>
<author>P Zhan M Woszczyna</author>
</authors>
<title>End-to-end Evaluation in JANUS: a Speech-to-speech Translation System.</title>
<date>1996</date>
<booktitle>In Proceedings of the 12th European Conference on Artificial Intelligence, Workshop on Dialogue,</booktitle>
<location>Budapest, Hungary.</location>
<contexts>
<context position="1048" citStr="Gates et al., 1996" startWordPosition="146" endWordPosition="149">to machine communication. I claim that system mediated human to human communication requires new evaluation criteria and metrics based on goal complexity and the speaker&apos;s prioritization of goals. 1 Introduction Task-based evaluations for spoken language systems focus on evaluating whether the speaker&apos;s task is achieved, rather than evaluating utterance translation accuracy or other aspects of system performance. Our MT project focuses on the travel reservation domain and facilitates on-line translation of speech between clients and travel agents arranging travel plans. Our prior evaluations (Gates et al., 1996) have focused on end-to-end translation accuracy at the utterance level (i.e., fraction of utterances translated perfectly, acceptably, and unacceptably). While this method of evaluation conveys translation accuracy, it does not give any information about how many of the client&apos;s travel arrangement goals have been conveyed, nor does it take into account the complexity of the speaker&apos;s goals and task, or the priority that they assign to their goals; for example, the same end-to-end score for two dialogues may hide the fact that in one dialogue the speakers were able to communicate their most im</context>
</contexts>
<marker>Gates, Levin, Gavalda, Woszczyna, 1996</marker>
<rawString>D.Gates, A.Lavie, L.Levin, A.Waibel, M.Gavalda, L.Mayfield, M.Woszczyna, P.Zhan. 1996. End-to-end Evaluation in JANUS: a Speech-to-speech Translation System. In Proceedings of the 12th European Conference on Artificial Intelligence, Workshop on Dialogue, Budapest, Hungary.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>