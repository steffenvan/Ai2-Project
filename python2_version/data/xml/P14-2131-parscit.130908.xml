<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.044035">
<title confidence="0.983075">
Tailoring Continuous Word Representations for Dependency Parsing
</title>
<author confidence="0.982567">
Mohit Bansal Kevin Gimpel Karen Livescu
</author>
<affiliation confidence="0.97188">
Toyota Technological Institute at Chicago, IL 60637, USA
</affiliation>
<email confidence="0.994247">
{mbansal,kgimpel,klivescu}@ttic.edu
</email>
<sectionHeader confidence="0.993756" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999834764705882">
Word representations have proven useful
for many NLP tasks, e.g., Brown clusters
as features in dependency parsing (Koo et
al., 2008). In this paper, we investigate the
use of continuous word representations as
features for dependency parsing. We com-
pare several popular embeddings to Brown
clusters, via multiple types of features, in
both news and web domains. We find that
all embeddings yield significant parsing
gains, including some recent ones that can
be trained in a fraction of the time of oth-
ers. Explicitly tailoring the representations
for the task leads to further improvements.
Moreover, an ensemble of all representa-
tions achieves the best results, suggesting
their complementarity.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999907654545455">
Word representations derived from unlabeled text
have proven useful for many NLP tasks, e.g., part-
of-speech (POS) tagging (Huang et al., 2014),
named entity recognition (Miller et al., 2004),
chunking (Turian et al., 2010), and syntactic
parsing (Koo et al., 2008; Finkel et al., 2008;
T¨ackstr¨om et al., 2012). Most word representa-
tions fall into one of two categories. Discrete rep-
resentations consist of memberships in a (possibly
hierarchical) hard clustering of words, e.g., via k-
means or the Brown et al. (1992) algorithm. Con-
tinuous representations (or distributed representa-
tions or embeddings) consist of low-dimensional,
real-valued vectors for each word, typically in-
duced via neural language models (Bengio et al.,
2003; Mnih and Hinton, 2007) or spectral meth-
ods (Deerwester et al., 1990; Dhillon et al., 2011).
Koo et al. (2008) found improvement on in-
domain dependency parsing using features based
on discrete Brown clusters. In this paper, we ex-
periment with parsing features derived from con-
tinuous representations. We find that simple at-
tempts based on discretization of individual word
vector dimensions do not improve parsing. We
see gains only after first performing a hierarchi-
cal clustering of the continuous word vectors and
then using features based on the hierarchy.
We compare several types of continuous rep-
resentations, including those made available by
other researchers (Turian et al., 2010; Collobert et
al., 2011; Huang et al., 2012), and embeddings we
have trained using the approach of Mikolov et al.
(2013a), which is orders of magnitude faster than
the others. The representations exhibit different
characteristics, which we demonstrate using both
intrinsic metrics and extrinsic parsing evaluation.
We report significant improvements over our base-
line on both the Penn Treebank (PTB; Marcus et
al., 1993) and the English Web treebank (Petrov
and McDonald, 2012).
While all embeddings yield some parsing im-
provements, we find larger gains by tailoring them
to capture similarity in terms of context within
syntactic parses. To this end, we use two sim-
ple modifications to the models of Mikolov et al.
(2013a): a smaller context window, and condition-
ing on syntactic context (dependency links and la-
bels). Interestingly, the Brown clusters of Koo et
al. (2008) prove to be difficult to beat, but we find
that our syntactic tailoring can lead to embeddings
that match the parsing performance of Brown (on
all test sets) in a fraction of the training time. Fi-
nally, a simple parser ensemble on all the represen-
tations achieves the best results, suggesting their
complementarity for dependency parsing.
</bodyText>
<sectionHeader confidence="0.987531" genericHeader="method">
2 Continuous Word Representations
</sectionHeader>
<bodyText confidence="0.999566">
There are many ways to train continuous represen-
tations; in this paper, we are primarily interested
in neural language models (Bengio et al., 2003),
which use neural networks and local context to
learn word vectors. Several researchers have
made their trained representations publicly avail-
</bodyText>
<page confidence="0.982415">
809
</page>
<note confidence="0.94742725">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 809–815,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
Representation Source Corpus Types, Tokens V D Time
BROWN Koo et al. (2008) BLLIP 317K, 43M 316,710 – 2.5 days†
SENNA Collobert et al. (2011) Wikipedia 8.3M, 1.8B 130,000 50 2 months*
TURIAN Turian et al. (2010) RCV1 269K, 37M 268,810 50 few weeks*
HUANG Huang et al. (2012) Wikipedia 8.3M, 1.8B 100,232 50 —
CBOW, SKIP, SKIPDEP Mikolov et al. (2013a) BLLIP 317K, 43M 316,697 100 2-4 mins.†
</note>
<tableCaption confidence="0.993055">
Table 1: Details of word representations used, including datasets, vocabulary size V , and dimensionality D. Continuous
representations require an additional 4 hours to run hierarchical clustering to generate features (§3.2). RCV1 = Reuters Corpus,
Volume 1. ∗ = time reported by authors. † = run by us on a 3.50 GHz desktop, using a single thread.
</tableCaption>
<bodyText confidence="0.999979142857143">
able, which we use directly in our experiments.
In particular, we use the SENNA embeddings of
Collobert et al. (2011); the scaled TURIAN em-
beddings (C&amp;W) of Turian et al. (2010); and the
HUANG global-context, single-prototype embed-
dings of Huang et al. (2012). We also use the
BROWN clusters trained by Koo et al. (2008). De-
tails are given in Table 1.
Below, we describe embeddings that we train
ourselves (§2.1), aiming to make them more useful
for parsing via smaller context windows (§2.1.1)
and conditioning on syntactic context (§2.1.2). We
then compare the representations using two intrin-
sic metrics (§2.2).
</bodyText>
<subsectionHeader confidence="0.979866">
2.1 Syntactically-tailored Representations
</subsectionHeader>
<bodyText confidence="0.9999874">
We train word embeddings using the continu-
ous bag-of-words (CBOW) and skip-gram (SKIP)
models described in Mikolov et al. (2013a;
2013b) as implemented in the open-source toolkit
word2vec. These models avoid hidden layers
in the neural network and hence can be trained
in only minutes, compared to days or even weeks
for the others, as shown in Table 1.1 We adapt
these embeddings to be more useful for depen-
dency parsing in two ways, described next.
</bodyText>
<subsectionHeader confidence="0.762079">
2.1.1 Smaller Context Windows
</subsectionHeader>
<bodyText confidence="0.999392">
The CBOW model learns vectors to predict a
word given its set of surrounding context words
in a window of size w. The SKIP model learns
embeddings to predict each individual surround-
ing word given one particular word, using an anal-
ogous window size w. We find that w affects
the embeddings substantially: with large w, words
group with others that are topically-related; with
small w, grouped words tend to share the same
POS tag. We discuss this further in the intrinsic
evaluation presented in §2.2.
</bodyText>
<footnote confidence="0.848287666666667">
1We train both models on BLLIP (LDC2000T43) with
PTB removed, the same corpus used by Koo et al. (2008) to
train their BROWN clusters. We created a special vector for
unknown words by averaging the vectors for the 50K least
frequent words; we did not use this vector for the SKIPDEP
(§2.1.2) setting because it performs slightly better without it.
</footnote>
<subsectionHeader confidence="0.759198">
2.1.2 Syntactic Context
</subsectionHeader>
<bodyText confidence="0.999421275862069">
We expect embeddings to help dependency pars-
ing the most when words that have similar parents
and children are close in the embedding space. To
target this type of similarity, we train the SKIP
model on dependency context instead of the linear
context in raw text. When ordinarily training SKIP
embeddings, words v&apos; are drawn from the neigh-
borhood of a target word v, and the sum of log-
probabilities of each v&apos; given v is maximized. We
propose to instead choose v&apos; from the set contain-
ing the grandparent, parent, and children words of
v in an automatic dependency parse.
A simple way to implement this idea is to train
the original SKIP model on a corpus of depen-
dency links and labels. For this, we parse the
BLLIP corpus (minus PTB) using our baseline de-
pendency parser, then build a corpus in which each
line contains a single child word c, its parent word
p, its grandparent g, and the dependency label f of
the (c, p) link:
“f&lt;L&gt; g&lt;G&gt; p c f&lt;L&gt;”,
that is, both the dependency label and grandparent
word are subscripted with a special token to avoid
collision with words.2 We train the SKIP model on
this corpus of tuples with window size w = 1, de-
noting the result SKIPDEP. Note that this approach
needs a parsed corpus, but there also already ex-
ist such resources (Napoles et al., 2012; Goldberg
and Orwant, 2013).
</bodyText>
<subsectionHeader confidence="0.993224">
2.2 Intrinsic Evaluation of Representations
</subsectionHeader>
<bodyText confidence="0.997848923076923">
Short of running end-to-end parsing experiments,
how can we choose which representations to use
for parsing tasks? Several methods have been pro-
posed for intrinsic evaluation of word representa-
2We use a subscript on g so that it will be treated dif-
ferently from c when considering the context of p. We re-
moved all g&lt;G&gt; from the vocabulary after training. We also
tried adding information about POS tags. This increases M-1
(§2.2), but harms parsing performance, likely because the em-
beddings become too tag-like. Similar ideas have been used
for clustering (Sagae and Gordon, 2009; Haffari et al., 2011;
Grave et al., 2013), semantic space models (Pad´o and Lapata,
2007), and topic modeling (Boyd-Graber and Blei, 2008).
</bodyText>
<page confidence="0.989703">
810
</page>
<table confidence="0.943152">
Representation SIM M-1
BROWN – 89.3
SENNA 49.8 85.2
TURIAN 29.5 87.2
HUANG 62.6 78.1
CBOW, w = 2 34.7 84.8
SKIP, w = 1 37.8 86.6
SKIP, w = 2 43.1 85.8
SKIP, w = 5 44.4 81.1
SKIP, w = 10 44.6 71.5
SKIPDEP 34.6 88.3
</table>
<tableCaption confidence="0.8766162">
Table 2: Intrinsic evaluation of representations. SIM column
has Spearman’s p × 100 for 353-pair word similarity dataset.
M-1 is our unsupervised POS tagging metric. For BROWN,
M-1 is simply many-to-one accuracy of the clusters. Best
score in each column is bold.
</tableCaption>
<bodyText confidence="0.999746285714286">
tions; we discuss two here:
Word similarity (SIM): One widely-used evalu-
ation compares distances in the continuous space
to human judgments of word similarity using the
353-pair dataset of Finkelstein et al. (2002). We
compute cosine similarity between the two vectors
in each word pair, then order the word pairs by
similarity and compute Spearman’s rank correla-
tion coefficient (p) with the gold similarities. Em-
beddings with high p capture similarity in terms of
paraphrase and topical relationships.
Clustering-based tagging accuracy (M-1): In-
tuitively, we expect embeddings to help parsing
the most if they can tell us when two words are
similar syntactically. To this end, we use a met-
ric based on unsupervised evaluation of POS tag-
gers. We perform clustering and map each cluster
to one POS tag so as to maximize tagging accu-
racy, where multiple clusters can map to the same
tag. We cluster vectors corresponding to the to-
kens in PTB WSJ sections 00-21.3
Table 2 shows these metrics for representations
used in this paper. The BROWN clusters have
the highest M-1, indicating high cluster purity in
terms of POS tags. The HUANG embeddings have
the highest SIM score but low M-1, presumably
because they were trained with global context,
making them more tuned to capture topical sim-
ilarity. We compare several values for the win-
dow size (w) used when training the SKIP embed-
dings, finding that small w leads to higher M-1 and
lower SIM. Table 3 shows examples of clusters
obtained by clustering SKIP embeddings of w = 1
versus w = 10, and we see that the former cor-
respond closely to POS tags, while the latter are
</bodyText>
<footnote confidence="0.996319">
3For clustering, we use k-means with k = 1000 and ini-
tialize by placing centroids on the 1000 most-frequent words.
</footnote>
<table confidence="0.9968355">
w Example clusters
1 [Mr., Mrs., Ms., Prof., ...], [Jeffrey, Dan, Robert,
Peter, ...], [Johnson, Collins, Schmidt, Freedman,
...], [Portugal, Iran, Cuba, Ecuador,...], [CST, 4:30,
9-10:30, CDT, ...], [his, your, her, its, ...], [truly,
wildly, politically, financially,...]
10 [takeoff, altitude, airport, carry-on, airplane, flown,
landings, ...], [health-insurance, clinic, physician,
doctor, medical, health-care, ...], [financing, equity,
investors, firms, stock, fund, market, ...]
</table>
<tableCaption confidence="0.963024">
Table 3: Example clusters for SKIP embeddings with win-
dow size w = 1 (syntactic) and w = 10 (topical).
</tableCaption>
<bodyText confidence="0.999625875">
much more topically-coherent and contain mixed
POS tags.4 For parsing experiments, we choose
w = 2 for CBOW and w = 1 for SKIP. Finally,
our SKIPDEP embeddings, trained with syntactic
context and w = 1 (§2.1.2), achieve the highest
M-1 of all continuous representations. In §4, we
will relate these intrinsic metrics to extrinsic pars-
ing performance.
</bodyText>
<sectionHeader confidence="0.998068" genericHeader="method">
3 Dependency Parsing Features
</sectionHeader>
<bodyText confidence="0.99993525">
We now discuss the features that we add to our
baseline dependency parser (second-order MST-
Parser; McDonald and Pereira, 2006) based on
discrete and continuous representations.
</bodyText>
<subsectionHeader confidence="0.99901">
3.1 Brown Cluster Features
</subsectionHeader>
<bodyText confidence="0.99998075">
We start by replicating the features of Koo et al.
(2008) using their BROWN clusters; each word is
represented by a 0-1 bit string indicating the path
from the root to the leaf in the binary merge tree.
We follow Koo et al. in adding cluster versions of
the first- and second-order features in MSTParser,
using bit string prefixes of the head, argument,
sibling, intermediate words, etc., to augment or
replace the POS and lexical identity information.
We tried various sets of prefix lengths on the devel-
opment set and found the best setting to use pre-
fixes of length 4, 6, 8, and 12.5
</bodyText>
<subsectionHeader confidence="0.997525">
3.2 Continuous Representation Features
</subsectionHeader>
<bodyText confidence="0.986255">
We tried two kinds of indicator features:
Bucket features: For both parent and child vec-
tors in a potential dependency, we fire one indi-
cator feature per dimension of each embedding
</bodyText>
<footnote confidence="0.9931512">
4A similar effect, when changing distributional context
window sizes, was found by Lin and Wu (2009).
5See Koo et al. (2008) for the exact feature templates.
They used the full string in place of the length-12 prefixes,
but that setting worked slightly worse for us. Note that the
baseline parser used by Koo et al. (2008) is different from the
second-order MSTParser that we use here; their parser allows
grandparent interactions in addition to the sibling interactions
in ours. We use their clusters, available at http://people.
csail.mit.edu/maestro/papers/bllip-clusters.gz.
</footnote>
<page confidence="0.995432">
811
</page>
<bodyText confidence="0.999987333333333">
vector, where the feature consists of the dimen-
sion index d and a bucketed version of the embed-
ding value in that dimension, i.e., bucketk(Evd)
for word index v and dimension d, where E is the
V × D embedding matrix.6 We also tried standard
conjunction variants of this feature consisting of
the bucket values of both the head and argument
along with their POS-tag or word information, and
the attachment distance and direction.7
Cluster bit string features: To take into account
all dimensions simultaneously, we perform ag-
glomerative hierarchical clustering of the embed-
ding vectors. We use Ward’s minimum variance
algorithm (Ward, 1963) for cluster distance and
the Euclidean metric for vector distance (via MAT-
LAB’s linkage function with {method=ward,
metric=euclidean}). Next, we fire features on the
hierarchical clustering bit strings using templates
identical to those for BROWN, except that we use
longer prefixes as our clustering hierarchies tend
to be deeper.8
</bodyText>
<sectionHeader confidence="0.985392" genericHeader="method">
4 Parsing Experiments
</sectionHeader>
<bodyText confidence="0.999977470588235">
Setup: We use the publicly-available MST-
Parser for all experiments, specifically its second-
order projective model.9 We remove all fea-
tures that occur only once in the training data.
For WSJ parsing, we use the standard train(02-
21)/dev(22)/test(23) split and apply the NP brack-
eting patch by Vadas and Curran (2007). For
Web parsing, we still train on WSJ 02-21, but
test on the five Web domains (answers, email,
newsgroup, reviews, and weblog) of the ‘English
Web Treebank’ (LDC2012T13), splitting each do-
main in half (in original order) for the develop-
ment and test sets.10 For both treebanks, we con-
vert from constituent to dependency format us-
ing pennconverter (Johansson and Nugues,
2007), and generate POS tags using the MXPOST
tagger (Ratnaparkhi, 1996). To evaluate, we use
</bodyText>
<footnote confidence="0.912161153846154">
6Our bucketing function bucketk(x) converts the real
value x to its closest multiple of k. We choose a k value
of around 1/5th of the embedding’s absolute range.
7We initially experimented directly with real-valued fea-
tures (instead of bucketed indicator features) and similar con-
junction variants, but these did not perform well.
8We use prefixes of length 4, 6, 8, 12, 16, 20, and full-
length, again tuned on the development set.
9We use the recommended MSTParser settings: training-
k:5 iters:10 loss-type:nopunc decode-type:proj
10Our setup is different from SANCL 2012 (Petrov and
McDonald, 2012) because the exact splits and test data were
only available to participants.
</footnote>
<table confidence="0.999478444444444">
System Dev Test
Baseline 92.38 91.95
BROWN 93.18 92.69
SENNA (Buckets) 92.64 92.04
SENNA (Bit strings) 92.88 92.30
HUANG (Buckets) 92.44 91.86
HUANG (Bit strings) 92.55 92.36
CBOW (Buckets) 92.57 91.93
CBOW (Bit strings) 93.06 92.53
</table>
<tableCaption confidence="0.996666">
Table 4: Bucket vs. bit string features (UAS on WSJ).
</tableCaption>
<table confidence="0.999868666666667">
System Dev Test
Baseline 92.38 91.95
BROWN 93.18 92.69
SENNA 92.88 92.30
TURIAN 92.84 92.26
HUANG 92.55 92.36
CBOW 93.06 92.53
SKIP 92.94 92.29
SKIPDEP 93.33 92.69
Ensemble Results
ALL – BROWN 93.46 92.90
ALL 93.54 92.98
</table>
<tableCaption confidence="0.999339">
Table 5: Full results with bit string features (UAS on WSJ).
</tableCaption>
<bodyText confidence="0.9996205">
unlabeled attachment score (UAS).11 We report
statistical significance (p &lt; 0.01, 100K sam-
ples) using the bootstrap test (Efron and Tibshi-
rani, 1994).
Comparing bucket and bit string features: In
Table 4, we find that bucket features based on in-
dividual embedding dimensions do not lead to im-
provements in test accuracy, while bit string fea-
tures generally do. This is likely because indi-
vidual embedding dimensions rarely correspond to
interpretable or useful distinctions among words,
whereas the hierarchical bit strings take into ac-
count all dimensions of the representations simul-
taneously. Their prefixes also naturally define fea-
tures at multiple levels of granularity.
WSJ results: Table 5 shows our main WSJ
results. Although BROWN yields one of the
highest individual gains, we also achieve statis-
tically significant gains over the baseline from
all embeddings. The CBOW embeddings per-
form as well as BROWN (i.e., no statistically
significant difference) but are orders of magni-
tude faster to train. Finally, the syntactically-
trained SKIPDEP embeddings are statistically indis-
tinguishable from BROWN and CBOW, and sig-
nificantly better than all other embeddings. This
suggests that targeting the similarity captured by
syntactic context is useful for dependency parsing.
</bodyText>
<footnote confidence="0.719003666666667">
11We find similar improvements under labeled attachment
score (LAS). We ignore punctuation : , “ ” . in our evalua-
tion (Yamada and Matsumoto, 2003; McDonald et al., 2005).
</footnote>
<page confidence="0.991279">
812
</page>
<table confidence="0.999403583333333">
System ans eml nwg rev blog Avg
Baseline 82.6 81.2 84.3 83.8 85.5 83.5
BROWN 83.4 81.7 85.2 84.5 86.1 84.2
SENNA 83.7 81.9 85.0 85.0 86.0 84.3
TURIAN 83.0 81.5 85.0 84.1 85.7 83.9
HUANG 83.1 81.8 85.1 84.7 85.9 84.1
CBOW 82.9 81.3 85.2 83.9 85.8 83.8
SKIP 83.1 81.1 84.7 84.1 85.4 83.7
SKIPDEP 83.3 81.5 85.2 84.3 86.0 84.1
Ensemble Results
ALL–BR 83.9 82.2 85.9 85.0 86.6 84.7
ALL 84.2 82.3 85.9 85.1 86.8 84.9
</table>
<tableCaption confidence="0.991481333333333">
Table 6: Main UAS test results on Web treebanks. Here,
ans=answers, eml=email, nwg=newsgroup, rev=reviews,
blog=weblog, BR=BROWN, Avg=Macro-average.
</tableCaption>
<bodyText confidence="0.999788741935484">
Web results: Table 6 shows our main Web re-
sults.12 Here, we see that the SENNA, BROWN,
and SKIPDEP embeddings perform the best on av-
erage (and are statistically indistinguishable, ex-
cept SENNA vs. SKIPDEP on the reviews domain).
They yield statistically significant UAS improve-
ments over the baseline across all domains, except
weblog for SENNA (narrowly misses significance,
p=0.014) and email for SKIPDEP.13
Ensemble results: When analyzing errors, we
see differences among the representations, e.g.,
BROWN does better at attaching proper nouns,
prepositions, and conjunctions, while CBOW
does better on plural common nouns and adverbs.
This suggests that the representations might be
complementary and could benefit from combina-
tion. To test this, we use a simple ensemble parser
that chooses the highest voted parent for each ar-
gument.14 As shown in the last two rows of Ta-
bles 5 and 6, this leads to substantial gains. The
‘ALL – BROWN’ ensemble combines votes from
all non-BROWN continuous representations, and
the ‘ALL’ ensemble also includes BROWN.
Characteristics of representations: We now re-
late the intrinsic metrics from §2.2 to parsing
performance. The clearest correlation appears
when comparing variations of a single model,
e.g., for SKIP, the WSJ dev accuracies are 93.33
(SKIPDEP), 92.94 (w = 1), 92.86 (w = 5), and
92.70 (w = 10), which matches the M-1 score or-
der and is the reverse of the SIM score order.
</bodyText>
<footnote confidence="0.992500666666667">
12We report individual domain results and macro-average
over domains. We do not tune any features/parameters on
Web dev sets; we only show the test results for brevity.
13Note that SENNA and HUANG are trained on Wikipedia
which may explain why they work better on Web parsing as
compared to WSJ parsing.
14This does not guarantee a valid tree. Combining features
from representations will allow training to weigh them appro-
priately and also guarantee a tree.
</footnote>
<sectionHeader confidence="0.997438" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999971342857143">
In addition to work mentioned above, relevant
work that uses discrete representations exists for
POS tagging (Ritter et al., 2011; Owoputi et
al., 2013), named entity recognition (Ratinov
and Roth, 2009), supersense tagging (Grave et
al., 2013), grammar induction (Spitkovsky et al.,
2011), constituency parsing (Finkel et al., 2008),
and dependency parsing (Tratz and Hovy, 2011).
Continuous representations in NLP have been
evaluated for their ability to capture syntactic and
semantic word similarity (Huang et al., 2012;
Mikolov et al., 2013a; Mikolov et al., 2013b) and
used for tasks like semantic role labeling, part-
of-speech tagging, NER, chunking, and sentiment
classification (Turian et al., 2010; Collobert et al.,
2011; Dhillon et al., 2012; Al-Rfou’ et al., 2013).
For dependency parsing, Hisamoto et al. (2013)
also used embedding features, but there are several
differences between their work and ours. First,
they use only one set of pre-trained embeddings
(TURIAN) while we compare several and also train
our own, tailored to the task. Second, their em-
bedding features are simpler than ours, only us-
ing flat (non-hierarchical) cluster IDs and binary
strings obtained via sign quantization (11[x &gt; 0])
of the vectors. They also compare to a first-order
baseline and only evaluate on the Web treebanks.
Concurrently, Andreas and Klein (2014) inves-
tigate the use of embeddings in constituent pars-
ing. There are several differences: we work on de-
pendency parsing, use clustering-based features,
and tailor our embeddings to dependency-style
syntax; their work additionally studies vocabulary
expansion and relating in-vocabulary words via
embeddings.
</bodyText>
<sectionHeader confidence="0.995035" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999977636363636">
We showed that parsing features based on hierar-
chical bit strings work better than those based on
discretized individual embedding values. While
the Brown clusters prove to be well-suited to pars-
ing, we are able to match their performance with
our SKIPDEP embeddings that train much faster.
Finally, we found the various representations to
be complementary, enabling a simple ensemble
to perform best. Our SKIPDEP embeddings and
bit strings are available at ttic.edu/bansal/
data/syntacticEmbeddings.zip.
</bodyText>
<page confidence="0.99593">
813
</page>
<note confidence="0.829071375">
Gholamreza Haffari, Marzieh Razavi, and Anoop
Sarkar. 2011. An ensemble model that combines
syntactic and semantic clustering for discriminative
dependency parsing. In Proceedings of ACL.
References
Rami Al-Rfou’, Bryan Perozzi, and Steven Skiena.
2013. Polyglot: Distributed word representations
for multilingual NLP. In Proceedings of CoNLL.
</note>
<reference confidence="0.998223127659574">
Jacob Andreas and Dan Klein. 2014. How much do
word embeddings encode about syntax? In Pro-
ceedings of ACL.
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137–1155, March.
Jordan L. Boyd-Graber and David M. Blei. 2008. Syn-
tactic topic models. In Proceedings of NIPS.
Peter F. Brown, Peter V. Desouza, Robert L. Mercer,
Vincent J. Della Pietra, and Jenifer C. Lai. 1992.
Class-based N-gram models of natural language.
Computational Linguistics, 18(4):467–479.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493–2537.
Scott C. Deerwester, Susan T. Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. JASIS,
41(6):391–407.
Paramveer Dhillon, Dean P. Foster, and Lyle H. Ungar.
2011. Multi-view learning of word embeddings via
CCA. In Proceedings of NIPS.
Paramveer Dhillon, Jordan Rodu, Dean P. Foster, and
Lyle H. Ungar. 2012. Two Step CCA: A new spec-
tral method for estimating vector models of words.
In Proceedings of ICML.
Bradley Efron and Robert J. Tibshirani. 1994. An in-
troduction to the bootstrap, volume 57. CRC press.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, condi-
tional random field parsing. In Proceedings of ACL.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2002. Placing search in context: The
concept revisited. ACM Transactions on Informa-
tion Systems, 20(1):116–131.
Yoav Goldberg and Jon Orwant. 2013. A dataset of
syntactic-ngrams over time from a very large cor-
pus of English books. In Second Joint Conference
on Lexical and Computational Semantics (* SEM),
volume 1, pages 241–247.
Edouard Grave, Guillaume Obozinski, and Francis
Bach. 2013. Hidden markov tree models for se-
mantic class induction. In Proceedings of CoNLL.
Sorami Hisamoto, Kevin Duh, and Yuji Matsumoto.
2013. An empirical investigation of word repre-
sentations for parsing the web. In Proceedings of
ANLP.
Eric H. Huang, Richard Socher, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Improving Word
Representations via Global Context and Multiple
Word Prototypes. In Proceedings of ACL.
Fei Huang, Arun Ahuja, Doug Downey, Yi Yang,
Yuhong Guo, and Alexander Yates. 2014. Learning
representations for weakly supervised natural lan-
guage processing tasks. Computational Linguistics,
40(1).
Richard Johansson and Pierre Nugues. 2007. Ex-
tended constituent-to-dependency conversion for
English. In 16th Nordic Conference of Computa-
tional Linguistics.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings ofACL.
Dekang Lin and Xiaoyun Wu. 2009. Phrase clustering
for discriminative learning. In Proceedings of ACL-
IJCNLP.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: The Penn Treebank. Com-
putational Linguistics, 19:313–330.
Ryan T. McDonald and Fernando C. Pereira. 2006.
Online learning of approximate dependency parsing
algorithms. In Proceedings of EACL.
Ryan T. McDonald, Koby Crammer, and Fernando C.
Pereira. 2005. Spanning tree methods for discrim-
inative training of dependency parsers. Technical
Report MS-CIS-05-11, University of Pennsylvania.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. In Proceedings of ICLR.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013b. Distributed repre-
sentations of words and phrases and their composi-
tionality. In Proceedings of NIPS.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and discrim-
inative training. In Proceedings of HLT-NAACL.
Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling.
In Proceedings of ICML.
</reference>
<page confidence="0.985759">
814
</page>
<reference confidence="0.999853327586207">
Courtney Napoles, Matthew Gormley, and Benjamin
Van Durme. 2012. Annotated gigaword. In Pro-
ceedings of the Joint Workshop on Automatic Knowl-
edge Base Construction and Web-scale Knowledge
Extraction, AKBC-WEKEX ’12, pages 95–100,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Olutobi Owoputi, Brendan OConnor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A.
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of NAACL.
Sebastian Pad´o and Mirella Lapata. 2007.
Dependency-based construction of semantic space
models. Computational Linguistics, 33(2):161–199.
Slav Petrov and Ryan McDonald. 2012. Overview of
the 2012 shared task on parsing the web. In Notes
of the First Workshop on Syntactic Analysis of Non-
Canonical Language (SANCL).
Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
Proceedings of CoNLL.
Adwait Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In Proceedings
of EMNLP.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: An ex-
perimental study. In Proceedings of EMNLP.
Kenji Sagae and Andrew S. Gordon. 2009. Cluster-
ing words by syntactic similarity improves depen-
dency parsing of predicate-argument structures. In
Proceedings of the 11th International Conference on
Parsing Technologies.
Valentin I. Spitkovsky, Hiyan Alshawi, Angel X.
Chang, and Daniel Jurafsky. 2011. Unsupervised
dependency parsing without gold part-of-speech
tags. In Proceedings of EMNLP.
Oscar T¨ackstr¨om, Ryan McDonald, and Jakob Uszko-
reit. 2012. Cross-lingual word clusters for direct
transfer of linguistic structure. In Proceedings of
NAACL.
Stephen Tratz and Eduard Hovy. 2011. A fast, ac-
curate, non-projective, semantically-enriched parser.
In Proceedings of EMNLP.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word representations: A simple and general
method for semi-supervised learning. In Proceed-
ings of ACL.
David Vadas and James R. Curran. 2007. Adding noun
phrase structure to the Penn Treebank. In Proceed-
ings of ACL.
Joe H. Ward. 1963. Hierarchical grouping to optimize
an objective function. Journal of the American sta-
tistical association, 58(301):236–244.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In Proceedings of International Conference
on Parsing Technologies.
</reference>
<page confidence="0.99884">
815
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.848211">
<title confidence="0.999766">Tailoring Continuous Word Representations for Dependency Parsing</title>
<author confidence="0.960654">Mohit Bansal Kevin Gimpel Karen Livescu</author>
<address confidence="0.895659">Toyota Technological Institute at Chicago, IL 60637, USA</address>
<abstract confidence="0.999148611111111">Word representations have proven useful for many NLP tasks, e.g., Brown clusters as features in dependency parsing (Koo et al., 2008). In this paper, we investigate the of representations as features for dependency parsing. We compare several popular embeddings to Brown clusters, via multiple types of features, in both news and web domains. We find that all embeddings yield significant parsing gains, including some recent ones that can be trained in a fraction of the time of others. Explicitly tailoring the representations for the task leads to further improvements. Moreover, an ensemble of all representations achieves the best results, suggesting their complementarity.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jacob Andreas</author>
<author>Dan Klein</author>
</authors>
<title>How much do word embeddings encode about syntax?</title>
<date>2014</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="22018" citStr="Andreas and Klein (2014)" startWordPosition="3554" endWordPosition="3557">Dhillon et al., 2012; Al-Rfou’ et al., 2013). For dependency parsing, Hisamoto et al. (2013) also used embedding features, but there are several differences between their work and ours. First, they use only one set of pre-trained embeddings (TURIAN) while we compare several and also train our own, tailored to the task. Second, their embedding features are simpler than ours, only using flat (non-hierarchical) cluster IDs and binary strings obtained via sign quantization (11[x &gt; 0]) of the vectors. They also compare to a first-order baseline and only evaluate on the Web treebanks. Concurrently, Andreas and Klein (2014) investigate the use of embeddings in constituent parsing. There are several differences: we work on dependency parsing, use clustering-based features, and tailor our embeddings to dependency-style syntax; their work additionally studies vocabulary expansion and relating in-vocabulary words via embeddings. 6 Conclusion We showed that parsing features based on hierarchical bit strings work better than those based on discretized individual embedding values. While the Brown clusters prove to be well-suited to parsing, we are able to match their performance with our SKIPDEP embeddings that train m</context>
</contexts>
<marker>Andreas, Klein, 2014</marker>
<rawString>Jacob Andreas and Dan Klein. 2014. How much do word embeddings encode about syntax? In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Janvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="1653" citStr="Bengio et al., 2003" startWordPosition="241" endWordPosition="244"> tagging (Huang et al., 2014), named entity recognition (Miller et al., 2004), chunking (Turian et al., 2010), and syntactic parsing (Koo et al., 2008; Finkel et al., 2008; T¨ackstr¨om et al., 2012). Most word representations fall into one of two categories. Discrete representations consist of memberships in a (possibly hierarchical) hard clustering of words, e.g., via kmeans or the Brown et al. (1992) algorithm. Continuous representations (or distributed representations or embeddings) consist of low-dimensional, real-valued vectors for each word, typically induced via neural language models (Bengio et al., 2003; Mnih and Hinton, 2007) or spectral methods (Deerwester et al., 1990; Dhillon et al., 2011). Koo et al. (2008) found improvement on indomain dependency parsing using features based on discrete Brown clusters. In this paper, we experiment with parsing features derived from continuous representations. We find that simple attempts based on discretization of individual word vector dimensions do not improve parsing. We see gains only after first performing a hierarchical clustering of the continuous word vectors and then using features based on the hierarchy. We compare several types of continuous</context>
<context position="3739" citStr="Bengio et al., 2003" startWordPosition="573" endWordPosition="576">actic context (dependency links and labels). Interestingly, the Brown clusters of Koo et al. (2008) prove to be difficult to beat, but we find that our syntactic tailoring can lead to embeddings that match the parsing performance of Brown (on all test sets) in a fraction of the training time. Finally, a simple parser ensemble on all the representations achieves the best results, suggesting their complementarity for dependency parsing. 2 Continuous Word Representations There are many ways to train continuous representations; in this paper, we are primarily interested in neural language models (Bengio et al., 2003), which use neural networks and local context to learn word vectors. Several researchers have made their trained representations publicly avail809 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 809–815, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics Representation Source Corpus Types, Tokens V D Time BROWN Koo et al. (2008) BLLIP 317K, 43M 316,710 – 2.5 days† SENNA Collobert et al. (2011) Wikipedia 8.3M, 1.8B 130,000 50 2 months* TURIAN Turian et al. (2010) RCV1 269K, 37M 268,810 50 few </context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137–1155, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordan L Boyd-Graber</author>
<author>David M Blei</author>
</authors>
<title>Syntactic topic models.</title>
<date>2008</date>
<booktitle>In Proceedings of NIPS.</booktitle>
<contexts>
<context position="8935" citStr="Boyd-Graber and Blei, 2008" startWordPosition="1443" endWordPosition="1446">se for parsing tasks? Several methods have been proposed for intrinsic evaluation of word representa2We use a subscript on g so that it will be treated differently from c when considering the context of p. We removed all g&lt;G&gt; from the vocabulary after training. We also tried adding information about POS tags. This increases M-1 (§2.2), but harms parsing performance, likely because the embeddings become too tag-like. Similar ideas have been used for clustering (Sagae and Gordon, 2009; Haffari et al., 2011; Grave et al., 2013), semantic space models (Pad´o and Lapata, 2007), and topic modeling (Boyd-Graber and Blei, 2008). 810 Representation SIM M-1 BROWN – 89.3 SENNA 49.8 85.2 TURIAN 29.5 87.2 HUANG 62.6 78.1 CBOW, w = 2 34.7 84.8 SKIP, w = 1 37.8 86.6 SKIP, w = 2 43.1 85.8 SKIP, w = 5 44.4 81.1 SKIP, w = 10 44.6 71.5 SKIPDEP 34.6 88.3 Table 2: Intrinsic evaluation of representations. SIM column has Spearman’s p × 100 for 353-pair word similarity dataset. M-1 is our unsupervised POS tagging metric. For BROWN, M-1 is simply many-to-one accuracy of the clusters. Best score in each column is bold. tions; we discuss two here: Word similarity (SIM): One widely-used evaluation compares distances in the continuous s</context>
</contexts>
<marker>Boyd-Graber, Blei, 2008</marker>
<rawString>Jordan L. Boyd-Graber and David M. Blei. 2008. Syntactic topic models. In Proceedings of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Peter V Desouza</author>
<author>Robert L Mercer</author>
<author>Vincent J Della Pietra</author>
<author>Jenifer C Lai</author>
</authors>
<title>Class-based N-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="1439" citStr="Brown et al. (1992)" startWordPosition="212" endWordPosition="215">e of all representations achieves the best results, suggesting their complementarity. 1 Introduction Word representations derived from unlabeled text have proven useful for many NLP tasks, e.g., partof-speech (POS) tagging (Huang et al., 2014), named entity recognition (Miller et al., 2004), chunking (Turian et al., 2010), and syntactic parsing (Koo et al., 2008; Finkel et al., 2008; T¨ackstr¨om et al., 2012). Most word representations fall into one of two categories. Discrete representations consist of memberships in a (possibly hierarchical) hard clustering of words, e.g., via kmeans or the Brown et al. (1992) algorithm. Continuous representations (or distributed representations or embeddings) consist of low-dimensional, real-valued vectors for each word, typically induced via neural language models (Bengio et al., 2003; Mnih and Hinton, 2007) or spectral methods (Deerwester et al., 1990; Dhillon et al., 2011). Koo et al. (2008) found improvement on indomain dependency parsing using features based on discrete Brown clusters. In this paper, we experiment with parsing features derived from continuous representations. We find that simple attempts based on discretization of individual word vector dimen</context>
</contexts>
<marker>Brown, Desouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>Peter F. Brown, Peter V. Desouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Class-based N-gram models of natural language. Computational Linguistics, 18(4):467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="2367" citStr="Collobert et al., 2011" startWordPosition="354" endWordPosition="357">. Koo et al. (2008) found improvement on indomain dependency parsing using features based on discrete Brown clusters. In this paper, we experiment with parsing features derived from continuous representations. We find that simple attempts based on discretization of individual word vector dimensions do not improve parsing. We see gains only after first performing a hierarchical clustering of the continuous word vectors and then using features based on the hierarchy. We compare several types of continuous representations, including those made available by other researchers (Turian et al., 2010; Collobert et al., 2011; Huang et al., 2012), and embeddings we have trained using the approach of Mikolov et al. (2013a), which is orders of magnitude faster than the others. The representations exhibit different characteristics, which we demonstrate using both intrinsic metrics and extrinsic parsing evaluation. We report significant improvements over our baseline on both the Penn Treebank (PTB; Marcus et al., 1993) and the English Web treebank (Petrov and McDonald, 2012). While all embeddings yield some parsing improvements, we find larger gains by tailoring them to capture similarity in terms of context within sy</context>
<context position="4238" citStr="Collobert et al. (2011)" startWordPosition="646" endWordPosition="649">train continuous representations; in this paper, we are primarily interested in neural language models (Bengio et al., 2003), which use neural networks and local context to learn word vectors. Several researchers have made their trained representations publicly avail809 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 809–815, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics Representation Source Corpus Types, Tokens V D Time BROWN Koo et al. (2008) BLLIP 317K, 43M 316,710 – 2.5 days† SENNA Collobert et al. (2011) Wikipedia 8.3M, 1.8B 130,000 50 2 months* TURIAN Turian et al. (2010) RCV1 269K, 37M 268,810 50 few weeks* HUANG Huang et al. (2012) Wikipedia 8.3M, 1.8B 100,232 50 — CBOW, SKIP, SKIPDEP Mikolov et al. (2013a) BLLIP 317K, 43M 316,697 100 2-4 mins.† Table 1: Details of word representations used, including datasets, vocabulary size V , and dimensionality D. Continuous representations require an additional 4 hours to run hierarchical clustering to generate features (§3.2). RCV1 = Reuters Corpus, Volume 1. ∗ = time reported by authors. † = run by us on a 3.50 GHz desktop, using a single thread. a</context>
<context position="21392" citStr="Collobert et al., 2011" startWordPosition="3454" endWordPosition="3457"> (Ritter et al., 2011; Owoputi et al., 2013), named entity recognition (Ratinov and Roth, 2009), supersense tagging (Grave et al., 2013), grammar induction (Spitkovsky et al., 2011), constituency parsing (Finkel et al., 2008), and dependency parsing (Tratz and Hovy, 2011). Continuous representations in NLP have been evaluated for their ability to capture syntactic and semantic word similarity (Huang et al., 2012; Mikolov et al., 2013a; Mikolov et al., 2013b) and used for tasks like semantic role labeling, partof-speech tagging, NER, chunking, and sentiment classification (Turian et al., 2010; Collobert et al., 2011; Dhillon et al., 2012; Al-Rfou’ et al., 2013). For dependency parsing, Hisamoto et al. (2013) also used embedding features, but there are several differences between their work and ours. First, they use only one set of pre-trained embeddings (TURIAN) while we compare several and also train our own, tailored to the task. Second, their embedding features are simpler than ours, only using flat (non-hierarchical) cluster IDs and binary strings obtained via sign quantization (11[x &gt; 0]) of the vectors. They also compare to a first-order baseline and only evaluate on the Web treebanks. Concurrently</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott C Deerwester</author>
<author>Susan T Dumais</author>
<author>Thomas K Landauer</author>
<author>George W Furnas</author>
<author>Richard A Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>JASIS,</journal>
<volume>41</volume>
<issue>6</issue>
<contexts>
<context position="1722" citStr="Deerwester et al., 1990" startWordPosition="253" endWordPosition="256">t al., 2004), chunking (Turian et al., 2010), and syntactic parsing (Koo et al., 2008; Finkel et al., 2008; T¨ackstr¨om et al., 2012). Most word representations fall into one of two categories. Discrete representations consist of memberships in a (possibly hierarchical) hard clustering of words, e.g., via kmeans or the Brown et al. (1992) algorithm. Continuous representations (or distributed representations or embeddings) consist of low-dimensional, real-valued vectors for each word, typically induced via neural language models (Bengio et al., 2003; Mnih and Hinton, 2007) or spectral methods (Deerwester et al., 1990; Dhillon et al., 2011). Koo et al. (2008) found improvement on indomain dependency parsing using features based on discrete Brown clusters. In this paper, we experiment with parsing features derived from continuous representations. We find that simple attempts based on discretization of individual word vector dimensions do not improve parsing. We see gains only after first performing a hierarchical clustering of the continuous word vectors and then using features based on the hierarchy. We compare several types of continuous representations, including those made available by other researchers</context>
</contexts>
<marker>Deerwester, Dumais, Landauer, Furnas, Harshman, 1990</marker>
<rawString>Scott C. Deerwester, Susan T. Dumais, Thomas K. Landauer, George W. Furnas, and Richard A. Harshman. 1990. Indexing by latent semantic analysis. JASIS, 41(6):391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paramveer Dhillon</author>
<author>Dean P Foster</author>
<author>Lyle H Ungar</author>
</authors>
<title>Multi-view learning of word embeddings via CCA. In</title>
<date>2011</date>
<booktitle>Proceedings of NIPS.</booktitle>
<contexts>
<context position="1745" citStr="Dhillon et al., 2011" startWordPosition="257" endWordPosition="260">urian et al., 2010), and syntactic parsing (Koo et al., 2008; Finkel et al., 2008; T¨ackstr¨om et al., 2012). Most word representations fall into one of two categories. Discrete representations consist of memberships in a (possibly hierarchical) hard clustering of words, e.g., via kmeans or the Brown et al. (1992) algorithm. Continuous representations (or distributed representations or embeddings) consist of low-dimensional, real-valued vectors for each word, typically induced via neural language models (Bengio et al., 2003; Mnih and Hinton, 2007) or spectral methods (Deerwester et al., 1990; Dhillon et al., 2011). Koo et al. (2008) found improvement on indomain dependency parsing using features based on discrete Brown clusters. In this paper, we experiment with parsing features derived from continuous representations. We find that simple attempts based on discretization of individual word vector dimensions do not improve parsing. We see gains only after first performing a hierarchical clustering of the continuous word vectors and then using features based on the hierarchy. We compare several types of continuous representations, including those made available by other researchers (Turian et al., 2010; </context>
</contexts>
<marker>Dhillon, Foster, Ungar, 2011</marker>
<rawString>Paramveer Dhillon, Dean P. Foster, and Lyle H. Ungar. 2011. Multi-view learning of word embeddings via CCA. In Proceedings of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paramveer Dhillon</author>
<author>Jordan Rodu</author>
<author>Dean P Foster</author>
<author>Lyle H Ungar</author>
</authors>
<title>Two Step CCA: A new spectral method for estimating vector models of words.</title>
<date>2012</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="21414" citStr="Dhillon et al., 2012" startWordPosition="3458" endWordPosition="3461">woputi et al., 2013), named entity recognition (Ratinov and Roth, 2009), supersense tagging (Grave et al., 2013), grammar induction (Spitkovsky et al., 2011), constituency parsing (Finkel et al., 2008), and dependency parsing (Tratz and Hovy, 2011). Continuous representations in NLP have been evaluated for their ability to capture syntactic and semantic word similarity (Huang et al., 2012; Mikolov et al., 2013a; Mikolov et al., 2013b) and used for tasks like semantic role labeling, partof-speech tagging, NER, chunking, and sentiment classification (Turian et al., 2010; Collobert et al., 2011; Dhillon et al., 2012; Al-Rfou’ et al., 2013). For dependency parsing, Hisamoto et al. (2013) also used embedding features, but there are several differences between their work and ours. First, they use only one set of pre-trained embeddings (TURIAN) while we compare several and also train our own, tailored to the task. Second, their embedding features are simpler than ours, only using flat (non-hierarchical) cluster IDs and binary strings obtained via sign quantization (11[x &gt; 0]) of the vectors. They also compare to a first-order baseline and only evaluate on the Web treebanks. Concurrently, Andreas and Klein (2</context>
</contexts>
<marker>Dhillon, Rodu, Foster, Ungar, 2012</marker>
<rawString>Paramveer Dhillon, Jordan Rodu, Dean P. Foster, and Lyle H. Ungar. 2012. Two Step CCA: A new spectral method for estimating vector models of words. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bradley Efron</author>
<author>Robert J Tibshirani</author>
</authors>
<title>An introduction to the bootstrap, volume 57.</title>
<date>1994</date>
<publisher>CRC press.</publisher>
<contexts>
<context position="16892" citStr="Efron and Tibshirani, 1994" startWordPosition="2737" endWordPosition="2741">it strings) 92.88 92.30 HUANG (Buckets) 92.44 91.86 HUANG (Bit strings) 92.55 92.36 CBOW (Buckets) 92.57 91.93 CBOW (Bit strings) 93.06 92.53 Table 4: Bucket vs. bit string features (UAS on WSJ). System Dev Test Baseline 92.38 91.95 BROWN 93.18 92.69 SENNA 92.88 92.30 TURIAN 92.84 92.26 HUANG 92.55 92.36 CBOW 93.06 92.53 SKIP 92.94 92.29 SKIPDEP 93.33 92.69 Ensemble Results ALL – BROWN 93.46 92.90 ALL 93.54 92.98 Table 5: Full results with bit string features (UAS on WSJ). unlabeled attachment score (UAS).11 We report statistical significance (p &lt; 0.01, 100K samples) using the bootstrap test (Efron and Tibshirani, 1994). Comparing bucket and bit string features: In Table 4, we find that bucket features based on individual embedding dimensions do not lead to improvements in test accuracy, while bit string features generally do. This is likely because individual embedding dimensions rarely correspond to interpretable or useful distinctions among words, whereas the hierarchical bit strings take into account all dimensions of the representations simultaneously. Their prefixes also naturally define features at multiple levels of granularity. WSJ results: Table 5 shows our main WSJ results. Although BROWN yields o</context>
</contexts>
<marker>Efron, Tibshirani, 1994</marker>
<rawString>Bradley Efron and Robert J. Tibshirani. 1994. An introduction to the bootstrap, volume 57. CRC press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Alex Kleeman</author>
<author>Christopher D Manning</author>
</authors>
<title>Efficient, feature-based, conditional random field parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1205" citStr="Finkel et al., 2008" startWordPosition="174" endWordPosition="177"> embeddings yield significant parsing gains, including some recent ones that can be trained in a fraction of the time of others. Explicitly tailoring the representations for the task leads to further improvements. Moreover, an ensemble of all representations achieves the best results, suggesting their complementarity. 1 Introduction Word representations derived from unlabeled text have proven useful for many NLP tasks, e.g., partof-speech (POS) tagging (Huang et al., 2014), named entity recognition (Miller et al., 2004), chunking (Turian et al., 2010), and syntactic parsing (Koo et al., 2008; Finkel et al., 2008; T¨ackstr¨om et al., 2012). Most word representations fall into one of two categories. Discrete representations consist of memberships in a (possibly hierarchical) hard clustering of words, e.g., via kmeans or the Brown et al. (1992) algorithm. Continuous representations (or distributed representations or embeddings) consist of low-dimensional, real-valued vectors for each word, typically induced via neural language models (Bengio et al., 2003; Mnih and Hinton, 2007) or spectral methods (Deerwester et al., 1990; Dhillon et al., 2011). Koo et al. (2008) found improvement on indomain dependency</context>
<context position="20995" citStr="Finkel et al., 2008" startWordPosition="3394" endWordPosition="3397">and HUANG are trained on Wikipedia which may explain why they work better on Web parsing as compared to WSJ parsing. 14This does not guarantee a valid tree. Combining features from representations will allow training to weigh them appropriately and also guarantee a tree. 5 Related Work In addition to work mentioned above, relevant work that uses discrete representations exists for POS tagging (Ritter et al., 2011; Owoputi et al., 2013), named entity recognition (Ratinov and Roth, 2009), supersense tagging (Grave et al., 2013), grammar induction (Spitkovsky et al., 2011), constituency parsing (Finkel et al., 2008), and dependency parsing (Tratz and Hovy, 2011). Continuous representations in NLP have been evaluated for their ability to capture syntactic and semantic word similarity (Huang et al., 2012; Mikolov et al., 2013a; Mikolov et al., 2013b) and used for tasks like semantic role labeling, partof-speech tagging, NER, chunking, and sentiment classification (Turian et al., 2010; Collobert et al., 2011; Dhillon et al., 2012; Al-Rfou’ et al., 2013). For dependency parsing, Hisamoto et al. (2013) also used embedding features, but there are several differences between their work and ours. First, they use</context>
</contexts>
<marker>Finkel, Kleeman, Manning, 2008</marker>
<rawString>Jenny Rose Finkel, Alex Kleeman, and Christopher D. Manning. 2008. Efficient, feature-based, conditional random field parsing. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Finkelstein</author>
<author>Evgeniy Gabrilovich</author>
<author>Yossi Matias</author>
<author>Ehud Rivlin</author>
<author>Zach Solan</author>
<author>Gadi Wolfman</author>
<author>Eytan Ruppin</author>
</authors>
<title>Placing search in context: The concept revisited.</title>
<date>2002</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="9633" citStr="Finkelstein et al. (2002)" startWordPosition="1567" endWordPosition="1570">2 HUANG 62.6 78.1 CBOW, w = 2 34.7 84.8 SKIP, w = 1 37.8 86.6 SKIP, w = 2 43.1 85.8 SKIP, w = 5 44.4 81.1 SKIP, w = 10 44.6 71.5 SKIPDEP 34.6 88.3 Table 2: Intrinsic evaluation of representations. SIM column has Spearman’s p × 100 for 353-pair word similarity dataset. M-1 is our unsupervised POS tagging metric. For BROWN, M-1 is simply many-to-one accuracy of the clusters. Best score in each column is bold. tions; we discuss two here: Word similarity (SIM): One widely-used evaluation compares distances in the continuous space to human judgments of word similarity using the 353-pair dataset of Finkelstein et al. (2002). We compute cosine similarity between the two vectors in each word pair, then order the word pairs by similarity and compute Spearman’s rank correlation coefficient (p) with the gold similarities. Embeddings with high p capture similarity in terms of paraphrase and topical relationships. Clustering-based tagging accuracy (M-1): Intuitively, we expect embeddings to help parsing the most if they can tell us when two words are similar syntactically. To this end, we use a metric based on unsupervised evaluation of POS taggers. We perform clustering and map each cluster to one POS tag so as to max</context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, Ruppin, 2002</marker>
<rawString>Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. 2002. Placing search in context: The concept revisited. ACM Transactions on Information Systems, 20(1):116–131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Jon Orwant</author>
</authors>
<title>A dataset of syntactic-ngrams over time from a very large corpus of English books.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (* SEM),</booktitle>
<volume>1</volume>
<pages>241--247</pages>
<contexts>
<context position="8169" citStr="Goldberg and Orwant, 2013" startWordPosition="1320" endWordPosition="1323">his, we parse the BLLIP corpus (minus PTB) using our baseline dependency parser, then build a corpus in which each line contains a single child word c, its parent word p, its grandparent g, and the dependency label f of the (c, p) link: “f&lt;L&gt; g&lt;G&gt; p c f&lt;L&gt;”, that is, both the dependency label and grandparent word are subscripted with a special token to avoid collision with words.2 We train the SKIP model on this corpus of tuples with window size w = 1, denoting the result SKIPDEP. Note that this approach needs a parsed corpus, but there also already exist such resources (Napoles et al., 2012; Goldberg and Orwant, 2013). 2.2 Intrinsic Evaluation of Representations Short of running end-to-end parsing experiments, how can we choose which representations to use for parsing tasks? Several methods have been proposed for intrinsic evaluation of word representa2We use a subscript on g so that it will be treated differently from c when considering the context of p. We removed all g&lt;G&gt; from the vocabulary after training. We also tried adding information about POS tags. This increases M-1 (§2.2), but harms parsing performance, likely because the embeddings become too tag-like. Similar ideas have been used for clusteri</context>
</contexts>
<marker>Goldberg, Orwant, 2013</marker>
<rawString>Yoav Goldberg and Jon Orwant. 2013. A dataset of syntactic-ngrams over time from a very large corpus of English books. In Second Joint Conference on Lexical and Computational Semantics (* SEM), volume 1, pages 241–247.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edouard Grave</author>
<author>Guillaume Obozinski</author>
<author>Francis Bach</author>
</authors>
<title>Hidden markov tree models for semantic class induction.</title>
<date>2013</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<contexts>
<context position="8838" citStr="Grave et al., 2013" startWordPosition="1429" endWordPosition="1432">t of running end-to-end parsing experiments, how can we choose which representations to use for parsing tasks? Several methods have been proposed for intrinsic evaluation of word representa2We use a subscript on g so that it will be treated differently from c when considering the context of p. We removed all g&lt;G&gt; from the vocabulary after training. We also tried adding information about POS tags. This increases M-1 (§2.2), but harms parsing performance, likely because the embeddings become too tag-like. Similar ideas have been used for clustering (Sagae and Gordon, 2009; Haffari et al., 2011; Grave et al., 2013), semantic space models (Pad´o and Lapata, 2007), and topic modeling (Boyd-Graber and Blei, 2008). 810 Representation SIM M-1 BROWN – 89.3 SENNA 49.8 85.2 TURIAN 29.5 87.2 HUANG 62.6 78.1 CBOW, w = 2 34.7 84.8 SKIP, w = 1 37.8 86.6 SKIP, w = 2 43.1 85.8 SKIP, w = 5 44.4 81.1 SKIP, w = 10 44.6 71.5 SKIPDEP 34.6 88.3 Table 2: Intrinsic evaluation of representations. SIM column has Spearman’s p × 100 for 353-pair word similarity dataset. M-1 is our unsupervised POS tagging metric. For BROWN, M-1 is simply many-to-one accuracy of the clusters. Best score in each column is bold. tions; we discuss t</context>
<context position="20906" citStr="Grave et al., 2013" startWordPosition="3382" endWordPosition="3385">arameters on Web dev sets; we only show the test results for brevity. 13Note that SENNA and HUANG are trained on Wikipedia which may explain why they work better on Web parsing as compared to WSJ parsing. 14This does not guarantee a valid tree. Combining features from representations will allow training to weigh them appropriately and also guarantee a tree. 5 Related Work In addition to work mentioned above, relevant work that uses discrete representations exists for POS tagging (Ritter et al., 2011; Owoputi et al., 2013), named entity recognition (Ratinov and Roth, 2009), supersense tagging (Grave et al., 2013), grammar induction (Spitkovsky et al., 2011), constituency parsing (Finkel et al., 2008), and dependency parsing (Tratz and Hovy, 2011). Continuous representations in NLP have been evaluated for their ability to capture syntactic and semantic word similarity (Huang et al., 2012; Mikolov et al., 2013a; Mikolov et al., 2013b) and used for tasks like semantic role labeling, partof-speech tagging, NER, chunking, and sentiment classification (Turian et al., 2010; Collobert et al., 2011; Dhillon et al., 2012; Al-Rfou’ et al., 2013). For dependency parsing, Hisamoto et al. (2013) also used embedding</context>
</contexts>
<marker>Grave, Obozinski, Bach, 2013</marker>
<rawString>Edouard Grave, Guillaume Obozinski, and Francis Bach. 2013. Hidden markov tree models for semantic class induction. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sorami Hisamoto</author>
<author>Kevin Duh</author>
<author>Yuji Matsumoto</author>
</authors>
<title>An empirical investigation of word representations for parsing the web.</title>
<date>2013</date>
<booktitle>In Proceedings of ANLP.</booktitle>
<contexts>
<context position="21486" citStr="Hisamoto et al. (2013)" startWordPosition="3469" endWordPosition="3472">), supersense tagging (Grave et al., 2013), grammar induction (Spitkovsky et al., 2011), constituency parsing (Finkel et al., 2008), and dependency parsing (Tratz and Hovy, 2011). Continuous representations in NLP have been evaluated for their ability to capture syntactic and semantic word similarity (Huang et al., 2012; Mikolov et al., 2013a; Mikolov et al., 2013b) and used for tasks like semantic role labeling, partof-speech tagging, NER, chunking, and sentiment classification (Turian et al., 2010; Collobert et al., 2011; Dhillon et al., 2012; Al-Rfou’ et al., 2013). For dependency parsing, Hisamoto et al. (2013) also used embedding features, but there are several differences between their work and ours. First, they use only one set of pre-trained embeddings (TURIAN) while we compare several and also train our own, tailored to the task. Second, their embedding features are simpler than ours, only using flat (non-hierarchical) cluster IDs and binary strings obtained via sign quantization (11[x &gt; 0]) of the vectors. They also compare to a first-order baseline and only evaluate on the Web treebanks. Concurrently, Andreas and Klein (2014) investigate the use of embeddings in constituent parsing. There are</context>
</contexts>
<marker>Hisamoto, Duh, Matsumoto, 2013</marker>
<rawString>Sorami Hisamoto, Kevin Duh, and Yuji Matsumoto. 2013. An empirical investigation of word representations for parsing the web. In Proceedings of ANLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric H Huang</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Improving Word Representations via Global Context and Multiple Word Prototypes.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="2388" citStr="Huang et al., 2012" startWordPosition="358" endWordPosition="361">d improvement on indomain dependency parsing using features based on discrete Brown clusters. In this paper, we experiment with parsing features derived from continuous representations. We find that simple attempts based on discretization of individual word vector dimensions do not improve parsing. We see gains only after first performing a hierarchical clustering of the continuous word vectors and then using features based on the hierarchy. We compare several types of continuous representations, including those made available by other researchers (Turian et al., 2010; Collobert et al., 2011; Huang et al., 2012), and embeddings we have trained using the approach of Mikolov et al. (2013a), which is orders of magnitude faster than the others. The representations exhibit different characteristics, which we demonstrate using both intrinsic metrics and extrinsic parsing evaluation. We report significant improvements over our baseline on both the Penn Treebank (PTB; Marcus et al., 1993) and the English Web treebank (Petrov and McDonald, 2012). While all embeddings yield some parsing improvements, we find larger gains by tailoring them to capture similarity in terms of context within syntactic parses. To th</context>
<context position="4371" citStr="Huang et al. (2012)" startWordPosition="670" endWordPosition="673">eural networks and local context to learn word vectors. Several researchers have made their trained representations publicly avail809 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 809–815, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics Representation Source Corpus Types, Tokens V D Time BROWN Koo et al. (2008) BLLIP 317K, 43M 316,710 – 2.5 days† SENNA Collobert et al. (2011) Wikipedia 8.3M, 1.8B 130,000 50 2 months* TURIAN Turian et al. (2010) RCV1 269K, 37M 268,810 50 few weeks* HUANG Huang et al. (2012) Wikipedia 8.3M, 1.8B 100,232 50 — CBOW, SKIP, SKIPDEP Mikolov et al. (2013a) BLLIP 317K, 43M 316,697 100 2-4 mins.† Table 1: Details of word representations used, including datasets, vocabulary size V , and dimensionality D. Continuous representations require an additional 4 hours to run hierarchical clustering to generate features (§3.2). RCV1 = Reuters Corpus, Volume 1. ∗ = time reported by authors. † = run by us on a 3.50 GHz desktop, using a single thread. able, which we use directly in our experiments. In particular, we use the SENNA embeddings of Collobert et al. (2011); the scaled TURI</context>
<context position="21185" citStr="Huang et al., 2012" startWordPosition="3422" endWordPosition="3425">ions will allow training to weigh them appropriately and also guarantee a tree. 5 Related Work In addition to work mentioned above, relevant work that uses discrete representations exists for POS tagging (Ritter et al., 2011; Owoputi et al., 2013), named entity recognition (Ratinov and Roth, 2009), supersense tagging (Grave et al., 2013), grammar induction (Spitkovsky et al., 2011), constituency parsing (Finkel et al., 2008), and dependency parsing (Tratz and Hovy, 2011). Continuous representations in NLP have been evaluated for their ability to capture syntactic and semantic word similarity (Huang et al., 2012; Mikolov et al., 2013a; Mikolov et al., 2013b) and used for tasks like semantic role labeling, partof-speech tagging, NER, chunking, and sentiment classification (Turian et al., 2010; Collobert et al., 2011; Dhillon et al., 2012; Al-Rfou’ et al., 2013). For dependency parsing, Hisamoto et al. (2013) also used embedding features, but there are several differences between their work and ours. First, they use only one set of pre-trained embeddings (TURIAN) while we compare several and also train our own, tailored to the task. Second, their embedding features are simpler than ours, only using fla</context>
</contexts>
<marker>Huang, Socher, Manning, Ng, 2012</marker>
<rawString>Eric H. Huang, Richard Socher, Christopher D. Manning, and Andrew Y. Ng. 2012. Improving Word Representations via Global Context and Multiple Word Prototypes. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Huang</author>
<author>Arun Ahuja</author>
<author>Doug Downey</author>
<author>Yi Yang</author>
<author>Yuhong Guo</author>
<author>Alexander Yates</author>
</authors>
<title>Learning representations for weakly supervised natural language processing tasks.</title>
<date>2014</date>
<journal>Computational Linguistics,</journal>
<volume>40</volume>
<issue>1</issue>
<contexts>
<context position="1063" citStr="Huang et al., 2014" startWordPosition="151" endWordPosition="154">rsing. We compare several popular embeddings to Brown clusters, via multiple types of features, in both news and web domains. We find that all embeddings yield significant parsing gains, including some recent ones that can be trained in a fraction of the time of others. Explicitly tailoring the representations for the task leads to further improvements. Moreover, an ensemble of all representations achieves the best results, suggesting their complementarity. 1 Introduction Word representations derived from unlabeled text have proven useful for many NLP tasks, e.g., partof-speech (POS) tagging (Huang et al., 2014), named entity recognition (Miller et al., 2004), chunking (Turian et al., 2010), and syntactic parsing (Koo et al., 2008; Finkel et al., 2008; T¨ackstr¨om et al., 2012). Most word representations fall into one of two categories. Discrete representations consist of memberships in a (possibly hierarchical) hard clustering of words, e.g., via kmeans or the Brown et al. (1992) algorithm. Continuous representations (or distributed representations or embeddings) consist of low-dimensional, real-valued vectors for each word, typically induced via neural language models (Bengio et al., 2003; Mnih and</context>
</contexts>
<marker>Huang, Ahuja, Downey, Yang, Guo, Yates, 2014</marker>
<rawString>Fei Huang, Arun Ahuja, Doug Downey, Yi Yang, Yuhong Guo, and Alexander Yates. 2014. Learning representations for weakly supervised natural language processing tasks. Computational Linguistics, 40(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Extended constituent-to-dependency conversion for English.</title>
<date>2007</date>
<booktitle>In 16th Nordic Conference of Computational Linguistics.</booktitle>
<contexts>
<context position="15410" citStr="Johansson and Nugues, 2007" startWordPosition="2501" endWordPosition="2504"> specifically its secondorder projective model.9 We remove all features that occur only once in the training data. For WSJ parsing, we use the standard train(02- 21)/dev(22)/test(23) split and apply the NP bracketing patch by Vadas and Curran (2007). For Web parsing, we still train on WSJ 02-21, but test on the five Web domains (answers, email, newsgroup, reviews, and weblog) of the ‘English Web Treebank’ (LDC2012T13), splitting each domain in half (in original order) for the development and test sets.10 For both treebanks, we convert from constituent to dependency format using pennconverter (Johansson and Nugues, 2007), and generate POS tags using the MXPOST tagger (Ratnaparkhi, 1996). To evaluate, we use 6Our bucketing function bucketk(x) converts the real value x to its closest multiple of k. We choose a k value of around 1/5th of the embedding’s absolute range. 7We initially experimented directly with real-valued features (instead of bucketed indicator features) and similar conjunction variants, but these did not perform well. 8We use prefixes of length 4, 6, 8, 12, 16, 20, and fulllength, again tuned on the development set. 9We use the recommended MSTParser settings: trainingk:5 iters:10 loss-type:nopun</context>
</contexts>
<marker>Johansson, Nugues, 2007</marker>
<rawString>Richard Johansson and Pierre Nugues. 2007. Extended constituent-to-dependency conversion for English. In 16th Nordic Conference of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>Simple semi-supervised dependency parsing.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="1184" citStr="Koo et al., 2008" startWordPosition="170" endWordPosition="173">. We find that all embeddings yield significant parsing gains, including some recent ones that can be trained in a fraction of the time of others. Explicitly tailoring the representations for the task leads to further improvements. Moreover, an ensemble of all representations achieves the best results, suggesting their complementarity. 1 Introduction Word representations derived from unlabeled text have proven useful for many NLP tasks, e.g., partof-speech (POS) tagging (Huang et al., 2014), named entity recognition (Miller et al., 2004), chunking (Turian et al., 2010), and syntactic parsing (Koo et al., 2008; Finkel et al., 2008; T¨ackstr¨om et al., 2012). Most word representations fall into one of two categories. Discrete representations consist of memberships in a (possibly hierarchical) hard clustering of words, e.g., via kmeans or the Brown et al. (1992) algorithm. Continuous representations (or distributed representations or embeddings) consist of low-dimensional, real-valued vectors for each word, typically induced via neural language models (Bengio et al., 2003; Mnih and Hinton, 2007) or spectral methods (Deerwester et al., 1990; Dhillon et al., 2011). Koo et al. (2008) found improvement o</context>
<context position="3218" citStr="Koo et al. (2008)" startWordPosition="489" endWordPosition="492"> both intrinsic metrics and extrinsic parsing evaluation. We report significant improvements over our baseline on both the Penn Treebank (PTB; Marcus et al., 1993) and the English Web treebank (Petrov and McDonald, 2012). While all embeddings yield some parsing improvements, we find larger gains by tailoring them to capture similarity in terms of context within syntactic parses. To this end, we use two simple modifications to the models of Mikolov et al. (2013a): a smaller context window, and conditioning on syntactic context (dependency links and labels). Interestingly, the Brown clusters of Koo et al. (2008) prove to be difficult to beat, but we find that our syntactic tailoring can lead to embeddings that match the parsing performance of Brown (on all test sets) in a fraction of the training time. Finally, a simple parser ensemble on all the representations achieves the best results, suggesting their complementarity for dependency parsing. 2 Continuous Word Representations There are many ways to train continuous representations; in this paper, we are primarily interested in neural language models (Bengio et al., 2003), which use neural networks and local context to learn word vectors. Several re</context>
<context position="5157" citStr="Koo et al. (2008)" startWordPosition="802" endWordPosition="805">ding datasets, vocabulary size V , and dimensionality D. Continuous representations require an additional 4 hours to run hierarchical clustering to generate features (§3.2). RCV1 = Reuters Corpus, Volume 1. ∗ = time reported by authors. † = run by us on a 3.50 GHz desktop, using a single thread. able, which we use directly in our experiments. In particular, we use the SENNA embeddings of Collobert et al. (2011); the scaled TURIAN embeddings (C&amp;W) of Turian et al. (2010); and the HUANG global-context, single-prototype embeddings of Huang et al. (2012). We also use the BROWN clusters trained by Koo et al. (2008). Details are given in Table 1. Below, we describe embeddings that we train ourselves (§2.1), aiming to make them more useful for parsing via smaller context windows (§2.1.1) and conditioning on syntactic context (§2.1.2). We then compare the representations using two intrinsic metrics (§2.2). 2.1 Syntactically-tailored Representations We train word embeddings using the continuous bag-of-words (CBOW) and skip-gram (SKIP) models described in Mikolov et al. (2013a; 2013b) as implemented in the open-source toolkit word2vec. These models avoid hidden layers in the neural network and hence can be t</context>
<context position="6581" citStr="Koo et al. (2008)" startWordPosition="1036" endWordPosition="1039">ext Windows The CBOW model learns vectors to predict a word given its set of surrounding context words in a window of size w. The SKIP model learns embeddings to predict each individual surrounding word given one particular word, using an analogous window size w. We find that w affects the embeddings substantially: with large w, words group with others that are topically-related; with small w, grouped words tend to share the same POS tag. We discuss this further in the intrinsic evaluation presented in §2.2. 1We train both models on BLLIP (LDC2000T43) with PTB removed, the same corpus used by Koo et al. (2008) to train their BROWN clusters. We created a special vector for unknown words by averaging the vectors for the 50K least frequent words; we did not use this vector for the SKIPDEP (§2.1.2) setting because it performs slightly better without it. 2.1.2 Syntactic Context We expect embeddings to help dependency parsing the most when words that have similar parents and children are close in the embedding space. To target this type of similarity, we train the SKIP model on dependency context instead of the linear context in raw text. When ordinarily training SKIP embeddings, words v&apos; are drawn from </context>
<context position="12387" citStr="Koo et al. (2008)" startWordPosition="2015" endWordPosition="2018">ent and contain mixed POS tags.4 For parsing experiments, we choose w = 2 for CBOW and w = 1 for SKIP. Finally, our SKIPDEP embeddings, trained with syntactic context and w = 1 (§2.1.2), achieve the highest M-1 of all continuous representations. In §4, we will relate these intrinsic metrics to extrinsic parsing performance. 3 Dependency Parsing Features We now discuss the features that we add to our baseline dependency parser (second-order MSTParser; McDonald and Pereira, 2006) based on discrete and continuous representations. 3.1 Brown Cluster Features We start by replicating the features of Koo et al. (2008) using their BROWN clusters; each word is represented by a 0-1 bit string indicating the path from the root to the leaf in the binary merge tree. We follow Koo et al. in adding cluster versions of the first- and second-order features in MSTParser, using bit string prefixes of the head, argument, sibling, intermediate words, etc., to augment or replace the POS and lexical identity information. We tried various sets of prefix lengths on the development set and found the best setting to use prefixes of length 4, 6, 8, and 12.5 3.2 Continuous Representation Features We tried two kinds of indicator</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple semi-supervised dependency parsing. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Xiaoyun Wu</author>
</authors>
<title>Phrase clustering for discriminative learning.</title>
<date>2009</date>
<booktitle>In Proceedings of ACLIJCNLP.</booktitle>
<contexts>
<context position="13238" citStr="Lin and Wu (2009)" startWordPosition="2160" endWordPosition="2163">in MSTParser, using bit string prefixes of the head, argument, sibling, intermediate words, etc., to augment or replace the POS and lexical identity information. We tried various sets of prefix lengths on the development set and found the best setting to use prefixes of length 4, 6, 8, and 12.5 3.2 Continuous Representation Features We tried two kinds of indicator features: Bucket features: For both parent and child vectors in a potential dependency, we fire one indicator feature per dimension of each embedding 4A similar effect, when changing distributional context window sizes, was found by Lin and Wu (2009). 5See Koo et al. (2008) for the exact feature templates. They used the full string in place of the length-12 prefixes, but that setting worked slightly worse for us. Note that the baseline parser used by Koo et al. (2008) is different from the second-order MSTParser that we use here; their parser allows grandparent interactions in addition to the sibling interactions in ours. We use their clusters, available at http://people. csail.mit.edu/maestro/papers/bllip-clusters.gz. 811 vector, where the feature consists of the dimension index d and a bucketed version of the embedding value in that dim</context>
</contexts>
<marker>Lin, Wu, 2009</marker>
<rawString>Dekang Lin and Xiaoyun Wu. 2009. Phrase clustering for discriminative learning. In Proceedings of ACLIJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="2764" citStr="Marcus et al., 1993" startWordPosition="414" endWordPosition="417">ontinuous word vectors and then using features based on the hierarchy. We compare several types of continuous representations, including those made available by other researchers (Turian et al., 2010; Collobert et al., 2011; Huang et al., 2012), and embeddings we have trained using the approach of Mikolov et al. (2013a), which is orders of magnitude faster than the others. The representations exhibit different characteristics, which we demonstrate using both intrinsic metrics and extrinsic parsing evaluation. We report significant improvements over our baseline on both the Penn Treebank (PTB; Marcus et al., 1993) and the English Web treebank (Petrov and McDonald, 2012). While all embeddings yield some parsing improvements, we find larger gains by tailoring them to capture similarity in terms of context within syntactic parses. To this end, we use two simple modifications to the models of Mikolov et al. (2013a): a smaller context window, and conditioning on syntactic context (dependency links and labels). Interestingly, the Brown clusters of Koo et al. (2008) prove to be difficult to beat, but we find that our syntactic tailoring can lead to embeddings that match the parsing performance of Brown (on al</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19:313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan T McDonald</author>
<author>Fernando C Pereira</author>
</authors>
<title>Online learning of approximate dependency parsing algorithms.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL.</booktitle>
<contexts>
<context position="12252" citStr="McDonald and Pereira, 2006" startWordPosition="1994" endWordPosition="1997">nd, market, ...] Table 3: Example clusters for SKIP embeddings with window size w = 1 (syntactic) and w = 10 (topical). much more topically-coherent and contain mixed POS tags.4 For parsing experiments, we choose w = 2 for CBOW and w = 1 for SKIP. Finally, our SKIPDEP embeddings, trained with syntactic context and w = 1 (§2.1.2), achieve the highest M-1 of all continuous representations. In §4, we will relate these intrinsic metrics to extrinsic parsing performance. 3 Dependency Parsing Features We now discuss the features that we add to our baseline dependency parser (second-order MSTParser; McDonald and Pereira, 2006) based on discrete and continuous representations. 3.1 Brown Cluster Features We start by replicating the features of Koo et al. (2008) using their BROWN clusters; each word is represented by a 0-1 bit string indicating the path from the root to the leaf in the binary merge tree. We follow Koo et al. in adding cluster versions of the first- and second-order features in MSTParser, using bit string prefixes of the head, argument, sibling, intermediate words, etc., to augment or replace the POS and lexical identity information. We tried various sets of prefix lengths on the development set and fo</context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>Ryan T. McDonald and Fernando C. Pereira. 2006. Online learning of approximate dependency parsing algorithms. In Proceedings of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan T McDonald</author>
<author>Koby Crammer</author>
<author>Fernando C Pereira</author>
</authors>
<title>Spanning tree methods for discriminative training of dependency parsers.</title>
<date>2005</date>
<tech>Technical Report MS-CIS-05-11,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="18193" citStr="McDonald et al., 2005" startWordPosition="2939" endWordPosition="2942">ains over the baseline from all embeddings. The CBOW embeddings perform as well as BROWN (i.e., no statistically significant difference) but are orders of magnitude faster to train. Finally, the syntacticallytrained SKIPDEP embeddings are statistically indistinguishable from BROWN and CBOW, and significantly better than all other embeddings. This suggests that targeting the similarity captured by syntactic context is useful for dependency parsing. 11We find similar improvements under labeled attachment score (LAS). We ignore punctuation : , “ ” . in our evaluation (Yamada and Matsumoto, 2003; McDonald et al., 2005). 812 System ans eml nwg rev blog Avg Baseline 82.6 81.2 84.3 83.8 85.5 83.5 BROWN 83.4 81.7 85.2 84.5 86.1 84.2 SENNA 83.7 81.9 85.0 85.0 86.0 84.3 TURIAN 83.0 81.5 85.0 84.1 85.7 83.9 HUANG 83.1 81.8 85.1 84.7 85.9 84.1 CBOW 82.9 81.3 85.2 83.9 85.8 83.8 SKIP 83.1 81.1 84.7 84.1 85.4 83.7 SKIPDEP 83.3 81.5 85.2 84.3 86.0 84.1 Ensemble Results ALL–BR 83.9 82.2 85.9 85.0 86.6 84.7 ALL 84.2 82.3 85.9 85.1 86.8 84.9 Table 6: Main UAS test results on Web treebanks. Here, ans=answers, eml=email, nwg=newsgroup, rev=reviews, blog=weblog, BR=BROWN, Avg=Macro-average. Web results: Table 6 shows our ma</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan T. McDonald, Koby Crammer, and Fernando C. Pereira. 2005. Spanning tree methods for discriminative training of dependency parsers. Technical Report MS-CIS-05-11, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<booktitle>In Proceedings of ICLR.</booktitle>
<contexts>
<context position="2463" citStr="Mikolov et al. (2013" startWordPosition="371" endWordPosition="374">ete Brown clusters. In this paper, we experiment with parsing features derived from continuous representations. We find that simple attempts based on discretization of individual word vector dimensions do not improve parsing. We see gains only after first performing a hierarchical clustering of the continuous word vectors and then using features based on the hierarchy. We compare several types of continuous representations, including those made available by other researchers (Turian et al., 2010; Collobert et al., 2011; Huang et al., 2012), and embeddings we have trained using the approach of Mikolov et al. (2013a), which is orders of magnitude faster than the others. The representations exhibit different characteristics, which we demonstrate using both intrinsic metrics and extrinsic parsing evaluation. We report significant improvements over our baseline on both the Penn Treebank (PTB; Marcus et al., 1993) and the English Web treebank (Petrov and McDonald, 2012). While all embeddings yield some parsing improvements, we find larger gains by tailoring them to capture similarity in terms of context within syntactic parses. To this end, we use two simple modifications to the models of Mikolov et al. (20</context>
<context position="4446" citStr="Mikolov et al. (2013" startWordPosition="683" endWordPosition="686">s have made their trained representations publicly avail809 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 809–815, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics Representation Source Corpus Types, Tokens V D Time BROWN Koo et al. (2008) BLLIP 317K, 43M 316,710 – 2.5 days† SENNA Collobert et al. (2011) Wikipedia 8.3M, 1.8B 130,000 50 2 months* TURIAN Turian et al. (2010) RCV1 269K, 37M 268,810 50 few weeks* HUANG Huang et al. (2012) Wikipedia 8.3M, 1.8B 100,232 50 — CBOW, SKIP, SKIPDEP Mikolov et al. (2013a) BLLIP 317K, 43M 316,697 100 2-4 mins.† Table 1: Details of word representations used, including datasets, vocabulary size V , and dimensionality D. Continuous representations require an additional 4 hours to run hierarchical clustering to generate features (§3.2). RCV1 = Reuters Corpus, Volume 1. ∗ = time reported by authors. † = run by us on a 3.50 GHz desktop, using a single thread. able, which we use directly in our experiments. In particular, we use the SENNA embeddings of Collobert et al. (2011); the scaled TURIAN embeddings (C&amp;W) of Turian et al. (2010); and the HUANG global-context, </context>
<context position="21207" citStr="Mikolov et al., 2013" startWordPosition="3426" endWordPosition="3429">ning to weigh them appropriately and also guarantee a tree. 5 Related Work In addition to work mentioned above, relevant work that uses discrete representations exists for POS tagging (Ritter et al., 2011; Owoputi et al., 2013), named entity recognition (Ratinov and Roth, 2009), supersense tagging (Grave et al., 2013), grammar induction (Spitkovsky et al., 2011), constituency parsing (Finkel et al., 2008), and dependency parsing (Tratz and Hovy, 2011). Continuous representations in NLP have been evaluated for their ability to capture syntactic and semantic word similarity (Huang et al., 2012; Mikolov et al., 2013a; Mikolov et al., 2013b) and used for tasks like semantic role labeling, partof-speech tagging, NER, chunking, and sentiment classification (Turian et al., 2010; Collobert et al., 2011; Dhillon et al., 2012; Al-Rfou’ et al., 2013). For dependency parsing, Hisamoto et al. (2013) also used embedding features, but there are several differences between their work and ours. First, they use only one set of pre-trained embeddings (TURIAN) while we compare several and also train our own, tailored to the task. Second, their embedding features are simpler than ours, only using flat (non-hierarchical) c</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. In Proceedings of ICLR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Proceedings of NIPS.</booktitle>
<contexts>
<context position="2463" citStr="Mikolov et al. (2013" startWordPosition="371" endWordPosition="374">ete Brown clusters. In this paper, we experiment with parsing features derived from continuous representations. We find that simple attempts based on discretization of individual word vector dimensions do not improve parsing. We see gains only after first performing a hierarchical clustering of the continuous word vectors and then using features based on the hierarchy. We compare several types of continuous representations, including those made available by other researchers (Turian et al., 2010; Collobert et al., 2011; Huang et al., 2012), and embeddings we have trained using the approach of Mikolov et al. (2013a), which is orders of magnitude faster than the others. The representations exhibit different characteristics, which we demonstrate using both intrinsic metrics and extrinsic parsing evaluation. We report significant improvements over our baseline on both the Penn Treebank (PTB; Marcus et al., 1993) and the English Web treebank (Petrov and McDonald, 2012). While all embeddings yield some parsing improvements, we find larger gains by tailoring them to capture similarity in terms of context within syntactic parses. To this end, we use two simple modifications to the models of Mikolov et al. (20</context>
<context position="4446" citStr="Mikolov et al. (2013" startWordPosition="683" endWordPosition="686">s have made their trained representations publicly avail809 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 809–815, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics Representation Source Corpus Types, Tokens V D Time BROWN Koo et al. (2008) BLLIP 317K, 43M 316,710 – 2.5 days† SENNA Collobert et al. (2011) Wikipedia 8.3M, 1.8B 130,000 50 2 months* TURIAN Turian et al. (2010) RCV1 269K, 37M 268,810 50 few weeks* HUANG Huang et al. (2012) Wikipedia 8.3M, 1.8B 100,232 50 — CBOW, SKIP, SKIPDEP Mikolov et al. (2013a) BLLIP 317K, 43M 316,697 100 2-4 mins.† Table 1: Details of word representations used, including datasets, vocabulary size V , and dimensionality D. Continuous representations require an additional 4 hours to run hierarchical clustering to generate features (§3.2). RCV1 = Reuters Corpus, Volume 1. ∗ = time reported by authors. † = run by us on a 3.50 GHz desktop, using a single thread. able, which we use directly in our experiments. In particular, we use the SENNA embeddings of Collobert et al. (2011); the scaled TURIAN embeddings (C&amp;W) of Turian et al. (2010); and the HUANG global-context, </context>
<context position="21207" citStr="Mikolov et al., 2013" startWordPosition="3426" endWordPosition="3429">ning to weigh them appropriately and also guarantee a tree. 5 Related Work In addition to work mentioned above, relevant work that uses discrete representations exists for POS tagging (Ritter et al., 2011; Owoputi et al., 2013), named entity recognition (Ratinov and Roth, 2009), supersense tagging (Grave et al., 2013), grammar induction (Spitkovsky et al., 2011), constituency parsing (Finkel et al., 2008), and dependency parsing (Tratz and Hovy, 2011). Continuous representations in NLP have been evaluated for their ability to capture syntactic and semantic word similarity (Huang et al., 2012; Mikolov et al., 2013a; Mikolov et al., 2013b) and used for tasks like semantic role labeling, partof-speech tagging, NER, chunking, and sentiment classification (Turian et al., 2010; Collobert et al., 2011; Dhillon et al., 2012; Al-Rfou’ et al., 2013). For dependency parsing, Hisamoto et al. (2013) also used embedding features, but there are several differences between their work and ours. First, they use only one set of pre-trained embeddings (TURIAN) while we compare several and also train our own, tailored to the task. Second, their embedding features are simpler than ours, only using flat (non-hierarchical) c</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013b. Distributed representations of words and phrases and their compositionality. In Proceedings of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Miller</author>
<author>Jethran Guinness</author>
<author>Alex Zamanian</author>
</authors>
<title>Name tagging with word clusters and discriminative training.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="1111" citStr="Miller et al., 2004" startWordPosition="158" endWordPosition="161"> Brown clusters, via multiple types of features, in both news and web domains. We find that all embeddings yield significant parsing gains, including some recent ones that can be trained in a fraction of the time of others. Explicitly tailoring the representations for the task leads to further improvements. Moreover, an ensemble of all representations achieves the best results, suggesting their complementarity. 1 Introduction Word representations derived from unlabeled text have proven useful for many NLP tasks, e.g., partof-speech (POS) tagging (Huang et al., 2014), named entity recognition (Miller et al., 2004), chunking (Turian et al., 2010), and syntactic parsing (Koo et al., 2008; Finkel et al., 2008; T¨ackstr¨om et al., 2012). Most word representations fall into one of two categories. Discrete representations consist of memberships in a (possibly hierarchical) hard clustering of words, e.g., via kmeans or the Brown et al. (1992) algorithm. Continuous representations (or distributed representations or embeddings) consist of low-dimensional, real-valued vectors for each word, typically induced via neural language models (Bengio et al., 2003; Mnih and Hinton, 2007) or spectral methods (Deerwester e</context>
</contexts>
<marker>Miller, Guinness, Zamanian, 2004</marker>
<rawString>Scott Miller, Jethran Guinness, and Alex Zamanian. 2004. Name tagging with word clusters and discriminative training. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Three new graphical models for statistical language modelling.</title>
<date>2007</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="1677" citStr="Mnih and Hinton, 2007" startWordPosition="245" endWordPosition="248">., 2014), named entity recognition (Miller et al., 2004), chunking (Turian et al., 2010), and syntactic parsing (Koo et al., 2008; Finkel et al., 2008; T¨ackstr¨om et al., 2012). Most word representations fall into one of two categories. Discrete representations consist of memberships in a (possibly hierarchical) hard clustering of words, e.g., via kmeans or the Brown et al. (1992) algorithm. Continuous representations (or distributed representations or embeddings) consist of low-dimensional, real-valued vectors for each word, typically induced via neural language models (Bengio et al., 2003; Mnih and Hinton, 2007) or spectral methods (Deerwester et al., 1990; Dhillon et al., 2011). Koo et al. (2008) found improvement on indomain dependency parsing using features based on discrete Brown clusters. In this paper, we experiment with parsing features derived from continuous representations. We find that simple attempts based on discretization of individual word vector dimensions do not improve parsing. We see gains only after first performing a hierarchical clustering of the continuous word vectors and then using features based on the hierarchy. We compare several types of continuous representations, includ</context>
</contexts>
<marker>Mnih, Hinton, 2007</marker>
<rawString>Andriy Mnih and Geoffrey Hinton. 2007. Three new graphical models for statistical language modelling. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Courtney Napoles</author>
<author>Matthew Gormley</author>
<author>Benjamin Van Durme</author>
</authors>
<title>Annotated gigaword.</title>
<date>2012</date>
<booktitle>In Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction, AKBC-WEKEX ’12,</booktitle>
<pages>95--100</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Napoles, Gormley, Van Durme, 2012</marker>
<rawString>Courtney Napoles, Matthew Gormley, and Benjamin Van Durme. 2012. Annotated gigaword. In Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction, AKBC-WEKEX ’12, pages 95–100, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olutobi Owoputi</author>
<author>Brendan OConnor</author>
<author>Chris Dyer</author>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Noah A Smith</author>
</authors>
<title>Improved part-of-speech tagging for online conversational text with word clusters.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="20814" citStr="Owoputi et al., 2013" startWordPosition="3369" endWordPosition="3372">report individual domain results and macro-average over domains. We do not tune any features/parameters on Web dev sets; we only show the test results for brevity. 13Note that SENNA and HUANG are trained on Wikipedia which may explain why they work better on Web parsing as compared to WSJ parsing. 14This does not guarantee a valid tree. Combining features from representations will allow training to weigh them appropriately and also guarantee a tree. 5 Related Work In addition to work mentioned above, relevant work that uses discrete representations exists for POS tagging (Ritter et al., 2011; Owoputi et al., 2013), named entity recognition (Ratinov and Roth, 2009), supersense tagging (Grave et al., 2013), grammar induction (Spitkovsky et al., 2011), constituency parsing (Finkel et al., 2008), and dependency parsing (Tratz and Hovy, 2011). Continuous representations in NLP have been evaluated for their ability to capture syntactic and semantic word similarity (Huang et al., 2012; Mikolov et al., 2013a; Mikolov et al., 2013b) and used for tasks like semantic role labeling, partof-speech tagging, NER, chunking, and sentiment classification (Turian et al., 2010; Collobert et al., 2011; Dhillon et al., 2012</context>
</contexts>
<marker>Owoputi, OConnor, Dyer, Gimpel, Schneider, Smith, 2013</marker>
<rawString>Olutobi Owoputi, Brendan OConnor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A. Smith. 2013. Improved part-of-speech tagging for online conversational text with word clusters. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
<author>Mirella Lapata</author>
</authors>
<title>Dependency-based construction of semantic space models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<marker>Pad´o, Lapata, 2007</marker>
<rawString>Sebastian Pad´o and Mirella Lapata. 2007. Dependency-based construction of semantic space models. Computational Linguistics, 33(2):161–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Ryan McDonald</author>
</authors>
<title>Overview of the 2012 shared task on parsing the web.</title>
<date>2012</date>
<booktitle>In Notes of the First Workshop on Syntactic Analysis of NonCanonical Language (SANCL).</booktitle>
<contexts>
<context position="2821" citStr="Petrov and McDonald, 2012" startWordPosition="423" endWordPosition="426">d on the hierarchy. We compare several types of continuous representations, including those made available by other researchers (Turian et al., 2010; Collobert et al., 2011; Huang et al., 2012), and embeddings we have trained using the approach of Mikolov et al. (2013a), which is orders of magnitude faster than the others. The representations exhibit different characteristics, which we demonstrate using both intrinsic metrics and extrinsic parsing evaluation. We report significant improvements over our baseline on both the Penn Treebank (PTB; Marcus et al., 1993) and the English Web treebank (Petrov and McDonald, 2012). While all embeddings yield some parsing improvements, we find larger gains by tailoring them to capture similarity in terms of context within syntactic parses. To this end, we use two simple modifications to the models of Mikolov et al. (2013a): a smaller context window, and conditioning on syntactic context (dependency links and labels). Interestingly, the Brown clusters of Koo et al. (2008) prove to be difficult to beat, but we find that our syntactic tailoring can lead to embeddings that match the parsing performance of Brown (on all test sets) in a fraction of the training time. Finally,</context>
<context position="16097" citStr="Petrov and McDonald, 2012" startWordPosition="2609" endWordPosition="2612"> 1996). To evaluate, we use 6Our bucketing function bucketk(x) converts the real value x to its closest multiple of k. We choose a k value of around 1/5th of the embedding’s absolute range. 7We initially experimented directly with real-valued features (instead of bucketed indicator features) and similar conjunction variants, but these did not perform well. 8We use prefixes of length 4, 6, 8, 12, 16, 20, and fulllength, again tuned on the development set. 9We use the recommended MSTParser settings: trainingk:5 iters:10 loss-type:nopunc decode-type:proj 10Our setup is different from SANCL 2012 (Petrov and McDonald, 2012) because the exact splits and test data were only available to participants. System Dev Test Baseline 92.38 91.95 BROWN 93.18 92.69 SENNA (Buckets) 92.64 92.04 SENNA (Bit strings) 92.88 92.30 HUANG (Buckets) 92.44 91.86 HUANG (Bit strings) 92.55 92.36 CBOW (Buckets) 92.57 91.93 CBOW (Bit strings) 93.06 92.53 Table 4: Bucket vs. bit string features (UAS on WSJ). System Dev Test Baseline 92.38 91.95 BROWN 93.18 92.69 SENNA 92.88 92.30 TURIAN 92.84 92.26 HUANG 92.55 92.36 CBOW 93.06 92.53 SKIP 92.94 92.29 SKIPDEP 93.33 92.69 Ensemble Results ALL – BROWN 93.46 92.90 ALL 93.54 92.98 Table 5: Full r</context>
</contexts>
<marker>Petrov, McDonald, 2012</marker>
<rawString>Slav Petrov and Ryan McDonald. 2012. Overview of the 2012 shared task on parsing the web. In Notes of the First Workshop on Syntactic Analysis of NonCanonical Language (SANCL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Ratinov</author>
<author>Dan Roth</author>
</authors>
<title>Design challenges and misconceptions in named entity recognition.</title>
<date>2009</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<contexts>
<context position="20865" citStr="Ratinov and Roth, 2009" startWordPosition="3376" endWordPosition="3379">e over domains. We do not tune any features/parameters on Web dev sets; we only show the test results for brevity. 13Note that SENNA and HUANG are trained on Wikipedia which may explain why they work better on Web parsing as compared to WSJ parsing. 14This does not guarantee a valid tree. Combining features from representations will allow training to weigh them appropriately and also guarantee a tree. 5 Related Work In addition to work mentioned above, relevant work that uses discrete representations exists for POS tagging (Ritter et al., 2011; Owoputi et al., 2013), named entity recognition (Ratinov and Roth, 2009), supersense tagging (Grave et al., 2013), grammar induction (Spitkovsky et al., 2011), constituency parsing (Finkel et al., 2008), and dependency parsing (Tratz and Hovy, 2011). Continuous representations in NLP have been evaluated for their ability to capture syntactic and semantic word similarity (Huang et al., 2012; Mikolov et al., 2013a; Mikolov et al., 2013b) and used for tasks like semantic role labeling, partof-speech tagging, NER, chunking, and sentiment classification (Turian et al., 2010; Collobert et al., 2011; Dhillon et al., 2012; Al-Rfou’ et al., 2013). For dependency parsing, H</context>
</contexts>
<marker>Ratinov, Roth, 2009</marker>
<rawString>Lev Ratinov and Dan Roth. 2009. Design challenges and misconceptions in named entity recognition. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A maximum entropy model for part-of-speech tagging.</title>
<date>1996</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="15477" citStr="Ratnaparkhi, 1996" startWordPosition="2513" endWordPosition="2514"> occur only once in the training data. For WSJ parsing, we use the standard train(02- 21)/dev(22)/test(23) split and apply the NP bracketing patch by Vadas and Curran (2007). For Web parsing, we still train on WSJ 02-21, but test on the five Web domains (answers, email, newsgroup, reviews, and weblog) of the ‘English Web Treebank’ (LDC2012T13), splitting each domain in half (in original order) for the development and test sets.10 For both treebanks, we convert from constituent to dependency format using pennconverter (Johansson and Nugues, 2007), and generate POS tags using the MXPOST tagger (Ratnaparkhi, 1996). To evaluate, we use 6Our bucketing function bucketk(x) converts the real value x to its closest multiple of k. We choose a k value of around 1/5th of the embedding’s absolute range. 7We initially experimented directly with real-valued features (instead of bucketed indicator features) and similar conjunction variants, but these did not perform well. 8We use prefixes of length 4, 6, 8, 12, 16, 20, and fulllength, again tuned on the development set. 9We use the recommended MSTParser settings: trainingk:5 iters:10 loss-type:nopunc decode-type:proj 10Our setup is different from SANCL 2012 (Petrov</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A maximum entropy model for part-of-speech tagging. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Sam Clark</author>
<author>Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>Named entity recognition in tweets: An experimental study.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="20791" citStr="Ritter et al., 2011" startWordPosition="3365" endWordPosition="3368">IM score order. 12We report individual domain results and macro-average over domains. We do not tune any features/parameters on Web dev sets; we only show the test results for brevity. 13Note that SENNA and HUANG are trained on Wikipedia which may explain why they work better on Web parsing as compared to WSJ parsing. 14This does not guarantee a valid tree. Combining features from representations will allow training to weigh them appropriately and also guarantee a tree. 5 Related Work In addition to work mentioned above, relevant work that uses discrete representations exists for POS tagging (Ritter et al., 2011; Owoputi et al., 2013), named entity recognition (Ratinov and Roth, 2009), supersense tagging (Grave et al., 2013), grammar induction (Spitkovsky et al., 2011), constituency parsing (Finkel et al., 2008), and dependency parsing (Tratz and Hovy, 2011). Continuous representations in NLP have been evaluated for their ability to capture syntactic and semantic word similarity (Huang et al., 2012; Mikolov et al., 2013a; Mikolov et al., 2013b) and used for tasks like semantic role labeling, partof-speech tagging, NER, chunking, and sentiment classification (Turian et al., 2010; Collobert et al., 201</context>
</contexts>
<marker>Ritter, Clark, Mausam, Etzioni, 2011</marker>
<rawString>Alan Ritter, Sam Clark, Mausam, and Oren Etzioni. 2011. Named entity recognition in tweets: An experimental study. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Andrew S Gordon</author>
</authors>
<title>Clustering words by syntactic similarity improves dependency parsing of predicate-argument structures.</title>
<date>2009</date>
<booktitle>In Proceedings of the 11th International Conference on Parsing Technologies.</booktitle>
<contexts>
<context position="8795" citStr="Sagae and Gordon, 2009" startWordPosition="1421" endWordPosition="1424">2 Intrinsic Evaluation of Representations Short of running end-to-end parsing experiments, how can we choose which representations to use for parsing tasks? Several methods have been proposed for intrinsic evaluation of word representa2We use a subscript on g so that it will be treated differently from c when considering the context of p. We removed all g&lt;G&gt; from the vocabulary after training. We also tried adding information about POS tags. This increases M-1 (§2.2), but harms parsing performance, likely because the embeddings become too tag-like. Similar ideas have been used for clustering (Sagae and Gordon, 2009; Haffari et al., 2011; Grave et al., 2013), semantic space models (Pad´o and Lapata, 2007), and topic modeling (Boyd-Graber and Blei, 2008). 810 Representation SIM M-1 BROWN – 89.3 SENNA 49.8 85.2 TURIAN 29.5 87.2 HUANG 62.6 78.1 CBOW, w = 2 34.7 84.8 SKIP, w = 1 37.8 86.6 SKIP, w = 2 43.1 85.8 SKIP, w = 5 44.4 81.1 SKIP, w = 10 44.6 71.5 SKIPDEP 34.6 88.3 Table 2: Intrinsic evaluation of representations. SIM column has Spearman’s p × 100 for 353-pair word similarity dataset. M-1 is our unsupervised POS tagging metric. For BROWN, M-1 is simply many-to-one accuracy of the clusters. Best score </context>
</contexts>
<marker>Sagae, Gordon, 2009</marker>
<rawString>Kenji Sagae and Andrew S. Gordon. 2009. Clustering words by syntactic similarity improves dependency parsing of predicate-argument structures. In Proceedings of the 11th International Conference on Parsing Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valentin I Spitkovsky</author>
<author>Hiyan Alshawi</author>
<author>Angel X Chang</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Unsupervised dependency parsing without gold part-of-speech tags.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="20951" citStr="Spitkovsky et al., 2011" startWordPosition="3388" endWordPosition="3391">the test results for brevity. 13Note that SENNA and HUANG are trained on Wikipedia which may explain why they work better on Web parsing as compared to WSJ parsing. 14This does not guarantee a valid tree. Combining features from representations will allow training to weigh them appropriately and also guarantee a tree. 5 Related Work In addition to work mentioned above, relevant work that uses discrete representations exists for POS tagging (Ritter et al., 2011; Owoputi et al., 2013), named entity recognition (Ratinov and Roth, 2009), supersense tagging (Grave et al., 2013), grammar induction (Spitkovsky et al., 2011), constituency parsing (Finkel et al., 2008), and dependency parsing (Tratz and Hovy, 2011). Continuous representations in NLP have been evaluated for their ability to capture syntactic and semantic word similarity (Huang et al., 2012; Mikolov et al., 2013a; Mikolov et al., 2013b) and used for tasks like semantic role labeling, partof-speech tagging, NER, chunking, and sentiment classification (Turian et al., 2010; Collobert et al., 2011; Dhillon et al., 2012; Al-Rfou’ et al., 2013). For dependency parsing, Hisamoto et al. (2013) also used embedding features, but there are several differences </context>
</contexts>
<marker>Spitkovsky, Alshawi, Chang, Jurafsky, 2011</marker>
<rawString>Valentin I. Spitkovsky, Hiyan Alshawi, Angel X. Chang, and Daniel Jurafsky. 2011. Unsupervised dependency parsing without gold part-of-speech tags. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oscar T¨ackstr¨om</author>
<author>Ryan McDonald</author>
<author>Jakob Uszkoreit</author>
</authors>
<title>Cross-lingual word clusters for direct transfer of linguistic structure.</title>
<date>2012</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<marker>T¨ackstr¨om, McDonald, Uszkoreit, 2012</marker>
<rawString>Oscar T¨ackstr¨om, Ryan McDonald, and Jakob Uszkoreit. 2012. Cross-lingual word clusters for direct transfer of linguistic structure. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Tratz</author>
<author>Eduard Hovy</author>
</authors>
<title>A fast, accurate, non-projective, semantically-enriched parser.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="21042" citStr="Tratz and Hovy, 2011" startWordPosition="3401" endWordPosition="3404">xplain why they work better on Web parsing as compared to WSJ parsing. 14This does not guarantee a valid tree. Combining features from representations will allow training to weigh them appropriately and also guarantee a tree. 5 Related Work In addition to work mentioned above, relevant work that uses discrete representations exists for POS tagging (Ritter et al., 2011; Owoputi et al., 2013), named entity recognition (Ratinov and Roth, 2009), supersense tagging (Grave et al., 2013), grammar induction (Spitkovsky et al., 2011), constituency parsing (Finkel et al., 2008), and dependency parsing (Tratz and Hovy, 2011). Continuous representations in NLP have been evaluated for their ability to capture syntactic and semantic word similarity (Huang et al., 2012; Mikolov et al., 2013a; Mikolov et al., 2013b) and used for tasks like semantic role labeling, partof-speech tagging, NER, chunking, and sentiment classification (Turian et al., 2010; Collobert et al., 2011; Dhillon et al., 2012; Al-Rfou’ et al., 2013). For dependency parsing, Hisamoto et al. (2013) also used embedding features, but there are several differences between their work and ours. First, they use only one set of pre-trained embeddings (TURIAN</context>
</contexts>
<marker>Tratz, Hovy, 2011</marker>
<rawString>Stephen Tratz and Eduard Hovy. 2011. A fast, accurate, non-projective, semantically-enriched parser. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev-Arie Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: A simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1143" citStr="Turian et al., 2010" startWordPosition="163" endWordPosition="166">pes of features, in both news and web domains. We find that all embeddings yield significant parsing gains, including some recent ones that can be trained in a fraction of the time of others. Explicitly tailoring the representations for the task leads to further improvements. Moreover, an ensemble of all representations achieves the best results, suggesting their complementarity. 1 Introduction Word representations derived from unlabeled text have proven useful for many NLP tasks, e.g., partof-speech (POS) tagging (Huang et al., 2014), named entity recognition (Miller et al., 2004), chunking (Turian et al., 2010), and syntactic parsing (Koo et al., 2008; Finkel et al., 2008; T¨ackstr¨om et al., 2012). Most word representations fall into one of two categories. Discrete representations consist of memberships in a (possibly hierarchical) hard clustering of words, e.g., via kmeans or the Brown et al. (1992) algorithm. Continuous representations (or distributed representations or embeddings) consist of low-dimensional, real-valued vectors for each word, typically induced via neural language models (Bengio et al., 2003; Mnih and Hinton, 2007) or spectral methods (Deerwester et al., 1990; Dhillon et al., 201</context>
<context position="4308" citStr="Turian et al. (2010)" startWordPosition="658" endWordPosition="661">ted in neural language models (Bengio et al., 2003), which use neural networks and local context to learn word vectors. Several researchers have made their trained representations publicly avail809 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 809–815, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics Representation Source Corpus Types, Tokens V D Time BROWN Koo et al. (2008) BLLIP 317K, 43M 316,710 – 2.5 days† SENNA Collobert et al. (2011) Wikipedia 8.3M, 1.8B 130,000 50 2 months* TURIAN Turian et al. (2010) RCV1 269K, 37M 268,810 50 few weeks* HUANG Huang et al. (2012) Wikipedia 8.3M, 1.8B 100,232 50 — CBOW, SKIP, SKIPDEP Mikolov et al. (2013a) BLLIP 317K, 43M 316,697 100 2-4 mins.† Table 1: Details of word representations used, including datasets, vocabulary size V , and dimensionality D. Continuous representations require an additional 4 hours to run hierarchical clustering to generate features (§3.2). RCV1 = Reuters Corpus, Volume 1. ∗ = time reported by authors. † = run by us on a 3.50 GHz desktop, using a single thread. able, which we use directly in our experiments. In particular, we use t</context>
<context position="21368" citStr="Turian et al., 2010" startWordPosition="3450" endWordPosition="3453">xists for POS tagging (Ritter et al., 2011; Owoputi et al., 2013), named entity recognition (Ratinov and Roth, 2009), supersense tagging (Grave et al., 2013), grammar induction (Spitkovsky et al., 2011), constituency parsing (Finkel et al., 2008), and dependency parsing (Tratz and Hovy, 2011). Continuous representations in NLP have been evaluated for their ability to capture syntactic and semantic word similarity (Huang et al., 2012; Mikolov et al., 2013a; Mikolov et al., 2013b) and used for tasks like semantic role labeling, partof-speech tagging, NER, chunking, and sentiment classification (Turian et al., 2010; Collobert et al., 2011; Dhillon et al., 2012; Al-Rfou’ et al., 2013). For dependency parsing, Hisamoto et al. (2013) also used embedding features, but there are several differences between their work and ours. First, they use only one set of pre-trained embeddings (TURIAN) while we compare several and also train our own, tailored to the task. Second, their embedding features are simpler than ours, only using flat (non-hierarchical) cluster IDs and binary strings obtained via sign quantization (11[x &gt; 0]) of the vectors. They also compare to a first-order baseline and only evaluate on the Web</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vadas</author>
<author>James R Curran</author>
</authors>
<title>Adding noun phrase structure to the Penn Treebank.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="15032" citStr="Vadas and Curran (2007)" startWordPosition="2439" endWordPosition="2442">r vector distance (via MATLAB’s linkage function with {method=ward, metric=euclidean}). Next, we fire features on the hierarchical clustering bit strings using templates identical to those for BROWN, except that we use longer prefixes as our clustering hierarchies tend to be deeper.8 4 Parsing Experiments Setup: We use the publicly-available MSTParser for all experiments, specifically its secondorder projective model.9 We remove all features that occur only once in the training data. For WSJ parsing, we use the standard train(02- 21)/dev(22)/test(23) split and apply the NP bracketing patch by Vadas and Curran (2007). For Web parsing, we still train on WSJ 02-21, but test on the five Web domains (answers, email, newsgroup, reviews, and weblog) of the ‘English Web Treebank’ (LDC2012T13), splitting each domain in half (in original order) for the development and test sets.10 For both treebanks, we convert from constituent to dependency format using pennconverter (Johansson and Nugues, 2007), and generate POS tags using the MXPOST tagger (Ratnaparkhi, 1996). To evaluate, we use 6Our bucketing function bucketk(x) converts the real value x to its closest multiple of k. We choose a k value of around 1/5th of the</context>
</contexts>
<marker>Vadas, Curran, 2007</marker>
<rawString>David Vadas and James R. Curran. 2007. Adding noun phrase structure to the Penn Treebank. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joe H Ward</author>
</authors>
<title>Hierarchical grouping to optimize an objective function.</title>
<date>1963</date>
<journal>Journal of the American statistical association,</journal>
<pages>58--301</pages>
<contexts>
<context position="14360" citStr="Ward, 1963" startWordPosition="2338" endWordPosition="2339">nsists of the dimension index d and a bucketed version of the embedding value in that dimension, i.e., bucketk(Evd) for word index v and dimension d, where E is the V × D embedding matrix.6 We also tried standard conjunction variants of this feature consisting of the bucket values of both the head and argument along with their POS-tag or word information, and the attachment distance and direction.7 Cluster bit string features: To take into account all dimensions simultaneously, we perform agglomerative hierarchical clustering of the embedding vectors. We use Ward’s minimum variance algorithm (Ward, 1963) for cluster distance and the Euclidean metric for vector distance (via MATLAB’s linkage function with {method=ward, metric=euclidean}). Next, we fire features on the hierarchical clustering bit strings using templates identical to those for BROWN, except that we use longer prefixes as our clustering hierarchies tend to be deeper.8 4 Parsing Experiments Setup: We use the publicly-available MSTParser for all experiments, specifically its secondorder projective model.9 We remove all features that occur only once in the training data. For WSJ parsing, we use the standard train(02- 21)/dev(22)/tes</context>
</contexts>
<marker>Ward, 1963</marker>
<rawString>Joe H. Ward. 1963. Hierarchical grouping to optimize an objective function. Journal of the American statistical association, 58(301):236–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyasu Yamada</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines.</title>
<date>2003</date>
<booktitle>In Proceedings of International Conference on Parsing Technologies.</booktitle>
<contexts>
<context position="18169" citStr="Yamada and Matsumoto, 2003" startWordPosition="2935" endWordPosition="2938"> statistically significant gains over the baseline from all embeddings. The CBOW embeddings perform as well as BROWN (i.e., no statistically significant difference) but are orders of magnitude faster to train. Finally, the syntacticallytrained SKIPDEP embeddings are statistically indistinguishable from BROWN and CBOW, and significantly better than all other embeddings. This suggests that targeting the similarity captured by syntactic context is useful for dependency parsing. 11We find similar improvements under labeled attachment score (LAS). We ignore punctuation : , “ ” . in our evaluation (Yamada and Matsumoto, 2003; McDonald et al., 2005). 812 System ans eml nwg rev blog Avg Baseline 82.6 81.2 84.3 83.8 85.5 83.5 BROWN 83.4 81.7 85.2 84.5 86.1 84.2 SENNA 83.7 81.9 85.0 85.0 86.0 84.3 TURIAN 83.0 81.5 85.0 84.1 85.7 83.9 HUANG 83.1 81.8 85.1 84.7 85.9 84.1 CBOW 82.9 81.3 85.2 83.9 85.8 83.8 SKIP 83.1 81.1 84.7 84.1 85.4 83.7 SKIPDEP 83.3 81.5 85.2 84.3 86.0 84.1 Ensemble Results ALL–BR 83.9 82.2 85.9 85.0 86.6 84.7 ALL 84.2 82.3 85.9 85.1 86.8 84.9 Table 6: Main UAS test results on Web treebanks. Here, ans=answers, eml=email, nwg=newsgroup, rev=reviews, blog=weblog, BR=BROWN, Avg=Macro-average. Web resul</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical dependency analysis with support vector machines. In Proceedings of International Conference on Parsing Technologies.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>