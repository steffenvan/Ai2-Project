<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.016029">
<title confidence="0.9871085">
Multilingual Dependency Learning:
A Huge Feature Engineering Method to Semantic Dependency Parsing �
</title>
<author confidence="0.999225">
Hai Zhao(赵海)†*, Wenliang Chen(陈文亮)$, Chunyu Kit†, Guodong Zhou*
</author>
<affiliation confidence="0.998245">
†Department of Chinese, Translation and Linguistics
City University of Hong Kong
</affiliation>
<address confidence="0.943866">
83 Tat Chee Avenue, Kowloon, Hong Kong, China
</address>
<affiliation confidence="0.863503">
$Language Infrastructure Group, MASTAR Project
National Institute of Information and Communications Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289
*School of Computer Science and Technology
</affiliation>
<address confidence="0.880644">
Soochow University, Suzhou, China 215006
</address>
<email confidence="0.998303">
haizhao@cityu.edu.hk, chenwl@nict.go.jp
</email>
<sectionHeader confidence="0.99563" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999571928571429">
This paper describes our system about mul-
tilingual semantic dependency parsing (SR-
Lonly) for our participation in the shared task
of CoNLL-2009. We illustrate that semantic
dependency parsing can be transformed into
a word-pair classification problem and im-
plemented as a single-stage machine learning
system. For each input corpus, a large scale
feature engineering is conducted to select the
best fit feature template set incorporated with a
proper argument pruning strategy. The system
achieved the top average score in the closed
challenge: 80.47% semantic labeled F1 for the
average score.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999496363636364">
The syntactic and semantic dependency parsing in
multiple languages introduced by the shared task
of CoNLL-2009 is an extension of the CoNLL-
2008 shared task (Hajiˇc et al., 2009). Seven lan-
guages, English plus Catalan, Chinese, Czech, Ger-
man, Japanese and Spanish, are involved (Taul´e et
al., 2008; Palmer and Xue, 2009; Hajiˇc et al., 2006;
Surdeanu et al., 2008; Burchardt et al., 2006; Kawa-
hara et al., 2002). This paper presents our research
for participation in the semantic-only (SRLonly)
challenge of the CoNLL-2009 shared task, with a
</bodyText>
<note confidence="0.952058666666666">
This study is partially supported by CERG grant 9040861
(CityU 1318/03H), CityU Strategic Research Grant 7002037,
Projects 60673041 and 60873041 under the National Natural
Science Foundation of China and Project 2006AA01Z147 un-
der the ”863” National High-Tech Research and Development
of China.
</note>
<page confidence="0.99413">
55
</page>
<bodyText confidence="0.991587">
highlight on our strategy to select features from a
large candidate set for maximum entropy learning.
</bodyText>
<sectionHeader confidence="0.987989" genericHeader="introduction">
2 System Survey
</sectionHeader>
<bodyText confidence="0.999994636363637">
We opt for the maximum entropy model with Gaus-
sian prior as our learning model for all classification
subtasks in the shared task. Our implementation of
the model adopts L-BFGS algorithm for parameter
optimization as usual. No additional feature selec-
tion techniques are applied.
Our system is basically improved from its early
version for CoNLL-2008 (Zhao and Kit, 2008). By
introducing a virtual root for every predicates, The
job to determine both argument labels and predicate
senses is formulated as a word-pair classification
task in four languages, namely, Catalan, Spanish,
Czech and Japanese. In other three languages, Chi-
nese, English and German, a predicate sense clas-
sifier is individually trained before argument label
classification. Note that traditionally (or you may
say that most semantic parsing systems did so) ar-
gument identification and classification are handled
in a two-stage pipeline, while ours always tackles
them in one step, in addition, predicate sense classi-
fication are also included in this unique learning/test
step for four of all languages.
</bodyText>
<sectionHeader confidence="0.956313" genericHeader="method">
3 Pruning Argument Candidates
</sectionHeader>
<bodyText confidence="0.999863666666667">
We keep using a word-pair classification procedure
to formulate semantic dependency parsing. Specif-
ically, we specify the first word in a word pair as a
predicate candidate (i.e., a semantic head, and noted
as p in our feature representation) and the next as an
argument candidate (i.e., a semantic dependent, and
</bodyText>
<note confidence="0.58051">
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 55–60,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.9944546">
noted as a). We do not differentiate between verbal
and non-verbal predicates and our system handles
them in the exactly same way.
When no constraint available, however, all word
pairs in the an input sequence must be considered,
leading to very poor efficiency in computation for
no gain in effectiveness. Thus, the training sample
needs to be pruned properly. As predicates overtly
known in the share task, we only consider how to
effectively prune argument candidates.
We adopt five types of argument pruning strate-
gies for seven languages. All of them assume that a
syntactic dependency parsing tree is available.
As for Chinese and English, we continue to use
a dependency version of the pruning algorithm of
(Xue and Palmer, 2004) as described in (Zhao and
Kit, 2008). The pruning algorithm is readdressed as
the following.
Initialization: Set the given predicate candidate
as the current node;
</bodyText>
<listItem confidence="0.93612675">
(1) The current node and all of its syntactic chil-
dren are selected as argument candidates.
(2) Reset the current node to its syntactic head and
repeat step (1) until the root is reached.
</listItem>
<bodyText confidence="0.9998047">
Note that the given predicate candidate itself is
excluded from the argument candidate list for Chi-
nese, that is slightly different from English.
The above pruning algorithm has been shown ef-
fective. However, it is still inefficient for a single-
stage argument identification/classification classifi-
cation task. Thus we introduce an assistant argument
label ‘ NoMoreArgument’ to alleviate this difficulty.
If an argument candidate in the above algorithm is
labeled as such a label, then the pruning algorithm
will end immediately. In training, this assistant label
means no more samples will be generated for the
current predicate, while in test, the decoder will not
search more argument candidates any more. This
adaptive technique more effectively prunes the ar-
gument candidates. In fact, our experiments show
1/3 training memory and time may be saved from it.
As for Catalan and Spanish, only syntactic chil-
dren of the predicate are considered as the argument
candidates.
As for Czech, only syntactic children, grandchil-
dren, great-grandchildren, parent and siblings of the
predicate are taken as the argument candidates.
As for German, only syntactic children, grand-
children, parent, siblings, siblings of parent and sib-
lings of grandparent of the predicate are taken as the
argument candidates.
The case is somewhat sophisticated for Japanese.
As we cannot identify a group of simple predicate-
argument relations from the syntactic tree. Thus
we consider top frequent 28 syntactic relations be-
tween the predicate and the argument. The parser
will search all words before and after the predicate,
and only those words that hold one of the 28 syn-
tactic relations to the predicate are considered as
the argument candidate. Similar to the pruning al-
gorithm for Chinese/English/German, we also in-
troduce two assistant labels ‘ leftNoMoreArgument’
and ‘ rightNoMoreArgument’ to adaptively prune
words too far away from the predicate.
</bodyText>
<sectionHeader confidence="0.991486" genericHeader="method">
4 Feature Templates
</sectionHeader>
<bodyText confidence="0.997816782608696">
As we don’t think that we can benefit from know-
ing seven languages, an automatic feature template
selection is conducted for each language.
About 1000 feature templates (hereafter this tem-
plate set is referred to FT) are initially considered.
These feature templates are from various combina-
tions or integrations of the following basic elements.
Word Property. This type of elements include
word form, lemma, part-of-speech tag (PoS), FEAT
(additional morphological features), syntactic de-
pendency label (dprel), semantic dependency label
(semdprel) and characters (char) in the word form
(only suitable for Chinese and Japanese)1.
Syntactic Connection. This includes syntactic
head (h), left(right) farthest(nearest) child (lm, ln,
rm, and rn), and high(low) support verb or noun.
We explain the last item, support verb(noun). From
the predicate or the argument to the syntactic root
along the syntactic tree, the first verb(noun) that is
met is called as the low support verb(noun), and the
nearest one to the root is called as the high support
verb(noun).
Semantic Connection. This includes semantic
</bodyText>
<footnote confidence="0.9690635">
1All lemmas, PoS, and FEAT for either training or test are
from automatically pre-analyzed columns of every input files.
</footnote>
<page confidence="0.985375">
56
</page>
<table confidence="0.821096333333333">
FEATn 1 2 3 4 5 6 7 8 9 10 11
Catalan/Spanish postype gen num person mood tense punct
Czech SubPOS Gen Num Cas Neg Gra Voi Var Sem Per Ten
</table>
<tableCaption confidence="0.997223">
Table 1: Notations of FEATs
</tableCaption>
<bodyText confidence="0.993798631578948">
head (semhead), left(right) farthest(nearest) seman-
tic child (semlm, semln, semrm, semrn). We say
a predicate is its argument’s semantic head, and the
latter is the former’s child. Features related to this
type may track the current semantic parsing status.
Path. There are two basic types of path between
the predicate and the argument candidates. One is
the linear path (linePath) in the sequence, the other
is the path in the syntactic parsing tree (dpPath). For
the latter, we further divide it into four sub-types
by considering the syntactic root, dpPath is the full
path in the syntactic tree. Leading two paths to the
root from the predicate and the argument, respec-
tively, the common part of these two paths will be
dpPathShare. Assume that dpPathShare starts from
a node r&apos;, then dpPathPred is from the predicate to
r&apos;, and dpPathArgu is from the argument to r&apos;.
Family. Two types of children sets for the predi-
cate or argument candidate are considered, the first
includes all syntactic children (children), the second
also includes all but excludes the left most and the
right most children (noFarChildren).
Concatenation of Elements. For all collected el-
ements according to linePath, children and so on, we
use three strategies to concatenate all those strings
to produce the feature value. The first is seq, which
concatenates all collected strings without doing any-
thing. The second is bag, which removes all dupli-
cated strings and sort the rest. The third is noDup,
which removes all duplicated neighbored strings.
In the following, we show some feature template
examples derived from the above mentioned items.
a.lm.lemma The lemma of the left most child of
the argument candidate.
p.h.dprel The dependant label of the syntactic
head of the predicate candidate.
a.pos+p.pos The concatenation of PoS of the ar-
gument and the predicate candidates.
p_i.pos+p.pos PoS of the previous word of the
predicate and PoS of the predicate itself.
a:pIdpPath.lemma.bag Collect all lemmas along
the syntactic tree path from the argument to the pred-
icate, then removed all duplicated ones and sort the
rest, finally concatenate all as a feature string.
a:p.highSupportNounllinePath.dprel.seq Collect
all dependant labels along the line path from the ar-
gument to the high support noun of the predicate,
then concatenate all as a feature string.
(a:pjdpPath.dprel.seq)+p.FEAT] Collect all de-
pendant labels along the line path from the argument
to the predicate and concatenate them plus the first
FEAT of the predicate.
An important feature for the task is dpTreeRela-
tion, which returns the relationship of a and p in a
syntactic parse tree and cannot be derived from com-
bining the above basic elements. The possible values
for this feature include parent, sibling etc.
</bodyText>
<sectionHeader confidence="0.8581925" genericHeader="method">
5 Automatically Discovered Feature
Template Sets
</sectionHeader>
<bodyText confidence="0.999953652173913">
For each language, starting from a basic feature tem-
plate set (a small subset of FT) according to our
previous result in English dependency parsing, each
feature template outside the basic set is added and
each feature template inside the basic set is removed
one by one to check the effectiveness of each fea-
ture template following the performance change in
the development set. This procedure will be contin-
uously repeated until no feature template is added or
removed or the performance is not improved.
There are some obvious heuristic rules that help
us avoid trivial feature template checking, for ex-
ample, FEAT features are only suitable for Cata-
lan, Czech and Spanish. Though FEAT features are
also available for Japanese, we don’t adopt them for
this language due to the hight training cost. To sim-
plify feature representation, we use FEAT], FEAT2,
and so on to represent different FEAT for every lan-
guages. A lookup list can be found in Table 1. Ac-
cording to the list, FEAT4 represents person for
Catalan or Spanish, but Cas for Czech.
As we don’t manually interfere the selection pro-
cedure for feature templates, ten quite different fea-
</bodyText>
<page confidence="0.986845">
57
</page>
<table confidence="0.999572">
Ca Ch Cz En Gr Jp Sp
Ca 53
Ch 5 75
Cz 11 10 76
En 11 11 12 73
Gr 7 7 7 14 45
Jp 6 22 13 15 10 96
Sp 22 9 18 15 9 12 66
</table>
<tableCaption confidence="0.932557">
Table 2: Feature template set: argument classifier
</tableCaption>
<table confidence="0.99905825">
Ch En Gr
Ch 46
En 5 9
Gr 17 2 40
</table>
<tableCaption confidence="0.999814">
Table 3: Feature template set: sense classifier
</tableCaption>
<bodyText confidence="0.998667173913043">
ture template sets are obtained at last. Statistical in-
formation of seven sets for argument classifiers is in
Table 2, and those for sense classifiers are in Table 3.
Numbers in the diagonals of these two tables mean
the numbers of feature templates, and others mean
how many feature templates are identical for every
language pairs. The most matched feature template
sets are for Catalan/Spanish and Chinese/Japanese.
As for the former, it is not so surprised because these
two corpora are from the same provider.
Besides the above statistics, these seven feature
template sets actually share little in common. For
example, the intersection set from six languages, as
Chinese is excluded, only includes one feature tem-
plate, p.lemma (the lemma of the predicate candi-
date). If all seven sets are involved, then such an in-
tersection set will be empty. Does this mean human
languages share little in semantic representation? :)
It is unlikely to completely demonstrate full fea-
ture template sets for all languages in this short re-
port, we thus only demonstrate two sets, one for En-
glish sense classification in Table 4 and the other for
Catalan argument classification in Table 52.
</bodyText>
<tableCaption confidence="0.995284">
Table 4: Feature set for English sense classification
</tableCaption>
<bodyText confidence="0.9998206">
example in the given dictionary. Unfortunately, we
late found this caused a poor performance in sense
determination. This time, an individual classifier is
used to determine the sense for Chinese, English or
German, and this is done by the argument classifier
by introducing a virtual root for every predicates for
the rest four languages3. Features used for sense
determination are also selected following the same
procedure in Section 5. The difference is only pred-
icate related features are used for selection.
</bodyText>
<sectionHeader confidence="0.993173" genericHeader="method">
7 Decoding
</sectionHeader>
<bodyText confidence="0.9999325">
The decoding for four languages, Catalan, Czech,
Japanese and Spanish is trivial, each word pairs will
be checked one by one. The first word of the pair
is the virtual root or the predicate, the second is the
predicate or every argument candidates. Argument
candidates are checked in the order of different syn-
tactic relations to their predicate, which are enumer-
ated by the pruning algorithms in Section 3, or from
left to right for the same syntactic relation. After
the sense of the predicate is determined, the label of
each argument candidate will be directly classified,
or, it is proved non-argument.
As for the rest languages, Chinese, English or
German, after the sense classifier outputs its result,
an optimal argument structure for each predicate is
determined by the following maximal probability.
</bodyText>
<figure confidence="0.979014818181818">
p.lm.pos
p.rm.pos
p.lemma
p.lemma + p.lemma1
p.lemma + p.children.dprel.noDup
p.lemma + p.currentSense
p.form
p.form−1 + p.form
p.form + p.form1
6 Word Sense Determination �Sp = argmax Pai|ai−1, ai−2, ...), (1)
i
</figure>
<bodyText confidence="0.978869333333333">
The shared task of CoNLL-2009 still asks for the
predicate sense. In our work for CoNLL-2008 (Zhao
and Kit, 2008), this was done by searching for a right
</bodyText>
<footnote confidence="0.848051">
2Full feature lists and their explanation for all languages will
be available at the website, http://bcmi.sjtu.edu.cn/˜zhaohai.
</footnote>
<bodyText confidence="0.972674">
where Sp is the argument structure, Pai|ai−1...)
is the conditional probability to determine the la-
bel of the i-th argument candidate label. Note that
</bodyText>
<footnote confidence="0.999704">
3For Japanese, no senses for predicates are defined. Thus it
is actually a trivial classification task in this case.
</footnote>
<page confidence="0.998839">
58
</page>
<tableCaption confidence="0.999427">
Table 5: Feature set for Catalan argument classification
</tableCaption>
<bodyText confidence="0.9888034">
P(aZ|az_1, ...) in equation (1) may be simplified as
P(az) if the input feature template set does not con-
cerned with the previous argument label output. A
beam search algorithm is used to find the parsing de-
cision sequence.
</bodyText>
<sectionHeader confidence="0.940597" genericHeader="evaluation">
8 Evaluation Results
</sectionHeader>
<bodyText confidence="0.999982487179487">
Our evaluation is carried out on two computational
servers, (1) LEGA, a 64-bit ubuntu Linux installed
server with double dual-core AMD Opteron proces-
sors of 2.8GHz and 24GB memory. This server was
also used for our previous participation in CoNLL-
2008 shared task. (2) MEGA, a 64-bit ubuntu Linux
installed server with six quad-core Intel Xeon pro-
cessors of 2.33GHz and 128GB memory.
Altogether nearly 60,000 machine learning rou-
tines were run to select the best fit feature template
sets for all seven languages within two months. Both
LEGA and MEGA were used for this task. How-
ever, training and test for the final submission of
Chinese, Czech and English run in MEGA, and the
rest in LEGA. As we used multiple thread training
and multiple routines run at the same time, the exact
time cost for either training or test is hard to esti-
mate. Here we just report the actual time and mem-
ory cost in Table 7 for reference.
The official evaluation results of our system are in
Table 6. Numbers in bold in the table stand for the
best performances for the specific languages. The
results in development sets are also given. The first
row of the table reports the results using golden in-
put features.
Two facts as the following suggest that our system
does output robust and stable results. The first is that
two results for development and test sets in the same
language are quite close. The second is about out-of-
domain (OOD) task. Though for each OOD task, we
just used the same model trained from the respective
language and did nothing to strengthen it, this does
not hinder our system to obtain top results in Czech
and English OOD tasks.
In addition, the feature template sets from auto-
matical selection procedure in this task were used
for the joint task of this shared task, and also output
top results according to the average score of seman-
tic labeled F1 (Zhao et al., 2009).
</bodyText>
<figure confidence="0.978716711538462">
p.currentSense + p.lemma
p.currentSense + p.pos
p.currentSense + a.pos
p−1.FEAT1
p.FEAT2
p1.FEAT3
p.semrm.semdprel
p.lm.dprel
p.form + p.children.dprel.bag
p.lemman (n = −1, 0)
p.lemma + p.lemma1
p.pos−1 + p.pos
p.pos1
p.pos + p.children.dprel.bag
a.FEAT1 + a.FEAT3 + a.FEAT4
+ a.FEAT5 + a.FEAT6
a−1.FEAT2 + a.FEAT2
a.FEAT3 + a1.FEAT3
a.FEAT3 + a.h.FEAT3
a.children.FEAT1.noDup
a.children.FEAT3.bag
a.h.lemma
a.lm.dprel + a.form
a.lm.form
a.lm−1.lemma
a.lmn.pos (n=0,1)
a.noFarChildren.pos.bag + a.rm.form
a.pphead.lemma
a.rm.dprel + a.form
a.rm−1.form
a.rm.lemma
a.rn.dprel + a.form
a.lowSupportVerb.lemma
a−1.form
a.form + a1.form
a.form + a.children.pos
a.lemma + a.h.form
a.lemma + a.pphead.form
a1.lemma
a1.pos + a.pos.seq
a.pos + a.children.dprel.bag
a.lemma + p.lemma
(a:p|dpPath.dprel) + p.FEAT1
a:p|linePath.distance
a:p|linePath.FEAT1.bag
a:p|linePath.form.seq
a:p|linePath.lemma.seq
a:p|linePath.dprel.seq
a:p|dpPath.lemma.seq
a:p|dpPath.lemma.bag
a:p|dpPathArgu.lemma.seq
a:p|dpPathArgu.lemma.bag
</figure>
<page confidence="0.991042">
59
</page>
<table confidence="0.9968634">
average Catalan Chinese Czech English German Japanese Spanish
Development with Gold 81.24 81.52 78.32 86.96 84.19 77.75 78.67 81.32
Development 80.46 80.66 77.90 85.35 84.01 76.55 78.41 80.39
Test (official scores) 80.47 80.32 77.72 85.19 85.44 75.99 78.15 80.46
Out-of-domain 74.34 85.44 73.31 64.26
</table>
<tableCaption confidence="0.912204">
Table 6: Semantic labeled F1
</tableCaption>
<table confidence="0.999649142857143">
Catalan Chinese Czech English German Japanese Spanish
Sense Training memory (MB) 418.0 136.0 63.0
Training time (Min.) 11.0 2.5 1.7
Test time (Min.) 0.7 0.2 0.03
Argument Training memory (GB) 0.4 3.7 3.2 3.8 0.2 1.4 0.4
Training time (Hours) 3.0 13.8 24.9 12.4 0.2 6.1 4.4
Test time (Min.) 3.0 144.0 27.1 88.0 1.0 4.2 7.0
</table>
<tableCaption confidence="0.99938">
Table 7: Computational cost
</tableCaption>
<sectionHeader confidence="0.995876" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.999942">
As presented in the above sections, we have tackled
semantic parsing for the CoNLL-2009 shared task
as a word-pair classification problem. Incorporated
with a proper argument candidate pruning strategy
and a large scale feature engineering for each lan-
guage, our system produced top results.
</bodyText>
<sectionHeader confidence="0.999458" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999881344827587">
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pad´o, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In Proceedings of the 5th Interna-
tional Conference on Language Resources and Evalu-
ation (LREC-2006), Genoa, Italy.
Jan Hajiˇc, Jarmila Panevov´a, Eva Hajiˇcov´a, Petr
Sgall, Petr Pajas, Jan ˇStˇep´anek, JiˇrfHavelka, Marie
Mikulov´a, and Zdenˇek ˇZabokrtsk´y. 2006. Prague De-
pendency Treebank 2.0.
Jan Hajiˇc, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Martf, Llufs
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad´o, Jan ˇStˇep´anek, Pavel Straˇn´ak, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic depen-
dencies in multiple languages. In Proceedings of
the 13th Conference on Computational Natural Lan-
guage Learning (CoNLL-2009), June 4-5, Boulder,
Colorado, USA.
Daisuke Kawahara, Sadao Kurohashi, and Kˆoiti Hasida.
2002. Construction of a Japanese relevance-tagged
corpus. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation
(LREC-2002), pages 2008–2013, Las Palmas, Canary
Islands.
Martha Palmer and Nianwen Xue. 2009. Adding seman-
tic roles to the Chinese Treebank. Natural Language
Engineering, 15(1):143–172.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llufs M`arquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In Proceedings of the 12th Con-
ference on Computational Natural Language Learning
(CoNLL-2008).
Mariona Taul´e, Maria Ant`onia Martf, and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Corpora
for Catalan and Spanish. In Proceedings of the 6th
International Conference on Language Resources and
Evaluation (LREC-2008), Marrakesh, Morroco.
Nianwen Xue and Martha Palmer. 2004. Calibrating fea-
tures for semantic role labeling. In 2004 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP-2004), pages 88–94, Barcelona, Spain,
July 25-26.
Hai Zhao and Chunyu Kit. 2008. Parsing syntac-
tic and semantic dependencies with two single-stage
maximum entropy models. In Twelfth Conference on
Computational Natural Language Learning (CoNLL-
2008), pages 203–207, Manchester, UK, August 16-
17.
Hai Zhao, Wenliang Chen, Jun’ichi Kazama, Kiyotaka
Uchimoto, and Kentaro Torisawa. 2009. Multilin-
gual dependency learning: Exploiting rich features for
tagging syntactic and semantic dependencies. In Pro-
ceedings of the 13th Conference on Computational
Natural Language Learning (CoNLL-2009), June 4-5,
Boulder, Colorado, USA.
</reference>
<page confidence="0.998415">
60
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.483416">
<title confidence="0.99921">Multilingual Dependency Learning: Huge Feature Engineering Method to Semantic Dependency Parsing</title>
<author confidence="0.913937">Wenliang Chunyu Guodong</author>
<affiliation confidence="0.9317215">of Chinese, Translation and City University of Hong</affiliation>
<address confidence="0.804404">83 Tat Chee Avenue, Kowloon, Hong Kong,</address>
<affiliation confidence="0.96602">Infrastructure Group, MASTAR National Institute of Information and Communications</affiliation>
<address confidence="0.976104">3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan,</address>
<affiliation confidence="0.8941045">of Computer Science and Soochow University, Suzhou, China</affiliation>
<email confidence="0.967332">haizhao@cityu.edu.hk,chenwl@nict.go.jp</email>
<abstract confidence="0.999682533333333">This paper describes our system about multilingual semantic dependency parsing (SR- Lonly) for our participation in the shared task of CoNLL-2009. We illustrate that semantic dependency parsing can be transformed into a word-pair classification problem and implemented as a single-stage machine learning system. For each input corpus, a large scale feature engineering is conducted to select the best fit feature template set incorporated with a proper argument pruning strategy. The system achieved the top average score in the closed challenge: 80.47% semantic labeled F1 for the average score.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Aljoscha Burchardt</author>
<author>Katrin Erk</author>
<author>Anette Frank</author>
<author>Andrea Kowalski</author>
<author>Sebastian Pad´o</author>
<author>Manfred Pinkal</author>
</authors>
<title>The SALSA corpus: a German corpus resource for lexical semantics.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC-2006),</booktitle>
<location>Genoa, Italy.</location>
<marker>Burchardt, Erk, Frank, Kowalski, Pad´o, Pinkal, 2006</marker>
<rawString>Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea Kowalski, Sebastian Pad´o, and Manfred Pinkal. 2006. The SALSA corpus: a German corpus resource for lexical semantics. In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC-2006), Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
<author>Jarmila Panevov´a</author>
<author>Eva Hajiˇcov´a</author>
<author>Petr Sgall</author>
<author>Petr Pajas</author>
<author>Jan ˇStˇep´anek</author>
<author>Marie Mikulov´a JiˇrfHavelka</author>
<author>Zdenˇek ˇZabokrtsk´y</author>
</authors>
<date>2006</date>
<journal>Prague Dependency Treebank</journal>
<volume>2</volume>
<marker>Hajiˇc, Panevov´a, Hajiˇcov´a, Sgall, Pajas, ˇStˇep´anek, JiˇrfHavelka, ˇZabokrtsk´y, 2006</marker>
<rawString>Jan Hajiˇc, Jarmila Panevov´a, Eva Hajiˇcov´a, Petr Sgall, Petr Pajas, Jan ˇStˇep´anek, JiˇrfHavelka, Marie Mikulov´a, and Zdenˇek ˇZabokrtsk´y. 2006. Prague Dependency Treebank 2.0.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jan Hajiˇc</author>
<author>Massimiliano Ciaramita</author>
<author>Richard Johansson</author>
<author>Daisuke Kawahara</author>
<author>Maria Ant`onia Martf</author>
<author>Llufs M`arquez</author>
<author>Adam Meyers</author>
<author>Joakim Nivre</author>
<author>Sebastian Pad´o</author>
<author>Jan ˇStˇep´anek</author>
<author>Pavel Straˇn´ak</author>
<author>Mihai Surdeanu</author>
<author>Nianwen Xue</author>
<author>Yi Zhang</author>
</authors>
<title>The CoNLL2009 shared task: Syntactic and semantic dependencies in multiple languages.</title>
<date>2009</date>
<booktitle>In Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL-2009),</booktitle>
<location>Boulder, Colorado, USA.</location>
<marker>Hajiˇc, Ciaramita, Johansson, Kawahara, Martf, M`arquez, Meyers, Nivre, Pad´o, ˇStˇep´anek, Straˇn´ak, Surdeanu, Xue, Zhang, 2009</marker>
<rawString>Jan Hajiˇc, Massimiliano Ciaramita, Richard Johansson, Daisuke Kawahara, Maria Ant`onia Martf, Llufs M`arquez, Adam Meyers, Joakim Nivre, Sebastian Pad´o, Jan ˇStˇep´anek, Pavel Straˇn´ak, Mihai Surdeanu, Nianwen Xue, and Yi Zhang. 2009. The CoNLL2009 shared task: Syntactic and semantic dependencies in multiple languages. In Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL-2009), June 4-5, Boulder, Colorado, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
<author>Kˆoiti Hasida</author>
</authors>
<title>Construction of a Japanese relevance-tagged corpus.</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd International Conference on Language Resources and Evaluation (LREC-2002),</booktitle>
<pages>2008--2013</pages>
<location>Las Palmas, Canary Islands.</location>
<contexts>
<context position="1620" citStr="Kawahara et al., 2002" startWordPosition="229" endWordPosition="233">it feature template set incorporated with a proper argument pruning strategy. The system achieved the top average score in the closed challenge: 80.47% semantic labeled F1 for the average score. 1 Introduction The syntactic and semantic dependency parsing in multiple languages introduced by the shared task of CoNLL-2009 is an extension of the CoNLL2008 shared task (Hajiˇc et al., 2009). Seven languages, English plus Catalan, Chinese, Czech, German, Japanese and Spanish, are involved (Taul´e et al., 2008; Palmer and Xue, 2009; Hajiˇc et al., 2006; Surdeanu et al., 2008; Burchardt et al., 2006; Kawahara et al., 2002). This paper presents our research for participation in the semantic-only (SRLonly) challenge of the CoNLL-2009 shared task, with a This study is partially supported by CERG grant 9040861 (CityU 1318/03H), CityU Strategic Research Grant 7002037, Projects 60673041 and 60873041 under the National Natural Science Foundation of China and Project 2006AA01Z147 under the ”863” National High-Tech Research and Development of China. 55 highlight on our strategy to select features from a large candidate set for maximum entropy learning. 2 System Survey We opt for the maximum entropy model with Gaussian p</context>
</contexts>
<marker>Kawahara, Kurohashi, Hasida, 2002</marker>
<rawString>Daisuke Kawahara, Sadao Kurohashi, and Kˆoiti Hasida. 2002. Construction of a Japanese relevance-tagged corpus. In Proceedings of the 3rd International Conference on Language Resources and Evaluation (LREC-2002), pages 2008–2013, Las Palmas, Canary Islands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Nianwen Xue</author>
</authors>
<title>Adding semantic roles to the Chinese Treebank.</title>
<date>2009</date>
<journal>Natural Language Engineering,</journal>
<volume>15</volume>
<issue>1</issue>
<contexts>
<context position="1528" citStr="Palmer and Xue, 2009" startWordPosition="213" endWordPosition="216">For each input corpus, a large scale feature engineering is conducted to select the best fit feature template set incorporated with a proper argument pruning strategy. The system achieved the top average score in the closed challenge: 80.47% semantic labeled F1 for the average score. 1 Introduction The syntactic and semantic dependency parsing in multiple languages introduced by the shared task of CoNLL-2009 is an extension of the CoNLL2008 shared task (Hajiˇc et al., 2009). Seven languages, English plus Catalan, Chinese, Czech, German, Japanese and Spanish, are involved (Taul´e et al., 2008; Palmer and Xue, 2009; Hajiˇc et al., 2006; Surdeanu et al., 2008; Burchardt et al., 2006; Kawahara et al., 2002). This paper presents our research for participation in the semantic-only (SRLonly) challenge of the CoNLL-2009 shared task, with a This study is partially supported by CERG grant 9040861 (CityU 1318/03H), CityU Strategic Research Grant 7002037, Projects 60673041 and 60873041 under the National Natural Science Foundation of China and Project 2006AA01Z147 under the ”863” National High-Tech Research and Development of China. 55 highlight on our strategy to select features from a large candidate set for ma</context>
</contexts>
<marker>Palmer, Xue, 2009</marker>
<rawString>Martha Palmer and Nianwen Xue. 2009. Adding semantic roles to the Chinese Treebank. Natural Language Engineering, 15(1):143–172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Richard Johansson</author>
<author>Adam Meyers</author>
<author>Llufs M`arquez</author>
<author>Joakim Nivre</author>
</authors>
<title>The CoNLL2008 shared task on joint parsing of syntactic and semantic dependencies.</title>
<date>2008</date>
<booktitle>In Proceedings of the 12th Conference on Computational Natural Language Learning (CoNLL-2008).</booktitle>
<marker>Surdeanu, Johansson, Meyers, M`arquez, Nivre, 2008</marker>
<rawString>Mihai Surdeanu, Richard Johansson, Adam Meyers, Llufs M`arquez, and Joakim Nivre. 2008. The CoNLL2008 shared task on joint parsing of syntactic and semantic dependencies. In Proceedings of the 12th Conference on Computational Natural Language Learning (CoNLL-2008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mariona Taul´e</author>
<author>Maria Ant`onia Martf</author>
<author>Marta Recasens</author>
</authors>
<title>AnCora: Multilevel Annotated Corpora for Catalan and Spanish.</title>
<date>2008</date>
<booktitle>In Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC-2008),</booktitle>
<location>Marrakesh, Morroco.</location>
<marker>Taul´e, Martf, Recasens, 2008</marker>
<rawString>Mariona Taul´e, Maria Ant`onia Martf, and Marta Recasens. 2008. AnCora: Multilevel Annotated Corpora for Catalan and Spanish. In Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC-2008), Marrakesh, Morroco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Martha Palmer</author>
</authors>
<title>Calibrating features for semantic role labeling.</title>
<date>2004</date>
<booktitle>In 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP-2004),</booktitle>
<pages>88--94</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="4524" citStr="Xue and Palmer, 2004" startWordPosition="677" endWordPosition="680">actly same way. When no constraint available, however, all word pairs in the an input sequence must be considered, leading to very poor efficiency in computation for no gain in effectiveness. Thus, the training sample needs to be pruned properly. As predicates overtly known in the share task, we only consider how to effectively prune argument candidates. We adopt five types of argument pruning strategies for seven languages. All of them assume that a syntactic dependency parsing tree is available. As for Chinese and English, we continue to use a dependency version of the pruning algorithm of (Xue and Palmer, 2004) as described in (Zhao and Kit, 2008). The pruning algorithm is readdressed as the following. Initialization: Set the given predicate candidate as the current node; (1) The current node and all of its syntactic children are selected as argument candidates. (2) Reset the current node to its syntactic head and repeat step (1) until the root is reached. Note that the given predicate candidate itself is excluded from the argument candidate list for Chinese, that is slightly different from English. The above pruning algorithm has been shown effective. However, it is still inefficient for a singlest</context>
</contexts>
<marker>Xue, Palmer, 2004</marker>
<rawString>Nianwen Xue and Martha Palmer. 2004. Calibrating features for semantic role labeling. In 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP-2004), pages 88–94, Barcelona, Spain, July 25-26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chunyu Kit</author>
</authors>
<title>Parsing syntactic and semantic dependencies with two single-stage maximum entropy models.</title>
<date>2008</date>
<booktitle>In Twelfth Conference on Computational Natural Language Learning (CoNLL2008),</booktitle>
<pages>203--207</pages>
<location>Manchester, UK,</location>
<contexts>
<context position="2539" citStr="Zhao and Kit, 2008" startWordPosition="370" endWordPosition="373">ral Science Foundation of China and Project 2006AA01Z147 under the ”863” National High-Tech Research and Development of China. 55 highlight on our strategy to select features from a large candidate set for maximum entropy learning. 2 System Survey We opt for the maximum entropy model with Gaussian prior as our learning model for all classification subtasks in the shared task. Our implementation of the model adopts L-BFGS algorithm for parameter optimization as usual. No additional feature selection techniques are applied. Our system is basically improved from its early version for CoNLL-2008 (Zhao and Kit, 2008). By introducing a virtual root for every predicates, The job to determine both argument labels and predicate senses is formulated as a word-pair classification task in four languages, namely, Catalan, Spanish, Czech and Japanese. In other three languages, Chinese, English and German, a predicate sense classifier is individually trained before argument label classification. Note that traditionally (or you may say that most semantic parsing systems did so) argument identification and classification are handled in a two-stage pipeline, while ours always tackles them in one step, in addition, pre</context>
<context position="4561" citStr="Zhao and Kit, 2008" startWordPosition="684" endWordPosition="687">lable, however, all word pairs in the an input sequence must be considered, leading to very poor efficiency in computation for no gain in effectiveness. Thus, the training sample needs to be pruned properly. As predicates overtly known in the share task, we only consider how to effectively prune argument candidates. We adopt five types of argument pruning strategies for seven languages. All of them assume that a syntactic dependency parsing tree is available. As for Chinese and English, we continue to use a dependency version of the pruning algorithm of (Xue and Palmer, 2004) as described in (Zhao and Kit, 2008). The pruning algorithm is readdressed as the following. Initialization: Set the given predicate candidate as the current node; (1) The current node and all of its syntactic children are selected as argument candidates. (2) Reset the current node to its syntactic head and repeat step (1) until the root is reached. Note that the given predicate candidate itself is excluded from the argument candidate list for Chinese, that is slightly different from English. The above pruning algorithm has been shown effective. However, it is still inefficient for a singlestage argument identification/classific</context>
<context position="15337" citStr="Zhao and Kit, 2008" startWordPosition="2448" endWordPosition="2451">el of each argument candidate will be directly classified, or, it is proved non-argument. As for the rest languages, Chinese, English or German, after the sense classifier outputs its result, an optimal argument structure for each predicate is determined by the following maximal probability. p.lm.pos p.rm.pos p.lemma p.lemma + p.lemma1 p.lemma + p.children.dprel.noDup p.lemma + p.currentSense p.form p.form−1 + p.form p.form + p.form1 6 Word Sense Determination �Sp = argmax Pai|ai−1, ai−2, ...), (1) i The shared task of CoNLL-2009 still asks for the predicate sense. In our work for CoNLL-2008 (Zhao and Kit, 2008), this was done by searching for a right 2Full feature lists and their explanation for all languages will be available at the website, http://bcmi.sjtu.edu.cn/˜zhaohai. where Sp is the argument structure, Pai|ai−1...) is the conditional probability to determine the label of the i-th argument candidate label. Note that 3For Japanese, no senses for predicates are defined. Thus it is actually a trivial classification task in this case. 58 Table 5: Feature set for Catalan argument classification P(aZ|az_1, ...) in equation (1) may be simplified as P(az) if the input feature template set does not c</context>
</contexts>
<marker>Zhao, Kit, 2008</marker>
<rawString>Hai Zhao and Chunyu Kit. 2008. Parsing syntactic and semantic dependencies with two single-stage maximum entropy models. In Twelfth Conference on Computational Natural Language Learning (CoNLL2008), pages 203–207, Manchester, UK, August 16-17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Wenliang Chen</author>
<author>Jun’ichi Kazama</author>
<author>Kiyotaka Uchimoto</author>
<author>Kentaro Torisawa</author>
</authors>
<title>Multilingual dependency learning: Exploiting rich features for tagging syntactic and semantic dependencies.</title>
<date>2009</date>
<booktitle>In Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL-2009),</booktitle>
<location>Boulder, Colorado, USA.</location>
<contexts>
<context position="17957" citStr="Zhao et al., 2009" startWordPosition="2898" endWordPosition="2901">ust and stable results. The first is that two results for development and test sets in the same language are quite close. The second is about out-ofdomain (OOD) task. Though for each OOD task, we just used the same model trained from the respective language and did nothing to strengthen it, this does not hinder our system to obtain top results in Czech and English OOD tasks. In addition, the feature template sets from automatical selection procedure in this task were used for the joint task of this shared task, and also output top results according to the average score of semantic labeled F1 (Zhao et al., 2009). p.currentSense + p.lemma p.currentSense + p.pos p.currentSense + a.pos p−1.FEAT1 p.FEAT2 p1.FEAT3 p.semrm.semdprel p.lm.dprel p.form + p.children.dprel.bag p.lemman (n = −1, 0) p.lemma + p.lemma1 p.pos−1 + p.pos p.pos1 p.pos + p.children.dprel.bag a.FEAT1 + a.FEAT3 + a.FEAT4 + a.FEAT5 + a.FEAT6 a−1.FEAT2 + a.FEAT2 a.FEAT3 + a1.FEAT3 a.FEAT3 + a.h.FEAT3 a.children.FEAT1.noDup a.children.FEAT3.bag a.h.lemma a.lm.dprel + a.form a.lm.form a.lm−1.lemma a.lmn.pos (n=0,1) a.noFarChildren.pos.bag + a.rm.form a.pphead.lemma a.rm.dprel + a.form a.rm−1.form a.rm.lemma a.rn.dprel + a.form a.lowSupportVe</context>
</contexts>
<marker>Zhao, Chen, Kazama, Uchimoto, Torisawa, 2009</marker>
<rawString>Hai Zhao, Wenliang Chen, Jun’ichi Kazama, Kiyotaka Uchimoto, and Kentaro Torisawa. 2009. Multilingual dependency learning: Exploiting rich features for tagging syntactic and semantic dependencies. In Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL-2009), June 4-5, Boulder, Colorado, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>