<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007025">
<title confidence="0.997919">
Dialog Input Ranking in a Multi-Domain Environment Using
Transferable Belief Model
</title>
<author confidence="0.997722">
Hong-I Ng
</author>
<affiliation confidence="0.999784">
Department of Computer Science
School of Computing
National University of Singapore
</affiliation>
<email confidence="0.990007">
nghi@comp.nus.edu.sg
</email>
<author confidence="0.995719">
Kim-Teng Lua
</author>
<affiliation confidence="0.999782333333333">
Department of Computer Science
School of Computing
National University of Singapore
</affiliation>
<email confidence="0.995117">
luakt@comp.nus.edu.sg
</email>
<sectionHeader confidence="0.995606" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999983190476191">
This paper presents results of using be-
lief functions to rank the list of candi-
date information provided in a noisy di-
alogue input. The information under con-
sideration is the intended task to be per-
formed and the information provided for
the completion of the task. As an exam-
ple, we use the task of information ac-
cess in a multi-domain dialogue system.
Currently, the system contains knowledge
of ten different domains. Callers calling
in are greeted with an open-ended “How
may I help you?” prompt (Thomson and
Wisowaty, 1999; Chu-Carroll and Carpen-
ter, 1999; Gorin et al., 1997). After re-
ceiving a reply from the caller, we extract
word evidences from the recognized utter-
ances. By using transferable belief model
(TBM), we in turn determine the task that
the caller intends to perform as well as any
information provided.
</bodyText>
<sectionHeader confidence="0.999337" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99997991011236">
Touch-tone menus are prevalent in call centers for
accessing personal records and pre-recorded infor-
mation. However, it can sometimes be very frus-
trating when we need to listen to a long list of op-
tions. Moreover, the information that we are look-
ing for may not seem to be relevant to any of the
given options. Recently, systems that allow people
to access information based on spoken inputs have
been built. They require a speech recognizer that
is trained on a specific set of key words and speech
grammars to understand the spoken inputs. Callers
are guided through a series of prompts. At each
prompt, the callers are supposed to speak out their
choices in a way that is easy for the systems to un-
derstand. However, new callers may not know what
should they say at different prompts and how should
they say it. They might have spoken their choices
too early, or the way they say it is not encoded in the
systems grammar. Thus, we are motivated to work
on the problem of accessing information using nat-
urally spoken dialogue. We allow callers to speak
in a natural way. Our ultimate aim is to provide the
caller with the exact piece of information that s/he is
looking for through a series of dialogue interaction.
The work reported in this paper is our first attempt
toward our ultimate aim, i.e., to determine what the
callers want and find out the information the callers
have provided that are useful for the task. To achieve
that, we use Smets’ (1988) TBM.
TBM is the concept used to justify the use of be-
lief functions (BFs), Dempster’s rule of condition-
ing and Dempster’s rule of combination to model
someone’s belief (Smets, 1988). Since early 1980’s,
BFs have generated considerable interest in the Ar-
tificial Intelligence community. In Smets (1999),
Denœux (2000) and Zouhal and Denœux (1998),
BFs are used to provide sound and elegant solu-
tions to real life problems where some information
is missing. As in Bayesian model, given the avail-
able evidences, parts of the amount of belief are al-
located to each object in our problem domain. How-
ever, some evidences might support something other
than only one of the various domain objects. In this
case, Principle of Insufficient Reason (Smets, 1988)
is invoked to decide that the belief mass must be split
equally among the domain objects involved. TBM
does not evoke this principle and leaves the belief
mass allocated to the disjunct of the domain objects
involved. Examples of the use of BFs include dis-
criminant analysis using a learning set where classes
are only partially known; determine the number of
sources in a multi-sensor environment by studying
the inter-sensors contradiction and pattern classifi-
cation. As far as we know, nobody has used BFs to
solve problems related to human-computer conver-
sational dialogue. However, we belief that BFs can
be applied on problems related to human-computer
conversational dialogue, where the recognized utter-
ances contain insertion, deletion and substitution er-
rors. Currently, our multi-domain dialogue system
contains knowledge of ten different domains. They
are phone directory service (T ), train schedule in-
quiry (T ), flight status inquiry (T ), travel book-
ing (T ), Bus Service inquiry (T ), financial plan-
ning (T ), phone banking (T ), checking of the em-
ployee’s account (T ), concert ticket booking (T )
and course registration (T )
Similar works have been reported is the past.
However, their main aim is to do call routing instead
of information access. Their approaches include the
use of a vector-based information retrieval technique
(Lee et al., 2000; Chu-Carroll and Carpenter, 1999)
/bin/bash: line 1: a: command not found Our do-
mains are more varied, which may results in more
recognition errors. In addition, we do not have a
training corpus. However, we have a knowledge
base that provides partial information based on word
evidences. For examples, the occurrence of word
evidence account indicates that the user wants to ac-
cess her/his employee’s account or bank account, the
occurrence of a person name indicates that the user
is not checking for a flight status or bus service, the
occurrence of word evidence time indicates that the
user probably wants to check the train schedules or
flight status.
Due to space limitation, readers are advised to re-
fer to Smets (1988; 1997; 1989) for more detailed
discussions on BFs, combination of BFs, decision
making using BFs and TBM.
</bodyText>
<sectionHeader confidence="0.625068" genericHeader="method">
2 Ranking Information from the
</sectionHeader>
<subsectionHeader confidence="0.77112">
Recognized Utterance of Naturally
Spoken Input
</subsectionHeader>
<bodyText confidence="0.999995466666667">
Our aim is to use TBM in dialogue management.
First, TBM is used to rank the information identi-
fied from the recognized input. Then, the rank list
is used in clarification dialogues if necessary. Other-
wise, the best result is treated as the user input. Our
experiments are done using Sphinx II speech recog-
nition system (Huang et al., 1992). Using a test cor-
pus of 1977 words, we find that the word recognition
accuracy is 54.5%. In our experiments, we use 139
naturally spoken responses to an open-ended “How
may I help you prompt” prompt. The callers are told
in advance the list of tasks that the system can per-
form. As notations, let U denotes a recognized ut-
terance, n the length of U in number of words and
the word evidences from U.
</bodyText>
<subsectionHeader confidence="0.993857">
2.1 Identifying the Intended Task
</subsectionHeader>
<bodyText confidence="0.996858">
In this experiment, we show whether TBM can be
used to identify the caller’s intended tasks. First, we
need to identify our problem domain or frame ofdis-
cernment, (Smets, 1988). For task identification,
, i.e., the list of tasks presented
in Section 1. , i.e., the basic belief mass
(bbm) of given to where is calculated
based on the occurrence frequency of word evidence
in the knowledge-bases of .
Currently the knowledge base of each
consists of (a) a task specification; (b) infor-
mation schemas for ; and (c) information for task,
i.e., the database records, facts and rules, and remote
tables and databases used in . A task specification
specifies the goal of the task and the list of steps re-
quired to attain the goal. Each step is linked to either
a basic operation, for examples, fetch some records
from a database and ask the caller for information,
or a sub-task. Information schemas specify the high-
level formats of the information used in . They in-
clude database schemas, XML schemas of facts and
rules, and format descriptions of some remote tables
and databases used in .
We do indexing for each so that it is
easy to calculate the bbm’s where
and . We then do the following adjust-
ments to make sure that : if
.
, then the BF is scaled to
one; otherwise,
where . is also called
the ignorance value relative to (Smets, 1988).
Larger implies that it is harder to decide
which is the intended task of the caller by looking
at evidence . The BF’s are then
combined using Dempster’s rule of combination,
where
and . is computed
by combining and
</bodyText>
<subsectionHeader confidence="0.995464">
2.2 Identifying the Provided Information
</subsectionHeader>
<bodyText confidence="0.996948">
In this experiment, we show whether TBM can be
used to identify the information provided by the
caller in U. Here, the frame of discernment
consists of the objects in the information schemas
for a specific task. As in Section 2.1, we use the
indices of to compute the bbm’s of
given to each object disjunct
. Lastly, we combine the BFs
and compute the pignistic probability measures of
each object . Experiment results will be
presented in Section 3.2.
</bodyText>
<sectionHeader confidence="0.986946" genericHeader="method">
3 Experiment Results
</sectionHeader>
<equation confidence="0.73834">
1 2 3 4 5 6 7 8 9 10
n
</equation>
<figureCaption confidence="0.977258333333333">
Figure 1: Percentage of time the correct task is in-
cluded when considering the top ranked tasks.
Figure 1 shows the results of selecting top-n-tasks
</figureCaption>
<bodyText confidence="0.999197230769231">
in the ranked list of . The labels task,
schema and info denote that only knowledge in the
task specifications, information schemas and basic
information respectively are included in the calcula-
tion of bbm’s. ’+’ denotes a combination of some
and all denotes the combination of all. The graphs
show that we obtain the best ranking of candidate
tasks when knowledge from task specifications and
information schemas are used to calculate the BF’s.
This is intuitive because callers will often say her/his
goal and mention the name of the piece of informa-
tion s/he’s looking for, e.g., “I want to buy a movie
ticket please.”
</bodyText>
<equation confidence="0.7927475">
1 2 3 4 5 6 7 8 9 10
n
</equation>
<figureCaption confidence="0.677064">
Figure 2: Percentage of time the correct task is in-
cluded when considering the top ranked tasks, tak-
ing similar words into considerations.
</figureCaption>
<bodyText confidence="0.997365176470588">
Next, we examine the result of taking similar
words into considerations. This is because callers
may use words different from those occurring in our
knowledge base. Thus, for each word evidence
in , we use WordNet (Fellbaum, 1997) to look for
similar words in our knowledge base.
For each , we calculate the BF as
discussed in Section 2.1. This time, we also multi-
ply the bbm’s in by the distance measure be-
tween and . The distance measures fall in the
range [0:1]. These results are shown in Figure 2.
Again, the results show that we obtain the best rank-
ing of candidate tasks when knowledge from task
specifications and information schemas are used to
calculate the BF’s. However, there is a decrease in
correct rate when only the best (-6.25%) and 2-best
(-1.58%) tasks in the ranked list are used to allow
</bodyText>
<figure confidence="0.980708675675676">
3.1 Identifying the Intended Task
Correct rate
100
40
90
70
30
20
80
60
50
10
task
schema
info
task+schema
task+info
schema+info
all
Correct rate
100
40
90
70
30
20
80
60
50
10
task
schema
info
task+schema
task+info
schema+info
all
</figure>
<bodyText confidence="0.925593">
. Lastly,
are ranked in descending order according
to their pignistic probability measure
with the top of the rank
being the most probable target task. Experiment re-
sults will be presented in Section 3.1.
</bodyText>
<equation confidence="0.7614125">
1 2 3 4 5 6 7 8 9 10
n
</equation>
<figureCaption confidence="0.96675375">
Figure 3: Percentage of time the correct task is in-
cluded when considering the top ranked tasks, tak-
ing similar words and correlation measures into con-
sideration.
</figureCaption>
<figure confidence="0.769562">
1 2 3 4 5 6 7 8 9 10
n
</figure>
<figureCaption confidence="0.99499925">
Figure 4: Percentage of time the correct task is in-
cluded when considering the top ranked tasks us-
ing dialogue transcripts, similar words and correla-
tion measures.
</figureCaption>
<figure confidence="0.999234882352941">
100
40
90
70
30
20
80
60
50
10
task
schema
info
task+schema
task+info
schema+info
all
100
40
90
70
30
80
60
50
task
schema
info
task+schema
task+info
schema+info
all
Correct rate
Correct rate
</figure>
<bodyText confidence="0.998778">
the callers to select. The correct rate is increased
only when more than 2 top-ranking tasks are used
for callers’ selection, i.e., 4.38%, 1.32%, 2.66% and
12.32% when n = 3, 4, 5 and 6 respectively.
From the results, we found that some words oc-
cur commonly across multiple domains. This phe-
nomena is common in problems related to natural
language processing. To alleviate the problem, we
have used words that only occur commonly in few
domains. We use correlation coefficient (Ng et al.,
1997) to measure the correlations of all words to all
domains. After that, we scale the correlation mea-
sures to 1. In calculating the bbm’s, we multiply
the original bbm’s with the corresponding correla-
tion measures. Figure 3 shows the results when sim-
ilar words and correlation measures are considered
in the calculation of BF’s. This time, the results
show that we obtain the best ranking of candidate
tasks when knowledge from task specifications and
basic information are used to calculate the BF’s. In
addition, there is a 67.31% improvement when the
top task in the ranked list is taken as the caller’s in-
tended tasks. When top-n tasks are used for callers’
selection, the improvements are 40.5%, 29.53% and
16.76% for n = 2, 3 and 4 respectively.
For the purpose of comparison, we show the re-
sults of task identification based on dialogue tran-
scripts, similar words and correlation measures in
Figure 4. The results show that with the use of only
basic information in the calculation of BF’s, a re-
sult of 99.1% can be achieved by select the top task
in the ranked list. Thus, when the word accuracy
of the speech recognizer is high, basic information
is sufficient to identify the callers’ intended tasks.
Otherwise, knowledge from task specifications and
information schema are required in target task iden-
tifications. We have shown that TBM can be used
for task identification in a noisy and multi-domain
environment. It would be interesting to compare
these results when we have enough corpus to train
a vector-based task identifier.
</bodyText>
<subsectionHeader confidence="0.998357">
3.2 Identifying the Provided Information
</subsectionHeader>
<bodyText confidence="0.999954571428572">
Figure 5 shows the percentage of time the cor-
rect information is included in the top-n selected
information after they have been sorted according
to their pignistic probability measures. SR-best-1
(SR-best-2) indicates that the best (respectively, two
best) speech recognition results are used for infor-
mation identification. The results show that there is
a 14.25% (10.54%) improvement when the best (re-
spectively, two best) speech recognition results are
used for information identification. ‘Transcript’ in-
dicates that the dialogue transcripts are used for in-
formation identification. The results show that there
is an average of 63.79% information lost between
’transcript’ and ‘SR-best-2’.
</bodyText>
<sectionHeader confidence="0.999488" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.9980055">
A new naturally spoken dialogue processing based
on the TBM has been presented. This approach
</bodyText>
<figureCaption confidence="0.597978">
Figure 5: Correct identification rate using the top n
information in the rank.
</figureCaption>
<bodyText confidence="0.997567923076923">
can be viewed as looking for evidences from noisy
speech inputs to identify the tasks that the callers
want to perform and the information that they have
provided. Our experiments are tested on a multi-
domain environment. The speech recognizer that we
use has a word accuracy of around 55%. The exper-
iment results show that there is some initial success
in using TBM to aid in task and information identi-
fication when the recognized input is noisy.
In order to improve users’ satisfaction, we are
looking into dialogue processing methods that are
able to improve the results of task and information
identification. In particular, instead of using word
evidences from the recognized inputs, we are look-
ing into the use other evidences such as phonemes.
We are also looking into dialogue strategies that are
able to collaborate with the callers to correct the
identified information. In particular, if the ignorance
value is high, our system should employ sys-
tem initiative strategies to disambiguate the identi-
fied information. If is high, which means
that the evidences do not point strongly to any object
in , then our system should employ system initia-
tive strategies to learn new task-related information.
If both and are low, out system can
employ a mixed initiative dialogue strategy.
</bodyText>
<sectionHeader confidence="0.998295" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999693">
Our thanks go to the undergraduate students who
have contributed their valuable time to help us in the
recordings without asking for any rewards.
</bodyText>
<sectionHeader confidence="0.996589" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.992790480769231">
Chu-Carroll, Jeniffer and Bob Carpenter. 1999. Vector-
based natural language call routing. Computational
Linguistics, 25(3):361–388.
Dena ux, Thierry. 2000. A neural network classifier based
on Dempster-Shafer theory. IEEE Transactions on
Systems, Man, and Cybernetics – Part A: Systems and
Humans, 30(2):131–150, March.
Fellbaum, Christiane (Ed). 1997. WordNet: An Electronic
Lexical Database. Imprint Cambridge, Mass: MIT
Press.
Gorin, Allen L., Giuseppe Riccardi and Jeremy H.
Wright. 1997. How may I help you? Speech Commu-
nication, 23:113–127.
Huang, Xuedong, Fileno Alleva, Hsiao-Wuen Hon, Mei-
Yuh Hwang, Ronald Rosenfeld. 1992. The SPHINX-
II speech recognition system: an overview. Computer
Speech and Language, 7(2):137–148.
Lee, Chin-Hui, Bob Carpenter, Wu Chou, Jennifer Chu-
Carroll, Wolfgang Reichl, Antoine Saad and Qiru
Zhou. 2000. On natural language call routing. Speech
Communication, 31(4):309-320, Aug.
Ng, Hwee Tou, Goh Wei Boon and Low Kok Leong.
1997. Feature selection, perceptron learning, and a us-
ability case study for text categorization. In Proceed-
ings of the 20th International ACM SIGIR Confer-
ence on Research and Development in Information Re-
trieval, 67-73. Philadelphia, Pennsylvania, USA.
Smets, Philippe. 1999. Practical uses of belief functions.
Uncertainty in Artificial Intelligence: Proceedings of
the Fifteenth Conference (UAI-1999), Morgan Kauf-
mann Publishers, San Francisco, CA, 612–621.
Smets, Philippe. 1989. Constructing the pignistic proba-
bility function in a context of uncertainty. Uncertainty
in Artificial Intelligence 5. Henrion M., Shachter R.
D., Kanal L. N. and Lemmer J. F. (Eds). North Hol-
land, Amsterdam, 29–40.
Smets, Philippe. 1988. Belief functions. Non-standard
Logic for Automated Reasoning. P. Smets, A. Mam-
dani, D. Dubois, and H. Prade (Eds). New York: Aca-
demic, 252–286.
Smets, Philippe. 1997. The axiomatic justification of
the transferable belief model. Artificial Intelligence,
92:229–242.
Thomson, David L. and Jack J. Wisowaty. 1999. User
confusion in natural language services. In Proc. ESCA
Workshop on Interactive Dialogue in Multi-Modal
Systems, Kloster Irsee, Germany, June, 189–196,
keynote address.
Zouhal, Lalla Merieme and Thierry Dena ux. 1998. An
evidence-theoretic k-NN rule with parameter opti-
mization. IEEE Transactions on Systems, Man and Cy-
bernetics – Part C, 28(2):263-271.
</reference>
<figure confidence="0.993911333333333">
n
Correct rate
100
20
0
1 2 3 4 5 6 7 8
’SR-best-1’
’SR-best-2’
’transcript’
80
60
40
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.201105">
<title confidence="0.91348">Dialog Input Ranking in a Multi-Domain Environment Transferable Belief Model</title>
<author confidence="0.635696">Hong-I</author>
<affiliation confidence="0.998511">Department of Computer School of National University of</affiliation>
<email confidence="0.606518">nghi@comp.nus.edu.sgKim-Teng</email>
<affiliation confidence="0.999101">Department of Computer School of National University of</affiliation>
<email confidence="0.934083">luakt@comp.nus.edu.sg</email>
<abstract confidence="0.999437409090909">This paper presents results of using belief functions to rank the list of candidate information provided in a noisy dialogue input. The information under consideration is the intended task to be performed and the information provided for the completion of the task. As an example, we use the task of information access in a multi-domain dialogue system. Currently, the system contains knowledge of ten different domains. Callers calling in are greeted with an open-ended “How may I help you?” prompt (Thomson and Wisowaty, 1999; Chu-Carroll and Carpenter, 1999; Gorin et al., 1997). After receiving a reply from the caller, we extract word evidences from the recognized utterances. By using transferable belief model (TBM), we in turn determine the task that the caller intends to perform as well as any information provided.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jeniffer Chu-Carroll</author>
<author>Bob Carpenter</author>
</authors>
<title>Vectorbased natural language call routing.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>3</issue>
<contexts>
<context position="886" citStr="Chu-Carroll and Carpenter, 1999" startWordPosition="131" endWordPosition="135">puting National University of Singapore luakt@comp.nus.edu.sg Abstract This paper presents results of using belief functions to rank the list of candidate information provided in a noisy dialogue input. The information under consideration is the intended task to be performed and the information provided for the completion of the task. As an example, we use the task of information access in a multi-domain dialogue system. Currently, the system contains knowledge of ten different domains. Callers calling in are greeted with an open-ended “How may I help you?” prompt (Thomson and Wisowaty, 1999; Chu-Carroll and Carpenter, 1999; Gorin et al., 1997). After receiving a reply from the caller, we extract word evidences from the recognized utterances. By using transferable belief model (TBM), we in turn determine the task that the caller intends to perform as well as any information provided. 1 Introduction Touch-tone menus are prevalent in call centers for accessing personal records and pre-recorded information. However, it can sometimes be very frustrating when we need to listen to a long list of options. Moreover, the information that we are looking for may not seem to be relevant to any of the given options. Recently</context>
<context position="4779" citStr="Chu-Carroll and Carpenter, 1999" startWordPosition="789" endWordPosition="792">Currently, our multi-domain dialogue system contains knowledge of ten different domains. They are phone directory service (T ), train schedule inquiry (T ), flight status inquiry (T ), travel booking (T ), Bus Service inquiry (T ), financial planning (T ), phone banking (T ), checking of the employee’s account (T ), concert ticket booking (T ) and course registration (T ) Similar works have been reported is the past. However, their main aim is to do call routing instead of information access. Their approaches include the use of a vector-based information retrieval technique (Lee et al., 2000; Chu-Carroll and Carpenter, 1999) /bin/bash: line 1: a: command not found Our domains are more varied, which may results in more recognition errors. In addition, we do not have a training corpus. However, we have a knowledge base that provides partial information based on word evidences. For examples, the occurrence of word evidence account indicates that the user wants to access her/his employee’s account or bank account, the occurrence of a person name indicates that the user is not checking for a flight status or bus service, the occurrence of word evidence time indicates that the user probably wants to check the train sch</context>
</contexts>
<marker>Chu-Carroll, Carpenter, 1999</marker>
<rawString>Chu-Carroll, Jeniffer and Bob Carpenter. 1999. Vectorbased natural language call routing. Computational Linguistics, 25(3):361–388.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dena ux</author>
<author>Thierry</author>
</authors>
<title>A neural network classifier based on Dempster-Shafer theory.</title>
<date>2000</date>
<journal>IEEE Transactions on Systems, Man, and Cybernetics – Part A: Systems and Humans,</journal>
<volume>30</volume>
<issue>2</issue>
<marker>ux, Thierry, 2000</marker>
<rawString>Dena ux, Thierry. 2000. A neural network classifier based on Dempster-Shafer theory. IEEE Transactions on Systems, Man, and Cybernetics – Part A: Systems and Humans, 30(2):131–150, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database. Imprint Cambridge,</title>
<date>1997</date>
<publisher>MIT Press.</publisher>
<location>Mass:</location>
<contexts>
<context position="9686" citStr="Fellbaum, 1997" startWordPosition="1659" endWordPosition="1660">mation schemas are used to calculate the BF’s. This is intuitive because callers will often say her/his goal and mention the name of the piece of information s/he’s looking for, e.g., “I want to buy a movie ticket please.” 1 2 3 4 5 6 7 8 9 10 n Figure 2: Percentage of time the correct task is included when considering the top ranked tasks, taking similar words into considerations. Next, we examine the result of taking similar words into considerations. This is because callers may use words different from those occurring in our knowledge base. Thus, for each word evidence in , we use WordNet (Fellbaum, 1997) to look for similar words in our knowledge base. For each , we calculate the BF as discussed in Section 2.1. This time, we also multiply the bbm’s in by the distance measure between and . The distance measures fall in the range [0:1]. These results are shown in Figure 2. Again, the results show that we obtain the best ranking of candidate tasks when knowledge from task specifications and information schemas are used to calculate the BF’s. However, there is a decrease in correct rate when only the best (-6.25%) and 2-best (-1.58%) tasks in the ranked list are used to allow 3.1 Identifying the </context>
</contexts>
<marker>Fellbaum, 1997</marker>
<rawString>Fellbaum, Christiane (Ed). 1997. WordNet: An Electronic Lexical Database. Imprint Cambridge, Mass: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Allen L Gorin</author>
<author>Giuseppe Riccardi</author>
<author>Jeremy H Wright</author>
</authors>
<title>How may I help you? Speech Communication,</title>
<date>1997</date>
<pages>23--113</pages>
<contexts>
<context position="907" citStr="Gorin et al., 1997" startWordPosition="136" endWordPosition="139">gapore luakt@comp.nus.edu.sg Abstract This paper presents results of using belief functions to rank the list of candidate information provided in a noisy dialogue input. The information under consideration is the intended task to be performed and the information provided for the completion of the task. As an example, we use the task of information access in a multi-domain dialogue system. Currently, the system contains knowledge of ten different domains. Callers calling in are greeted with an open-ended “How may I help you?” prompt (Thomson and Wisowaty, 1999; Chu-Carroll and Carpenter, 1999; Gorin et al., 1997). After receiving a reply from the caller, we extract word evidences from the recognized utterances. By using transferable belief model (TBM), we in turn determine the task that the caller intends to perform as well as any information provided. 1 Introduction Touch-tone menus are prevalent in call centers for accessing personal records and pre-recorded information. However, it can sometimes be very frustrating when we need to listen to a long list of options. Moreover, the information that we are looking for may not seem to be relevant to any of the given options. Recently, systems that allow </context>
</contexts>
<marker>Gorin, Riccardi, Wright, 1997</marker>
<rawString>Gorin, Allen L., Giuseppe Riccardi and Jeremy H. Wright. 1997. How may I help you? Speech Communication, 23:113–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuedong Huang</author>
</authors>
<title>Fileno Alleva, Hsiao-Wuen Hon, MeiYuh Hwang, Ronald Rosenfeld.</title>
<date>1992</date>
<journal>Computer Speech and Language,</journal>
<volume>7</volume>
<issue>2</issue>
<marker>Huang, 1992</marker>
<rawString>Huang, Xuedong, Fileno Alleva, Hsiao-Wuen Hon, MeiYuh Hwang, Ronald Rosenfeld. 1992. The SPHINXII speech recognition system: an overview. Computer Speech and Language, 7(2):137–148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Hui Lee</author>
<author>Bob Carpenter</author>
<author>Wu Chou</author>
<author>Jennifer ChuCarroll</author>
<author>Wolfgang Reichl</author>
<author>Antoine Saad</author>
<author>Qiru Zhou</author>
</authors>
<title>On natural language call routing.</title>
<date>2000</date>
<journal>Speech Communication,</journal>
<pages>31--4</pages>
<contexts>
<context position="4745" citStr="Lee et al., 2000" startWordPosition="785" endWordPosition="788">stitution errors. Currently, our multi-domain dialogue system contains knowledge of ten different domains. They are phone directory service (T ), train schedule inquiry (T ), flight status inquiry (T ), travel booking (T ), Bus Service inquiry (T ), financial planning (T ), phone banking (T ), checking of the employee’s account (T ), concert ticket booking (T ) and course registration (T ) Similar works have been reported is the past. However, their main aim is to do call routing instead of information access. Their approaches include the use of a vector-based information retrieval technique (Lee et al., 2000; Chu-Carroll and Carpenter, 1999) /bin/bash: line 1: a: command not found Our domains are more varied, which may results in more recognition errors. In addition, we do not have a training corpus. However, we have a knowledge base that provides partial information based on word evidences. For examples, the occurrence of word evidence account indicates that the user wants to access her/his employee’s account or bank account, the occurrence of a person name indicates that the user is not checking for a flight status or bus service, the occurrence of word evidence time indicates that the user pro</context>
</contexts>
<marker>Lee, Carpenter, Chou, ChuCarroll, Reichl, Saad, Zhou, 2000</marker>
<rawString>Lee, Chin-Hui, Bob Carpenter, Wu Chou, Jennifer ChuCarroll, Wolfgang Reichl, Antoine Saad and Qiru Zhou. 2000. On natural language call routing. Speech Communication, 31(4):309-320, Aug.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Goh Wei Boon</author>
<author>Low Kok Leong</author>
</authors>
<title>Feature selection, perceptron learning, and a usability case study for text categorization.</title>
<date>1997</date>
<booktitle>In Proceedings of the 20th International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>67--73</pages>
<location>Philadelphia, Pennsylvania, USA.</location>
<contexts>
<context position="11762" citStr="Ng et al., 1997" startWordPosition="2037" endWordPosition="2040">schema+info all 100 40 90 70 30 80 60 50 task schema info task+schema task+info schema+info all Correct rate Correct rate the callers to select. The correct rate is increased only when more than 2 top-ranking tasks are used for callers’ selection, i.e., 4.38%, 1.32%, 2.66% and 12.32% when n = 3, 4, 5 and 6 respectively. From the results, we found that some words occur commonly across multiple domains. This phenomena is common in problems related to natural language processing. To alleviate the problem, we have used words that only occur commonly in few domains. We use correlation coefficient (Ng et al., 1997) to measure the correlations of all words to all domains. After that, we scale the correlation measures to 1. In calculating the bbm’s, we multiply the original bbm’s with the corresponding correlation measures. Figure 3 shows the results when similar words and correlation measures are considered in the calculation of BF’s. This time, the results show that we obtain the best ranking of candidate tasks when knowledge from task specifications and basic information are used to calculate the BF’s. In addition, there is a 67.31% improvement when the top task in the ranked list is taken as the calle</context>
</contexts>
<marker>Ng, Boon, Leong, 1997</marker>
<rawString>Ng, Hwee Tou, Goh Wei Boon and Low Kok Leong. 1997. Feature selection, perceptron learning, and a usability case study for text categorization. In Proceedings of the 20th International ACM SIGIR Conference on Research and Development in Information Retrieval, 67-73. Philadelphia, Pennsylvania, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philippe Smets</author>
</authors>
<title>Practical uses of belief functions.</title>
<date>1999</date>
<booktitle>Uncertainty in Artificial Intelligence: Proceedings of the Fifteenth Conference (UAI-1999),</booktitle>
<pages>612--621</pages>
<publisher>Morgan Kaufmann Publishers,</publisher>
<location>San Francisco, CA,</location>
<contexts>
<context position="2928" citStr="Smets (1999)" startWordPosition="489" endWordPosition="490">n that s/he is looking for through a series of dialogue interaction. The work reported in this paper is our first attempt toward our ultimate aim, i.e., to determine what the callers want and find out the information the callers have provided that are useful for the task. To achieve that, we use Smets’ (1988) TBM. TBM is the concept used to justify the use of belief functions (BFs), Dempster’s rule of conditioning and Dempster’s rule of combination to model someone’s belief (Smets, 1988). Since early 1980’s, BFs have generated considerable interest in the Artificial Intelligence community. In Smets (1999), Denœux (2000) and Zouhal and Denœux (1998), BFs are used to provide sound and elegant solutions to real life problems where some information is missing. As in Bayesian model, given the available evidences, parts of the amount of belief are allocated to each object in our problem domain. However, some evidences might support something other than only one of the various domain objects. In this case, Principle of Insufficient Reason (Smets, 1988) is invoked to decide that the belief mass must be split equally among the domain objects involved. TBM does not evoke this principle and leaves the be</context>
</contexts>
<marker>Smets, 1999</marker>
<rawString>Smets, Philippe. 1999. Practical uses of belief functions. Uncertainty in Artificial Intelligence: Proceedings of the Fifteenth Conference (UAI-1999), Morgan Kaufmann Publishers, San Francisco, CA, 612–621.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philippe Smets</author>
</authors>
<title>Constructing the pignistic probability function in a context of uncertainty.</title>
<date>1989</date>
<booktitle>Uncertainty in Artificial Intelligence 5. Henrion</booktitle>
<pages>29--40</pages>
<location>Amsterdam,</location>
<marker>Smets, 1989</marker>
<rawString>Smets, Philippe. 1989. Constructing the pignistic probability function in a context of uncertainty. Uncertainty in Artificial Intelligence 5. Henrion M., Shachter R. D., Kanal L. N. and Lemmer J. F. (Eds). North Holland, Amsterdam, 29–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philippe Smets</author>
</authors>
<title>Belief functions. Non-standard Logic for Automated Reasoning.</title>
<date>1988</date>
<pages>252--286</pages>
<publisher>Academic,</publisher>
<location>New York:</location>
<contexts>
<context position="2808" citStr="Smets, 1988" startWordPosition="472" endWordPosition="473">We allow callers to speak in a natural way. Our ultimate aim is to provide the caller with the exact piece of information that s/he is looking for through a series of dialogue interaction. The work reported in this paper is our first attempt toward our ultimate aim, i.e., to determine what the callers want and find out the information the callers have provided that are useful for the task. To achieve that, we use Smets’ (1988) TBM. TBM is the concept used to justify the use of belief functions (BFs), Dempster’s rule of conditioning and Dempster’s rule of combination to model someone’s belief (Smets, 1988). Since early 1980’s, BFs have generated considerable interest in the Artificial Intelligence community. In Smets (1999), Denœux (2000) and Zouhal and Denœux (1998), BFs are used to provide sound and elegant solutions to real life problems where some information is missing. As in Bayesian model, given the available evidences, parts of the amount of belief are allocated to each object in our problem domain. However, some evidences might support something other than only one of the various domain objects. In this case, Principle of Insufficient Reason (Smets, 1988) is invoked to decide that the </context>
<context position="5472" citStr="Smets (1988" startWordPosition="909" endWordPosition="910">sults in more recognition errors. In addition, we do not have a training corpus. However, we have a knowledge base that provides partial information based on word evidences. For examples, the occurrence of word evidence account indicates that the user wants to access her/his employee’s account or bank account, the occurrence of a person name indicates that the user is not checking for a flight status or bus service, the occurrence of word evidence time indicates that the user probably wants to check the train schedules or flight status. Due to space limitation, readers are advised to refer to Smets (1988; 1997; 1989) for more detailed discussions on BFs, combination of BFs, decision making using BFs and TBM. 2 Ranking Information from the Recognized Utterance of Naturally Spoken Input Our aim is to use TBM in dialogue management. First, TBM is used to rank the information identified from the recognized input. Then, the rank list is used in clarification dialogues if necessary. Otherwise, the best result is treated as the user input. Our experiments are done using Sphinx II speech recognition system (Huang et al., 1992). Using a test corpus of 1977 words, we find that the word recognition accu</context>
<context position="7768" citStr="Smets, 1988" startWordPosition="1319" endWordPosition="1320">Each step is linked to either a basic operation, for examples, fetch some records from a database and ask the caller for information, or a sub-task. Information schemas specify the highlevel formats of the information used in . They include database schemas, XML schemas of facts and rules, and format descriptions of some remote tables and databases used in . We do indexing for each so that it is easy to calculate the bbm’s where and . We then do the following adjustments to make sure that : if . , then the BF is scaled to one; otherwise, where . is also called the ignorance value relative to (Smets, 1988). Larger implies that it is harder to decide which is the intended task of the caller by looking at evidence . The BF’s are then combined using Dempster’s rule of combination, where and . is computed by combining and 2.2 Identifying the Provided Information In this experiment, we show whether TBM can be used to identify the information provided by the caller in U. Here, the frame of discernment consists of the objects in the information schemas for a specific task. As in Section 2.1, we use the indices of to compute the bbm’s of given to each object disjunct . Lastly, we combine the BFs and co</context>
</contexts>
<marker>Smets, 1988</marker>
<rawString>Smets, Philippe. 1988. Belief functions. Non-standard Logic for Automated Reasoning. P. Smets, A. Mamdani, D. Dubois, and H. Prade (Eds). New York: Academic, 252–286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philippe Smets</author>
</authors>
<title>The axiomatic justification of the transferable belief model.</title>
<date>1997</date>
<journal>Artificial Intelligence,</journal>
<pages>92--229</pages>
<marker>Smets, 1997</marker>
<rawString>Smets, Philippe. 1997. The axiomatic justification of the transferable belief model. Artificial Intelligence, 92:229–242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David L Thomson</author>
<author>Jack J Wisowaty</author>
</authors>
<title>User confusion in natural language services.</title>
<date>1999</date>
<booktitle>In Proc. ESCA Workshop on Interactive Dialogue in Multi-Modal Systems,</booktitle>
<location>Kloster Irsee, Germany,</location>
<note>keynote address.</note>
<contexts>
<context position="853" citStr="Thomson and Wisowaty, 1999" startWordPosition="127" endWordPosition="130">mputer Science School of Computing National University of Singapore luakt@comp.nus.edu.sg Abstract This paper presents results of using belief functions to rank the list of candidate information provided in a noisy dialogue input. The information under consideration is the intended task to be performed and the information provided for the completion of the task. As an example, we use the task of information access in a multi-domain dialogue system. Currently, the system contains knowledge of ten different domains. Callers calling in are greeted with an open-ended “How may I help you?” prompt (Thomson and Wisowaty, 1999; Chu-Carroll and Carpenter, 1999; Gorin et al., 1997). After receiving a reply from the caller, we extract word evidences from the recognized utterances. By using transferable belief model (TBM), we in turn determine the task that the caller intends to perform as well as any information provided. 1 Introduction Touch-tone menus are prevalent in call centers for accessing personal records and pre-recorded information. However, it can sometimes be very frustrating when we need to listen to a long list of options. Moreover, the information that we are looking for may not seem to be relevant to a</context>
</contexts>
<marker>Thomson, Wisowaty, 1999</marker>
<rawString>Thomson, David L. and Jack J. Wisowaty. 1999. User confusion in natural language services. In Proc. ESCA Workshop on Interactive Dialogue in Multi-Modal Systems, Kloster Irsee, Germany, June, 189–196, keynote address.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lalla Merieme Zouhal</author>
<author>Thierry Dena ux</author>
</authors>
<title>An evidence-theoretic k-NN rule with parameter optimization.</title>
<date>1998</date>
<journal>IEEE Transactions on Systems, Man and Cybernetics – Part C,</journal>
<pages>28--2</pages>
<marker>Zouhal, ux, 1998</marker>
<rawString>Zouhal, Lalla Merieme and Thierry Dena ux. 1998. An evidence-theoretic k-NN rule with parameter optimization. IEEE Transactions on Systems, Man and Cybernetics – Part C, 28(2):263-271.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>