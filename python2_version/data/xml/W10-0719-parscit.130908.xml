<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001876">
<title confidence="0.921452">
Crowdsourcing and language studies: the new generation of linguistic data
</title>
<author confidence="0.891087">
Robert Munro&apos; Steven Bethardb Victor Kuperman&apos; Vicky Tzuyin Lail
Robin Melnick&apos; Christopher Potts&apos; Tyler Schnoebelen&apos; Harry Tily&apos;
</author>
<affiliation confidence="0.999374666666667">
&apos;Department of Linguistics, Stanford University
bDepartment of Computer Science, Stanford University
lDepartment of Linguistics, University of Colorado
</affiliation>
<email confidence="0.838831333333333">
{rmunro,bethard,vickup,rmelnick,cgpotts,tylers,hjt}
@stanford.edu
vicky.lai@colorado.edu
</email>
<sectionHeader confidence="0.99554" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997365">
We present a compendium of recent and cur-
rent projects that utilize crowdsourcing tech-
nologies for language studies, finding that the
quality is comparable to controlled labora-
tory experiments, and in some cases superior.
While crowdsourcing has primarily been used
for annotation in recent language studies, the
results here demonstrate that far richer data
may be generated in a range of linguistic dis-
ciplines from semantics to psycholinguistics.
For these, we report a number of successful
methods for evaluating data quality in the ab-
sence of a ‘correct’ response for any given
data point.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999225">
Crowdsourcing’s greatest contribution to language
studies might be the ability to generate new kinds
of data, especially within experimental paradigms.
The speed and cost benefits for annotation are cer-
tainly impressive (Snow et al., 2008; Callison-
Burch, 2009; Hsueh et al., 2009) but we hope to
show that some of the greatest gains are in the very
nature of the phenomena that we can now study.
For psycholinguistic experiments in particular, we
are not so much utilizing ‘artificial artificial’ intelli-
gence as the plain intelligence and linguistic intu-
itions of each crowdsourced worker – the ‘voices
in the crowd’, so to speak. In many experiments
we are studying gradient phenomena where there
are no right answers. Even when there is binary
response we are often interested in the distribution
of responses over many speakers rather than spe-
cific data points. This differentiates experimentation
from more common means of determining the qual-
ity of crowdsourced results as there is no gold stan-
dard against which to evaluate the quality or ‘cor-
rectness’ of each individual response.
The purpose of this paper is therefore two-fold.
We summarize seven current projects that are utiliz-
ing crowdsourcing technologies, all of them some-
what novel to the NLP community but with potential
for future research in computational linguistics. For
each, we also discuss methods for evaluating quality,
finding the crowdsourced results to often be indistin-
guishable from controlled laboratory experiments.
In Section 2 we present the results from seman-
tic transparency experiments showing near-perfect
interworker reliability and a strong correlation be-
tween crowdsourced data and lab results. Ex-
tending to audio data, we show in Section 3
that crowdsourced subjects were statistically in-
distinguishable from a lab control group in seg-
mentation tasks. Section 4 shows that labora-
tory results from simple Cloze tasks can be repro-
duced with crowdsourcing. In Section 5 we offer
strong evidence that crowdsourcing can also repli-
cate limited-population, controlled-condition lab re-
sults for grammaticality judgments. In Section 6 we
use crowdsourcing to support corpus studies with a
precision not possible with even very large corpora.
Moving to the brain itself, Section 7 demonstrates
that ERP brainwave analysis can be enhanced by
crowdsourced analysis of experimental stimuli. Fi-
nally, in Section 8 we outline simple heuristics for
ensuring that microtasking workers are applying the
linguistic attentiveness required to undertake more
complex tasks.
</bodyText>
<page confidence="0.958128">
122
</page>
<note confidence="0.9953065">
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 122–130,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.483817" genericHeader="introduction">
2 Transparency of phrasal verbs
</sectionHeader>
<bodyText confidence="0.999648230769231">
Phrasal verbs are those verbs that spread their mean-
ing out across both a verb and a particle, as in ‘lift
up’. Semantic transparency is a measure of how
strongly the phrasal verb entails the component verb.
For example, to what extent does ‘lifting up’ entail
‘lifting’? We can see the variation between phrasal
verbs when we compare the transparency of ‘lift up’
to the opacity of ‘give up’.
We conducted five experiments around seman-
tic transparency, with results showing that crowd-
sourced results correlate well with each other and
against lab data (p up to 0.9). Interrater reliability is
also very high: n = 0.823, which Landis and Koch
</bodyText>
<equation confidence="0.620491">
(1977) would call ‘almost perfect agreement.’ 4 5
2 4 6
</equation>
<bodyText confidence="0.998567516129032">
The crowdsourced results reported here represent
judgments by 215 people. Two experiments were
performed using Stanford University undergradu-
ates. The first involved a questionnaire asking par-
ticipants to rate the semantic transparency of 96
phrasal verbs. The second experiment consisted of
a paper questionnaire with the phrasal verbs in con-
text. That is, the first group of ‘StudentLong’ par-
ticipants rated the similarity of ‘cool’ to ‘cool down’
on a scale 1-7:
cool cool down
The ‘StudentContext’ participants performed the
same basic task but saw each verb/phrasal verb pair
with an example of the phrasal verb in context.
With Mechanical Turk, we had three conditions:
TurkLong: A replication of the first questionnaire
and its 96 questions.
TurkShort: The 96-questions were randomized into
batches of 6. Thus, some participants ended up giv-
ing responses to all phrasal verbs, while others only
gave 6, 12, 18, etc responses.
TurkContext: A variation of the ‘StudentContext’
task – participants were given examples of the
phrasal verbs, though as with ‘TurkShort’, they were
only asked to rate 6 phrasal verbs at a time.
What we find is a split into relatively high and low
correlations, as Figure 1 shows. All Mechanical
Turk tests correlate very well with one another (all
p &gt; 0.7), although the tasks and raters are differ-
ent. The correlation between the student participants
who were given sentence contexts and the workers
</bodyText>
<equation confidence="0.9999175">
p = 0
p = 0
p = 0
p = 0
</equation>
<figureCaption confidence="0.886780857142857">
Figure 1: Panels at the diagonal report histograms of dis-
tributions of ratings across populations of participants;
panels above the diagonal plot the locally weighted scat-
terplot smoothing Lowess functions for a pair of corre-
lated variables; panels below the diagonal report correla-
tion coefficients (the r value is Pearson’s r, the rs value
is Spearman’s p) and respective p values.
</figureCaption>
<bodyText confidence="0.992700333333333">
who saw context is especially high (0.9). All corre-
lations with StudentLong are relatively low, but this
is actually true for StudentLong vs. StudentContext,
too (p = 0.44), even though both groups are Stan-
ford undergraduates.
Intra-class correlation coefficients (ICC) measure
the agreement among participants, and these are
high for all groups except StudentLong. Just among
StudentLong participants, the ICC consistency is
only 0.0934 and their ICC agreement is 0.0854.
Once we drop StudentLong, we see that all of the
remaining tests have high consistency (average of
0.78 for ICC consistency, 0.74 for ICC agreement).
For example, if we combine TurkContext and Stu-
dentContext, ICC consistency is 0.899 and ICC
agreement of 0.900. Cohen’s kappa measurement
also measures how well raters agree, weeding out
chance agreements. Again, StudentLong is an out-
lier. Together, TurkContext / StudentContext gets a
weighted kappa score of 0.823 – the overall average
(excepting StudentLong) is n = 0.700.
More details about the results in this section can
be found in Schnoebelen and Kuperman (submit-
ted).
</bodyText>
<equation confidence="0.994799863636364">
2.0 3.5 5.0
3 4 5 6
2.5 3.5 4.5 5.5
p = 0
p = 0
r = 0.74
rs = 0.73
p = 0
r = 0.77
rs = 0.75
TurkContext
p = 0
p = 0
r = 0.46
p = 0
rs = 0.46
r = 0.7
p = 0
p = 0
r = 0.48
p = 0
rs = 0.48
p = 0
rs = 0.9
p = 0
r = 0.46
p = 0
rs = 0.45
r = 0.41
p = 0
rs = 0.44
StudentLong
r = 0.92
rs = 0.92
TurkShort
TurkLong
p = 0
p = 0
rs = 0.67
r = 0.9
p = 0
r = 0.68
rs = 0.67
StudentContext
</equation>
<page confidence="0.994051">
123
</page>
<sectionHeader confidence="0.442259" genericHeader="method">
3 Segmentation of an audio speech stream
</sectionHeader>
<bodyText confidence="0.999979630434783">
The ability of browsers to present multimedia re-
sources makes it feasible to use crowdsourcing tech-
niques to generate data using spoken as well as writ-
ten stimuli. In this section we report an MTurk repli-
cation of a classic psycholinguistic result that relies
on audio presentation of speech. We developed a
web-based interface that allows us to collect data
in a statistical word segmentation paradigm. The
core is a Flash applet developed using Adobe Flex
which presents audio stimuli and collects participant
responses (Frank et al., submitted).
Human children possess a remarkable ability to
learn the words and structures of languages they are
exposed to without explicit instruction. One partic-
ularly remarkable aspect is that unlike many written
languages, spoken language lacks spaces between
words: from spoken input, children learn not only
the mapping between meanings and words but also
what the words themselves are, with no direct infor-
mation about where one ends and the next begins.
Research in statistical word segmentation has shown
that both infants and adults use statistical properties
of speech in an unknown language to infer a proba-
ble vocabulary. In one classic study, Saffran, New-
port &amp; Aslin (1996) showed that after a few minutes
of exposure to a language made by randomly con-
catenating copies of invented words, adult partici-
pants could discriminate those words from syllable
sequences that also occurred in the input but crossed
a word boundary. We replicated this study showing
that cheap and readily accessible data from crowd-
sourced workers compares well to data from partic-
ipants recorded in person in the lab.
Participants heard 75 sentences from one of 16 ar-
tificially constructed languages. Each language con-
tained 2 two-syllable, 2 three-syllable, and 2 four
syllable words, with syllables drawn from a possi-
ble set of 18. Each sentence consisted of four words
sampled without replacement from this set and con-
catenated. Sentences were rendered as audio by
the MBROLA synthesizer (Dutoit et al., 1996) at a
constant pitch of 100Hz with 25ms consonants and
225ms vowels. Between each sentence, participants
were required to click a “next” button to continue,
preventing workers from leaving their computer dur-
ing this training phase. To ensure workers could ac-
</bodyText>
<figureCaption confidence="0.993051">
Figure 2: Per-subject correct responses for lab and MTurk
participants. Bars show group means, and the dashed line
indicates the chance baseline.
</figureCaption>
<bodyText confidence="0.999977482758621">
tually hear the stimuli, they were first asked to enter
an English word presented auditorily.
Workers then completed ten test trials in which
they heard one word from the language and one non-
word made by concatenating all but the first syllable
of one word with the first syllable of another. If the
words “bapu” and “gudi” had been presented adja-
cently, the string “pugu” would have been heard, de-
spite not being a word of the language. Both were
also displayed orthographically, and the worker was
instructed to click on the one which had appeared in
the previously heard language.
The language materials described above were
taken from a Saffran et al. (1996) replication re-
ported as Experiment 2 in Frank, Goldwater, Grif-
fiths &amp; Tenenbaum (under review). We compared
the results from lab participants reported in that ar-
ticle to data from MTurk workers using the applet
described above. Each response was marked “cor-
rect” if the participant chose the word rather than the
nonword. 12 lab subjects achieved 71% correct re-
sponses, while 24 MTurk workers were only slightly
lower at 66%. The MTurk results proved signif-
icantly different from a “random clicking” base-
line of 50% (t(23) = 5.92, p = 4.95 x 10−06)
but not significantly different from the lab subjects
(Welch two-sample t-test for unequal sample sizes,
t(21.21) = −.92, p = .37). Per-subject means for
the lab and MTurk data are plotted in Figure 2.
</bodyText>
<page confidence="0.996972">
124
</page>
<sectionHeader confidence="0.971978" genericHeader="method">
4 Contextual predictability
</sectionHeader>
<bodyText confidence="0.999985780487805">
As psycholinguists build models of sentence pro-
cessing (e.g., from eye tracking studies), they need
to understand the effect of the available sentence
context. One way to gauge this is the Cloze task pro-
posed in Taylor (1953): participants are presented
with a sentence fragment and asked to provide the
upcoming word. Researchers do this for every word
in every stimulus and use the percentage of ‘correct’
guesses as input into their statistical and computa-
tional models.
Rather than running such norming studies on un-
dergraduates in lab settings (as is typical), our results
suggest that psycholinguists will be able to crowd-
source these tasks, saving time and money without
sacrificing reliability (Schnoebelen and Kuperman,
submitted).
Our results are taken from 488 Americans, rang-
ing from age 16-80 (mean: 34.49, median: 32,
mode: 27) with about 25% each from the East and
Midwest, 31% from the South, the rest from the
West and Alaska. They represent a range of educa-
tion levels, though the majority had been to college:
about 33.8% had bachelor’s degrees, another 28.1%
had some college but without a degree.
By contrast, the lab data was gathered from 20
participants, all undergraduates at the University of
Massachusetts at Amherst in the mid-1990’s (Re-
ichle et al., 1998). Both populations provided judg-
ments on 488 words in 48 sentences. In general,
crowdsourcing gave more diverse responses, as we
would expect from a more diverse population.
The correlation between lab and crowdsourced
data by Spearman’s rank correlation is 0.823 (p &lt;
0.0001), but we can be even more conservative by
eliminating the 124 words that had predictability
scores of 0 across both groups. By and large, the
lab participants and the workers are consistent in
which words they fail to predict. Even when we
eliminate these shared zeros, the correlation is still
high between the two data sets: weighted K = 0.759
(p &lt; 0.0001).
</bodyText>
<sectionHeader confidence="0.941643" genericHeader="method">
5 Judgment studies of fine-grained
probabilistic grammatical knowledge
</sectionHeader>
<bodyText confidence="0.714511">
Moving to syntax, we demonstrate here that gram-
maticality judgments from lab studies can also be
</bodyText>
<figureCaption confidence="0.993174">
Figure 3: Mean ‘that’-inclusion ratings plotted against
corresponding corpus-model predictions. The solid line
would represent perfect alignment between judgments
and corpus model. Non-parametric Lowess smoothers il-
lustrate the significant correlation between lab and crowd
population results.
</figureCaption>
<bodyText confidence="0.999155625">
reproduced through crowdsourcing.
Corpus studies of spontaneous speech suggest
that grammaticality is gradient (Wasow, 2008), and
models of English complement clause (CC) and rel-
ative clause (RC) ‘that’-optionality have as their
most significant factor the predictability of embed-
ding, given verb (CC) and head noun (RC) lemma
(Jaeger, 2006; Jaeger, in press). Establishing that
these highly gradient factors are similarly involved
in judgments could provide evidence that such fine-
grained probabilistic knowledge is part of linguistic
competence.
We undertook six such judgment experiments:
two baseline studies with lab populations then four
additional crowdsourced trials via MTurk.
Experiment 1, a lab trial (26 participants, 30
items), began with the models of RC-reduction de-
veloped in Jaeger (2006). Corpus tokens were
binned by relative model-predicted probability of
‘that’-omission. Six tokens were extracted at ran-
dom from each of five bins (0&lt;p&lt;20% likelihood of
‘that’-inclusion; 20&lt;p&lt;40%; and so on). In a gra-
dient scoring paradigm with 100 points distributed
between available options (Bresnan, 2007) partici-
</bodyText>
<page confidence="0.994226">
125
</page>
<bodyText confidence="0.9998758125">
pants rated how likely each choice – with or without
‘that’ – was as the continuation of a segment of dis-
course. As hypothesized, mean participant ratings
significantly correlate with corpus model predictions
(r = 0.614, p = 0.0003).
Experiment 2 (29 participants) replicated Exper-
iment 1 to address concerns that subjects might be
‘over-thinking’ the process. We used a timed forced-
choice paradigm where participants had from 5 to 24
seconds (varied as a linear function of token length)
to choose between the reduced/unreduced RC stim-
uli. These results correlate even more closely with
predictions (r = 0.838, p &lt; 0.0001).
Experiments 3 and 4 replicated 1 and 2 on MTurk
(1200 tasks each). Results were filtered by volun-
teered demographics to select the same subject pro-
file as the lab experiments. Response-time outliers
were also excluded to avoid fast-click-through and
distracted-worker data. Combined, these steps elim-
inated 384 (32.0%) and 378 (31.5%) tasks, respec-
tively, with 89 and 66 unique participants remaining.
While crowdsourced measures might be expected to
yield lower correlations due to such unbalanced data
sets, the results remain significant in both trials (r =
0.562, p = 0.0009; r = 0.364, p = 0.0285), offer-
ing strong evidence that crowdsourcing can replicate
limited-population, controlled-condition lab results,
and of the robustness of the alignment between pro-
duction and judgment models. Figure 3 compares
lab and crowd population results in the 100-point
task (Experiments 1 and 3).
Experiments 5 and 6 (1600 hits each) employed
the same paradigms via MTurk to investigate ‘that’-
mentioning in CCs, where predictability of embed-
ding is an even stronger factor in the corpus model.
Filtering reduced the data by 590 (36.9%) and 863
(53.9%) hits. As with the first four experiments,
each of these trials produced significant correlations
(r = 0.433, p = 0.0107; r = 0.500, p = 0.0034; re-
spectively). Finally, mixed-effect binary logistic re-
gression models – with verb lemma and test subject
ID as random effects – were fitted to these judgment
data. As in the corpus-derived models, predictability
of embedding remains the most significant factor in
all experimental models.
The results across both lab and crowdsourced
studies suggest that speakers consider the same fac-
tors in judgment as in production, offering evidence
</bodyText>
<figureCaption confidence="0.989299666666667">
Figure 4: Odds ratio of a Nominal Agent being embed-
ded within a Sentential Agent or non-Agent, relative to
random chance. (p &lt; 0.001 for all)
</figureCaption>
<bodyText confidence="0.9986558">
that competence grammar includes access to prob-
ability distributions. Meanwhile, the strong cor-
relations across populations offer encouraging evi-
dence in support of using the latter in psycholinguis-
tic judgment research.
</bodyText>
<sectionHeader confidence="0.990944" genericHeader="method">
6 Confirming corpus trends
</sectionHeader>
<bodyText confidence="0.9997308">
Crowdsourcing can also be used to establish the va-
lidity of corpus trends found in otherwise skewed
data. The experiments in this section were mo-
tivated by the NomBank corpus of nominal pred-
icate/arguments (Meyers et al., 2004) where we
found that an Agent semantic role was much more
likely to be embedded within a sentential Agent. For
example, (1) is more likely than (2) to receive the
Agent interpretation for the ‘the police’, but both
have same potential range of meanings:
</bodyText>
<listItem confidence="0.98382325">
(1) “The investigation of the police took 3 weeks to
complete”
(2) “It took 3 weeks to complete the investigation of
the police”
</listItem>
<bodyText confidence="0.999585153846154">
While the trend is significant (p &lt; 0.001), the
corpus is not representative speech.
First, there are no minimal pairs of sentences in
NomBank like (1) and (2) that have the same poten-
tial range of meanings. Second, the s-genitive (“the
police’s investigation”) is inherently more Agen-
tive than the of-genitive (“the investigation of the
police”) and it is also more compact. Sentential
subjects tend to be lighter than objects, and more
likely to realize Agents, so the resulting correlation
could be indirect. Finally, if we sampled only the
predicates/arguments in NomBank that are frequent
in different sentential positions, we are limited to:
</bodyText>
<page confidence="0.997252">
126
</page>
<bodyText confidence="0.999623127659575">
“earning, product, profit, trading, loss, share, rate,
sale, price”. This purely financial terminology is not
representative of a typical acquisition environment –
no child should be exposed to only such language –
so it is difficult to draw broad conclusions about the
cognitive viability of this correlation, even within
English. It is because of factors like these that cor-
pus linguistics has been somewhat of a ‘poor cousin’
to theoretical linguistics.
Therefore, two sets of experiments were under-
taken to confirm that the trend is not epiphenomenal,
one testing comprehension and one testing produc-
tion.
The first tested thousands of workers’ interpre-
tations of sentences like those in (1) and (2), over
a number of predicate/argument pairs (“shooting of
the hunters”, “destruction of the army” etc). Work-
ers were asked their interpretation of the most likely
meaning. For example, does (1) mean: “a: the po-
lice were doing the investigation” or “b: the po-
lice are being investigated”. To control for errors
or click-throughs, two plainly incorrect options were
included. We estimate the erroneous response rate at
about 0.4% – less than many lab studies.
For the second set of experiments, workers were
asked to reword an unambiguous sentence using a
given phrase. For example, rewording the following
using “the investigation of the police”:
(3) “Following the shooting of a commuter in Oak-
land last week, a reporter has uncovered new evi-
dence while investigating the police involved.”
We then (manually) recorded whether the required
phrase was in a sentential Agent or non-Agent posi-
tion.
Figure 4 gives the results from the corpus analy-
sis and both experiments. The results clearly show
a significant trend for all, and that the NomBank
trend falls between the comprehension and produc-
tion tasks, which would be expected for this highly
edited register. It therefore supports the validity of
the corpus results.
The phenomena likely exists to aid comprehen-
sion, as the cognitive realization of just one role
needs to be activated at a given moment. Despite
the near-ubiquity of ‘Agent’ in studies of semantic
roles, we do not yet have a clear theory of this lin-
guistic entity, or even firm evidence of its existence
</bodyText>
<figureCaption confidence="0.999736">
Figure 5: Distribution of metaphorical frequencies.
</figureCaption>
<bodyText confidence="0.9669525625">
(Parikh, 2010). This study therefore goes some way
towards illuminating this. More broadly, the experi-
ments in this section support the wider use of crowd-
sourcing as a tool for language cognition research in
conjunction with more traditional corpus studies.
7 Post-hoc metaphorical frequency
analysis of electrophysiological responses
Beyond reproducing laboratory and corpus studies,
crowdsourcing also offers the opportunity to newly
analyze data drawn from many other experimental
stimuli. In this section, we demonstrate that crowd-
sourced workers can help us better understand ERP
brainwave data by looking at how frequently words
are used metaphorically.
Recent work in event related potentials (ERP) has
suggested that even conventional metaphors, such as
“All my ideas were attacked” require additional pro-
cessing effort in the brain as compared to literal sen-
tences like “All the soldiers were attacked” (Lai et
al., 2009). This study in particular observed an N400
effect where negative waves 400 milliseconds after
the presentation of the target words (e.g. attacked)
were larger when the word was used metaphorically
than when used literally.
The proposed explanation for this effect is that
metaphors really do demand more from the brain
than literal sentences. However, N400 effects are
also observed when subjects encounter something
that is semantically inappropriate or unexpected.
While the Lai experiment controlled for overall
word frequency, it might be possible to explain away
these N400 effects if it turned out that in the real
</bodyText>
<page confidence="0.994365">
127
</page>
<bodyText confidence="0.999607066666667">
world the target words were almost always used
literally, so that seeing them used metaphorically
would be semantically incongruous.
To test this alternative hypothesis, we gathered
sense frequency distributions for each of the target
words – the hypothesis predicts that these should
be skewed towards literal senses. For each of the
104 target words, we selected 50 random sentences
from the American National Corpus (ANC), fill-
ing in with British National Corpus sentences when
there were too few in the ANC. We gave the sen-
tences to crowdsourced workers and asked them to
label each target word as being used literally or
metaphorically. Each task contained one sentence
for each of the 104 target words, with the order of
words and the literal/metaphorical buttons random-
ized. Each sentence was annotated 5 times.
To encourage native speakers of English, we had
the MTurk service require that our workers be within
the United States, and posted the text “Please ac-
cept this HIT only if you are a native speaker of En-
glish” in bold at the top of each HIT. We also used
Javascript to force workers to spend at least 2 sec-
onds on each sentence and we rejected results from
workers that had chance level (50%) agreement with
the other workers.
Though our tasks produced words annotated with
literal and metaphorical tags, we were less inter-
ested in the individual annotations (though agree-
ment was decent at 73%) and more interested in the
overall pattern for each target word. Some words,
like fruit, were almost always used literally (92%),
while other words, like hurdle were almost always
used metaphorically (91%) .
Overall, the target words had a mean metaphor-
ical frequency of 53%, indicating that their literal
and metaphorical senses were used in nearly equal
proportions. Figure 5 shows that the metaphorical
frequencies follow roughly a bell-curved distribu-
tion1, which is especially interesting given that the
target words were hand-selected for the Lai experi-
ment and not drawn randomly from a corpus. We did
not observe any skew towards literal senses as the
alternative hypothesis would have predicted. This
suggests that the findings of Lai, Curran, and Menn
</bodyText>
<footnote confidence="0.830774">
1A Shapiro-Wilk test fails to reject the null hypothesis of a
normal distribution (p=0.09).
</footnote>
<table confidence="0.978586">
Item type correct incorrect
‘easy’ 60 2
‘promise’ 59 3
stacked genitive 55 7
</table>
<tableCaption confidence="0.8348455">
Table 1: Response data for three control items, with the
goal of identifying workers who lack the requisite atten-
tiveness. All show high attentiveness. The difference be-
tween the ‘easy’ and ‘stacked genitive’ is trending but not
significant (p = 0.0835), indicating that any of these may
be used.
</tableCaption>
<bodyText confidence="0.996395176470588">
(2009) cannot be dismissed based on a sense fre-
quency argument.
We also took advantage of the collected sense fre-
quency distributions to re-analyze data from the Lai
experiment. We split the target words into a high bin
(average 72% metaphorical) and a low bin (average
33% metaphorical), matching the number of items
and average word log-frequency per bin. Looking at
the average ERPs (brain waves) over time for each
bin revealed that when subjects were reading novel
metaphors, there was a significant difference (p =
.01) at about 200ms (P200) between the ERPs for
the highly literal words and the ERPs for the highly
metaphorical words. Thus, not only does metaphori-
cal frequency influence figurative language process-
ing, but it does so much earlier than semantic effects
are usually observed (e.g. N400 effects at 400ms)2.
</bodyText>
<sectionHeader confidence="0.980129" genericHeader="method">
8 Screening for linguistic attentiveness
</sectionHeader>
<bodyText confidence="0.99993">
For annotation tasks, crowdsourcing is most suc-
cessful when the tasks are designed to be as simple
as possible, but in experimental work we don’t al-
ways want to target the shallowest knowledge of the
workers, so here we seek to discover just how atten-
tive the workers really are.
When running psycholinguistics experiments in
the lab, the experimenters generally have the chance
to interact with participants. It is not uncommon
for prospective subjects to be visibly exhausted, dis-
tracted, or inebriated, or not fluent in the given lan-
guage to a requisite level of competence. When
these participants turn up as outliers in the experi-
mental data, it is easy enough to see why — they
fell asleep, couldn’t understand the instructions, etc.
</bodyText>
<footnote confidence="0.980958">
2These results are consistent with recent findings that irony
frequency may also produce P200 effects (Regel et al., 2010).
</footnote>
<page confidence="0.99407">
128
</page>
<bodyText confidence="0.9998104">
With crowdsourcing we lose the chance to have
these brief but valuable encounters, and so anoma-
lous response data are harder to interpret.
We present two simple experiments for measuring
linguistic attentiveness, which can be used as one
component of a language study or to broadly evalu-
ate the linguistic competency of the workers. Taking
well-known constructions from the literature, we se-
lected constructions that: (a) exist in most (perhaps
all) dialects of English; (b) involve high frequency
lexical items; and (c) tend to be acquired relatively
late by first-language learners.
We have found two constructions from Carol
Chomsky’s (1969) work on first-language acquisi-
tion to be particularly useful:
</bodyText>
<listItem confidence="0.9838485">
(4) John is easy to see.
(5) John is eager to see.
</listItem>
<bodyText confidence="0.9317584">
Example (4) is accurately paraphrased as ‘It is easy
to see John’, where John is the object of ‘see’,
whereas (5) is accurately paraphrased as ‘John is ea-
ger for John to see’, where John is the subject of
‘see’. A similar shift happens with ‘promise’:
</bodyText>
<listItem confidence="0.986401">
(6) Bozo told Donald to sing.
(7) Bozo promised Donald to sing.
</listItem>
<bodyText confidence="0.999862666666667">
We presented workers with a multiple-choice ques-
tion that contained both subject and object para-
phrases as options.
In similar experiments, we adapted examples
from Roeper (2007), who looked at stacked prenom-
inal possessive constructions:
</bodyText>
<listItem confidence="0.703075">
(8) John’s sister’s friend’s car.
</listItem>
<bodyText confidence="0.999980857142857">
These are cross-linguistically rare and challenging
even for native speakers. As above, the workers
were asked to choose between paraphrases.
Workers who provide accurate judgments are
likely to have a level of English competence and de-
votion to the task that suffices for many language
experiments. The results from one short audio study
are given in Table 1. They indicate a high degree of
attentiveness; as a group, our subjects performed at
the near-perfect levels we expect for fluent adults.
We predict that adding tasks like these to experi-
ments will not only screen for attentiveness, but also
prompt for greater attention from an otherwise dis-
tracted worker, improving results at both ends.
</bodyText>
<sectionHeader confidence="0.996141" genericHeader="conclusions">
9 Conclusions
</sectionHeader>
<bodyText confidence="0.999991326086957">
While crowdsourcing was first used by linguists for
annotation, we hope that the results here demon-
strate the potential for far richer studies. In a
range of linguistic disciplines from semantics to
psycholinguistics it enables systematic, large-scale
judgment studies that are more affordable and con-
venient than expensive, time-consuming lab-based
studies. With crowdsourcing technologies, linguists
have a reliable new tool for experimentally investi-
gating language processing and linguistic theory.
Here, we have reproduced many ‘classic’ large-
scale lab studies with a relative ease. We can en-
vision many more ways that crowdsourcing might
come to shape new methodologies for language
studies. The affordability and agility brings experi-
mental linguistics closer to corpus linguistics, allow-
ing the quick generation of targeted corpora. Multi-
ple iterations that were previously possible only over
many years and several grants (and therefore never
attempted) are now possible in a matter of days. This
could launch whole new multi-tiered experimental
designs, or at the very least allow ‘rapid prototyp-
ing’ of experiments for later lab-based verification.
Crowdsourcing also brings psycholinguistics
much closer to computational linguistics. The
two fields have always shared empirical data-driven
methodologies and computer-aided methods. We
now share a work-space too. Historically, NLP has
necessarily drawn corpora from the parts of linguis-
tic theory that have stayed still long enough to sup-
port time-consuming annotation projects. The re-
sults here have implications for such tasks, includ-
ing parsing, word-sense disambiguation and seman-
tic role labeling, but the most static parts of a field
are rarely the most exciting. We therefore predict
that crowdsourcing will also lead to an expanded,
more dynamic NLP repertoire.
Finally, for the past half-century theoretical lin-
guistics has relied heavily on ‘introspective’ corpus
generation, as the rare edge cases often tell us the
most about the boundaries of a given language. Now
that we can quickly and confidently generate empir-
ical results to evaluate hypotheses drawn from intu-
itions about the most infrequent linguistic phenom-
ena, the need for this particular fallback has dimin-
ished – the stimuli are abundant.
</bodyText>
<page confidence="0.998209">
129
</page>
<sectionHeader confidence="0.997374" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999909333333333">
We owe thanks to many people, especially within
the Department of Linguistics at Stanford, which has
quickly become a hive of activitiy for crowdsourced
linguistic research. In particular, we thank Tom Wa-
sow for his guidance in Section 5, Chris Manning for
his guidance in Section 6, and Florian T. Jaeger for
providing the corpus-derived base models in Section
5 (Jaeger, 2006). We also thank Michael C. Frank
for providing the design, materials, and lab data used
to evaluate the methods in Section 3. Several of the
projects reported here were supported by Stanford
Graduate Fellowships.
</bodyText>
<sectionHeader confidence="0.999434" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9995755375">
Joan Bresnan. 2007. Is syntactic knowledge probabilis-
tic? Experiments with the English dative alternation.
In Sam Featherston and Wolfgang Sternefeld, editors,
Roots: Linguistics in search of its evidential base,
pages 75–96. Mouton de Gruyter, Berlin.
Chris Callison-Burch. 2009. Fast, cheap, and cre-
ative: evaluating translation quality using Amazon’s
Mechanical Turk. In EMNLP ’09: Proceedings of
the 2009 Conference on Empirical Methods in Natu-
ral Language Processing, pages 286–295.
Carol Chomsky. 1969. The Acquisition of Syntax in Chil-
dren from 5 to 10. MIT Press, Cambridge, MA.
Thierry Dutoit, Vincent Pagel, Nicolas Pierret, Franois
Bataille, and Olivier van der Vrecken. 1996. The
MBROLA project: Towards a set of high quality
speech synthesizers free of use for non commercial
purposes. In Fourth International Conference on Spo-
ken Language Processing, pages 75–96.
Michael Frank, Harry Tily, Inbal Arnon, and Sharon
Goldwater. submitted. Beyond transitional probabili-
ties: Human learners impose a parsimony bias in sta-
tistical word segmentation.
Michael Frank, Sharon Goldwater, Thomas Griffiths, and
Joshua Tenenbaum. under review. Modeling human
performance in statistical word segmentation.
Pei-Yun Hsueh, Prem Melville, and Vikas Sindhwani.
2009. Data quality from crowdsourcing: a study of
annotation selection criteria. In Proceedings of the
NAACL HLT 2009 Workshop on Active Learning for
Natural Language Processing, pages 27–35.
Florian Jaeger. 2006. Redundancy and syntactic reduc-
tion in spontaneous speech. Ph.D. thesis, Stanford
University, Stanford, CA.
Florian Jaeger. in press. Redundancy and reduction:
Speakers manage syntactic information density. Cog-
nitive Psychology.
Vicky Tzuyin Lai, Tim Curran, and Lise Menn. 2009.
Comprehending conventional and novel metaphors:
An ERP study. Brain Research, 1284:145–155, Au-
gust.
Richard Landis and Gary Koch. 1977. The measurement
of observer agreement for categorical data. Biomet-
rics, 33(1).
Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel
Szekely, Veronika Zielinska, Brian Young, , and Ralph
Grishman. 2004. Annotating noun argument structure
for NomBank. In Proceedings of LREC-2004.
Prashant Parikh. 2010. Language and Equilibrium. MIT
Press, Cambridge, MA.
Stefanie Regel, Seana Coulson, and Thomas C. Gunter.
2010. The communicative style of a speaker can af-
fect language comprehension? ERP evidence from the
comprehension of irony. Brain Research, 1311:121–
135.
Erik D. Reichle, Alexander Pollatsek, Donald L. Fisher,
and Keith Rayner. 1998. Toward a model of eye
movement control in reading. Psychological Review,
105:125–157.
Tom Roeper. 2007. The Prism of Grammar: How Child
Language Illuminates Humanism. MIT Press, Cam-
bridge, MA.
Jenny R. Saffran, Richard N. Aslin, and Elissa L. New-
port. 1996. Word segmentation: The role of distribu-
tional cues. Journal of memory and language, 35:606–
621.
Tyler Schnoebelen and Victor Kuperman. submitted. Us-
ing Amazon Mechanical Turk for linguistic research:
Fast, cheap, easy, and reliable.
Rion Snow, Brendan O’Connor, Daniel Jurafsky, and An-
drew T. Ng. 2008. Cheap and fast—but is it good?:
evaluating non-expert annotations for natural language
tasks. In EMNLP ’08: Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing, pages 254–263.
Wilson Taylor. 1953. Cloze procedure: A new tool for
measuring readability. Journalism Quarterly, 30:415–
433.
Tom Wasow. 2008. Gradient data and gradient gram-
mars. In Proceedings of the 43rd Annual Meeting of
the Chicago Linguistics Society, pages 255–271.
</reference>
<page confidence="0.997574">
130
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.508777">
<title confidence="0.999621">Crowdsourcing and language studies: the new generation of linguistic data</title>
<author confidence="0.947592">Victor Tzuyin</author>
<degree confidence="0.712535">of Linguistics, Stanford of Computer Science, Stanford of Linguistics, University of</degree>
<email confidence="0.994673">vicky.lai@colorado.edu</email>
<abstract confidence="0.998726666666667">We present a compendium of recent and current projects that utilize crowdsourcing technologies for language studies, finding that the quality is comparable to controlled laboratory experiments, and in some cases superior. While crowdsourcing has primarily been used for annotation in recent language studies, the results here demonstrate that far richer data may be generated in a range of linguistic disciplines from semantics to psycholinguistics. For these, we report a number of successful methods for evaluating data quality in the absence of a ‘correct’ response for any given data point.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Joan Bresnan</author>
</authors>
<title>Is syntactic knowledge probabilistic? Experiments with the English dative alternation.</title>
<date>2007</date>
<booktitle>Mouton de Gruyter,</booktitle>
<pages>75--96</pages>
<editor>In Sam Featherston and Wolfgang Sternefeld, editors, Roots:</editor>
<location>Berlin.</location>
<contexts>
<context position="15306" citStr="Bresnan, 2007" startWordPosition="2469" endWordPosition="2470">ic knowledge is part of linguistic competence. We undertook six such judgment experiments: two baseline studies with lab populations then four additional crowdsourced trials via MTurk. Experiment 1, a lab trial (26 participants, 30 items), began with the models of RC-reduction developed in Jaeger (2006). Corpus tokens were binned by relative model-predicted probability of ‘that’-omission. Six tokens were extracted at random from each of five bins (0&lt;p&lt;20% likelihood of ‘that’-inclusion; 20&lt;p&lt;40%; and so on). In a gradient scoring paradigm with 100 points distributed between available options (Bresnan, 2007) partici125 pants rated how likely each choice – with or without ‘that’ – was as the continuation of a segment of discourse. As hypothesized, mean participant ratings significantly correlate with corpus model predictions (r = 0.614, p = 0.0003). Experiment 2 (29 participants) replicated Experiment 1 to address concerns that subjects might be ‘over-thinking’ the process. We used a timed forcedchoice paradigm where participants had from 5 to 24 seconds (varied as a linear function of token length) to choose between the reduced/unreduced RC stimuli. These results correlate even more closely with </context>
</contexts>
<marker>Bresnan, 2007</marker>
<rawString>Joan Bresnan. 2007. Is syntactic knowledge probabilistic? Experiments with the English dative alternation. In Sam Featherston and Wolfgang Sternefeld, editors, Roots: Linguistics in search of its evidential base, pages 75–96. Mouton de Gruyter, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
</authors>
<title>Fast, cheap, and creative: evaluating translation quality using Amazon’s Mechanical Turk.</title>
<date>2009</date>
<booktitle>In EMNLP ’09: Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>286--295</pages>
<marker>Callison-Burch, 2009</marker>
<rawString>Chris Callison-Burch. 2009. Fast, cheap, and creative: evaluating translation quality using Amazon’s Mechanical Turk. In EMNLP ’09: Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 286–295.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carol Chomsky</author>
</authors>
<title>The Acquisition of Syntax</title>
<date>1969</date>
<booktitle>in Children from 5 to 10.</booktitle>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Chomsky, 1969</marker>
<rawString>Carol Chomsky. 1969. The Acquisition of Syntax in Children from 5 to 10. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thierry Dutoit</author>
<author>Vincent Pagel</author>
<author>Nicolas Pierret</author>
<author>Franois Bataille</author>
<author>Olivier van der Vrecken</author>
</authors>
<title>The MBROLA project: Towards a set of high quality speech synthesizers free of use for non commercial purposes.</title>
<date>1996</date>
<booktitle>In Fourth International Conference on Spoken Language Processing,</booktitle>
<pages>75--96</pages>
<marker>Dutoit, Pagel, Pierret, Bataille, van der Vrecken, 1996</marker>
<rawString>Thierry Dutoit, Vincent Pagel, Nicolas Pierret, Franois Bataille, and Olivier van der Vrecken. 1996. The MBROLA project: Towards a set of high quality speech synthesizers free of use for non commercial purposes. In Fourth International Conference on Spoken Language Processing, pages 75–96.</rawString>
</citation>
<citation valid="false">
<authors>
<author>submitted</author>
</authors>
<title>Beyond transitional probabilities: Human learners impose a parsimony bias in statistical word segmentation.</title>
<marker>submitted, </marker>
<rawString>Michael Frank, Harry Tily, Inbal Arnon, and Sharon Goldwater. submitted. Beyond transitional probabilities: Human learners impose a parsimony bias in statistical word segmentation.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Michael Frank</author>
<author>Sharon Goldwater</author>
<author>Thomas Griffiths</author>
<author>Joshua Tenenbaum</author>
</authors>
<title>under review. Modeling human performance in statistical word segmentation.</title>
<marker>Frank, Goldwater, Griffiths, Tenenbaum, </marker>
<rawString>Michael Frank, Sharon Goldwater, Thomas Griffiths, and Joshua Tenenbaum. under review. Modeling human performance in statistical word segmentation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pei-Yun Hsueh</author>
<author>Prem Melville</author>
<author>Vikas Sindhwani</author>
</authors>
<title>Data quality from crowdsourcing: a study of annotation selection criteria.</title>
<date>2009</date>
<booktitle>In Proceedings of the NAACL HLT 2009 Workshop on Active Learning for Natural Language Processing,</booktitle>
<pages>27--35</pages>
<contexts>
<context position="1344" citStr="Hsueh et al., 2009" startWordPosition="182" endWordPosition="185">nnotation in recent language studies, the results here demonstrate that far richer data may be generated in a range of linguistic disciplines from semantics to psycholinguistics. For these, we report a number of successful methods for evaluating data quality in the absence of a ‘correct’ response for any given data point. 1 Introduction Crowdsourcing’s greatest contribution to language studies might be the ability to generate new kinds of data, especially within experimental paradigms. The speed and cost benefits for annotation are certainly impressive (Snow et al., 2008; CallisonBurch, 2009; Hsueh et al., 2009) but we hope to show that some of the greatest gains are in the very nature of the phenomena that we can now study. For psycholinguistic experiments in particular, we are not so much utilizing ‘artificial artificial’ intelligence as the plain intelligence and linguistic intuitions of each crowdsourced worker – the ‘voices in the crowd’, so to speak. In many experiments we are studying gradient phenomena where there are no right answers. Even when there is binary response we are often interested in the distribution of responses over many speakers rather than specific data points. This different</context>
</contexts>
<marker>Hsueh, Melville, Sindhwani, 2009</marker>
<rawString>Pei-Yun Hsueh, Prem Melville, and Vikas Sindhwani. 2009. Data quality from crowdsourcing: a study of annotation selection criteria. In Proceedings of the NAACL HLT 2009 Workshop on Active Learning for Natural Language Processing, pages 27–35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Florian Jaeger</author>
</authors>
<title>Redundancy and syntactic reduction in spontaneous speech.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>Stanford University,</institution>
<location>Stanford, CA.</location>
<contexts>
<context position="14531" citStr="Jaeger, 2006" startWordPosition="2357" endWordPosition="2358">inclusion ratings plotted against corresponding corpus-model predictions. The solid line would represent perfect alignment between judgments and corpus model. Non-parametric Lowess smoothers illustrate the significant correlation between lab and crowd population results. reproduced through crowdsourcing. Corpus studies of spontaneous speech suggest that grammaticality is gradient (Wasow, 2008), and models of English complement clause (CC) and relative clause (RC) ‘that’-optionality have as their most significant factor the predictability of embedding, given verb (CC) and head noun (RC) lemma (Jaeger, 2006; Jaeger, in press). Establishing that these highly gradient factors are similarly involved in judgments could provide evidence that such finegrained probabilistic knowledge is part of linguistic competence. We undertook six such judgment experiments: two baseline studies with lab populations then four additional crowdsourced trials via MTurk. Experiment 1, a lab trial (26 participants, 30 items), began with the models of RC-reduction developed in Jaeger (2006). Corpus tokens were binned by relative model-predicted probability of ‘that’-omission. Six tokens were extracted at random from each o</context>
</contexts>
<marker>Jaeger, 2006</marker>
<rawString>Florian Jaeger. 2006. Redundancy and syntactic reduction in spontaneous speech. Ph.D. thesis, Stanford University, Stanford, CA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Florian Jaeger</author>
</authors>
<title>in press. Redundancy and reduction: Speakers manage syntactic information density.</title>
<publisher>Cognitive Psychology.</publisher>
<marker>Jaeger, </marker>
<rawString>Florian Jaeger. in press. Redundancy and reduction: Speakers manage syntactic information density. Cognitive Psychology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vicky Tzuyin Lai</author>
<author>Tim Curran</author>
<author>Lise Menn</author>
</authors>
<date>2009</date>
<booktitle>Comprehending conventional and novel metaphors: An ERP study. Brain Research,</booktitle>
<pages>1284--145</pages>
<contexts>
<context position="22534" citStr="Lai et al., 2009" startWordPosition="3622" endWordPosition="3625">ysiological responses Beyond reproducing laboratory and corpus studies, crowdsourcing also offers the opportunity to newly analyze data drawn from many other experimental stimuli. In this section, we demonstrate that crowdsourced workers can help us better understand ERP brainwave data by looking at how frequently words are used metaphorically. Recent work in event related potentials (ERP) has suggested that even conventional metaphors, such as “All my ideas were attacked” require additional processing effort in the brain as compared to literal sentences like “All the soldiers were attacked” (Lai et al., 2009). This study in particular observed an N400 effect where negative waves 400 milliseconds after the presentation of the target words (e.g. attacked) were larger when the word was used metaphorically than when used literally. The proposed explanation for this effect is that metaphors really do demand more from the brain than literal sentences. However, N400 effects are also observed when subjects encounter something that is semantically inappropriate or unexpected. While the Lai experiment controlled for overall word frequency, it might be possible to explain away these N400 effects if it turned</context>
</contexts>
<marker>Lai, Curran, Menn, 2009</marker>
<rawString>Vicky Tzuyin Lai, Tim Curran, and Lise Menn. 2009. Comprehending conventional and novel metaphors: An ERP study. Brain Research, 1284:145–155, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Landis</author>
<author>Gary Koch</author>
</authors>
<title>The measurement of observer agreement for categorical data.</title>
<date>1977</date>
<journal>Biometrics,</journal>
<volume>33</volume>
<issue>1</issue>
<contexts>
<context position="4505" citStr="Landis and Koch (1977)" startWordPosition="678" endWordPosition="681">bs that spread their meaning out across both a verb and a particle, as in ‘lift up’. Semantic transparency is a measure of how strongly the phrasal verb entails the component verb. For example, to what extent does ‘lifting up’ entail ‘lifting’? We can see the variation between phrasal verbs when we compare the transparency of ‘lift up’ to the opacity of ‘give up’. We conducted five experiments around semantic transparency, with results showing that crowdsourced results correlate well with each other and against lab data (p up to 0.9). Interrater reliability is also very high: n = 0.823, which Landis and Koch (1977) would call ‘almost perfect agreement.’ 4 5 2 4 6 The crowdsourced results reported here represent judgments by 215 people. Two experiments were performed using Stanford University undergraduates. The first involved a questionnaire asking participants to rate the semantic transparency of 96 phrasal verbs. The second experiment consisted of a paper questionnaire with the phrasal verbs in context. That is, the first group of ‘StudentLong’ participants rated the similarity of ‘cool’ to ‘cool down’ on a scale 1-7: cool cool down The ‘StudentContext’ participants performed the same basic task but s</context>
</contexts>
<marker>Landis, Koch, 1977</marker>
<rawString>Richard Landis and Gary Koch. 1977. The measurement of observer agreement for categorical data. Biometrics, 33(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Meyers</author>
<author>Ruth Reeves</author>
<author>Catherine Macleod</author>
<author>Rachel Szekely</author>
<author>Veronika Zielinska</author>
<author>Brian Young</author>
</authors>
<title>Annotating noun argument structure for NomBank.</title>
<date>2004</date>
<booktitle>In Proceedings of LREC-2004.</booktitle>
<contexts>
<context position="18291" citStr="Meyers et al., 2004" startWordPosition="2944" endWordPosition="2947">ce Figure 4: Odds ratio of a Nominal Agent being embedded within a Sentential Agent or non-Agent, relative to random chance. (p &lt; 0.001 for all) that competence grammar includes access to probability distributions. Meanwhile, the strong correlations across populations offer encouraging evidence in support of using the latter in psycholinguistic judgment research. 6 Confirming corpus trends Crowdsourcing can also be used to establish the validity of corpus trends found in otherwise skewed data. The experiments in this section were motivated by the NomBank corpus of nominal predicate/arguments (Meyers et al., 2004) where we found that an Agent semantic role was much more likely to be embedded within a sentential Agent. For example, (1) is more likely than (2) to receive the Agent interpretation for the ‘the police’, but both have same potential range of meanings: (1) “The investigation of the police took 3 weeks to complete” (2) “It took 3 weeks to complete the investigation of the police” While the trend is significant (p &lt; 0.001), the corpus is not representative speech. First, there are no minimal pairs of sentences in NomBank like (1) and (2) that have the same potential range of meanings. Second, t</context>
</contexts>
<marker>Meyers, Reeves, Macleod, Szekely, Zielinska, Young, 2004</marker>
<rawString>Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel Szekely, Veronika Zielinska, Brian Young, , and Ralph Grishman. 2004. Annotating noun argument structure for NomBank. In Proceedings of LREC-2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Prashant Parikh</author>
</authors>
<title>Language and Equilibrium.</title>
<date>2010</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="21618" citStr="Parikh, 2010" startWordPosition="3487" endWordPosition="3488">ults clearly show a significant trend for all, and that the NomBank trend falls between the comprehension and production tasks, which would be expected for this highly edited register. It therefore supports the validity of the corpus results. The phenomena likely exists to aid comprehension, as the cognitive realization of just one role needs to be activated at a given moment. Despite the near-ubiquity of ‘Agent’ in studies of semantic roles, we do not yet have a clear theory of this linguistic entity, or even firm evidence of its existence Figure 5: Distribution of metaphorical frequencies. (Parikh, 2010). This study therefore goes some way towards illuminating this. More broadly, the experiments in this section support the wider use of crowdsourcing as a tool for language cognition research in conjunction with more traditional corpus studies. 7 Post-hoc metaphorical frequency analysis of electrophysiological responses Beyond reproducing laboratory and corpus studies, crowdsourcing also offers the opportunity to newly analyze data drawn from many other experimental stimuli. In this section, we demonstrate that crowdsourced workers can help us better understand ERP brainwave data by looking at </context>
</contexts>
<marker>Parikh, 2010</marker>
<rawString>Prashant Parikh. 2010. Language and Equilibrium. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefanie Regel</author>
<author>Seana Coulson</author>
<author>Thomas C Gunter</author>
</authors>
<title>The communicative style of a speaker can affect language comprehension? ERP evidence from the comprehension of irony.</title>
<date>2010</date>
<journal>Brain Research,</journal>
<volume>1311</volume>
<pages>135</pages>
<contexts>
<context position="27534" citStr="Regel et al., 2010" startWordPosition="4439" endWordPosition="4442">ow attentive the workers really are. When running psycholinguistics experiments in the lab, the experimenters generally have the chance to interact with participants. It is not uncommon for prospective subjects to be visibly exhausted, distracted, or inebriated, or not fluent in the given language to a requisite level of competence. When these participants turn up as outliers in the experimental data, it is easy enough to see why — they fell asleep, couldn’t understand the instructions, etc. 2These results are consistent with recent findings that irony frequency may also produce P200 effects (Regel et al., 2010). 128 With crowdsourcing we lose the chance to have these brief but valuable encounters, and so anomalous response data are harder to interpret. We present two simple experiments for measuring linguistic attentiveness, which can be used as one component of a language study or to broadly evaluate the linguistic competency of the workers. Taking well-known constructions from the literature, we selected constructions that: (a) exist in most (perhaps all) dialects of English; (b) involve high frequency lexical items; and (c) tend to be acquired relatively late by first-language learners. We have f</context>
</contexts>
<marker>Regel, Coulson, Gunter, 2010</marker>
<rawString>Stefanie Regel, Seana Coulson, and Thomas C. Gunter. 2010. The communicative style of a speaker can affect language comprehension? ERP evidence from the comprehension of irony. Brain Research, 1311:121– 135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik D Reichle</author>
<author>Alexander Pollatsek</author>
<author>Donald L Fisher</author>
<author>Keith Rayner</author>
</authors>
<title>Toward a model of eye movement control in reading.</title>
<date>1998</date>
<journal>Psychological Review,</journal>
<pages>105--125</pages>
<contexts>
<context position="13093" citStr="Reichle et al., 1998" startWordPosition="2139" endWordPosition="2143"> money without sacrificing reliability (Schnoebelen and Kuperman, submitted). Our results are taken from 488 Americans, ranging from age 16-80 (mean: 34.49, median: 32, mode: 27) with about 25% each from the East and Midwest, 31% from the South, the rest from the West and Alaska. They represent a range of education levels, though the majority had been to college: about 33.8% had bachelor’s degrees, another 28.1% had some college but without a degree. By contrast, the lab data was gathered from 20 participants, all undergraduates at the University of Massachusetts at Amherst in the mid-1990’s (Reichle et al., 1998). Both populations provided judgments on 488 words in 48 sentences. In general, crowdsourcing gave more diverse responses, as we would expect from a more diverse population. The correlation between lab and crowdsourced data by Spearman’s rank correlation is 0.823 (p &lt; 0.0001), but we can be even more conservative by eliminating the 124 words that had predictability scores of 0 across both groups. By and large, the lab participants and the workers are consistent in which words they fail to predict. Even when we eliminate these shared zeros, the correlation is still high between the two data set</context>
</contexts>
<marker>Reichle, Pollatsek, Fisher, Rayner, 1998</marker>
<rawString>Erik D. Reichle, Alexander Pollatsek, Donald L. Fisher, and Keith Rayner. 1998. Toward a model of eye movement control in reading. Psychological Review, 105:125–157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Roeper</author>
</authors>
<title>The Prism of Grammar: How Child Language Illuminates Humanism.</title>
<date>2007</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="28792" citStr="Roeper (2007)" startWordPosition="4648" endWordPosition="4649">1969) work on first-language acquisition to be particularly useful: (4) John is easy to see. (5) John is eager to see. Example (4) is accurately paraphrased as ‘It is easy to see John’, where John is the object of ‘see’, whereas (5) is accurately paraphrased as ‘John is eager for John to see’, where John is the subject of ‘see’. A similar shift happens with ‘promise’: (6) Bozo told Donald to sing. (7) Bozo promised Donald to sing. We presented workers with a multiple-choice question that contained both subject and object paraphrases as options. In similar experiments, we adapted examples from Roeper (2007), who looked at stacked prenominal possessive constructions: (8) John’s sister’s friend’s car. These are cross-linguistically rare and challenging even for native speakers. As above, the workers were asked to choose between paraphrases. Workers who provide accurate judgments are likely to have a level of English competence and devotion to the task that suffices for many language experiments. The results from one short audio study are given in Table 1. They indicate a high degree of attentiveness; as a group, our subjects performed at the near-perfect levels we expect for fluent adults. We pred</context>
</contexts>
<marker>Roeper, 2007</marker>
<rawString>Tom Roeper. 2007. The Prism of Grammar: How Child Language Illuminates Humanism. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny R Saffran</author>
<author>Richard N Aslin</author>
<author>Elissa L Newport</author>
</authors>
<title>Word segmentation: The role of distributional cues.</title>
<date>1996</date>
<journal>Journal of memory and language,</journal>
<pages>35--606</pages>
<contexts>
<context position="11025" citStr="Saffran et al. (1996)" startWordPosition="1798" endWordPosition="1801">asked to enter an English word presented auditorily. Workers then completed ten test trials in which they heard one word from the language and one nonword made by concatenating all but the first syllable of one word with the first syllable of another. If the words “bapu” and “gudi” had been presented adjacently, the string “pugu” would have been heard, despite not being a word of the language. Both were also displayed orthographically, and the worker was instructed to click on the one which had appeared in the previously heard language. The language materials described above were taken from a Saffran et al. (1996) replication reported as Experiment 2 in Frank, Goldwater, Griffiths &amp; Tenenbaum (under review). We compared the results from lab participants reported in that article to data from MTurk workers using the applet described above. Each response was marked “correct” if the participant chose the word rather than the nonword. 12 lab subjects achieved 71% correct responses, while 24 MTurk workers were only slightly lower at 66%. The MTurk results proved significantly different from a “random clicking” baseline of 50% (t(23) = 5.92, p = 4.95 x 10−06) but not significantly different from the lab subje</context>
</contexts>
<marker>Saffran, Aslin, Newport, 1996</marker>
<rawString>Jenny R. Saffran, Richard N. Aslin, and Elissa L. Newport. 1996. Word segmentation: The role of distributional cues. Journal of memory and language, 35:606– 621.</rawString>
</citation>
<citation valid="false">
<authors>
<author>submitted</author>
</authors>
<title>Using Amazon Mechanical Turk for linguistic research: Fast, cheap, easy, and reliable.</title>
<marker>submitted, </marker>
<rawString>Tyler Schnoebelen and Victor Kuperman. submitted. Using Amazon Mechanical Turk for linguistic research: Fast, cheap, easy, and reliable.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Brendan O’Connor</author>
<author>Daniel Jurafsky</author>
<author>Andrew T Ng</author>
</authors>
<title>Cheap and fast—but is it good?: evaluating non-expert annotations for natural language tasks.</title>
<date>2008</date>
<booktitle>In EMNLP ’08: Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>254--263</pages>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>Rion Snow, Brendan O’Connor, Daniel Jurafsky, and Andrew T. Ng. 2008. Cheap and fast—but is it good?: evaluating non-expert annotations for natural language tasks. In EMNLP ’08: Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 254–263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wilson Taylor</author>
</authors>
<title>Cloze procedure: A new tool for measuring readability.</title>
<date>1953</date>
<journal>Journalism Quarterly,</journal>
<volume>30</volume>
<pages>433</pages>
<contexts>
<context position="12034" citStr="Taylor (1953)" startWordPosition="1970" endWordPosition="1971">re only slightly lower at 66%. The MTurk results proved significantly different from a “random clicking” baseline of 50% (t(23) = 5.92, p = 4.95 x 10−06) but not significantly different from the lab subjects (Welch two-sample t-test for unequal sample sizes, t(21.21) = −.92, p = .37). Per-subject means for the lab and MTurk data are plotted in Figure 2. 124 4 Contextual predictability As psycholinguists build models of sentence processing (e.g., from eye tracking studies), they need to understand the effect of the available sentence context. One way to gauge this is the Cloze task proposed in Taylor (1953): participants are presented with a sentence fragment and asked to provide the upcoming word. Researchers do this for every word in every stimulus and use the percentage of ‘correct’ guesses as input into their statistical and computational models. Rather than running such norming studies on undergraduates in lab settings (as is typical), our results suggest that psycholinguists will be able to crowdsource these tasks, saving time and money without sacrificing reliability (Schnoebelen and Kuperman, submitted). Our results are taken from 488 Americans, ranging from age 16-80 (mean: 34.49, media</context>
</contexts>
<marker>Taylor, 1953</marker>
<rawString>Wilson Taylor. 1953. Cloze procedure: A new tool for measuring readability. Journalism Quarterly, 30:415– 433.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Wasow</author>
</authors>
<title>Gradient data and gradient grammars.</title>
<date>2008</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Chicago Linguistics Society,</booktitle>
<pages>255--271</pages>
<contexts>
<context position="14315" citStr="Wasow, 2008" startWordPosition="2323" endWordPosition="2324">d K = 0.759 (p &lt; 0.0001). 5 Judgment studies of fine-grained probabilistic grammatical knowledge Moving to syntax, we demonstrate here that grammaticality judgments from lab studies can also be Figure 3: Mean ‘that’-inclusion ratings plotted against corresponding corpus-model predictions. The solid line would represent perfect alignment between judgments and corpus model. Non-parametric Lowess smoothers illustrate the significant correlation between lab and crowd population results. reproduced through crowdsourcing. Corpus studies of spontaneous speech suggest that grammaticality is gradient (Wasow, 2008), and models of English complement clause (CC) and relative clause (RC) ‘that’-optionality have as their most significant factor the predictability of embedding, given verb (CC) and head noun (RC) lemma (Jaeger, 2006; Jaeger, in press). Establishing that these highly gradient factors are similarly involved in judgments could provide evidence that such finegrained probabilistic knowledge is part of linguistic competence. We undertook six such judgment experiments: two baseline studies with lab populations then four additional crowdsourced trials via MTurk. Experiment 1, a lab trial (26 particip</context>
</contexts>
<marker>Wasow, 2008</marker>
<rawString>Tom Wasow. 2008. Gradient data and gradient grammars. In Proceedings of the 43rd Annual Meeting of the Chicago Linguistics Society, pages 255–271.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>