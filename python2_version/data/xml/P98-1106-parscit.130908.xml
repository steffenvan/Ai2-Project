<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000018">
<title confidence="0.9978855">
Pseudo-Projectivity: A Polynomially Parsable Non-Projective
Dependency Grammar
</title>
<author confidence="0.751002">
Sylvain Kahane* and Alexis Nasrt and Owen Rambowt
</author>
<affiliation confidence="0.309832">
* TALANA Universite Paris 7 (sk@ccr.jussieu.fr)
</affiliation>
<address confidence="0.195704">
t LIA Universite d&apos;Avignon (alexis.nasr@lia.univ-avignon.fr)
</address>
<email confidence="0.326953">
$CoGenTex, Inc. (owen@cogentex corn)
</email>
<sectionHeader confidence="0.99158" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999976233333333">
Dependency grammar has a long tradition
in syntactic theory, dating back to at least
Tesniere&apos;s work from the thirties.&apos; Recently, it
has gained renewed attention as empirical meth-
ods in parsing are discovering the importance
of relations between words (see, e.g., (Collins,
1997)), which is what dependency grammars
model explicitly do, but context-free phrase-
structure grammars do not. One problem that
has posed an impediment to more wide-spread
acceptance of dependency grammars is the fact
that there is no computationally tractable ver-
sion of dependency grammar which is not re-
stricted to projective analyses. However, it is
well known that there are some syntactic phe-
nomena (such as wh-movement in English or
clitic climbing in Romance) that require non-
projective analyses. In this paper, we present
a form of projectivity which we call pseudo-
projectivity, and we present a generative string-
rewriting formalism that can generate pseudo-
projective analyses and which is polynomially
parsable.
The paper is structured as follows. In Sec-
tion 2, we introduce our notion of pseudo-
projectivity. We briefly review a previously pro-
posed formalization of projective dependency
grammars in Section 3. In Section 4, we extend
this formalism to handle pseudo-projectivity.
We informally present a parser in Section 5.
</bodyText>
<sectionHeader confidence="0.776745" genericHeader="keywords">
2 Linear and Syntactic Order of a
Sentence
</sectionHeader>
<subsectionHeader confidence="0.97904">
2.1 Some Notation and Terminology
</subsectionHeader>
<bodyText confidence="0.996851">
We will use the following terminology and no-
tation in this paper. The hierarchical order
</bodyText>
<footnote confidence="0.5869695">
1The work presented in this paper is collective and
the order of authors is alphabetical.
</footnote>
<bodyText confidence="0.999684363636364">
(dominance) between the nodes of a tree T will
be represented with the symbol -&lt;T and
Whenever they are unambiguous, the notations
-&lt; and -‹ will be used. When x y, we will say
that x is a descendent of y and y an ancestor
of x. The projection of a node x, belonging to
a tree T, is the set of the nodes y of T such that
y x. An arc between two nodes y and x of a
tree T, directed from y to x will be noted either
(y, x) or The node x will be referred to as
the dependent and y as the governor. The
latter will be noted, when convenient, X+T (X+
when unambiguous). The notations t- and x+
are unambiguous because a node x has at most
one governor in a tree. As usual, an ordered
tree is a tree enriched with a linear order over
the set of its nodes. Finally, if 1 is an arc of
an ordered tree T, then Supp(1) represents the
support of 1, i.e. the set of the nodes of T
situated between the extremities of 1, extremi-
ties included. We will say that the elements of
Supp(1) are covered by 1.
</bodyText>
<subsectionHeader confidence="0.983381">
2.2 Projectivity
</subsectionHeader>
<bodyText confidence="0.9991643125">
The notion of projectivity was introduced by
(Lecerf, 1960) and has received several different
definitions since then. The definition given here
is borrowed from (Marcus, 1965) and (Robin-
son, 1970):
Definition: An arc t- is projective if and
only if for every y covered by y x+ . A
tree T is projective if and only if every arc of
T is projective
A projective tree has been represented in Fig-
ure 1.
A projective dependency tree can be associ-
ated with a phrase structure tree whose con-
stituents are the projections of the nodes of
the dependency tree. Projectivity is therefore
equivalent, in phrase structure markers, to con-
</bodyText>
<page confidence="0.997224">
646
</page>
<figure confidence="0.436138">
The big cat sometimes eats white mice
</figure>
<figureCaption confidence="0.999834">
Figure 1: A projective sub-categorization tree
</figureCaption>
<bodyText confidence="0.999151875">
tinuity of constituent.
The strong constraints introduced by the pro-
jectivity property on the relationship between
hierarchical order and linear order allow us to
describe word order of a projective dependency
tree at a local level: in order to describe the
linear position of a node, it is sufficient to de-
scribe its position towards its governor and sis-
ter nodes. The domain of locality of the linear
order rules is therefore limited to a subtree of
depth equal to one. It can be noted that this do-
main of locality is equal to the domain of local-
ity of sub-categorization rules. Both rules can
therefore be represented together as in (Gaff-
man, 1965) or separately as will be proposed
in 3.
</bodyText>
<subsectionHeader confidence="0.999774">
2.3 Pseudo-Projectivity
</subsectionHeader>
<bodyText confidence="0.803665285714286">
Although most linguistic structures can be
represented as projective trees, it is well known
that projectivity is too strong a constraint for
dependency trees, as shown by the example of
Figure 2, which includes a non-projective arc
(marked with a star).
Who do you think she invited ?
</bodyText>
<figureCaption confidence="0.951167">
Figure 2: A non projective sub-categorization
tree
</figureCaption>
<bodyText confidence="0.999552395348837">
The non projective structures found in
linguistics represent a small subset of the
potential non projective structures. We will
define a property (more exactly a family of
properties), weaker than projectivity, called
pseudo-projectivity, which describes a
subset of the set of ordered dependency trees,
containing the non-projective linguistic struc-
tures.
In order to define pseudo-projectivity, we in-
troduce an operation on dependency trees called
lifting. When applied to a tree, this operation
leads to the creation of a second tree, a lift of
the first one. An ordered tree T&apos; is a lift of
the ordered tree T if and only if T and T&apos; have
the same nodes in the same order and for ev-
ery node X, X+T &lt;TX±T . We will say that the
node x has been lifted from X+T (its syntactic
governor) to X+Ti (its linear governor).
Recall that the linear position of a node in
a projective tree can be defined relative to its
governor and its sisters. In order to define the
linear order in a non projective tree, we will
use a projective lift of the tree. In this case,
the position of a node can be defined only with
regards to its governor and sisters in the lift,
i.e., its linear governor and sisters.
Definition: An ordered tree T is said
pseudo-projective if there exists a lift T&apos; of
tree T which is projective.
If there is no restriction on the lifting, the
previous definition is not very interesting since
we can in fact take any non-projective tree and
lift all nodes to the root node and obtain a pro-
jective tree.
We will therefore constrain the lifting by a
set of rules, called lifting rules. Consider a set
of (syntactic) categories. The following defini-
tions make sense only for trees whose nodes are
labeled with categories.&apos;
The lifting rules are of the following form
(LD, SG and LG are categories and w is a reg-
ular expression on the set of categories):
</bodyText>
<equation confidence="0.925014">
LD t SG w LG (1)
</equation>
<bodyText confidence="0.997441941176471">
This rule says that a node of category LD
can be lifted from its syntactic governor of cat-
egory SG to its linear governor of category LG
through a path consisting of nodes of category
C1, , Cn, where the string belongs
to L(w). Every set of lifting rules defines a par-
ticular property of pseudo-projectivity by im-
posing particular constraints on the lifting. A
21t is possible to define pseudo-projectivity purely
structurally (i.e. without referring to the labeling). For
example, we can impose that each node x is lifted to
the highest ancestor of x covered by t- ((Nasr, 1996)).
The resulting pseudo-projectivity is a fairly weak exten-
sion to projectivity, which nevertheless covers major non-
projective linguistic structures. However, we do not pur-
sue a purely structural definition of pseudo-projectivity
in this paper.
</bodyText>
<page confidence="0.993255">
647
</page>
<bodyText confidence="0.997915">
linguistic example of lifting rule is given in Sec-
tion 4.
The idea of building a projective tree by
means of lifting appears in (Kunze, 1968) and
is used by (Hudson, 1990) and (Hudson, un-
published). This idea can also be compared to
the notion of word order domain (Reape, 1990;
Broker and Neuhaus, 1997), to the Slash feature
of GPSG and HPSG, to the functional uncer-
tainty of LFG, and to the Move-a of GB theory.
</bodyText>
<sectionHeader confidence="0.9823685" genericHeader="method">
3 Projective Dependency Grammars
Revisited
</sectionHeader>
<bodyText confidence="0.9998495">
We (informally) define a projective Dependency
Grammar as a string-rewriting system3 by giv-
ing a set of categories such as N, V and Adv,4
a set of distinguished start categories (the root
categories of well-formed trees), a mapping from
strings to categories, and two types of rules: de-
pendency rules which state hierarchical order
(dominance) and LP rules which state linear
order. The dependency rules are further sub-
divided into subcategorization rules (or s-rules)
and modification rules (or m-rules). Here are
some sample s-rules:
</bodyText>
<equation confidence="0.86925775">
d1 : Vtrans —4 Nnom, Nob, (2)
d2 Vclause —4 Nnom, V (3)
Here is a sample m-rule.
d3 : V Adv (4)
</equation>
<bodyText confidence="0.9997294">
LP rules are represented as regular expressions
(actually, only a limited form of regular expres-
sions) associated with each category. We use
the hash sign (#) to denote the position of the
governor (head). For example:
</bodyText>
<subsectionHeader confidence="0.665767">
Pi :Vtrans = (Adv)Nnorn(Aux)Adv•#NobjAdv* Vtrans (5)
</subsectionHeader>
<bodyText confidence="0.980075375">
3We follow (Gaifman, 1965) throughout this paper by
modeling a dependency grammar with a string-rewriting
system. However, we will identify a derivation with its
representation as a tree, and we will sometimes refer
to symbols introduced in a rewrite step as &amp;quot;dependent
nodes&amp;quot;. For a model of a DG based on tree-rewriting
(in the spirit of Tree Adjoining Grammar (Joshi et al.,
1975)), see (Nasr, 1995).
</bodyText>
<footnote confidence="0.855349">
41n this paper, we will allow finite feature structures
on categories, which we will notate using subscripts; e.g.,
Vtrans • Since the feature structures are finite, this is sim-
ply a notational variant of a system defined only with
simple category labels.
</footnote>
<figure confidence="0.991122166666667">
Vclause
Adv Nnom thought Vtrans
• yesterday Fernando thought Vtrans
yesterday Fernando thought Nnom eats Nb Adv
yesterday Fernando thought Carlos eats beans slowly
V
</figure>
<figureCaption confidence="0.999945">
Figure 3: A sample GDG derivation
</figureCaption>
<bodyText confidence="0.999402538461539">
We will call this system generative depen-
dency grammar or GDG for short.
Derivations in GDG are defined as follows.
In a rewrite step, we choose a multiset of de-
pendency rules (i.e., a set of instances of de-
pendency rules) which contains exactly one s-
rule and zero or more m-rules. The left-hand
side nonterminal is the same as that we want to
rewrite. Call this multiset the rewrite-multiset.
In the rewriting operation, we introduce a mul-
tiset of new nonterminals and exactly one termi-
nal symbol (the head). The rewriting operation
then must meet the following three conditions:
</bodyText>
<listItem confidence="0.980226444444445">
• There is a bijection between the set of de-
pendents of the instances of rules in the
rewrite-multiset and the set of newly intro-
duced dependents.
• The order of the newly introduced depen-
dents is consistent with the LP rule associ-
ated with the governor.
• The introduced terminal string (head) is
mapped to the rewritten category.
</listItem>
<bodyText confidence="0.999668090909091">
As an example, consider a grammar contain-
ing the three dependency rules di (rule 2), d2
(rule 3), and d3 (rule 4), as well as the LP rule pi
(rule 5). In addition, we have some lexical map-
pings (they are obvious from the example), and
the start symbol is Vfinite,±. A sample deriva-
tion is shown in Figure 3, with the sentential
form representation on top and the correspond-
ing tree representation below.
Using this kind of representation, we can
derive a bottom-up parser in the following
</bodyText>
<figure confidence="0.9674555">
Adv Nnom thought
yesterday Fernando Nnom
Carlos
trans
eats Nobj Adv
beans slowly
</figure>
<page confidence="0.967039">
648
</page>
<bodyText confidence="0.997843972972973">
straightforward manner.5 Since syntactic and
linear governors coincide, we can derive de-
terministic finite-state machines which capture
both the dependency and the LP rules for a
given governor category. We will refer to these
FSMs as rule-FSMs, and if the governor is of
category C, we will refer to a C-rule-FSM. In
a rule-FSM, the transitions are labeled by cate-
gories, and the transition corresponding to the
governor labeled by its category and a special
mark (such as #). This transition is called the
&amp;quot;head transition&amp;quot;.
The entries in the parse matrix M are of the
form (in, q), where in is a rule-FSM and q a state
of it, except for the entries in squares M(i, i),
1 &lt; j&lt; n, which also contain category labels.
Let WO • • wn be the input word. We initialize
the parse matrix as follows. Let C be a category
of word wi. First, we add C to M(i, i). Then,
we add to M(i, i) every pair (7n, q) such that m
is a rule-FSM with a transition labeled C from
a start state and q the state reached after that
transition.6
Embedded in the usual three loops on i, j, k,
we add an entry (mi, q) to M(i, j) if (m1, qi) is
in M (k, j), (m2, q2) is in M (i, k+1), q2 is a final
state of m2, m2 is a C-rule-FSM, and mi transi-
tions from qi to q on C (a non-head transition).
There is a special case for the head transitions
in mi: if k = i — 1, C is in M(i, i), mi is a C-
rule-FSM, and there is a head transition from
qi to q in ml, then we add (mi, q) to M(i, j).
The time complexity of the algorithm is
0(n3IGIQmax), where G is the number of rule-
FSMs derived from the dependency and LP
rules in the grammar and Qmax is the maximum
number of states in any of the rule-FSMs.
</bodyText>
<sectionHeader confidence="0.998318" genericHeader="method">
4 A Formalization of
</sectionHeader>
<subsectionHeader confidence="0.783181">
PP-Dependency Grammars
</subsectionHeader>
<bodyText confidence="0.998610742857143">
Recall that in a pseudo-projective tree, we make
a distinction between a syntactic governor and
a linear governor. A node can be &amp;quot;lifted&amp;quot; along
a lifting path from being a dependent of its syn-
tactic governor to being a dependent of its linear
&apos;This type of parser has been proposed previously.
See for example (Lombardi, 1996; Eisner, 1996), who
also discuss Early-style parsers for projective depen-
dency grammars.
&apos;We can use pre-computed top-down prediction to
limit the number of pairs added.
governor, which must be an ancestor of the gov-
ernor. In defining a formal rewriting system for
pseudo-projective trees, we will not attempt to
model the &amp;quot;lifting&amp;quot; as a transformational step in
the derivation. Rather, we will directly derive
the &amp;quot;lifted&amp;quot; version of the tree, where a node
is dependent of its linear governor. Thus, the
derived structure resembles more a unistratal
dependency representation like those used by
(Hudson, 1990) than the multistratal represen-
tations of, for example, (Mel&apos;euk, 1988). How-
ever, from a formal point of view, the distinction
is not significant.
In order to capture pseudo-projectivity, we
will interpret rules of the form (2) (for subcate-
gorization of arguments by a head) and (4) (for
selection of a head by an adjunct) as introducing
syntactic dependents which may lift to a higher
linear governor. An LP rule of the form (5) or-
ders all linear dependents of the linear governor,
no matter whose syntactic dependents they are.
In addition, we need a third type of rule,
namely a lifting rule, or 1-rule (see 2.3). The
1-rule (1) can be rewrited on the following form:
</bodyText>
<equation confidence="0.549269">
: LG LD {LG • w Sc LD} (6)
</equation>
<bodyText confidence="0.99997912">
This rule resembles normal dependency rules
but instead of introducing syntactic dependents
of a category, it introduces a lifted dependent.
Besides introducing a linear dependent LD, a
1-rule should make sure that the syntactic gov-
ernor of LD will be introduced at a later stage of
the derivation, and prevent it to introduce LD
as its syntactic dependent, otherwise non pro-
jective nodes would be introduced twice, a first
time by their linear governor and a second time
by their syntactic governor. This condition is
represented in the rule by means of a constraint
on the categories found along the lifting path.
This condition, which we call the lifting con-
dition, is represented by the regular expression
LG • w SG. The regular expression representing
the lifting condition is enriched with a dot sep-
arating, on its left, the part of the lifting path
which has already been introduced during the
rewriting and on its right the part which is still
to be introduced for the rewriting to be valid.
The dot is an unperfect way of representing the
current state in a finite state automaton equiv-
alent to the regular expression. We can further
notice that the lifting condition ends with a rep-
</bodyText>
<page confidence="0.998534">
649
</page>
<bodyText confidence="0.899972954545455">
etition of LD for reasons which will be made
clear when discussing the rewriting process.
A sentential form contains terminal strings
and categories paired with a multiset of lifting
conditions, called the lift multiset. The lift mul-
tiset associated to a category C contains &apos;tran-
siting&apos; lifting conditions: introduced by ances-
tors of C and passing across C.
Three cases must be distinguished when
rewriting a category C and its lifting multiset
LM:
• LM contains a single lifting condi-
tion which dot is situated to its right:
LG w SG C.. In such a case, C must be
rewritten by the empty string. The situ-
ation of the dot at the right of the lifting
condition indicates that C has been intro-
duced by its syntactic governor although it
has already been introduced by its linear
governor earlier in the rewriting process.
This is the reason why C has been added
at the end of the lifting condition.
</bodyText>
<listItem confidence="0.980336333333333">
• LM contains several lifting conditions one
of which has its dot to the right. In such
a case, the rewriting fails since, in accor-
dance with the preceding case, C must be
rewritten by the empty string. Therefore,
the other lifting conditions of LM will not
be satisfied. Furthermore, a single instance
of a category cannot anchor more than one
lifting condition.
• LM contains several lifting conditions none
of which having the dot to their right. In
this case, a rewrite multiset of dependency
rules and lifting rules, both having C as
their left hand side, is selected. The result
of the rewriting then must meet the follow-
ing conditions:
1. The order of the newly introduced de-
pendents is consistent with the LP rule
associated with C.
2. The union7 of the lift multisets asso-
ciated with all the newly introduced
(instances of) categories is equal to the
union of the lift multiset of C and the
multiset composed of the lift condition
</listItem>
<footnote confidence="0.756648">
7When discussing set operations on multisets, we of
course mean the corresponding multiset operations.
</footnote>
<bodyText confidence="0.979185642857143">
of the 1-rules used in the rewriting op-
eration.
3. The lifting conditions contained in the
lift multiset of all the newly introduced
dependents D should be compatible
with D, with the dot advanced appro-
priately.
In addition, we require that, when we rewrite
a category as a terminal, the lift multiset is
empty.
Let us consider an example. Suppose we have
have a grammar containing the dependency
rules di (rule 2), d2 (rule 3), and d3 (rule 4);
the LP rule pi (rule 5) and p2:
</bodyText>
<equation confidence="0.70986375">
P2:Volause = (Ntop:-1- INwh:+)(Adv)Nnom(Aux)Ade # Adv. v,•,rans
Furthermore, we have the following 1-rule:
,
:Vbridge:÷—&gt;Ncasembj top:+ {1l1;ridge:+VNoase:obj top-1- }
</equation>
<bodyText confidence="0.998398">
This rule says that an objective wh-noun with
feature top:+ which depends on a verb with no
further restrictions (the third V in the lifting
path) can raise to any verb that dominates its
immediate governor as long as the raising paths
contains only verb with feature bridge:+, i.e.,
bridge verbs.
</bodyText>
<figure confidence="0.940810666666667">
Vclause
Nobi Nnom thought Adv V{*Vb7ridge:+ V Ncase:obj top:+}
- beans Fernando thought yesterday
V {.V.I.:fidge:+ V Noase:obj top:+}
- beans Fernando thought yesterday Nnom claims
V {.V.ridge„,_ V Neasmobj top:+}
beans Fernando thought yesterday Milagro claims
V{-VI:ridge* V Ncase:obj top:+}
beans yesterday Fernando thought yesterday Milagro
claims Nnom eats N{V,.idge,+V Ncase:obj top:+•} Adv
beans Fernando thought yesterday Milagro claims Carlos
eats slowly
Now Nnom thought Adv Vclause
beans Fernando yesterday
Nnom claims Vtrems
Milagro
N„„&amp;quot;.„ eats Adv
Carlos slowly
</figure>
<figureCaption confidence="0.999981">
Figure 4: A sample PP-GDG derivation
</figureCaption>
<bodyText confidence="0.99965">
A sample derivation is shown in Figure 4,
with the sentential form representation on top
</bodyText>
<page confidence="0.995068">
650
</page>
<bodyText confidence="0.999964133333333">
and the corresponding tree representation be-
low. We start our derivation with the start
symbol Klause and rewrite it using dependency
rules d2 and d3, and the lifting rule /1 which
introduces an objective NP argument. The lift-
ing condition of /I is passed to the V dependent
but the dot remains at the left of Vbridge:+ be-
cause of the Kleene star. When we rewrite the
embedded V, we choose to rewrite again with
Klause, and the lifting condition is passed on to
the next verb. This verb is a V+. „rans which re-
quires a Nobj. The lifting condition is passed to
Nobj and the dot is moved to the right of the
regular expression, therefore Nobj is rewritten
as the empty string.
</bodyText>
<sectionHeader confidence="0.977152" genericHeader="method">
5 A Polynomial Parser for PP-GDG
</sectionHeader>
<bodyText confidence="0.982274626506024">
In this section, we show that pseudo-projective
dependency grammars as defined in Section 2.3
are polynomially parsable.
We can extend the bottom-up parser for GDG
to a parser for PP-GDG in the following man-
ner. In PP-GDG, syntactic and linear governors
do not necessarily coincide, and we must keep
track separately of linear precedence and of lift-
ing (i.e., &amp;quot;long distance&amp;quot; syntactic dependence).
The entries in the parse matrix M are of
the form (m, q, LM), where m is a rule-FSM,
q a state of m, and LM is a multiset of lift-
ing conditions as defined in Section 4. An entry
(m, q, LM) in a square M(i, j) of the parse ma-
trix means that the sub-word wi • • • wj of the
entry can be analyzed by in up to state q (i.e.,
it matches the beginning of an LP rule), but
that nodes corresponding to the lifting rules in
LM are being lifted from the subtrees span-
ning wz • • • wj. Put differently, in this bottom-
up view LM represents the set of nodes which
have a syntactic governor in the subtree span-
ning ID, • • w3 and a lifting rule, but are still
looking for a linear governor.
Suppose we have an entry in the parse matrix
M of the form (m, q, L). As we traverse the C-
rule-FSM m, we recognize one by one the linear
dependents of a node of category C. Call this
governor n. The action of adding a new entry to
the parse matrix corresponds to adding a single
new linear dependent to n. (While we are work-
ing on the C-rule-FSM 771 and are not yet in a
final state, we have not yet recognized n itself.)
Each new dependent 71&apos; brings with it a multiset
of nodes being lifted from the, subtree it is the
root of. Call this multiset LM. The new entry
will be (m, q&apos;, LMULM ) (where q is the state
that m transitions to when n&apos; is recognized as
the next linear dependent.
When we have reached a final state q of the
rule-FSM m, we have recognized a complete
subtree rooted in the new governor, 7/. Some
of the dependent nodes of ?I will be both syn-
tactic and linear dependents of n, and the others
will be linear dependents of 7/, but lifted from a
descendent of i. In addition, i may have syn-
tactic dependents which are not realized as its
own linear dependent and are lifted away. (No
other options are possible.) Therefore, when we
have reached the final state of a rule-FSM, we
must connect up all nodes and lifting conditions
before we can proceed to put an entry (in, q, L)
in the parse matrix. This involves these steps:
1. For every lifting condition in LM, we en-
sure that it is compatible with the category
of 77. This is done by moving the dot left-
wards in accordance with the category of
(The dot is moved leftwards since we
are doing bottom-up recognition.)
The obvious special provisions deal with
the Kleene star and optional elements.
If the category matches a catgeory with
Kleene start in the lifting condition, we do
not move the dot. If the category matches
a category which is to the left of an op-
tional category, or to the left of category
with Kleene star, then we can move the dot
to the left of that category.
If the dot cannot be placed in accordance
with the category of ij, then no new entry
is made in the parse matrix for n.
2. We then choose a multiset of s-, m-, and 1-
rules whose left-hand side is the category of
n. For every dependent of n introduced by
an 1-rule, the dependent must be compati-
ble with an instance of a lifting condition in
LM (whose dot must be at its beginning, or
seperated from the beginning by optional
or categories only); the lifting condition is
then removed from L.
3. If, after the above repositioning of the dot
and the linking up of all linear dependents
to lifting conditions, there are still lifting
</bodyText>
<page confidence="0.996729">
651
</page>
<bodyText confidence="0.999618578947368">
conditions in LM such that the dot is at
the beginning of the lifting condition, then
no new entry is made in the parse matrix
for n.
4. For every syntactic dependent of rh we de-
termine if it is a linear dependent of n which
has not yet been identified as lifted. For
each syntactic dependents which is not also
a linear dependent, we check whether there
is an applicable lifting rule. If not, no entry
is made in the parse matrix for 77. If yes,
we add the lifting rule to LM.
This procedure determines a new multiset
LM so we can add entry (in, q, LM) in the parse
matrix. (In fact, it may determine several pos-
sible new multisets, resulting in multiple new
entries.) The parse is complete if there is an
entry (m, qm, 0) in square M(n, 1) of the parse
matrix, where in is a C-rule-FSM for a start
category and qm is a final state of M. If we keep
backpointers at each step in the algorithm, we
have a compact representation of the parse for-
est.
The maximum number of entries in each
square of the parse matrix is 0(GQ4), where
G is the number of rule-FSMs corresponding to
LP rules in the grammar, Q is the maximum
number of states in any of the rule-FSMs, and
L is the maximum number of states that the
lifting rules can be in (i.e., the number of lift-
ing conditions in the grammar multiplied by the
maximum number of dot positions of any lifting
condition). Note that the exponent is a gram-
mar constant, but this number can be rather
small since the lifting rules are not lexicalized
- they are construction-specific, not lexeme-
specific. The time complexity of the algorithm
is therefore 0(GQn3+2ILI).
</bodyText>
<sectionHeader confidence="0.999138" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997104666666666">
Norbert Broker and Peter Neuhaus. 1997. The
complexity of recognition of linguistically ad-
equate dependency grammars. In 35th Meet-
ing of the Association for Computational Lin-
guistics (ACL &apos;97), Madrid, Spain. ACL.
M. Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings
of the 35th Annual Meeting of the Associa-
tion for Computational Linguistics, Madrid,
Spain, July.
Jason M. Eisner. 1996. Three new probabilis-
tic models for dependency parsing: An ex-
ploration. In Proceedings of the 16th Inter-
national Conference on Computational Lin-
guistics (COLING &apos;96), Copenhagen.
Haim Gaifman. 1965. Dependency systems and
phrase-structure systems. Information and
Control, 8:304-337.
Richard Hudson. 1990. English Word Gram-
mar. Basil Blackwell, Oxford, RU.
Richard Hudson. unpublished. Discontinuity.
e-preprint (ftp.phon.ucl.ac.uk).
Aravind K. Joshi, Leon Levy, and M Takahashi.
1975. Tree adjunct grammars. J. Comput.
Syst. Sci., 10:136-163.
Jurgen Kunze. 1968. The treatment of non-
projective structures in the syntactic analysis
and synthesis of english and german. Com-
putational Linguistics, 7:67-77.
Yves Lecerf. 1960. Programme des conflits,
modele des conflits. Bulletin bimestriel de
PATALA, 4,5.
Vicenzo Lombardi. 1996. An Earley-style
parser for dependency grammars. In Pro-
ceedings of the 16th International Conference
on Computational Linguistics (COLING &apos;96),
Copenhagen.
Solomon Marcus. 1965. Sur la notion de projec-
tivite. Zeitschr. f. math. Logik und Grundla-
gen d. Math., 11:181-192.
Igor A. Merauk. 1988. Dependency Syntax:
Theory and Practice. State University of New
York Press, New York.
Alexis Nasr. 1995. A formalism and a parser for
lexicalised dependency grammars. In 4th In-
ternational Workshop on Parsing Technolo-
gies, pages 186-195, Prague.
Alexis Nasr. 1996. Un systeme de reformu-
lation automatique de phrases fonde sur la
Theorie Sens-Texte : application aux langues
controlees. Ph.D. thesis, Universite Paris 7.
Michael Reape. 1990. Getting things in order.
In Proceedings of the Symposium on Discon-
tinuous Constituents, Tilburg, Holland.
Jane J. Robinson. 1970. Dependency struc-
tures and transformational rules. Language,
46(2):259-285.
</reference>
<page confidence="0.998229">
652
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.149758">
<title confidence="0.734402">Pseudo-Projectivity: A Polynomially Parsable Non-Projective Dependency Grammar Kahane* Nasrt Rambowt</title>
<note confidence="0.510426666666667">TALANA Universite Paris (sk@ccr.jussieu.fr) Universite d&apos;Avignon (alexis.nasr@lia.univ-avignon.fr) Inc. corn</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Norbert Broker</author>
<author>Peter Neuhaus</author>
</authors>
<title>The complexity of recognition of linguistically adequate dependency grammars.</title>
<date>1997</date>
<booktitle>In 35th Meeting of the Association for Computational Linguistics (ACL &apos;97),</booktitle>
<publisher>ACL.</publisher>
<location>Madrid,</location>
<contexts>
<context position="7642" citStr="Broker and Neuhaus, 1997" startWordPosition="1314" endWordPosition="1317">ode x is lifted to the highest ancestor of x covered by t- ((Nasr, 1996)). The resulting pseudo-projectivity is a fairly weak extension to projectivity, which nevertheless covers major nonprojective linguistic structures. However, we do not pursue a purely structural definition of pseudo-projectivity in this paper. 647 linguistic example of lifting rule is given in Section 4. The idea of building a projective tree by means of lifting appears in (Kunze, 1968) and is used by (Hudson, 1990) and (Hudson, unpublished). This idea can also be compared to the notion of word order domain (Reape, 1990; Broker and Neuhaus, 1997), to the Slash feature of GPSG and HPSG, to the functional uncertainty of LFG, and to the Move-a of GB theory. 3 Projective Dependency Grammars Revisited We (informally) define a projective Dependency Grammar as a string-rewriting system3 by giving a set of categories such as N, V and Adv,4 a set of distinguished start categories (the root categories of well-formed trees), a mapping from strings to categories, and two types of rules: dependency rules which state hierarchical order (dominance) and LP rules which state linear order. The dependency rules are further subdivided into subcategorizat</context>
</contexts>
<marker>Broker, Neuhaus, 1997</marker>
<rawString>Norbert Broker and Peter Neuhaus. 1997. The complexity of recognition of linguistically adequate dependency grammars. In 35th Meeting of the Association for Computational Linguistics (ACL &apos;97), Madrid, Spain. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Madrid, Spain,</location>
<marker>Collins, 1997</marker>
<rawString>M. Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics, Madrid, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason M Eisner</author>
</authors>
<title>Three new probabilistic models for dependency parsing: An exploration.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th International Conference on Computational Linguistics (COLING &apos;96),</booktitle>
<location>Copenhagen.</location>
<contexts>
<context position="13094" citStr="Eisner, 1996" startWordPosition="2291" endWordPosition="2292">mi, q) to M(i, j). The time complexity of the algorithm is 0(n3IGIQmax), where G is the number of ruleFSMs derived from the dependency and LP rules in the grammar and Qmax is the maximum number of states in any of the rule-FSMs. 4 A Formalization of PP-Dependency Grammars Recall that in a pseudo-projective tree, we make a distinction between a syntactic governor and a linear governor. A node can be &amp;quot;lifted&amp;quot; along a lifting path from being a dependent of its syntactic governor to being a dependent of its linear &apos;This type of parser has been proposed previously. See for example (Lombardi, 1996; Eisner, 1996), who also discuss Early-style parsers for projective dependency grammars. &apos;We can use pre-computed top-down prediction to limit the number of pairs added. governor, which must be an ancestor of the governor. In defining a formal rewriting system for pseudo-projective trees, we will not attempt to model the &amp;quot;lifting&amp;quot; as a transformational step in the derivation. Rather, we will directly derive the &amp;quot;lifted&amp;quot; version of the tree, where a node is dependent of its linear governor. Thus, the derived structure resembles more a unistratal dependency representation like those used by (Hudson, 1990) tha</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>Jason M. Eisner. 1996. Three new probabilistic models for dependency parsing: An exploration. In Proceedings of the 16th International Conference on Computational Linguistics (COLING &apos;96), Copenhagen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haim Gaifman</author>
</authors>
<title>Dependency systems and phrase-structure systems.</title>
<date>1965</date>
<journal>Information and Control,</journal>
<pages>8--304</pages>
<contexts>
<context position="8726" citStr="Gaifman, 1965" startWordPosition="1496" endWordPosition="1497">rarchical order (dominance) and LP rules which state linear order. The dependency rules are further subdivided into subcategorization rules (or s-rules) and modification rules (or m-rules). Here are some sample s-rules: d1 : Vtrans —4 Nnom, Nob, (2) d2 Vclause —4 Nnom, V (3) Here is a sample m-rule. d3 : V Adv (4) LP rules are represented as regular expressions (actually, only a limited form of regular expressions) associated with each category. We use the hash sign (#) to denote the position of the governor (head). For example: Pi :Vtrans = (Adv)Nnorn(Aux)Adv•#NobjAdv* Vtrans (5) 3We follow (Gaifman, 1965) throughout this paper by modeling a dependency grammar with a string-rewriting system. However, we will identify a derivation with its representation as a tree, and we will sometimes refer to symbols introduced in a rewrite step as &amp;quot;dependent nodes&amp;quot;. For a model of a DG based on tree-rewriting (in the spirit of Tree Adjoining Grammar (Joshi et al., 1975)), see (Nasr, 1995). 41n this paper, we will allow finite feature structures on categories, which we will notate using subscripts; e.g., Vtrans • Since the feature structures are finite, this is simply a notational variant of a system defined </context>
</contexts>
<marker>Gaifman, 1965</marker>
<rawString>Haim Gaifman. 1965. Dependency systems and phrase-structure systems. Information and Control, 8:304-337.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Hudson</author>
</authors>
<title>English Word Grammar.</title>
<date>1990</date>
<publisher>Basil Blackwell,</publisher>
<location>Oxford, RU.</location>
<contexts>
<context position="7509" citStr="Hudson, 1990" startWordPosition="1293" endWordPosition="1294"> pseudo-projectivity purely structurally (i.e. without referring to the labeling). For example, we can impose that each node x is lifted to the highest ancestor of x covered by t- ((Nasr, 1996)). The resulting pseudo-projectivity is a fairly weak extension to projectivity, which nevertheless covers major nonprojective linguistic structures. However, we do not pursue a purely structural definition of pseudo-projectivity in this paper. 647 linguistic example of lifting rule is given in Section 4. The idea of building a projective tree by means of lifting appears in (Kunze, 1968) and is used by (Hudson, 1990) and (Hudson, unpublished). This idea can also be compared to the notion of word order domain (Reape, 1990; Broker and Neuhaus, 1997), to the Slash feature of GPSG and HPSG, to the functional uncertainty of LFG, and to the Move-a of GB theory. 3 Projective Dependency Grammars Revisited We (informally) define a projective Dependency Grammar as a string-rewriting system3 by giving a set of categories such as N, V and Adv,4 a set of distinguished start categories (the root categories of well-formed trees), a mapping from strings to categories, and two types of rules: dependency rules which state </context>
<context position="13690" citStr="Hudson, 1990" startWordPosition="2384" endWordPosition="2385">96; Eisner, 1996), who also discuss Early-style parsers for projective dependency grammars. &apos;We can use pre-computed top-down prediction to limit the number of pairs added. governor, which must be an ancestor of the governor. In defining a formal rewriting system for pseudo-projective trees, we will not attempt to model the &amp;quot;lifting&amp;quot; as a transformational step in the derivation. Rather, we will directly derive the &amp;quot;lifted&amp;quot; version of the tree, where a node is dependent of its linear governor. Thus, the derived structure resembles more a unistratal dependency representation like those used by (Hudson, 1990) than the multistratal representations of, for example, (Mel&apos;euk, 1988). However, from a formal point of view, the distinction is not significant. In order to capture pseudo-projectivity, we will interpret rules of the form (2) (for subcategorization of arguments by a head) and (4) (for selection of a head by an adjunct) as introducing syntactic dependents which may lift to a higher linear governor. An LP rule of the form (5) orders all linear dependents of the linear governor, no matter whose syntactic dependents they are. In addition, we need a third type of rule, namely a lifting rule, or 1</context>
</contexts>
<marker>Hudson, 1990</marker>
<rawString>Richard Hudson. 1990. English Word Grammar. Basil Blackwell, Oxford, RU.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Richard Hudson</author>
</authors>
<marker>Hudson, </marker>
<rawString>Richard Hudson. unpublished. Discontinuity. e-preprint (ftp.phon.ucl.ac.uk).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
<author>Leon Levy</author>
<author>M Takahashi</author>
</authors>
<title>Tree adjunct grammars.</title>
<date>1975</date>
<journal>J. Comput. Syst. Sci.,</journal>
<pages>10--136</pages>
<contexts>
<context position="9083" citStr="Joshi et al., 1975" startWordPosition="1553" endWordPosition="1556">expressions (actually, only a limited form of regular expressions) associated with each category. We use the hash sign (#) to denote the position of the governor (head). For example: Pi :Vtrans = (Adv)Nnorn(Aux)Adv•#NobjAdv* Vtrans (5) 3We follow (Gaifman, 1965) throughout this paper by modeling a dependency grammar with a string-rewriting system. However, we will identify a derivation with its representation as a tree, and we will sometimes refer to symbols introduced in a rewrite step as &amp;quot;dependent nodes&amp;quot;. For a model of a DG based on tree-rewriting (in the spirit of Tree Adjoining Grammar (Joshi et al., 1975)), see (Nasr, 1995). 41n this paper, we will allow finite feature structures on categories, which we will notate using subscripts; e.g., Vtrans • Since the feature structures are finite, this is simply a notational variant of a system defined only with simple category labels. Vclause Adv Nnom thought Vtrans • yesterday Fernando thought Vtrans yesterday Fernando thought Nnom eats Nb Adv yesterday Fernando thought Carlos eats beans slowly V Figure 3: A sample GDG derivation We will call this system generative dependency grammar or GDG for short. Derivations in GDG are defined as follows. In a re</context>
</contexts>
<marker>Joshi, Levy, Takahashi, 1975</marker>
<rawString>Aravind K. Joshi, Leon Levy, and M Takahashi. 1975. Tree adjunct grammars. J. Comput. Syst. Sci., 10:136-163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jurgen Kunze</author>
</authors>
<title>The treatment of nonprojective structures in the syntactic analysis and synthesis of english and german.</title>
<date>1968</date>
<journal>Computational Linguistics,</journal>
<pages>7--67</pages>
<contexts>
<context position="7479" citStr="Kunze, 1968" startWordPosition="1287" endWordPosition="1288">. A 21t is possible to define pseudo-projectivity purely structurally (i.e. without referring to the labeling). For example, we can impose that each node x is lifted to the highest ancestor of x covered by t- ((Nasr, 1996)). The resulting pseudo-projectivity is a fairly weak extension to projectivity, which nevertheless covers major nonprojective linguistic structures. However, we do not pursue a purely structural definition of pseudo-projectivity in this paper. 647 linguistic example of lifting rule is given in Section 4. The idea of building a projective tree by means of lifting appears in (Kunze, 1968) and is used by (Hudson, 1990) and (Hudson, unpublished). This idea can also be compared to the notion of word order domain (Reape, 1990; Broker and Neuhaus, 1997), to the Slash feature of GPSG and HPSG, to the functional uncertainty of LFG, and to the Move-a of GB theory. 3 Projective Dependency Grammars Revisited We (informally) define a projective Dependency Grammar as a string-rewriting system3 by giving a set of categories such as N, V and Adv,4 a set of distinguished start categories (the root categories of well-formed trees), a mapping from strings to categories, and two types of rules:</context>
</contexts>
<marker>Kunze, 1968</marker>
<rawString>Jurgen Kunze. 1968. The treatment of nonprojective structures in the syntactic analysis and synthesis of english and german. Computational Linguistics, 7:67-77.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Lecerf</author>
</authors>
<title>Programme des conflits, modele des conflits.</title>
<date>1960</date>
<journal>Bulletin bimestriel de PATALA,</journal>
<volume>4</volume>
<contexts>
<context position="2924" citStr="Lecerf, 1960" startWordPosition="494" endWordPosition="495">to as the dependent and y as the governor. The latter will be noted, when convenient, X+T (X+ when unambiguous). The notations t- and x+ are unambiguous because a node x has at most one governor in a tree. As usual, an ordered tree is a tree enriched with a linear order over the set of its nodes. Finally, if 1 is an arc of an ordered tree T, then Supp(1) represents the support of 1, i.e. the set of the nodes of T situated between the extremities of 1, extremities included. We will say that the elements of Supp(1) are covered by 1. 2.2 Projectivity The notion of projectivity was introduced by (Lecerf, 1960) and has received several different definitions since then. The definition given here is borrowed from (Marcus, 1965) and (Robinson, 1970): Definition: An arc t- is projective if and only if for every y covered by y x+ . A tree T is projective if and only if every arc of T is projective A projective tree has been represented in Figure 1. A projective dependency tree can be associated with a phrase structure tree whose constituents are the projections of the nodes of the dependency tree. Projectivity is therefore equivalent, in phrase structure markers, to con646 The big cat sometimes eats whit</context>
</contexts>
<marker>Lecerf, 1960</marker>
<rawString>Yves Lecerf. 1960. Programme des conflits, modele des conflits. Bulletin bimestriel de PATALA, 4,5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vicenzo Lombardi</author>
</authors>
<title>An Earley-style parser for dependency grammars.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th International Conference on Computational Linguistics (COLING &apos;96),</booktitle>
<location>Copenhagen.</location>
<contexts>
<context position="13079" citStr="Lombardi, 1996" startWordPosition="2289" endWordPosition="2290">l, then we add (mi, q) to M(i, j). The time complexity of the algorithm is 0(n3IGIQmax), where G is the number of ruleFSMs derived from the dependency and LP rules in the grammar and Qmax is the maximum number of states in any of the rule-FSMs. 4 A Formalization of PP-Dependency Grammars Recall that in a pseudo-projective tree, we make a distinction between a syntactic governor and a linear governor. A node can be &amp;quot;lifted&amp;quot; along a lifting path from being a dependent of its syntactic governor to being a dependent of its linear &apos;This type of parser has been proposed previously. See for example (Lombardi, 1996; Eisner, 1996), who also discuss Early-style parsers for projective dependency grammars. &apos;We can use pre-computed top-down prediction to limit the number of pairs added. governor, which must be an ancestor of the governor. In defining a formal rewriting system for pseudo-projective trees, we will not attempt to model the &amp;quot;lifting&amp;quot; as a transformational step in the derivation. Rather, we will directly derive the &amp;quot;lifted&amp;quot; version of the tree, where a node is dependent of its linear governor. Thus, the derived structure resembles more a unistratal dependency representation like those used by (Hu</context>
</contexts>
<marker>Lombardi, 1996</marker>
<rawString>Vicenzo Lombardi. 1996. An Earley-style parser for dependency grammars. In Proceedings of the 16th International Conference on Computational Linguistics (COLING &apos;96), Copenhagen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Solomon Marcus</author>
</authors>
<title>Sur la notion de projectivite.</title>
<date>1965</date>
<booktitle>Zeitschr. f. math. Logik und Grundlagen d. Math.,</booktitle>
<pages>11--181</pages>
<contexts>
<context position="3041" citStr="Marcus, 1965" startWordPosition="511" endWordPosition="512">notations t- and x+ are unambiguous because a node x has at most one governor in a tree. As usual, an ordered tree is a tree enriched with a linear order over the set of its nodes. Finally, if 1 is an arc of an ordered tree T, then Supp(1) represents the support of 1, i.e. the set of the nodes of T situated between the extremities of 1, extremities included. We will say that the elements of Supp(1) are covered by 1. 2.2 Projectivity The notion of projectivity was introduced by (Lecerf, 1960) and has received several different definitions since then. The definition given here is borrowed from (Marcus, 1965) and (Robinson, 1970): Definition: An arc t- is projective if and only if for every y covered by y x+ . A tree T is projective if and only if every arc of T is projective A projective tree has been represented in Figure 1. A projective dependency tree can be associated with a phrase structure tree whose constituents are the projections of the nodes of the dependency tree. Projectivity is therefore equivalent, in phrase structure markers, to con646 The big cat sometimes eats white mice Figure 1: A projective sub-categorization tree tinuity of constituent. The strong constraints introduced by th</context>
</contexts>
<marker>Marcus, 1965</marker>
<rawString>Solomon Marcus. 1965. Sur la notion de projectivite. Zeitschr. f. math. Logik und Grundlagen d. Math., 11:181-192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor A Merauk</author>
</authors>
<title>Dependency Syntax: Theory and Practice.</title>
<date>1988</date>
<publisher>State University of New York Press,</publisher>
<location>New York.</location>
<marker>Merauk, 1988</marker>
<rawString>Igor A. Merauk. 1988. Dependency Syntax: Theory and Practice. State University of New York Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexis Nasr</author>
</authors>
<title>A formalism and a parser for lexicalised dependency grammars.</title>
<date>1995</date>
<booktitle>In 4th International Workshop on Parsing Technologies,</booktitle>
<pages>186--195</pages>
<location>Prague.</location>
<contexts>
<context position="9102" citStr="Nasr, 1995" startWordPosition="1558" endWordPosition="1559"> a limited form of regular expressions) associated with each category. We use the hash sign (#) to denote the position of the governor (head). For example: Pi :Vtrans = (Adv)Nnorn(Aux)Adv•#NobjAdv* Vtrans (5) 3We follow (Gaifman, 1965) throughout this paper by modeling a dependency grammar with a string-rewriting system. However, we will identify a derivation with its representation as a tree, and we will sometimes refer to symbols introduced in a rewrite step as &amp;quot;dependent nodes&amp;quot;. For a model of a DG based on tree-rewriting (in the spirit of Tree Adjoining Grammar (Joshi et al., 1975)), see (Nasr, 1995). 41n this paper, we will allow finite feature structures on categories, which we will notate using subscripts; e.g., Vtrans • Since the feature structures are finite, this is simply a notational variant of a system defined only with simple category labels. Vclause Adv Nnom thought Vtrans • yesterday Fernando thought Vtrans yesterday Fernando thought Nnom eats Nb Adv yesterday Fernando thought Carlos eats beans slowly V Figure 3: A sample GDG derivation We will call this system generative dependency grammar or GDG for short. Derivations in GDG are defined as follows. In a rewrite step, we choo</context>
</contexts>
<marker>Nasr, 1995</marker>
<rawString>Alexis Nasr. 1995. A formalism and a parser for lexicalised dependency grammars. In 4th International Workshop on Parsing Technologies, pages 186-195, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexis Nasr</author>
</authors>
<title>Un systeme de reformulation automatique de phrases fonde sur la Theorie Sens-Texte : application aux langues controlees.</title>
<date>1996</date>
<tech>Ph.D. thesis,</tech>
<institution>Universite Paris</institution>
<contexts>
<context position="7089" citStr="Nasr, 1996" startWordPosition="1226" endWordPosition="1227">f categories): LD t SG w LG (1) This rule says that a node of category LD can be lifted from its syntactic governor of category SG to its linear governor of category LG through a path consisting of nodes of category C1, , Cn, where the string belongs to L(w). Every set of lifting rules defines a particular property of pseudo-projectivity by imposing particular constraints on the lifting. A 21t is possible to define pseudo-projectivity purely structurally (i.e. without referring to the labeling). For example, we can impose that each node x is lifted to the highest ancestor of x covered by t- ((Nasr, 1996)). The resulting pseudo-projectivity is a fairly weak extension to projectivity, which nevertheless covers major nonprojective linguistic structures. However, we do not pursue a purely structural definition of pseudo-projectivity in this paper. 647 linguistic example of lifting rule is given in Section 4. The idea of building a projective tree by means of lifting appears in (Kunze, 1968) and is used by (Hudson, 1990) and (Hudson, unpublished). This idea can also be compared to the notion of word order domain (Reape, 1990; Broker and Neuhaus, 1997), to the Slash feature of GPSG and HPSG, to the</context>
</contexts>
<marker>Nasr, 1996</marker>
<rawString>Alexis Nasr. 1996. Un systeme de reformulation automatique de phrases fonde sur la Theorie Sens-Texte : application aux langues controlees. Ph.D. thesis, Universite Paris 7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Reape</author>
</authors>
<title>Getting things in order.</title>
<date>1990</date>
<booktitle>In Proceedings of the Symposium on Discontinuous Constituents,</booktitle>
<location>Tilburg, Holland.</location>
<contexts>
<context position="7615" citStr="Reape, 1990" startWordPosition="1312" endWordPosition="1313">e that each node x is lifted to the highest ancestor of x covered by t- ((Nasr, 1996)). The resulting pseudo-projectivity is a fairly weak extension to projectivity, which nevertheless covers major nonprojective linguistic structures. However, we do not pursue a purely structural definition of pseudo-projectivity in this paper. 647 linguistic example of lifting rule is given in Section 4. The idea of building a projective tree by means of lifting appears in (Kunze, 1968) and is used by (Hudson, 1990) and (Hudson, unpublished). This idea can also be compared to the notion of word order domain (Reape, 1990; Broker and Neuhaus, 1997), to the Slash feature of GPSG and HPSG, to the functional uncertainty of LFG, and to the Move-a of GB theory. 3 Projective Dependency Grammars Revisited We (informally) define a projective Dependency Grammar as a string-rewriting system3 by giving a set of categories such as N, V and Adv,4 a set of distinguished start categories (the root categories of well-formed trees), a mapping from strings to categories, and two types of rules: dependency rules which state hierarchical order (dominance) and LP rules which state linear order. The dependency rules are further sub</context>
</contexts>
<marker>Reape, 1990</marker>
<rawString>Michael Reape. 1990. Getting things in order. In Proceedings of the Symposium on Discontinuous Constituents, Tilburg, Holland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane J Robinson</author>
</authors>
<title>Dependency structures and transformational rules.</title>
<date>1970</date>
<journal>Language,</journal>
<pages>46--2</pages>
<contexts>
<context position="3062" citStr="Robinson, 1970" startWordPosition="514" endWordPosition="516"> are unambiguous because a node x has at most one governor in a tree. As usual, an ordered tree is a tree enriched with a linear order over the set of its nodes. Finally, if 1 is an arc of an ordered tree T, then Supp(1) represents the support of 1, i.e. the set of the nodes of T situated between the extremities of 1, extremities included. We will say that the elements of Supp(1) are covered by 1. 2.2 Projectivity The notion of projectivity was introduced by (Lecerf, 1960) and has received several different definitions since then. The definition given here is borrowed from (Marcus, 1965) and (Robinson, 1970): Definition: An arc t- is projective if and only if for every y covered by y x+ . A tree T is projective if and only if every arc of T is projective A projective tree has been represented in Figure 1. A projective dependency tree can be associated with a phrase structure tree whose constituents are the projections of the nodes of the dependency tree. Projectivity is therefore equivalent, in phrase structure markers, to con646 The big cat sometimes eats white mice Figure 1: A projective sub-categorization tree tinuity of constituent. The strong constraints introduced by the projectivity proper</context>
</contexts>
<marker>Robinson, 1970</marker>
<rawString>Jane J. Robinson. 1970. Dependency structures and transformational rules. Language, 46(2):259-285.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>