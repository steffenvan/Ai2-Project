<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.023673">
<title confidence="0.9983085">
Exploring Vector Space Models to Predict the Compositionality of
German Noun-Noun Compounds
</title>
<author confidence="0.922549">
Sabine Schulte im Walde and Stefan M¨uller and Stephen Roller
</author>
<affiliation confidence="0.6296355">
Institut f¨ur Maschinelle Sprachverarbeitung
Universit¨at Stuttgart
</affiliation>
<address confidence="0.936462">
Pfaffenwaldring 5b, 70569 Stuttgart, Germany
</address>
<email confidence="0.998912">
{schulte,muellesn,roller}@ims.uni-stuttgart.de
</email>
<sectionHeader confidence="0.998599" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999852111111111">
This paper explores two hypotheses regarding
vector space models that predict the compo-
sitionality of German noun-noun compounds:
(1) Against our intuition, we demonstrate that
window-based rather than syntax-based distri-
butional features perform better predictions,
and that not adjectives or verbs but nouns rep-
resent the most salient part-of-speech. Our
overall best result is state-of-the-art, reach-
ing Spearman’s p = 0.65 with a word-
space model of nominal features from a 20-
word window of a 1.5 billion word web cor-
pus. (2) While there are no significant dif-
ferences in predicting compound–modifier vs.
compound–head ratings on compositionality,
we show that the modifier (rather than the
head) properties predominantly influence the
degree of compositionality of the compound.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999965444444445">
Vector space models and distributional information
have been a steadily increasing, integral part of lex-
ical semantic research over the past 20 years. On
the one hand, vector space models (see Turney and
Pantel (2010) and Erk (2012) for two recent sur-
veys) have been exploited in psycholinguistic (Lund
and Burgess, 1996) and computational linguistic re-
search (Sch¨utze, 1992) to explore the notion of “sim-
ilarity” between a set of target objects within a ge-
ometric setting. On the other hand, the distribu-
tional hypothesis (Firth, 1957; Harris, 1968) has
been exploited to determine co-occurrence features
for vector space models that best describe the words,
phrases, sentences, etc. of interest.
While the emergence of vector space models is in-
creasingly pervasive within data-intensive lexical se-
mantics, and even though useful features have been
identified in general terms:1 when it comes to a spe-
cific semantic phenomenon, we need to explore the
relevant distributional features in order to investigate
the respective phenomenon. Our research is inter-
ested in the meaning of German compounds. More
specifically, we aim to predict the degrees of compo-
sitionality of German noun-noun compounds (e.g.,
Feuerwerk ‘fire works’) with regard to the mean-
ings of their constituents (e.g., Feuer ‘fire’ and Werk
‘opus’). This prediction uses vector space models,
and our goal is to identify salient features that de-
termine the degree of compositionality of the com-
pounds by relying on the distributional similarities
between the compounds and their constituents.
In this vein, we systematically explore window-
based and syntax-based contextual clues. Since the
targets in our vector space models are all nouns
(i.e., the compound nouns, the modifier nouns, and
the head nouns), our hypothesis is that adjectives
and verbs are expected to provide salient distri-
butional properties, as adjective/verb meaning and
noun meaning are in a strong interdependent rela-
tionship. Even more, we expect adjectives and verbs
that are syntactically bound to the nouns under con-
sideration (syntax-based, i.e., attributive adjectives
and subcategorising verbs) to outperform those that
“just” appear in the window contexts of the nouns
(window-based). In order to investigate this first
</bodyText>
<footnote confidence="0.994265333333333">
1See Agirre et al. (2009) and Bullinaria and Levy (2007;
2012), among others, for systematic comparisons of co-
occurrence features on various semantic relatedness tasks.
</footnote>
<page confidence="0.930322">
255
</page>
<note confidence="0.9013025">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 255–265, Atlanta, Georgia, June 13-14, 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999924115384615">
hypothesis, we compare window-based and syntax-
based distributional features across parts-of-speech.
Concerning a more specific aspect of compound
meaning, we are interested in the contributions of
the modifier noun versus head noun properties with
regard to the meaning of the noun-noun compounds.
While there has been prior psycholinguistic research
on the constituent contributions (e.g., Gagn´e and
Spalding (2009; 2011)), computational linguistics
has not yet paid much attention to this issue, as
far as we know. Our hypothesis is that the dis-
tributional properties of the head constituents are
more salient than the distributional properties of
the modifier constituents in predicting the degree
of compositionality of the compounds. In order to
assess this second hypothesis, we compare the vec-
tor space similarities between the compounds and
their modifier constituents with those of the com-
pounds and their head constituents, with regard to
the overall most successful features.
The paper is organised as follows. Section 2 in-
troduces the compound data that is relevant for this
paper, i.e., the noun-noun compounds and the com-
positionality ratings. Section 3 performs and dis-
cusses the vector space experiments to explore our
hypotheses, and Section 4 describes related work.
</bodyText>
<sectionHeader confidence="0.999386" genericHeader="introduction">
2 Data
</sectionHeader>
<subsectionHeader confidence="0.991642">
2.1 German Noun-Noun Compounds
</subsectionHeader>
<bodyText confidence="0.999861352941177">
Compounds are combinations of two or more sim-
plex words. Traditionally, a number of criteria (such
as compounds being syntactically inseparable, and
that compounds have a specific stress pattern) have
been proposed, in order to establish a border be-
tween compounds and non-compounds. However,
Lieber and Stekauer (2009a) demonstrated that none
of these tests are universally reliable to distinguish
compounds from other types of derived words.
Compounds have thus been a recurrent focus
of attention within theoretical, cognitive, and in
the last decade also within computational linguis-
tics. Recent evidence of this strong interest are the
Handbook of Compounding (Lieber and Stekauer,
2009b) on theoretical perspectives, and a series of
workshops2 and special journal issues with respect
to multi-word expressions (including various types
</bodyText>
<footnote confidence="0.667543">
2www.multiword.sourceforge.net
</footnote>
<bodyText confidence="0.999951525">
of compounds) and the computational perspective
(Journal of Computer Speech and Language, 2005;
Language Resources and Evaluation, 2010; ACM
Transactions on Speech and Language Processing,
to appear).
Our focus of interest is on German noun-noun
compounds (see Fleischer and Barz (2012) for a de-
tailed overview and Klos (2011) for a recent de-
tailed exploration), such as Ahornblatt ‘maple leaf’,
Feuerwerk ‘fireworks’, and Obstkuchen ‘fruit cake’
where both the grammatical head (in German, this
is the rightmost constituent) and the modifier are
nouns. More specifically, we are interested in the
degrees of compositionality of German noun-noun
compounds, i.e., the semantic relatedness between
the meaning of a compound (e.g., Feuerwerk) and
the meanings of its constituents (e.g., Feuer ‘fire’
and Werk ‘opus’).
Our work is based on a selection of noun com-
pounds by von der Heide and Borgwaldt (2009),
who created a set of 450 concrete, depictable Ger-
man noun compounds according to four compo-
sitionality classes: compounds that are transpar-
ent with regard to both constituents (e.g., Ahorn-
blatt ‘maple leaf’); compounds that are opaque
with regard to both constituents (e.g., L¨owenzahn
‘lion+tooth —* dandelion’); compounds that are
transparent with regard to the modifier but opaque
with regard to the head (e.g., Feuerzeug ‘fire+stuff
—* lighter’); and compounds that are opaque with
regard to the modifier but transparent with regard to
the head (e.g., Fliegenpilz ‘fly+mushroom —* toad-
stool’).
From the compound set by von der Heide and
Borgwaldt, we disregarded noun compounds with
more than two constituents (in some cases, the mod-
ifier or the head was complex itself) as well as com-
pounds where the modifiers were not nouns. Our
final set comprises a subset of their compounds in-
cluding 244 two-part noun-noun compounds.
</bodyText>
<subsectionHeader confidence="0.999258">
2.2 Compositionality Ratings
</subsectionHeader>
<bodyText confidence="0.998986166666667">
von der Heide and Borgwaldt (2009) collected hu-
man ratings on compositionality for all their 450
compounds. The compounds were distributed over
5 lists, and 270 participants judged the degree of
compositionality of the compounds with respect to
their first as well as their second constituent, on
</bodyText>
<page confidence="0.997717">
256
</page>
<table confidence="0.973151833333333">
Compounds Mean Ratings and Standard Deviations
whole literal meanings of constituents whole modifier head mean range
Ahornblatt ‘maple leaf’ maple leaf 6.03 f 1.49 5.64 f 1.63 5.71 f 1.70 (1) high/high
Postbote ‘post man’ mail messenger 6.33 f 0.96 5.87 f 1.55 5.10 f 1.99
Seezunge ‘sole’ sea tongue 1.85 f 1.28 3.57 f 2.42 3.27 f 2.32 (2) mid/mid
Windlicht ‘storm lamp’ wind light 3.52 f 2.08 3.07 f 2.12 4.27 f 2.36
L¨owenzahn ‘dandelion’ lion tooth 1.66 f 1.54 2.10 f 1.84 2.23 f 1.92 (3) low/low
Maulwurf ‘mole’ mouth throw 1.58 f 1.43 2.21 f 1.68 2.76 f 2.10
Fliegenpilz ‘toadstool’ fly/bow tie mushroom 2.00 f 1.20 1.93 f 1.28 6.55 f 0.63 (4) low/high
Flohmarkt ‘flea market’ flea market 2.31 f 1.65 1.50 f 1.22 6.03 f 1.50
Feuerzeug ‘lighter’ fire stuff 4.58 f 1.75 5.87 f 1.01 1.90 f 1.03 (5) high/low
Fleischwolf ‘meat chopper’ meat wolf 1.70 f 1.05 6.00 f 1.44 1.90 f 1.42
</table>
<tableCaption confidence="0.99993">
Table 1: Examples of compound ratings.
</tableCaption>
<bodyText confidence="0.998897090909091">
a scale between 1 (definitely opaque) and 7 (defi-
nitely transparent). For each compound–constituent
pair, they collected judgements from 30 participants,
and calculated the rating mean and the standard de-
viation. We refer to this set as our compound–
constituent ratings.
A second experiment collected human ratings on
compositionality for our subset of 244 noun-noun
compounds. In this case, we asked the participants
to provide a unique score for each compound as
a whole, again on a scale between 1 and 7. The
collection was performed via Amazon Mechanical
Turk (AMT)3. We randomly distributed our subset
of 244 compounds over 21 batches, with 12 com-
pounds each, in random order. In order to control for
spammers, we also included two German fake com-
pound nouns into each of the batches, in random po-
sitions of the lists. If participants did not recognise
the fake words, all of their ratings were rejected. We
collected between 27 and 34 ratings per target com-
pound. For each of the compounds we calculated the
rating mean and the standard deviation. We refer to
this second set as our compound whole ratings.
Table 1 presents example mean ratings for the
compound–constituent ratings as well as for the
compound whole ratings, accompanied by the stan-
dard deviations. We selected two examples each
for five categories of mean ratings: the compound–
constituent ratings were (1) high or (2) mid or (3)
low with regard to both constituents; the compound–
constituent ratings were (4) low with regard to the
modifier but high with regard to the head; (5) vice
versa. Roller et al. (2013) performed a thorough
</bodyText>
<footnote confidence="0.983239">
3www.mturk.com
</footnote>
<figureCaption confidence="0.999972">
Figure 1: Distribution of compound ratings.
</figureCaption>
<bodyText confidence="0.9995835">
analysis of the two sets of ratings, and assessed their
reliability from several perspectives.
Figure 1 shows how the mean ratings for the com-
pounds as a whole, for the compound–modifier pairs
as well as for the compound–head pairs are dis-
tributed over the range [1, 7]: For each set, we in-
dependently sorted the 244 values and plotted them.
The purpose of the figure is to illustrate that the rat-
ings for our 244 noun-noun compounds are not par-
ticularly skewed to any area within the range.4
Figure 2 again shows the mean ratings for the
compounds as a whole as well as for the compound–
constituent pairs, but in this case only the compound
whole ratings were sorted, and the compound–
constituent ratings were plotted against the com-
pound whole ratings. According to the plot, the
compound–modifier ratings (red) seem to correlate
better with the compound whole ratings than the
compound–head ratings (yellow) do. This intuition
will be confirmed in Section 3.1.
</bodyText>
<footnote confidence="0.898268">
4The illustration idea was taken from Reddy et al. (2011b).
</footnote>
<page confidence="0.991529">
257
</page>
<figureCaption confidence="0.997629">
Figure 2: Compounds ratings sorted by whole ratings.
</figureCaption>
<sectionHeader confidence="0.989209" genericHeader="method">
3 Vector Space Models (VSMs)
</sectionHeader>
<bodyText confidence="0.995992931034483">
The goal of our vector space models is to identify
distributional features that are salient to predict the
degree of compositionality of the compounds, by re-
lying on the similarities between the compound and
constituent properties.
In all our vector space experiments, we used co-
occurrence frequency counts as induced from Ger-
man web corpora, and calculated local mutual in-
formation (LMI)5 values (Evert, 2005), to instantiate
the empirical properties of our target nouns with re-
gard to the various corpus-based features. LMI is a
measure from information theory that compares the
observed frequencies O with expected frequencies
E, taking marginal frequencies into account:
LMI = O x log E,
with E representing the product of the marginal fre-
quencies over the sample size.6 In comparison to
(pointwise) mutual information (Church and Hanks,
1990), LMI improves the problem of propagating
low-frequent events, by multiplying mutual infor-
mation by the observed frequency.
Relying on the LMI vector space models, the co-
sine determined the distributional similarity between
the compounds and their constituents, which was in
turn used to predict the compositionality between
the compound and the constituents, assuming that
the stronger the distributional similarity (i.e., the co-
sine values), the larger the degree of compositional-
ity.
</bodyText>
<footnote confidence="0.67763">
5Alternatively, we also used the raw frequencies in all ex-
periments below. The insights into the various features were
identical to those based on LMI, but the predictions were worse.
6See http://www.collocations.de/AM/ for a
more detailed illustration of association measures (incl. LMI).
</footnote>
<bodyText confidence="0.999969304347826">
The vector space predictions were evaluated
against the human ratings on the degree of compo-
sitionality, using the Spearman Rank-Order Correla-
tion Coefficient p (Siegel and Castellan, 1988). The
p correlation is a non-parametric statistical test that
measures the association between two variables that
are ranked in two ordered series. In Section 3.3 we
will compare the overall effect of the various fea-
ture types and correlate all 488 compound–modifier
and compound–head predictions against the ratings
at the same time; in Section 3.4 we will compare
the different effects of the features for compound–
modifier pairs vs. compound–head pairs and thus
correlate 244 predictions in both cases.
After introducing a baseline and an upper bound
for our vector space experiments in Section 3.1 as
well as our web corpora in Section 3.2, Section 3.3
presents window-based in comparison to syntax-
based vector space models (distinguishing various
part-of-speech features). In Section 3.4 we then fo-
cus on the contribution of modifiers vs. heads in the
vector space models, with regard to the overall most
successful features.
</bodyText>
<subsectionHeader confidence="0.998039">
3.1 Baseline and Upper Bound
</subsectionHeader>
<bodyText confidence="0.999993652173913">
Table 2 presents the baseline and the upper bound
values for the vector space experiments. The
baseline in the first two lines follows a proce-
dure performed by Reddy et al. (2011b), and re-
lies on a random assignment of rating values [1, 7]
to the compound–modifier and the compound–
head pairs. The 244 random values for the
compound–constituent pairs were then each corre-
lated against the compound whole ratings. The
random compound–modifier ratings show a base-
line correlation of p = 0.0959 with the compound
whole ratings, and the random compound–head rat-
ings show a baseline correlation of p = 0.1019 with
the compound whole ratings.
The upper bound in the first two lines shows the
correlations between the human ratings from the two
experiments, i.e., between the 244 compound whole
ratings and the respective compound–modifier and
compound–head ratings. The compound–modifier
ratings exhibit a strong correlation with the com-
pound whole ratings (p = 0.6002), while the cor-
relation between the compound–head ratings and
the compound whole ratings is not even moderate
</bodyText>
<page confidence="0.99193">
258
</page>
<table confidence="0.999411166666667">
Function �
Baseline Upper Bound
modifier only .0959 .6002
head only .1019 .1385
addition .1168 .7687
multiplication .1079 .7829
</table>
<tableCaption confidence="0.998642">
Table 2: Baseline/Upper bound p correlations.
</tableCaption>
<bodyText confidence="0.999946">
(p = 0.1385). Obviously, the semantics of the mod-
ifiers had a much stronger impact on the semantic
judgements of the compounds, thus confirming our
intuition from Section 2.2.
The lower part of the table shows the respec-
tive baseline and upper bound values when the
compound–modifier ratings and the compound–
head ratings were combined by standard arith-
metic operations, cf. Widdows (2008) and
Mitchell and Lapata (2010), among others: the
compound–modifier and compound–head ratings
were treated as vectors, and the vector fea-
tures (i.e., the compound–constituent ratings) were
added/multiplied to predict the compound whole rat-
ings. As in the related work, the arithmetic op-
erations strengthen the predictions, and multiplica-
tion reached an upper bound of p = 0.7829, thus
outperforming not only the head-only but also the
modifier-only upper bound.
</bodyText>
<subsectionHeader confidence="0.99892">
3.2 German Web Corpora
</subsectionHeader>
<bodyText confidence="0.9999746875">
Most of our experiments rely on the sdeWaC corpus
(Faaß et al., 2010), a cleaned version of the German
web corpus deWaC created by the WaCky group (Ba-
roni et al., 2009). The corpus cleaning had focused
mainly on removing duplicates from the deWaC, and
on disregarding sentences that were syntactically ill-
formed (relying on a parsability index provided by a
standard dependency parser (Schiehlen, 2003)). The
sdeWaC contains approx. 880 million words and can
be downloaded from http://wacky.sslmit.
unibo.it/.
While the sdeWaC is an attractive corpus choice
because it is a web corpus with a reasonable size,
and yet has been cleaned and parsed (so that we
can induce syntax-based distributional features), it
has one serious drawback for a window-based ap-
proach (and, in general, for corpus work going be-
yond the sentence border): The sentences in the cor-
pus have been sorted alphabetically, so going be-
yond the sentence border is likely to entering a sen-
tence that did not originally precede or follow the
sentence of interest. So window co-occurrence in
the sdeWaC actually refers to x words to the left and
right BUT within the same sentence. Thus, enlarg-
ing the window size does not effectively change the
co-occurrence information any more at some point.
For this reason, we additionally use WebKo, a pre-
decessor version of the sdeWaC, which comprises
more data (approx. 1.5 billion words in compari-
son to 880 million words) and is not alphabetically
sorted, but is less clean and had not been parsed (be-
cause it was not clean enough).
</bodyText>
<subsectionHeader confidence="0.997381">
3.3 Window-based vs. Syntax-based VSMs
</subsectionHeader>
<bodyText confidence="0.996347030303031">
Window-based Co-Occurrence When applying
window-based co-occurrence features to our vec-
tor space models, we specified a corpus, a part-of-
speech and a window size, and then determined the
co-occurrence strengths of our compound nouns and
their constituents with regard to the respective con-
text words. For example, when restricting the part-
of-speech to adjectives and the window size to 5, we
counted how often our targets appeared with any ad-
jectives in a window of five words to the left and
to the right. We looked at lemmas, and deleted
any kind of sentence punctuation. In general, we
checked windows of sizes 1, 2, 5, 10, and 20. In one
case we extended the window up to 100 words.
The window-based models compared the effect
of varying the parts-of-speech of the co-occurring
words, motivated by the hypothesis that adjectives
and verbs were expected to provide salient distribu-
tional properties. So we checked which parts-of-
speech provided specific insight into the distribu-
tional similarity between nominal compounds and
nominal constituents: We used common nouns vs.
adjectives vs. main verbs that co-occurred with the
target nouns in the corpora. Figure 3 illustrates the
behaviour of the Spearman Rank-Order Correlation
Coefficient values p over the window sizes 1, 2, 5,
10, and 20 within sdeWaC (sentence-internal) and
WebKo (beyond sentence borders), when restricting
and combining the co-occurring parts-of-speech. It
is clear from the figure that relying on nouns was
the best choice, even better than combining nouns
with adjectives and verbs. The differences for nouns
vs. adjectives or verbs in the 20-word windows were
</bodyText>
<page confidence="0.996283">
259
</page>
<figureCaption confidence="0.999539">
Figure 3: Window-based sdeWaC and WebKo p correlations across part-of-speech features.
</figureCaption>
<bodyText confidence="0.999604566666667">
significant.7 Furthermore, the larger WebKo data
outperformed the cleaned sdeWaC data, reaching an
optimal prediction of p = 0.6497.8 The corpus dif-
ferences for NN and NN+ADJ+VV were significant.
As none of the window lines had reached an op-
timal correlation with a window size of 20 yet (i.e.,
the correlation values were still increasing), we en-
larged the window size up to 100 words, in order
to check on the most successful window size. We
restricted the experiment to nominal features (with
nouns representing the overall most successful fea-
tures). The correlations did not increase with larger
windows: the optimal prediction was still performed
at a window size of 20.
Syntax-based Co-Occurrence When applying
syntax-based co-occurrence features to our vector
space models, we relied only on the sdeWaC cor-
pus because WebKo was not parsed and thus did
not provide syntactic information. We specified a
syntax-based feature type and then determined the
co-occurrence strengths of our compounds and con-
stituents with regard to the respective context words.
In order to test our hypothesis that syntax-based
information is more salient than window-based in-
formation to predict the compositionality of our
compound nouns, we compared a number of po-
tentially salient syntactic features for noun similar-
ity: the syntactic functions of nouns in verb subcat-
egorisation (intransitive and transitive subjects; di-
rect and PP objects), and those categories that fre-
</bodyText>
<footnote confidence="0.991035">
7All significance tests in this paper were performed by
Fisher r-to-z transformation.
8For a fair corpus comparison, we repeated the experiments
with WebKo on sentence-internal data. It still outperformed the
sdeWaC corpus.
</footnote>
<bodyText confidence="0.999714085714286">
quently modify nouns or are modified by nouns (ad-
jectives and prepositions). With regard to subcate-
gorisation functions, verbs subcategorising our tar-
get nouns represented the dimensions in the vector
space models. For example, we used all verbs as
vector dimensions that took our targets as direct ob-
jects, and vector values were based on these syntac-
tic co-occurrences. For a noun like Buch ‘book’,
the strongest verb dimensions were lesen ‘read’,
schreiben ‘write’, and kaufen ‘buy’. With regard
to modification, we considered the adjectives and
prepositions that modified our target nouns, as well
as the prepositions that were modified by our target
nouns. For the noun Buch, strong modifying adjec-
tive dimensions were neu ‘new’, erschienen ‘pub-
lished’, and heilig ‘holy’; strong modifying prepo-
sition dimensions were in ‘in’, mit ‘with’, and zu
‘on’; and strong modified preposition dimensions
were von ‘by’, ¨uber ‘about’, and f¨ur ‘for’.
Figure 4 demonstrates that the potentially salient
syntactic functions had different effects on predict-
ing compositionality. The top part of the figure
shows the modification-based correlations, the mid-
dle part shows the subcategorisation-based corre-
lations, and at the bottom of the figure we repeat
the p correlation values for window-based adjec-
tives and verbs (within a window of 20 words)
from the sdeWaC. The syntax-based predictions by
modification and subcategorisation were all signif-
icantly worse than the predictions by the respec-
tive window-based parts-of-speech. Furthermore,
the figure shows that there are strong differences
with regard to the types of syntactic functions,
when predicting compositionality: Relying on our
target nouns as transitive subjects of verbs is al-
</bodyText>
<page confidence="0.996518">
260
</page>
<figureCaption confidence="0.999939">
Figure 4: Syntax-based correlations.
</figureCaption>
<bodyText confidence="0.999971666666666">
most useless (p = 0.1194); using the intransi-
tive subject function improves the prediction (p =
0.2121); interestingly, when abstracting over subject
(in)transitivity, i.e., when we use all verbs as vector
space features that appeared with our target nouns as
subjects –independently whether this was an intran-
sitive or a transitive subject– was again more suc-
cessful (p = 0.2749). Relying on our noun targets as
direct objects is again slightly better (p = 0.2988);
as pp objects it is again slightly worse (p = 0.2485).
None of these differences were significant, though.
Last but not least, we concatenated all syntax-
based features to a large syntactic VSM (and we
also considered variations of syntax-based feature
set concatenations), but the results of any unified
combinations were clearly below the best individ-
ual predictions. So the best syntax-based predic-
tors were adjectives that modified our compound and
constituent nouns, with p = 0.3455, which how-
ever just (non-significantly) outperformed the best
adjective setting in our window-based vector space
(p = 0.3394). Modification by prepositions did
not provide salient distributional information, with
p = 0.2044/0.1725 relying on modifying/modified
prepositions.
In sum, attributive adjectives and verbs that sub-
categorised our target nouns as direct objects were
the most salient syntax-based distributional fea-
tures but nevertheless predicted worse than “just”
window-based adjectives and verbs, respectively.
</bodyText>
<subsectionHeader confidence="0.998095">
3.4 Role of Modifiers vs. Heads
</subsectionHeader>
<bodyText confidence="0.993513156862745">
This section tests our hypothesis that the distribu-
tional properties of the head constituents are more
salient than the distributional properties of the mod-
ifier constituents in predicting the degree of compo-
sitionality of the compounds. Our rating data enables
us to explore the modifier/head distinction with re-
gard to two perspectives.
Perspective (i): Salient Features for Compound–
Modifier vs. Compound–Head Pairs Instead of
correlating all 488 compound–constituent predic-
tions against the ratings, we distinguished between
the 244 compound–modifier predictions and the 244
compound–head predictions. This perspective al-
lowed us to distinguish between the salience of the
various feature types with regard to the semantic
relatedness between compound–modifier pairs vs.
compound–head pairs.
Figure 5 presents the correlation values when
predicting the degrees of compositionality of
compound–modifier (M in the left panel) vs.
compound–head (H in the right panel) pairs, as
based on the window features and the various parts-
of-speech. The prediction of the parts-of-speech is
NN &gt; NN+ADJ+VV &gt; VV &gt; ADJ
and –with few exceptions– the predictions are im-
proving with increasing window sizes, as the over-
all predictions in the previous section did. But
while in smaller window sizes the predictions of
the compound–head ratings are better than those of
the compound–modifier ratings, this difference van-
ishes with larger windows. With regard to a win-
dow size of 20 there is no significant difference be-
tween predicting the semantic relatedness between
compound–modifier vs. compound–head pairs.
When using the syntactic features to predict the
degrees of compositionality of compound–modifier
vs. head–compound pairs, in all but one of the
syntactic feature types the verb subcategorisation as
well as the modification functions allowed a stronger
prediction of compound–head ratings in compari-
son to compound–modifier ratings. The only syn-
tactic feature that was significantly better to predict
compound–modifier ratings was relying on transi-
tive subjects. In sum, the predictions based on syn-
tactic features in most but not all cases behaved in
accordance with our hypothesis.
As in our original experiments in Section 3.3,
the syntax-based features were significantly outper-
formed by the window-based features. The syn-
tactic features reached an optimum of p = 0.2224
and p = 0.3502 for predicting modifier–compound
</bodyText>
<page confidence="0.99718">
261
</page>
<figureCaption confidence="0.99964">
Figure 5: Window-based correlations (modifiers vs. heads).
</figureCaption>
<bodyText confidence="0.9968485">
vs. head–compound degrees of compositionality (in
both cases relying on attributive adjectives), in com-
parison to p = 0.5698 and p = 0.5745 when relying
on nouns in a window of 20 words.
Perspective (ii): Contribution of Modifiers/Heads
to Compound Meaning This final analysis ex-
plores the contributions of the modifiers and of the
heads with regard to the compound meaning, by cor-
relating only one type of compound–constituent pre-
dictions with the compound whole ratings. I.e., we
predicted the compositionality of the compound by
the distributional similarity between the compound
and only one of its constituents, checking if the
meaning of the compound is determined more by the
meaning of the modifier or the head. This analysis is
in accordance with the upper bound in Section 3.1,
where the compound–constituent ratings were cor-
related with the compound whole ratings.
Figure 6 presents the correlation values when
determining the compound whole ratings by
only compound–modifier predictions, or only
compound–head predictions, or by adding or multi-
plying the modifier and head predictions. The under-
lying features rely on a 20-word window (adjectives,
verbs, nouns, and across parts-of-speech). It is strik-
ing that in three out of four cases the predictions of
the compound whole ratings were performed simi-
larly well (i) by only the compound–modifier pre-
dictions, and (ii) by multiplying the compound–
modifier and the compound–head predictions. So,
as in the calculation of the upper bound, the dis-
tributional semantics of the modifiers had a much
stronger impact on the semantics of the compound
than the distributional semantics of the heads did.
</bodyText>
<figureCaption confidence="0.994693">
Figure 6: Predicting the compound whole ratings.
</figureCaption>
<subsectionHeader confidence="0.804931">
3.5 Discussion
</subsectionHeader>
<bodyText confidence="0.999970428571428">
The vector space models explored two hypotheses
to predict the compositionality of German noun-
noun compounds by distributional features. Re-
garding hypothesis 1, we demonstrated that –against
our intuitions– not adjectives or verbs whose mean-
ings are strongly interdependent with the mean-
ings of nouns provided the most salient distribu-
tional information, but that relying on nouns was
the best choice, in combination with a 20-word win-
dow, reaching state-of-the-art p = 0.6497. The
larger but less clean web corpus WebKo outper-
formed the smaller but cleaner successor sdeWaC.
Furthermore, the syntax-based predictions by adjec-
tive/preposition modification and by verb subcate-
gorisation (as well as various concatenations of syn-
tactic VSMs) were all worse than the predictions by
the respective window-based parts-of-speech.
Regarding hypothesis 2, we distinguished the
contributions of modifiers vs. heads to the com-
pound meaning from two perspectives. (i) The pre-
dictions of the compound–modifier vs. compound–
</bodyText>
<page confidence="0.991566">
262
</page>
<bodyText confidence="0.999994947368421">
head ratings did not differ significantly when us-
ing features from increasing window sizes, but with
small window sizes the compound–head ratings
were predicted better than the compound–modifier
ratings. This insight fits well to the stronger im-
pact of syntax-based features on compound–head
in comparison to compound–modifier predictions
because –even though German is a language with
comparably free word order– we can expect many
syntax-based features (especially attributive adjec-
tives and prepositions) to appear in close vicinity
to the nouns they depend on or subcategorise. We
conclude that the features that are salient to predict
similarities between the compound–modifier vs. the
compound–head pairs are different, and that based
on small windows the distributional similarity be-
tween compounds and heads is stronger than be-
tween compounds and modifiers, but based on larger
contexts this difference vanishes. (ii) With regard
to the overall meaning of the compound, the influ-
ence of the modifiers was not only much stronger
in the human ratings (cf. Section 2) and in the up-
per bound (cf. Section 3.1), but also in the vector
space models (cf. Figure 6). While this insight con-
tradicts our second hypothesis (that the head proper-
ties are more salient than the modifier properties in
predicting the compositionality of the compound),
it fits into a larger picture that has primarily been
discussed in psycholinguistic research on compound
meaning, where various factors such as the semantic
relation between the modifier and the head (Gagn´e
and Spalding, 2009) and the modifier properties, in-
ferential processing and world knowledge (Gagn´e
and Spalding, 2011) were taken into account. How-
ever, also in psycholinguistic studies that explore
the semantic role of modifiers and heads in noun
compounds there is no agreement about which con-
stituent properties are inherited by the compound.
</bodyText>
<sectionHeader confidence="0.999991" genericHeader="related work">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999930952380953">
Most computational approaches to model the mean-
ing or compositionality of compounds have been
performed for English, including work on parti-
cle verbs (McCarthy et al., 2003; Bannard, 2005;
Cook and Stevenson, 2006); adjective-noun com-
binations (Baroni and Zamparelli, 2010; Boleda et
al., 2013); and noun-noun compounds (Reddy et
al., 2011b; Reddy et al., 2011a). Most closely re-
lated to our work is Reddy et al. (2011b), who
relied on window-based distributional models to
predict the compositionality of English noun-noun
compounds. Their gold standard also comprised
compound–constituent ratings as well as compound
whole ratings, but the resources had been cleaned
more extensively, and they reached p = 0.714.
Concerning vector space explorations and seman-
tic relatedness in more general terms, Bullinaria and
Levy (2007; 2012) also systematically assessed a
range of factors in VSMs (corpus type and size,
window size, association measures, and corpus pre-
processing, among others) against four semantic
tasks, however not including compositionality rat-
ings. Similarly, Agirre et al. (2009) compared and
combined a WordNet-based and various distribu-
tional models to predict the pair similarity of the 65
Rubenstein and Goodenough word pairs and the 353
word pairs in WordSim353. They varied window
sizes, dependency relations and raw words in the
models. On WordSim353, they reached p = 0.66,
which is slightly better than our best result, but at
the same time the dataset is smaller.
Concerning computational models of German
compounds, there is not much previous work. Our
own work (Schulte im Walde, 2005; K¨uhner and
Schulte im Walde, 2010) has addressed the degrees
of compositionality of German particle verbs. Zins-
meister and Heid (2004) are most closely related to
our current study. They suggested a distributional
model to identify lexicalised German noun com-
pounds by comparing the verbs that subcategorise
the noun compound with those that subcategorise
the head noun as direct objects.
</bodyText>
<sectionHeader confidence="0.999301" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999462">
This paper presented experiments to predict the
compositionality of German noun-noun compounds.
Our overall best result is state-of-the-art, reaching
Spearman’s p = 0.65 with a word-space model of
nominal features from a 20-word window of a 1.5
billion word web corpus. Our experiments demon-
strated that (1) window-based features outperformed
syntax-based features, and nouns outperformed ad-
jectives and verbs; (2) the modifier properties pre-
dominantly influenced the compositionality.
</bodyText>
<page confidence="0.997751">
263
</page>
<sectionHeader confidence="0.995521" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999422932692308">
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pasca, and Aitor Soroa. 2009. A
Study on Similarity and Relatedness Using Distribu-
tional and WordNet-based Approaches. In Proceed-
ings of the North American Chapter of the Association
for Computational Linguistics and Human Language
Technologies Conference, pages 19–27, Boulder, Col-
orado.
Collin Bannard. 2005. Learning about the Meaning of
Verb–Particle Constructions from Corpora. Computer
Speech and Language, 19:467–478.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are Vectors, Adjectives are Matrices: Represent-
ing Adjective-Noun Constructions in Semantic Space.
In Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1183–1193, Cambridge, MA, October.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The WaCky Wide Web:
A Collection of Very Large Linguistically Processed
Web-Crawled Corpora. Language Resources and
Evaluation, 43(3):209–226.
Gemma Boleda, Marco Baroni, Nghia The Pham, and
Louise McNally. 2013. On Adjective-Noun Compo-
sition in Distributional Semantics. In Proceedings of
the 10th International Conference on Computational
Semantics, Potsdam, Germany.
John A. Bullinaria and Joseph P. Levy. 2007. Extracting
Semantic Representations from Word Co-Occurrence
Statistics: A Computational Study. Behavior Research
Methods, 39(3):510–526.
John A. Bullinaria and Joseph P. Levy. 2012. Extracting
Semantic Representations from Word Co-Occurrence
Statistics: Stop-Lists, Stemming, and SVD. Behavior
Research Methods, 44:890–907.
Kenneth W. Church and Patrick Hanks. 1990. Word As-
sociation Norms, Mutual Information, and Lexicogra-
phy. Computational Linguistics, 16(1):22–29.
Paul Cook and Suzanne Stevenson. 2006. Classifying
Particle Semantics in English Verb-Particle Construc-
tions. In Proceedings of the ACL/COLING Workshop
on Multiword Expressions: Identifying and Exploiting
Underlying Properties, Sydney, Australia.
Katrin Erk. 2012. Vector Space Models of Word Mean-
ing and Phrase Meaning: A Survey. Language and
Linguistics Compass, 6(10):635–653.
Stefan Evert. 2005. The Statistics of Word Co-
Occurrences: Word Pairs and Collocations. Ph.D.
thesis, Institut f¨ur Maschinelle Sprachverarbeitung,
Universit¨at Stuttgart.
Gertrud Faaß Ulrich Heid, and Helmut Schmid. 2010.
Design and Application of a Gold Standard for Mor-
phological Analysis: SMOR in Validation. In Pro-
ceedings of the 7th International Conference on Lan-
guage Resources and Evaluation, pages 803–810, Val-
letta, Malta.
John R. Firth. 1957. Papers in Linguistics 1934-51.
Longmans, London, UK.
Wolfgang Fleischer and Irmhild Barz. 2012. Wortbil-
dung der deutschen Gegenwartssprache. de Gruyter.
Christina L. Gagn´e and Thomas L. Spalding. 2009.
Constituent Integration during the Processing of Com-
pound Words: Does it involve the Use of Relational
Structures? Journal ofMemory and Language, 60:20–
35.
Christina L. Gagn´e and Thomas L. Spalding. 2011. In-
ferential Processing and Meta-Knowledge as the Bases
for Property Inclusion in Combined Concepts. Journal
of Memory and Language, 65:176–192.
Zellig Harris. 1968. Distributional Structure. In Jerold J.
Katz, editor, The Philosophy of Linguistics, Oxford
Readings in Philosophy, pages 26–47. Oxford Univer-
sity Press.
Verena Klos. 2011. Komposition und Kompositionalit¨at.
Number 292 in Reihe Germanistische Linguistik. Wal-
ter de Gruyter, Berlin.
Natalie K¨uhner and Sabine Schulte im Walde. 2010. De-
termining the Degree of Compositionality of German
Particle Verbs by Clustering Approaches. In Proceed-
ings of the 10th Conference on Natural Language Pro-
cessing, pages 47–56, Saarbr¨ucken, Germany.
Rochelle Lieber and Pavol Stekauer. 2009a. Intro-
duction: Status and Definition of Compounding. In
The Oxford Handbook on Compounding (Lieber and
Stekauer, 2009b), chapter 1, pages 3–18.
Rochelle Lieber and Pavol Stekauer, editors. 2009b. The
Oxford Handbook of Compounding. Oxford Univer-
sity Press.
Kevin Lund and Curt Burgess. 1996. Producing
High-Dimensional Semantic Spaces from Lexical Co-
Occurrence. Behavior Research Methods, Instru-
ments, and Computers, 28(2):203–208.
Diana McCarthy, Bill Keller, and John Carroll. 2003.
Detecting a Continuum of Compositionality in Phrasal
Verbs. In Proceedings of the ACL-SIGLEX Workshop
on Multiword Expressions: Analysis, Acquisition and
Treatment, Sapporo, Japan.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in Distributional Models of Semantics. Cognitive Sci-
ence, 34:1388–1429.
Siva Reddy, Ioannis P. Klapaftis, Diana McCarthy, and
Suresh Manandhar. 2011a. Dynamic and Static Pro-
totype Vectors for Semantic Composition. In Pro-
ceedings of the 5th International Joint Conference on
</reference>
<page confidence="0.97686">
264
</page>
<reference confidence="0.999640744186047">
Natural Language Processing, pages 705–713, Chiang
Mai, Thailand.
Siva Reddy, Diana McCarthy, and Suresh Manandhar.
2011b. An Empirical Study on Compositionality in
Compound Nouns. In Proceedings of the 5th Interna-
tional Joint Conference on Natural Language Process-
ing, pages 210–218, Chiang Mai, Thailand.
Stephen Roller, Sabine Schulte im Walde, and Silke
Scheible. 2013. The (Un)expected Effects of Apply-
ing Standard Cleansing Models to Human Ratings on
Compositionality. In Proceedings of the 9th Workshop
on Multiword Expressions, Atlanta, GA.
Michael Schiehlen. 2003. A Cascaded Finite-State
Parser for German. In Proceedings of the 10th Con-
ference of the European Chapter of the Association for
Computational Linguistics, pages 163–166, Budapest,
Hungary.
Sabine Schulte im Walde. 2005. Exploring Features to
Identify Semantic Nearest Neighbours: A Case Study
on German Particle Verbs. In Proceedings of the In-
ternational Conference on RecentAdvances in Natural
Language Processing, pages 608–614, Borovets, Bul-
garia.
Hinrich Sch¨utze. 1992. Dimensions of Meaning. In Pro-
ceedings of Supercomputing, pages 787–796.
Sidney Siegel and N. John Castellan. 1988. Non-
parametric Statistics for the Behavioral Sciences.
McGraw-Hill, Boston, MA.
Peter D. Turney and Patrick Pantel. 2010. From Fre-
quency to Meaning: Vector Space Models of Se-
mantics. Journal of Artificial Intelligence Research,
37:141–188.
Claudia von der Heide and Susanne Borgwaldt. 2009.
Assoziationen zu Unter-, Basis- und Oberbegriffen.
Eine explorative Studie. In Proceedings of the 9th
Norddeutsches Linguistisches Kolloquium, pages 51–
74.
Dominic Widdows. 2008. Semantic Vector Products:
Some Initial Investigations. In Proceedings of the 2nd
Conference on Quantum Interaction, Oxford, UK.
Heike Zinsmeister and Ulrich Heid. 2004. Collocations
of Complex Nouns: Evidence for Lexicalisation. In
Proceedings of Konvens, Vienna, Austria.
</reference>
<page confidence="0.998462">
265
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.309020">
<title confidence="0.8775515">Exploring Vector Space Models to Predict the Compositionality German Noun-Noun Compounds</title>
<author confidence="0.98916">Sabine Schulte im Walde</author>
<author confidence="0.98916">Stefan M¨uller</author>
<author confidence="0.98916">Stephen</author>
<affiliation confidence="0.930126">Institut f¨ur Maschinelle Universit¨at</affiliation>
<address confidence="0.423408">Pfaffenwaldring 5b, 70569 Stuttgart,</address>
<abstract confidence="0.998846157894737">This paper explores two hypotheses regarding vector space models that predict the compositionality of German noun-noun compounds: (1) Against our intuition, we demonstrate that window-based rather than syntax-based distributional features perform better predictions, and that not adjectives or verbs but nouns represent the most salient part-of-speech. Our overall best result is state-of-the-art, reach- Spearman’s a wordspace model of nominal features from a 20word window of a 1.5 billion word web corpus. (2) While there are no significant differences in predicting compound–modifier vs. compound–head ratings on compositionality, we show that the modifier (rather than the head) properties predominantly influence the degree of compositionality of the compound.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Enrique Alfonseca</author>
<author>Keith Hall</author>
<author>Jana Kravalova</author>
<author>Marius Pasca</author>
<author>Aitor Soroa</author>
</authors>
<title>A Study on Similarity and Relatedness Using Distributional and WordNet-based Approaches.</title>
<date>2009</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics and Human Language Technologies Conference,</booktitle>
<pages>19--27</pages>
<location>Boulder, Colorado.</location>
<contexts>
<context position="3414" citStr="Agirre et al. (2009)" startWordPosition="504" endWordPosition="507"> vector space models are all nouns (i.e., the compound nouns, the modifier nouns, and the head nouns), our hypothesis is that adjectives and verbs are expected to provide salient distributional properties, as adjective/verb meaning and noun meaning are in a strong interdependent relationship. Even more, we expect adjectives and verbs that are syntactically bound to the nouns under consideration (syntax-based, i.e., attributive adjectives and subcategorising verbs) to outperform those that “just” appear in the window contexts of the nouns (window-based). In order to investigate this first 1See Agirre et al. (2009) and Bullinaria and Levy (2007; 2012), among others, for systematic comparisons of cooccurrence features on various semantic relatedness tasks. 255 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task, pages 255–265, Atlanta, Georgia, June 13-14, 2013. c�2013 Association for Computational Linguistics hypothesis, we compare window-based and syntaxbased distributional features across parts-of-speech. Concerning a more specific aspect of compound meaning, we are interested in the contributions of the modifier noun </context>
<context position="33274" citStr="Agirre et al. (2009)" startWordPosition="5159" endWordPosition="5162">redict the compositionality of English noun-noun compounds. Their gold standard also comprised compound–constituent ratings as well as compound whole ratings, but the resources had been cleaned more extensively, and they reached p = 0.714. Concerning vector space explorations and semantic relatedness in more general terms, Bullinaria and Levy (2007; 2012) also systematically assessed a range of factors in VSMs (corpus type and size, window size, association measures, and corpus preprocessing, among others) against four semantic tasks, however not including compositionality ratings. Similarly, Agirre et al. (2009) compared and combined a WordNet-based and various distributional models to predict the pair similarity of the 65 Rubenstein and Goodenough word pairs and the 353 word pairs in WordSim353. They varied window sizes, dependency relations and raw words in the models. On WordSim353, they reached p = 0.66, which is slightly better than our best result, but at the same time the dataset is smaller. Concerning computational models of German compounds, there is not much previous work. Our own work (Schulte im Walde, 2005; K¨uhner and Schulte im Walde, 2010) has addressed the degrees of compositionality</context>
</contexts>
<marker>Agirre, Alfonseca, Hall, Kravalova, Pasca, Soroa, 2009</marker>
<rawString>Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kravalova, Marius Pasca, and Aitor Soroa. 2009. A Study on Similarity and Relatedness Using Distributional and WordNet-based Approaches. In Proceedings of the North American Chapter of the Association for Computational Linguistics and Human Language Technologies Conference, pages 19–27, Boulder, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Collin Bannard</author>
</authors>
<title>Learning about the Meaning of Verb–Particle Constructions from Corpora. Computer Speech and Language,</title>
<date>2005</date>
<contexts>
<context position="32366" citStr="Bannard, 2005" startWordPosition="5027" endWordPosition="5028">the semantic relation between the modifier and the head (Gagn´e and Spalding, 2009) and the modifier properties, inferential processing and world knowledge (Gagn´e and Spalding, 2011) were taken into account. However, also in psycholinguistic studies that explore the semantic role of modifiers and heads in noun compounds there is no agreement about which constituent properties are inherited by the compound. 4 Related Work Most computational approaches to model the meaning or compositionality of compounds have been performed for English, including work on particle verbs (McCarthy et al., 2003; Bannard, 2005; Cook and Stevenson, 2006); adjective-noun combinations (Baroni and Zamparelli, 2010; Boleda et al., 2013); and noun-noun compounds (Reddy et al., 2011b; Reddy et al., 2011a). Most closely related to our work is Reddy et al. (2011b), who relied on window-based distributional models to predict the compositionality of English noun-noun compounds. Their gold standard also comprised compound–constituent ratings as well as compound whole ratings, but the resources had been cleaned more extensively, and they reached p = 0.714. Concerning vector space explorations and semantic relatedness in more ge</context>
</contexts>
<marker>Bannard, 2005</marker>
<rawString>Collin Bannard. 2005. Learning about the Meaning of Verb–Particle Constructions from Corpora. Computer Speech and Language, 19:467–478.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Nouns are Vectors, Adjectives are Matrices: Representing Adjective-Noun Constructions in Semantic Space.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1183--1193</pages>
<location>Cambridge, MA,</location>
<contexts>
<context position="32451" citStr="Baroni and Zamparelli, 2010" startWordPosition="5036" endWordPosition="5039">lding, 2009) and the modifier properties, inferential processing and world knowledge (Gagn´e and Spalding, 2011) were taken into account. However, also in psycholinguistic studies that explore the semantic role of modifiers and heads in noun compounds there is no agreement about which constituent properties are inherited by the compound. 4 Related Work Most computational approaches to model the meaning or compositionality of compounds have been performed for English, including work on particle verbs (McCarthy et al., 2003; Bannard, 2005; Cook and Stevenson, 2006); adjective-noun combinations (Baroni and Zamparelli, 2010; Boleda et al., 2013); and noun-noun compounds (Reddy et al., 2011b; Reddy et al., 2011a). Most closely related to our work is Reddy et al. (2011b), who relied on window-based distributional models to predict the compositionality of English noun-noun compounds. Their gold standard also comprised compound–constituent ratings as well as compound whole ratings, but the resources had been cleaned more extensively, and they reached p = 0.714. Concerning vector space explorations and semantic relatedness in more general terms, Bullinaria and Levy (2007; 2012) also systematically assessed a range of</context>
</contexts>
<marker>Baroni, Zamparelli, 2010</marker>
<rawString>Marco Baroni and Roberto Zamparelli. 2010. Nouns are Vectors, Adjectives are Matrices: Representing Adjective-Noun Constructions in Semantic Space. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1183–1193, Cambridge, MA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
<author>Adriano Ferraresi</author>
<author>Eros Zanchetta</author>
</authors>
<title>The WaCky Wide Web: A Collection of Very Large Linguistically Processed Web-Crawled Corpora. Language Resources and Evaluation,</title>
<date>2009</date>
<pages>43--3</pages>
<contexts>
<context position="16931" citStr="Baroni et al., 2009" startWordPosition="2644" endWordPosition="2648">mong others: the compound–modifier and compound–head ratings were treated as vectors, and the vector features (i.e., the compound–constituent ratings) were added/multiplied to predict the compound whole ratings. As in the related work, the arithmetic operations strengthen the predictions, and multiplication reached an upper bound of p = 0.7829, thus outperforming not only the head-only but also the modifier-only upper bound. 3.2 German Web Corpora Most of our experiments rely on the sdeWaC corpus (Faaß et al., 2010), a cleaned version of the German web corpus deWaC created by the WaCky group (Baroni et al., 2009). The corpus cleaning had focused mainly on removing duplicates from the deWaC, and on disregarding sentences that were syntactically illformed (relying on a parsability index provided by a standard dependency parser (Schiehlen, 2003)). The sdeWaC contains approx. 880 million words and can be downloaded from http://wacky.sslmit. unibo.it/. While the sdeWaC is an attractive corpus choice because it is a web corpus with a reasonable size, and yet has been cleaned and parsed (so that we can induce syntax-based distributional features), it has one serious drawback for a window-based approach (and,</context>
</contexts>
<marker>Baroni, Bernardini, Ferraresi, Zanchetta, 2009</marker>
<rawString>Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The WaCky Wide Web: A Collection of Very Large Linguistically Processed Web-Crawled Corpora. Language Resources and Evaluation, 43(3):209–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gemma Boleda</author>
<author>Marco Baroni</author>
<author>Nghia The Pham</author>
<author>Louise McNally</author>
</authors>
<title>On Adjective-Noun Composition in Distributional Semantics.</title>
<date>2013</date>
<booktitle>In Proceedings of the 10th International Conference on Computational Semantics,</booktitle>
<location>Potsdam, Germany.</location>
<contexts>
<context position="32473" citStr="Boleda et al., 2013" startWordPosition="5040" endWordPosition="5043"> properties, inferential processing and world knowledge (Gagn´e and Spalding, 2011) were taken into account. However, also in psycholinguistic studies that explore the semantic role of modifiers and heads in noun compounds there is no agreement about which constituent properties are inherited by the compound. 4 Related Work Most computational approaches to model the meaning or compositionality of compounds have been performed for English, including work on particle verbs (McCarthy et al., 2003; Bannard, 2005; Cook and Stevenson, 2006); adjective-noun combinations (Baroni and Zamparelli, 2010; Boleda et al., 2013); and noun-noun compounds (Reddy et al., 2011b; Reddy et al., 2011a). Most closely related to our work is Reddy et al. (2011b), who relied on window-based distributional models to predict the compositionality of English noun-noun compounds. Their gold standard also comprised compound–constituent ratings as well as compound whole ratings, but the resources had been cleaned more extensively, and they reached p = 0.714. Concerning vector space explorations and semantic relatedness in more general terms, Bullinaria and Levy (2007; 2012) also systematically assessed a range of factors in VSMs (corp</context>
</contexts>
<marker>Boleda, Baroni, Pham, McNally, 2013</marker>
<rawString>Gemma Boleda, Marco Baroni, Nghia The Pham, and Louise McNally. 2013. On Adjective-Noun Composition in Distributional Semantics. In Proceedings of the 10th International Conference on Computational Semantics, Potsdam, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John A Bullinaria</author>
<author>Joseph P Levy</author>
</authors>
<title>Extracting Semantic Representations from Word Co-Occurrence Statistics: A Computational Study.</title>
<date>2007</date>
<journal>Behavior Research Methods,</journal>
<volume>39</volume>
<issue>3</issue>
<contexts>
<context position="3444" citStr="Bullinaria and Levy (2007" startWordPosition="509" endWordPosition="512">all nouns (i.e., the compound nouns, the modifier nouns, and the head nouns), our hypothesis is that adjectives and verbs are expected to provide salient distributional properties, as adjective/verb meaning and noun meaning are in a strong interdependent relationship. Even more, we expect adjectives and verbs that are syntactically bound to the nouns under consideration (syntax-based, i.e., attributive adjectives and subcategorising verbs) to outperform those that “just” appear in the window contexts of the nouns (window-based). In order to investigate this first 1See Agirre et al. (2009) and Bullinaria and Levy (2007; 2012), among others, for systematic comparisons of cooccurrence features on various semantic relatedness tasks. 255 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task, pages 255–265, Atlanta, Georgia, June 13-14, 2013. c�2013 Association for Computational Linguistics hypothesis, we compare window-based and syntaxbased distributional features across parts-of-speech. Concerning a more specific aspect of compound meaning, we are interested in the contributions of the modifier noun versus head noun properties wi</context>
<context position="33004" citStr="Bullinaria and Levy (2007" startWordPosition="5120" endWordPosition="5123">evenson, 2006); adjective-noun combinations (Baroni and Zamparelli, 2010; Boleda et al., 2013); and noun-noun compounds (Reddy et al., 2011b; Reddy et al., 2011a). Most closely related to our work is Reddy et al. (2011b), who relied on window-based distributional models to predict the compositionality of English noun-noun compounds. Their gold standard also comprised compound–constituent ratings as well as compound whole ratings, but the resources had been cleaned more extensively, and they reached p = 0.714. Concerning vector space explorations and semantic relatedness in more general terms, Bullinaria and Levy (2007; 2012) also systematically assessed a range of factors in VSMs (corpus type and size, window size, association measures, and corpus preprocessing, among others) against four semantic tasks, however not including compositionality ratings. Similarly, Agirre et al. (2009) compared and combined a WordNet-based and various distributional models to predict the pair similarity of the 65 Rubenstein and Goodenough word pairs and the 353 word pairs in WordSim353. They varied window sizes, dependency relations and raw words in the models. On WordSim353, they reached p = 0.66, which is slightly better th</context>
</contexts>
<marker>Bullinaria, Levy, 2007</marker>
<rawString>John A. Bullinaria and Joseph P. Levy. 2007. Extracting Semantic Representations from Word Co-Occurrence Statistics: A Computational Study. Behavior Research Methods, 39(3):510–526.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John A Bullinaria</author>
<author>Joseph P Levy</author>
</authors>
<date>2012</date>
<booktitle>Extracting Semantic Representations from Word Co-Occurrence Statistics: Stop-Lists, Stemming, and SVD. Behavior Research Methods,</booktitle>
<pages>44--890</pages>
<marker>Bullinaria, Levy, 2012</marker>
<rawString>John A. Bullinaria and Joseph P. Levy. 2012. Extracting Semantic Representations from Word Co-Occurrence Statistics: Stop-Lists, Stemming, and SVD. Behavior Research Methods, 44:890–907.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth W Church</author>
<author>Patrick Hanks</author>
</authors>
<date>1990</date>
<journal>Word Association Norms, Mutual Information, and Lexicography. Computational Linguistics,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="12708" citStr="Church and Hanks, 1990" startWordPosition="1985" endWordPosition="1988">erties. In all our vector space experiments, we used cooccurrence frequency counts as induced from German web corpora, and calculated local mutual information (LMI)5 values (Evert, 2005), to instantiate the empirical properties of our target nouns with regard to the various corpus-based features. LMI is a measure from information theory that compares the observed frequencies O with expected frequencies E, taking marginal frequencies into account: LMI = O x log E, with E representing the product of the marginal frequencies over the sample size.6 In comparison to (pointwise) mutual information (Church and Hanks, 1990), LMI improves the problem of propagating low-frequent events, by multiplying mutual information by the observed frequency. Relying on the LMI vector space models, the cosine determined the distributional similarity between the compounds and their constituents, which was in turn used to predict the compositionality between the compound and the constituents, assuming that the stronger the distributional similarity (i.e., the cosine values), the larger the degree of compositionality. 5Alternatively, we also used the raw frequencies in all experiments below. The insights into the various features</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>Kenneth W. Church and Patrick Hanks. 1990. Word Association Norms, Mutual Information, and Lexicography. Computational Linguistics, 16(1):22–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Cook</author>
<author>Suzanne Stevenson</author>
</authors>
<title>Classifying Particle Semantics in English Verb-Particle Constructions.</title>
<date>2006</date>
<booktitle>In Proceedings of the ACL/COLING Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="32393" citStr="Cook and Stevenson, 2006" startWordPosition="5029" endWordPosition="5032">lation between the modifier and the head (Gagn´e and Spalding, 2009) and the modifier properties, inferential processing and world knowledge (Gagn´e and Spalding, 2011) were taken into account. However, also in psycholinguistic studies that explore the semantic role of modifiers and heads in noun compounds there is no agreement about which constituent properties are inherited by the compound. 4 Related Work Most computational approaches to model the meaning or compositionality of compounds have been performed for English, including work on particle verbs (McCarthy et al., 2003; Bannard, 2005; Cook and Stevenson, 2006); adjective-noun combinations (Baroni and Zamparelli, 2010; Boleda et al., 2013); and noun-noun compounds (Reddy et al., 2011b; Reddy et al., 2011a). Most closely related to our work is Reddy et al. (2011b), who relied on window-based distributional models to predict the compositionality of English noun-noun compounds. Their gold standard also comprised compound–constituent ratings as well as compound whole ratings, but the resources had been cleaned more extensively, and they reached p = 0.714. Concerning vector space explorations and semantic relatedness in more general terms, Bullinaria and</context>
</contexts>
<marker>Cook, Stevenson, 2006</marker>
<rawString>Paul Cook and Suzanne Stevenson. 2006. Classifying Particle Semantics in English Verb-Particle Constructions. In Proceedings of the ACL/COLING Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
</authors>
<title>Vector Space Models of Word Meaning and Phrase Meaning: A Survey. Language and Linguistics Compass,</title>
<date>2012</date>
<pages>6--10</pages>
<contexts>
<context position="1355" citStr="Erk (2012)" startWordPosition="190" endWordPosition="191">0.65 with a wordspace model of nominal features from a 20- word window of a 1.5 billion word web corpus. (2) While there are no significant differences in predicting compound–modifier vs. compound–head ratings on compositionality, we show that the modifier (rather than the head) properties predominantly influence the degree of compositionality of the compound. 1 Introduction Vector space models and distributional information have been a steadily increasing, integral part of lexical semantic research over the past 20 years. On the one hand, vector space models (see Turney and Pantel (2010) and Erk (2012) for two recent surveys) have been exploited in psycholinguistic (Lund and Burgess, 1996) and computational linguistic research (Sch¨utze, 1992) to explore the notion of “similarity” between a set of target objects within a geometric setting. On the other hand, the distributional hypothesis (Firth, 1957; Harris, 1968) has been exploited to determine co-occurrence features for vector space models that best describe the words, phrases, sentences, etc. of interest. While the emergence of vector space models is increasingly pervasive within data-intensive lexical semantics, and even though useful </context>
</contexts>
<marker>Erk, 2012</marker>
<rawString>Katrin Erk. 2012. Vector Space Models of Word Meaning and Phrase Meaning: A Survey. Language and Linguistics Compass, 6(10):635–653.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Evert</author>
</authors>
<title>The Statistics of Word CoOccurrences: Word Pairs and Collocations.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>Institut f¨ur Maschinelle Sprachverarbeitung, Universit¨at Stuttgart.</institution>
<contexts>
<context position="12271" citStr="Evert, 2005" startWordPosition="1918" endWordPosition="1919">w) do. This intuition will be confirmed in Section 3.1. 4The illustration idea was taken from Reddy et al. (2011b). 257 Figure 2: Compounds ratings sorted by whole ratings. 3 Vector Space Models (VSMs) The goal of our vector space models is to identify distributional features that are salient to predict the degree of compositionality of the compounds, by relying on the similarities between the compound and constituent properties. In all our vector space experiments, we used cooccurrence frequency counts as induced from German web corpora, and calculated local mutual information (LMI)5 values (Evert, 2005), to instantiate the empirical properties of our target nouns with regard to the various corpus-based features. LMI is a measure from information theory that compares the observed frequencies O with expected frequencies E, taking marginal frequencies into account: LMI = O x log E, with E representing the product of the marginal frequencies over the sample size.6 In comparison to (pointwise) mutual information (Church and Hanks, 1990), LMI improves the problem of propagating low-frequent events, by multiplying mutual information by the observed frequency. Relying on the LMI vector space models,</context>
</contexts>
<marker>Evert, 2005</marker>
<rawString>Stefan Evert. 2005. The Statistics of Word CoOccurrences: Word Pairs and Collocations. Ph.D. thesis, Institut f¨ur Maschinelle Sprachverarbeitung, Universit¨at Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gertrud Faaß Ulrich Heid</author>
<author>Helmut Schmid</author>
</authors>
<title>Design and Application of a Gold Standard for Morphological Analysis: SMOR in Validation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 7th International Conference on Language Resources and Evaluation,</booktitle>
<pages>803--810</pages>
<location>Valletta,</location>
<marker>Heid, Schmid, 2010</marker>
<rawString>Gertrud Faaß Ulrich Heid, and Helmut Schmid. 2010. Design and Application of a Gold Standard for Morphological Analysis: SMOR in Validation. In Proceedings of the 7th International Conference on Language Resources and Evaluation, pages 803–810, Valletta, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John R Firth</author>
</authors>
<date>1957</date>
<booktitle>Papers in Linguistics 1934-51.</booktitle>
<location>Longmans, London, UK.</location>
<contexts>
<context position="1659" citStr="Firth, 1957" startWordPosition="239" endWordPosition="240">y influence the degree of compositionality of the compound. 1 Introduction Vector space models and distributional information have been a steadily increasing, integral part of lexical semantic research over the past 20 years. On the one hand, vector space models (see Turney and Pantel (2010) and Erk (2012) for two recent surveys) have been exploited in psycholinguistic (Lund and Burgess, 1996) and computational linguistic research (Sch¨utze, 1992) to explore the notion of “similarity” between a set of target objects within a geometric setting. On the other hand, the distributional hypothesis (Firth, 1957; Harris, 1968) has been exploited to determine co-occurrence features for vector space models that best describe the words, phrases, sentences, etc. of interest. While the emergence of vector space models is increasingly pervasive within data-intensive lexical semantics, and even though useful features have been identified in general terms:1 when it comes to a specific semantic phenomenon, we need to explore the relevant distributional features in order to investigate the respective phenomenon. Our research is interested in the meaning of German compounds. More specifically, we aim to predict</context>
</contexts>
<marker>Firth, 1957</marker>
<rawString>John R. Firth. 1957. Papers in Linguistics 1934-51. Longmans, London, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Fleischer</author>
<author>Irmhild Barz</author>
</authors>
<title>Wortbildung der deutschen Gegenwartssprache. de Gruyter.</title>
<date>2012</date>
<contexts>
<context position="6281" citStr="Fleischer and Barz (2012)" startWordPosition="922" endWordPosition="925">d in the last decade also within computational linguistics. Recent evidence of this strong interest are the Handbook of Compounding (Lieber and Stekauer, 2009b) on theoretical perspectives, and a series of workshops2 and special journal issues with respect to multi-word expressions (including various types 2www.multiword.sourceforge.net of compounds) and the computational perspective (Journal of Computer Speech and Language, 2005; Language Resources and Evaluation, 2010; ACM Transactions on Speech and Language Processing, to appear). Our focus of interest is on German noun-noun compounds (see Fleischer and Barz (2012) for a detailed overview and Klos (2011) for a recent detailed exploration), such as Ahornblatt ‘maple leaf’, Feuerwerk ‘fireworks’, and Obstkuchen ‘fruit cake’ where both the grammatical head (in German, this is the rightmost constituent) and the modifier are nouns. More specifically, we are interested in the degrees of compositionality of German noun-noun compounds, i.e., the semantic relatedness between the meaning of a compound (e.g., Feuerwerk) and the meanings of its constituents (e.g., Feuer ‘fire’ and Werk ‘opus’). Our work is based on a selection of noun compounds by von der Heide and</context>
</contexts>
<marker>Fleischer, Barz, 2012</marker>
<rawString>Wolfgang Fleischer and Irmhild Barz. 2012. Wortbildung der deutschen Gegenwartssprache. de Gruyter.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christina L Gagn´e</author>
<author>Thomas L Spalding</author>
</authors>
<title>Constituent Integration during the Processing of Compound Words: Does it involve the Use of Relational Structures? Journal ofMemory and Language,</title>
<date>2009</date>
<pages>60--20</pages>
<marker>Gagn´e, Spalding, 2009</marker>
<rawString>Christina L. Gagn´e and Thomas L. Spalding. 2009. Constituent Integration during the Processing of Compound Words: Does it involve the Use of Relational Structures? Journal ofMemory and Language, 60:20– 35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christina L Gagn´e</author>
<author>Thomas L Spalding</author>
</authors>
<title>Inferential Processing and Meta-Knowledge as the Bases for Property Inclusion in Combined Concepts.</title>
<date>2011</date>
<journal>Journal of Memory and Language,</journal>
<pages>65--176</pages>
<marker>Gagn´e, Spalding, 2011</marker>
<rawString>Christina L. Gagn´e and Thomas L. Spalding. 2011. Inferential Processing and Meta-Knowledge as the Bases for Property Inclusion in Combined Concepts. Journal of Memory and Language, 65:176–192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig Harris</author>
</authors>
<title>Distributional Structure. In</title>
<date>1968</date>
<booktitle>The Philosophy of Linguistics, Oxford Readings in Philosophy,</booktitle>
<pages>26--47</pages>
<editor>Jerold J. Katz, editor,</editor>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="1674" citStr="Harris, 1968" startWordPosition="241" endWordPosition="242">he degree of compositionality of the compound. 1 Introduction Vector space models and distributional information have been a steadily increasing, integral part of lexical semantic research over the past 20 years. On the one hand, vector space models (see Turney and Pantel (2010) and Erk (2012) for two recent surveys) have been exploited in psycholinguistic (Lund and Burgess, 1996) and computational linguistic research (Sch¨utze, 1992) to explore the notion of “similarity” between a set of target objects within a geometric setting. On the other hand, the distributional hypothesis (Firth, 1957; Harris, 1968) has been exploited to determine co-occurrence features for vector space models that best describe the words, phrases, sentences, etc. of interest. While the emergence of vector space models is increasingly pervasive within data-intensive lexical semantics, and even though useful features have been identified in general terms:1 when it comes to a specific semantic phenomenon, we need to explore the relevant distributional features in order to investigate the respective phenomenon. Our research is interested in the meaning of German compounds. More specifically, we aim to predict the degrees of</context>
</contexts>
<marker>Harris, 1968</marker>
<rawString>Zellig Harris. 1968. Distributional Structure. In Jerold J. Katz, editor, The Philosophy of Linguistics, Oxford Readings in Philosophy, pages 26–47. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Verena Klos</author>
</authors>
<title>Komposition und Kompositionalit¨at.</title>
<date>2011</date>
<journal>Number</journal>
<booktitle>in Reihe Germanistische Linguistik. Walter de Gruyter,</booktitle>
<volume>292</volume>
<location>Berlin.</location>
<contexts>
<context position="6321" citStr="Klos (2011)" startWordPosition="932" endWordPosition="933">tics. Recent evidence of this strong interest are the Handbook of Compounding (Lieber and Stekauer, 2009b) on theoretical perspectives, and a series of workshops2 and special journal issues with respect to multi-word expressions (including various types 2www.multiword.sourceforge.net of compounds) and the computational perspective (Journal of Computer Speech and Language, 2005; Language Resources and Evaluation, 2010; ACM Transactions on Speech and Language Processing, to appear). Our focus of interest is on German noun-noun compounds (see Fleischer and Barz (2012) for a detailed overview and Klos (2011) for a recent detailed exploration), such as Ahornblatt ‘maple leaf’, Feuerwerk ‘fireworks’, and Obstkuchen ‘fruit cake’ where both the grammatical head (in German, this is the rightmost constituent) and the modifier are nouns. More specifically, we are interested in the degrees of compositionality of German noun-noun compounds, i.e., the semantic relatedness between the meaning of a compound (e.g., Feuerwerk) and the meanings of its constituents (e.g., Feuer ‘fire’ and Werk ‘opus’). Our work is based on a selection of noun compounds by von der Heide and Borgwaldt (2009), who created a set of </context>
</contexts>
<marker>Klos, 2011</marker>
<rawString>Verena Klos. 2011. Komposition und Kompositionalit¨at. Number 292 in Reihe Germanistische Linguistik. Walter de Gruyter, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Natalie K¨uhner</author>
<author>Sabine Schulte im Walde</author>
</authors>
<title>Determining the Degree of Compositionality of German Particle Verbs by Clustering Approaches.</title>
<date>2010</date>
<booktitle>In Proceedings of the 10th Conference on Natural Language Processing,</booktitle>
<pages>47--56</pages>
<location>Saarbr¨ucken, Germany.</location>
<marker>K¨uhner, Walde, 2010</marker>
<rawString>Natalie K¨uhner and Sabine Schulte im Walde. 2010. Determining the Degree of Compositionality of German Particle Verbs by Clustering Approaches. In Proceedings of the 10th Conference on Natural Language Processing, pages 47–56, Saarbr¨ucken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rochelle Lieber</author>
<author>Pavol Stekauer</author>
</authors>
<title>Introduction: Status and Definition of Compounding.</title>
<date>2009</date>
<booktitle>In The Oxford Handbook on Compounding (Lieber and Stekauer, 2009b), chapter 1,</booktitle>
<pages>3--18</pages>
<contexts>
<context position="5441" citStr="Lieber and Stekauer (2009" startWordPosition="805" endWordPosition="808">lows. Section 2 introduces the compound data that is relevant for this paper, i.e., the noun-noun compounds and the compositionality ratings. Section 3 performs and discusses the vector space experiments to explore our hypotheses, and Section 4 describes related work. 2 Data 2.1 German Noun-Noun Compounds Compounds are combinations of two or more simplex words. Traditionally, a number of criteria (such as compounds being syntactically inseparable, and that compounds have a specific stress pattern) have been proposed, in order to establish a border between compounds and non-compounds. However, Lieber and Stekauer (2009a) demonstrated that none of these tests are universally reliable to distinguish compounds from other types of derived words. Compounds have thus been a recurrent focus of attention within theoretical, cognitive, and in the last decade also within computational linguistics. Recent evidence of this strong interest are the Handbook of Compounding (Lieber and Stekauer, 2009b) on theoretical perspectives, and a series of workshops2 and special journal issues with respect to multi-word expressions (including various types 2www.multiword.sourceforge.net of compounds) and the computational perspectiv</context>
</contexts>
<marker>Lieber, Stekauer, 2009</marker>
<rawString>Rochelle Lieber and Pavol Stekauer. 2009a. Introduction: Status and Definition of Compounding. In The Oxford Handbook on Compounding (Lieber and Stekauer, 2009b), chapter 1, pages 3–18.</rawString>
</citation>
<citation valid="false">
<booktitle>2009b. The Oxford Handbook of Compounding.</booktitle>
<editor>Rochelle Lieber and Pavol Stekauer, editors.</editor>
<publisher>Oxford University Press.</publisher>
<marker></marker>
<rawString>Rochelle Lieber and Pavol Stekauer, editors. 2009b. The Oxford Handbook of Compounding. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Lund</author>
<author>Curt Burgess</author>
</authors>
<date>1996</date>
<booktitle>Producing High-Dimensional Semantic Spaces from Lexical CoOccurrence. Behavior Research Methods, Instruments, and Computers,</booktitle>
<volume>28</volume>
<issue>2</issue>
<contexts>
<context position="1444" citStr="Lund and Burgess, 1996" startWordPosition="202" endWordPosition="205">1.5 billion word web corpus. (2) While there are no significant differences in predicting compound–modifier vs. compound–head ratings on compositionality, we show that the modifier (rather than the head) properties predominantly influence the degree of compositionality of the compound. 1 Introduction Vector space models and distributional information have been a steadily increasing, integral part of lexical semantic research over the past 20 years. On the one hand, vector space models (see Turney and Pantel (2010) and Erk (2012) for two recent surveys) have been exploited in psycholinguistic (Lund and Burgess, 1996) and computational linguistic research (Sch¨utze, 1992) to explore the notion of “similarity” between a set of target objects within a geometric setting. On the other hand, the distributional hypothesis (Firth, 1957; Harris, 1968) has been exploited to determine co-occurrence features for vector space models that best describe the words, phrases, sentences, etc. of interest. While the emergence of vector space models is increasingly pervasive within data-intensive lexical semantics, and even though useful features have been identified in general terms:1 when it comes to a specific semantic phe</context>
</contexts>
<marker>Lund, Burgess, 1996</marker>
<rawString>Kevin Lund and Curt Burgess. 1996. Producing High-Dimensional Semantic Spaces from Lexical CoOccurrence. Behavior Research Methods, Instruments, and Computers, 28(2):203–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Bill Keller</author>
<author>John Carroll</author>
</authors>
<title>Detecting a Continuum of Compositionality in Phrasal Verbs.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL-SIGLEX Workshop on Multiword Expressions: Analysis, Acquisition and Treatment,</booktitle>
<location>Sapporo, Japan.</location>
<contexts>
<context position="32351" citStr="McCarthy et al., 2003" startWordPosition="5023" endWordPosition="5026">arious factors such as the semantic relation between the modifier and the head (Gagn´e and Spalding, 2009) and the modifier properties, inferential processing and world knowledge (Gagn´e and Spalding, 2011) were taken into account. However, also in psycholinguistic studies that explore the semantic role of modifiers and heads in noun compounds there is no agreement about which constituent properties are inherited by the compound. 4 Related Work Most computational approaches to model the meaning or compositionality of compounds have been performed for English, including work on particle verbs (McCarthy et al., 2003; Bannard, 2005; Cook and Stevenson, 2006); adjective-noun combinations (Baroni and Zamparelli, 2010; Boleda et al., 2013); and noun-noun compounds (Reddy et al., 2011b; Reddy et al., 2011a). Most closely related to our work is Reddy et al. (2011b), who relied on window-based distributional models to predict the compositionality of English noun-noun compounds. Their gold standard also comprised compound–constituent ratings as well as compound whole ratings, but the resources had been cleaned more extensively, and they reached p = 0.714. Concerning vector space explorations and semantic related</context>
</contexts>
<marker>McCarthy, Keller, Carroll, 2003</marker>
<rawString>Diana McCarthy, Bill Keller, and John Carroll. 2003. Detecting a Continuum of Compositionality in Phrasal Verbs. In Proceedings of the ACL-SIGLEX Workshop on Multiword Expressions: Analysis, Acquisition and Treatment, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<date>2010</date>
<booktitle>Composition in Distributional Models of Semantics. Cognitive Science,</booktitle>
<pages>34--1388</pages>
<contexts>
<context position="16308" citStr="Mitchell and Lapata (2010)" startWordPosition="2545" endWordPosition="2548">tings is not even moderate 258 Function � Baseline Upper Bound modifier only .0959 .6002 head only .1019 .1385 addition .1168 .7687 multiplication .1079 .7829 Table 2: Baseline/Upper bound p correlations. (p = 0.1385). Obviously, the semantics of the modifiers had a much stronger impact on the semantic judgements of the compounds, thus confirming our intuition from Section 2.2. The lower part of the table shows the respective baseline and upper bound values when the compound–modifier ratings and the compound– head ratings were combined by standard arithmetic operations, cf. Widdows (2008) and Mitchell and Lapata (2010), among others: the compound–modifier and compound–head ratings were treated as vectors, and the vector features (i.e., the compound–constituent ratings) were added/multiplied to predict the compound whole ratings. As in the related work, the arithmetic operations strengthen the predictions, and multiplication reached an upper bound of p = 0.7829, thus outperforming not only the head-only but also the modifier-only upper bound. 3.2 German Web Corpora Most of our experiments rely on the sdeWaC corpus (Faaß et al., 2010), a cleaned version of the German web corpus deWaC created by the WaCky grou</context>
</contexts>
<marker>Mitchell, Lapata, 2010</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2010. Composition in Distributional Models of Semantics. Cognitive Science, 34:1388–1429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siva Reddy</author>
<author>Ioannis P Klapaftis</author>
<author>Diana McCarthy</author>
<author>Suresh Manandhar</author>
</authors>
<title>Dynamic and Static Prototype Vectors for Semantic Composition.</title>
<date>2011</date>
<booktitle>In Proceedings of the 5th International Joint Conference on Natural Language Processing,</booktitle>
<pages>705--713</pages>
<location>Chiang Mai, Thailand.</location>
<contexts>
<context position="11771" citStr="Reddy et al. (2011" startWordPosition="1837" endWordPosition="1840">r our 244 noun-noun compounds are not particularly skewed to any area within the range.4 Figure 2 again shows the mean ratings for the compounds as a whole as well as for the compound– constituent pairs, but in this case only the compound whole ratings were sorted, and the compound– constituent ratings were plotted against the compound whole ratings. According to the plot, the compound–modifier ratings (red) seem to correlate better with the compound whole ratings than the compound–head ratings (yellow) do. This intuition will be confirmed in Section 3.1. 4The illustration idea was taken from Reddy et al. (2011b). 257 Figure 2: Compounds ratings sorted by whole ratings. 3 Vector Space Models (VSMs) The goal of our vector space models is to identify distributional features that are salient to predict the degree of compositionality of the compounds, by relying on the similarities between the compound and constituent properties. In all our vector space experiments, we used cooccurrence frequency counts as induced from German web corpora, and calculated local mutual information (LMI)5 values (Evert, 2005), to instantiate the empirical properties of our target nouns with regard to the various corpus-base</context>
<context position="14813" citStr="Reddy et al. (2011" startWordPosition="2310" endWordPosition="2313">ing a baseline and an upper bound for our vector space experiments in Section 3.1 as well as our web corpora in Section 3.2, Section 3.3 presents window-based in comparison to syntaxbased vector space models (distinguishing various part-of-speech features). In Section 3.4 we then focus on the contribution of modifiers vs. heads in the vector space models, with regard to the overall most successful features. 3.1 Baseline and Upper Bound Table 2 presents the baseline and the upper bound values for the vector space experiments. The baseline in the first two lines follows a procedure performed by Reddy et al. (2011b), and relies on a random assignment of rating values [1, 7] to the compound–modifier and the compound– head pairs. The 244 random values for the compound–constituent pairs were then each correlated against the compound whole ratings. The random compound–modifier ratings show a baseline correlation of p = 0.0959 with the compound whole ratings, and the random compound–head ratings show a baseline correlation of p = 0.1019 with the compound whole ratings. The upper bound in the first two lines shows the correlations between the human ratings from the two experiments, i.e., between the 244 comp</context>
<context position="32518" citStr="Reddy et al., 2011" startWordPosition="5047" endWordPosition="5050">nowledge (Gagn´e and Spalding, 2011) were taken into account. However, also in psycholinguistic studies that explore the semantic role of modifiers and heads in noun compounds there is no agreement about which constituent properties are inherited by the compound. 4 Related Work Most computational approaches to model the meaning or compositionality of compounds have been performed for English, including work on particle verbs (McCarthy et al., 2003; Bannard, 2005; Cook and Stevenson, 2006); adjective-noun combinations (Baroni and Zamparelli, 2010; Boleda et al., 2013); and noun-noun compounds (Reddy et al., 2011b; Reddy et al., 2011a). Most closely related to our work is Reddy et al. (2011b), who relied on window-based distributional models to predict the compositionality of English noun-noun compounds. Their gold standard also comprised compound–constituent ratings as well as compound whole ratings, but the resources had been cleaned more extensively, and they reached p = 0.714. Concerning vector space explorations and semantic relatedness in more general terms, Bullinaria and Levy (2007; 2012) also systematically assessed a range of factors in VSMs (corpus type and size, window size, association me</context>
</contexts>
<marker>Reddy, Klapaftis, McCarthy, Manandhar, 2011</marker>
<rawString>Siva Reddy, Ioannis P. Klapaftis, Diana McCarthy, and Suresh Manandhar. 2011a. Dynamic and Static Prototype Vectors for Semantic Composition. In Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 705–713, Chiang Mai, Thailand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siva Reddy</author>
<author>Diana McCarthy</author>
<author>Suresh Manandhar</author>
</authors>
<title>An Empirical Study on Compositionality in Compound Nouns.</title>
<date>2011</date>
<booktitle>In Proceedings of the 5th International Joint Conference on Natural Language Processing,</booktitle>
<pages>210--218</pages>
<location>Chiang Mai, Thailand.</location>
<contexts>
<context position="11771" citStr="Reddy et al. (2011" startWordPosition="1837" endWordPosition="1840">r our 244 noun-noun compounds are not particularly skewed to any area within the range.4 Figure 2 again shows the mean ratings for the compounds as a whole as well as for the compound– constituent pairs, but in this case only the compound whole ratings were sorted, and the compound– constituent ratings were plotted against the compound whole ratings. According to the plot, the compound–modifier ratings (red) seem to correlate better with the compound whole ratings than the compound–head ratings (yellow) do. This intuition will be confirmed in Section 3.1. 4The illustration idea was taken from Reddy et al. (2011b). 257 Figure 2: Compounds ratings sorted by whole ratings. 3 Vector Space Models (VSMs) The goal of our vector space models is to identify distributional features that are salient to predict the degree of compositionality of the compounds, by relying on the similarities between the compound and constituent properties. In all our vector space experiments, we used cooccurrence frequency counts as induced from German web corpora, and calculated local mutual information (LMI)5 values (Evert, 2005), to instantiate the empirical properties of our target nouns with regard to the various corpus-base</context>
<context position="14813" citStr="Reddy et al. (2011" startWordPosition="2310" endWordPosition="2313">ing a baseline and an upper bound for our vector space experiments in Section 3.1 as well as our web corpora in Section 3.2, Section 3.3 presents window-based in comparison to syntaxbased vector space models (distinguishing various part-of-speech features). In Section 3.4 we then focus on the contribution of modifiers vs. heads in the vector space models, with regard to the overall most successful features. 3.1 Baseline and Upper Bound Table 2 presents the baseline and the upper bound values for the vector space experiments. The baseline in the first two lines follows a procedure performed by Reddy et al. (2011b), and relies on a random assignment of rating values [1, 7] to the compound–modifier and the compound– head pairs. The 244 random values for the compound–constituent pairs were then each correlated against the compound whole ratings. The random compound–modifier ratings show a baseline correlation of p = 0.0959 with the compound whole ratings, and the random compound–head ratings show a baseline correlation of p = 0.1019 with the compound whole ratings. The upper bound in the first two lines shows the correlations between the human ratings from the two experiments, i.e., between the 244 comp</context>
<context position="32518" citStr="Reddy et al., 2011" startWordPosition="5047" endWordPosition="5050">nowledge (Gagn´e and Spalding, 2011) were taken into account. However, also in psycholinguistic studies that explore the semantic role of modifiers and heads in noun compounds there is no agreement about which constituent properties are inherited by the compound. 4 Related Work Most computational approaches to model the meaning or compositionality of compounds have been performed for English, including work on particle verbs (McCarthy et al., 2003; Bannard, 2005; Cook and Stevenson, 2006); adjective-noun combinations (Baroni and Zamparelli, 2010; Boleda et al., 2013); and noun-noun compounds (Reddy et al., 2011b; Reddy et al., 2011a). Most closely related to our work is Reddy et al. (2011b), who relied on window-based distributional models to predict the compositionality of English noun-noun compounds. Their gold standard also comprised compound–constituent ratings as well as compound whole ratings, but the resources had been cleaned more extensively, and they reached p = 0.714. Concerning vector space explorations and semantic relatedness in more general terms, Bullinaria and Levy (2007; 2012) also systematically assessed a range of factors in VSMs (corpus type and size, window size, association me</context>
</contexts>
<marker>Reddy, McCarthy, Manandhar, 2011</marker>
<rawString>Siva Reddy, Diana McCarthy, and Suresh Manandhar. 2011b. An Empirical Study on Compositionality in Compound Nouns. In Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 210–218, Chiang Mai, Thailand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Roller</author>
<author>Sabine Schulte im Walde</author>
<author>Silke Scheible</author>
</authors>
<title>The (Un)expected Effects of Applying Standard Cleansing Models to Human Ratings on Compositionality.</title>
<date>2013</date>
<booktitle>In Proceedings of the 9th Workshop on Multiword Expressions,</booktitle>
<location>Atlanta, GA.</location>
<contexts>
<context position="10668" citStr="Roller et al. (2013)" startWordPosition="1653" endWordPosition="1656">d. For each of the compounds we calculated the rating mean and the standard deviation. We refer to this second set as our compound whole ratings. Table 1 presents example mean ratings for the compound–constituent ratings as well as for the compound whole ratings, accompanied by the standard deviations. We selected two examples each for five categories of mean ratings: the compound– constituent ratings were (1) high or (2) mid or (3) low with regard to both constituents; the compound– constituent ratings were (4) low with regard to the modifier but high with regard to the head; (5) vice versa. Roller et al. (2013) performed a thorough 3www.mturk.com Figure 1: Distribution of compound ratings. analysis of the two sets of ratings, and assessed their reliability from several perspectives. Figure 1 shows how the mean ratings for the compounds as a whole, for the compound–modifier pairs as well as for the compound–head pairs are distributed over the range [1, 7]: For each set, we independently sorted the 244 values and plotted them. The purpose of the figure is to illustrate that the ratings for our 244 noun-noun compounds are not particularly skewed to any area within the range.4 Figure 2 again shows the m</context>
</contexts>
<marker>Roller, Walde, Scheible, 2013</marker>
<rawString>Stephen Roller, Sabine Schulte im Walde, and Silke Scheible. 2013. The (Un)expected Effects of Applying Standard Cleansing Models to Human Ratings on Compositionality. In Proceedings of the 9th Workshop on Multiword Expressions, Atlanta, GA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Schiehlen</author>
</authors>
<title>A Cascaded Finite-State Parser for German.</title>
<date>2003</date>
<booktitle>In Proceedings of the 10th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>163--166</pages>
<location>Budapest, Hungary.</location>
<contexts>
<context position="17165" citStr="Schiehlen, 2003" startWordPosition="2681" endWordPosition="2682">e arithmetic operations strengthen the predictions, and multiplication reached an upper bound of p = 0.7829, thus outperforming not only the head-only but also the modifier-only upper bound. 3.2 German Web Corpora Most of our experiments rely on the sdeWaC corpus (Faaß et al., 2010), a cleaned version of the German web corpus deWaC created by the WaCky group (Baroni et al., 2009). The corpus cleaning had focused mainly on removing duplicates from the deWaC, and on disregarding sentences that were syntactically illformed (relying on a parsability index provided by a standard dependency parser (Schiehlen, 2003)). The sdeWaC contains approx. 880 million words and can be downloaded from http://wacky.sslmit. unibo.it/. While the sdeWaC is an attractive corpus choice because it is a web corpus with a reasonable size, and yet has been cleaned and parsed (so that we can induce syntax-based distributional features), it has one serious drawback for a window-based approach (and, in general, for corpus work going beyond the sentence border): The sentences in the corpus have been sorted alphabetically, so going beyond the sentence border is likely to entering a sentence that did not originally precede or follo</context>
</contexts>
<marker>Schiehlen, 2003</marker>
<rawString>Michael Schiehlen. 2003. A Cascaded Finite-State Parser for German. In Proceedings of the 10th Conference of the European Chapter of the Association for Computational Linguistics, pages 163–166, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Schulte im Walde</author>
</authors>
<title>Exploring Features to Identify Semantic Nearest Neighbours: A Case Study on German Particle Verbs.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Conference on RecentAdvances in Natural Language Processing,</booktitle>
<pages>608--614</pages>
<location>Borovets, Bulgaria.</location>
<contexts>
<context position="33791" citStr="Walde, 2005" startWordPosition="5246" endWordPosition="5247">emantic tasks, however not including compositionality ratings. Similarly, Agirre et al. (2009) compared and combined a WordNet-based and various distributional models to predict the pair similarity of the 65 Rubenstein and Goodenough word pairs and the 353 word pairs in WordSim353. They varied window sizes, dependency relations and raw words in the models. On WordSim353, they reached p = 0.66, which is slightly better than our best result, but at the same time the dataset is smaller. Concerning computational models of German compounds, there is not much previous work. Our own work (Schulte im Walde, 2005; K¨uhner and Schulte im Walde, 2010) has addressed the degrees of compositionality of German particle verbs. Zinsmeister and Heid (2004) are most closely related to our current study. They suggested a distributional model to identify lexicalised German noun compounds by comparing the verbs that subcategorise the noun compound with those that subcategorise the head noun as direct objects. 5 Conclusion This paper presented experiments to predict the compositionality of German noun-noun compounds. Our overall best result is state-of-the-art, reaching Spearman’s p = 0.65 with a word-space model o</context>
</contexts>
<marker>Walde, 2005</marker>
<rawString>Sabine Schulte im Walde. 2005. Exploring Features to Identify Semantic Nearest Neighbours: A Case Study on German Particle Verbs. In Proceedings of the International Conference on RecentAdvances in Natural Language Processing, pages 608–614, Borovets, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Dimensions of Meaning.</title>
<date>1992</date>
<booktitle>In Proceedings of Supercomputing,</booktitle>
<pages>787--796</pages>
<marker>Sch¨utze, 1992</marker>
<rawString>Hinrich Sch¨utze. 1992. Dimensions of Meaning. In Proceedings of Supercomputing, pages 787–796.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sidney Siegel</author>
<author>N John Castellan</author>
</authors>
<title>Nonparametric Statistics for the Behavioral Sciences.</title>
<date>1988</date>
<publisher>McGraw-Hill,</publisher>
<location>Boston, MA.</location>
<contexts>
<context position="13674" citStr="Siegel and Castellan, 1988" startWordPosition="2126" endWordPosition="2129">nd the constituents, assuming that the stronger the distributional similarity (i.e., the cosine values), the larger the degree of compositionality. 5Alternatively, we also used the raw frequencies in all experiments below. The insights into the various features were identical to those based on LMI, but the predictions were worse. 6See http://www.collocations.de/AM/ for a more detailed illustration of association measures (incl. LMI). The vector space predictions were evaluated against the human ratings on the degree of compositionality, using the Spearman Rank-Order Correlation Coefficient p (Siegel and Castellan, 1988). The p correlation is a non-parametric statistical test that measures the association between two variables that are ranked in two ordered series. In Section 3.3 we will compare the overall effect of the various feature types and correlate all 488 compound–modifier and compound–head predictions against the ratings at the same time; in Section 3.4 we will compare the different effects of the features for compound– modifier pairs vs. compound–head pairs and thus correlate 244 predictions in both cases. After introducing a baseline and an upper bound for our vector space experiments in Section 3</context>
</contexts>
<marker>Siegel, Castellan, 1988</marker>
<rawString>Sidney Siegel and N. John Castellan. 1988. Nonparametric Statistics for the Behavioral Sciences. McGraw-Hill, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From Frequency to Meaning: Vector Space Models of Semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>37--141</pages>
<contexts>
<context position="1340" citStr="Turney and Pantel (2010)" startWordPosition="185" endWordPosition="188">art, reaching Spearman’s p = 0.65 with a wordspace model of nominal features from a 20- word window of a 1.5 billion word web corpus. (2) While there are no significant differences in predicting compound–modifier vs. compound–head ratings on compositionality, we show that the modifier (rather than the head) properties predominantly influence the degree of compositionality of the compound. 1 Introduction Vector space models and distributional information have been a steadily increasing, integral part of lexical semantic research over the past 20 years. On the one hand, vector space models (see Turney and Pantel (2010) and Erk (2012) for two recent surveys) have been exploited in psycholinguistic (Lund and Burgess, 1996) and computational linguistic research (Sch¨utze, 1992) to explore the notion of “similarity” between a set of target objects within a geometric setting. On the other hand, the distributional hypothesis (Firth, 1957; Harris, 1968) has been exploited to determine co-occurrence features for vector space models that best describe the words, phrases, sentences, etc. of interest. While the emergence of vector space models is increasingly pervasive within data-intensive lexical semantics, and even</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D. Turney and Patrick Pantel. 2010. From Frequency to Meaning: Vector Space Models of Semantics. Journal of Artificial Intelligence Research, 37:141–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia von der Heide</author>
<author>Susanne Borgwaldt</author>
</authors>
<title>Assoziationen zu Unter-, Basis- und Oberbegriffen. Eine explorative Studie.</title>
<date>2009</date>
<booktitle>In Proceedings of the 9th Norddeutsches Linguistisches Kolloquium,</booktitle>
<pages>51--74</pages>
<marker>von der Heide, Borgwaldt, 2009</marker>
<rawString>Claudia von der Heide and Susanne Borgwaldt. 2009. Assoziationen zu Unter-, Basis- und Oberbegriffen. Eine explorative Studie. In Proceedings of the 9th Norddeutsches Linguistisches Kolloquium, pages 51– 74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominic Widdows</author>
</authors>
<title>Semantic Vector Products: Some Initial Investigations.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2nd Conference on Quantum Interaction,</booktitle>
<location>Oxford, UK.</location>
<contexts>
<context position="16277" citStr="Widdows (2008)" startWordPosition="2542" endWordPosition="2543">e compound whole ratings is not even moderate 258 Function � Baseline Upper Bound modifier only .0959 .6002 head only .1019 .1385 addition .1168 .7687 multiplication .1079 .7829 Table 2: Baseline/Upper bound p correlations. (p = 0.1385). Obviously, the semantics of the modifiers had a much stronger impact on the semantic judgements of the compounds, thus confirming our intuition from Section 2.2. The lower part of the table shows the respective baseline and upper bound values when the compound–modifier ratings and the compound– head ratings were combined by standard arithmetic operations, cf. Widdows (2008) and Mitchell and Lapata (2010), among others: the compound–modifier and compound–head ratings were treated as vectors, and the vector features (i.e., the compound–constituent ratings) were added/multiplied to predict the compound whole ratings. As in the related work, the arithmetic operations strengthen the predictions, and multiplication reached an upper bound of p = 0.7829, thus outperforming not only the head-only but also the modifier-only upper bound. 3.2 German Web Corpora Most of our experiments rely on the sdeWaC corpus (Faaß et al., 2010), a cleaned version of the German web corpus </context>
</contexts>
<marker>Widdows, 2008</marker>
<rawString>Dominic Widdows. 2008. Semantic Vector Products: Some Initial Investigations. In Proceedings of the 2nd Conference on Quantum Interaction, Oxford, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heike Zinsmeister</author>
<author>Ulrich Heid</author>
</authors>
<title>Collocations of Complex Nouns: Evidence for Lexicalisation.</title>
<date>2004</date>
<booktitle>In Proceedings of Konvens,</booktitle>
<location>Vienna, Austria.</location>
<contexts>
<context position="33928" citStr="Zinsmeister and Heid (2004)" startWordPosition="5264" endWordPosition="5268">rdNet-based and various distributional models to predict the pair similarity of the 65 Rubenstein and Goodenough word pairs and the 353 word pairs in WordSim353. They varied window sizes, dependency relations and raw words in the models. On WordSim353, they reached p = 0.66, which is slightly better than our best result, but at the same time the dataset is smaller. Concerning computational models of German compounds, there is not much previous work. Our own work (Schulte im Walde, 2005; K¨uhner and Schulte im Walde, 2010) has addressed the degrees of compositionality of German particle verbs. Zinsmeister and Heid (2004) are most closely related to our current study. They suggested a distributional model to identify lexicalised German noun compounds by comparing the verbs that subcategorise the noun compound with those that subcategorise the head noun as direct objects. 5 Conclusion This paper presented experiments to predict the compositionality of German noun-noun compounds. Our overall best result is state-of-the-art, reaching Spearman’s p = 0.65 with a word-space model of nominal features from a 20-word window of a 1.5 billion word web corpus. Our experiments demonstrated that (1) window-based features ou</context>
</contexts>
<marker>Zinsmeister, Heid, 2004</marker>
<rawString>Heike Zinsmeister and Ulrich Heid. 2004. Collocations of Complex Nouns: Evidence for Lexicalisation. In Proceedings of Konvens, Vienna, Austria.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>