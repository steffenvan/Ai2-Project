<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000028">
<title confidence="0.9593485">
LEARNING PERCEPTUALLY-GROUNDED SEMANTICS IN
THE Lo PROJECT
</title>
<author confidence="0.998597">
Terry Regier*
</author>
<affiliation confidence="0.993526">
International Computer Science Institute
</affiliation>
<address confidence="0.7881475">
1947 Center Street, Berkeley, CA, 94704
(415) 642-4274 x 184
</address>
<email confidence="0.99867">
regier@cogsci.Berkeley.EDU
</email>
<sectionHeader confidence="0.993026" genericHeader="abstract">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.999521333333333">
A method is presented for acquiring perceptually-
grounded semantics for spatial terms in a simple visual
domain, as a part of the Lo miniature language acquisi-
tion project. Two central problems in this learning task
are (a) ensuring that the terms learned generalize well,
so that they can be accurately applied to new scenes,
and (b) learning in the absence of explicit negative ev-
idence. Solutions to these two problems are presented,
and the results discussed.
</bodyText>
<sectionHeader confidence="0.999462" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.952120034482759">
The Lo language learning project at the International
Computer Science Institute [Feldman et al., 1990; We-
ber and Stolcke, 1990] seeks to provide an account of lan-
guage acquisition in the semantic domain of spatial rela-
tions between geometrical objects. Within this domain,
the work reported here addresses the subtask of learn-
ing to associate scenes, containing several simple objects,
with terms to describe the spatial relations among the
objects in the scenes. This is illustrated in Figure 1.
For each scene, the learning system is supplied with an
indication of which object is the reference object (we call
this object the landmark, or LM), and which object is the
one being located relative to the reference object (this is
the trajector, or TR). The system is also supplied with
a single spatial term that describes the spatial relation
*Supported through the International Computer Science
Institute.
portrayed in the scene. It is to learn to associate all
applicable terms to novel scenes.
The TR is restricted to be a single point for the time
being; current work is directed at addressing the more
general case of an arbitrarily shaped TR.
Another aspect of the task is that learning must take
place in the absence of explicit negative instances. This
condition is imposed so that the conditions under which
learning takes place will be similar in this respect to
those under which children learn.
Given this, there are two central problems in the sub-
task as stated:
</bodyText>
<listItem confidence="0.915896833333333">
• Ensuring that the learning will generalize to scenes
which were not a part of the training set. This
means that the region in which a TR will be consid-
ered &amp;quot;above&amp;quot; a LM may have to change size, shape,
and position when a novel LM is presented.
• Learning without explicit negative evidence.
</listItem>
<bodyText confidence="0.9944766">
This paper presents solutions to both of these prob-
lems. It begins with a general discussion of each of the
two problems and their solutions. Results of training
are then presented. Then, implementation details are
discussed. And finally, some conclusions are presented.
</bodyText>
<sectionHeader confidence="0.99957" genericHeader="introduction">
2 Generalization and Parameterized
Regions
</sectionHeader>
<subsectionHeader confidence="0.996603">
2.1 The Problem
</subsectionHeader>
<bodyText confidence="0.999436058823529">
The problem of learning whether a particular point lies in
a given region of space is a foundational one, with sev-
eral widely-known &amp;quot;classic&amp;quot; solutions [Minsky and Pa-
pert, 1988; Rumelhart and McClelland, 1986]. The task
at hand is very similar to this problem, since learning
when &amp;quot;above&amp;quot; is an appropriate description of the spatial
relation between a LM and a point TR really amounts
to learning what the extent of the region &amp;quot;above&amp;quot; a LM
is.
However, there is an important difference from the
classic problem. We are interested here in learning
whether or not a given point (the TR) lies in a region
(say &amp;quot;above&amp;quot;, &amp;quot;in&amp;quot;) which is itself located relative to a
LM. Thus, the shape, size, and position of the region are
dependent on the shape, size, and position of the current
LM. For example, the area &amp;quot;above&amp;quot; a small triangle to-
ward the top of the visual field will differ in shape, size,
</bodyText>
<figure confidence="0.853507">
• TR
LM &amp;quot;--41&amp;quot; &amp;quot;Above&amp;quot;
</figure>
<figureCaption confidence="0.914995">
Figure 1: Learning to Associate Scenes with Spatial
Terms
</figureCaption>
<page confidence="0.996421">
138
</page>
<bodyText confidence="0.9974695">
and position from the area &amp;quot;above&amp;quot; a large circle in the
middle of the visual field.
</bodyText>
<subsectionHeader confidence="0.999563">
2.2 Parameterized Regions
</subsectionHeader>
<bodyText confidence="0.999880608695652">
Part of the solution to this problem lies in the use of pa-
rameterized regions. Rather than learn a fixed region of
space, the system learns a region which is parameterized
by several features of the LM, and is thus dependent on
them.
The LM features used are the location of the center of
mass, and the locations of the four corners of the smallest
rectangle enclosing the LM (the LM&apos;s &amp;quot;bounding-box&amp;quot;).
Learning takes place relative to these five &amp;quot;key points&amp;quot;.
Consider Figure 2. The figure in (a) shows a region
in 2-space learned using the intersection of three half-
planes, as might be done using an ordinary perceptron.
In (b), we see the same region, but learned relative to
the five key points of an LM. This means simply that the
lines which define the half-planes have been constrained
to pass through the key points of the LM. The method
by which this is done is covered in Section 5. Further
details can be found in [Regier, 1990].
The critical point here is that now that this region has
been learned relative to the LM key points, it will change
position and size when the LM key points change. This
is illustrated in (c). Thus, the region is parameterized
by the LM key points.
</bodyText>
<subsectionHeader confidence="0.999908">
2.3 Combining Representations
</subsectionHeader>
<bodyText confidence="0.991889793103448">
While the use of parameterized regions solves much of
the problem of generalizability across LMs, it is not suf-
ficient by itself. Two objects could have identical key
points, and yet differ in actual shape. Since part of the
definition of &amp;quot;above&amp;quot; is that the TR is not in the inte-
rior of the LM, and since the shape of the interior of
the LM cannot be derived from the key points alone, the
key points are an underspecification of the LM for our
purposes.
The complete LM specification includes a bitmap of
the interior of the LM, the &amp;quot;LM interior map&amp;quot;. This is
simply a bitmap representation of the LM, with those
bits set which fall in the interior of the object. As we
shall see in greater detail in Section 5, this representa-
tion is used together with parameterized regions in learn-
ing the perceptual grounding for spatial term semantics.
This bitmap representation helps in the case mentioned
above, since although the triangle and square will have
identical key points, their LM interior maps will differ.
In particular, since part of the learned &amp;quot;definition&amp;quot; of a
point being above a LM should be that it may not be in
the interior of the LM, that would account for the dif-
ference in shape of the regions located above the square
and above the triangle.
Parameterized regions and the bitmap representation,
when used together, provide the system with the ability
to generalize across LMs. We shall see examples of this
after a presentation of the second major problem to be
tackled.
</bodyText>
<figureCaption confidence="0.977667">
Figure 2: Parameterized Regions
</figureCaption>
<page confidence="0.991321">
139
</page>
<figureCaption confidence="0.998133">
Figure 3: Learning &amp;quot;Above&amp;quot; Without Negative Instances
</figureCaption>
<sectionHeader confidence="0.9949865" genericHeader="method">
3 Learning Without Explicit Negative
Evidence
</sectionHeader>
<subsectionHeader confidence="0.9999">
3.1 The Problem
</subsectionHeader>
<bodyText confidence="0.999964921052631">
Researchers in child language acquisition have often ob-
served that the child learns language apparently with-
out the benefit of negative evidence [Braine, 1971;
Bowerman, 1983; Pinker, 1989]. While these researchers
have focused on the &amp;quot;no negative evidence&amp;quot; problem as
it relates to the acquisition of grammar, the problem is
a general one, and appears in several different aspects
of language acquisition. In particular, it surfaces in the
context of the learning of the semantics of lexemes for
spatial relations. The methods used to solve the prob-
lem here are of general applicability, however, and are
not restricted to this particular domain.
The problem is best illustrated by example. Consider
Figure 3. Given the landmark (labeled &amp;quot;LM&amp;quot;), the task
is to learn the concept &amp;quot;above&amp;quot;. We have been given
four positive instances, marked as small dotted circles in
the figure, and no negative instances. The problem is
that we want to generalize so that we can recognize new
instances of &amp;quot;above&amp;quot; when they are presented, but since
there are no negative instances, it is not clear where the
boundaries of the region &amp;quot;above&amp;quot; the LM should be. One
possible generalization is the white region containing the
four instances. Another possibility is the union of that
white region with the dark region surrounding the LM.
Yet another is the union of the light and dark regions
with the interior of the LM. And yet another is the cor-
rect one, which is not closed at the top. In the absence of
negative examples, we have no obvious reason to prefer
one of these generalizations over the others.
One possible approach would be to take the smallest
region that encompasses all the positive instances. It
should be clear, however, that this will always lead to
closed regions, which are incorrect characterizations of
such spatial concepts as &amp;quot;above&amp;quot; and &amp;quot;outside&amp;quot;. Thus,
this cannot be the answer.
And yet, humans do learn these concepts, apparently
in the absence of negative instances. The following sec-
tions indicate how that learning might take place.
</bodyText>
<subsectionHeader confidence="0.999839">
3.2 A Possible Solution and its Drawbacks
</subsectionHeader>
<bodyText confidence="0.997544333333333">
One solution to the &amp;quot;no negative evidence&amp;quot; problem
which suggests itself is to take every positive instance
for one concept to be an implicit negative instance for
all other spatial concepts being learned. There are prob-
lems with this approach, as we shall see, but they are
surmountable.
There are related ideas present in the child lan-
guage literature, which support the work presented here.
[Markman, 1987] posits a &amp;quot;principle of mutual exclusiv-
ity&amp;quot; for object naming, whereby a child assumes that
each object may only have one name. This is to be
viewed more as a learning strategy than as a hard-and-
fast rule: clearly, a given object may have many names
(an office chair, a chair, a piece of furniture, etc.). The
method being suggested really amounts to a principle of
mutual exclusivity for spatial relation terms: since each
spatial relation can only have one name, we take a pos-
itive instance of one to be an implicit negative instance
for all others.
In a related vein, [Johnston and Slobin, 1979] note
that in a study of children learning locative terms in En-
glish, Italian, Serbo-Croatian, and Turkish, terms were
learned more quickly when there was little or no syn-
onymy among terms. They point out that children seem
to prefer a one-to-one meaning-to-morpheme mapping;
this is similar to, although not quite the same as, the
mutual exclusivity notion put forth here.&apos;
In linguistics, the notion that the meaning of a given
word is partly defined by the meanings of other words in
the language is a central idea of structuralism. This has
been recently reiterated by [MacWhinney, 1989]: &amp;quot;the
semantic range of words is determined by the particular
contrasts in which they are involved&amp;quot;. This is consonant
with the view taken here, in that contrasting words will
serve as implicit negative instances to help define the
boundaries of applicability of a given spatial term.
There is a problem with mutual exclusivity, however.
Using it as a method for generating implicit negative in-
stances can yield many false negatives in the training set,
i.e. implicit negatives which really should be positives.
Consider the following set of terms, which are the ones
learned by the system described here:
</bodyText>
<listItem confidence="0.999946">
• above
• below
• on
• off
</listItem>
<footnote confidence="0.8581415">
&apos;They are not quite the same since a difference in meaning
need not correspond to a difference in actual reference. When
we call a given object both a &amp;quot;chair&amp;quot; and a &amp;quot;throne&amp;quot;, these are
different meanings, and this would thus be consistent with a
one-to-one meaning-to-morpheme mapping. It would not be
consistent with the principle of mutual exclusivity, however.
</footnote>
<page confidence="0.978609">
140
</page>
<figure confidence="0.9433115">
(b)
Figure 4:
Ideal and Realistic Training Sets for &amp;quot;Outside&amp;quot;
(a)
0
0 0
0
0 r • • w 7 0
0 x x • • 0
XX. • .0
0 0
X. • •
0. • X • X
0
•x• •xo 0
o Lx • • 4 o
O0
0
</figure>
<equation confidence="0.8939158125">
XX xe x x
xx xx x
• y- • • x x x 0
x op • x • • •ex
• • • • x x
X x • • 0
XXO• • X • •0
X
X • • • x
X X() • •
• • •X • X
X X OX 0 X
X 0 04- • X • -4k 0
O0
OX XXX
X X
</equation>
<listItem confidence="0.9988265">
• inside
• outside
• to the left of
• to the right of
</listItem>
<bodyText confidence="0.998089612903226">
If we apply mutual exclusivity here, the problem of false
negatives arises. For example, not all positive instances
of &amp;quot;outside&amp;quot; are accurate negative instances for &amp;quot;above&amp;quot;,
and indeed all positive instances of &amp;quot;above&amp;quot; should in
fact be positive instances of &amp;quot;outside&amp;quot;, and are instead
taken as negatives, under mutual exclusivity.
&amp;quot;Outside&amp;quot; is a term that is particularly badly affected
by this problem of false implicit negatives: all of the
spatial terms listed above except for &amp;quot;in&amp;quot; (and &amp;quot;outside&amp;quot;
itself, of course) will supply false negatives to the training
set for &amp;quot;outside&amp;quot;.
The severity of this problem is illustrated in Figure 4.
In these figures, which represent training data for the
spatial concept &amp;quot;outside&amp;quot;, we have tall, rectangular land-
marks, and training points2 relative to the landmarks.
Positive training points (instances) are marked with cir-
cles, while negative instances are marked with X&apos;s. In
(a), the negative instances were placed there by the
teacher, showing exactly where the region not outside
the landmark is. This gives us a &amp;quot;clean&amp;quot; training set, but
the use of teacher-supplied explicit negative instances is
precisely what we are trying to get away from. In (b), the
negative instances shown were derived from positive in-
stances for the other spatial terms listed above, through
the principle of mutual exclusivity. Thus, this is the sort
of training data we are going to have to use. Note that
in (b) there are many false negative instances among the
positives, to say nothing of the positions which have been
marked as both positive and negative.
This issue of false implicit negatives is the central
problem with mutual exclusivity.
</bodyText>
<subsectionHeader confidence="0.999581">
3.3 Salvaging Mutual Exclusivity
</subsectionHeader>
<bodyText confidence="0.99975">
The basic idea used here, in salvaging the idea of mu-
tual exclusivity, is to treat positive instances and implicit
negative instances differently during training:
</bodyText>
<subsectionHeader confidence="0.53343">
Implicit negatives are viewed as supplying only
weak negative evidence.
</subsectionHeader>
<bodyText confidence="0.96990494117647">
The intuition behind this is as follows: since the im-
plicit negatives are arrived at through the application of
a fallible heuristic rule (mutual exclusivity), they should
count for less than the positive instances, which are all
assumed to be correct. Clearly, the implicit negatives
should not be seen as supplying excessively weak neg-
ative evidence, or we revert to the original problem of
learning in the (virtual) absence of negative instances.
But equally clearly, the training set noise supplied by
false negatives is quite severe, as seen in the figure above.
So this approach is to be seen as a compromise, so that
we can use implicit negative evidence without being over-
whelmed by the noise it introduces in the training sets
for the various spatial concepts.
The details of this method, and its implementation un-
der back-propagation, are covered in Section 5. However,
21.e. trajectors consisting of a single point each
</bodyText>
<page confidence="0.994266">
141
</page>
<bodyText confidence="0.999310363636363">
this is a very general solution to the &amp;quot;no negative evi-
dence&amp;quot; problem, and can be understood independently of
the actual implementation details. Any learning method
which allows for weakening of evidence should be able to
make use of it. In addition, it could serve as a means for
addressing the &amp;quot;no negative evidence&amp;quot; problem in other
domains. For example, a method analogous to the one
suggested here could be used for object naming, the do-
main for which Markman suggested mutual exclusivity.
This would be necessary if the problem of false implicit
negatives is as serious in that domain as it is in this one.
</bodyText>
<sectionHeader confidence="0.999943" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.98858190625">
This section presents the results of training.
Figure 5 shows the results of learning the spatial term
&amp;quot;outside&amp;quot;, first without negative instances, then using
implicit negatives obtained through mutual exclusivity,
but without weakening the evidence given by these, and
finally with the negative evidence weakened.
The landmark in each of these figures is a triangle.
The system was trained using only rectangular land-
marks.
The size of the black circles indicates the appropri-
ateness, as judged by the trained system, of using the
term &amp;quot;outside&amp;quot; to refer to a particular position, relative
to the LM shown. Clearly, the concept is learned best
when implicit negative evidence is weakened, as in (c).
When no negatives at all axe used, the system overgen-
eralizes, and considers even the interior of the LM to be
&amp;quot;outside&amp;quot; (as in (a)). When mutual exclusivity is used,
but the evidence from implicit negatives is not weakened,
the concept is learned very poorly, as the noise from the
false implicit negatives hinders the learning of the con-
cept (as in (b)). Having all implicit negatives supply
only weak negative evidence greatly alleviates the prob-
lem of false implicit negatives in the training set, while
still enabling us to learn without using explicit, teacher-
supplied negative instances.
It should be noted that in general, when using mutual
exclusivity without weakening the evidence given by im-
plicit negatives, the results are not always identical with
those shown in Figure 5(b), but are always of approxi-
mately the same quality.
Regarding the issue of generalizability across LMs, two
points of interest are that:
</bodyText>
<listItem confidence="0.9606065">
• The system had not been trained on an LM in ex-
actly this position.
• The system had never been trained on a triangle of
any sort.
</listItem>
<bodyText confidence="0.975954">
Thus, the system generalizes well to new LMs, and
learns in the absence of explicit negative instances, as
desired. All eight concepts were learned successfully, and
exhibited similar generalization to new LMs.
</bodyText>
<sectionHeader confidence="0.99682" genericHeader="evaluation">
5 Details
</sectionHeader>
<bodyText confidence="0.64807175">
The system described in this section learns perceptually-
grounded semantics for spatial terms using the
00,000000400000000004
00000000000000000004
</bodyText>
<page confidence="0.984278117647059">
O00000000000410000004
00000000000000000004
00000000041000000•004
000000041000000000004
00000000000000000004
110000000000000000004
00000000000000,00004
000000000000111.100004
000000000011.641,00004
O0000000.411.611111100004
000000.19.119.***00004
00000119MMENNOM§.00004
00004111111111011.1111,M00004
00000000000000000004
00000000000000000004
00000000000000000004
*0000000000000000004
</page>
<figure confidence="0.999247571428572">
•
• • ••00000400 0004
• • ••••••••• 0004
*00.0*******0.......
seeo.*******WHIMID041.4
•••••••••••••8143••••4
• •• • ••••RIEENERIMilip• • • • I
• • • • • 011110111191.011Me • • • 4
•••••1_01%,...§.§Ne••••
oboofbmv_m MOOMMOMOOO*4
0041006000000410000004
0000•.000041004,11)00004
0000.&amp;quot;4110000000000,4
411.4,410.&amp;quot;0004100000,004
Saba • • • • •
•••••••••••••••••••Il
•••••••••••••••••••11
••••••••••••••••••••
•••••••••••••••••••11
•••••••••••••••••••1
•••••••••••••••••••4I
••••••••••••••••••••
•••••••••••••••••••■
••••••••••••_“.11.011111•1
•••••••••••• I 1 *MI
•••••••••••••••••••I
••••••■•••••••••••••
•••••••*0111••••••••••
eloo000*********000414
••••1:•••••••••:10••••
•••••••••••••••••••Il
••••••••••••••••••••
•••••••••••••••••••■1
•••••••••••••••••••■1
•
</figure>
<figureCaption confidence="0.962467">
Figure 5: &amp;quot;Outside&amp;quot; without Negatives, and with Strong
and Weak Implicit Negatives
</figureCaption>
<page confidence="0.993612">
142
</page>
<bodyText confidence="0.999066571428571">
quickprop3 algorithm [Fahlman, 1988], a variant on
back-propagation [Rumelhart and McClelland, 1986].
This presentation begins with an exposition of the rep-
resentation used, and then moves on to the specific net-
work architecture, and the basic ideas embodied in it.
The weakening of evidence from implicit negative in-
stances is then discussed.
</bodyText>
<subsectionHeader confidence="0.998252">
5.1 Representation of the LM and TR
</subsectionHeader>
<bodyText confidence="0.999614">
As mentioned above, the representation scheme for the
LM comprises the following:
</bodyText>
<listItem confidence="0.985866">
• A bitmap in which those pixels corresponding to the
interior of the LM are the only ones set.
• The x, y coordinates of several &amp;quot;key points&amp;quot; of the
</listItem>
<bodyText confidence="0.911561615384615">
LM, where x and y each vary between 0.0 and 1.0,
and indicate the location of the point in question
as a fraction of the width or height of the image.
The key points currently being used are the center
of mass (CoM) of the LM, and the four corners of
the LM&apos;s bounding box (UL: upper left, UR: upper
right, LL: lower left, LR: lower right).
The (punctate) TR is specified by the x, y coordinates
of the point.
The activation of an output node of the system, once
trained for a particular spatial concept, represents the
appropriateness of using the spatial term in describing
the TR&apos;s location, relative to the LM.
</bodyText>
<subsectionHeader confidence="0.999765">
5.2 Architecture
</subsectionHeader>
<bodyText confidence="0.996536333333333">
Figure 6 presents the architecture of the system. The
eight spatial terms mentioned above are learned simul-
taneously, and they share hidden-layer representations.
</bodyText>
<subsectionHeader confidence="0.681187">
5.2.1 Receptive Fields
</subsectionHeader>
<bodyText confidence="0.99995625">
Consider the right-hand part of the network, which
receives input from the LM interior map. Each of the
three nodes in the cluster labeled &amp;quot;I&amp;quot; (for interior) has a
receptive field of five pixels.
When a TR location is specified, the values of the
five neighboring locations shown in the LM interior map,
centered on the current TR location, are copied up to the
five input nodes. The weights on the links between these
five nodes and the three nodes labeled &amp;quot;I&amp;quot; in the layer
above define the receptive fields learned. When the TR
position changes, five new LM interior map pixels will be
&amp;quot;viewed&amp;quot; by the receptive fields formed. This allows the
system to detect the LM interior (or a border between
interior and exterior) at a given point and to bring that
to bear if that is a relevant semantic feature for the set
of spatial terms being learned.
</bodyText>
<subsectionHeader confidence="0.649845">
5.2.2 Parameterized Regions
</subsectionHeader>
<bodyText confidence="0.999674">
The remainder of the network is dedicated to com-
puting parameterized regions. Recall that a parameter-
ized region is much the same as any other region which
might be learned by a perceptron, except that the lines
</bodyText>
<footnote confidence="0.871311">
3Quickprop gets its name from its ability to quickly con-
verge on a solution. In most cases, it exhibits faster conver-
gence than that obtained using conjugate gradient methods
[Fahlman, 1990].
</footnote>
<bodyText confidence="0.99997">
which define the relevant half-planes are constrained to
go through specific points. In this case, these are the key
points of the LM.
A simple two-input perceptron unit defines a line in
the x, y plane, and selects a half-plane on one side of it.
Let w, and wy refer to the weights on the links from
the x and y inputs to the perceptron unit. In general,
if the unit&apos;s function is a simple threshold, the equation
for such a line will be
</bodyText>
<equation confidence="0.726844">
xw, + ywy = 0, (1)
i.e. the net input to the perceptron unit will be
netin = xtur + ywy (2)
</equation>
<bodyText confidence="0.998359470588235">
Note that this line always passes through the origin:
(0,0).
If we want to force the line to pass through a particular
point (xt, /it) in the plane, we simply shift the entire
coordinate system so that the origin is now at (xt, ye).
This is trivially done by adjusting the input values such
that the net input to the unit is now
net in = (x — xs)wx (y — Yt)wy• (3)
Given this, we can easily force lines to pass through
the key points of an LM, as discussed above, by setting
(xt, Yt) appropriately for each key point. Once the sys-
tem has learned, the regions will be parameterized by
the coordinates of the key points, so that the spatial
concepts will be independent of the size and position of
any particular LM.
Now consider the left-hand part of the network. This
accepts as input the x, y coordinates of the TR location
and the LM key points, and the layer above the input
layer performs the appropriate subtractions, in line with
equation 3. Now each of the nodes in the layer above
that is viewing the TR in a different coordinate system,
shifted by the amount specified by the LM key points.
Note that in the BB cluster there is one node for each
corner of the LM&apos;s bounding-box, while the CoM clus-
ter has three nodes dedicated to the LM&apos;s center of mass
(and thus three lines passing through the center of mass).
This results in the computation, and through weight up-
dates, the learning, of a parameterized region.
Of course, the hidden nodes (labeled &amp;quot;I&amp;quot;) that receive
input from the LM interior map are also in this hidden
layer. Thus, receptive fields and parameterized regions
are learned together, and both may contribute to the
learned semantics of each spatial term. Further details
can be found in [Regier, 1990].
</bodyText>
<subsectionHeader confidence="0.9597575">
5.3 Implementing &amp;quot;Weakened&amp;quot; Mutual
Exclusivity
</subsectionHeader>
<bodyText confidence="0.999840375">
Now that the basic architecture and representations have
been covered, we present the means by which the evi-
dence from implicit negative instances is weakened. It
is assumed that training sets have been constructed us-
ing mutual exclusivity as a guiding principle, such that
each negative instance in the training set for a given spa-
tial term results from a positive instance for some other
term.
</bodyText>
<page confidence="0.995614">
143
</page>
<figure confidence="0.874804142857143">
above below on off in out left right
LM
Interior
•
Map
VTR
zTR
</figure>
<figureCaption confidence="0.99849">
Figure 6: Network Architecture
</figureCaption>
<page confidence="0.991535">
144
</page>
<listItem confidence="0.972355428571429">
• Evidence from implicit negative instances is weak-
ened simply by attenuating the error caused by
these implicit negatives.
• Thus, an implicit negative instance which yields an
error of a given magnitude will contribute less to the
weight changes in the network than will a positive
instance of the same error magnitude.
</listItem>
<bodyText confidence="0.9960104">
This is done as follows:
Referring back to Figure 6, note that output nodes
have been allocated for each of the spatial terms to be
learned. For a network such as this, the usual error term
in back-propagation is
</bodyText>
<equation confidence="0.971824">
1
E=- E(t • - o. )2
2 .1 IP IP (4)
,P
</equation>
<bodyText confidence="0.999964">
where j indexes over output nodes, and p indexes over
input patterns.
We modify this by dividing the error at each output
node by some number fli,p, dependent on both the node
and the current input pattern.
</bodyText>
<equation confidence="0.96764325">
E= (
lE 0j,p )2
2 ■P
i
</equation>
<bodyText confidence="0.999987615384615">
The general idea is that for positive instances of some
spatial term, 1 will be 1.0, so that the error is not at-
tenuated. For an implicit negative instance of a term,
however, f3,,,,, will be some value Mien, which corre-
sponds to the amount by which the error signals from
implicit negatives are to be attenuated.
Assume that we are currently viewing input pattern
p, a positive instance of &amp;quot;above&amp;quot;. Then the target value
for the &amp;quot;above&amp;quot; node will be 1.0, while the target values
for all others will be 0.0, as they are implicit negatives.
Here, ,Rabove,p = 1.0, and Ad, = Atten,Vi 0 above.
The value Alien = 32.0 was used successfully in the
experiments reported here.
</bodyText>
<sectionHeader confidence="0.999734" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99972675">
The system presented here learns perceptually-grounded
semantics for the core senses of eight English preposi-
tions, successfully generalizing to scenes involving land-
marks to which the system had not been previously ex-
posed. Moreover, the principle of mutual exclusivity is
successfully used to allow learning without explicit nega-
tive instances, despite the false negatives in the resulting
training sets.
Current research is directed at extending this work to
the case of arbitrarily shaped trajectors, and to handling
polysemy. Work is also being directed toward the learn-
ing of non-English spatial systems.
</bodyText>
<sectionHeader confidence="0.999564" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999025254545455">
[Bowerman, 1983] Melissa Bowerman, &amp;quot;How Do Chil-
dren Avoid Constructing an Overly General Grammar
in the Absence of Feedback about What is Not a Sen-
tence?,&amp;quot; In Papers and Reports on Child Language
Development. Stanford University, 1983.
[Braine, 1971] M. Braine, &amp;quot;Ott Two Types of Models
of the Internalization of Grammars,&amp;quot; In D. Slobin,
editor, The Ontogenesis of Grammar. Academic Press,
1971.
[Fahlman, 1988] Scott Fahlman, &amp;quot;Faster-Learning Vari-
ations on Back Propagation: An Empirical Study,&amp;quot; In
Proceedings of the 1988 Connectionist Models Summer
School. Morgan Kaufmann, 1988.
[Fahlman, 1990] Scott Fahlman, (personal communica-
tion), 1990.
[Feldman et al., 1990] J. Feldman, G. Lakoff, A. Stolcke,
and S. Weber, &amp;quot;Miniature Language Acquisition: A
Touchstone for Cognitive Science,&amp;quot; Technical Report
TR-90-009, International Computer Science Institute,
Berkeley, CA, 1990, also in the Proceedings of the 12th
Annual Conference of the Cognitive Science Society,
pp. 686-693.
[Johnston and Slobin, 1979] Judith Johnston and Dan
Slobin, &amp;quot;The Development of Locative Expressions in
English, Italian, Serbo-Croatian and Turkish,&amp;quot; Jour-
nal of Child Language, 6:529-545, 1979.
[MacWhinney, 1989] Brian MacWhinney, &amp;quot;Competition
and Lexical Categorization,&amp;quot; In Linguistic Categoriza-
tion, number 61 in Current Issues in Linguistic The-
ory. John Benjamins Publishing Co., Amsterdam and
Philadelphia, 1989.
[Markman, 1987] Ellen M. Markman, &amp;quot;How Children
Constrain the Possible Meanings of Words,&amp;quot; In Con-
cepts and conceptual development: Ecological and in-
tellectual factors in categorization. Cambridge Univer-
sity Press, 1987.
[Minsky and Papert, 1988] Marvin Minsky and Sey-
mour Papert, Perceptrons (Expanded Edition), MIT
Press, 1988.
[Pinker, 1989] Steven Pinker, Learnability and Cogni-
tion: The Acquisition of Argument Structure, MIT
Press, 1989.
[Regier, 1990] Terry Regier, &amp;quot;Learning Spatial Terms
Without Explicit Negative Evidence,&amp;quot; Technical Re-
port 57, International Computer Science Institute,
Berkeley, California, November 1990.
[Rumelhart and McClelland, 1986] David Rumelhart
and James McClelland, Parallel Distributed Proccess-
ing: Explorations in the microstructure of cognition,
MIT Press, 1986.
[Weber and Stolcke, 1990J Susan Hollbach Weber and
Andreas Stolcke, &amp;quot;Lo: A Testbed for Miniature Lan-
guage Acquisition,&amp;quot; Technical Report TR-90-010, In-
ternational Computer Science Institute, Berkeley, CA,
1990.
</reference>
<figure confidence="0.529225">
(5)
</figure>
<page confidence="0.985952">
145
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.865933">
<title confidence="0.969227">LEARNING PERCEPTUALLY-GROUNDED SEMANTICS IN THE Lo PROJECT</title>
<author confidence="0.99993">Terry Regier</author>
<affiliation confidence="0.99999">International Computer Science Institute</affiliation>
<address confidence="0.999984">1947 Center Street, Berkeley, CA, 94704</address>
<phone confidence="0.948846">(415) 642-4274 x 184</phone>
<email confidence="0.984792">regier@cogsci.Berkeley.EDU</email>
<abstract confidence="0.9986518">A method is presented for acquiring perceptuallygrounded semantics for spatial terms in a simple visual domain, as a part of the Lo miniature language acquisition project. Two central problems in this learning task are (a) ensuring that the terms learned generalize well, so that they can be accurately applied to new scenes, and (b) learning in the absence of explicit negative evidence. Solutions to these two problems are presented, and the results discussed.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Melissa Bowerman</author>
</authors>
<title>How Do Children Avoid Constructing an Overly General Grammar in the Absence of Feedback about What is Not a Sentence?,&amp;quot; In Papers and Reports on Child Language Development.</title>
<date>1983</date>
<institution>Stanford University,</institution>
<marker>[Bowerman, 1983]</marker>
<rawString>Melissa Bowerman, &amp;quot;How Do Children Avoid Constructing an Overly General Grammar in the Absence of Feedback about What is Not a Sentence?,&amp;quot; In Papers and Reports on Child Language Development. Stanford University, 1983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Braine</author>
</authors>
<title>Ott Two Types of Models of the Internalization of Grammars,&amp;quot; In</title>
<date>1971</date>
<booktitle>The Ontogenesis of Grammar.</booktitle>
<editor>D. Slobin, editor,</editor>
<publisher>Academic Press,</publisher>
<marker>[Braine, 1971]</marker>
<rawString>M. Braine, &amp;quot;Ott Two Types of Models of the Internalization of Grammars,&amp;quot; In D. Slobin, editor, The Ontogenesis of Grammar. Academic Press, 1971.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Fahlman</author>
</authors>
<title>Faster-Learning Variations on Back Propagation: An Empirical Study,&amp;quot;</title>
<date>1988</date>
<booktitle>In Proceedings of the 1988 Connectionist Models Summer School.</booktitle>
<publisher>Morgan Kaufmann,</publisher>
<marker>[Fahlman, 1988]</marker>
<rawString>Scott Fahlman, &amp;quot;Faster-Learning Variations on Back Propagation: An Empirical Study,&amp;quot; In Proceedings of the 1988 Connectionist Models Summer School. Morgan Kaufmann, 1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Fahlman</author>
</authors>
<date>1990</date>
<tech>(personal communication),</tech>
<marker>[Fahlman, 1990]</marker>
<rawString>Scott Fahlman, (personal communication), 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Feldman</author>
<author>G Lakoff</author>
<author>A Stolcke</author>
<author>S Weber</author>
</authors>
<title>Miniature Language Acquisition: A Touchstone for Cognitive Science,&amp;quot;</title>
<date>1990</date>
<booktitle>in the Proceedings of the 12th Annual Conference of the Cognitive Science Society,</booktitle>
<tech>Technical Report TR-90-009,</tech>
<pages>686--693</pages>
<institution>International Computer Science Institute,</institution>
<location>Berkeley, CA,</location>
<marker>[Feldman et al., 1990]</marker>
<rawString>J. Feldman, G. Lakoff, A. Stolcke, and S. Weber, &amp;quot;Miniature Language Acquisition: A Touchstone for Cognitive Science,&amp;quot; Technical Report TR-90-009, International Computer Science Institute, Berkeley, CA, 1990, also in the Proceedings of the 12th Annual Conference of the Cognitive Science Society, pp. 686-693.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judith Johnston</author>
<author>Dan Slobin</author>
</authors>
<title>The Development of Locative Expressions in English, Italian, Serbo-Croatian and Turkish,&amp;quot;</title>
<date>1979</date>
<journal>Journal of Child Language,</journal>
<pages>6--529</pages>
<marker>[Johnston and Slobin, 1979]</marker>
<rawString>Judith Johnston and Dan Slobin, &amp;quot;The Development of Locative Expressions in English, Italian, Serbo-Croatian and Turkish,&amp;quot; Journal of Child Language, 6:529-545, 1979.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian MacWhinney</author>
</authors>
<title>Competition and Lexical Categorization,&amp;quot; In Linguistic Categorization, number 61 in Current Issues in Linguistic Theory. John Benjamins Publishing Co.,</title>
<date>1989</date>
<location>Amsterdam and Philadelphia,</location>
<marker>[MacWhinney, 1989]</marker>
<rawString>Brian MacWhinney, &amp;quot;Competition and Lexical Categorization,&amp;quot; In Linguistic Categorization, number 61 in Current Issues in Linguistic Theory. John Benjamins Publishing Co., Amsterdam and Philadelphia, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Markman</author>
</authors>
<title>How Children Constrain the Possible Meanings of Words,&amp;quot; In Concepts and conceptual development: Ecological and intellectual factors in categorization.</title>
<date>1987</date>
<publisher>Cambridge University Press,</publisher>
<marker>[Markman, 1987]</marker>
<rawString>Ellen M. Markman, &amp;quot;How Children Constrain the Possible Meanings of Words,&amp;quot; In Concepts and conceptual development: Ecological and intellectual factors in categorization. Cambridge University Press, 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marvin Minsky</author>
<author>Seymour Papert</author>
</authors>
<title>Perceptrons (Expanded Edition),</title>
<date>1988</date>
<publisher>MIT Press,</publisher>
<marker>[Minsky and Papert, 1988]</marker>
<rawString>Marvin Minsky and Seymour Papert, Perceptrons (Expanded Edition), MIT Press, 1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Pinker</author>
</authors>
<title>Learnability and Cognition: The Acquisition of Argument Structure,</title>
<date>1989</date>
<publisher>MIT Press,</publisher>
<marker>[Pinker, 1989]</marker>
<rawString>Steven Pinker, Learnability and Cognition: The Acquisition of Argument Structure, MIT Press, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Regier</author>
</authors>
<title>Learning Spatial Terms Without Explicit Negative Evidence,&amp;quot;</title>
<date>1990</date>
<tech>Technical Report 57,</tech>
<institution>International Computer Science Institute,</institution>
<location>Berkeley, California,</location>
<marker>[Regier, 1990]</marker>
<rawString>Terry Regier, &amp;quot;Learning Spatial Terms Without Explicit Negative Evidence,&amp;quot; Technical Report 57, International Computer Science Institute, Berkeley, California, November 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Rumelhart</author>
<author>James McClelland</author>
</authors>
<title>Parallel Distributed Proccessing: Explorations in the microstructure of cognition,</title>
<date>1986</date>
<tech>Technical Report TR-90-010,</tech>
<publisher>MIT Press,</publisher>
<institution>International Computer Science Institute,</institution>
<location>Berkeley, CA,</location>
<marker>[Rumelhart and McClelland, 1986]</marker>
<rawString>David Rumelhart and James McClelland, Parallel Distributed Proccessing: Explorations in the microstructure of cognition, MIT Press, 1986. [Weber and Stolcke, 1990J Susan Hollbach Weber and Andreas Stolcke, &amp;quot;Lo: A Testbed for Miniature Language Acquisition,&amp;quot; Technical Report TR-90-010, International Computer Science Institute, Berkeley, CA, 1990.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>