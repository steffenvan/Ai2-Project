<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001695">
<title confidence="0.978896">
Improving sparse word similarity models with asymmetric measures
</title>
<author confidence="0.976291">
Jean Mark Gawron
</author>
<affiliation confidence="0.93358">
San Diego State University
</affiliation>
<email confidence="0.995204">
gawron@mail.sdsu.edu
</email>
<sectionHeader confidence="0.993819" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99990925">
We show that asymmetric models based on
Tversky (1977) improve correlations with
human similarity judgments and nearest
neighbor discovery for both frequent and
middle-rank words. In accord with Tver-
sky’s discovery that asymmetric similarity
judgments arise when comparing sparse
and rich representations, improvement on
our two tasks can be traced to heavily
weighting the feature bias toward the rarer
word when comparing high- and mid-
frequency words.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999953777777778">
A key assumption of most models of similarity is
that a similarity relation is symmetric. This as-
sumption is foundational for some conceptions,
such as the idea of a similarity space, in which
similarity is the inverse of distance; and it is deeply
embedded into many of the algorithms that build
on a similarity relation among objects, such as
clustering algorithms. The symmetry assumption
is not, however, universal, and it is not essential
to all applications of similarity, especially when it
comes to modeling human similarity judgments.
Citing a number of empirical studies, Tversky
(1977) calls symmetry directly into question, and
proposes two general models that abandon sym-
metry. The one most directly related to a large
body of word similarity work that followed is what
he calls the ratio model, which defines sim(a, b)
as:
</bodyText>
<equation confidence="0.9502175">
f(AnB) (1)
f (A n B) + α f (A\B) + Qf (B\A)
</equation>
<bodyText confidence="0.999949090909091">
Here A and B represent feature sets for the objects
a and b respectively; the term in the numerator is a
function of the set of shared features, a measure of
similarity, and the last two terms in the denomina-
tor measure dissimilarity: α and Q are real-number
weights; when α =� Q, symmetry is abandoned.
To motivate such a measure, Tversky presents
experimental data with asymmetric similarity re-
sults, including similarity comparisons of coun-
tries, line drawings of faces, and letters. Tversky
shows that many similarity judgment tasks have
an inherent asymmetry; but he also argues, fol-
lowing Rosch (1975), that certain kinds of stimuli
are more naturally used as foci or standards than
others. Goldstone (in press) summarizes the re-
sults succinctly: “Asymmetrical similarity occurs
when an object with many features is judged as
less similar to a sparser object than vice versa; for
example, North Korea is judged to be more like
China than China is [like] North Korea.” Thus,
one source of asymmetry is the comparison of
sparse and dense representations.
The relevance of such considerations to word
similarity becomes clear when we consider that
for many applications, word similarity measures
need to be well-defined when comparing very fre-
quent words with infrequent words. To make this
concrete, let us consider a word representation
in the word-as-vector paradigm (Lee, 1997; Lin,
1998), using a dependency-based model. Sup-
pose we want to measure the semantic similarity
of boat, rank 682 among the nouns in the BNC
corpus studied below, which has 1057 nonzero
dependency features based on 50 million words
of data, with dinghy, rank 6200, which has only
113 nonzero features. At the level of the vec-
tor representations we are using, these are events
of very different dimensionality; that is, there are
ten times as many features in the representation of
boat as there are in the representation of dinghy. If
in Tversky/Rosch terms, the more frequent word
is also a more likely focus, then this is exactly
the kind of situation in which asymmetric similar-
ity judgments will arise. Below we show that an
</bodyText>
<page confidence="0.971828">
296
</page>
<bodyText confidence="0.974471071428571">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 296–301,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
asymmetric measure, using α and β biased in fa-
vor of the less frequent word, greatly improves the
performance of a dependency-based vector model
in capturing human similarity judgments.
Before presenting these results, it will be help-
ful to slightly reformulate and slightly generalize
Tversky’s ratio model. The reformulation will al-
low us to directly draw the connection between
the ratio model and a set of similarity measures
that have played key roles in the similarity litera-
ture. First, since Tversky has primarily additive f
in mind, we can reformulate f(A n B) as follows
</bodyText>
<equation confidence="0.9913755">
f(A n B) = X wght(f) (2)
fEAnB
</equation>
<bodyText confidence="0.784216">
Next, since we are interested in generalizing from
sets of features, to real-valued vectors of features,
w1, w2, we define
</bodyText>
<equation confidence="0.911959">
σSI(w1,w2) = PfEw1nw2 SI(w1[f],w2[f]).
</equation>
<listItem confidence="0.9224685">
Here SI is some numerical operation on real-
number feature values (SI stands for shared infor-
mation). If the operation is MIN and w1[f] and
w2[f] both contain the feature weights for f, then
</listItem>
<equation confidence="0.998455">
wght(f)= σMIN(w1,w2)
fEw1nw2 MIN(w1[f], w2[f]),
</equation>
<bodyText confidence="0.860040833333333">
so with SI set to MIN, Equation (3) includes Equa-
tion (2) as a special case. Similarly, σ(w1, w1)
represents the summed feature weights of w1, and
therefore,
f(w1\ w2) = σ(w1, w1) − σ(w1, w2)
In this generalized form, then, (1) becomes
</bodyText>
<equation confidence="0.985304">
σ(w1,w2)
σ(w1,w2)+α[σ(w1,w1)−σ(w1,w2)]+β[σ(w2,w2)−σ(w1,w2)]
σ(w1,w2)
=
ασ(w1,w1)+βσ(w2,w2)+σ(w1,w2)−(α+β)σ(w1,w2)
Thus, if α + β = 1, Tversky’s ratio model be-
comes simply:
σ(w1,w2)
sim(w1, w2) = ασ(w1,w1)+(1−α)σ(w2,w2)
</equation>
<bodyText confidence="0.989124615384615">
The computational advantage of this reformula-
tion is that the core similarity operation σ(w1, w2)
is done on what is generally only a small number
of shared features, and the σ(wi, wi) calculations
(which we will call self-similarities), can be com-
puted in advance. Note that sim(w1, w2) is sym-
metric if and only if α = 0.5. When α &gt; 0.5,
sim(w1, w2) is biased in favor of w1 as the refer-
ent; When α &lt; 0.5, sim(w1, w2) is biased in favor
of w2.
Consider four similarity functions that have
played important roles in the literature on similar-
ity:
</bodyText>
<equation confidence="0.9986875">
DICE PROD(w1, w2) = 11w111 2*2 1·w2 2
+11w211
2*P fEw1∩w2 min(w1[f], w2[f])
P w1[f]+P w2[f]
P
fEw1∩w2 w1[f]+ w2[f]
</equation>
<bodyText confidence="0.996816444444444">
=
The function DICE PROD is not well known in the
word similarity literature, but in the data mining
literature it is often just called Dice coefficient, be-
cause it generalized the set comparison function
of Dice (1945). Observe that cosine is a special
case of DICE PROD. DICE† was introduced in Cur-
ran (2004) and was the most successful function
in his evaluation. Since LIN was introduced in Lin
(1998); several different functions have born that
name. The version used here is the one used in
Curran (2004).
The three distinct functions in Equation 6 have
a similar form. In fact, all can be defined in terms
of σ functions differing only in their SI operation.
Let σSI be a shared feature sum for operation SI,
as defined in Equation (3). We define the Tversky-
normalized version of σSI, written TSI, as:1
</bodyText>
<equation confidence="0.997816333333333">
TSI(w1,w2) = (7)
σSI(w1, w1) + σSI(w2, w2)
2 σSI(w1,w2)
</equation>
<bodyText confidence="0.9988468">
Note that TSI is just the special case of Tversky’s
ratio model (5) in which α = 0.5 and the similarity
measure is symmetric.
We define three SI operations σPROD2, σMIN, and
σAVG as follows:
</bodyText>
<equation confidence="0.998573">
SI σSI(w1, w2)
P
fEw1nw2 w1[f] * w2[f]
w1[f]+w2[f]
PfEw1nw2 2
P fEw1nw2 MIN(w1[f], w2[f])
1Paralleling (7) is Jaccard-family normalization:
UJACC(w1, w2) = U(w1, w1) + U(w2, w2) − U(w1, w2)
U(w1, w2)
</equation>
<bodyText confidence="0.966132">
It is easy to generalize the result from van Rijsbergen (1979)
for the original set-specific versions of Dice and Jaccard, and
show that all of the Tversky family functions discussed above
are monotonic in Jaccard.
2UPROD, of course, is dot product.
</bodyText>
<equation confidence="0.881941272727273">
X
fEAnB
= P
DICE†(w1, w2) =
LIN(w1,w2)
P w1[f]+Pw2[f]
COS(w1, w2) = DICE PROD applied
to unit vectors
PROD
AVG
MIN
</equation>
<page confidence="0.968527">
297
</page>
<bodyText confidence="0.9918715">
This yields the three similarity functions cited
above:
</bodyText>
<equation confidence="0.996830666666667">
DICE PROD(w1, w2) = TPROD(w1, w2) (8)
DICE†(w1, w2) = TMIN(w1, w2)
LIN(w1, w2) = TAVG(w1, w2)
</equation>
<bodyText confidence="0.99940725">
Thus, all three of these functions are special cases
of symmetric ratio models. Below, we investigate
asymmetric versions of all three, which we write
as Tα,SI(w1, w2), defined as:
</bodyText>
<equation confidence="0.9905545">
σSI(w1, w2)
α · σSI(w1, w1) + (1 − α) · σSI(w2, w2) (9)
</equation>
<bodyText confidence="0.949022868421053">
Following Lee (1997), who investigates a different
family of asymmetric similarity functions, we will
refer to these as α-skewed measures.
We also will look at a rank-biased family of
measures:
Rα,SI(w1, w2) = Tα,SI(wh, wl)
where wl = arg min w∈{w1,w2}Rank(w)
wh = arg max w∈{w1,w2} Rank(w)
(10)
Here, Tα,SI(wh, wl) is as defined in (9), and the α-
weighted word is always the less frequent word.
For example, consider comparing the 100-feature
vector for dinghy to the 1000 feature vector for
boat: if α is high, we give more weight to the pro-
portion of dinghy’s features that are shared than
we give to the proportion of boat’s features that
are shared.
In the following sections we present data show-
ing that the performance of a dependency-based
similarity system in capturing human similarity
judgments can be greatly improved with rank-
bias and α-skewing. We will investigate the three
asymmetric functions defined above.3 We argue
that the advantages of rank bias are tied to im-
proved similarity estimation when comparing vec-
tors of very different dimensionality. We then
turn to the problem of finding a word’s nearest
semantic neighbors. The nearest neighbor prob-
lem is a rather a natural ground in which to try
out ideas on asymmetry, since the nearest neigh-
bor relation is itself not symmetrical. We show
that α-skewing can be used to improve the quality
of nearest neighbors found for both high- and mid-
frequency words.
3Interestingly, Equation (9) does not yield an asymmetric
version of cosine. Plugging unit vectors into the α-skewed
version of DICE PROD still leaves us with a symmetric func-
tion (COS), whatever the value of α.
</bodyText>
<sectionHeader confidence="0.544317" genericHeader="introduction">
2 Systems
</sectionHeader>
<listItem confidence="0.94862">
1. We parsed the BNC with the Malt Depen-
dency parser (Nivre, 2003) and the Stanford
parser (Klein and Manning, 2003), creating
two dependency DBs, using basically the de-
sign in Lin (1998), with features weighted by
PMI (Church and Hanks, 1990).
2. For each of the 3 rank-biased similarity sys-
tems (Rα,SI) and cosine, we computed corre-
lations with human judgments for the pairs
in 2 standard wordsets: the combined Miller-
Charles/Rubenstein-Goodenough word sets
(Miller and Charles, 1991; Rubenstein and
Goodenough, 1965) and the Wordsim 353
word set (Finkelstein et al., 2002), as well as
to a subset of the Wordsim set restricted to
reflect semantic similarity judgments, which
we will refer to as Wordsim 201.
3. For each of 3 α-skewed similarity systems
(Tα,SI) and cosine, we found the nearest
neighbor from among BNC nouns (of any
rank) for the 10,000 most frequent BNC
nouns using the the dependency DB created
in step 2.
4. To evaluate of the quality of the nearest
neighbors pairs found in Step 4, we scored
them using the Wordnet-based Personalized
Pagerank system described in Agirre (2009)
(UKB), a non distributional WordNet based
measure, and the best system in Table 1.
</listItem>
<sectionHeader confidence="0.978193" genericHeader="method">
3 Human correlations
</sectionHeader>
<bodyText confidence="0.999849764705882">
Table 1 presents the Spearman’s correlation
with human judgments for Cosine, UKB, and
our 3 α-skewed models using Malt-parser
based vectors applied to the combined Miller-
Charles/Rubenstein-Goodenough word sets, the
Wordsim 353 word set, and the Wordsim 202
word set.
The first of each of the column pairs is a sym-
metric system, and the second a rank-biased vari-
ant, based on Equation (10). In all cases, the bi-
ased system improves on the performance of its
symmetric counterpart; in the case of DICE†and
DICE PROD, that improvement is enough for the
biased system to outperform cosine, the best of
the symmetric distributionally based systems. The
value .97 was chosen for α because it produced the
best α-system on the MC/RG corpus. That value
</bodyText>
<page confidence="0.99493">
298
</page>
<table confidence="0.999754428571428">
MC/RG Wdsm201 Wdsm353
α = .5 α = .97 α = .5 α = .97 α = .5 α = .97
Dice DICE PROD .59 .71 .50 .60 .35 .44
LIN .48 .62 .42 .54 .29 .39
DICE† .58 .67 .49 .58 .34 .43
Euc Cosine .65 NA .56 NA .41 NA
WN UKB WN .80 NA .75 NA .68 NA
</table>
<tableCaption confidence="0.999975">
Table 1: System/Human correlations. Above the line: MALT Parser-based systems
</tableCaption>
<figure confidence="0.997590285714286">
0.42
0.40
0.38
0.36
0.34
0.5 0.6 0.7 0.8 0.9 1.0
α value
</figure>
<figureCaption confidence="0.999993">
Figure 1: Scores monotonically increase with α
</figureCaption>
<bodyText confidence="0.999847421052632">
is probably probably an overtrained optimum. The
point is that α-skewing always helps: For all three
systems, the improvement shown in raising α from
.5 to whatever the optimum is is monotonic. This
is shown in Figure 1. Table 2 shows very simi-
lar results using the Stanford parser, demonstrat-
ing the pattern is not limited to a single parsing
model.
In Table 3, we list the pairs whose reranking
on the MC/RG dataset contributed most to the im-
provement of the α = .9 system over the default
α = .5 system. In the last column an approxi-
mation of the amount of correlation improvement
provided by that pair (S):4 Note the 3 of the 5
items contributing the most improvement this sys-
tem were pairs with a large difference in rank.
Choosing α = .9, weights recall toward the rarer
word. We conjecture that the reason this helps is
Tversky’s principle: It is natural to use the sparser
</bodyText>
<footnote confidence="0.7315674">
4The approximation is based on the formula for comput-
ing Spearman’s R with no ties. If n is the number of items,
then the improvement on that item is:
6 * [(baseline − gold)2 − (test − gold)2]
n * (n2 − 1)
</footnote>
<table confidence="0.954038833333333">
Word 1 Rank Word 2 Rank S
automobile 7411 car 100 0.030
asylum 3540 madhouse 14703 0.020
coast 708 hill 949 0.018
mound 3089 stove 2885 0.017
autograph 10136 signature 2743 0.009
</table>
<tableCaption confidence="0.9865545">
Table 3: Pairs contributing the biggest improve-
ment, MC/RG word set
</tableCaption>
<bodyText confidence="0.988043">
representation as the focus in the comparison.
</bodyText>
<sectionHeader confidence="0.937465" genericHeader="method">
4 Nearest neighbors
</sectionHeader>
<bodyText confidence="0.999774571428571">
Figure 2 gives the results of our nearest neighbor
study on the BNC for the case of DICE PROD. The
graphs for the other two α-skewed systems are
nearly identical, and are not shown due to space
limitations. The target word, the word whose
nearest neighbor is being found, always receives
the weight 1 − α. The x-axis shows target word
rank; the y-axis shows the average UKB simi-
larity scores assigned to nearest neighbors every
50 ranks. All the systems show degraded nearest
neighbor quality as target words grow rare, but at
lower ranks, the α = .04 nearest neighbor system
fares considerably better than the symmetric α =
.50 system; the line across the bottom tracks the
score of a system with randomly generated near-
est neighbors. The symmetric DICE PROD sys-
tem is as an excellent nearest neighbor system at
high ranks but drops below the α = .04 system at
around rank 3500. We see that the α = .8 system
is even better than the symmetric system at high
ranks, but degrades much more quickly.
We explain these results on the basis of the prin-
ciple developed for the human correlation data: To
reflect natural judgments of similarity for compar-
isons of representations of differing sparseness, α
should be tipped toward the sparser representation.
Thus, α = .80 works best for high rank tar-
get words, because most nearest neighbor candi-
</bodyText>
<page confidence="0.996789">
299
</page>
<table confidence="0.9997102">
MC/RG opt α Wdsm201 opt α Wdsm353 opt α
α = .5 opt α = .5 opt α = .5 opt
DICE PROD .65 .70 .86 .42 .57 .99 .36 .44 .98
LIN .58 .68 .90 .41 .56 .94 .30 .41 .99
DICE† .60 .71 .91 .43 .53 .99 .32 .43 .99
</table>
<tableCaption confidence="0.999396">
Table 2: System/Human correlations for Stanford parser systems
</tableCaption>
<figure confidence="0.999034357142857">
0.035
alpha04
alpha_50
alpha_80
random
0.025
0.020
0.015
0.010
0.005
0.0000 2000 4000 6000 8000 10000
Word rank
-IN
0.030
</figure>
<figureCaption confidence="0.732653333333333">
Figure 2: UKB evaluation scores for nearest
neighbor pairs across word ranks, sampled every
50 ranks.
</figureCaption>
<bodyText confidence="0.988494285714286">
dates are less frequent, and α = .8 tips the bal-
ance toward the nontarget words. On the other
hand, when the target word is a low ranking word,
a high α weight means it never receives the high-
est weight, and this is disastrous, since most good
candidates are higher ranking. Conversely, α =
.04 works better.
</bodyText>
<sectionHeader confidence="0.996761" genericHeader="method">
5 Previous work
</sectionHeader>
<bodyText confidence="0.999188466666667">
The debt owed to Tversky (1977) has been made
clear in the introduction. Less clear is the debt
owed to Jimenez et al. (2012), which also pro-
poses an asymmetric similarity framework based
on Tversky’s insights. Jimenez et al. showed the
continued relevance of Tversky’s work.
Motivated by the problem of measuring how
well the distribution of one word w1 captures the
distribution of another w2, Weeds and Weir (2005)
also explore asymmetric models, expressing sim-
ilarity calculations as weighted combinations of
several variants of what they call precision and re-
call. Some of their models are also Tverskyan ratio
models. To see this, we divide (9) everywhere by
U(w1, w2):
</bodyText>
<equation confidence="0.923216666666667">
TSI(w1,w2) = α·σ(w1,w1) (1−α)·σ(w2,w2)
σ(w1,w2) + σ(w1,w2)
1
</equation>
<bodyText confidence="0.999395333333333">
If the SI is MIN, then the two terms in the de-
nominator are the inverses of what W&amp;W call
difference-weighted precision and recall:
</bodyText>
<equation confidence="0.995283875">
PREC(w1,w2) = σMIN(w1,w2)
σMIN(w1,w1)
REC(w1, w2) = σMIN(w1,w2)
σMIN(w2,w2),
So for TMIN, (9) can be rewritten:
1
α 1−α
PREC(w1,w2) + REC(w1,w2)
</equation>
<bodyText confidence="0.9973210625">
That is, TMIN is a weighted harmonic mean of
precision and recall, the so-called weighted F-
measure (Manning and Sch¨utze, 1999). W&amp;W’s
additive precision/recall models appear not to be
Tversky models, since they compute separate
sums for precision and recall from the f ∈ w1 ∩
w2, one using w1[f], and one using w2[f].
Long before Weed and Weir, Lee (1999) pro-
posed an asymmetric similarity measure as well.
Like Weeds and Weir, her perspective was to cal-
culate the effectiveness of using one distribution as
a proxy for the other, a fundamentally asymmetric
problem. For distributions q and r, Lee’s α-skew
divergence takes the KL-divergence of a mixture
of q and r from q, using the α parameter to define
the proportions in the mixture.
</bodyText>
<sectionHeader confidence="0.999249" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999993875">
We have shown that Tversky’s asymmetric ratio
models can improve performance in capturing
human judgments and produce better nearest
neighbors. To validate these very preliminary
results, we need to explore applications compat-
ible with asymmetry, such as the TOEFL-like
synonym discovery task in Freitag et al. (2005),
and the PP-attachment task in Dagan et al. (1999).
</bodyText>
<sectionHeader confidence="0.998666" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9539155">
This work reported here was supported by NSF
CDI grant # 1028177.
</bodyText>
<page confidence="0.995992">
300
</page>
<sectionHeader confidence="0.98884" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99979567948718">
E. Agirre, E. Alfonseca, K. Hall, J. Kravalova,
M. Pasca, and A. Soroa. 2009. A study on similar-
ity and relatedness using distributional and wordnet-
based approaches. In Proceedings of NAACL-HLT
09, Boulder, Co.
K.W. Church and P. Hanks. 1990. Word association
norms, mutual information, and lexicography. Com-
putational linguistics, 16(1):22–29.
J.R. Curran. 2004. From Distributional to Semantic
Similarity. Ph.D. thesis, University of Edinburgh.
College of Science and Engineering. School of In-
formatics.
I. Dagan, L. Lee, and F.C.N. Pereira. 1999. Similarity-
based models of word cooccurrence probabilities.
Machine Learning, 34(1):43–69.
L.R. Dice. 1945. Measures of the amount of ecologic
association between species. Ecology, 26(3):297–
302.
L. Finkelstein, E. Gabrilovich, Yossi Matias, Ehud
Rivlin, Zach Solan, Gadi Wolfman, and Eytan Rup-
pin. 2002. Placing search in context: The concept
revisited. ACM Transactions on Information Sys-
tems, 20(1):116–131.
D. Freitag, M. Blume, J. Byrnes, E. Chow, S. Kapadia,
R. Rohwer, and Z. Wang. 2005. New experiments in
distributional representations of synonymy. In Pro-
ceedings of the Ninth Conference on Computational
Natural Language Learning, pages 25–32. Associa-
tion for Computational Linguistics.
R. L. Goldstone. in press. Similarity. In R.A. Wilson
Wilson and F. C. Keil, editors, MIT Encylcopedia of
Cognitive Sciences. MIT Press, Cambridge, MA.
S. Jimenez, C. Becerra, and A. Gelbukh. 2012. Soft
cardinality: A parameterized similarity function for
text comparison. In Proceedings of the First Joint
Conference on Lexical and Computational Seman-
tics, pages 449–453. Association for Computational
Linguistics.
D. Klein and Christopher D. Manning. 2003. Fast ex-
act inference with a factored model for natural lan-
guage parsing. In Advances in Neural Information
Processing Systems 15 (NIPS 2002), pages 3–10,
Cambridge, MA. MIT Press.
L. Lee. 1997. Similarity-based approaches to natural
language processing. Ph.D. thesis, Harvard Univer-
sity.
L. Lee. 1999. Measures of distributional similarity.
In Proceedings of the 37th annual meeting of the
Association for Computational Linguistics on Com-
putational Linguistics, pages 25–32. Association for
Computational Linguistics.
D. Lin. 1998. Automatic retrieval and clustering of
similar words. In Annual Meeting-Association for
Computational Linguistics, volume 36, pages 768–
774. Association for Computational Linguistics.
C.D. Manning and H. Sch¨utze. 1999. Foundations of
statistical natural language processing. MIT Press,
Cambridge.
G.A. Miller and W.G. Charles. 1991. Contextual cor-
relates of semantic similarity. Language and Cogni-
tive Processes, 6(1):1–28.
J. Nivre. 2003. An efficient algorithm for projective
dependency parsing. In Proceedings of the 8th Inter-
national Workshop on Parsing Technologies (IWPT
03), pages 149–160.
E. Rosch and C. B. Mervis. 1975. Family resem-
blances: Studies in the internal structure of cate-
gories. Cognitive psychology, 7(4):573–605.
H. Rubenstein and J.B. Goodenough. 1965. Contex-
tual correlates of synonymy. Communications of the
ACM, 8:627–633.
A. Tversky. 1977. Features of similarity. Psychologi-
cal Review, 84:327–352.
C. J. van Rijsbergen. 1979. Information retrieval.
Butterworth-Heinemann, Oxford.
J. Weeds and D. Weir. 2005. Co-occurrence retrieval:
A flexible framework for lexical distributional simi-
larity. Computational Linguistics, 31(4):439–475.
</reference>
<page confidence="0.998916">
301
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.876463">
<title confidence="0.999776">Improving sparse word similarity models with asymmetric measures</title>
<author confidence="0.996068">Jean Mark Gawron San Diego State</author>
<email confidence="0.999777">gawron@mail.sdsu.edu</email>
<abstract confidence="0.991024923076923">We show that asymmetric models based on Tversky (1977) improve correlations with human similarity judgments and nearest neighbor discovery for both frequent and middle-rank words. In accord with Tversky’s discovery that asymmetric similarity judgments arise when comparing sparse and rich representations, improvement on our two tasks can be traced to heavily weighting the feature bias toward the rarer word when comparing highand midfrequency words.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>E Alfonseca</author>
<author>K Hall</author>
<author>J Kravalova</author>
<author>M Pasca</author>
<author>A Soroa</author>
</authors>
<title>A study on similarity and relatedness using distributional and wordnetbased approaches.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL-HLT 09,</booktitle>
<location>Boulder, Co.</location>
<marker>Agirre, Alfonseca, Hall, Kravalova, Pasca, Soroa, 2009</marker>
<rawString>E. Agirre, E. Alfonseca, K. Hall, J. Kravalova, M. Pasca, and A. Soroa. 2009. A study on similarity and relatedness using distributional and wordnetbased approaches. In Proceedings of NAACL-HLT 09, Boulder, Co.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K W Church</author>
<author>P Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1990</date>
<booktitle>Computational linguistics,</booktitle>
<pages>16--1</pages>
<contexts>
<context position="9853" citStr="Church and Hanks, 1990" startWordPosition="1634" endWordPosition="1637">lation is itself not symmetrical. We show that α-skewing can be used to improve the quality of nearest neighbors found for both high- and midfrequency words. 3Interestingly, Equation (9) does not yield an asymmetric version of cosine. Plugging unit vectors into the α-skewed version of DICE PROD still leaves us with a symmetric function (COS), whatever the value of α. 2 Systems 1. We parsed the BNC with the Malt Dependency parser (Nivre, 2003) and the Stanford parser (Klein and Manning, 2003), creating two dependency DBs, using basically the design in Lin (1998), with features weighted by PMI (Church and Hanks, 1990). 2. For each of the 3 rank-biased similarity systems (Rα,SI) and cosine, we computed correlations with human judgments for the pairs in 2 standard wordsets: the combined MillerCharles/Rubenstein-Goodenough word sets (Miller and Charles, 1991; Rubenstein and Goodenough, 1965) and the Wordsim 353 word set (Finkelstein et al., 2002), as well as to a subset of the Wordsim set restricted to reflect semantic similarity judgments, which we will refer to as Wordsim 201. 3. For each of 3 α-skewed similarity systems (Tα,SI) and cosine, we found the nearest neighbor from among BNC nouns (of any rank) fo</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>K.W. Church and P. Hanks. 1990. Word association norms, mutual information, and lexicography. Computational linguistics, 16(1):22–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Curran</author>
</authors>
<title>From Distributional to Semantic Similarity.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh. College of Science and Engineering. School of Informatics.</institution>
<contexts>
<context position="6255" citStr="Curran (2004)" startWordPosition="1018" endWordPosition="1020">ed in favor of w1 as the referent; When α &lt; 0.5, sim(w1, w2) is biased in favor of w2. Consider four similarity functions that have played important roles in the literature on similarity: DICE PROD(w1, w2) = 11w111 2*2 1·w2 2 +11w211 2*P fEw1∩w2 min(w1[f], w2[f]) P w1[f]+P w2[f] P fEw1∩w2 w1[f]+ w2[f] = The function DICE PROD is not well known in the word similarity literature, but in the data mining literature it is often just called Dice coefficient, because it generalized the set comparison function of Dice (1945). Observe that cosine is a special case of DICE PROD. DICE† was introduced in Curran (2004) and was the most successful function in his evaluation. Since LIN was introduced in Lin (1998); several different functions have born that name. The version used here is the one used in Curran (2004). The three distinct functions in Equation 6 have a similar form. In fact, all can be defined in terms of σ functions differing only in their SI operation. Let σSI be a shared feature sum for operation SI, as defined in Equation (3). We define the Tverskynormalized version of σSI, written TSI, as:1 TSI(w1,w2) = (7) σSI(w1, w1) + σSI(w2, w2) 2 σSI(w1,w2) Note that TSI is just the special case of Tv</context>
</contexts>
<marker>Curran, 2004</marker>
<rawString>J.R. Curran. 2004. From Distributional to Semantic Similarity. Ph.D. thesis, University of Edinburgh. College of Science and Engineering. School of Informatics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>L Lee</author>
<author>F C N Pereira</author>
</authors>
<title>Similaritybased models of word cooccurrence probabilities.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>34</volume>
<issue>1</issue>
<marker>Dagan, Lee, Pereira, 1999</marker>
<rawString>I. Dagan, L. Lee, and F.C.N. Pereira. 1999. Similaritybased models of word cooccurrence probabilities. Machine Learning, 34(1):43–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Dice</author>
</authors>
<title>Measures of the amount of ecologic association between species.</title>
<date>1945</date>
<journal>Ecology,</journal>
<volume>26</volume>
<issue>3</issue>
<pages>302</pages>
<contexts>
<context position="6164" citStr="Dice (1945)" startWordPosition="1002" endWordPosition="1003">e that sim(w1, w2) is symmetric if and only if α = 0.5. When α &gt; 0.5, sim(w1, w2) is biased in favor of w1 as the referent; When α &lt; 0.5, sim(w1, w2) is biased in favor of w2. Consider four similarity functions that have played important roles in the literature on similarity: DICE PROD(w1, w2) = 11w111 2*2 1·w2 2 +11w211 2*P fEw1∩w2 min(w1[f], w2[f]) P w1[f]+P w2[f] P fEw1∩w2 w1[f]+ w2[f] = The function DICE PROD is not well known in the word similarity literature, but in the data mining literature it is often just called Dice coefficient, because it generalized the set comparison function of Dice (1945). Observe that cosine is a special case of DICE PROD. DICE† was introduced in Curran (2004) and was the most successful function in his evaluation. Since LIN was introduced in Lin (1998); several different functions have born that name. The version used here is the one used in Curran (2004). The three distinct functions in Equation 6 have a similar form. In fact, all can be defined in terms of σ functions differing only in their SI operation. Let σSI be a shared feature sum for operation SI, as defined in Equation (3). We define the Tverskynormalized version of σSI, written TSI, as:1 TSI(w1,w2</context>
</contexts>
<marker>Dice, 1945</marker>
<rawString>L.R. Dice. 1945. Measures of the amount of ecologic association between species. Ecology, 26(3):297– 302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Finkelstein</author>
<author>E Gabrilovich</author>
<author>Yossi Matias</author>
<author>Ehud Rivlin</author>
<author>Zach Solan</author>
<author>Gadi Wolfman</author>
<author>Eytan Ruppin</author>
</authors>
<title>Placing search in context: The concept revisited.</title>
<date>2002</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="10185" citStr="Finkelstein et al., 2002" startWordPosition="1685" endWordPosition="1688">unction (COS), whatever the value of α. 2 Systems 1. We parsed the BNC with the Malt Dependency parser (Nivre, 2003) and the Stanford parser (Klein and Manning, 2003), creating two dependency DBs, using basically the design in Lin (1998), with features weighted by PMI (Church and Hanks, 1990). 2. For each of the 3 rank-biased similarity systems (Rα,SI) and cosine, we computed correlations with human judgments for the pairs in 2 standard wordsets: the combined MillerCharles/Rubenstein-Goodenough word sets (Miller and Charles, 1991; Rubenstein and Goodenough, 1965) and the Wordsim 353 word set (Finkelstein et al., 2002), as well as to a subset of the Wordsim set restricted to reflect semantic similarity judgments, which we will refer to as Wordsim 201. 3. For each of 3 α-skewed similarity systems (Tα,SI) and cosine, we found the nearest neighbor from among BNC nouns (of any rank) for the 10,000 most frequent BNC nouns using the the dependency DB created in step 2. 4. To evaluate of the quality of the nearest neighbors pairs found in Step 4, we scored them using the Wordnet-based Personalized Pagerank system described in Agirre (2009) (UKB), a non distributional WordNet based measure, and the best system in T</context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, Ruppin, 2002</marker>
<rawString>L. Finkelstein, E. Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. 2002. Placing search in context: The concept revisited. ACM Transactions on Information Systems, 20(1):116–131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Freitag</author>
<author>M Blume</author>
<author>J Byrnes</author>
<author>E Chow</author>
<author>S Kapadia</author>
<author>R Rohwer</author>
<author>Z Wang</author>
</authors>
<title>New experiments in distributional representations of synonymy.</title>
<date>2005</date>
<booktitle>In Proceedings of the Ninth Conference on Computational Natural Language Learning,</booktitle>
<pages>25--32</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Freitag, Blume, Byrnes, Chow, Kapadia, Rohwer, Wang, 2005</marker>
<rawString>D. Freitag, M. Blume, J. Byrnes, E. Chow, S. Kapadia, R. Rohwer, and Z. Wang. 2005. New experiments in distributional representations of synonymy. In Proceedings of the Ninth Conference on Computational Natural Language Learning, pages 25–32. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>R L Goldstone</author>
</authors>
<booktitle>MIT Encylcopedia of Cognitive Sciences.</booktitle>
<editor>in press. Similarity. In R.A. Wilson Wilson and F. C. Keil, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Goldstone, </marker>
<rawString>R. L. Goldstone. in press. Similarity. In R.A. Wilson Wilson and F. C. Keil, editors, MIT Encylcopedia of Cognitive Sciences. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Jimenez</author>
<author>C Becerra</author>
<author>A Gelbukh</author>
</authors>
<title>Soft cardinality: A parameterized similarity function for text comparison.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics,</booktitle>
<pages>449--453</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="15655" citStr="Jimenez et al. (2012)" startWordPosition="2683" endWordPosition="2686">015 0.010 0.005 0.0000 2000 4000 6000 8000 10000 Word rank -IN 0.030 Figure 2: UKB evaluation scores for nearest neighbor pairs across word ranks, sampled every 50 ranks. dates are less frequent, and α = .8 tips the balance toward the nontarget words. On the other hand, when the target word is a low ranking word, a high α weight means it never receives the highest weight, and this is disastrous, since most good candidates are higher ranking. Conversely, α = .04 works better. 5 Previous work The debt owed to Tversky (1977) has been made clear in the introduction. Less clear is the debt owed to Jimenez et al. (2012), which also proposes an asymmetric similarity framework based on Tversky’s insights. Jimenez et al. showed the continued relevance of Tversky’s work. Motivated by the problem of measuring how well the distribution of one word w1 captures the distribution of another w2, Weeds and Weir (2005) also explore asymmetric models, expressing similarity calculations as weighted combinations of several variants of what they call precision and recall. Some of their models are also Tverskyan ratio models. To see this, we divide (9) everywhere by U(w1, w2): TSI(w1,w2) = α·σ(w1,w1) (1−α)·σ(w2,w2) σ(w1,w2) +</context>
</contexts>
<marker>Jimenez, Becerra, Gelbukh, 2012</marker>
<rawString>S. Jimenez, C. Becerra, and A. Gelbukh. 2012. Soft cardinality: A parameterized similarity function for text comparison. In Proceedings of the First Joint Conference on Lexical and Computational Semantics, pages 449–453. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Fast exact inference with a factored model for natural language parsing.</title>
<date>2003</date>
<booktitle>In Advances in Neural Information Processing Systems 15 (NIPS</booktitle>
<pages>3--10</pages>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="9726" citStr="Klein and Manning, 2003" startWordPosition="1613" endWordPosition="1616"> The nearest neighbor problem is a rather a natural ground in which to try out ideas on asymmetry, since the nearest neighbor relation is itself not symmetrical. We show that α-skewing can be used to improve the quality of nearest neighbors found for both high- and midfrequency words. 3Interestingly, Equation (9) does not yield an asymmetric version of cosine. Plugging unit vectors into the α-skewed version of DICE PROD still leaves us with a symmetric function (COS), whatever the value of α. 2 Systems 1. We parsed the BNC with the Malt Dependency parser (Nivre, 2003) and the Stanford parser (Klein and Manning, 2003), creating two dependency DBs, using basically the design in Lin (1998), with features weighted by PMI (Church and Hanks, 1990). 2. For each of the 3 rank-biased similarity systems (Rα,SI) and cosine, we computed correlations with human judgments for the pairs in 2 standard wordsets: the combined MillerCharles/Rubenstein-Goodenough word sets (Miller and Charles, 1991; Rubenstein and Goodenough, 1965) and the Wordsim 353 word set (Finkelstein et al., 2002), as well as to a subset of the Wordsim set restricted to reflect semantic similarity judgments, which we will refer to as Wordsim 201. 3. Fo</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>D. Klein and Christopher D. Manning. 2003. Fast exact inference with a factored model for natural language parsing. In Advances in Neural Information Processing Systems 15 (NIPS 2002), pages 3–10, Cambridge, MA. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Lee</author>
</authors>
<title>Similarity-based approaches to natural language processing.</title>
<date>1997</date>
<tech>Ph.D. thesis,</tech>
<institution>Harvard University.</institution>
<contexts>
<context position="2872" citStr="Lee, 1997" startWordPosition="457" endWordPosition="458">ity occurs when an object with many features is judged as less similar to a sparser object than vice versa; for example, North Korea is judged to be more like China than China is [like] North Korea.” Thus, one source of asymmetry is the comparison of sparse and dense representations. The relevance of such considerations to word similarity becomes clear when we consider that for many applications, word similarity measures need to be well-defined when comparing very frequent words with infrequent words. To make this concrete, let us consider a word representation in the word-as-vector paradigm (Lee, 1997; Lin, 1998), using a dependency-based model. Suppose we want to measure the semantic similarity of boat, rank 682 among the nouns in the BNC corpus studied below, which has 1057 nonzero dependency features based on 50 million words of data, with dinghy, rank 6200, which has only 113 nonzero features. At the level of the vector representations we are using, these are events of very different dimensionality; that is, there are ten times as many features in the representation of boat as there are in the representation of dinghy. If in Tversky/Rosch terms, the more frequent word is also a more li</context>
<context position="7978" citStr="Lee (1997)" startWordPosition="1320" endWordPosition="1321">ctions discussed above are monotonic in Jaccard. 2UPROD, of course, is dot product. X fEAnB = P DICE†(w1, w2) = LIN(w1,w2) P w1[f]+Pw2[f] COS(w1, w2) = DICE PROD applied to unit vectors PROD AVG MIN 297 This yields the three similarity functions cited above: DICE PROD(w1, w2) = TPROD(w1, w2) (8) DICE†(w1, w2) = TMIN(w1, w2) LIN(w1, w2) = TAVG(w1, w2) Thus, all three of these functions are special cases of symmetric ratio models. Below, we investigate asymmetric versions of all three, which we write as Tα,SI(w1, w2), defined as: σSI(w1, w2) α · σSI(w1, w1) + (1 − α) · σSI(w2, w2) (9) Following Lee (1997), who investigates a different family of asymmetric similarity functions, we will refer to these as α-skewed measures. We also will look at a rank-biased family of measures: Rα,SI(w1, w2) = Tα,SI(wh, wl) where wl = arg min w∈{w1,w2}Rank(w) wh = arg max w∈{w1,w2} Rank(w) (10) Here, Tα,SI(wh, wl) is as defined in (9), and the α- weighted word is always the less frequent word. For example, consider comparing the 100-feature vector for dinghy to the 1000 feature vector for boat: if α is high, we give more weight to the proportion of dinghy’s features that are shared than we give to the proportion </context>
</contexts>
<marker>Lee, 1997</marker>
<rawString>L. Lee. 1997. Similarity-based approaches to natural language processing. Ph.D. thesis, Harvard University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Lee</author>
</authors>
<title>Measures of distributional similarity.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics,</booktitle>
<pages>25--32</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="16900" citStr="Lee (1999)" startWordPosition="2887" endWordPosition="2888">the two terms in the denominator are the inverses of what W&amp;W call difference-weighted precision and recall: PREC(w1,w2) = σMIN(w1,w2) σMIN(w1,w1) REC(w1, w2) = σMIN(w1,w2) σMIN(w2,w2), So for TMIN, (9) can be rewritten: 1 α 1−α PREC(w1,w2) + REC(w1,w2) That is, TMIN is a weighted harmonic mean of precision and recall, the so-called weighted Fmeasure (Manning and Sch¨utze, 1999). W&amp;W’s additive precision/recall models appear not to be Tversky models, since they compute separate sums for precision and recall from the f ∈ w1 ∩ w2, one using w1[f], and one using w2[f]. Long before Weed and Weir, Lee (1999) proposed an asymmetric similarity measure as well. Like Weeds and Weir, her perspective was to calculate the effectiveness of using one distribution as a proxy for the other, a fundamentally asymmetric problem. For distributions q and r, Lee’s α-skew divergence takes the KL-divergence of a mixture of q and r from q, using the α parameter to define the proportions in the mixture. 6 Conclusion We have shown that Tversky’s asymmetric ratio models can improve performance in capturing human judgments and produce better nearest neighbors. To validate these very preliminary results, we need to explo</context>
</contexts>
<marker>Lee, 1999</marker>
<rawString>L. Lee. 1999. Measures of distributional similarity. In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics, pages 25–32. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Annual Meeting-Association for Computational Linguistics,</booktitle>
<volume>36</volume>
<pages>768--774</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2884" citStr="Lin, 1998" startWordPosition="459" endWordPosition="460">when an object with many features is judged as less similar to a sparser object than vice versa; for example, North Korea is judged to be more like China than China is [like] North Korea.” Thus, one source of asymmetry is the comparison of sparse and dense representations. The relevance of such considerations to word similarity becomes clear when we consider that for many applications, word similarity measures need to be well-defined when comparing very frequent words with infrequent words. To make this concrete, let us consider a word representation in the word-as-vector paradigm (Lee, 1997; Lin, 1998), using a dependency-based model. Suppose we want to measure the semantic similarity of boat, rank 682 among the nouns in the BNC corpus studied below, which has 1057 nonzero dependency features based on 50 million words of data, with dinghy, rank 6200, which has only 113 nonzero features. At the level of the vector representations we are using, these are events of very different dimensionality; that is, there are ten times as many features in the representation of boat as there are in the representation of dinghy. If in Tversky/Rosch terms, the more frequent word is also a more likely focus, </context>
<context position="6350" citStr="Lin (1998)" startWordPosition="1035" endWordPosition="1036">ur similarity functions that have played important roles in the literature on similarity: DICE PROD(w1, w2) = 11w111 2*2 1·w2 2 +11w211 2*P fEw1∩w2 min(w1[f], w2[f]) P w1[f]+P w2[f] P fEw1∩w2 w1[f]+ w2[f] = The function DICE PROD is not well known in the word similarity literature, but in the data mining literature it is often just called Dice coefficient, because it generalized the set comparison function of Dice (1945). Observe that cosine is a special case of DICE PROD. DICE† was introduced in Curran (2004) and was the most successful function in his evaluation. Since LIN was introduced in Lin (1998); several different functions have born that name. The version used here is the one used in Curran (2004). The three distinct functions in Equation 6 have a similar form. In fact, all can be defined in terms of σ functions differing only in their SI operation. Let σSI be a shared feature sum for operation SI, as defined in Equation (3). We define the Tverskynormalized version of σSI, written TSI, as:1 TSI(w1,w2) = (7) σSI(w1, w1) + σSI(w2, w2) 2 σSI(w1,w2) Note that TSI is just the special case of Tversky’s ratio model (5) in which α = 0.5 and the similarity measure is symmetric. We define thr</context>
<context position="9797" citStr="Lin (1998)" startWordPosition="1627" endWordPosition="1628">on asymmetry, since the nearest neighbor relation is itself not symmetrical. We show that α-skewing can be used to improve the quality of nearest neighbors found for both high- and midfrequency words. 3Interestingly, Equation (9) does not yield an asymmetric version of cosine. Plugging unit vectors into the α-skewed version of DICE PROD still leaves us with a symmetric function (COS), whatever the value of α. 2 Systems 1. We parsed the BNC with the Malt Dependency parser (Nivre, 2003) and the Stanford parser (Klein and Manning, 2003), creating two dependency DBs, using basically the design in Lin (1998), with features weighted by PMI (Church and Hanks, 1990). 2. For each of the 3 rank-biased similarity systems (Rα,SI) and cosine, we computed correlations with human judgments for the pairs in 2 standard wordsets: the combined MillerCharles/Rubenstein-Goodenough word sets (Miller and Charles, 1991; Rubenstein and Goodenough, 1965) and the Wordsim 353 word set (Finkelstein et al., 2002), as well as to a subset of the Wordsim set restricted to reflect semantic similarity judgments, which we will refer to as Wordsim 201. 3. For each of 3 α-skewed similarity systems (Tα,SI) and cosine, we found th</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>D. Lin. 1998. Automatic retrieval and clustering of similar words. In Annual Meeting-Association for Computational Linguistics, volume 36, pages 768– 774. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>H Sch¨utze</author>
</authors>
<title>Foundations of statistical natural language processing.</title>
<date>1999</date>
<publisher>MIT Press,</publisher>
<location>Cambridge.</location>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>C.D. Manning and H. Sch¨utze. 1999. Foundations of statistical natural language processing. MIT Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
<author>W G Charles</author>
</authors>
<title>Contextual correlates of semantic similarity.</title>
<date>1991</date>
<booktitle>Language and Cognitive Processes,</booktitle>
<pages>6--1</pages>
<contexts>
<context position="10095" citStr="Miller and Charles, 1991" startWordPosition="1671" endWordPosition="1674">ng unit vectors into the α-skewed version of DICE PROD still leaves us with a symmetric function (COS), whatever the value of α. 2 Systems 1. We parsed the BNC with the Malt Dependency parser (Nivre, 2003) and the Stanford parser (Klein and Manning, 2003), creating two dependency DBs, using basically the design in Lin (1998), with features weighted by PMI (Church and Hanks, 1990). 2. For each of the 3 rank-biased similarity systems (Rα,SI) and cosine, we computed correlations with human judgments for the pairs in 2 standard wordsets: the combined MillerCharles/Rubenstein-Goodenough word sets (Miller and Charles, 1991; Rubenstein and Goodenough, 1965) and the Wordsim 353 word set (Finkelstein et al., 2002), as well as to a subset of the Wordsim set restricted to reflect semantic similarity judgments, which we will refer to as Wordsim 201. 3. For each of 3 α-skewed similarity systems (Tα,SI) and cosine, we found the nearest neighbor from among BNC nouns (of any rank) for the 10,000 most frequent BNC nouns using the the dependency DB created in step 2. 4. To evaluate of the quality of the nearest neighbors pairs found in Step 4, we scored them using the Wordnet-based Personalized Pagerank system described in</context>
</contexts>
<marker>Miller, Charles, 1991</marker>
<rawString>G.A. Miller and W.G. Charles. 1991. Contextual correlates of semantic similarity. Language and Cognitive Processes, 6(1):1–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
</authors>
<title>An efficient algorithm for projective dependency parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 8th International Workshop on Parsing Technologies (IWPT 03),</booktitle>
<pages>149--160</pages>
<contexts>
<context position="9676" citStr="Nivre, 2003" startWordPosition="1607" endWordPosition="1608">g a word’s nearest semantic neighbors. The nearest neighbor problem is a rather a natural ground in which to try out ideas on asymmetry, since the nearest neighbor relation is itself not symmetrical. We show that α-skewing can be used to improve the quality of nearest neighbors found for both high- and midfrequency words. 3Interestingly, Equation (9) does not yield an asymmetric version of cosine. Plugging unit vectors into the α-skewed version of DICE PROD still leaves us with a symmetric function (COS), whatever the value of α. 2 Systems 1. We parsed the BNC with the Malt Dependency parser (Nivre, 2003) and the Stanford parser (Klein and Manning, 2003), creating two dependency DBs, using basically the design in Lin (1998), with features weighted by PMI (Church and Hanks, 1990). 2. For each of the 3 rank-biased similarity systems (Rα,SI) and cosine, we computed correlations with human judgments for the pairs in 2 standard wordsets: the combined MillerCharles/Rubenstein-Goodenough word sets (Miller and Charles, 1991; Rubenstein and Goodenough, 1965) and the Wordsim 353 word set (Finkelstein et al., 2002), as well as to a subset of the Wordsim set restricted to reflect semantic similarity judgm</context>
</contexts>
<marker>Nivre, 2003</marker>
<rawString>J. Nivre. 2003. An efficient algorithm for projective dependency parsing. In Proceedings of the 8th International Workshop on Parsing Technologies (IWPT 03), pages 149–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Rosch</author>
<author>C B Mervis</author>
</authors>
<title>Family resemblances: Studies in the internal structure of categories.</title>
<date>1975</date>
<journal>Cognitive psychology,</journal>
<volume>7</volume>
<issue>4</issue>
<marker>Rosch, Mervis, 1975</marker>
<rawString>E. Rosch and C. B. Mervis. 1975. Family resemblances: Studies in the internal structure of categories. Cognitive psychology, 7(4):573–605.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Rubenstein</author>
<author>J B Goodenough</author>
</authors>
<title>Contextual correlates of synonymy.</title>
<date>1965</date>
<journal>Communications of the ACM,</journal>
<pages>8--627</pages>
<contexts>
<context position="10129" citStr="Rubenstein and Goodenough, 1965" startWordPosition="1675" endWordPosition="1678">-skewed version of DICE PROD still leaves us with a symmetric function (COS), whatever the value of α. 2 Systems 1. We parsed the BNC with the Malt Dependency parser (Nivre, 2003) and the Stanford parser (Klein and Manning, 2003), creating two dependency DBs, using basically the design in Lin (1998), with features weighted by PMI (Church and Hanks, 1990). 2. For each of the 3 rank-biased similarity systems (Rα,SI) and cosine, we computed correlations with human judgments for the pairs in 2 standard wordsets: the combined MillerCharles/Rubenstein-Goodenough word sets (Miller and Charles, 1991; Rubenstein and Goodenough, 1965) and the Wordsim 353 word set (Finkelstein et al., 2002), as well as to a subset of the Wordsim set restricted to reflect semantic similarity judgments, which we will refer to as Wordsim 201. 3. For each of 3 α-skewed similarity systems (Tα,SI) and cosine, we found the nearest neighbor from among BNC nouns (of any rank) for the 10,000 most frequent BNC nouns using the the dependency DB created in step 2. 4. To evaluate of the quality of the nearest neighbors pairs found in Step 4, we scored them using the Wordnet-based Personalized Pagerank system described in Agirre (2009) (UKB), a non distri</context>
</contexts>
<marker>Rubenstein, Goodenough, 1965</marker>
<rawString>H. Rubenstein and J.B. Goodenough. 1965. Contextual correlates of synonymy. Communications of the ACM, 8:627–633.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Tversky</author>
</authors>
<title>Features of similarity.</title>
<date>1977</date>
<journal>Psychological Review,</journal>
<pages>84--327</pages>
<contexts>
<context position="1204" citStr="Tversky (1977)" startWordPosition="179" endWordPosition="180">s. 1 Introduction A key assumption of most models of similarity is that a similarity relation is symmetric. This assumption is foundational for some conceptions, such as the idea of a similarity space, in which similarity is the inverse of distance; and it is deeply embedded into many of the algorithms that build on a similarity relation among objects, such as clustering algorithms. The symmetry assumption is not, however, universal, and it is not essential to all applications of similarity, especially when it comes to modeling human similarity judgments. Citing a number of empirical studies, Tversky (1977) calls symmetry directly into question, and proposes two general models that abandon symmetry. The one most directly related to a large body of word similarity work that followed is what he calls the ratio model, which defines sim(a, b) as: f(AnB) (1) f (A n B) + α f (A\B) + Qf (B\A) Here A and B represent feature sets for the objects a and b respectively; the term in the numerator is a function of the set of shared features, a measure of similarity, and the last two terms in the denominator measure dissimilarity: α and Q are real-number weights; when α =� Q, symmetry is abandoned. To motivate</context>
<context position="15561" citStr="Tversky (1977)" startWordPosition="2667" endWordPosition="2668">tions for Stanford parser systems 0.035 alpha04 alpha_50 alpha_80 random 0.025 0.020 0.015 0.010 0.005 0.0000 2000 4000 6000 8000 10000 Word rank -IN 0.030 Figure 2: UKB evaluation scores for nearest neighbor pairs across word ranks, sampled every 50 ranks. dates are less frequent, and α = .8 tips the balance toward the nontarget words. On the other hand, when the target word is a low ranking word, a high α weight means it never receives the highest weight, and this is disastrous, since most good candidates are higher ranking. Conversely, α = .04 works better. 5 Previous work The debt owed to Tversky (1977) has been made clear in the introduction. Less clear is the debt owed to Jimenez et al. (2012), which also proposes an asymmetric similarity framework based on Tversky’s insights. Jimenez et al. showed the continued relevance of Tversky’s work. Motivated by the problem of measuring how well the distribution of one word w1 captures the distribution of another w2, Weeds and Weir (2005) also explore asymmetric models, expressing similarity calculations as weighted combinations of several variants of what they call precision and recall. Some of their models are also Tverskyan ratio models. To see </context>
</contexts>
<marker>Tversky, 1977</marker>
<rawString>A. Tversky. 1977. Features of similarity. Psychological Review, 84:327–352.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J van Rijsbergen</author>
</authors>
<title>Information retrieval.</title>
<date>1979</date>
<publisher>Butterworth-Heinemann,</publisher>
<location>Oxford.</location>
<marker>van Rijsbergen, 1979</marker>
<rawString>C. J. van Rijsbergen. 1979. Information retrieval. Butterworth-Heinemann, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Weeds</author>
<author>D Weir</author>
</authors>
<title>Co-occurrence retrieval: A flexible framework for lexical distributional similarity.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>4</issue>
<contexts>
<context position="15947" citStr="Weeds and Weir (2005)" startWordPosition="2729" endWordPosition="2732">is a low ranking word, a high α weight means it never receives the highest weight, and this is disastrous, since most good candidates are higher ranking. Conversely, α = .04 works better. 5 Previous work The debt owed to Tversky (1977) has been made clear in the introduction. Less clear is the debt owed to Jimenez et al. (2012), which also proposes an asymmetric similarity framework based on Tversky’s insights. Jimenez et al. showed the continued relevance of Tversky’s work. Motivated by the problem of measuring how well the distribution of one word w1 captures the distribution of another w2, Weeds and Weir (2005) also explore asymmetric models, expressing similarity calculations as weighted combinations of several variants of what they call precision and recall. Some of their models are also Tverskyan ratio models. To see this, we divide (9) everywhere by U(w1, w2): TSI(w1,w2) = α·σ(w1,w1) (1−α)·σ(w2,w2) σ(w1,w2) + σ(w1,w2) 1 If the SI is MIN, then the two terms in the denominator are the inverses of what W&amp;W call difference-weighted precision and recall: PREC(w1,w2) = σMIN(w1,w2) σMIN(w1,w1) REC(w1, w2) = σMIN(w1,w2) σMIN(w2,w2), So for TMIN, (9) can be rewritten: 1 α 1−α PREC(w1,w2) + REC(w1,w2) Tha</context>
</contexts>
<marker>Weeds, Weir, 2005</marker>
<rawString>J. Weeds and D. Weir. 2005. Co-occurrence retrieval: A flexible framework for lexical distributional similarity. Computational Linguistics, 31(4):439–475.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>