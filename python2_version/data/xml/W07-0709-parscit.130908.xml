<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.9340735">
Meta-Structure Transformation Model
for Statistical Machine Translation
</title>
<author confidence="0.969046">
Jiadong Sun, Tiejun, Zhao and Huashen Liang
</author>
<affiliation confidence="0.9619955">
MOE-MS Key Lab of National Language Processing and speech
Harbin Institute of Technology
</affiliation>
<address confidence="0.943859">
No. 92, West Da-zhi Street ,Harbin Heilongjiang ,150001 ,China
</address>
<email confidence="0.9876885">
jiadongsun@hit.edu.cn
{tjzhao, hsliang }@mtlab.hit.edu.cn
</email>
<sectionHeader confidence="0.994636" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999792352941177">
We propose a novel syntax-based model
for statistical machine translation in which
meta-structure (MS) and meta-structure se-
quence (SMS) of a parse tree are defined.
In this framework, a parse tree is decom-
posed into SMS to deal with the structure
divergence and the alignment can be recon-
structed at different levels of recombination
of MS (RM). RM pairs extracted can per-
form the mapping between the sub-
structures across languages. As a result,
we have got not only the translation for the
target language, but an SMS of its parse
tree at the same time. Experiments with
BLEU metric show that the model signifi-
cantly outperforms Pharaoh, a state-art-the-
art phrase-based system.
</bodyText>
<sectionHeader confidence="0.998786" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999965288461539">
The statistical approach has been widely used in
machine translation, which use the noisy-channel-
based model. A joint probability model, proposed
by Marcu and Wong (2002), is a kind of phrase-
based one. Och and Ney (2004) gave a framework
of alignment templates for this kind of models. All
of the phrase-based models outperformed the
word-based models, by automatically learning
word and phrase equivalents from bilingual corpus
and reordering at the phrase level. But it has been
found that phrases longer than three words have
little improvement in the performance (Koehn,
2003). Above the phrase level, these models have a
simple distortion model that reorders phrases inde-
pendently, without consideration of their contents
and syntactic information.
In recent years, applying different statistical
learning methods to structured data has attracted
various researchers. Syntax-based MT approaches
began with Wu (1997), who introduced the Inver-
sion Transduction Grammars. Utilizing syntactic
structure as the channel input was introduced into
MT by Yamada (2001). Syntax-based models have
been presented in different grammar formalisms.
The model based on Head-transducer was pre-
sented by Alshawi (2000). Daniel Gildea (2003)
dealt with the problem of the parse tree isomor-
phism with a cloning operation to either tree-to-
string or tree-to-tree alignment models. Ding and
Palmer (2005) introduced a version of probabilistic
extension of Synchronous Dependency Insertion
Grammars (SDIG) to deal with the pervasive
structure divergence. All these approaches don’t
model the translation process, but formalize a
model that generates two languages at the same
time, which can be considered as some kind of tree
transducers. Graehl and Knight (2004) described
the use of tree transducers for natural language
processing and addressed the training problems for
this kind of transducers.
In this paper, we define a model based on the
MS decomposition of the parse trees for statistical
machine translation, which can capture structural
variations and has a proven generation capacity.
During the translation process of our model, the
parse tree of the source language is decomposed
into different levels of MS and then transformed
into the ones of the target language in the form of
RM. The source language can be reordered accord-
ing to the structure transformation. At last, the tar-
get translation string is generated in the scopes of
RM. In the framework of this model,
</bodyText>
<page confidence="0.912097">
64 1
</page>
<note confidence="0.9976205">
Proceedings of the Second Workshop on Statistical Machine Translation, pages 64–71,
Prague, June 2007. c�2007 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999919">
Figure 1: MS and the SMS and RM for a given parser tree
</figureCaption>
<bodyText confidence="0.967065615384615">
the RM transformation can be regarded as produc-
tion rules and be extracted automatically from the
bilingual corpus. The overall translation probabil-
ity is thus decomposed.
In the rest of this paper, we first give the
definitions for MS, SMS, RM and the
decomposition of the parse tree in section 2.1, we
give a detailed description of our model in section
2.2, section 3 describes the training details and
section 4 describes the decoding algorithms, and
then the experiment (section 5) proves that our
model can outperform the baseline model,
pharaoh, under the same condition.
</bodyText>
<sectionHeader confidence="0.993964" genericHeader="method">
2 The model
</sectionHeader>
<subsectionHeader confidence="0.957872">
2.1 MS for a parse tree
</subsectionHeader>
<bodyText confidence="0.873958875">
A source language sentence (s1 s2 s3 s4 s5 s6),
and its parse tree S-P, are given in Figure 1.We
also give the translation of the sentence, which is
illustrated as (t1 t2 t3).Its parse tree is T-P.
Definition 1
MS of a parse tree
We call a sub-tree a MS of a parse tree, if it sat-
isfies the following constraints:
</bodyText>
<listItem confidence="0.990465">
1. An MS should be a sub-tree of a parse tree
2. Its direct sons of the leaf nodes in the sub-
tree are the words or punctuations of the sen-
tence
</listItem>
<bodyText confidence="0.999434166666667">
For example, each of the sub-trees in the right-
hand of Figure 1 is an MS for the parse tree of S-P.
The sub-tree of [I [G, D, H]] of S-P is not an MS,
because the direct sons of the leaf nodes, G, D, H,
are not words in the sentence of (s1 s2 s3 s4 s5
s6).
</bodyText>
<subsectionHeader confidence="0.780547">
Definition 2 SMS and RM
</subsectionHeader>
<bodyText confidence="0.9894195">
A sequence of MS is called a meta-structure
sequence (SMS) of a parse tree if and only if,
</bodyText>
<listItem confidence="0.921331333333333">
1. Its elements are MS of the parse tree
2. The parse tree can be reconstructed with the
elements in the same order as in the sequence.
</listItem>
<bodyText confidence="0.999799714285714">
It is denoted as SMS [T(S)].1 Two examples
for the concept of SMS can be found in Figure1.
RM(recombination of MS) is a sub-sequence
of SMS. We can express an SMS as differ-
ent RM 1k [T (S )] .The parse tree of S-P in Figure1
is decomposed into SMS and expressed in the
framework of RM. The two RM, RM 1 2[S − P ] ,
are used to express its parse tree in Figure1.It is
noted that there is structure divergence between
the two parse trees in Figure1. The corresponding
node of Node I in the tree S-P cannot be found in
the tree T-P. But under the conception of RM, the
structure alignments can be achieved at the level
of RM, which is illustrated in Figure2.
</bodyText>
<footnote confidence="0.853404">
Figure2.The RM alignments for S-P and T-P
1 T[S] denotes the parse tree of a given sentence
f and e denote the foreign and target sentences
</footnote>
<page confidence="0.563043">
65 2
</page>
<bodyText confidence="0.801472333333333">
=
=
In Figure2, both of the parse trees are decom-
posed and reconstructed in the forms of RM. The
alignments based on RM are illustrated at the
same time.
</bodyText>
<subsectionHeader confidence="0.999408">
2.2 Description of the model
</subsectionHeader>
<bodyText confidence="0.99998775">
In the framework of Statistical machine transla-
tion, the task is to find the sentence e for the given
foreign language f, which can be described in the
following formulation.
</bodyText>
<equation confidence="0.98630825">
~
e = arg max{ (  |)}
P e f (1)
e
</equation>
<bodyText confidence="0.999800375">
To make the model have the ability to model
the structure transformation, some hidden vari-
ables are introduced into the probability equation.
To make the equations simple to read, we take
some denotations different from the above defini-
tions. SMS[T(S)] is denoted as SM[T(S)].
The first variable is the SM[T(S)], we induce
the equation as follows,
</bodyText>
<equation confidence="0.999106722222222">
SM ( ( ))
T f
∑ ( [ ( )] |
P SM T f f ) (  |[ ( )] , )
P e SM T f f
[T( f)]
(2)
P e SM T f f
(  |[ ( )], ) =
∑ P ( SM[ ( )]  |[ ( )], )
e, T e SM T f f
SM[
T(e)]
= ∑ P(SM[ ( )]  |[ ( )], )
T e SM T f f ×
SM[
T(e)]
P(e |SM[T(e)] ,SM[T(f)] ,f) (3)
</equation>
<bodyText confidence="0.944763">
In order to simplify this model we have two as-
sumptions:
</bodyText>
<equation confidence="0.994894166666667">
An assumption is that the generation of SMS [T
(e)] is only related with SMS[T(f)]:
P(SM[T(e)]  |SM[T(f )], f
)
≡ P(SM [T(e)]  |SM [T (f )])
(4)
</equation>
<bodyText confidence="0.9989615">
Here we do all segmentations for any SMS
of [T (f)] to get different RMk1 [T (f)] .
</bodyText>
<equation confidence="0.995709">
P(SM[T ( )]  |[ ( )])
e SM T f
k
∑∏ P(RMi [T(e)]  |RMi [T(f)
</equation>
<page confidence="0.59585">
=1
</page>
<bodyText confidence="0.99679">
The use of RM is to decompose bi-lingual
parse trees and get the alignments in different
hierarchical levels of the structure.
Now we have another assumption that all
P(SM[T(f )] |f) should have the same prob-
abilityα . A simplified form for this model is
derived:
</bodyText>
<equation confidence="0.98589725">
P(e  |f
)
∑ ∑ α
×
SM(T(f)) SM(T(e))
k
∑ ∏ P RM T e RM T f
( [ ( )]  |[ ( )])
i i
RM [T( f)] i=1
× P(e  |RMi [T(e)] , RMi [T(f)] , f)
(6)
</equation>
<bodyText confidence="0.933101">
, Where P(e |RMi[T(e)] ,RMi[T(f)] ,f) can be re-
garded as a lexical transformation process, which
will be further decomposed.
In order to model the direct translation process
better by extending the feature functions, the di-
rect translation probability is obtained in the
framework of maximum entropy model:
</bodyText>
<equation confidence="0.9948436">
Pe|f()
∑ M
exp[ (e SMT e SMT f f
, [ ( )] , [ ( )] , )
m = 1λ m hm
</equation>
<bodyText confidence="0.992953">
We can achieve the translation according to
the function below:
</bodyText>
<equation confidence="0.9419626">
~ M
e = argmax exp[
{ ∑ =1 h e SMT e SM Tf f
λ ( , [ ( )] , [ ( )] , )}
m m m
</equation>
<bodyText confidence="0.997191">
A detailed list of the feature functions for the
model and some explanations are given as below:
</bodyText>
<listItem confidence="0.76609">
• Just as the derivation in the model, we take
into consideration of the structure trans-
formation when selecting the features. The
MS are combined in the forms of RM and
transformed as a whole structure.
</listItem>
<equation confidence="0.987949">
k
h e f
1 ( ) ∏=
, = log ( [ ( )]  |[ ( )])
P RM i T e RM i T f
i 1
k
(10)
</equation>
<listItem confidence="0.972287666666667">
• Features to model lexical transformation
processes, and its inverted version, where
the symbol L (RMi [T(S)]) denotes the
</listItem>
<equation confidence="0.957661823529412">
∑ exp[ ∑ M
e SM T e SM T f
, [ ( )], [ ( )] m =1
(eSMTe SM T f f
, [ ( )] , [ ( )] , )
λmhm
e f = ∑ P e SM T f f
(  |) ( , [ ( )]  |)
P
SM
])
(5)
h 2 (e, f ) = log ∏
= i=1
(9)
P RM i T f RM i T e
( [ ( )]  |[ ( )])
</equation>
<page confidence="0.9009125">
66
3
</page>
<bodyText confidence="0.999926666666667">
words belonging to this sub-structure in the
sentence. In Figure1, L (RM1) denotes the
words, s1 s2 s3, in the source language.
This part of transformation happens in the
scope of each RM, which means that all
the words in any RM can be transformed
into the target language words just in the
way of phrase-based model, serving as an-
other reordering factor at a different level:
</bodyText>
<equation confidence="0.999565">
P L RMi T e L RMi T f
( ( [ ( )]))  |( [ ( )])
P L RMi T f L RMi T e
( ( [ ( )]))  |( [ ( )])
</equation>
<bodyText confidence="0.996291545454545">
• We define a 3-gram model for the RM of
the target language, which is called a struc-
ture model according to the function of it
in this model.
the parse tree of the sentence, and set some special
feature weights to zero.
From the description above, we know the
framework of this model. When transformed to
target languages, the source language is reordered
at the RM level first. In this process, only the
knowledge of the structure is taken into
er generative ability. At the
same time, RM is a subsequence of SMS, which
consists of different hierarchical MS. So RM is a
structure, which can model the structure mapping
across the sub-tree structure. By decomposing the
source parse tree, the isomorphic between the
parse trees can be obtained, at the level of RM.
When reordering at the RM level, this model
just takes an RM as a symbol, and it can perform a
long distance reordering job according to the
knowledge of RM alignments.
</bodyText>
<equation confidence="0.970301272727273">
k
h3 (e, f) = log∏
i =1
k
h 4(e, f) = log∏
i =1
k
Wef
,) = log∏P(RJg. [T(e)]  |RJg.−2[T(e)] ,RMi−JT(e)])
i=1
3 Training
</equation>
<bodyText confidence="0.999438333333333">
For training the model, a parallel tree corpus is
needed. The methods and details are described as
follows:
</bodyText>
<equation confidence="0.582876">
(13)
</equation>
<bodyText confidence="0.9407334">
This feature can model the recombination of
the parse structure of the target sentences. For
example in Figure3, P(CC |AA,BB) is used to de-
scribe the probability of the RM sequence, (AA,
BB) should be followed by RM (CC) in the
translation process. This function can ensure
that a more reasonable sub-tree can be generated
for the target language. That would be explained
further in section 3.
Figure3. The 3-gram structure model
</bodyText>
<listItem confidence="0.4006225">
• The 3-gram language model is also used
h6e f = P e
( , ) log ( )
special case of this fr
</listItem>
<bodyText confidence="0.973771625">
amework, if we take the
whole structure of the parse tree as the only MS of
consideration. It is obvious that a lot of sentences
in the source language can have the same RM. So
this model has bett
height of the sub-tree
be
greater than a fix
</bodyText>
<equation confidence="0.986303166666667">
(1).The
shouldn’t
ed valueα ;
( 2). N(Leaf − nodes)≥ β
N(height)
(14)
</equation>
<bodyText confidence="0.980384">
The phrase-based model (Koehn, 2003) is a
</bodyText>
<subsectionHeader confidence="0.999095">
3.1 Decomposition of the parse tree
</subsectionHeader>
<bodyText confidence="0.9999415">
To reduce the amount of MS used in decoding
and training, we take some constrains for the MS.
</bodyText>
<subsectionHeader confidence="0.467567">
Figure3. Decomposition of a parse tree
</subsectionHeader>
<bodyText confidence="0.99740825">
Given a parse tree, we get the initial SMS in
such a top -down and left- to –right way.
Any node is deleted if the sub-tree can’t satisfy
the constrains (1), (2).
</bodyText>
<page confidence="0.9919725">
67
4
</page>
<table confidence="0.997873666666667">
RMS for Ch-Parse Tree RMS for EN-Parse Tree Pro for transformation
AP[AP[AP[a-a]-usde]-m] NPB [DT-JJ-NN-PUNC.] 0.000155497
AP[AP[AP[r-a]-usde]-m] NPB[PDT-DT-JJ-NN] 0.0151515
AP[AP[BMP[m-q]-a]-usde] wj ADVP [RB-RB-PUNC.] 0.00344828
AP[AP[BMP[m-q]-a]-usde] wj DT CD JJ NNS PUNC 0.0833333
AP[AP[BMP[m-q]-a]-usde] wj DT JJ NN NNS PUNC. 0.015625
</table>
<tableCaption confidence="0.995719">
Table 1 some examples of the RM transformation
</tableCaption>
<table confidence="0.9997726">
RM1 RM2 RM3 P(RM3|RM1,RM2)
IN NP-A[NPB[PRP-NN] IN 0.2479237
NPB NP-A[NPB[PRP-NN] VBZ 0.2479235
IN NP-A[NPB[PRP-NN] MD 0.6458637
&lt;s&gt; NP-A[NPB[PRP-NN] VBD 0.904308
</table>
<tableCaption confidence="0.983726">
Table 2 Examples for the 3-gram structure model of RM
</tableCaption>
<figureCaption confidence="0.643583333333333">
Generate all of the SMS by deleting a node in
any Ms to generate new SMS, applying the same
operation to any SMS
</figureCaption>
<bodyText confidence="0.98125475">
structure model is based on the English RM of
each parse tree, selected from the parallel tree cor-
pus. The 3-gram structure model is defined as fol-
lows:
</bodyText>
<subsectionHeader confidence="0.800338">
3.2 Parallel SMS and Estimation of the pa-
</subsectionHeader>
<equation confidence="0.941695545454545">
rameters for RM transformations
P RM T e RM 2 T e RM 1 T e
( [ ( )]  |− [ ( )], − [ ( )])
I i i
, RM
2 I−1
(RMI
Count
=
)
, RMI
</equation>
<bodyText confidence="0.989783314285714">
We can get bi-lingual SMS by recombining all
the possible SMS obtained from the parallel
parse trees. m ∗ n Parallel SMS can be obtained
if m is the number of SMS for a parse tree in the
source language, n for the target one.
The alignments of the parallel MS and extrac-
tion can be performed in such a simple way.
Given the parallel tree corpus, we first get the
alignments based on the level of words, for which
we used GIZA++ in both of the directions. Ac-
cording to the knowledge of the word alignments,
we derived the alignments of leave nodes of the
given parse trees, which are the direct root nodes
of the words. Then all the knowledge of the words
is discarded for the RM extraction. The next step
for the extraction of the RM is based on the popu-
lar phrase-extraction algorithm of the phrase-
based statistical machine translation model. The
present alignment and phrase extraction methods
can be applied to the extraction of the MS and RM
[T(S)].
Count(A, B) is the expected number of times A
is aligned with B in the training corpus.Table1
shows some parameters for this part in the model.
Training n-gram model for the monolingual
Count(A, B, C) is the times of the situation, in
which the RM is consecutive sub-trees of the
parse trees in the training set. Some 3-gram pa-
rameters in the training task are given in Table2.
We didn’t meet with the serious data sparseness
problem in this part of work, because most of the
MS structures have occurred enough times for
parameters estimation. But we still set some
fixed value for the unseen parameters in the
training set.
</bodyText>
<sectionHeader confidence="0.985359" genericHeader="method">
4 Decoding
</sectionHeader>
<bodyText confidence="0.999925">
A beam search algorithm is applied to this
model for decoding, which is based on the frame
of the beam search for phrase-based statistical
machine translation (Koehn et al, 03).
Here the process of the hypothesis generation is
presented. Given a sentence and its parse tree, all
the possible candidate RM are collected, which
can cover a part of the parse tree at the bottom.
With the candidates, the hypotheses can be
formed and extended.
For example, all the parse tree’s leaf nodes of a
Chinese sentence in Figure4, are covered by [r],
[ pron ] and VP[vg-BNP[pron-n]] in the order of
choosing candidate RM{ (1), (2), (3)}.
</bodyText>
<figure confidence="0.912958266666667">
RM
Ei
P(RMEI  |RM
Fi) = ∑Count(RMFi,RMEi
Count(RMFi , RMEI )
)
,
Count
)
∑
, RM
2 I − 1
(RMI
RMj
j
</figure>
<page confidence="0.940374">
68
</page>
<figureCaption confidence="0.751075">
Figure4. Process of translation based on RM
</figureCaption>
<equation confidence="0.999367111111111">
(r,WRB VBD) (1)
#gyp何 → how did
( pron, PRP) (2)
你 → you
([
VP vg BNP pron n
− [ − ]],
VP VB NPB DT NN
[ − [ − ]])
</equation>
<bodyText confidence="0.962154612903226">
得到这些信息→ find the information
lated into the target lan
guage and the correspond-
ing RMi [T (e)] is generated. For example, when
(r, WRB VBD) , is used to expand the hypothe-
sis
words in the sub-tree are translated into
the target language,
did.
We also need to calculate the cost for the hy-
potheses according to the parameters in the model
to perform the beam search. The task for the beam
search is to find the hypothesis with the least cost.
When the expansion of a hypothesis comes to the
final
, the
#gyp何→how
state, the target language is generated. All of
the leave nodes of the parse tree for the source
language are covered. The parser for the target
language isn’t used for decoding. But a target
SMS is generated during the process of decoding
to achieve better reordering performance.
length of 14.287 Chinese words an
d 4 references.
To evaluate the result of the translation, the BLEU
metric (Papineni et al. 2002) was used.
ne
to train a 3-gram lan
guage model. After training,
164 MB language model were obtained.
</bodyText>
<subsectionHeader confidence="0.917544">
5.2 Our model
</subsectionHeader>
<bodyText confidence="0.859976666666667">
(3)
Before the next expansion of a hypothesis, the
words in the scope of the present RM are trans-
</bodyText>
<page confidence="0.996928">
6
</page>
<sectionHeader confidence="0.999817" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999018333333333">
The experiment was conducted for the task of
Chinese-to-English translation. A corpus, which
consists of 602,701 sentence pairs, was used as
the training set. We took CLDC 863 test set as our
test set (http://www.chineseldc.org/resourse.asp),
which consists of 467 sentences with an average
</bodyText>
<subsectionHeader confidence="0.968955">
5.1 The baseli
</subsectionHeader>
<bodyText confidence="0.999943578947368">
System used for comparison was Pharaoh
(Koehn et al., 2003; Koehn, 2004), which uses a
beam search algorithm for decoding. In its model,
it takes the following features: language model,
phrase translation probability in the two directions,
distortion model, word penalty and phrase penalty,
all of which can be achieved with the training
toolkits distributed by Koehn. The training set and
development set mentioned above were used to
perform the training task and to tune the feature
weights by the minimum error training algorithm.
All the other settings were the same as the default
ones. SRI Language Modeling Toolkit was used
All the common features shared with Pharaoh
were trained with the same toolkits and the same
corpus. Besides those features, we need to train
the structure transformation model and the mono-
lingual structure model for our model. First,
10,000 sentence pairs were selected to achieve the
</bodyText>
<page confidence="0.998203">
69
</page>
<table confidence="0.999521166666667">
System BLEU-n n-gram precisions
4
1 2 3 4 5 6 7 8
Pharaoh 0.2053 0.6449 0.4270 0.2919 0.2053 0.1480 0.1061 0.0752 0.0534
Ms sys- 0.2232 0.6917 0.4605 0.3160 0.2232 0.1615 0.1163 0.0826 0.0587
tem
</table>
<tableCaption confidence="0.834827">
Table3. Comparison of Pharaoh and our system
</tableCaption>
<table confidence="0.9989904">
Features
System Plm(e) P(RT) P( IRT ) Pw( f|e ) Pw( e|f ) Word Phr Ph(RM)
Pharaoh 0.151 ---- 0.08 0.14 -0.29 0.26
MS sys- 0.157 0.16 0.23 0.06 0.11 -0.20 0.22 0.36
tem
</table>
<tableCaption confidence="0.857356">
Table4.Feature weights obtained by minimum error rate training on development set
</tableCaption>
<bodyText confidence="0.999792">
training set for this part of task. The Collins parser
and a Chinese parser of our own lab were used.
After processing this corpus, we get a parallel tree
corpus. SRI Language Modeling Toolkits were
used again to train this part of parameters. In this
experiment, we set α = 3 ,and Q = 1 . 5 . 149MB
RMS [T (s)] pairs and a 25 MB 3-gram mono-
lingual structure model were obtained.
</bodyText>
<sectionHeader confidence="0.991669" genericHeader="conclusions">
6. Conclusion and Future work
</sectionHeader>
<bodyText confidence="0.9999234">
A framework for statistical machine translation
is created in this paper. The results of the experi-
ments show that this model gives better perform-
ance, compared with the baseline system.
This model can incorporate the syntactic infor-
mation into the process of translation and model
the sub-structure projections across the parallel
parse trees.
The advantage of this frame work lies in that
the reordering operations can be performed at the
different levels according to the hierarchical RM
of the parse tree.
But we should notice that some independent as-
sumptions were made in the decomposition of the
parse tree. In the future, a proper method should
be introduced into this model to achieve the most
possible decomposition of the parse tree. In fact,
we can incorporate some other feature functions
into the model to model the structure transforma-
tion more effectively.
</bodyText>
<sectionHeader confidence="0.959011" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.999687666666667">
Thanks to the reviewers for their reviews and
comments on improving our presentation of this
paper.
</bodyText>
<sectionHeader confidence="0.996974" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9943935">
A.P.Dempster,N.M.Laird, and D.B.Rubin
1977.Maximum likelihood from imcomplete data
via the EM algorithm. Journal of the Royal Statisti-
cal Society, 39(Ser B):1-38.
Christoph Tillman. A projection extension algorithm
for statistical machine translation. Proceedings of
the Conference on Empirical Methods in Natural
Language Processing, Sapporo, Japan, June 30-July
4, 2003, 1-8.
Daniel Gildea.2003.Loosely tree based alignment for
machine translation. In Proceedings of ACL-03
Daniel Marcu, William Wong. A phrase-based, joint
probability model for statistical machine translation.
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, Philadelphia,
PA, USA, July 11-13, 2002, 133-139.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):3-403.
F.Casacuberta, E. Vidal: Machine Translation with
Inferred Stochastic Finite-state Transducers. Com-
putational Linguistics, Vol. 30, No. 2, pp. 205-225,
June 2004
Franz J. Och, C. Tillmann, Hermann Ney. Improved
alignment models for statistical machine translation.
Proceedings of the Joint SIGDAT Conference on
Empirical Methods in Natural Language Processing
and Very Large Corpora (EMNLP), College Park,
MD, USA, June 21-22, 1999, 20-28.
Franz J. Och, Hermann Ney.2002 Discriminative train-
ing and maximum entropy models. In Proceedings of
ACL-00, pages 440-447, Hong Kong, Octorber.
Hiyan Alshawi, Srinvas Bangalore, and Shona Douglas.
2000. Learning dependency translation models as
</reference>
<page confidence="0.997206">
70
7
</page>
<reference confidence="0.998770083333334">
collections of finite state head transducers Compu-
tational Linguistics, 26(1):45-60.
Ilya D. Melamed. Automatic evaluation and uniform
filter cascades for inducing n-best translation lexi-
cons. Proceedings of the Third Workshop on Very
Large Corpora, Boston, USA, July 30, 1995, 197-
211.
Jonathan Graehl Kevin Knight Training Tree Trans-
ducers In Proceedings of NAACL-HLT 2004,
pages 105-112.
Kenji Yamada and Kevin Knight 2001. A Syntax-
based statistical translation model. In Proceedings
of the 39th Annual Meeting of the association for
computational Linguists(ACL 01), Toulouse,
France, July 6-11
Michael John Collins. 1999. Head-driven statistical
Models for Natural Language Parsing. Ph.D. the-
sis,University of Pennsyvania,Philadelphia.
P. Koehn, Franz Josef Och, Daniel Marcu. Statistical
phrase-based translation. Proceedings of the Con-
ference on Human Language Technology, Edmon-
ton, Canada, May 27-June 1, 2003, 127-133.
P. Koehn: Pharaoh: a Beam Search Decoder for
Phrase-based Statistical Machine Translation Mod-
els . Meeting of the American Association for ma-
chine translation(AMTA), Washington DC, pp. 115-
124 Sep./Oct. 2004
Peter F. Brown ,Stephen A. Della Pietra,Vincent
J.Della Pietra, and Robert Merrcer.1993. The
mathematics of statistical machine transla-
tion:Parameter estimation.Computational Linguis-
tics,19(2).:263-311.
Quirk, Chris, Arul Menezes, and Colin Cherry. De-
pendency Tree Translation. Microsoft Research
Technical Report: MSR-TR-2004-113.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An supervised approach using multiple-
sequence alignment. In Proceedings of
HLT/NAACL
S. Nie en , H. Ney: Statistical Machine Translation
β
with Scarce Resources using Morpho-syntactic
Information. Computational Linguistics, Vol. 30 No.
2, pp. 181-204, June 20
Yuan Ding and Martha Palmer. 2005. Machine transla-
tion using probabilistic synchronous dependency in-
sert grammars. In Proceedings of 43rd Annual
Meeting of the NAACL-HLT2004, pages 273-280..
</reference>
<page confidence="0.997863">
71
8
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.662859">
<title confidence="0.997712">Meta-Structure Transformation for Statistical Machine Translation</title>
<author confidence="0.914978">Tiejun Sun</author>
<author confidence="0.914978">Zhao</author>
<affiliation confidence="0.9582315">MOE-MS Key Lab of National Language Processing and Harbin Institute of</affiliation>
<address confidence="0.915269">No. 92, West Da-zhi Street ,Harbin Heilongjiang ,150001</address>
<email confidence="0.794612">tjzhao@mtlab.hit.edu.cn</email>
<email confidence="0.794612">hsliang@mtlab.hit.edu.cn</email>
<abstract confidence="0.998952277777778">We propose a novel syntax-based model for statistical machine translation in which and meta-structure seof a parse tree are defined. In this framework, a parse tree is decominto deal with the structure divergence and the alignment can be reconstructed at different levels of recombination extracted can perform the mapping between the substructures across languages. As a result, we have got not only the translation for the language, but an its parse tree at the same time. Experiments with BLEU metric show that the model significantly outperforms Pharaoh, a state-art-theart phrase-based system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>N M Laird A P Dempster</author>
</authors>
<title>D.B.Rubin 1977.Maximum likelihood from imcomplete data via the EM algorithm.</title>
<journal>Journal of the Royal Statistical Society,</journal>
<volume>39</volume>
<pages>1--38</pages>
<marker>Dempster, </marker>
<rawString>A.P.Dempster,N.M.Laird, and D.B.Rubin 1977.Maximum likelihood from imcomplete data via the EM algorithm. Journal of the Royal Statistical Society, 39(Ser B):1-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillman</author>
</authors>
<title>A projection extension algorithm for statistical machine translation.</title>
<date></date>
<booktitle>Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<volume>30</volume>
<pages>1--8</pages>
<location>Sapporo, Japan,</location>
<marker>Tillman, </marker>
<rawString>Christoph Tillman. A projection extension algorithm for statistical machine translation. Proceedings of the Conference on Empirical Methods in Natural Language Processing, Sapporo, Japan, June 30-July 4, 2003, 1-8.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Daniel</author>
</authors>
<title>Gildea.2003.Loosely tree based alignment for machine translation.</title>
<booktitle>In Proceedings of ACL-03</booktitle>
<marker>Daniel, </marker>
<rawString>Daniel Gildea.2003.Loosely tree based alignment for machine translation. In Proceedings of ACL-03</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>William Wong</author>
</authors>
<title>A phrase-based, joint probability model for statistical machine translation.</title>
<date>2002</date>
<booktitle>Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>133--139</pages>
<location>Philadelphia, PA, USA,</location>
<contexts>
<context position="1200" citStr="Marcu and Wong (2002)" startWordPosition="178" endWordPosition="181">ure divergence and the alignment can be reconstructed at different levels of recombination of MS (RM). RM pairs extracted can perform the mapping between the substructures across languages. As a result, we have got not only the translation for the target language, but an SMS of its parse tree at the same time. Experiments with BLEU metric show that the model significantly outperforms Pharaoh, a state-art-theart phrase-based system. 1 Introduction The statistical approach has been widely used in machine translation, which use the noisy-channelbased model. A joint probability model, proposed by Marcu and Wong (2002), is a kind of phrasebased one. Och and Ney (2004) gave a framework of alignment templates for this kind of models. All of the phrase-based models outperformed the word-based models, by automatically learning word and phrase equivalents from bilingual corpus and reordering at the phrase level. But it has been found that phrases longer than three words have little improvement in the performance (Koehn, 2003). Above the phrase level, these models have a simple distortion model that reorders phrases independently, without consideration of their contents and syntactic information. In recent years,</context>
</contexts>
<marker>Marcu, Wong, 2002</marker>
<rawString>Daniel Marcu, William Wong. A phrase-based, joint probability model for statistical machine translation. Proceedings of the Conference on Empirical Methods in Natural Language Processing, Philadelphia, PA, USA, July 11-13, 2002, 133-139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<pages>23--3</pages>
<contexts>
<context position="1950" citStr="Wu (1997)" startWordPosition="292" endWordPosition="293">odels outperformed the word-based models, by automatically learning word and phrase equivalents from bilingual corpus and reordering at the phrase level. But it has been found that phrases longer than three words have little improvement in the performance (Koehn, 2003). Above the phrase level, these models have a simple distortion model that reorders phrases independently, without consideration of their contents and syntactic information. In recent years, applying different statistical learning methods to structured data has attracted various researchers. Syntax-based MT approaches began with Wu (1997), who introduced the Inversion Transduction Grammars. Utilizing syntactic structure as the channel input was introduced into MT by Yamada (2001). Syntax-based models have been presented in different grammar formalisms. The model based on Head-transducer was presented by Alshawi (2000). Daniel Gildea (2003) dealt with the problem of the parse tree isomorphism with a cloning operation to either tree-tostring or tree-to-tree alignment models. Ding and Palmer (2005) introduced a version of probabilistic extension of Synchronous Dependency Insertion Grammars (SDIG) to deal with the pervasive struct</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):3-403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E F Casacuberta</author>
</authors>
<title>Vidal: Machine Translation with Inferred Stochastic Finite-state Transducers.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<pages>205--225</pages>
<marker>Casacuberta, 2004</marker>
<rawString>F.Casacuberta, E. Vidal: Machine Translation with Inferred Stochastic Finite-state Transducers. Computational Linguistics, Vol. 30, No. 2, pp. 205-225, June 2004</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>C Tillmann</author>
<author>Hermann Ney</author>
</authors>
<title>Improved alignment models for statistical machine translation.</title>
<date>1999</date>
<booktitle>Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP),</booktitle>
<location>College Park, MD, USA,</location>
<marker>Och, Tillmann, Ney, 1999</marker>
<rawString>Franz J. Och, C. Tillmann, Hermann Ney. Improved alignment models for statistical machine translation. Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP), College Park, MD, USA, June 21-22, 1999, 20-28.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Franz J Och</author>
</authors>
<title>Hermann Ney.2002 Discriminative training and maximum entropy models.</title>
<booktitle>In Proceedings of ACL-00,</booktitle>
<pages>440--447</pages>
<location>Hong Kong, Octorber.</location>
<marker>Och, </marker>
<rawString>Franz J. Och, Hermann Ney.2002 Discriminative training and maximum entropy models. In Proceedings of ACL-00, pages 440-447, Hong Kong, Octorber.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiyan Alshawi</author>
<author>Srinvas Bangalore</author>
<author>Shona Douglas</author>
</authors>
<title>Learning dependency translation models as collections of finite state head transducers Computational Linguistics,</title>
<date>2000</date>
<pages>26--1</pages>
<marker>Alshawi, Bangalore, Douglas, 2000</marker>
<rawString>Hiyan Alshawi, Srinvas Bangalore, and Shona Douglas. 2000. Learning dependency translation models as collections of finite state head transducers Computational Linguistics, 26(1):45-60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilya D Melamed</author>
</authors>
<title>Automatic evaluation and uniform filter cascades for inducing n-best translation lexicons.</title>
<date>1995</date>
<booktitle>Proceedings of the Third Workshop on Very Large Corpora,</booktitle>
<pages>197--211</pages>
<location>Boston, USA,</location>
<marker>Melamed, 1995</marker>
<rawString>Ilya D. Melamed. Automatic evaluation and uniform filter cascades for inducing n-best translation lexicons. Proceedings of the Third Workshop on Very Large Corpora, Boston, USA, July 30, 1995, 197-211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Graehl</author>
</authors>
<title>Kevin Knight Training Tree Transducers In</title>
<date>2004</date>
<booktitle>Proceedings of NAACL-HLT</booktitle>
<pages>105--112</pages>
<marker>Graehl, 2004</marker>
<rawString>Jonathan Graehl Kevin Knight Training Tree Transducers In Proceedings of NAACL-HLT 2004, pages 105-112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A Syntaxbased statistical translation model.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the association for computational Linguists(ACL 01),</booktitle>
<pages>6--11</pages>
<location>Toulouse, France,</location>
<marker>Yamada, Knight, 2001</marker>
<rawString>Kenji Yamada and Kevin Knight 2001. A Syntaxbased statistical translation model. In Proceedings of the 39th Annual Meeting of the association for computational Linguists(ACL 01), Toulouse, France, July 6-11</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael John Collins</author>
</authors>
<title>Head-driven statistical Models for Natural Language Parsing. Ph.D. thesis,University of Pennsyvania,Philadelphia.</title>
<date>1999</date>
<marker>Collins, 1999</marker>
<rawString>Michael John Collins. 1999. Head-driven statistical Models for Natural Language Parsing. Ph.D. thesis,University of Pennsyvania,Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>Proceedings of the Conference on Human Language Technology,</booktitle>
<pages>127--133</pages>
<location>Edmonton, Canada,</location>
<contexts>
<context position="16929" citStr="Koehn et al., 2003" startWordPosition="3162" endWordPosition="3165">ic (Papineni et al. 2002) was used. ne to train a 3-gram lan guage model. After training, 164 MB language model were obtained. 5.2 Our model (3) Before the next expansion of a hypothesis, the words in the scope of the present RM are trans6 5 Experiments The experiment was conducted for the task of Chinese-to-English translation. A corpus, which consists of 602,701 sentence pairs, was used as the training set. We took CLDC 863 test set as our test set (http://www.chineseldc.org/resourse.asp), which consists of 467 sentences with an average 5.1 The baseli System used for comparison was Pharaoh (Koehn et al., 2003; Koehn, 2004), which uses a beam search algorithm for decoding. In its model, it takes the following features: language model, phrase translation probability in the two directions, distortion model, word penalty and phrase penalty, all of which can be achieved with the training toolkits distributed by Koehn. The training set and development set mentioned above were used to perform the training task and to tune the feature weights by the minimum error training algorithm. All the other settings were the same as the default ones. SRI Language Modeling Toolkit was used All the common features sha</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn, Franz Josef Och, Daniel Marcu. Statistical phrase-based translation. Proceedings of the Conference on Human Language Technology, Edmonton, Canada, May 27-June 1, 2003, 127-133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Pharaoh: a Beam Search Decoder for Phrase-based Statistical Machine Translation Models . Meeting of the American Association for machine translation(AMTA),</title>
<date>2004</date>
<pages>115--124</pages>
<location>Washington DC,</location>
<contexts>
<context position="16943" citStr="Koehn, 2004" startWordPosition="3166" endWordPosition="3167">2002) was used. ne to train a 3-gram lan guage model. After training, 164 MB language model were obtained. 5.2 Our model (3) Before the next expansion of a hypothesis, the words in the scope of the present RM are trans6 5 Experiments The experiment was conducted for the task of Chinese-to-English translation. A corpus, which consists of 602,701 sentence pairs, was used as the training set. We took CLDC 863 test set as our test set (http://www.chineseldc.org/resourse.asp), which consists of 467 sentences with an average 5.1 The baseli System used for comparison was Pharaoh (Koehn et al., 2003; Koehn, 2004), which uses a beam search algorithm for decoding. In its model, it takes the following features: language model, phrase translation probability in the two directions, distortion model, word penalty and phrase penalty, all of which can be achieved with the training toolkits distributed by Koehn. The training set and development set mentioned above were used to perform the training task and to tune the feature weights by the minimum error training algorithm. All the other settings were the same as the default ones. SRI Language Modeling Toolkit was used All the common features shared with Phara</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>P. Koehn: Pharaoh: a Beam Search Decoder for Phrase-based Statistical Machine Translation Models . Meeting of the American Association for machine translation(AMTA), Washington DC, pp. 115-124 Sep./Oct. 2004</rawString>
</citation>
<citation valid="false">
<authors>
<author>Peter F Brown</author>
</authors>
<title>Della Pietra,Vincent J.Della Pietra, and Robert Merrcer.1993. The mathematics of statistical machine translation:Parameter estimation.Computational Linguistics,19(2).:263-311.</title>
<marker>Brown, </marker>
<rawString>Peter F. Brown ,Stephen A. Della Pietra,Vincent J.Della Pietra, and Robert Merrcer.1993. The mathematics of statistical machine translation:Parameter estimation.Computational Linguistics,19(2).:263-311.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
<author>Colin Cherry</author>
</authors>
<title>Dependency Tree Translation. Microsoft Research</title>
<tech>Technical Report: MSR-TR-2004-113.</tech>
<marker>Quirk, Menezes, Cherry, </marker>
<rawString>Quirk, Chris, Arul Menezes, and Colin Cherry. Dependency Tree Translation. Microsoft Research Technical Report: MSR-TR-2004-113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Learning to paraphrase: An supervised approach using multiplesequence alignment.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT/NAACL</booktitle>
<marker>Barzilay, Lee, 2003</marker>
<rawString>Regina Barzilay and Lillian Lee. 2003. Learning to paraphrase: An supervised approach using multiplesequence alignment. In Proceedings of HLT/NAACL</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Nie en</author>
</authors>
<title>Statistical Machine Translation β with Scarce Resources using Morpho-syntactic Information.</title>
<date></date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<pages>181--204</pages>
<marker>en, </marker>
<rawString>S. Nie en , H. Ney: Statistical Machine Translation β with Scarce Resources using Morpho-syntactic Information. Computational Linguistics, Vol. 30 No. 2, pp. 181-204, June 20</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuan Ding</author>
<author>Martha Palmer</author>
</authors>
<title>Machine translation using probabilistic synchronous dependency insert grammars.</title>
<date>2005</date>
<booktitle>In Proceedings of 43rd Annual Meeting of the NAACL-HLT2004,</booktitle>
<pages>273--280</pages>
<contexts>
<context position="2416" citStr="Ding and Palmer (2005)" startWordPosition="360" endWordPosition="363">years, applying different statistical learning methods to structured data has attracted various researchers. Syntax-based MT approaches began with Wu (1997), who introduced the Inversion Transduction Grammars. Utilizing syntactic structure as the channel input was introduced into MT by Yamada (2001). Syntax-based models have been presented in different grammar formalisms. The model based on Head-transducer was presented by Alshawi (2000). Daniel Gildea (2003) dealt with the problem of the parse tree isomorphism with a cloning operation to either tree-tostring or tree-to-tree alignment models. Ding and Palmer (2005) introduced a version of probabilistic extension of Synchronous Dependency Insertion Grammars (SDIG) to deal with the pervasive structure divergence. All these approaches don’t model the translation process, but formalize a model that generates two languages at the same time, which can be considered as some kind of tree transducers. Graehl and Knight (2004) described the use of tree transducers for natural language processing and addressed the training problems for this kind of transducers. In this paper, we define a model based on the MS decomposition of the parse trees for statistical machin</context>
</contexts>
<marker>Ding, Palmer, 2005</marker>
<rawString>Yuan Ding and Martha Palmer. 2005. Machine translation using probabilistic synchronous dependency insert grammars. In Proceedings of 43rd Annual Meeting of the NAACL-HLT2004, pages 273-280..</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>