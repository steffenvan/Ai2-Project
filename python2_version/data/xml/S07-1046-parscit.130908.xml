<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001102">
<title confidence="0.9883805">
LCC-TE: A Hybrid Approach to
Temporal Relation Identification in News Text
</title>
<note confidence="0.82159175">
Congmin Min
Language Computer Corporation
1701 N. Collins Blvd. Suite 2000
Richardson, TX 75080
</note>
<email confidence="0.861642">
cmin@languagecomputer.com
</email>
<note confidence="0.9252096">
Munirathnam Srikanth
Language Computer Corporation
1701 N. Collins Blvd, Suite 2000
Richardson, TX 75080
Srikanth.munirathnam
</note>
<email confidence="0.863463">
@languagecomputer.com
</email>
<note confidence="0.98679575">
Abraham Fowler
Language Computer Corporation
1701 N. Collins Blvd, Suite 2000
Richardson, TX 75080
</note>
<email confidence="0.984043">
abraham@languagecomputer.com
</email>
<sectionHeader confidence="0.99352" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997835">
This paper explores a hybrid approach to
temporal information extraction within the
TimeML framework. Particularly, we focus on
our initial efforts to apply machine learning
techniques to identify temporal relations as
defined in a constrained manner by the
TempEval-2007 task. We explored several
machine learning models and human rules to
infer temporal relations based on the features
available in TimeBank, as well as a number of
other features extracted by our in-house tools.
We participated in all three sub-tasks of the
TempEval task in SemEval-2007 workshop
and the evaluation shows that we achieved
comparable results in Task A &amp; B and
competitive results in Task C.
</bodyText>
<sectionHeader confidence="0.998989" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999788923076923">
There has been a growing interest in temporal
information extraction in recent years, as more and
more operational NLP systems demands dealing
with time-related issues in natural languages. In
this paper, we report on an end-to-end system that
is capable of automating identification of temporal
referring expressions, events and temporal
relations in text by leveraging various NLP tools
and linguistic resources at LCC.
It has to be noted that the system we report here
is not only intended for TempEval 2007
evaluation, but will also be used as a NLP tool for
our other applications (e.g. temporal Question
Answering). That is why we experimented to use
our own temporal and event extraction capabilities
in this work, although time and event tags have
already been provided in the testing/training data.
Another reason we use our own temporal tagging
is that our temporal tagger extracts more
information than that available in the
training/testing data. For instance, temporal signals
are removed from the data that the task organizers
provide, but our temporal tagger detects that, as
part of the tagging procedure. The following is an
example for the tagged expression &amp;quot;on this coming
Sunday&amp;quot;.
</bodyText>
<construct confidence="0.9703927">
&lt;ArgStructure id=&amp;quot;65&amp;quot; type=&amp;quot;timex&amp;quot;&gt;
&lt;argRef type=&amp;quot;determiner&amp;quot; tokStr=&amp;quot;this&amp;quot;/&gt;
&lt;argRef type=&amp;quot;directionIndicator&amp;quot; tokStr=&amp;quot;coming&amp;quot;/&gt;
&lt;argRef type=&amp;quot;focus&amp;quot; tokStr=&amp;quot;Sunday&amp;quot;/&gt;
&lt;argRef type=&amp;quot;prepSignal&amp;quot; tokStr=&amp;quot;on&amp;quot;/&gt;
&lt;argRef type=&amp;quot;head&amp;quot; tokStr=&amp;quot;this coming Sunday&amp;quot;/&gt;
&lt;argRef type=&amp;quot;root&amp;quot; tokStr=&amp;quot;on this coming Sunday&amp;quot;/&gt;
&lt;argValue type=&amp;quot;focusType&amp;quot; value=&amp;quot;weekOfDay&amp;quot;/&gt;
&lt;argValue type=&amp;quot;subType&amp;quot; value=&amp;quot;Fuzzy&amp;quot;/&gt;
&lt;argValue type=&amp;quot;type&amp;quot; value=&amp;quot;Date&amp;quot;/&gt;
</construct>
<subsectionHeader confidence="0.461241">
&lt;/ArgStructure&gt;
</subsectionHeader>
<bodyText confidence="0.999728578947368">
Our data structure allows us to easily access and
manipulate any part of the tagged chunk of text,
which leaves the interpretation of whether the
temporal signal on in the example is part of the
temporal expression to users of temporal tagger.
Taking as input this data structure, the
normalization, including relative date resolution, is
a straightforward process, provided that the
reference time can be computed from the context.
For temporal relation identification, by
leveraging the capabilities of our temporal tagger,
event tagger and several other in-house NLP tools,
we derive a rich set of syntactic and semantic
features for use by machine learning. We also
explored the possibility of combining the rule-
based approach with machine learning in an
integrated manner so that our system can take
advantage of these two approaches for temporal
relation identification.
</bodyText>
<sectionHeader confidence="0.948162" genericHeader="method">
2 System Architecture
</sectionHeader>
<bodyText confidence="0.9904955">
The overall architecture of our end-to-end system
is illustrated in Figure 1 (Page 2).
</bodyText>
<page confidence="0.984268">
219
</page>
<bodyText confidence="0.99199615625">
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 219–222,
Prague, June 2007. c�2007 Association for Computational Linguistics
In addition to several common NLP tools, e.g.
Named Entity Recognizer, we use syntactic and
semantic parsers to identify syntactic and semantic
roles (e.g. AGENT or SUBJECT) of event terms
and a context detector to detect linguistic contexts
in a discourse. We use such information as
extended features for machine learning. The
Temporal Tagger tags and normalizes temporal
expressions conforming to the TimeML guideline.
The Temporal Merger compares our own temporal
and event tagging with those supplied in
training/testing data. If there is any inconsistency,
it will replace the former with the latter, which
guarantees that our temporal and event tagging are
the same as those in training/testing data. Feature
Extractor extracts and composes features from
documents processed by the NLP tools. Machine
Learner and Human Rule Predictor take as input
the feature vector for each instance to predict
temporal relation. The Human Rule Predictor is a
rule interpreter that read hand-crafted rules from
plain text file to match each event instance
represented by a feature vector.
Note that in Figure &apos;, Syntactic Parsing is done
by a probabilistic chart parser, which generates full
parse tree for each sentence. Syntactic Pattern
Matching is performed by a syntactic pattern
matcher, which operates on parse trees produced
by chart parser and used by Temporal Tagger to
tag and normalize temporal expressions.
</bodyText>
<subsectionHeader confidence="0.929771">
Human Rule Predictor
</subsectionHeader>
<bodyText confidence="0.6777345">
Documents with New
TLINKs
</bodyText>
<figureCaption confidence="0.994058">
Figure 1. Overall System Architecture
</figureCaption>
<sectionHeader confidence="0.957437" genericHeader="method">
3 Feature Engineering
</sectionHeader>
<bodyText confidence="0.999827">
While temporal tagging and normalization is rule-
based in our system, temporal relation
identification is a combination of machine learning
and rule-based approaches. For machine learning,
the feature set for the three tasks A, B and C we
engineered consist of what we call &apos;) first-class
features; 2) derived features; 3) extended features,
and 4) merged features. The way we name the type
of features is primarily for illustrating purpose.
</bodyText>
<subsectionHeader confidence="0.999382">
3.1 First-class Features
</subsectionHeader>
<bodyText confidence="0.896655">
The first-class features consist of:
</bodyText>
<listItem confidence="0.999668555555556">
• Event Class
• Event Stem
• Event and time strings
• Part of Speech of event terms
• Event Polarity
• Event Tense
• Event Aspect
• Type of temporal expression
• Value of temporal expression
</listItem>
<bodyText confidence="0.999874285714286">
The set of first-class features, which are directly
obtained from the markups of training/testing data,
are important, because most of them, including
Event Class, Event Stem, POS, Tense and Type of
Temporal Expression, have a great impact on
performance of machine learning classifiers,
compared with effects of other features.
</bodyText>
<subsubsectionHeader confidence="0.520839">
3.2.2 Derived Features
</subsubsectionHeader>
<bodyText confidence="0.9995905">
From the first-class features, we derive and
compute a number of other features:
</bodyText>
<listItem confidence="0.958992285714286">
• Tense and aspect shifts&apos;
• Temporal Signal
• Whether an event is enclosed in quotes
• Whether an event has modals prior to it
• Temporal relation between the Document
Creation Time and temporal expression in the
target sentence.
</listItem>
<bodyText confidence="0.99949025">
The way we compute tense and aspect shifts is
taking pair of contiguous events and assign a
true/false value to each relation instance based on
whether tense or shift change in this pair. Our
experiments show that these two features didn&apos;t
contribute to the overall score, probably because
they are redundant with the Tense and Aspect
features of each event term. Temporal Signal
</bodyText>
<tableCaption confidence="0.560752">
&apos; Initially used in (Mani, et. al. 2003)
</tableCaption>
<table confidence="0.984316384615385">
Documents with
TempEval Markups
NE &amp; POS Tagging
Syntactic Parsing
Context Detection
Semantic Parsing
Feature Extraction &amp; Composition
Temporal Tagging &amp; Normalizing
Syntactic Pattern Matching
Word Sense Disambiguation
Temporal Merging
Machine Learning
ML Testing
</table>
<page confidence="0.996735">
220
</page>
<bodyText confidence="0.9999413125">
represents temporal prepositions and they slightly
contribute to the overall score of classifiers.
The last feature in this category is the Temporal
Relation between the Document Creation Time and
the Temporal Expression in the target sentence.
The value of this feature could be &amp;quot;greater than&amp;quot;,
&amp;quot;less than&amp;quot;, &amp;quot;equal&amp;quot;, or &amp;quot;none&amp;quot;. Experiments show
that this is an important feature for Task A and B,
because it contributes several points to the overall
score. This value may be approximate for a
number of reasons. For example, we can&apos;t directly
compare a temporal expression of type Date with
another expression of type Duration. However,
even if we apply a simple algorithm to compute
this relationship, it results in a noticeably positive
effect on the performance of the classifier.
</bodyText>
<subsectionHeader confidence="0.564815">
3.2.3 Extended Features
</subsectionHeader>
<bodyText confidence="0.9897365">
Features in the third category are extracted by our
in-house tools, including:
</bodyText>
<listItem confidence="0.960049666666667">
• Whether an event term plays primary semantic
or syntactic roles in a sentence
• Whether an event and a temporal expression
are situated within the same linguistic context
• Whether two event terms co-refer in a
discourse (This feature is only used for Task C)
</listItem>
<bodyText confidence="0.966750972222222">
Investigation reveals that different types of
events defined in TimeML may or may not have
specific semantic or syntactic roles (e.g. THM or
OBJECT) in a particular context, therefore having
an impact on their ways to convey temporal
meanings. Experiments show that use of semantic
and syntactic roles as binary features slightly
increases performance.
The second feature in this category is Context
feature. We use a context detection tool, which
detects typical linguistic contexts, such as
Reporting, Belief, Modal, etc. to decide whether an
event and a temporal expression are within one
context. For example2,
• The company has reported declines in
operating profit in each of the past three
years, despite steady sales growth.
In this example, we identify a Reporting context
with its signal reported. The temporal expression
each of the past three years and the event declines
are within the same context (the feature value
would be TRUE). We intend this feature can help
2 This sentence is taken from the file wsj_0027.tml in
TempEval 2007&apos;s training data.
solve the problem of anchoring an event to its
actual temporal expressions. In fact, we don&apos;t
benefit from the use of this feature, probably
because detecting those linguistic contexts is a
problem in itself.
The third feature in this category is co-
referential feature, which is only used for Task C.
This feature indicates if two event terms within or
outside one sentence are referring to the same
event. Experiments show that this global feature
produces a positive effect on the overall
performance of the classifier.
</bodyText>
<subsectionHeader confidence="0.899823">
3.2.4 Merged Features
</subsectionHeader>
<bodyText confidence="0.999986882352941">
The last type of feature we engineered is the
merged feature. Due to time constraint, as well as
the fact that the system for Task B produces better
results than Task A and C, we only experimented
merging the output of the system for Task B into
the feature set of Task C and we achieved
noticeable improvements because of adding this
feature.
Most of the features introduced above are
experimented in all three tasks A, B and C, except
that the co-referential feature and the merged
feature are only used in Task C. Also, in Task C
since for each relation there are two events and
possibly two temporal expressions, the number of
features used is much more than that in Task A and
B. The total number of features for Task C&apos;s
training is 35 and 33 for testing.
</bodyText>
<subsectionHeader confidence="0.999838">
3.1 Combination of Machine Learning and
Human Rule
</subsectionHeader>
<bodyText confidence="0.999957941176471">
The design of our system allows both human rule-
based and machine learning-based decision
making. However, we have not decided exactly in
what situations machine learning and human rule
prediction should be used given a particular
instance. The basic idea here is that we want to
have the option to call either component on the fly
in different situations so that we can take
advantage of the two empirical approaches in an
integrated way. We did some initial experiments
on dynamically applying Human Rule Predictor
and Machine Learner on Task B and we were able
to obtain comparable results with or without using
hand-crafted rules. As pointed out in (Li, et, al.
2006), Support Vector Machine, as well as other
classifiers, makes most mistakes near the decision
plane in feature space. We will investigate the
</bodyText>
<page confidence="0.992226">
221
</page>
<bodyText confidence="0.989837">
possibility of applying human rule prediction to
those relation instances where Machine Learning
makes most mistakes.
</bodyText>
<subsectionHeader confidence="0.998847">
3.2 Experiments and Results
</subsectionHeader>
<bodyText confidence="0.997857357142857">
Based on the features discussed in Section 3.3, we
did a series of experiments for each task on four
models: Naive-Bayes, Decision Tree (C5.0),
Maximum Entropy and Support Vector Machine.
Due to space constraint, we only report results
from SVM model 3 , which produces best
performance in our case.
We here report two sets of performance numbers.
The first set is based on our evaluation against a
set of held-out data, 20 documents for each task,
which were taken from the training data. The
second set of performance numbers is based on
evaluation against the final testing data provided
by task organizers.
</bodyText>
<table confidence="0.9975534">
strict relaxed
P R F P R F
Task A 0.68 0.68 0.68 0.69 0.69 0.69
Task B 0.80 0.80 0.80 0.82 0.82 0.82
Task C 0.63 0.63 0.63 0.67 0.67 0.67
</table>
<tableCaption confidence="0.999157">
Table 1. Performance figures evaluated against held-out data
</tableCaption>
<table confidence="0.9993976">
strict relaxed
P R F P R F
Task A 0.59 0.57 0.58 0.61 0.60 0.60
Task B 0.75 0.71 0.73 0.75 0.72 0.74
Task C 0.55 0.55 0.55 0.60 0.60 0.60
</table>
<tableCaption confidence="0.998842">
Table 2. Performance figures evaluated against testing data
</tableCaption>
<table confidence="0.9995078">
Team strict relax
P R F P R F
Ours 0.59 0.57 0.58 0.61 0.60 0.60
Average 0.59 0.54 0.56 0.62 0.57 0.59
Best 0.62 0.62 0.62 0.64 0.64 0.64
</table>
<tableCaption confidence="0.993986">
Table 3. Performance figures in Comparison for Task A
</tableCaption>
<table confidence="0.9996394">
Team strict relax
P R F P R F
Ours 0.75 0.71 0.73 0.76 0.72 0.74
Average 0.76 0.72 0.74 0.78 0.74 0.75
Best 0.80 0.80 0.80 0.84 0.81 0.81
</table>
<tableCaption confidence="0.998855">
Table 4. Performance figures in comparison for Task B
</tableCaption>
<table confidence="0.9993302">
Team strict relax
P R F P R F
Ours 0.55 0.55 0.55 0.60 0.60 0.60
Average 0.51 0.51 0.51 0.60 0.60 0.60
Best 0.55 0.55 0.55 0.66 0.66 0.66
</table>
<tableCaption confidence="0.99985">
Table 5. Performance figures in comparison for Task C
</tableCaption>
<footnote confidence="0.968967">
3 We use the LIBSVM implementation of SVM,
available at http://www.csie.ntu.edu.tw/cjlin/libsvm
</footnote>
<bodyText confidence="0.9977838">
According to Table 1 and 2, it appears that there
are significant differences between the TLINK
patterns in the held-out data and the final testing
data, since the performance of the classifier shows
an apparent discrepancy in two cases.
Table 3, 4 and 5 show performance numbers of
our system, the average and the best system in
comparison. There are six teams in total
participating in the TempEval 2007 evaluation this
year.
</bodyText>
<sectionHeader confidence="0.999523" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999980857142857">
We participated in the SemEval2007 workshop and
achieved encouraging results by devoting our
initial efforts in this area. In next step, we plan to
seek ways to expand the training data, implement
quality human rules by performing rigorous data
analysis, and explore use of more features for
machine learning through feature engineering.
</bodyText>
<sectionHeader confidence="0.999435" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995499681818182">
B. Boguraev and R.K. Ando. 2005. TimeML-compliant
Text Analysis for Temporal Reasoning. Proceedings
ofIJCAI, UK
D. Ahn, S.F. Adafre and M.D. Rijke. 2005. Towards
Task-based Temporal Extraction and Recognition.
Dagstuhl Seminar Proceedings 05151.
Inderjeet Mani and George Wilson. 2000. Robust
Temporal Processing of News. Proceedings of
ACL&apos;2000.
Inderjeet Mani, Barry Schiffman, and Ranping Zhang.
2003. Inferring Temporal Ordering of Events in
News. Proceedings ofHLT-NAACL&apos;03, 55-57.
K. Hacioglu, Y. Chen and B. Douglas. 2005. Automatic
Time Expression Labeling for English and Chinese
Text, Proceedings of CICLing-2005.
L. Li, T. Mao, D. Huang and Y. Yang. 2006. Hybrid
Models for Chinese Named Entity Recognition.
Proceedings of the Fifth SIGHAN Workshop on
Chinese Language Processing.
The TimeML Working Group. 2005. The TimeML 1.2
Specification.
http://www.timeml.org/site/publications/specs.html
</reference>
<page confidence="0.99747">
222
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.389250">
<title confidence="0.9532225">LCC-TE: A Hybrid Approach to Temporal Relation Identification in News Text</title>
<author confidence="0.602307">Congmin Min</author>
<affiliation confidence="0.997384">Language Computer Corporation</affiliation>
<address confidence="0.9966725">1701 N. Collins Blvd. Suite 2000 Richardson, TX 75080</address>
<email confidence="0.999438">cmin@languagecomputer.com</email>
<author confidence="0.948351">Munirathnam Srikanth</author>
<affiliation confidence="0.999757">Language Computer Corporation</affiliation>
<address confidence="0.996401">1701 N. Collins Blvd, Suite 2000 Richardson, TX 75080</address>
<email confidence="0.866922">Srikanth.munirathnam@languagecomputer.com</email>
<author confidence="0.999087">Abraham Fowler</author>
<affiliation confidence="0.99986">Language Computer Corporation</affiliation>
<address confidence="0.9967845">1701 N. Collins Blvd, Suite 2000 Richardson, TX 75080</address>
<email confidence="0.99924">abraham@languagecomputer.com</email>
<abstract confidence="0.997187411764706">This paper explores a hybrid approach to temporal information extraction within the TimeML framework. Particularly, we focus on our initial efforts to apply machine learning techniques to identify temporal relations as defined in a constrained manner by the TempEval-2007 task. We explored several machine learning models and human rules to infer temporal relations based on the features available in TimeBank, as well as a number of other features extracted by our in-house tools. We participated in all three sub-tasks of the TempEval task in SemEval-2007 workshop and the evaluation shows that we achieved comparable results in Task A &amp; B and competitive results in Task C.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>B Boguraev</author>
<author>R K Ando</author>
</authors>
<title>TimeML-compliant Text Analysis for Temporal Reasoning. Proceedings ofIJCAI,</title>
<date>2005</date>
<publisher>UK</publisher>
<marker>Boguraev, Ando, 2005</marker>
<rawString>B. Boguraev and R.K. Ando. 2005. TimeML-compliant Text Analysis for Temporal Reasoning. Proceedings ofIJCAI, UK</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Ahn</author>
<author>S F Adafre</author>
<author>M D Rijke</author>
</authors>
<title>Towards Task-based Temporal Extraction and Recognition. Dagstuhl Seminar</title>
<date>2005</date>
<booktitle>Proceedings</booktitle>
<pages>05151</pages>
<marker>Ahn, Adafre, Rijke, 2005</marker>
<rawString>D. Ahn, S.F. Adafre and M.D. Rijke. 2005. Towards Task-based Temporal Extraction and Recognition. Dagstuhl Seminar Proceedings 05151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjeet Mani</author>
<author>George Wilson</author>
</authors>
<title>Robust Temporal Processing of News.</title>
<date>2000</date>
<booktitle>Proceedings of ACL&apos;2000.</booktitle>
<marker>Mani, Wilson, 2000</marker>
<rawString>Inderjeet Mani and George Wilson. 2000. Robust Temporal Processing of News. Proceedings of ACL&apos;2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjeet Mani</author>
<author>Barry Schiffman</author>
<author>Ranping Zhang</author>
</authors>
<title>Inferring Temporal Ordering of Events in News.</title>
<date>2003</date>
<booktitle>Proceedings ofHLT-NAACL&apos;03,</booktitle>
<pages>55--57</pages>
<marker>Mani, Schiffman, Zhang, 2003</marker>
<rawString>Inderjeet Mani, Barry Schiffman, and Ranping Zhang. 2003. Inferring Temporal Ordering of Events in News. Proceedings ofHLT-NAACL&apos;03, 55-57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Hacioglu</author>
<author>Y Chen</author>
<author>B Douglas</author>
</authors>
<title>Automatic Time Expression Labeling for English and Chinese Text,</title>
<date>2005</date>
<booktitle>Proceedings of CICLing-2005.</booktitle>
<marker>Hacioglu, Chen, Douglas, 2005</marker>
<rawString>K. Hacioglu, Y. Chen and B. Douglas. 2005. Automatic Time Expression Labeling for English and Chinese Text, Proceedings of CICLing-2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Li</author>
<author>T Mao</author>
<author>D Huang</author>
<author>Y Yang</author>
</authors>
<title>Hybrid Models for Chinese Named Entity Recognition.</title>
<date>2006</date>
<booktitle>Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing.</booktitle>
<marker>Li, Mao, Huang, Yang, 2006</marker>
<rawString>L. Li, T. Mao, D. Huang and Y. Yang. 2006. Hybrid Models for Chinese Named Entity Recognition. Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing.</rawString>
</citation>
<citation valid="true">
<title>The TimeML Working Group.</title>
<date>2005</date>
<journal>The TimeML</journal>
<volume>1</volume>
<note>Specification.</note>
<marker>2005</marker>
<rawString>The TimeML Working Group. 2005. The TimeML 1.2 Specification.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>