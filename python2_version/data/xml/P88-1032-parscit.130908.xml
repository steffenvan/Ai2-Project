<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.966632">
AN EARLEY-TYPE PARSING ALGORITHM
FOR TREE ADJOINING GRAMMARS *
</title>
<author confidence="0.99061">
Yves Schabes and Aravind K. Joshi
</author>
<affiliation confidence="0.954241666666667">
Department of Computer and Information Science
University of Pennsylvania
Philadelphia PA 19104-6389 USA
</affiliation>
<email confidence="0.911318">
schabesalinc.cis.upenn.edu joshiOcis.upenn.edu
</email>
<sectionHeader confidence="0.978074" genericHeader="abstract">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.999929588235294">
We will describe an Earley-type parser for Tree
Adjoining Grammars (TAGs). Although a CKY-
type parser for TAGs has been developed earlier
(Vijay-Shanker and Joshi, 1985), this is the first
practical parser for TAGs because as is well known
for CFGs, the average behavior of Earley-type
parsers is superior to that of CKY-type parsers.
The core of the algorithm is described. Then we
discuss modifications of the parsing algorithm that
can parse extensions of TAGs such as constraints
on adjunction, substitution, and feature structures
for TAGs. We show how with the use of substi-
tution in TAGs the system is able to parse di-
rectly CFGs and TAGs. The system parses unifi-
cation formalisms that have a CFG skeleton and
also those with a TAG skeleton. Thus it also al-
lows us to embed the essential aspects of PATR-II.
</bodyText>
<sectionHeader confidence="0.999091" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.991347230769231">
Although formal properties of Tree Adjoining
Grammars (TAGs) have been investigated (Vijay-
Shanker, 1987)—for example, there is an 0(n6)-
time CKY-like algorithm for TAGs (Vijay-Shanker
and Joshi, 1985)—so far there has been no at-
tempt to develop an Earley-type parser for TAGs.
This paper presents an Earley parser for TAGs
and discusses modifications to the parsing algo-
rithm that make it possible to handle extensions
of TAGs such as constraints on adjunction, sub-
*This work is partially supported by ARO grant
DAA29-84-9-007, DARPA grant N0014-85-K0018, NSF
grants MCS-82-191169 and DCR-84-10413. The authors
would like to express their gratitude to Vijay-Shanlcer for
his helpful comments relating to the core of the algorithm,
Richard Billington and Andrew Chalnicic for their graphi-
cal TAG editor which we integrated in our system and for
their programming advice Thanks are also due to Anne
Abeille and Ellen Hays.
stitution, and feature structure representation for
TAGs.
TAGs were first introduced by Joshi, Levy and
Takahashi (1975) and Joshi (1983). We describe
very briefly the Tree Adjoining Grammar formal-
ism. For more details we refer the reader to Joshi
(1983), Kroch and Joshi (1985) or Vijay-Shanker
(1987).
Definition 1 (Tree Adjoining Grammar) :
A TAG is a 5-tuple G = (VN VT S I , A) where
VN is a finite set of non-terminal symbols, VT is
a finite set of terminals, S is a distinguished non-
terminal, I is a finite set of trees called initial
trees and A is a finite set of trees called auxiliary
trees. The trees in / U A are called elementary
trees.
Initial trees (see left tree in Figure 1) are char-
acterized as follows: internal nodes are labeled by
non-terminals; leaf nodes are labeled by either ter-
minal symbols or the empty string.
</bodyText>
<sectionHeader confidence="0.339497" genericHeader="method">
Z X
</sectionHeader>
<subsectionHeader confidence="0.283183">
ts&amp;quot;..termlnels terminals -.of **•—.torminals
</subsectionHeader>
<figureCaption confidence="0.998345">
Figure 1: Schematic initial and auxiliary trees
</figureCaption>
<bodyText confidence="0.9998387">
Auxiliary trees (see right tree in Figure 1)
are characterized as follows: internal nodes are la-
beled by non-terminals; leaf nodes are labeled by
a terminal or by the empty string except for ex-
actly one node (called the foot node) labeled by
a non-terminal; furthermore the label of the foot
node is the same as the label of the root node.
We now define a composition operation called
adjoining or adjunction which builds a new tree
from an auxiliary tree fl and a tree a (a is any tree,
</bodyText>
<page confidence="0.994484">
258
</page>
<bodyText confidence="0.999664571428571">
initial, auxiliary or tree derived by adjunction).
The resulting tree is called a derived tree. Let
a be a tree containing a node n labeled by X and
let ft be an auxiliary tree whose root node is also
labeled by X. Then the adjunction of # to a at
node n will be the tree 7 shown in Figure 2. The
resulting tree, 7, is built as follows:
</bodyText>
<listItem confidence="0.984788">
• The sub-tree of a dominated by n, call it t, is
excised, leaving a copy of n behind.
• The auxiliary tree # is attached at n and its root
node is identified with n.
• The sub-tree t is attached to the foot node of #
and the root node n oft is identified with the foot
node of #.
</listItem>
<figureCaption confidence="0.999398">
Figure 2: The mechanism of adjunction
</figureCaption>
<bodyText confidence="0.996043866666667">
Then define the tree set of a TAG G, T(G) to
be the set of all derived trees starting from initial
trees in I. Furthermore, the string language
generated by a TAG, L(G), is defined to be the
set of all terminal strings of the trees in T(G).
TAGs factor recursion and dependencies by ex-
tending the domain of locality. They offer novel
ways to encode the syntax of natural language
grammars as discussed in Kroch and Joshi (1985)
and Abeille (1988).
In 1985, Vijay-Shanker and Joshi introduced a
CKY-like algorithm for TAGs. They therefore es-
tablished 0(n6) time as an upper bound for pars-
ing TAGs. The algorithm was implemented, but
in our opinion the result was more theoretical than
practical for several reasons. First the algorithm
assumes that elementary trees are binary branch-
ing and that there are no empty categories on the
frontiers of the elementary trees. Second, since it
works on nodes that have been isolated from the
tree they belong to, it isolates them from their
domain of locality. However all important linguis-
tic and computational properties of TAGs follow
from this extended domain of locality. And most
importantly, although it runs in 0(n6) worst time,
it also runs in 0(n6) best time. As a consequence,
the CKY algorithm is in practice very slow.
Since the average time complexity of Earley&apos;s
parser depends on the grammar and in practice
runs much better than its worst time complex-
ity, we decided to try to adapt Earley&apos;s parser
for CFGs to TAGs. Earley&apos;s algorithm for CFGs
(Earley, 1970, Aho and Ullman, 1973) is a bottom-
up parser which uses top-down information. It
manipulates states of the form A --• a./3{i] while
using three processors: the predictor, the comple-
tor and the scanner. The algorithm for CFGs runs
in 0(IGI2n3) time and in 0(IGIn2) space in all
cases, and parses unambiguous grammars in 0(n2)
time (n being the length of the input, ClI the size
of the grammar).
Given a context-free grammar in any form and
an input string al • • • an, Earley&apos;s parser for CFGs
maintains the following invariant:
The state A —• a.#[i] is in states set Sk if
</bodyText>
<equation confidence="0.885665">
S 6A7, al • -a; and a ai+i • • ak
</equation>
<bodyText confidence="0.997237615384616">
The correctness of the algorithm is a corollary of
this invariant.
Finding a Earley-type parser for TAGs was a
difficult task because it was not clear how to
parse TAGs bottom up using top-down informa-
tion while scanning the input string from left to
right. In order to construct an Earley-type parser
for TAGs, we will extend the notions of dotted
rules and states to trees. Anticipating the proof
of correctness and soundness of our algorithm, we
will state an invariant similar to Earley&apos;s original
invariant. Then we present the algorithm and its
main extensions.
</bodyText>
<sectionHeader confidence="0.693432" genericHeader="method">
2 Dotted symbols, dotted
trees, tree traversal
</sectionHeader>
<bodyText confidence="0.998352">
The full algorithm is explained in the next section.
This section introduces preliminary concepts that
will be used by the algorithm. We first show how
dotted rules can be extended to trees. Then we
introduce a tree traversal that the algorithm will
mimic in order to scan the input from left to right.
We define a dotted symbol as a symbol asso-
ciated with a dot above or below and either to the
left or to the right of it. The four positions of the
dot are annotated by la, lb, ra, rb (resp. left above,
left below, right above, right below):
Then we define a dotted tree as a tree with
exactly one dotted symbol.
Given a dotted tree with the dot above and to
the left of the root, we define a tree traversal of a
dotted tree as follows (see Figure 3):
</bodyText>
<figure confidence="0.7260075">
/.\
(a) (11)
</figure>
<page confidence="0.672277">
259
</page>
<figure confidence="0.96938175">
START •••■\ END
if A 0
A.„,.....„.•••&apos;&apos;_.....„,;&amp;quot;0„„..
B 1 • D1 3
• 4,. \ 0117 \fr\
• -4*&amp;quot;. 49. e •••• &amp;quot;&apos;4/1&amp;quot; • •
E F G H I
2.1 2.2 2.3 3.1 3.2
</figure>
<figureCaption confidence="0.999648">
Figure 3: Example of a tree traversal
</figureCaption>
<listItem confidence="0.9992826">
• if the dot is at position la of an internal node,
we move the dot down to position lb,
• if the dot is at position lb of an internal node,
we move to position /a of its leftmost child,
• if the dot is at position la of a leaf, we move the
dot to the right to position ra of the leaf,
• if the dot is at position rb of a node, we move
the dot up to position ra of the same node,
• if the dot is at position ra of a node, there are
two cases:
</listItem>
<bodyText confidence="0.991938625">
- if the node has a right sibling, then move the
dot to the right sibling at position la.
- if the node does not have a right sibling, then
move the dot to its parent at position rb.
This traversal will enable us to scan the frontier
of an elementary tree from left to right while try-
ing to recognize possible adjunctions between the
above and below positions of the dot.
</bodyText>
<sectionHeader confidence="0.984378" genericHeader="method">
3 The algorithm
</sectionHeader>
<bodyText confidence="0.9998695">
We define an appropriate data structure for the
algorithm. We explain how to interpret the struc-
tures that the parser produces. Then we describe
the algorithm itself.
</bodyText>
<subsectionHeader confidence="0.999811">
3.1 Data structures
</subsectionHeader>
<bodyText confidence="0.991364833333333">
The algorithm uses two basic data structures:
state and states set.
A states set S is defined as a set of states. The
states sets will be indexed by an integer: Si with
i E N. The presence of any state in states set i
will mean that the input string ai...ai has been
recognized.
Any tree a will be considered as a function from
tree addresses to symbols of the grammar (termi-
nal and non-terminal symbols): if x is a valid ad-
dress in a, then a(x) is the symbol at address x
in the tree a.
</bodyText>
<construct confidence="0.6659695">
Definition 2 A state s is defined as a 10-tuple,
[a, dot, side , pos , 1, ft, fr, star, t; • , bn where:
</construct>
<listItem confidence="0.983423">
• a: is the name of the dotted tree.
• dot: is the address of the dot in the tree a.
• side: is the side of the symbol the dot is on;
side E {left, right}.
• pos: is the position of the dot;
pas E {above,below}
• star is an address in a. The corresponding node
in a is called the starred node.
• 1 (left), ft (foot left), fr (foot right), t; (top left
</listItem>
<bodyText confidence="0.78024775">
of starred node), b; (bottom left of starred node)
are indices of positions in the input string ranging
over [O, n], n being the length of the input string.
They will be explained further below.
</bodyText>
<subsectionHeader confidence="0.99977">
3.2 Invariant of the algorithm
</subsectionHeader>
<bodyText confidence="0.958615">
The states s in a states set Si have a common prop-
erty. The following section describes this invariant
in order to give an intuitive interpretation of what
the algorithm does. This invariant is similar to
Earley&apos;s invariant.
Before explaining the main characterization of
the algorithm, we need to define the set of nodes
on which an adjunction is allowed for a given state.
Definition 3 The set of nodes &apos;Ns) on which an
adjunction is possible for a given state
S = [a, dot, side, pos, 1, ft, fy. , star,t; , is de-
fined as the union of the following sets of nodes
in a:
</bodyText>
<listItem confidence="0.865269142857143">
• the set of nodes that have been traversed on the
left and right sides, i.e., the four positions of the
dot have been traversed;
• the set of nodes on the path from the root node
to the starred node, root node and starred node
included. Note that if there is no star this set is
empty.
</listItem>
<bodyText confidence="0.9994927">
Definition 4 (Left part of a dotted tree)
The left part of a dotted tree is the union of the
set of nodes in the tree that have been traversed
on the left and right sides and the set of nodes
that have been traversed on the left side only.
We will first give an intuitive interpretation of
the ten components of a state, and then give the
necessary and sufficient conditions for membership
of a state in a states set.
We interpret informally a state
</bodyText>
<subsubsectionHeader confidence="0.340939">
s = [a, dot, side, pos, 1, ft, f,. , star, tr , bn in the fol-
</subsubsectionHeader>
<bodyText confidence="0.615068">
lowing way (see Figure 4):
</bodyText>
<page confidence="0.990451">
260
</page>
<figureCaption confidence="0.999343">
Figure 4: Meaning of s E
</figureCaption>
<listItem confidence="0.564658066666667">
• 1 is an index in the input string indicating where
the tree derived from a begins.
• fi is an index in the input string corresponding
to the point just before the foot node (if any) in
the tree derived from a.
• fr is an index in the input string corresponding
to the point just after the foot node (if any) in the
tree derived from a.The pair fi and f,. will mean
that the foot node subsumes the string af,+,...af r.
• star is the address in a of the deepest node that
subsumes the dot on which an adjunction has been
partially recognized. If there is no adjunction in
the tree a along the path from the root to the dot-
ted node, star is unbound.
• t; is an index in the input string corresponding
</listItem>
<bodyText confidence="0.829970333333333">
to the point in the tree where the adjunction on
the starred node was made. If star is unbound,
then t; is also unbound.
</bodyText>
<listItem confidence="0.783046857142857">
• b;&apos; is an index in the input string corresponding
to the point in the tree just before the foot node of
the tree adjoined at the starred node. The pair t;
and b; will mean that the string as far as the foot
node of the auxiliary tree adjoined at the starred
node matches the substring at;+1...abt of the in-
put string. If star is unbound, then b; is also
unbound.
• s E Si means that the recognized part of the dot-
ted tree a, which is the left part of it, is consistent
with the input string from al to a/ and from a/ to
af, and from afr to Oh or from al to ai and from at
to ai when the foot node is not in the recognized
part of the tree.
</listItem>
<bodyText confidence="0.9985295">
We are now ready to characterize the member-
ship of s in Si:
</bodyText>
<subsectionHeader confidence="0.553556">
Invariant 1
</subsectionHeader>
<bodyText confidence="0.959505380952381">
A state s = [a, dot, side,pos,l, fr,star,t7 ,b7] is
in Si if and only if there is a derived tree from an
initial tree such that (see Figure 4):
1. The tree a is part of the derivation.
2. The tree derived from a in the derivation tree,
7F, has adjunctions only on nodes in P(s).
3. The part Of the tree to the left of the dot in the
tree derived spans the string al
4. The tree derived from a, re, has a yield that
starts just after at, ends at af, before the foot node
(if af, is defined), and starts after the foot node
just after cif, (if af, is defined).
5. If there are adjunctions on the path from the
dotted node to the root of a, then star is the ad-
dress of the deepest adjunction on that path and
the auxiliary tree adjoined at that node star has
a yield that starts just after at and stops at its
foot node at ab.
The proof of this invariant has as corollaries the
soundness, completeness, and therefore the cor-
rectness of the algorithm.
</bodyText>
<subsectionHeader confidence="0.999478">
3.3 The recognizer
</subsectionHeader>
<bodyText confidence="0.9029586">
The Earley-type recognizer for TAGs follows:
Let G be a TAG.
Let al...an be the input string.
program recognizer
begin
</bodyText>
<equation confidence="0.486142">
So = { [a,0,1eft,above,O, „ „
</equation>
<bodyText confidence="0.9479354">
kk is an initial tree }
For i := 0 to n do
begin
Process the states of Si, performing one of
the following seven operations on each state
</bodyText>
<listItem confidence="0.838339555555556">
s = [a, dot, side, pos,l, fr,star,t; ,b7J
until no more states can be added:
1. Scanner
2. Move dot down
3. Move dot up
4. Left Predictor
6. Left Completor
6. Right Predictor
7. Right Completor
</listItem>
<construct confidence="0.4465765">
If Si+1 is empty and i &lt; n, return rejection.
end.
If there is in S„ a state
s = [a, 0, right, above,O, „
</construct>
<bodyText confidence="0.88667125">
such that a is an initial tree
then return acceptance.
end.
,
</bodyText>
<page confidence="0.98608">
261
</page>
<bodyText confidence="0.9999785">
The algorithm is a general recognizer for TAGs.
Unlike the CKY algorithm, it requires no condi-
tion on the grammar: the trees can be binary or
not, the elementary (initial or auxiliary) trees can
have the empty string as frontier. It is an off-line
algorithm: it needs to know the length n of the
input string. However we will see later that it can
very easily be modified to an on-line algorithm by
the use of an end-marker in the input string.
We now describe one by one the seven processes.
The current states set is presumed to be Si and the
state to be processed is
</bodyText>
<subsubsectionHeader confidence="0.364179">
s = [a, dot, side, pos, I, star,t].
</subsubsectionHeader>
<bodyText confidence="0.994342">
Only one of the seven processes can be applied
to a given state. The side, the position, and the
address of the dot determine the unique process
that can be applied to the given state.
Definition 5 (Adjunct(a, address)) Given
a TAG G, define Adjund(a, address) as the set
of auxiliary trees that can be adjoined in the ele-
mentary tree a at the node n which has the given
address. In a TAG without any constraints on
adjunction, if n is a non-terminal node, this set
consists of all auxiliary trees that are rooted by a
node with same label as the label of n.
</bodyText>
<subsectionHeader confidence="0.509633">
3.3.1 Scanner
</subsectionHeader>
<bodyText confidence="0.997971285714286">
The scanner scans the input string. Suppose that
the dot is to the left of and above a terminal sym-
bol (see Figure 5). Then if the terminal symbol
matches the next input token, the program should
record that a new token has been recognized and
try to recognize the rest of the tree.
Therefore the scanner applies to
</bodyText>
<listItem confidence="0.791291">
s = [a, dot, le ft, above, 1, ft, fr, star,V ,bn
such that a(dot) is a terminal symbol and
a(dot) = ai+1 or a(dot) is the empty symbol
C.
• Case 1: a(dot) = ai+i
</listItem>
<subsectionHeader confidence="0.464201">
The scanner adds
</subsectionHeader>
<bodyText confidence="0.937475">
[a, dot, right, above,!, fj, fr,siar,i, bfl to
Si+i •
</bodyText>
<listItem confidence="0.995809">
• Case 2: a(dot) = e
</listItem>
<subsectionHeader confidence="0.406721">
The scanner adds
</subsectionHeader>
<bodyText confidence="0.9616635">
[a, dot, right, above, l, fi, star,t7 ,bn to
.
</bodyText>
<subsectionHeader confidence="0.659275">
3.3.2 Move Dot Down
</subsectionHeader>
<bodyText confidence="0.969138">
Move dot down (See Figure 6), moves the dot
down, from position lb of the dotted node to path-
</bodyText>
<figureCaption confidence="0.999042333333333">
Figure 5: Scanner
Figure 6: Move dot down
tion la of its leftmost child.
</figureCaption>
<bodyText confidence="0.981465">
It therefore applies to
</bodyText>
<equation confidence="0.362553">
s = [a, dot, le ft, below,l, ft, fr,star,t7 , bn
</equation>
<bodyText confidence="0.99593">
such that the node where the dot is has a
leftmost child at address u.
It adds [a, u,le ft, above,&apos; fr, star, tl, bn to
.
</bodyText>
<subsectionHeader confidence="0.751477">
3.3.3 Move Dot Up
</subsectionHeader>
<bodyText confidence="0.945610125">
Move dot up (See Figure 7), moves the dot &amp;quot;up&amp;quot;,
from position ra of the dotted node to position la
of its right sibling if it has a right sibling, other-
wise to position rb of its parent.
It therefore applies to
S = [a, dot, right, above, I, ,star,t7 ,
such that the node on which the dot is
has a parent node.
</bodyText>
<listItem confidence="0.90805175">
• Case 1: the node where the dot is
has a right sibling at address r.
It adds [a, r,le ft, above,l, Ii, fr, star,t7 ,bn
to Si.
• Case 2: the node where the dot is is
the rightmost child of the parent
node p.
It adds
</listItem>
<equation confidence="0.614032333333333">
[a, p, right, below, I, star,t7 ,bn to Si .
Ceee.2:a e
[14,fr,31*,b11
</equation>
<page confidence="0.728072">
262
</page>
<figure confidence="0.999652">
(1. ft , fr , 1,11
UXIAAP.W9
[1, ft , fr • II* , No]
[1, fl, fr , [Is , bl•]
Cus.2 A iesi A
.A
added t o Si
IC\
Dee Al* ,b19 (41:41*.bis)
MSI( ;A added to Si
hh
L
tieee.-J
added to Si
Casa.LIChatinaLntlinla
UJIA0.x$P1
Casc.24.2C:n.11nsightmastcliikl
(1.11.,*.1s.m9 [1,11.1:41*.b1•1
</figure>
<figureCaption confidence="0.7428725">
Figure 7: Move dot up
3.3.4 Left Predictor
</figureCaption>
<bodyText confidence="0.993989">
Suppose that there is a dot to the left of and above
a non-terminal symbol A (see Figure 8). Then the
algorithm takes two paths in parallel: it makes a
prediction of adjunction on the node labeled by
A and tries to recognize the adjunction (stepl)
and it also considers the case where no adjunction
has been done (step2). These operations are per-
formed by the Left Predictor.
It applies to
</bodyText>
<listItem confidence="0.862633">
s = [a, dot, le ft, above,l, star,q ,bn
such that a(dot) is a non-terminal.
• Step 1. It adds the states
UP, 0,1e ft, above, i„ „ ,
1,8 E Adjunct(cx, dot) } to Si.
• Step 2.
- Case 1: the dot is not on the
foot node.
It adds the state
[a, dot,le ft,below, I, fr,star,t; ,14]
to Si.
- Case 2: the dot is on the foot
node. Necessarily, since the
foot node has not been already
traversed, fl and f,. are
unspecified.
</listItem>
<bodyText confidence="0.712015333333333">
It adds the state
[a, dot, le ft,below,l,i,-,star,t; ,bn to
Si.
</bodyText>
<subsectionHeader confidence="0.968873">
3.3.5 Left Completor
</subsectionHeader>
<bodyText confidence="0.996745333333333">
Suppose that the auxiliary that we left-predicted
has been recognized as far as its foot (see Fig-
ure 9). Then the algorithm should try to recognize
</bodyText>
<figureCaption confidence="0.998247">
Figure 8: Left Predictor
</figureCaption>
<figure confidence="0.782262">
WONAMbin
</figure>
<figureCaption confidence="0.999492">
Figure 9: Left completor
</figureCaption>
<bodyText confidence="0.9915438">
what was pushed under the foot node. (A star in
the original tree will signal that an adjunction has
been made and half recognized.) This operation
is performed by the Left Completor.
It applies to
</bodyText>
<subsubsectionHeader confidence="0.383284">
s = [a, dot, le ft , below , I, star,t7 ,bn
</subsubsectionHeader>
<bodyText confidence="0.949166">
such that the dot is on the foot node.
For all
</bodyText>
<subsubsectionHeader confidence="0.321669">
s = [fl, doe ,le ft, above,1&apos; , f,!, star&apos; ,tr ,b71 in
</subsubsectionHeader>
<bodyText confidence="0.537043">
SI such that a E Adjunct(13,doe)
</bodyText>
<listItem confidence="0.920969666666667">
• Case 1: doe is on the foot node of
fl. Then necessary, fl and f; are
unbound.
</listItem>
<bodyText confidence="0.7814785">
It adds the state
[Adoe ,left,below,11,i,-,doe,l,i] to Si .
</bodyText>
<listItem confidence="0.98375">
• Case 2: doe is not on the toot node
of fl.
</listItem>
<bodyText confidence="0.6791765">
It adds the state
p, doe , le ft,below,l&apos; , fi, f&amp;quot;., doe , I, i] to Si.
</bodyText>
<figure confidence="0.7112715">
CA\.A
[11-0.1*.b11
</figure>
<page confidence="0.962574">
263
</page>
<figureCaption confidence="0.940575">
Figure 10: Right Predictor
</figureCaption>
<subsectionHeader confidence="0.622255">
3.3.6 Right Predictor
</subsectionHeader>
<bodyText confidence="0.985299454545454">
Suppose that there is a dot to the right of and be-
low a node A (see Figure 10). If there has been
an adjunction made on A (case 1), the program
should try to recognize the right part of the aux-
iliary tree adjoined at A. However if there was no
adjunction on A (case 2), then the dot should be
moved up. Note that the star will tell us if an ad-
junction has been made or not. These operations
are performed by the Right predictor.
The right predictor applies to
s = [a, dot, right,below,l, ft, fr, star,t; , b]
</bodyText>
<listItem confidence="0.956848">
• Case 1: dot= star
For all states
</listItem>
<bodyText confidence="0.84002225">
s = doe , le ft, below ,t; ,—,star!,tr ,br]
in Sb7 such that )5 E Adjunct(a, dot),
it adds the state
dot&apos; ,right,below,V , , star&apos; ,tr , br] to
</bodyText>
<listItem confidence="0.9526306">
• .
• case 2: dot 0 star
It adds the state
[a, dot, right, above, 1, ft, f ,.,star,t7 , bn to
• .
</listItem>
<subsectionHeader confidence="0.816967">
3.3.7 Right Completor
</subsectionHeader>
<bodyText confidence="0.999893">
Suppose that the dot is to the right of and above
the root of an auxiliary tree (see Figure 11). Then
the adjunction has been totally recognized and the
program should try to recognize the rest of the tree
in which the auxiliary tree has been adjoined. This
operation is performed by the Right Completor.
</bodyText>
<figure confidence="0.7221325">
inSI:
[1&apos;,fr,fe,t1•&apos;,b1•1
</figure>
<figureCaption confidence="0.99179">
Figure 11: Right Completor
</figureCaption>
<bodyText confidence="0.8459705">
It applies to
8= [a, 0, right, above,l, ft,
</bodyText>
<figure confidence="0.508693444444444">
For all states
81 = ffi , dot&apos; , le ft, above,1&apos; , , f;., star&apos;, tr , br]
in Si
and for all states
[ft, dot&apos; ,rig ht, below, l&apos; ,f(, dot&apos; ,1, in S „
such that a E Adjunct(f3 , dot&apos;)
It adds
f./3, dot&apos;, right, above, i&apos;,1,&apos;,7, star&apos; ,tr ,b71 to
.
</figure>
<figureCaption confidence="0.916910333333333">
Where 7 = f, if f is bound in state sr,
and 7 can have any value, if f is unbound
in state sr.
</figureCaption>
<subsectionHeader confidence="0.902727">
3.4 Handling constraints on adjunc-
tion
</subsectionHeader>
<bodyText confidence="0.999914666666667">
In a TAG, one can, for each node of an elementary
tree, specify one of the following three constraints
on adjunction (Joshi, 1987):
</bodyText>
<listItem confidence="0.96558">
• Null adjunction (NA): disallow any adjunc-
tion on the given node.
• Obligatory adjunction (OA): an auxiliary
tree must be adjoined on the given node.
• Selective adjunction (SA(T)): a set T of aux-
iliary trees that can be adjoined on the given node
is specified.
</listItem>
<bodyText confidence="0.998902333333333">
The algorithm can be very easily modified to
handle those constraints. First, the function
Adjunct(a , address) must be modified as follows:
</bodyText>
<listItem confidence="0.945915428571429">
• Adjunct(a, address) = 0, if there is NA on the
node..
• Adjunct(a , address) as previously defined, if
there is OA on the node.
• Adjunct(a , address) = T, if there is SA(T) on
the node.
Second, step 2 of the left predictor must be done
</listItem>
<figure confidence="0.942362136363637">
(141.frAl*.bil
ClatfrAl•,bri
in SbP:
A
W.WeAMtel
NnA.Ambri
Added to Si
A.
RNIe.10°.121e1
inSfr:
264
$ o
a s 2 d 3
21 s 2.2 2.3
(NA)
Figure 12: L = {anbnecndnin &gt; 0}
Step 1
make sure that no sdjunetian
is possible on the root of an initial tress
S.
s.
s
</figure>
<figureCaption confidence="0.999806">
Figure 13: Use of end marker in TAG
</figureCaption>
<bodyText confidence="0.759325">
only if there is no obligatory adjunction on the
node at address dot in the tree a.
</bodyText>
<subsectionHeader confidence="0.986618">
3.5 An example
</subsectionHeader>
<bodyText confidence="0.961248909090909">
We give one example that illustrates how the rec-
ognizer works. The grammar used for the exam-
ple generates the language L = {anbn ecn dnIn &gt;
0}. The input string given to the recognizer
is: aabbeccdd. The grammar is shown in Fig-
ure 12. The states sets are shown in Figure 14.
Next to each state we have printed in paren-
theses the name of the processor that was ap-
plied to the state. The input is recognized since
[a, 0, right, above,0 „ ] is in states set
59.
</bodyText>
<subsectionHeader confidence="0.942219">
3.6 Remarks
</subsectionHeader>
<bodyText confidence="0.991695166666667">
Use of move dot up and move dot down
Move dot down and move dot up can be eliminated
in the algorithm by merging the original dot and
the position it is moved to. However for explana-
tory purposes we chose to use these two processors
in this paper.
</bodyText>
<subsectionHeader confidence="0.968014">
Off-line vs on-line
</subsectionHeader>
<bodyText confidence="0.999984">
The algorithm given is an off-line recognizer. It
can be very easily modified to work on line by
adding an end marker to all initial trees in the
grammar (see Figure 13).
</bodyText>
<subsectionHeader confidence="0.99496">
Extracting a parse
</subsectionHeader>
<bodyText confidence="0.9984974">
The algorithm that we describe in section 3.3 is a
recognizer. However, if we include pointers from
a state to the other states which caused it to be
placed in the states set, the recognizer can be mod-
ified to produce all parses of the input string.
</bodyText>
<subsectionHeader confidence="0.996894">
3.7 Correctness
</subsectionHeader>
<bodyText confidence="0.999988769230769">
The correctness of the parser has been proven and
is fully reported in Schabes and Joshi (1988). It
consists of the proof of the invariant given in sec-
tion 3.2. Our proof is similar in its concept to the
proof of the correctness of Earley&apos;s parser given in
Aho and Ullman 1973. The &amp;quot;only if&amp;quot; part of the
invariant is proved by induction on the number of
states that have been added so far to all states sets.
The &amp;quot;if&amp;quot; part is proved by induction on a defined
rank of a state. The soundness (the algorithm rec-
ognizes only valid strings) and the completeness (if
a string is valid, then the algorithm will recognize
it) are corollaries of this invariant.
</bodyText>
<subsectionHeader confidence="0.985029">
3.8 Implementation
</subsectionHeader>
<bodyText confidence="0.999946833333333">
The parser has been implemented on Symbolics
Lisp machines in Flavors. More details of the
actual implementation can be found in Schabes
and Joshi (1988). The current implementation
has an 0(102n9) worst case time complexity and
0(IGIn6) worst case space complexity. We have
not as yet been able to reduce the worst case time
complexity to 0(lG12n6). We are currently at-
tempting to reduce this bound. However, the main
purpose of constructing an Earley-type parser is to
improve the average complexity, which is crucial in
practice.
</bodyText>
<sectionHeader confidence="0.996835" genericHeader="method">
4 Extensions
</sectionHeader>
<bodyText confidence="0.9999485">
We describe how substitution is defined in a TAG.
We discuss the consequences of introducing substi-
tution in TAGs. Then we show how substitution
can be parsed. We extend the parser to deal with
feature structures for TAGs. Finally the relation-
ship with PATR-II is discussed.
</bodyText>
<subsectionHeader confidence="0.996056">
4.1 Introducing substitution in
TAGs
</subsectionHeader>
<bodyText confidence="0.9906695">
TAGs use adjunction as their basic composition
operation. It is well known that Tree Adjoining
Languages (TALs) are mildly context-sensitive.
TALs properly contain context-free languages. It
is also possible to encode a context-free grammar
with auxiliary trees using adjunction only. How-
ever, although the languages correspond, the pos-
sible encoding does not reflect directly the original
Step 2
add the and merlon to each initial atm
</bodyText>
<page confidence="0.996139">
265
</page>
<table confidence="0.984929466666667">
S0 a,0, left, above, 0, - , -, -, -, -] (left predictor) [P, 0, le f t, above, 0, -, - - -, -] (left predictor)
a,0, left, below, 0, -, -, -, -, -I (move dot down) [p, 0, left, below, 0, -, - - -, -] (move dot down)
(3 ,l, left, above, 0, -, -, -, -, -1 (scanner) [a, left,1, above, 0, -, - - -, -] (scanner),
51 p,1, right, above, 0, -, -, -, -, -] (move dot up) 4/3, 2, left, above, 0, -, - - -, -] (left predictor)
/3, 2, left, below, 0, -, -, -, -, -3 (move dot down) [0, 0, left, above,l, - , - - -, -] (left predictor)
0, 2.1, left, above, 0, -, -, -, -, -] (scanner) [p, 0, left, below,l, -, - - -, -] (move dot down)
/3, 1, left, above,1,-, -, -, -, -] (scanner)
S2 S, 0, left, above, 2, -, -, -., -, -] (left predictor) (-0, 2.1, left, above,l, -, -, -, -, -] (scanner)
0, 2, left, below,1,-, -,-, -, -] (move dot down) [0, left,1, above, 2, - , - - -, -] (scanner)
p, 0, left, below, 2, -, -, -, -, -] (move dot down) [p, 2, le f t,above,1,-, - - -, -] (left predictor)
0, 1, right, above,1,- , -,-, -, -] move dot up)
S3 /3, 2.2, left, below,l, 3, -, -, -, -] left completor) [0, 2, lef t, below, 0, -, -, 2, 1, 3] (move dot down)
0, 2.1, right, above,l, -,-, -, -,- (move dot up) [0, 2.2, left, above,l, -, -,-, -, -1 (left predictor)
(3, 2.1, left, above, 0, -, -, 2,1 31 (scanner)
54 a,1,1ef t, above, 0, -, -, 0,0, 4] (scanner) (0,0, left, below, 0, -, -, 0, 0, 4] (move dot down)
p, 2.2, left, above, 0, -, -, 2, 1, 3] (left predictor) (/3, 2.1, right, above, 0, -, -, 2, 1, 3] (move dot up)
p, 2.2, le/ t, belotu , 0, 4, -,2, 1,3J (left completor)
53 S, 2.3, left, above, 0, 4, 5, 2, 1, 3] (scanner) [0, 2.2, right, below, 0, 4, 5, 2, 1, 3] (right predictor, case 2)
0, 2.2, right, above, 0, 4, 5, 2, 1,3] (move dot up) (a, 0, right, below, 0,-, -, 0, 0, 4) (right predictor, case 1)
a, 1, right, above, 0, -, -, 0, 0, 4] (move dot up)
56 p, 2.2, right, above,1,3, 6, -, -, -1 (move dot up) [p, 2.3, right, above, 0, 4, 5, 2, 1,3] (move dot up)
p, 2.3, left, above,l, 3, 6, -, -, -j (scanner) [fi, 2, right, below, 0, 4, 5, 2, 1, 3] (right predictor, case 1)
[p, 2.2,,right, below, 1, 3, 6, -, - , -] (right predictor, case 2)
57 [0, 2, right, belotu, 1, 3, 6, -, -, -] (right predictor, case 2) [0, 2, right, above, 1, 3, 6, -, -, -] (move dot up)
0, 3, left, above, 1, 3, 6, -, -, -1 (scanner) [123., 2.3,,right, above, 1, 3, 6, -, -,_-] (move dot up)
Ss 0,0, rsght, below, 1, 3, 6, -, -, -1 (right predictor, case 2) [0, 0, right, above, 1, 3, 6, -, -, -] (right completor)
/3, 3, left, above, 0, 4, 5, -, -, -] (scanner) [/3, 3, right, above, 1, 3, 6, -, -, -] (move dot up)
p, 2, right, above, 0, 4, 5, -, -, -] (move dot up)
Sg p, 0, right , below, 0, 4, 5, -, -, -] (right predictor, case 2) [ot, 0, right, above,0,- , - ,-, -, -] (end test)
[fi, 0, right, above, 0, 4, 5, -, -, -] (right completor) [0, 3, right, above, 0, 4, 5, - , -, -] (move dot up)
</table>
<figureCaption confidence="0.663596">
Figure 14: States sets for the input aabbeccdd
</figureCaption>
<figure confidence="0.484176">
A A VP NP
/ NP VPv V NPv det N
</figure>
<figureCaption confidence="0.943219">
Figure 16: Writing a CFG in TAG
Figure 15: Mechanism of substitution
</figureCaption>
<bodyText confidence="0.984714621621622">
context free grammar since this encoding uses ad-
junction.
Substitution is the basic operation used in CFG.
A CFG can be viewed as a tree rewriting system.
It uses substitution as basic operation and it con-
sists of a set of one-level trees. Substitution is a
less powerful operation than adjunction.
However, recent linguistic work in TAG gram-
mar development (Abeille, 1988) showed the need
for substitution in TAGs as an additional opera-
tion for obtaining appropriate structural descrip-
tions in certain cases such as verbs taking two sen-
tential arguments (e.g. &amp;quot;John equates solving this
problem with doing the impossible&amp;quot;) or compound
categories. It has also been shown to be useful
for lexical insertion (Schabes, Abeille and Joshi,
1988). It should be emphasized that the intro-
duction of substitution in TAGs does not increase
their generative capacity. Neither is it a step back
from the original idea of TAGs.
Definition 6 (Substitution in TAG) We de-
fine substitution in TAGs to take place on specified
nodes on the frontiers of elementary trees. When
a node is marked to be substituted, no adjunction
can take place on that node. Furthermore, sub-
stitution is always mandatory. Only trees derived
from initial trees rooted by a node of the same la-
bel can be substituted on a substitution node. The
resulting tree is obtained by replacing the node by
the tree derived from the initial tree. Substitution
is illustrated in Figure 15.
We conventionally mark substitution nodes by
a down arrow (1).
As a consequence, we can now encode directly
a CFG in a TAG with substitution. The resulting
TAG has only one-level initial trees and uses only
substitution. An example is shown in Figure 16.
</bodyText>
<subsectionHeader confidence="0.997395">
4.2 Parsing substitution
</subsectionHeader>
<bodyText confidence="0.999942666666667">
The parser can be extended very easily to handle
substitution. We use Earley&apos;s original predictor
and completor to handle substitution.
</bodyText>
<page confidence="0.997115">
266
</page>
<figureCaption confidence="0.99465">
Figure 17: Substitution Predictor
</figureCaption>
<bodyText confidence="0.9879476875">
The left predictor is restricted to apply to nodes
to which adjunction can be applied.
A flag subst? is added to the states. When set,
it indicates that the tree (initial) has been pre-
dicted for substitution. We use the index 1 (as
in Earley&apos;s original parser) to know where it has
been predicted for substitution. When the initial
tree that has been predicted for substitution has
been totally recognized, we complete the state as
Earley&apos;s original parser does.
A state $ is now an 11—tuple
[a, dot, side, pos , ,f,, f,., star, t;&apos; ,
where subst? is a boolean that indicates whether
the tree has been predicted for substitution. The
other components have not been changed.
We add two more processors to the parser.
</bodyText>
<subsectionHeader confidence="0.974137">
Substitution Predictor
</subsectionHeader>
<bodyText confidence="0.884199142857143">
Suppose that there is a dot to the left of and above
a non-terminal symbol on the frontier A that is
marked for substitution (see Figure 17). Then the
algorithm predicts for substitution all initial trees
rooted by A and tries to recognize the initial tree.
This operation is performed by the substitution
predictor.
It applies to
s =[a, dot, left, above, 1, fj, f, star, t7 ,b7 , subst?]
such that a(dot) is a non-terminal on the
frontier of a which is marked for
substitution:
It adds the states
f[8, o, le ft, above, i, „ „ ,true]
</bodyText>
<figure confidence="0.5518795">
1# is an initial tree s.t./3(0) = a(dot)}
to Si .
</figure>
<subsectionHeader confidence="0.854763">
Substitution Cornpletor
</subsectionHeader>
<bodyText confidence="0.998499333333333">
Suppose that the initial tree that we predicted for
substitution has been recognized (see Figure 18).
Then the algorithm should try to recognize the
rest of the tree in which we predicted a substitu-
tion. This operation is performed by the substi-
tution completor.
</bodyText>
<figure confidence="0.861353">
[1&apos;,fl&apos;,fe,t1.0,61*&apos;,substr]
</figure>
<figureCaption confidence="0.990267">
Figure 18: Substitution completor
</figureCaption>
<figure confidence="0.525591666666667">
It applies to
s = [a,O, right, above , I „ „ „true]
For all states s =
</figure>
<bodyText confidence="0.8518928">
[/ 3, dot&apos; , le ft, above, l&apos; , f, f;, star&apos; ,t7&apos; , br , subsel
in Si s.t. Adoti) is marked for
substitution and O(dot) = a(0) .
It adds the following state to Si:
[j3, dot&apos;, right, above, 1&apos;, f; , f,&apos;., star&apos;, tr , br, subst?&apos;] .
</bodyText>
<subsectionHeader confidence="0.605709">
Complexity
</subsectionHeader>
<bodyText confidence="0.997981692307692">
The introduction of the substitution predictor and
the substitution completor does not increase the
complexity of the overall TAG parser.
If we encode a CFG with substitution in TAG,
the parser behaves in 0(IGI2n3) worst case time
and 0(IGIn2) worst case space like Earley&apos;s orig-
inal parser. This comes from the fact that when
there are no auxiliary trees and when only substi-
tution is used, the indices f, t7, b7 of a state
will never be set. The algorithm will use only the
substitution predictor and the substitution corn-
pletor. Thus, it behaves exactly like Earley&apos;s orig-
inal parser on CFGs.
</bodyText>
<subsectionHeader confidence="0.989248">
4.3 Parsing feature structures for
TAGs
</subsectionHeader>
<bodyText confidence="0.9988479">
The definition of feature structures for TAGs and
their semantics was proposed by Vijay-Shanker
(1987) and Vijay-Shanker and Joshi (1988). We
first explain briefly how they work in TAGs and
show. how we have implemented them. We in-
troduce in a TAG framework a language simi-
lar to PATR-II which was investigated by Shieber
(Shieber, 1984 and 1986). We then show how one
can embed the essential aspects of PATR-II in this
system.
</bodyText>
<figure confidence="0.962501333333333">
*A1
(I, fl , fr ti* , bl*,subst?)
in Si A added to Si
bah
.A+
[1&apos;,f1&apos;,fr.,t1*&apos;,b1*&apos;,subst?&apos;]
/\
.
gala to Si
</figure>
<page confidence="0.757291">
267
</page>
<figureCaption confidence="0.97137">
Figure 19: Updating of features
</figureCaption>
<figure confidence="0.704829333333333">
NP VP (a)
PRO V PP
to go to the movies
S.top:xtenseth +
S.bottom::&lt;tensed&gt; V.bottom:xtensed&gt;
V.bottom::&lt;tensed&gt; =-
</figure>
<bodyText confidence="0.983804388888889">
Feature structures in TAGs
As defined by Vijay-Shanker (1987) and Vijay-
Shanker and Joshi(1988), to each adjunction node
in an elementary tree two feature structures are at-
tached: a top and a bottom feature structure. The
top feature corresponds to a top view in the tree
from the node. The bottom feature corresponds
to the bottom view. When the derivation is com-
pleted, the top and bottom features of all nodes
are unified. If the top and bottom features of a
node do not unify, then a tree must be adjoined
at that node.
This definition can be trivially extended to sub-
stitution nodes. To each substitution node we at-
tach two identical feature structures (top and bot-
tom).
The updating of features in case of adjunction
is shown in Figure 19.
</bodyText>
<subsectionHeader confidence="0.624752">
Unification equations
</subsectionHeader>
<bodyText confidence="0.9496842">
As in PATR-II, we express with unification equa-
tions dependencies between DAGs in an elemen-
tary tree. The system therefore consists of a TAG
and a set of unification equations on the DAGs
associated with nodes in elementary trees.
An example of the use of unification equations
in TAGs is given in Figure 20. Note that the top
and bottom features of node S in a can not be uni-
fied. This forces an adjunction to be performed on
S. Thus, the following sentence is not accepted:
*to go to the movies.
The auxiliary tree $1 can be adjoined at S in a:
John wants to go to the movies.
But since the bottom feature of S has tensed value
— in a and since the bottom feature of S has
tensed value + in 02, 01 can not be adjoined at
node S in a:
*Bob thinks to go to the movies.
But 132 can be adjoined in #1, which itself can be
adjoined in a:
</bodyText>
<figure confidence="0.942298722222222">
Bob thinks John wants to go to the
NP VP mo
A / \
John V S1
wants
S.top::&lt;tensed&gt;
S.bottom::ctensed • V.bonom::&lt;tensed&gt;
Si.bottom::&lt;tensed&gt; V.bottom::&lt;tensed-S1&gt;
V.banom::&lt;tensed-St&gt; -
V.bottom::&lt;tensed&gt; +
IAP /V\ (i32)
Bob V s,
thinks
S.top:xtensed&gt; • +
S.bottom:xtensed&gt; V.bottomutensed,
S J.bottom:xtensed&gt; V.bottom:xtensed-S1&gt;
V.bonom:xtensed-St&gt; . +
V.bonom:Aensed&gt; . +
</figure>
<figureCaption confidence="0.9388595">
Figure 20: Example of unification equations
movies.
</figureCaption>
<bodyText confidence="0.9996335">
We refer the reader to Abeille (1988) and to
Schabes, Abeille and Josh i (1988) for further ex-
planation of the use of unification equations and
substitution in TAGs.
</bodyText>
<page confidence="0.988908">
268
</page>
<bodyText confidence="0.999645272727273">
Parsing and the relationship with PATR-II
By adding to each state the set of DAGs cor-
responding to the top and bottom features of
each node, and by making sure that the unifica-
tion equations are satisfied, we have extended the
parser to parse TAGs with feature structures.
Since we introduced substitution and since we
are able to encode a CFG directly, the system
has the main functionalities of PATR-II. The sys-
tem parses unification formalisms that have a CFG
skeleton and a TAG skeleton.
</bodyText>
<sectionHeader confidence="0.999198" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999989333333333">
We described an Earley-type parser for TAGs. We
extended it to deal with substitution and feature
structures for TAGs. By doing this, we have built
a system that parses unification formalisms that
have a CFG skeleton and also those that have a
TAG skeleton. The system is being used for Tree
Adjoining Grammar development (Aherne, 1988).
This work has led us to a new general parsing
strategy (Schabes, Aherne and Joshi, 1988) which
allows us to construct a two-stage parser. In the
first stage a subset of the elementary trees is ex-
tracted and in the second stage the sentence is
parsed with respect to this subset. This strategy
significantly improves performance, especially as
the grammar size increases.
</bodyText>
<sectionHeader confidence="0.996446" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999833235294117">
Abellle, Anne, 1988. A Computational Grammar for
French in TAG. In Proceeding of the 12th International
Conference on Computational Linguistics.
Aho, A. V. and Ullman, J. D., 1973. Theory of
Parsing, Translation and Compiling. Vol 1: Parsing.
Prentice-Hall, Englewood Cliffs, NJ.
Earley, J., 1970. An Efficient Context-Free Parsing
Algorithm. Commun. ACM 13(2):94-102.
Joshi, Aravind K., 1985. How Much Context-
Sensitivity is Necessary for Characterizing Structural
Descriptions — Tree Adjoining Grammars. In Dowty,
D.; Karttunen, L.; and Zwicky, A. (editors), Natural
Language Processing — Theoretical, Computational
and Psychological Perspectives. Cambridge University
Press, New York. Originally presented in 1983.
Joshi, Aravind K., 1987. An Introduction to Tree Ad-
joining Grammars. In Manaster-Ramer, A. (editor),
Mathematics of Language. John Benjamins, Amster-
dam.
Joshi, A. K.; Levy, L. S.; and Takahashi, M., 1975.
Tree Adjunct Grammars. J. Comput. Syst. Sci. 10(1).
Kroch, A. and Joshi, A. K., 1985. Linguistic Relevance
of Tree Adjoining Grammars. Technical Report MS-
CIS-85-18, Department of Computer and Information
Science, University of Pennsylvania.
Schabes, Yves and Joshi, Aravind K., 1988. An
Earley-type Parser for Tree Adjoining Grammars.
Technical Report, Department of Computer and In-
formation Science, University of Pennsylvania.
Schabes, Yves; Abeillé, Anne; and Joshi, Aravind K.,
1988. New Parsing Strategies for Tree Adjoining
Grammars. In Proceedings of the 12th International
Conference on Computational Linguistics.
Shieber, Stuart M., 1984. The Design of a Computer
Language for Linguistic Information. In 22&amp;quot; Meet-
ing of the Association for Computational Linguistics,
pages 362-366.
Shieber, Stuart M., 1986. An Introduction to Unifi-
cation-Based Approaches to Grammar. Center for the
Study of Language and Information, Stanford, CA.
Vijay-Shanker, K., 1987. A Study of Tree Adjoining
Grammars. PhD thesis, Department of Computer and
Information Science, University of Pennsylvania.
Vijay-Shanker, K. and Joshi, A. K., 1985. Some Com-
putational Properties of Tree Adjoining Grammars.
In 23&amp;quot; Meeting of the Association for Computational
Linguistics, pages 82-93.
Vijay-Shanker, K. and Joshi, A.K., 1988. Feature
Structure Based Tree Adjoining Grammars. In Pro-
ceedings of then&apos; International Conference on Com-
putational Linguistics.
</reference>
<page confidence="0.998574">
269
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.893725">
<title confidence="0.9988805">AN EARLEY-TYPE PARSING ALGORITHM FOR TREE ADJOINING GRAMMARS *</title>
<author confidence="0.99999">Yves Schabes</author>
<author confidence="0.99999">Aravind K Joshi</author>
<affiliation confidence="0.9998955">Department of Computer and Information Science University of Pennsylvania</affiliation>
<address confidence="0.936541">Philadelphia PA 19104-6389 USA</address>
<email confidence="0.999413">schabesalinc.cis.upenn.edujoshiOcis.upenn.edu</email>
<abstract confidence="0.997555777777778">We will describe an Earley-type parser for Tree Adjoining Grammars (TAGs). Although a CKYtype parser for TAGs has been developed earlier (Vijay-Shanker and Joshi, 1985), this is the first practical parser for TAGs because as is well known for CFGs, the average behavior of Earley-type parsers is superior to that of CKY-type parsers. The core of the algorithm is described. Then we discuss modifications of the parsing algorithm that can parse extensions of TAGs such as constraints on adjunction, substitution, and feature structures for TAGs. We show how with the use of substitution in TAGs the system is able to parse directly CFGs and TAGs. The system parses unification formalisms that have a CFG skeleton and also those with a TAG skeleton. Thus it also allows us to embed the essential aspects of PATR-II.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Anne Abellle</author>
</authors>
<title>A Computational Grammar for French in TAG.</title>
<date>1988</date>
<booktitle>In Proceeding of the 12th International Conference on Computational Linguistics.</booktitle>
<marker>Abellle, 1988</marker>
<rawString>Abellle, Anne, 1988. A Computational Grammar for French in TAG. In Proceeding of the 12th International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A V Aho</author>
<author>J D Ullman</author>
</authors>
<date>1973</date>
<booktitle>Theory of Parsing, Translation and Compiling. Vol 1: Parsing. Prentice-Hall,</booktitle>
<location>Englewood Cliffs, NJ.</location>
<contexts>
<context position="5631" citStr="Aho and Ullman, 1973" startWordPosition="968" endWordPosition="971">lated from the tree they belong to, it isolates them from their domain of locality. However all important linguistic and computational properties of TAGs follow from this extended domain of locality. And most importantly, although it runs in 0(n6) worst time, it also runs in 0(n6) best time. As a consequence, the CKY algorithm is in practice very slow. Since the average time complexity of Earley&apos;s parser depends on the grammar and in practice runs much better than its worst time complexity, we decided to try to adapt Earley&apos;s parser for CFGs to TAGs. Earley&apos;s algorithm for CFGs (Earley, 1970, Aho and Ullman, 1973) is a bottomup parser which uses top-down information. It manipulates states of the form A --• a./3{i] while using three processors: the predictor, the completor and the scanner. The algorithm for CFGs runs in 0(IGI2n3) time and in 0(IGIn2) space in all cases, and parses unambiguous grammars in 0(n2) time (n being the length of the input, ClI the size of the grammar). Given a context-free grammar in any form and an input string al • • • an, Earley&apos;s parser for CFGs maintains the following invariant: The state A —• a.#[i] is in states set Sk if S 6A7, al • -a; and a ai+i • • ak The correctness </context>
<context position="23715" citStr="Aho and Ullman 1973" startWordPosition="4537" endWordPosition="4540">rker to all initial trees in the grammar (see Figure 13). Extracting a parse The algorithm that we describe in section 3.3 is a recognizer. However, if we include pointers from a state to the other states which caused it to be placed in the states set, the recognizer can be modified to produce all parses of the input string. 3.7 Correctness The correctness of the parser has been proven and is fully reported in Schabes and Joshi (1988). It consists of the proof of the invariant given in section 3.2. Our proof is similar in its concept to the proof of the correctness of Earley&apos;s parser given in Aho and Ullman 1973. The &amp;quot;only if&amp;quot; part of the invariant is proved by induction on the number of states that have been added so far to all states sets. The &amp;quot;if&amp;quot; part is proved by induction on a defined rank of a state. The soundness (the algorithm recognizes only valid strings) and the completeness (if a string is valid, then the algorithm will recognize it) are corollaries of this invariant. 3.8 Implementation The parser has been implemented on Symbolics Lisp machines in Flavors. More details of the actual implementation can be found in Schabes and Joshi (1988). The current implementation has an 0(102n9) worst </context>
</contexts>
<marker>Aho, Ullman, 1973</marker>
<rawString>Aho, A. V. and Ullman, J. D., 1973. Theory of Parsing, Translation and Compiling. Vol 1: Parsing. Prentice-Hall, Englewood Cliffs, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Earley</author>
</authors>
<title>An Efficient Context-Free Parsing Algorithm.</title>
<date>1970</date>
<journal>Commun. ACM</journal>
<pages>13--2</pages>
<contexts>
<context position="5608" citStr="Earley, 1970" startWordPosition="966" endWordPosition="967"> have been isolated from the tree they belong to, it isolates them from their domain of locality. However all important linguistic and computational properties of TAGs follow from this extended domain of locality. And most importantly, although it runs in 0(n6) worst time, it also runs in 0(n6) best time. As a consequence, the CKY algorithm is in practice very slow. Since the average time complexity of Earley&apos;s parser depends on the grammar and in practice runs much better than its worst time complexity, we decided to try to adapt Earley&apos;s parser for CFGs to TAGs. Earley&apos;s algorithm for CFGs (Earley, 1970, Aho and Ullman, 1973) is a bottomup parser which uses top-down information. It manipulates states of the form A --• a./3{i] while using three processors: the predictor, the completor and the scanner. The algorithm for CFGs runs in 0(IGI2n3) time and in 0(IGIn2) space in all cases, and parses unambiguous grammars in 0(n2) time (n being the length of the input, ClI the size of the grammar). Given a context-free grammar in any form and an input string al • • • an, Earley&apos;s parser for CFGs maintains the following invariant: The state A —• a.#[i] is in states set Sk if S 6A7, al • -a; and a ai+i </context>
</contexts>
<marker>Earley, 1970</marker>
<rawString>Earley, J., 1970. An Efficient Context-Free Parsing Algorithm. Commun. ACM 13(2):94-102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
</authors>
<title>How Much ContextSensitivity is Necessary for Characterizing Structural Descriptions — Tree Adjoining Grammars.</title>
<date>1985</date>
<booktitle>Natural Language Processing — Theoretical, Computational and Psychological Perspectives.</booktitle>
<editor>In Dowty, D.; Karttunen, L.; and Zwicky, A. (editors),</editor>
<publisher>Cambridge University Press,</publisher>
<location>New York.</location>
<note>Originally presented in</note>
<contexts>
<context position="1288" citStr="Joshi, 1985" startWordPosition="198" endWordPosition="199"> algorithm that can parse extensions of TAGs such as constraints on adjunction, substitution, and feature structures for TAGs. We show how with the use of substitution in TAGs the system is able to parse directly CFGs and TAGs. The system parses unification formalisms that have a CFG skeleton and also those with a TAG skeleton. Thus it also allows us to embed the essential aspects of PATR-II. 1 Introduction Although formal properties of Tree Adjoining Grammars (TAGs) have been investigated (VijayShanker, 1987)—for example, there is an 0(n6)- time CKY-like algorithm for TAGs (Vijay-Shanker and Joshi, 1985)—so far there has been no attempt to develop an Earley-type parser for TAGs. This paper presents an Earley parser for TAGs and discusses modifications to the parsing algorithm that make it possible to handle extensions of TAGs such as constraints on adjunction, sub*This work is partially supported by ARO grant DAA29-84-9-007, DARPA grant N0014-85-K0018, NSF grants MCS-82-191169 and DCR-84-10413. The authors would like to express their gratitude to Vijay-Shanlcer for his helpful comments relating to the core of the algorithm, Richard Billington and Andrew Chalnicic for their graphical TAG edito</context>
<context position="4520" citStr="Joshi (1985)" startWordPosition="784" endWordPosition="785"> root node is identified with n. • The sub-tree t is attached to the foot node of # and the root node n oft is identified with the foot node of #. Figure 2: The mechanism of adjunction Then define the tree set of a TAG G, T(G) to be the set of all derived trees starting from initial trees in I. Furthermore, the string language generated by a TAG, L(G), is defined to be the set of all terminal strings of the trees in T(G). TAGs factor recursion and dependencies by extending the domain of locality. They offer novel ways to encode the syntax of natural language grammars as discussed in Kroch and Joshi (1985) and Abeille (1988). In 1985, Vijay-Shanker and Joshi introduced a CKY-like algorithm for TAGs. They therefore established 0(n6) time as an upper bound for parsing TAGs. The algorithm was implemented, but in our opinion the result was more theoretical than practical for several reasons. First the algorithm assumes that elementary trees are binary branching and that there are no empty categories on the frontiers of the elementary trees. Second, since it works on nodes that have been isolated from the tree they belong to, it isolates them from their domain of locality. However all important ling</context>
</contexts>
<marker>Joshi, 1985</marker>
<rawString>Joshi, Aravind K., 1985. How Much ContextSensitivity is Necessary for Characterizing Structural Descriptions — Tree Adjoining Grammars. In Dowty, D.; Karttunen, L.; and Zwicky, A. (editors), Natural Language Processing — Theoretical, Computational and Psychological Perspectives. Cambridge University Press, New York. Originally presented in 1983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
</authors>
<title>An Introduction to Tree Adjoining Grammars.</title>
<date>1987</date>
<booktitle>Mathematics of Language. John Benjamins,</booktitle>
<editor>In Manaster-Ramer, A. (editor),</editor>
<location>Amsterdam.</location>
<contexts>
<context position="21211" citStr="Joshi, 1987" startWordPosition="4060" endWordPosition="4061">tor. inSI: [1&apos;,fr,fe,t1•&apos;,b1•1 Figure 11: Right Completor It applies to 8= [a, 0, right, above,l, ft, For all states 81 = ffi , dot&apos; , le ft, above,1&apos; , , f;., star&apos;, tr , br] in Si and for all states [ft, dot&apos; ,rig ht, below, l&apos; ,f(, dot&apos; ,1, in S „ such that a E Adjunct(f3 , dot&apos;) It adds f./3, dot&apos;, right, above, i&apos;,1,&apos;,7, star&apos; ,tr ,b71 to . Where 7 = f, if f is bound in state sr, and 7 can have any value, if f is unbound in state sr. 3.4 Handling constraints on adjunction In a TAG, one can, for each node of an elementary tree, specify one of the following three constraints on adjunction (Joshi, 1987): • Null adjunction (NA): disallow any adjunction on the given node. • Obligatory adjunction (OA): an auxiliary tree must be adjoined on the given node. • Selective adjunction (SA(T)): a set T of auxiliary trees that can be adjoined on the given node is specified. The algorithm can be very easily modified to handle those constraints. First, the function Adjunct(a , address) must be modified as follows: • Adjunct(a, address) = 0, if there is NA on the node.. • Adjunct(a , address) as previously defined, if there is OA on the node. • Adjunct(a , address) = T, if there is SA(T) on the node. Secon</context>
</contexts>
<marker>Joshi, 1987</marker>
<rawString>Joshi, Aravind K., 1987. An Introduction to Tree Adjoining Grammars. In Manaster-Ramer, A. (editor), Mathematics of Language. John Benjamins, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K Joshi</author>
<author>L S Levy</author>
<author>M Takahashi</author>
</authors>
<date>1975</date>
<journal>Tree Adjunct Grammars. J. Comput. Syst. Sci.</journal>
<volume>10</volume>
<issue>1</issue>
<marker>Joshi, Levy, Takahashi, 1975</marker>
<rawString>Joshi, A. K.; Levy, L. S.; and Takahashi, M., 1975. Tree Adjunct Grammars. J. Comput. Syst. Sci. 10(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kroch</author>
<author>A K Joshi</author>
</authors>
<title>Linguistic Relevance of Tree Adjoining Grammars.</title>
<date>1985</date>
<tech>Technical Report MSCIS-85-18,</tech>
<institution>Department of Computer and Information Science, University of Pennsylvania.</institution>
<contexts>
<context position="2287" citStr="Kroch and Joshi (1985)" startWordPosition="355" endWordPosition="358">169 and DCR-84-10413. The authors would like to express their gratitude to Vijay-Shanlcer for his helpful comments relating to the core of the algorithm, Richard Billington and Andrew Chalnicic for their graphical TAG editor which we integrated in our system and for their programming advice Thanks are also due to Anne Abeille and Ellen Hays. stitution, and feature structure representation for TAGs. TAGs were first introduced by Joshi, Levy and Takahashi (1975) and Joshi (1983). We describe very briefly the Tree Adjoining Grammar formalism. For more details we refer the reader to Joshi (1983), Kroch and Joshi (1985) or Vijay-Shanker (1987). Definition 1 (Tree Adjoining Grammar) : A TAG is a 5-tuple G = (VN VT S I , A) where VN is a finite set of non-terminal symbols, VT is a finite set of terminals, S is a distinguished nonterminal, I is a finite set of trees called initial trees and A is a finite set of trees called auxiliary trees. The trees in / U A are called elementary trees. Initial trees (see left tree in Figure 1) are characterized as follows: internal nodes are labeled by non-terminals; leaf nodes are labeled by either terminal symbols or the empty string. Z X ts&amp;quot;..termlnels terminals -.of **•—.</context>
<context position="4520" citStr="Kroch and Joshi (1985)" startWordPosition="782" endWordPosition="785"> n and its root node is identified with n. • The sub-tree t is attached to the foot node of # and the root node n oft is identified with the foot node of #. Figure 2: The mechanism of adjunction Then define the tree set of a TAG G, T(G) to be the set of all derived trees starting from initial trees in I. Furthermore, the string language generated by a TAG, L(G), is defined to be the set of all terminal strings of the trees in T(G). TAGs factor recursion and dependencies by extending the domain of locality. They offer novel ways to encode the syntax of natural language grammars as discussed in Kroch and Joshi (1985) and Abeille (1988). In 1985, Vijay-Shanker and Joshi introduced a CKY-like algorithm for TAGs. They therefore established 0(n6) time as an upper bound for parsing TAGs. The algorithm was implemented, but in our opinion the result was more theoretical than practical for several reasons. First the algorithm assumes that elementary trees are binary branching and that there are no empty categories on the frontiers of the elementary trees. Second, since it works on nodes that have been isolated from the tree they belong to, it isolates them from their domain of locality. However all important ling</context>
</contexts>
<marker>Kroch, Joshi, 1985</marker>
<rawString>Kroch, A. and Joshi, A. K., 1985. Linguistic Relevance of Tree Adjoining Grammars. Technical Report MSCIS-85-18, Department of Computer and Information Science, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Schabes</author>
<author>Aravind K Joshi</author>
</authors>
<title>An Earley-type Parser for Tree Adjoining Grammars.</title>
<date>1988</date>
<tech>Technical Report,</tech>
<institution>Department of Computer and Information Science, University of Pennsylvania.</institution>
<contexts>
<context position="23534" citStr="Schabes and Joshi (1988)" startWordPosition="4502" endWordPosition="4505"> we chose to use these two processors in this paper. Off-line vs on-line The algorithm given is an off-line recognizer. It can be very easily modified to work on line by adding an end marker to all initial trees in the grammar (see Figure 13). Extracting a parse The algorithm that we describe in section 3.3 is a recognizer. However, if we include pointers from a state to the other states which caused it to be placed in the states set, the recognizer can be modified to produce all parses of the input string. 3.7 Correctness The correctness of the parser has been proven and is fully reported in Schabes and Joshi (1988). It consists of the proof of the invariant given in section 3.2. Our proof is similar in its concept to the proof of the correctness of Earley&apos;s parser given in Aho and Ullman 1973. The &amp;quot;only if&amp;quot; part of the invariant is proved by induction on the number of states that have been added so far to all states sets. The &amp;quot;if&amp;quot; part is proved by induction on a defined rank of a state. The soundness (the algorithm recognizes only valid strings) and the completeness (if a string is valid, then the algorithm will recognize it) are corollaries of this invariant. 3.8 Implementation The parser has been imp</context>
</contexts>
<marker>Schabes, Joshi, 1988</marker>
<rawString>Schabes, Yves and Joshi, Aravind K., 1988. An Earley-type Parser for Tree Adjoining Grammars. Technical Report, Department of Computer and Information Science, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Schabes</author>
<author>Anne Abeillé</author>
<author>Aravind K Joshi</author>
</authors>
<title>New Parsing Strategies for Tree Adjoining Grammars.</title>
<date>1988</date>
<booktitle>In Proceedings of the 12th International Conference on Computational Linguistics.</booktitle>
<marker>Schabes, Abeillé, Joshi, 1988</marker>
<rawString>Schabes, Yves; Abeillé, Anne; and Joshi, Aravind K., 1988. New Parsing Strategies for Tree Adjoining Grammars. In Proceedings of the 12th International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
</authors>
<title>The Design of a Computer Language for Linguistic Information. In 22&amp;quot; Meeting of the Association for Computational Linguistics,</title>
<date>1984</date>
<pages>362--366</pages>
<contexts>
<context position="33283" citStr="Shieber, 1984" startWordPosition="6322" endWordPosition="6323"> when only substitution is used, the indices f, t7, b7 of a state will never be set. The algorithm will use only the substitution predictor and the substitution cornpletor. Thus, it behaves exactly like Earley&apos;s original parser on CFGs. 4.3 Parsing feature structures for TAGs The definition of feature structures for TAGs and their semantics was proposed by Vijay-Shanker (1987) and Vijay-Shanker and Joshi (1988). We first explain briefly how they work in TAGs and show. how we have implemented them. We introduce in a TAG framework a language similar to PATR-II which was investigated by Shieber (Shieber, 1984 and 1986). We then show how one can embed the essential aspects of PATR-II in this system. *A1 (I, fl , fr ti* , bl*,subst?) in Si A added to Si bah .A+ [1&apos;,f1&apos;,fr.,t1*&apos;,b1*&apos;,subst?&apos;] /\ . gala to Si 267 Figure 19: Updating of features NP VP (a) PRO V PP to go to the movies S.top:xtenseth + S.bottom::&lt;tensed&gt; V.bottom:xtensed&gt; V.bottom::&lt;tensed&gt; =- Feature structures in TAGs As defined by Vijay-Shanker (1987) and VijayShanker and Joshi(1988), to each adjunction node in an elementary tree two feature structures are attached: a top and a bottom feature structure. The top feature corresponds to </context>
</contexts>
<marker>Shieber, 1984</marker>
<rawString>Shieber, Stuart M., 1984. The Design of a Computer Language for Linguistic Information. In 22&amp;quot; Meeting of the Association for Computational Linguistics, pages 362-366.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
</authors>
<title>An Introduction to Unification-Based Approaches to Grammar. Center for the Study of Language and Information,</title>
<date>1986</date>
<location>Stanford, CA.</location>
<marker>Shieber, 1986</marker>
<rawString>Shieber, Stuart M., 1986. An Introduction to Unification-Based Approaches to Grammar. Center for the Study of Language and Information, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
</authors>
<title>A Study of Tree Adjoining Grammars.</title>
<date>1987</date>
<tech>PhD thesis,</tech>
<institution>Department of Computer and Information Science, University of Pennsylvania.</institution>
<contexts>
<context position="2311" citStr="Vijay-Shanker (1987)" startWordPosition="360" endWordPosition="361">authors would like to express their gratitude to Vijay-Shanlcer for his helpful comments relating to the core of the algorithm, Richard Billington and Andrew Chalnicic for their graphical TAG editor which we integrated in our system and for their programming advice Thanks are also due to Anne Abeille and Ellen Hays. stitution, and feature structure representation for TAGs. TAGs were first introduced by Joshi, Levy and Takahashi (1975) and Joshi (1983). We describe very briefly the Tree Adjoining Grammar formalism. For more details we refer the reader to Joshi (1983), Kroch and Joshi (1985) or Vijay-Shanker (1987). Definition 1 (Tree Adjoining Grammar) : A TAG is a 5-tuple G = (VN VT S I , A) where VN is a finite set of non-terminal symbols, VT is a finite set of terminals, S is a distinguished nonterminal, I is a finite set of trees called initial trees and A is a finite set of trees called auxiliary trees. The trees in / U A are called elementary trees. Initial trees (see left tree in Figure 1) are characterized as follows: internal nodes are labeled by non-terminals; leaf nodes are labeled by either terminal symbols or the empty string. Z X ts&amp;quot;..termlnels terminals -.of **•—.torminals Figure 1: Sche</context>
<context position="33049" citStr="Vijay-Shanker (1987)" startWordPosition="6281" endWordPosition="6282">overall TAG parser. If we encode a CFG with substitution in TAG, the parser behaves in 0(IGI2n3) worst case time and 0(IGIn2) worst case space like Earley&apos;s original parser. This comes from the fact that when there are no auxiliary trees and when only substitution is used, the indices f, t7, b7 of a state will never be set. The algorithm will use only the substitution predictor and the substitution cornpletor. Thus, it behaves exactly like Earley&apos;s original parser on CFGs. 4.3 Parsing feature structures for TAGs The definition of feature structures for TAGs and their semantics was proposed by Vijay-Shanker (1987) and Vijay-Shanker and Joshi (1988). We first explain briefly how they work in TAGs and show. how we have implemented them. We introduce in a TAG framework a language similar to PATR-II which was investigated by Shieber (Shieber, 1984 and 1986). We then show how one can embed the essential aspects of PATR-II in this system. *A1 (I, fl , fr ti* , bl*,subst?) in Si A added to Si bah .A+ [1&apos;,f1&apos;,fr.,t1*&apos;,b1*&apos;,subst?&apos;] /\ . gala to Si 267 Figure 19: Updating of features NP VP (a) PRO V PP to go to the movies S.top:xtenseth + S.bottom::&lt;tensed&gt; V.bottom:xtensed&gt; V.bottom::&lt;tensed&gt; =- Feature struct</context>
</contexts>
<marker>Vijay-Shanker, 1987</marker>
<rawString>Vijay-Shanker, K., 1987. A Study of Tree Adjoining Grammars. PhD thesis, Department of Computer and Information Science, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
<author>A K Joshi</author>
</authors>
<title>Some Computational Properties of Tree Adjoining Grammars. In 23&amp;quot; Meeting of the Association for Computational Linguistics,</title>
<date>1985</date>
<pages>82--93</pages>
<contexts>
<context position="1288" citStr="Vijay-Shanker and Joshi, 1985" startWordPosition="196" endWordPosition="199">ons of the parsing algorithm that can parse extensions of TAGs such as constraints on adjunction, substitution, and feature structures for TAGs. We show how with the use of substitution in TAGs the system is able to parse directly CFGs and TAGs. The system parses unification formalisms that have a CFG skeleton and also those with a TAG skeleton. Thus it also allows us to embed the essential aspects of PATR-II. 1 Introduction Although formal properties of Tree Adjoining Grammars (TAGs) have been investigated (VijayShanker, 1987)—for example, there is an 0(n6)- time CKY-like algorithm for TAGs (Vijay-Shanker and Joshi, 1985)—so far there has been no attempt to develop an Earley-type parser for TAGs. This paper presents an Earley parser for TAGs and discusses modifications to the parsing algorithm that make it possible to handle extensions of TAGs such as constraints on adjunction, sub*This work is partially supported by ARO grant DAA29-84-9-007, DARPA grant N0014-85-K0018, NSF grants MCS-82-191169 and DCR-84-10413. The authors would like to express their gratitude to Vijay-Shanlcer for his helpful comments relating to the core of the algorithm, Richard Billington and Andrew Chalnicic for their graphical TAG edito</context>
</contexts>
<marker>Vijay-Shanker, Joshi, 1985</marker>
<rawString>Vijay-Shanker, K. and Joshi, A. K., 1985. Some Computational Properties of Tree Adjoining Grammars. In 23&amp;quot; Meeting of the Association for Computational Linguistics, pages 82-93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
<author>A K Joshi</author>
</authors>
<title>Feature Structure Based Tree Adjoining Grammars.</title>
<date>1988</date>
<booktitle>In Proceedings of then&apos; International Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="33084" citStr="Vijay-Shanker and Joshi (1988)" startWordPosition="6284" endWordPosition="6287"> encode a CFG with substitution in TAG, the parser behaves in 0(IGI2n3) worst case time and 0(IGIn2) worst case space like Earley&apos;s original parser. This comes from the fact that when there are no auxiliary trees and when only substitution is used, the indices f, t7, b7 of a state will never be set. The algorithm will use only the substitution predictor and the substitution cornpletor. Thus, it behaves exactly like Earley&apos;s original parser on CFGs. 4.3 Parsing feature structures for TAGs The definition of feature structures for TAGs and their semantics was proposed by Vijay-Shanker (1987) and Vijay-Shanker and Joshi (1988). We first explain briefly how they work in TAGs and show. how we have implemented them. We introduce in a TAG framework a language similar to PATR-II which was investigated by Shieber (Shieber, 1984 and 1986). We then show how one can embed the essential aspects of PATR-II in this system. *A1 (I, fl , fr ti* , bl*,subst?) in Si A added to Si bah .A+ [1&apos;,f1&apos;,fr.,t1*&apos;,b1*&apos;,subst?&apos;] /\ . gala to Si 267 Figure 19: Updating of features NP VP (a) PRO V PP to go to the movies S.top:xtenseth + S.bottom::&lt;tensed&gt; V.bottom:xtensed&gt; V.bottom::&lt;tensed&gt; =- Feature structures in TAGs As defined by Vijay-Sh</context>
</contexts>
<marker>Vijay-Shanker, Joshi, 1988</marker>
<rawString>Vijay-Shanker, K. and Joshi, A.K., 1988. Feature Structure Based Tree Adjoining Grammars. In Proceedings of then&apos; International Conference on Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>