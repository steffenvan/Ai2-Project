<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.985569">
Subtree Extractive Summarization via Submodular Maximization
</title>
<author confidence="0.996898">
Hajime Morita
</author>
<affiliation confidence="0.998981">
Tokyo Institute of Technology, Japan
</affiliation>
<email confidence="0.991444">
morita@lr.pi.titech.ac.jp
</email>
<author confidence="0.99768">
Hiroya Takamura
</author>
<affiliation confidence="0.999843">
Tokyo Institute of Technology, Japan
</affiliation>
<email confidence="0.995573">
takamura@pi.titech.ac.jp
</email>
<author confidence="0.986702">
Ryohei Sasano
</author>
<affiliation confidence="0.995755">
Tokyo Institute of Technology, Japan
</affiliation>
<email confidence="0.989816">
sasano@pi.titech.ac.jp
</email>
<author confidence="0.996934">
Manabu Okumura
</author>
<affiliation confidence="0.999826">
Tokyo Institute of Technology, Japan
</affiliation>
<email confidence="0.997888">
oku@pi.titech.ac.jp
</email>
<sectionHeader confidence="0.993888" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999951529411765">
This study proposes a text summarization
model that simultaneously performs sen-
tence extraction and compression. We
translate the text summarization task into
a problem of extracting a set of depen-
dency subtrees in the document cluster.
We also encode obligatory case constraints
as must-link dependency constraints in or-
der to guarantee the readability of the gen-
erated summary. In order to handle the
subtree extraction problem, we investigate
a new class of submodular maximization
problem, and a new algorithm that has
the approximation ratio 2(1 − e−1). Our
experiments with the NTCIR ACLIA test
collections show that our approach outper-
forms a state-of-the-art algorithm.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999983127272728">
Text summarization is often addressed as a task
of simultaneously performing sentence extraction
and sentence compression (Berg-Kirkpatrick et
al., 2011; Martins and Smith, 2009). Joint mod-
els of sentence extraction and compression have
a great benefit in that they have a large degree of
freedom as far as controlling redundancy goes. In
contrast, conventional two-stage approaches (Za-
jic et al., 2006), which first generate candidate
compressed sentences and then use them to gen-
erate a summary, have less computational com-
plexity than joint models. However, two-stage ap-
proaches are suboptimal for text summarization.
For example, when we compress sentences first,
the compressed sentences may fail to contain im-
portant pieces of information due to the length
limit imposed on each sentence. On the other
hand, when we extract sentences first, an impor-
tant sentence may fail to be selected, simply be-
cause it is long. Enumerating a huge number
of compressed sentences is also infeasible. Joint
models can prune unimportant or redundant de-
scriptions without resorting to enumeration.
Meanwhile, submodular maximization has re-
cently been applied to the text summarization task,
and the methods thereof have performed very well
(Lin and Bilmes, 2010; Lin and Bilmes, 2011;
Morita et al., 2011). Formalizing summarization
as a submodular maximization problem has an im-
portant benefit inthat the problem can be solved by
using a greedy algorithm with a performance guar-
antee.
We therefore decided to formalize the task of si-
multaneously performing sentence extraction and
compression as a submodular maximization prob-
lem. That is, we extract subsentences for mak-
ing the summary directly from all available sub-
sentences in the documents and not in a stepwise
fashion. However, there is a difficulty with such
a formalization. In the past, the resulting maxi-
mization problem has been often accompanied by
thousands of linear constraints representing logi-
cal relations between words. The existing greedy
algorithm for solving submodular maximization
problems cannot work in the presence of such nu-
merous constraints although monotone and non-
monotone submodular maximization with con-
straints other than budget constraints have been
studied (Lee et al., 2009; Kulik et al., 2009; Gupta
et al., 2010). In this study, we avoid this difficulty
by reducing the task to one of extracting depen-
dency subtrees from sentences in the source doc-
uments. The reduction replaces the difficulty of
numerous linear constraints with another difficulty
wherein two subtrees can share the same word to-
</bodyText>
<page confidence="0.830955">
1023
</page>
<note confidence="0.914307">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1023–1032,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999940869565217">
ken when they are selected from the same sen-
tence, and as a result, the cost of the union of the
two subtrees is not always the mere sum of their
costs. We can overcome this difficulty by tackling
a new class of submodular maximization prob-
lem: a budgeted monotone nondecreasing sub-
modular function maximization with a cost func-
tion, where the cost of an extraction unit varies
depending on what other extraction units are se-
lected. By formalizing the subtree extraction prob-
lem as this new maximization problem, we can
treat the constraints regarding the grammaticality
of the compressed sentences in a straightforward
way and use an arbitrary monotone submodular
word score function for words including our word
score function (shown later). We also propose a
new greedy algorithm that solves this new class of
maximization problem with a performance guar-
antee 12(1− e−1).
We evaluated our method on by using it to per-
form query-oriented summarization (Tang et al.,
2009). Experimental results show that it is supe-
rior to state-of-the-art methods.
</bodyText>
<sectionHeader confidence="0.999748" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999820609756098">
Submodularity is formally defined as a property of
a set function for a finite universe V . The function
f : 2v -+ R maps a subset S C_ V to a real value.
If for any S,T C_ V , f(SUT)+ f(SnT) G
f(S) + f(T), f is called submodular. This defini-
tion is equivalent to that of diminishing returns,
which is well known in the field of economics:
f(S U Jul) − f(S) G f(T U Jul) − f(T), where
T C_ S C_ V and u is an element of V . Di-
minishing returns means that the value of an el-
ement u remains the same or decreases as S be-
comes larger. This property is suitable for sum-
marization purposes, because the gain of adding a
new sentence to a summary that already contains
sufficient information should be small. Therefore,
many studies have formalized text summarization
as a submodular maximization problem (Lin and
Bilmes, 2010; Lin and Bilmes, 2011; Morita et
al., 2011). Their approaches, however, have been
based on sentence extraction. To our knowledge,
there is no study that addresses the joint task of
simultaneously performing compression and ex-
traction through an approximate submodular max-
imization with a performance guarantee.
In the field of constrained maximization prob-
lems, Kulik et al. (2009) proposed an algorithm
that solves the submodular maximization problem
under multiple linear constraints with a perfor-
mance guarantee 1 − e−1 in polynomial time. Al-
though their approach can represent more flexible
constraints, we cannot use their algorithm to solve
our problem, because their algorithm needs to enu-
merate many combinations of elements. Integer
linear programming (ILP) formulations can repre-
sent such flexible constraints, and they are com-
monly used to model text summarization (McDon-
ald, 2007). Berg-Kirkpatrick et al. (2011) formu-
lated a unified task of sentence extraction and sen-
tence compression as an ILP. However, it is hard to
solve large-scale ILP problems exactly in a practi-
cal amount of time.
</bodyText>
<sectionHeader confidence="0.9936665" genericHeader="method">
3 Budgeted Submodular Maximization
with Cost Function
</sectionHeader>
<subsectionHeader confidence="0.999656">
3.1 Problem Definition
</subsectionHeader>
<bodyText confidence="0.999833823529412">
Let V be the finite set of all valid subtrees in
the source documents, where valid subtrees are
defined to be the ones that can be regarded as
grammatical sentences. In this paper, we regard
subtrees containing the root node of the sentence
as valid. Accordingly, V denotes a set of all
rooted subtrees in all sentences. A subtree con-
tains a set of elements that are units in a de-
pendency structure (e.g., morphemes, words or
clauses). Let us consider the following problem
of budgeted monotone nondecreasing submodu-
lar function maximization with a cost function:
maxscv Jf(S) : c (S) G Ll, where S is a sum-
mary represented as a set of subtrees, c(·) is the
cost function for the set of subtrees, L is our bud-
get, and the submodular function f(·) scores the
summary quality. The cost function is not always
the sum of the costs of the covered subtrees, but
depends on the set of the covered elements by the
subtrees. Here, we will assume that the generated
summary has to be as long as or shorter than the
given summary length limit, as measured by the
number of characters. This means the cost of a
subtree is the integer number of characters it con-
tains.
V is partitioned into exclusive subsets B of valid
subtrees, and each subset corresponds to the orig-
inal sentence from which the valid subtrees de-
rived. However, the cost of a union of subtrees
from different sentences is simply the sum of the
costs of subtrees, while the cost of a union of sub-
trees from the same sentence is smaller than the
sum of the costs. Therefore, the problem can be
represented as follows:
</bodyText>
<page confidence="0.978303">
1024
</page>
<figure confidence="0.3077405">
max If (S) : E c (B ∩ S) ≤ L1 . (1)
B∈IM J
</figure>
<bodyText confidence="0.998619">
For example, if we add a subtree t containing
words {wa, wb, wc} to a summary that already
covers words {wa, wb, wd} from the same sen-
tence, the additional cost of t is only c({wc}) be-
cause wa and wb are already covered1.
The problem has two requirements. The first
requirement is that the union of valid subtrees is
also a valid subtree. The second requirement is
that the union of subtrees and a single valid sub-
tree have the same score and the same cost if they
cover the same elements. We will refer to the sin-
gle valid subtree as the equivalent subtree of the
union of subtrees. These requirements enable us
to represent sentence compression as the extrac-
tion of subtrees from a sentence. This is because
the requirements guarantee that the extracted sub-
trees represent a sentence.
</bodyText>
<subsectionHeader confidence="0.998383">
3.2 Greedy Algorithm
</subsectionHeader>
<bodyText confidence="0.999859190476191">
We propose Algorithm 1 that solves the maximiza-
tion problem (Eq.1). The algorithm is based on
ones proposed by Khuller et al. (1999) and Krause
et al. (2005). Instead of enumerating all candidate
subtrees, we use a local search to extract the ele-
ment that has the highest gain per cost. In the al-
gorithm, Gi indicates a summary set obtained by
adding element si to Gi−1. U means the set of
subtrees that are not extracted. The algorithm it-
eratively adds to the current summary the element
si that has the largest ratio of the objective func-
tion gain to the additional cost, unless adding it
violates the budget constraint. We set a parame-
ter r that is the scaling factor proposed by Lin and
Bilmes (2010). After the loop, the algorithm com-
pares Gi with the {s∗} that has the largest value of
the objective function among all subtrees that are
under the budget, and it outputs the summary can-
didate with the largest value.
Let us analyze the performance guarantee of Al-
gorithm 12.
</bodyText>
<footnote confidence="0.659373153846154">
1Each subset B corresponds to a kind of greedoid con-
straint. V implicitly constrains the model such that it can
only select valid subtrees from a set of nodes and edges.
2Our performance guarantee is lower than that reported
by Lin and Bilmes (2010). However, their proof is er-
roneous. In their proof of Lemma 2, they derive `du E
S∗\Gi−1, ρu(Gi−1) &lt; ρvi(Gi−1)
Cur Crvi
from line 4 of their Algorithm 1, which selects the densest
element out of all available elements. However, the inequal-
ity does not hold for i, for which element u selected on line
4 is discarded on line 5 of their algorithm. The performance
guarantee of their algorithm is actually the same as ours, since
</footnote>
<figure confidence="0.3623809375">
Algorithm 1 Modified greedy algorithm for budgeted
submodular function maximization with a cost function.
1: G0 ← 0
2: U ← V
3: i ← 1
4: while U =6 0 do
f(Gi−1∪{s})−f(Gi−1)
5: si ← arg maxs∈U (c(Gi−1∪{s})−c(Gi−1))r
6: if c({si} ∪ Gi−1) ≤ L then
7: Gi ← Gi−1 ∪ {si}
8: i ← i + 1
9: end if
10: U ← U\{si}
11: end while
12: s¯ ← arg maxs∈V,c(s)≤L f({s})
13: return Gf = arg maxS∈{{¯s},Gi} f(S)
</figure>
<construct confidence="0.625901">
Theorem 1 For a normalized monotone submod-
ular function f(·), Algorithm 1 has a constant
approximation factor when r = 1 as follows:
</construct>
<equation confidence="0.8811">
f (Gf) ≥ ( 2 (1 − e−1)� f(S∗), (2)
</equation>
<bodyText confidence="0.891447">
where S∗ is the optimal solution and, Gf is the
solution obtained by Greedy Algorithm 1.
Proof. See appendix.
</bodyText>
<subsectionHeader confidence="0.999577">
3.3 Relation with Discrete Optimization
</subsectionHeader>
<bodyText confidence="0.97349716">
We argue that our optimization problem can be
regarded as an extraction of subtrees rooted at a
given node from a directed graph, instead of from
a tree. Let D be the set of edges of the directed
graph, F be a subset of D that is a subtree. In the
field of combinatorial optimization, a pair (D, F)
is a kind of greedoid: directed branching greedoid
(Schmidt, 1991). A greedoid is a generalization of
the matroid concept. However, while matroids are
often used to represent constraints on submodular
maximization problems (Conforti and Cornu´ejols,
1984; Calinescu et al., 2011), greedoids have not
been used for that purpose, in spite of their high
representation ability. To our knowledge, this is
the first study that gives a constant performance
guarantee for the submodular maximization under
greedoid (non-matroid) constraints.
the guarantee 12 (1− e−1) was already proved by Krause and
Guestrin (2005). We show a counterexample. Suppose that
V is { e1(density 4:cost 6), e2(density 2:cost 4), e3(density
3:cost 1), e4(density 1:cost 1) 1, and cost limit K is 10. The
optimal solution is S∗ = {e1, e21. Their algorithm selects
e1, e3, e4 in this order. However the algorithm selects e2 on
line 4 after selecting e3, and it drops e2 on line 5. As a result,
e4 selected by the algorithm does not satisfy the inequality
</bodyText>
<equation confidence="0.964916">
`du E S∗\Gi−1, ρu(Gi−1)
Cr u
, for any i(1 &lt; i &lt; |G|),
&lt; ρvi(Gi−1)
Cr vi
.
</equation>
<page confidence="0.981592">
1025
</page>
<sectionHeader confidence="0.994923" genericHeader="method">
4 Joint Model of Extraction and
Compression
</sectionHeader>
<bodyText confidence="0.999998291139241">
We will formalize the unified task of sentence
compression and extraction as a budgeted mono-
tone nondecreasing submodular function maxi-
mization with a cost function. In this formaliza-
tion, a valid subtree of a sentence represents a
candidate of a compressed sentence. We will re-
fer to all valid subtrees of a given sentence as a
valid set. A valid set corresponds to all candi-
dates of the compression of a sentence. Note that
although we use the valid set in the formaliza-
tion, we do not have to enumerate all the candi-
dates for each sentence. Since, from the require-
ments, the union of valid subtrees is also a valid
subtree in the valid set, the model can extract one
or more subtrees from one sentence, and generate
a compressed sentence by merging those subtrees
to generate an equivalent subtree. Therefore, the
joint model can extract an arbitrarily compressed
sentence as a subtree without enumerating all can-
didates. The joint model can remove the redundant
part as well as the irrelevant part of a sentence, be-
cause the model simultaneously extracts and com-
presses sentences. We can approximately solve the
subtree extraction problem by using Algorithm 1.
On line 5 of the algorithm, the subtree extraction
is performed as a local search that finds maximal
density subtrees from the whole documents. The
maximal density subtree is a subtree that has the
highest score per cost of subtree. We use a cost
function to represent the cost, which indicates the
length of word tokens in the subtree.
In this paper, we address the task of summariza-
tion of Japanese text by means of sentence com-
pression and extraction. In Japanese, syntactic
subtrees that contain the root of the dependency
tree of the original sentence often make gram-
matical sentences. This means that the require-
ments mentioned in Section 3.1 that a union of
valid subtrees is a valid and equivalent tree is of-
ten true for Japanese. The root indicates the pred-
icate of a sentence, and it is syntactically modi-
fied by other prior words. Some modifying words
can be pruned. Therefore, sentence compression
can be represented as edge pruning. The linguis-
tic units we extract are bunsetsu phrases, which
are syntactic chunks often containing a functional
word after one or more content words. We will re-
fer to bunsetsu phrases as phrases for simplicity.
Since Japanese syntactic dependency is generally
defined between two phrases, we use the phrases
as the nodes of subtrees.
In this joint model, we generate a compressed
sentence by extracting an arbitrary subtree from a
dependency tree of a sentence. However, not all
subtrees are always valid. The sentence generated
by a subtree can be unnatural even though the sub-
tree contains the root node of the sentence. To
avoid generating such ungrammatical sentences,
we need to detect and retain the obligatory de-
pendency relations in the dependency tree. We
address this problem by imposing must-link con-
straints if a phrase corresponds to an obligatory
case of the main predicate. We merge obligatory
phrases with the predicate beforehand so that the
merged nodes make a single large node.
Although we focus on Japanese in this pa-
per, our approach can be applied to English and
other languages if certain conditions are satisfied.
First, we need a dependency parser of the lan-
guage in order to represent sentence compression
as dependency tree pruning. Moreover, although,
in Japanese, obligatory cases distinguish which
edges of the dependency tree can be pruned or not,
we need another technique to distinguish them in
other languages. For example we can distinguish
obligatory phrases from optional ones by using se-
mantic role labeling to detect arguments of predi-
cates. The adaptation to other languages is left for
future work.
</bodyText>
<subsectionHeader confidence="0.981417">
4.1 Objective Function
</subsectionHeader>
<bodyText confidence="0.99999635">
We extract subtrees from sentences in order to
solve the query-oriented summarization problem
as a unified one consisting of sentence compres-
sion and extraction. We thus need to allocate a
query relevance score to each node. Off-the-shelf
similarity measures such as the cosine similarity of
bag-of-words vectors with query terms would al-
locate scores to the terms that appear in the query,
but would give no scores to terms that do not ap-
pear in it. With such a similarity, sentence com-
pression extracts nearly only the query terms and
fails to contain important information. Instead,
we used Query SnowBall (QSB) (Morita et al.,
2011) to calculate the query relevance score of
each phrase. QSB is a method for query-oriented
summarization, which calculates the similarity be-
tween query terms and each word by using co-
occurrences within the source documents. Al-
though the authors of QSB also provided scores
of word pairs to avoid putting excessive penalties
</bodyText>
<page confidence="0.977252">
1026
</page>
<bodyText confidence="0.999978821428571">
on word overlaps, we do not score word pairs. The
score function is supermodular as a score function
of subtree extraction3, because the union of two
subtrees can have extra word pairs that are not in-
cluded in either subtree. If the extra pair has a pos-
itive score, the score of the union is greater than
the sum of the score of the subtrees. This violates
the definition of submodularity, and invalidates the
performance guarantee of our algorithms.
We designed our objective function by combin-
ing this relevance score with a penalty for redun-
dancy and too-compressed sentences. Important
words that describe the main topic should occur
multiple times in a good summary. However, ex-
cessive overlap undermines the quality of a sum-
mary, as do irrelevant words. Therefore, the scores
of overlapping words should be lower than thoseof
new words. The behavior can be represented by a
submodular objective function that reduces word
scores depending on those already included in the
summary. Furthermore, a summary consisting of
many too-compressed sentences would lack read-
ability. We thus gives a positive reward to long
sentences. The positive reward leads to a natu-
ral summary being generated with fewer sentences
and indirectly penalizes too short sentences. Our
positive reward for long sentences is represented
as
</bodyText>
<equation confidence="0.995689">
reward(S) = c(S) − |S|, (3)
</equation>
<bodyText confidence="0.999985">
where c(S) is the cost of summary S, and |S |is the
number of sentences in S. Since a sentence must
contain more than one character, the reward con-
sistently gives a positive score, and gives a higher
score to a summary that consists of fewer sen-
tences.
Let d be the damping rate, counts(w) be the
number of sentences containing word w in sum-
mary S, words(S) be the set of words included in
summary S, gsb(w) be the query relevance score
of word w, and -y be a parameter that adjusts the
rate of sentence compression. Our score function
for a summary S is as follows:
</bodyText>
<equation confidence="0.7907375">
+ -y reward(S).
(4)
</equation>
<bodyText confidence="0.996607">
An optimization problem with this objective
function cannot be regarded as an ILP problem be-
cause it contains non-linear terms. It is also ad-
</bodyText>
<footnote confidence="0.9791645">
3The score is still submodular for the purpose of sentence
extraction.
</footnote>
<bodyText confidence="0.999932">
vantageous that the submodular maximization can
deal with such objective functions. Note that the
objective function is such that it can be calculated
according to the type of word. Due to the na-
ture of the objective function, we can use dynamic
programming to effectively search for the subtree
with the maximal density.
</bodyText>
<subsectionHeader confidence="0.930309">
4.2 Local Search for Maximal Density
Subtree
</subsectionHeader>
<bodyText confidence="0.999955454545454">
Let us now discuss the local search used on line
5 of Algorithm 1. We will use a fast algorithm to
find the maximal density subtree (MDS) of a given
sentence for each cost in Algorithm 1.
Consider the objective function Eq. 4, We can
ignore the second term of the reward function
while looking for the MDS in a sentence because
the number of sentences is the same for every
MDS in a sentence. That is, the gain function of
adding a subtree to a summary can be represented
as the sum of gains for words:
</bodyText>
<equation confidence="0.999857">
g(t) = ∑ {gains(w)+ fregt(w)c(w)-y},
wEt
gains(w) = gsb(w)dcounts(w),
</equation>
<bodyText confidence="0.998773037037037">
where fregt(w) is the number of ws in subtree
t, and gains(w) is the gain of adding the word
w to the summary S. Our algorithm is based on
dynamic programming, and it selects a subtree that
maximizes the gain function per cost.
When the word gain is a constant, the algorithm
proposed by Hsieh et al. (2010) can be used to
find the MDS. We extended this algorithm to work
for submodular word gain functions that are not
constant. Note that the gain of a word that oc-
curs only once in the sentence, can be treated as
a constant. In what follows, we will describe an
extended algorithm to find the MDS even if there
is word overlap.
For example, let us describe how to obtain the
MDS in the case of a binary tree. First let us tackle
the case in which the gain is always constant. Let
n be a node in the tree, a and b be child nodes of n,
c(n) be the cost of n, mdsca be the MDS rooted at
a and have cost c. mdsn = {mdsc(n)
n , ... , mdsn}
denotes the set of MDSs for each cost and its root
node n. The valid subtrees rooted at n can be ob-
tained by taking unions of n with one or both of
t1 E mdsa and t2 E mdsb. mdscn is the union that
has the largest gain over the union with the cost of
c (by enumerating all the unions). The MDS for
</bodyText>
<figure confidence="0.910688">

∑ 
f(S) = 
wEwords(s)



countS(w)−1
gsb(w)di
∑
i=0
</figure>
<page confidence="0.989897">
1027
</page>
<bodyText confidence="0.999872">
the sentence root can be found by calculating each
mdscn from the bottom of the tree to the top.
Next, let us consider the objective function that
returns the sum of values of submodular word gain
functions. When there is no word overlap within
the union, we can obtain mdscn in the same man-
ner as for the constant gain. In contrast, if the
union includes word overlap, the gain is less than
the sum of gains: g(mdscn) G g(n) + g(mdsk�) +
g(mdsc−k−c(n)), where k and c are variables. The
</bodyText>
<figure confidence="0.441065">
b
</figure>
<bodyText confidence="0.988068441860465">
score reduction can change the order of the gains
of the union. That is, it is possible that another
union without word overlaps will have a larger
gain. Therefore, the algorithm needs to know
whether each t E mdsn has the potential to have
word overlaps with other MDSs. Let O be the set
of words that occur twice or more in the sentence
on which the local seach focuses. The algorithm
stores MDS for each o C_ O, as well as each cost.
By storing MDS for each o and cost as shown
in Fig. 1, the algorithm can find MDS with the
largest gain over the combinations of subtrees.
Algorithm 2 shows the procedure. In it, t and m
denote subtrees, words(t) returns a set of words
in the subtree, g(t) returns the gain of t, tree(n)
means a tree consisting of node n, and t U m de-
notes the union of subtrees: t and m. subt in-
dicates a set of current maximal density subtrees
among the combinations calculated before. newt
indicates a set of temporary maximal density sub-
trees for the combinations calculated from line 4
to 8. subt[cost,ws] indicates a element of subt that
has a cost cost and contains a set of words ws.
newt[cost,ws] is defined similarly. Line 1 sets subt
to a set consisting of a subtree that indicates node
n itself. The algorithm calculates maximal den-
sity subtrees within combinations of the root node
n and MDSs rooted at child nodes of n. Line 3
iteratively adds MDSs rooted at a next child node
to the combinations; the algorithm then calculates
MDSs newt between subt and the MDSs of the
child node. The procedure from line 6 to 8 selects
a subtree that has a larger gain from the tempo-
rary maximal subtree and the union of t and m.
The computational complexity of this algorithm is
O(NC2) when there is no word overlap within the
sentence, where C denotes the cost of the whole
sentence, and N denotes the number of nodes in
the sentence. The complexity order is the same
as that of the algorithm of Hsieh et al. (2010).
When we treat word overlaps, we need to count
Algorithm 2 Algorithm for finding maximal density
subtree for each cost: XMSs.
</bodyText>
<figure confidence="0.521330928571429">
Function: MDSs
Require: root node n
1: subt[c(n),words(n)1O] = tree(n)
2: newt = 0
3: for i E child node of n do
4: for t E MDSs(i) do
5: for m E subt do
6: index = [c(t U m), words(t U m) n O]
7: newtindex = arg maxicinewtind—tUnbl g(9)
8: end for
9: end for
10: subt = newt
11: end for
12: return subt
</figure>
<figureCaption confidence="0.999207">
Figure 1: Maximal density subtree extraction. The
</figureCaption>
<bodyText confidence="0.8981684">
right table enumerates the subtrees rooted at w2 in
the left tree for all indices. The number in each
tree node is the score of the word.
all unions of combinations of the stored MDSs.
There are at most (C2|O|) MDSs that the algo-
rithm needs to store at each node. Therefore the
total computational complexity is O(NC222|O|).
Since it is unlikely that a sentence contains many
word tokens of one type, the computational cost
may not be so large in practical situations.
</bodyText>
<sectionHeader confidence="0.997732" genericHeader="method">
5 Experimental Settings
</sectionHeader>
<bodyText confidence="0.999979058823529">
We evaluate our method on Japanese QA test
collections from NTCIR-7 ACLIA1 and NTCIR-
8 ACLIA2 (Mitamura et al., 2008; Mitamura et
al., 2010). The collections contain questions and
weighted answer nuggets. Our experimental set-
tings followed the settings of (Morita et al., 2011),
except for the maximum summary length. We
generated summaries consisting of 140 Japanese
characters or less, with the question as the query
terms. We did this because our aim is to use our
method in mobile situations. We used “ACLIA1
test data” to tune the parameters, and evaluated our
method on “ACLIA2 test” data.
We used JUMAN (Kurohashi and Kawahara,
2009a) for word segmentation and part-of-speech
tagging, and we calculated idf over Mainichi
newspaper articles from 1991 to 2005. For the de-
</bodyText>
<page confidence="0.975285">
1028
</page>
<table confidence="0.99971">
POURPRE Precision Recall F1 F3
Lin and Bilmes (2011) 0.215 0.126 0.201 0.135 0.174
Subtree extraction (SbE) 0.268 0.238 0.213 0.159 0.190
Sentence extraction (NC) 0.278 0.206 0.215 0.139 0.183
</table>
<tableCaption confidence="0.99996">
Table 1: Results on ACLIA2 test data.
</tableCaption>
<bodyText confidence="0.9998492">
pendency parsing, we used KNP (Kurohashi and
Kawahara, 2009b). Since KNP internally has a
flag that indicates either an “obligatory case” or an
“adjacent case”, we regarded dependency relations
flagged by KNP as obligatory in the sentence com-
pression. KNP utilizes Kyoto University’s case
frames (Kawahara and Kurohashi, 2006) as the re-
source for detecting obligatory or adjacent cases.
To evaluate the summaries, we followed the
practices of the TAC summarization tasks (Dang,
2008) and NTCIR ACLIA tasks, and computed
pyramid-based precision with the allowance pa-
rameter, recall, and Fβ (where 0 is 1 or 3)
scores. The allowance parameter was determined
from the average nugget length for each question
type of the ACLIA2 collection (Mitamura et al.,
2010). Precision and recall are computed from the
nuggets that the summary covered along with their
weights. One of the authors of this paper man-
ually evaluated whether each nugget matched the
summary. We also used the automatic evaluation
measure, POURPRE (Lin and Demner-Fushman,
2006). POURPRE is based on word matching
of reference nuggets and system outputs. We re-
garded as stopwords the most frequent 100 words
in Mainichi articles from 1991 to 2005 (the doc-
ument frequency was used to measure the fre-
quency). We also set the threshold of nugget
matching as 0.5 and binarized the nugget match-
ing, following the previous study (Mitamura et al.,
2010). We tuned the parameters by using POUR-
PRE on the development dataset.
Lin and Bilmes (2011) designed a monotone
submodular function for query-oriented summa-
rization. Their succinct method performed well
in DUC from 2004 to 2007. They proposed a
positive diversity reward function in order to de-
fine a monotone submodular objective function for
generating a non-redundant summary. The diver-
sity reward gives a smaller gain for a biased sum-
mary, because it consists of gains based on three
clusters and calculates a square root score with
respect to each sentence. The reward also con-
tains a score for the similarity of a sentence to
the query, for purposes of query-oriented summa-
</bodyText>
<table confidence="0.999657666666667">
Recall Length # of nuggets
Subtree extraction 0.213 11,143 100
Reconstructed (RC) 0.228 13,797 108
</table>
<tableCaption confidence="0.999635">
Table 2: Effect of sentence compression.
</tableCaption>
<bodyText confidence="0.999849">
rization. Their objective function also includes a
coverage function based on the similarity wi,j be-
tween sentences. In the coverage function min
function limits the maximum gain α Ei∈V wi,j,
which is a small fraction α of the similarity be-
tween a sentence j and the all source documents.
The objective function is the sum of the positive
reward R and the coverage function L over the
source documents V , as follows:
</bodyText>
<equation confidence="0.997231">
F(S) = L(S) +
Y_
L(S) =
i∈V
RQ,k =
c∈Ck
Y_ V wi,j + (1 − 0)rj,Q),
� � �Y_ ( 0 Y_
j∈S∪c i∈V
N
</equation>
<bodyText confidence="0.9999668">
where α, 0 and Ak are parameters, and rj,Q repre-
sents the similarity between sentence j and query
Q. We tuned the parameters on the development
dataset. Lin and Bilmes (2011) used three clusters
Ck with different granularities, which were calcu-
lated in advance. We set the granularity to (0.2N,
0.15N, 0.05N) according to the settings of them,
where N is the number of sentences in a docu-
ment.
We also regarded as stopwords “AX6 (tell),”
“06 (know),” “ful (what)” and their conjugated
forms, which are excessively common in ques-
tions. For the query expansion in the baseline, we
used Japanese WordNet to obtain synonyms and
hypernyms of query terms.
</bodyText>
<sectionHeader confidence="0.999913" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.999816857142857">
Table 1 summarizes our results. “Subtree ex-
traction (SbE)” is our method, and “Sentence ex-
traction (NC)” is a version of our method with-
out compression. The NC has the same objec-
tive function but only extracts sentences. The F1-
measure and F3-measure of our method are 0.159
and 0.190 respectively, while those of the state-of-
</bodyText>
<figure confidence="0.989853692307692">
min
3
Y_
k=1
AkRQ,k(S),
,
wi,k
Y_
j∈S
Y_ wi,j, α
k∈V
1
I
</figure>
<page confidence="0.995401">
1029
</page>
<bodyText confidence="0.999961536585366">
the-art baseline are 0.135 and 0.174 respectively.
Unfortunately, since the document set is small, the
difference is not statistically significant. Compar-
ing our method with the one without compression,
we can see that there are improvements in the F1
and F3 scores of the human evaluation, whereas
the POURPRE score of the version of our method
without compression is higher than that of our
method with compression. The compression im-
proved the precision of our method, but slightly
decreased the recall.
For the error analyses, we reconstructed the
original sentences from which our method ex-
tracted the subtrees. Table 2 shows the statistics
of the summaries of SbE and reconstructed sum-
maries (RC). The original sentences covered 108
answer nuggets in total, and 8 of these answer
nuggets were dropped by the sentence compres-
sion. Comparing the results of SbE and RC, we
can see that the sentence compression caused the
recall of SbE to be 7% lower than that of RC.
However, the drop is relatively small in light of
the fact that the sentence compression can discard
19% of the original character length with SbE.
This suggests that the compression can efficiently
prune words while avoiding pruning informative
content.
Since the summary length is short, we can select
only two or three sentences for a summary. As
Morita et al. (2011) mentioned, answer nuggets
overlap each other. The baseline objective func-
tion R tends to extract sentences from various
clusters. If the answer nuggets are present in the
same cluster, the objective function does not fit the
situation. However, our methods (SbE and NC)
have a parameter d that can directly adjust overlap
penalty with respect to word importance as well
as query relevance. This may help our methods to
cover similar answer nuggets. In fact, the develop-
ment data resulted in a relatively high parameter d
(0.8) for NC compared with 0.2 for SbE.
</bodyText>
<sectionHeader confidence="0.997866" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999972545454546">
We formalized a query-oriented summarization,
which is a task in which one simultaneously per-
forms sentence compression and extraction, as a
new optimization problem: budgeted monotone
nondecreasing submodular function maximization
with a cost function. We devised an approximate
algorithm to solve the problem in a reasonable
computational time and proved that its approxima-
tion rate is 12(1 − e−1). Our approach achieved
an F3-measure of 0.19 on the ACLIA2 Japanese
test collection, which is 9.2 % improvement over
a state-of-the-art method using a submodular ob-
jective function.
Since our algorithm requires that the objective
function is the sum of word score functions, our
proposed method has a restriction that we cannot
use an arbitrary monotone submodular function as
the objective function for the summary. Our fu-
ture work will improve the local search algorithm
to remove this restriction. As mentioned before,
we also plan to adapt of our system to other lan-
guages.
</bodyText>
<sectionHeader confidence="0.985919" genericHeader="acknowledgments">
Appendix
</sectionHeader>
<bodyText confidence="0.999565105263158">
Here, we analyze the performance guarantee of
Algorithm 1. We use the following notation. S* is
the optimal solution, cu(S) is the residual cost of
subtree u when S is already covered, and i* is the
last step before the algorithm discards a subtree
s ∈ S* or a part of the subtree s. This is because
the subtree does not belong to either the approxi-
mate solution or the optimal solution. We can re-
move the subtree s&apos; from V without changing the
approximate rate. si is the i-th subtree obtained by
line 5 of Algorithm 1. Gi is the set obtained after
adding subtree si to Gi−1 from the valid set Bi.
Gf is the final solution obtained by Algorithm 1.
f(·) : 2V → R is a monotone submodular func-
tion.
We assume that there is an equivalent sub-
tree with any union of subtrees in a valid set B:
∀t1, t2, ∃te, te ≡ {t1, t2}. Note that for any or-
der of the set, the cost or profit of the set is fixed:
</bodyText>
<equation confidence="0.643054">
u
uiES={u1,...,u|S|} cui(Si−1) = c(S).
Lemma 1 ∀X, Y ⊆ V, f(X) ≤ f(Y ) +
</equation>
<bodyText confidence="0.9727098">
EuEX\Y ρu(Y ), where ρu(S) = f(S ∪ {u}) −
f(S).
The inequality can be derived from the definition
of submodularity. ❑
Lemma 2 For i = 1, ... , i*+1, when 0 ≤ r ≤ 1,
</bodyText>
<equation confidence="0.703797">
f(S∗)−f(Gi−1)&lt;Lr|S∗|1−r
csi (Gi−1) (f(Gi−1U{si})−f(Gi−1)),
</equation>
<bodyText confidence="0.998736">
where cu(S)=c(SU{u})−c(S).
Proof. From line 5 of Algorithm 1, we have
</bodyText>
<equation confidence="0.6006715">
∀u ∈ S*\Gi−1, ρu(Gi−1) ρsi(Gi−1)
cu(Gi−1)r ≤ csi(Gi−1)r
</equation>
<bodyText confidence="0.899737666666667">
Let B be a valid set, and union be a func-
tion that returns the union of subtrees. We have
.
</bodyText>
<page confidence="0.865324">
1030
</page>
<bodyText confidence="0.999893">
∀T ⊆ B, ∃b ∈ B, b = union(T), because we
have an equivalent tree b ∈ B for each union
of trees T in a valid set B. That is, for any
set of subtrees, we have an equivalent set of sub-
trees, where bi ∈ Bi. Without loss of generality,
we can replace the difference set S∗\Gi−1 with
a set T0i−1 = {b0, ... , b|T0i−1|} that does not con-
tain any two elements extracted from the same
valid set. Thus when 0 ≤ r ≤ 1 and 0 ≤
</bodyText>
<equation confidence="0.998849666666667">
i ≤ i∗+ 1, ρs∗\Gi−1(Gi−1) ρT0 i−1(Gi−1)
cS∗\Gi−1(Gi−1)r = cT0i−1(Gi−1)r, and
∀bj ∈ T0i−1,
ρbj (Gi−1) cbj(Gi−1)r ≤ ρsi(Gi−1)
csi(Gi−1)r . Thus,
ρT0 (Gi-1) _ /lET! Pu(Gi−1)
i−1 z
ρsZ(Gi−1) C&gt;�
≤cs?Gi-1)r
i( a-1 u∈T 0
&lt; ρsi(Gi−1) csi(Gi−1 Ti 1\ u∈i−1 J1
—) i−1|
T0i−1
r
≤ Psi(Gi−1 ) 0 1−r
csi(Gi−1)r Ti−1 |�u∈T0i−1 cu(φ)
≤ ρsi (Gi−1)
csi (Gi−1)r |S∗|1−rLr,
</equation>
<bodyText confidence="0.963660571428571">
where the second inequality is from H¨older’s in-
equality. The third inequality uses the submodu-
larity of the cost function,
cu(Gi−1) = c({u} ∪ Gi−1) − c(Gi−1) ≤ cu(φ)
and the fact that |S∗ |≥ |S∗\Gi−1 |≥ |T0i−1|, and
Eu∈T0i−1 cu(φ) = c(T0i−1) ≤ L.
As a result, we have
</bodyText>
<equation confidence="0.997416">
ρs∗\Gi−1(Gi−1) = ρT0i−1(Gi−1)
≤ csi(Gi−1)r |S∗|1−rLr.
ρsi(Gi−1)
Let X = S∗ and Y = Gi−1. Applying Lemma
1 yields
f(S∗) ≤ f(Gi−1) + ρu∈S∗\Gi−1(Gi−1).
≤ f(Gi−1) + ρsi(Gi−1)
csi(Gi−1) |S∗|1−rLr.
</equation>
<bodyText confidence="0.9974698">
The lemma follows as a result.
Lemma 3 For a normalized monotone submodu-
lar f(·), for i = 1, ... , i∗ + 1 and 0 ≤ r ≤ 1 and
letting si be the i-th unit added into G and Gi be
the set after adding si, we have
</bodyText>
<equation confidence="0.912577">
�f (Gi) ≥ 1 − rl (1 − cLriSG,* 11-)r l
k=1 \ J
</equation>
<bodyText confidence="0.9907028">
Proof. This is proved similarly to Lemma 3 of
(Krause and Guestrin, 2005) using Lemma 2.
Proof of Theorem 1. This is proved similarly to
Theorem 1 of (Krause and Guestrin, 2005) using
Lemma 3.
</bodyText>
<sectionHeader confidence="0.998314" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999635782608696">
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies - Volume 1, HLT ’11, pages
481–490, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Calinescu Calinescu, Chandra Chekuri, Martin P´al,
and Jan Vondr´ak. 2011. Maximizing a monotone
submodular function subject to a matroid constraint.
SIAM Journal on Computing, 40(6):1740–1766.
Michele Conforti and G´erard Cornu´ejols. 1984. Sub-
modular set functions, matroids and the greedy al-
gorithm: Tight worst-case bounds and some gener-
alizations of the rado-edmonds theorem. Discrete
Applied Mathematics, 7(3):251 – 274.
Hoa Trang Dang. 2008. Overview of the tac
2008 opinion question answering and summariza-
tion tasks. In Proceedings of Text Analysis Confer-
ence.
Anupam Gupta, Aaron Roth, Grant Schoenebeck, and
Kunal Talwar. 2010. Constrained non-monotone
submodular maximization: offline and secretary
algorithms. In Proceedings of the 6th interna-
tional conference on Internet and network eco-
nomics, WINE’10, pages 246–257, Berlin, Heidel-
berg. Springer-Verlag.
Sun-Yuan Hsieh and Ting-Yu Chou. 2010. The
weight-constrained maximum-density subtree prob-
lem and related problems in trees. The Journal of
Supercomputing, 54(3):366–380, December.
Daisuke Kawahara and Sadao Kurohashi. 2006. A
fully-lexicalized probabilistic model for japanese
syntactic and case structure analysis. In Proceedings
of the main conference on Human Language Tech-
nology Conference of the North American Chap-
ter of the Association of Computational Linguistics,
HLT-NAACL ’06, pages 176–183, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Samir Khuller, Anna Moss, and Joseph S. Naor. 1999.
The budgeted maximum coverage problem. Infor-
mation Processing Letters, 70(1):39–45.
Andreas Krause and Carlos Guestrin. 2005. A
note on the budgeted maximization on submodular
functions. Technical Report CMU-CALD-05-103,
Carnegie Mellon University.
</reference>
<page confidence="0.797103">
1031
</page>
<reference confidence="0.999747046511628">
Ariel Kulik, Hadas Shachnai, and Tami Tamir. 2009.
Maximizing submodular set functions subject to
multiple linear constraints. In Proceedings of
the twentieth Annual ACM-SIAM Symposium on
Discrete Algorithms, SODA ’09, pages 545–554,
Philadelphia, PA, USA. Society for Industrial and
Applied Mathematics.
Sadao Kurohashi and Daisuke Kawahara, 2009a.
Japanese Morphological Analysis System JUMAN
6.0 Users Manual. http://nlp.ist.i.
kyoto-u.ac.jp/EN/index.php?JUMAN.
Sadao Kurohashi and Daisuke Kawahara, 2009b. KN
parser (Kurohashi-Nagao parser) 3.0 Users Man-
ual. http://nlp.ist.i.kyoto-u.ac.jp/
EN/index.php?KNP.
Jon Lee, Vahab S. Mirrokni, Viswanath Nagarajan, and
Maxim Sviridenko. 2009. Non-monotone submod-
ular maximization under matroid and knapsack con-
straints. In Proceedings of the 41st annual ACM
symposium on Theory of computing, STOC ’09,
pages 323–332, New York, NY, USA. ACM.
Hui Lin and Jeff Bilmes. 2010. Multi-document sum-
marization via budgeted maximization of submod-
ular functions. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, HLT ’10, pages 912–920, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Hui Lin and Jeff Bilmes. 2011. A class of submodular
functions for document summarization. In Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies - Volume 1, HLT ’11, pages 510–520,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Jimmy Lin and Dina Demner-Fushman. 2006. Meth-
ods for automatically evaluating answers to com-
plex questions. Information Retrieval, 9(5):565–
587, November.
Andr´e F. T. Martins and Noah A. Smith. 2009. Sum-
marization with a joint model for sentence extraction
and compression. In Proceedings of the Workshop
on Integer Linear Programming for Natural Lan-
gauge Processing, ILP ’09, pages 1–9, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Ryan McDonald. 2007. A study of global inference
algorithms in multi-document summarization. In
Proceedings of the 29th European conference on IR
research, ECIR’07, pages 557–564, Berlin, Heidel-
berg. Springer-Verlag.
Teruko Mitamura, Eric Nyberg, Hideki Shima,
Tsuneaki Kato, Tatsunori Mori, Chin-Yew Lin, Rui-
hua Song, Chuan-Jie Lin, Tetsuya Sakai, Donghong
Ji, and Noriko Kando. 2008. Overview of the
NTCIR-7 ACLIA Tasks: Advanced Cross-Lingual
Information Access. In Proceedings of the 7th NT-
CIR Workshop.
Teruko Mitamura, Hideki Shima, Tetsuya Sakai,
Noriko Kando, Tatsunori Mori, Koichi Takeda,
Chin-Yew Lin, Ruihua Song, Chuan-Jie Lin, and
Cheng-Wei Lee. 2010. Overview of the ntcir-8 aclia
tasks: Advanced cross-lingual information access.
In Proceedings of the 8th NTCIR Workshop.
Hajime Morita, Tetsuya Sakai, and Manabu Okumura.
2011. Query snowball: a co-occurrence-based ap-
proach to multi-document summarization for ques-
tion answering. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies: short pa-
pers - Volume 2, HLT ’11, pages 223–229, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Wolfgang Schmidt. 1991. Greedoids and searches in
directed graphs. Discrete Mathmatics, 93(1):75–88,
November.
Jie Tang, Limin Yao, and Dewei Chen. 2009. Multi-
topic based query-oriented summarization. In Pro-
ceedings of 2009 SIAM International Conference
Data Mining (SDM’2009), pages 1147–1158.
David M. Zajic, Bonnie J. Dorr, Jimmy Lin, and
Richard Schwartz. 2006. Sentence compression
as a component of a multi-document summariza-
tion system. In Proceedings of the 2006 Doc-
ument Understanding Conference (DUC 2006) at
NLT/NAACL 2006.
</reference>
<page confidence="0.994657">
1032
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.322635">
<title confidence="0.999253">Subtree Extractive Summarization via Submodular Maximization</title>
<author confidence="0.944596">Hajime Morita</author>
<affiliation confidence="0.999854">Tokyo Institute of Technology,</affiliation>
<email confidence="0.892962">morita@lr.pi.titech.ac.jp</email>
<author confidence="0.568962">Hiroya Takamura</author>
<affiliation confidence="0.999511">Tokyo Institute of Technology,</affiliation>
<email confidence="0.919632">takamura@pi.titech.ac.jp</email>
<author confidence="0.857366">Ryohei Sasano</author>
<affiliation confidence="0.999757">Tokyo Institute of Technology,</affiliation>
<email confidence="0.913393">sasano@pi.titech.ac.jp</email>
<author confidence="0.969585">Manabu Okumura</author>
<affiliation confidence="0.999901">Tokyo Institute of Technology,</affiliation>
<email confidence="0.949855">oku@pi.titech.ac.jp</email>
<abstract confidence="0.9986125">This study proposes a text summarization model that simultaneously performs sentence extraction and compression. We translate the text summarization task into a problem of extracting a set of dependency subtrees in the document cluster. We also encode obligatory case constraints as must-link dependency constraints in order to guarantee the readability of the generated summary. In order to handle the subtree extraction problem, we investigate a new class of submodular maximization problem, and a new algorithm that has approximation ratio Our experiments with the NTCIR ACLIA test collections show that our approach outperforms a state-of-the-art algorithm.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Taylor Berg-Kirkpatrick</author>
<author>Dan Gillick</author>
<author>Dan Klein</author>
</authors>
<title>Jointly learning to extract and compress.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11,</booktitle>
<pages>481--490</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1216" citStr="Berg-Kirkpatrick et al., 2011" startWordPosition="161" endWordPosition="164">he document cluster. We also encode obligatory case constraints as must-link dependency constraints in order to guarantee the readability of the generated summary. In order to handle the subtree extraction problem, we investigate a new class of submodular maximization problem, and a new algorithm that has the approximation ratio 2(1 − e−1). Our experiments with the NTCIR ACLIA test collections show that our approach outperforms a state-of-the-art algorithm. 1 Introduction Text summarization is often addressed as a task of simultaneously performing sentence extraction and sentence compression (Berg-Kirkpatrick et al., 2011; Martins and Smith, 2009). Joint models of sentence extraction and compression have a great benefit in that they have a large degree of freedom as far as controlling redundancy goes. In contrast, conventional two-stage approaches (Zajic et al., 2006), which first generate candidate compressed sentences and then use them to generate a summary, have less computational complexity than joint models. However, two-stage approaches are suboptimal for text summarization. For example, when we compress sentences first, the compressed sentences may fail to contain important pieces of information due to </context>
<context position="6644" citStr="Berg-Kirkpatrick et al. (2011)" startWordPosition="1042" endWordPosition="1045">mance guarantee. In the field of constrained maximization problems, Kulik et al. (2009) proposed an algorithm that solves the submodular maximization problem under multiple linear constraints with a performance guarantee 1 − e−1 in polynomial time. Although their approach can represent more flexible constraints, we cannot use their algorithm to solve our problem, because their algorithm needs to enumerate many combinations of elements. Integer linear programming (ILP) formulations can represent such flexible constraints, and they are commonly used to model text summarization (McDonald, 2007). Berg-Kirkpatrick et al. (2011) formulated a unified task of sentence extraction and sentence compression as an ILP. However, it is hard to solve large-scale ILP problems exactly in a practical amount of time. 3 Budgeted Submodular Maximization with Cost Function 3.1 Problem Definition Let V be the finite set of all valid subtrees in the source documents, where valid subtrees are defined to be the ones that can be regarded as grammatical sentences. In this paper, we regard subtrees containing the root node of the sentence as valid. Accordingly, V denotes a set of all rooted subtrees in all sentences. A subtree contains a se</context>
</contexts>
<marker>Berg-Kirkpatrick, Gillick, Klein, 2011</marker>
<rawString>Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein. 2011. Jointly learning to extract and compress. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11, pages 481–490, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Calinescu Calinescu</author>
<author>Chandra Chekuri</author>
<author>Martin P´al</author>
<author>Jan Vondr´ak</author>
</authors>
<title>Maximizing a monotone submodular function subject to a matroid constraint.</title>
<date>2011</date>
<journal>SIAM Journal on Computing,</journal>
<volume>40</volume>
<issue>6</issue>
<marker>Calinescu, Chekuri, P´al, Vondr´ak, 2011</marker>
<rawString>Calinescu Calinescu, Chandra Chekuri, Martin P´al, and Jan Vondr´ak. 2011. Maximizing a monotone submodular function subject to a matroid constraint. SIAM Journal on Computing, 40(6):1740–1766.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Conforti</author>
<author>G´erard Cornu´ejols</author>
</authors>
<title>Submodular set functions, matroids and the greedy algorithm: Tight worst-case bounds and some generalizations of the rado-edmonds theorem.</title>
<date>1984</date>
<journal>Discrete Applied Mathematics,</journal>
<volume>7</volume>
<issue>3</issue>
<pages>274</pages>
<marker>Conforti, Cornu´ejols, 1984</marker>
<rawString>Michele Conforti and G´erard Cornu´ejols. 1984. Submodular set functions, matroids and the greedy algorithm: Tight worst-case bounds and some generalizations of the rado-edmonds theorem. Discrete Applied Mathematics, 7(3):251 – 274.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoa Trang Dang</author>
</authors>
<title>Overview of the tac</title>
<date>2008</date>
<booktitle>In Proceedings of Text Analysis Conference.</booktitle>
<contexts>
<context position="27159" citStr="Dang, 2008" startWordPosition="4689" endWordPosition="4690">SbE) 0.268 0.238 0.213 0.159 0.190 Sentence extraction (NC) 0.278 0.206 0.215 0.139 0.183 Table 1: Results on ACLIA2 test data. pendency parsing, we used KNP (Kurohashi and Kawahara, 2009b). Since KNP internally has a flag that indicates either an “obligatory case” or an “adjacent case”, we regarded dependency relations flagged by KNP as obligatory in the sentence compression. KNP utilizes Kyoto University’s case frames (Kawahara and Kurohashi, 2006) as the resource for detecting obligatory or adjacent cases. To evaluate the summaries, we followed the practices of the TAC summarization tasks (Dang, 2008) and NTCIR ACLIA tasks, and computed pyramid-based precision with the allowance parameter, recall, and Fβ (where 0 is 1 or 3) scores. The allowance parameter was determined from the average nugget length for each question type of the ACLIA2 collection (Mitamura et al., 2010). Precision and recall are computed from the nuggets that the summary covered along with their weights. One of the authors of this paper manually evaluated whether each nugget matched the summary. We also used the automatic evaluation measure, POURPRE (Lin and Demner-Fushman, 2006). POURPRE is based on word matching of refe</context>
</contexts>
<marker>Dang, 2008</marker>
<rawString>Hoa Trang Dang. 2008. Overview of the tac 2008 opinion question answering and summarization tasks. In Proceedings of Text Analysis Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anupam Gupta</author>
<author>Aaron Roth</author>
<author>Grant Schoenebeck</author>
<author>Kunal Talwar</author>
</authors>
<title>Constrained non-monotone submodular maximization: offline and secretary algorithms.</title>
<date>2010</date>
<booktitle>In Proceedings of the 6th international conference on Internet and network economics, WINE’10,</booktitle>
<pages>246--257</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="3356" citStr="Gupta et al., 2010" startWordPosition="496" endWordPosition="499">ry directly from all available subsentences in the documents and not in a stepwise fashion. However, there is a difficulty with such a formalization. In the past, the resulting maximization problem has been often accompanied by thousands of linear constraints representing logical relations between words. The existing greedy algorithm for solving submodular maximization problems cannot work in the presence of such numerous constraints although monotone and nonmonotone submodular maximization with constraints other than budget constraints have been studied (Lee et al., 2009; Kulik et al., 2009; Gupta et al., 2010). In this study, we avoid this difficulty by reducing the task to one of extracting dependency subtrees from sentences in the source documents. The reduction replaces the difficulty of numerous linear constraints with another difficulty wherein two subtrees can share the same word to1023 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1023–1032, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics ken when they are selected from the same sentence, and as a result, the cost of the union of the two subtrees is not alway</context>
</contexts>
<marker>Gupta, Roth, Schoenebeck, Talwar, 2010</marker>
<rawString>Anupam Gupta, Aaron Roth, Grant Schoenebeck, and Kunal Talwar. 2010. Constrained non-monotone submodular maximization: offline and secretary algorithms. In Proceedings of the 6th international conference on Internet and network economics, WINE’10, pages 246–257, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sun-Yuan Hsieh</author>
<author>Ting-Yu Chou</author>
</authors>
<title>The weight-constrained maximum-density subtree problem and related problems in trees.</title>
<date>2010</date>
<journal>The Journal of Supercomputing,</journal>
<volume>54</volume>
<issue>3</issue>
<marker>Hsieh, Chou, 2010</marker>
<rawString>Sun-Yuan Hsieh and Ting-Yu Chou. 2010. The weight-constrained maximum-density subtree problem and related problems in trees. The Journal of Supercomputing, 54(3):366–380, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
</authors>
<title>A fully-lexicalized probabilistic model for japanese syntactic and case structure analysis.</title>
<date>2006</date>
<booktitle>In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, HLT-NAACL ’06,</booktitle>
<pages>176--183</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="27002" citStr="Kawahara and Kurohashi, 2006" startWordPosition="4662" endWordPosition="4665"> idf over Mainichi newspaper articles from 1991 to 2005. For the de1028 POURPRE Precision Recall F1 F3 Lin and Bilmes (2011) 0.215 0.126 0.201 0.135 0.174 Subtree extraction (SbE) 0.268 0.238 0.213 0.159 0.190 Sentence extraction (NC) 0.278 0.206 0.215 0.139 0.183 Table 1: Results on ACLIA2 test data. pendency parsing, we used KNP (Kurohashi and Kawahara, 2009b). Since KNP internally has a flag that indicates either an “obligatory case” or an “adjacent case”, we regarded dependency relations flagged by KNP as obligatory in the sentence compression. KNP utilizes Kyoto University’s case frames (Kawahara and Kurohashi, 2006) as the resource for detecting obligatory or adjacent cases. To evaluate the summaries, we followed the practices of the TAC summarization tasks (Dang, 2008) and NTCIR ACLIA tasks, and computed pyramid-based precision with the allowance parameter, recall, and Fβ (where 0 is 1 or 3) scores. The allowance parameter was determined from the average nugget length for each question type of the ACLIA2 collection (Mitamura et al., 2010). Precision and recall are computed from the nuggets that the summary covered along with their weights. One of the authors of this paper manually evaluated whether each</context>
</contexts>
<marker>Kawahara, Kurohashi, 2006</marker>
<rawString>Daisuke Kawahara and Sadao Kurohashi. 2006. A fully-lexicalized probabilistic model for japanese syntactic and case structure analysis. In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, HLT-NAACL ’06, pages 176–183, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samir Khuller</author>
<author>Anna Moss</author>
<author>Joseph S Naor</author>
</authors>
<title>The budgeted maximum coverage problem.</title>
<date>1999</date>
<journal>Information Processing Letters,</journal>
<volume>70</volume>
<issue>1</issue>
<contexts>
<context position="9461" citStr="Khuller et al. (1999)" startWordPosition="1554" endWordPosition="1557">so a valid subtree. The second requirement is that the union of subtrees and a single valid subtree have the same score and the same cost if they cover the same elements. We will refer to the single valid subtree as the equivalent subtree of the union of subtrees. These requirements enable us to represent sentence compression as the extraction of subtrees from a sentence. This is because the requirements guarantee that the extracted subtrees represent a sentence. 3.2 Greedy Algorithm We propose Algorithm 1 that solves the maximization problem (Eq.1). The algorithm is based on ones proposed by Khuller et al. (1999) and Krause et al. (2005). Instead of enumerating all candidate subtrees, we use a local search to extract the element that has the highest gain per cost. In the algorithm, Gi indicates a summary set obtained by adding element si to Gi−1. U means the set of subtrees that are not extracted. The algorithm iteratively adds to the current summary the element si that has the largest ratio of the objective function gain to the additional cost, unless adding it violates the budget constraint. We set a parameter r that is the scaling factor proposed by Lin and Bilmes (2010). After the loop, the algori</context>
</contexts>
<marker>Khuller, Moss, Naor, 1999</marker>
<rawString>Samir Khuller, Anna Moss, and Joseph S. Naor. 1999. The budgeted maximum coverage problem. Information Processing Letters, 70(1):39–45.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Krause</author>
<author>Carlos Guestrin</author>
</authors>
<title>A note on the budgeted maximization on submodular functions.</title>
<date>2005</date>
<tech>Technical Report CMU-CALD-05-103,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="12603" citStr="Krause and Guestrin (2005)" startWordPosition="2117" endWordPosition="2120">n, a pair (D, F) is a kind of greedoid: directed branching greedoid (Schmidt, 1991). A greedoid is a generalization of the matroid concept. However, while matroids are often used to represent constraints on submodular maximization problems (Conforti and Cornu´ejols, 1984; Calinescu et al., 2011), greedoids have not been used for that purpose, in spite of their high representation ability. To our knowledge, this is the first study that gives a constant performance guarantee for the submodular maximization under greedoid (non-matroid) constraints. the guarantee 12 (1− e−1) was already proved by Krause and Guestrin (2005). We show a counterexample. Suppose that V is { e1(density 4:cost 6), e2(density 2:cost 4), e3(density 3:cost 1), e4(density 1:cost 1) 1, and cost limit K is 10. The optimal solution is S∗ = {e1, e21. Their algorithm selects e1, e3, e4 in this order. However the algorithm selects e2 on line 4 after selecting e3, and it drops e2 on line 5. As a result, e4 selected by the algorithm does not satisfy the inequality `du E S∗\Gi−1, ρu(Gi−1) Cr u , for any i(1 &lt; i &lt; |G|), &lt; ρvi(Gi−1) Cr vi . 1025 4 Joint Model of Extraction and Compression We will formalize the unified task of sentence compression an</context>
</contexts>
<marker>Krause, Guestrin, 2005</marker>
<rawString>Andreas Krause and Carlos Guestrin. 2005. A note on the budgeted maximization on submodular functions. Technical Report CMU-CALD-05-103, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ariel Kulik</author>
<author>Hadas Shachnai</author>
<author>Tami Tamir</author>
</authors>
<title>Maximizing submodular set functions subject to multiple linear constraints.</title>
<date>2009</date>
<journal>Society for Industrial and Applied Mathematics.</journal>
<booktitle>In Proceedings of the twentieth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA ’09,</booktitle>
<pages>545--554</pages>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="3335" citStr="Kulik et al., 2009" startWordPosition="492" endWordPosition="495">for making the summary directly from all available subsentences in the documents and not in a stepwise fashion. However, there is a difficulty with such a formalization. In the past, the resulting maximization problem has been often accompanied by thousands of linear constraints representing logical relations between words. The existing greedy algorithm for solving submodular maximization problems cannot work in the presence of such numerous constraints although monotone and nonmonotone submodular maximization with constraints other than budget constraints have been studied (Lee et al., 2009; Kulik et al., 2009; Gupta et al., 2010). In this study, we avoid this difficulty by reducing the task to one of extracting dependency subtrees from sentences in the source documents. The reduction replaces the difficulty of numerous linear constraints with another difficulty wherein two subtrees can share the same word to1023 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1023–1032, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics ken when they are selected from the same sentence, and as a result, the cost of the union of the two </context>
<context position="6101" citStr="Kulik et al. (2009)" startWordPosition="961" endWordPosition="964">s, because the gain of adding a new sentence to a summary that already contains sufficient information should be small. Therefore, many studies have formalized text summarization as a submodular maximization problem (Lin and Bilmes, 2010; Lin and Bilmes, 2011; Morita et al., 2011). Their approaches, however, have been based on sentence extraction. To our knowledge, there is no study that addresses the joint task of simultaneously performing compression and extraction through an approximate submodular maximization with a performance guarantee. In the field of constrained maximization problems, Kulik et al. (2009) proposed an algorithm that solves the submodular maximization problem under multiple linear constraints with a performance guarantee 1 − e−1 in polynomial time. Although their approach can represent more flexible constraints, we cannot use their algorithm to solve our problem, because their algorithm needs to enumerate many combinations of elements. Integer linear programming (ILP) formulations can represent such flexible constraints, and they are commonly used to model text summarization (McDonald, 2007). Berg-Kirkpatrick et al. (2011) formulated a unified task of sentence extraction and sen</context>
</contexts>
<marker>Kulik, Shachnai, Tamir, 2009</marker>
<rawString>Ariel Kulik, Hadas Shachnai, and Tami Tamir. 2009. Maximizing submodular set functions subject to multiple linear constraints. In Proceedings of the twentieth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA ’09, pages 545–554, Philadelphia, PA, USA. Society for Industrial and Applied Mathematics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sadao Kurohashi</author>
<author>Daisuke Kawahara</author>
</authors>
<date>2009</date>
<booktitle>Japanese Morphological Analysis System JUMAN 6.0 Users Manual. http://nlp.ist.i. kyoto-u.ac.jp/EN/index.php?JUMAN.</booktitle>
<contexts>
<context position="26303" citStr="Kurohashi and Kawahara, 2009" startWordPosition="4554" endWordPosition="4557">e our method on Japanese QA test collections from NTCIR-7 ACLIA1 and NTCIR8 ACLIA2 (Mitamura et al., 2008; Mitamura et al., 2010). The collections contain questions and weighted answer nuggets. Our experimental settings followed the settings of (Morita et al., 2011), except for the maximum summary length. We generated summaries consisting of 140 Japanese characters or less, with the question as the query terms. We did this because our aim is to use our method in mobile situations. We used “ACLIA1 test data” to tune the parameters, and evaluated our method on “ACLIA2 test” data. We used JUMAN (Kurohashi and Kawahara, 2009a) for word segmentation and part-of-speech tagging, and we calculated idf over Mainichi newspaper articles from 1991 to 2005. For the de1028 POURPRE Precision Recall F1 F3 Lin and Bilmes (2011) 0.215 0.126 0.201 0.135 0.174 Subtree extraction (SbE) 0.268 0.238 0.213 0.159 0.190 Sentence extraction (NC) 0.278 0.206 0.215 0.139 0.183 Table 1: Results on ACLIA2 test data. pendency parsing, we used KNP (Kurohashi and Kawahara, 2009b). Since KNP internally has a flag that indicates either an “obligatory case” or an “adjacent case”, we regarded dependency relations flagged by KNP as obligatory in t</context>
</contexts>
<marker>Kurohashi, Kawahara, 2009</marker>
<rawString>Sadao Kurohashi and Daisuke Kawahara, 2009a. Japanese Morphological Analysis System JUMAN 6.0 Users Manual. http://nlp.ist.i. kyoto-u.ac.jp/EN/index.php?JUMAN.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sadao Kurohashi</author>
<author>Daisuke Kawahara</author>
</authors>
<date>2009</date>
<note>KN parser (Kurohashi-Nagao parser) 3.0 Users Manual. http://nlp.ist.i.kyoto-u.ac.jp/ EN/index.php?KNP.</note>
<contexts>
<context position="26303" citStr="Kurohashi and Kawahara, 2009" startWordPosition="4554" endWordPosition="4557">e our method on Japanese QA test collections from NTCIR-7 ACLIA1 and NTCIR8 ACLIA2 (Mitamura et al., 2008; Mitamura et al., 2010). The collections contain questions and weighted answer nuggets. Our experimental settings followed the settings of (Morita et al., 2011), except for the maximum summary length. We generated summaries consisting of 140 Japanese characters or less, with the question as the query terms. We did this because our aim is to use our method in mobile situations. We used “ACLIA1 test data” to tune the parameters, and evaluated our method on “ACLIA2 test” data. We used JUMAN (Kurohashi and Kawahara, 2009a) for word segmentation and part-of-speech tagging, and we calculated idf over Mainichi newspaper articles from 1991 to 2005. For the de1028 POURPRE Precision Recall F1 F3 Lin and Bilmes (2011) 0.215 0.126 0.201 0.135 0.174 Subtree extraction (SbE) 0.268 0.238 0.213 0.159 0.190 Sentence extraction (NC) 0.278 0.206 0.215 0.139 0.183 Table 1: Results on ACLIA2 test data. pendency parsing, we used KNP (Kurohashi and Kawahara, 2009b). Since KNP internally has a flag that indicates either an “obligatory case” or an “adjacent case”, we regarded dependency relations flagged by KNP as obligatory in t</context>
</contexts>
<marker>Kurohashi, Kawahara, 2009</marker>
<rawString>Sadao Kurohashi and Daisuke Kawahara, 2009b. KN parser (Kurohashi-Nagao parser) 3.0 Users Manual. http://nlp.ist.i.kyoto-u.ac.jp/ EN/index.php?KNP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jon Lee</author>
<author>Vahab S Mirrokni</author>
<author>Viswanath Nagarajan</author>
<author>Maxim Sviridenko</author>
</authors>
<title>Non-monotone submodular maximization under matroid and knapsack constraints.</title>
<date>2009</date>
<booktitle>In Proceedings of the 41st annual ACM symposium on Theory of computing, STOC ’09,</booktitle>
<pages>323--332</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="3315" citStr="Lee et al., 2009" startWordPosition="488" endWordPosition="491">ract subsentences for making the summary directly from all available subsentences in the documents and not in a stepwise fashion. However, there is a difficulty with such a formalization. In the past, the resulting maximization problem has been often accompanied by thousands of linear constraints representing logical relations between words. The existing greedy algorithm for solving submodular maximization problems cannot work in the presence of such numerous constraints although monotone and nonmonotone submodular maximization with constraints other than budget constraints have been studied (Lee et al., 2009; Kulik et al., 2009; Gupta et al., 2010). In this study, we avoid this difficulty by reducing the task to one of extracting dependency subtrees from sentences in the source documents. The reduction replaces the difficulty of numerous linear constraints with another difficulty wherein two subtrees can share the same word to1023 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1023–1032, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics ken when they are selected from the same sentence, and as a result, the cost of t</context>
</contexts>
<marker>Lee, Mirrokni, Nagarajan, Sviridenko, 2009</marker>
<rawString>Jon Lee, Vahab S. Mirrokni, Viswanath Nagarajan, and Maxim Sviridenko. 2009. Non-monotone submodular maximization under matroid and knapsack constraints. In Proceedings of the 41st annual ACM symposium on Theory of computing, STOC ’09, pages 323–332, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Lin</author>
<author>Jeff Bilmes</author>
</authors>
<title>Multi-document summarization via budgeted maximization of submodular functions.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10,</booktitle>
<pages>912--920</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2313" citStr="Lin and Bilmes, 2010" startWordPosition="334" endWordPosition="337">ple, when we compress sentences first, the compressed sentences may fail to contain important pieces of information due to the length limit imposed on each sentence. On the other hand, when we extract sentences first, an important sentence may fail to be selected, simply because it is long. Enumerating a huge number of compressed sentences is also infeasible. Joint models can prune unimportant or redundant descriptions without resorting to enumeration. Meanwhile, submodular maximization has recently been applied to the text summarization task, and the methods thereof have performed very well (Lin and Bilmes, 2010; Lin and Bilmes, 2011; Morita et al., 2011). Formalizing summarization as a submodular maximization problem has an important benefit inthat the problem can be solved by using a greedy algorithm with a performance guarantee. We therefore decided to formalize the task of simultaneously performing sentence extraction and compression as a submodular maximization problem. That is, we extract subsentences for making the summary directly from all available subsentences in the documents and not in a stepwise fashion. However, there is a difficulty with such a formalization. In the past, the resulting</context>
<context position="5719" citStr="Lin and Bilmes, 2010" startWordPosition="903" endWordPosition="906"> f(T), f is called submodular. This definition is equivalent to that of diminishing returns, which is well known in the field of economics: f(S U Jul) − f(S) G f(T U Jul) − f(T), where T C_ S C_ V and u is an element of V . Diminishing returns means that the value of an element u remains the same or decreases as S becomes larger. This property is suitable for summarization purposes, because the gain of adding a new sentence to a summary that already contains sufficient information should be small. Therefore, many studies have formalized text summarization as a submodular maximization problem (Lin and Bilmes, 2010; Lin and Bilmes, 2011; Morita et al., 2011). Their approaches, however, have been based on sentence extraction. To our knowledge, there is no study that addresses the joint task of simultaneously performing compression and extraction through an approximate submodular maximization with a performance guarantee. In the field of constrained maximization problems, Kulik et al. (2009) proposed an algorithm that solves the submodular maximization problem under multiple linear constraints with a performance guarantee 1 − e−1 in polynomial time. Although their approach can represent more flexible cons</context>
<context position="10033" citStr="Lin and Bilmes (2010)" startWordPosition="1659" endWordPosition="1662">s based on ones proposed by Khuller et al. (1999) and Krause et al. (2005). Instead of enumerating all candidate subtrees, we use a local search to extract the element that has the highest gain per cost. In the algorithm, Gi indicates a summary set obtained by adding element si to Gi−1. U means the set of subtrees that are not extracted. The algorithm iteratively adds to the current summary the element si that has the largest ratio of the objective function gain to the additional cost, unless adding it violates the budget constraint. We set a parameter r that is the scaling factor proposed by Lin and Bilmes (2010). After the loop, the algorithm compares Gi with the {s∗} that has the largest value of the objective function among all subtrees that are under the budget, and it outputs the summary candidate with the largest value. Let us analyze the performance guarantee of Algorithm 12. 1Each subset B corresponds to a kind of greedoid constraint. V implicitly constrains the model such that it can only select valid subtrees from a set of nodes and edges. 2Our performance guarantee is lower than that reported by Lin and Bilmes (2010). However, their proof is erroneous. In their proof of Lemma 2, they derive</context>
</contexts>
<marker>Lin, Bilmes, 2010</marker>
<rawString>Hui Lin and Jeff Bilmes. 2010. Multi-document summarization via budgeted maximization of submodular functions. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10, pages 912–920, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Lin</author>
<author>Jeff Bilmes</author>
</authors>
<title>A class of submodular functions for document summarization.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11,</booktitle>
<pages>510--520</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2335" citStr="Lin and Bilmes, 2011" startWordPosition="338" endWordPosition="341">sentences first, the compressed sentences may fail to contain important pieces of information due to the length limit imposed on each sentence. On the other hand, when we extract sentences first, an important sentence may fail to be selected, simply because it is long. Enumerating a huge number of compressed sentences is also infeasible. Joint models can prune unimportant or redundant descriptions without resorting to enumeration. Meanwhile, submodular maximization has recently been applied to the text summarization task, and the methods thereof have performed very well (Lin and Bilmes, 2010; Lin and Bilmes, 2011; Morita et al., 2011). Formalizing summarization as a submodular maximization problem has an important benefit inthat the problem can be solved by using a greedy algorithm with a performance guarantee. We therefore decided to formalize the task of simultaneously performing sentence extraction and compression as a submodular maximization problem. That is, we extract subsentences for making the summary directly from all available subsentences in the documents and not in a stepwise fashion. However, there is a difficulty with such a formalization. In the past, the resulting maximization problem </context>
<context position="5741" citStr="Lin and Bilmes, 2011" startWordPosition="907" endWordPosition="910">modular. This definition is equivalent to that of diminishing returns, which is well known in the field of economics: f(S U Jul) − f(S) G f(T U Jul) − f(T), where T C_ S C_ V and u is an element of V . Diminishing returns means that the value of an element u remains the same or decreases as S becomes larger. This property is suitable for summarization purposes, because the gain of adding a new sentence to a summary that already contains sufficient information should be small. Therefore, many studies have formalized text summarization as a submodular maximization problem (Lin and Bilmes, 2010; Lin and Bilmes, 2011; Morita et al., 2011). Their approaches, however, have been based on sentence extraction. To our knowledge, there is no study that addresses the joint task of simultaneously performing compression and extraction through an approximate submodular maximization with a performance guarantee. In the field of constrained maximization problems, Kulik et al. (2009) proposed an algorithm that solves the submodular maximization problem under multiple linear constraints with a performance guarantee 1 − e−1 in polynomial time. Although their approach can represent more flexible constraints, we cannot use</context>
<context position="26497" citStr="Lin and Bilmes (2011)" startWordPosition="4585" endWordPosition="4588">erimental settings followed the settings of (Morita et al., 2011), except for the maximum summary length. We generated summaries consisting of 140 Japanese characters or less, with the question as the query terms. We did this because our aim is to use our method in mobile situations. We used “ACLIA1 test data” to tune the parameters, and evaluated our method on “ACLIA2 test” data. We used JUMAN (Kurohashi and Kawahara, 2009a) for word segmentation and part-of-speech tagging, and we calculated idf over Mainichi newspaper articles from 1991 to 2005. For the de1028 POURPRE Precision Recall F1 F3 Lin and Bilmes (2011) 0.215 0.126 0.201 0.135 0.174 Subtree extraction (SbE) 0.268 0.238 0.213 0.159 0.190 Sentence extraction (NC) 0.278 0.206 0.215 0.139 0.183 Table 1: Results on ACLIA2 test data. pendency parsing, we used KNP (Kurohashi and Kawahara, 2009b). Since KNP internally has a flag that indicates either an “obligatory case” or an “adjacent case”, we regarded dependency relations flagged by KNP as obligatory in the sentence compression. KNP utilizes Kyoto University’s case frames (Kawahara and Kurohashi, 2006) as the resource for detecting obligatory or adjacent cases. To evaluate the summaries, we foll</context>
<context position="28176" citStr="Lin and Bilmes (2011)" startWordPosition="4856" endWordPosition="4859">s of this paper manually evaluated whether each nugget matched the summary. We also used the automatic evaluation measure, POURPRE (Lin and Demner-Fushman, 2006). POURPRE is based on word matching of reference nuggets and system outputs. We regarded as stopwords the most frequent 100 words in Mainichi articles from 1991 to 2005 (the document frequency was used to measure the frequency). We also set the threshold of nugget matching as 0.5 and binarized the nugget matching, following the previous study (Mitamura et al., 2010). We tuned the parameters by using POURPRE on the development dataset. Lin and Bilmes (2011) designed a monotone submodular function for query-oriented summarization. Their succinct method performed well in DUC from 2004 to 2007. They proposed a positive diversity reward function in order to define a monotone submodular objective function for generating a non-redundant summary. The diversity reward gives a smaller gain for a biased summary, because it consists of gains based on three clusters and calculates a square root score with respect to each sentence. The reward also contains a score for the similarity of a sentence to the query, for purposes of query-oriented summaRecall Lengt</context>
<context position="29589" citStr="Lin and Bilmes (2011)" startWordPosition="5104" endWordPosition="5107">tion based on the similarity wi,j between sentences. In the coverage function min function limits the maximum gain α Ei∈V wi,j, which is a small fraction α of the similarity between a sentence j and the all source documents. The objective function is the sum of the positive reward R and the coverage function L over the source documents V , as follows: F(S) = L(S) + Y_ L(S) = i∈V RQ,k = c∈Ck Y_ V wi,j + (1 − 0)rj,Q), � � �Y_ ( 0 Y_ j∈S∪c i∈V N where α, 0 and Ak are parameters, and rj,Q represents the similarity between sentence j and query Q. We tuned the parameters on the development dataset. Lin and Bilmes (2011) used three clusters Ck with different granularities, which were calculated in advance. We set the granularity to (0.2N, 0.15N, 0.05N) according to the settings of them, where N is the number of sentences in a document. We also regarded as stopwords “AX6 (tell),” “06 (know),” “ful (what)” and their conjugated forms, which are excessively common in questions. For the query expansion in the baseline, we used Japanese WordNet to obtain synonyms and hypernyms of query terms. 6 Results Table 1 summarizes our results. “Subtree extraction (SbE)” is our method, and “Sentence extraction (NC)” is a vers</context>
</contexts>
<marker>Lin, Bilmes, 2011</marker>
<rawString>Hui Lin and Jeff Bilmes. 2011. A class of submodular functions for document summarization. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11, pages 510–520, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jimmy Lin</author>
<author>Dina Demner-Fushman</author>
</authors>
<title>Methods for automatically evaluating answers to complex questions.</title>
<date>2006</date>
<journal>Information Retrieval,</journal>
<volume>9</volume>
<issue>5</issue>
<pages>587</pages>
<contexts>
<context position="27716" citStr="Lin and Demner-Fushman, 2006" startWordPosition="4776" endWordPosition="4779">s, we followed the practices of the TAC summarization tasks (Dang, 2008) and NTCIR ACLIA tasks, and computed pyramid-based precision with the allowance parameter, recall, and Fβ (where 0 is 1 or 3) scores. The allowance parameter was determined from the average nugget length for each question type of the ACLIA2 collection (Mitamura et al., 2010). Precision and recall are computed from the nuggets that the summary covered along with their weights. One of the authors of this paper manually evaluated whether each nugget matched the summary. We also used the automatic evaluation measure, POURPRE (Lin and Demner-Fushman, 2006). POURPRE is based on word matching of reference nuggets and system outputs. We regarded as stopwords the most frequent 100 words in Mainichi articles from 1991 to 2005 (the document frequency was used to measure the frequency). We also set the threshold of nugget matching as 0.5 and binarized the nugget matching, following the previous study (Mitamura et al., 2010). We tuned the parameters by using POURPRE on the development dataset. Lin and Bilmes (2011) designed a monotone submodular function for query-oriented summarization. Their succinct method performed well in DUC from 2004 to 2007. Th</context>
</contexts>
<marker>Lin, Demner-Fushman, 2006</marker>
<rawString>Jimmy Lin and Dina Demner-Fushman. 2006. Methods for automatically evaluating answers to complex questions. Information Retrieval, 9(5):565– 587, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e F T Martins</author>
<author>Noah A Smith</author>
</authors>
<title>Summarization with a joint model for sentence extraction and compression.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Integer Linear Programming for Natural Langauge Processing, ILP ’09,</booktitle>
<pages>1--9</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1242" citStr="Martins and Smith, 2009" startWordPosition="165" endWordPosition="168">code obligatory case constraints as must-link dependency constraints in order to guarantee the readability of the generated summary. In order to handle the subtree extraction problem, we investigate a new class of submodular maximization problem, and a new algorithm that has the approximation ratio 2(1 − e−1). Our experiments with the NTCIR ACLIA test collections show that our approach outperforms a state-of-the-art algorithm. 1 Introduction Text summarization is often addressed as a task of simultaneously performing sentence extraction and sentence compression (Berg-Kirkpatrick et al., 2011; Martins and Smith, 2009). Joint models of sentence extraction and compression have a great benefit in that they have a large degree of freedom as far as controlling redundancy goes. In contrast, conventional two-stage approaches (Zajic et al., 2006), which first generate candidate compressed sentences and then use them to generate a summary, have less computational complexity than joint models. However, two-stage approaches are suboptimal for text summarization. For example, when we compress sentences first, the compressed sentences may fail to contain important pieces of information due to the length limit imposed o</context>
</contexts>
<marker>Martins, Smith, 2009</marker>
<rawString>Andr´e F. T. Martins and Noah A. Smith. 2009. Summarization with a joint model for sentence extraction and compression. In Proceedings of the Workshop on Integer Linear Programming for Natural Langauge Processing, ILP ’09, pages 1–9, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
</authors>
<title>A study of global inference algorithms in multi-document summarization.</title>
<date>2007</date>
<booktitle>In Proceedings of the 29th European conference on IR research, ECIR’07,</booktitle>
<pages>557--564</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="6612" citStr="McDonald, 2007" startWordPosition="1039" endWordPosition="1041">ion with a performance guarantee. In the field of constrained maximization problems, Kulik et al. (2009) proposed an algorithm that solves the submodular maximization problem under multiple linear constraints with a performance guarantee 1 − e−1 in polynomial time. Although their approach can represent more flexible constraints, we cannot use their algorithm to solve our problem, because their algorithm needs to enumerate many combinations of elements. Integer linear programming (ILP) formulations can represent such flexible constraints, and they are commonly used to model text summarization (McDonald, 2007). Berg-Kirkpatrick et al. (2011) formulated a unified task of sentence extraction and sentence compression as an ILP. However, it is hard to solve large-scale ILP problems exactly in a practical amount of time. 3 Budgeted Submodular Maximization with Cost Function 3.1 Problem Definition Let V be the finite set of all valid subtrees in the source documents, where valid subtrees are defined to be the ones that can be regarded as grammatical sentences. In this paper, we regard subtrees containing the root node of the sentence as valid. Accordingly, V denotes a set of all rooted subtrees in all se</context>
</contexts>
<marker>McDonald, 2007</marker>
<rawString>Ryan McDonald. 2007. A study of global inference algorithms in multi-document summarization. In Proceedings of the 29th European conference on IR research, ECIR’07, pages 557–564, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Teruko Mitamura</author>
</authors>
<title>Eric Nyberg, Hideki Shima, Tsuneaki Kato, Tatsunori Mori, Chin-Yew Lin, Ruihua Song, Chuan-Jie Lin, Tetsuya Sakai, Donghong Ji, and Noriko Kando.</title>
<date>2008</date>
<booktitle>In Proceedings of the 7th NTCIR Workshop.</booktitle>
<marker>Mitamura, 2008</marker>
<rawString>Teruko Mitamura, Eric Nyberg, Hideki Shima, Tsuneaki Kato, Tatsunori Mori, Chin-Yew Lin, Ruihua Song, Chuan-Jie Lin, Tetsuya Sakai, Donghong Ji, and Noriko Kando. 2008. Overview of the NTCIR-7 ACLIA Tasks: Advanced Cross-Lingual Information Access. In Proceedings of the 7th NTCIR Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Teruko Mitamura</author>
<author>Hideki Shima</author>
<author>Tetsuya Sakai</author>
<author>Noriko Kando</author>
<author>Tatsunori Mori</author>
<author>Koichi Takeda</author>
<author>Chin-Yew Lin</author>
<author>Ruihua Song</author>
<author>Chuan-Jie Lin</author>
<author>Cheng-Wei Lee</author>
</authors>
<title>Overview of the ntcir-8 aclia tasks: Advanced cross-lingual information access.</title>
<date>2010</date>
<booktitle>In Proceedings of the 8th NTCIR Workshop.</booktitle>
<contexts>
<context position="25804" citStr="Mitamura et al., 2010" startWordPosition="4473" endWordPosition="4476">e enumerates the subtrees rooted at w2 in the left tree for all indices. The number in each tree node is the score of the word. all unions of combinations of the stored MDSs. There are at most (C2|O|) MDSs that the algorithm needs to store at each node. Therefore the total computational complexity is O(NC222|O|). Since it is unlikely that a sentence contains many word tokens of one type, the computational cost may not be so large in practical situations. 5 Experimental Settings We evaluate our method on Japanese QA test collections from NTCIR-7 ACLIA1 and NTCIR8 ACLIA2 (Mitamura et al., 2008; Mitamura et al., 2010). The collections contain questions and weighted answer nuggets. Our experimental settings followed the settings of (Morita et al., 2011), except for the maximum summary length. We generated summaries consisting of 140 Japanese characters or less, with the question as the query terms. We did this because our aim is to use our method in mobile situations. We used “ACLIA1 test data” to tune the parameters, and evaluated our method on “ACLIA2 test” data. We used JUMAN (Kurohashi and Kawahara, 2009a) for word segmentation and part-of-speech tagging, and we calculated idf over Mainichi newspaper ar</context>
<context position="27434" citStr="Mitamura et al., 2010" startWordPosition="4732" endWordPosition="4735">” or an “adjacent case”, we regarded dependency relations flagged by KNP as obligatory in the sentence compression. KNP utilizes Kyoto University’s case frames (Kawahara and Kurohashi, 2006) as the resource for detecting obligatory or adjacent cases. To evaluate the summaries, we followed the practices of the TAC summarization tasks (Dang, 2008) and NTCIR ACLIA tasks, and computed pyramid-based precision with the allowance parameter, recall, and Fβ (where 0 is 1 or 3) scores. The allowance parameter was determined from the average nugget length for each question type of the ACLIA2 collection (Mitamura et al., 2010). Precision and recall are computed from the nuggets that the summary covered along with their weights. One of the authors of this paper manually evaluated whether each nugget matched the summary. We also used the automatic evaluation measure, POURPRE (Lin and Demner-Fushman, 2006). POURPRE is based on word matching of reference nuggets and system outputs. We regarded as stopwords the most frequent 100 words in Mainichi articles from 1991 to 2005 (the document frequency was used to measure the frequency). We also set the threshold of nugget matching as 0.5 and binarized the nugget matching, fo</context>
</contexts>
<marker>Mitamura, Shima, Sakai, Kando, Mori, Takeda, Lin, Song, Lin, Lee, 2010</marker>
<rawString>Teruko Mitamura, Hideki Shima, Tetsuya Sakai, Noriko Kando, Tatsunori Mori, Koichi Takeda, Chin-Yew Lin, Ruihua Song, Chuan-Jie Lin, and Cheng-Wei Lee. 2010. Overview of the ntcir-8 aclia tasks: Advanced cross-lingual information access. In Proceedings of the 8th NTCIR Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hajime Morita</author>
<author>Tetsuya Sakai</author>
<author>Manabu Okumura</author>
</authors>
<title>Query snowball: a co-occurrence-based approach to multi-document summarization for question answering.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers - Volume 2, HLT ’11,</booktitle>
<pages>223--229</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2357" citStr="Morita et al., 2011" startWordPosition="342" endWordPosition="345">ompressed sentences may fail to contain important pieces of information due to the length limit imposed on each sentence. On the other hand, when we extract sentences first, an important sentence may fail to be selected, simply because it is long. Enumerating a huge number of compressed sentences is also infeasible. Joint models can prune unimportant or redundant descriptions without resorting to enumeration. Meanwhile, submodular maximization has recently been applied to the text summarization task, and the methods thereof have performed very well (Lin and Bilmes, 2010; Lin and Bilmes, 2011; Morita et al., 2011). Formalizing summarization as a submodular maximization problem has an important benefit inthat the problem can be solved by using a greedy algorithm with a performance guarantee. We therefore decided to formalize the task of simultaneously performing sentence extraction and compression as a submodular maximization problem. That is, we extract subsentences for making the summary directly from all available subsentences in the documents and not in a stepwise fashion. However, there is a difficulty with such a formalization. In the past, the resulting maximization problem has been often accompa</context>
<context position="5763" citStr="Morita et al., 2011" startWordPosition="911" endWordPosition="914">on is equivalent to that of diminishing returns, which is well known in the field of economics: f(S U Jul) − f(S) G f(T U Jul) − f(T), where T C_ S C_ V and u is an element of V . Diminishing returns means that the value of an element u remains the same or decreases as S becomes larger. This property is suitable for summarization purposes, because the gain of adding a new sentence to a summary that already contains sufficient information should be small. Therefore, many studies have formalized text summarization as a submodular maximization problem (Lin and Bilmes, 2010; Lin and Bilmes, 2011; Morita et al., 2011). Their approaches, however, have been based on sentence extraction. To our knowledge, there is no study that addresses the joint task of simultaneously performing compression and extraction through an approximate submodular maximization with a performance guarantee. In the field of constrained maximization problems, Kulik et al. (2009) proposed an algorithm that solves the submodular maximization problem under multiple linear constraints with a performance guarantee 1 − e−1 in polynomial time. Although their approach can represent more flexible constraints, we cannot use their algorithm to so</context>
<context position="17552" citStr="Morita et al., 2011" startWordPosition="2956" endWordPosition="2959">ees from sentences in order to solve the query-oriented summarization problem as a unified one consisting of sentence compression and extraction. We thus need to allocate a query relevance score to each node. Off-the-shelf similarity measures such as the cosine similarity of bag-of-words vectors with query terms would allocate scores to the terms that appear in the query, but would give no scores to terms that do not appear in it. With such a similarity, sentence compression extracts nearly only the query terms and fails to contain important information. Instead, we used Query SnowBall (QSB) (Morita et al., 2011) to calculate the query relevance score of each phrase. QSB is a method for query-oriented summarization, which calculates the similarity between query terms and each word by using cooccurrences within the source documents. Although the authors of QSB also provided scores of word pairs to avoid putting excessive penalties 1026 on word overlaps, we do not score word pairs. The score function is supermodular as a score function of subtree extraction3, because the union of two subtrees can have extra word pairs that are not included in either subtree. If the extra pair has a positive score, the s</context>
<context position="25941" citStr="Morita et al., 2011" startWordPosition="4493" endWordPosition="4496">of combinations of the stored MDSs. There are at most (C2|O|) MDSs that the algorithm needs to store at each node. Therefore the total computational complexity is O(NC222|O|). Since it is unlikely that a sentence contains many word tokens of one type, the computational cost may not be so large in practical situations. 5 Experimental Settings We evaluate our method on Japanese QA test collections from NTCIR-7 ACLIA1 and NTCIR8 ACLIA2 (Mitamura et al., 2008; Mitamura et al., 2010). The collections contain questions and weighted answer nuggets. Our experimental settings followed the settings of (Morita et al., 2011), except for the maximum summary length. We generated summaries consisting of 140 Japanese characters or less, with the question as the query terms. We did this because our aim is to use our method in mobile situations. We used “ACLIA1 test data” to tune the parameters, and evaluated our method on “ACLIA2 test” data. We used JUMAN (Kurohashi and Kawahara, 2009a) for word segmentation and part-of-speech tagging, and we calculated idf over Mainichi newspaper articles from 1991 to 2005. For the de1028 POURPRE Precision Recall F1 F3 Lin and Bilmes (2011) 0.215 0.126 0.201 0.135 0.174 Subtree extra</context>
<context position="31805" citStr="Morita et al. (2011)" startWordPosition="5480" endWordPosition="5483">ences covered 108 answer nuggets in total, and 8 of these answer nuggets were dropped by the sentence compression. Comparing the results of SbE and RC, we can see that the sentence compression caused the recall of SbE to be 7% lower than that of RC. However, the drop is relatively small in light of the fact that the sentence compression can discard 19% of the original character length with SbE. This suggests that the compression can efficiently prune words while avoiding pruning informative content. Since the summary length is short, we can select only two or three sentences for a summary. As Morita et al. (2011) mentioned, answer nuggets overlap each other. The baseline objective function R tends to extract sentences from various clusters. If the answer nuggets are present in the same cluster, the objective function does not fit the situation. However, our methods (SbE and NC) have a parameter d that can directly adjust overlap penalty with respect to word importance as well as query relevance. This may help our methods to cover similar answer nuggets. In fact, the development data resulted in a relatively high parameter d (0.8) for NC compared with 0.2 for SbE. 7 Conclusions and Future Work We forma</context>
</contexts>
<marker>Morita, Sakai, Okumura, 2011</marker>
<rawString>Hajime Morita, Tetsuya Sakai, and Manabu Okumura. 2011. Query snowball: a co-occurrence-based approach to multi-document summarization for question answering. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers - Volume 2, HLT ’11, pages 223–229, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Schmidt</author>
</authors>
<title>Greedoids and searches in directed graphs.</title>
<date>1991</date>
<journal>Discrete Mathmatics,</journal>
<volume>93</volume>
<issue>1</issue>
<contexts>
<context position="12060" citStr="Schmidt, 1991" startWordPosition="2039" endWordPosition="2040">hm 1 has a constant approximation factor when r = 1 as follows: f (Gf) ≥ ( 2 (1 − e−1)� f(S∗), (2) where S∗ is the optimal solution and, Gf is the solution obtained by Greedy Algorithm 1. Proof. See appendix. 3.3 Relation with Discrete Optimization We argue that our optimization problem can be regarded as an extraction of subtrees rooted at a given node from a directed graph, instead of from a tree. Let D be the set of edges of the directed graph, F be a subset of D that is a subtree. In the field of combinatorial optimization, a pair (D, F) is a kind of greedoid: directed branching greedoid (Schmidt, 1991). A greedoid is a generalization of the matroid concept. However, while matroids are often used to represent constraints on submodular maximization problems (Conforti and Cornu´ejols, 1984; Calinescu et al., 2011), greedoids have not been used for that purpose, in spite of their high representation ability. To our knowledge, this is the first study that gives a constant performance guarantee for the submodular maximization under greedoid (non-matroid) constraints. the guarantee 12 (1− e−1) was already proved by Krause and Guestrin (2005). We show a counterexample. Suppose that V is { e1(densit</context>
</contexts>
<marker>Schmidt, 1991</marker>
<rawString>Wolfgang Schmidt. 1991. Greedoids and searches in directed graphs. Discrete Mathmatics, 93(1):75–88, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jie Tang</author>
<author>Limin Yao</author>
<author>Dewei Chen</author>
</authors>
<title>Multitopic based query-oriented summarization.</title>
<date>2009</date>
<booktitle>In Proceedings of 2009 SIAM International Conference Data Mining (SDM’2009),</booktitle>
<pages>1147--1158</pages>
<contexts>
<context position="4807" citStr="Tang et al., 2009" startWordPosition="729" endWordPosition="732">extraction unit varies depending on what other extraction units are selected. By formalizing the subtree extraction problem as this new maximization problem, we can treat the constraints regarding the grammaticality of the compressed sentences in a straightforward way and use an arbitrary monotone submodular word score function for words including our word score function (shown later). We also propose a new greedy algorithm that solves this new class of maximization problem with a performance guarantee 12(1− e−1). We evaluated our method on by using it to perform query-oriented summarization (Tang et al., 2009). Experimental results show that it is superior to state-of-the-art methods. 2 Related Work Submodularity is formally defined as a property of a set function for a finite universe V . The function f : 2v -+ R maps a subset S C_ V to a real value. If for any S,T C_ V , f(SUT)+ f(SnT) G f(S) + f(T), f is called submodular. This definition is equivalent to that of diminishing returns, which is well known in the field of economics: f(S U Jul) − f(S) G f(T U Jul) − f(T), where T C_ S C_ V and u is an element of V . Diminishing returns means that the value of an element u remains the same or decreas</context>
</contexts>
<marker>Tang, Yao, Chen, 2009</marker>
<rawString>Jie Tang, Limin Yao, and Dewei Chen. 2009. Multitopic based query-oriented summarization. In Proceedings of 2009 SIAM International Conference Data Mining (SDM’2009), pages 1147–1158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Zajic</author>
<author>Bonnie J Dorr</author>
<author>Jimmy Lin</author>
<author>Richard Schwartz</author>
</authors>
<title>Sentence compression as a component of a multi-document summarization system.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Document Understanding Conference (DUC 2006) at NLT/NAACL</booktitle>
<contexts>
<context position="1467" citStr="Zajic et al., 2006" startWordPosition="201" endWordPosition="205">ization problem, and a new algorithm that has the approximation ratio 2(1 − e−1). Our experiments with the NTCIR ACLIA test collections show that our approach outperforms a state-of-the-art algorithm. 1 Introduction Text summarization is often addressed as a task of simultaneously performing sentence extraction and sentence compression (Berg-Kirkpatrick et al., 2011; Martins and Smith, 2009). Joint models of sentence extraction and compression have a great benefit in that they have a large degree of freedom as far as controlling redundancy goes. In contrast, conventional two-stage approaches (Zajic et al., 2006), which first generate candidate compressed sentences and then use them to generate a summary, have less computational complexity than joint models. However, two-stage approaches are suboptimal for text summarization. For example, when we compress sentences first, the compressed sentences may fail to contain important pieces of information due to the length limit imposed on each sentence. On the other hand, when we extract sentences first, an important sentence may fail to be selected, simply because it is long. Enumerating a huge number of compressed sentences is also infeasible. Joint models</context>
</contexts>
<marker>Zajic, Dorr, Lin, Schwartz, 2006</marker>
<rawString>David M. Zajic, Bonnie J. Dorr, Jimmy Lin, and Richard Schwartz. 2006. Sentence compression as a component of a multi-document summarization system. In Proceedings of the 2006 Document Understanding Conference (DUC 2006) at NLT/NAACL 2006.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>