<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.996007">
Evaluation of Features for Sentence Extraction
on Different Types of Corpora
</title>
<author confidence="0.66298">
Chikashi Nobata†, Satoshi Sekine‡ and Hitoshi Isahara†† Communications Research Laboratory
</author>
<address confidence="0.57295">
3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0289, Japan
{nova, isahara}@crl.go.jp
‡ Computer Science Department, New York University
715 Broadway, 7th floor, New York, NY 10003, USA
</address>
<email confidence="0.998866">
sekine@cs.nyu.edu
</email>
<sectionHeader confidence="0.995638" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999913789473684">
We report evaluation results for our sum-
marization system and analyze the result-
ing summarization data for three differ-
ent types of corpora. To develop a ro-
bust summarization system, we have cre-
ated a system based on sentence extraction
and applied it to summarize Japanese and
English newspaper articles, obtained some
of the top results at two evaluation work-
shops. We have also created sentence ex-
traction data from Japanese lectures and
evaluated our system with these data. In
addition to the evaluation results, we an-
alyze the relationships between key sen-
tences and the features used in sentence
extraction. We find that discrete combi-
nations of features match distributions of
key sentences better than sequential com-
binations.
</bodyText>
<sectionHeader confidence="0.999337" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999954088235294">
Our ultimate goal is to create a robust summariza-
tion system that can handle different types of docu-
ments in a uniform way. To achieve this goal, we
have developed a summarization system based on
sentence extraction. We have participated in eval-
uation workshops on automatic summarization for
both Japanese and English written corpora. We have
also evaluated the performance of the sentence ex-
traction system for Japanese lectures. At both work-
shops we obtained some of the top results, and for
the speech corpus we obtained results comparable
with those for the written corpora. This means that
the features we use are worth analyzing.
Sentence extraction is one of the main methods
required for a summarization system to reduce the
size of a document. Edmundson (1969) proposed a
method of integrating several features, such as the
positions of sentences and the frequencies of words
in an article, in order to extract sentences. He man-
ually assigned parameter values to integrate features
for estimating the significance scores of sentences.
On the other hand, machine learning methods can
also be applied to integrate features. For sentence
extraction from training data, Kupiec et al. (1995)
and Aone et al. (1998) used Bayes’ rule, Lin (1999)
and Nomoto and Matsumoto (1997) generated a de-
cision tree, and Hirao et al. (2002) generated an
SVM.
In this paper, we not only show evaluation results
for our sentence extraction system using combina-
tions of features but also analyze the features for dif-
ferent types of corpora. The analysis gives us some
indication about how to use these features and how
to combine them.
</bodyText>
<sectionHeader confidence="0.949899" genericHeader="method">
2 Summarization data
</sectionHeader>
<bodyText confidence="0.999955333333333">
The summarization data we used for this research
were prepared from Japanese newspaper articles,
Japanese lectures, and English newspaper articles.
By using these three types of data, we could com-
pare two languages and also two different types of
corpora, a written corpus and a speech corpus.
</bodyText>
<subsectionHeader confidence="0.8521325">
2.1 Summarization data from Japanese
newspaper articles
</subsectionHeader>
<bodyText confidence="0.999901791666667">
Text Summarization Challenge (TSC) is an evalua-
tion workshop for automatic summarization, which
is run by the National Institute of Informatics in
Japan (TSC, 2001). Three tasks were presented at
TSC-2001: extracting important sentences, creating
summaries to be compared with summaries prepared
by humans, and creating summaries for informa-
tion retrieval. We focus on the first task here, i.e.,
the sentence extraction task. At TSC-2001, a dry
run and a formal run were performed. The dry run
data consisted of 30 newspaper articles and manu-
ally created summaries of each. The formal run data
consisted of another 30 pairs of articles and sum-
maries. The average number of sentences per article
was 28.5 (1709 sentences / 60 articles). The news-
paper articles included 15 editorials and 15 news re-
ports in both data sets. The summaries were created
from extracted sentences with three compression ra-
tios (10%, 30%, and 50%). In our analysis, we used
the extraction data for the 10% compression ratio.
In the following sections, we call these summa-
rization data the “TSC data”. We use the TSC data
as an example of a Japanese written corpus to eval-
uate the performance of sentence extraction.
</bodyText>
<subsectionHeader confidence="0.9944305">
2.2 Summarization data from Japanese
lectures
</subsectionHeader>
<bodyText confidence="0.999922304347826">
The speech corpus we used for this experiment
is part of the Corpus of Spontaneous Japanese
(CSJ) (Maekawa et al., 2000), which is being cre-
ated by NIJLA, TITech, and CRL as an ongoing
joint project. The CSJ is a large collection of mono-
logues, such as lectures, and it includes transcrip-
tions of each speech as well as the voice data. We
selected 60 transcriptions from the CSJ for both sen-
tence segmentation and sentence extraction. Since
these transcription data do not have sentence bound-
aries, sentence segmentation is necessary before
sentence extraction. Three annotators manually gen-
erated sentence segmentation and summarization re-
sults. The target compression ratio was set to 10%.
The results of sentence segmentation were unified
to form the key data, and the average number of
sentences was 68.7 (4123 sentences / 60 speeches).
The results of sentence extraction, however, were
not unified, but were used separately for evaluation.
In the following sections, we call these summa-
rization data the “CSJ data”. We use the CSJ data as
an example of a Japanese speech corpus to evaluate
the performance of sentence extraction.
</bodyText>
<subsectionHeader confidence="0.995013">
2.3 Summarization data from English
newspaper articles
</subsectionHeader>
<bodyText confidence="0.99977688">
Document Understanding Conference (DUC) is an
evaluation workshop in the U.S. for automatic sum-
marization, which is sponsored by TIDES of the
DARPA program and run by NIST (DUC, 2001).
At DUC-2001, there were two types of tasks:
single-document summarization (SDS) and multi-
document summarization (MDS). The organizers of
DUC-2001 provided 30 sets of documents for a dry
run and another 30 sets for a formal run. These data
were shared by both the SDS and MDS tasks, and
the average number of sentences was 42.5 (25779
sentences / 607 articles). Each document set had a
topic, such as “Hurricane Andrew” or “Police Mis-
conduct”, and contained around 10 documents rele-
vant to the topic. We focus on the SDS task here, for
which the size of each summary output was set to
100 words. Model summaries for the articles were
also created by hand and provided. Since these sum-
maries were abstracts, we created sentence extrac-
tion data from the abstracts by word-based compar-
ison.
In the following sections, we call these summa-
rization data the “DUC data”. We use the DUC data
as an example of an English written corpus to eval-
uate the performance of sentence extraction.
</bodyText>
<sectionHeader confidence="0.924543" genericHeader="method">
3 Overview of our sentence extraction
system
</sectionHeader>
<bodyText confidence="0.998217727272727">
In this section, we give an overview of our sentence
extraction system, which uses multiple components.
For each sentence, each component outputs a score.
The system then combines these independent scores
by interpolation. Some components have more than
one scoring function, using various features. The
weights and function types used are decided by op-
timizing the performance of the system on training
data.
Our system includes parts that are either common
to the TSC, CSJ, and DUC data or specific to one of
</bodyText>
<listItem confidence="0.55141">
these data sets. We stipulate which parts are specific.
</listItem>
<subsectionHeader confidence="0.983685">
3.1 Features for sentence extraction
3.1.1 Sentence position
</subsectionHeader>
<bodyText confidence="0.99790275">
We implemented three functions for sentence po-
sition. The first function returns 1 if the position of
the sentence is within a given threshold N from the
beginning, and returns 0 otherwise:
</bodyText>
<equation confidence="0.9760855">
P1. Scorepst(Si)(1 &lt; i &lt; n) = 1(if i &lt; N)
= 0(otherwise)
</equation>
<bodyText confidence="0.938130166666667">
The threshold N is determined by the number of
words in the summary.
The second function isthe reciprocal of the po-
sition of the sentence, i.e., the score is highest for
the first sentence, gradually decreases, and goes to a
minimum at the final sentence:
</bodyText>
<equation confidence="0.9976165">
1
P2. Scorepst(Si) = i
</equation>
<bodyText confidence="0.998958166666667">
These first two functions are based on the hypoth-
esis that the sentences at the beginning of an article
are more important than those in the remaining part.
The third function is the maximum of the recipro-
cal of the position from either the beginning or the
end of the document:
</bodyText>
<equation confidence="0.7605325">
P3. Scorepst(Si) = max(1 1 )
i , n − i + 1
</equation>
<bodyText confidence="0.999125666666667">
This method is based on the hypothesis that the sen-
tences at both the beginning and the end of an article
are more important than those in the middle.
</bodyText>
<subsectionHeader confidence="0.953108">
3.1.2 Sentence length
</subsectionHeader>
<bodyText confidence="0.9999408">
The second type of scoring function uses sen-
tence length to determine the significance of sen-
tences. We implemented three scoring functions for
sentence length. The first function only returns the
length of each sentence (Li):
</bodyText>
<equation confidence="0.646083">
L1. Scorelen(Si) = Li
</equation>
<bodyText confidence="0.950380555555556">
The second function sets the score to a negative
value as a penalty when the sentence is shorter than
a certain length (C):
L2. Scorelen(Si) = 0 (if Li &gt; C)
Li − C (otherwise)
The third function combines the above two ap-
proaches, i.e., it returns the length of a sentence that
has at least a certain length, and otherwise returns a
negative value as a penalty:
</bodyText>
<listItem confidence="0.7031205">
L3. Scorelen(Si) = Li (if Li &gt; C)
= Li − C (otherwise)
</listItem>
<bodyText confidence="0.999246">
The length of a sentence means the number of let-
ters, and based on the results of an experiment with
the training data, we set C to 20 for the TSC and
CSJ data. For the DUC data, the length of a sen-
tence means the number of words, and we set C to
10 during the training stage.
</bodyText>
<subsectionHeader confidence="0.95928">
3.1.3 Tf*idf
</subsectionHeader>
<bodyText confidence="0.999645375">
The third type of scoring function is based on term
frequency (tf) and document frequency (df). We ap-
plied three scoring functions for tf*idf, in which the
term frequencies are calculated differently. The first
function uses the raw term frequencies, while the
other two are two different ways of normalizing the
frequencies, as follows, where DN is the number of
documents given:
</bodyText>
<equation confidence="0.997328285714286">
DN
T1. tf*idf(w) = tf(w) log
df(w)
T2. tf*idf(w) = 1 log DN
tf(w) df(w)
T3. tf*idf(w) = tf(w) log DN
tf(w)+1 df(w)
</equation>
<bodyText confidence="0.999957388888889">
For the TSC and CSJ data, we only used the third
method (T3), which was reported to be effective
for the task of information retrieval (Robertson and
Walker, 1994). The target words for these functions
are nouns (excluding temporal or adverbial nouns).
For each of the nouns in a sentence, the system cal-
culates a Tf*idf score. The total score is the sig-
nificance of the sentence. The word segmentation
was generated by Juman3.61 (Kurohashi and Nagao,
1999). We used articles from the Mainichi newspa-
per in 1994 and 1995 to count document frequen-
cies.
For the DUC data, the raw term frequency (T1)
was selected during the training stage from among
the three tf*idf definitions. A list of stop words were
used to exclude functional words, and articles from
the Wall Street Journal in 1994 and 1995 were used
to count document frequencies.
</bodyText>
<sectionHeader confidence="0.718661" genericHeader="method">
3.1.4 Headline
</sectionHeader>
<bodyText confidence="0.989049625">
We used a similarity measure of the sentence to
the headline as another type of scoring function. The
basic idea is that the more words in the sentence
overlap with the words in the headline, the more im-
portant the sentence is. The function estimates the
relevance between a headline (H) and a sentence
(Si) by using the tf*idf values of the words (w) in
the headline: Table 1: Evaluation results for the TSC data.
</bodyText>
<equation confidence="0.947716583333333">
tf(w) log DN
tf(w)+1 df(w)
tf(w) log DN
tf(w)+1 df(w)
Ratio 10% 30% 50% Avg.
System 0.363 (1) 0.435 (5) 0.589 (2) 0.463 (2)
Lead 0.284 0.432 0.586 0.434
Scorehl(Si) =
�
w∈H
E
w∈H∩S{
</equation>
<bodyText confidence="0.999798">
We also evaluated another method based on this
scoring function by using only named entities (NEs)
instead of words for the TSC data and DUC data.
Only the term frequency was used for NEs, because
we judged that the document frequency for an entity
was usually quite small, thereby making the differ-
ences between entities negligible.
</bodyText>
<sectionHeader confidence="0.521801" genericHeader="method">
3.1.5 Patterns
</sectionHeader>
<bodyText confidence="0.99978175">
For the DUC data, we used dependency patterns
as a type of scoring function. These patterns were
extracted by pattern discovery during information
extraction (Sudo et al., 2001). The details of this ap-
proach are not explained here, because this feature
is not among the features we analyze in Section 5.
The definition of the function appears in (Nobata et
al., 2002).
</bodyText>
<subsectionHeader confidence="0.995141">
3.2 Optimal weight
</subsectionHeader>
<bodyText confidence="0.9994385">
Our system set weights for each scoring function in
order to calculate the total score of a sentence. The
total score (Si) is defined from the scoring functions
(Scorej()) and weights (αj) as follows:
</bodyText>
<equation confidence="0.962317">
TotalScore(Si) = � αjScorej(Si) (1)
j
</equation>
<bodyText confidence="0.999869625">
We estimated the optimal values of these weights
from the training data. After the range of each
weight was set manually, the system changed the
values of the weights within a range and summarized
the training data for each set of weights. Each score
was recorded after the weights were changed, and
the weights with the best scores were stored.
A particular scoring method was also selected in
the cases of features with more than one defined
scoring methods. We used the dry run data from
each workshop as TSC and DUC training data. For
the TSC data, since the 30 articles contained 15 ed-
itorials and 15 news reports, we estimated optimal
values separately for editorials and news reports. For
the CSJ data, we used 50 transcriptions for training
and 10 for testing, as mentioned in Section 2.2.
</bodyText>
<sectionHeader confidence="0.994587" genericHeader="evaluation">
4 Evaluation results
</sectionHeader>
<bodyText confidence="0.999661666666667">
In this section, we show our evaluation results on the
three sets of data for the sentence extraction system
described in the previous section.
</bodyText>
<subsectionHeader confidence="0.993065">
4.1 Evaluation results for the TSC data
</subsectionHeader>
<bodyText confidence="0.999978125">
Table 1 shows the evaluation results for our sys-
tem and some baseline systems on the task of sen-
tence extraction at TSC-2001. The figures in Ta-
ble 1 are values of the F-measure1. The ‘System’
column shows the performance of our system and its
rank among the nine systems that were applied to the
task, and the ‘Lead’ column shows the performance
of a baseline system which extracts as many sen-
tences as the threshold from the beginning of a doc-
ument. Since all participants could output as many
sentences as the allowed upper limit, the values of
the recall, precision, and F-measure were the same.
Our system obtained better results than the baseline
systems, especially when the compression ratio was
10%. The average performance was second among
the nine systems.
</bodyText>
<subsectionHeader confidence="0.980728">
4.2 Evaluation results for the DUC data
</subsectionHeader>
<bodyText confidence="0.999988555555556">
Table 2 shows the results of a subjective evalua-
tion in the SDS task at DUC-2001. In this subjec-
tive evaluation, assessors gave a score to each sys-
tem’s outputs, on a zero-to-four scale (where four is
the best), as compared with summaries made by hu-
mans. The figures shown are the average scores over
all documents. The ‘System’ column shows the per-
formance of our system and its rank among the 12
systems that were applied to this task. The ‘Lead’
</bodyText>
<footnote confidence="0.62947">
1The definitions of each measurement are as follows:
</footnote>
<equation confidence="0.664317666666667">
Recall (REC) = COR / GLD
Precision (PRE) = COR / SYS
F-measure = 2 * REC * PRE / (REC + PRE),
</equation>
<bodyText confidence="0.5631534">
where COR is the number of correct sentences marked by the
system, GLD is the total number of correct sentences marked
by humans, and SYS is the total number of sentences marked by
the system. After calculating these scores for each transcription,
the average is calculated as the final score.
</bodyText>
<tableCaption confidence="0.91136575">
Table 4: Contribution (weight × s.d.) of each feature
for each set of summarization data.
Table 2: Evaluation results for the DUC data (sub-
jective evaluation).
</tableCaption>
<table confidence="0.9999132">
System Lead Avg.
Grammaticality 3.711 (5) 3.236 3.580
Cohesion 3.054 (1) 2.926 2.676
Organization 3.215 (1) 3.081 2.870
Total 9.980 (1) 9.243 9.126
</table>
<tableCaption confidence="0.991918">
Table 3: Evaluation results for the CSJ data.
</tableCaption>
<table confidence="0.9994388">
Annotators
A B C Avg.
REC 0.407 0.331 0.354 0.364
PRE 0.416 0.397 0.322 0.378
F 0.411 0.359 0.334 0.368
</table>
<bodyText confidence="0.9995365">
column shows the performance of a baseline system
that always outputs the first 100 words of a given
document, while the ‘Avg.’ column shows the aver-
age for all systems. Our system ranked 5th in gram-
maticality and was ranked at the top for the other
measurements, including the total value.
</bodyText>
<subsectionHeader confidence="0.992538">
4.3 Evaluation results for the CSJ data
</subsectionHeader>
<bodyText confidence="0.999983692307692">
The evaluation results for sentence extraction with
the CSJ data are shown in Table 3. We compared the
system’s results with each annotator’s key data. As
mentioned previously, we used 50 transcriptions for
training and 10 for testing.
These results are comparable with the perfor-
mance on sentence segmentation for written doc-
uments, because the system’s performance for the
TSC data was 0.363 when the compression ratio was
set to 10%. The results of our experiments thus show
that for transcriptions, sentence extraction achieves
results comparable to those for written documents,
if the are well defined.
</bodyText>
<subsectionHeader confidence="0.999787">
4.4 Contributions of features
</subsectionHeader>
<bodyText confidence="0.999841666666667">
Table 4 shows the contribution vectors for each set
of training data. The contribution here means the
product of the optimized weight and the standard
deviation of the score for the test data. The vec-
tors were normalized so that the sum of the com-
ponents is equal to 1, and the selected function types
for the features are also shown in the table. Our sys-
tem used the NE-based headline function (HL (N))
for the DUC data and the word-based function (HL
</bodyText>
<table confidence="0.998923125">
Features TSC DUC CSJ
Editorial Report
Pst. P3. 0.446 P1. 0.254 P1. 0.691 P3. 0.055
Len. L3. 0.000 L3. 0.000 L2. 0.020 L2. 0.881
Tf*idf T3. 0.169 T3. 0.185 T1. 0.239 T3. 0.057
HL (W) 0.171 0.292 - 0.007
HL (N) 0.214 0.269 0.045 -
Pattern - - 0.005 -
</table>
<bodyText confidence="0.97649616">
(W)) for the CSJ data, and both functions for the
TSC data. The columns for the TSC data show the
contributions when the compression ratio was 10%.
We can see that the feature with the biggest con-
tribution varies among the data sets. While the posi-
tion feature was the most effective for the TSC and
DUC data, the length feature was dominant for the
CSJ data. Most of the short sentences in the lectures
were specific expressions, such as “This is the result
of the experiment.” or “Let me summarize my pre-
sentation.”. Since these sentences were not extracted
as key sentences by the annotators, it is believed that
the function giving short sentences a penalty score
matched the manual extraction results.
5 Analysis of the summarization data
In Section 4, we showed how our system, which
combines major features, has performed well as
compared with current summarization systems.
However, the evaluation results alone do not suffi-
ciently explain how such a combination of features
is effective. In this section, we investigate the corre-
lations between each pair of features. We also match
feature pairs with distributions of extracted key sen-
tences as answer summaries to find effective combi-
nation of features for sentence extraction.
</bodyText>
<subsectionHeader confidence="0.996235">
5.1 Correlation between features
</subsectionHeader>
<bodyText confidence="0.999631833333333">
Table 5 shows Spearman’s rank correlation coeffi-
cients among the four features. Significantly corre-
lated feature pairs are indicated by ‘∗’(α = 0.001).
Here, the word-based feature is used as the headline
feature. We see the following tendencies for any of
the data sets:
</bodyText>
<listItem confidence="0.9982365">
• “Position” is relatively independent of the other features.
• “Length” and “Tf*idf” have high correlation2.
</listItem>
<tableCaption confidence="0.989533">
Table 5: Rank correlation coefficients between fea-
</tableCaption>
<figure confidence="0.8339056">
tures.
TSC Report
TSC Editorial
DUC Data
CSJ Data
</figure>
<listItem confidence="0.682905">
• “TF*idf” and “Headline ” also have high correlation.
</listItem>
<bodyText confidence="0.99967375">
These results show that while combinations of these
four features enabled us to obtain good evaluation
results, as shown in Section 4, the features are not
necessarily independent of one another.
</bodyText>
<subsectionHeader confidence="0.999778">
5.2 Combination of features
</subsectionHeader>
<bodyText confidence="0.999879470588235">
Tables 6 and 7 show the distributions of extracted
key sentences as answer summaries with two pairs
of features: sentence position and the tf*idf value,
and sentence position and the headline information.
In these tables, each sentence is ranked by each of
the two feature values, and the rankings are split ev-
ery 10 percent. For example, if a sentence is ranked
in the first 10 percent by sentence position and the
last 10 percent by the tf*idf feature, the sentence be-
longs to the cell with a position rank of 0.1 and a
tf*idf rank of 1.0 in Table 6.
Each cell thus has two letters. The left letter is the
number of key sentences, and the right letter is the
ratio of key sentences to all sentences in the cell. The
left letter shows how the number of sentences differs
from the average when all the key sentences appear
equally, regardless of the feature values. Let T be
</bodyText>
<footnote confidence="0.8008645">
2Here we used equation T1 for the tf*idf feature, and the
score of each sentence was normalized with the sentence length.
Hence, the high correlation between “Length” and “Tf*idf” is
not trivial.
</footnote>
<bodyText confidence="0.803288">
the total number of key sentences, M(= T) be the
</bodyText>
<subsectionHeader confidence="0.472861">
100
</subsectionHeader>
<bodyText confidence="0.996332">
average number of key sentences in each range, and
S be the standard deviation of the number of key
sentences among all cells. The number of key sen-
tences for cell Ti,j is then categorized according to
one of the following letters:
</bodyText>
<figure confidence="0.889719">
A: Ti,j &gt; M + 2S
B: M + S &lt; Ti,j &lt; M + 2S
C: M − S &lt; Ti,j &lt; M + S
D: M − 2S &lt; Ti,j &lt; M − S
E: Ti,j &lt; M − 2S
O: Ti,j = 0
-: No sentences exist in the cell.
</figure>
<bodyText confidence="0.999758111111111">
Similarly, the right letter in a cell shows how the ra-
tio of key sentences differs from the average ratio
when all the key sentences appear equally, regard-
less of feature values. Let N be the total number
of sentences, m(= TN) be the average ratio of key
sentences, and s be the standard deviation of the ra-
tio among all cells. The ratio of key sentences for
cell ti,j is then categorized according to one of the
following letters:
</bodyText>
<figure confidence="0.683011">
a: ti,j &gt; m + 2s
b: m + s &lt; ti,j &lt; m + 2s
c: m − s &lt; ti,j &lt; m + s
d: m − 2s &lt; ti,j &lt; m − s
e: ti,j &lt; m − 2s
o: ti,j = 0
</figure>
<bodyText confidence="0.990528409090909">
-: No sentences exist in the cell.
When key sentences appear uniformly regardless of
feature values, every cell is defined as ‘Cc’. We
show both the range of the number of key sentences
and the ratio of key sentences, because both are nec-
essary to show how effectively a cell has key sen-
tences. If a cell includes many sentences, the num-
ber of key sentences can be large even though the
ratio is not. On the other hand, when the ratio of key
sentences is large and the number is not, the contri-
bution to key sentence extraction is small.
Table 6 shows the distributions of key sentences
when the features of sentence position and tf*idf
were combined. For the DUC data, both the num-
ber and ratio of key sentences were large when the
sentence position was ranked within the first 20 per-
cent and the value of the tf*idf feature was ranked
in the bottom 50 percent (i.e., Pst. ≤ 0.2, Tf*idf ≥
0.5). On the other hand, both the number and ratio
of key sentences were large for the CSJ data when
the sentence position was ranked in the last 10 per-
cent and the value of the tf*idf feature was ranked
</bodyText>
<figure confidence="0.99597484375">
Features
Position
Length
Tf*idf
Length Tf*idf Headline
0.019 -0.095 -0.139
– 0.546* 0.338*
– 0.696*
Features
Position
Length
Tf*idf
Length Tf*idf Headline
-0.047 -0.099 0.046
– 0.532* 0.289*
– 0.658*
Features
Position
Length
Tf*idf
Length Tf*idf Headline
-0.130* -0.108* -0.134*
– 0.471* 0.293*
– 0.526*
Features
Position
Length
Tf*idf
Length Tf*idf Headline
-0.092* -0.069* -0.106*
– 0.460* 0.224*
– 0.533*
</figure>
<tableCaption confidence="0.9581135">
Table 6: Distributions of key sentences based on the combination of the sentence position (Pst.) and tf*idf
features.
</tableCaption>
<table confidence="0.995832653846154">
DUC data
Tf*idf
Pst. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
0.1 Cc Cc Cc Cb Ba Ba Aa Aa Aa Aa
0.2 Cd Cc Cc Cc Cc Cc Bb Bb Bb Bb
0.3 Cd Cc Cc Cc Cc Cc Cc Cc Cc Cc
0.4 Dd Cc Cc Cc Cc Cc Cc Cc Cc Cc
0.5 Dd Cc Cc Cc Cc Cc Cc Cc Cc Cc
0.6 Dd Cc Cc Cc Cc Cc Cc Cc Cc Cc
0.7 Dd Cc Cc Cc Cc Cc Cc Cc Cc Cc
0.8 Cd Dd Cc Cc Cc Cc Cc Cc Cc Cc
0.9 Dd Dd Cc Cc Cc Cc Cc Cc Cc Cc
1.0 Dd Dd Cc Cc Cc Cc Cc Cc Cc Cc
CSJ data
Tf*idf
Pst. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
0.1 Cc Cc Cc Cc Bc Cc Ab Bb Bb Bb
0.2 Oo Oo Cc Cc Cc Cc Cc Bb Bc Cc
0.3 Oo Cc Cc Cc Cc Cc Cc Cc Cc Cc
0.4 Cc Cc Cc Cc Oo Cc Cc Cc Cc Cc
0.5 Oo Cc Oo Oo Cc Oo Cc Cc Cc Cc
0.6 Cc Cc Cc Cc Cc Cc Cc Cc Cc Cc
0.7 Oo Cc Cc Cc Cc Cc Cc Cc Cc Cc
0.8 Oo Cc Cc Oo Oo Cc Cc Cc Cc Cc
0.9 Cc Cc Cc Cc Cc Cc Cc Cc Cc Bb
1.0 Cc Cc Ba Bb Bb Aa Aa Bb Aa Aa
</table>
<bodyText confidence="0.995904921568627">
after the first 30 percent (i.e., Pst. = 1.0, Tf*idf &gt;
0.3),. When the tf*idf feature was low, the number
and ratio of key sentences were not large, regardless
of the sentence position values. These results show
that the tf*idf feature is effective when the values
are used as a filter after the sentences are ranked by
sentence position.
Table 7 shows the distributions of key sentences
with the combination of the sentence position and
headline features. About half the sentences did not
share words with the headlines and had a value of
0 for the headline feature. As a result, the cells in
the middle of the table do not have corresponding
sentences. The headline feature cannot be used as
a filter, unlike the tf*idf feature, because many key
sentences are found when the value of the headline
feature is 0. A high value of the headline feature is,
however, a good indicator of key sentences when it
is combined with the position feature. The ratio of
key sentences was large when the headline ranking
was high and the sentence was near the beginning
(at Pst. ≤ 0.2, Headline &gt; 0.7) for the DUC data.
For the CSJ data, the ratio of key sentences was also
large when the headline ranking was within the top
10 percent (Pst. = 0.1, Headline = 1.0), as well as
for the sentences near the ends of speeches.
These results indicate that the number and ratio
of key sentences sometimes vary discretely accord-
ing to the changes in feature values when features
are combined for sentence extraction. That is, the
performance of a sentence extraction system can be
improved by categorizing feature values into sev-
eral ranges and then combining ranges. While most
sentence extraction systems use sequential combi-
nations of features, as we do in our system based
on Equation 1, the performance of these systems
can possibly be improved by introducing the cat-
egorization of feature values, without adding any
new features. We have shown that discrete combi-
nations match the distributions of key sentences in
two different corpora, the DUC data and the CSJ
data. This indicates that discrete combinations of
corpora are effective across both different languages
and different types of corpora. Hirao et al. (2002)
reported the results of a sentence extraction system
using an SVM, which categorized sequential feature
values into ranges in order to make the features bi-
nary. Some effective combinations of the binary fea-
Table 7: Distributions of key sentences based on
the combination of the sentence position (Pst.) and
headline features.
</bodyText>
<figure confidence="0.998702807692307">
DUC data
Headline
Pst. 0.1 0.2–0.5 0.6 0.7 0.8 0.9 1.0
0.1 Ab -- -- Ca Ba Ba Aa
0.2 Ac -- -- Cb Cc Ca Ca
0.3 Ac -- -- Cc Cc Cb Cb
0.4 Ac -- -- Cc Cc Cc Cb
0.5 Ac -- -- Cc Cc Cc Cc
0.6 Bc -- -- Cc Cc Cc Cc
0.7 Bc -- -- Cc Cc Cc Cc
0.8 Ac -- -- Cd Cc Cc Cc
0.9 Bd -- -- Cd Cc Cc Cc
1.0 Bd -- -- Cd Cc Cc Cc
CSJ data
Headline
Pst. 0.1 0.2–0.5 0.6 0.7 0.8 0.9 1.0
0.1 Bc -- Cc Cc Bb Cc Aa
0.2 Bc -- Cc Cb Cc Cc Bb
0.3 Cc -- Cc Cc Cc Cc Cc
0.4 Cc -- Oo Cc Cc Cc Cc
0.5 Cc -- Oo Cc Oo Cc Cc
0.6 Cc -- Cc Cc Cc Cc Cc
0.7 Cc -- Oo Cc Cc Cc Cc
0.8 Cc -- Cc Cc Cc Cc Cc
0.9 Ac -- Ca Cc Cc Cc Cb
1.0 Ab -- Ca Aa Ba Ba Ba
</figure>
<bodyText confidence="0.8570715">
tures in that report also indicate the effectiveness of
discrete combinations of features.
</bodyText>
<sectionHeader confidence="0.999207" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99999648">
We have shown evaluation results for our sentence
extraction system and analyzed its features for dif-
ferent types of corpora, which included corpora dif-
fering in both language (Japanese and English) and
type (newspaper articles and lectures). The sys-
tem is based on four major features, and it achieved
some of the top results at evaluation workshops in
2001 for summarizing Japanese newspaper articles
(TSC) and English newspaper articles (DUC). For
Japanese lectures, the sentence extraction system
also obtained comparable results when the sentence
boundary was given.
Our analysis of the features used in this sentence
extraction system has shown that they are not neces-
sarily independent of one another, based on the re-
sults of their rank correlation coefficients. The anal-
ysis also indicated that the categorization of feature
values matches the distribution of key sentences bet-
ter than sequential feature values.
There are several features that were not described
here but are also used in sentence extraction sys-
tems, such as some specific lexical expressions and
syntactic information. In our future work, we will
analyze and use these features to improve the per-
formance of our sentence extraction system.
</bodyText>
<sectionHeader confidence="0.999194" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999836777777778">
C. Aone, M. E. Okurowski, and J. Gorlinsky. 1998. Train-
able, Scalable Summarization Using Robust NLP and Ma-
chine Learning. In Proc. of COLING-ACL’98, pages 62–66.
DUC. 2001. http://duc.nist.gov. Document Understanding
Conference.
H. Edmundson. 1969. New methods in automatic abstracting.
Journal ofACM, 16(2):264–285.
T. Hirao, H. Isozaki, E. Maeda, and Y. Matsumoto. 2002. Ex-
tracting Important Sentences with Support Vector Machines.
In Proc. of COLING-2002.
J. Kupiec, J. Pedersen, and F. Chen. 1995. A Trainable Docu-
ment Summarizaer. In Proc. of SIGIR’95, pages 68–73.
S. Kurohashi and M. Nagao, 1999. Japanese Morphological
Analyzing System: JUMAN version 3.61. Kyoto University.
Chin-Yew Lin. 1999. Training a selection function for extrac-
tion. In Proc. of the CIKM’99.
K. Maekawa, H. Koiso, S. Furui, and H. Isahara. 2000. Spon-
taneous Speech Corpus of Japanese. In Proc. ofLREC2000,
pages 947–952.
C. Nobata, S. Sekine, H. Isahara, and R. Grishman. 2002. Sum-
marization System Integrated with Named Entity Tagging
and IE pattern Discovery. In Proceedings of the LREC-2002
Conference, pages 1742–1745, May.
T. Nomoto and Y. Matsumoto. 1997. The Reliability of Human
Coding and Effects on Automatic Abstracting (in Japanese).
In IPSJ-NL 120-11, pages 71–76, July.
S. E. Robertson and S. Walker. 1994. Some simple effec-
tive approximations to the 2-poisson model for probabilistic
weighted retreival. In Proc. of SIGIR’94.
K. Sudo, S. Sekine, and R. Grishman. 2001. Automatic pattern
acquisition for japanese information extraction. In Proc. of
HLT-2001.
TSC. 2001. Proceedings of the Second NTCIR Workshop
on Research in Chinese &amp; Japanese Text Retrieval and Text
Summarization (NTCIR2). National Institute of Informat-
ics.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.147056">
<title confidence="0.8321345">Evaluation of Features for Sentence on Different Types of Corpora</title>
<affiliation confidence="0.644179">Satoshi Research</affiliation>
<address confidence="0.655784333333333">3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0289, Science Department, New York 715 Broadway, 7th floor, New York, NY 10003,</address>
<email confidence="0.999758">sekine@cs.nyu.edu</email>
<abstract confidence="0.9943485">We report evaluation results for our summarization system and analyze the resulting summarization data for three different types of corpora. To develop a robust summarization system, we have created a system based on sentence extraction and applied it to summarize Japanese and English newspaper articles, obtained some of the top results at two evaluation workshops. We have also created sentence extraction data from Japanese lectures and evaluated our system with these data. In addition to the evaluation results, we analyze the relationships between key sentences and the features used in sentence extraction. We find that discrete combinations of features match distributions of key sentences better than sequential combinations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Aone</author>
<author>M E Okurowski</author>
<author>J Gorlinsky</author>
</authors>
<title>Trainable, Scalable Summarization Using Robust NLP and Machine Learning.</title>
<date>1998</date>
<booktitle>In Proc. of COLING-ACL’98,</booktitle>
<pages>62--66</pages>
<contexts>
<context position="2351" citStr="Aone et al. (1998)" startWordPosition="363" endWordPosition="366">eatures we use are worth analyzing. Sentence extraction is one of the main methods required for a summarization system to reduce the size of a document. Edmundson (1969) proposed a method of integrating several features, such as the positions of sentences and the frequencies of words in an article, in order to extract sentences. He manually assigned parameter values to integrate features for estimating the significance scores of sentences. On the other hand, machine learning methods can also be applied to integrate features. For sentence extraction from training data, Kupiec et al. (1995) and Aone et al. (1998) used Bayes’ rule, Lin (1999) and Nomoto and Matsumoto (1997) generated a decision tree, and Hirao et al. (2002) generated an SVM. In this paper, we not only show evaluation results for our sentence extraction system using combinations of features but also analyze the features for different types of corpora. The analysis gives us some indication about how to use these features and how to combine them. 2 Summarization data The summarization data we used for this research were prepared from Japanese newspaper articles, Japanese lectures, and English newspaper articles. By using these three types</context>
</contexts>
<marker>Aone, Okurowski, Gorlinsky, 1998</marker>
<rawString>C. Aone, M. E. Okurowski, and J. Gorlinsky. 1998. Trainable, Scalable Summarization Using Robust NLP and Machine Learning. In Proc. of COLING-ACL’98, pages 62–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>DUC</author>
</authors>
<date>2001</date>
<note>http://duc.nist.gov. Document Understanding Conference.</note>
<contexts>
<context position="5737" citStr="DUC, 2001" startWordPosition="918" endWordPosition="919"> and the average number of sentences was 68.7 (4123 sentences / 60 speeches). The results of sentence extraction, however, were not unified, but were used separately for evaluation. In the following sections, we call these summarization data the “CSJ data”. We use the CSJ data as an example of a Japanese speech corpus to evaluate the performance of sentence extraction. 2.3 Summarization data from English newspaper articles Document Understanding Conference (DUC) is an evaluation workshop in the U.S. for automatic summarization, which is sponsored by TIDES of the DARPA program and run by NIST (DUC, 2001). At DUC-2001, there were two types of tasks: single-document summarization (SDS) and multidocument summarization (MDS). The organizers of DUC-2001 provided 30 sets of documents for a dry run and another 30 sets for a formal run. These data were shared by both the SDS and MDS tasks, and the average number of sentences was 42.5 (25779 sentences / 607 articles). Each document set had a topic, such as “Hurricane Andrew” or “Police Misconduct”, and contained around 10 documents relevant to the topic. We focus on the SDS task here, for which the size of each summary output was set to 100 words. Mod</context>
</contexts>
<marker>DUC, 2001</marker>
<rawString>DUC. 2001. http://duc.nist.gov. Document Understanding Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Edmundson</author>
</authors>
<title>New methods in automatic abstracting.</title>
<date>1969</date>
<journal>Journal ofACM,</journal>
<pages>16--2</pages>
<contexts>
<context position="1902" citStr="Edmundson (1969)" startWordPosition="294" endWordPosition="295">developed a summarization system based on sentence extraction. We have participated in evaluation workshops on automatic summarization for both Japanese and English written corpora. We have also evaluated the performance of the sentence extraction system for Japanese lectures. At both workshops we obtained some of the top results, and for the speech corpus we obtained results comparable with those for the written corpora. This means that the features we use are worth analyzing. Sentence extraction is one of the main methods required for a summarization system to reduce the size of a document. Edmundson (1969) proposed a method of integrating several features, such as the positions of sentences and the frequencies of words in an article, in order to extract sentences. He manually assigned parameter values to integrate features for estimating the significance scores of sentences. On the other hand, machine learning methods can also be applied to integrate features. For sentence extraction from training data, Kupiec et al. (1995) and Aone et al. (1998) used Bayes’ rule, Lin (1999) and Nomoto and Matsumoto (1997) generated a decision tree, and Hirao et al. (2002) generated an SVM. In this paper, we no</context>
</contexts>
<marker>Edmundson, 1969</marker>
<rawString>H. Edmundson. 1969. New methods in automatic abstracting. Journal ofACM, 16(2):264–285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hirao</author>
<author>H Isozaki</author>
<author>E Maeda</author>
<author>Y Matsumoto</author>
</authors>
<title>Extracting Important Sentences with Support Vector Machines.</title>
<date>2002</date>
<booktitle>In Proc. of COLING-2002.</booktitle>
<contexts>
<context position="2463" citStr="Hirao et al. (2002)" startWordPosition="383" endWordPosition="386"> system to reduce the size of a document. Edmundson (1969) proposed a method of integrating several features, such as the positions of sentences and the frequencies of words in an article, in order to extract sentences. He manually assigned parameter values to integrate features for estimating the significance scores of sentences. On the other hand, machine learning methods can also be applied to integrate features. For sentence extraction from training data, Kupiec et al. (1995) and Aone et al. (1998) used Bayes’ rule, Lin (1999) and Nomoto and Matsumoto (1997) generated a decision tree, and Hirao et al. (2002) generated an SVM. In this paper, we not only show evaluation results for our sentence extraction system using combinations of features but also analyze the features for different types of corpora. The analysis gives us some indication about how to use these features and how to combine them. 2 Summarization data The summarization data we used for this research were prepared from Japanese newspaper articles, Japanese lectures, and English newspaper articles. By using these three types of data, we could compare two languages and also two different types of corpora, a written corpus and a speech </context>
<context position="25837" citStr="Hirao et al. (2002)" startWordPosition="4576" endWordPosition="4579">feature values into several ranges and then combining ranges. While most sentence extraction systems use sequential combinations of features, as we do in our system based on Equation 1, the performance of these systems can possibly be improved by introducing the categorization of feature values, without adding any new features. We have shown that discrete combinations match the distributions of key sentences in two different corpora, the DUC data and the CSJ data. This indicates that discrete combinations of corpora are effective across both different languages and different types of corpora. Hirao et al. (2002) reported the results of a sentence extraction system using an SVM, which categorized sequential feature values into ranges in order to make the features binary. Some effective combinations of the binary feaTable 7: Distributions of key sentences based on the combination of the sentence position (Pst.) and headline features. DUC data Headline Pst. 0.1 0.2–0.5 0.6 0.7 0.8 0.9 1.0 0.1 Ab -- -- Ca Ba Ba Aa 0.2 Ac -- -- Cb Cc Ca Ca 0.3 Ac -- -- Cc Cc Cb Cb 0.4 Ac -- -- Cc Cc Cc Cb 0.5 Ac -- -- Cc Cc Cc Cc 0.6 Bc -- -- Cc Cc Cc Cc 0.7 Bc -- -- Cc Cc Cc Cc 0.8 Ac -- -- Cd Cc Cc Cc 0.9 Bd -- -- Cd Cc</context>
</contexts>
<marker>Hirao, Isozaki, Maeda, Matsumoto, 2002</marker>
<rawString>T. Hirao, H. Isozaki, E. Maeda, and Y. Matsumoto. 2002. Extracting Important Sentences with Support Vector Machines. In Proc. of COLING-2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kupiec</author>
<author>J Pedersen</author>
<author>F Chen</author>
</authors>
<title>A Trainable Document Summarizaer.</title>
<date>1995</date>
<booktitle>In Proc. of SIGIR’95,</booktitle>
<pages>68--73</pages>
<contexts>
<context position="2328" citStr="Kupiec et al. (1995)" startWordPosition="358" endWordPosition="361">ra. This means that the features we use are worth analyzing. Sentence extraction is one of the main methods required for a summarization system to reduce the size of a document. Edmundson (1969) proposed a method of integrating several features, such as the positions of sentences and the frequencies of words in an article, in order to extract sentences. He manually assigned parameter values to integrate features for estimating the significance scores of sentences. On the other hand, machine learning methods can also be applied to integrate features. For sentence extraction from training data, Kupiec et al. (1995) and Aone et al. (1998) used Bayes’ rule, Lin (1999) and Nomoto and Matsumoto (1997) generated a decision tree, and Hirao et al. (2002) generated an SVM. In this paper, we not only show evaluation results for our sentence extraction system using combinations of features but also analyze the features for different types of corpora. The analysis gives us some indication about how to use these features and how to combine them. 2 Summarization data The summarization data we used for this research were prepared from Japanese newspaper articles, Japanese lectures, and English newspaper articles. By </context>
</contexts>
<marker>Kupiec, Pedersen, Chen, 1995</marker>
<rawString>J. Kupiec, J. Pedersen, and F. Chen. 1995. A Trainable Document Summarizaer. In Proc. of SIGIR’95, pages 68–73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kurohashi</author>
<author>M Nagao</author>
</authors>
<title>Japanese Morphological Analyzing System: JUMAN version 3.61.</title>
<date>1999</date>
<institution>Kyoto University.</institution>
<contexts>
<context position="10319" citStr="Kurohashi and Nagao, 1999" startWordPosition="1729" endWordPosition="1732">s follows, where DN is the number of documents given: DN T1. tf*idf(w) = tf(w) log df(w) T2. tf*idf(w) = 1 log DN tf(w) df(w) T3. tf*idf(w) = tf(w) log DN tf(w)+1 df(w) For the TSC and CSJ data, we only used the third method (T3), which was reported to be effective for the task of information retrieval (Robertson and Walker, 1994). The target words for these functions are nouns (excluding temporal or adverbial nouns). For each of the nouns in a sentence, the system calculates a Tf*idf score. The total score is the significance of the sentence. The word segmentation was generated by Juman3.61 (Kurohashi and Nagao, 1999). We used articles from the Mainichi newspaper in 1994 and 1995 to count document frequencies. For the DUC data, the raw term frequency (T1) was selected during the training stage from among the three tf*idf definitions. A list of stop words were used to exclude functional words, and articles from the Wall Street Journal in 1994 and 1995 were used to count document frequencies. 3.1.4 Headline We used a similarity measure of the sentence to the headline as another type of scoring function. The basic idea is that the more words in the sentence overlap with the words in the headline, the more imp</context>
</contexts>
<marker>Kurohashi, Nagao, 1999</marker>
<rawString>S. Kurohashi and M. Nagao, 1999. Japanese Morphological Analyzing System: JUMAN version 3.61. Kyoto University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>Training a selection function for extraction.</title>
<date>1999</date>
<booktitle>In Proc. of the CIKM’99.</booktitle>
<contexts>
<context position="2380" citStr="Lin (1999)" startWordPosition="370" endWordPosition="371">entence extraction is one of the main methods required for a summarization system to reduce the size of a document. Edmundson (1969) proposed a method of integrating several features, such as the positions of sentences and the frequencies of words in an article, in order to extract sentences. He manually assigned parameter values to integrate features for estimating the significance scores of sentences. On the other hand, machine learning methods can also be applied to integrate features. For sentence extraction from training data, Kupiec et al. (1995) and Aone et al. (1998) used Bayes’ rule, Lin (1999) and Nomoto and Matsumoto (1997) generated a decision tree, and Hirao et al. (2002) generated an SVM. In this paper, we not only show evaluation results for our sentence extraction system using combinations of features but also analyze the features for different types of corpora. The analysis gives us some indication about how to use these features and how to combine them. 2 Summarization data The summarization data we used for this research were prepared from Japanese newspaper articles, Japanese lectures, and English newspaper articles. By using these three types of data, we could compare tw</context>
</contexts>
<marker>Lin, 1999</marker>
<rawString>Chin-Yew Lin. 1999. Training a selection function for extraction. In Proc. of the CIKM’99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Maekawa</author>
<author>H Koiso</author>
<author>S Furui</author>
<author>H Isahara</author>
</authors>
<title>Spontaneous Speech Corpus of Japanese. In</title>
<date>2000</date>
<booktitle>Proc. ofLREC2000,</booktitle>
<pages>947--952</pages>
<contexts>
<context position="4484" citStr="Maekawa et al., 2000" startWordPosition="714" endWordPosition="717">The newspaper articles included 15 editorials and 15 news reports in both data sets. The summaries were created from extracted sentences with three compression ratios (10%, 30%, and 50%). In our analysis, we used the extraction data for the 10% compression ratio. In the following sections, we call these summarization data the “TSC data”. We use the TSC data as an example of a Japanese written corpus to evaluate the performance of sentence extraction. 2.2 Summarization data from Japanese lectures The speech corpus we used for this experiment is part of the Corpus of Spontaneous Japanese (CSJ) (Maekawa et al., 2000), which is being created by NIJLA, TITech, and CRL as an ongoing joint project. The CSJ is a large collection of monologues, such as lectures, and it includes transcriptions of each speech as well as the voice data. We selected 60 transcriptions from the CSJ for both sentence segmentation and sentence extraction. Since these transcription data do not have sentence boundaries, sentence segmentation is necessary before sentence extraction. Three annotators manually generated sentence segmentation and summarization results. The target compression ratio was set to 10%. The results of sentence segm</context>
</contexts>
<marker>Maekawa, Koiso, Furui, Isahara, 2000</marker>
<rawString>K. Maekawa, H. Koiso, S. Furui, and H. Isahara. 2000. Spontaneous Speech Corpus of Japanese. In Proc. ofLREC2000, pages 947–952.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Nobata</author>
<author>S Sekine</author>
<author>H Isahara</author>
<author>R Grishman</author>
</authors>
<date>2002</date>
<booktitle>Summarization System Integrated with Named Entity Tagging and IE pattern Discovery. In Proceedings of the LREC-2002 Conference,</booktitle>
<pages>1742--1745</pages>
<contexts>
<context position="12028" citStr="Nobata et al., 2002" startWordPosition="2027" endWordPosition="2030">s) instead of words for the TSC data and DUC data. Only the term frequency was used for NEs, because we judged that the document frequency for an entity was usually quite small, thereby making the differences between entities negligible. 3.1.5 Patterns For the DUC data, we used dependency patterns as a type of scoring function. These patterns were extracted by pattern discovery during information extraction (Sudo et al., 2001). The details of this approach are not explained here, because this feature is not among the features we analyze in Section 5. The definition of the function appears in (Nobata et al., 2002). 3.2 Optimal weight Our system set weights for each scoring function in order to calculate the total score of a sentence. The total score (Si) is defined from the scoring functions (Scorej()) and weights (αj) as follows: TotalScore(Si) = � αjScorej(Si) (1) j We estimated the optimal values of these weights from the training data. After the range of each weight was set manually, the system changed the values of the weights within a range and summarized the training data for each set of weights. Each score was recorded after the weights were changed, and the weights with the best scores were st</context>
</contexts>
<marker>Nobata, Sekine, Isahara, Grishman, 2002</marker>
<rawString>C. Nobata, S. Sekine, H. Isahara, and R. Grishman. 2002. Summarization System Integrated with Named Entity Tagging and IE pattern Discovery. In Proceedings of the LREC-2002 Conference, pages 1742–1745, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Nomoto</author>
<author>Y Matsumoto</author>
</authors>
<title>The Reliability of Human Coding and Effects on Automatic Abstracting (in Japanese).</title>
<date>1997</date>
<booktitle>In IPSJ-NL 120-11,</booktitle>
<pages>71--76</pages>
<contexts>
<context position="2412" citStr="Nomoto and Matsumoto (1997)" startWordPosition="373" endWordPosition="376">ion is one of the main methods required for a summarization system to reduce the size of a document. Edmundson (1969) proposed a method of integrating several features, such as the positions of sentences and the frequencies of words in an article, in order to extract sentences. He manually assigned parameter values to integrate features for estimating the significance scores of sentences. On the other hand, machine learning methods can also be applied to integrate features. For sentence extraction from training data, Kupiec et al. (1995) and Aone et al. (1998) used Bayes’ rule, Lin (1999) and Nomoto and Matsumoto (1997) generated a decision tree, and Hirao et al. (2002) generated an SVM. In this paper, we not only show evaluation results for our sentence extraction system using combinations of features but also analyze the features for different types of corpora. The analysis gives us some indication about how to use these features and how to combine them. 2 Summarization data The summarization data we used for this research were prepared from Japanese newspaper articles, Japanese lectures, and English newspaper articles. By using these three types of data, we could compare two languages and also two differe</context>
</contexts>
<marker>Nomoto, Matsumoto, 1997</marker>
<rawString>T. Nomoto and Y. Matsumoto. 1997. The Reliability of Human Coding and Effects on Automatic Abstracting (in Japanese). In IPSJ-NL 120-11, pages 71–76, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Robertson</author>
<author>S Walker</author>
</authors>
<title>Some simple effective approximations to the 2-poisson model for probabilistic weighted retreival.</title>
<date>1994</date>
<booktitle>In Proc. of SIGIR’94.</booktitle>
<contexts>
<context position="10025" citStr="Robertson and Walker, 1994" startWordPosition="1680" endWordPosition="1683">on is based on term frequency (tf) and document frequency (df). We applied three scoring functions for tf*idf, in which the term frequencies are calculated differently. The first function uses the raw term frequencies, while the other two are two different ways of normalizing the frequencies, as follows, where DN is the number of documents given: DN T1. tf*idf(w) = tf(w) log df(w) T2. tf*idf(w) = 1 log DN tf(w) df(w) T3. tf*idf(w) = tf(w) log DN tf(w)+1 df(w) For the TSC and CSJ data, we only used the third method (T3), which was reported to be effective for the task of information retrieval (Robertson and Walker, 1994). The target words for these functions are nouns (excluding temporal or adverbial nouns). For each of the nouns in a sentence, the system calculates a Tf*idf score. The total score is the significance of the sentence. The word segmentation was generated by Juman3.61 (Kurohashi and Nagao, 1999). We used articles from the Mainichi newspaper in 1994 and 1995 to count document frequencies. For the DUC data, the raw term frequency (T1) was selected during the training stage from among the three tf*idf definitions. A list of stop words were used to exclude functional words, and articles from the Wal</context>
</contexts>
<marker>Robertson, Walker, 1994</marker>
<rawString>S. E. Robertson and S. Walker. 1994. Some simple effective approximations to the 2-poisson model for probabilistic weighted retreival. In Proc. of SIGIR’94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sudo</author>
<author>S Sekine</author>
<author>R Grishman</author>
</authors>
<title>Automatic pattern acquisition for japanese information extraction.</title>
<date>2001</date>
<booktitle>In Proc. of HLT-2001.</booktitle>
<contexts>
<context position="11838" citStr="Sudo et al., 2001" startWordPosition="1993" endWordPosition="1996">(1) 0.435 (5) 0.589 (2) 0.463 (2) Lead 0.284 0.432 0.586 0.434 Scorehl(Si) = � w∈H E w∈H∩S{ We also evaluated another method based on this scoring function by using only named entities (NEs) instead of words for the TSC data and DUC data. Only the term frequency was used for NEs, because we judged that the document frequency for an entity was usually quite small, thereby making the differences between entities negligible. 3.1.5 Patterns For the DUC data, we used dependency patterns as a type of scoring function. These patterns were extracted by pattern discovery during information extraction (Sudo et al., 2001). The details of this approach are not explained here, because this feature is not among the features we analyze in Section 5. The definition of the function appears in (Nobata et al., 2002). 3.2 Optimal weight Our system set weights for each scoring function in order to calculate the total score of a sentence. The total score (Si) is defined from the scoring functions (Scorej()) and weights (αj) as follows: TotalScore(Si) = � αjScorej(Si) (1) j We estimated the optimal values of these weights from the training data. After the range of each weight was set manually, the system changed the value</context>
</contexts>
<marker>Sudo, Sekine, Grishman, 2001</marker>
<rawString>K. Sudo, S. Sekine, and R. Grishman. 2001. Automatic pattern acquisition for japanese information extraction. In Proc. of HLT-2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>TSC</author>
</authors>
<date>2001</date>
<booktitle>Proceedings of the Second NTCIR Workshop on Research in Chinese &amp; Japanese Text Retrieval and Text Summarization (NTCIR2). National Institute of Informatics.</booktitle>
<contexts>
<context position="3291" citStr="TSC, 2001" startWordPosition="517" endWordPosition="518">s some indication about how to use these features and how to combine them. 2 Summarization data The summarization data we used for this research were prepared from Japanese newspaper articles, Japanese lectures, and English newspaper articles. By using these three types of data, we could compare two languages and also two different types of corpora, a written corpus and a speech corpus. 2.1 Summarization data from Japanese newspaper articles Text Summarization Challenge (TSC) is an evaluation workshop for automatic summarization, which is run by the National Institute of Informatics in Japan (TSC, 2001). Three tasks were presented at TSC-2001: extracting important sentences, creating summaries to be compared with summaries prepared by humans, and creating summaries for information retrieval. We focus on the first task here, i.e., the sentence extraction task. At TSC-2001, a dry run and a formal run were performed. The dry run data consisted of 30 newspaper articles and manually created summaries of each. The formal run data consisted of another 30 pairs of articles and summaries. The average number of sentences per article was 28.5 (1709 sentences / 60 articles). The newspaper articles inclu</context>
</contexts>
<marker>TSC, 2001</marker>
<rawString>TSC. 2001. Proceedings of the Second NTCIR Workshop on Research in Chinese &amp; Japanese Text Retrieval and Text Summarization (NTCIR2). National Institute of Informatics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>