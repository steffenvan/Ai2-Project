<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9937405">
Tagging Inflective Languages: Prediction of Morphological
Categories for a Rich, Structured Tagset
</title>
<author confidence="0.883686">
Jan Hajie and Barbora Hladka
</author>
<affiliation confidence="0.558786333333333">
Institute of Formal and Applied Linguistics MFF UK
Charles University, Prague, Czech Republic
Ihajic,hladkal©ufal.mfficuni.cz
</affiliation>
<sectionHeader confidence="0.72557" genericHeader="abstract">
Abstrakt (aesky)
</sectionHeader>
<bodyText confidence="0.781189333333333">
{This short abstract is in Czech. For illustration
purposes, it has been tagged by our tagger; errors
are printed underlined and corrections are shown.}
</bodyText>
<note confidence="0.7631665625">
Hlavnim/AAIS7----1A--
problemem/NNIS7 A--
pi/RR--6
morfologickem/AANS6----1A--
znaekovani/NNNS6 A-
(/Z:
nekdy/Db
teZ/Db
zvanem/AAIS6 1A-- Correct: N
morfologicko/A2
syntakticke/AAIP1----1A-- Correct: NS
)/Z:
jazykii/NNIP2 A-
s/RR -7
bohatou/AAFS7----1A--
flexi/NNFS7 A
</note>
<keyword confidence="0.7797885">
,/Z:
jako/J,
je/VB-S---3P-AA-
napiiklad/Db
eekina/NNFS1 A-
nebo/J-
ruStina/NNFS1 A-
,/Z:
je/VB-S---3P-AA-
-/Z:
pii/RR 6
omezene/AAFS6----1A--
</keyword>
<note confidence="0.515434393939394">
velikosti/NNFS2 A- Correct: 6
zdrojfi/NNIP2 A
-/Z:
poeet/NNIS1 A-
mo.214ch/AAFP2----1A--
znat&apos;ek/NNFP2 A-
kterk/P4YS1
jde/VB-S---3P-AA-
obvykle/Dg 1A--
do/RR--2
tisicii/NNIP2 A-
Na§e/PSHS1-P1
metoda/NNFS1 A-
piitom/Db
vyuilva/VB-S---3P-AA-
exponencialniho/AAIS2----1A--
pravdepodobnostniho/AAIS2----1A--
modelu/NNIS2 A-
zaloieneho/AAIS2----1A--
na/RR -6
automaticky/Dg 1A--
vybranSrch/AAMP6----1A--
rysech/NNIP6 A-
Parametry/NNIP1 A-
tohoto/PDZS2
modelu/NNIS2 A
se/P7-X4
poeftajf/VB-P---3P-AA-
pomoci/NNFS7
j ednoduchkch/AA IP 2---- 1A- -
odhadii/NNIP2 A
(/Z.
trenink/NNIS1 A-
</note>
<bodyText confidence="0.94280325">
je/VB-S---3P-AA-
tak/Db
mnohem/Db
rychlej§f/AAFS1----2A
ne2/J,
kdybychom/J,-P 1
pouiili/VpMP---XR-AA-
metodu/NNFS4 A-
maximalni/AAFS1----1A--
entropie/NNFS2 A
)/Z.
a/J&amp;quot;
pfitom/Db
se/P7-X4
piimo/Dg 1A--
minimalizuje/VB-S---3P-AA-
</bodyText>
<sectionHeader confidence="0.4541365" genericHeader="keywords">
poeet/NNIS1 A
chyb/NNFP2 A--
</sectionHeader>
<figure confidence="0.486453">
Correct: I
Correct: 2
Correct: 1
Correct: I
Correct: RR--2,-
</figure>
<page confidence="0.998804">
483
</page>
<sectionHeader confidence="0.987523" genericHeader="method">
Abstract
</sectionHeader>
<bodyText confidence="0.999963388888889">
The major obstacle in morphological (sometimes
called morpho-syntactic, or extended POS) tagging
of highly inflective languages, such as Czech or Rus-
sian, is — given the resources possibly available — the
tagset size. Typically, it is in the order of thou-
sands. Our method uses an exponential probabilis-
tic model based on automatically selected features.
The parameters of the model are computed using
simple estimates (which makes training much faster
than when one uses Maximum Entropy) to directly
minimize the error rate on training data.
The results obtained so far not only show good
performance on disambiguation of most of the indi-
vidual morphological categories, but they also show
a significant improvement on the overall prediction
of the resulting combined tag over a HMM-based tag
n-gram model, using even substantially less training
data.
</bodyText>
<sectionHeader confidence="0.999827" genericHeader="method">
1 Introduction
</sectionHeader>
<subsectionHeader confidence="0.998474">
1.1 Orthogonality of morphological
categories of inflective languages
</subsectionHeader>
<bodyText confidence="0.999833615384616">
The major obstacle in morphologicall tagging of
highly inflective languages, such as Czech or Rus-
sian, is — given the resources possibly available — the
tagset size. Typically, it is in the order of thou-
sands. This is due to the (partial) &amp;quot;orthogonality&amp;quot;2
of simple morphological categories, which then mul-
tiply when creating a &amp;quot;flat&amp;quot; list of tags. However,
the individual categories contain only a very small
number of different values; e.g., number has five (Sg,
Pl, Dual, Any, and &amp;quot;not applicable&amp;quot;), case nine etc.
The &amp;quot;orthogonality&amp;quot; should not be taken to mean
complete independence, though. Inflectional lan-
guages (as opposed to agglutinative languages such
as Finnish or Hungarian) typically combine several
certain categories into one morpheme (suffix or end-
ing). At the same time, the morphemes display a
high degree of ambiguity, even across major POS
categories.
For example, most of the Czech nouns can form
singular and plural forms in all seven cases, most
adjectives can (at least potentially) form all (4) gen-
ders, both numbers, all (7) cases, all (3) degrees of
comparison, and can be either of positive or nega-
tive polarity. That gives 336 possibilities (for ad-
jectives), many of them homonymous on the sur-
face. On the other hand, pronouns and numerals do
</bodyText>
<footnote confidence="0.918098555555555">
1This type of tagging is sometimes called morpho-syntactic
tagging. However, to stress that we are not dealing with syn-
tactic categories such as Object or Attribute (but rather with
morphological categories such as Number or Case) we will use
the term &amp;quot;morphological&amp;quot; here.
2By orthogonality we mean that all combinations of values
of two (or more) categories are systematically possible, i.e.
that every member of the cartesian product of the two (or
more) sets of values do appear in the language.
</footnote>
<bodyText confidence="0.999780428571429">
not display such an orthogonality, and even adjec-
tives are not fully orthogonal — an ancient &amp;quot;dual&amp;quot;
number, happily living in modern Czech in the fem-
inine, plural and instrumental case adds another 6
sub-orthogonal possibilities to almost every adjec-
tive. Together, we employ 3127 plausible combina-
tions (including style and diachronic variants).
</bodyText>
<subsectionHeader confidence="0.998483">
1.2 The individual categories
</subsectionHeader>
<bodyText confidence="0.99989915">
There are 13 morphological categories currently used
for morphological tagging of Czech: part of speech,
detailed POS (called &amp;quot;subpart of speech&amp;quot;), gender,
number, case, possessor&apos;s gender, possessor&apos;s num-
ber, person, tense, degree of comparison, negative-
ness (affirmative/negative), voice (active/passive),
and variant/register.
The POS category contains only the major part of
speech values (noun (N), verb (V), adjective (A), pro-
noun (P), verb (V), adjective (A), adverb (D), numeral
(C), preposition (R), conjunction (J), interjection (I),
particle (T), punctuation (Z), and &amp;quot;undefined&amp;quot; (X)).
The &amp;quot;subpart of speech&amp;quot; (SUBPOS) contains details
about the major category and has 75 different values.
For example, verbs (POS: V) are divided into simple
finite form in present or future tense (B), conditional
(c), infinitive (f), imperative (i), etc.3
All the categories vary in their size as well as in
their unigram entropy (see Table 1) computed using
the standard entropy definition
</bodyText>
<equation confidence="0.995457">
H„ = - E pcoog(p(o) (1)
yEY
</equation>
<bodyText confidence="0.99962375">
where p is the unigram distribution estimate based
on the training data, and Y is the set of possible
values of the category in question. This formula can
be rewritten as
</bodyText>
<equation confidence="0.9987905">
Hp,D = /09 (P(Yi))
1 IDI (2)
</equation>
<bodyText confidence="0.999959428571429">
where p is the unigram distribution, D is the data
and IDI its size, and yi is the value of the category
in question at the i — th event (or position) in the
data. The form (2) is usually used for cross-entropy
computation on data (such as test data) different
from those used for estimating p. The base of the
log function is always taken to be 2.
</bodyText>
<subsectionHeader confidence="0.997178">
1.3 The morphological analyzer
</subsectionHeader>
<bodyText confidence="0.999635833333333">
Given the nature of inflectional languages, which can
generate many (sometimes thousands of) forms for a
given lemma (or &amp;quot;dictionary entry&amp;quot;), it is necessary
to employ morphological analysis before the tagging
proper. In Czech, there are as many as 5 differ-
ent lemmas (not counting underlying derivations nor
</bodyText>
<footnote confidence="0.997847">
3The categories POS and SUBPOS are the only two categories
which are rather lexically (and not inflectionally) based.
</footnote>
<page confidence="0.998824">
484
</page>
<tableCaption confidence="0.95382">
Table 1: Most Difficult Individual Morphological
Categories
</tableCaption>
<table confidence="0.9983784">
Category Number Unigram entropy
of values Hp (in bits)
POS 12 2.99
SUBPOS 75 3.83
GENDER 11 2.05
NUMBER 6 1.62
CASE 9 2.24
POSSGENDER 5 0.04
POSSNUMBER 3 0.04
PERSON 5 0.64
TENSE 6 0.55
GRADE 4 0.55
NEGATION 3 1.07
VOICE 3 0.45
VAR 10 0.07
</table>
<bodyText confidence="0.999535857142857">
word senses) and up to 108 different tags for an in-
put word form. The morphological analyzer used for
this purpose (Haji, in prep.), (Haji, 1994) covers
about 98% of running unrestricted text (newspaper,
magazines, novels, etc.). It is based on a lexicon
containing about 228,000 lemmas and it can analyze
about 20,000,000 word forms.
</bodyText>
<sectionHeader confidence="0.900242" genericHeader="method">
2 The Training Data
</sectionHeader>
<bodyText confidence="0.999821827586207">
Our training data consists of about 130,000 tokens
of newspaper and magazine text, manually double-
tagged and then corrected by a single judge.
Our training data consists of about 130,000 tokens
of newspaper and magazine text, manually tagged
using a special-purpose tool which allows for easy
disambiguation of morphological output. The data
has been tagged twice, with manual resolution of
discrepancies (the discrepancy rate being about 5%,
most of them being simple tagging errors rather than
opinion differences).
One data item contains several fields: the input
word form (token), the disambiguated tag, the set of
all possible tags for the input word form, the disam-
biguated lemma, and the set of all possible lemmas
with links to their possible tags. Out of these, we
are currently interested in the form, its possible tags
and the disambiguated tag. The lemmas are ignored
for tagging purposes.4
The tag from the &amp;quot;disambiguated tag&amp;quot; field as
well as the tags from the &amp;quot;possible tags&amp;quot; field are
further divided into so called subtags (by morpho-
logical category). In the set &amp;quot;possible tags field&amp;quot;,
41n fact, tagging helps in most cases to disambiguate the
lemmas. Lemma disambiguation is a separate process follow-
ing tagging. The lemma disambiguation is a much simpler
problem — the average number of different lemmas per token
(as output by the morphological analyzer) is only 1.15. We
do not cover the lemma disambiguation procedure here.
</bodyText>
<figure confidence="0.992826454545455">
RR--6
4/4/INN/S/6/-/-/-/-/1/4/-/-/Ipoettadovem
NNIS6 A--IN/N/I/3/236/-/-/-/-/-/A/-/-/Imodelu
Z:
P4YS1
VpYS---IR-AA-V/p/7/3/-/-/-/X/R/-/A/-/-/Isimuloval
NNIS4 A--
AANS2----14--4/1/IMN/S/24/-/-/-/-/1/1/-/-/IsvAtoviho
NNNS2 A-- N/N/N/S/236/-/-/-/-/-/A/-/-/Iklielatu
AANP6----14--14/4/FIMN/P/26/-/-/-/-/1/4/-/-/Ipti4tich
NNNP6 A--IN/N/N/P/6/-/-/-/-/-/A/-/-/Idesetiletich
</figure>
<figureCaption confidence="0.753390666666667">
Figure 1: Training Data: lit: on computer(adj.)
model , which was-simulating development of-world
climate in next decades
</figureCaption>
<bodyText confidence="0.98520425">
the ambiguity on the level of full (combined) tags is
mapped onto so called &amp;quot;ambiguity classes&amp;quot; (AC-s)
of subtags. This mapping is generally not reversible,
which means that the links across categories might
not be preserved. For example, the word form jen
for which the morphology generates three possible
tags, namely, TT (particle &amp;quot;only&amp;quot;), and
NNIS1 A- and NNIS4 A (noun, masc.
inanimate, singular, nominative (1) or accusative
(4) case; &amp;quot;yen&amp;quot; (the Japanese currency)), will be
assigned six ambiguous ambiguity classes (NT, NT,
-I, -S, -14, -A, for POS, subpart of speech, gen-
der, number, case, and negation) and 7 unambiguous
ambiguity classes (all -). An example of the train-
ing data is presented in Fig. 1. It contains three
columns, separated by the vertical bar (I):
</bodyText>
<listItem confidence="0.9904196">
1. the &amp;quot;truth&amp;quot; (the correct tag, i.e. a sequence of
13 subtags, each represented by a single charac-
ter, which is the true value for each individual
category in the order defined in Fig. 1 (1st col-
umn: POS, 2nd: SUBPOS, etc.)
2. the 13-tuple of ambiguity classes, separated by
a slash (/), in the same order; each ambiguity
class is named using the single character subtags
used for all the possible values of that category;
3. the original word form.
</listItem>
<bodyText confidence="0.9974883">
Please note that it is customary to number the
seven grammatical cases in Czech: (instead of nam-
ing them): &amp;quot;nominative&amp;quot; gets 1, &amp;quot;genitive&amp;quot; 2, etc.
There are four genders, as the Czech masculine gen-
der is divided into masculine animate (M) and inan-
imate (I).
Fig. 1 is a typical example of the ambiguities en-
countered in a running text: little POS ambigu-
ity, but a lot of gender, number and case ambiguity
(columns 3 to 5).
</bodyText>
<page confidence="0.998519">
485
</page>
<sectionHeader confidence="0.996862" genericHeader="method">
3 The Model
</sectionHeader>
<bodyText confidence="0.9998668">
Instead of employing the source-channel paradigm
for tagging (more or less explicitly present e.g. in
(Merialdo, 1992), (Church, 1988), (Haji, Hladka,
1997)) used in the past (notwithstanding some ex-
ceptions, such as Maximum Entropy and rule-based
taggers), we are using here a &amp;quot;direct&amp;quot; approach to
modeling, for which we have chosen an exponential
probabilistic model. Such model (when predicting
an event5 y E Y in a context x) has the general
form
</bodyText>
<equation confidence="0.992192666666667">
exp(E7_1 Ai (y, X))
PAC,e(ylx - (3)
Z(x)
</equation>
<bodyText confidence="0.9284112">
where f (y, x) is the set (of size n) of binary-valued
(yes/no) features of the event value being predicted
and its context, Ai is a &amp;quot;weigth&amp;quot; (in the exponential
sense) of the feature A, and the normalization factor
Z(x) is defined naturally as
</bodyText>
<equation confidence="0.9996605">
Z(x) = E exp(E Ai (y, x)) (4)
yEY i=1
</equation>
<bodyText confidence="0.992783166666667">
We use a separate model for each ambiguity class
AC (which actually appeared in the training data)
of each of the 13 morphological categories6. The
final PAC (Yix) distribution is further smoothed using
unigram distributions on subtags (again, separately
for each category).
</bodyText>
<equation confidence="0.999948">
PAc(Y1x) = crpAc,e(y1x) + (1— cr)PAc,i (y) (5)
</equation>
<bodyText confidence="0.999960789473684">
Such smoothing takes care of any unseen context;
for ambiguity classes not seen in the training data,
for which there is no model, we use unigram proba-
bilities of subtags, one distribution per category.
In the general case, features can operate on any
imaginable context (such as the speed of the wind
over Mt. Washington, the last word of yesterday
TV news, or the absence of a noun in the next 1000
words, etc.). In practice, we view the context as a
set of attribute-value pairs with a discrete range of
values (from now on, we will use the word &amp;quot;context&amp;quot;
for such a set). Every feature can thus be repre-
sented by a set of contexts, in which it is positive.
There is, of course, also a distinguished attribute for
the value of the variable being predicted (y); the rest
of the attributes is denoted by x as expected. Values
of attributes will be denoted by an overstrike (y,1).
The pool of contexts of prospective features is for
the purpose of morphological tagging defined as a
</bodyText>
<footnote confidence="0.946788571428571">
5a subtag, i.e. (in our case) the unique value of a morpho-
logical category.
6Every category is, of course, treated separately. It means
that e.g. the ambiguity class 23 for category CASE (mean-
ing that there is an ambiguity between genitive and dative
cases) is different from ambiguity class 23 for category GRADE
or PERSON.
</footnote>
<listItem confidence="0.987616692307692">
full cross-product of the category being predicted
(y) and of the x specified as a combination of:
1. an ambiguity class of a single category, which
may be different from the category being pre-
dicted, or
2. a word form
and
1. the current position, or
2. immediately preceding (following) position in
text, or
3. closest preceding (following) position (up to
four positions away) having a certain ambiguity
class in the POS category
</listItem>
<figure confidence="0.312605875">
Let now
Categories = {POS, SUBPOS, GENDER,
NUMBER, CASE, POSSGENDER,
POSSNUMBER, PERSON, TENSE,
GRADE, NEGATION, VOICE, VAR};
then the feature function fcciiAcs,7(Y, x) {0,1}
is well-defined iff
E CatAc (6)
</figure>
<bodyText confidence="0.994325214285714">
where Cat E Categories and CatAc is the ambi-
guity class AC (such as AN, for adjective/noun am-
biguity of the part of speech category) of a mor-
phological category Cat (such as POS). For exam-
ple, the function f POS AN ,A,7 is well-defined (A E
{A,N}), whereas the function fc As is not
(6 ft {1,4, 5}). We will introduce the notation of the
context part in the examples of feature value com-
putation below. The indexes may be omitted if it
is clear what category, ambiguity class, the value of
the category being predicted and/or the context the
feature belongs to.
The value of a well-defined feature function
f c at Ac ,v,7(Y 7x) is determined by
</bodyText>
<equation confidence="0.966568">
ai Ac ,V,7(Y x) = l&lt;=&gt;y=yAYC x. (7)
</equation>
<bodyText confidence="0.985868727272727">
This definition excludes features which are positive
for more than one y in any context x. This property
will be used later in the feature selection algorithm.
As an example of a feature, let&apos;s assume we are
predicting the category CASE from the ambiguity
class 145, i.e. the morphology gives us the possibility
to assign nominative (1), accusative (4) or vocative
(5) case. A feature then is e.g.
The resulting case is nominative (1) and
the following word form is pracuj e (lit.
(it) works)
</bodyText>
<footnote confidence="0.992102">
7From now on, we will assume that all features are well-
defined.
</footnote>
<page confidence="0.995144">
486
</page>
<table confidence="0.712266">
AAIS1----1A--1 A/A/III/S/145/-/-/-/-/I/A/-/-/ 1 tvrdt
NIIIS1 A-- 1 WI/Ni/-I /S/-14/-/-/-2/-/-//1/-/-/ lboj
</table>
<figureCaption confidence="0.91246">
Figure 2: Context where the feature
</figureCaption>
<figure confidence="0.563065">
iPosNv,N,(Pos_i=A,cAsE_,=145) is positive (lit.
heavy fighting).
AAIS6----11-- 1 WA/INN/S/8/-/-/-/-/1/A/-/-/ IpraSskim
MISS A-- 1 NV/Ile/IY/S/-6/-/-/-/-/-/A/-/-/ 1hradi
</figure>
<figureCaption confidence="0.997006">
Figure 3: Context where the feature
</figureCaption>
<equation confidence="0.5750915">
.fPosN v,N, (PO S _1=A,C A S E-1=145) is negative (lit.
(at the) Prague castle).
</equation>
<bodyText confidence="0.99083024137931">
denoted as icAsE145,1,(FoRm+i=pracuje), or
The resulting case is accusative (4) and the
closest preceding preposition&apos;s case has the
ambiguity class 46
denoted as .fcA sEi45,4, (C A S E_ pos=R=46)•
The feature fposNv,N,(POS-1=--A,CASE_1=145) will
be positive in the context of Fig. 2, but not in the
context of Fig. 3.
The full cross-product of all the possibilities out-
lined above is again restricted to those features
which have actually appeared in the training data
more than a certain number of times.
Using ambiguity classes instead of unique values
of morphological categories for evaluating the (con-
text part of the) features has the advantage of giv-
ing us the possibility to avoid Viterbi search during
tagging. This then allows to easily add loolcahead
(right) context.8
There is no &amp;quot;forced relationship&amp;quot; among categories
of the same tag. Instead, the model is allowed to
learn also from the same-position &amp;quot;context&amp;quot; of the
subtag being predicted. However, when using the
model for tagging one can choose between two modes
of operation: separate, which is the same mode
used when training as described herein, and VTC
(Valid Tag Combinations) method, which does
not allow for impossible combinations of categories.
See Sect. 5 for more details and for the impact on
the tagging accuracy.
</bodyText>
<sectionHeader confidence="0.999408" genericHeader="method">
4 Training
</sectionHeader>
<subsectionHeader confidence="0.999276">
4.1 Feature Weights
</subsectionHeader>
<bodyText confidence="0.985867611111111">
The usual method for computing the feature weights
(the Ai parameters) is Maximum Entropy (Berger
8It remains to be seen whether using the unique values -
at least for the left context - and employing Viterbi would
help. The results obtained so far suggest that probably not
much, and if yes, then it would restrict the number of features
selected rather than increase tagging accuracy.
Sz al., 1996). This method is generally slow, as it
requires lot of computing power.
Based on our experience with tagging as well as
with other projects involving statistical modeling,
we assume that actually the weights are much less
important than the features themselves.
We therefore employ very simple weight estima-
tion. It is based on the ratio of conditional proba-
bility of y in the context defined by the feature .6;7
and the uniform distribution for the ambiguity class
AC.
</bodyText>
<subsectionHeader confidence="0.995577">
4.2 Feature Selection
</subsectionHeader>
<bodyText confidence="0.999636730769231">
The usual guiding principle for selecting features of
exponential models is the Maximum Likelihood prin-
ciple, i.e. the probability of the training data is being
maximized. (or the cross-entropy of the model and
the training data is being minimized, which is the
same thing). Even though we are eventually inter-
ested in the final error rate of the resulting model,
this might be the only solution in the usual source-
channel setting where two independent models (a
language model and a &amp;quot;translation&amp;quot; model of some
sort — acoustic, real translation etc.) are being used.
The improvement of one model influences the error
rate of the combined model only indirectly.
This is not the case of tagging. Tagging can be
seen as a &amp;quot;final application&amp;quot; problem for which we
assume to have enough data at hand to train and
use just one model, abandoning the source-channel
paradigm. We have therefore used the error rate
directly as the objective function which we try to
minimize when selecting the model&apos;s features. This
idea is not new, but as far as we know it has been
implemented in rule-based taggers and parsers, such
as (Brill, 1993a), (Brill, 1993b), (Brill, 1993c) and
(Ribarov, 1996), but not in models based on proba-
bility distributions.
Let&apos;s define the set of contexts of a set of features:
</bodyText>
<equation confidence="0.980448">
X (F) =-- {7 : 3fily E F}, (8)
</equation>
<bodyText confidence="0.9879281">
where F is some set of features of interest.
The features can therefore be grouped together
based on the context they operate on. In the cur-
rent implementation, we actually add features in
&amp;quot;batches&amp;quot;. A &amp;quot;batch&amp;quot; of features is defined as a set
of features which share the same context 7 (see the
definition below). Computationaly, adding features
in batches is relatively cheap both time- and space-
wise.
For example, the features
</bodyText>
<equation confidence="0.571266">
fPOSN v ,N,(POS-1=A,CA SE-1=145)
</equation>
<footnote confidence="0.317578">
and
fPOSNv,V,(POS-1=-A,CASE-1=145)
</footnote>
<page confidence="0.995256">
487
</page>
<bodyText confidence="0.9982">
share the context (POS_i = A,CASE_i = 145).
Let further
</bodyText>
<listItem confidence="0.995602">
• FAG, be the pool of features available for selec-
tion.
• SAC be the set of features selected so far for a
model for ambiguity class AC,
• PsAc(Yid) the probability, using model (3-5)
with features SAC, of subtag y in a context de-
fined by position d in the training data, and
• FAc,,y be the set (&amp;quot;batch&amp;quot;) of features sharing
the same context 7, i.e.
</listItem>
<figureCaption confidence="0.486642">
FAC, 7 = {f E FAG : Riv,7 : f = fv,7}. (9)
</figureCaption>
<bodyText confidence="0.994833">
Note that the size of AC is equal to the size of
any batch of features (fACI = IFAc,71 for any
T).
The selection process then proceeds as follows:
</bodyText>
<listItem confidence="0.989564333333333">
1. For all contexts Y E X(FAC) do the following:
2. For all features f = fy,y E FAc,2- compute their
associated weights )f using the formula:
</listItem>
<equation confidence="0.661242333333333">
Af
1 = log( ), (10)
177
</equation>
<bodyText confidence="0.765894">
where
</bodyText>
<table confidence="0.809828">
EldT-11 fv,7(Y d, xd) (11)
15 AC ,7(Y) -
EyEACEldr-il f(Y,xd)•
</table>
<listItem confidence="0.991935333333333">
3. Compute the error rate of the training data by
going through it and at each position d selecting
the best subtag by maximizing PSAC UFA c ,7 (Y I d)
over all y E AC.
4. Select such a feature set FAcvi which results in
the maximal improvement in the error rate of
the training data and add all f E FAC,Y Perma-
nently to SAC; with SAC now extended, start
from the beginning (unless the termination con-
dition is met),
5. Termination condition: improvement in error
rate smaller than a preset minimum.
</listItem>
<bodyText confidence="0.999942363636363">
The probability defined by the formula (11) can
easily be computed despite its ugly general form, as
the denominator is in fact the number of (positive)
occurrences of all the features from the batch defined
by the context 7 in the training data. It also helps
if the underlying ambiguity class AC is found only
in a fraction of the training data, which is typically
the case. Also, the size of the batch (equal to IACI)
is usually very small.
On top of rather roughly estimating the Af param-
eters, we use another implementation shortcut here:
we do not necessarily compute the best batch of fea-
tures in each iteration, but rather add all (batches
of) features which improve the error rate by more
than a threshold 6. This threshold is set to half the
number of data items which contain the ambiguity
class AC at the beginning of the loop, and then is cut
in half at every iteration. The positive consequence
of this shortcut (which certainly adds some unnec-
essary features) is that the number of iterations is
much smaller than if the maximum is regularly com-
puted at each iteration.
</bodyText>
<sectionHeader confidence="0.999853" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.993672023809524">
We have used 130,000 words as the training set and a
test set of 1000 words. There have been 378 different
ambiguity classes (of subtags) across all categories.
We have used two evaluation metrics: one which
evaluates each category separately and one &amp;quot;flat-
list&amp;quot; error rate which is used for comparison with
other methods which do not predict the morpho-
logical categories separately. We compare the new
method with results obtained on Czech previously,
as reported in (Hladka, 1994) and (Hajit, Hladka,
1997). The apparently high baseline when compared
to previously reported experiments is undoubtedly
due to the introduction of multiple models based on
ambiguity classes.
In all cases, since the percentage of text tokens
which are at least two-way ambiguous is about 55%,
the error rate should be almost doubled if one wants
to know the error rate based on ambiguous words
only.
The baseline, or &amp;quot;smoothing-only&amp;quot; error rate was
at 20.7% in the test data and 22.18 % in the training
data.
Table 2 presents the initial error rates for the indi-
vidual categories computed using only the smooth-
ing part of the model (n = 0 in equation 3).
Training took slightly under 20 hours on a Linux-
powered Pentium 90, with feature adding threshold
set to 4 (which means that a feature batch was not
added if it improved the absolute error rate on train-
ing data by 4 errors or less). 840 (batches) of fea-
tures (which corresponds to about 2000 fully spec-
ified features) have been learned. The tagging it-
self is (contrary to training) very fast. The average
speed is about 300 words/sec. on morphologically
prepared data on the same machine. The results are
summarized in Table 3.
There is no apparent overtraining yet. However,
it does appear when the threshold is lowered (we
have tested that on a smaller set of training data
consisting of 35,000 words: overtraining started to
occur when the threshold was down to 2-3).
Table 4 contains comparison of the results
</bodyText>
<page confidence="0.98309">
488
</page>
<table confidence="0.998084133333333">
Category training data test data
POS 1.10 2.1
SUBPOS 1.06 1.1
GENDER 6.35 6.1
NUMBER 5.34 4.2
CASE 14.55 14.5
POSSGENDER 0.05 0.0
POSSNUMBER 0.13 0.1
PERSON 0.28 0.0
TENSE 0.36 0.1
GRADE 0.48 0.3
NEGATION 1.33 1.0
VOICE 0.40 0.1
VAR 0.30 0.3
Overall 22.18 20.7
</table>
<tableCaption confidence="0.640019">
Table 2: Initial Error Rate
</tableCaption>
<table confidence="0.999137866666667">
Category training data test data
POS 0.02 0.9
SUBPOS 0.49 1.0
GENDER 1.78 2.0
NUMBER 2.73 0.9
CASE 6.01 5.0
POSSGENDER 0.04 0.0
POSSNUMBER 0.01 0.0
PERSON 0.12 0.0
TENSE 0.12 0.1
GRADE 0.11 0.1
NEGATION 0.25 0.0
VOICE 0.11 0.0
VAR 0.10 0.2
Overall 8.75 8.0
</table>
<tableCaption confidence="0.99949">
Table 3: Resulting Error Rate
</tableCaption>
<bodyText confidence="0.987759888888889">
achieved with the previous experiments on Czech
tagging (Haji, Hladka, 1997). It shows that we
got more than 50% improvement on the best error
rate achieved so far. Also the amount of training
data used was lower than needed for the HMM ex-
periments. We have also performed an experiment
using 35,000 training words which yielded by about
4% worse results (88% combined tag accuracy).
Finally, Table 5 compares results (given differ-
</bodyText>
<table confidence="0.999135222222222">
Experiment training best error
data size rate (in %)
Unigram HMM 621,015 34.30
Rule-based (Brill&apos;s) 37,892 20.25
Trigram HMM 621,015 18.86
Bigram HMM 621,015 18.46
Exponential 35,000 12.00
Exponential 130,000 8.00
Exponential, VTC 160,000 6.20
</table>
<tableCaption confidence="0.99942">
Table 4: Comparing Various Methods
</tableCaption>
<bodyText confidence="0.999827928571429">
ent training thresholds9) obtained on larger train-
ing data using the &amp;quot;separate&amp;quot; prediction method dis-
cussed so far with results obtained through a mod-
ification, the key point of which is that it considers
only &amp;quot;Valid (sub)Tag Combinations (VTC)&amp;quot;. The
probability of a tag is computed as a simple product
of subtag probabilities (normalized), thus assuming
subtag independence. The &amp;quot;winner&amp;quot; is presented in
boldface. As expected, the overall error rate is al-
ways better using the VTC method, but some of the
subtags are (sometimes) better predicted using the
&amp;quot;separate&amp;quot; prediction method19. This could have
important practical consequences - if, for example,
the POS or SUBPOS is all that&apos;s interesting.
</bodyText>
<sectionHeader confidence="0.991618" genericHeader="conclusions">
6 Conclusion and Further Research
</sectionHeader>
<bodyText confidence="0.9940335">
The combined error rate results are still far below
the results reported for English, but we believe that
there is still room for improvement. Moreover, split-
ting the tags into subtags showed that &amp;quot;pure&amp;quot; part of
speech (as well as the even more detailed &amp;quot;subpart&amp;quot;
of speech) tagging gives actually better results than
those for English.
We see several ways how to proceed to possibly
improve the performance of the tagger (we are still
talking here about the &amp;quot;single best tag&amp;quot; approach;
the n-best case will be explored separately):
• Disambiguated tags (in the left context) plus
Viterbi search. Some errors might be eliminated
if features asking questions about the disam-
biguated context are being used. The disam-
biguated tags concentrate - or transfer - in-
formation about the more distant context. It
would avoid &amp;quot;repeated&amp;quot; learning of the same
or similar features for different but related dis-
ambiguation problems. The final effect on the
overall accuracy is yet to be seen. Moreover,
the transition function assumed by the Viterbi
algorithm must be reasonably defined (approx-
imated).
</bodyText>
<listItem confidence="0.636795">
• Final re-estimation using maximum entropy.
</listItem>
<bodyText confidence="0.8379803">
Let&apos;s imagine that after selecting all the features
using the training method described here we
recompute the feature weights using the usual
maximum entropy objective function. This will
produce better (read: more principled) weight
estimates for the features already selected, but
it might help as well as hurt the performance.
• Improved feature pool. This is, according to
our opinion, the source of major improvement.
The error analysis shows that in many cases the
</bodyText>
<footnote confidence="0.9712802">
9No overtraining occurred here either, but the results for
thresholds 2-4 do not differ significantly.
19For English, using the Penn Treebank data, we have al-
ways obtained better accuracy using the VTC method (and
redefinition of the tag set based on 4 categories).
</footnote>
<page confidence="0.996152">
489
</page>
<table confidence="0.999871294117647">
Threshold: 128 16 8 4 2
Features learned: 23 213 772 1529 4571
Category Sep VTC Sep VTC Sep VTC Sep VTC Sep VTC
POS 1.50 1.32 0.86 0.78 0.66 0.60 0.44 0.42 0.36 0.44
SUBPOS 1.24 1.40 0.78 0.84 0.70 0.64 0.36 0.48 0.30 0.48
GENDER 4.50 4.06 3.00 2.80 2.40 2.14 2.14 1.80 2.08 1.90
NUMBER 3.46 2.94 2.62 2.40 1.86 1.72 1.72 1.56 1.80 1.50
CASE 11.10 10.52 7.74 7.66 5.30 5.34 4.82 4.80 4.88 4.84
POSSGENDER 0.08 0.10 0.08 0.12 0.08 0.04 0.04 0.06 0.02 0.04
POSSNUMBER 0.14 0.04 0.04 0.04 0.04 0.00 0.02 0.02 0.00 0.00
PERSON 0.28 0.18 0.14 0.16 0.16 0.10 0.14 0.12 0.12 0.06
TENSE 0.36 0.18 0.16 0.14 0.10 0.12 0.10 0.12 0.10 0.08
GRADE 0.88 1.00 0.70 0.30 0.44 0.30 0.22 0.18 0.22 0.16
NEGATION 0.62 0.26 0.34 0.36 0.28 0.26 0.24 0.24 0.26 0.24
VOICE 0.38 0.18 0.16 0.14 0.10 0.12 0.10 0.12 0.08 0.08
VAR 0.26 0.18 0.24 0.22 0.14 0.14 0.12 0.14 0.12 0.04
Overall 16.50 13.22 12.20 9.58 8.42 6.98 7.62 6.22 7.66 6.20
</table>
<tableCaption confidence="0.999739">
Table 5: Resulting Error Rate in % (newspaper, training size: 160,000, test size: 5000 tokens)
</tableCaption>
<bodyText confidence="0.999949666666667">
context to be used for disambiguation has not
been used by the tagger simply because more
sophisticated features have not been considered
for selection. An example of such a feature,
which would possibly help to solve the very hard
and relatively frequent problem of disambiguat-
ing between nominative and accusative cases of
certain nouns, would be a question &amp;quot;Is there
a noun in nominative case only in the same
clause?&amp;quot; - every clause may usually have only
one noun phrase in nominative, constituting its
subject. For such feature to work we will have
to correctly determine or at least approximate
the clause boundaries, which is obviously a non-
trivial task by itself.
</bodyText>
<sectionHeader confidence="0.999422" genericHeader="acknowledgments">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.99805225">
Various parts of this work has been supported by
the following grants: Open Foundation RSS/HESP
195/1995, Grant Agency of the Czech Republic
(GAR) 405/96/K214, and Ministry of Education
Project No. VS96151. The authors would also like
to thank Fred Jelinek of CLSP JHU Baltimore for
valuable comments and suggestions which helped to
improve this paper a lot.
</bodyText>
<sectionHeader confidence="0.999618" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999904244444444">
Adam Berger, Stephen Della Pietra, Vincent Della
Pietra. 1996. Maximum Entropy Approach. In
Computational Linguistics, vol. 3, MIT Press,
Cambridge, MA.
Eric Brill. 1993a. A Corpus Based Approach To
Language Learning. PhD Dissertation, Depart-
ment of Computer and Information Science, Uni-
versity of Pennsylvania.
Eric Brill. 1993b. Automatic grammar induc-
tion and parsing free text: A Transformation-
Based Approach. In: Proceedings of the 3rd In-
ternational Workshop on Parsing Tech nologies,
Tilburg, The Netherlands.
Eric Brill. 1993c. Transformation-Based Error-
Driven Parsing. In: Proceedings of the Twelfth
National Conference on Artificial Intelligence.
Kenneth W. Church. 1988. A stochastic parts pro-
gram and noun phrase parser for unrestricted
text. In Proceedings of the Second Conference
on Applied Natural Language Processing, pages
136-143, Austin, Texas. Association for Compu-
tational Linguistics, Morristown, New Jersey.
Jan Hajie. 1994. Unification Morphology Grammar.
PhD Dissertation. MFF UK, Charles University,
Prague.
Jan Hajie. In prep. Automatic Processing of Czech:
between Morphology and Syntax. MFF UK,
Charles University, Prague.
Jan Hajie, Barbora Hladka. 1997. Tagging of Inflec-
tive Languages: a Comparison. In Proceedings of
the ANLP&apos;97, pages 136-143, Washington, DC.
Association for Computational Linguistics, Mor-
ristown, New Jersey.
Barbora Hladlca. 1994. Programove vybavenf pro
zpracovanf vellcSrch deskSrch textovSrch korpusa.
MSc Thesis, Institute of Formal and Applied Lin-
guistics, Charles University, Prague, Czech Re-
public.
Bernard Merialdo. 1992. Tagging Text With A
Probabilistic Model. Computational Linguistics,
20(2):155-171
Kiril Ribarov. 1996. Automaticka tvorba gramatiky
piirozeneho jazylca. MSc Thesis, Institute of For-
mal and Applied Linguistics, Charles University,
Prague, Czech Republic. In Czech.
</reference>
<page confidence="0.998272">
490
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000646">
<title confidence="0.9896365">Tagging Inflective Languages: Prediction of Morphological Categories for a Rich, Structured Tagset</title>
<author confidence="0.865468">Hajie Hladka</author>
<affiliation confidence="0.978818">Institute of Formal and Applied Linguistics MFF UK Charles University, Prague, Czech Republic</affiliation>
<email confidence="0.418136">Ihajic,hladkal©ufal.mfficuni.cz</email>
<abstract confidence="0.688200733333333">Abstrakt (aesky) {This short abstract is in Czech. For illustration purposes, it has been tagged by our tagger; errors printed underlinedand corrections are shown.} Hlavnim/AAIS7----1A-- A-pi/RR--6 morfologickem/AANS6----1A-- A- (/Z: nekdy/Db teZ/Db zvanem/AAIS6 1A-- Correct: N morfologicko/A2 Correct: NS )/Z: As/RR -7 bohatou/AAFS7----1A-flexi/NNFS7 A ,/Z: jako/J, je/VB-S---3P-AAnapiiklad/Db A- ,/Z: je/VB-S---3P-AA- -/Z: pii/RR 6 omezene/AAFS6----1A--</abstract>
<note confidence="0.8944399">velikosti/NNFS2 A- Correct: 6 zdrojfi/NNIP2 A -/Z: Akterk/P4YS1 jde/VB-S---3P-AA- 1A-do/RR--2 A- Na§e/PSHS1-P1</note>
<abstract confidence="0.929873542857143">Apiitom/Db vyuilva/VB-S---3P-AAexponencialniho/AAIS2----1A-pravdepodobnostniho/AAIS2----1A-zaloieneho/AAIS2----1A-na/RR -6 automaticky/Dg 1A-vybranSrch/AAMP6----1A-- A- Atohoto/PDZS2 modelu/NNIS2 A se/P7-X4 poeftajf/VB-P---3P-AAj ednoduchkch/AA IP 2---- 1A- odhadii/NNIP2 A (/Z. je/VB-S---3P-AAtak/Db mnohem/Db rychlej§f/AAFS1----2A ne2/J, kdybychom/J,-P 1 pouiili/VpMP---XR-AAmaximalni/AAFS1----1A-entropie/NNFS2 A )/Z. a/J&amp;quot; pfitom/Db se/P7-X4 piimo/Dg 1A-minimalizuje/VB-S---3P-AApoeet/NNIS1 A A--</abstract>
<note confidence="0.919846333333333">Correct: I Correct: 2 Correct: 1 Correct: I RR--2,- 483</note>
<abstract confidence="0.998244368421052">The major obstacle in morphological (sometimes called morpho-syntactic, or extended POS) tagging highly inflective languages, such or Russian, is — given the resources possibly available — the tagset size. Typically, it is in the order of thousands. Our method uses an exponential probabilistic model based on automatically selected features. The parameters of the model are computed using simple estimates (which makes training much faster than when one uses Maximum Entropy) to directly minimize the error rate on training data. The results obtained so far not only show good performance on disambiguation of most of the individual morphological categories, but they also show a significant improvement on the overall prediction of the resulting combined tag over a HMM-based tag n-gram model, using even substantially less training data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Adam Berger</author>
<author>Stephen Della Pietra</author>
<author>Vincent Della Pietra</author>
</authors>
<title>Maximum Entropy Approach.</title>
<date>1996</date>
<booktitle>In Computational Linguistics,</booktitle>
<volume>3</volume>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam Berger, Stephen Della Pietra, Vincent Della Pietra. 1996. Maximum Entropy Approach. In Computational Linguistics, vol. 3, MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>A Corpus Based Approach To Language Learning.</title>
<date>1993</date>
<tech>PhD Dissertation,</tech>
<institution>Department of Computer and Information Science, University of Pennsylvania.</institution>
<contexts>
<context position="19133" citStr="Brill, 1993" startWordPosition="3027" endWordPosition="3028"> — acoustic, real translation etc.) are being used. The improvement of one model influences the error rate of the combined model only indirectly. This is not the case of tagging. Tagging can be seen as a &amp;quot;final application&amp;quot; problem for which we assume to have enough data at hand to train and use just one model, abandoning the source-channel paradigm. We have therefore used the error rate directly as the objective function which we try to minimize when selecting the model&apos;s features. This idea is not new, but as far as we know it has been implemented in rule-based taggers and parsers, such as (Brill, 1993a), (Brill, 1993b), (Brill, 1993c) and (Ribarov, 1996), but not in models based on probability distributions. Let&apos;s define the set of contexts of a set of features: X (F) =-- {7 : 3fily E F}, (8) where F is some set of features of interest. The features can therefore be grouped together based on the context they operate on. In the current implementation, we actually add features in &amp;quot;batches&amp;quot;. A &amp;quot;batch&amp;quot; of features is defined as a set of features which share the same context 7 (see the definition below). Computationaly, adding features in batches is relatively cheap both time- and spacewise. Fo</context>
</contexts>
<marker>Brill, 1993</marker>
<rawString>Eric Brill. 1993a. A Corpus Based Approach To Language Learning. PhD Dissertation, Department of Computer and Information Science, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Automatic grammar induction and parsing free text: A TransformationBased Approach. In:</title>
<date>1993</date>
<booktitle>Proceedings of the 3rd International Workshop on Parsing Tech nologies,</booktitle>
<location>Tilburg, The Netherlands.</location>
<contexts>
<context position="19133" citStr="Brill, 1993" startWordPosition="3027" endWordPosition="3028"> — acoustic, real translation etc.) are being used. The improvement of one model influences the error rate of the combined model only indirectly. This is not the case of tagging. Tagging can be seen as a &amp;quot;final application&amp;quot; problem for which we assume to have enough data at hand to train and use just one model, abandoning the source-channel paradigm. We have therefore used the error rate directly as the objective function which we try to minimize when selecting the model&apos;s features. This idea is not new, but as far as we know it has been implemented in rule-based taggers and parsers, such as (Brill, 1993a), (Brill, 1993b), (Brill, 1993c) and (Ribarov, 1996), but not in models based on probability distributions. Let&apos;s define the set of contexts of a set of features: X (F) =-- {7 : 3fily E F}, (8) where F is some set of features of interest. The features can therefore be grouped together based on the context they operate on. In the current implementation, we actually add features in &amp;quot;batches&amp;quot;. A &amp;quot;batch&amp;quot; of features is defined as a set of features which share the same context 7 (see the definition below). Computationaly, adding features in batches is relatively cheap both time- and spacewise. Fo</context>
</contexts>
<marker>Brill, 1993</marker>
<rawString>Eric Brill. 1993b. Automatic grammar induction and parsing free text: A TransformationBased Approach. In: Proceedings of the 3rd International Workshop on Parsing Tech nologies, Tilburg, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Transformation-Based ErrorDriven Parsing. In:</title>
<date>1993</date>
<booktitle>Proceedings of the Twelfth National Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="19133" citStr="Brill, 1993" startWordPosition="3027" endWordPosition="3028"> — acoustic, real translation etc.) are being used. The improvement of one model influences the error rate of the combined model only indirectly. This is not the case of tagging. Tagging can be seen as a &amp;quot;final application&amp;quot; problem for which we assume to have enough data at hand to train and use just one model, abandoning the source-channel paradigm. We have therefore used the error rate directly as the objective function which we try to minimize when selecting the model&apos;s features. This idea is not new, but as far as we know it has been implemented in rule-based taggers and parsers, such as (Brill, 1993a), (Brill, 1993b), (Brill, 1993c) and (Ribarov, 1996), but not in models based on probability distributions. Let&apos;s define the set of contexts of a set of features: X (F) =-- {7 : 3fily E F}, (8) where F is some set of features of interest. The features can therefore be grouped together based on the context they operate on. In the current implementation, we actually add features in &amp;quot;batches&amp;quot;. A &amp;quot;batch&amp;quot; of features is defined as a set of features which share the same context 7 (see the definition below). Computationaly, adding features in batches is relatively cheap both time- and spacewise. Fo</context>
</contexts>
<marker>Brill, 1993</marker>
<rawString>Eric Brill. 1993c. Transformation-Based ErrorDriven Parsing. In: Proceedings of the Twelfth National Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth W Church</author>
</authors>
<title>A stochastic parts program and noun phrase parser for unrestricted text.</title>
<date>1988</date>
<booktitle>In Proceedings of the Second Conference on Applied Natural Language Processing,</booktitle>
<pages>136--143</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics,</institution>
<location>Austin, Texas.</location>
<contexts>
<context position="11205" citStr="Church, 1988" startWordPosition="1695" endWordPosition="1696">at category; 3. the original word form. Please note that it is customary to number the seven grammatical cases in Czech: (instead of naming them): &amp;quot;nominative&amp;quot; gets 1, &amp;quot;genitive&amp;quot; 2, etc. There are four genders, as the Czech masculine gender is divided into masculine animate (M) and inanimate (I). Fig. 1 is a typical example of the ambiguities encountered in a running text: little POS ambiguity, but a lot of gender, number and case ambiguity (columns 3 to 5). 485 3 The Model Instead of employing the source-channel paradigm for tagging (more or less explicitly present e.g. in (Merialdo, 1992), (Church, 1988), (Haji, Hladka, 1997)) used in the past (notwithstanding some exceptions, such as Maximum Entropy and rule-based taggers), we are using here a &amp;quot;direct&amp;quot; approach to modeling, for which we have chosen an exponential probabilistic model. Such model (when predicting an event5 y E Y in a context x) has the general form exp(E7_1 Ai (y, X)) PAC,e(ylx - (3) Z(x) where f (y, x) is the set (of size n) of binary-valued (yes/no) features of the event value being predicted and its context, Ai is a &amp;quot;weigth&amp;quot; (in the exponential sense) of the feature A, and the normalization factor Z(x) is defined naturally </context>
</contexts>
<marker>Church, 1988</marker>
<rawString>Kenneth W. Church. 1988. A stochastic parts program and noun phrase parser for unrestricted text. In Proceedings of the Second Conference on Applied Natural Language Processing, pages 136-143, Austin, Texas. Association for Computational Linguistics, Morristown, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajie</author>
</authors>
<title>Unification Morphology Grammar. PhD Dissertation.</title>
<date>1994</date>
<institution>MFF UK, Charles University,</institution>
<location>Prague.</location>
<marker>Hajie, 1994</marker>
<rawString>Jan Hajie. 1994. Unification Morphology Grammar. PhD Dissertation. MFF UK, Charles University, Prague.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jan Hajie</author>
</authors>
<booktitle>In prep. Automatic Processing of Czech: between Morphology and Syntax. MFF UK,</booktitle>
<institution>Charles University,</institution>
<location>Prague.</location>
<marker>Hajie, </marker>
<rawString>Jan Hajie. In prep. Automatic Processing of Czech: between Morphology and Syntax. MFF UK, Charles University, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajie</author>
<author>Barbora Hladka</author>
</authors>
<title>Tagging of Inflective Languages: a Comparison.</title>
<date>1997</date>
<booktitle>In Proceedings of the ANLP&apos;97,</booktitle>
<pages>136--143</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics,</institution>
<location>Washington, DC.</location>
<marker>Hajie, Hladka, 1997</marker>
<rawString>Jan Hajie, Barbora Hladka. 1997. Tagging of Inflective Languages: a Comparison. In Proceedings of the ANLP&apos;97, pages 136-143, Washington, DC. Association for Computational Linguistics, Morristown, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbora Hladlca</author>
</authors>
<title>Programove vybavenf pro zpracovanf vellcSrch deskSrch textovSrch korpusa. MSc Thesis,</title>
<date>1994</date>
<institution>Institute of Formal and Applied Linguistics, Charles University,</institution>
<location>Prague, Czech Republic.</location>
<marker>Hladlca, 1994</marker>
<rawString>Barbora Hladlca. 1994. Programove vybavenf pro zpracovanf vellcSrch deskSrch textovSrch korpusa. MSc Thesis, Institute of Formal and Applied Linguistics, Charles University, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Merialdo</author>
</authors>
<title>Tagging Text With A Probabilistic Model.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<pages>20--2</pages>
<contexts>
<context position="11189" citStr="Merialdo, 1992" startWordPosition="1693" endWordPosition="1694">sible values of that category; 3. the original word form. Please note that it is customary to number the seven grammatical cases in Czech: (instead of naming them): &amp;quot;nominative&amp;quot; gets 1, &amp;quot;genitive&amp;quot; 2, etc. There are four genders, as the Czech masculine gender is divided into masculine animate (M) and inanimate (I). Fig. 1 is a typical example of the ambiguities encountered in a running text: little POS ambiguity, but a lot of gender, number and case ambiguity (columns 3 to 5). 485 3 The Model Instead of employing the source-channel paradigm for tagging (more or less explicitly present e.g. in (Merialdo, 1992), (Church, 1988), (Haji, Hladka, 1997)) used in the past (notwithstanding some exceptions, such as Maximum Entropy and rule-based taggers), we are using here a &amp;quot;direct&amp;quot; approach to modeling, for which we have chosen an exponential probabilistic model. Such model (when predicting an event5 y E Y in a context x) has the general form exp(E7_1 Ai (y, X)) PAC,e(ylx - (3) Z(x) where f (y, x) is the set (of size n) of binary-valued (yes/no) features of the event value being predicted and its context, Ai is a &amp;quot;weigth&amp;quot; (in the exponential sense) of the feature A, and the normalization factor Z(x) is de</context>
</contexts>
<marker>Merialdo, 1992</marker>
<rawString>Bernard Merialdo. 1992. Tagging Text With A Probabilistic Model. Computational Linguistics, 20(2):155-171</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiril Ribarov</author>
</authors>
<title>Automaticka tvorba gramatiky piirozeneho jazylca. MSc Thesis,</title>
<date>1996</date>
<institution>Institute of Formal and Applied Linguistics, Charles University,</institution>
<location>Prague, Czech Republic. In Czech.</location>
<contexts>
<context position="19187" citStr="Ribarov, 1996" startWordPosition="3034" endWordPosition="3035"> The improvement of one model influences the error rate of the combined model only indirectly. This is not the case of tagging. Tagging can be seen as a &amp;quot;final application&amp;quot; problem for which we assume to have enough data at hand to train and use just one model, abandoning the source-channel paradigm. We have therefore used the error rate directly as the objective function which we try to minimize when selecting the model&apos;s features. This idea is not new, but as far as we know it has been implemented in rule-based taggers and parsers, such as (Brill, 1993a), (Brill, 1993b), (Brill, 1993c) and (Ribarov, 1996), but not in models based on probability distributions. Let&apos;s define the set of contexts of a set of features: X (F) =-- {7 : 3fily E F}, (8) where F is some set of features of interest. The features can therefore be grouped together based on the context they operate on. In the current implementation, we actually add features in &amp;quot;batches&amp;quot;. A &amp;quot;batch&amp;quot; of features is defined as a set of features which share the same context 7 (see the definition below). Computationaly, adding features in batches is relatively cheap both time- and spacewise. For example, the features fPOSN v ,N,(POS-1=A,CA SE-1=14</context>
</contexts>
<marker>Ribarov, 1996</marker>
<rawString>Kiril Ribarov. 1996. Automaticka tvorba gramatiky piirozeneho jazylca. MSc Thesis, Institute of Formal and Applied Linguistics, Charles University, Prague, Czech Republic. In Czech.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>