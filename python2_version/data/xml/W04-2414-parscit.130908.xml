<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.015470">
<title confidence="0.862333">
Memory-based semantic role labeling:
Optimizing features, algorithm, and output
</title>
<author confidence="0.527847">
Antal van den Bosch, Sander Canisius, Walter Daelemans,
Iris Hendrickx Erik Tjong Kim Sang
</author>
<affiliation confidence="0.7001085">
ILK / Computational Linguistics CNTS / Department of Linguistics
Tilburg University, P.O. Box 90153, University of Antwerp, Universiteitsplein 1,
</affiliation>
<address confidence="0.807364">
NL-5000 LE Tilburg, The Netherlands B-2610 Antwerpen, Belgium
{Antal.vdnBosch,S.V.M.Canisius, {Walter.Daelemans,
</address>
<email confidence="0.993109">
I.H.E.Hendrickx}@uvt.nl Erik.TjongKimSang}@ua.ac.be
</email>
<sectionHeader confidence="0.999626" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999567133333333">
In this paper we interpret the semantic role labeling prob-
lem as a classification task, and apply memory-based
learning to it in an approach similar to Buchholz et al.
(1999) and Buchholz (2002) for grammatical relation la-
beling. We apply feature selection and algorithm parame-
ter optimization strategies to our learner. In addition, we
investigate the effect of two innovations: (i) the use of
sequences of classes as classification output, combined
with a simple voting mechanism, and (ii) the use of iter-
ative classifier stacking which takes as input the original
features and a pattern of outputs of a first-stage classifier.
Our claim is that both methods avoid errors in sequences
of predictions typically made by simple classifiers that
are unaware of their previous or subsequent decisions in
a sequence.
</bodyText>
<sectionHeader confidence="0.870801" genericHeader="categories and subject descriptors">
2 Data and Features
</sectionHeader>
<bodyText confidence="0.988853727272727">
The CoNLL-2004 shared task (Carreras and M`arquez,
2004) supplied data sets for the semantic role labeling
task with several levels of annotation apart from the role
labels to be predicted. Central to our approach is the
choice to adopt the instance encoding analogous to Buch-
holz et al. (1999) to have our examples represent relations
between pairs of verbs and chunks. That is, we transform
the semantic role labeling task to a classification task in
which we decide for all pairs of verbs and chunks whether
they stand in a semantic role relation. Afterwards we con-
sider all adjacent chunks to which the same role label is
assigned by our classifier as belonging to the same argu-
ment. All results reported below use this task representa-
tion. Processing focuses on one verb at a time; verbs are
treated independently.
We did not employ the provided Propbank data nor
the verb sense information available, nor did we use any
other external source of information.
Apart from the provided words and the predicted PoS
tags, chunk labels, clause labels, and named-entity labels,
provided beforehand, we have considered an additional
set of automatically derived features:
</bodyText>
<listItem confidence="0.991801736842105">
1. attenuated words (Eisner, 1996), i.e. wordforms
occurring below a frequency threshold (of 10) are
converted to a string capturing some of the original
word form’s features (capitalization, whether it con-
tains numbers or a hyphen, or suffix letters);
2. the distance between the candidate role word and the
verb, measured in intervening words, chunks, NP
chunks or VP chunks (negative if the word is to the
left, positive if it is to the right of the verb);
3. preceding preposition: a feature containing the head
word of the previous chunk if that was labeled as
preposition;
4. passive main verb: a binary feature which is on if
the main verb is used in a passive construction;
5. current clause: a binary feature which is on if the
current word is in the same clause as the main verb;
6. role pattern: the most frequently occurring role pat-
tern of the main verb in the training data (contains
the order of V and A0-A5).
</listItem>
<bodyText confidence="0.994482555555556">
For every target verb in every sentence, the data sup-
plied to the learners contains instances for every head
word of non-VP chunks and for all words in VP chunks,
and all words in all chunks containing a target verb (i.e.,
more instances than chunks, to account for the fact that
some roles are contained within chunks). Here is an ex-
ample instance for the second chunk of the training data:
expect -2 -1 0 morph-cap in IN
NN PP passive clause A0VA1 A1
This instance contains 12 features: the verb (1), dis-
tance to the verb measured in chunks (2), NP chunks (3)
and VP chunks (4), attenuated words (5–6), PoS tags (7–
8), a chunk tag (9), passive main verb (10), current clause
(11) and role pattern (12). The final item of the line is
the required output class. Our choice of instance format
is only slightly harmful for performance: with a perfect
classifier we can still obtain a maximal F0=1 score of 99.1
on the development data.
</bodyText>
<sectionHeader confidence="0.993282" genericHeader="method">
3 Approach
</sectionHeader>
<bodyText confidence="0.999544555555556">
In this section we describe our approach to semantic role
labeling. The core part of our system is a memory-based
learner. During the development of the system we have
used feature selection and parameter optimization by it-
erative deepening. Additionally we have evaluated three
extensions of the basic memory-based learning method:
class n-grams, i.e. complex classes composed of se-
quences of simple classes, iterative classifier stacking and
automatic output post-processing.
</bodyText>
<subsectionHeader confidence="0.9915">
3.1 Memory-based learning
</subsectionHeader>
<bodyText confidence="0.999978913043478">
Memory-based learning is a supervised inductive algo-
rithm for learning classification tasks based on the k-
nn algorithm (Cover and Hart, 1967; Aha et al., 1991)
with various extensions for dealing with nominal features
and feature relevance weighting. Memory-based learn-
ing stores feature representations of training instances in
memory without abstraction and classifies new (test) in-
stances by matching their feature representation to all in-
stances in memory, finding the most similar instances.
From these “nearest neighbors”, the class of the test item
is extrapolated. See Daelemans et al. (2003) for a de-
tailed description of the algorithms and metrics used in
our experiments. All memory-based learning experi-
ments were done with the TiMBL software packager.
In previous research, we have found that memory-
based learning is rather sensitive to the chosen features
and the particular setting of its algorithmic parameters
(e.g. the number of nearest neighbors taken into account,
the function for extrapolation from the nearest neighbors,
the feature relevance weighting method used, etc.). In or-
der to minimize the effects of this sensitivity, we have put
much effort in trying to find the best set of features and
the optimal learner parameters for this particular task.
</bodyText>
<subsectionHeader confidence="0.999324">
3.2 Feature selection
</subsectionHeader>
<bodyText confidence="0.99969375">
We have employed bi-directional hill-climbing (Caruana
and Freitag, 1994) for finding the features that were most
suited for this task. This wrapper approach starts with the
empty set of features and evaluates the learner for every
individual feature on the development set. The feature
associated with the best performance is selected and the
process is repeated for every pair of features that includes
the best feature. For every next best set of features, the
</bodyText>
<footnote confidence="0.749908">
&apos;We used TiMBL version 5.0, available freely for research
from http://ilk.uvt.nl.
</footnote>
<bodyText confidence="0.999719">
system evaluates each set that contains one extra feature
or has one feature less. This process is repeated until the
local search does not lead to a performance gain.
</bodyText>
<subsectionHeader confidence="0.998568">
3.3 Parameter optimization
</subsectionHeader>
<bodyText confidence="0.9999922">
We used iterative deepening (ID) as a heuristic way of
searching for optimal algorithm parameters. This tech-
nique combines classifier wrapping (using the training
material internally to test experimental variants) (Kohavi
and John, 1997) with progressive sampling of training
material (Provost et al., 1999). We start with a large pool
of experiments, each with a unique combination of algo-
rithmic parameter settings. Each settings combination is
applied to a small amount of training material and tested
on a small held-out set also taken from the training set.
Only the best settings are kept; the others are removed
from the pool of competing settings. In subsequent itera-
tions, this step is repeated, retaining the best-performing
settings, with an exponentially growing amount of train-
ing and held-out data – until all training data is used or
one best setting is left. Selecting the best settings at each
step is based on classification accuracy on the held-out
data; a simple one-dimensional clustering on the ranked
list of accuracies determines which group of settings is
selected for the next iteration.
</bodyText>
<subsectionHeader confidence="0.948098">
3.4 Class n-grams
</subsectionHeader>
<bodyText confidence="0.964356423076923">
Alternative to predicting simple classes, sequential tasks
can be rephrased as mappings from input examples to
sequences of classes. Instead of predicting just A1 in
the example given earlier, it is possible to predict a tri-
gram of classes. The second example in the training data
which we used earlier, is now labeled with the trigram
A1 A1 A1, indicating that the chunk in focus has an A1
relation with the verb, along with its left and right neigh-
bor chunks (which are all part of the same A1 argument).
expect -2 -1 0 morph-cap in
IN NN PP passive clause A0VA1
A1 A1 A1
Predicting class trigrams offers two potential benefits.
First, the classifier is forced to predict ‘legal’ sequences
of classes; this potentially fixes a problem with simple
classifiers which are blind to their previous or subsequent
simple classifications in sequences, potentially resulting
in impossible sequences such as A1 A0 A1. Second,
if the classifier predicts the trigrams example by exam-
ple, it produces a sequence of overlapping trigrams which
may contain information that can boost classification ac-
curacy. Effectively, each class is predicted three times, so
that a simple majority voting can be applied: we simply
take the middle prediction as the actual classification of
the example unless the two other votes together suggest
another class label.
</bodyText>
<table confidence="0.999980461538462">
Prec. Recall FO=1 method
51.6% 51.9% 51.8 feature selection
57.3% 52.7% 54.9 parameter optimization
58.8% 54.2% 56.4 feature selection
59.5% 53.9% 56.5 parameter optimization
64.3% 54.2% 58.8 classifier stacking
66.3% 56.3% 60.9 parameter optimization
66.5% 56.3% 60.9 feature selection
68.1% 56.8% 61.9 classifier stacking
68.3% 57.5% 62.4 feature selection
68.9% 57.8% 62.9 classifier stacking
69.1% 57.8% 63.0 classifier stacking
50.6% 30.3% 37.9 baseline
</table>
<tableCaption confidence="0.98589">
Table 1: Effects of cascaded feature selection, parameter
optimization and classifier stacking on the performance
measured on the development data set.
</tableCaption>
<subsectionHeader confidence="0.70378">
3.5 Iterative classifier stacking
</subsectionHeader>
<bodyText confidence="0.999940066666667">
Stacking (Wolpert, 1992) refers to a class of meta-
learning systems that learn to correct errors made by
lower-level classifiers. We implement stacking by adding
a windowed sequence of previous and subsequent output
class labels to the original input features. To generate
the training material, we copy these windowed (unigram)
class labels into the input, excluding the focus class label
(which is a perfect predictor of the output class). To gen-
erate test material, the output of the first-stage classifier
trained on the original data is used.
Stacking can be repeated; an nth-stage classifier can be
built on the output of the n-1th-stage classifier. We im-
plemented this by replacing the class features in the input
of each nth-stage classifier by the output of the previous
classifier.
</bodyText>
<subsectionHeader confidence="0.997823">
3.6 Automatic output post-processing
</subsectionHeader>
<bodyText confidence="0.999979">
Even while employing n-gram output classes and clas-
sifier stacking, we noticed that our learner made sys-
tematic errors caused by the lack of broader (sentential)
contextual information in the instances and the classes.
The most obvious of these errors was having multiple in-
stances of arguments A0-A5 in one sentence. Although
sentences with multiple A0-A3 arguments appear in the
training data, they are quite rare (0.17%). When the
learner assigns an A0 role to three different arguments
in a sentence, most likely at least two of these are wrong.
In order to reflect this fact, we have restricted the sys-
tem to outputting at most one phrase of type A0-A5. If
the learner predicts multiple arguments then only the one
closest to the main verb is kept.
</bodyText>
<table confidence="0.998568846153846">
Features a-b c-d e-f g-h i-k
words -1–0 -2–1 -2–1 -2–1 -2–1
PoS tags 0–1 0–1 0–1 -1–1 -1–1
chunk tags 0 0–2 0–2 -1–1 -1–1
NE tags – – – – –
output classes NA NA -3–3 -3–3 -3–3
distances cNV cNVw cNVw Vw cNV
main verb + + + + +
role pattern + + + + +
passive verb + + + + +
current clause + + + + +
previous prep. – + + + –
Total 12 18 24 23 24
</table>
<tableCaption confidence="0.986175">
Table 2: Features used in the different runs mentioned
</tableCaption>
<bodyText confidence="0.979877666666667">
in Table 1. The numbers mentioned for words, part-of-
speech tags, chunk tags, named entity tags and output
classes show the position of the tokens with respect to
the focus token (0). Distances are measured in chunks,
NP chunks, VP chunks and words. In all other table en-
tries, + denotes selection and – omission.
</bodyText>
<table confidence="0.853920714285714">
Parameters a b-c d-e f-k
algorithm I131 I131 I131 I131
distance metric O M J O
switching threshold NA 2 2 NA
feature weighting nw nw nw nw
neighborhood size 1 15 19 1
class weights Z ED1 ED1 Z
</table>
<tableCaption confidence="0.87736">
Table 3: Parameters of the machines learner that were
</tableCaption>
<bodyText confidence="0.970926">
used in the different runs mentioned in Table 1. More
information about the parameters and their values can be
found in Daelemans et al. (2003).
</bodyText>
<sectionHeader confidence="0.999868" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999969125">
We started with a feature selection process with the fea-
tures described in section 2. This experiment used a
basic k-nn classifier without feature weighting, a near-
est neighborhood of size 1, attenuated words, and output
post-processing. We evaluated the effect of trigram out-
put classes by performing an experiment with and with-
out them. The feature selection experiment without tri-
gram output classes selected 10 features and obtained an
F0=1 score of 46.3 on the development data set. The ex-
periment that made use of combined classes selected 12
features and reached a score of 51.8.
We decided to continue using trigram output classes.
Subsequently, we optimized the parameters of our ma-
chine learner based on the features in the second experi-
ment and performed another feature selection experiment
with these parameters. The performance effects can be
</bodyText>
<figure confidence="0.993495636363636">
a
b
c
d
e
f
g
h
i
j
k
</figure>
<bodyText confidence="0.999517466666667">
found in Table 1 (rows b and c). An additional parameter
optimization step did not have a substantial effect (Ta-
ble 1, row d).
After training a stacked classifier while using the out-
put of the best first stage learner, performance went
up from 56.5 to 58.8. Additional feature selection
and parameter optimization were useful at this level
(F0=1=60.9, see Table 1). Most of our other performance
gain was obtained by a continued process of classifier
stacking. Parameter optimization did not result in im-
proved performance when stacking more than one classi-
fier. Feature selection was useful for the third-stage clas-
sifier but not for the next one. Our final system obtained
an F0=1 score of 63.0 on the development data (Table 1)
and 60.1 on the test set (Table 4).
</bodyText>
<sectionHeader confidence="0.9993" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999978">
We have described a memory-based semantic role labeler.
In the development of the system we have used feature
selection through bi-directional hill-climbing and param-
eter optimization through iterative deepening search. We
have evaluated n-gram output classes, classifier stacking
and output post-processing, all of which increased per-
formance. An overview of the performance of the system
on the test data can be found in Table 4.
</bodyText>
<sectionHeader confidence="0.994759" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.995555">
Sander Canisius, Iris Hendrickx, and Antal van den
Bosch are funded by NWO (Netherlands Organisation for
Scientific Research). Erik Tjong Kim Sang is funded by
IWT STWW as a researcher in the ATraNoS project.
</bodyText>
<sectionHeader confidence="0.997783" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.972672684210526">
D. W. Aha, D. Kibler, and M. Albert. 1991. Instance-
based learning algorithms. Machine Learning, 6:37–
66.
S. Buchholz, J. Veenstra, and W. Daelemans. 1999. Cas-
caded grammatical relation assignment. In EMNLP-
VLC’99, the Joint SIGDAT Conference on Empirical
Methods in Natural Language Processing and Very
Large Corpora, June.
S. Buchholz. 2002. Memory-Based Grammatical Rela-
tion Finding. PhD thesis, University of Tilburg.
X. Carreras and L. M`arquez. 2004. Introduction to the
conll-2004 shared task: Semantic role labe ling. In
Proceedings of CoNLL-2004. Boston, MA, USA.
R. Caruana and D. Freitag. 1994. Greedy attribute se-
lection. In Proceedings of the Eleventh International
Conference on Machine Learning, pages 28–36, New
Brunswick, NJ, USA. Morgan Kaufman.
T. M. Cover and P. E. Hart. 1967. Nearest neighbor
pattern classification. Institute of Electrical and Elec-
</reference>
<table confidence="0.999981466666667">
Precision Recall F0=1
Overall 67.12% 54.46% 60.13
A0 80.41% 70.18% 74.95
A1 62.04% 59.67% 60.83
A2 46.29% 35.85% 40.41
A3 59.42% 27.33% 37.44
A4 67.44% 58.00% 62.37
A5 0.00% 0.00% 0.00
AM-ADV 25.00% 4.56% 7.71
AM-CAU 0.00% 0.00% 0.00
AM-DIR 33.33% 12.00% 17.65
AM-DIS 58.38% 50.70% 54.27
AM-EXT 53.85% 50.00% 51.85
AM-LOC 38.79% 19.74% 26.16
AM-MNR 48.00% 18.82% 27.04
AM-MOD 97.11% 89.61% 93.21
AM-NEG 74.67% 88.19% 80.87
AM-PNC 44.44% 4.71% 8.51
AM-PRD 0.00% 0.00% 0.00
AM-TMP 58.84% 32.53% 41.90
R-A0 80.26% 76.73% 78.46
R-A1 78.95% 42.86% 55.56
R-A2 100.00% 22.22% 36.36
R-A3 0.00% 0.00% 0.00
R-AA 0.00% 0.00% 0.00
R-AM-LOC 0.00% 0.00% 0.00
R-AM-MNR 0.00% 0.00% 0.00
R-AM-PNC 0.00% 0.00% 0.00
R-AM-TMP 66.67% 14.29% 23.53
V 97.93% 97.93% 97.93
</table>
<tableCaption confidence="0.8409595">
Table 4: The performance of our system measured on the
test data.
</tableCaption>
<reference confidence="0.997124047619048">
tronics Engineers Transactions on Information Theory,
13:21–27.
W. Daelemans, J. Zavrel, K. Van der Sloot, and A. Van
den Bosch. 2003. TiMBL: Tilburg memory based
learner, version 5.0, reference guide. ILK Techni-
cal Report 03-08, Tilburg University. available from
http://ilk.uvt.nl/downloads/pub/papers/ilk.0308.ps.
J. Eisner. 1996. An Empirical Comparison of Probabil-
ity Models for Dependency Grammar. Technical Re-
port IRCS-96-11, Institute for Research in Cognitive
Science, University of Pennsylvania.
R. Kohavi and G. John. 1997. Wrappers for feature
subset selection. Artificial Intelligence Journal, 97(1–
2):273–324.
F. Provost, D. Jensen, and T. Oates. 1999. Efficient pro-
gressive sampling. In Proceedings of the Fifth Interna-
tional Conference on Knowledge Discovery and Data
Mining, pages 23–32.
D. H. Wolpert. 1992. On overfitting avoidance as bias.
Technical Report SFI TR 92-03-5001, The Santa Fe
Institute.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.635500">
<title confidence="0.995351">Memory-based semantic role Optimizing features, algorithm, and output</title>
<author confidence="0.834386">Antal van_den_Bosch</author>
<author confidence="0.834386">Sander Canisius</author>
<author confidence="0.834386">Walter Daelemans</author>
<author confidence="0.834386">Iris Hendrickx Erik Tjong Kim Sang</author>
<affiliation confidence="0.984025">ILK / Computational Linguistics CNTS / Department of Tilburg University, P.O. Box 90153, University of Antwerp, Universiteitsplein 1,</affiliation>
<address confidence="0.989923">NL-5000 LE Tilburg, The Netherlands B-2610 Antwerpen, Belgium</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D W Aha</author>
<author>D Kibler</author>
<author>M Albert</author>
</authors>
<title>Instancebased learning algorithms.</title>
<date>1991</date>
<journal>Machine Learning,</journal>
<volume>6</volume>
<pages>66</pages>
<contexts>
<context position="5033" citStr="Aha et al., 1991" startWordPosition="813" endWordPosition="816">mantic role labeling. The core part of our system is a memory-based learner. During the development of the system we have used feature selection and parameter optimization by iterative deepening. Additionally we have evaluated three extensions of the basic memory-based learning method: class n-grams, i.e. complex classes composed of sequences of simple classes, iterative classifier stacking and automatic output post-processing. 3.1 Memory-based learning Memory-based learning is a supervised inductive algorithm for learning classification tasks based on the knn algorithm (Cover and Hart, 1967; Aha et al., 1991) with various extensions for dealing with nominal features and feature relevance weighting. Memory-based learning stores feature representations of training instances in memory without abstraction and classifies new (test) instances by matching their feature representation to all instances in memory, finding the most similar instances. From these “nearest neighbors”, the class of the test item is extrapolated. See Daelemans et al. (2003) for a detailed description of the algorithms and metrics used in our experiments. All memory-based learning experiments were done with the TiMBL software pack</context>
</contexts>
<marker>Aha, Kibler, Albert, 1991</marker>
<rawString>D. W. Aha, D. Kibler, and M. Albert. 1991. Instancebased learning algorithms. Machine Learning, 6:37– 66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Buchholz</author>
<author>J Veenstra</author>
<author>W Daelemans</author>
</authors>
<title>Cascaded grammatical relation assignment.</title>
<date>1999</date>
<booktitle>In EMNLPVLC’99, the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,</booktitle>
<contexts>
<context position="671" citStr="Buchholz et al. (1999)" startWordPosition="82" endWordPosition="85">eatures, algorithm, and output Antal van den Bosch, Sander Canisius, Walter Daelemans, Iris Hendrickx Erik Tjong Kim Sang ILK / Computational Linguistics CNTS / Department of Linguistics Tilburg University, P.O. Box 90153, University of Antwerp, Universiteitsplein 1, NL-5000 LE Tilburg, The Netherlands B-2610 Antwerpen, Belgium {Antal.vdnBosch,S.V.M.Canisius, {Walter.Daelemans, I.H.E.Hendrickx}@uvt.nl Erik.TjongKimSang}@ua.ac.be 1 Introduction In this paper we interpret the semantic role labeling problem as a classification task, and apply memory-based learning to it in an approach similar to Buchholz et al. (1999) and Buchholz (2002) for grammatical relation labeling. We apply feature selection and algorithm parameter optimization strategies to our learner. In addition, we investigate the effect of two innovations: (i) the use of sequences of classes as classification output, combined with a simple voting mechanism, and (ii) the use of iterative classifier stacking which takes as input the original features and a pattern of outputs of a first-stage classifier. Our claim is that both methods avoid errors in sequences of predictions typically made by simple classifiers that are unaware of their previous </context>
</contexts>
<marker>Buchholz, Veenstra, Daelemans, 1999</marker>
<rawString>S. Buchholz, J. Veenstra, and W. Daelemans. 1999. Cascaded grammatical relation assignment. In EMNLPVLC’99, the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Buchholz</author>
</authors>
<title>Memory-Based Grammatical Relation Finding.</title>
<date>2002</date>
<tech>PhD thesis,</tech>
<institution>University of Tilburg.</institution>
<contexts>
<context position="691" citStr="Buchholz (2002)" startWordPosition="87" endWordPosition="88">put Antal van den Bosch, Sander Canisius, Walter Daelemans, Iris Hendrickx Erik Tjong Kim Sang ILK / Computational Linguistics CNTS / Department of Linguistics Tilburg University, P.O. Box 90153, University of Antwerp, Universiteitsplein 1, NL-5000 LE Tilburg, The Netherlands B-2610 Antwerpen, Belgium {Antal.vdnBosch,S.V.M.Canisius, {Walter.Daelemans, I.H.E.Hendrickx}@uvt.nl Erik.TjongKimSang}@ua.ac.be 1 Introduction In this paper we interpret the semantic role labeling problem as a classification task, and apply memory-based learning to it in an approach similar to Buchholz et al. (1999) and Buchholz (2002) for grammatical relation labeling. We apply feature selection and algorithm parameter optimization strategies to our learner. In addition, we investigate the effect of two innovations: (i) the use of sequences of classes as classification output, combined with a simple voting mechanism, and (ii) the use of iterative classifier stacking which takes as input the original features and a pattern of outputs of a first-stage classifier. Our claim is that both methods avoid errors in sequences of predictions typically made by simple classifiers that are unaware of their previous or subsequent decisi</context>
</contexts>
<marker>Buchholz, 2002</marker>
<rawString>S. Buchholz. 2002. Memory-Based Grammatical Relation Finding. PhD thesis, University of Tilburg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Carreras</author>
<author>L M`arquez</author>
</authors>
<title>Introduction to the conll-2004 shared task: Semantic role labe ling.</title>
<date>2004</date>
<booktitle>In Proceedings of CoNLL-2004.</booktitle>
<location>Boston, MA, USA.</location>
<marker>Carreras, M`arquez, 2004</marker>
<rawString>X. Carreras and L. M`arquez. 2004. Introduction to the conll-2004 shared task: Semantic role labe ling. In Proceedings of CoNLL-2004. Boston, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Caruana</author>
<author>D Freitag</author>
</authors>
<title>Greedy attribute selection.</title>
<date>1994</date>
<booktitle>In Proceedings of the Eleventh International Conference on Machine Learning,</booktitle>
<pages>28--36</pages>
<publisher>Morgan Kaufman.</publisher>
<location>New Brunswick, NJ, USA.</location>
<contexts>
<context position="6247" citStr="Caruana and Freitag, 1994" startWordPosition="997" endWordPosition="1000">oftware packager. In previous research, we have found that memorybased learning is rather sensitive to the chosen features and the particular setting of its algorithmic parameters (e.g. the number of nearest neighbors taken into account, the function for extrapolation from the nearest neighbors, the feature relevance weighting method used, etc.). In order to minimize the effects of this sensitivity, we have put much effort in trying to find the best set of features and the optimal learner parameters for this particular task. 3.2 Feature selection We have employed bi-directional hill-climbing (Caruana and Freitag, 1994) for finding the features that were most suited for this task. This wrapper approach starts with the empty set of features and evaluates the learner for every individual feature on the development set. The feature associated with the best performance is selected and the process is repeated for every pair of features that includes the best feature. For every next best set of features, the &apos;We used TiMBL version 5.0, available freely for research from http://ilk.uvt.nl. system evaluates each set that contains one extra feature or has one feature less. This process is repeated until the local sea</context>
</contexts>
<marker>Caruana, Freitag, 1994</marker>
<rawString>R. Caruana and D. Freitag. 1994. Greedy attribute selection. In Proceedings of the Eleventh International Conference on Machine Learning, pages 28–36, New Brunswick, NJ, USA. Morgan Kaufman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T M Cover</author>
<author>P E Hart</author>
</authors>
<title>Nearest neighbor pattern classification.</title>
<date>1967</date>
<booktitle>Institute of Electrical and Electronics Engineers Transactions on Information Theory,</booktitle>
<pages>13--21</pages>
<contexts>
<context position="5014" citStr="Cover and Hart, 1967" startWordPosition="809" endWordPosition="812">ibe our approach to semantic role labeling. The core part of our system is a memory-based learner. During the development of the system we have used feature selection and parameter optimization by iterative deepening. Additionally we have evaluated three extensions of the basic memory-based learning method: class n-grams, i.e. complex classes composed of sequences of simple classes, iterative classifier stacking and automatic output post-processing. 3.1 Memory-based learning Memory-based learning is a supervised inductive algorithm for learning classification tasks based on the knn algorithm (Cover and Hart, 1967; Aha et al., 1991) with various extensions for dealing with nominal features and feature relevance weighting. Memory-based learning stores feature representations of training instances in memory without abstraction and classifies new (test) instances by matching their feature representation to all instances in memory, finding the most similar instances. From these “nearest neighbors”, the class of the test item is extrapolated. See Daelemans et al. (2003) for a detailed description of the algorithms and metrics used in our experiments. All memory-based learning experiments were done with the </context>
</contexts>
<marker>Cover, Hart, 1967</marker>
<rawString>T. M. Cover and P. E. Hart. 1967. Nearest neighbor pattern classification. Institute of Electrical and Electronics Engineers Transactions on Information Theory, 13:21–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>J Zavrel</author>
<author>K Van der Sloot</author>
<author>A Van den Bosch</author>
</authors>
<title>TiMBL: Tilburg memory based learner, version 5.0, reference guide.</title>
<date>2003</date>
<tech>ILK Technical Report 03-08,</tech>
<institution>Tilburg University.</institution>
<note>available from http://ilk.uvt.nl/downloads/pub/papers/ilk.0308.ps.</note>
<marker>Daelemans, Zavrel, Van der Sloot, Van den Bosch, 2003</marker>
<rawString>W. Daelemans, J. Zavrel, K. Van der Sloot, and A. Van den Bosch. 2003. TiMBL: Tilburg memory based learner, version 5.0, reference guide. ILK Technical Report 03-08, Tilburg University. available from http://ilk.uvt.nl/downloads/pub/papers/ilk.0308.ps.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
</authors>
<title>An Empirical Comparison of Probability Models for Dependency Grammar.</title>
<date>1996</date>
<tech>Technical Report IRCS-96-11,</tech>
<institution>Institute for Research in Cognitive Science, University of Pennsylvania.</institution>
<contexts>
<context position="2533" citStr="Eisner, 1996" startWordPosition="385" endWordPosition="386">nks to which the same role label is assigned by our classifier as belonging to the same argument. All results reported below use this task representation. Processing focuses on one verb at a time; verbs are treated independently. We did not employ the provided Propbank data nor the verb sense information available, nor did we use any other external source of information. Apart from the provided words and the predicted PoS tags, chunk labels, clause labels, and named-entity labels, provided beforehand, we have considered an additional set of automatically derived features: 1. attenuated words (Eisner, 1996), i.e. wordforms occurring below a frequency threshold (of 10) are converted to a string capturing some of the original word form’s features (capitalization, whether it contains numbers or a hyphen, or suffix letters); 2. the distance between the candidate role word and the verb, measured in intervening words, chunks, NP chunks or VP chunks (negative if the word is to the left, positive if it is to the right of the verb); 3. preceding preposition: a feature containing the head word of the previous chunk if that was labeled as preposition; 4. passive main verb: a binary feature which is on if t</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>J. Eisner. 1996. An Empirical Comparison of Probability Models for Dependency Grammar. Technical Report IRCS-96-11, Institute for Research in Cognitive Science, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kohavi</author>
<author>G John</author>
</authors>
<title>Wrappers for feature subset selection.</title>
<date>1997</date>
<journal>Artificial Intelligence Journal,</journal>
<volume>97</volume>
<issue>1</issue>
<pages>2--273</pages>
<contexts>
<context position="7152" citStr="Kohavi and John, 1997" startWordPosition="1139" endWordPosition="1142">epeated for every pair of features that includes the best feature. For every next best set of features, the &apos;We used TiMBL version 5.0, available freely for research from http://ilk.uvt.nl. system evaluates each set that contains one extra feature or has one feature less. This process is repeated until the local search does not lead to a performance gain. 3.3 Parameter optimization We used iterative deepening (ID) as a heuristic way of searching for optimal algorithm parameters. This technique combines classifier wrapping (using the training material internally to test experimental variants) (Kohavi and John, 1997) with progressive sampling of training material (Provost et al., 1999). We start with a large pool of experiments, each with a unique combination of algorithmic parameter settings. Each settings combination is applied to a small amount of training material and tested on a small held-out set also taken from the training set. Only the best settings are kept; the others are removed from the pool of competing settings. In subsequent iterations, this step is repeated, retaining the best-performing settings, with an exponentially growing amount of training and held-out data – until all training data</context>
</contexts>
<marker>Kohavi, John, 1997</marker>
<rawString>R. Kohavi and G. John. 1997. Wrappers for feature subset selection. Artificial Intelligence Journal, 97(1– 2):273–324.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Provost</author>
<author>D Jensen</author>
<author>T Oates</author>
</authors>
<title>Efficient progressive sampling.</title>
<date>1999</date>
<booktitle>In Proceedings of the Fifth International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>23--32</pages>
<contexts>
<context position="7222" citStr="Provost et al., 1999" startWordPosition="1149" endWordPosition="1152">every next best set of features, the &apos;We used TiMBL version 5.0, available freely for research from http://ilk.uvt.nl. system evaluates each set that contains one extra feature or has one feature less. This process is repeated until the local search does not lead to a performance gain. 3.3 Parameter optimization We used iterative deepening (ID) as a heuristic way of searching for optimal algorithm parameters. This technique combines classifier wrapping (using the training material internally to test experimental variants) (Kohavi and John, 1997) with progressive sampling of training material (Provost et al., 1999). We start with a large pool of experiments, each with a unique combination of algorithmic parameter settings. Each settings combination is applied to a small amount of training material and tested on a small held-out set also taken from the training set. Only the best settings are kept; the others are removed from the pool of competing settings. In subsequent iterations, this step is repeated, retaining the best-performing settings, with an exponentially growing amount of training and held-out data – until all training data is used or one best setting is left. Selecting the best settings at e</context>
</contexts>
<marker>Provost, Jensen, Oates, 1999</marker>
<rawString>F. Provost, D. Jensen, and T. Oates. 1999. Efficient progressive sampling. In Proceedings of the Fifth International Conference on Knowledge Discovery and Data Mining, pages 23–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D H Wolpert</author>
</authors>
<title>On overfitting avoidance as bias.</title>
<date>1992</date>
<tech>Technical Report SFI TR 92-03-5001,</tech>
<institution>The Santa Fe Institute.</institution>
<contexts>
<context position="10056" citStr="Wolpert, 1992" startWordPosition="1597" endWordPosition="1598">tion 57.3% 52.7% 54.9 parameter optimization 58.8% 54.2% 56.4 feature selection 59.5% 53.9% 56.5 parameter optimization 64.3% 54.2% 58.8 classifier stacking 66.3% 56.3% 60.9 parameter optimization 66.5% 56.3% 60.9 feature selection 68.1% 56.8% 61.9 classifier stacking 68.3% 57.5% 62.4 feature selection 68.9% 57.8% 62.9 classifier stacking 69.1% 57.8% 63.0 classifier stacking 50.6% 30.3% 37.9 baseline Table 1: Effects of cascaded feature selection, parameter optimization and classifier stacking on the performance measured on the development data set. 3.5 Iterative classifier stacking Stacking (Wolpert, 1992) refers to a class of metalearning systems that learn to correct errors made by lower-level classifiers. We implement stacking by adding a windowed sequence of previous and subsequent output class labels to the original input features. To generate the training material, we copy these windowed (unigram) class labels into the input, excluding the focus class label (which is a perfect predictor of the output class). To generate test material, the output of the first-stage classifier trained on the original data is used. Stacking can be repeated; an nth-stage classifier can be built on the output </context>
</contexts>
<marker>Wolpert, 1992</marker>
<rawString>D. H. Wolpert. 1992. On overfitting avoidance as bias. Technical Report SFI TR 92-03-5001, The Santa Fe Institute.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>