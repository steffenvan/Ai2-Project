<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.169134">
<title confidence="0.996646">
Referring Expression Generation Using Speaker-based Attribute Selection
and Trainable Realization (ATTR)
</title>
<author confidence="0.540262">
Giuseppe Di Fabbrizio and Amanda J. Stent and Srinivas Bangalore
</author>
<affiliation confidence="0.463226">
AT&amp;T Labs - Research, Inc.
</affiliation>
<address confidence="0.87667">
180 Park Avenue
Florham Park, NJ 07932, USA
</address>
<email confidence="0.998563">
{pino,stent,srini}@research.att.com
</email>
<sectionHeader confidence="0.993859" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997948833333333">
In the first REG competition, researchers
proposed several general-purpose algorithms
for attribute selection for referring expression
generation. However, most of this work did
not take into account: a) stylistic differences
between speakers; or b) trainable surface re-
alization approaches that combine semantic
and word order information. In this paper we
describe and evaluate several end-to-end re-
ferring expression generation algorithms that
take into consideration speaker style and use
data-driven surface realization techniques.
</bodyText>
<sectionHeader confidence="0.998795" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999983771929825">
There now exist numerous general-purpose algo-
rithms for attribute selection used in referring ex-
pression generation (e.g., (Dale and Reiter, 1995;
Krahmer et al., 2003; Belz and Gatt, 2007)). How-
ever, these algorithms by-and-large focus on the al-
gorithmic aspects of referring expression generation
rather than on psycholinguistic factors that influence
language production. For example, we know that
humans exhibit individual style differences during
language production that can be quite pronounced
(e.g. (Belz, 2007)). We also know that the lan-
guage production process is subject to lexical prim-
ing, which means that words and concepts that have
been used recently are likely to appear again (Levelt,
1989).
In this paper, we first explore the impact of indi-
vidual style and priming on attribute selection for
referring expression generation. To get an idea
of the potential improvement when modeling these
factors, we implemented a version of full brevity
search (Dale, 1992) that uses speaker-specific con-
straints, and another version that also uses recency
constraints. We found that using speaker-specific
constraints led to big performance gains for both
TUNA domains, while the use of recency constraints
was not as effective for TUNA-style tasks. We then
modified Dale and Reiter’s classic attribute selection
algorithm (Dale and Reiter, 1995) to model speaker-
specific constraints, and found performance gains in
this more greedy approach as well.
Then we looked at surface realization for referring
expression generation. There are several approaches
to surface realization described in the literature (Re-
iter and Dale, 2000) ranging from hand-crafted
template-based realizers to data-driven syntax-based
realizers (Langkilde and Knight, 2000; Bangalore
and Rambow, 2000). Template-based realization
involves the insertion of attribute values into pre-
determined templates. Data-driven syntax-based
methods use syntactic relations between words (in-
cluding long-distance relations) for word ordering.
Other data-driven techniques exhaustively generate
possible realizations with recourse to syntax in as
much as it is reflected in local n-grams. Such tech-
niques have the advantage of being robust although
they are inadequate to capture long-range depen-
dencies. In this paper, we explore three techniques
for the task of referring expression generation that
are different hybrids of hand-crafted and data-driven
methods.
The remainder of this paper is organized as fol-
lows: In Section 2, we present the algorithms for
attribute selection. The different methods for sur-
face realizers are presented in Section 3. The exper-
iments concerning the attribute selection and surface
realization are presented in Section 4 and Section 5.
The final remarks are discussed in Section 6.
</bodyText>
<sectionHeader confidence="0.842678" genericHeader="method">
2 Attribute Selection Algorithms
</sectionHeader>
<footnote confidence="0.7618435">
Full Brevity (FB) We implemented a version of
full brevity search (Dale, 1992). It does the follow-
</footnote>
<page confidence="0.998543">
211
</page>
<bodyText confidence="0.996928638888889">
ing: first, it constructs AS, the set of attribute sets
that uniquely identify the referent given the distrac-
tors. Then, it selects an attribute set ASu E AS
based on a selection criterion. The minimality (FB-
m) criterion selects from among the smallest ele-
ments of AS at random. The frequency (FB-f) cri-
terion selects from among the elements of AS the
one that occurred most often in the training data.
The speaker frequency (FB-sf) criterion selects
from among the elements of AS the one used most
often by this speaker in the training data, backing off
to FB-f if necessary. This criterion models speaker-
specific constraints. Finally, the speaker recency
(FB-sr) criterion selects from among the elements
of AS the one used most recently by this speaker in
the training data, backing off to FB-sf if necessary.
This criterion models priming and speaker-specific
constraints.
Dale and Reiter We implemented two variants of
the classic Dale &amp; Reiter attribute selection (Dale
and Reiter, 1995) algorithm. For Dale &amp; Reiter
basic (DR-b), we first build the preferred list of
attributes by sorting the most frequently used at-
tributes in the training set. We keep separate lists
based upon the “+LOC” and “-LOC” conditions
and backoff to a global preferred frequency list in
case the attributes are not covered in the current list
(merge and sort by frequency). Next, we iterate over
the list of preferred attributes and select the next one
that rules out at least one entity in the contrast set
until no distractors are left. The Dale &amp; Reiter
speaker frequency (DR-sf) uses a speaker-specific
preferred list, backing off to the DR-b preferred list
if an attribute is not in the current speaker’s preferred
list. For this task, we ignored any further attribute
knowledge base or taxonomy abstraction.
</bodyText>
<sectionHeader confidence="0.937849" genericHeader="method">
3 Surface Realization Approaches
</sectionHeader>
<bodyText confidence="0.999965285714286">
We summarize our approaches to surface realization
in this section. All three surface realizers have the
same four stages: (a) lexical choice of words and
phrases for the attribute values; (b) generation of a
space of surface realizations (T); (c) ranking the set
of realizations using a language model (LM); (d)
selecting the best scoring realization.
</bodyText>
<equation confidence="0.995712">
T∗ = BestPath(Rank(T, LM)) (1)
</equation>
<bodyText confidence="0.999305098039216">
Template-Based Realizer To construct our
template-based realizer, we extract the annotated
word string from each trial in the training data
and replace each annotated text segment with the
attribute type with which it is annotated. The key
for each template is the lexicographically sorted list
of attribute types it contains. Consequently, any
attribute lists not found in the training data cannot
be realized by the template-based realizer; however,
if there is a template for an input attribute list it is
quite likely to be coherent.
At generation time, we find all possible realiza-
tions of each attribute in the input attribute set, and
fill in each possible template with each combina-
tion of the attribute realizations. We report results
for two versions of this realizer: one with speaker-
specific lexicon and templates (Template-S), and
one without (Template).
Dependency-Based Realizer To construct our
dependency-based realizer, we first parse all the
word strings from the training data using the depen-
dency parser described in (Bangalore et al., 2005;
Nasr and Rambow, 2004). Then, for every pair
of words wi, wj that occur in the same referring
expression (RE) in the training data, we compute:
freq(i &lt; j), the frequency with which wi pre-
cedes wj in any RE; freq(i = j − 1), the fre-
quency with which wi immediately precedes wj in
any RE; freq(dep(wi, wj) n i &lt; j), the frequency
with which wi depends on and precedes wj in any
RE, and freq(dep(wi, wj) n j &lt; i), the frequency
with which wi depends on and follows wj in any RE.
At generation time, we find all possible realiza-
tions of each attribute in the input attribute set, and
for each combination of attribute realizations, we
find the most likely set of dependencies and prece-
dences given the training data.
Permute and Rank In this method, the lexical
items associated with each of the attribute value to
be realized are treated as a disjunctive set of tokens.
This disjunctive set is represented as a finite-state
automaton with two states and transitions between
them labeled with the tokens of the set. The transi-
tions are weighted by the negative logarithm of the
probability of the lexical token (w) being associated
with that attribute value (attr): (−log(P(w|attr))).
These sets are treated as unordered bags of tokens;
we create permutations of these bags of tokens to
represent the set of possible surface realizations. We
then use the language model to rank this set of possi-
ble realizations and recover the highest scoring RE.
</bodyText>
<page confidence="0.982808">
212
</page>
<table confidence="0.999951772727273">
DICE MASI Acc. Uniq. Min.
Furniture
FB-m .36 .16 0 1 1
FB-f .81 .58 .40 1 0
FB-sf .95 .87 .79 1 0
FB-sr .93 .81 .71 1 0
DR-b .81 .60 .45 1 0
DR-sf .86 .64 .45 1 .04
People
FB-m .26 .12 0 1 1
FB-f .58 .37 .28 1 0
FB-sf .94 .88 .84 1 .01
FB-sr .93 .85 .79 1 .01
DR-b .70 .45 .25 1 0
DR-sf .78 .55 .35 1 0
Overall
FB-m .32 .14 0 1 1
FB-f .70 .48 .34 1 0
FB-sf .95 .87 .81 1 .01
FB-sr .93 .83 .75 1 .01
DR-b .76 .53 .36 1 0
DR-sf .82 .60 .41 1 .02
</table>
<tableCaption confidence="0.999504">
Table 1: Results for attribute selection
</tableCaption>
<bodyText confidence="0.99993175">
Unfortunately, the number of states of the min-
imal permutation automaton of even a linear au-
tomata (finite-state machine representation of a
string) grows exponentially with the number of
words of the string. So, instead of creating a full
permutation automaton, we choose to constrain per-
mutations to be within a local window of adjustable
size (also see (Kanthak et al., 2005)).
</bodyText>
<sectionHeader confidence="0.993441" genericHeader="method">
4 Attribute Selection Experiments
</sectionHeader>
<bodyText confidence="0.998838">
Data Preparation The training data were used to
build the models outlined above. The development
data were then processed one-by-one. For our final
submissions, we use training and development data
to build our models.
Results Table 1 shows the results for variations of
full brevity. As we would expect, all approaches
achieve a perfect score on uniqueness. For both cor-
pora, we see a large performance jump when we
use speaker constraints. However, when we incor-
porate recency constraints as well performance de-
clines slightly. We think this is due to two factors:
first, the speakers are not in a conversation, and self-
priming may have less impact; and second, we do
not always have the most recent prior utterance for a
given speaker in the training data.
</bodyText>
<tableCaption confidence="0.934779">
Table 1 also shows the results for variations of
Dale and Reiter’s algorithm. When we incorpo-
</tableCaption>
<table confidence="0.999886764705882">
String-Edit Dist. Accuracy
Furniture
DEV FB-sf DR-sf DEV FB-sf DR-sf
Permute&amp;Rank 4.39 4.60 4.74 0.07 0.04 0.03
Dependency 3.90 4.25 5.50 0.14 0.06 0.03
Template 4.36 4.33 5.39 0.07 0.05 0.03
Template-S 3.52 3.81 5.16 0.28 0.20 0.04
People
Permute&amp;Rank 6.26 6.46 7.01 0.01 0.01 0.00
Dependency 3.96 4.32 7.03 0.06 0.06 0.00
Template 5.16 4.62 7.26 0.03 0.06 0.00
Template-S 4.25 4.31 7.04 0.18 0.13 0.00
Overall
Permute&amp;Rank 5.25 5.45 5.78 0.05 0.03 0.01
Dependency 3.93 4.28 6.20 0.07 0.06 0.01
Template 4.73 4.46 6.25 0.05 0.05 0.01
Template-S 3.86 4.04 6.03 0.23 0.17 0.02
</table>
<tableCaption confidence="0.990277">
Table 2: Results for realization
</tableCaption>
<bodyText confidence="0.999785692307692">
rate speaker constraints, we again see a performance
jump, although compared to the best possible case
(full brevity) there is still room for improvement.
Discussion We have shown that by using speaker
and recency constraints in standard algorithms, it
is possible to achieve performance gains on the at-
tribute selection task.
The most relevant previous research is the work of
(Gupta and Stent, 2005), who modified Dale and Re-
iter’s algorithm to model speaker adaptation in dia-
log. However, this corpus does not involve dialog so
there are no cross-speaker constraints, only within-
speaker constraints (style and priming).
</bodyText>
<sectionHeader confidence="0.956975" genericHeader="method">
5 Surface Realization Experiments
</sectionHeader>
<bodyText confidence="0.995235625">
Data Preparation We first normalize the training
data to correct misspellings and remove punctuation
and capitalization. We then extract a phrasal lexi-
con. For each attribute value we extract the count of
all realizations of that value in the training data. We
treat locations as a special case, storing separately
the realizations of x-y coordinate pairs and single
x- or y-coordinates. We add a small number of re-
alizations to the lexicon by hand to cover possible
attribute values not seen in the training data.
Results Table 2 shows the evaluation results for
string-edit distance and string accuracy on the devel-
opment set with three different attributes sets: DEV
– attributes selected by the human test; FB-sf – at-
tributes generated by the full brevity algorithm with
speaker frequency; and DR-sf – attributes selected
</bodyText>
<page confidence="0.997881">
213
</page>
<bodyText confidence="0.999624323529412">
by the Dale &amp; Reiter algorithm with speaker fre-
quency.
For the TUNA realization task (DEV attributes),
our approaches work better for the furniture domain,
where there are fewer attributes, than for the people
domain. For the furniture domain, the Template-S
approach achieves lowest string-edit distance, while
for the people domain, the Dependency approach
achieves lowest string-edit distance. The latter
method was submitted for human evaluation.
When we consider the “end-to-end” referring
expression generation task (FB-sf and DR-sf at-
tributes), the best overall performing system is the
speaker-based template generator with full-brevity
and speaker frequency attribute selection. In terms
of generated sentence quality, a preliminary and
qualitative analysis shows that the combination Per-
mute &amp; Rank and DR-sf produces more naturalistic
phrases.
Discussion Although the Template-S approach
achieves the best string edit distance scores over-
all, it is not very robust. If no examples were found
in the training data neither Template approach will
produce no output. (This happens twice for each of
the domains on the development data.) The Depen-
dency approach achieves good overall performance
with more robustness.
The biggest cause of errors for the Permute
and Reorder approach was missing determiners and
missing modifiers. The biggest cause of errors for
the Dependency approach was missing determiners
and reordered words. The Template approach some-
times had repeated words (e.g. “middle”, where
“middle” referred to both x- and y-coordinates).
</bodyText>
<sectionHeader confidence="0.999499" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999929705882353">
When building computational models of language,
knowledge about the factors that influence human
language production can prove very helpful. This
knowledge can be incorporated in frequentist and
heuristic approaches as constraints or features. In
the experiments described in this paper, we used
data-driven, speaker-aware approaches to attribute
selection and referring expression realization. We
showed that individual speaking style can be use-
fully modeled even for quite ’small’ generation
tasks, and confirmed that data-driven approaches to
surface realization can work well using a range of
lexical, syntactic and semantic information.
In addition to individual style and priming, an-
other potentially fruitful area for exploration with
TUNA-style tasks is human visual search strategies
(Rayner, 1998). We leave this idea for future work.
</bodyText>
<sectionHeader confidence="0.996558" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99980425">
We thank Anja Belz, Albert Gatt, and Eric Kow
for organizing the REG competition and providing
data, and Gregory Zelinsky for discussions about
visually-based constraints.
</bodyText>
<sectionHeader confidence="0.999182" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99974595">
S. Bangalore and O. Rambow. 2000. Exploiting a prob-
abilistic hierarchical model for generation. In Proc.
COLING.
S. Bangalore, A. Emami, and P. Haffner. 2005. Factor-
ing global inference by enriching local representations.
Technical report, AT&amp;T Labs-Research.
A. Belz and A. Gatt. 2007. The attribute selection for
GRE challenge: Overview and evaluation results. In
Proceedings of UCNLG+MT at MT Summit XI.
A. Belz. 2007. Probabilistic generation of weather fore-
cast texts. In Proceedings of NAACL/HLT.
R. Dale and E. Reiter. 1995. Computational interpreta-
tions of the Gricean maxims in the generation of refer-
ring expressions. Cognitive Science, 19(2).
Robert Dale. 1992. Generating Referring Expressions:
Constructing Descriptions in a Domain of Objects and
Processes. MIT Press, Cambridge, MA.
S. Gupta and A. Stent. 2005. Automatic evaluation of
referring expression generation using corpora. In Pro-
ceedings of UCNLG.
S. Kanthak, D. Vilar, E. Matusov, R. Zens, and H. Ney.
2005. Novel reordering approaches in phrase-based
statistical machine translation. In Proc. ACL Work-
shop on Building and Using Parallel Texts.
E. Krahmer, S. van Erk, and A. Verleg. 2003. Graph-
based generation of referring expressions. Computa-
tional Linguistics, 29(1).
I. Langkilde and K. Knight. 2000. Forest-based statisti-
cal sentence generation. In Proc. NAACL.
W. Levelt, 1989. Speaking: From intention to articula-
tion, pages 222–226. MIT Press.
A. Nasr and O. Rambow. 2004. Supertagging and
full parsing. In Proc. 7th International Workshop
on Tree Adjoining Grammar and Related Formalisms
(TAG+7).
K. Rayner. 1998. Eye movements in reading and infor-
mation processing: 20 years of research. Psychologi-
cal Bulletin, 124(3).
E. Reiter and R. Dale. 2000. Building Natural Language
Generation Systems. Cambridge University Press.
</reference>
<page confidence="0.998953">
214
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.955156">
<title confidence="0.983884">Referring Expression Generation Using Speaker-based Attribute and Trainable Realization (ATTR)</title>
<author confidence="0.999992">Di_Fabbrizio J Stent</author>
<affiliation confidence="0.999753">AT&amp;T Labs - Research,</affiliation>
<address confidence="0.9958705">180 Park Florham Park, NJ 07932,</address>
<abstract confidence="0.999634461538461">In the first REG competition, researchers proposed several general-purpose algorithms for attribute selection for referring expression generation. However, most of this work did not take into account: a) stylistic differences between speakers; or b) trainable surface realization approaches that combine semantic and word order information. In this paper we describe and evaluate several end-to-end referring expression generation algorithms that take into consideration speaker style and use data-driven surface realization techniques.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Bangalore</author>
<author>O Rambow</author>
</authors>
<title>Exploiting a probabilistic hierarchical model for generation.</title>
<date>2000</date>
<booktitle>In Proc. COLING.</booktitle>
<contexts>
<context position="2620" citStr="Bangalore and Rambow, 2000" startWordPosition="370" endWordPosition="373">or both TUNA domains, while the use of recency constraints was not as effective for TUNA-style tasks. We then modified Dale and Reiter’s classic attribute selection algorithm (Dale and Reiter, 1995) to model speakerspecific constraints, and found performance gains in this more greedy approach as well. Then we looked at surface realization for referring expression generation. There are several approaches to surface realization described in the literature (Reiter and Dale, 2000) ranging from hand-crafted template-based realizers to data-driven syntax-based realizers (Langkilde and Knight, 2000; Bangalore and Rambow, 2000). Template-based realization involves the insertion of attribute values into predetermined templates. Data-driven syntax-based methods use syntactic relations between words (including long-distance relations) for word ordering. Other data-driven techniques exhaustively generate possible realizations with recourse to syntax in as much as it is reflected in local n-grams. Such techniques have the advantage of being robust although they are inadequate to capture long-range dependencies. In this paper, we explore three techniques for the task of referring expression generation that are different h</context>
</contexts>
<marker>Bangalore, Rambow, 2000</marker>
<rawString>S. Bangalore and O. Rambow. 2000. Exploiting a probabilistic hierarchical model for generation. In Proc. COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bangalore</author>
<author>A Emami</author>
<author>P Haffner</author>
</authors>
<title>Factoring global inference by enriching local representations.</title>
<date>2005</date>
<tech>Technical report, AT&amp;T Labs-Research.</tech>
<contexts>
<context position="7033" citStr="Bangalore et al., 2005" startWordPosition="1069" endWordPosition="1072">er; however, if there is a template for an input attribute list it is quite likely to be coherent. At generation time, we find all possible realizations of each attribute in the input attribute set, and fill in each possible template with each combination of the attribute realizations. We report results for two versions of this realizer: one with speakerspecific lexicon and templates (Template-S), and one without (Template). Dependency-Based Realizer To construct our dependency-based realizer, we first parse all the word strings from the training data using the dependency parser described in (Bangalore et al., 2005; Nasr and Rambow, 2004). Then, for every pair of words wi, wj that occur in the same referring expression (RE) in the training data, we compute: freq(i &lt; j), the frequency with which wi precedes wj in any RE; freq(i = j − 1), the frequency with which wi immediately precedes wj in any RE; freq(dep(wi, wj) n i &lt; j), the frequency with which wi depends on and precedes wj in any RE, and freq(dep(wi, wj) n j &lt; i), the frequency with which wi depends on and follows wj in any RE. At generation time, we find all possible realizations of each attribute in the input attribute set, and for each combinat</context>
</contexts>
<marker>Bangalore, Emami, Haffner, 2005</marker>
<rawString>S. Bangalore, A. Emami, and P. Haffner. 2005. Factoring global inference by enriching local representations. Technical report, AT&amp;T Labs-Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Belz</author>
<author>A Gatt</author>
</authors>
<title>The attribute selection for GRE challenge: Overview and evaluation results.</title>
<date>2007</date>
<booktitle>In Proceedings of UCNLG+MT at MT Summit XI.</booktitle>
<contexts>
<context position="1027" citStr="Belz and Gatt, 2007" startWordPosition="135" endWordPosition="138">ng expression generation. However, most of this work did not take into account: a) stylistic differences between speakers; or b) trainable surface realization approaches that combine semantic and word order information. In this paper we describe and evaluate several end-to-end referring expression generation algorithms that take into consideration speaker style and use data-driven surface realization techniques. 1 Introduction There now exist numerous general-purpose algorithms for attribute selection used in referring expression generation (e.g., (Dale and Reiter, 1995; Krahmer et al., 2003; Belz and Gatt, 2007)). However, these algorithms by-and-large focus on the algorithmic aspects of referring expression generation rather than on psycholinguistic factors that influence language production. For example, we know that humans exhibit individual style differences during language production that can be quite pronounced (e.g. (Belz, 2007)). We also know that the language production process is subject to lexical priming, which means that words and concepts that have been used recently are likely to appear again (Levelt, 1989). In this paper, we first explore the impact of individual style and priming on </context>
</contexts>
<marker>Belz, Gatt, 2007</marker>
<rawString>A. Belz and A. Gatt. 2007. The attribute selection for GRE challenge: Overview and evaluation results. In Proceedings of UCNLG+MT at MT Summit XI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Belz</author>
</authors>
<title>Probabilistic generation of weather forecast texts.</title>
<date>2007</date>
<booktitle>In Proceedings of NAACL/HLT.</booktitle>
<contexts>
<context position="1357" citStr="Belz, 2007" startWordPosition="182" endWordPosition="183">sideration speaker style and use data-driven surface realization techniques. 1 Introduction There now exist numerous general-purpose algorithms for attribute selection used in referring expression generation (e.g., (Dale and Reiter, 1995; Krahmer et al., 2003; Belz and Gatt, 2007)). However, these algorithms by-and-large focus on the algorithmic aspects of referring expression generation rather than on psycholinguistic factors that influence language production. For example, we know that humans exhibit individual style differences during language production that can be quite pronounced (e.g. (Belz, 2007)). We also know that the language production process is subject to lexical priming, which means that words and concepts that have been used recently are likely to appear again (Levelt, 1989). In this paper, we first explore the impact of individual style and priming on attribute selection for referring expression generation. To get an idea of the potential improvement when modeling these factors, we implemented a version of full brevity search (Dale, 1992) that uses speaker-specific constraints, and another version that also uses recency constraints. We found that using speaker-specific constr</context>
</contexts>
<marker>Belz, 2007</marker>
<rawString>A. Belz. 2007. Probabilistic generation of weather forecast texts. In Proceedings of NAACL/HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Dale</author>
<author>E Reiter</author>
</authors>
<title>Computational interpretations of the Gricean maxims in the generation of referring expressions.</title>
<date>1995</date>
<journal>Cognitive Science,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="983" citStr="Dale and Reiter, 1995" startWordPosition="127" endWordPosition="130">lgorithms for attribute selection for referring expression generation. However, most of this work did not take into account: a) stylistic differences between speakers; or b) trainable surface realization approaches that combine semantic and word order information. In this paper we describe and evaluate several end-to-end referring expression generation algorithms that take into consideration speaker style and use data-driven surface realization techniques. 1 Introduction There now exist numerous general-purpose algorithms for attribute selection used in referring expression generation (e.g., (Dale and Reiter, 1995; Krahmer et al., 2003; Belz and Gatt, 2007)). However, these algorithms by-and-large focus on the algorithmic aspects of referring expression generation rather than on psycholinguistic factors that influence language production. For example, we know that humans exhibit individual style differences during language production that can be quite pronounced (e.g. (Belz, 2007)). We also know that the language production process is subject to lexical priming, which means that words and concepts that have been used recently are likely to appear again (Levelt, 1989). In this paper, we first explore th</context>
<context position="4749" citStr="Dale and Reiter, 1995" startWordPosition="703" endWordPosition="706">ost often in the training data. The speaker frequency (FB-sf) criterion selects from among the elements of AS the one used most often by this speaker in the training data, backing off to FB-f if necessary. This criterion models speakerspecific constraints. Finally, the speaker recency (FB-sr) criterion selects from among the elements of AS the one used most recently by this speaker in the training data, backing off to FB-sf if necessary. This criterion models priming and speaker-specific constraints. Dale and Reiter We implemented two variants of the classic Dale &amp; Reiter attribute selection (Dale and Reiter, 1995) algorithm. For Dale &amp; Reiter basic (DR-b), we first build the preferred list of attributes by sorting the most frequently used attributes in the training set. We keep separate lists based upon the “+LOC” and “-LOC” conditions and backoff to a global preferred frequency list in case the attributes are not covered in the current list (merge and sort by frequency). Next, we iterate over the list of preferred attributes and select the next one that rules out at least one entity in the contrast set until no distractors are left. The Dale &amp; Reiter speaker frequency (DR-sf) uses a speaker-specific p</context>
</contexts>
<marker>Dale, Reiter, 1995</marker>
<rawString>R. Dale and E. Reiter. 1995. Computational interpretations of the Gricean maxims in the generation of referring expressions. Cognitive Science, 19(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
</authors>
<title>Generating Referring Expressions: Constructing Descriptions in a Domain of Objects and Processes.</title>
<date>1992</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="1817" citStr="Dale, 1992" startWordPosition="258" endWordPosition="259">ction. For example, we know that humans exhibit individual style differences during language production that can be quite pronounced (e.g. (Belz, 2007)). We also know that the language production process is subject to lexical priming, which means that words and concepts that have been used recently are likely to appear again (Levelt, 1989). In this paper, we first explore the impact of individual style and priming on attribute selection for referring expression generation. To get an idea of the potential improvement when modeling these factors, we implemented a version of full brevity search (Dale, 1992) that uses speaker-specific constraints, and another version that also uses recency constraints. We found that using speaker-specific constraints led to big performance gains for both TUNA domains, while the use of recency constraints was not as effective for TUNA-style tasks. We then modified Dale and Reiter’s classic attribute selection algorithm (Dale and Reiter, 1995) to model speakerspecific constraints, and found performance gains in this more greedy approach as well. Then we looked at surface realization for referring expression generation. There are several approaches to surface realiz</context>
<context position="3732" citStr="Dale, 1992" startWordPosition="536" endWordPosition="537">r, we explore three techniques for the task of referring expression generation that are different hybrids of hand-crafted and data-driven methods. The remainder of this paper is organized as follows: In Section 2, we present the algorithms for attribute selection. The different methods for surface realizers are presented in Section 3. The experiments concerning the attribute selection and surface realization are presented in Section 4 and Section 5. The final remarks are discussed in Section 6. 2 Attribute Selection Algorithms Full Brevity (FB) We implemented a version of full brevity search (Dale, 1992). It does the follow211 ing: first, it constructs AS, the set of attribute sets that uniquely identify the referent given the distractors. Then, it selects an attribute set ASu E AS based on a selection criterion. The minimality (FBm) criterion selects from among the smallest elements of AS at random. The frequency (FB-f) criterion selects from among the elements of AS the one that occurred most often in the training data. The speaker frequency (FB-sf) criterion selects from among the elements of AS the one used most often by this speaker in the training data, backing off to FB-f if necessary.</context>
</contexts>
<marker>Dale, 1992</marker>
<rawString>Robert Dale. 1992. Generating Referring Expressions: Constructing Descriptions in a Domain of Objects and Processes. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Gupta</author>
<author>A Stent</author>
</authors>
<title>Automatic evaluation of referring expression generation using corpora.</title>
<date>2005</date>
<booktitle>In Proceedings of UCNLG.</booktitle>
<contexts>
<context position="11247" citStr="Gupta and Stent, 2005" startWordPosition="1822" endWordPosition="1825"> 0.18 0.13 0.00 Overall Permute&amp;Rank 5.25 5.45 5.78 0.05 0.03 0.01 Dependency 3.93 4.28 6.20 0.07 0.06 0.01 Template 4.73 4.46 6.25 0.05 0.05 0.01 Template-S 3.86 4.04 6.03 0.23 0.17 0.02 Table 2: Results for realization rate speaker constraints, we again see a performance jump, although compared to the best possible case (full brevity) there is still room for improvement. Discussion We have shown that by using speaker and recency constraints in standard algorithms, it is possible to achieve performance gains on the attribute selection task. The most relevant previous research is the work of (Gupta and Stent, 2005), who modified Dale and Reiter’s algorithm to model speaker adaptation in dialog. However, this corpus does not involve dialog so there are no cross-speaker constraints, only withinspeaker constraints (style and priming). 5 Surface Realization Experiments Data Preparation We first normalize the training data to correct misspellings and remove punctuation and capitalization. We then extract a phrasal lexicon. For each attribute value we extract the count of all realizations of that value in the training data. We treat locations as a special case, storing separately the realizations of x-y coord</context>
</contexts>
<marker>Gupta, Stent, 2005</marker>
<rawString>S. Gupta and A. Stent. 2005. Automatic evaluation of referring expression generation using corpora. In Proceedings of UCNLG.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kanthak</author>
<author>D Vilar</author>
<author>E Matusov</author>
<author>R Zens</author>
<author>H Ney</author>
</authors>
<title>Novel reordering approaches in phrase-based statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. ACL Workshop on Building and Using Parallel Texts.</booktitle>
<contexts>
<context position="9347" citStr="Kanthak et al., 2005" startWordPosition="1509" endWordPosition="1512">.93 .85 .79 1 .01 DR-b .70 .45 .25 1 0 DR-sf .78 .55 .35 1 0 Overall FB-m .32 .14 0 1 1 FB-f .70 .48 .34 1 0 FB-sf .95 .87 .81 1 .01 FB-sr .93 .83 .75 1 .01 DR-b .76 .53 .36 1 0 DR-sf .82 .60 .41 1 .02 Table 1: Results for attribute selection Unfortunately, the number of states of the minimal permutation automaton of even a linear automata (finite-state machine representation of a string) grows exponentially with the number of words of the string. So, instead of creating a full permutation automaton, we choose to constrain permutations to be within a local window of adjustable size (also see (Kanthak et al., 2005)). 4 Attribute Selection Experiments Data Preparation The training data were used to build the models outlined above. The development data were then processed one-by-one. For our final submissions, we use training and development data to build our models. Results Table 1 shows the results for variations of full brevity. As we would expect, all approaches achieve a perfect score on uniqueness. For both corpora, we see a large performance jump when we use speaker constraints. However, when we incorporate recency constraints as well performance declines slightly. We think this is due to two facto</context>
</contexts>
<marker>Kanthak, Vilar, Matusov, Zens, Ney, 2005</marker>
<rawString>S. Kanthak, D. Vilar, E. Matusov, R. Zens, and H. Ney. 2005. Novel reordering approaches in phrase-based statistical machine translation. In Proc. ACL Workshop on Building and Using Parallel Texts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Krahmer</author>
<author>S van Erk</author>
<author>A Verleg</author>
</authors>
<title>Graphbased generation of referring expressions.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<marker>Krahmer, van Erk, Verleg, 2003</marker>
<rawString>E. Krahmer, S. van Erk, and A. Verleg. 2003. Graphbased generation of referring expressions. Computational Linguistics, 29(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Langkilde</author>
<author>K Knight</author>
</authors>
<title>Forest-based statistical sentence generation.</title>
<date>2000</date>
<booktitle>In Proc. NAACL.</booktitle>
<contexts>
<context position="2591" citStr="Langkilde and Knight, 2000" startWordPosition="366" endWordPosition="369">d to big performance gains for both TUNA domains, while the use of recency constraints was not as effective for TUNA-style tasks. We then modified Dale and Reiter’s classic attribute selection algorithm (Dale and Reiter, 1995) to model speakerspecific constraints, and found performance gains in this more greedy approach as well. Then we looked at surface realization for referring expression generation. There are several approaches to surface realization described in the literature (Reiter and Dale, 2000) ranging from hand-crafted template-based realizers to data-driven syntax-based realizers (Langkilde and Knight, 2000; Bangalore and Rambow, 2000). Template-based realization involves the insertion of attribute values into predetermined templates. Data-driven syntax-based methods use syntactic relations between words (including long-distance relations) for word ordering. Other data-driven techniques exhaustively generate possible realizations with recourse to syntax in as much as it is reflected in local n-grams. Such techniques have the advantage of being robust although they are inadequate to capture long-range dependencies. In this paper, we explore three techniques for the task of referring expression ge</context>
</contexts>
<marker>Langkilde, Knight, 2000</marker>
<rawString>I. Langkilde and K. Knight. 2000. Forest-based statistical sentence generation. In Proc. NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Levelt</author>
</authors>
<title>Speaking: From intention to articulation,</title>
<date>1989</date>
<pages>222--226</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="1547" citStr="Levelt, 1989" startWordPosition="215" endWordPosition="216">pression generation (e.g., (Dale and Reiter, 1995; Krahmer et al., 2003; Belz and Gatt, 2007)). However, these algorithms by-and-large focus on the algorithmic aspects of referring expression generation rather than on psycholinguistic factors that influence language production. For example, we know that humans exhibit individual style differences during language production that can be quite pronounced (e.g. (Belz, 2007)). We also know that the language production process is subject to lexical priming, which means that words and concepts that have been used recently are likely to appear again (Levelt, 1989). In this paper, we first explore the impact of individual style and priming on attribute selection for referring expression generation. To get an idea of the potential improvement when modeling these factors, we implemented a version of full brevity search (Dale, 1992) that uses speaker-specific constraints, and another version that also uses recency constraints. We found that using speaker-specific constraints led to big performance gains for both TUNA domains, while the use of recency constraints was not as effective for TUNA-style tasks. We then modified Dale and Reiter’s classic attribute</context>
</contexts>
<marker>Levelt, 1989</marker>
<rawString>W. Levelt, 1989. Speaking: From intention to articulation, pages 222–226. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nasr</author>
<author>O Rambow</author>
</authors>
<title>Supertagging and full parsing.</title>
<date>2004</date>
<booktitle>In Proc. 7th International Workshop on Tree Adjoining Grammar and Related Formalisms (TAG+7).</booktitle>
<contexts>
<context position="7057" citStr="Nasr and Rambow, 2004" startWordPosition="1073" endWordPosition="1076"> a template for an input attribute list it is quite likely to be coherent. At generation time, we find all possible realizations of each attribute in the input attribute set, and fill in each possible template with each combination of the attribute realizations. We report results for two versions of this realizer: one with speakerspecific lexicon and templates (Template-S), and one without (Template). Dependency-Based Realizer To construct our dependency-based realizer, we first parse all the word strings from the training data using the dependency parser described in (Bangalore et al., 2005; Nasr and Rambow, 2004). Then, for every pair of words wi, wj that occur in the same referring expression (RE) in the training data, we compute: freq(i &lt; j), the frequency with which wi precedes wj in any RE; freq(i = j − 1), the frequency with which wi immediately precedes wj in any RE; freq(dep(wi, wj) n i &lt; j), the frequency with which wi depends on and precedes wj in any RE, and freq(dep(wi, wj) n j &lt; i), the frequency with which wi depends on and follows wj in any RE. At generation time, we find all possible realizations of each attribute in the input attribute set, and for each combination of attribute realiza</context>
</contexts>
<marker>Nasr, Rambow, 2004</marker>
<rawString>A. Nasr and O. Rambow. 2004. Supertagging and full parsing. In Proc. 7th International Workshop on Tree Adjoining Grammar and Related Formalisms (TAG+7).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Rayner</author>
</authors>
<title>Eye movements in reading and information processing: 20 years of research.</title>
<date>1998</date>
<journal>Psychological Bulletin,</journal>
<volume>124</volume>
<issue>3</issue>
<marker>Rayner, 1998</marker>
<rawString>K. Rayner. 1998. Eye movements in reading and information processing: 20 years of research. Psychological Bulletin, 124(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Reiter</author>
<author>R Dale</author>
</authors>
<title>Building Natural Language Generation Systems.</title>
<date>2000</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="2474" citStr="Reiter and Dale, 2000" startWordPosition="352" endWordPosition="356">ints, and another version that also uses recency constraints. We found that using speaker-specific constraints led to big performance gains for both TUNA domains, while the use of recency constraints was not as effective for TUNA-style tasks. We then modified Dale and Reiter’s classic attribute selection algorithm (Dale and Reiter, 1995) to model speakerspecific constraints, and found performance gains in this more greedy approach as well. Then we looked at surface realization for referring expression generation. There are several approaches to surface realization described in the literature (Reiter and Dale, 2000) ranging from hand-crafted template-based realizers to data-driven syntax-based realizers (Langkilde and Knight, 2000; Bangalore and Rambow, 2000). Template-based realization involves the insertion of attribute values into predetermined templates. Data-driven syntax-based methods use syntactic relations between words (including long-distance relations) for word ordering. Other data-driven techniques exhaustively generate possible realizations with recourse to syntax in as much as it is reflected in local n-grams. Such techniques have the advantage of being robust although they are inadequate t</context>
</contexts>
<marker>Reiter, Dale, 2000</marker>
<rawString>E. Reiter and R. Dale. 2000. Building Natural Language Generation Systems. Cambridge University Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>