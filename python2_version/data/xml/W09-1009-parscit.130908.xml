<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000062">
<title confidence="0.909405">
Upper Bounds for Unsupervised Parsing with Unambiguous
Non-Terminally Separated Grammars
</title>
<author confidence="0.5885">
Franco M. Luque and Gabriel Infante-Lopez
</author>
<affiliation confidence="0.477785">
Grupo de Procesamiento de Lenguaje Natural
Universidad Nacional de Córdoba &amp; CONICET
Argentina
</affiliation>
<email confidence="0.995516">
{francolq|gabriel}@famaf.unc.edu.ar
</email>
<sectionHeader confidence="0.997339" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998501590909091">
Unambiguous Non-Terminally Separated
(UNTS) grammars have properties that
make them attractive for grammatical in-
ference. However, these properties do not
state the maximal performance they can
achieve when they are evaluated against a
gold treebank that is not produced by an
UNTS grammar. In this paper we inves-
tigate such an upper bound. We develop
a method to find an upper bound for the
unlabeled F1 performance that any UNTS
grammar can achieve over a given tree-
bank. Our strategy is to characterize all
possible versions of the gold treebank that
UNTS grammars can produce and to find
the one that optimizes a metric we define.
We show a way to translate this score into
an upper bound for the F1. In particular,
we show that the F1 parsing score of any
UNTS grammar can not be beyond 82.2%
when the gold treebank is the WSJ10 cor-
pus.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999878428571429">
Unsupervised learning of natural language has re-
ceived a lot of attention in the last years, e.g., Klein
and Manning (2004), Bod (2006a) and Seginer
(2007). Most of them use sentences from a tree-
bank for training and trees from the same treebank
for evaluation. As such, the best model for un-
supervised parsing is the one that reports the best
performance.
Unambiguous Non-Terminally Separated
(UNTS) grammars have properties that make
them attractive for grammatical inference. These
grammars have been shown to be PAC-learnable
in polynomial time (Clark, 2006), meaning that
under certain circumstances, the underlying
grammar can be learned from a sample of the
underlying language. Moreover, UNTS grammars
have been successfully used to induce grammars
from unannotated corpora in competitions of
learnability of formal languages (Clark, 2007).
UNTS grammars can be used for modeling nat-
ural language. They can be induced using any
training material, the induced models can be eval-
uated using trees from a treebank, and their per-
formance can be compared against state-of-the-
art unsupervised models. Different learning al-
gorithms might produce different grammars and,
consequently, different scores. The fact that the
class of UNTS grammars is PAC learnable does
not convey any information on the possible scores
that different UNTS grammars might produce.
From a performance oriented perspective it might
be possible to have an upper bound over the set
of possible scores of UNTS grammars. Knowing
an upper bound is complementary to knowing that
the class of UNTS grammars is PAC learnable.
Such upper bound has to be defined specifically
for UNTS grammars and has to take into account
the treebank used as test set. The key question
is how to compute it. Suppose that we want to
evaluate the performance of a given UNTS gram-
mar using a treebank. The candidate grammar pro-
duces a tree for each sentence and those trees are
compared to the original treebank. We can think
that the candidate grammar has produced a new
version of the treebank, and that the score of the
grammar is a measure of the closeness of the new
treebank to the original treebank. Finding the best
upper bound is equivalent to finding the closest
UNTS version of the treebank to the original one.
Such bounds are difficult to find for most classes
of languages because the search space is the
set of all possible versions of the treebank that
might have been produced by any grammar in the
class under study. In order to make the problem
tractable, we need the formalism to have an easy
way to characterize all the versions of a treebank
</bodyText>
<note confidence="0.9373535">
Proceedings of the EACL 2009 Workshop on Computational Linguistic Aspects of Grammatical Inference, pages 58–65,
Athens, Greece, 30 March 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.998445">
58
</page>
<bodyText confidence="0.999847333333333">
it might produce. UNTS grammars have a special
characterization that makes the search space easy
to define but whose exploration is NP-hard.
In this paper we present a way to characterize
UNTS grammars and a metric function to mea-
sure the closeness between two different version
of a treebank. We show that the problem of find-
ing the closest UNTS version of the treebank can
be described as Maximum Weight Independent Set
(MWIS) problem, a well known NP-hard problem
(Karp, 1972). The exploration algorithm returns
a version of the treebank that is the closest to the
gold standard in terms of our own metric.
We show that the F1-measure is related to our
measure and that it is possible to find and upper
bound of the F1-performance for all UNTS gram-
mars. Moreover, we compute this upper bound for
the WSJ10, a subset of the Penn Treebank (Mar-
cus et al., 1994) using POS tags as the alphabet.
The upper bound we found is 82.2% for the F1
measure. Our result suggest that UNTS grammars
are a formalism that has the potential to achieve
state-of-the-art unsupervised parsing performance
but does not guarantee that there exists a grammar
that can actually achieve the 82.2%.
To the best of our knowledge, there is no pre-
vious research on finding upper bounds for perfor-
mance over a concrete class of grammars. In Klein
and Manning (2004), the authors compute an up-
per bound for parsing with binary trees a gold tree-
bank that is not binary. This upper bound, that is
88.1% for the WSJ10, is for any parser that returns
binary trees, including the concrete models devel-
oped in the same work. But their upper bound does
not use any specific information of the concrete
models that may help them to find better ones.
The rest of the paper is organized as follows.
Section 2 presents our characterization of UNTS
grammars. Section 3 introduces the metric we op-
timized and explains how the closest version of the
treebank is found. Section 4 explains how the up-
per bound for our metric is translated to an up-
per bound of the F1 score. Section 5 presents our
bound for UNTS grammars using the WSJ10 and
finally Section 6 concludes the paper.
</bodyText>
<sectionHeader confidence="0.983476" genericHeader="method">
2 UNTS Grammars and Languages
</sectionHeader>
<bodyText confidence="0.999068763636364">
Formally, a context free grammar G =
(E, N, 5, P) is said to be Non-Terminally Sepa-
rated (NTS) if, for all X, Y E N and α, Q, -y E
(E U N)∗ such that X =*=&gt;. αQ-y and Y =*=&gt;. Q, we
have that X =*=&gt;. αY-y (Clark, 2007). Unambiguous
NTS (UNTS) grammars are those NTS grammars
that parses unambiguously every instance of the
language.
Given any grammar G, a substring s of r E
L(G) is called a constituent of r if and only if there
is an X in N such that 5 =*&gt;. uXv =*&gt;. usv = r.
In contrast, a string s is called a non-constituent or
distituent of r E L(G) if s is not a constituent of r.
We say that s is a constituent of a language L(G)
if for every r that contains s, s is a constituent of
r. In contrast, s is a distituent of L(G) if for every
r where s occurs, s is a distituent of r.
An interesting characterization of finite UNTS
grammars is that every substring that appear in
some string of the language is always a constituent
or always a distituent. In other words, if there is a
string r in L(G) for which s is a constituent, then
s is a constituent of L(G). By means of this prop-
erty, if we ignore the non-terminal labels, a finite
UNTS language is fully determined by its set of
constituents C. We can show this property for fi-
nite UNTS languages. We believe that it can also
be shown for non-finite cases, but for our purposes
the finite cases suffices, because we use grammars
to parse finite sets of sentences, specifically, the
sentences of test treebanks. We know that for ev-
ery finite subset of an infinite language produced
by a UNTS grammar G, there is a UNTS gram-
mar G′ whose language is finite and that parses
the finite subset as G. If we look for the upper
bound among the grammars that produce a finite
language, this upper bound is also an upper bound
for the class of infinite UNTS grammars.
The UNTS characterization plays a very im-
portant role in the way we look for the upper
bound. Our method focuses on how to determine
which of the constituents that appear in the gold
are actually the constituents that produce the up-
per bound. Suppose that a given gold treebank
contains two strings α and Q such that they occur
overlapped. That is, there exist non-empty strings
α′, -y, Q′ such that α = α′-y and Q = -yQ′ and
α′-yQ′ occurs in the treebank. If C is the set of
constituents of a UNTS grammar it can not have
both α and Q. It might have one or the other, but
if both belong to C the resulting language can not
be UNTS. In order to find the closest UNTS gram-
mar we design a procedure that looks for the sub-
set of all substrings that occur in the sentences of
the gold treebank that can be the constituent set C
</bodyText>
<page confidence="0.997979">
59
</page>
<bodyText confidence="0.999795051282051">
of a grammar. We do not explicitly build a UNTS
grammar, but find the set C that produces the best
score.
We say that two strings α and Q are compatible
in a language L if they do not occur overlapped
in L, and hence they both can be members of C.
If we think of L as a subset of an infinite lan-
guage, it is not possible to check that two overlap-
ping strings do not appear overlapped in the “real”
language and hence that they are actually com-
patible. Nevertheless, we can guarantee compat-
ibility between two strings α, Q by requiring that
they do not overlap at all, this is, that there are
no non-empty strings α′, -y, Q′ such that α = α′-y
and Q = -yQ′. We call this type of compatibility
strong compatibility. Strong compatibility ensures
that two strings can belong to C regardless of L.
In our experiments we focus on finding the best set
C of compatible strings.
Any set of compatible strings C extracted from
the gold treebank can be used to produce a new
version of the treebank. For example, Figure 1
shows two trees from the WSJ Penn Treebank.
The string “in the dark” occurs as a constituent in
(a) and as a distituent in (b). If C contains “in the
dark”, it can not contain “the dark clouds” given
that they overlap in the yield of (b). As a con-
sequence, the new treebank correctly contains the
subtree in (a) but not the one in (b). Instead, the
yield of (b) is described as in (c) in the new tree-
bank.
C defines a new version of the treebank that sat-
isfies the UNTS property. Our goal is to obtain a
treebank T′ such that (a) T′ and T are treebanks
over the same set of sentences, (b) T′ is UNTS,
and (c) T′ is the closest treebank to T in terms of
performance. The three of them imply that any
other UNTS grammar is not as similar as the one
we found.
</bodyText>
<sectionHeader confidence="0.883072" genericHeader="method">
3 Finding the Best UNTS Grammar
</sectionHeader>
<bodyText confidence="0.999990114754098">
As our goal is to find the closest grammar in terms
of performance, we need to define first a weight
for each possible grammar and second, an algo-
rithm that searches for the grammar with the best
weight. Ideally, the weight of a candidate gram-
mar should be in terms of F1, but we can show
that optimization of this particular metric is com-
putationally hard. Instead of defining F1 as their
score, we introduce a new metric that is easier to
optimize, we find the best grammar for this met-
ric, and we show that the possible values of F1
can be bounded by a function that takes this score
as argument. In this section we present our metric
and the technique we use to find a grammar that
reports the best value for our metric.
If the original treebank T is not produced by any
UNTS grammar, then there are strings in T that
are constituents in some sentences and that are dis-
tituents in some other sentences. For each one of
them we need a procedure to decide whether they
are members of C or not. If a string α appears a
significant number of times more as a constituent
than as a distituent the procedure may choose to
include it in C at the price of being wrong a few
times. That is, the new version of T has all occur-
rences of α either as constituents or as distituents.
The treebank that has all of its occurrences as con-
stituents differs from the original in that there are
some occurrences of α that were originally dis-
tituents and are marked as constituents. Similarly,
if α is marked as distituent in the new treebank, it
has occurrences of α that were constituents in T.
The decision procedure becomes harder when
all the substrings that appear in the treebank are
considered. The increase in complexity is a con-
sequence of the number of decisions the procedure
needs to take and the way these decisions interfere
one with another. We show that the problem of
determining the set C is naturally embedded in a
graph NP-hard problem. We define a way to look
for the optimal grammars by translating our prob-
lem to a well known graph problem. Let L be the
the set of sentences in a treebank, and let 5(L) be
all the possible non-empty proper substrings of L.
We build a weighted undirected graph G in terms
of the treebank as follows. Nodes in G correspond
to strings in 5(L). The weight of a node is a func-
tion w(s) that models our interest of having s se-
lected as a constituent; w(s) is defined in terms of
some information derived from the gold treebank
T and we discuss it later in this section. Finally,
two nodes a and b are connected by an edge if their
two corresponding strings conflict in a sentence of
T (i.e., they are not compatible in L).
Not all elements of L are in 5(L). We did not
include L in 5(L) for two practical reasons. The
first one is that to require L in 5(L) is too re-
strictive. It states that all strings in L are in fact
constituents. If two string ab and bc of L oc-
cur overlapped in a third string abc then there is
no UNTS grammar capable of having the three of
</bodyText>
<page confidence="0.993279">
60
</page>
<figureCaption confidence="0.992748">
Figure 1: (a) and (b) are two subtrees that show “in the dark” as a constituent and as a distituent respec-
tively. (c) shows the result of choosing “in the dark” as a constituent.
</figureCaption>
<figure confidence="0.998408178571429">
we
’re
DT
IN
in
JJ
dark
the
(a)
PRP
VBP
IN
in DT
the
dark
JJ
clouds
NNS
(b)
(c)
DT
clouds
IN
in
the
JJ
dark
NNS
</figure>
<bodyText confidence="0.98030245">
them as constituents. The second one is that in-
cluding them produces graphs that are too sparse.
If they are included in the graph, we know that
any solution should contain them, consequently,
all their neighbors do not belong to any solution
and they can be removed from the graph. Our ex-
periments show that the graph that results from re-
moving nodes related to nodes representing strings
in L are too small to produce any interesting result.
By means of representing the treebank as a
graph, selecting a set of constituents C ⊆ 5(L)
is equivalent to selecting an independent set of
nodes in the graph. An independent set is a sub-
set of the set of nodes that do not have any pair
of nodes connected by an edge. Clearly, there are
exponentially many possible ways to select an in-
dependent set, and each of these sets represents a
set of constituents. But, since we are interested in
the best set of constituents, we associate to each
independent set C the weight W(C) defined as
</bodyText>
<equation confidence="0.790515">
E
</equation>
<bodyText confidence="0.994525846153846">
sEC w(s). Our aim is then to find a set Cmax
that maximizes this weight. This problem is a well
known problem of graph theory known in the lit-
erature as the Maximum Weight Independent Set
(MWIS) problem. This problem is also known to
be NP-hard (Karp, 1972).
We still have to choose a definition for w(s).
We want to find the grammar that maximizes F1.
Unfortunately, F 1 can not be expressed in terms of
a sum of weights. Maximization of F1 is beyond
the expressiveness of our model, but our strategy
is to define a measure that correlates with F1 and
that can be expressed as a sum of weights.
In order to introduce our measure, we first de-
fine c(s) and d(s) as the number of times a string
s appears in the gold treebank T as a constituent
and as a distituent respectively. Observe that if
we choose to include s as a constituent of C, the
resulting treebank T� contains all the c(s) + d(s)
occurrences of s as a constituent. c(s) of the s oc-
currences in T� are constituents as they are in T
and d(s) of the occurrences are constituents in T�
but are in fact distituents in T. We want to max-
imize c(s) and minimize d(s) at the same time.
This can be done by defining the contribution of a
string s to the overall score as
</bodyText>
<equation confidence="0.85249275">
w(s) = c(s) − d(s).
With this definition of w, the weight W(C) =
E
sEC w(s) becomes the number of constituents
</equation>
<bodyText confidence="0.9363355">
of T� that are in T minus the number of con-
stituents that do not. If we define the number of
hits to be H(C) = EsEC c(s) and the number of
misses to be M(C) = E
</bodyText>
<equation confidence="0.998344">
W(C) = H(C) − M(C). (1)
</equation>
<bodyText confidence="0.9999235">
As we confirm in Section 5, graphs tend to be
very big. In order to reduce the size of the graphs,
if a string s has w(s) G 0, we do not include its
corresponding node in the graph. An independent
set that does not include s has an equal or higher
W than the same set including s.
For example, let T be the treebank in Fig-
ure 2 (a). The sets of substrings such that
w(c) ≥ 0 is {da, cd, bc, cda, ab, bch}. The
graph that corresponds to this set of strings is
given in Figure 3. Nodes corresponding to
strings {dabch, bcda, abe, abf, abg, bci, daj} are
not shown in the figure because the strings do
not belong to 5(L). The figure also shows the
weights associated to the substrings according to
their counts in Figure 2 (a). The shadowed nodes
correspond to the independent set that maximizes
W. The trees in the Figure 2 (b) are the sentences
of the treebank parsed according the optimal inde-
pendent set.
</bodyText>
<sectionHeader confidence="0.970202" genericHeader="method">
4 An Upper Bound for F1
</sectionHeader>
<bodyText confidence="0.951361625">
Even though finding the independent set that max-
imizes W is an NP-Hard problem, there are in-
stances where it can be effectively computed, as
we show in the next section. The set Cmax max-
imizes W for the WSJ10 and we know that all
others C produces a lower value of W. In other
words, the set Cmax produce a treebank Tmax that
sEC d(s) we have that
</bodyText>
<page confidence="0.880572">
61
</page>
<figure confidence="0.99949205">
h
b c
(da)((bc)h)
b
c d a
b((cd)a)
a b e
(ab)e
a b f
(ab)f
a b g
(ab)g
i
b c
(bc)i
d a j
(da)j
d a
(a)
(b)
</figure>
<figureCaption confidence="0.779077">
Figure 2: (a) A gold treebank. (b) The treebank generated by the grammar C = L U {cd, ab, cda}.
</figureCaption>
<figure confidence="0.99994925">
a b c h
d(ab)ch
b
c d a
b((cd)a)
a b e
(ab)e
a b f
(ab)f
a b g
(ab)g
b c i
bci
d a j
daj
d
</figure>
<figureCaption confidence="0.999873">
Figure 3: Graph for the treebank of Figure 2.
</figureCaption>
<bodyText confidence="0.993902942857143">
is the closest UNTS version to the WSJ10 in terms
of W. We can compute the precision, recall and
F1 for Cmax but there is no warranty that the F1
score is the best for all the UNTS grammars. This
is the case because F1 and W do not define the
same ordering over the family of candidate con-
stituent sets C: there are gold treebanks T (used
for computing the metrics), and sets C1, C2 such
that F1(C1) &lt; F1(C2) and W(C1) &gt; W(C2).
For example, consider the gold treebank T in Fig-
ure 4 (a). The table in Figure 4 (b) displays two
sets C1 and C2, the treebanks they produce, and
their values of F1 and W. Note that C2 is the re-
sult of adding the string ef to C1, also note that
c(ef) = 1 and d(ef) = 2. This improves the F1
score but produces a lower W.
The F1 measure we work with is the one de-
fined in the recent literature of unsupervised pars-
ing (Klein and Manning, 2004). F1 is defined in
terms of Precision and Recall as usual, and the last
two measures are micro-averaged measures that
include full-span brackets, and that ignore both
unary branches and brackets of span one. For sim-
plicity, the previous example does not count the
full-span brackets.
As the example shows, the upper bound for W
might not be an upper bound of F1, but it is pos-
sible to find a way to define an upper bound of
F1 using the upper bound of W. In this section
we define a function f with the following prop-
erty. Let X and Y be the sets of W-weights and
F1-weights for all possible UNTS grammars re-
spectively. Then, if w is an upper bound of X,
then f(w) is an upper bound of Y . The function f
is defined as follows: / 1
</bodyText>
<equation confidence="0.99994">
f (w) = F1 C 2−1 w 1/ (2)
K /
</equation>
<bodyText confidence="0.998360730769231">
where F1(p, r) = 2pr
p+r, and K = �s∈ST c(s) is
the total number of constituents in the gold tree-
bank T. From it, we can also derive values for
precision and recall: precision 1
2− w K and recall 1.
A recall of 1 is clearly an upper bound for all the
possible values of recall, but the value given for
precision is not necessarily an upper bound for all
the possible values of precision. It might exist a
grammar having a higher value of precision but
whose F 1 has to be below our upper bound.
The rest of section shows that f(W) is an up-
per bound for F1, the reader not interested in the
technicalities can skip it.
The key insight for the proof is that both metrics
F1 and W can be written in terms of precision and
recall. Let T be the treebank that is used to com-
pute all the metrics. And let T′ be the treebank
produced by a given constituent set C. If a string
s belongs to C, then its c(s) + d(s) occurrences
in T′ are marked as constituents. Moreover, s is
correctly tagged a c(s) number of times while it
is incorrectly tagged a d(s) number of times. Us-
ing this, P, R and F1 can be computed for C as
follows:
</bodyText>
<equation confidence="0.99116">
P(C) = [� P ∈C c(s)
l / Li-EC c(s)+d(s)
H(C)
= (3)
H(C)+M(C)
R(C) = Ps ∈C c(s) K
H(KC)
(4)
F1(C) = 2P(C)R(C)
P(C)+R(C)
2H(C)
K+H(C)+M(C)
</equation>
<page confidence="0.995401">
62
</page>
<figure confidence="0.9877305">
e f g
(ef)g
e f i
efi
e f h
efh
a b c
(ab)c
a b d
a(bd)
(a)
(b)
C T′ P R F1 W
C
C1 = {abc, abd, efg, efh, efi, ab} {(ab)c, (ab)d, efg, efh, efi} 50% 33% 40% 1 − 1 = 0
C2 = {abc, abd, efg, efh, efi, ab, ef} {(ab)c, (ab)d, (ef)g, (ef)h, (ef)i} 40% 67% 50% 2 − 3 = −1
</figure>
<figureCaption confidence="0.951998">
Figure 4: (a) A gold treebank. (b) Two grammars, the treebanks they generate, and their scores.
</figureCaption>
<equation confidence="0.825180666666667">
W can also be written in terms of P and R as
1
W(C) = (2 − P (C))R(C)K (5)
</equation>
<bodyText confidence="0.9981105">
This formula is proved to be equivalent to Equa-
tion (1) by replacing P(C) and R(C) with equa-
tions (3) and (4) respectively. Using the last two
equations, we can rewrite F1 and W taking p and
r, representing values of precision and recall, as
parameters:
</bodyText>
<equation confidence="0.9476738">
2pr
F1(p, r) = p + r
1
W (p, r) = (2 − )rK (6)
p
</equation>
<bodyText confidence="0.99980925">
Using these equations, we can prove that f
correctly translates upper bounds of W to upper
bounds of F1 using calculus. In contrast to F1,
W not necessarily take values between 0 and 1. In-
stead, it takes values between K and −oo. More-
over, it is negative when p &lt; 12, and goes to −oo
when p goes to 0. Let C be an arbitrary UNTS
grammar, and let pC, rC and wC be its precision,
recall and W-weight respectively. Let w be our
upper bound, so that wC &lt; w. If f1C is defined
as F1(pC, rC) we need to show that f1C &lt; f(w).
We bound f1C in two steps. First, we show that
</bodyText>
<equation confidence="0.763796">
f1C &lt; f(wC)
</equation>
<bodyText confidence="0.984801">
and second, we show that
</bodyText>
<equation confidence="0.783848166666667">
f(wC) &lt; f(w).
The first inequality is proved by observing that
f1C and f(wC) are the values of the function
f 1(r) = F1 (2 −1wC
, r
Kr /
</equation>
<bodyText confidence="0.999914545454546">
at the points r = rC and r = 1 respectively.
This function corresponds to the line defined by
the F1 values of all possible models that have a
fixed weight W = wC. The function is monoton-
ically increasing in r, so we can apply it to both
sides of the following inequality rC &lt; 1, which is
trivially true. As result, we get f1C &lt; f(wC) as
required. The second inequality is proved by ob-
serving that f(w) is monotonically increasing in
w, and by applying it to both sides of the hypothe-
sis wc &lt; w.
</bodyText>
<sectionHeader confidence="0.998349" genericHeader="method">
5 UNTS Bounds for the WSJ10 Treebank
</sectionHeader>
<bodyText confidence="0.999939777777778">
In this section we focus on trying to find real upper
bounds building the graph for a particular treebank
T. We find the best independent set, we build the
UNTS version Tmax of T and we compute the up-
per bound for F 1. The treebank we use for exper-
iments is the WSJ10, which consists of the sen-
tences of the WSJ Penn Treebank whose length
is at most 10 words after removing punctuation
marks (Klein and Manning, 2004). We also re-
moved lexical entries transforming POS tags into
our terminal symbols as it is usually done (Klein
and Manning, 2004; Bod, 2006a).
We start by finding the best independent set. To
solve the problem in the practice, we convert it
into an Integer Linear Programming (ILP) prob-
lem. ILP is also NP-hard (Karp, 1972), but there
is software that implements efficient strategies for
solving some of its instances (Achterberg, 2004).
ILP problems are defined by three parameters.
First, there is a set of variables that can take val-
ues from a finite set. Second, there is an objective
function that has to be maximized, and third, there
is a set of constraints that must be satisfied. In our
case, we define a binary variable xs E 10, 11 for
every node s in the graph. Its value is 1 or 0, that
respectively determines the presence or absence of
s in the set Cmax. The objective function is
</bodyText>
<equation confidence="0.5570885">
� xsw(s)
s∈S(L)
</equation>
<bodyText confidence="0.970426">
The constraints are defined using the edges of the
</bodyText>
<page confidence="0.998158">
63
</page>
<bodyText confidence="0.996187">
graph. For every edge (s1, s2) in the graph, we
add the following constraint to the problem:
</bodyText>
<equation confidence="0.835836">
xs1 + xs2 &lt; 1
</equation>
<bodyText confidence="0.989086694444445">
The 7422 trees of the WSJ10 treebank have a
total of 181476 substrings of length &gt; 2, that
form the set 5(L) of 68803 different substrings.
The number of substrings in 5(L) does not grow
too much with respect to the number of strings in
L because substrings are sequences of POS tags,
meaning that each substring is very frequent in the
corpus. If substrings were made out of words in-
stead of POS tags, the number of substrings would
grow much faster, making the problem harder to
solve. Moreover, removing the strings s such that
w(s) &lt; 0 gives a total of only 7029 substrings.
Since there is a node for each substring, the result-
ing graph contains 7029 nodes. Recall that there
is an edge between two strings if they occur over-
lapped. Our graph contains 1204 edges. The ILP
version has 7029 variables, 1204 constraints and
the objective function sums over 7029 variables.
These numbers are summarized in Table 1.
The solution of the ILP problem is a set of
6583 variables that are set to one. This set corre-
sponds to a set Cmax of nodes in our graph of the
same number of elements. Using Cmax we build
a new version Tmax of the WSJ10, and compute
its weight W, precision, recall and F1. Their val-
ues are displayed in Table 2. Since the elements
of L were not introduced in 5(L), elements of L
are not necessarily in Cmax, but in order to com-
pute precision and recall, we add them by hand.
Strictly speaking, the set of constituents that we
use for building Tmax is Cmax plus the full span
brackets.
We can, using equation (2), compute the up-
per bound of F1 for all the possible scores of all
UNTS grammars that use POS tags as alphabet:
f (wmax) = F1 (2 − wmax , 1) = 82.2%
</bodyText>
<sectionHeader confidence="0.545774" genericHeader="method">
K
</sectionHeader>
<bodyText confidence="0.939594">
The precision for this upper bound is
</bodyText>
<equation confidence="0.89357125">
1 = 69.8%
P(wmax) =
2 − wmax
K
</equation>
<bodyText confidence="0.99899275">
while its recall is R = 100%. Note from the pre-
vious section that P(wmax) is not an upper bound
for precision but just the precision associated to
the upper bound f(wmax).
</bodyText>
<table confidence="0.99582175">
Gold constituents K 35302
Strings J5(L)J 68803
Nodes 7029
Edges 1204
</table>
<tableCaption confidence="0.646945">
Table 1: Figures for the WSJ10 and its graph.
</tableCaption>
<table confidence="0.999884666666667">
Hits H 22169
Misses M 2127
Weight W 20042
Precision P 91.2%
Recall R 62.8%
F1 F1 74.4%
</table>
<tableCaption confidence="0.999511">
Table 2: Summary of the scores for Cmax.
</tableCaption>
<bodyText confidence="0.998994428571429">
Table 3 shows results that allow us to com-
pare the upper bounds with state-of-the-art pars-
ing scores. BestW corresponds to the scores of
Tmax and UBoundF1 is the result of our transla-
tion function f. From the table we can see that
an unsupervised parser based on UNTS grammars
may reach a sate-of-the-art performance over the
WSJ10. RBranch is a WSJ10 version where all
trees are binary and right branching. DMV, CCM
and DMV+CCM are the results reported in Klein
and Manning (2004). U-DOP and UML-DOP
are the results reported in Bod (2006b) and Bod
(2006a) respectively. Incremental refers to the re-
sults reported in Seginer (2007).
We believe that our upper bound is a generous
one and that it might be difficult to achieve it for
two reasons. First, since the WSJ10 corpus is
a rather flat treebank, from the 68803 substrings
only 10% of them are such that c(s) &gt; d(s). Our
procedure has to decide among this 10% which
of the strings are constituents. An unsupervised
method has to choose the set of constituents from
the set of all 68803 possible substrings. Second,
we are supposing a recall of 100% which is clearly
too optimistic. We believe that we can find a
tighter upper bound by finding an upper bound for
recall, and by rewriting f in equation (2) in terms
of the upper bound for recall.
It must be clear the scope of the upper bound
we found. First, note that it has been computed
over the WSJ10 treebank using the POS tags as
the alphabet. Any other alphabet we use, like for
example words, or pairs of words and POS tags,
changes the relation of compatibility among the
substrings, making a completely different universe
</bodyText>
<page confidence="0.999037">
64
</page>
<table confidence="0.9996871">
Model UP UR F1
RBranch 55.1 70.0 61.7
DMV 46.6 59.2 52.1
CCM 64.2 81.6 71.9
DMV+CCM 69.3 88.0 77.6
U-DOP 70.8 88.2 78.5
UML-DOP 82.9
Incremental 75.6 76.2 75.9
BestW(UNTS) 91.2 62.8 74.4
UBoundF1(UNTS) 69.8 100.0 82.2
</table>
<tableCaption confidence="0.998579">
Table 3: Performance on the WSJ10 of the most
</tableCaption>
<bodyText confidence="0.982456058823529">
recent unsupervised parsers, and our upper bounds
on UNTS.
of UNTS grammars. Second, our computation of
the upper bound was not made for supersets of the
WSJ10. Supersets such as the entire Penn Tree-
bank produce bigger graphs because they contain
longer sentences and various different sequences
of substrings. As the maximization of W is an
NP-hard problem, the computational cost of solv-
ing bigger instances grows exponentially. A third
limitation that must be clear is about the models
affected by the bound. The upper bound, and in
general the method, is only applicable to the class
of formal UNTS grammars, with only some very
slight variants mentioned in the previous sections.
Just moving to probabilistic or weighted UNTS
grammars invalidates all the presented results.
</bodyText>
<sectionHeader confidence="0.999823" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999958933333333">
We present a method for assessing the potential of
UNTS grammars as a formalism for unsupervised
parsing of natural language. We assess their po-
tential by finding an upper bound of their perfor-
mance when they are evaluated using the WSJ10
treebank. We show that any UNTS grammars can
achieve at most 82.2% of F1 measure, a value
comparable to most state-of-the-art models. In or-
der to compute this upper bound we introduced
a measure that does not define the same ordering
among UNTS grammars as the F1, but that has
the advantage of being computationally easier to
optimize. Our measure can be used, by means of
a translation function, to find an upper bound for
F1. We also showed that the optimization proce-
dure for our metric maps into an NP-Hard prob-
lem, but despite this fact we present experimen-
tal results that compute the upper bound for the
WSJ10 when POS tags are treated as the grammar
alphabet.
From a more abstract perspective, we intro-
duced a different approach to assess the usefulness
of a grammatical formalism. Usually, formalism
are proved to have interesting learnability proper-
ties such as PAC-learnability or convergence of a
probabilistic distribution. We present an approach
that even though it does not provide an effective
way of computing the best grammar in an unsu-
pervised fashion, it states the upper bound of per-
formance for all the class of UNTS grammars.
</bodyText>
<sectionHeader confidence="0.998952" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9834476">
This work was supported in part by grant PICT
2006-00969, ANPCyT, Argentina. We would like
to thank Pablo Rey (UDP, Chile) for his help
with ILP, and Demetrio Martín Vilela (UNC, Ar-
gentina) for his detailed review.
</bodyText>
<sectionHeader confidence="0.999054" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999873888888889">
Tobias Achterberg. 2004. SCIP - a framework to in-
tegrate Constraint and Mixed Integer Programming.
Technical report.
Rens Bod. 2006a. An all-subtrees approach to unsu-
pervised parsing. In Proceedings of COLING-ACL
2006.
Rens Bod. 2006b. Unsupervised parsing with U-DOP.
In Proceedings of CoNLL-X.
Alexander Clark. 2006. PAC-learning unambiguous
NTS languages. In Proceedings ofICGI-2006.
Alexander Clark. 2007. Learning deterministic con-
text free grammars: The Omphalos competition.
Machine Learning, 66(1):93–110.
Richard M. Karp. 1972. Reducibility among com-
binatorial problems. In R. E. Miller and J. W.
Thatcher, editors, Complexity of Computer Compu-
tations, pages 85–103. Plenum Press.
Dan Klein and Christopher D. Manning. 2004.
Corpus-based induction of syntactic structure: Mod-
els of dependency and constituency. In Proceedings
ofACL 42.
Mitchell P. Marcus, Beatrice Santorini, and Mary A.
Marcinkiewicz. 1994. Building a large annotated
corpus of english: The Penn treebank. Computa-
tional Linguistics, 19(2):313–330.
Yoav Seginer. 2007. Fast unsupervised incremental
parsing. In Proceedings ofACL 45.
</reference>
<page confidence="0.999616">
65
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.661583">
<title confidence="0.999454">Upper Bounds for Unsupervised Parsing with Non-Terminally Separated Grammars</title>
<author confidence="0.999025">M Luque</author>
<affiliation confidence="0.9683795">Grupo de Procesamiento de Lenguaje Universidad Nacional de Córdoba &amp;</affiliation>
<email confidence="0.929716">{francolq|gabriel}@famaf.unc.edu.ar</email>
<abstract confidence="0.988630608695652">Unambiguous Non-Terminally Separated (UNTS) grammars have properties that make them attractive for grammatical inference. However, these properties do not state the maximal performance they can achieve when they are evaluated against a gold treebank that is not produced by an UNTS grammar. In this paper we investigate such an upper bound. We develop a method to find an upper bound for the that any UNTS grammar can achieve over a given treebank. Our strategy is to characterize all possible versions of the gold treebank that UNTS grammars can produce and to find the one that optimizes a metric we define. We show a way to translate this score into upper bound for the In particular, show that the score of any grammar can not be beyond when the gold treebank is the WSJ10 corpus.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Tobias Achterberg</author>
</authors>
<title>SCIP - a framework to integrate Constraint and Mixed Integer Programming.</title>
<date>2004</date>
<tech>Technical report.</tech>
<contexts>
<context position="23718" citStr="Achterberg, 2004" startWordPosition="4524" endWordPosition="4525">se for experiments is the WSJ10, which consists of the sentences of the WSJ Penn Treebank whose length is at most 10 words after removing punctuation marks (Klein and Manning, 2004). We also removed lexical entries transforming POS tags into our terminal symbols as it is usually done (Klein and Manning, 2004; Bod, 2006a). We start by finding the best independent set. To solve the problem in the practice, we convert it into an Integer Linear Programming (ILP) problem. ILP is also NP-hard (Karp, 1972), but there is software that implements efficient strategies for solving some of its instances (Achterberg, 2004). ILP problems are defined by three parameters. First, there is a set of variables that can take values from a finite set. Second, there is an objective function that has to be maximized, and third, there is a set of constraints that must be satisfied. In our case, we define a binary variable xs E 10, 11 for every node s in the graph. Its value is 1 or 0, that respectively determines the presence or absence of s in the set Cmax. The objective function is � xsw(s) s∈S(L) The constraints are defined using the edges of the 63 graph. For every edge (s1, s2) in the graph, we add the following const</context>
</contexts>
<marker>Achterberg, 2004</marker>
<rawString>Tobias Achterberg. 2004. SCIP - a framework to integrate Constraint and Mixed Integer Programming. Technical report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rens Bod</author>
</authors>
<title>An all-subtrees approach to unsupervised parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL</booktitle>
<contexts>
<context position="1262" citStr="Bod (2006" startWordPosition="202" endWordPosition="203">d for the unlabeled F1 performance that any UNTS grammar can achieve over a given treebank. Our strategy is to characterize all possible versions of the gold treebank that UNTS grammars can produce and to find the one that optimizes a metric we define. We show a way to translate this score into an upper bound for the F1. In particular, we show that the F1 parsing score of any UNTS grammar can not be beyond 82.2% when the gold treebank is the WSJ10 corpus. 1 Introduction Unsupervised learning of natural language has received a lot of attention in the last years, e.g., Klein and Manning (2004), Bod (2006a) and Seginer (2007). Most of them use sentences from a treebank for training and trees from the same treebank for evaluation. As such, the best model for unsupervised parsing is the one that reports the best performance. Unambiguous Non-Terminally Separated (UNTS) grammars have properties that make them attractive for grammatical inference. These grammars have been shown to be PAC-learnable in polynomial time (Clark, 2006), meaning that under certain circumstances, the underlying grammar can be learned from a sample of the underlying language. Moreover, UNTS grammars have been successfully u</context>
<context position="23421" citStr="Bod, 2006" startWordPosition="4476" endWordPosition="4477">othesis wc &lt; w. 5 UNTS Bounds for the WSJ10 Treebank In this section we focus on trying to find real upper bounds building the graph for a particular treebank T. We find the best independent set, we build the UNTS version Tmax of T and we compute the upper bound for F 1. The treebank we use for experiments is the WSJ10, which consists of the sentences of the WSJ Penn Treebank whose length is at most 10 words after removing punctuation marks (Klein and Manning, 2004). We also removed lexical entries transforming POS tags into our terminal symbols as it is usually done (Klein and Manning, 2004; Bod, 2006a). We start by finding the best independent set. To solve the problem in the practice, we convert it into an Integer Linear Programming (ILP) problem. ILP is also NP-hard (Karp, 1972), but there is software that implements efficient strategies for solving some of its instances (Achterberg, 2004). ILP problems are defined by three parameters. First, there is a set of variables that can take values from a finite set. Second, there is an objective function that has to be maximized, and third, there is a set of constraints that must be satisfied. In our case, we define a binary variable xs E 10, </context>
<context position="27054" citStr="Bod (2006" startWordPosition="5152" endWordPosition="5153">2% Recall R 62.8% F1 F1 74.4% Table 2: Summary of the scores for Cmax. Table 3 shows results that allow us to compare the upper bounds with state-of-the-art parsing scores. BestW corresponds to the scores of Tmax and UBoundF1 is the result of our translation function f. From the table we can see that an unsupervised parser based on UNTS grammars may reach a sate-of-the-art performance over the WSJ10. RBranch is a WSJ10 version where all trees are binary and right branching. DMV, CCM and DMV+CCM are the results reported in Klein and Manning (2004). U-DOP and UML-DOP are the results reported in Bod (2006b) and Bod (2006a) respectively. Incremental refers to the results reported in Seginer (2007). We believe that our upper bound is a generous one and that it might be difficult to achieve it for two reasons. First, since the WSJ10 corpus is a rather flat treebank, from the 68803 substrings only 10% of them are such that c(s) &gt; d(s). Our procedure has to decide among this 10% which of the strings are constituents. An unsupervised method has to choose the set of constituents from the set of all 68803 possible substrings. Second, we are supposing a recall of 100% which is clearly too optimistic. W</context>
</contexts>
<marker>Bod, 2006</marker>
<rawString>Rens Bod. 2006a. An all-subtrees approach to unsupervised parsing. In Proceedings of COLING-ACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rens Bod</author>
</authors>
<title>Unsupervised parsing with U-DOP.</title>
<date>2006</date>
<booktitle>In Proceedings of CoNLL-X.</booktitle>
<contexts>
<context position="1262" citStr="Bod (2006" startWordPosition="202" endWordPosition="203">d for the unlabeled F1 performance that any UNTS grammar can achieve over a given treebank. Our strategy is to characterize all possible versions of the gold treebank that UNTS grammars can produce and to find the one that optimizes a metric we define. We show a way to translate this score into an upper bound for the F1. In particular, we show that the F1 parsing score of any UNTS grammar can not be beyond 82.2% when the gold treebank is the WSJ10 corpus. 1 Introduction Unsupervised learning of natural language has received a lot of attention in the last years, e.g., Klein and Manning (2004), Bod (2006a) and Seginer (2007). Most of them use sentences from a treebank for training and trees from the same treebank for evaluation. As such, the best model for unsupervised parsing is the one that reports the best performance. Unambiguous Non-Terminally Separated (UNTS) grammars have properties that make them attractive for grammatical inference. These grammars have been shown to be PAC-learnable in polynomial time (Clark, 2006), meaning that under certain circumstances, the underlying grammar can be learned from a sample of the underlying language. Moreover, UNTS grammars have been successfully u</context>
<context position="23421" citStr="Bod, 2006" startWordPosition="4476" endWordPosition="4477">othesis wc &lt; w. 5 UNTS Bounds for the WSJ10 Treebank In this section we focus on trying to find real upper bounds building the graph for a particular treebank T. We find the best independent set, we build the UNTS version Tmax of T and we compute the upper bound for F 1. The treebank we use for experiments is the WSJ10, which consists of the sentences of the WSJ Penn Treebank whose length is at most 10 words after removing punctuation marks (Klein and Manning, 2004). We also removed lexical entries transforming POS tags into our terminal symbols as it is usually done (Klein and Manning, 2004; Bod, 2006a). We start by finding the best independent set. To solve the problem in the practice, we convert it into an Integer Linear Programming (ILP) problem. ILP is also NP-hard (Karp, 1972), but there is software that implements efficient strategies for solving some of its instances (Achterberg, 2004). ILP problems are defined by three parameters. First, there is a set of variables that can take values from a finite set. Second, there is an objective function that has to be maximized, and third, there is a set of constraints that must be satisfied. In our case, we define a binary variable xs E 10, </context>
<context position="27054" citStr="Bod (2006" startWordPosition="5152" endWordPosition="5153">2% Recall R 62.8% F1 F1 74.4% Table 2: Summary of the scores for Cmax. Table 3 shows results that allow us to compare the upper bounds with state-of-the-art parsing scores. BestW corresponds to the scores of Tmax and UBoundF1 is the result of our translation function f. From the table we can see that an unsupervised parser based on UNTS grammars may reach a sate-of-the-art performance over the WSJ10. RBranch is a WSJ10 version where all trees are binary and right branching. DMV, CCM and DMV+CCM are the results reported in Klein and Manning (2004). U-DOP and UML-DOP are the results reported in Bod (2006b) and Bod (2006a) respectively. Incremental refers to the results reported in Seginer (2007). We believe that our upper bound is a generous one and that it might be difficult to achieve it for two reasons. First, since the WSJ10 corpus is a rather flat treebank, from the 68803 substrings only 10% of them are such that c(s) &gt; d(s). Our procedure has to decide among this 10% which of the strings are constituents. An unsupervised method has to choose the set of constituents from the set of all 68803 possible substrings. Second, we are supposing a recall of 100% which is clearly too optimistic. W</context>
</contexts>
<marker>Bod, 2006</marker>
<rawString>Rens Bod. 2006b. Unsupervised parsing with U-DOP. In Proceedings of CoNLL-X.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
</authors>
<title>PAC-learning unambiguous NTS languages.</title>
<date>2006</date>
<booktitle>In Proceedings ofICGI-2006.</booktitle>
<contexts>
<context position="1690" citStr="Clark, 2006" startWordPosition="268" endWordPosition="269">gold treebank is the WSJ10 corpus. 1 Introduction Unsupervised learning of natural language has received a lot of attention in the last years, e.g., Klein and Manning (2004), Bod (2006a) and Seginer (2007). Most of them use sentences from a treebank for training and trees from the same treebank for evaluation. As such, the best model for unsupervised parsing is the one that reports the best performance. Unambiguous Non-Terminally Separated (UNTS) grammars have properties that make them attractive for grammatical inference. These grammars have been shown to be PAC-learnable in polynomial time (Clark, 2006), meaning that under certain circumstances, the underlying grammar can be learned from a sample of the underlying language. Moreover, UNTS grammars have been successfully used to induce grammars from unannotated corpora in competitions of learnability of formal languages (Clark, 2007). UNTS grammars can be used for modeling natural language. They can be induced using any training material, the induced models can be evaluated using trees from a treebank, and their performance can be compared against state-of-theart unsupervised models. Different learning algorithms might produce different gramm</context>
</contexts>
<marker>Clark, 2006</marker>
<rawString>Alexander Clark. 2006. PAC-learning unambiguous NTS languages. In Proceedings ofICGI-2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
</authors>
<title>Learning deterministic context free grammars: The Omphalos competition.</title>
<date>2007</date>
<booktitle>Machine Learning,</booktitle>
<volume>66</volume>
<issue>1</issue>
<contexts>
<context position="1975" citStr="Clark, 2007" startWordPosition="308" endWordPosition="309">e treebank for evaluation. As such, the best model for unsupervised parsing is the one that reports the best performance. Unambiguous Non-Terminally Separated (UNTS) grammars have properties that make them attractive for grammatical inference. These grammars have been shown to be PAC-learnable in polynomial time (Clark, 2006), meaning that under certain circumstances, the underlying grammar can be learned from a sample of the underlying language. Moreover, UNTS grammars have been successfully used to induce grammars from unannotated corpora in competitions of learnability of formal languages (Clark, 2007). UNTS grammars can be used for modeling natural language. They can be induced using any training material, the induced models can be evaluated using trees from a treebank, and their performance can be compared against state-of-theart unsupervised models. Different learning algorithms might produce different grammars and, consequently, different scores. The fact that the class of UNTS grammars is PAC learnable does not convey any information on the possible scores that different UNTS grammars might produce. From a performance oriented perspective it might be possible to have an upper bound ove</context>
<context position="6318" citStr="Clark, 2007" startWordPosition="1073" endWordPosition="1074">presents our characterization of UNTS grammars. Section 3 introduces the metric we optimized and explains how the closest version of the treebank is found. Section 4 explains how the upper bound for our metric is translated to an upper bound of the F1 score. Section 5 presents our bound for UNTS grammars using the WSJ10 and finally Section 6 concludes the paper. 2 UNTS Grammars and Languages Formally, a context free grammar G = (E, N, 5, P) is said to be Non-Terminally Separated (NTS) if, for all X, Y E N and α, Q, -y E (E U N)∗ such that X =*=&gt;. αQ-y and Y =*=&gt;. Q, we have that X =*=&gt;. αY-y (Clark, 2007). Unambiguous NTS (UNTS) grammars are those NTS grammars that parses unambiguously every instance of the language. Given any grammar G, a substring s of r E L(G) is called a constituent of r if and only if there is an X in N such that 5 =*&gt;. uXv =*&gt;. usv = r. In contrast, a string s is called a non-constituent or distituent of r E L(G) if s is not a constituent of r. We say that s is a constituent of a language L(G) if for every r that contains s, s is a constituent of r. In contrast, s is a distituent of L(G) if for every r where s occurs, s is a distituent of r. An interesting characterizati</context>
</contexts>
<marker>Clark, 2007</marker>
<rawString>Alexander Clark. 2007. Learning deterministic context free grammars: The Omphalos competition. Machine Learning, 66(1):93–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard M Karp</author>
</authors>
<title>Reducibility among combinatorial problems.</title>
<date>1972</date>
<journal>Complexity of Computer Computations,</journal>
<pages>85--103</pages>
<editor>In R. E. Miller and J. W. Thatcher, editors,</editor>
<publisher>Plenum Press.</publisher>
<contexts>
<context position="4416" citStr="Karp, 1972" startWordPosition="720" endWordPosition="721">c Aspects of Grammatical Inference, pages 58–65, Athens, Greece, 30 March 2009. c�2009 Association for Computational Linguistics 58 it might produce. UNTS grammars have a special characterization that makes the search space easy to define but whose exploration is NP-hard. In this paper we present a way to characterize UNTS grammars and a metric function to measure the closeness between two different version of a treebank. We show that the problem of finding the closest UNTS version of the treebank can be described as Maximum Weight Independent Set (MWIS) problem, a well known NP-hard problem (Karp, 1972). The exploration algorithm returns a version of the treebank that is the closest to the gold standard in terms of our own metric. We show that the F1-measure is related to our measure and that it is possible to find and upper bound of the F1-performance for all UNTS grammars. Moreover, we compute this upper bound for the WSJ10, a subset of the Penn Treebank (Marcus et al., 1994) using POS tags as the alphabet. The upper bound we found is 82.2% for the F1 measure. Our result suggest that UNTS grammars are a formalism that has the potential to achieve state-of-the-art unsupervised parsing perfo</context>
<context position="15077" citStr="Karp, 1972" startWordPosition="2756" endWordPosition="2757"> subset of the set of nodes that do not have any pair of nodes connected by an edge. Clearly, there are exponentially many possible ways to select an independent set, and each of these sets represents a set of constituents. But, since we are interested in the best set of constituents, we associate to each independent set C the weight W(C) defined as E sEC w(s). Our aim is then to find a set Cmax that maximizes this weight. This problem is a well known problem of graph theory known in the literature as the Maximum Weight Independent Set (MWIS) problem. This problem is also known to be NP-hard (Karp, 1972). We still have to choose a definition for w(s). We want to find the grammar that maximizes F1. Unfortunately, F 1 can not be expressed in terms of a sum of weights. Maximization of F1 is beyond the expressiveness of our model, but our strategy is to define a measure that correlates with F1 and that can be expressed as a sum of weights. In order to introduce our measure, we first define c(s) and d(s) as the number of times a string s appears in the gold treebank T as a constituent and as a distituent respectively. Observe that if we choose to include s as a constituent of C, the resulting tree</context>
<context position="23605" citStr="Karp, 1972" startWordPosition="4508" endWordPosition="4509">ependent set, we build the UNTS version Tmax of T and we compute the upper bound for F 1. The treebank we use for experiments is the WSJ10, which consists of the sentences of the WSJ Penn Treebank whose length is at most 10 words after removing punctuation marks (Klein and Manning, 2004). We also removed lexical entries transforming POS tags into our terminal symbols as it is usually done (Klein and Manning, 2004; Bod, 2006a). We start by finding the best independent set. To solve the problem in the practice, we convert it into an Integer Linear Programming (ILP) problem. ILP is also NP-hard (Karp, 1972), but there is software that implements efficient strategies for solving some of its instances (Achterberg, 2004). ILP problems are defined by three parameters. First, there is a set of variables that can take values from a finite set. Second, there is an objective function that has to be maximized, and third, there is a set of constraints that must be satisfied. In our case, we define a binary variable xs E 10, 11 for every node s in the graph. Its value is 1 or 0, that respectively determines the presence or absence of s in the set Cmax. The objective function is � xsw(s) s∈S(L) The constrai</context>
</contexts>
<marker>Karp, 1972</marker>
<rawString>Richard M. Karp. 1972. Reducibility among combinatorial problems. In R. E. Miller and J. W. Thatcher, editors, Complexity of Computer Computations, pages 85–103. Plenum Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Corpus-based induction of syntactic structure: Models of dependency and constituency.</title>
<date>2004</date>
<booktitle>In Proceedings ofACL</booktitle>
<pages>42</pages>
<contexts>
<context position="1251" citStr="Klein and Manning (2004)" startWordPosition="198" endWordPosition="201">thod to find an upper bound for the unlabeled F1 performance that any UNTS grammar can achieve over a given treebank. Our strategy is to characterize all possible versions of the gold treebank that UNTS grammars can produce and to find the one that optimizes a metric we define. We show a way to translate this score into an upper bound for the F1. In particular, we show that the F1 parsing score of any UNTS grammar can not be beyond 82.2% when the gold treebank is the WSJ10 corpus. 1 Introduction Unsupervised learning of natural language has received a lot of attention in the last years, e.g., Klein and Manning (2004), Bod (2006a) and Seginer (2007). Most of them use sentences from a treebank for training and trees from the same treebank for evaluation. As such, the best model for unsupervised parsing is the one that reports the best performance. Unambiguous Non-Terminally Separated (UNTS) grammars have properties that make them attractive for grammatical inference. These grammars have been shown to be PAC-learnable in polynomial time (Clark, 2006), meaning that under certain circumstances, the underlying grammar can be learned from a sample of the underlying language. Moreover, UNTS grammars have been suc</context>
<context position="5273" citStr="Klein and Manning (2004)" startWordPosition="870" endWordPosition="873">of the F1-performance for all UNTS grammars. Moreover, we compute this upper bound for the WSJ10, a subset of the Penn Treebank (Marcus et al., 1994) using POS tags as the alphabet. The upper bound we found is 82.2% for the F1 measure. Our result suggest that UNTS grammars are a formalism that has the potential to achieve state-of-the-art unsupervised parsing performance but does not guarantee that there exists a grammar that can actually achieve the 82.2%. To the best of our knowledge, there is no previous research on finding upper bounds for performance over a concrete class of grammars. In Klein and Manning (2004), the authors compute an upper bound for parsing with binary trees a gold treebank that is not binary. This upper bound, that is 88.1% for the WSJ10, is for any parser that returns binary trees, including the concrete models developed in the same work. But their upper bound does not use any specific information of the concrete models that may help them to find better ones. The rest of the paper is organized as follows. Section 2 presents our characterization of UNTS grammars. Section 3 introduces the metric we optimized and explains how the closest version of the treebank is found. Section 4 e</context>
<context position="18832" citStr="Klein and Manning, 2004" startWordPosition="3530" endWordPosition="3533">e ordering over the family of candidate constituent sets C: there are gold treebanks T (used for computing the metrics), and sets C1, C2 such that F1(C1) &lt; F1(C2) and W(C1) &gt; W(C2). For example, consider the gold treebank T in Figure 4 (a). The table in Figure 4 (b) displays two sets C1 and C2, the treebanks they produce, and their values of F1 and W. Note that C2 is the result of adding the string ef to C1, also note that c(ef) = 1 and d(ef) = 2. This improves the F1 score but produces a lower W. The F1 measure we work with is the one defined in the recent literature of unsupervised parsing (Klein and Manning, 2004). F1 is defined in terms of Precision and Recall as usual, and the last two measures are micro-averaged measures that include full-span brackets, and that ignore both unary branches and brackets of span one. For simplicity, the previous example does not count the full-span brackets. As the example shows, the upper bound for W might not be an upper bound of F1, but it is possible to find a way to define an upper bound of F1 using the upper bound of W. In this section we define a function f with the following property. Let X and Y be the sets of W-weights and F1-weights for all possible UNTS gra</context>
<context position="23282" citStr="Klein and Manning, 2004" startWordPosition="4450" endWordPosition="4453"> &lt; f(wC) as required. The second inequality is proved by observing that f(w) is monotonically increasing in w, and by applying it to both sides of the hypothesis wc &lt; w. 5 UNTS Bounds for the WSJ10 Treebank In this section we focus on trying to find real upper bounds building the graph for a particular treebank T. We find the best independent set, we build the UNTS version Tmax of T and we compute the upper bound for F 1. The treebank we use for experiments is the WSJ10, which consists of the sentences of the WSJ Penn Treebank whose length is at most 10 words after removing punctuation marks (Klein and Manning, 2004). We also removed lexical entries transforming POS tags into our terminal symbols as it is usually done (Klein and Manning, 2004; Bod, 2006a). We start by finding the best independent set. To solve the problem in the practice, we convert it into an Integer Linear Programming (ILP) problem. ILP is also NP-hard (Karp, 1972), but there is software that implements efficient strategies for solving some of its instances (Achterberg, 2004). ILP problems are defined by three parameters. First, there is a set of variables that can take values from a finite set. Second, there is an objective function th</context>
<context position="26997" citStr="Klein and Manning (2004)" startWordPosition="5140" endWordPosition="5143">and its graph. Hits H 22169 Misses M 2127 Weight W 20042 Precision P 91.2% Recall R 62.8% F1 F1 74.4% Table 2: Summary of the scores for Cmax. Table 3 shows results that allow us to compare the upper bounds with state-of-the-art parsing scores. BestW corresponds to the scores of Tmax and UBoundF1 is the result of our translation function f. From the table we can see that an unsupervised parser based on UNTS grammars may reach a sate-of-the-art performance over the WSJ10. RBranch is a WSJ10 version where all trees are binary and right branching. DMV, CCM and DMV+CCM are the results reported in Klein and Manning (2004). U-DOP and UML-DOP are the results reported in Bod (2006b) and Bod (2006a) respectively. Incremental refers to the results reported in Seginer (2007). We believe that our upper bound is a generous one and that it might be difficult to achieve it for two reasons. First, since the WSJ10 corpus is a rather flat treebank, from the 68803 substrings only 10% of them are such that c(s) &gt; d(s). Our procedure has to decide among this 10% which of the strings are constituents. An unsupervised method has to choose the set of constituents from the set of all 68803 possible substrings. Second, we are supp</context>
</contexts>
<marker>Klein, Manning, 2004</marker>
<rawString>Dan Klein and Christopher D. Manning. 2004. Corpus-based induction of syntactic structure: Models of dependency and constituency. In Proceedings ofACL 42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of english: The Penn treebank.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="4798" citStr="Marcus et al., 1994" startWordPosition="788" endWordPosition="792">oseness between two different version of a treebank. We show that the problem of finding the closest UNTS version of the treebank can be described as Maximum Weight Independent Set (MWIS) problem, a well known NP-hard problem (Karp, 1972). The exploration algorithm returns a version of the treebank that is the closest to the gold standard in terms of our own metric. We show that the F1-measure is related to our measure and that it is possible to find and upper bound of the F1-performance for all UNTS grammars. Moreover, we compute this upper bound for the WSJ10, a subset of the Penn Treebank (Marcus et al., 1994) using POS tags as the alphabet. The upper bound we found is 82.2% for the F1 measure. Our result suggest that UNTS grammars are a formalism that has the potential to achieve state-of-the-art unsupervised parsing performance but does not guarantee that there exists a grammar that can actually achieve the 82.2%. To the best of our knowledge, there is no previous research on finding upper bounds for performance over a concrete class of grammars. In Klein and Manning (2004), the authors compute an upper bound for parsing with binary trees a gold treebank that is not binary. This upper bound, that</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1994</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary A. Marcinkiewicz. 1994. Building a large annotated corpus of english: The Penn treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Seginer</author>
</authors>
<title>Fast unsupervised incremental parsing.</title>
<date>2007</date>
<booktitle>In Proceedings ofACL 45.</booktitle>
<contexts>
<context position="1283" citStr="Seginer (2007)" startWordPosition="205" endWordPosition="206">led F1 performance that any UNTS grammar can achieve over a given treebank. Our strategy is to characterize all possible versions of the gold treebank that UNTS grammars can produce and to find the one that optimizes a metric we define. We show a way to translate this score into an upper bound for the F1. In particular, we show that the F1 parsing score of any UNTS grammar can not be beyond 82.2% when the gold treebank is the WSJ10 corpus. 1 Introduction Unsupervised learning of natural language has received a lot of attention in the last years, e.g., Klein and Manning (2004), Bod (2006a) and Seginer (2007). Most of them use sentences from a treebank for training and trees from the same treebank for evaluation. As such, the best model for unsupervised parsing is the one that reports the best performance. Unambiguous Non-Terminally Separated (UNTS) grammars have properties that make them attractive for grammatical inference. These grammars have been shown to be PAC-learnable in polynomial time (Clark, 2006), meaning that under certain circumstances, the underlying grammar can be learned from a sample of the underlying language. Moreover, UNTS grammars have been successfully used to induce grammar</context>
<context position="27147" citStr="Seginer (2007)" startWordPosition="5166" endWordPosition="5167">ults that allow us to compare the upper bounds with state-of-the-art parsing scores. BestW corresponds to the scores of Tmax and UBoundF1 is the result of our translation function f. From the table we can see that an unsupervised parser based on UNTS grammars may reach a sate-of-the-art performance over the WSJ10. RBranch is a WSJ10 version where all trees are binary and right branching. DMV, CCM and DMV+CCM are the results reported in Klein and Manning (2004). U-DOP and UML-DOP are the results reported in Bod (2006b) and Bod (2006a) respectively. Incremental refers to the results reported in Seginer (2007). We believe that our upper bound is a generous one and that it might be difficult to achieve it for two reasons. First, since the WSJ10 corpus is a rather flat treebank, from the 68803 substrings only 10% of them are such that c(s) &gt; d(s). Our procedure has to decide among this 10% which of the strings are constituents. An unsupervised method has to choose the set of constituents from the set of all 68803 possible substrings. Second, we are supposing a recall of 100% which is clearly too optimistic. We believe that we can find a tighter upper bound by finding an upper bound for recall, and by</context>
</contexts>
<marker>Seginer, 2007</marker>
<rawString>Yoav Seginer. 2007. Fast unsupervised incremental parsing. In Proceedings ofACL 45.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>