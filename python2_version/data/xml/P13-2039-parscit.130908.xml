<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000329">
<title confidence="0.976212">
TopicSpam: a Topic-Model-Based Approach for Spam Detection
</title>
<author confidence="0.999684">
Jiwei Li , Claire Cardie
</author>
<affiliation confidence="0.9967605">
School of Computer Science
Cornell University
</affiliation>
<address confidence="0.879118">
Ithaca, NY, 14853
</address>
<email confidence="0.9917115">
jl3226@cornell.edu
cardie@cs.cornell.edu
</email>
<author confidence="0.99534">
Sujian Li
</author>
<affiliation confidence="0.9977475">
Laboratory of Computational Linguistics
Peking University
</affiliation>
<address confidence="0.916985">
Bejing, P.R.China, 150001
</address>
<email confidence="0.998269">
lisujian@pku.edu.cn
</email>
<sectionHeader confidence="0.997385" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999796823529412">
Product reviews are now widely used by
individuals and organizations for decision
making (Litvin et al., 2008; Jansen, 2010).
And because of the profits at stake, peo-
ple have been known to try to game the
system by writing fake reviews to promote
target products. As a result, the task of de-
ceptive review detection has been gaining
increasing attention. In this paper, we pro-
pose a generative LDA-based topic mod-
eling approach for fake review detection.
Our model can aptly detect the subtle dif-
ferences between deceptive reviews and
truthful ones and achieves about 95% ac-
curacy on review spam datasets, outper-
forming existing baselines by a large mar-
gin.
</bodyText>
<sectionHeader confidence="0.999515" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99974675">
Consumers rely increasingly on user-generated
online reviews to make purchase decisions. Pos-
itive opinions can result in significant financial
gains. This gives rise to deceptive opinion spam
(Ott et al., 2011; Jindal et al., 2008), fake reviews
written to sound authentic and deliberately mis-
lead readers. Previous research has shown that
humans have difficulty distinguishing fake from
truthful reviews, operating for the most part at
chance (Ott et al., 2011). Consider, for example,
the following two hotel reviews. One is truthful
and the other is deceptive1:
</bodyText>
<footnote confidence="0.8802834">
1. My husband and I stayed for two nights at the Hilton
Chicago. We were very pleased with the accommoda-
tions and enjoyed the service every minute of it! The
bedrooms are immaculate, and the linens are very soft.
We also appreciated the free wifi, as we could stay in
touch with friends while staying in Chicago. The bath-
room was quite spacious, and I loved the smell of the
shampoo they provided. Their service was amazing,
1The first example is a deceptive review.
and we absolutely loved the beautiful indoor pool. I
would recommend staying here to anyone.
2. We stayed at the Sheraton by Navy Pier the first week-
end of November. The view from both rooms was spec-
tacular (as you can tell from the picture attached). They
also left a plate of cookies and treats in the kids room
</footnote>
<construct confidence="0.793329">
upon check-in made us all feel very special. The hotel
is central to both Navy Pier and Michigan Ave. so we
walked, trolleyed, and cabbed all around the area. We
ate the breakfast buffet on both mornings and thought
it was pretty good. The eggs were a little runny. Our
six year old ate free and our two eleven year old were
$14 (instead of the adult $20). The rooms were clean,
the concierge and reception staff were both friendly
and helpful...we will definitely visit this Sheraton again
when we stay in Chicago next time.
</construct>
<bodyText confidence="0.999940642857143">
Because of the difficulty of recognizing deceptive
opinions, there has been a widespread and growing
interest in developing automatic, usually learning-
based methods to help users identify deceptive re-
views (Ott et al., 2011; Jindal et al., 2008; Jindal
et al., 2010; Li et al., 2011; Lim et al., 2011; Wang
et al., 2011).
The state-of-the-art approach treats the task of
spam detection as a text categorization prob-
lem and was first introduced by Jindal and Liu
(2009) who trained a supervised classifier to dis-
tinguish duplicated reviews (assumed deceptive)
from original ones (assumed truthful). Since then,
many supervised approaches have been proposed
for spam detection. Ott et al. (2011) employed
standard word and part-of-speech (POS) n-gram
features for supervised learning and built a gold −
standard opinion dataset of 800 reviews. Lim et
al. (2010) proposed the inclusion of user behavior-
based features and found that behavior abnormali-
ties of reviewers could predict spammers, without
using any textual features. Li et al. (2011) care-
fully explored review-related features based on
content and sentiment, training a semi-supervised
classifier for opinion spam detection. However,
the disadvantages of standard supervised learning
methods are obvious. First, they do not gener-
ally provide readers with a clear probabilistic pre-
</bodyText>
<page confidence="0.971397">
217
</page>
<bodyText confidence="0.962372243243243">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 217–221,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
diction of how likely a review is to be deceptive
vs. truthful. Furthermore, identifying features that
provide direct evidence against deceptive reviews
is always a hard problem.
LDA topic models (Blei et al., 2003) have
widely been used for their ability to model latent
topics in document collection. In LDA, each docu-
ment is presented as a mixture distribution of top-
ics and each topic is presented as a mixture distri-
bution of words. Researchers also integrated dif-
ferent levels of information into LDA topic mod-
els to model the specific knowledge that they are
interested in, such as user-specific information
(Rosen-zvi et al., 2006), document-specific infor-
mation (Li et al., 2010) and time-specific infor-
mation (Diao et al., 2012). Ramage et al. (2009)
developed a Labeled LDA model to define a one-
to-one correspondence between LDA latent topics
and tags. Chemudugunta et al. (2008) illustrated
that by considering background information and
document-specific information, we can largely im-
prove the performance of topic modeling.
In this paper, we propose a Bayesian approach
called TopicSpam for deceptive review detection.
Our approach, which is a variation of Latent
Dirichlet Allocation (LDA) (Blei et al., 2003),
aims to detect the subtle differences between the
topic-word distributions of deceptive reviews vs.
truthful ones. In addition, our model can give
a clear probabilistic prediction on how likely a
review should be treated as deceptive or truth-
ful. Performance is tested on dataset from Ott et
al.(2011) that contains 800 reviews of 20 Chicago
hotels. Our model achieves more than 94% accu-
racy on that dataset.
</bodyText>
<sectionHeader confidence="0.991762" genericHeader="introduction">
2 TopicSpam
</sectionHeader>
<bodyText confidence="0.991365142857143">
We are presented with four subsets of ho-
tel reviews, M = {Mi}i=4
i=1, representing
deceptive train, truthful train, deceptive test
and truthful test data, respectively. Each re-
view r is comprised of a number of words r =
{wt}t=nr
</bodyText>
<equation confidence="0.52522">
t=1 .
</equation>
<bodyText confidence="0.999140333333333">
the datasets M; output is the label (deceptive,
truthful) for each review in M3 and M4. V denotes
vocabulary size.
</bodyText>
<subsectionHeader confidence="0.999077">
2.1 Details of TopicSpam
</subsectionHeader>
<bodyText confidence="0.99978525">
In TopicSpam, each document is modeled as a
bag of words, which are assumed to be gener-
ated from a mixture of latent topics. Each word
is associated with a latent variable that specifies
</bodyText>
<figureCaption confidence="0.985299">
Figure 1: Graphical Model for TopicSpam
</figureCaption>
<bodyText confidence="0.997476833333333">
the topic from which it is generated. Words in a
document are assumed to be conditionally inde-
pendent given the hidden topics. A general back-
ground distribution OB and hotel-specific distri-
butions OHj(j = 1, ..., 20) are first introduced
to capture the background information and hotel-
specific information. To capture the difference
between deceptive reviews and truthful reviews,
TopicSpam also learns a deceptive topic distribu-
tion OD and truthful topic distribution OT. The
generative model of TopicSpam is shown as fol-
lows:
</bodyText>
<listItem confidence="0.999355444444444">
• For a training review in r1j E M1, words are
originated from one of the three different top-
ics: OB, OHj and OD.
• For a training review in r2j E M2, words are
originated from one of the three different top-
ics: OB, OHj and OT.
• For a test review in rmj E Mm, m = 3, 4,
words are originated from one of the four dif-
ferent topics: OB, OHj OD and OT .
</listItem>
<bodyText confidence="0.999746285714286">
The generation process of TopicSpam is shown
in Figure 1 and the corresponding graphical
model is illustrated in Figure 2. We use
A = (AG, AHi, AD, AT) to represent the asym-
metric priors for topic-word distribution genera-
tion. In our experiments, we set AG = 0.1,
and AHi = AD = AT = 0.01. The intu-
ition for the asymmetric priors is that there should
be more words assigned to the background topic.
&apos;Y = [&apos;YB, &apos;YHi,&apos;YD,&apos;YT] denotes the priors for
the document-level topic distribution in the LDA
model. We set &apos;YB = 2 and &apos;YT = &apos;YD = &apos;YHi = 1,
reflecting the intuition that more words in each
document should cover the background topic.
</bodyText>
<subsectionHeader confidence="0.656412">
2.2 Inference
</subsectionHeader>
<bodyText confidence="0.990813">
We adopt the collapsed Gibbs sampling strategy to
infer the latent parameters in TopicSpam. In Gibbs
Input for the TopicSpam algorithm is
</bodyText>
<page confidence="0.986189">
218
</page>
<listItem confidence="0.9980952">
1. sample OG — Dir(AG)
2. sample OD — Dir(AD)
3. sample OT — Dir(AT)
4. for each hotel j E [1, N]: sample OH; — AH
5. for each review r
</listItem>
<construct confidence="0.9978655">
if i=1: sample Br — Dir(&apos;YB,&apos;YH;,&apos;YD)
if i=2: sample Br — Dir(&apos;YB,&apos;YH;,&apos;YT)
if i=3: sample Br — Dir(&apos;YB, &apos;YH;, &apos;YD, &apos;YT)
if i=4: sample Br — Dir(&apos;YB,&apos;YH;,&apos;YD, &apos;YT)
</construct>
<figure confidence="0.359861">
for each word w in R
sample z — Br sample w — Oz
</figure>
<figureCaption confidence="0.996037">
Figure 2: Generative Model for TopicSpam
</figureCaption>
<bodyText confidence="0.99951575">
sampling, for each word w in review r, we need
to calculate P(zw|w, z_w, &apos;Y, A) in each iteration,
where z_w denotes the topic assignments for all
words except that of the current word zw.
</bodyText>
<equation confidence="0.99949">
P(zw = m|z_w, i, j, &apos;Y, A)
Nmr + &apos;Ym
E m0(Nm0
r+ &apos;Y�m) �
</equation>
<bodyText confidence="0.989187166666667">
where Nmr denotes the number of times that topic
m appears in current review r and Ewm denotes the
number of times that word w is assigned to topic
m. After each sampling iteration, the latent pa-
rameters can be estimated using the following for-
mulas:
</bodyText>
<table confidence="0.838252666666667">
Bm Nmr + &apos;Ym O(w) = Ewm + Am
r Em0 (Nrm0 + &apos;Ym) m Ew0 Ew0 m + V Am
(2)
</table>
<subsectionHeader confidence="0.998037">
2.3 Labeling the Test Data
</subsectionHeader>
<bodyText confidence="0.999685">
For each review r in the test data, let NDr denote
the number of words generated from the decep-
tive topic and NTr , the number of words generated
from the truthful topic. The decision for whether a
review is deceptive or truthful is made as follows:
</bodyText>
<listItem confidence="0.998914">
• if NDr &gt; NTr , r is deceptive.
• if NDr &lt; NTr , r is truthful.
• if NDr = NTr , it is hard to decide.
</listItem>
<bodyText confidence="0.994116">
Let P(D) denote the probability that r is deceptive
and P(T) denote the probability that r is truthful.
</bodyText>
<equation confidence="0.988135">
ND NT
r r
P(D) = ND + Nr P(T) = ND + Nr (3)
</equation>
<sectionHeader confidence="0.999687" genericHeader="method">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999402">
3.1 System Description
</subsectionHeader>
<bodyText confidence="0.9994927">
Our experiments are conducted on the dataset
from Ott et al.(2011), which contains reviews of
the 20 most popular hotels on TripAdvisor in the
Chicago areas. There are 20 truthful and 20 decep-
tive reviews for each of the chosen hotels (800 re-
views total). Deceptive reviews are gathered using
Amazon Mechanical Turk2. In our experiments,
we adopt the same 5-fold cross-validation strat-
egy as in Ott et al., using the same data partitions.
Words are stemmed using PorterStemmer3.
</bodyText>
<subsectionHeader confidence="0.997128">
3.2 Baselines
</subsectionHeader>
<bodyText confidence="0.99151353125">
We employ a number of techniques as baselines:
TopicTD: A topic-modeling approach that only
considers two topics: deceptive and truthful.
Words in deceptive train are all generated from
the deceptive topic and words in truthful train
are generated from the truthful topic. Test docu-
ments are presented with a mixture of the decep-
tive and truthful topics.
TopicTDB: A topic-modeling approach that
only considers background, deceptive and truthful
information.
SVM-Unigram: Using SVMlight(Joachims,
1999) to train linear SVM models on unigram fea-
tures.
SVM-Bigram: Using SVMlight(Joachims,
1999) to train linear SVM models on bigram fea-
tures.
SVM-Unigram-Removal1: In SVM-Unigram-
Removal, we first train TopicSpam. Then words
generated from hotel-specific topics are removed.
We use the remaining words as features in SVM-
light.
SVM-Unigram-Removal2: Same as SVM-
Unigram-removal-1 but removing all background
words and hotel-specific words.
Experimental results are shown in Table 14.
As we can see, the accuracy of TopicSpam is
0.948, outperforming TopicTD by 6.4%. This il-
lustrates the effectiveness of modeling background
and hotel-specific information for the opinion
spam detection problem. We also see that Top-
icSpam slightly outperforms TopicTDB, which
</bodyText>
<footnote confidence="0.995343">
2https://www.mturk.com/mturk/.
3http://tartarus.org/martin/PorterStemmer/
4Reviews with NP = Nr are regarded as incorrectly
classified by TopicSpam.
</footnote>
<equation confidence="0.994622">
Ewm + Am (1)
EVw0 Ewm + V Am
</equation>
<page confidence="0.997835">
219
</page>
<table confidence="0.99991425">
Approach Accuracy T-P T-R T-F D-P D-R D-F
TopicSpam 0.948 0.954 0.942 0.944 0.941 0.952 0.946
TopicTD 0.888 0.901 0.878 0.889 0.875 0.897 0.886
TopicTDB 0.931 0.938 0.926 0.932 0.925 0.937 0.930
SVM-Unigram 0.884 0.899 0.865 0.882 0.870 0.903 0.886
SVM-Bigram 0.896 0.901 0.890 0.896 0.891 0.903 0.897
SVM-Unigram-Removal1 0.895 0.906 0.889 0.898 0.887 0.907 0.898
SVM-Unigram-Removal2 0.822 0.852 0.806 0.829 0.793 0.840 0.817
</table>
<tableCaption confidence="0.998964">
Table 1: Performance for different approaches based on nested 5-fold cross-validation experiments.
</tableCaption>
<bodyText confidence="0.999956419354839">
neglects hotel-specific information. By check-
ing the results of Gibbs sampling, we find that
this is because only a small number of words
are generated by the hotel-specific topics. Top-
icTD and SVM-Unigram get comparative accu-
racy rates. This can be explained by the fact
that both models use unigram frequency as fea-
tures for the classifier or topic distribution train-
ing. SVM-Unigram-Removal1 is also slightly
better than SVM-Unigram. In SVM-Unigram-
removal1, hotel-specific words are removed for
classifier training. So the first-step LDA model
can be viewed as a feature selection process for the
SVM, giving rise to better results. We can also see
that the performance of SVM-Unigram-removal2
is worse than other baselines. This can be ex-
plained as follows: for example, word ”my” has
large probability to be generated from the back-
ground topic. However it can also be generated by
deceptive topic occasionaly but can hardly be gen-
erated from the truthful topic. So the removal of
these words results in the loss of useful informa-
tion, and leads to low accuracy rate.
Our topic-modeling approach uses word fre-
quency as features and does not involve any fea-
ture selection process. Here we present the re-
sults of the sample reviews from Section 1. Stop
words are labeled in black, background topics (B)
in blue, hotel specific topics (H) in orange, de-
ceptive topics (D) in red and truthful topic (T) in
green.
</bodyText>
<footnote confidence="0.883972538461539">
1. My husband and I stayed for two nights at the Hilton
Chicago. We were very pleased with the accommoda-
tions and enjoyed the service every minute of it! The
bedrooms are immaculate,and the linens are very soft.
We also appreciated the free wifi, as we could stay in
touch with friends while staying in Chicago. The bath-
room was quite spacious, and I loved the smell of the
shampoo they provided not like most hotel shampoos.
Their service was amazing,and we absolutely loved the
beautiful indoor pool. I would recommend staying here
to anyone.
[B,H,D,T]=[41,6,10,1] p(D)=0.909 P(T)=0.091
2. We stayed at the Sheraton by Navy Pier the first week-
end of November. The view from both rooms was spec-
tacular (as you can tell from the picture attached). They
also left a plate of cookies and treats in the kids room
upon check-in made us all feel very special. The ho-
tel is central to both Navy Pier and Michigan Ave. so
we walked, trolleyed, and cabbed all around the area.
We ate the breakfast buffet both mornings and thought
it was pretty good. The eggs were a little runny. Our
six year old ate free and our two eleven year old were
$14 ( instead of the adult $20) The rooms were clean,
the concierge and reception staff were both friendly
and helpful...we will definitely visit this Sheraton again
when we’re in Chicago next time.
</footnote>
<table confidence="0.991852">
[B,H,D,T]=[80,15,3,18] p(D)=0.143 P(T)=0.857
background deceptive truthful Hilton
hotel hotel room Hilton
stay my ) palmer
we chicago ( millennium
room will but lockwood
! room $ park
Chicago very bathroom lobby
my visit location line
great husband night valet
I city walk shampoo
very experience park dog
Omni Amalfi Sheraton James
Omni Amalfi tower James
pool breakfast Sheraton service
plasma view pool spa
sundeck floor river bar
chocolate bathroom lake upgrade
indoor cocktail navy primehouse
request morning indoor design
pillow wine shower overlook
suitable great kid romantic
area room theater home
</table>
<tableCaption confidence="0.9193785">
Table 2: Top words in different topics from Topic-
Spam
</tableCaption>
<sectionHeader confidence="0.998178" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.99981775">
In this paper, we propose a novel topic model
for deceptive opinion spam detection. Our model
achieves an accuracy of 94.8%, demonstrating its
effectiveness on the task.
</bodyText>
<sectionHeader confidence="0.997628" genericHeader="acknowledgments">
5 Acknowledgements
</sectionHeader>
<bodyText confidence="0.560882666666667">
We thank Myle Ott for his insightful comments and sugges-
tions. This work was supported in part by NSF Grant BCS-
0904822, a DARPA Deft grant, and by a gift from Google.
</bodyText>
<page confidence="0.994384">
220
</page>
<sectionHeader confidence="0.996252" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999376404040404">
David Blei, Andrew Ng and Micheal Jordan. Latent
Dirichlet allocation. 2003. In Journal of Machine
Learning Research.
Carlos Castillo, Debora Donato, Luca Becchetti, Paolo
Boldi, Stefano Leonardi Massimo Santini, and Se-
bastiano Vigna. A reference collection for web
spam. In Proceedings of annual international ACM
SIGIR conference on Research and development in
information retrieval, 2006.
Chaltanya Chemudugunta, Padhraic Smyth and Mark
Steyers. Modeling General and Specific Aspects of
Documents with a Probabilistic Topic Model.. In
Advances in Neural Information Processing Systems
19: Proceedings of the 2006 Conference.
Paul-Alexandru Chirita, Jorg Diederich, and Wolfgang
Nejdl. MailRank: using ranking for spam detection.
In Proceedings of ACM international conference on
Information and knowledge management. 2005.
Harris Drucke, Donghui Wu, and Vladimir Vapnik.
2002. Support vector machines for spam categoriza-
tion. In Neural Networks.
Qiming Diao, Jing Jiang, Feida Zhu and Ee-Peng Lim.
In Proceeding of the 50th Annual Meeting of the As-
sociation for Computational Linguistics. 2012
Thorsten Joachims. 1999. Making large-scale support
vector machine learning practical. In Advances in
kernel methods.
Jack Jansen. 2010. Online product research. In Pew In-
ternet and American Life Project Report.
Nitin Jindal, and Bing Liu. Opinion spam and analysis.
2008. In Proceedings of the international conference
on Web search and web data mining
Nitin Jindal, Bing Liu, and Ee-Peng Lim. Finding
Unusual Review Patterns Using Unexpected Rules.
2010. In Proceedings of the 19th ACM international
conference on Information and knowledge manage-
ment
Pranam Kolari, Akshay Java, Tim Finin, Tim Oates and
Anupam Joshi. Detecting Spam Blogs: A Machine
Learning Approach. In Proceedings of Association
for the Advancement of Artificial Intelligence. 2006.
Peng Li, Jing Jiang and Yinglin Wang. 2010. Gener-
ating templates of entity summaries with an entity-
aspect model and pattern mining. In Proceedings of
the 48th Annual Meeting of the Association for Com-
putational Linguistics.
Fangtao Li, Minlie Huang, Yi Yang, and Xiaoyan Zhu.
Learning to identify review Spam. 2011. In Proceed-
ings of the Twenty-Second international joint confer-
ence on Artificial Intelligence.
Ee-Peng Lim, Viet-An Nguyen, Nitin Jindal, Bing Liu,
and Hady Wirawan Lauw. Detecting Product Re-
view Spammers Using Rating Behavior. 2010. In
Proceedings of the 19th ACM international confer-
ence on Information and knowledge management.
Stephen Litvina, Ronald Goldsmithb and Bing Pana.
2008. Electronic word-of-mouth in hospitality
and tourism management. Tourism management,
29(3):458468.
Juan Martinez-Romo and Lourdes Araujo. Web Spam
Identification Through Language Model Analysis.
In AIRWeb. 2009.
Arjun Mukherjee, Bing Liu and Natalie Glance. Spot-
ting Fake Reviewer Groups in Consumer Reviews.
In Proceedings of the 18th international conference
on World wide web, 2012.
Alexandros Ntoulas, Marc Najork, Mark Manasse and
Dennis Fetterly. Detecting Spam Web Pages through
Content Analysis. In Proceedings of international
conference on World Wide Web 2006
Myle Ott, Yejin Choi, Claire Cardie and Jeffrey Han-
cock. Finding deceptive opinion spam by any stretch
of the imagination. 2011. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies
Bo Pang and Lillian Lee. Opinion mining and senti-
ment analysis. In Found. Trends Inf. Retr.
Daniel Ramage, David Hall, Ramesh Nallapati and
Christopher D. Manning. Labeled LDA: a super-
vised topic model for credit attribution in multi-
labeled corpora. 2009. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing 2009.
Michal Rosen-zvi, Thomas Griffith, Mark Steyvers and
Padhraic Smyth. The author-topic model for authors
and documents. In Proceedings of the 20th confer-
ence on Uncertainty in artificial intelligence.
Guan Wang, Sihong Xie, Bing Liu and Philip Yu. Re-
view Graph based Online Store Review Spammer
Detection. 2011. In Proceedings of 11th Interna-
tional Conference of Data Mining.
Baoning Wu, Vinay Goel and Brian Davison. Topical
TrustRank: using topicality to combat Web spam.
In Proceedings of international conference on World
Wide Web 2006 .
Kyang Yoo and Ulrike Gretzel. 2009. Compari-
son of Deceptive and Truthful Travel Reviews.
InInformation and Communication Technologies in
Tourism 2009.
</reference>
<page confidence="0.998302">
221
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.132330">
<title confidence="0.999881">TopicSpam: a Topic-Model-Based Approach for Spam Detection</title>
<author confidence="0.999492">Jiwei Li</author>
<affiliation confidence="0.967999">School of Computer Cornell</affiliation>
<address confidence="0.690219">Ithaca, NY,</address>
<email confidence="0.999264">cardie@cs.cornell.edu</email>
<author confidence="0.695606">Sujian</author>
<affiliation confidence="0.675934">Laboratory of Computational Peking</affiliation>
<address confidence="0.682344">Bejing, P.R.China,</address>
<email confidence="0.929124">lisujian@pku.edu.cn</email>
<abstract confidence="0.988249333333333">Product reviews are now widely used by individuals and organizations for decision making (Litvin et al., 2008; Jansen, 2010). And because of the profits at stake, people have been known to try to game the system by writing fake reviews to promote target products. As a result, the task of deceptive review detection has been gaining increasing attention. In this paper, we propose a generative LDA-based topic modeling approach for fake review detection. Our model can aptly detect the subtle differences between deceptive reviews and ones and achieves about accuracy on review spam datasets, outperforming existing baselines by a large margin.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Blei</author>
<author>Andrew Ng</author>
<author>Micheal Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>In Journal of Machine Learning Research.</journal>
<contexts>
<context position="4601" citStr="Blei et al., 2003" startWordPosition="733" endWordPosition="736">a semi-supervised classifier for opinion spam detection. However, the disadvantages of standard supervised learning methods are obvious. First, they do not generally provide readers with a clear probabilistic pre217 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 217–221, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics diction of how likely a review is to be deceptive vs. truthful. Furthermore, identifying features that provide direct evidence against deceptive reviews is always a hard problem. LDA topic models (Blei et al., 2003) have widely been used for their ability to model latent topics in document collection. In LDA, each document is presented as a mixture distribution of topics and each topic is presented as a mixture distribution of words. Researchers also integrated different levels of information into LDA topic models to model the specific knowledge that they are interested in, such as user-specific information (Rosen-zvi et al., 2006), document-specific information (Li et al., 2010) and time-specific information (Diao et al., 2012). Ramage et al. (2009) developed a Labeled LDA model to define a oneto-one co</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David Blei, Andrew Ng and Micheal Jordan. Latent Dirichlet allocation. 2003. In Journal of Machine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos Castillo</author>
<author>Debora Donato</author>
<author>Luca Becchetti</author>
<author>Paolo Boldi</author>
<author>Stefano Leonardi Massimo Santini</author>
<author>Sebastiano Vigna</author>
</authors>
<title>A reference collection for web spam.</title>
<date>2006</date>
<booktitle>In Proceedings of annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<marker>Castillo, Donato, Becchetti, Boldi, Santini, Vigna, 2006</marker>
<rawString>Carlos Castillo, Debora Donato, Luca Becchetti, Paolo Boldi, Stefano Leonardi Massimo Santini, and Sebastiano Vigna. A reference collection for web spam. In Proceedings of annual international ACM SIGIR conference on Research and development in information retrieval, 2006.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Chaltanya Chemudugunta</author>
</authors>
<title>Padhraic Smyth and Mark Steyers. Modeling General and Specific Aspects of Documents with a Probabilistic Topic Model..</title>
<booktitle>In Advances in Neural Information Processing Systems 19: Proceedings of the 2006 Conference.</booktitle>
<marker>Chemudugunta, </marker>
<rawString>Chaltanya Chemudugunta, Padhraic Smyth and Mark Steyers. Modeling General and Specific Aspects of Documents with a Probabilistic Topic Model.. In Advances in Neural Information Processing Systems 19: Proceedings of the 2006 Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul-Alexandru Chirita</author>
<author>Jorg Diederich</author>
<author>Wolfgang Nejdl</author>
</authors>
<title>MailRank: using ranking for spam detection.</title>
<date>2005</date>
<booktitle>In Proceedings of ACM international conference on Information and knowledge management.</booktitle>
<marker>Chirita, Diederich, Nejdl, 2005</marker>
<rawString>Paul-Alexandru Chirita, Jorg Diederich, and Wolfgang Nejdl. MailRank: using ranking for spam detection. In Proceedings of ACM international conference on Information and knowledge management. 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harris Drucke</author>
<author>Donghui Wu</author>
<author>Vladimir Vapnik</author>
</authors>
<title>Support vector machines for spam categorization. In Neural Networks.</title>
<date>2002</date>
<marker>Drucke, Wu, Vapnik, 2002</marker>
<rawString>Harris Drucke, Donghui Wu, and Vladimir Vapnik. 2002. Support vector machines for spam categorization. In Neural Networks.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiming Diao</author>
<author>Jing Jiang</author>
</authors>
<title>Feida Zhu and Ee-Peng Lim.</title>
<date>2012</date>
<booktitle>In Proceeding of the 50th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<marker>Diao, Jiang, 2012</marker>
<rawString>Qiming Diao, Jing Jiang, Feida Zhu and Ee-Peng Lim. In Proceeding of the 50th Annual Meeting of the Association for Computational Linguistics. 2012</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making large-scale support vector machine learning practical.</title>
<date>1999</date>
<booktitle>In Advances in kernel methods.</booktitle>
<contexts>
<context position="10828" citStr="Joachims, 1999" startWordPosition="1843" endWordPosition="1844">strategy as in Ott et al., using the same data partitions. Words are stemmed using PorterStemmer3. 3.2 Baselines We employ a number of techniques as baselines: TopicTD: A topic-modeling approach that only considers two topics: deceptive and truthful. Words in deceptive train are all generated from the deceptive topic and words in truthful train are generated from the truthful topic. Test documents are presented with a mixture of the deceptive and truthful topics. TopicTDB: A topic-modeling approach that only considers background, deceptive and truthful information. SVM-Unigram: Using SVMlight(Joachims, 1999) to train linear SVM models on unigram features. SVM-Bigram: Using SVMlight(Joachims, 1999) to train linear SVM models on bigram features. SVM-Unigram-Removal1: In SVM-UnigramRemoval, we first train TopicSpam. Then words generated from hotel-specific topics are removed. We use the remaining words as features in SVMlight. SVM-Unigram-Removal2: Same as SVMUnigram-removal-1 but removing all background words and hotel-specific words. Experimental results are shown in Table 14. As we can see, the accuracy of TopicSpam is 0.948, outperforming TopicTD by 6.4%. This illustrates the effectiveness of mo</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Making large-scale support vector machine learning practical. In Advances in kernel methods.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jack Jansen</author>
</authors>
<title>Online product research.</title>
<date>2010</date>
<booktitle>In Pew Internet and American Life Project Report.</booktitle>
<marker>Jansen, 2010</marker>
<rawString>Jack Jansen. 2010. Online product research. In Pew Internet and American Life Project Report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Jindal</author>
<author>Bing Liu</author>
</authors>
<title>Opinion spam and analysis.</title>
<date>2008</date>
<booktitle>In Proceedings of the international conference on Web</booktitle>
<marker>Jindal, Liu, 2008</marker>
<rawString>Nitin Jindal, and Bing Liu. Opinion spam and analysis. 2008. In Proceedings of the international conference on Web search and web data mining</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Jindal</author>
<author>Bing Liu</author>
<author>Ee-Peng Lim</author>
</authors>
<title>Finding Unusual Review Patterns Using Unexpected Rules.</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th ACM international conference on Information and knowledge management</booktitle>
<contexts>
<context position="3122" citStr="Jindal et al., 2010" startWordPosition="511" endWordPosition="514">eakfast buffet on both mornings and thought it was pretty good. The eggs were a little runny. Our six year old ate free and our two eleven year old were $14 (instead of the adult $20). The rooms were clean, the concierge and reception staff were both friendly and helpful...we will definitely visit this Sheraton again when we stay in Chicago next time. Because of the difficulty of recognizing deceptive opinions, there has been a widespread and growing interest in developing automatic, usually learningbased methods to help users identify deceptive reviews (Ott et al., 2011; Jindal et al., 2008; Jindal et al., 2010; Li et al., 2011; Lim et al., 2011; Wang et al., 2011). The state-of-the-art approach treats the task of spam detection as a text categorization problem and was first introduced by Jindal and Liu (2009) who trained a supervised classifier to distinguish duplicated reviews (assumed deceptive) from original ones (assumed truthful). Since then, many supervised approaches have been proposed for spam detection. Ott et al. (2011) employed standard word and part-of-speech (POS) n-gram features for supervised learning and built a gold − standard opinion dataset of 800 reviews. Lim et al. (2010) propo</context>
</contexts>
<marker>Jindal, Liu, Lim, 2010</marker>
<rawString>Nitin Jindal, Bing Liu, and Ee-Peng Lim. Finding Unusual Review Patterns Using Unexpected Rules. 2010. In Proceedings of the 19th ACM international conference on Information and knowledge management</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pranam Kolari</author>
<author>Akshay Java</author>
<author>Tim Finin</author>
<author>Tim Oates</author>
<author>Anupam Joshi</author>
</authors>
<title>Detecting Spam Blogs: A Machine Learning Approach.</title>
<date>2006</date>
<booktitle>In Proceedings of Association for the Advancement of Artificial Intelligence.</booktitle>
<marker>Kolari, Java, Finin, Oates, Joshi, 2006</marker>
<rawString>Pranam Kolari, Akshay Java, Tim Finin, Tim Oates and Anupam Joshi. Detecting Spam Blogs: A Machine Learning Approach. In Proceedings of Association for the Advancement of Artificial Intelligence. 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peng Li</author>
<author>Jing Jiang</author>
<author>Yinglin Wang</author>
</authors>
<title>Generating templates of entity summaries with an entityaspect model and pattern mining.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5074" citStr="Li et al., 2010" startWordPosition="811" endWordPosition="814">ore, identifying features that provide direct evidence against deceptive reviews is always a hard problem. LDA topic models (Blei et al., 2003) have widely been used for their ability to model latent topics in document collection. In LDA, each document is presented as a mixture distribution of topics and each topic is presented as a mixture distribution of words. Researchers also integrated different levels of information into LDA topic models to model the specific knowledge that they are interested in, such as user-specific information (Rosen-zvi et al., 2006), document-specific information (Li et al., 2010) and time-specific information (Diao et al., 2012). Ramage et al. (2009) developed a Labeled LDA model to define a oneto-one correspondence between LDA latent topics and tags. Chemudugunta et al. (2008) illustrated that by considering background information and document-specific information, we can largely improve the performance of topic modeling. In this paper, we propose a Bayesian approach called TopicSpam for deceptive review detection. Our approach, which is a variation of Latent Dirichlet Allocation (LDA) (Blei et al., 2003), aims to detect the subtle differences between the topic-word </context>
</contexts>
<marker>Li, Jiang, Wang, 2010</marker>
<rawString>Peng Li, Jing Jiang and Yinglin Wang. 2010. Generating templates of entity summaries with an entityaspect model and pattern mining. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fangtao Li</author>
<author>Minlie Huang</author>
<author>Yi Yang</author>
<author>Xiaoyan Zhu</author>
</authors>
<title>Learning to identify review Spam.</title>
<date>2011</date>
<booktitle>In Proceedings of the Twenty-Second international joint conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="3139" citStr="Li et al., 2011" startWordPosition="515" endWordPosition="518">h mornings and thought it was pretty good. The eggs were a little runny. Our six year old ate free and our two eleven year old were $14 (instead of the adult $20). The rooms were clean, the concierge and reception staff were both friendly and helpful...we will definitely visit this Sheraton again when we stay in Chicago next time. Because of the difficulty of recognizing deceptive opinions, there has been a widespread and growing interest in developing automatic, usually learningbased methods to help users identify deceptive reviews (Ott et al., 2011; Jindal et al., 2008; Jindal et al., 2010; Li et al., 2011; Lim et al., 2011; Wang et al., 2011). The state-of-the-art approach treats the task of spam detection as a text categorization problem and was first introduced by Jindal and Liu (2009) who trained a supervised classifier to distinguish duplicated reviews (assumed deceptive) from original ones (assumed truthful). Since then, many supervised approaches have been proposed for spam detection. Ott et al. (2011) employed standard word and part-of-speech (POS) n-gram features for supervised learning and built a gold − standard opinion dataset of 800 reviews. Lim et al. (2010) proposed the inclusion</context>
</contexts>
<marker>Li, Huang, Yang, Zhu, 2011</marker>
<rawString>Fangtao Li, Minlie Huang, Yi Yang, and Xiaoyan Zhu. Learning to identify review Spam. 2011. In Proceedings of the Twenty-Second international joint conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ee-Peng Lim</author>
<author>Viet-An Nguyen</author>
<author>Nitin Jindal</author>
<author>Bing Liu</author>
<author>Hady Wirawan Lauw</author>
</authors>
<title>Detecting Product Review Spammers Using Rating Behavior.</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th ACM international conference on Information and knowledge management.</booktitle>
<contexts>
<context position="3716" citStr="Lim et al. (2010)" startWordPosition="605" endWordPosition="608">08; Jindal et al., 2010; Li et al., 2011; Lim et al., 2011; Wang et al., 2011). The state-of-the-art approach treats the task of spam detection as a text categorization problem and was first introduced by Jindal and Liu (2009) who trained a supervised classifier to distinguish duplicated reviews (assumed deceptive) from original ones (assumed truthful). Since then, many supervised approaches have been proposed for spam detection. Ott et al. (2011) employed standard word and part-of-speech (POS) n-gram features for supervised learning and built a gold − standard opinion dataset of 800 reviews. Lim et al. (2010) proposed the inclusion of user behaviorbased features and found that behavior abnormalities of reviewers could predict spammers, without using any textual features. Li et al. (2011) carefully explored review-related features based on content and sentiment, training a semi-supervised classifier for opinion spam detection. However, the disadvantages of standard supervised learning methods are obvious. First, they do not generally provide readers with a clear probabilistic pre217 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 217–221, Sofia, Bulgar</context>
</contexts>
<marker>Lim, Nguyen, Jindal, Liu, Lauw, 2010</marker>
<rawString>Ee-Peng Lim, Viet-An Nguyen, Nitin Jindal, Bing Liu, and Hady Wirawan Lauw. Detecting Product Review Spammers Using Rating Behavior. 2010. In Proceedings of the 19th ACM international conference on Information and knowledge management.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Litvina</author>
<author>Ronald Goldsmithb</author>
<author>Bing Pana</author>
</authors>
<title>Electronic word-of-mouth in hospitality and tourism management. Tourism management,</title>
<date>2008</date>
<pages>29--3</pages>
<marker>Litvina, Goldsmithb, Pana, 2008</marker>
<rawString>Stephen Litvina, Ronald Goldsmithb and Bing Pana. 2008. Electronic word-of-mouth in hospitality and tourism management. Tourism management, 29(3):458468.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juan Martinez-Romo</author>
<author>Lourdes Araujo</author>
</authors>
<title>Web Spam Identification Through Language Model Analysis. In AIRWeb.</title>
<date>2009</date>
<marker>Martinez-Romo, Araujo, 2009</marker>
<rawString>Juan Martinez-Romo and Lourdes Araujo. Web Spam Identification Through Language Model Analysis. In AIRWeb. 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arjun Mukherjee</author>
<author>Bing Liu</author>
<author>Natalie Glance</author>
</authors>
<title>Spotting Fake Reviewer Groups in Consumer Reviews.</title>
<date>2012</date>
<booktitle>In Proceedings of the 18th international conference on World wide web,</booktitle>
<marker>Mukherjee, Liu, Glance, 2012</marker>
<rawString>Arjun Mukherjee, Bing Liu and Natalie Glance. Spotting Fake Reviewer Groups in Consumer Reviews. In Proceedings of the 18th international conference on World wide web, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandros Ntoulas</author>
<author>Marc Najork</author>
<author>Mark Manasse</author>
<author>Dennis Fetterly</author>
</authors>
<title>Detecting Spam Web Pages through Content Analysis.</title>
<date>2006</date>
<booktitle>In Proceedings of international conference on World Wide Web</booktitle>
<marker>Ntoulas, Najork, Manasse, Fetterly, 2006</marker>
<rawString>Alexandros Ntoulas, Marc Najork, Mark Manasse and Dennis Fetterly. Detecting Spam Web Pages through Content Analysis. In Proceedings of international conference on World Wide Web 2006</rawString>
</citation>
<citation valid="true">
<authors>
<author>Myle Ott</author>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
<author>Jeffrey Hancock</author>
</authors>
<title>Finding deceptive opinion spam by any stretch of the imagination.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</booktitle>
<contexts>
<context position="1194" citStr="Ott et al., 2011" startWordPosition="176" endWordPosition="179"> a result, the task of deceptive review detection has been gaining increasing attention. In this paper, we propose a generative LDA-based topic modeling approach for fake review detection. Our model can aptly detect the subtle differences between deceptive reviews and truthful ones and achieves about 95% accuracy on review spam datasets, outperforming existing baselines by a large margin. 1 Introduction Consumers rely increasingly on user-generated online reviews to make purchase decisions. Positive opinions can result in significant financial gains. This gives rise to deceptive opinion spam (Ott et al., 2011; Jindal et al., 2008), fake reviews written to sound authentic and deliberately mislead readers. Previous research has shown that humans have difficulty distinguishing fake from truthful reviews, operating for the most part at chance (Ott et al., 2011). Consider, for example, the following two hotel reviews. One is truthful and the other is deceptive1: 1. My husband and I stayed for two nights at the Hilton Chicago. We were very pleased with the accommodations and enjoyed the service every minute of it! The bedrooms are immaculate, and the linens are very soft. We also appreciated the free wi</context>
<context position="3080" citStr="Ott et al., 2011" startWordPosition="503" endWordPosition="506">bbed all around the area. We ate the breakfast buffet on both mornings and thought it was pretty good. The eggs were a little runny. Our six year old ate free and our two eleven year old were $14 (instead of the adult $20). The rooms were clean, the concierge and reception staff were both friendly and helpful...we will definitely visit this Sheraton again when we stay in Chicago next time. Because of the difficulty of recognizing deceptive opinions, there has been a widespread and growing interest in developing automatic, usually learningbased methods to help users identify deceptive reviews (Ott et al., 2011; Jindal et al., 2008; Jindal et al., 2010; Li et al., 2011; Lim et al., 2011; Wang et al., 2011). The state-of-the-art approach treats the task of spam detection as a text categorization problem and was first introduced by Jindal and Liu (2009) who trained a supervised classifier to distinguish duplicated reviews (assumed deceptive) from original ones (assumed truthful). Since then, many supervised approaches have been proposed for spam detection. Ott et al. (2011) employed standard word and part-of-speech (POS) n-gram features for supervised learning and built a gold − standard opinion datas</context>
</contexts>
<marker>Ott, Choi, Cardie, Hancock, 2011</marker>
<rawString>Myle Ott, Yejin Choi, Claire Cardie and Jeffrey Hancock. Finding deceptive opinion spam by any stretch of the imagination. 2011. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</rawString>
</citation>
<citation valid="false">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis.</title>
<booktitle>In Found. Trends Inf. Retr.</booktitle>
<marker>Pang, Lee, </marker>
<rawString>Bo Pang and Lillian Lee. Opinion mining and sentiment analysis. In Found. Trends Inf. Retr.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Ramage</author>
<author>David Hall</author>
<author>Ramesh Nallapati</author>
<author>Christopher D Manning</author>
</authors>
<title>Labeled LDA: a supervised topic model for credit attribution in multilabeled corpora.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</booktitle>
<contexts>
<context position="5146" citStr="Ramage et al. (2009)" startWordPosition="823" endWordPosition="826">tive reviews is always a hard problem. LDA topic models (Blei et al., 2003) have widely been used for their ability to model latent topics in document collection. In LDA, each document is presented as a mixture distribution of topics and each topic is presented as a mixture distribution of words. Researchers also integrated different levels of information into LDA topic models to model the specific knowledge that they are interested in, such as user-specific information (Rosen-zvi et al., 2006), document-specific information (Li et al., 2010) and time-specific information (Diao et al., 2012). Ramage et al. (2009) developed a Labeled LDA model to define a oneto-one correspondence between LDA latent topics and tags. Chemudugunta et al. (2008) illustrated that by considering background information and document-specific information, we can largely improve the performance of topic modeling. In this paper, we propose a Bayesian approach called TopicSpam for deceptive review detection. Our approach, which is a variation of Latent Dirichlet Allocation (LDA) (Blei et al., 2003), aims to detect the subtle differences between the topic-word distributions of deceptive reviews vs. truthful ones. In addition, our m</context>
</contexts>
<marker>Ramage, Hall, Nallapati, Manning, 2009</marker>
<rawString>Daniel Ramage, David Hall, Ramesh Nallapati and Christopher D. Manning. Labeled LDA: a supervised topic model for credit attribution in multilabeled corpora. 2009. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing 2009.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Michal Rosen-zvi</author>
<author>Thomas Griffith</author>
</authors>
<title>Mark Steyvers and Padhraic Smyth. The author-topic model for authors and documents.</title>
<booktitle>In Proceedings of the 20th conference on Uncertainty in artificial intelligence.</booktitle>
<marker>Rosen-zvi, Griffith, </marker>
<rawString>Michal Rosen-zvi, Thomas Griffith, Mark Steyvers and Padhraic Smyth. The author-topic model for authors and documents. In Proceedings of the 20th conference on Uncertainty in artificial intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guan Wang</author>
<author>Sihong Xie</author>
<author>Bing Liu</author>
<author>Philip Yu</author>
</authors>
<title>Review Graph based Online Store Review Spammer Detection.</title>
<date>2011</date>
<booktitle>In Proceedings of 11th International Conference of Data Mining.</booktitle>
<contexts>
<context position="3177" citStr="Wang et al., 2011" startWordPosition="523" endWordPosition="526">y good. The eggs were a little runny. Our six year old ate free and our two eleven year old were $14 (instead of the adult $20). The rooms were clean, the concierge and reception staff were both friendly and helpful...we will definitely visit this Sheraton again when we stay in Chicago next time. Because of the difficulty of recognizing deceptive opinions, there has been a widespread and growing interest in developing automatic, usually learningbased methods to help users identify deceptive reviews (Ott et al., 2011; Jindal et al., 2008; Jindal et al., 2010; Li et al., 2011; Lim et al., 2011; Wang et al., 2011). The state-of-the-art approach treats the task of spam detection as a text categorization problem and was first introduced by Jindal and Liu (2009) who trained a supervised classifier to distinguish duplicated reviews (assumed deceptive) from original ones (assumed truthful). Since then, many supervised approaches have been proposed for spam detection. Ott et al. (2011) employed standard word and part-of-speech (POS) n-gram features for supervised learning and built a gold − standard opinion dataset of 800 reviews. Lim et al. (2010) proposed the inclusion of user behaviorbased features and fo</context>
</contexts>
<marker>Wang, Xie, Liu, Yu, 2011</marker>
<rawString>Guan Wang, Sihong Xie, Bing Liu and Philip Yu. Review Graph based Online Store Review Spammer Detection. 2011. In Proceedings of 11th International Conference of Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Baoning Wu</author>
<author>Vinay Goel</author>
<author>Brian Davison</author>
</authors>
<title>Topical TrustRank: using topicality to combat Web spam.</title>
<date>2006</date>
<booktitle>In Proceedings of international conference on World Wide Web</booktitle>
<pages>.</pages>
<marker>Wu, Goel, Davison, 2006</marker>
<rawString>Baoning Wu, Vinay Goel and Brian Davison. Topical TrustRank: using topicality to combat Web spam. In Proceedings of international conference on World Wide Web 2006 .</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kyang Yoo</author>
<author>Ulrike Gretzel</author>
</authors>
<title>Comparison of Deceptive and Truthful Travel Reviews. InInformation and Communication Technologies in Tourism</title>
<date>2009</date>
<marker>Yoo, Gretzel, 2009</marker>
<rawString>Kyang Yoo and Ulrike Gretzel. 2009. Comparison of Deceptive and Truthful Travel Reviews. InInformation and Communication Technologies in Tourism 2009.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>