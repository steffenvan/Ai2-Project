<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.033960">
<title confidence="0.754197">
UKP-WSI: UKP Lab Semeval-2013 Task 11 System Description
</title>
<author confidence="0.66273">
Hans-Peter Zorn† and Iryna Gurevych†t
</author>
<affiliation confidence="0.80056775">
†Ubiquitous Knowledge Processing Lab (UKP-TUDA)
Department of Computer Science, Technische Universit¨at Darmstadt
$Ubiquitous Knowledge Processing Lab (UKP-DIPF)
German Institute for Educational Research and Educational Information
</affiliation>
<email confidence="0.902455">
www.ukp.tu-darmstadt.de
</email>
<sectionHeader confidence="0.994574" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998771818181818">
In this paper, we describe the UKP Lab sys-
tem participating in the Semeval-2013 task
“Word Sense Induction and Disambiguation
within an End-User Application”. Our ap-
proach uses preprocessing, co-occurrence ex-
traction, graph clustering, and a state-of-the-
art word sense disambiguation system. We
developed a configurable pipeline which can
be used to integrate and evaluate other com-
ponents for the various steps of the complex
task.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999742117647059">
The task “Evaluating Word Sense Induction and
Word Sense Disambiguation in an End-User Ap-
plication” of SemEval-2013 (Navigli and Vannella,
2013) aims at an extrinsic evaluation scheme for
WSI to overcome the difficulties inherent to WSI
evaluation. The task requires building a WSI sys-
tem and combining it with a WSD step to assign the
induced sentences to example instances.
Word sense disambiguation (WSD) is the task
of determining the correct meaning for an ambigu-
ous word from its context. WSD algorithms usu-
ally choose one sense out of a given set of possible
senses for each word. A resource that enumerates
possible senses for each word is called a sense in-
ventory. Manually created inventories come usually
in form of lexical semantic resources, such as Word-
Net or more specifically created inventories such as
OntoNotes (Hovy et al., 2006).
Word sense induction (WSI) on the other hand
aims to create such an inventory from a corpus in
an unsupervised manner. For each word that should
be disambiguated, a WSI algorithm creates a set of
context clusters that will be used to define and de-
scribe the senses.
We build our system upon the open-source DKPro
framework 1 and a corresponding WSD component
(upcoming).
Input for the task comes as two files. One contains
the search queries, also referred as topics. Sense in-
duction will be performed for each of those topics.
The second file contains 6400 entries from the re-
sult pages of a search engine. Each entry consists of
the title, a snippet and the URL of the corresponding
web page.
</bodyText>
<sectionHeader confidence="0.999694" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9997335625">
One of the early approaches to WSI (Sch¨utze, 1998)
maps words into a vector space and represents
word contexts as vector-sums and use cosine vec-
tor similarity, clustering is performed by expectation
maximization (EM) clustering. Dorow and Wid-
dows (2003) use the BNC to build a co-occurrence
graph for nouns, based on a co-occurrence fre-
quency threshold. They perform Markov cluster-
ing on this graph. Pantel and Lin (2002) proposes
a clustering approach called clustering by commit-
tee (CBC). This algorithm first selects the words
with the highest similarity based on mutual infor-
mation and then builds groups of highly connected
words called committees. It then iteratively assigns
the remaining words to one of the committee clus-
ters by comparing them to the averaged the com-
</bodyText>
<footnote confidence="0.992818">
1http://code.google.com/p/dkpro-core-asl/
</footnote>
<page confidence="0.96761">
212
</page>
<bodyText confidence="0.991958708333334">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 212–216, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics
mittee feature vectors. This exploits the assumption
that two or more words together disambiguate each
other, Bordag (2006) extends on this idea by using
word triples to form non-ambiguous seed-clusters.
Many approaches use a variety of graph clustering
algorithms for WSI: Others (Klapaftis and Manand-
har, 2010) use hierarchical agglomerative clustering
on hierarchical random graphs created from word
co-occurrences. Di Marco and Navigli (2013) use
word sense induction for web search result cluster-
ing. They introduce a maximum spanning tree al-
gorithm that operates on co-occurrence graphs built
from large corpora, such as ukWaC (Baroni et al.,
2009). The system by Pedersen (2010) employs
clustering first- and second-order co-occurrences as
well as singular value decomposition on the co-
occurrence matrix, which is clustered using repeated
bisections. Jurgens (2011) employ a graph-based
community detection algorithm on a co-occurrence
graph. Distributional approaches for WSI include
LSA Apidianaki and Van de Cruys (2011) or LDA
(Brody and Lapata, 2009).
</bodyText>
<sectionHeader confidence="0.976405" genericHeader="method">
3 Our Approach
</sectionHeader>
<bodyText confidence="0.957305653846154">
Our system consists of two independent parts. The
first is a batch process that creates database con-
taining co-occurrence statistics derived from a back-
ground corpus. The second is the actual WSI and
WSD pipeline doing the result clustering. Both parts
include identical preprocessing steps for segmenta-
tion and lemmatization.
The pipeline (Figure 1) first performs Word Sense
Induction, resulting in an induced sense inventory.
A WSD algorithm then uses this inventory to dis-
ambiguate all instances of the search query within a
web-page. A majority voting finally assigns a sense
to each result-snippet.
The sense induction algorithm is based on graph
clustering on a co-occurrence graph, similar to the
approach by Di Marco and Navigli (2013). Our ap-
proach differs from previous work in the way we
perform a greedy search for additional context and
how it combines WSI with an advanced WSD step
using lexical expansions. Moreover, we consider
our generic UIMA-based WSD and WSI system as
a useful basis for experimentation and evaluation of
WSI systems.
# words # co-occurrences
Wikipedia 3,011,397 96,979,920
ukWaC 8,687,711 441,005,478
</bodyText>
<tableCaption confidence="0.995623">
Table 1: Size of co-occurrence databases
</tableCaption>
<subsectionHeader confidence="0.997984">
3.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.999965428571429">
The pipeline first reads topics and snippets. If the
web-page can be downloaded at the URL that cor-
responds to the result, it is cleaned by an HTML
parser and the plain text is appended to the snippet.
As further steps we segment and lemmatize the in-
put. We apply the same preprocessing to snippets,
queries and the corpora.
</bodyText>
<subsectionHeader confidence="0.997964">
3.2 Co-occurrence Extraction
</subsectionHeader>
<bodyText confidence="0.999759882352941">
We calculate the log-likelihood ratio (LLR) (Dun-
ning, 1993) and point-wise mutual information
(PMI) (Church and Hanks, 1990) of a word pair co-
occurring at sentence level using a modified version
of the collocation statistics implemented in Apache
Mahout 2. Even when sorting the co-occurrences by
PMI, we employ a minimum support cut-off based
on the LLR, which is based on significance. All
pairs with a log-likelihood ratio &lt; 1 are discarded.
This value is lower than the significance level of ˜3.8
we found in the literature, but because in the ex-
pand step (see algorithm 2) we require more than
two words to co-occur with the target word, we used
a lower value. We use the English Wikipedia 3 and
ukWaC (Baroni et al., 2009) as background corpus.
Table 1 gives an overview about the obtained co-
occurrence pairs.
</bodyText>
<subsectionHeader confidence="0.999768">
3.3 Clustering Algorithm
</subsectionHeader>
<bodyText confidence="0.99999325">
The algorithm is a two-step approach that first cre-
ates an initial clustering of a graph G = (V, E)
and then improves this clustering in a second step.
The initial step (Algorithm 1) starts by retrieving the
top n = 150 most similar terms for the target word
by querying the co-occurrence database we created
in section 3.2. These represent vertices in a graph.
We then construct 4 a minimum spanning tree (mst)
</bodyText>
<footnote confidence="0.99707275">
2http://mahout.apache.org
3Dump from April 2011
4For all of our graph operations, we employ the igraph li-
brary for R, http://igraph.sf.net
</footnote>
<page confidence="0.997236">
213
</page>
<figureCaption confidence="0.99992">
Figure 1: WSI and WSD Pipeline
</figureCaption>
<bodyText confidence="0.998775777777778">
by inserting edges {vi, vj} from the co-occurrence
database. The weight w({vi, vj}) of each edge is
set to the inverse of the used similarity measure
dist (LLR or PMI) between those terms. The min-
imum spanning tree then is cut into subtrees be it-
eratively removing the edge with the highest edge-
betweenness (Freeman, 1977) (betweeness) until
the size of the largest component of G falls below
a threshold Sinitial.
</bodyText>
<equation confidence="0.953065142857143">
Algorithm 1 initialClusters
V (Go) top n most similar words to target word
w(vi, vj) dist(termi, termj)
G mst(Go)
V (G) V (G) \ vtarget
while max(|C(G)|) &gt; Sintitial do
E(G) E(G) \ argmaxe(betweeness(e))
</equation>
<bodyText confidence="0.973999523809524">
end while
The resulting partitioning of the graph is the start-
ing point for the second phase of the algorithm,
which we call expand/join step (Algorithm 2). Dur-
ing this step, the algorithm looks iteratively at all
clusters Csmall of size s smaller than Smax = 9 (de-
termined empirically), starting with the largest ones.
From each of these clusters, it constructs a query
to the co-occurrence database, retrieving all terms
that significantly co-occur together with all terms in
the respective cluster (querys) and with the target
word (E). This list of terms is then compared to
all clusters Clarge with |C |&gt; s . If the normalized
intersection between one of those Clarge is above a
threshold t = 0.3 (determined empirically), we as-
sume that the Csmall represents the same sense as
the Clarge and merge those clusters. If this is not the
case for any of the larger clusters, we assume that
Csmall represents a sense of its own extend the clus-
ter by adding edges between vertices representing
the expansion terms and Csmall.
</bodyText>
<figure confidence="0.977951">
Algorithm 2 expandJoin
Require: G is a minimum spanning forest
for s = Smax —* 1 do
for all Csmall(G), |Csmall |= s do
E querys(v1, .., vi)
for all Clarge E G, |Clarge |&gt; s do
if |Clarge n E|/|Clarge |&gt; t then
Clarge Clarge U Csmall
else
Csmall Csmall U E
</figure>
<listItem confidence="0.91165275">
end if
end for
end for
end for
</listItem>
<subsectionHeader confidence="0.976675">
3.4 Word Sense Disambiguation
</subsectionHeader>
<bodyText confidence="0.999842294117647">
We use the DKPro WSD framework, which imple-
ments various WSD algorithms, with the same sys-
tem configuration as reported by Miller et al. (2012).
It uses a variant of the Simplified Lesk Algorithm
(Kilgarriff et al., 2000). This algorithm measures
the overlap between a words context and the tex-
tual descriptions of senses within a machine read-
able dictionary, such as WordNet. The senses that
have been induced in the previous step are provided
to the framework as a sense inventory. Instead of
using sense descriptions, we now compute the over-
lap between the sense clusters and the context of
the target word. The WSD system expands both
the word context and the sense clusters with syn-
onyms from a distributional thesaurus (DT), similar
to Lin (1998). The DT has been created from 10M
dependency-parsed sentences of English newswire
</bodyText>
<page confidence="0.997083">
214
</page>
<table confidence="0.99782375">
Run Fl ARI RI JI # clusters avg cl. size
wacky-llr 0.5826 0.0253 0.5002 0.3394 3.6400 32.3434
wp-llr 0.5864 0.0377 0.5109 0.3177 4.1700 21.8702
wp-pmi 0.6048 0.0364 0.5050 0.2932 5.8600 30.3098
</table>
<tableCaption confidence="0.995737">
Table 2: Results for the submitted runs
</tableCaption>
<bodyText confidence="0.999178428571429">
from the Leipzig Corpora Collection (Biemann et
al., 2007) for word similarity5. Besides knowledge-
based WSD, the DT also has been successfully used
for improving the performance of semantic text sim-
ilarity (B¨ar et al., 2012). The WSD component dis-
ambiguates each instance of the search query within
the snippet and web page individually.
</bodyText>
<sectionHeader confidence="0.999964" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999997620689655">
The clustering was evaluated using four different
metrics as described by Di Marco and Navigli
(2013). The Rand index and its chance-adjusted
variant ARI are common cluster evaluation metrics.
The adjusted rand index gives special weight to less
frequent senses. The Jaccard index (JI) disregards
the cases where two results are assigned to differ-
ent clusters in the gold standard, therefore it is less
sensitive to the granularity of the clustering. The
Fl-Measure gives more attention to the individual
clusters and how they cover the topics in the gold
standard.
We submitted several runs for different config-
urations of the co-occurrence extraction (Table 2).
Between runs, we did not modify the configuration
of the sense induction or disambiguation step. The
first run used collocations extracted from ukWaC
scored by LLR metric (wacky-llr), and two others
used Wikipedia as background corpus. One of the
WP-based runs used PMI as association metric (wp-
pmi), the other one used LLR (wp-llr). The run on
the larger ukWaC corpus scored best with regard to
the Jaccard measure, but worst in the adjusted Rand
index measure. We attribute low scores for ARI to
the fact that our system did not induce certain less
frequent senses, resulting in small average number
of clusters. The coarse grained clusters however,
have been assigned quite well by our WSD system,
as shown by relatively high Jaccard Index. For the
</bodyText>
<footnote confidence="0.9984505">
5The software used to create the DT is available from
http://www.jobimtext.org
</footnote>
<bodyText confidence="0.998993875">
WP-based runs, the clustering based on PMI pro-
duced more clusters and therefore scored higher on
the Fl measure than the LLR-based run. From an
exploratory analysis of the created clusters, we as-
sume that the WP-based runs have a higher chance
to find more rare senses in this specific task, since
the gold standard was also based on Wikipedia dis-
ambiguation pages.
</bodyText>
<sectionHeader confidence="0.996543" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999982409090909">
We presented our word sense induction and dis-
ambiguation pipeline for search result clustering.
Our contribution is a sense induction algorithm that
incrementally retrieves more context from a co-
occurrence database and the integration of WSI and
WSD into a UIMA-based pipeline for easy experi-
mentation. The system scored best with regard to
Jaccard similarity of clusters, while performing low
especially with the adjusted rand index. We assume
that our sense granularity was too low for this task
and failed to create clusters for rare senses. This
could be improved by making the merge phase of
the induction algorithm less eager. Furthermore,
increasing the size of the background corpus, e.g.
by combining the both corpora that have been used
could increase the size of the context clusters espe-
cially for rare senses, which should further improve
the performance in these cases. We attribute the
good results with regard to the Fl and Jaccard mea-
sures also to our state-of-the-art word sense disam-
biguation step and the use of the distributional the-
saurus.
</bodyText>
<sectionHeader confidence="0.995683" genericHeader="acknowledgments">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.9998295">
We thank Tristan Miller for helping us with the
DKPro WSD framework and Chris Biemann for
providing the distributional thesaurus. This work
has been supported by the Volkswagen Foundation
as part of the Lichtenberg-Professorship Program
under grant No. I/82806.
</bodyText>
<page confidence="0.998495">
215
</page>
<sectionHeader confidence="0.990157" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99983791">
Marianna Apidianaki and Tim Van de Cruys. 2011. La-
tent Semantic Word Sense Induction and Disambigua-
tion. In ACL HLT 2011, pages 1476–1485, June.
Daniel B¨ar, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. UKP: Computing Semantic Textual
Similarity by Combining Multiple Content Similarity
Measures. In Proceedings of First Joint Conference on
Lexical and Computational Semantics (*SEM), pages
435–440.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The WaCky wide web: a
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209–226, February.
Chris Biemann, Gerhard Heyer, Uwe Quasthoff, and
Matthias Richter. 2007. The Leipzig Corpora Collec-
tion - Monolingual corpora of standard size. In Pro-
ceedings of Corpus Linguistic 2007, Birmingham, UK.
Stefan Bordag. 2006. Word Sense Induction: Triplet-
Based Clustering and Automatic Evaluation. In Pro-
ceedings of the 11th Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 137–144, Trento, Italy.
Samuel Brody and Mirella Lapata. 2009. Bayesian word
sense induction. In Proceedings of the 12th Con-
ference of the European Chapter of the Association
for Computational Linguistics, EACL ’09, pages 103–
111, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicogra-
phy. Computational Linguistics, 16(1):22–29, March.
Antonio Di Marco and Roberto Navigli. 2013. Cluster-
ing and Diversifying Web Search Results with Graph-
Based Word Sense Induction. Computational Linguis-
tics, 39(4):1–46, November.
Beate Dorow and Dominic Widdows. 2003. Discover-
ing corpus-specific word senses. In Proceedings of the
Tenth Conference on European Chapter of the Associ-
ation for Computational Linguistics - EACL ’03, vol-
ume 2, page 79, Morristown, NJ, USA, April. Associ-
ation for Computational Linguistics.
Ted Dunning. 1993. Accurate Methods for the Statistics
of Surprise and Coincidence. Computational Linguis-
tics, 19(1):61 – 74.
Linton C Freeman. 1977. A set of measures of centrality
based on betweenness. Sociometry, 40(1):35–41.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. OntoNotes:
the 90% solution. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL, Com-
panion Volume: Short Papers, NAACL-Short ’06,
pages 57–60, Stroudsburg, PA, USA. Association for
Computational Linguistics.
David Jurgens. 2011. Word Sense Induction by Commu-
nity Detection. In HLT ’11: Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics Human Language Technologies, pages 24–
28, Portland, Oregon.
Adam Kilgarriff, Brighton England, and Joseph Rosen-
zweig. 2000. English Senseval: Report and Results.
In Proceedings of the 2nd International Conference on
Language Resources and Evaluation, Athens, Greece.
Ioannis P. Klapaftis and Suresh Manandhar. 2010. Word
sense induction &amp; disambiguation using hierarchical
random graphs. In Proceedings of the 2010 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 745–755, Cambridge, Massachusetts,
October. Association for Computational Linguistics.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th annual
meeting on Association for Computational Linguistics,
pages 768–774, Morristown, NJ, USA, August. Asso-
ciation for Computational Linguistics.
Tristan Miller, Chris Biemann, Torsten Zesch, and Iryna
Gurevych. 2012. Using Distributional Similarity for
Lexical Expansion in Knowledge-based Word Sense
Disambiguation. In Proceedings of the 24th In-
ternational Conference on Computational Linguistics
(COLING 2012).
Roberto Navigli and Daniele Vannella. 2013. SemEval-
2013 Task 11: Evaluating Word Sense Induction &amp;
Disambiguation within An End-User Application. In
Proceedings of the 7th International Workshop on Se-
mantic Evaluation (SemEval 2013), in conjunction
with the Second Joint Conference on Lexical and Com-
putational Semantcis (*SEM 2013), Atlanta, USA.
Patrick Pantel and Dekang Lin. 2002. Discovering word
senses from text. In Proceedings of the eighth ACM
SIGKDD international conference on Knowledge dis-
covery and data mining - KDD ’02, page 613, New
York, New York, USA, July. ACM Press.
Ted Pedersen. 2010. Duluth-WSI: SenseClusters applied
to the sense induction task of SemEval-2. In Proceed-
ings of the 5th International Workshop on Semantic
Evaluation, pages 363–366, Stroudsburg, PA, USA,
July. Association for Computational Linguistics.
Hinrich Sch¨utze. 1998. Automatic word sense discrim-
ination. Computational Linguistics, 24(1):97–123,
March.
</reference>
<page confidence="0.999144">
216
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.395774">
<title confidence="0.760485">UKP-WSI: UKP Lab Semeval-2013 Task 11 System Description Iryna</title>
<author confidence="0.509071">Knowledge Processing Lab</author>
<affiliation confidence="0.997113">Department of Computer Science, Technische Universit¨at Knowledge Processing Lab German Institute for Educational Research and Educational</affiliation>
<email confidence="0.982861">www.ukp.tu-darmstadt.de</email>
<abstract confidence="0.993270333333333">In this paper, we describe the UKP Lab system participating in the Semeval-2013 task “Word Sense Induction and Disambiguation within an End-User Application”. Our approach uses preprocessing, co-occurrence extraction, graph clustering, and a state-of-theart word sense disambiguation system. We developed a configurable pipeline which can be used to integrate and evaluate other components for the various steps of the complex task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marianna Apidianaki</author>
<author>Tim Van de Cruys</author>
</authors>
<title>Latent Semantic Word Sense Induction and Disambiguation.</title>
<date>2011</date>
<booktitle>In ACL HLT 2011,</booktitle>
<pages>1476--1485</pages>
<marker>Apidianaki, Van de Cruys, 2011</marker>
<rawString>Marianna Apidianaki and Tim Van de Cruys. 2011. Latent Semantic Word Sense Induction and Disambiguation. In ACL HLT 2011, pages 1476–1485, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel B¨ar</author>
<author>Chris Biemann</author>
<author>Iryna Gurevych</author>
<author>Torsten Zesch</author>
</authors>
<title>UKP: Computing Semantic Textual Similarity by Combining Multiple Content Similarity Measures.</title>
<date>2012</date>
<booktitle>In Proceedings of First Joint Conference on Lexical and Computational Semantics (*SEM),</booktitle>
<pages>435--440</pages>
<marker>B¨ar, Biemann, Gurevych, Zesch, 2012</marker>
<rawString>Daniel B¨ar, Chris Biemann, Iryna Gurevych, and Torsten Zesch. 2012. UKP: Computing Semantic Textual Similarity by Combining Multiple Content Similarity Measures. In Proceedings of First Joint Conference on Lexical and Computational Semantics (*SEM), pages 435–440.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
<author>Adriano Ferraresi</author>
<author>Eros Zanchetta</author>
</authors>
<title>The WaCky wide web: a collection of very large linguistically processed webcrawled corpora. Language Resources and Evaluation,</title>
<date>2009</date>
<volume>43</volume>
<issue>3</issue>
<contexts>
<context position="4097" citStr="Baroni et al., 2009" startWordPosition="627" endWordPosition="630">its the assumption that two or more words together disambiguate each other, Bordag (2006) extends on this idea by using word triples to form non-ambiguous seed-clusters. Many approaches use a variety of graph clustering algorithms for WSI: Others (Klapaftis and Manandhar, 2010) use hierarchical agglomerative clustering on hierarchical random graphs created from word co-occurrences. Di Marco and Navigli (2013) use word sense induction for web search result clustering. They introduce a maximum spanning tree algorithm that operates on co-occurrence graphs built from large corpora, such as ukWaC (Baroni et al., 2009). The system by Pedersen (2010) employs clustering first- and second-order co-occurrences as well as singular value decomposition on the cooccurrence matrix, which is clustered using repeated bisections. Jurgens (2011) employ a graph-based community detection algorithm on a co-occurrence graph. Distributional approaches for WSI include LSA Apidianaki and Van de Cruys (2011) or LDA (Brody and Lapata, 2009). 3 Our Approach Our system consists of two independent parts. The first is a batch process that creates database containing co-occurrence statistics derived from a background corpus. The seco</context>
<context position="6803" citStr="Baroni et al., 2009" startWordPosition="1063" endWordPosition="1066">and Hanks, 1990) of a word pair cooccurring at sentence level using a modified version of the collocation statistics implemented in Apache Mahout 2. Even when sorting the co-occurrences by PMI, we employ a minimum support cut-off based on the LLR, which is based on significance. All pairs with a log-likelihood ratio &lt; 1 are discarded. This value is lower than the significance level of ˜3.8 we found in the literature, but because in the expand step (see algorithm 2) we require more than two words to co-occur with the target word, we used a lower value. We use the English Wikipedia 3 and ukWaC (Baroni et al., 2009) as background corpus. Table 1 gives an overview about the obtained cooccurrence pairs. 3.3 Clustering Algorithm The algorithm is a two-step approach that first creates an initial clustering of a graph G = (V, E) and then improves this clustering in a second step. The initial step (Algorithm 1) starts by retrieving the top n = 150 most similar terms for the target word by querying the co-occurrence database we created in section 3.2. These represent vertices in a graph. We then construct 4 a minimum spanning tree (mst) 2http://mahout.apache.org 3Dump from April 2011 4For all of our graph opera</context>
</contexts>
<marker>Baroni, Bernardini, Ferraresi, Zanchetta, 2009</marker>
<rawString>Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The WaCky wide web: a collection of very large linguistically processed webcrawled corpora. Language Resources and Evaluation, 43(3):209–226, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Biemann</author>
<author>Gerhard Heyer</author>
<author>Uwe Quasthoff</author>
<author>Matthias Richter</author>
</authors>
<title>The Leipzig Corpora Collection - Monolingual corpora of standard size.</title>
<date>2007</date>
<booktitle>In Proceedings of Corpus Linguistic</booktitle>
<location>Birmingham, UK.</location>
<contexts>
<context position="10594" citStr="Biemann et al., 2007" startWordPosition="1716" endWordPosition="1719">criptions, we now compute the overlap between the sense clusters and the context of the target word. The WSD system expands both the word context and the sense clusters with synonyms from a distributional thesaurus (DT), similar to Lin (1998). The DT has been created from 10M dependency-parsed sentences of English newswire 214 Run Fl ARI RI JI # clusters avg cl. size wacky-llr 0.5826 0.0253 0.5002 0.3394 3.6400 32.3434 wp-llr 0.5864 0.0377 0.5109 0.3177 4.1700 21.8702 wp-pmi 0.6048 0.0364 0.5050 0.2932 5.8600 30.3098 Table 2: Results for the submitted runs from the Leipzig Corpora Collection (Biemann et al., 2007) for word similarity5. Besides knowledgebased WSD, the DT also has been successfully used for improving the performance of semantic text similarity (B¨ar et al., 2012). The WSD component disambiguates each instance of the search query within the snippet and web page individually. 4 Results The clustering was evaluated using four different metrics as described by Di Marco and Navigli (2013). The Rand index and its chance-adjusted variant ARI are common cluster evaluation metrics. The adjusted rand index gives special weight to less frequent senses. The Jaccard index (JI) disregards the cases wh</context>
</contexts>
<marker>Biemann, Heyer, Quasthoff, Richter, 2007</marker>
<rawString>Chris Biemann, Gerhard Heyer, Uwe Quasthoff, and Matthias Richter. 2007. The Leipzig Corpora Collection - Monolingual corpora of standard size. In Proceedings of Corpus Linguistic 2007, Birmingham, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Bordag</author>
</authors>
<title>Word Sense Induction: TripletBased Clustering and Automatic Evaluation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>137--144</pages>
<location>Trento, Italy.</location>
<contexts>
<context position="3566" citStr="Bordag (2006)" startWordPosition="549" endWordPosition="550">on and then builds groups of highly connected words called committees. It then iteratively assigns the remaining words to one of the committee clusters by comparing them to the averaged the com1http://code.google.com/p/dkpro-core-asl/ 212 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 212–216, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics mittee feature vectors. This exploits the assumption that two or more words together disambiguate each other, Bordag (2006) extends on this idea by using word triples to form non-ambiguous seed-clusters. Many approaches use a variety of graph clustering algorithms for WSI: Others (Klapaftis and Manandhar, 2010) use hierarchical agglomerative clustering on hierarchical random graphs created from word co-occurrences. Di Marco and Navigli (2013) use word sense induction for web search result clustering. They introduce a maximum spanning tree algorithm that operates on co-occurrence graphs built from large corpora, such as ukWaC (Baroni et al., 2009). The system by Pedersen (2010) employs clustering first- and second-</context>
</contexts>
<marker>Bordag, 2006</marker>
<rawString>Stefan Bordag. 2006. Word Sense Induction: TripletBased Clustering and Automatic Evaluation. In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics, pages 137–144, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel Brody</author>
<author>Mirella Lapata</author>
</authors>
<title>Bayesian word sense induction.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, EACL ’09,</booktitle>
<pages>103--111</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4505" citStr="Brody and Lapata, 2009" startWordPosition="685" endWordPosition="688"> (2013) use word sense induction for web search result clustering. They introduce a maximum spanning tree algorithm that operates on co-occurrence graphs built from large corpora, such as ukWaC (Baroni et al., 2009). The system by Pedersen (2010) employs clustering first- and second-order co-occurrences as well as singular value decomposition on the cooccurrence matrix, which is clustered using repeated bisections. Jurgens (2011) employ a graph-based community detection algorithm on a co-occurrence graph. Distributional approaches for WSI include LSA Apidianaki and Van de Cruys (2011) or LDA (Brody and Lapata, 2009). 3 Our Approach Our system consists of two independent parts. The first is a batch process that creates database containing co-occurrence statistics derived from a background corpus. The second is the actual WSI and WSD pipeline doing the result clustering. Both parts include identical preprocessing steps for segmentation and lemmatization. The pipeline (Figure 1) first performs Word Sense Induction, resulting in an induced sense inventory. A WSD algorithm then uses this inventory to disambiguate all instances of the search query within a web-page. A majority voting finally assigns a sense to</context>
</contexts>
<marker>Brody, Lapata, 2009</marker>
<rawString>Samuel Brody and Mirella Lapata. 2009. Bayesian word sense induction. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, EACL ’09, pages 103– 111, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Ward Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="6199" citStr="Church and Hanks, 1990" startWordPosition="955" endWordPosition="958">. # words # co-occurrences Wikipedia 3,011,397 96,979,920 ukWaC 8,687,711 441,005,478 Table 1: Size of co-occurrence databases 3.1 Preprocessing The pipeline first reads topics and snippets. If the web-page can be downloaded at the URL that corresponds to the result, it is cleaned by an HTML parser and the plain text is appended to the snippet. As further steps we segment and lemmatize the input. We apply the same preprocessing to snippets, queries and the corpora. 3.2 Co-occurrence Extraction We calculate the log-likelihood ratio (LLR) (Dunning, 1993) and point-wise mutual information (PMI) (Church and Hanks, 1990) of a word pair cooccurring at sentence level using a modified version of the collocation statistics implemented in Apache Mahout 2. Even when sorting the co-occurrences by PMI, we employ a minimum support cut-off based on the LLR, which is based on significance. All pairs with a log-likelihood ratio &lt; 1 are discarded. This value is lower than the significance level of ˜3.8 we found in the literature, but because in the expand step (see algorithm 2) we require more than two words to co-occur with the target word, we used a lower value. We use the English Wikipedia 3 and ukWaC (Baroni et al., 2</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>Kenneth Ward Church and Patrick Hanks. 1990. Word association norms, mutual information, and lexicography. Computational Linguistics, 16(1):22–29, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antonio Di Marco</author>
<author>Roberto Navigli</author>
</authors>
<title>Clustering and Diversifying Web Search Results with GraphBased Word Sense Induction.</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>4</issue>
<marker>Di Marco, Navigli, 2013</marker>
<rawString>Antonio Di Marco and Roberto Navigli. 2013. Clustering and Diversifying Web Search Results with GraphBased Word Sense Induction. Computational Linguistics, 39(4):1–46, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beate Dorow</author>
<author>Dominic Widdows</author>
</authors>
<title>Discovering corpus-specific word senses.</title>
<date>2003</date>
<booktitle>In Proceedings of the Tenth Conference on European Chapter of the Association for Computational Linguistics - EACL ’03,</booktitle>
<volume>2</volume>
<pages>79</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA,</location>
<contexts>
<context position="2623" citStr="Dorow and Widdows (2003)" startWordPosition="409" endWordPosition="413">ding WSD component (upcoming). Input for the task comes as two files. One contains the search queries, also referred as topics. Sense induction will be performed for each of those topics. The second file contains 6400 entries from the result pages of a search engine. Each entry consists of the title, a snippet and the URL of the corresponding web page. 2 Related Work One of the early approaches to WSI (Sch¨utze, 1998) maps words into a vector space and represents word contexts as vector-sums and use cosine vector similarity, clustering is performed by expectation maximization (EM) clustering. Dorow and Widdows (2003) use the BNC to build a co-occurrence graph for nouns, based on a co-occurrence frequency threshold. They perform Markov clustering on this graph. Pantel and Lin (2002) proposes a clustering approach called clustering by committee (CBC). This algorithm first selects the words with the highest similarity based on mutual information and then builds groups of highly connected words called committees. It then iteratively assigns the remaining words to one of the committee clusters by comparing them to the averaged the com1http://code.google.com/p/dkpro-core-asl/ 212 Second Joint Conference on Lexi</context>
</contexts>
<marker>Dorow, Widdows, 2003</marker>
<rawString>Beate Dorow and Dominic Widdows. 2003. Discovering corpus-specific word senses. In Proceedings of the Tenth Conference on European Chapter of the Association for Computational Linguistics - EACL ’03, volume 2, page 79, Morristown, NJ, USA, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Dunning</author>
</authors>
<title>Accurate Methods for the Statistics of Surprise and Coincidence.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="6134" citStr="Dunning, 1993" startWordPosition="947" endWordPosition="949"> basis for experimentation and evaluation of WSI systems. # words # co-occurrences Wikipedia 3,011,397 96,979,920 ukWaC 8,687,711 441,005,478 Table 1: Size of co-occurrence databases 3.1 Preprocessing The pipeline first reads topics and snippets. If the web-page can be downloaded at the URL that corresponds to the result, it is cleaned by an HTML parser and the plain text is appended to the snippet. As further steps we segment and lemmatize the input. We apply the same preprocessing to snippets, queries and the corpora. 3.2 Co-occurrence Extraction We calculate the log-likelihood ratio (LLR) (Dunning, 1993) and point-wise mutual information (PMI) (Church and Hanks, 1990) of a word pair cooccurring at sentence level using a modified version of the collocation statistics implemented in Apache Mahout 2. Even when sorting the co-occurrences by PMI, we employ a minimum support cut-off based on the LLR, which is based on significance. All pairs with a log-likelihood ratio &lt; 1 are discarded. This value is lower than the significance level of ˜3.8 we found in the literature, but because in the expand step (see algorithm 2) we require more than two words to co-occur with the target word, we used a lower </context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>Ted Dunning. 1993. Accurate Methods for the Statistics of Surprise and Coincidence. Computational Linguistics, 19(1):61 – 74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Linton C Freeman</author>
</authors>
<title>A set of measures of centrality based on betweenness.</title>
<date>1977</date>
<journal>Sociometry,</journal>
<volume>40</volume>
<issue>1</issue>
<contexts>
<context position="7824" citStr="Freeman, 1977" startWordPosition="1239" endWordPosition="1240">ase we created in section 3.2. These represent vertices in a graph. We then construct 4 a minimum spanning tree (mst) 2http://mahout.apache.org 3Dump from April 2011 4For all of our graph operations, we employ the igraph library for R, http://igraph.sf.net 213 Figure 1: WSI and WSD Pipeline by inserting edges {vi, vj} from the co-occurrence database. The weight w({vi, vj}) of each edge is set to the inverse of the used similarity measure dist (LLR or PMI) between those terms. The minimum spanning tree then is cut into subtrees be iteratively removing the edge with the highest edgebetweenness (Freeman, 1977) (betweeness) until the size of the largest component of G falls below a threshold Sinitial. Algorithm 1 initialClusters V (Go) top n most similar words to target word w(vi, vj) dist(termi, termj) G mst(Go) V (G) V (G) \ vtarget while max(|C(G)|) &gt; Sintitial do E(G) E(G) \ argmaxe(betweeness(e)) end while The resulting partitioning of the graph is the starting point for the second phase of the algorithm, which we call expand/join step (Algorithm 2). During this step, the algorithm looks iteratively at all clusters Csmall of size s smaller than Smax = 9 (determined empirically), starting with t</context>
</contexts>
<marker>Freeman, 1977</marker>
<rawString>Linton C Freeman. 1977. A set of measures of centrality based on betweenness. Sociometry, 40(1):35–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Mitchell Marcus</author>
<author>Martha Palmer</author>
<author>Lance Ramshaw</author>
<author>Ralph Weischedel</author>
</authors>
<title>OntoNotes: the 90% solution.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers, NAACL-Short ’06,</booktitle>
<contexts>
<context position="1657" citStr="Hovy et al., 2006" startWordPosition="244" endWordPosition="247">o WSI evaluation. The task requires building a WSI system and combining it with a WSD step to assign the induced sentences to example instances. Word sense disambiguation (WSD) is the task of determining the correct meaning for an ambiguous word from its context. WSD algorithms usually choose one sense out of a given set of possible senses for each word. A resource that enumerates possible senses for each word is called a sense inventory. Manually created inventories come usually in form of lexical semantic resources, such as WordNet or more specifically created inventories such as OntoNotes (Hovy et al., 2006). Word sense induction (WSI) on the other hand aims to create such an inventory from a corpus in an unsupervised manner. For each word that should be disambiguated, a WSI algorithm creates a set of context clusters that will be used to define and describe the senses. We build our system upon the open-source DKPro framework 1 and a corresponding WSD component (upcoming). Input for the task comes as two files. One contains the search queries, also referred as topics. Sense induction will be performed for each of those topics. The second file contains 6400 entries from the result pages of a searc</context>
</contexts>
<marker>Hovy, Marcus, Palmer, Ramshaw, Weischedel, 2006</marker>
<rawString>Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2006. OntoNotes: the 90% solution. In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers, NAACL-Short ’06,</rawString>
</citation>
<citation valid="false">
<pages>57--60</pages>
<publisher>Association for</publisher>
<institution>Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker></marker>
<rawString>pages 57–60, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Jurgens</author>
</authors>
<title>Word Sense Induction by Community Detection.</title>
<date>2011</date>
<booktitle>In HLT ’11: Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics Human Language Technologies,</booktitle>
<pages>24--28</pages>
<location>Portland, Oregon.</location>
<contexts>
<context position="4315" citStr="Jurgens (2011)" startWordPosition="659" endWordPosition="660">rithms for WSI: Others (Klapaftis and Manandhar, 2010) use hierarchical agglomerative clustering on hierarchical random graphs created from word co-occurrences. Di Marco and Navigli (2013) use word sense induction for web search result clustering. They introduce a maximum spanning tree algorithm that operates on co-occurrence graphs built from large corpora, such as ukWaC (Baroni et al., 2009). The system by Pedersen (2010) employs clustering first- and second-order co-occurrences as well as singular value decomposition on the cooccurrence matrix, which is clustered using repeated bisections. Jurgens (2011) employ a graph-based community detection algorithm on a co-occurrence graph. Distributional approaches for WSI include LSA Apidianaki and Van de Cruys (2011) or LDA (Brody and Lapata, 2009). 3 Our Approach Our system consists of two independent parts. The first is a batch process that creates database containing co-occurrence statistics derived from a background corpus. The second is the actual WSI and WSD pipeline doing the result clustering. Both parts include identical preprocessing steps for segmentation and lemmatization. The pipeline (Figure 1) first performs Word Sense Induction, resul</context>
</contexts>
<marker>Jurgens, 2011</marker>
<rawString>David Jurgens. 2011. Word Sense Induction by Community Detection. In HLT ’11: Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics Human Language Technologies, pages 24– 28, Portland, Oregon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
<author>Brighton England</author>
<author>Joseph Rosenzweig</author>
</authors>
<title>English Senseval: Report and Results.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2nd International Conference on Language Resources and Evaluation,</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="9684" citStr="Kilgarriff et al., 2000" startWordPosition="1566" endWordPosition="1569">cluster by adding edges between vertices representing the expansion terms and Csmall. Algorithm 2 expandJoin Require: G is a minimum spanning forest for s = Smax —* 1 do for all Csmall(G), |Csmall |= s do E querys(v1, .., vi) for all Clarge E G, |Clarge |&gt; s do if |Clarge n E|/|Clarge |&gt; t then Clarge Clarge U Csmall else Csmall Csmall U E end if end for end for end for 3.4 Word Sense Disambiguation We use the DKPro WSD framework, which implements various WSD algorithms, with the same system configuration as reported by Miller et al. (2012). It uses a variant of the Simplified Lesk Algorithm (Kilgarriff et al., 2000). This algorithm measures the overlap between a words context and the textual descriptions of senses within a machine readable dictionary, such as WordNet. The senses that have been induced in the previous step are provided to the framework as a sense inventory. Instead of using sense descriptions, we now compute the overlap between the sense clusters and the context of the target word. The WSD system expands both the word context and the sense clusters with synonyms from a distributional thesaurus (DT), similar to Lin (1998). The DT has been created from 10M dependency-parsed sentences of Eng</context>
</contexts>
<marker>Kilgarriff, England, Rosenzweig, 2000</marker>
<rawString>Adam Kilgarriff, Brighton England, and Joseph Rosenzweig. 2000. English Senseval: Report and Results. In Proceedings of the 2nd International Conference on Language Resources and Evaluation, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ioannis P Klapaftis</author>
<author>Suresh Manandhar</author>
</authors>
<title>Word sense induction &amp; disambiguation using hierarchical random graphs.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>745--755</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, Massachusetts,</location>
<contexts>
<context position="3755" citStr="Klapaftis and Manandhar, 2010" startWordPosition="575" endWordPosition="579"> the averaged the com1http://code.google.com/p/dkpro-core-asl/ 212 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 212–216, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics mittee feature vectors. This exploits the assumption that two or more words together disambiguate each other, Bordag (2006) extends on this idea by using word triples to form non-ambiguous seed-clusters. Many approaches use a variety of graph clustering algorithms for WSI: Others (Klapaftis and Manandhar, 2010) use hierarchical agglomerative clustering on hierarchical random graphs created from word co-occurrences. Di Marco and Navigli (2013) use word sense induction for web search result clustering. They introduce a maximum spanning tree algorithm that operates on co-occurrence graphs built from large corpora, such as ukWaC (Baroni et al., 2009). The system by Pedersen (2010) employs clustering first- and second-order co-occurrences as well as singular value decomposition on the cooccurrence matrix, which is clustered using repeated bisections. Jurgens (2011) employ a graph-based community detectio</context>
</contexts>
<marker>Klapaftis, Manandhar, 2010</marker>
<rawString>Ioannis P. Klapaftis and Suresh Manandhar. 2010. Word sense induction &amp; disambiguation using hierarchical random graphs. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 745–755, Cambridge, Massachusetts, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th annual meeting on Association for Computational Linguistics,</booktitle>
<pages>768--774</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA,</location>
<contexts>
<context position="10215" citStr="Lin (1998)" startWordPosition="1658" endWordPosition="1659">). It uses a variant of the Simplified Lesk Algorithm (Kilgarriff et al., 2000). This algorithm measures the overlap between a words context and the textual descriptions of senses within a machine readable dictionary, such as WordNet. The senses that have been induced in the previous step are provided to the framework as a sense inventory. Instead of using sense descriptions, we now compute the overlap between the sense clusters and the context of the target word. The WSD system expands both the word context and the sense clusters with synonyms from a distributional thesaurus (DT), similar to Lin (1998). The DT has been created from 10M dependency-parsed sentences of English newswire 214 Run Fl ARI RI JI # clusters avg cl. size wacky-llr 0.5826 0.0253 0.5002 0.3394 3.6400 32.3434 wp-llr 0.5864 0.0377 0.5109 0.3177 4.1700 21.8702 wp-pmi 0.6048 0.0364 0.5050 0.2932 5.8600 30.3098 Table 2: Results for the submitted runs from the Leipzig Corpora Collection (Biemann et al., 2007) for word similarity5. Besides knowledgebased WSD, the DT also has been successfully used for improving the performance of semantic text similarity (B¨ar et al., 2012). The WSD component disambiguates each instance of the</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 36th annual meeting on Association for Computational Linguistics, pages 768–774, Morristown, NJ, USA, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tristan Miller</author>
<author>Chris Biemann</author>
<author>Torsten Zesch</author>
<author>Iryna Gurevych</author>
</authors>
<title>Using Distributional Similarity for Lexical Expansion in Knowledge-based Word Sense Disambiguation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 24th International Conference on Computational Linguistics (COLING</booktitle>
<contexts>
<context position="9606" citStr="Miller et al. (2012)" startWordPosition="1553" endWordPosition="1556"> clusters, we assume that Csmall represents a sense of its own extend the cluster by adding edges between vertices representing the expansion terms and Csmall. Algorithm 2 expandJoin Require: G is a minimum spanning forest for s = Smax —* 1 do for all Csmall(G), |Csmall |= s do E querys(v1, .., vi) for all Clarge E G, |Clarge |&gt; s do if |Clarge n E|/|Clarge |&gt; t then Clarge Clarge U Csmall else Csmall Csmall U E end if end for end for end for 3.4 Word Sense Disambiguation We use the DKPro WSD framework, which implements various WSD algorithms, with the same system configuration as reported by Miller et al. (2012). It uses a variant of the Simplified Lesk Algorithm (Kilgarriff et al., 2000). This algorithm measures the overlap between a words context and the textual descriptions of senses within a machine readable dictionary, such as WordNet. The senses that have been induced in the previous step are provided to the framework as a sense inventory. Instead of using sense descriptions, we now compute the overlap between the sense clusters and the context of the target word. The WSD system expands both the word context and the sense clusters with synonyms from a distributional thesaurus (DT), similar to L</context>
</contexts>
<marker>Miller, Biemann, Zesch, Gurevych, 2012</marker>
<rawString>Tristan Miller, Chris Biemann, Torsten Zesch, and Iryna Gurevych. 2012. Using Distributional Similarity for Lexical Expansion in Knowledge-based Word Sense Disambiguation. In Proceedings of the 24th International Conference on Computational Linguistics (COLING 2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Daniele Vannella</author>
</authors>
<title>SemEval2013 Task 11: Evaluating Word Sense Induction &amp; Disambiguation within An End-User Application.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013), in conjunction with the Second Joint Conference on Lexical and Computational Semantcis (*SEM 2013),</booktitle>
<location>Atlanta, USA.</location>
<contexts>
<context position="952" citStr="Navigli and Vannella, 2013" startWordPosition="125" endWordPosition="128">n www.ukp.tu-darmstadt.de Abstract In this paper, we describe the UKP Lab system participating in the Semeval-2013 task “Word Sense Induction and Disambiguation within an End-User Application”. Our approach uses preprocessing, co-occurrence extraction, graph clustering, and a state-of-theart word sense disambiguation system. We developed a configurable pipeline which can be used to integrate and evaluate other components for the various steps of the complex task. 1 Introduction The task “Evaluating Word Sense Induction and Word Sense Disambiguation in an End-User Application” of SemEval-2013 (Navigli and Vannella, 2013) aims at an extrinsic evaluation scheme for WSI to overcome the difficulties inherent to WSI evaluation. The task requires building a WSI system and combining it with a WSD step to assign the induced sentences to example instances. Word sense disambiguation (WSD) is the task of determining the correct meaning for an ambiguous word from its context. WSD algorithms usually choose one sense out of a given set of possible senses for each word. A resource that enumerates possible senses for each word is called a sense inventory. Manually created inventories come usually in form of lexical semantic </context>
</contexts>
<marker>Navigli, Vannella, 2013</marker>
<rawString>Roberto Navigli and Daniele Vannella. 2013. SemEval2013 Task 11: Evaluating Word Sense Induction &amp; Disambiguation within An End-User Application. In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013), in conjunction with the Second Joint Conference on Lexical and Computational Semantcis (*SEM 2013), Atlanta, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Dekang Lin</author>
</authors>
<title>Discovering word senses from text.</title>
<date>2002</date>
<booktitle>In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining - KDD ’02,</booktitle>
<pages>613</pages>
<publisher>ACM Press.</publisher>
<location>New York, New York, USA,</location>
<contexts>
<context position="2791" citStr="Pantel and Lin (2002)" startWordPosition="439" endWordPosition="442">of those topics. The second file contains 6400 entries from the result pages of a search engine. Each entry consists of the title, a snippet and the URL of the corresponding web page. 2 Related Work One of the early approaches to WSI (Sch¨utze, 1998) maps words into a vector space and represents word contexts as vector-sums and use cosine vector similarity, clustering is performed by expectation maximization (EM) clustering. Dorow and Widdows (2003) use the BNC to build a co-occurrence graph for nouns, based on a co-occurrence frequency threshold. They perform Markov clustering on this graph. Pantel and Lin (2002) proposes a clustering approach called clustering by committee (CBC). This algorithm first selects the words with the highest similarity based on mutual information and then builds groups of highly connected words called committees. It then iteratively assigns the remaining words to one of the committee clusters by comparing them to the averaged the com1http://code.google.com/p/dkpro-core-asl/ 212 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 212–216, Atlanta, Georgia, June 14-15, 201</context>
</contexts>
<marker>Pantel, Lin, 2002</marker>
<rawString>Patrick Pantel and Dekang Lin. 2002. Discovering word senses from text. In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining - KDD ’02, page 613, New York, New York, USA, July. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
</authors>
<title>Duluth-WSI: SenseClusters applied to the sense induction task of SemEval-2.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>363--366</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA,</location>
<contexts>
<context position="4128" citStr="Pedersen (2010)" startWordPosition="634" endWordPosition="635">words together disambiguate each other, Bordag (2006) extends on this idea by using word triples to form non-ambiguous seed-clusters. Many approaches use a variety of graph clustering algorithms for WSI: Others (Klapaftis and Manandhar, 2010) use hierarchical agglomerative clustering on hierarchical random graphs created from word co-occurrences. Di Marco and Navigli (2013) use word sense induction for web search result clustering. They introduce a maximum spanning tree algorithm that operates on co-occurrence graphs built from large corpora, such as ukWaC (Baroni et al., 2009). The system by Pedersen (2010) employs clustering first- and second-order co-occurrences as well as singular value decomposition on the cooccurrence matrix, which is clustered using repeated bisections. Jurgens (2011) employ a graph-based community detection algorithm on a co-occurrence graph. Distributional approaches for WSI include LSA Apidianaki and Van de Cruys (2011) or LDA (Brody and Lapata, 2009). 3 Our Approach Our system consists of two independent parts. The first is a batch process that creates database containing co-occurrence statistics derived from a background corpus. The second is the actual WSI and WSD pi</context>
</contexts>
<marker>Pedersen, 2010</marker>
<rawString>Ted Pedersen. 2010. Duluth-WSI: SenseClusters applied to the sense induction task of SemEval-2. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 363–366, Stroudsburg, PA, USA, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<marker>Sch¨utze, 1998</marker>
<rawString>Hinrich Sch¨utze. 1998. Automatic word sense discrimination. Computational Linguistics, 24(1):97–123, March.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>