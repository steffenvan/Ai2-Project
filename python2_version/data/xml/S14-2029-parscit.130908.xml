<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.048697">
<title confidence="0.9972945">
CMUQ@Qatar:Using Rich Lexical Features for
Sentiment Analysis on Twitter
</title>
<author confidence="0.95404">
Sabih Bin Wasi, Rukhsar Neyaz, Houda Bouamor, Behrang Mohit
</author>
<affiliation confidence="0.942108">
Carnegie Mellon University in Qatar
</affiliation>
<email confidence="0.986879">
{sabih, rukhsar, hbouamor, behrang}@cmu.edu
</email>
<sectionHeader confidence="0.993623" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.996966055555556">
In this paper, we describe our system for
the Sentiment Analysis of Twitter shared
task in SemEval 2014. Our system uses
an SVM classifier along with rich set of
lexical features to detect the sentiment of
a phrase within a tweet (Task-A) and also
the sentiment of the whole tweet (Task-
B). We start from the lexical features that
were used in the 2013 shared tasks, we en-
hance the underlying lexicon and also in-
troduce new features. We focus our fea-
ture engineering effort mainly on Task-
A. Moreover, we adapt our initial frame-
work and introduce new features for Task-
B. Our system reaches weighted score of
87.11% in Task-A and 64.52% in Task-B.
This places us in the 4th rank in the Task-
A and 15th in the Task-B.
</bodyText>
<sectionHeader confidence="0.998986" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999914">
With more than 500 million tweets sent per day,
containing opinions and messages, Twitter1 has
become a gold-mine for organizations to monitor
their brand reputation. As more and more users
post about products and services they use, Twit-
ter becomes a valuable source of people’s opin-
ions and sentiments: what people can think about
a product or a service, how positive they can be
about it or what would people prefer the product to
be like. Such data can be efficiently used for mar-
keting. However, with the increasing amount of
tweets posted on a daily basis, it is challenging and
expensive to manually analyze them and locate the
meaningful ones. There has been a body of re-
cent work to automatically learn the public sen-
</bodyText>
<footnote confidence="0.9277368">
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
1http://twitter.com
</footnote>
<bodyText confidence="0.999904707317073">
timents from tweets using natural language pro-
cessing techniques (Pang and Lee, 2008; Jansen
et al., 2009; Pak and Paroubek, 2010; Tang et al.,
2014). However, the task of sentiment analysis of
tweets in their free format is harder than that of any
well-structured document. Tweet messages usu-
ally contain different kinds of orthographic errors
such as the use of special and decorative charac-
ters, letter or word duplication, extra punctuation,
as well as the use of special abbreviations.
In this paper, we present our machine learn-
ing based system for sentiment analysis of Twitter
shared task in SemEval 2014. Our system takes
as input an arbitrary tweet and assigns it to one
of the following classes that best reflects its sen-
timent: positive, negative or neutral. While pos-
itive and negative tweets are subjective, neutral
class encompasses not only objective tweets but
also subjective tweets that does not contain any
”polar” emotion. Our classifier was developed as
an undergrad course project but later pursued as
a research topic. Our training, development and
testing experiments were performed on data sets
published in SemEval 2013 (Nakov et al., 2013).
Motivated with its performance, we participated
in SemEval 2014 Task 9 (Rosenthal et al., 2014).
Our approach includes an extensive usage of off-
the-shelf resources that have been developed for
conducting NLP on social media text. Our orig-
inal aim was enhancement of the task-A. More-
over, we adapted our framework and introduced
new features for task-B and participated in both
shared tasks. We reached an F-score of 83.3% in
Task-A and an F-score of 65.57% in Task-B. That
placed us in the 4th rank in the task-A and 15th
rank in the task-B.
Our approach includes an extensive usage of
off-the-shelf resources that have been developed
for conducting NLP on social media text. That
includes the Twitter Tokenizer and also the Twit-
ter POS tagger, several sentiment analysis lexica
</bodyText>
<page confidence="0.984285">
186
</page>
<note confidence="0.73069">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 186–191,
Dublin, Ireland, August 23-24, 2014.
</note>
<bodyText confidence="0.9999659">
and finally our own enhanced resources for spe-
cial handling of Twitter-specific text. Our origi-
nal aim in introducing and evaluating many of the
features was enhancement of the task-A. More-
over, we adapted our framework and introduced
new features for task-B and participated in both
shared tasks. We reached an F-score of 83.3% in
Task-A and an F-score of 65.57% in Task-B. That
placed us in the 4th rank in the task-A and 15th
rank in the task-B.
</bodyText>
<sectionHeader confidence="0.9405" genericHeader="method">
2 System Overview
</sectionHeader>
<bodyText confidence="0.999703833333333">
We participate in tasks A and B. We use three-
way classification framework in which we design
and use a rich feature representation of the Twitter
text. In order to process the tweets, we start with
a pre-processing step, followed by feature extrac-
tion and classifier training.
</bodyText>
<subsectionHeader confidence="0.946182">
2.1 Data Pre-processing
</subsectionHeader>
<bodyText confidence="0.99974475">
Before the tweet is fed to the system, it goes
through pre-processing phase that breaks tweet
string into words (tokenization), attaches more in-
formation to each word (POS tagging), and other
treatments.
Tokenization: We use CMU ARK Tok-
enizer (Owoputi et al., 2013) to tokenize each
tweet. This tokenizer is developed to tokenize
not only space-separated text but also tokens that
need to be analyzed separately.
POS tagging: We use CMU ARK POS Tag-
ger (Owoputi et al., 2013) to assign POS tags to
the different tokens. In addition to the grammat-
ical tags, this tagger assigns also twitter-specific
tags like @ mentions, hash tags, etc. This infor-
mation is used later for feature extraction.
Other processing: In order to normalize the dif-
ferent tokens and convert them into a correct En-
glish, we find acronyms in the text and add their
expanded forms at the end of the list. We decide
to keep both the acronym and the new word to en-
sure that if the token without its expansion was
the word the user meant, then we are not losing
any information by getting its acronym. We ex-
tend the NetLingo2 top 50 Internet acronym list to
add some missing acronyms. In order to reduce in-
flectional forms of a word to a common base form
we use WordNetlemmatizer in NLTK (Bird et al.,
</bodyText>
<footnote confidence="0.946561">
2http://www.netlingo.com/top50/
popular-text-terms.php
</footnote>
<table confidence="0.995676272727273">
Tweet ”This is so awesome
@henry:D! #excited”
Bag of Words ”This”:1, ”is”:1, ”so”:1,
”awesome”:1, ”@henry”:1,
”:D”:1, ”!,”:1, #excited”:1
POS features numHashTags:1, numAd-
verb:1, numAdjective:1
Polarity features positiveWords:1, negWords:0,
avgScore: -0.113
Task-B specific numCapsWords:0, numEmo-
features ticons:1, numUrls:0
</table>
<tableCaption confidence="0.974308">
Table 1: Set of Features demonstrated on a sample
tweet for Task-B.
</tableCaption>
<bodyText confidence="0.99068125">
2009)3. This could be useful for the feature extrac-
tion, to get as much matches as possible between
the train and test data (e.g., for bag-of-words fea-
ture).
</bodyText>
<subsectionHeader confidence="0.997808">
2.2 Feature Extraction
</subsectionHeader>
<bodyText confidence="0.999506888888889">
Assigning a sentiment to a single word, phrase or
a full tweet message requires a rich set of fea-
tures. For this, we adopt a forward selection ap-
proach (Ladha and Deepa, 2011) to select the fea-
tures that characterize to the best the different sen-
timents and help distinguishing them. In this ap-
proach, we incrementally add the features one by
one and test whether this boosts the development
results. We heavily rely on a binary feature rep-
resentation (Heinly et al., 2012) to ensure the ef-
ficiency and robustness of our classifier. The dif-
ferent features used are illustrated in the example
given in Table 1.
Bag-of-words feature: indicates whether a
given token is present in the phrase.
Morpho-syntactic feature: we use the POS and
twitter-specific tags extracted for each token. We
count the number of adjectives, adverbs and hash-
tags present in the focused part of the tweet mes-
sage (entire tweet or phrase). We tried adding
other POS based features (e.g., number of posses-
sive pronouns, etc.), but only the aforementioned
tags increased the result figures for both tasks.
Polarity-based features: we use freely avail-
able sentiment resources to explicitly define the
polarity at a token-level. We define three feature
categories, based on the lexicon used:
</bodyText>
<footnote confidence="0.9619755">
3http://www.nltk.org/api/nltk.stem.
html
</footnote>
<page confidence="0.987526">
187
</page>
<table confidence="0.999161333333333">
Task-A Task-B
Dev Train Test Dev Train Test
Positive 57.09 % 62.06% 59.49% 34.76% 37.59% 39.01%
Negative 37.89% 33.01% 35.31% 20.56% 15.06% 17.15%
Neutral 5.02% 4.93% 5.21% 44.68% 47.36% 43.84%
All 1,135 9,451 10,681 1,654 9,684 8,987
</table>
<tableCaption confidence="0.994306">
Table 2: Class size distribution for all the three sets for both Task-A and Task-B.
</tableCaption>
<listItem confidence="0.995143722222222">
• Subjectivity: number of words mapped to
”positive” from the MPQA Subjectivity lexi-
con (Wilson et al., 2005).
• Hybrid Lexicon: We combine the Senti-
ment140 lexicon (Mohammad et al., 2013)
with the Bing Liu’s bag of positive and neg-
ative words (Hu and Liu, 2004) to create a
dictionary in which each token is assigned a
sentiment.
• Token weight: we use the SentiWordNet
lexicon (Baccianella et al., 2010) to define
this feature. SentiWordNet contains positive,
negative and objective scores between 0 and
1 for all senses in WordNet. Based on this
sense level annotation, we first map each to-
ken to its weight in this lexicon and then the
sum of all these weights was used as the tweet
weight.
</listItem>
<bodyText confidence="0.99966595">
Furthermore, in order to take into account the
presence of negative words, which modify the po-
larity of the context within which they are invoked,
we reverse the polarity score of adjectives or ad-
verbs that come within 1-2 token distance after a
negative word.
Task specific features: In addition to the fea-
tures described above, we also define some task-
specific ones. For example, we indicate the num-
ber of capital letters in the phrase as a feature in
Task-A. This could help in this task, since we are
focusing on short text. For Task-B we indicate
instead the number of capital words. This relies
on the intuition that polarized tweets would carry
more (sometimes all) capital words than the neu-
tral or objective ones. We also added the number
of emoticons and number of URL links as fea-
tures for Task-B. Here, the goal is to segregate
fact-containing objective tweets from emotion-
containing subjective tweets.
</bodyText>
<subsectionHeader confidence="0.974839">
2.3 Classifier
</subsectionHeader>
<bodyText confidence="0.9994663">
We use a Support Vector Machine (SVM) classi-
fier (Chang and Lin, 2011) to which we provide
the rich set of features described in the previous
section. We use a linear kernel and tune its param-
eter C separately for the two tasks. Task-A sys-
tem was bound tight to the development set with
C=0.18 whereas in Task-B the system was given
freedom by setting C=0.55. These values were
optimized during the development using a brute-
force mechanism.
</bodyText>
<table confidence="0.999377">
Task-A Task-B
LiveJournal 2014 83.89 65.63
SMS 2013 88.08 62.95
Twitter 2013 89.85 65.11
Twitter 2014 83.45 65.53
Sarcasm 78.07 40.52
Weighted average 87.11 64.52
</table>
<tableCaption confidence="0.988681">
Table 3: F1 measures and final results of the sys-
</tableCaption>
<bodyText confidence="0.7838925">
tem for Task-A and Task-B for all the data sets
including the weighted average of the sets.
</bodyText>
<sectionHeader confidence="0.997542" genericHeader="method">
3 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999922">
In this section, we explain details of the data and
the general settings for the different experiments
we conducted. We train and evaluate our classifier
for both tasks with the training, development and
testing datasets provided for the SemEval 2014
shared task. The size of the three datasets we
use as well as their class distributions are illus-
trated in Table 2 . It is important to note that
the total dataset size for training and development
set (10,586) is about the same as test set mak-
ing the learning considerably challenging for cor-
rect predictions. Positive instances covered more
than half of each dataset for Task-A while Neutral
were the most popular class for Task-B. The class
distribution of training set is the same as the test
set.
</bodyText>
<page confidence="0.987514">
188
</page>
<figure confidence="0.8912185">
all features
all-preprocessing
all-ARK tokenization
all-other treatments
only BOW
all-bow
all-pos
all-polarity based features
all-SVM tuning
all-SVM c=0.01
all-SVM c=selected
all-SVM c=1
</figure>
<table confidence="0.858842230769231">
Task-A Task-B
87.11 64.52
80.79(-6.32) 59.20(-5.32)
83.69(-3.42) 60.61(-3.91)
85.06(-2.05) 62.19(-2.33)
81.69(-5.42) 57.85(-6.67)
82.05(-5.06) 52.04(-12.48)
86.92(-0.19) 64.31(-0.21)
81.80(-5.31) 57.95(-6.57)
80.82(-6.29) 21.41(-43.11)
84.20(-2.91) 59.87(-4.65)
87.11(0.00) 64.52(0.00)
86.39(-0.72) 62.51(-2.01)
</table>
<tableCaption confidence="0.99795">
Table 4: F-scores obtained on the test sets with the specific feature removed.
</tableCaption>
<bodyText confidence="0.99994275">
The test dataset is composed of five differ-
ent sets: Twitter2013 a set of tweets collected
for the SemEval2013 test set, Twitter2014, tweets
collected for this years version, LiveJournal2014
consisting of formal tweets, SMS2013, a collec-
tion of sms messages,TwitterSarcasm, a collection
of sarcastic tweets. The results of our system are
shown in Table 3. The top five rows shows the
results by the SemEval scorer for all the data sets
used by them. This scorer took the average of F1-
score of only positive and negative classes. The
last row shows the weighted average score of all
the scores for Task A and B from the different data
sets.
Our scores for Task-A and Task-B were 83.45
and 65.53 respectively for Twitter 2014.
Our system performed better on Twitter and
SMS test sets from 2013. This was reasonable
since we tuned our system on these datasets. On
the other hand, the system performed worst on sar-
casm test set. This drop is extremely evident in
Task-B where the results were dropped by 25%.
To analyze the effects of each step of our sys-
tem, we experimented with our system using dif-
ferent configurations. The results are shown in Ta-
ble 4 and our analysis is described in the following
subsections. The results were scored by SemEval
2014 scorer and we took the weighted average of
all data sets to accurately reflect the performance
of our system.
We show the polarities values assigned to each
token of a tweet by our classifier, in Table 5.
</bodyText>
<table confidence="0.999286428571429">
Tokens POS Tags Sentiments Polarity
This O Neutral -0.194
Is V Neutral -0.115
So R Neutral -0.253
Awesome A Positive 2.351
@Henry @ - -
#excited # Positive 1.84
</table>
<tableCaption confidence="0.983201">
Table 5: Polarity assigned using our classifier to
each word of a Tweet message.
</tableCaption>
<subsectionHeader confidence="0.999773">
3.1 Preprocessing Effects
</subsectionHeader>
<bodyText confidence="0.999972166666667">
We compared the effects of basic tokenization
(based on white space) against the richer ARK
Twitter tokenizer. The scores dropped by 3.42%
and 3.91% for Task-A and Task-B, respectively.
Other preprocessing enhancements like lemmati-
zation and acronym additions also gave our sys-
tem performance a boost. Again, the effects were
more visible for Task-B than for Task-A. Over-
all, the system performance was boosted by 6.32%
for Task-A and 5.32% for Task-B. Considering
the overall score for Task-B, this is a significant
change.
</bodyText>
<subsectionHeader confidence="0.999653">
3.2 Feature Engineering Effects
</subsectionHeader>
<bodyText confidence="0.999636555555555">
To analyze the effect of feature extraction pro-
cess, we ran our system with different kind of
features disabled - one at a time. For Task-A,
unigram model and polarity based features were
equally important. For Task-B, bag of words fea-
ture easily outperformed the effects of any other
feature. However, polarity based features were
second important class of features for our system.
These suggest that if more accurate, exhaustive
</bodyText>
<page confidence="0.996099">
189
</page>
<sectionHeader confidence="0.495592" genericHeader="method">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.999917444444444">
We would like to thank Kemal Oflazer and the
shared task organizers for their support through-
out this work.
and social media representative lexicons are made,
it would help both tasks significantly. POS based
features were not directly influential in our system.
However, these tags helped us find better matches
in lexicons where words are further identified with
their POS tag.
</bodyText>
<subsectionHeader confidence="0.999049">
3.3 Classifier Tuning
</subsectionHeader>
<bodyText confidence="0.999966181818182">
We also analyzed the significance of SVM tuning
to our system. Without setting any parameter to
SVMutil library (Chang and Lin, 2011), we no-
ticed a drop of 6.29% to scores of Task-A and a
significant drop of 43.11% to scores of Task-B.
Since the library use poly kernel by default, the
results were drastically worse for Task-B due to
large feature set. We also compared the perfor-
mance with SVM kernel set to C=1. In this re-
stricted setting, the results were slightly lower than
the result obtained for our final system.
</bodyText>
<sectionHeader confidence="0.999863" genericHeader="evaluation">
4 Discussion
</sectionHeader>
<bodyText confidence="0.999974666666667">
During this work, we found that two improve-
ments to our system would have yielded better
scores. The first would be lexicons: Since the
lexicons like Sentiment140 Lexicon are automati-
cally generated, we found that they contain some
noise. As we noticed a drop of that our results
were critically dependent on these lexicons, this
noise would have resulted in incorrect predictions.
Hence, more accurate and larger lexicons are re-
quired for better classification, especially for the
tweet-level task. Unlike SentiWordNet these lexi-
cons should contain more informal words that are
common in social media. Additionally, as we can
see our system was not able to confidently predict
sarcasm tweets on both expression and message
level, special attention is required to analyze the
nature of sarcasm on Twitter and build a feature
set that can capture the true sentiment of the tweet.
</bodyText>
<sectionHeader confidence="0.998954" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999851">
We demonstrated our classification system that
could predict sentiment of an input tweet. Our
system performed more accurately in expression-
level prediction than on entire tweet-level predic-
tion. Our system relied heavily on bag-of-words
feature and polarity based features which in turn
relied on correct part-of-speech tagging and third-
party lexicons. With this system, we ranked 4th
in SemEval 2014 expression-level prediction task
and 15th in tweet-level prediction task.
</bodyText>
<sectionHeader confidence="0.989622" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999511340425532">
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. SentiWordNet 3.0: An Enhanced Lex-
ical Resource for Sentiment Analysis and Opinion
Mining. In Proceedings of the Seventh conference
on International Language Resources and Evalua-
tion (LREC’10), pages 2200–2204, Valletta, Malta.
Steven Bird, Ewan Klein, and Edward Loper.
2009. Natural Language Processing with Python.
O’Reilly Media, Inc.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A Library for Support Vector Machines.
ACM Transactions on Intelligent Systems and Tech-
nology, 2:27:1–27:27.
Jared Heinly, Enrique Dunn, and Jan-Michael Frahm.
2012. Comparative Evaluation of Binary Features.
In Proceedings of the 12th European Conference on
Computer Vision, pages 759–773, Firenze, Italy.
Minqing Hu and Bing Liu. 2004. Mining and Sum-
marizing Customer Reviews. In Proceedings of the
Tenth ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, pages 168–
177, Seattle, WA, USA.
Bernard J Jansen, Mimi Zhang, Kate Sobel, and Ab-
dur Chowdury. 2009. Twitter Power: Tweets as
Electronic Word of Mouth. Journal of the Ameri-
can society for information science and technology,
60(11):2169–2188.
L. Ladha and T. Deepa. 2011. Feature Selection Meth-
ods and Algorithms. International Journal on Com-
puter Science and Engineering (IJCSE), 3:1787–
1797.
Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. NRC-Canada: Building the State-of-
the-Art in Sentiment Analysis of Tweets. In Second
Joint Conference on Lexical and Computational Se-
mantics (*SEM), Volume 2: Proceedings of the Sev-
enth International Workshop on Semantic Evalua-
tion (SemEval 2013), pages 321–327, Atlanta, Geor-
gia, USA.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. SemEval-2013 Task 2: Sentiment Analysis in
Twitter. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 2: Pro-
ceedings of the Seventh International Workshop on
Semantic Evaluation (SemEval 2013), pages 312–
320, Atlanta, Georgia, USA.
</reference>
<page confidence="0.972425">
190
</page>
<reference confidence="0.99967046875">
Olutobi Owoputi, Brendan O’Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A.
Smith. 2013. Improved Part-of-Speech Tagging for
Online Conversational Text with Word Clusters. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 380–390, Atlanta, Georgia.
Alexander Pak and Patrick Paroubek. 2010. Twitter as
a Corpus for Sentiment Analysis and Opinion Min-
ing. In Proceedings of the Seventh conference on
International Language Resources and Evaluation
(LREC’10), pages 1320–1326, Valletta, Malta.
Bo Pang and Lillian Lee. 2008. Opinion Mining and
Sentiment Analysis, volume 2. Now Publishers Inc.
Sara Rosenthal, Preslav Nakov, Alan Ritter, and
Veselin Stoyanov. 2014. SemEval-2014 Task 9:
Sentiment Analysis in Twitter. In Proceedings of the
Eighth International Workshop on Semantic Evalu-
ation (SemEval’14), Dublin, Ireland.
Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting
Liu, and Bing Qin. 2014. Learning Sentiment-
Specific Word Embedding for Twitter Sentiment
Classification. In Proceedings of the 52nd Annual
Meeting of the Association for Computational Lin-
guistics, pages 1555–1565, Baltimore, Maryland.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing Contextual Polarity in Phrase-
level Sentiment Analysis. In Proceedings of the con-
ference on human language technology and empiri-
cal methods in natural language processing, pages
347–354.
</reference>
<page confidence="0.998474">
191
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.652330">
<title confidence="0.9502015">CMUQ@Qatar:Using Rich Lexical Features Sentiment Analysis on Twitter</title>
<author confidence="0.968018">Sabih Bin Wasi</author>
<author confidence="0.968018">Rukhsar Neyaz</author>
<author confidence="0.968018">Houda Bouamor</author>
<author confidence="0.968018">Behrang</author>
<affiliation confidence="0.843867">Carnegie Mellon University in Qatar</affiliation>
<email confidence="0.966127">rukhsar,hbouamor,</email>
<abstract confidence="0.985130176470588">In this paper, we describe our system for the Sentiment Analysis of Twitter shared task in SemEval 2014. Our system uses an SVM classifier along with rich set of lexical features to detect the sentiment of a phrase within a tweet (Task-A) and also the sentiment of the whole tweet (Task- B). We start from the lexical features that were used in the 2013 shared tasks, we enhance the underlying lexicon and also introduce new features. We focus our feaengineering effort mainly on Task- A. Moreover, we adapt our initial frameand introduce new features for Task- B. Our system reaches weighted score of 87.11% in Task-A and 64.52% in Task-B.</abstract>
<note confidence="0.9881385">This places us in the 4th rank in the Task- A and 15th in the Task-B.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Stefano Baccianella</author>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>SentiWordNet 3.0: An Enhanced Lexical Resource for Sentiment Analysis and Opinion Mining.</title>
<date>2010</date>
<booktitle>In Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC’10),</booktitle>
<pages>2200--2204</pages>
<location>Valletta,</location>
<contexts>
<context position="8693" citStr="Baccianella et al., 2010" startWordPosition="1424" endWordPosition="1427">ative 37.89% 33.01% 35.31% 20.56% 15.06% 17.15% Neutral 5.02% 4.93% 5.21% 44.68% 47.36% 43.84% All 1,135 9,451 10,681 1,654 9,684 8,987 Table 2: Class size distribution for all the three sets for both Task-A and Task-B. • Subjectivity: number of words mapped to ”positive” from the MPQA Subjectivity lexicon (Wilson et al., 2005). • Hybrid Lexicon: We combine the Sentiment140 lexicon (Mohammad et al., 2013) with the Bing Liu’s bag of positive and negative words (Hu and Liu, 2004) to create a dictionary in which each token is assigned a sentiment. • Token weight: we use the SentiWordNet lexicon (Baccianella et al., 2010) to define this feature. SentiWordNet contains positive, negative and objective scores between 0 and 1 for all senses in WordNet. Based on this sense level annotation, we first map each token to its weight in this lexicon and then the sum of all these weights was used as the tweet weight. Furthermore, in order to take into account the presence of negative words, which modify the polarity of the context within which they are invoked, we reverse the polarity score of adjectives or adverbs that come within 1-2 token distance after a negative word. Task specific features: In addition to the featur</context>
</contexts>
<marker>Baccianella, Esuli, Sebastiani, 2010</marker>
<rawString>Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. 2010. SentiWordNet 3.0: An Enhanced Lexical Resource for Sentiment Analysis and Opinion Mining. In Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC’10), pages 2200–2204, Valletta, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bird</author>
<author>Ewan Klein</author>
<author>Edward Loper</author>
</authors>
<date>2009</date>
<booktitle>Natural Language Processing with Python.</booktitle>
<publisher>O’Reilly Media, Inc.</publisher>
<marker>Bird, Klein, Loper, 2009</marker>
<rawString>Steven Bird, Ewan Klein, and Edward Loper. 2009. Natural Language Processing with Python. O’Reilly Media, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: A Library for Support Vector Machines.</title>
<date>2011</date>
<booktitle>ACM Transactions on Intelligent Systems and Technology,</booktitle>
<pages>2--27</pages>
<contexts>
<context position="9983" citStr="Chang and Lin, 2011" startWordPosition="1648" endWordPosition="1651">, we indicate the number of capital letters in the phrase as a feature in Task-A. This could help in this task, since we are focusing on short text. For Task-B we indicate instead the number of capital words. This relies on the intuition that polarized tweets would carry more (sometimes all) capital words than the neutral or objective ones. We also added the number of emoticons and number of URL links as features for Task-B. Here, the goal is to segregate fact-containing objective tweets from emotioncontaining subjective tweets. 2.3 Classifier We use a Support Vector Machine (SVM) classifier (Chang and Lin, 2011) to which we provide the rich set of features described in the previous section. We use a linear kernel and tune its parameter C separately for the two tasks. Task-A system was bound tight to the development set with C=0.18 whereas in Task-B the system was given freedom by setting C=0.55. These values were optimized during the development using a bruteforce mechanism. Task-A Task-B LiveJournal 2014 83.89 65.63 SMS 2013 88.08 62.95 Twitter 2013 89.85 65.11 Twitter 2014 83.45 65.53 Sarcasm 78.07 40.52 Weighted average 87.11 64.52 Table 3: F1 measures and final results of the system for Task-A an</context>
<context position="15289" citStr="Chang and Lin, 2011" startWordPosition="2506" endWordPosition="2509">s for our system. These suggest that if more accurate, exhaustive 189 Acknowledgment We would like to thank Kemal Oflazer and the shared task organizers for their support throughout this work. and social media representative lexicons are made, it would help both tasks significantly. POS based features were not directly influential in our system. However, these tags helped us find better matches in lexicons where words are further identified with their POS tag. 3.3 Classifier Tuning We also analyzed the significance of SVM tuning to our system. Without setting any parameter to SVMutil library (Chang and Lin, 2011), we noticed a drop of 6.29% to scores of Task-A and a significant drop of 43.11% to scores of Task-B. Since the library use poly kernel by default, the results were drastically worse for Task-B due to large feature set. We also compared the performance with SVM kernel set to C=1. In this restricted setting, the results were slightly lower than the result obtained for our final system. 4 Discussion During this work, we found that two improvements to our system would have yielded better scores. The first would be lexicons: Since the lexicons like Sentiment140 Lexicon are automatically generated</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A Library for Support Vector Machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1–27:27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jared Heinly</author>
<author>Enrique Dunn</author>
<author>Jan-Michael Frahm</author>
</authors>
<title>Comparative Evaluation of Binary Features.</title>
<date>2012</date>
<booktitle>In Proceedings of the 12th European Conference on Computer Vision,</booktitle>
<pages>759--773</pages>
<location>Firenze, Italy.</location>
<contexts>
<context position="7135" citStr="Heinly et al., 2012" startWordPosition="1173" endWordPosition="1176">he feature extraction, to get as much matches as possible between the train and test data (e.g., for bag-of-words feature). 2.2 Feature Extraction Assigning a sentiment to a single word, phrase or a full tweet message requires a rich set of features. For this, we adopt a forward selection approach (Ladha and Deepa, 2011) to select the features that characterize to the best the different sentiments and help distinguishing them. In this approach, we incrementally add the features one by one and test whether this boosts the development results. We heavily rely on a binary feature representation (Heinly et al., 2012) to ensure the efficiency and robustness of our classifier. The different features used are illustrated in the example given in Table 1. Bag-of-words feature: indicates whether a given token is present in the phrase. Morpho-syntactic feature: we use the POS and twitter-specific tags extracted for each token. We count the number of adjectives, adverbs and hashtags present in the focused part of the tweet message (entire tweet or phrase). We tried adding other POS based features (e.g., number of possessive pronouns, etc.), but only the aforementioned tags increased the result figures for both ta</context>
</contexts>
<marker>Heinly, Dunn, Frahm, 2012</marker>
<rawString>Jared Heinly, Enrique Dunn, and Jan-Michael Frahm. 2012. Comparative Evaluation of Binary Features. In Proceedings of the 12th European Conference on Computer Vision, pages 759–773, Firenze, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and Summarizing Customer Reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>168--177</pages>
<location>Seattle, WA, USA.</location>
<contexts>
<context position="8550" citStr="Hu and Liu, 2004" startWordPosition="1400" endWordPosition="1403">ww.nltk.org/api/nltk.stem. html 187 Task-A Task-B Dev Train Test Dev Train Test Positive 57.09 % 62.06% 59.49% 34.76% 37.59% 39.01% Negative 37.89% 33.01% 35.31% 20.56% 15.06% 17.15% Neutral 5.02% 4.93% 5.21% 44.68% 47.36% 43.84% All 1,135 9,451 10,681 1,654 9,684 8,987 Table 2: Class size distribution for all the three sets for both Task-A and Task-B. • Subjectivity: number of words mapped to ”positive” from the MPQA Subjectivity lexicon (Wilson et al., 2005). • Hybrid Lexicon: We combine the Sentiment140 lexicon (Mohammad et al., 2013) with the Bing Liu’s bag of positive and negative words (Hu and Liu, 2004) to create a dictionary in which each token is assigned a sentiment. • Token weight: we use the SentiWordNet lexicon (Baccianella et al., 2010) to define this feature. SentiWordNet contains positive, negative and objective scores between 0 and 1 for all senses in WordNet. Based on this sense level annotation, we first map each token to its weight in this lexicon and then the sum of all these weights was used as the tweet weight. Furthermore, in order to take into account the presence of negative words, which modify the polarity of the context within which they are invoked, we reverse the polar</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and Summarizing Customer Reviews. In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 168– 177, Seattle, WA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard J Jansen</author>
<author>Mimi Zhang</author>
<author>Kate Sobel</author>
<author>Abdur Chowdury</author>
</authors>
<title>Twitter Power: Tweets as Electronic Word of Mouth.</title>
<date>2009</date>
<journal>Journal of the American</journal>
<pages>60--11</pages>
<contexts>
<context position="2012" citStr="Jansen et al., 2009" startWordPosition="329" endWordPosition="332">uch data can be efficiently used for marketing. However, with the increasing amount of tweets posted on a daily basis, it is challenging and expensive to manually analyze them and locate the meaningful ones. There has been a body of recent work to automatically learn the public senThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 1http://twitter.com timents from tweets using natural language processing techniques (Pang and Lee, 2008; Jansen et al., 2009; Pak and Paroubek, 2010; Tang et al., 2014). However, the task of sentiment analysis of tweets in their free format is harder than that of any well-structured document. Tweet messages usually contain different kinds of orthographic errors such as the use of special and decorative characters, letter or word duplication, extra punctuation, as well as the use of special abbreviations. In this paper, we present our machine learning based system for sentiment analysis of Twitter shared task in SemEval 2014. Our system takes as input an arbitrary tweet and assigns it to one of the following classes</context>
</contexts>
<marker>Jansen, Zhang, Sobel, Chowdury, 2009</marker>
<rawString>Bernard J Jansen, Mimi Zhang, Kate Sobel, and Abdur Chowdury. 2009. Twitter Power: Tweets as Electronic Word of Mouth. Journal of the American society for information science and technology, 60(11):2169–2188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Ladha</author>
<author>T Deepa</author>
</authors>
<title>Feature Selection Methods and Algorithms.</title>
<date>2011</date>
<journal>International Journal on Computer Science and Engineering (IJCSE),</journal>
<volume>3</volume>
<pages>1797</pages>
<contexts>
<context position="6837" citStr="Ladha and Deepa, 2011" startWordPosition="1122" endWordPosition="1125">excited”:1 POS features numHashTags:1, numAdverb:1, numAdjective:1 Polarity features positiveWords:1, negWords:0, avgScore: -0.113 Task-B specific numCapsWords:0, numEmofeatures ticons:1, numUrls:0 Table 1: Set of Features demonstrated on a sample tweet for Task-B. 2009)3. This could be useful for the feature extraction, to get as much matches as possible between the train and test data (e.g., for bag-of-words feature). 2.2 Feature Extraction Assigning a sentiment to a single word, phrase or a full tweet message requires a rich set of features. For this, we adopt a forward selection approach (Ladha and Deepa, 2011) to select the features that characterize to the best the different sentiments and help distinguishing them. In this approach, we incrementally add the features one by one and test whether this boosts the development results. We heavily rely on a binary feature representation (Heinly et al., 2012) to ensure the efficiency and robustness of our classifier. The different features used are illustrated in the example given in Table 1. Bag-of-words feature: indicates whether a given token is present in the phrase. Morpho-syntactic feature: we use the POS and twitter-specific tags extracted for each</context>
</contexts>
<marker>Ladha, Deepa, 2011</marker>
<rawString>L. Ladha and T. Deepa. 2011. Feature Selection Methods and Algorithms. International Journal on Computer Science and Engineering (IJCSE), 3:1787– 1797.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammad</author>
<author>Svetlana Kiritchenko</author>
<author>Xiaodan Zhu</author>
</authors>
<title>NRC-Canada: Building the State-ofthe-Art in Sentiment Analysis of Tweets.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>321--327</pages>
<location>Atlanta, Georgia, USA.</location>
<contexts>
<context position="8476" citStr="Mohammad et al., 2013" startWordPosition="1385" endWordPosition="1388">level. We define three feature categories, based on the lexicon used: 3http://www.nltk.org/api/nltk.stem. html 187 Task-A Task-B Dev Train Test Dev Train Test Positive 57.09 % 62.06% 59.49% 34.76% 37.59% 39.01% Negative 37.89% 33.01% 35.31% 20.56% 15.06% 17.15% Neutral 5.02% 4.93% 5.21% 44.68% 47.36% 43.84% All 1,135 9,451 10,681 1,654 9,684 8,987 Table 2: Class size distribution for all the three sets for both Task-A and Task-B. • Subjectivity: number of words mapped to ”positive” from the MPQA Subjectivity lexicon (Wilson et al., 2005). • Hybrid Lexicon: We combine the Sentiment140 lexicon (Mohammad et al., 2013) with the Bing Liu’s bag of positive and negative words (Hu and Liu, 2004) to create a dictionary in which each token is assigned a sentiment. • Token weight: we use the SentiWordNet lexicon (Baccianella et al., 2010) to define this feature. SentiWordNet contains positive, negative and objective scores between 0 and 1 for all senses in WordNet. Based on this sense level annotation, we first map each token to its weight in this lexicon and then the sum of all these weights was used as the tweet weight. Furthermore, in order to take into account the presence of negative words, which modify the p</context>
</contexts>
<marker>Mohammad, Kiritchenko, Zhu, 2013</marker>
<rawString>Saif Mohammad, Svetlana Kiritchenko, and Xiaodan Zhu. 2013. NRC-Canada: Building the State-ofthe-Art in Sentiment Analysis of Tweets. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 321–327, Atlanta, Georgia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Sara Rosenthal</author>
<author>Zornitsa Kozareva</author>
<author>Veselin Stoyanov</author>
<author>Alan Ritter</author>
<author>Theresa Wilson</author>
</authors>
<title>SemEval-2013 Task 2: Sentiment Analysis in Twitter.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>312--320</pages>
<location>Atlanta, Georgia, USA.</location>
<contexts>
<context position="3074" citStr="Nakov et al., 2013" startWordPosition="500" endWordPosition="503">em for sentiment analysis of Twitter shared task in SemEval 2014. Our system takes as input an arbitrary tweet and assigns it to one of the following classes that best reflects its sentiment: positive, negative or neutral. While positive and negative tweets are subjective, neutral class encompasses not only objective tweets but also subjective tweets that does not contain any ”polar” emotion. Our classifier was developed as an undergrad course project but later pursued as a research topic. Our training, development and testing experiments were performed on data sets published in SemEval 2013 (Nakov et al., 2013). Motivated with its performance, we participated in SemEval 2014 Task 9 (Rosenthal et al., 2014). Our approach includes an extensive usage of offthe-shelf resources that have been developed for conducting NLP on social media text. Our original aim was enhancement of the task-A. Moreover, we adapted our framework and introduced new features for task-B and participated in both shared tasks. We reached an F-score of 83.3% in Task-A and an F-score of 65.57% in Task-B. That placed us in the 4th rank in the task-A and 15th rank in the task-B. Our approach includes an extensive usage of off-the-shel</context>
</contexts>
<marker>Nakov, Rosenthal, Kozareva, Stoyanov, Ritter, Wilson, 2013</marker>
<rawString>Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva, Veselin Stoyanov, Alan Ritter, and Theresa Wilson. 2013. SemEval-2013 Task 2: Sentiment Analysis in Twitter. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 312– 320, Atlanta, Georgia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olutobi Owoputi</author>
<author>Brendan O’Connor</author>
<author>Chris Dyer</author>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Noah A Smith</author>
</authors>
<title>Improved Part-of-Speech Tagging for Online Conversational Text with Word Clusters.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>380--390</pages>
<location>Atlanta,</location>
<marker>Owoputi, O’Connor, Dyer, Gimpel, Schneider, Smith, 2013</marker>
<rawString>Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A. Smith. 2013. Improved Part-of-Speech Tagging for Online Conversational Text with Word Clusters. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 380–390, Atlanta, Georgia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Pak</author>
<author>Patrick Paroubek</author>
</authors>
<title>Twitter as a Corpus for Sentiment Analysis and Opinion Mining.</title>
<date>2010</date>
<booktitle>In Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC’10),</booktitle>
<pages>1320--1326</pages>
<location>Valletta,</location>
<contexts>
<context position="2036" citStr="Pak and Paroubek, 2010" startWordPosition="333" endWordPosition="336">iently used for marketing. However, with the increasing amount of tweets posted on a daily basis, it is challenging and expensive to manually analyze them and locate the meaningful ones. There has been a body of recent work to automatically learn the public senThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 1http://twitter.com timents from tweets using natural language processing techniques (Pang and Lee, 2008; Jansen et al., 2009; Pak and Paroubek, 2010; Tang et al., 2014). However, the task of sentiment analysis of tweets in their free format is harder than that of any well-structured document. Tweet messages usually contain different kinds of orthographic errors such as the use of special and decorative characters, letter or word duplication, extra punctuation, as well as the use of special abbreviations. In this paper, we present our machine learning based system for sentiment analysis of Twitter shared task in SemEval 2014. Our system takes as input an arbitrary tweet and assigns it to one of the following classes that best reflects its </context>
</contexts>
<marker>Pak, Paroubek, 2010</marker>
<rawString>Alexander Pak and Patrick Paroubek. 2010. Twitter as a Corpus for Sentiment Analysis and Opinion Mining. In Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC’10), pages 1320–1326, Valletta, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion Mining and Sentiment Analysis,</title>
<date>2008</date>
<volume>2</volume>
<publisher>Now Publishers Inc.</publisher>
<contexts>
<context position="1991" citStr="Pang and Lee, 2008" startWordPosition="325" endWordPosition="328">roduct to be like. Such data can be efficiently used for marketing. However, with the increasing amount of tweets posted on a daily basis, it is challenging and expensive to manually analyze them and locate the meaningful ones. There has been a body of recent work to automatically learn the public senThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 1http://twitter.com timents from tweets using natural language processing techniques (Pang and Lee, 2008; Jansen et al., 2009; Pak and Paroubek, 2010; Tang et al., 2014). However, the task of sentiment analysis of tweets in their free format is harder than that of any well-structured document. Tweet messages usually contain different kinds of orthographic errors such as the use of special and decorative characters, letter or word duplication, extra punctuation, as well as the use of special abbreviations. In this paper, we present our machine learning based system for sentiment analysis of Twitter shared task in SemEval 2014. Our system takes as input an arbitrary tweet and assigns it to one of </context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion Mining and Sentiment Analysis, volume 2. Now Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Rosenthal</author>
<author>Preslav Nakov</author>
<author>Alan Ritter</author>
<author>Veselin Stoyanov</author>
</authors>
<title>SemEval-2014 Task 9: Sentiment Analysis in Twitter.</title>
<date>2014</date>
<booktitle>In Proceedings of the Eighth International Workshop on Semantic Evaluation (SemEval’14),</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="3171" citStr="Rosenthal et al., 2014" startWordPosition="515" endWordPosition="518">n arbitrary tweet and assigns it to one of the following classes that best reflects its sentiment: positive, negative or neutral. While positive and negative tweets are subjective, neutral class encompasses not only objective tweets but also subjective tweets that does not contain any ”polar” emotion. Our classifier was developed as an undergrad course project but later pursued as a research topic. Our training, development and testing experiments were performed on data sets published in SemEval 2013 (Nakov et al., 2013). Motivated with its performance, we participated in SemEval 2014 Task 9 (Rosenthal et al., 2014). Our approach includes an extensive usage of offthe-shelf resources that have been developed for conducting NLP on social media text. Our original aim was enhancement of the task-A. Moreover, we adapted our framework and introduced new features for task-B and participated in both shared tasks. We reached an F-score of 83.3% in Task-A and an F-score of 65.57% in Task-B. That placed us in the 4th rank in the task-A and 15th rank in the task-B. Our approach includes an extensive usage of off-the-shelf resources that have been developed for conducting NLP on social media text. That includes the T</context>
</contexts>
<marker>Rosenthal, Nakov, Ritter, Stoyanov, 2014</marker>
<rawString>Sara Rosenthal, Preslav Nakov, Alan Ritter, and Veselin Stoyanov. 2014. SemEval-2014 Task 9: Sentiment Analysis in Twitter. In Proceedings of the Eighth International Workshop on Semantic Evaluation (SemEval’14), Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Duyu Tang</author>
<author>Furu Wei</author>
<author>Nan Yang</author>
<author>Ming Zhou</author>
<author>Ting Liu</author>
<author>Bing Qin</author>
</authors>
<title>Learning SentimentSpecific Word Embedding for Twitter Sentiment Classification.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1555--1565</pages>
<location>Baltimore, Maryland.</location>
<contexts>
<context position="2056" citStr="Tang et al., 2014" startWordPosition="337" endWordPosition="340">g. However, with the increasing amount of tweets posted on a daily basis, it is challenging and expensive to manually analyze them and locate the meaningful ones. There has been a body of recent work to automatically learn the public senThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 1http://twitter.com timents from tweets using natural language processing techniques (Pang and Lee, 2008; Jansen et al., 2009; Pak and Paroubek, 2010; Tang et al., 2014). However, the task of sentiment analysis of tweets in their free format is harder than that of any well-structured document. Tweet messages usually contain different kinds of orthographic errors such as the use of special and decorative characters, letter or word duplication, extra punctuation, as well as the use of special abbreviations. In this paper, we present our machine learning based system for sentiment analysis of Twitter shared task in SemEval 2014. Our system takes as input an arbitrary tweet and assigns it to one of the following classes that best reflects its sentiment: positive,</context>
</contexts>
<marker>Tang, Wei, Yang, Zhou, Liu, Qin, 2014</marker>
<rawString>Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting Liu, and Bing Qin. 2014. Learning SentimentSpecific Word Embedding for Twitter Sentiment Classification. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1555–1565, Baltimore, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing Contextual Polarity in Phraselevel Sentiment Analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on human language technology and empirical methods in natural language processing,</booktitle>
<pages>347--354</pages>
<contexts>
<context position="8397" citStr="Wilson et al., 2005" startWordPosition="1372" endWordPosition="1375">y available sentiment resources to explicitly define the polarity at a token-level. We define three feature categories, based on the lexicon used: 3http://www.nltk.org/api/nltk.stem. html 187 Task-A Task-B Dev Train Test Dev Train Test Positive 57.09 % 62.06% 59.49% 34.76% 37.59% 39.01% Negative 37.89% 33.01% 35.31% 20.56% 15.06% 17.15% Neutral 5.02% 4.93% 5.21% 44.68% 47.36% 43.84% All 1,135 9,451 10,681 1,654 9,684 8,987 Table 2: Class size distribution for all the three sets for both Task-A and Task-B. • Subjectivity: number of words mapped to ”positive” from the MPQA Subjectivity lexicon (Wilson et al., 2005). • Hybrid Lexicon: We combine the Sentiment140 lexicon (Mohammad et al., 2013) with the Bing Liu’s bag of positive and negative words (Hu and Liu, 2004) to create a dictionary in which each token is assigned a sentiment. • Token weight: we use the SentiWordNet lexicon (Baccianella et al., 2010) to define this feature. SentiWordNet contains positive, negative and objective scores between 0 and 1 for all senses in WordNet. Based on this sense level annotation, we first map each token to its weight in this lexicon and then the sum of all these weights was used as the tweet weight. Furthermore, i</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing Contextual Polarity in Phraselevel Sentiment Analysis. In Proceedings of the conference on human language technology and empirical methods in natural language processing, pages 347–354.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>