<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000009">
<title confidence="0.860191">
CELI: An Experiment with Cross Language Textual Entailment
</title>
<author confidence="0.5866">
Milen Kouylekov
</author>
<address confidence="0.674835">
Celi S.R.L.
via San Quintino 31
Torino, Italy
</address>
<email confidence="0.994617">
kouylekov@celi.it
</email>
<note confidence="0.483636">
Luca Dini
Celi S.R.L.
</note>
<address confidence="0.8196345">
via San Quintino 31
Torino, Italy
</address>
<email confidence="0.991764">
dini@celi.it
</email>
<note confidence="0.676356">
Alessio Bosca
Celi S.R.L.
</note>
<address confidence="0.7881655">
via San Quintino 31
Torino, Italy
</address>
<email confidence="0.990096">
bosca@celi.it
</email>
<note confidence="0.470459">
Marco Trevisan
Celi S.R.L.
</note>
<address confidence="0.826043">
via San Quintino 31
Torino, Italy
</address>
<email confidence="0.996488">
trevisan@celi.it
</email>
<sectionHeader confidence="0.995884" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.332859333333333">
This paper presents CELI’s participation in the
SemEval Cross-lingual Textual Entailment for
Content Synchronization task.
</bodyText>
<sectionHeader confidence="0.997881" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999517">
The Cross-Lingual Textual Entailment task (CLTE)
is a new task that addresses textual entailment (TE)
(Bentivogli et. al., 2011), targeting the cross-
lingual content synchronization scenario proposed
in (Mehdad et. al., 2011) and (Negri et. al., 2011).
The task has interesting application scenarios that
can be investigated. Some of them are content syn-
chronization and cross language query alignment.
The task is defined by the organizers as follows:
Given a pair of topically related text fragments (T1
and T2) in different languages, the CLTE task con-
sists of automatically annotating it with one of the
following entailment judgments:
</bodyText>
<listItem confidence="0.999258">
• Bidirectional: the two fragments entail each
other (semantic equivalence)
• Forward: unidirectional entailment from T1 to
T2
• Backward: unidirectional entailment from T2
to T1
• No Entailment: there is no entailment between
T1 and T2
</listItem>
<bodyText confidence="0.77966025">
In this task, both T1 and T2 are assumed to be
TRUE statements; hence in the dataset there are no
contradictory pairs.
Example for Spanish English pairs:
</bodyText>
<listItem confidence="0.821748">
• bidirectional
</listItem>
<bodyText confidence="0.8717625">
Mozart naci en la ciudad de Salzburgo
Mozart was born in Salzburg.
</bodyText>
<listItem confidence="0.947038">
• forward
</listItem>
<bodyText confidence="0.957001666666667">
Mozart naci en la ciudad de Salzburgo
Mozart was born on the 27th January 1756 in
Salzburg.
</bodyText>
<listItem confidence="0.8636">
• backward
</listItem>
<bodyText confidence="0.992735">
Mozart naci el 27 de enero de 1756 en
Salzburgo
Mozart was born in 1756 in the city of Salzburg
</bodyText>
<listItem confidence="0.659025">
• no entailment
</listItem>
<bodyText confidence="0.99934725">
Mozart naci el 27 de enero de 1756 en
Salzburgo
Mozart was born to Leopold and Anna Maria
Pertl Mozart.
</bodyText>
<sectionHeader confidence="0.850243" genericHeader="method">
2 Our Approach to CLTE
</sectionHeader>
<bodyText confidence="0.998798571428571">
In our participation in the 2012 SemEval Cross-
lingual Textual Entailment for Content Synchroniza-
tion task (Negri et. al., 2012) we have developed
an approach based on cross-language text similarity.
We have modified our cross-language query similar-
ity system TLike to handle longer texts.
Our approach is based on four main resources:
</bodyText>
<listItem confidence="0.997169666666667">
• A system for Natural Language Processing able
to perform for each relevant language basic
tasks such as part of speech disambiguation,
lemmatization and named entity recognition.
• A set of word based bilingual translation mod-
ules.
</listItem>
<page confidence="0.955293">
696
</page>
<note confidence="0.4801545">
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 696–700,
Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics
</note>
<listItem confidence="0.999246333333333">
• A semantic component able to associate a se-
mantic vectorial representation to words.
• We use Wikipedia as multilingual corpus.
</listItem>
<bodyText confidence="0.99626606">
NLP modules are described in (Bosca and Dini,
2008), and will be no further detailed here.
Word-based translation modules are composed by
a bilingual lexicon look-up component coupled with
a vector based translation filter, such as the one de-
scribed in (Curtoni and Dini, 2008). In the context of
the present experiments, such a filters has been de-
activated, which means that for any input word the
component will return the set of all possible transla-
tions. For unavailable pairs, we make use of trian-
gular translation (Kraaij, 2003).
As for the semantic component we experimented
with a corpus-based distributional approach capable
of detecting the interrelation between different terms
in a corpus; the strategy we adopted is similar to La-
tent Semantic Analysis (Deerwester et. al., 1990)
although it uses a less expensive computational solu-
tion based on the Random Projection algorithm (Lin
et. al., 2003) and (Bingham et. al., 2001). Different
works debate on similar issues: (Turney, 2001) uses
LSA in order to solve synonymy detection questions
from the well-known TOEFL test while the method
presented by (Inkpen, 2001) or by (Baroni and Bisi,
2001) proposes the use of the Web as a corpus to
compute mutual information scores between candi-
date terms.
More technically, Random Indexing exploits an
algebraic model in order to represent the seman-
tics of terms in a Nth dimensional space (a vector
of length N); approaches falling into this category,
actually create a Terms By Contexts matrix where
each cell represents the degree of memberships of a
given term to the different contexts. The algorithm
assigns a random signature to each context (a highly
sparse vector of length N, with few, randomly cho-
sen, non-zero elements) and then generates the vec-
tor space model by performing a statistical analysis
of the documents in the domain corpus and by ac-
cumulating on terms rows all the signatures of the
contexts where terms appear.
According to this approach if two different terms
have a similar meaning they should appear in similar
contexts (within the same documents or surrounded
by the same words), resulting into close coordinates
in the so generated semantic space.
In our case study semantic vectors have been gen-
erated taking as corpus the set of metadata available
via the CACAO project (Cacao Project, 2007) fed-
eration (about 6 millions records). After processing
for each word in the corpus we have:
</bodyText>
<listItem confidence="0.9688672">
• A vector of float from 0 to 1 representing its
contextual meaning;
• A set of neighbors terms selected among the
terms with a higher semantic similarity, calcu-
lated as cosine distance among vectors.
</listItem>
<bodyText confidence="0.998170363636364">
We use Wikipedia as a corpus for calculating
word statistics in different languages. We have in-
dexed using Lucene1 the English, Italian, French,
German, Spanish distributions of the resource.
The basic idea behind our algorithm is to detect
the probability for two texts to be one a translation
of the other. In the simple case we expect that if all
the words in text TS have a translation in text TT and
if TS and TT have the same number of terms, then
TS and TT are entailed. Things are of course more
complex than this, due to the following facts:
</bodyText>
<listItem confidence="0.995978636363636">
• The presence of compound words make the
constraints on cardinality of search terms not
feasible (e.g. the Italian Carta di Credito vs.
the German KreditCarte).
• One or more words in TS could be absent from
translation dictionaries.
• One or more words in TS could be present
in the translation dictionaries, but contextually
correct translation might be missing.
• There might be items which do not need to be
translated, notably Named Entities.
</listItem>
<bodyText confidence="0.999910857142857">
The first point, compounding, is only partially
an obstacle. NLP technology developed during
CACAO Project, which adopted translation dictio-
naries, deals with compound words both in terms
of identification and translation. Thus the Italian
”Carta di Credito” would be recognized and cor-
rectly translated into ”KreditCarte”. So, in an ideal
</bodyText>
<footnote confidence="0.997855">
1http://lucene.apache.org
</footnote>
<page confidence="0.994117">
697
</page>
<bodyText confidence="0.999926927710844">
word, the cardinality principle could be considered
strict. In reality, however, there are many com-
pounding phenomena which are not covered by our
dictionaries, and this forces us to consider that a mis-
match in text term cardinality decrease the probabil-
ity that the two translations are translation of each
other, without necessarily setting it to zero.
Concerning the second aspect, the absence of
source (T1) words in translation dictionaries, it is
dealt with by accessing the semantic repository de-
scribed in the previous section. We first obtain the
list of neighbor terms for the untranslatable source
word. This list is likely to contain many words that
have one or more translations. For each translation,
again, we consult our semantic repository and we
obtain its semantic vector.
Finally, we compose all vectors of all available
translations and we search in the target text (T2) for
the word whose semantic vector best matches the
composed one (cosine distance). Of course we can-
not assume that the best matching vector is a transla-
tion of the original word, but we can use the distance
between the two vectors as a further weight for de-
ciding whether the two texts are translations one of
the other.
There are of course cases when the source word
is correctly missing in the source dictionary. This
is typically the case of most named entities, such
as geographical and person names. These entities
should be appropriately recognized and searched as
exact matches in the target text, thus by-passing any
dictionary look-up and any semantic based match-
ing. Notice that the recognition of named entities
it is not just a matter of generalizing the statement
according to which ”if something is not in the dic-
tionaries, it is a named entity”. Indeed there are well
known cases where the named entity is homograph
with common words (e.g. the French author ”La
Fontaine”), and in these cases we must detect them
in order to avoid the rejection of likely translation
pairs. In other words we must avoid that the two
texts ”La fontaine fables” and ”La Fontaine fav-
ole” are rejected as translation pairs, just by virtue
of the fact that ”La fontaine” is treated as a com-
mon word, thus generating the Italian translation”La
fontana”. Fortunately CACAO disposes of a quite
accurate subsystem for recognizing named entities
in texts, mixing standard NLP technologies with sta-
tistical processing and other corpus-oriented heuris-
tics.
We concentrated our work on handling cases
where two texts are candidates to be mutual trans-
lations, but one or more words receive a translation
which is not contained in the target text. Typically
these cases are a symptom of a non-optimal quality
in translation dictionaries: the lexicographer prob-
ably did not consider some translation candidate.
To address this problem we have created a solution
based on a weighting scheme. For each word of the
source language we assign a weight that reflects its
importance to the semantic interpretation of the text.
We define a matchweight of a word using the for-
mula represented in Figure 2.In this formula wis is
a word from the source text, wkt is a word from the
target text, w is a word in the source language and
trans is a boolean function that searches in the dic-
tionary for translations between two words.
The matchweight is relevant to the matching of a
translation of a word from the source with one of
the words of the target. If the system finds a direct
correspondence the weight is 0. If the match was
made using random indexing the weight is inverse
to the cosine similarity between the vectors.
In order to make an approximation of the signif-
icance of the word to the meaning of the phrase we
have used as its cost the inverse document frequency
(IDF) of the word calculated using Wikipedia as a
corpus. IDF is a most popular measure (a measure
commonly used in Information Retrieval) for calcu-
lating the importance of a word to a text. If N is the
number of documents in a text collection and Nw is
the number of documents of the collection that con-
tain w then the IDF of w is given by the formula:
</bodyText>
<equation confidence="0.9628945">
weight(wis) = idf(w) = log( N ) (2)
Nw
</equation>
<bodyText confidence="0.999077">
Using the matchweight and weight we define the
matchscore of a source target pair as:
</bodyText>
<equation confidence="0.995104">
� matchweigth(wis)
matchscore(Ts7Tt) = Eweight(wis) (3)
</equation>
<bodyText confidence="0.984183">
If all the words of the source text have a transla-
tion in the target text the score is 0. If none is found
the score is 1. We have calculated the scores for each
</bodyText>
<page confidence="0.975328">
698
</page>
<figure confidence="0.518496666666667">
matchweight(wis) = � 0 ]wkt trans(wis) = wkt (1)
�� w * (wis) * (1 − d) ]w &amp;wkt distance(wis, w) = d&amp;trans(w) = wkt
�� w * (wis) otherwise
</figure>
<figureCaption confidence="0.996962">
Figure 1: Match Weight of a Word
</figureCaption>
<bodyText confidence="0.9820055">
pair taking t1 as a source and t2 as a target and vice
versa.
</bodyText>
<sectionHeader confidence="0.993157" genericHeader="method">
3 Systems
</sectionHeader>
<bodyText confidence="0.999681421052631">
We have submitted four runs in the SemEval CLTE
challenge. We used the NaiveBayse algorithm im-
plemented in Mallet2 to create a classifier that will
produce the output for each of the four categories
Forward, Backward, Bidirectional and No Entail-
ment.
System 1 As our first system we have created a
binary classifier in the classical RTE (Bentivogli et.
al., 2011) classification (YES &amp; NO) for each direc-
tion Forward and Backward. We assigned the Bidi-
rectional category if both classifiers returned YES.
As features the classifiers used only the match scores
obtained for the corresponding direction as one and
only numeric feature.
System 2 For the second system we trained a clas-
sifier using all four categories as output. Apart of the
scores obtained matching the texts in both directions
we have included also a set of eight simple surface
measures. Some of these are:
</bodyText>
<listItem confidence="0.999164333333333">
• The length of the two texts.
• The number of common words without transla-
tions.
• The cosine similarity between the tokens of the
two texts without translation.
• Levenshtein distance between the texts.
</listItem>
<bodyText confidence="0.9729674">
System 3 For the third system we trained a classi-
fier using all four categories as output. We used as
features scores obtained matching the texts in both
directions without the surface features used in the
System 2.
</bodyText>
<footnote confidence="0.853573">
2http://mallet.cs.umass.edu/
</footnote>
<bodyText confidence="0.993751">
System 4 In the last system we trained a classifier
using all four categories as output. We used as fea-
tures the simple surface measures used in System 2.
The results obtained are shown in Table 1.
</bodyText>
<sectionHeader confidence="0.99661" genericHeader="conclusions">
4 Analysis
</sectionHeader>
<bodyText confidence="0.99994025">
Analyzing the results of our participation we have
reached several important conclusions.
The dataset provided by the organizers presented
a significant challenge for our system which was
adapted from a query similarity approach. The re-
sults obtained demonstrate that only a similarity
based approach will not provide good results for this
task. This fact is also confirmed by the poor perfor-
mance of the simple similarity measures by them-
selves (System 4) and by their contribution to the
combined run (System 2).
The poor performance of our system can be par-
tially explained also by the small dimensions of the
cross-language dictionaries we used. Expanding
them with more words and phrases can potentially
increase our results.
The classifier with four categories clearly outper-
forms the two directional one (System 1 vs. System
3).
Overall we are not satisfied with our experi-
ment. A radically new approach is needed to address
the problem of Cross-Language Textual Entailment,
which our similarity based system could not model
correctly.
In the future we intend to integrate our approach
in our RTE open source system EDITS (Kouylekov
et. al., 2011) (Kouylekov and Negri, 2010) available
at http://edits.sf.net.
</bodyText>
<sectionHeader confidence="0.998297" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.997469666666667">
This work has been partially supported by the
ECfunded project Galateas (CIP-ICT PSP-2009-3-
250430).
</bodyText>
<page confidence="0.996517">
699
</page>
<table confidence="0.9994546">
SPA-ENG ITA-ENG FRA-ENG DEU-ENG
System 1 0.276 0.278 0.278 0.280
System 2 0.336 0.336 0.300 0.352
System 3 0.322 0.334 0.298 0.350
System 4 0.268 0.280 0.280 0.274
</table>
<tableCaption confidence="0.999874">
Table 1: Results obtained.
</tableCaption>
<sectionHeader confidence="0.994336" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998927196428572">
Baroni M., Bisi S. 2004. Using cooccurrence statistics
and the web to discover synonyms in technical lan-
guage In Proceedings of LREC 2004
Bentivogli L., Clark P., Dagan I., Dang H, Giampic-
colo D. 2011. The Seventh PASCAL Recognizing
Textual Entailment Challenge In Proceedings of TAC
2011
Bingham E., Mannila H. 2001. Random projection in
dimensionality reduction: Applications to image and
text data. In Knowledge Discovery and Data Mining,
ACM Press pages 245250
Bosca A., Dini L. 2008. Query expansion via library
classification system. In CLEF 2008. Springer Verlag,
LNCS
Cacao Project CACAO - project supported by the eCon-
tentplus Programme of the European Commission.
http://www.cacaoproject.eu/
Curtoni P., Dini L. 2006. Celi participation at clef 2006
Cross language delegated search. In CLEF2006 Work-
ing notes.
Deerwester S., Dumais S.T., Furnas G.W., Landauer T.K.,
Harshman R. 1990. Indexing by latent semantic anal-
ysis. Journal of the American Society for Information
Science 41 391407
Inkpen D. 2007. A statistical model for near-synonym
choice. ACM Trans. Speech Language Processing
4(1)
Kraaij W. 2003. Exploring transitive translation meth-
ods. In Vries, A.P.D., ed.: Proceedings of DIR 2003.
Kouylekov M., Negri M. An Open-Source Package for
Recognizing Textual Entailment. 48th Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2010) ,Uppsala, Sweden. July 11-16, 2010
Kouylekov M., Bosca A., Dini L. 2011. EDITS 3.0 at
RTE-7. Proceedings of the Seventh Recognizing Tex-
tual Entailment Challenge (2011).
Lin J., Gunopulos D. 2003. Dimensionality reduction
by random projection and latent semantic indexing. In
proceedings of the Text Mining Workshop, at the 3rd
SIAM International Conference on Data Mining.
Mehdad Y.,Negri M., Federico M.. 2011. Using Paral-
lel Corpora for Cross-lingual Textual Entailment. In
Proceedings of ACL-HLT 2011.
Negri M., Bentivogli L., Mehdad Y., Giampiccolo D.,
Marchetti A. 2011. Divide and Conquer: Crowd-
sourcing the Creation of Cross-Lingual Textual Entail-
ment Corpora. In Proceedings of EMNLP 2011.
Negri M., Marchetti A., Mehdad Y., Bentivogli L., Gi-
ampiccolo D. Semeval-2012 Task 8: Cross-lingual
Textual Entailment for Content Synchronization. In
Proceedings of the 6th International Workshop on Se-
mantic Evaluation (SemEval 2012). 2012.
Turney P.D. 2001. Mining the web for synonyms: Pmi-
ir versus lsa on toefl. In EMCL 01: Proceedings of
the 12th European Conference on Machine Learning,
London, UK, Springer-Verlag pages 491502
</reference>
<page confidence="0.99641">
700
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.030396">
<title confidence="0.8620675">CELI: An Experiment with Cross Language Textual Entailment Milen</title>
<author confidence="0.841392">Celi</author>
<affiliation confidence="0.90402">via San Quintino</affiliation>
<address confidence="0.811356">Torino,</address>
<email confidence="0.998795">kouylekov@celi.it</email>
<author confidence="0.7615225">Luca Celi S R L</author>
<affiliation confidence="0.869288">via San Quintino</affiliation>
<address confidence="0.873832">Torino,</address>
<email confidence="0.997992">dini@celi.it</email>
<author confidence="0.846264">Alessio</author>
<affiliation confidence="0.828508">Celi via San Quintino</affiliation>
<address confidence="0.839105">Torino,</address>
<email confidence="0.998898">bosca@celi.it</email>
<author confidence="0.733825">Marco Celi</author>
<affiliation confidence="0.944124">via San Quintino</affiliation>
<address confidence="0.830451">Torino,</address>
<email confidence="0.999317">trevisan@celi.it</email>
<abstract confidence="0.85526775">This paper presents CELI’s participation in the SemEval Cross-lingual Textual Entailment for Content Synchronization task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Baroni</author>
<author>S Bisi</author>
</authors>
<title>Using cooccurrence statistics and the web to discover synonyms in technical language</title>
<date>2004</date>
<booktitle>In Proceedings of LREC</booktitle>
<marker>Baroni, Bisi, 2004</marker>
<rawString>Baroni M., Bisi S. 2004. Using cooccurrence statistics and the web to discover synonyms in technical language In Proceedings of LREC 2004</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Bentivogli</author>
<author>P Clark</author>
<author>I Dagan</author>
<author>H Dang</author>
<author>D Giampiccolo</author>
</authors>
<title>The Seventh PASCAL Recognizing Textual Entailment Challenge In</title>
<date>2011</date>
<booktitle>Proceedings of TAC</booktitle>
<marker>Bentivogli, Clark, Dagan, Dang, Giampiccolo, 2011</marker>
<rawString>Bentivogli L., Clark P., Dagan I., Dang H, Giampiccolo D. 2011. The Seventh PASCAL Recognizing Textual Entailment Challenge In Proceedings of TAC 2011</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Bingham</author>
<author>H Mannila</author>
</authors>
<title>Random projection in dimensionality reduction: Applications to image and text data.</title>
<date>2001</date>
<booktitle>In Knowledge Discovery and Data Mining,</booktitle>
<pages>245250</pages>
<publisher>ACM Press</publisher>
<marker>Bingham, Mannila, 2001</marker>
<rawString>Bingham E., Mannila H. 2001. Random projection in dimensionality reduction: Applications to image and text data. In Knowledge Discovery and Data Mining, ACM Press pages 245250</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bosca</author>
<author>L Dini</author>
</authors>
<title>Query expansion via library classification system.</title>
<date>2008</date>
<booktitle>In CLEF</booktitle>
<publisher>Springer Verlag, LNCS</publisher>
<contexts>
<context position="2895" citStr="Bosca and Dini, 2008" startWordPosition="452" endWordPosition="455">is based on four main resources: • A system for Natural Language Processing able to perform for each relevant language basic tasks such as part of speech disambiguation, lemmatization and named entity recognition. • A set of word based bilingual translation modules. 696 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 696–700, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics • A semantic component able to associate a semantic vectorial representation to words. • We use Wikipedia as multilingual corpus. NLP modules are described in (Bosca and Dini, 2008), and will be no further detailed here. Word-based translation modules are composed by a bilingual lexicon look-up component coupled with a vector based translation filter, such as the one described in (Curtoni and Dini, 2008). In the context of the present experiments, such a filters has been deactivated, which means that for any input word the component will return the set of all possible translations. For unavailable pairs, we make use of triangular translation (Kraaij, 2003). As for the semantic component we experimented with a corpus-based distributional approach capable of detecting the </context>
</contexts>
<marker>Bosca, Dini, 2008</marker>
<rawString>Bosca A., Dini L. 2008. Query expansion via library classification system. In CLEF 2008. Springer Verlag, LNCS</rawString>
</citation>
<citation valid="false">
<title>Cacao Project CACAO - project supported by the eContentplus Programme of the European Commission.</title>
<note>http://www.cacaoproject.eu/</note>
<marker></marker>
<rawString>Cacao Project CACAO - project supported by the eContentplus Programme of the European Commission. http://www.cacaoproject.eu/</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Curtoni</author>
<author>L Dini</author>
</authors>
<title>Celi participation at clef 2006 Cross language delegated search.</title>
<date>2006</date>
<booktitle>In CLEF2006 Working notes.</booktitle>
<marker>Curtoni, Dini, 2006</marker>
<rawString>Curtoni P., Dini L. 2006. Celi participation at clef 2006 Cross language delegated search. In CLEF2006 Working notes.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Deerwester</author>
<author>S T Dumais</author>
<author>G W Furnas</author>
<author>T K Landauer</author>
<author>R Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society for Information Science</journal>
<volume>41</volume>
<pages>391407</pages>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>Deerwester S., Dumais S.T., Furnas G.W., Landauer T.K., Harshman R. 1990. Indexing by latent semantic analysis. Journal of the American Society for Information Science 41 391407</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Inkpen</author>
</authors>
<title>A statistical model for near-synonym choice.</title>
<date>2007</date>
<journal>ACM Trans. Speech Language Processing</journal>
<volume>4</volume>
<issue>1</issue>
<marker>Inkpen, 2007</marker>
<rawString>Inkpen D. 2007. A statistical model for near-synonym choice. ACM Trans. Speech Language Processing 4(1)</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Kraaij</author>
</authors>
<title>Exploring transitive translation methods.</title>
<date>2003</date>
<booktitle>In Vries, A.P.D., ed.: Proceedings of DIR 2003.</booktitle>
<contexts>
<context position="3378" citStr="Kraaij, 2003" startWordPosition="534" endWordPosition="535"> vectorial representation to words. • We use Wikipedia as multilingual corpus. NLP modules are described in (Bosca and Dini, 2008), and will be no further detailed here. Word-based translation modules are composed by a bilingual lexicon look-up component coupled with a vector based translation filter, such as the one described in (Curtoni and Dini, 2008). In the context of the present experiments, such a filters has been deactivated, which means that for any input word the component will return the set of all possible translations. For unavailable pairs, we make use of triangular translation (Kraaij, 2003). As for the semantic component we experimented with a corpus-based distributional approach capable of detecting the interrelation between different terms in a corpus; the strategy we adopted is similar to Latent Semantic Analysis (Deerwester et. al., 1990) although it uses a less expensive computational solution based on the Random Projection algorithm (Lin et. al., 2003) and (Bingham et. al., 2001). Different works debate on similar issues: (Turney, 2001) uses LSA in order to solve synonymy detection questions from the well-known TOEFL test while the method presented by (Inkpen, 2001) or by </context>
</contexts>
<marker>Kraaij, 2003</marker>
<rawString>Kraaij W. 2003. Exploring transitive translation methods. In Vries, A.P.D., ed.: Proceedings of DIR 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kouylekov</author>
<author>M Negri</author>
</authors>
<title>An Open-Source Package for Recognizing Textual Entailment.</title>
<date>2010</date>
<booktitle>48th Annual Meeting of the Association for Computational Linguistics (ACL 2010)</booktitle>
<location>Uppsala, Sweden.</location>
<marker>Kouylekov, Negri, 2010</marker>
<rawString>Kouylekov M., Negri M. An Open-Source Package for Recognizing Textual Entailment. 48th Annual Meeting of the Association for Computational Linguistics (ACL 2010) ,Uppsala, Sweden. July 11-16, 2010</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kouylekov</author>
<author>A Bosca</author>
<author>L Dini</author>
</authors>
<date>2011</date>
<booktitle>EDITS 3.0 at RTE-7. Proceedings of the Seventh Recognizing Textual Entailment Challenge</booktitle>
<marker>Kouylekov, Bosca, Dini, 2011</marker>
<rawString>Kouylekov M., Bosca A., Dini L. 2011. EDITS 3.0 at RTE-7. Proceedings of the Seventh Recognizing Textual Entailment Challenge (2011).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lin</author>
<author>D Gunopulos</author>
</authors>
<title>Dimensionality reduction by random projection and latent semantic indexing.</title>
<date>2003</date>
<booktitle>In proceedings of the Text Mining Workshop, at the 3rd SIAM International Conference on Data Mining.</booktitle>
<marker>Lin, Gunopulos, 2003</marker>
<rawString>Lin J., Gunopulos D. 2003. Dimensionality reduction by random projection and latent semantic indexing. In proceedings of the Text Mining Workshop, at the 3rd SIAM International Conference on Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Mehdad</author>
<author>M Negri</author>
<author>M Federico</author>
</authors>
<title>Using Parallel Corpora for Cross-lingual Textual Entailment.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL-HLT</booktitle>
<marker>Mehdad, Negri, Federico, 2011</marker>
<rawString>Mehdad Y.,Negri M., Federico M.. 2011. Using Parallel Corpora for Cross-lingual Textual Entailment. In Proceedings of ACL-HLT 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Negri</author>
<author>L Bentivogli</author>
<author>Y Mehdad</author>
<author>D Giampiccolo</author>
<author>A Marchetti</author>
</authors>
<title>Divide and Conquer: Crowdsourcing the Creation of Cross-Lingual Textual Entailment Corpora.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<marker>Negri, Bentivogli, Mehdad, Giampiccolo, Marchetti, 2011</marker>
<rawString>Negri M., Bentivogli L., Mehdad Y., Giampiccolo D., Marchetti A. 2011. Divide and Conquer: Crowdsourcing the Creation of Cross-Lingual Textual Entailment Corpora. In Proceedings of EMNLP 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Negri</author>
<author>A Marchetti</author>
<author>Y Mehdad</author>
<author>L Bentivogli</author>
<author>D Giampiccolo</author>
</authors>
<title>Semeval-2012 Task 8: Cross-lingual Textual Entailment for Content Synchronization.</title>
<date>2012</date>
<booktitle>In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval</booktitle>
<marker>Negri, Marchetti, Mehdad, Bentivogli, Giampiccolo, 2012</marker>
<rawString>Negri M., Marchetti A., Mehdad Y., Bentivogli L., Giampiccolo D. Semeval-2012 Task 8: Cross-lingual Textual Entailment for Content Synchronization. In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012). 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P D Turney</author>
</authors>
<title>Mining the web for synonyms: Pmiir versus lsa on toefl.</title>
<date>2001</date>
<booktitle>In EMCL 01: Proceedings of the 12th European Conference on Machine Learning,</booktitle>
<pages>491502</pages>
<publisher>Springer-Verlag</publisher>
<location>London, UK,</location>
<contexts>
<context position="3839" citStr="Turney, 2001" startWordPosition="604" endWordPosition="605">y input word the component will return the set of all possible translations. For unavailable pairs, we make use of triangular translation (Kraaij, 2003). As for the semantic component we experimented with a corpus-based distributional approach capable of detecting the interrelation between different terms in a corpus; the strategy we adopted is similar to Latent Semantic Analysis (Deerwester et. al., 1990) although it uses a less expensive computational solution based on the Random Projection algorithm (Lin et. al., 2003) and (Bingham et. al., 2001). Different works debate on similar issues: (Turney, 2001) uses LSA in order to solve synonymy detection questions from the well-known TOEFL test while the method presented by (Inkpen, 2001) or by (Baroni and Bisi, 2001) proposes the use of the Web as a corpus to compute mutual information scores between candidate terms. More technically, Random Indexing exploits an algebraic model in order to represent the semantics of terms in a Nth dimensional space (a vector of length N); approaches falling into this category, actually create a Terms By Contexts matrix where each cell represents the degree of memberships of a given term to the different contexts.</context>
</contexts>
<marker>Turney, 2001</marker>
<rawString>Turney P.D. 2001. Mining the web for synonyms: Pmiir versus lsa on toefl. In EMCL 01: Proceedings of the 12th European Conference on Machine Learning, London, UK, Springer-Verlag pages 491502</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>