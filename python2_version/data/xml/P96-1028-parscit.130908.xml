<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9991625">
Evaluating the Portability of Revision Rules
for Incremental Summary Generation
</title>
<author confidence="0.926007">
Jacques Robin
</author>
<affiliation confidence="0.739539">
http://www.di.ufpe.brrjr
jradi.ufpe.br
Departamento de Informatica, Universidade Federal de Pernambuco
</affiliation>
<address confidence="0.482738">
Caixa Postal 7851, Cidade Universitaria
Recife, PE 50732-970 Brazil
</address>
<sectionHeader confidence="0.986753" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999943538461538">
This paper presents a quantitative evalu-
ation of the portability to the stock mar-
ket domain of the revision rule hierarchy
used by the system STREAK to incremen-
tally generate newswire sports summaries.
The evaluation consists of searching a test
corpus of stock market reports for sentence
pairs whose (semantic and syntactic) struc-
tures respectively match the triggering con-
dition and application result of each revi-
sion rule. The results show that at least
59% of all rule classes are fully portable,
with at least another 7% partially portable.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99963335">
The project STREAK&apos; focuses on the specific issues
involved in generating short, newswire style, natural
language texts that summarize vast amount of in-
put tabular data in their historical context. A series
of previous publications presented complementary
aspects of this project: motivating corpus analysis
in (Robin and McKeown, 1993), new revision-based
text generation model in (Robin, 1993), system im-
plementation and rule base in (Robin, 1994a) and
empirical evaluation of the robustness and scalabil-
ity of this new model as compared to the traditional
single pass pipeline model in (Robin and McKeown,
1995). The present paper completes this series by
describing a second, empirical, corpus-based evalu-
ation, this time quantifying the portability to an-
other domain (the stock market) of the revision rule
hierarchy acquired in the sports domain and imple-
mented in STREAK. The goal of this paper is twofold:
(1) assessing the generality of this particular rule hi-
erarchy and (2) providing a general, semi-automatic
</bodyText>
<note confidence="0.3583735">
&apos;Surface Text Reviser Expressing Additional
Knowledge.
</note>
<bodyText confidence="0.9998936875">
methodology for evaluating the portability of seman-
tic and syntactic knowledge structures used for nat-
ural language generation. The results reveal that at
least 59% of the revision rule hierarchy abstracted
from the sports domain could also be used to incre-
mentally generate the complex sentences observed in
a corpus of stock market reports.
I start by providing the context of the evalua-
tion with a brief overview of STREAK&apos;S revision-based
generation model, followed by some details about the
empirical acquisition of its revision rules from cor-
pus data. I then present the methodology of this
evaluation, followed by a discussion of its quantita-
tive results. Finally, I compare this evaluation with
other empirical evaluations in text generation and
conclude by discussing future directions.
</bodyText>
<sectionHeader confidence="0.929397" genericHeader="method">
2 An overview of STREAK
</sectionHeader>
<bodyText confidence="0.99999152631579">
The project STREAK was initially motivated by an-
alyzing a corpus of newswire summaries written by
professional sportswriters2. This analysis revealed
four characteristics of summaries that challenge the
capabilities of previous text generators: concise lin-
guistic forms, complex sentences, optional and back-
ground facts opportunistically slipped as modifiers
of obligatory facts and high paraphrasing power. By
greatly increasing the number of content planning
and linguistic realization options that the genera-
tor must consider, as well as the mutual constraints
among them, these characteristics make generating
summaries in a single pass impractical.
The example run given in Fig. 1 illustrates how
STREAK overcomes these difficulties. It first gener-
ates a simple draft sentence that contains only the
obligatory facts to include in any game report (lo-
cation, date, game result and key player statistic).
It then applies a series of revision rules3, each one
</bodyText>
<footnote confidence="0.835789666666667">
&apos;This 800,000 word corpus covers a whole NBA (Na-
tional Basketball Association) season.
&apos;In Fig. 1, the rule used is indicated above each re-
</footnote>
<page confidence="0.997217">
205
</page>
<figure confidence="0.941934789473684">
1. Initial draft (basic sentence pattern):
&amp;quot;Dallas, TX — Charles Barkley scored 42 points Sunday as the Phoenix Suns defeated the Dallas Mavericks
123-97.&amp;quot;
2. Adjunct izat ion of Created into Instrument:
&amp;quot;Dallas, TX — Charles Barkley tied a season high with 42 points Sunday as the Phoenix Suns defeated the
Dallas Mavericks 123-97.&amp;quot;
3. Coordinative Conjoin of Clause:
&amp;quot;Dallas, TX — Charles Barkley tied a season high with 42 points and Danny Ainge added 21 Sunday as the
Phoenix Suns defeated the Dallas Mavericks 123-97.&amp;quot;
4. Absorb of Clause in Clause as Result with Agent Control:
&amp;quot;Dallas, TX — Charles Barkley tied a season high with 42 points and Danny Ainge came off the bench
to add 21 Sunday as the Phoenix Suns defeated the Dallas Mavericks 123-97.&amp;quot;
5. Nominal i z at ion with Ordinal Adjoin:
&amp;quot;Dallas, TX — Charles Barkley tied a season high with 42 points and Danny Ainge came off the bench to add
21 Sunday as the Phoenix Suns handed the Dallas Mavericks their 13th straight home defeat 123-97.&amp;quot;
6. Adjoin of Classifier to NP:
&amp;quot;Dallas, TX — Charles Barkley tied a season high with 42 points and Danny Ainge came off the bench to add
21 Sunday as the Phoenix Suns handed the Dallas Mavericks their league worst 13th straight home defeat
123-97.&amp;quot;
</figure>
<figureCaption confidence="0.998877">
Figure 1: Complex sentence generation through incremental revisions in STREAK
</figureCaption>
<bodyText confidence="0.834498">
opportunistically adding a new face that either:
</bodyText>
<listItem confidence="0.9984385">
• Complements an already included fact (e.g., re-
vision of sentence 2 into 3).
• Justifies its relevance by providing its historical
background (e.g., revision of sentence 1 into 2).
</listItem>
<bodyText confidence="0.9359606">
Some of these revisions are non-monotonic,
rewording5 a draft fact to more concisely accommo-
date the additional fact (e.g., revision of sentence
1 into 2). Popping additional facts from a prior-
ity stack, STREAK stops revising when the summary
</bodyText>
<footnote confidence="0.884742533333333">
vised sentence.
4Highlighted in bold in Fig. 1.
5In Fig. 1, words that get deleted are italicized and
words that get modified are underlined.
Charles Barkley scored 42 points. Those 42 points equal
his best scoring performance of the season. Danny Ainge
is a teammate of Barkley. They play for the Phoenix
Suns. Ainge is a reserve player. Yet he scored 21 points.
The high scoring performances by Barkley and Ainge
helped the Sims defeat the Dallas Mavericks. The Mav-
ericks played on their homecourt in Texas. They had
already lost their 12 previous games there. No other
team in the league has lost so many games in a row at
home. The final score was 123-97. The game was played
Sunday.
</footnote>
<figureCaption confidence="0.9858215">
Figure 2: Paragraph of simple sentences
paraphrasing a single complex sentence
</figureCaption>
<bodyText confidence="0.98735625">
sentence reaches linguistic complexity limits empir-
ically observed in the corpus (e.g., 50 word long or
parse tree of depth 10).
While STREAK generates only single sentences,
those complex sentences convey as much information
as whole paragraphs made of simple sentences, only
far more fluently and concisely. This is illustrated
by the 12 sentence paragraph6 of Fig. 2, which para-
phrases sentence 6 of Fig. 1. Because they express
facts essentially independently of one another, such
multi-sentence paragraphs are much easier to gener-
ate than the complex single sentences generated by
</bodyText>
<sectionHeader confidence="0.494503" genericHeader="method">
STREAK.
</sectionHeader>
<subsectionHeader confidence="0.6533915">
3 Acquiring revision rules from
corpus data
</subsectionHeader>
<bodyText confidence="0.989922">
The rules driving the revision process in STREAK
were acquired by reverse engineering7 about 300 cor-
pus sentences. These sentences were initially classi-
fied in terms of:
</bodyText>
<listItem confidence="0.99146">
• The combination of domain concepts they ex-
pressed.
• The thematic role and top-level syntactic cate-
gory used for each of these concepts.
</listItem>
<footnote confidence="0.998283">
6This paragraph was not generated by STREAK, it is
shown here only for contrastive purposes.
&apos; i.e., analyzing how they could be incrementally gen-
erated through gradual revisions.
</footnote>
<page confidence="0.998833">
206
</page>
<bodyText confidence="0.9983975">
The resulting classes, called realization patterns,
abstract the mapping from semantic to syntactic
structure by factoring out lexical material and syn-
tactic details. Two examples of realization patterns
are given in Fig. 3. Realization patterns were then
grouped into surface decrement pairs consisting of:
</bodyText>
<listItem confidence="0.9975112">
• A more complex pattern (called the target pat-
tern).
• A simpler pattern (called the source pattern)
that is structurally the closest to the target pat-
tern among patterns with one less concept8.
</listItem>
<bodyText confidence="0.998625517241379">
The structural transformations from source to tar-
get pattern in each surface decrement pair were
then hierarchically classified, resulting in the revi-
sion rule hierarchy shown in Fig. 4-10. For ex-
ample, the surface decrement pair &lt; R12„ R &gt;,
shown in Fig. 3, is one of the pairs from which
the revision rule Adjunctization of Range into
Instrument, shown in Fig. 10 was abstracted.
It involves displacing the Range argument of the
source clause as an Instrument adjunct to accom-
modate a new verb and its argument. This revi-
sion rule is a sibling of the rule Adjunctization of
Created into Instrument used to revise sentence
1 into 2 in STREAK&apos;S run shown in Fig. 1 (where the
Created argument role &amp;quot;42 points&amp;quot; of the verb &amp;quot;to
score&amp;quot; in 1 becomes an Instrument adjunct in 2).
The bottom level of the revision rule hierarchy
specifies the side revisions that are orthogonal and
sometimes accompany the restructuring revisions
discussed up to this point. Side revisions do not
make the draft more informative, but instead im-
prove its style, conciseness and unambiguity. For ex-
ample, when STREAK revises sentence (3) into (4) in
the example run of Fig. 1, the Agent of the absorbed
clause &amp;quot;Danny Ainge added 21 points&amp;quot; becomes con-
trolled by the new embedding clause &amp;quot;Danny Ainge
came off the bench&amp;quot; to avoid the verbose form:
? &amp;quot;Danny Ainge came off the bench for Danny Ainge
to add 21 points&amp;quot;.
</bodyText>
<sectionHeader confidence="0.99698" genericHeader="method">
4 Evaluation methodology
</sectionHeader>
<bodyText confidence="0.99644">
In the spectrum of possible evaluations, the eval-
uation presented in this paper is characterized as
follows:
</bodyText>
<listItem confidence="0.90282745">
• Its object is the revision rule hierarchy acquired
from the sports summary corpus. It thus does
not directly evaluate the output of STREAK, but
rather the special knowledge structures required
by its underlying revision-based model.
8 i.e., the source pattern expresses the same concept
combination than the target pattern minus one concept.
• The particular property of this revision rule hi-
erarchy that is evaluated is cross-domain porta-
bility: how much of it could be re-used to gener-
ate summaries in another domain, namely the
stock market?
• The basis for this evaluation is corpus data9
The original sports summary corpus from which
the revision rules were acquired is used as the
&apos;training&apos; (or acquisition) corpus and a cor-
pus of stock market reports taken from several
newswires is used as the &apos;test&apos; corpus. This test
corpus comprises over 18,000 sentences.
• The evaluation procedure is quantitative, mea-
</listItem>
<bodyText confidence="0.885682375">
suring percentages of revision rules whose target
and source realization patterns are observable
in the test corpus. It is also semi-automated
through the use of the corpus search tool CREP
(Duford, 1993) (as explained below).
Basic principle As explained in section 3, a re-
vision rule is associated with a list of surface decre-
ment pairs, each one consisting of:
</bodyText>
<listItem confidence="0.882226111111111">
• A source pattern whose content and linguistic
form match the triggering conditions of the rule
(e.g., RI in Fig. 3 for the rule Adjunctization
of Range into Instrument).
• A target pattern whose content and linguis-
tic form can be derived from the source pat-
tern by applying the rule (e.g., R in Fig. 3
for the rule Adjunctization of Range into
Instrument).
</listItem>
<bodyText confidence="0.963020263157895">
This list of decrement pairs can thus be used as
the signature of the revision rule to detect its usage
in the test corpus. The needed evidence is the simul-
taneous presence of two test corpus sentencesl° , each
one respectively matching the source and target pat-
terns of at least one element in this list. Requiring
occurrence of the source pattern in the test corpus is
necessary for the computation of conservative porta-
bility estimates: while it may seem that one target
pattern alone is enough evidence, without the pres-
ence of the corresponding source pattern, one cannot
rule out the possibility that, in the test domain, this
target pattern is either a basic pattern or derived
from another source pattern using another revision
rule.
9 Only the corpus analysis was performed for both do-
mains. The implementation was not actually ported to
the stock market domain.
&apos;°In general, not from the same report.
</bodyText>
<page confidence="0.986874">
207
</page>
<bodyText confidence="0.552335">
Realization pattern lit:
</bodyText>
<listItem confidence="0.985976">
• Expresses the concept pair:
&lt; game-result (winner,loser,score) , st reek ( wi n n er ,aspect ,result-type,length)&gt; .
• Is a target pattern of the revision rule Adjunctization of Range into Instrument.
</listItem>
<bodyText confidence="0.936984">
winner aspect .1 type I streak length I score game-result I loser
agent action affected/located location instrument
proper verb NP PP PP
Utah extended det I classifier I noun to 6 games prep NP
Boston stretching to 9 outings
its win streak with det I number I noun PP
its winning spree with
a 99-84 triumph over Denver
a 118-94 rout of Utah
Realization pattern FIL:
</bodyText>
<listItem confidence="0.999619333333333">
• Expresses the single concept &lt;game-result(winner,loser,score)&gt;.
• Is a source pattern of the revision rule Adjunctization of Range into Instrument.
• Is a surface decrement of pattern RI above.
</listItem>
<bodyText confidence="0.943266">
winner score 1 game-result 1 loser
agent action range
proper support-verb NP
det I number 1 noun 1 PP
utucago claimeo a 128-94 ctory over New Jersey
Orlando recorded a 101-95 triumph against New York
</bodyText>
<figureCaption confidence="0.997948">
Figure 3: Realization pattern examples
</figureCaption>
<bodyText confidence="0.9996708">
Partially automating the evaluation The soft-
ware tool cREP11 was developed to partially auto-
mate detection of realization patterns in a text cor-
pus. The basic idea behind CREP is to approximate
a realization pattern by a regular expression whose
terminals are words or parts-of-speech tags (POS-
tags). CREP will then automatically retrieve the cor-
pus sentences matching those expressions. For ex-
ample, the CREP expression el below approximates
the realization pattern RI shown in Fig. 3:
</bodyText>
<equation confidence="0.715164">
(C1) TEAM 0= (claimed&apos; recorded)OVEID 1- SCORE 0=
(victoryltriumph)ONN 0= (overlagainst)OIN 0= TEAM
</equation>
<bodyText confidence="0.9999085">
In the expression above, `VED&apos;, &apos;NW and &apos;IN&apos; are the
POS-tags for past verb, singular noun and preposi-
tion (respectively), and the sub-expressions &apos;TEAM&apos;
and &apos;SCORE&apos; (whose recursive definitions are not
shown here) match the team names and possible fi-
nal scores (respectively) in the NBA. The CREP op-
erators &apos;N=&apos; and &apos;N-&apos; (N being an arbitrary integer)
respectively specify exact and minimal distance of
N words, and &apos; I &apos; encodes disjunction.
11CREP was implemented (on top of FLEX, GNUS&apos; ver-
sion of LEx) and to a large extent also designed by Du-
ford. It uses Ken Church&apos;s POS tagger.
Because a realization pattern abstracts away from
lexical items to capture the mapping from concepts
to syntactic structure, approximating such a pattern
by a regular expression of words and POS-tags in-
volves encoding each concept of the pattern by the
disjunction of its alternative lexicalizations. In a
given domain, there are therefore two sources of in-
accuracy for such an approximation:
</bodyText>
<listItem confidence="0.99785975">
• Lexical ambiguity resulting in false positives by
over-generalization.
• Incomplete vocabulary resulting in false nega-
tives by over-specialization12.
</listItem>
<bodyText confidence="0.992065727272727">
Lexical ambiguities can be alleviated by writing
more context-sensitive expressions. The vocabu-
lary can be acquired through additional exploratory
CREP runs with expressions containing wild-cards
for some concept slots. Although automated corpus
search using CREP expressions considerably speeds-
up corpus analysis, manual intervention remains
&apos;This is the case for example of C1 above, which is a
simplification of the actual expression that was used to
search occurrences of lit in the test corpus (e.g., C1 is
missing &amp;quot;win&amp;quot; and &amp;quot;rout&amp;quot; as alternatives for &amp;quot;victory&amp;quot;).
</bodyText>
<page confidence="0.992114">
208
</page>
<bodyText confidence="0.998734157894737">
necessary to filter out incorrect matches resulting
from imperfect approximations.
Cross-domain discrepancies Basic similarities
between the finance and sports domains form the
basis for the portability of the revision rules. In
both domains, the core facts reported are statis-
tics compiled within a standard temporal unit (in
sports, one ballgame; in finance, one stock market
session) together with streaks13 and records com-
piled across several such units. This correspondence
is, however, imperfect. Consequently, before they
can track down usage of a revision rule in the test do-
main, the CREP expressions approximating the sig-
nature of the rule in the acquisition domain must be
adjusted for cross-domain discrepancies to prevent
false negatives. Two major types of adjustments are
necessary: lexical and thematic.
Lexical adjustments handle cases of partial mis-
match between the respective vocabularies used to
lexicalize matching conceptual structures in each do-
main. (e.g.„ the verb &amp;quot;to rebound from&amp;quot; expresses
the interruption of a streak in the stock market do-
main, while in the basketball domain &amp;quot;to break&amp;quot; or
&amp;quot;to snap&amp;quot; are preferred since &amp;quot;to rebound&amp;quot; is used to
express a different concept).
Thematic adjustments handle cases of partial dif-
ferences between corresponding conceptual struc-
tures in the acquisition and test domains. For ex-
ample, while in sports game-result involves an-
tagonistic teams, its financial domain counterpart
session-result concerns only a single indicator.
Consequently, the sub-expression for the loser role
in the example CREP expression (-1 shown before,
and which approximates realization pattern RI for
game-result (shown in Fig. 3), needs to become
optional in order to also approximate patterns for
session-result. This is done using the CREP op-
erator ? as shown below:
</bodyText>
<equation confidence="0.827685666666667">
(CA: TEAM 0= (claimedIrecorded)OVED 1-
SCORE 0= (victoryItriumph)ONN 0=
((overiagainst)OIN 0= TEAM)?
</equation>
<bodyText confidence="0.9980395">
Note that it is the CREP expressions used to auto-
matically retrieve test corpus sentence pairs attest-
ing usage of a revision rule that require this type
of adjustment and not the revision rule itself&amp;quot;. For
example, the Adjoin of Frequency PP to Clause
revision rule attaches a streak to a session-result
clause without loser role in exactly the same way
than it attaches a streak to a game-result with
</bodyText>
<page confidence="0.721034">
13 i.e., series of events with similar outcome.
</page>
<bodyText confidence="0.7964265">
&amp;quot;Some revision rules do require adjustment, but of
another type (cf. Sect. 5).
</bodyText>
<listItem confidence="0.88805516">
loser role. This is illustrated by the two corpus
sentences below:
P&apos;22: &amp;quot;The Chicago Bulls beat the Phoenix Suns 99
91 for their 3rd straight win&amp;quot;
11: &amp;quot;The Amex Market Value Index inched up 0.16
to 481.94 for its sixth straight advance&amp;quot;
Detailed evaluation procedure The overall
procedure to test portability of a revision rule con-
sists of considering the surface decrement pairs in the
rule signature in order, and repeating the following
steps:
1. Write a CREP expression for the acquisition tar-
get pattern.
2. Iteratively delete, replace or generalize sub-
expressions in the CREP expression - to gloss
over thematic and lexical discrepancies between
the acquisition and test domains, and prevent
false negatives - until it matches some test cor-
pus sentence(s).
3. Post-edit the file containing these matched sen-
tences. If it contains only false positives of the
sought target pattern, go back to step 2. Oth-
erwise, proceed to step 4.
4. Repeat step (1-3) with the source pattern of the
pair under consideration. If a valid match can
</listItem>
<bodyText confidence="0.95205988">
also be found for this source pattern, stop: the
revision rule is portable. Otherwise, start over
from step 1 with the next surface decrement pair
in the revision rule signature. If there is no next
pair left, stop: the revision rule is considered
non-portable.
Steps (2,3) constitute a general, generate-and-test
procedure to detect realization patterns usage in a
corpus15. Changing one CREP sub-expression may
result in going from too specific an expression with
no valid match to either: (1) a well-adjusted ex-
pression with a valid match, (2) still too specific an
expression with no valid match, or (3) already too
general an expression with too many matches to be
manually post-edited.
It is in fact always possible to write more context-
sensitive expressions, to manually edit larger no-
match files, or even to consider larger test corpora in
the hope of finding a match. At some point however,
one has to estimate, guided by the results of previ-
ous runs, that the likelihood of finding a match is too
&apos;And since most generators rely on knowledge struc-
tures equivalent to realization patterns, this procedure
can probably be adapted to semi-automatically evaluate
the portability of virtually any corpus-based generator.
</bodyText>
<page confidence="0.99775">
209
</page>
<bodyText confidence="0.999963428571429">
small to justify the cost of further attempts. This is
why the last line in the algorithm reads &amp;quot;considered
non-portable&amp;quot; as opposed to &amp;quot;non-portable&amp;quot;. The
algorithm guarantees the validity of positive (i.e.,
portable) results only. Therefore, the figures pre-
sented in the next section constitute in fact a lower-
bound estimate of the actual revision rule portability.
</bodyText>
<sectionHeader confidence="0.988162" genericHeader="method">
5 Evaluation results
</sectionHeader>
<bodyText confidence="0.999851694444445">
The results of the evaluation are summarized in
Fig. 4-10. They show the revision rule hierarchy,
with portable classes highlighted in bold. The fre-
quency of occurrence of each rule in the acquisition
corpus is given below the leaves of the hierarchy.
Some rules are same-concept portable: they are
used to attach corresponding concepts in each do-
main (e.g., Adjoin of Frequency PP to Clause,
as explained in Sect. 4). They could be re-used &amp;quot;as
is&amp;quot; in the financial domain. Other rules, however,
are only different-concept portable: they are used to
attach altogether different concepts in each domain.
This is the case for example of Adjoin Finite Time
Clause to Clause, as illustrated by the two corpus
sentences below, where the added temporal adjunct
(in bold) conveys a streak in the sports sentence, but
a complementary statistics in the financial one:
Tg: &amp;quot;to lead Utah to a 119-89 trouncing of Denver
as the Jazz defeated the Nuggets for the 12th
straight time at home.&amp;quot;
TA: &amp;quot;Volume amounted to a solid 349 million shares
as advances out-paced declines 299 to 218.&amp;quot;.
For different-concept portable rules, the left-hand
side field specifying the concepts incorporable to the
draft using this rule will need to be changed when
porting the rule to the stock market domain. In
Fig. 4-10, the arcs leading same-concept portable
classes are full and thick, those leading to different-
concept portable classes are dotted, and those lead-
ing to a non-portable classes are full but thin.
59% of all revision rule classes turned out to be
same-concept portable, with another 7% different-
concept portable. Remarkably, all eight top-level
classes identified in the sports domain had instances
same-concept portable to the financial domain, even
those involving the most complex non-monotonic re-
visions, or those with only a few instances in the
sports corpus. Among the bottom-level classes that
distinguish between revision rule applications in very
specific semantic and syntactic contexts, 42% are
same-concept portable with another 10% different-
concept portable. Finally, the correlation between
high usage frequency in the acquisition corpus and
portability to the test corpus is not statistically sig-
nificant (i.e., the hypothesis that the more common
a rule, the more likely it is to be portable could not
be confirmed on the analyzed sample). See (Robin,
1994b) for further details on the evaluation results.
There are two main stumbling blocks to porta-
bility: thematic role mismatch and side revisions.
Thematic role mismatches are cases where the se-
mantic label or syntactic sub-category of a con-
stituent added or displaced by the rule differ in
each domain (e.g., Adjunctization of Created
into Instrument vs. Adjoin of Affected into
Instrument). They push portability from 92% down
to 71%. Their effect could be reduced by allowing
STREAK&apos;S reviser to manipulate the draft down to
the surface syntactic role level (e.g., in both cor-
pora Created and Affected surface as object). Cur-
rently, the reviser stops at the thematic role level to
allow STREAK to take full advantage of the syntac-
tic processing front-end SURGE (Elhadad and Robin,
1996), which accepts such thematic structures as in-
put.
Accompanying side revisions push portability
from 71% to 52%. This suggests that the design of
STREAK could be improved by keeping side revisions
separate from re-structuring revisions and interleav-
ing the applications of the two. Currently, they are
integrated together at the bottom of the revision rule
hierarchy.
</bodyText>
<sectionHeader confidence="0.999889" genericHeader="method">
6 Related work
</sectionHeader>
<bodyText confidence="0.999935444444444">
Apart from STREAK, only three generation projects
feature an empirical and quantitative evaluation:
ANA (Kukich, 1983), KNIGHT (Lester, 1993) and Im-
AGENE (Van der Linden, 1993).
ANA generates short, newswire style summaries of
the daily fluctuations of several stock market indexes
from half-hourly updates of their values. For eval-
uation, Kukich measures both the conceptual and
linguistic (lexical and syntactic) coverages of ANA
by comparing the number of concepts and realiza-
tion patterns identified during a corpus analysis with
those actually implemented in the system.
KNIGHT generates natural language concept defi-
nitions from a large biological knowledge base, rely-
ing on SURGE for syntactic realization. For evalua-
tion, Lester performs a Turing test in which a panel
of human judges rates 120 sample definitions by as-
signing grades (from A to F) for:
</bodyText>
<listItem confidence="0.666679">
• Semantic accuracy (defined as &amp;quot;Is the definition
adequate, providing correct information and fo-
cusing on what&apos;s important?&amp;quot; in the instruc-
tions provided to the judges).
• Stylistic accuracy (defined as &amp;quot;Does the defini-
tion use good prose and is the information it
</listItem>
<page confidence="0.996148">
210
</page>
<subsectionHeader confidence="0.555434">
Monotonic Revisions Non-monotonic Revisions
Adjoin Absorb Conjoin Recast Adjunctization Nominalization Demotion Promotion
</subsectionHeader>
<figureCaption confidence="0.997551">
Figure 4: Revision rule hierarchy: top-levels
</figureCaption>
<figure confidence="0.996689">
Absorb
:rn***********
as-qualifier as-instL....ent as-affected-apposition
1 2 1
of clause
inside-clause
as-dean as-co-event
1 3
......
inside-NP inside:clause
of NP
..........
</figure>
<figureCaption confidence="0.998117">
Figure 5: Absorb revision rule sub-hierarchy
</figureCaption>
<table confidence="0.94216825">
Recast Nominalization
of NP of clause
+ordinal 4-ordinal 4-ordinal
+classifier +qualifier
1 2 2
from classifier from location from range
to qualifier to instrument to time to instrument
10 9 1 1
Figure 6: Recast and Nominalize revision rule sub-hierarchy
from score Demotion from affected Coordination Promotion
to score(co-event) from created to determiner(affected) simple w/ Adjunctization
1 to qualifier(affected) 1 1 1
</table>
<page confidence="0.735936">
2
</page>
<figureCaption confidence="0.999014">
Figure 7: Demotion and Promotion revision rule sub-hierarchy
</figureCaption>
<page confidence="0.869643">
211
</page>
<figure confidence="0.976869277777778">
Adjoin
to NP to clause
partitive classifier describer qualifier
1 25 5
relative-clause non-finite-clause
tolr.&amp;quot;.&amp;quot;1:rm
frequency tilt&amp;quot;.......&amp;quot;&amp;quot;ifibe co-e ent
I I
PP non-finite finite non-finite
clause clause clause
4 1
abridged lull
deleted A +reorder
ref ref ref
20 3 9 2
full j abridged full abridged abridged deleted
ref ref ref ref ref ref ref
2 10 13 3 1 124
</figure>
<figureCaption confidence="0.986376">
Figure 8: Adjoin revision rule sub-hierarchy
</figureCaption>
<figure confidence="0.842801666666667">
Conjoin
NPs clauses
by apposition by coordination by coordination
</figure>
<figureCaption confidence="0.999792">
Figure 9: Conjoin revision rule sub-hierarchy
Figure 10: Adjunctization revision rule sub-hierarchy
</figureCaption>
<page confidence="0.843505">
212
</page>
<figure confidence="0.9830595">
1.
: mark fullged
abrid
ref ref&amp;ged full brid
ref ref
Vscope
1 2 1 5 1 1
abridged full deleted
re ref ref
15 5 4
top bottom top bottom
Into instrument
into opposition
of affected
1
of range
+agent
demotion
abridged deleted
r.- ref
7 273
full/ abridged
ref ref
14 5 4
</figure>
<table confidence="0.9871554">
Object of Evaluation Evaluated Properties Empirical Basis Evaluation Procedure
ANA knowledge structures conceptual coverage textual corpus manual
linguistic coverage ,, ”
KNIGHT output text semantic accuracy stylistic accuracy manual
)9 human judges
IMAGENE output text stylistic accuracy stylistic robustness manual
textual corpus
STREAK knowledge structures cross-domain portability same-domain robustness semi-automatic
)) same-domain scalability textual corpus ”
&amp;quot;
</table>
<figureCaption confidence="0.956825">
Figure 11: Empirical evaluations in language generation
</figureCaption>
<bodyText confidence="0.993741333333333">
conveys well organized&amp;quot; in the instructions pro-
vided to the judges).
The judges did not know that half the definitions
were computer-generated while the other half were
written by four human domain experts. Impres-
sively, the results show that:
</bodyText>
<listItem confidence="0.9179355">
• With respect to semantic accuracy, the human
judges could not tell KNIGHT apart from the hu-
man writers.
• While as a group, humans got statistically sig-
</listItem>
<bodyText confidence="0.9829254">
nificantly better grades for stylistic accuracy
than KNIGHT, the best human writer was single-
handly responsible for this difference.
IMAGENE generates instructions on how to oper-
ate household devices relying on NIGEL (Mann and
Matthiessen, 1983) for syntactic realization. The
implementation focuses on a very limited aspect of
text generation: the realization of purpose relations.
Taking as input the description of a pair &lt;operation,
purpose of the operation&gt;, augmented by a set of
features simulating the communicative context of
generation, IMAGENE selects, among the many real-
izations of purpose generable by NIGEL (e.g., fronted
to-infinitive clause vs. trailing for-gerund clauses),
the one that is most appropriate for the simulated
context (e.g., in the context of several operations
sharing the same purpose, the latter is preferentially
expressed before those actions than after them). IDA-
AGENE&apos;S contextual preference rules were abstracted
by analyzing an acquisition corpus of about 300 pur-
pose clauses from cordless telephone manuals. For
evaluation, Van der Linden compares the purpose
realizations picked by IMAGENE to the one in the
corresponding corpus text, first on the acquisition
corpus and then on a test corpus of about 300 other
purpose clauses from manuals for other devices than
cordless telephones (ranging from clock radio to au-
tomobile). The results show a 71% match on the
acquisition corpus16 and a 52% match on the test
corpus.
The table of Fig. 11 summarizes the difference
on both goal and methodology between the eval-
uations carried out in the projects ANA, KNIGHT,
IMAGENE and STREAK. In terms of goals, while
Kukich and Lester evaluate the coverage or accu-
racy of a particular implementation, I instead fo-
cus on three properties inherent to the use of the
revision-based generation model underlying STREAK:
robustness (how much of other text samples from the
same domain can be generated without acquiring
new knowledge?) and scalability (how much more
new knowledge is needed to fully cover these other
samples?) discussed in (Robin and McKeown, 1995),
and portability to another domain in the present pa-
per. Van der Linden does a little bit of both by first
measuring the stylistic accuracy of his system for a
very restricted sub-domain, and then measuring how
it degrades for a more general domain.
In itself, measuring the accuracy and coverage of
a particular implementation in the sub-domain for
which it was designed brings little insights about
what generation approach should be adopted in fu-
ture work. Indeed, even a system using mere canned
text can be very accurate and attain substantial cov-
erage if enough hand-coding effort is put into it.
However, all this effort will have to be entirely du-
plicated each time the system is scaled up or ported
to a new domain. Measuring how much of this effort
duplication can be avoided when relying on revision-
based generation was the very object of the three
evaluations carried in the STREAK project.
&amp;quot;This imperfect match on the acquisition corpus
seems to result from the heuristic nature of IMAGENE&apos;s
stylistic preferences: individually, none of them needs to
apply to the whole corpus.
</bodyText>
<page confidence="0.996349">
213
</page>
<bodyText confidence="0.9999765">
In terms of methodology, the main originality of
these three evaluations is the use of CREP to par-
tially automate reverse engineering of corpus sen-
tences. Beyond evaluation, CREP is a simple, but
general and very handy tool that should prove use-
ful to speed-up a wide range of corpora analyses.
</bodyText>
<sectionHeader confidence="0.998902" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999978608695653">
In this paper, I presented a quantitative evaluation
of the portability to the stock market domain of the
revision rule hierarchy used by the system STREAK to
incrementally generate newswire sports summaries.
The evaluation procedure consists of searching a test
corpus of stock market reports for sentence pairs
whose (semantic and syntactic) structures respec-
tively match the triggering condition and application
result of each revision rule. The results show that at
least 59% of all rule classes are fully portable, with
at least another 7% partially portable.
Since the sports domain is not closer to the finan-
cial domain than to other quantitative domains such
as meteorology, demography, business auditing or
computer surveillance, these results are very encour-
aging with respect to the general cross-domain re-
usability potential of the knowledge structures used
in revision-based generation. However, the present
evaluation concerned only one type of such knowl-
edge structures: revision rules. In future work, sim-
ilar evaluations will be needed for the other types of
knowledge structures: content selection rules, phrase
planning rules and lexicalization rules.
</bodyText>
<sectionHeader confidence="0.999721" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<reference confidence="0.993391785714286">
Many thanks to Kathy McKeown for stressing the im-
portance of empirically evaluating STREAK. The re-
search presented in this paper is currently supported by
CNPq (Brazilian Research Council) under post-doctoral
research grant 150130-95.3. It started out while I was
at Columbia University supported by of a joint grant
from the Office of Naval Research, by the Advanced
Research Projects Agency under contract N00014-89-J-
1782, by National Science Foundation Grants IRT-84-
51438 and GER-90-2406, and by the New York State
Science and Technology Foundation under this auspices
of the Columbia University CAT in High Performance
Computing and Communications in Health Care, a New
York State Center for Advanced Technology.
</reference>
<sectionHeader confidence="0.903781" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998088942307692">
CU-CS-005-93. Computer Science Department,
Columbia University, New York.
Elhadad, M. and Robin, J. 1996. An overview
of SURGE: a re-usable comprehensive syntactic
realization component. Technical Report 96-03.
Computer Science and Mathematics Department,
Ben Gurion University, Beer Sheva, Israel.
Kukich, K. 1983. Knowledge-based report genera-
tion: a knowledge engineering approach to natural
language report generation. PhD. Thesis. Depart-
ment of Information Science. University of Pitts-
burgh.
Lester, J.C. 1993. Generating natural language
explanations from large-scale knowledge bases.
PhD. Thesis. Computer Science Department,
University of Texas at Austin.
Mann, W.C. and Matthiessen, C. M. 1983. NIGEL:
a systemic grammar for text generation. Research
Report RR-83-105. ISI. Marina Del Rey, CA.
Robin, J. and McKeown, K.R. 1993. Corpus anal-
ysis for revision-based generation of complex sen-
tences. In Proceedings of the 11th National Con-
ference on Artificial Intelligence, Washington DC.
(AAAI&apos;93).
Robin, J. and McKeown, K.R. 1995. Empirically
designing and evaluating a new revision-based
model for summary generation. Artificial Intel-
ligence. Vol 85. Special Issue on Empirical Meth-
ods. North-Holland.
Robin, J. 1993. A revision-based generation archi-
tecture for reporting facts in their historical con-
text. In New Concepts in Natural Language Gen-
eration: Planning, Realization and System. Ho-
racek, H. and Zock, M., Eds. Frances Pinter.
Robin, J. 1994a. Automatic generation and revision
of natural language summaries providing histori-
cal background In Proceedings of the 11th Brazil-
ian Symposium on Artificial Intelligence, Fort-
aleza, Brazil. (SBIA&apos;94).
Robin, J. 1994b. Revision-based generation of natu-
ral language summaries providing historical back-
ground: corpus-based analysis, design, implemen-
tation and evaluation. PhD. Thesis. Available
as Technical Report CU-CS-034-94. Computer
Science Department, Columbia University, New
York.
Van der Linden, K. and Martin, J.H. 1995. Ex-
pressing rhetorical relations in instructional texts:
a case study of the purpose relation. Computa-
tional Linguistics, 21(1). MIT Press.
Duford, D. 1993. CREP: a regular expression-
matching textual corpus tool. Technical Report
</reference>
<page confidence="0.973002">
214
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.267678">
<title confidence="0.999804">Evaluating the Portability of Revision Rules for Incremental Summary Generation</title>
<author confidence="0.999826">Jacques Robin</author>
<web confidence="0.973032">http://www.di.ufpe.brrjr</web>
<email confidence="0.760621">jradi.ufpe.br</email>
<affiliation confidence="0.89682">Departamento de Informatica, Universidade Federal de Pernambuco</affiliation>
<address confidence="0.745356">Caixa Postal 7851, Cidade Universitaria Recife, PE 50732-970 Brazil</address>
<abstract confidence="0.962151357142857">This paper presents a quantitative evaluation of the portability to the stock market domain of the revision rule hierarchy by the system to incrementally generate newswire sports summaries. The evaluation consists of searching a test corpus of stock market reports for sentence pairs whose (semantic and syntactic) structures respectively match the triggering condition and application result of each revision rule. The results show that at least 59% of all rule classes are fully portable, with at least another 7% partially portable.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>Many thanks to Kathy McKeown for stressing the importance of empirically evaluating STREAK. The research presented in this paper is currently supported by CNPq (Brazilian Research Council) under post-doctoral research grant 150130-95.3. It started out while I was at Columbia University supported by of a joint grant from the Office of Naval Research, by the Advanced Research Projects Agency under contract N00014-89-J1782, by National Science Foundation Grants IRT-84-51438 and GER-90-2406, and by the</title>
<booktitle>State Science and Technology Foundation under this auspices of the Columbia University CAT in High Performance Computing and Communications in Health Care, a</booktitle>
<institution>State Center for Advanced Technology. CU-CS-005-93. Computer Science Department, Columbia University,</institution>
<location>New York</location>
<marker></marker>
<rawString>Many thanks to Kathy McKeown for stressing the importance of empirically evaluating STREAK. The research presented in this paper is currently supported by CNPq (Brazilian Research Council) under post-doctoral research grant 150130-95.3. It started out while I was at Columbia University supported by of a joint grant from the Office of Naval Research, by the Advanced Research Projects Agency under contract N00014-89-J1782, by National Science Foundation Grants IRT-84-51438 and GER-90-2406, and by the New York State Science and Technology Foundation under this auspices of the Columbia University CAT in High Performance Computing and Communications in Health Care, a New York State Center for Advanced Technology. CU-CS-005-93. Computer Science Department, Columbia University, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Elhadad</author>
<author>J Robin</author>
</authors>
<title>An overview of SURGE: a re-usable comprehensive syntactic realization component.</title>
<date>1996</date>
<tech>Technical Report 96-03.</tech>
<institution>Computer Science and Mathematics Department, Ben Gurion University, Beer Sheva, Israel.</institution>
<contexts>
<context position="23697" citStr="Elhadad and Robin, 1996" startWordPosition="3810" endWordPosition="3813">atches are cases where the semantic label or syntactic sub-category of a constituent added or displaced by the rule differ in each domain (e.g., Adjunctization of Created into Instrument vs. Adjoin of Affected into Instrument). They push portability from 92% down to 71%. Their effect could be reduced by allowing STREAK&apos;S reviser to manipulate the draft down to the surface syntactic role level (e.g., in both corpora Created and Affected surface as object). Currently, the reviser stops at the thematic role level to allow STREAK to take full advantage of the syntactic processing front-end SURGE (Elhadad and Robin, 1996), which accepts such thematic structures as input. Accompanying side revisions push portability from 71% to 52%. This suggests that the design of STREAK could be improved by keeping side revisions separate from re-structuring revisions and interleaving the applications of the two. Currently, they are integrated together at the bottom of the revision rule hierarchy. 6 Related work Apart from STREAK, only three generation projects feature an empirical and quantitative evaluation: ANA (Kukich, 1983), KNIGHT (Lester, 1993) and ImAGENE (Van der Linden, 1993). ANA generates short, newswire style sum</context>
</contexts>
<marker>Elhadad, Robin, 1996</marker>
<rawString>Elhadad, M. and Robin, J. 1996. An overview of SURGE: a re-usable comprehensive syntactic realization component. Technical Report 96-03. Computer Science and Mathematics Department, Ben Gurion University, Beer Sheva, Israel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kukich</author>
</authors>
<title>Knowledge-based report generation: a knowledge engineering approach to natural language report generation.</title>
<date>1983</date>
<tech>PhD. Thesis.</tech>
<institution>Department of Information Science. University of Pittsburgh.</institution>
<contexts>
<context position="24198" citStr="Kukich, 1983" startWordPosition="3887" endWordPosition="3888">vel to allow STREAK to take full advantage of the syntactic processing front-end SURGE (Elhadad and Robin, 1996), which accepts such thematic structures as input. Accompanying side revisions push portability from 71% to 52%. This suggests that the design of STREAK could be improved by keeping side revisions separate from re-structuring revisions and interleaving the applications of the two. Currently, they are integrated together at the bottom of the revision rule hierarchy. 6 Related work Apart from STREAK, only three generation projects feature an empirical and quantitative evaluation: ANA (Kukich, 1983), KNIGHT (Lester, 1993) and ImAGENE (Van der Linden, 1993). ANA generates short, newswire style summaries of the daily fluctuations of several stock market indexes from half-hourly updates of their values. For evaluation, Kukich measures both the conceptual and linguistic (lexical and syntactic) coverages of ANA by comparing the number of concepts and realization patterns identified during a corpus analysis with those actually implemented in the system. KNIGHT generates natural language concept definitions from a large biological knowledge base, relying on SURGE for syntactic realization. For </context>
</contexts>
<marker>Kukich, 1983</marker>
<rawString>Kukich, K. 1983. Knowledge-based report generation: a knowledge engineering approach to natural language report generation. PhD. Thesis. Department of Information Science. University of Pittsburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J C Lester</author>
</authors>
<title>Generating natural language explanations from large-scale knowledge bases.</title>
<date>1993</date>
<tech>PhD. Thesis.</tech>
<institution>Computer Science Department, University of Texas at Austin.</institution>
<contexts>
<context position="24221" citStr="Lester, 1993" startWordPosition="3890" endWordPosition="3891">take full advantage of the syntactic processing front-end SURGE (Elhadad and Robin, 1996), which accepts such thematic structures as input. Accompanying side revisions push portability from 71% to 52%. This suggests that the design of STREAK could be improved by keeping side revisions separate from re-structuring revisions and interleaving the applications of the two. Currently, they are integrated together at the bottom of the revision rule hierarchy. 6 Related work Apart from STREAK, only three generation projects feature an empirical and quantitative evaluation: ANA (Kukich, 1983), KNIGHT (Lester, 1993) and ImAGENE (Van der Linden, 1993). ANA generates short, newswire style summaries of the daily fluctuations of several stock market indexes from half-hourly updates of their values. For evaluation, Kukich measures both the conceptual and linguistic (lexical and syntactic) coverages of ANA by comparing the number of concepts and realization patterns identified during a corpus analysis with those actually implemented in the system. KNIGHT generates natural language concept definitions from a large biological knowledge base, relying on SURGE for syntactic realization. For evaluation, Lester perf</context>
</contexts>
<marker>Lester, 1993</marker>
<rawString>Lester, J.C. 1993. Generating natural language explanations from large-scale knowledge bases. PhD. Thesis. Computer Science Department, University of Texas at Austin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W C Mann</author>
<author>C M Matthiessen</author>
</authors>
<title>NIGEL: a systemic grammar for text generation.</title>
<date>1983</date>
<tech>Research Report RR-83-105.</tech>
<institution>ISI. Marina Del Rey, CA.</institution>
<contexts>
<context position="28104" citStr="Mann and Matthiessen, 1983" startWordPosition="4475" endWordPosition="4478">anized&amp;quot; in the instructions provided to the judges). The judges did not know that half the definitions were computer-generated while the other half were written by four human domain experts. Impressively, the results show that: • With respect to semantic accuracy, the human judges could not tell KNIGHT apart from the human writers. • While as a group, humans got statistically significantly better grades for stylistic accuracy than KNIGHT, the best human writer was singlehandly responsible for this difference. IMAGENE generates instructions on how to operate household devices relying on NIGEL (Mann and Matthiessen, 1983) for syntactic realization. The implementation focuses on a very limited aspect of text generation: the realization of purpose relations. Taking as input the description of a pair &lt;operation, purpose of the operation&gt;, augmented by a set of features simulating the communicative context of generation, IMAGENE selects, among the many realizations of purpose generable by NIGEL (e.g., fronted to-infinitive clause vs. trailing for-gerund clauses), the one that is most appropriate for the simulated context (e.g., in the context of several operations sharing the same purpose, the latter is preferenti</context>
</contexts>
<marker>Mann, Matthiessen, 1983</marker>
<rawString>Mann, W.C. and Matthiessen, C. M. 1983. NIGEL: a systemic grammar for text generation. Research Report RR-83-105. ISI. Marina Del Rey, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Robin</author>
<author>K R McKeown</author>
</authors>
<title>Corpus analysis for revision-based generation of complex sentences.</title>
<date>1993</date>
<booktitle>In Proceedings of the 11th National Conference on Artificial Intelligence,</booktitle>
<location>Washington DC. (AAAI&apos;93).</location>
<contexts>
<context position="1173" citStr="Robin and McKeown, 1993" startWordPosition="167" endWordPosition="170">s for sentence pairs whose (semantic and syntactic) structures respectively match the triggering condition and application result of each revision rule. The results show that at least 59% of all rule classes are fully portable, with at least another 7% partially portable. 1 Introduction The project STREAK&apos; focuses on the specific issues involved in generating short, newswire style, natural language texts that summarize vast amount of input tabular data in their historical context. A series of previous publications presented complementary aspects of this project: motivating corpus analysis in (Robin and McKeown, 1993), new revision-based text generation model in (Robin, 1993), system implementation and rule base in (Robin, 1994a) and empirical evaluation of the robustness and scalability of this new model as compared to the traditional single pass pipeline model in (Robin and McKeown, 1995). The present paper completes this series by describing a second, empirical, corpus-based evaluation, this time quantifying the portability to another domain (the stock market) of the revision rule hierarchy acquired in the sports domain and implemented in STREAK. The goal of this paper is twofold: (1) assessing the gene</context>
</contexts>
<marker>Robin, McKeown, 1993</marker>
<rawString>Robin, J. and McKeown, K.R. 1993. Corpus analysis for revision-based generation of complex sentences. In Proceedings of the 11th National Conference on Artificial Intelligence, Washington DC. (AAAI&apos;93).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Robin</author>
<author>K R McKeown</author>
</authors>
<title>Empirically designing and evaluating a new revision-based model for summary generation. Artificial Intelligence. Vol 85. Special Issue on Empirical Methods.</title>
<date>1995</date>
<publisher>North-Holland.</publisher>
<contexts>
<context position="1451" citStr="Robin and McKeown, 1995" startWordPosition="211" endWordPosition="214">roduction The project STREAK&apos; focuses on the specific issues involved in generating short, newswire style, natural language texts that summarize vast amount of input tabular data in their historical context. A series of previous publications presented complementary aspects of this project: motivating corpus analysis in (Robin and McKeown, 1993), new revision-based text generation model in (Robin, 1993), system implementation and rule base in (Robin, 1994a) and empirical evaluation of the robustness and scalability of this new model as compared to the traditional single pass pipeline model in (Robin and McKeown, 1995). The present paper completes this series by describing a second, empirical, corpus-based evaluation, this time quantifying the portability to another domain (the stock market) of the revision rule hierarchy acquired in the sports domain and implemented in STREAK. The goal of this paper is twofold: (1) assessing the generality of this particular rule hierarchy and (2) providing a general, semi-automatic &apos;Surface Text Reviser Expressing Additional Knowledge. methodology for evaluating the portability of semantic and syntactic knowledge structures used for natural language generation. The result</context>
<context position="29951" citStr="Robin and McKeown, 1995" startWordPosition="4765" endWordPosition="4768">able of Fig. 11 summarizes the difference on both goal and methodology between the evaluations carried out in the projects ANA, KNIGHT, IMAGENE and STREAK. In terms of goals, while Kukich and Lester evaluate the coverage or accuracy of a particular implementation, I instead focus on three properties inherent to the use of the revision-based generation model underlying STREAK: robustness (how much of other text samples from the same domain can be generated without acquiring new knowledge?) and scalability (how much more new knowledge is needed to fully cover these other samples?) discussed in (Robin and McKeown, 1995), and portability to another domain in the present paper. Van der Linden does a little bit of both by first measuring the stylistic accuracy of his system for a very restricted sub-domain, and then measuring how it degrades for a more general domain. In itself, measuring the accuracy and coverage of a particular implementation in the sub-domain for which it was designed brings little insights about what generation approach should be adopted in future work. Indeed, even a system using mere canned text can be very accurate and attain substantial coverage if enough hand-coding effort is put into </context>
</contexts>
<marker>Robin, McKeown, 1995</marker>
<rawString>Robin, J. and McKeown, K.R. 1995. Empirically designing and evaluating a new revision-based model for summary generation. Artificial Intelligence. Vol 85. Special Issue on Empirical Methods. North-Holland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Robin</author>
</authors>
<title>A revision-based generation architecture for reporting facts in their historical context.</title>
<date>1993</date>
<booktitle>In New Concepts in Natural Language Generation: Planning, Realization</booktitle>
<contexts>
<context position="1232" citStr="Robin, 1993" startWordPosition="177" endWordPosition="178">vely match the triggering condition and application result of each revision rule. The results show that at least 59% of all rule classes are fully portable, with at least another 7% partially portable. 1 Introduction The project STREAK&apos; focuses on the specific issues involved in generating short, newswire style, natural language texts that summarize vast amount of input tabular data in their historical context. A series of previous publications presented complementary aspects of this project: motivating corpus analysis in (Robin and McKeown, 1993), new revision-based text generation model in (Robin, 1993), system implementation and rule base in (Robin, 1994a) and empirical evaluation of the robustness and scalability of this new model as compared to the traditional single pass pipeline model in (Robin and McKeown, 1995). The present paper completes this series by describing a second, empirical, corpus-based evaluation, this time quantifying the portability to another domain (the stock market) of the revision rule hierarchy acquired in the sports domain and implemented in STREAK. The goal of this paper is twofold: (1) assessing the generality of this particular rule hierarchy and (2) providing </context>
</contexts>
<marker>Robin, 1993</marker>
<rawString>Robin, J. 1993. A revision-based generation architecture for reporting facts in their historical context. In New Concepts in Natural Language Generation: Planning, Realization and System. Horacek, H. and Zock, M., Eds. Frances Pinter.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Robin</author>
</authors>
<title>Automatic generation and revision of natural language summaries providing historical background</title>
<date>1994</date>
<booktitle>In Proceedings of the 11th Brazilian Symposium on Artificial Intelligence,</booktitle>
<location>Fortaleza,</location>
<contexts>
<context position="1285" citStr="Robin, 1994" startWordPosition="186" endWordPosition="187">sult of each revision rule. The results show that at least 59% of all rule classes are fully portable, with at least another 7% partially portable. 1 Introduction The project STREAK&apos; focuses on the specific issues involved in generating short, newswire style, natural language texts that summarize vast amount of input tabular data in their historical context. A series of previous publications presented complementary aspects of this project: motivating corpus analysis in (Robin and McKeown, 1993), new revision-based text generation model in (Robin, 1993), system implementation and rule base in (Robin, 1994a) and empirical evaluation of the robustness and scalability of this new model as compared to the traditional single pass pipeline model in (Robin and McKeown, 1995). The present paper completes this series by describing a second, empirical, corpus-based evaluation, this time quantifying the portability to another domain (the stock market) of the revision rule hierarchy acquired in the sports domain and implemented in STREAK. The goal of this paper is twofold: (1) assessing the generality of this particular rule hierarchy and (2) providing a general, semi-automatic &apos;Surface Text Reviser Expre</context>
<context position="22910" citStr="Robin, 1994" startWordPosition="3684" endWordPosition="3685">ving the most complex non-monotonic revisions, or those with only a few instances in the sports corpus. Among the bottom-level classes that distinguish between revision rule applications in very specific semantic and syntactic contexts, 42% are same-concept portable with another 10% differentconcept portable. Finally, the correlation between high usage frequency in the acquisition corpus and portability to the test corpus is not statistically significant (i.e., the hypothesis that the more common a rule, the more likely it is to be portable could not be confirmed on the analyzed sample). See (Robin, 1994b) for further details on the evaluation results. There are two main stumbling blocks to portability: thematic role mismatch and side revisions. Thematic role mismatches are cases where the semantic label or syntactic sub-category of a constituent added or displaced by the rule differ in each domain (e.g., Adjunctization of Created into Instrument vs. Adjoin of Affected into Instrument). They push portability from 92% down to 71%. Their effect could be reduced by allowing STREAK&apos;S reviser to manipulate the draft down to the surface syntactic role level (e.g., in both corpora Created and Affect</context>
</contexts>
<marker>Robin, 1994</marker>
<rawString>Robin, J. 1994a. Automatic generation and revision of natural language summaries providing historical background In Proceedings of the 11th Brazilian Symposium on Artificial Intelligence, Fortaleza, Brazil. (SBIA&apos;94).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Robin</author>
</authors>
<title>Revision-based generation of natural language summaries providing historical background: corpus-based analysis, design, implementation and evaluation.</title>
<date>1994</date>
<tech>PhD. Thesis. Available as Technical Report CU-CS-034-94.</tech>
<institution>Computer Science Department, Columbia University,</institution>
<location>New York.</location>
<contexts>
<context position="1285" citStr="Robin, 1994" startWordPosition="186" endWordPosition="187">sult of each revision rule. The results show that at least 59% of all rule classes are fully portable, with at least another 7% partially portable. 1 Introduction The project STREAK&apos; focuses on the specific issues involved in generating short, newswire style, natural language texts that summarize vast amount of input tabular data in their historical context. A series of previous publications presented complementary aspects of this project: motivating corpus analysis in (Robin and McKeown, 1993), new revision-based text generation model in (Robin, 1993), system implementation and rule base in (Robin, 1994a) and empirical evaluation of the robustness and scalability of this new model as compared to the traditional single pass pipeline model in (Robin and McKeown, 1995). The present paper completes this series by describing a second, empirical, corpus-based evaluation, this time quantifying the portability to another domain (the stock market) of the revision rule hierarchy acquired in the sports domain and implemented in STREAK. The goal of this paper is twofold: (1) assessing the generality of this particular rule hierarchy and (2) providing a general, semi-automatic &apos;Surface Text Reviser Expre</context>
<context position="22910" citStr="Robin, 1994" startWordPosition="3684" endWordPosition="3685">ving the most complex non-monotonic revisions, or those with only a few instances in the sports corpus. Among the bottom-level classes that distinguish between revision rule applications in very specific semantic and syntactic contexts, 42% are same-concept portable with another 10% differentconcept portable. Finally, the correlation between high usage frequency in the acquisition corpus and portability to the test corpus is not statistically significant (i.e., the hypothesis that the more common a rule, the more likely it is to be portable could not be confirmed on the analyzed sample). See (Robin, 1994b) for further details on the evaluation results. There are two main stumbling blocks to portability: thematic role mismatch and side revisions. Thematic role mismatches are cases where the semantic label or syntactic sub-category of a constituent added or displaced by the rule differ in each domain (e.g., Adjunctization of Created into Instrument vs. Adjoin of Affected into Instrument). They push portability from 92% down to 71%. Their effect could be reduced by allowing STREAK&apos;S reviser to manipulate the draft down to the surface syntactic role level (e.g., in both corpora Created and Affect</context>
</contexts>
<marker>Robin, 1994</marker>
<rawString>Robin, J. 1994b. Revision-based generation of natural language summaries providing historical background: corpus-based analysis, design, implementation and evaluation. PhD. Thesis. Available as Technical Report CU-CS-034-94. Computer Science Department, Columbia University, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Van der Linden</author>
<author>J H Martin</author>
</authors>
<title>Expressing rhetorical relations in instructional texts: a case study of the purpose relation.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>1</issue>
<publisher>MIT Press.</publisher>
<marker>Van der Linden, Martin, 1995</marker>
<rawString>Van der Linden, K. and Martin, J.H. 1995. Expressing rhetorical relations in instructional texts: a case study of the purpose relation. Computational Linguistics, 21(1). MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Duford</author>
</authors>
<title>CREP: a regular expressionmatching textual corpus tool.</title>
<date>1993</date>
<tech>Technical Report</tech>
<contexts>
<context position="10694" citStr="Duford, 1993" startWordPosition="1719" endWordPosition="1720">other domain, namely the stock market? • The basis for this evaluation is corpus data9 The original sports summary corpus from which the revision rules were acquired is used as the &apos;training&apos; (or acquisition) corpus and a corpus of stock market reports taken from several newswires is used as the &apos;test&apos; corpus. This test corpus comprises over 18,000 sentences. • The evaluation procedure is quantitative, measuring percentages of revision rules whose target and source realization patterns are observable in the test corpus. It is also semi-automated through the use of the corpus search tool CREP (Duford, 1993) (as explained below). Basic principle As explained in section 3, a revision rule is associated with a list of surface decrement pairs, each one consisting of: • A source pattern whose content and linguistic form match the triggering conditions of the rule (e.g., RI in Fig. 3 for the rule Adjunctization of Range into Instrument). • A target pattern whose content and linguistic form can be derived from the source pattern by applying the rule (e.g., R in Fig. 3 for the rule Adjunctization of Range into Instrument). This list of decrement pairs can thus be used as the signature of the revision ru</context>
</contexts>
<marker>Duford, 1993</marker>
<rawString>Duford, D. 1993. CREP: a regular expressionmatching textual corpus tool. Technical Report</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>