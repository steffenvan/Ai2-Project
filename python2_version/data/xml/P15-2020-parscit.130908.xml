<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003336">
<title confidence="0.996346">
Exploiting Image Generality for Lexical Entailment Detection
</title>
<author confidence="0.984682">
Douwe Kiela
</author>
<affiliation confidence="0.9894025">
Computer Laboratory
University of Cambridge
</affiliation>
<email confidence="0.979461">
douwe.kiela@cl.cam.ac.uk
</email>
<author confidence="0.994343">
Ivan Vuli´c
</author>
<affiliation confidence="0.999695">
Department of Computer Science
</affiliation>
<address confidence="0.579307">
KU Leuven
</address>
<email confidence="0.993102">
ivan.vulic@cs.kuleuven.be
</email>
<author confidence="0.989384">
Laura Rimell
</author>
<affiliation confidence="0.992135">
Computer Laboratory
University of Cambridge
</affiliation>
<email confidence="0.980875">
laura.rimell@cl.cam.ac.uk
</email>
<author confidence="0.996181">
Stephen Clark
</author>
<affiliation confidence="0.993593">
Computer Laboratory
University of Cambridge
</affiliation>
<email confidence="0.996736">
stephen.clark@cl.cam.ac.uk
</email>
<sectionHeader confidence="0.993864" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999863416666667">
We exploit the visual properties of con-
cepts for lexical entailment detection by
examining a concept’s generality. We in-
troduce three unsupervised methods for
determining a concept’s generality, based
on its related images, and obtain state-of-
the-art performance on two standard se-
mantic evaluation datasets. We also intro-
duce a novel task that combines hypernym
detection and directionality, significantly
outperforming a competitive frequency-
based baseline.
</bodyText>
<sectionHeader confidence="0.998797" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999497">
Automatic detection of lexical entailment is useful
for a number of NLP tasks including search query
expansion (Shekarpour et al., 2013), recognising
textual entailment (Garrette et al., 2011), metaphor
detection (Mohler et al., 2013), and text genera-
tion (Biran and McKeown, 2013). Given two se-
mantically related words, a key aspect of detecting
lexical entailment, or the hyponym-hypernym re-
lation, is the generality of the hypernym compared
to the hyponym. For example, bird is more general
than eagle, having a broader intension and a larger
extension. This property has led to the introduc-
tion of lexical entailment measures that compare
the entropy of distributional word representations,
under the assumption that a more general term has
a higher-entropy distribution (Herbelot and Gane-
salingam, 2013; Santus et al., 2014).
A strand of distributional semantics has recently
emerged that exploits the fact that meaning is of-
ten grounded in the perceptual system, known as
multi-modal distributional semantics (Bruni et al.,
2014). Such models enhance purely linguistic
models with extra-linguistic perceptual informa-
tion, and outperform language-only models on a
range of tasks, including modelling semantic sim-
ilarity and conceptual relatedness (Silberer and
Lapata, 2014). In fact, under some conditions
uni-modal visual representations outperform tradi-
tional linguistic representations on semantic tasks
(Kiela and Bottou, 2014).
We hypothesize that visual representations can
be particularly useful for lexical entailment detec-
tion. Deselaers and Ferrari (2011) have shown that
sets of images corresponding to terms at higher
levels in the WordNet hierarchy have greater vi-
sual variability than those at lower levels. We ex-
ploit this tendency using sets of images returned
by Google’s image search. The intuition is that
the set of images returned for animal will consist
of pictures of different kinds of animals, the set of
images for bird will consist of pictures of differ-
ent birds, while the set for owl will mostly consist
only of images of owls, as can be seen in Figure 1.
Here we evaluate three different vision-based
methods for measuring term generality on the se-
mantic tasks of hypernym detection and hypernym
directionality. Using this simple yet effective un-
supervised approach, we obtain state-of-the-art re-
sults compared with supervised algorithms which
use linguistic data.
</bodyText>
<sectionHeader confidence="0.999766" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999937909090909">
In the linguistic modality, the most closely related
work is by Herbelot and Ganesalingam (2013) and
Santus et al. (2014), who use unsupervised distri-
butional generality measures to identify the hyper-
nym in a hyponym-hypernym pair. Herbelot and
Ganesalingam (2013) use KL divergence to com-
pare the probability distribution of context words,
given a term, to the background probability dis-
tribution of context words. Santus et al. (2014)
use the median entropy of the probability distribu-
tions associated with a term’s top-weighted con-
</bodyText>
<page confidence="0.917939">
119
</page>
<note confidence="0.378204">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 119–124,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<figureCaption confidence="0.966781">
Figure 1: Example of how vulture and owl are less dispersed concepts than bird and animal, according
to images returned by Google image search.
</figureCaption>
<bodyText confidence="0.999727">
text words as a measure of information content.
In the visual modality, the intuition that visual
representations may be useful for detecting lexi-
cal entailment is inspired by Deselaers and Ferrari
(2011). Using manually annotated images from
ImageNet (Deng et al., 2009), they find that con-
cepts and categories with narrower intensions and
smaller extensions tend to have less visual vari-
ability. We extend this intuition to the unsuper-
vised setting of Google image search results and
apply it to the lexical entailment task.
</bodyText>
<sectionHeader confidence="0.994995" genericHeader="method">
3 Approach
</sectionHeader>
<bodyText confidence="0.999930782608696">
We use two standard evaluations for lexical entail-
ment: hypernym directionality, where the task is to
predict which of two words is the hypernym; and
hypernym detection, where the task is to predict
whether two words are in a hypernym-hyponym
relation (Weeds et al., 2014; Santus et al., 2014).
We also introduce a third, more challenging, eval-
uation that combines detection and directionality.
For the directionality experiment, we evaluate
on the hypernym subset of the well-known BLESS
dataset (Baroni and Lenci, 2011), which consists
of 1337 hyponym-hypernym pairs. In this case, it
is known that the words are in an entailment re-
lation and the task is to predict the directionality
of the relation. BLESS data is always presented
with the hyponym first, so we report how often our
measures predict that the second term in the pair is
more general than the first.
For the detection experiment, we evaluate on the
BLESS-based dataset of Weeds et al. (2014), which
consists of 1168 word pairs and which we call
WBLESS. In this dataset, the positive examples are
hyponym-hypernym pairs. The negative examples
</bodyText>
<equation confidence="0.416506">
BLESS turtle—animal 1
owl—creature 1
WBLESS owl—vulture 0
</equation>
<bodyText confidence="0.92321775">
animal—owl 0
owl—creature 1
BIBLESS owl—vulture 0
animal—owl -1
</bodyText>
<tableCaption confidence="0.947239">
Table 1: Examples for evaluation datasets.
</tableCaption>
<bodyText confidence="0.999955416666667">
include pairs in the reversed hypernym-hyponym
order, as well as holonym-meronym pairs, co-
hyponyms, and randomly matched nouns. Ac-
curacy on WBLESS reflects the ability to distin-
guish hypernymy from other relations, but does
not require detection of directionality, since re-
versed pairs are grouped with the other negatives.
For the combined experiment, we assign re-
versed hyponym-hypernym pairs a value of -1 in-
stead of 0. We call this more challenging dataset
BIBLESS. Examples of pairs in the respective
datasets can be found in Table 1.
</bodyText>
<subsectionHeader confidence="0.999451">
3.1 Image representations
</subsectionHeader>
<bodyText confidence="0.999264555555555">
Following previous work in multi-modal seman-
tics (Bergsma and Goebel, 2011; Kiela et al.,
2014), we obtain images from Google Images1
for the words in the evaluation datasets. It has
been shown that images from Google yield higher-
quality representations than comparable resources
such as Flickr and are competitive with “hand pre-
pared datasets” (Bergsma and Goebel, 2011; Fer-
gus et al., 2005).
</bodyText>
<footnote confidence="0.980194">
1www.google.com/imghp. Images were retrieved on
10 April, 2015 from Cambridge in the United Kingdom.
</footnote>
<page confidence="0.993465">
120
</page>
<bodyText confidence="0.999851666666667">
For each image, we extract the pre-softmax
layer from a forward pass in a convolutional neural
network (CNN) that has been trained on the Im-
ageNet classification task using Caffe (Jia et al.,
2014). As such, this work is an instance of deep
transfer learning; that is, a deep learning represen-
tation trained on one task (image classification) is
used to make predictions on a different task (im-
age generality). We chose to use CNN-derived im-
age representations because they have been found
to be of higher quality than the traditional bag of
visual words models (Sivic and Zisserman, 2003)
that have previously been used in multi-modal dis-
tributional semantics (Bruni et al., 2014; Kiela and
Bottou, 2014).
</bodyText>
<subsectionHeader confidence="0.99696">
3.2 Generality measures
</subsectionHeader>
<bodyText confidence="0.999989176470588">
We propose three measures that can be used to cal-
culate the generality of a set of images. The image
dispersion d of a concept word w is defined as the
average pairwise cosine distance between all im-
age representations { ~w1 ... ~wn} of the set of im-
ages returned for w:
This measure was originally introduced to account
for the fact that perceptual information is more rel-
evant for e.g. elephant than it is for happiness. It
acts as a substitute for the concreteness of a word
and can be used to regulate how much perceptual
information should be included in a multi-modal
model (Kiela et al., 2014).
Our second measure follows Deselaers and Fer-
rari (2011), who take a similar approach but in-
stead of calculating the pairwise distance calculate
the distance to the centroid µ~ of { ~w1 ... ~wn}:
</bodyText>
<equation confidence="0.9973015">
1 � c(w) = 1 − cos( ~wi, ~µ) (2)
n 1&lt;i&lt;n
</equation>
<bodyText confidence="0.999958">
For our third measure we follow Lazaridou et al.
(2015), who try different ways of modulating the
inclusion of perceptual input in their multi-modal
skip-gram model, and find that the entropy of the
</bodyText>
<equation confidence="0.8281656">
centroid vector µ~ works well (where p(µj) = µ&apos;
||~µ||
and m is the vector length):
H(w) = − �m p(µj)log2(p(µj)) (3)
j=1
</equation>
<subsectionHeader confidence="0.998489">
3.3 Hypernym Detection and Directionality
</subsectionHeader>
<bodyText confidence="0.999988166666667">
We calculate the directionality of a hyponym-
hypernym pair with a measure f using the follow-
ing formula for a word pair (p, q). Since even co-
hyponyms will not have identical values for f, we
introduce a threshold α which sets a minimum dif-
ference in generality for hypernym identification:
</bodyText>
<equation confidence="0.995175">
s(p,q) = 1 − f(p) + α (4)
f(q)
</equation>
<bodyText confidence="0.999488571428571">
In other words, s(p, q) &gt; 0 iff f(q) &gt; f(p) + α,
i.e. if the second word (q) is (sufficiently) more
general. To avoid false positives where one word
is more general but the pair is not semantically
related, we introduce a second threshold θ which
sets f to zero if the two concepts have low cosine
similarity. This leads to the following formula:
</bodyText>
<equation confidence="0.8968405">
� 1 − f(p)+α if cos(µ~p ~µq) ≥ θ
(5)
</equation>
<bodyText confidence="0.999917777777778">
We experimented with different methods for ob-
taining the mean vector representations for co-
sine (hereafter µc) in Equation (5), and found
that multi-modal representations worked best. We
concatenate an L2-normalized linguistic vector
with the L2-normalized centroid of image vectors
to obtain a multi-modal representation, following
Kiela and Bottou (2014). For a word p with im-
age representations {pimg
</bodyText>
<equation confidence="0.568952">
1 ... pimg
n }, we thus set
µc = pling   ||n �i pimg, after normalizing both
</equation>
<bodyText confidence="0.998318692307692">
representations. For comparison, we also report
results for a visual-only µc.
For BLESS, we know the words in a pair stand
in an entailment relation, so we set α = θ =
0 and evaluate whether s(p, q) &gt; 0, indicating
that q is a hypernym of p. For WBLESS, we set
α = 0.02 and θ = 0.2 without tuning, and eval-
uate whether sθ(p, q) &gt; 0 (hypernym relation) or
sθ(p, q) G 0 (no hypernym relation). For BIB-
LESS, we set α = 0.02 and θ = 0.25 with-
out tuning, and evaluate whether sθ(p, q) &gt; 0
(hyponym-hypernym), s(p, q) = 0 (no relation),
or s(p, q) G 0 (hypernym-hyponym).
</bodyText>
<sectionHeader confidence="0.999964" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.9999774">
The results can be found in Table 2. We com-
pare our methods with a frequency baseline, set-
ting f(p) = freq(p) in Equation 4 and using
the frequency scores from Turney et al. (2011).
Frequency has been proven to be a surprisingly
</bodyText>
<equation confidence="0.987566428571429">
d(w) = n(n2 1) 1 − cos ( ~wi, ~wj) (1)
i&lt;j&lt;n
q
sθ(p,q) = f(
)
0
otherwise
</equation>
<page confidence="0.992628">
121
</page>
<table confidence="0.998640888888889">
BLESS WBLESS BIBLESS
Frequency 0.58 0.57 0.39
WeedsPrec 0.63 — —
WeedsSVM — 0.75 —
WeedsUnSup — 0.58 —
SLQS 0.87 — —
Dispersion 0.88 0.75 (0.74) 0.57 (0.55)
Centroid 0.87 0.74 (0.74) 0.57 (0.54)
Entropy 0.83 0.71 (0.71) 0.56 (0.53)
</table>
<tableCaption confidence="0.977775333333333">
Table 2: Accuracy. For WBLESS and BIBLESS we
report results for multi-modal µC, with visual-only
µC in brackets.
</tableCaption>
<bodyText confidence="0.99538">
challenging baseline for hypernym directionality
(Herbelot and Ganesalingam, 2013; Weeds et al.,
2014). In addition, we compare to the reported re-
sults of Santus et al. (2014) for WeedsPrec (Weeds
et al., 2004), an early lexical entailment mea-
sure, and SLQS, the entropy-based method of
Santus et al. (2014). Note, however, that these
are on a subsampled corpus of 1277 word pairs
from BLESS, so the results are indicative but not
directly comparable. On WBLESS we compare
to the reported results of Weeds et al. (2014):
we include results for the highest-performing su-
pervised method (WeedsSVM) and the highest-
performing unsupervised method (WeedsUnSup).
For BLESS, both dispersion and centroid dis-
tance reach or outperform the best other measure
(SLQS). They beat the frequency baseline by a
large margin (+30% and +29%). Taking the en-
tropy of the mean image representations does not
appear to do as well as the other two methods
but still outperforms the baseline and WeedsPrec
(+25% and +20% respectively).
In the case of WBLESS and BIBLESS, we
see a similar pattern in that dispersion and cen-
troid distance perform best. For WBLESS, these
methods outperform the other unsupervised ap-
proach, WeedsUnsup, by +17% and match the
best-performing support vector machine (SVM)
approach in Weeds et al. (2014). In fact, Weeds et
al. (2014) report results for a total of 6 supervised
methods (based on SVM and k-nearest neighbor
(k-NN) classifiers): our unsupervised image dis-
persion method outperforms all of these except for
the highest-performing one, reported here.
We can see that the task becomes increasingly
difficult as we go from directionality to detection
to the combination: the dispersion-based method
goes from 0.88 to 0.75 to 0.57, for example. BIB-
LESS is the most difficult, as shown by the fre-
</bodyText>
<figureCaption confidence="0.965614">
Figure 2: Accuracy by WordNet shortest path
bucket (1 is shortest, 5 is longest).
</figureCaption>
<bodyText confidence="0.995443809523809">
quency baseline obtaining only 0.39. Our methods
do much better than this baseline (+18%). Image
dispersion appears to be the most robust measure.
To examine our results further, we divided the
test data into buckets by the shortest WordNet path
connecting word pairs (Miller, 1995). We expect
our method to be less accurate on word pairs with
short paths, since the difference in generality may
be difficult to discern. It has also been suggested
that very abstract hypernyms such as object and
entity are difficult to detect because their linguistic
distributions are not supersets of their hyponyms’
distributions (Rimell, 2014), a factor that should
not affect the visual modality. We find that con-
cept comparisons with a very short path (bucket 1)
are indeed the least accurate. We also find some
drop in accuracy on the longest paths (bucket 5),
especially for WBLESS and BIBLESS, perhaps be-
cause semantic similarity is difficult to detect in
these cases. For a histogram of the accuracy scores
according to WordNet similarity, see Figure 2.
</bodyText>
<sectionHeader confidence="0.997162" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999488578947368">
We have evaluated three unsupervised methods for
determining the generality of a concept based on
its visual properties. Our best-performing method,
image dispersion, reaches the state-of-the-art on
two standard semantic evaluation datasets. We
introduced a novel, more difficult task combin-
ing hypernym detection and directionality, and
showed that our methods outperform a frequency
baseline by a large margin.
We believe that image generality may be par-
ticularly suited to entailment detection because it
does not suffer from the same issues as linguis-
tic distributional generality. Herbelot and Gane-
salingam (2013) found that general terms like liq-
uid do not always have higher entropy distribu-
tions than their hyponyms, since speakers use
them in very specific contexts, e.g. liquid is often
coordinated with gas.
We also acknowledge that our method depends
</bodyText>
<page confidence="0.994787">
122
</page>
<bodyText confidence="0.9998233">
to some degree on Google’s search result diversifi-
cation, but do not feel this detracts from the utility
of the method, since the fact that general concepts
achieve greater maximum image dispersion than
specific concepts is not dependent on any partic-
ular diversification algorithm. In future work, we
plan to explore more sophisticated visual gener-
ality measures, other semantic relations and dif-
ferent ways of fusing visual representations with
linguistic knowledge.
</bodyText>
<sectionHeader confidence="0.984004" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.994900625">
DK and LR are supported by EPSRC grant
EP/I037512/1. IV is supported by the PARIS
project (IWT-SBO 110067) and the PDM Kort
postdoctoral fellowship from KU Leuven. SC
is supported by ERC Starting Grant DisCoTex
(306920) and EPSRC grant EP/I037512/1. We
thank the anonymous reviewers for their helpful
comments.
</bodyText>
<sectionHeader confidence="0.998052" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999378269230769">
Marco Baroni and Alessandro Lenci. 2011. How
we BLESSed distributional semantic evaluation. In
Proceedings of the GEMS 2011 Workshop, pages 1–
10.
Shane Bergsma and Randy Goebel. 2011. Using vi-
sual information to predict lexical preference. In
Proceedings of RANLP, pages 399–405.
Or Biran and Kathleen McKeown. 2013. Classifying
taxonomic relations between pairs of wikipedia arti-
cles. In Proceedings of IJCNLP, pages 788–794.
Johan Bos and Katja Markert. 2005. Recognising tex-
tual entailment with logical inference. In Proceed-
ings of EMNLP, pages 628–635.
Elia Bruni, Nam-Khanh Tran, and Marco Baroni.
2014. Multimodal distributional semantics. Journal
of Artificial Intelligence Research, 49:1–47.
Daoud Clarke. 2009. Context-theoretic semantics for
natural language: An overview. In Proceedings of
the GEMS 2009 Workshop, pages 112–119.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Fei-Fei Li. 2009. ImageNet: A large-scale hier-
archical image database. In Proceedings of CVPR,
pages 248–255.
Thomas Deselaers and Vittorio Ferrari. 2011. Visual
and semantic similarity in imagenet. In Proceedings
of CVPR, pages 1777–1784.
Robert Fergus, Li Fei-Fei, Pietro Perona, and Andrew
Zisserman. 2005. Learning object categories from
Google’s image search. In Proceedings of ICCV,
pages 1816–1823.
Dan Garrette, Katrin Erk, and Raymond Mooney.
2011. Integrating logical representations with prob-
abilistic information using Markov logic. In Pro-
ceedings of IWCS, pages 105–114.
M. Geffet and I. Dagan. 2005. The distributional in-
clusion hypotheses and lexical entailment. In Pro-
ceedings of ACL, pages 107–114.
Aur´elie Herbelot and Mohan Ganesalingam. 2013.
Measuring semantic content in distributional vec-
tors. In Proceedings of ACL, pages 440–445.
Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey
Karayev, Jonathan Long, Ross Girshick, Sergio
Guadarrama, and Trevor Darrell. 2014. Caffe: Con-
volutional architecture for fast feature embedding.
arXiv preprint arXiv:1408.5093.
Hongyan Jing. 1998. Usage of wordnet in natural
language generation. In Proceedings of COLING-
ACL’98 Workshop on Usage of WordNet in Natural
Language Processing Systems.
Douwe Kiela and L´eon Bottou. 2014. Learning image
embeddings using convolutional neural networks for
improved multi-modal semantics. In Proceedings of
EMNLP, pages 36–45.
Douwe Kiela, Felix Hill, Anna Korhonen, and Stephen
Clark. 2014. Improving multi-modal representa-
tions using image dispersion: Why less is sometimes
more. In Proceedings of ACL, pages 835–841.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distribu-
tional similarity for lexical inference. Natural Lan-
guage Engineering, 16(4):359–389.
Angeliki Lazaridou, Nghia The Pham, and Marco Ba-
roni. 2015. Combining language and vision with
a multimodal skip-gram model. In Proceedings of
NAACL-HLT.
Alessandro Lenci and Giulia Benotto. 2012. Identify-
ing hypernyms in distributional semantic spaces. In
Proceedings of *SEM, pages 75–79.
Omer Levy, Steffen Remus, Chris Biemann, and Ido
Dagan. 2015. Do supervised distributional methods
really learn lexical inference relations? In Proceed-
ings of NAACL.
J.H. Martin. 1990. A Computational Model of
Metaphor Interpretation. Academic Press Profes-
sional, Inc.
George A. Miller. 1995. WordNet: A lexical database
for English. In Communications of the ACM, vol-
ume 38, pages 39–41.
</reference>
<page confidence="0.986574">
123
</page>
<reference confidence="0.999819357142857">
Michael Mohler, David Bracewell, Marc Tomlinson,
and David Hinote. 2013. Semantic signatures for
example-based linguistic metaphor detection. In
Proceedings of the 1st Workshop on Metaphor in
NLP.
Laura Rimell. 2014. Distributional lexical entailment
by topic coherence. In Proceedings of EACL, pages
511–519.
Enrico Santus, Alessandro Lenci, Qin Lu, and
Sabine Schulte im Walde. 2014. Chasing hyper-
nyms in vector spaces with entropy. In Proceedings
of EACL, pages 38–42.
Saeedeh Shekarpour, Konrad H¨offner, Jens Lehmann,
and S¨oren Auer. 2013. Keyword query expansion
on linked data using linguistic and semantic features.
In Proceedings of the 7th IEEE International Con-
ference on Semantic Computing, pages 191–197.
Carina Silberer and Mirella Lapata. 2014. Learn-
ing grounded meaning representations with autoen-
coders. In Proceedings of ACL, pages 721–732.
Josef Sivic and Andrew Zisserman. 2003. Video
Google: A text retrieval approach to object match-
ing in videos. In Proceedings of ICCV, pages 1470–
1477.
Peter D. Turney, Yair Neuman, Dan Assaf, and Yohai
Cohen. 2011. Literal and metaphorical sense iden-
tification through concrete and abstract context. In
Proceedings of EMNLP, pages 680–690.
Julie Weeds, David Weir, and Diana McCarthy. 2004.
Characterising measures of lexical distributional
similarity. In Proceedings of COLING.
Julie Weeds, Daoud Clarke, Jeremy Reffin, David Weir,
and Bill Keller. 2014. Learning to distinguish hy-
pernyms and co-hyponyms. In Proceedings of COL-
ING, pages 2249–2259.
W. A. Woods, Stephen Green, Paul Martin, and Ann
Houston. 2001. Aggressive morphology and lexi-
cal relations for query expansion. In Proceedings of
TREC.
M. Zhitomirsky-Geffet and I. Dagan. 2009. Bootstrap-
ping distributional feature vector quality. Computa-
tional Linguistics, 35(3):435461.
</reference>
<page confidence="0.998317">
124
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.163863">
<title confidence="0.999852">Exploiting Image Generality for Lexical Entailment Detection</title>
<author confidence="0.692598">Douwe</author>
<affiliation confidence="0.9682105">Computer University of</affiliation>
<email confidence="0.959886">douwe.kiela@cl.cam.ac.uk</email>
<author confidence="0.982483">Ivan</author>
<affiliation confidence="0.998366">Department of Computer</affiliation>
<address confidence="0.374853">KU</address>
<email confidence="0.661349">ivan.vulic@cs.kuleuven.be</email>
<author confidence="0.805032">Laura</author>
<affiliation confidence="0.992919">Computer University of</affiliation>
<email confidence="0.97504">laura.rimell@cl.cam.ac.uk</email>
<author confidence="0.988303">Stephen</author>
<affiliation confidence="0.995603">Computer University of</affiliation>
<email confidence="0.98918">stephen.clark@cl.cam.ac.uk</email>
<abstract confidence="0.995575461538461">We exploit the visual properties of concepts for lexical entailment detection by a concept’s We introduce three unsupervised methods for determining a concept’s generality, based on its related images, and obtain state-ofthe-art performance on two standard semantic evaluation datasets. We also introduce a novel task that combines hypernym detection and directionality, significantly outperforming a competitive frequencybased baseline.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Alessandro Lenci</author>
</authors>
<title>How we BLESSed distributional semantic evaluation.</title>
<date>2011</date>
<booktitle>In Proceedings of the GEMS 2011 Workshop,</booktitle>
<pages>1--10</pages>
<contexts>
<context position="5306" citStr="Baroni and Lenci, 2011" startWordPosition="784" endWordPosition="787">rvised setting of Google image search results and apply it to the lexical entailment task. 3 Approach We use two standard evaluations for lexical entailment: hypernym directionality, where the task is to predict which of two words is the hypernym; and hypernym detection, where the task is to predict whether two words are in a hypernym-hyponym relation (Weeds et al., 2014; Santus et al., 2014). We also introduce a third, more challenging, evaluation that combines detection and directionality. For the directionality experiment, we evaluate on the hypernym subset of the well-known BLESS dataset (Baroni and Lenci, 2011), which consists of 1337 hyponym-hypernym pairs. In this case, it is known that the words are in an entailment relation and the task is to predict the directionality of the relation. BLESS data is always presented with the hyponym first, so we report how often our measures predict that the second term in the pair is more general than the first. For the detection experiment, we evaluate on the BLESS-based dataset of Weeds et al. (2014), which consists of 1168 word pairs and which we call WBLESS. In this dataset, the positive examples are hyponym-hypernym pairs. The negative examples BLESS turtl</context>
</contexts>
<marker>Baroni, Lenci, 2011</marker>
<rawString>Marco Baroni and Alessandro Lenci. 2011. How we BLESSed distributional semantic evaluation. In Proceedings of the GEMS 2011 Workshop, pages 1– 10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Randy Goebel</author>
</authors>
<title>Using visual information to predict lexical preference.</title>
<date>2011</date>
<booktitle>In Proceedings of RANLP,</booktitle>
<pages>399--405</pages>
<contexts>
<context position="6700" citStr="Bergsma and Goebel, 2011" startWordPosition="1008" endWordPosition="1011"> the reversed hypernym-hyponym order, as well as holonym-meronym pairs, cohyponyms, and randomly matched nouns. Accuracy on WBLESS reflects the ability to distinguish hypernymy from other relations, but does not require detection of directionality, since reversed pairs are grouped with the other negatives. For the combined experiment, we assign reversed hyponym-hypernym pairs a value of -1 instead of 0. We call this more challenging dataset BIBLESS. Examples of pairs in the respective datasets can be found in Table 1. 3.1 Image representations Following previous work in multi-modal semantics (Bergsma and Goebel, 2011; Kiela et al., 2014), we obtain images from Google Images1 for the words in the evaluation datasets. It has been shown that images from Google yield higherquality representations than comparable resources such as Flickr and are competitive with “hand prepared datasets” (Bergsma and Goebel, 2011; Fergus et al., 2005). 1www.google.com/imghp. Images were retrieved on 10 April, 2015 from Cambridge in the United Kingdom. 120 For each image, we extract the pre-softmax layer from a forward pass in a convolutional neural network (CNN) that has been trained on the ImageNet classification task using Ca</context>
</contexts>
<marker>Bergsma, Goebel, 2011</marker>
<rawString>Shane Bergsma and Randy Goebel. 2011. Using visual information to predict lexical preference. In Proceedings of RANLP, pages 399–405.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Or Biran</author>
<author>Kathleen McKeown</author>
</authors>
<title>Classifying taxonomic relations between pairs of wikipedia articles.</title>
<date>2013</date>
<booktitle>In Proceedings of IJCNLP,</booktitle>
<pages>788--794</pages>
<contexts>
<context position="1154" citStr="Biran and McKeown, 2013" startWordPosition="146" endWordPosition="149">e unsupervised methods for determining a concept’s generality, based on its related images, and obtain state-ofthe-art performance on two standard semantic evaluation datasets. We also introduce a novel task that combines hypernym detection and directionality, significantly outperforming a competitive frequencybased baseline. 1 Introduction Automatic detection of lexical entailment is useful for a number of NLP tasks including search query expansion (Shekarpour et al., 2013), recognising textual entailment (Garrette et al., 2011), metaphor detection (Mohler et al., 2013), and text generation (Biran and McKeown, 2013). Given two semantically related words, a key aspect of detecting lexical entailment, or the hyponym-hypernym relation, is the generality of the hypernym compared to the hyponym. For example, bird is more general than eagle, having a broader intension and a larger extension. This property has led to the introduction of lexical entailment measures that compare the entropy of distributional word representations, under the assumption that a more general term has a higher-entropy distribution (Herbelot and Ganesalingam, 2013; Santus et al., 2014). A strand of distributional semantics has recently </context>
</contexts>
<marker>Biran, McKeown, 2013</marker>
<rawString>Or Biran and Kathleen McKeown. 2013. Classifying taxonomic relations between pairs of wikipedia articles. In Proceedings of IJCNLP, pages 788–794.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bos</author>
<author>Katja Markert</author>
</authors>
<title>Recognising textual entailment with logical inference.</title>
<date>2005</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>628--635</pages>
<marker>Bos, Markert, 2005</marker>
<rawString>Johan Bos and Katja Markert. 2005. Recognising textual entailment with logical inference. In Proceedings of EMNLP, pages 628–635.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elia Bruni</author>
<author>Nam-Khanh Tran</author>
<author>Marco Baroni</author>
</authors>
<title>Multimodal distributional semantics.</title>
<date>2014</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>49--1</pages>
<contexts>
<context position="1908" citStr="Bruni et al., 2014" startWordPosition="262" endWordPosition="265">ty of the hypernym compared to the hyponym. For example, bird is more general than eagle, having a broader intension and a larger extension. This property has led to the introduction of lexical entailment measures that compare the entropy of distributional word representations, under the assumption that a more general term has a higher-entropy distribution (Herbelot and Ganesalingam, 2013; Santus et al., 2014). A strand of distributional semantics has recently emerged that exploits the fact that meaning is often grounded in the perceptual system, known as multi-modal distributional semantics (Bruni et al., 2014). Such models enhance purely linguistic models with extra-linguistic perceptual information, and outperform language-only models on a range of tasks, including modelling semantic similarity and conceptual relatedness (Silberer and Lapata, 2014). In fact, under some conditions uni-modal visual representations outperform traditional linguistic representations on semantic tasks (Kiela and Bottou, 2014). We hypothesize that visual representations can be particularly useful for lexical entailment detection. Deselaers and Ferrari (2011) have shown that sets of images corresponding to terms at higher</context>
<context position="7805" citStr="Bruni et al., 2014" startWordPosition="1189" endWordPosition="1192">d pass in a convolutional neural network (CNN) that has been trained on the ImageNet classification task using Caffe (Jia et al., 2014). As such, this work is an instance of deep transfer learning; that is, a deep learning representation trained on one task (image classification) is used to make predictions on a different task (image generality). We chose to use CNN-derived image representations because they have been found to be of higher quality than the traditional bag of visual words models (Sivic and Zisserman, 2003) that have previously been used in multi-modal distributional semantics (Bruni et al., 2014; Kiela and Bottou, 2014). 3.2 Generality measures We propose three measures that can be used to calculate the generality of a set of images. The image dispersion d of a concept word w is defined as the average pairwise cosine distance between all image representations { ~w1 ... ~wn} of the set of images returned for w: This measure was originally introduced to account for the fact that perceptual information is more relevant for e.g. elephant than it is for happiness. It acts as a substitute for the concreteness of a word and can be used to regulate how much perceptual information should be i</context>
</contexts>
<marker>Bruni, Tran, Baroni, 2014</marker>
<rawString>Elia Bruni, Nam-Khanh Tran, and Marco Baroni. 2014. Multimodal distributional semantics. Journal of Artificial Intelligence Research, 49:1–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daoud Clarke</author>
</authors>
<title>Context-theoretic semantics for natural language: An overview.</title>
<date>2009</date>
<booktitle>In Proceedings of the GEMS 2009 Workshop,</booktitle>
<pages>112--119</pages>
<marker>Clarke, 2009</marker>
<rawString>Daoud Clarke. 2009. Context-theoretic semantics for natural language: An overview. In Proceedings of the GEMS 2009 Workshop, pages 112–119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jia Deng</author>
<author>Wei Dong</author>
<author>Richard Socher</author>
<author>Li-Jia Li</author>
<author>Kai Li</author>
<author>Fei-Fei Li</author>
</authors>
<title>ImageNet: A large-scale hierarchical image database.</title>
<date>2009</date>
<booktitle>In Proceedings of CVPR,</booktitle>
<pages>248--255</pages>
<contexts>
<context position="4518" citStr="Deng et al., 2009" startWordPosition="658" endWordPosition="661">al Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 119–124, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Figure 1: Example of how vulture and owl are less dispersed concepts than bird and animal, according to images returned by Google image search. text words as a measure of information content. In the visual modality, the intuition that visual representations may be useful for detecting lexical entailment is inspired by Deselaers and Ferrari (2011). Using manually annotated images from ImageNet (Deng et al., 2009), they find that concepts and categories with narrower intensions and smaller extensions tend to have less visual variability. We extend this intuition to the unsupervised setting of Google image search results and apply it to the lexical entailment task. 3 Approach We use two standard evaluations for lexical entailment: hypernym directionality, where the task is to predict which of two words is the hypernym; and hypernym detection, where the task is to predict whether two words are in a hypernym-hyponym relation (Weeds et al., 2014; Santus et al., 2014). We also introduce a third, more challe</context>
</contexts>
<marker>Deng, Dong, Socher, Li, Li, Li, 2009</marker>
<rawString>Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li. 2009. ImageNet: A large-scale hierarchical image database. In Proceedings of CVPR, pages 248–255.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Deselaers</author>
<author>Vittorio Ferrari</author>
</authors>
<title>Visual and semantic similarity in imagenet.</title>
<date>2011</date>
<booktitle>In Proceedings of CVPR,</booktitle>
<pages>1777--1784</pages>
<contexts>
<context position="2444" citStr="Deselaers and Ferrari (2011)" startWordPosition="332" endWordPosition="335">in the perceptual system, known as multi-modal distributional semantics (Bruni et al., 2014). Such models enhance purely linguistic models with extra-linguistic perceptual information, and outperform language-only models on a range of tasks, including modelling semantic similarity and conceptual relatedness (Silberer and Lapata, 2014). In fact, under some conditions uni-modal visual representations outperform traditional linguistic representations on semantic tasks (Kiela and Bottou, 2014). We hypothesize that visual representations can be particularly useful for lexical entailment detection. Deselaers and Ferrari (2011) have shown that sets of images corresponding to terms at higher levels in the WordNet hierarchy have greater visual variability than those at lower levels. We exploit this tendency using sets of images returned by Google’s image search. The intuition is that the set of images returned for animal will consist of pictures of different kinds of animals, the set of images for bird will consist of pictures of different birds, while the set for owl will mostly consist only of images of owls, as can be seen in Figure 1. Here we evaluate three different vision-based methods for measuring term general</context>
<context position="4451" citStr="Deselaers and Ferrari (2011)" startWordPosition="648" endWordPosition="651">119 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 119–124, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Figure 1: Example of how vulture and owl are less dispersed concepts than bird and animal, according to images returned by Google image search. text words as a measure of information content. In the visual modality, the intuition that visual representations may be useful for detecting lexical entailment is inspired by Deselaers and Ferrari (2011). Using manually annotated images from ImageNet (Deng et al., 2009), they find that concepts and categories with narrower intensions and smaller extensions tend to have less visual variability. We extend this intuition to the unsupervised setting of Google image search results and apply it to the lexical entailment task. 3 Approach We use two standard evaluations for lexical entailment: hypernym directionality, where the task is to predict which of two words is the hypernym; and hypernym detection, where the task is to predict whether two words are in a hypernym-hyponym relation (Weeds et al.,</context>
<context position="8513" citStr="Deselaers and Ferrari (2011)" startWordPosition="1314" endWordPosition="1318">t can be used to calculate the generality of a set of images. The image dispersion d of a concept word w is defined as the average pairwise cosine distance between all image representations { ~w1 ... ~wn} of the set of images returned for w: This measure was originally introduced to account for the fact that perceptual information is more relevant for e.g. elephant than it is for happiness. It acts as a substitute for the concreteness of a word and can be used to regulate how much perceptual information should be included in a multi-modal model (Kiela et al., 2014). Our second measure follows Deselaers and Ferrari (2011), who take a similar approach but instead of calculating the pairwise distance calculate the distance to the centroid µ~ of { ~w1 ... ~wn}: 1 � c(w) = 1 − cos( ~wi, ~µ) (2) n 1&lt;i&lt;n For our third measure we follow Lazaridou et al. (2015), who try different ways of modulating the inclusion of perceptual input in their multi-modal skip-gram model, and find that the entropy of the centroid vector µ~ works well (where p(µj) = µ&apos; ||~µ|| and m is the vector length): H(w) = − �m p(µj)log2(p(µj)) (3) j=1 3.3 Hypernym Detection and Directionality We calculate the directionality of a hyponymhypernym pair</context>
</contexts>
<marker>Deselaers, Ferrari, 2011</marker>
<rawString>Thomas Deselaers and Vittorio Ferrari. 2011. Visual and semantic similarity in imagenet. In Proceedings of CVPR, pages 1777–1784.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Fergus</author>
<author>Li Fei-Fei</author>
<author>Pietro Perona</author>
<author>Andrew Zisserman</author>
</authors>
<title>Learning object categories from Google’s image search.</title>
<date>2005</date>
<booktitle>In Proceedings of ICCV,</booktitle>
<pages>1816--1823</pages>
<contexts>
<context position="7018" citStr="Fergus et al., 2005" startWordPosition="1059" endWordPosition="1063">ed experiment, we assign reversed hyponym-hypernym pairs a value of -1 instead of 0. We call this more challenging dataset BIBLESS. Examples of pairs in the respective datasets can be found in Table 1. 3.1 Image representations Following previous work in multi-modal semantics (Bergsma and Goebel, 2011; Kiela et al., 2014), we obtain images from Google Images1 for the words in the evaluation datasets. It has been shown that images from Google yield higherquality representations than comparable resources such as Flickr and are competitive with “hand prepared datasets” (Bergsma and Goebel, 2011; Fergus et al., 2005). 1www.google.com/imghp. Images were retrieved on 10 April, 2015 from Cambridge in the United Kingdom. 120 For each image, we extract the pre-softmax layer from a forward pass in a convolutional neural network (CNN) that has been trained on the ImageNet classification task using Caffe (Jia et al., 2014). As such, this work is an instance of deep transfer learning; that is, a deep learning representation trained on one task (image classification) is used to make predictions on a different task (image generality). We chose to use CNN-derived image representations because they have been found to </context>
</contexts>
<marker>Fergus, Fei-Fei, Perona, Zisserman, 2005</marker>
<rawString>Robert Fergus, Li Fei-Fei, Pietro Perona, and Andrew Zisserman. 2005. Learning object categories from Google’s image search. In Proceedings of ICCV, pages 1816–1823.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Garrette</author>
<author>Katrin Erk</author>
<author>Raymond Mooney</author>
</authors>
<title>Integrating logical representations with probabilistic information using Markov logic.</title>
<date>2011</date>
<booktitle>In Proceedings of IWCS,</booktitle>
<pages>105--114</pages>
<contexts>
<context position="1065" citStr="Garrette et al., 2011" startWordPosition="132" endWordPosition="135">for lexical entailment detection by examining a concept’s generality. We introduce three unsupervised methods for determining a concept’s generality, based on its related images, and obtain state-ofthe-art performance on two standard semantic evaluation datasets. We also introduce a novel task that combines hypernym detection and directionality, significantly outperforming a competitive frequencybased baseline. 1 Introduction Automatic detection of lexical entailment is useful for a number of NLP tasks including search query expansion (Shekarpour et al., 2013), recognising textual entailment (Garrette et al., 2011), metaphor detection (Mohler et al., 2013), and text generation (Biran and McKeown, 2013). Given two semantically related words, a key aspect of detecting lexical entailment, or the hyponym-hypernym relation, is the generality of the hypernym compared to the hyponym. For example, bird is more general than eagle, having a broader intension and a larger extension. This property has led to the introduction of lexical entailment measures that compare the entropy of distributional word representations, under the assumption that a more general term has a higher-entropy distribution (Herbelot and Gan</context>
</contexts>
<marker>Garrette, Erk, Mooney, 2011</marker>
<rawString>Dan Garrette, Katrin Erk, and Raymond Mooney. 2011. Integrating logical representations with probabilistic information using Markov logic. In Proceedings of IWCS, pages 105–114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Geffet</author>
<author>I Dagan</author>
</authors>
<title>The distributional inclusion hypotheses and lexical entailment.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>107--114</pages>
<marker>Geffet, Dagan, 2005</marker>
<rawString>M. Geffet and I. Dagan. 2005. The distributional inclusion hypotheses and lexical entailment. In Proceedings of ACL, pages 107–114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aur´elie Herbelot</author>
<author>Mohan Ganesalingam</author>
</authors>
<title>Measuring semantic content in distributional vectors.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>440--445</pages>
<contexts>
<context position="1680" citStr="Herbelot and Ganesalingam, 2013" startWordPosition="226" endWordPosition="230">te et al., 2011), metaphor detection (Mohler et al., 2013), and text generation (Biran and McKeown, 2013). Given two semantically related words, a key aspect of detecting lexical entailment, or the hyponym-hypernym relation, is the generality of the hypernym compared to the hyponym. For example, bird is more general than eagle, having a broader intension and a larger extension. This property has led to the introduction of lexical entailment measures that compare the entropy of distributional word representations, under the assumption that a more general term has a higher-entropy distribution (Herbelot and Ganesalingam, 2013; Santus et al., 2014). A strand of distributional semantics has recently emerged that exploits the fact that meaning is often grounded in the perceptual system, known as multi-modal distributional semantics (Bruni et al., 2014). Such models enhance purely linguistic models with extra-linguistic perceptual information, and outperform language-only models on a range of tasks, including modelling semantic similarity and conceptual relatedness (Silberer and Lapata, 2014). In fact, under some conditions uni-modal visual representations outperform traditional linguistic representations on semantic </context>
<context position="3385" citStr="Herbelot and Ganesalingam (2013)" startWordPosition="487" endWordPosition="490">t of pictures of different kinds of animals, the set of images for bird will consist of pictures of different birds, while the set for owl will mostly consist only of images of owls, as can be seen in Figure 1. Here we evaluate three different vision-based methods for measuring term generality on the semantic tasks of hypernym detection and hypernym directionality. Using this simple yet effective unsupervised approach, we obtain state-of-the-art results compared with supervised algorithms which use linguistic data. 2 Related Work In the linguistic modality, the most closely related work is by Herbelot and Ganesalingam (2013) and Santus et al. (2014), who use unsupervised distributional generality measures to identify the hypernym in a hyponym-hypernym pair. Herbelot and Ganesalingam (2013) use KL divergence to compare the probability distribution of context words, given a term, to the background probability distribution of context words. Santus et al. (2014) use the median entropy of the probability distributions associated with a term’s top-weighted con119 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Process</context>
<context position="11548" citStr="Herbelot and Ganesalingam, 2013" startWordPosition="1871" endWordPosition="1874">freq(p) in Equation 4 and using the frequency scores from Turney et al. (2011). Frequency has been proven to be a surprisingly d(w) = n(n2 1) 1 − cos ( ~wi, ~wj) (1) i&lt;j&lt;n q sθ(p,q) = f( ) 0 otherwise 121 BLESS WBLESS BIBLESS Frequency 0.58 0.57 0.39 WeedsPrec 0.63 — — WeedsSVM — 0.75 — WeedsUnSup — 0.58 — SLQS 0.87 — — Dispersion 0.88 0.75 (0.74) 0.57 (0.55) Centroid 0.87 0.74 (0.74) 0.57 (0.54) Entropy 0.83 0.71 (0.71) 0.56 (0.53) Table 2: Accuracy. For WBLESS and BIBLESS we report results for multi-modal µC, with visual-only µC in brackets. challenging baseline for hypernym directionality (Herbelot and Ganesalingam, 2013; Weeds et al., 2014). In addition, we compare to the reported results of Santus et al. (2014) for WeedsPrec (Weeds et al., 2004), an early lexical entailment measure, and SLQS, the entropy-based method of Santus et al. (2014). Note, however, that these are on a subsampled corpus of 1277 word pairs from BLESS, so the results are indicative but not directly comparable. On WBLESS we compare to the reported results of Weeds et al. (2014): we include results for the highest-performing supervised method (WeedsSVM) and the highestperforming unsupervised method (WeedsUnSup). For BLESS, both dispersio</context>
<context position="15036" citStr="Herbelot and Ganesalingam (2013)" startWordPosition="2431" endWordPosition="2435">5 Conclusions We have evaluated three unsupervised methods for determining the generality of a concept based on its visual properties. Our best-performing method, image dispersion, reaches the state-of-the-art on two standard semantic evaluation datasets. We introduced a novel, more difficult task combining hypernym detection and directionality, and showed that our methods outperform a frequency baseline by a large margin. We believe that image generality may be particularly suited to entailment detection because it does not suffer from the same issues as linguistic distributional generality. Herbelot and Ganesalingam (2013) found that general terms like liquid do not always have higher entropy distributions than their hyponyms, since speakers use them in very specific contexts, e.g. liquid is often coordinated with gas. We also acknowledge that our method depends 122 to some degree on Google’s search result diversification, but do not feel this detracts from the utility of the method, since the fact that general concepts achieve greater maximum image dispersion than specific concepts is not dependent on any particular diversification algorithm. In future work, we plan to explore more sophisticated visual general</context>
</contexts>
<marker>Herbelot, Ganesalingam, 2013</marker>
<rawString>Aur´elie Herbelot and Mohan Ganesalingam. 2013. Measuring semantic content in distributional vectors. In Proceedings of ACL, pages 440–445.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yangqing Jia</author>
<author>Evan Shelhamer</author>
<author>Jeff Donahue</author>
<author>Sergey Karayev</author>
<author>Jonathan Long</author>
<author>Ross Girshick</author>
<author>Sergio Guadarrama</author>
<author>Trevor Darrell</author>
</authors>
<title>Caffe: Convolutional architecture for fast feature embedding. arXiv preprint arXiv:1408.5093.</title>
<date>2014</date>
<contexts>
<context position="7322" citStr="Jia et al., 2014" startWordPosition="1109" endWordPosition="1112">la et al., 2014), we obtain images from Google Images1 for the words in the evaluation datasets. It has been shown that images from Google yield higherquality representations than comparable resources such as Flickr and are competitive with “hand prepared datasets” (Bergsma and Goebel, 2011; Fergus et al., 2005). 1www.google.com/imghp. Images were retrieved on 10 April, 2015 from Cambridge in the United Kingdom. 120 For each image, we extract the pre-softmax layer from a forward pass in a convolutional neural network (CNN) that has been trained on the ImageNet classification task using Caffe (Jia et al., 2014). As such, this work is an instance of deep transfer learning; that is, a deep learning representation trained on one task (image classification) is used to make predictions on a different task (image generality). We chose to use CNN-derived image representations because they have been found to be of higher quality than the traditional bag of visual words models (Sivic and Zisserman, 2003) that have previously been used in multi-modal distributional semantics (Bruni et al., 2014; Kiela and Bottou, 2014). 3.2 Generality measures We propose three measures that can be used to calculate the genera</context>
</contexts>
<marker>Jia, Shelhamer, Donahue, Karayev, Long, Girshick, Guadarrama, Darrell, 2014</marker>
<rawString>Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, and Trevor Darrell. 2014. Caffe: Convolutional architecture for fast feature embedding. arXiv preprint arXiv:1408.5093.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongyan Jing</author>
</authors>
<title>Usage of wordnet in natural language generation.</title>
<date>1998</date>
<booktitle>In Proceedings of COLINGACL’98 Workshop on Usage of WordNet in Natural Language Processing Systems.</booktitle>
<marker>Jing, 1998</marker>
<rawString>Hongyan Jing. 1998. Usage of wordnet in natural language generation. In Proceedings of COLINGACL’98 Workshop on Usage of WordNet in Natural Language Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douwe Kiela</author>
<author>L´eon Bottou</author>
</authors>
<title>Learning image embeddings using convolutional neural networks for improved multi-modal semantics.</title>
<date>2014</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>36--45</pages>
<contexts>
<context position="2310" citStr="Kiela and Bottou, 2014" startWordPosition="314" endWordPosition="317">s et al., 2014). A strand of distributional semantics has recently emerged that exploits the fact that meaning is often grounded in the perceptual system, known as multi-modal distributional semantics (Bruni et al., 2014). Such models enhance purely linguistic models with extra-linguistic perceptual information, and outperform language-only models on a range of tasks, including modelling semantic similarity and conceptual relatedness (Silberer and Lapata, 2014). In fact, under some conditions uni-modal visual representations outperform traditional linguistic representations on semantic tasks (Kiela and Bottou, 2014). We hypothesize that visual representations can be particularly useful for lexical entailment detection. Deselaers and Ferrari (2011) have shown that sets of images corresponding to terms at higher levels in the WordNet hierarchy have greater visual variability than those at lower levels. We exploit this tendency using sets of images returned by Google’s image search. The intuition is that the set of images returned for animal will consist of pictures of different kinds of animals, the set of images for bird will consist of pictures of different birds, while the set for owl will mostly consis</context>
<context position="7830" citStr="Kiela and Bottou, 2014" startWordPosition="1193" endWordPosition="1196">ional neural network (CNN) that has been trained on the ImageNet classification task using Caffe (Jia et al., 2014). As such, this work is an instance of deep transfer learning; that is, a deep learning representation trained on one task (image classification) is used to make predictions on a different task (image generality). We chose to use CNN-derived image representations because they have been found to be of higher quality than the traditional bag of visual words models (Sivic and Zisserman, 2003) that have previously been used in multi-modal distributional semantics (Bruni et al., 2014; Kiela and Bottou, 2014). 3.2 Generality measures We propose three measures that can be used to calculate the generality of a set of images. The image dispersion d of a concept word w is defined as the average pairwise cosine distance between all image representations { ~w1 ... ~wn} of the set of images returned for w: This measure was originally introduced to account for the fact that perceptual information is more relevant for e.g. elephant than it is for happiness. It acts as a substitute for the concreteness of a word and can be used to regulate how much perceptual information should be included in a multi-modal </context>
<context position="10116" citStr="Kiela and Bottou (2014)" startWordPosition="1598" endWordPosition="1601"> false positives where one word is more general but the pair is not semantically related, we introduce a second threshold θ which sets f to zero if the two concepts have low cosine similarity. This leads to the following formula: � 1 − f(p)+α if cos(µ~p ~µq) ≥ θ (5) We experimented with different methods for obtaining the mean vector representations for cosine (hereafter µc) in Equation (5), and found that multi-modal representations worked best. We concatenate an L2-normalized linguistic vector with the L2-normalized centroid of image vectors to obtain a multi-modal representation, following Kiela and Bottou (2014). For a word p with image representations {pimg 1 ... pimg n }, we thus set µc = pling ||n �i pimg, after normalizing both representations. For comparison, we also report results for a visual-only µc. For BLESS, we know the words in a pair stand in an entailment relation, so we set α = θ = 0 and evaluate whether s(p, q) &gt; 0, indicating that q is a hypernym of p. For WBLESS, we set α = 0.02 and θ = 0.2 without tuning, and evaluate whether sθ(p, q) &gt; 0 (hypernym relation) or sθ(p, q) G 0 (no hypernym relation). For BIBLESS, we set α = 0.02 and θ = 0.25 without tuning, and evaluate whether sθ(p, </context>
</contexts>
<marker>Kiela, Bottou, 2014</marker>
<rawString>Douwe Kiela and L´eon Bottou. 2014. Learning image embeddings using convolutional neural networks for improved multi-modal semantics. In Proceedings of EMNLP, pages 36–45.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douwe Kiela</author>
<author>Felix Hill</author>
<author>Anna Korhonen</author>
<author>Stephen Clark</author>
</authors>
<title>Improving multi-modal representations using image dispersion: Why less is sometimes more.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>835--841</pages>
<contexts>
<context position="6721" citStr="Kiela et al., 2014" startWordPosition="1012" endWordPosition="1015">onym order, as well as holonym-meronym pairs, cohyponyms, and randomly matched nouns. Accuracy on WBLESS reflects the ability to distinguish hypernymy from other relations, but does not require detection of directionality, since reversed pairs are grouped with the other negatives. For the combined experiment, we assign reversed hyponym-hypernym pairs a value of -1 instead of 0. We call this more challenging dataset BIBLESS. Examples of pairs in the respective datasets can be found in Table 1. 3.1 Image representations Following previous work in multi-modal semantics (Bergsma and Goebel, 2011; Kiela et al., 2014), we obtain images from Google Images1 for the words in the evaluation datasets. It has been shown that images from Google yield higherquality representations than comparable resources such as Flickr and are competitive with “hand prepared datasets” (Bergsma and Goebel, 2011; Fergus et al., 2005). 1www.google.com/imghp. Images were retrieved on 10 April, 2015 from Cambridge in the United Kingdom. 120 For each image, we extract the pre-softmax layer from a forward pass in a convolutional neural network (CNN) that has been trained on the ImageNet classification task using Caffe (Jia et al., 2014</context>
<context position="8456" citStr="Kiela et al., 2014" startWordPosition="1306" endWordPosition="1309">enerality measures We propose three measures that can be used to calculate the generality of a set of images. The image dispersion d of a concept word w is defined as the average pairwise cosine distance between all image representations { ~w1 ... ~wn} of the set of images returned for w: This measure was originally introduced to account for the fact that perceptual information is more relevant for e.g. elephant than it is for happiness. It acts as a substitute for the concreteness of a word and can be used to regulate how much perceptual information should be included in a multi-modal model (Kiela et al., 2014). Our second measure follows Deselaers and Ferrari (2011), who take a similar approach but instead of calculating the pairwise distance calculate the distance to the centroid µ~ of { ~w1 ... ~wn}: 1 � c(w) = 1 − cos( ~wi, ~µ) (2) n 1&lt;i&lt;n For our third measure we follow Lazaridou et al. (2015), who try different ways of modulating the inclusion of perceptual input in their multi-modal skip-gram model, and find that the entropy of the centroid vector µ~ works well (where p(µj) = µ&apos; ||~µ|| and m is the vector length): H(w) = − �m p(µj)log2(p(µj)) (3) j=1 3.3 Hypernym Detection and Directionality </context>
</contexts>
<marker>Kiela, Hill, Korhonen, Clark, 2014</marker>
<rawString>Douwe Kiela, Felix Hill, Anna Korhonen, and Stephen Clark. 2014. Improving multi-modal representations using image dispersion: Why less is sometimes more. In Proceedings of ACL, pages 835–841.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lili Kotlerman</author>
<author>Ido Dagan</author>
<author>Idan Szpektor</author>
<author>Maayan Zhitomirsky-Geffet</author>
</authors>
<title>Directional distributional similarity for lexical inference.</title>
<date>2010</date>
<journal>Natural Language Engineering,</journal>
<volume>16</volume>
<issue>4</issue>
<marker>Kotlerman, Dagan, Szpektor, Zhitomirsky-Geffet, 2010</marker>
<rawString>Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan Zhitomirsky-Geffet. 2010. Directional distributional similarity for lexical inference. Natural Language Engineering, 16(4):359–389.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Angeliki Lazaridou</author>
</authors>
<title>Nghia The Pham, and</title>
<date>2015</date>
<booktitle>In Proceedings of NAACL-HLT.</booktitle>
<marker>Lazaridou, 2015</marker>
<rawString>Angeliki Lazaridou, Nghia The Pham, and Marco Baroni. 2015. Combining language and vision with a multimodal skip-gram model. In Proceedings of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Lenci</author>
<author>Giulia Benotto</author>
</authors>
<title>Identifying hypernyms in distributional semantic spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of *SEM,</booktitle>
<pages>75--79</pages>
<marker>Lenci, Benotto, 2012</marker>
<rawString>Alessandro Lenci and Giulia Benotto. 2012. Identifying hypernyms in distributional semantic spaces. In Proceedings of *SEM, pages 75–79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Steffen Remus</author>
<author>Chris Biemann</author>
<author>Ido Dagan</author>
</authors>
<title>Do supervised distributional methods really learn lexical inference relations?</title>
<date>2015</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<marker>Levy, Remus, Biemann, Dagan, 2015</marker>
<rawString>Omer Levy, Steffen Remus, Chris Biemann, and Ido Dagan. 2015. Do supervised distributional methods really learn lexical inference relations? In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J H Martin</author>
</authors>
<title>A Computational Model of Metaphor Interpretation.</title>
<date>1990</date>
<publisher>Academic Press Professional, Inc.</publisher>
<marker>Martin, 1990</marker>
<rawString>J.H. Martin. 1990. A Computational Model of Metaphor Interpretation. Academic Press Professional, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>WordNet: A lexical database for English.</title>
<date>1995</date>
<journal>In Communications of the ACM,</journal>
<volume>38</volume>
<pages>39--41</pages>
<contexts>
<context position="13638" citStr="Miller, 1995" startWordPosition="2216" endWordPosition="2217">d here. We can see that the task becomes increasingly difficult as we go from directionality to detection to the combination: the dispersion-based method goes from 0.88 to 0.75 to 0.57, for example. BIBLESS is the most difficult, as shown by the freFigure 2: Accuracy by WordNet shortest path bucket (1 is shortest, 5 is longest). quency baseline obtaining only 0.39. Our methods do much better than this baseline (+18%). Image dispersion appears to be the most robust measure. To examine our results further, we divided the test data into buckets by the shortest WordNet path connecting word pairs (Miller, 1995). We expect our method to be less accurate on word pairs with short paths, since the difference in generality may be difficult to discern. It has also been suggested that very abstract hypernyms such as object and entity are difficult to detect because their linguistic distributions are not supersets of their hyponyms’ distributions (Rimell, 2014), a factor that should not affect the visual modality. We find that concept comparisons with a very short path (bucket 1) are indeed the least accurate. We also find some drop in accuracy on the longest paths (bucket 5), especially for WBLESS and BIBL</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. WordNet: A lexical database for English. In Communications of the ACM, volume 38, pages 39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Mohler</author>
<author>David Bracewell</author>
<author>Marc Tomlinson</author>
<author>David Hinote</author>
</authors>
<title>Semantic signatures for example-based linguistic metaphor detection.</title>
<date>2013</date>
<booktitle>In Proceedings of the 1st Workshop on Metaphor in NLP.</booktitle>
<contexts>
<context position="1107" citStr="Mohler et al., 2013" startWordPosition="138" endWordPosition="141">g a concept’s generality. We introduce three unsupervised methods for determining a concept’s generality, based on its related images, and obtain state-ofthe-art performance on two standard semantic evaluation datasets. We also introduce a novel task that combines hypernym detection and directionality, significantly outperforming a competitive frequencybased baseline. 1 Introduction Automatic detection of lexical entailment is useful for a number of NLP tasks including search query expansion (Shekarpour et al., 2013), recognising textual entailment (Garrette et al., 2011), metaphor detection (Mohler et al., 2013), and text generation (Biran and McKeown, 2013). Given two semantically related words, a key aspect of detecting lexical entailment, or the hyponym-hypernym relation, is the generality of the hypernym compared to the hyponym. For example, bird is more general than eagle, having a broader intension and a larger extension. This property has led to the introduction of lexical entailment measures that compare the entropy of distributional word representations, under the assumption that a more general term has a higher-entropy distribution (Herbelot and Ganesalingam, 2013; Santus et al., 2014). A s</context>
</contexts>
<marker>Mohler, Bracewell, Tomlinson, Hinote, 2013</marker>
<rawString>Michael Mohler, David Bracewell, Marc Tomlinson, and David Hinote. 2013. Semantic signatures for example-based linguistic metaphor detection. In Proceedings of the 1st Workshop on Metaphor in NLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Rimell</author>
</authors>
<title>Distributional lexical entailment by topic coherence.</title>
<date>2014</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>511--519</pages>
<contexts>
<context position="13987" citStr="Rimell, 2014" startWordPosition="2271" endWordPosition="2272">taining only 0.39. Our methods do much better than this baseline (+18%). Image dispersion appears to be the most robust measure. To examine our results further, we divided the test data into buckets by the shortest WordNet path connecting word pairs (Miller, 1995). We expect our method to be less accurate on word pairs with short paths, since the difference in generality may be difficult to discern. It has also been suggested that very abstract hypernyms such as object and entity are difficult to detect because their linguistic distributions are not supersets of their hyponyms’ distributions (Rimell, 2014), a factor that should not affect the visual modality. We find that concept comparisons with a very short path (bucket 1) are indeed the least accurate. We also find some drop in accuracy on the longest paths (bucket 5), especially for WBLESS and BIBLESS, perhaps because semantic similarity is difficult to detect in these cases. For a histogram of the accuracy scores according to WordNet similarity, see Figure 2. 5 Conclusions We have evaluated three unsupervised methods for determining the generality of a concept based on its visual properties. Our best-performing method, image dispersion, re</context>
</contexts>
<marker>Rimell, 2014</marker>
<rawString>Laura Rimell. 2014. Distributional lexical entailment by topic coherence. In Proceedings of EACL, pages 511–519.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Enrico Santus</author>
<author>Alessandro Lenci</author>
<author>Qin Lu</author>
<author>Sabine Schulte im Walde</author>
</authors>
<title>Chasing hypernyms in vector spaces with entropy.</title>
<date>2014</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>38--42</pages>
<contexts>
<context position="1702" citStr="Santus et al., 2014" startWordPosition="231" endWordPosition="234">ion (Mohler et al., 2013), and text generation (Biran and McKeown, 2013). Given two semantically related words, a key aspect of detecting lexical entailment, or the hyponym-hypernym relation, is the generality of the hypernym compared to the hyponym. For example, bird is more general than eagle, having a broader intension and a larger extension. This property has led to the introduction of lexical entailment measures that compare the entropy of distributional word representations, under the assumption that a more general term has a higher-entropy distribution (Herbelot and Ganesalingam, 2013; Santus et al., 2014). A strand of distributional semantics has recently emerged that exploits the fact that meaning is often grounded in the perceptual system, known as multi-modal distributional semantics (Bruni et al., 2014). Such models enhance purely linguistic models with extra-linguistic perceptual information, and outperform language-only models on a range of tasks, including modelling semantic similarity and conceptual relatedness (Silberer and Lapata, 2014). In fact, under some conditions uni-modal visual representations outperform traditional linguistic representations on semantic tasks (Kiela and Botto</context>
<context position="3410" citStr="Santus et al. (2014)" startWordPosition="492" endWordPosition="495">nimals, the set of images for bird will consist of pictures of different birds, while the set for owl will mostly consist only of images of owls, as can be seen in Figure 1. Here we evaluate three different vision-based methods for measuring term generality on the semantic tasks of hypernym detection and hypernym directionality. Using this simple yet effective unsupervised approach, we obtain state-of-the-art results compared with supervised algorithms which use linguistic data. 2 Related Work In the linguistic modality, the most closely related work is by Herbelot and Ganesalingam (2013) and Santus et al. (2014), who use unsupervised distributional generality measures to identify the hypernym in a hyponym-hypernym pair. Herbelot and Ganesalingam (2013) use KL divergence to compare the probability distribution of context words, given a term, to the background probability distribution of context words. Santus et al. (2014) use the median entropy of the probability distributions associated with a term’s top-weighted con119 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages</context>
<context position="5078" citStr="Santus et al., 2014" startWordPosition="751" endWordPosition="754"> manually annotated images from ImageNet (Deng et al., 2009), they find that concepts and categories with narrower intensions and smaller extensions tend to have less visual variability. We extend this intuition to the unsupervised setting of Google image search results and apply it to the lexical entailment task. 3 Approach We use two standard evaluations for lexical entailment: hypernym directionality, where the task is to predict which of two words is the hypernym; and hypernym detection, where the task is to predict whether two words are in a hypernym-hyponym relation (Weeds et al., 2014; Santus et al., 2014). We also introduce a third, more challenging, evaluation that combines detection and directionality. For the directionality experiment, we evaluate on the hypernym subset of the well-known BLESS dataset (Baroni and Lenci, 2011), which consists of 1337 hyponym-hypernym pairs. In this case, it is known that the words are in an entailment relation and the task is to predict the directionality of the relation. BLESS data is always presented with the hyponym first, so we report how often our measures predict that the second term in the pair is more general than the first. For the detection experim</context>
<context position="11642" citStr="Santus et al. (2014)" startWordPosition="1889" endWordPosition="1892"> to be a surprisingly d(w) = n(n2 1) 1 − cos ( ~wi, ~wj) (1) i&lt;j&lt;n q sθ(p,q) = f( ) 0 otherwise 121 BLESS WBLESS BIBLESS Frequency 0.58 0.57 0.39 WeedsPrec 0.63 — — WeedsSVM — 0.75 — WeedsUnSup — 0.58 — SLQS 0.87 — — Dispersion 0.88 0.75 (0.74) 0.57 (0.55) Centroid 0.87 0.74 (0.74) 0.57 (0.54) Entropy 0.83 0.71 (0.71) 0.56 (0.53) Table 2: Accuracy. For WBLESS and BIBLESS we report results for multi-modal µC, with visual-only µC in brackets. challenging baseline for hypernym directionality (Herbelot and Ganesalingam, 2013; Weeds et al., 2014). In addition, we compare to the reported results of Santus et al. (2014) for WeedsPrec (Weeds et al., 2004), an early lexical entailment measure, and SLQS, the entropy-based method of Santus et al. (2014). Note, however, that these are on a subsampled corpus of 1277 word pairs from BLESS, so the results are indicative but not directly comparable. On WBLESS we compare to the reported results of Weeds et al. (2014): we include results for the highest-performing supervised method (WeedsSVM) and the highestperforming unsupervised method (WeedsUnSup). For BLESS, both dispersion and centroid distance reach or outperform the best other measure (SLQS). They beat the frequ</context>
</contexts>
<marker>Santus, Lenci, Lu, Walde, 2014</marker>
<rawString>Enrico Santus, Alessandro Lenci, Qin Lu, and Sabine Schulte im Walde. 2014. Chasing hypernyms in vector spaces with entropy. In Proceedings of EACL, pages 38–42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saeedeh Shekarpour</author>
<author>Konrad H¨offner</author>
<author>Jens Lehmann</author>
<author>S¨oren Auer</author>
</authors>
<title>Keyword query expansion on linked data using linguistic and semantic features.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th IEEE International Conference on Semantic Computing,</booktitle>
<pages>191--197</pages>
<marker>Shekarpour, H¨offner, Lehmann, Auer, 2013</marker>
<rawString>Saeedeh Shekarpour, Konrad H¨offner, Jens Lehmann, and S¨oren Auer. 2013. Keyword query expansion on linked data using linguistic and semantic features. In Proceedings of the 7th IEEE International Conference on Semantic Computing, pages 191–197.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carina Silberer</author>
<author>Mirella Lapata</author>
</authors>
<title>Learning grounded meaning representations with autoencoders.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>721--732</pages>
<contexts>
<context position="2152" citStr="Silberer and Lapata, 2014" startWordPosition="294" endWordPosition="297">y of distributional word representations, under the assumption that a more general term has a higher-entropy distribution (Herbelot and Ganesalingam, 2013; Santus et al., 2014). A strand of distributional semantics has recently emerged that exploits the fact that meaning is often grounded in the perceptual system, known as multi-modal distributional semantics (Bruni et al., 2014). Such models enhance purely linguistic models with extra-linguistic perceptual information, and outperform language-only models on a range of tasks, including modelling semantic similarity and conceptual relatedness (Silberer and Lapata, 2014). In fact, under some conditions uni-modal visual representations outperform traditional linguistic representations on semantic tasks (Kiela and Bottou, 2014). We hypothesize that visual representations can be particularly useful for lexical entailment detection. Deselaers and Ferrari (2011) have shown that sets of images corresponding to terms at higher levels in the WordNet hierarchy have greater visual variability than those at lower levels. We exploit this tendency using sets of images returned by Google’s image search. The intuition is that the set of images returned for animal will consi</context>
</contexts>
<marker>Silberer, Lapata, 2014</marker>
<rawString>Carina Silberer and Mirella Lapata. 2014. Learning grounded meaning representations with autoencoders. In Proceedings of ACL, pages 721–732.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josef Sivic</author>
<author>Andrew Zisserman</author>
</authors>
<title>Video Google: A text retrieval approach to object matching in videos.</title>
<date>2003</date>
<booktitle>In Proceedings of ICCV,</booktitle>
<pages>1470--1477</pages>
<contexts>
<context position="7714" citStr="Sivic and Zisserman, 2003" startWordPosition="1175" endWordPosition="1178">Cambridge in the United Kingdom. 120 For each image, we extract the pre-softmax layer from a forward pass in a convolutional neural network (CNN) that has been trained on the ImageNet classification task using Caffe (Jia et al., 2014). As such, this work is an instance of deep transfer learning; that is, a deep learning representation trained on one task (image classification) is used to make predictions on a different task (image generality). We chose to use CNN-derived image representations because they have been found to be of higher quality than the traditional bag of visual words models (Sivic and Zisserman, 2003) that have previously been used in multi-modal distributional semantics (Bruni et al., 2014; Kiela and Bottou, 2014). 3.2 Generality measures We propose three measures that can be used to calculate the generality of a set of images. The image dispersion d of a concept word w is defined as the average pairwise cosine distance between all image representations { ~w1 ... ~wn} of the set of images returned for w: This measure was originally introduced to account for the fact that perceptual information is more relevant for e.g. elephant than it is for happiness. It acts as a substitute for the con</context>
</contexts>
<marker>Sivic, Zisserman, 2003</marker>
<rawString>Josef Sivic and Andrew Zisserman. 2003. Video Google: A text retrieval approach to object matching in videos. In Proceedings of ICCV, pages 1470– 1477.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Yair Neuman</author>
<author>Dan Assaf</author>
<author>Yohai Cohen</author>
</authors>
<title>Literal and metaphorical sense identification through concrete and abstract context.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>680--690</pages>
<contexts>
<context position="10995" citStr="Turney et al. (2011)" startWordPosition="1775" endWordPosition="1778">ent relation, so we set α = θ = 0 and evaluate whether s(p, q) &gt; 0, indicating that q is a hypernym of p. For WBLESS, we set α = 0.02 and θ = 0.2 without tuning, and evaluate whether sθ(p, q) &gt; 0 (hypernym relation) or sθ(p, q) G 0 (no hypernym relation). For BIBLESS, we set α = 0.02 and θ = 0.25 without tuning, and evaluate whether sθ(p, q) &gt; 0 (hyponym-hypernym), s(p, q) = 0 (no relation), or s(p, q) G 0 (hypernym-hyponym). 4 Results The results can be found in Table 2. We compare our methods with a frequency baseline, setting f(p) = freq(p) in Equation 4 and using the frequency scores from Turney et al. (2011). Frequency has been proven to be a surprisingly d(w) = n(n2 1) 1 − cos ( ~wi, ~wj) (1) i&lt;j&lt;n q sθ(p,q) = f( ) 0 otherwise 121 BLESS WBLESS BIBLESS Frequency 0.58 0.57 0.39 WeedsPrec 0.63 — — WeedsSVM — 0.75 — WeedsUnSup — 0.58 — SLQS 0.87 — — Dispersion 0.88 0.75 (0.74) 0.57 (0.55) Centroid 0.87 0.74 (0.74) 0.57 (0.54) Entropy 0.83 0.71 (0.71) 0.56 (0.53) Table 2: Accuracy. For WBLESS and BIBLESS we report results for multi-modal µC, with visual-only µC in brackets. challenging baseline for hypernym directionality (Herbelot and Ganesalingam, 2013; Weeds et al., 2014). In addition, we compare </context>
</contexts>
<marker>Turney, Neuman, Assaf, Cohen, 2011</marker>
<rawString>Peter D. Turney, Yair Neuman, Dan Assaf, and Yohai Cohen. 2011. Literal and metaphorical sense identification through concrete and abstract context. In Proceedings of EMNLP, pages 680–690.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julie Weeds</author>
<author>David Weir</author>
<author>Diana McCarthy</author>
</authors>
<title>Characterising measures of lexical distributional similarity.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="11677" citStr="Weeds et al., 2004" startWordPosition="1895" endWordPosition="1898"> 1 − cos ( ~wi, ~wj) (1) i&lt;j&lt;n q sθ(p,q) = f( ) 0 otherwise 121 BLESS WBLESS BIBLESS Frequency 0.58 0.57 0.39 WeedsPrec 0.63 — — WeedsSVM — 0.75 — WeedsUnSup — 0.58 — SLQS 0.87 — — Dispersion 0.88 0.75 (0.74) 0.57 (0.55) Centroid 0.87 0.74 (0.74) 0.57 (0.54) Entropy 0.83 0.71 (0.71) 0.56 (0.53) Table 2: Accuracy. For WBLESS and BIBLESS we report results for multi-modal µC, with visual-only µC in brackets. challenging baseline for hypernym directionality (Herbelot and Ganesalingam, 2013; Weeds et al., 2014). In addition, we compare to the reported results of Santus et al. (2014) for WeedsPrec (Weeds et al., 2004), an early lexical entailment measure, and SLQS, the entropy-based method of Santus et al. (2014). Note, however, that these are on a subsampled corpus of 1277 word pairs from BLESS, so the results are indicative but not directly comparable. On WBLESS we compare to the reported results of Weeds et al. (2014): we include results for the highest-performing supervised method (WeedsSVM) and the highestperforming unsupervised method (WeedsUnSup). For BLESS, both dispersion and centroid distance reach or outperform the best other measure (SLQS). They beat the frequency baseline by a large margin (+3</context>
</contexts>
<marker>Weeds, Weir, McCarthy, 2004</marker>
<rawString>Julie Weeds, David Weir, and Diana McCarthy. 2004. Characterising measures of lexical distributional similarity. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julie Weeds</author>
<author>Daoud Clarke</author>
<author>Jeremy Reffin</author>
<author>David Weir</author>
<author>Bill Keller</author>
</authors>
<title>Learning to distinguish hypernyms and co-hyponyms.</title>
<date>2014</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>2249--2259</pages>
<contexts>
<context position="5056" citStr="Weeds et al., 2014" startWordPosition="747" endWordPosition="750">errari (2011). Using manually annotated images from ImageNet (Deng et al., 2009), they find that concepts and categories with narrower intensions and smaller extensions tend to have less visual variability. We extend this intuition to the unsupervised setting of Google image search results and apply it to the lexical entailment task. 3 Approach We use two standard evaluations for lexical entailment: hypernym directionality, where the task is to predict which of two words is the hypernym; and hypernym detection, where the task is to predict whether two words are in a hypernym-hyponym relation (Weeds et al., 2014; Santus et al., 2014). We also introduce a third, more challenging, evaluation that combines detection and directionality. For the directionality experiment, we evaluate on the hypernym subset of the well-known BLESS dataset (Baroni and Lenci, 2011), which consists of 1337 hyponym-hypernym pairs. In this case, it is known that the words are in an entailment relation and the task is to predict the directionality of the relation. BLESS data is always presented with the hyponym first, so we report how often our measures predict that the second term in the pair is more general than the first. For</context>
<context position="11569" citStr="Weeds et al., 2014" startWordPosition="1875" endWordPosition="1878">he frequency scores from Turney et al. (2011). Frequency has been proven to be a surprisingly d(w) = n(n2 1) 1 − cos ( ~wi, ~wj) (1) i&lt;j&lt;n q sθ(p,q) = f( ) 0 otherwise 121 BLESS WBLESS BIBLESS Frequency 0.58 0.57 0.39 WeedsPrec 0.63 — — WeedsSVM — 0.75 — WeedsUnSup — 0.58 — SLQS 0.87 — — Dispersion 0.88 0.75 (0.74) 0.57 (0.55) Centroid 0.87 0.74 (0.74) 0.57 (0.54) Entropy 0.83 0.71 (0.71) 0.56 (0.53) Table 2: Accuracy. For WBLESS and BIBLESS we report results for multi-modal µC, with visual-only µC in brackets. challenging baseline for hypernym directionality (Herbelot and Ganesalingam, 2013; Weeds et al., 2014). In addition, we compare to the reported results of Santus et al. (2014) for WeedsPrec (Weeds et al., 2004), an early lexical entailment measure, and SLQS, the entropy-based method of Santus et al. (2014). Note, however, that these are on a subsampled corpus of 1277 word pairs from BLESS, so the results are indicative but not directly comparable. On WBLESS we compare to the reported results of Weeds et al. (2014): we include results for the highest-performing supervised method (WeedsSVM) and the highestperforming unsupervised method (WeedsUnSup). For BLESS, both dispersion and centroid distan</context>
<context position="12803" citStr="Weeds et al. (2014)" startWordPosition="2079" endWordPosition="2082">perform the best other measure (SLQS). They beat the frequency baseline by a large margin (+30% and +29%). Taking the entropy of the mean image representations does not appear to do as well as the other two methods but still outperforms the baseline and WeedsPrec (+25% and +20% respectively). In the case of WBLESS and BIBLESS, we see a similar pattern in that dispersion and centroid distance perform best. For WBLESS, these methods outperform the other unsupervised approach, WeedsUnsup, by +17% and match the best-performing support vector machine (SVM) approach in Weeds et al. (2014). In fact, Weeds et al. (2014) report results for a total of 6 supervised methods (based on SVM and k-nearest neighbor (k-NN) classifiers): our unsupervised image dispersion method outperforms all of these except for the highest-performing one, reported here. We can see that the task becomes increasingly difficult as we go from directionality to detection to the combination: the dispersion-based method goes from 0.88 to 0.75 to 0.57, for example. BIBLESS is the most difficult, as shown by the freFigure 2: Accuracy by WordNet shortest path bucket (1 is shortest, 5 is longest). quency baseline obtaining only 0.39. Our method</context>
</contexts>
<marker>Weeds, Clarke, Reffin, Weir, Keller, 2014</marker>
<rawString>Julie Weeds, Daoud Clarke, Jeremy Reffin, David Weir, and Bill Keller. 2014. Learning to distinguish hypernyms and co-hyponyms. In Proceedings of COLING, pages 2249–2259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A Woods</author>
<author>Stephen Green</author>
<author>Paul Martin</author>
<author>Ann Houston</author>
</authors>
<title>Aggressive morphology and lexical relations for query expansion.</title>
<date>2001</date>
<booktitle>In Proceedings of TREC.</booktitle>
<marker>Woods, Green, Martin, Houston, 2001</marker>
<rawString>W. A. Woods, Stephen Green, Paul Martin, and Ann Houston. 2001. Aggressive morphology and lexical relations for query expansion. In Proceedings of TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Zhitomirsky-Geffet</author>
<author>I Dagan</author>
</authors>
<title>Bootstrapping distributional feature vector quality.</title>
<date>2009</date>
<journal>Computational Linguistics,</journal>
<volume>35</volume>
<issue>3</issue>
<marker>Zhitomirsky-Geffet, Dagan, 2009</marker>
<rawString>M. Zhitomirsky-Geffet and I. Dagan. 2009. Bootstrapping distributional feature vector quality. Computational Linguistics, 35(3):435461.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>