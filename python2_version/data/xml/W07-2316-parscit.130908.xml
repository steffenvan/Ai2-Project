<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000481">
<title confidence="0.9517">
An Experiment on “Free Generation” from Single RDF triples
</title>
<author confidence="0.96198">
Xiantang Sun
</author>
<affiliation confidence="0.968878">
Department of Computing Science, University
</affiliation>
<address confidence="0.628721">
of Aberdeen
Aberdeen, UK, Ab24 3UE
</address>
<email confidence="0.950433">
xsun@csd.abdn.ac.uk
</email>
<author confidence="0.946654">
Chris Mellish
</author>
<affiliation confidence="0.8066935">
Department of Computing Science, University of
Aberdeen
</affiliation>
<address confidence="0.662549">
Aberdeen, UK, Ab24 3UE
</address>
<email confidence="0.877731">
cmellish@csd.abdn.ac.uk
</email>
<sectionHeader confidence="0.988838" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999981538461538">
This paper introduces our domain
independent approach to “free generation”
from single RDF triples without using any
domain dependent knowledge. Our approach
is developed based on our argument that
RDF representations carry rich linguistic
information, which can be used to achieve
readable domain independent generation. In
order to examine to what extent our argument
is realistic, we carry out an evaluation
experiment, which is the first evaluation of
this kind of domain independent generation
in the field.
</bodyText>
<sectionHeader confidence="0.95959" genericHeader="introduction">
Introduction
</sectionHeader>
<bodyText confidence="0.999350857142857">
In the Semantic Web, both instance data and
ontological data&apos; are represented as graphs based
on the Resource Description Framework (RDF)
(W3C 2004). In order to facilitate non-technician
users to access the knowledge and information
coded in RDF, we are eventually aiming at
developing a domain independent approach to
presenting RDF graphs in natural language,
which can greatly reduce the cost of applying
NLG techniques to various RDF domains (e.g.,
medical RDF data and chemical RDF data). In
this paper we introduce our domain independent
approach to generating phrases or sentences from
single RDF triples2 without using any domain
knowledge but only generic linguistic knowledge
sources. This contrasts with almost all existing
work generating natural language from
&apos;Ontological languages are developed based on the
RDF syntax, so ontological data are still RDF graphs.
2 Generation from RDF triples in our case means only
presenting the information in the triples, rather than
explaining the information.
ontologies, which assumes the existence of
domain-dependent lexicons. This work is a key
part of our final system because generation from
single RDF triples, which are the atomic units of
RDF graphs, is the foundation and also the first
step for any further generation from larger RDF
graphs. In order to examine to what extent the
linguistic structures can be used to achieve
domain independent generation from single RDF
triples, we have built a generation system,
Triple-Text (TT), and here we compare TT’s
generation with human experts’ generation in an
evaluation experiment.
</bodyText>
<sectionHeader confidence="0.59198" genericHeader="method">
1 Linguistic structures in ontologies
</sectionHeader>
<bodyText confidence="0.996053608695652">
Let’s start with an example of a triple, which
consists of a subject, a predicate (also known as a
property) and an object. The triple
(LongridgeMerlot HasMaker Longridge), may
be realised as, LongridgeMerlot has a property
‘HasMaker’ with a value ‘Longridge’, if this
triple is viewed as a pure logical representation.
But there could be a much better way of realising
it, if the implicit linguistic information embedded
in the triple could be correctly recognised and
exploited:
Longridge Merlot has a maker, Longridge.
More interestingly, we find that the generation
process in fact does not require understanding the
real semantics of the terms, ‘Longridge’,
‘Merlot’, ‘has’, ‘maker’ and ‘Longridge’. That is,
the words embedded in the names of these terms
could form most of the final sentence and they
have been already placed in a sensible way by
their creators. This implies that what is required
to make the final sentence is to just to fill in
“missing” words, e.g., determiners. Obviously,
the name of the property in the triple plays the
</bodyText>
<page confidence="0.998704">
105
</page>
<bodyText confidence="0.993533166666667">
most crucial role in the generation.
As our earlier work (Mellish and Sun 2005) (Sun
and Mellish 2006) shows, RDF representations
indeed contain rich linguistic information. In our
corpus of 882 OWL files which include 37260
class names and 1218 property names, only 14%
of class names consist totally of meaningless
strings and 97% of the properties’ names fully or
partially consist of natural words. Our further
analysis has shown that the properties may be
classified into 6 categories based on their
“patterns”. 37.2% of the properties start with
‘has’, 12.3% start with ‘is’, 11.3% end with a
preposition, 18.0% are single words, 12.5% have
two words, and 8.3% have more than two words.
In each category, we further define some specific
patterns3, for instance those shown in figure 1. In
total, we define 23 patterns in the six categories.
</bodyText>
<table confidence="0.5017038">
Patterns of ‘has’ Examples
‘has’+...+noun hasColor, hasName
‘has’+...+preposition hasExposureTo,
hasShapeAnalagousTo
‘has’+...+adj hasTimeClose, hasTimeOpen
</table>
<figureCaption confidence="0.997648">
Figure 1: examples of the patterns for ‘has’
</figureCaption>
<sectionHeader confidence="0.812784" genericHeader="method">
2 Generating single sentences from
RDF triples
</sectionHeader>
<bodyText confidence="0.992558678571428">
Based on the patterns found from the corpus, we
developed a system (TT) to generate single
sentences from single RDF triples without any
domain dependent knowledge. The key idea here
is to construct VPs from the properties of input
triples and treat the subjects and objects of the
triples as domain dependent terms, which are
simply seen as proper nouns e.g., (John
Surname Murphy ) -) ”John has a surname
Murphy.” So the main part of the system is about
constructing proper VPs from properties. In the
process of constructing VPs, every property is
tokenised into a sequence of units, e.g., hasEmail
3 We use QTAG (Mason 2003) to recognise the
parts-of-speech (POS) of the units extracted from the
property names. For example, QTAG can recognise
‘has’ and ‘colour’ (from ‘hasColour’) as a verb and a
noun. We achieved 99.2% accuracy of POS
recognition on our corpus using QTAG with the
assistance of some manually-added rules.
is separated into has and email. Then, the
property is classified into one of our 6 categories,
and in this case hasEmail belongs to the ‘has’
category. In each category, we have a set of rules
to construct VPs from input properties. A rule’s
LHS is a pattern and the RHS is a corresponding
linguistic form (VP) for the pattern. For example,
we have a rule like,
</bodyText>
<equation confidence="0.717916">
LHS: ‘has’ + [unit]* + [noun] �
RHS: ‘has’+det*4+’units’+’noun’
</equation>
<bodyText confidence="0.9951274">
which can construct a VP, has an email, for the
property, hasEmail. Assuming that TT is given a
triple, (Peter, hasEmail, X@Y.com), then the
generated single sentence is, Peter has an email,
X@Y.com, where the subject and the object are
treated as proper nouns without any analysis. In
the case of input that our rules cannot cover, TT
outputs a kind of “RDF flavoured” text, e.g., TT
generates, abg has a property k34 with value 377,
from the triple (abg k34 377).
</bodyText>
<sectionHeader confidence="0.999144" genericHeader="evaluation">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.999989125">
In order to examine to what extent our domain
independent generation is realistic in terms of
syntax, comprehensibility and overall quality, we
carried out an experiment to compare TT’s
generation with human experts’ generation and
pure “RDF flavoured” generation (the “RDF
generator”), as our baseline (as in the example
shown at the end of section 2).
</bodyText>
<subsectionHeader confidence="0.997313">
3.1 Experiment design
</subsectionHeader>
<bodyText confidence="0.9999065">
We applied a “Two-Panel” methodology to
compare the three kinds of generation, which is
similar to the methodology applied in KNIGHT5
(Lester and Porter 1997). The Two-Panel
evaluation methodology can be used to
empirically evaluate NLG generation by
comparing computer generation with human
generation. We take Computer Blindness as a
central principle through the experiment in order
to guarantee the integrity of the evaluation results.
This means that our judges do not know that any
texts are generated by computer..
</bodyText>
<footnote confidence="0.57291825">
4 We simply add an indefinite determiner if the ‘noun’
is not in its plural form.
5 In KNIGHT only the system’s generation and human
generation are compared.
</footnote>
<page confidence="0.989562">
106
</page>
<bodyText confidence="0.976959">
There are four steps in our methodology:
</bodyText>
<listItem confidence="0.860360875">
• randomly selecting 90 triples with
different properties and then generating
from them with TT and the “RDF
generator” (we took the 90 triples from
ontologies collected in an knowledge
engineer’s ongoing project);
• arranging two panels consisting of RDF
experts and PhD students whose areas
are irrelevant to computing. NB the RDF
experts in the first panel did not know the
domains that the 90 triples were from, in
order to make their generation “domain
independent” like TT;
• asking panel 1 to manually generate 90
short sentences from the triples;
• evaluating all generations by panel 2.
</listItem>
<subsectionHeader confidence="0.988775">
3.2 Experiment
</subsectionHeader>
<bodyText confidence="0.99996525">
The source RDF data for the experiment were
collected from 7 domains in order to test TT’s
performance in general. The input data were not
known to us until we started the experiment. 90
different single RDF triples were randomly
collected from the data and input to TT and the
“RDF generator”. We avoided having 2
sentences with the same property. Now we had
90 sentences from TT and another 90 sentences
from the “RDF generator”. We invited 3 RDF
experts (2 PhD students and a Post-Doc) for
panel 1, 6 law PhD students for panel 2. Each of
the experts in panel 1 was asked to present 30
different triples from the 90 triples in natural
language. Panel 2 judged the generation in terms
of syntax, comprehensibility, and overall quality.
Panel 2 was given mixtures of the generations
from TT, the “RDF generator” and the experts,
but they were not shown the source triples and
did not know that there were computers involved
in the experiment. Each judge was given 90 short
sentences and asked to judge them in terms of
syntax, comprehensibility, and overall quality by
choosing between possible options.
</bodyText>
<listItem confidence="0.997734333333333">
• Syntax: we asked “does the sentence
have any grammar mistakes?” and gave
five options, A) all wrong B) basically
wrong C) some mistakes but
understandable D) minor mistakes E) no
mistakes
• Comprehensibility: we asked “do you
understand what the sentence says?” and
gave options, from not at all, a little bit,
some of it, understand most of it, and
understand it all
• Overall quality: we asked “Do you like
the way the sentence is written?” and
gave options from not at all, a little bit,
generally ok, good and excellent.
</listItem>
<bodyText confidence="0.999774666666667">
When we distributed these sentences to the
judges, we followed the four principles that
applied in KNIGHT’s evaluation. They are
</bodyText>
<listItem confidence="0.9807034">
• System-Human division: Each judge in
panel 2 received 90 different sentences
from TT, the “RDF generator” and
experts in random order (30 of each).
• Domain Division: Each judge in panel 1
and panel 2 received sentences that were
approximately evenly divided among the
domains which the 90 RDF triples were
from.
• Single-generation restriction: No judge
in panel 2 received more than one
sentence from the same RDF triple.
• Multijudge Stipulation: Each sentence is
judged exactly twice in order to obtain
relatively unbiased judgements.
</listItem>
<subsectionHeader confidence="0.999399">
3.3 Experiment results
</subsectionHeader>
<bodyText confidence="0.999973571428572">
After the experiment, for each triple we had 3
sentences (generated from TT, the “RDF
generator” and panel 1) judged by panel 2. Then,
we had three samples of quantitative data of the
judges’ opinions of each dimension by mapping
options A-E onto 1-5 (a sample of TT, a sample
of the “RDF generator” and a sample of the
experts’ text). The two-tailed Standard T-test is a
good way to detect differences between these
samples if the differences exist (as in KNIGHT).
We compared TT with the experts, TT with the
program and the program with the experts. Here
are the results for the means6 and the differences
and their significance7 (tables 1, 2, 3 and 4).
</bodyText>
<equation confidence="0.599159">
6 ± in table1 stands for the standard error.
</equation>
<bodyText confidence="0.549879">
7 The t-tests used in our case are unpaired, two-tailed.
The results are reported for a 0.05 level of confidence.
Significance does not depend on whether we apply a
multiple test correction.
</bodyText>
<page confidence="0.977468">
107
</page>
<table confidence="0.951947863636364">
Generator Syntax Comprehensibility Overall
RDF gen. 2.44±0.09 2.01±0.09 1.81±0.08
TT 3.14±0.12 2.76±0.12 2.29±0.11
Experts 3.3±0.12 2.88±0.12 2.4±0.11
Table1: Means
RDF gen. Syntax Comprehensibi Overall
VS TT lity
Difference 0.70 0.75 0.48
Significance 3.72E-06 1.89E-06 1.89E-06
Significant? Yes Yes Yes
Table2: Differences and significance
RDF gen. Syntax Comprehensibility Overall
VS Expert
Difference 0.86 0.87 0.59
Significance 2.42E-08 1.16E-08 6.26E-06
Significant? Yes Yes Yes
Table3: Differences and significance
TT VS Syntax Comprehensibility Overall
Expert 0.16 0.12 0.11
Difference
Significance 0.35 0.47 0.46
Significant? No No No
</table>
<tableCaption confidence="0.819358">
Table4: Differences and significance
</tableCaption>
<bodyText confidence="0.9999474">
As shown in table 1, experts score the highest and
the “RDF generator” scores the lowest in every
dimension. TT’s performance is worse than but
close to the experts’, however neither of them
scores very high. Indeed, both of them score less
than 2.5 in overall quality. The reason for the low
scores is probably that the data for the test
contained many domain dependent terms, which
the readers did not understand or felt were “odd”,
e.g., area125. In syntax and comprehensibility,
both experts and TT achieve an “average” level.
However, it seems that the readers do not like the
“RDF flavoured” text. We talked with the readers
about the texts after the experiment and found out
that the “odd” domain dependent terms lowered
readers’ scores, though the readers understood
most of the texts. According to table 2 and table 3,
both experts and TT differ significantly from the
“RDF generator”. According to table 4, we could
not find a significant difference between the
experts and TT. This does not indicate that TT is
as good as the experts because a bigger sample
may show a significant difference. As an
example of where TT is not as good as the
humans, one of our experts writes “The industry
of the North is manufacturing sector.” from
(North industryOfArea manufacturing_sector),
which is more “natural” than TT’s generation,
“North is the industry of area manufacturing
sector.” So we may only say that TT’s
performance is to some extent close to the experts.
On the other hand, the fact that we were with this
sample able to show a significant difference
between the other pairs gives us some confidence
in the adequacy of TT’s output.
</bodyText>
<sectionHeader confidence="0.934096" genericHeader="conclusions">
Conclusion
</sectionHeader>
<bodyText confidence="0.99845">
Our corpus analysis and the evaluation
experiment show that there is an opportunity to
achieve adequate quality domain independent
generation from RDF data. Our future work will
focus on generating from larger RDF graphs.
</bodyText>
<sectionHeader confidence="0.997602" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.998345">
Our thanks go to EPSRC, who funds our research
project through grant GR/S62932/01.
</bodyText>
<sectionHeader confidence="0.998905" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998323842105263">
J. C. Lester, and B. W. Porter. (1997) Developing
and empirically evaluating robust explanation
generators: The Knight Experiments.
Computational Linguistics 23(1):65–101.
O. Mason, (2003). Qtag 3.1. Department of
English, School of Humanities, University of
Birmingham,http://web.bham.ac.uk/O.Mason/s
oftware/tagger/
C. Mellish and X. Sun. (2006). The Semantic
Web as a Linguistic Resource. Knowledge
Based Systems 19, pp 298-303.
W3C, (1999). &amp;quot;Resource Description Framework
(RDF) Model and Syntax Specification.&amp;quot;,
http://www.w3.org/TR/PR-rdf-syntax/.
X Sun and C. Mellish. (2006). Domain
Independent Sentence Generation from RDF
Representations for the Semantic Web.
Combined Workshop on Language-Enabled
Educational Technology (ECAI&apos;06), Italy.
</reference>
<page confidence="0.998369">
108
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.722682">
<title confidence="0.999544">An Experiment on “Free Generation” from Single RDF triples</title>
<author confidence="0.93682">Xiantang</author>
<affiliation confidence="0.999893">Department of Computing Science,</affiliation>
<address confidence="0.9767795">of Aberdeen, UK, Ab24</address>
<email confidence="0.998241">xsun@csd.abdn.ac.uk</email>
<author confidence="0.999891">Chris Mellish</author>
<affiliation confidence="0.9197395">Department of Computing Science, University of Aberdeen</affiliation>
<address confidence="0.989233">Aberdeen, UK, Ab24 3UE</address>
<email confidence="0.997853">cmellish@csd.abdn.ac.uk</email>
<abstract confidence="0.998119928571429">This paper introduces our domain independent approach to “free generation” from single RDF triples without using any domain dependent knowledge. Our approach is developed based on our argument that RDF representations carry rich linguistic information, which can be used to achieve readable domain independent generation. In order to examine to what extent our argument is realistic, we carry out an evaluation experiment, which is the first evaluation of this kind of domain independent generation in the field.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J C Lester</author>
<author>B W Porter</author>
</authors>
<title>Developing and empirically evaluating robust explanation generators: The Knight Experiments.</title>
<date>1997</date>
<journal>Computational Linguistics</journal>
<volume>23</volume>
<issue>1</issue>
<contexts>
<context position="6975" citStr="Lester and Porter 1997" startWordPosition="1104" endWordPosition="1107">ates, abg has a property k34 with value 377, from the triple (abg k34 377). 3 Evaluation In order to examine to what extent our domain independent generation is realistic in terms of syntax, comprehensibility and overall quality, we carried out an experiment to compare TT’s generation with human experts’ generation and pure “RDF flavoured” generation (the “RDF generator”), as our baseline (as in the example shown at the end of section 2). 3.1 Experiment design We applied a “Two-Panel” methodology to compare the three kinds of generation, which is similar to the methodology applied in KNIGHT5 (Lester and Porter 1997). The Two-Panel evaluation methodology can be used to empirically evaluate NLG generation by comparing computer generation with human generation. We take Computer Blindness as a central principle through the experiment in order to guarantee the integrity of the evaluation results. This means that our judges do not know that any texts are generated by computer.. 4 We simply add an indefinite determiner if the ‘noun’ is not in its plural form. 5 In KNIGHT only the system’s generation and human generation are compared. 106 There are four steps in our methodology: • randomly selecting 90 triples w</context>
</contexts>
<marker>Lester, Porter, 1997</marker>
<rawString>J. C. Lester, and B. W. Porter. (1997) Developing and empirically evaluating robust explanation generators: The Knight Experiments. Computational Linguistics 23(1):65–101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Mason</author>
</authors>
<date>2003</date>
<journal>Qtag</journal>
<volume>3</volume>
<institution>Department of English, School of Humanities, University of Birmingham,http://web.bham.ac.uk/O.Mason/s oftware/tagger/</institution>
<contexts>
<context position="5234" citStr="Mason 2003" startWordPosition="814" endWordPosition="815">erns found from the corpus, we developed a system (TT) to generate single sentences from single RDF triples without any domain dependent knowledge. The key idea here is to construct VPs from the properties of input triples and treat the subjects and objects of the triples as domain dependent terms, which are simply seen as proper nouns e.g., (John Surname Murphy ) -) ”John has a surname Murphy.” So the main part of the system is about constructing proper VPs from properties. In the process of constructing VPs, every property is tokenised into a sequence of units, e.g., hasEmail 3 We use QTAG (Mason 2003) to recognise the parts-of-speech (POS) of the units extracted from the property names. For example, QTAG can recognise ‘has’ and ‘colour’ (from ‘hasColour’) as a verb and a noun. We achieved 99.2% accuracy of POS recognition on our corpus using QTAG with the assistance of some manually-added rules. is separated into has and email. Then, the property is classified into one of our 6 categories, and in this case hasEmail belongs to the ‘has’ category. In each category, we have a set of rules to construct VPs from input properties. A rule’s LHS is a pattern and the RHS is a corresponding linguist</context>
</contexts>
<marker>Mason, 2003</marker>
<rawString>O. Mason, (2003). Qtag 3.1. Department of English, School of Humanities, University of Birmingham,http://web.bham.ac.uk/O.Mason/s oftware/tagger/</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Mellish</author>
<author>X Sun</author>
</authors>
<title>The Semantic Web as a Linguistic Resource. Knowledge Based Systems 19,</title>
<date>2006</date>
<pages>298--303</pages>
<marker>Mellish, Sun, 2006</marker>
<rawString>C. Mellish and X. Sun. (2006). The Semantic Web as a Linguistic Resource. Knowledge Based Systems 19, pp 298-303.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W3C</author>
</authors>
<title>Resource Description Framework (RDF) Model and Syntax Specification.&amp;quot;,</title>
<date>1999</date>
<location>http://www.w3.org/TR/PR-rdf-syntax/.</location>
<marker>W3C, 1999</marker>
<rawString>W3C, (1999). &amp;quot;Resource Description Framework (RDF) Model and Syntax Specification.&amp;quot;, http://www.w3.org/TR/PR-rdf-syntax/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Sun</author>
<author>C Mellish</author>
</authors>
<date>2006</date>
<booktitle>Domain Independent Sentence Generation from RDF Representations for the Semantic Web. Combined Workshop on Language-Enabled Educational Technology (ECAI&apos;06),</booktitle>
<contexts>
<context position="3612" citStr="Sun and Mellish 2006" startWordPosition="554" endWordPosition="557">ingly, we find that the generation process in fact does not require understanding the real semantics of the terms, ‘Longridge’, ‘Merlot’, ‘has’, ‘maker’ and ‘Longridge’. That is, the words embedded in the names of these terms could form most of the final sentence and they have been already placed in a sensible way by their creators. This implies that what is required to make the final sentence is to just to fill in “missing” words, e.g., determiners. Obviously, the name of the property in the triple plays the 105 most crucial role in the generation. As our earlier work (Mellish and Sun 2005) (Sun and Mellish 2006) shows, RDF representations indeed contain rich linguistic information. In our corpus of 882 OWL files which include 37260 class names and 1218 property names, only 14% of class names consist totally of meaningless strings and 97% of the properties’ names fully or partially consist of natural words. Our further analysis has shown that the properties may be classified into 6 categories based on their “patterns”. 37.2% of the properties start with ‘has’, 12.3% start with ‘is’, 11.3% end with a preposition, 18.0% are single words, 12.5% have two words, and 8.3% have more than two words. In each c</context>
</contexts>
<marker>Sun, Mellish, 2006</marker>
<rawString>X Sun and C. Mellish. (2006). Domain Independent Sentence Generation from RDF Representations for the Semantic Web. Combined Workshop on Language-Enabled Educational Technology (ECAI&apos;06), Italy.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>