<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000903">
<title confidence="0.998995">
Punctuation Processing for Projective Dependency Parsing∗
</title>
<author confidence="0.996771">
Ji Ma†, Yue Zhang‡ and Jingbo Zhu†*
</author>
<affiliation confidence="0.945300333333333">
†Northeastern University, Shenyang, China
‡Singapore University of Technology and Design, Singapore
*Hangzhou YaTuo Company, 358 Wener Rd., Hangzhou, China, 310012
</affiliation>
<email confidence="0.790941666666667">
majineu@gmail.com
yue zhang@sutd.edu.sg
zhujingbo@mail.neu.edu.cn
</email>
<sectionHeader confidence="0.993439" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9986271875">
Modern statistical dependency parsers as-
sign lexical heads to punctuations as well
as words. Punctuation parsing errors lead
to low parsing accuracy on words. In this
work, we propose an alternative approach
to addressing punctuation in dependency
parsing. Rather than assigning lexical
heads to punctuations, we treat punctu-
ations as properties of their neighbour-
ing words, used as features to guide the
parser to build the dependency graph. In-
tegrating our method with an arc-standard
parser yields a 93.06% unlabelled attach-
ment score, which is the best accuracy by
a single-model transition-based parser re-
ported so far.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996765704918033">
The task of dependency parsing is to identify the
lexical head of each of the tokens in a string.
Modern statistical parsers (McDonald et al., 2005;
Nivre et al., 2007; Huang and Sagae, 2010; Zhang
and Nivre, 2011) treat all the tokens equally, as-
signing lexical heads to punctuations as well as
words. Punctuations arguably play an important
role in syntactic analysis. However, there are a
number of reasons that it is not necessary to parse
punctuations:
First, the lexical heads of punctuations are not
as well defined as those of words. Consequently,
punctuations are not as consistently annotated in
treebanks as words, making it harder to parse
punctuations. For example, modern statistical
parsers achieve above 90% unlabelled attachment
score (UAS) on words. However, the UAS on
punctuations are generally below 85%.
∗This work was done while the first author was visiting
SUTD
Moreover, experimental results showed that
parsing accuracy of content words drops on sen-
tences which contain higher ratios of punctuations.
One reason for this result is that projective de-
pendency parsers satisfy the “no crossing links”
constraint, and errors in punctuations may pre-
vent correct word-word dependencies from being
created (see section 2). In addition, punctuations
cause certain type of features inaccurate. Take va-
lency features for example, previous work (Zhang
and Nivre, 2011) has shown that such features are
important to parsing accuracy, e.g., it may inform
the parser that a verb already has two objects at-
tached to it. However, such information might
be inaccurate when the verb’s modifiers contain
punctuations.
Ultimately, it is the dependencies between
words that provide useful information for real
world applications. Take machine translation or
information extraction for example, most systems
take advantage of the head-modifier relationships
between word pairs rather than word-punctuation
pairs to make better predictions. The fact that most
previous work evaluates parsing accuracies with-
out taking punctuations into account is also largely
due to this reason.
Given the above reasons, we propose an alterna-
tive approach to punctuation processing for depen-
dency parsing. In this method, punctuations are
not associated with lexical heads, but are treated
as properties of their neighbouring words.
Our method is simple and can be easily incor-
porated into state-of-the-art parsers. In this work,
we report results on an arc-standard transition-
based parser. Experiments show that our method
achieves about 0.90% UAS improvement over the
greedy baseline parser on the standard Penn Tree-
bank test set. Although the improvement becomes
smaller as the beam width grows larger, we still
achieved 93.06% UAS with a beam of width 64,
which is the best result for transition-based parsers
</bodyText>
<page confidence="0.971093">
791
</page>
<note confidence="0.45093">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 791–796,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<table confidence="0.999059166666667">
Length 1-20 21-40 41-60
Punc % 0-15 15-30 &gt; 30 0-15 15-30 &gt; 30 0-15 15-30 &gt; 30
E-F 94.56 92.88 87.67 91.84 91.82 83.87 89.83 88.01 -
A-S 93.87 92.00 90.05 90.81 90.15 75.00 88.06 88.89 -
A-S-64 95.28 94.43 88.15 92.96 92.63 76.61 90.78 88.76 -
MST 94.90 93.55 88.15 92.45 93.11 77.42 90.89 89.77 -
</table>
<tableCaption confidence="0.980967">
Table 2: Parsing accuracies vs punctuation ratios, on the development set
</tableCaption>
<table confidence="0.999818428571429">
System E-F A-S A-S-64 MST
Dev UAS 91.83 90.71 93.02 92.56
Test UAS 91.75 90.34 92.84 92.10
Dev UAS-p 83.20 79.69 84.80 84.42
Test UAS-p 84.67 79.64 87.80 85.67
Dev− UAS 90.64 89.55 91.87 90.11
Test− UAS 90.40 89.33 91.75 89.82
</table>
<tableCaption confidence="0.999626">
Table 1: Parsing accuracies. “E-F” and “MST” de-
</tableCaption>
<bodyText confidence="0.993536375">
note easy-first parser and MSTparser, respectively.
“A-S” and “A-S 64” denote our arc-standard parser
with beam width 1 and 64, respectively. “UAS”
and “UAS-p” denote word and punctuation unla-
belled attachment score, respectively. “−” denotes
the data set with punctuations removed.
reported so far. Our code will be available at
https://github.com/majineu/Parser/Punc/A-STD.
</bodyText>
<sectionHeader confidence="0.746246" genericHeader="method">
2 Influence of Punctuations on Parsing
</sectionHeader>
<bodyText confidence="0.999894333333333">
In this section, we conduct a set of experiments to
show the influence of punctuations on dependency
parsing accuracies.
</bodyText>
<subsectionHeader confidence="0.979808">
2.1 Setup
</subsectionHeader>
<bodyText confidence="0.999968307692308">
We use the Wall Street Journal portion of the Penn
Treebank with the standard splits: sections 02-21
are used as the training set; section 22 and sec-
tion 23 are used as the development and test set,
respectively. Penn2Malt is used to convert brack-
eted structures into dependencies. We use our own
implementation of the Part-Of-Speech (POS) tag-
ger proposed by Collins (2002) to tag the devel-
opment and test sets. Training set POS tags are
generated using 10-fold jack-knifing. Parsing ac-
curacy is evaluated using unlabelled attachment
score (UAS), which is the percentage of words that
are assigned the correct lexical heads.
To show that the influence of punctuations
on parsing is independent of specific pars-
ing algorithms, we conduct experiments us-
ing three parsers, each representing a different
parsing methodology: the open source MST-
Parser1(McDonald and Pereira, 2006), our own
re-implementation of an arc-standard transition-
based parser (Nivre, 2008), which is trained us-
ing global learning and beam-search (Zhang and
Clark, 2008) with a rich feature set (Zhang and
Nivre, 2011) 2, and our own re-implementation of
the easy-first parser (Goldberg and Elhadad, 2010)
with an extended feature set (Ma et al., 2013).
</bodyText>
<subsectionHeader confidence="0.999062">
2.2 Punctuations and Parsing Accuracy
</subsectionHeader>
<bodyText confidence="0.999993791666667">
Our first experiment is to show that, compared
with words, punctuations are more difficult to
parse and to learn. To see this, we evaluate the
parsing accuracies of the selected parsers on words
and punctuations, separately. Results are listed
in Table 1, where row 2 and row 3 list the UAS
of words (all excluding punctuations) on the de-
velopment and test set, respectively. Row 4 and
row 5 list accuracies of punctuations (all excluding
words) on the development and test set, respec-
tively. We can see that although all the parsers
achieve above 90% UAS on words, the UAS on
punctuations are mostly below 85%.
As for learning, we calculate the percentage of
parameter updates that are caused by associating
punctuations with incorrect heads during training
of the easy-first parser3. The result is that more
than 31% of the parameter updates are caused due
to punctuations, though punctuations account for
only 11.6% of the total tokens in the training set.
The fact that parsers achieve low accuracies on
punctuations is to some degree expected, because
the head of a punctuation mark is linguistically
less well-defined. However, a related problem is
</bodyText>
<footnote confidence="0.625672">
1We trained a second order labelled parser with all the
configurations set to the default value. The code is publicly
available at http://sourceforge.net/projects/mstparser/
2Some feature templates in Zhang and Nivre (2011) in-
volve head word and head POS tags which are not avail-
able for an arc-standard parser. Interestingly, without those
features our arc-standard parser still achieves 92.84% UAS
which is comparable to the 92.90% UAS obtained by the arc-
eager parser of Zhang and Nivre (2011)
3For the greedy easy-first parser, whether a parameter up-
date is caused by punctuation error can be determined with
no ambiguity.
</footnote>
<page confidence="0.991487">
792
</page>
<figureCaption confidence="0.9990325">
Figure 1: Illustration of processing paired punctuation. The property of a word is denoted by the punc-
tuation below that word.
</figureCaption>
<bodyText confidence="0.996207911111111">
that parsing accuracy on words tends to drop on
the sentences which contain high ratio of punc-
tuations. To see this, we divide the sentences in
the development set into sub-sets according the
punctuation ratio (percentage of punctuations that
a sentence contains), and then evaluate parsing ac-
curacies on the sub-sets separately.
The results are listed in Table 2. Since long
sentences are inherently more difficult to parse,
to make a fair comparison, we further divide the
development set according to sentence lengths as
shown in the first row4. We can see that most of the
cases, parsing accuracies drop on sentences with
higher punctuation ratios. Note that this negative
effect on parsing accuracy might be overlooked
since most previous work evaluates parsing accu-
racy without taking punctuations into account.
By inspecting the parser outputs, we found that
error propagation caused by assigning incorrect
head to punctuations is one of the main reason that
leads to this result. Take the sentence shown in
Figure 1 (a) for example, the word Mechanisms
is a modifier of entitled according to the gold ref-
erence. However, if the quotation mark, “, is in-
correctly recognized as a modifier of was, due to
the “no crossing links” constraint, the arc between
Mechanisms and entitled can never be created.
A natural question is whether it is possible to
reduce such error propagation by simply remov-
ing all punctuations from parsing. Our next ex-
periment aims at answering this question. In this
experiment, we first remove all punctuations from
the original data and then modify the dependency
arcs accordingly in order to maintain word-word
dependencies in the original data. We re-train the
parsers on the modified training set and evaluate
41694 out of 1700 sentences on the development set with
length no larger than 60 tokens
parsing accuracies on the modified data.
Results are listed in row 6 and row 7 of Table 1.
We can see that parsing accuracies on the modified
data drop significantly compared with that on the
original data. The result indicates that by remov-
ing punctuations, we lose some information that is
important for dependency parsing.
</bodyText>
<sectionHeader confidence="0.943021" genericHeader="method">
3 Punctuation as Properties
</sectionHeader>
<bodyText confidence="0.99984575">
In our method, punctuations are treated as prop-
erties of its neighbouring words. Such properties
are used as additional features to guide the parser
to construct the dependency graph.
</bodyText>
<subsectionHeader confidence="0.996326">
3.1 Paired Punctuation
</subsectionHeader>
<bodyText confidence="0.993367636363636">
Our method distinguishes paired punctuations
from other punctuations. Here paired punctuations
include brackets and quotations marks, whose
Penn Treebank POS tags are the following four:
-LRB- -RRB- “ ”
The characteristics of paired punctuations include:
(1) they typically exist in pairs; (2) they serve as
boundaries that there is only one dependency arc
between the words inside the boundaries and the
words outside. Take the sentence in Figure 1 (a)
for example, the only arc cross the boundary is
(Mechanisms, entitled) where entitled is the head.
To utilize such boundary information, we fur-
ther classify paired punctuations into two cate-
gories: those that serve as the beginning of the
boundary, whose POS tags are either -LRB- or “,
denoted by BPUNC; and those that serve as the end
of the boundary, denoted by EPUNC.
Before parsing starts, a preprocessing step is
used to first attach the paired punctuations as
properties of their neighbouring words, and then
remove them from the sentence. In particular,
</bodyText>
<page confidence="0.990314">
793
</page>
<table confidence="0.9832951">
unigram for p in β0, β1, β2, β3, σ0, σ1, σ2
for p in β0, β1, β2, σ0, σ1
bigram for p, q in (σ0, β0), (σ0, β1), (σ0, β2), (σ0, σ1), (σ0, σ2)
for p, q in (σ2, σ0), (σ1, σ0), (σ2, σ0)
for p, q in (σ2, σ0), (σ1, σ0), (σ0, β0)
ppunc
ppunc G pw, ppunc G pt
ppunc G qpunc, ppunc G qt, ppunc G qw
ppunc O qt, ppunc O pt O qt
dpq O ppunc O pt O qt
</table>
<tableCaption confidence="0.99589">
Table 3: Feature templates. For an element p either on σ or β of an arc-standard parser, we use ppunc,
</tableCaption>
<bodyText confidence="0.989825684210526">
pw and pt to denote the punctuation property, head word and head tag of p, respectively. dpq denotes the
distance between the two elements p and q.
we attach BPUNCs to their right neighbours and
EPUNCs to their left neighbours, as shown in Fig-
ure 1 (b). Note that in Figure 1 (a), the left neigh-
bour of ” is also a punctuation. In such cases, we
simply remove these punctuations since the exis-
tence of paired punctuations already indicates that
there should be a boundary.
During parsing, when a dependency arc with
lexical head wh is created, the property of wh is
updated by the property of its left (or right) most
child to keep track whether there is a BPUNC (or
EPUNC) to the left (or right) side of the sub-tree
rooted at wh, as shown in Figure 1 (c). When
BPUNCs and EPUNCs meet each other at wh, a
PAIRED property is assigned to wh to capture that
the words within the paired punctuations form a
sub-tree, rooted at wh. See Figure 1 (d).
</bodyText>
<subsectionHeader confidence="0.999079">
3.2 Practical Issues
</subsectionHeader>
<bodyText confidence="0.999976">
It is not uncommon that two BPUNCS appear ad-
jacent to each other. For example,
</bodyText>
<subsubsectionHeader confidence="0.515218">
(“Congress’s Environmental Buccaneers,”
</subsubsectionHeader>
<bodyText confidence="0.999060416666667">
Sept. 18).
In our implementation, BPUNC or EPUNC prop-
erties are implemented using flags. In the exam-
ple, we set two flags “ and ( on the word Con-
grees’s. When BPUNC and EPUNC meet each
other, the corresponding flags are turned off. In
the example, when Congrees’s is identified as a
modifier of Buccaneers, the ” flag of Buccaneers
is turned off. However, we do not assign a PAIRED
property to Buccaneers since its ( flag is still on.
The PAIRED property is assigned only when all
the flags are turned off.
</bodyText>
<subsectionHeader confidence="0.971557">
3.3 Non-Paired Punctuations
</subsectionHeader>
<bodyText confidence="0.999968695652174">
Though some types of non-paired punctuations
may capture certain syntactic patterns, we do not
make further distinctions between them, and treat
these punctuations uniformly for simplicity.
Before parsing starts and after the preprocessing
step for paired punctuations, our method employs
a second preprocessing step to attach non-paired
punctuations to their left neighbouring words. It
is guaranteed that the property of the left neigh-
bouring words of non-paired punctuations must be
empty. Otherwise, it means the non-paired punc-
tuation is adjacent to a paired punctuation. In
such cases, the non-paired punctuation would be
removed in the first processing step.
During parsing, non-paired punctuations are
also passed bottom-up: the property of wh is up-
dated by its right-most dependent to keep track
whether there is a punctuation to the right side
of the tree rooted at wh. The only special case is
that if wh already contains a BPUNC property, then
our method simply ignores the non-paired prop-
erty since we maintain the boundary information
with the highest priority.
</bodyText>
<subsectionHeader confidence="0.659001">
3.4 Features
</subsectionHeader>
<bodyText confidence="0.9766984375">
We incorporate our method into the arc-standard
transition-based parser, which uses a stack σ to
maintain partially constructed trees and a buffer β
for the incoming words (Nivre, 2008). We design
a set of features to exploit the potential of using
punctuation properties for the arc-standard parser.
The feature templates are listed in Table 3.
In addition to the features designed for paired
punctuations, such as bigram punctuation features
listed in line 3 of Table 3, we also design features
for non-paired punctuations. For example, the dis-
tance features in line 5 of Table 3 is used to capture
the pattern that if a word w with comma property
is the left modifier of a noun or a verb, the distance
between w and its lexical head is often larger than
1. In other words, they are not adjacent.
</bodyText>
<sectionHeader confidence="0.999855" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.9999848">
Our first experiment is to investigate the effect of
processing paired punctuations on parsing accu-
racy. In this experiment, the method introduced
in Section 3.1 is used to process paired punctua-
tions, and the non-paired punctuations are left un-
</bodyText>
<page confidence="0.996006">
794
</page>
<table confidence="0.999476222222222">
system UAS Comp Root
Baseline 90.38 37.71 89.45
All-Punc 91.32 41.35 92.43
Baseline-64 92.84 46.90 95.57
All-Punc-64 93.06 48.55 95.53
Huang 10 92.10 − −
Zhang 11 92.90 48.00 91.80
Choi 13 92.96 − −
Bohnet 12 93.03 − −
</table>
<tableCaption confidence="0.73709">
Table 5: Final result on the test set.
</tableCaption>
<table confidence="0.997923714285714">
s Baseline Paired All
1 90.76 91.25 91.47
2 91.88 92.06 92.34
4 92.50 92.61 92.70
8 92.73 92.76 92.82
16 92.90 92.94 92.99
64 92.99 93.04 93.10
</table>
<tableCaption confidence="0.7426405">
Table 4: Parsing accuracies on the development
set. s denotes the beam width.
</tableCaption>
<bodyText confidence="0.999398865384615">
touched. Feature templates used in this experi-
ment are those listed in the top three rows of Ta-
ble 3 together with those used for the baseline arc-
standard parser.
Results on the development set are shown in the
second column of Table 4. We can see that when
the beam width is set to 1, our method achieves an
0.49 UAS improvement. By comparing the out-
puts of the two parsers, two types of errors made
by the baseline parser are effectively corrected.
The first is that our method is able to cap-
ture the pattern that there is only one depen-
dency arc between the words within the paired-
punctuations and the words outside, while the
baseline parser sometimes creates more depen-
dency arcs that cross the boundary.
The second is more interesting. Our method is
able to capture that the root, wh, of the sub-tree
within the paired-punctuation, such as “Mecha-
nisms” in Figure 1, generally serves as a modifier
of the words outside, while the baseline parser oc-
casionally make wh as the head of the sentence.
As we increase the beam width, the improve-
ment of our method over the baseline becomes
smaller. This is as expected, since beam search
also has the effect of reducing error propagation
(Zhang and Nivre, 2012), thereby alleviating the
errors caused by punctuations.
In the last experiment, we examine the effect
of incorporating all punctuations using the method
introduced in Section 2. In this experiment, we
use all the feature templates in Table 3 and those
in the baseline parser. Results are listed in the
fourth column of Table 4, which shows that pars-
ing accuracies can be further improved by also
processing non-paired punctuations. The overall
accuracy improvement when the beam width is 1
reaches 0.91%. The extra improvements mainly
come from better accuracies on the sentences with
comma. However, the exact type of errors that
are corrected by using non-paired punctuations is
more difficult to summarize.
The final results on the test set are listed in Ta-
ble 55. Table 5 also lists the accuracies of state-
of-the-art transition-based parsers. In particular,
“Huang 10” and “Zhang 11” denote Huang and
Sagae (2010) and Zhang and Nivre (2011), re-
spectively. “Bohnet 12” and “Choi 13” denote
Bohnet and Nivre (2012) and Choi and Mccal-
lum (2013), respectively. We can see that our
method achieves the best accuracy for single-
model transition-based parsers.
</bodyText>
<sectionHeader confidence="0.990638" genericHeader="conclusions">
5 Conclusion and Related Work
</sectionHeader>
<bodyText confidence="0.999951928571429">
In this work, we proposed to treat punctuations
as properties of context words for dependency
parsing. Experiments with an arc-standard parser
showed that our method effectively improves pars-
ing performance and we achieved the best accu-
racy for single-model transition-based parser.
Regarding punctuation processing for depen-
dency parsing, Li et al. (2010) proposed to uti-
lize punctuations to segment sentences into small
fragments and then parse the fragments separately.
A similar approach is proposed by Spitkovsky et
al. (2011) which also designed a set of constraints
on the fragments to improve unsupervised depen-
dency parsing.
</bodyText>
<sectionHeader confidence="0.994949" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999063888888889">
We highly appreciate the anonymous reviewers
for their insightful suggestions. This research
was supported by the National Science Founda-
tion of China (61272376; 61300097; 61100089),
the Fundamental Research Funds for the Cen-
tral Universities (N110404012), the research grant
T2MOE1301 from Singapore Ministry of Ed-
ucation (MOE) and the start-up grant SRG
ISTD2012038 from SUTD.
</bodyText>
<footnote confidence="0.9898135">
5The number of training iteration is determined using the
development set.
</footnote>
<page confidence="0.996683">
795
</page>
<sectionHeader confidence="0.982165" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.971665355555556">
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and la-
beled non-projective dependency parsing. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, EMNLP-
CoNLL ’12, pages 1455–1465, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Jinho D. Choi and Andrew Mccallum. 2013.
Transition-based dependency parsing with selec-
tional branching. In In Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistics.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In Proceedings
of the ACL-02 conference on Empirical methods in
natural language processing - Volume 10, EMNLP
’02, pages 1–8, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Yoav Goldberg and Michael Elhadad. 2010. An effi-
cient algorithm for easy-first non-directional depen-
dency parsing. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, HLT ’10, pages 742–750, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In
Jan Hajic, Sandra Carberry, and Stephen Clark, ed-
itors, ACL, pages 1077–1086. The Association for
Computer Linguistics.
Zhenghua Li, Wanxiang Che, and Ting Liu. 2010. Im-
proving dependency parsing using punctuation. In
Minghui Dong, Guodong Zhou, Haoliang Qi, and
Min Zhang, editors, IALP, pages 53–56. IEEE Com-
puter Society.
Ji Ma, Jingbo Zhu, Tong Xiao, and Nan Yang. 2013.
Easy-first pos tagging and dependency parsing with
beam search. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers), pages 110–114,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Ryan McDonald and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In Proceedings of 11th Conference of the
European Chapter of the Association for Computa-
tional Linguistics (EACL-2006)), volume 6, pages
81–88.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of the 43rd An-
nual Meeting on Association for Computational Lin-
guistics, ACL ’05, pages 91–98, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav
Marinov, and Erwin Marsi. 2007. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95–135.
Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. Computational Lin-
guistics, 34(4):513–553.
Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Ju-
rafsky. 2011. Punctuation: Making a point in un-
supervised dependency parsing. In Proceedings of
the Fifteenth Conference on Computational Natural
Language Learning (CoNLL-2011).
Yue Zhang and Stephen Clark. 2008. A tale of two
parsers: Investigating and combining graph-based
and transition-based dependency parsing. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 562–
571, Honolulu, Hawaii, October. Association for
Computational Linguistics.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 188–193, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Yue Zhang and Joakim Nivre. 2012. Analyzing
the effect of global learning and beam-search on
transition-based dependency parsing. In Proceed-
ings of COLING 2012: Posters, pages 1391–1400,
Mumbai, India, December. The COLING 2012 Or-
ganizing Committee.
</reference>
<page confidence="0.998548">
796
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.456028">
<title confidence="0.999617">Processing for Projective Dependency</title>
<author confidence="0.997003">Yue Jingbo</author>
<affiliation confidence="0.998819">University, Shenyang, University of Technology and Design,</affiliation>
<address confidence="0.759428">YaTuo Company, 358 Wener Rd., Hangzhou, China,</address>
<email confidence="0.754057">yuezhujingbo@mail.neu.edu.cn</email>
<abstract confidence="0.994086470588235">Modern statistical dependency parsers assign lexical heads to punctuations as well as words. Punctuation parsing errors lead to low parsing accuracy on words. In this work, we propose an alternative approach to addressing punctuation in dependency parsing. Rather than assigning lexical heads to punctuations, we treat punctuations as properties of their neighbouring words, used as features to guide the parser to build the dependency graph. Integrating our method with an arc-standard yields a unlabelled attachment score, which is the best accuracy by a single-model transition-based parser reported so far.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
<author>Joakim Nivre</author>
</authors>
<title>A transitionbased system for joint part-of-speech tagging and labeled non-projective dependency parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLPCoNLL ’12,</booktitle>
<pages>1455--1465</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="18684" citStr="Bohnet and Nivre (2012)" startWordPosition="3077" endWordPosition="3080">so processing non-paired punctuations. The overall accuracy improvement when the beam width is 1 reaches 0.91%. The extra improvements mainly come from better accuracies on the sentences with comma. However, the exact type of errors that are corrected by using non-paired punctuations is more difficult to summarize. The final results on the test set are listed in Table 55. Table 5 also lists the accuracies of stateof-the-art transition-based parsers. In particular, “Huang 10” and “Zhang 11” denote Huang and Sagae (2010) and Zhang and Nivre (2011), respectively. “Bohnet 12” and “Choi 13” denote Bohnet and Nivre (2012) and Choi and Mccallum (2013), respectively. We can see that our method achieves the best accuracy for singlemodel transition-based parsers. 5 Conclusion and Related Work In this work, we proposed to treat punctuations as properties of context words for dependency parsing. Experiments with an arc-standard parser showed that our method effectively improves parsing performance and we achieved the best accuracy for single-model transition-based parser. Regarding punctuation processing for dependency parsing, Li et al. (2010) proposed to utilize punctuations to segment sentences into small fragmen</context>
</contexts>
<marker>Bohnet, Nivre, 2012</marker>
<rawString>Bernd Bohnet and Joakim Nivre. 2012. A transitionbased system for joint part-of-speech tagging and labeled non-projective dependency parsing. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLPCoNLL ’12, pages 1455–1465, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinho D Choi</author>
<author>Andrew Mccallum</author>
</authors>
<title>Transition-based dependency parsing with selectional branching. In</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="18713" citStr="Choi and Mccallum (2013)" startWordPosition="3082" endWordPosition="3086">ctuations. The overall accuracy improvement when the beam width is 1 reaches 0.91%. The extra improvements mainly come from better accuracies on the sentences with comma. However, the exact type of errors that are corrected by using non-paired punctuations is more difficult to summarize. The final results on the test set are listed in Table 55. Table 5 also lists the accuracies of stateof-the-art transition-based parsers. In particular, “Huang 10” and “Zhang 11” denote Huang and Sagae (2010) and Zhang and Nivre (2011), respectively. “Bohnet 12” and “Choi 13” denote Bohnet and Nivre (2012) and Choi and Mccallum (2013), respectively. We can see that our method achieves the best accuracy for singlemodel transition-based parsers. 5 Conclusion and Related Work In this work, we proposed to treat punctuations as properties of context words for dependency parsing. Experiments with an arc-standard parser showed that our method effectively improves parsing performance and we achieved the best accuracy for single-model transition-based parser. Regarding punctuation processing for dependency parsing, Li et al. (2010) proposed to utilize punctuations to segment sentences into small fragments and then parse the fragmen</context>
</contexts>
<marker>Choi, Mccallum, 2013</marker>
<rawString>Jinho D. Choi and Andrew Mccallum. 2013. Transition-based dependency parsing with selectional branching. In In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 conference on Empirical methods in natural language processing - Volume 10, EMNLP ’02,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5555" citStr="Collins (2002)" startWordPosition="856" endWordPosition="857">ode will be available at https://github.com/majineu/Parser/Punc/A-STD. 2 Influence of Punctuations on Parsing In this section, we conduct a set of experiments to show the influence of punctuations on dependency parsing accuracies. 2.1 Setup We use the Wall Street Journal portion of the Penn Treebank with the standard splits: sections 02-21 are used as the training set; section 22 and section 23 are used as the development and test set, respectively. Penn2Malt is used to convert bracketed structures into dependencies. We use our own implementation of the Part-Of-Speech (POS) tagger proposed by Collins (2002) to tag the development and test sets. Training set POS tags are generated using 10-fold jack-knifing. Parsing accuracy is evaluated using unlabelled attachment score (UAS), which is the percentage of words that are assigned the correct lexical heads. To show that the influence of punctuations on parsing is independent of specific parsing algorithms, we conduct experiments using three parsers, each representing a different parsing methodology: the open source MSTParser1(McDonald and Pereira, 2006), our own re-implementation of an arc-standard transitionbased parser (Nivre, 2008), which is trai</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms. In Proceedings of the ACL-02 conference on Empirical methods in natural language processing - Volume 10, EMNLP ’02, pages 1–8, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Michael Elhadad</author>
</authors>
<title>An efficient algorithm for easy-first non-directional dependency parsing.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10,</booktitle>
<pages>742--750</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6355" citStr="Goldberg and Elhadad, 2010" startWordPosition="977" endWordPosition="980"> which is the percentage of words that are assigned the correct lexical heads. To show that the influence of punctuations on parsing is independent of specific parsing algorithms, we conduct experiments using three parsers, each representing a different parsing methodology: the open source MSTParser1(McDonald and Pereira, 2006), our own re-implementation of an arc-standard transitionbased parser (Nivre, 2008), which is trained using global learning and beam-search (Zhang and Clark, 2008) with a rich feature set (Zhang and Nivre, 2011) 2, and our own re-implementation of the easy-first parser (Goldberg and Elhadad, 2010) with an extended feature set (Ma et al., 2013). 2.2 Punctuations and Parsing Accuracy Our first experiment is to show that, compared with words, punctuations are more difficult to parse and to learn. To see this, we evaluate the parsing accuracies of the selected parsers on words and punctuations, separately. Results are listed in Table 1, where row 2 and row 3 list the UAS of words (all excluding punctuations) on the development and test set, respectively. Row 4 and row 5 list accuracies of punctuations (all excluding words) on the development and test set, respectively. We can see that alth</context>
</contexts>
<marker>Goldberg, Elhadad, 2010</marker>
<rawString>Yoav Goldberg and Michael Elhadad. 2010. An efficient algorithm for easy-first non-directional dependency parsing. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10, pages 742–750, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kenji Sagae</author>
</authors>
<title>Dynamic programming for linear-time incremental parsing.</title>
<date>2010</date>
<pages>1077--1086</pages>
<editor>In Jan Hajic, Sandra Carberry, and Stephen Clark, editors, ACL,</editor>
<publisher>The Association for Computer Linguistics.</publisher>
<contexts>
<context position="1163" citStr="Huang and Sagae, 2010" startWordPosition="167" endWordPosition="170">o addressing punctuation in dependency parsing. Rather than assigning lexical heads to punctuations, we treat punctuations as properties of their neighbouring words, used as features to guide the parser to build the dependency graph. Integrating our method with an arc-standard parser yields a 93.06% unlabelled attachment score, which is the best accuracy by a single-model transition-based parser reported so far. 1 Introduction The task of dependency parsing is to identify the lexical head of each of the tokens in a string. Modern statistical parsers (McDonald et al., 2005; Nivre et al., 2007; Huang and Sagae, 2010; Zhang and Nivre, 2011) treat all the tokens equally, assigning lexical heads to punctuations as well as words. Punctuations arguably play an important role in syntactic analysis. However, there are a number of reasons that it is not necessary to parse punctuations: First, the lexical heads of punctuations are not as well defined as those of words. Consequently, punctuations are not as consistently annotated in treebanks as words, making it harder to parse punctuations. For example, modern statistical parsers achieve above 90% unlabelled attachment score (UAS) on words. However, the UAS on pu</context>
<context position="18585" citStr="Huang and Sagae (2010)" startWordPosition="3060" endWordPosition="3063">in the fourth column of Table 4, which shows that parsing accuracies can be further improved by also processing non-paired punctuations. The overall accuracy improvement when the beam width is 1 reaches 0.91%. The extra improvements mainly come from better accuracies on the sentences with comma. However, the exact type of errors that are corrected by using non-paired punctuations is more difficult to summarize. The final results on the test set are listed in Table 55. Table 5 also lists the accuracies of stateof-the-art transition-based parsers. In particular, “Huang 10” and “Zhang 11” denote Huang and Sagae (2010) and Zhang and Nivre (2011), respectively. “Bohnet 12” and “Choi 13” denote Bohnet and Nivre (2012) and Choi and Mccallum (2013), respectively. We can see that our method achieves the best accuracy for singlemodel transition-based parsers. 5 Conclusion and Related Work In this work, we proposed to treat punctuations as properties of context words for dependency parsing. Experiments with an arc-standard parser showed that our method effectively improves parsing performance and we achieved the best accuracy for single-model transition-based parser. Regarding punctuation processing for dependency</context>
</contexts>
<marker>Huang, Sagae, 2010</marker>
<rawString>Liang Huang and Kenji Sagae. 2010. Dynamic programming for linear-time incremental parsing. In Jan Hajic, Sandra Carberry, and Stephen Clark, editors, ACL, pages 1077–1086. The Association for Computer Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhenghua Li</author>
<author>Wanxiang Che</author>
<author>Ting Liu</author>
</authors>
<title>Improving dependency parsing using punctuation.</title>
<date>2010</date>
<pages>53--56</pages>
<editor>In Minghui Dong, Guodong Zhou, Haoliang Qi, and Min Zhang, editors, IALP,</editor>
<publisher>IEEE Computer Society.</publisher>
<contexts>
<context position="19211" citStr="Li et al. (2010)" startWordPosition="3156" endWordPosition="3159"> and Nivre (2011), respectively. “Bohnet 12” and “Choi 13” denote Bohnet and Nivre (2012) and Choi and Mccallum (2013), respectively. We can see that our method achieves the best accuracy for singlemodel transition-based parsers. 5 Conclusion and Related Work In this work, we proposed to treat punctuations as properties of context words for dependency parsing. Experiments with an arc-standard parser showed that our method effectively improves parsing performance and we achieved the best accuracy for single-model transition-based parser. Regarding punctuation processing for dependency parsing, Li et al. (2010) proposed to utilize punctuations to segment sentences into small fragments and then parse the fragments separately. A similar approach is proposed by Spitkovsky et al. (2011) which also designed a set of constraints on the fragments to improve unsupervised dependency parsing. Acknowledgements We highly appreciate the anonymous reviewers for their insightful suggestions. This research was supported by the National Science Foundation of China (61272376; 61300097; 61100089), the Fundamental Research Funds for the Central Universities (N110404012), the research grant T2MOE1301 from Singapore Mini</context>
</contexts>
<marker>Li, Che, Liu, 2010</marker>
<rawString>Zhenghua Li, Wanxiang Che, and Ting Liu. 2010. Improving dependency parsing using punctuation. In Minghui Dong, Guodong Zhou, Haoliang Qi, and Min Zhang, editors, IALP, pages 53–56. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ji Ma</author>
<author>Jingbo Zhu</author>
<author>Tong Xiao</author>
<author>Nan Yang</author>
</authors>
<title>Easy-first pos tagging and dependency parsing with beam search.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>110--114</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="6402" citStr="Ma et al., 2013" startWordPosition="986" endWordPosition="989">rrect lexical heads. To show that the influence of punctuations on parsing is independent of specific parsing algorithms, we conduct experiments using three parsers, each representing a different parsing methodology: the open source MSTParser1(McDonald and Pereira, 2006), our own re-implementation of an arc-standard transitionbased parser (Nivre, 2008), which is trained using global learning and beam-search (Zhang and Clark, 2008) with a rich feature set (Zhang and Nivre, 2011) 2, and our own re-implementation of the easy-first parser (Goldberg and Elhadad, 2010) with an extended feature set (Ma et al., 2013). 2.2 Punctuations and Parsing Accuracy Our first experiment is to show that, compared with words, punctuations are more difficult to parse and to learn. To see this, we evaluate the parsing accuracies of the selected parsers on words and punctuations, separately. Results are listed in Table 1, where row 2 and row 3 list the UAS of words (all excluding punctuations) on the development and test set, respectively. Row 4 and row 5 list accuracies of punctuations (all excluding words) on the development and test set, respectively. We can see that although all the parsers achieve above 90% UAS on w</context>
</contexts>
<marker>Ma, Zhu, Xiao, Yang, 2013</marker>
<rawString>Ji Ma, Jingbo Zhu, Tong Xiao, and Nan Yang. 2013. Easy-first pos tagging and dependency parsing with beam search. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 110–114, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Online learning of approximate dependency parsing algorithms.</title>
<date>2006</date>
<booktitle>In Proceedings of 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL-2006)),</booktitle>
<volume>6</volume>
<pages>81--88</pages>
<contexts>
<context position="6057" citStr="McDonald and Pereira, 2006" startWordPosition="930" endWordPosition="934">ted structures into dependencies. We use our own implementation of the Part-Of-Speech (POS) tagger proposed by Collins (2002) to tag the development and test sets. Training set POS tags are generated using 10-fold jack-knifing. Parsing accuracy is evaluated using unlabelled attachment score (UAS), which is the percentage of words that are assigned the correct lexical heads. To show that the influence of punctuations on parsing is independent of specific parsing algorithms, we conduct experiments using three parsers, each representing a different parsing methodology: the open source MSTParser1(McDonald and Pereira, 2006), our own re-implementation of an arc-standard transitionbased parser (Nivre, 2008), which is trained using global learning and beam-search (Zhang and Clark, 2008) with a rich feature set (Zhang and Nivre, 2011) 2, and our own re-implementation of the easy-first parser (Goldberg and Elhadad, 2010) with an extended feature set (Ma et al., 2013). 2.2 Punctuations and Parsing Accuracy Our first experiment is to show that, compared with words, punctuations are more difficult to parse and to learn. To see this, we evaluate the parsing accuracies of the selected parsers on words and punctuations, se</context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>Ryan McDonald and Fernando Pereira. 2006. Online learning of approximate dependency parsing algorithms. In Proceedings of 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL-2006)), volume 6, pages 81–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05,</booktitle>
<pages>91--98</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1120" citStr="McDonald et al., 2005" startWordPosition="159" endWordPosition="162"> work, we propose an alternative approach to addressing punctuation in dependency parsing. Rather than assigning lexical heads to punctuations, we treat punctuations as properties of their neighbouring words, used as features to guide the parser to build the dependency graph. Integrating our method with an arc-standard parser yields a 93.06% unlabelled attachment score, which is the best accuracy by a single-model transition-based parser reported so far. 1 Introduction The task of dependency parsing is to identify the lexical head of each of the tokens in a string. Modern statistical parsers (McDonald et al., 2005; Nivre et al., 2007; Huang and Sagae, 2010; Zhang and Nivre, 2011) treat all the tokens equally, assigning lexical heads to punctuations as well as words. Punctuations arguably play an important role in syntactic analysis. However, there are a number of reasons that it is not necessary to parse punctuations: First, the lexical heads of punctuations are not as well defined as those of words. Consequently, punctuations are not as consistently annotated in treebanks as words, making it harder to parse punctuations. For example, modern statistical parsers achieve above 90% unlabelled attachment s</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05, pages 91–98, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="1140" citStr="Nivre et al., 2007" startWordPosition="163" endWordPosition="166">ternative approach to addressing punctuation in dependency parsing. Rather than assigning lexical heads to punctuations, we treat punctuations as properties of their neighbouring words, used as features to guide the parser to build the dependency graph. Integrating our method with an arc-standard parser yields a 93.06% unlabelled attachment score, which is the best accuracy by a single-model transition-based parser reported so far. 1 Introduction The task of dependency parsing is to identify the lexical head of each of the tokens in a string. Modern statistical parsers (McDonald et al., 2005; Nivre et al., 2007; Huang and Sagae, 2010; Zhang and Nivre, 2011) treat all the tokens equally, assigning lexical heads to punctuations as well as words. Punctuations arguably play an important role in syntactic analysis. However, there are a number of reasons that it is not necessary to parse punctuations: First, the lexical heads of punctuations are not as well defined as those of words. Consequently, punctuations are not as consistently annotated in treebanks as words, making it harder to parse punctuations. For example, modern statistical parsers achieve above 90% unlabelled attachment score (UAS) on words.</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi. 2007. Maltparser: A language-independent system for data-driven dependency parsing. Natural Language Engineering, 13(2):95–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Algorithms for deterministic incremental dependency parsing.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>4</issue>
<contexts>
<context position="6140" citStr="Nivre, 2008" startWordPosition="944" endWordPosition="945"> proposed by Collins (2002) to tag the development and test sets. Training set POS tags are generated using 10-fold jack-knifing. Parsing accuracy is evaluated using unlabelled attachment score (UAS), which is the percentage of words that are assigned the correct lexical heads. To show that the influence of punctuations on parsing is independent of specific parsing algorithms, we conduct experiments using three parsers, each representing a different parsing methodology: the open source MSTParser1(McDonald and Pereira, 2006), our own re-implementation of an arc-standard transitionbased parser (Nivre, 2008), which is trained using global learning and beam-search (Zhang and Clark, 2008) with a rich feature set (Zhang and Nivre, 2011) 2, and our own re-implementation of the easy-first parser (Goldberg and Elhadad, 2010) with an extended feature set (Ma et al., 2013). 2.2 Punctuations and Parsing Accuracy Our first experiment is to show that, compared with words, punctuations are more difficult to parse and to learn. To see this, we evaluate the parsing accuracies of the selected parsers on words and punctuations, separately. Results are listed in Table 1, where row 2 and row 3 list the UAS of word</context>
<context position="15105" citStr="Nivre, 2008" startWordPosition="2456" endWordPosition="2457">ep. During parsing, non-paired punctuations are also passed bottom-up: the property of wh is updated by its right-most dependent to keep track whether there is a punctuation to the right side of the tree rooted at wh. The only special case is that if wh already contains a BPUNC property, then our method simply ignores the non-paired property since we maintain the boundary information with the highest priority. 3.4 Features We incorporate our method into the arc-standard transition-based parser, which uses a stack σ to maintain partially constructed trees and a buffer β for the incoming words (Nivre, 2008). We design a set of features to exploit the potential of using punctuation properties for the arc-standard parser. The feature templates are listed in Table 3. In addition to the features designed for paired punctuations, such as bigram punctuation features listed in line 3 of Table 3, we also design features for non-paired punctuations. For example, the distance features in line 5 of Table 3 is used to capture the pattern that if a word w with comma property is the left modifier of a noun or a verb, the distance between w and its lexical head is often larger than 1. In other words, they are </context>
</contexts>
<marker>Nivre, 2008</marker>
<rawString>Joakim Nivre. 2008. Algorithms for deterministic incremental dependency parsing. Computational Linguistics, 34(4):513–553.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valentin I Spitkovsky</author>
<author>Hiyan Alshawi</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Punctuation: Making a point in unsupervised dependency parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifteenth Conference on Computational Natural Language Learning (CoNLL-2011).</booktitle>
<marker>Spitkovsky, Alshawi, Jurafsky, 2011</marker>
<rawString>Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Jurafsky. 2011. Punctuation: Making a point in unsupervised dependency parsing. In Proceedings of the Fifteenth Conference on Computational Natural Language Learning (CoNLL-2011).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>A tale of two parsers: Investigating and combining graph-based and transition-based dependency parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>562--571</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="6220" citStr="Zhang and Clark, 2008" startWordPosition="955" endWordPosition="958">ning set POS tags are generated using 10-fold jack-knifing. Parsing accuracy is evaluated using unlabelled attachment score (UAS), which is the percentage of words that are assigned the correct lexical heads. To show that the influence of punctuations on parsing is independent of specific parsing algorithms, we conduct experiments using three parsers, each representing a different parsing methodology: the open source MSTParser1(McDonald and Pereira, 2006), our own re-implementation of an arc-standard transitionbased parser (Nivre, 2008), which is trained using global learning and beam-search (Zhang and Clark, 2008) with a rich feature set (Zhang and Nivre, 2011) 2, and our own re-implementation of the easy-first parser (Goldberg and Elhadad, 2010) with an extended feature set (Ma et al., 2013). 2.2 Punctuations and Parsing Accuracy Our first experiment is to show that, compared with words, punctuations are more difficult to parse and to learn. To see this, we evaluate the parsing accuracies of the selected parsers on words and punctuations, separately. Results are listed in Table 1, where row 2 and row 3 list the UAS of words (all excluding punctuations) on the development and test set, respectively. Ro</context>
</contexts>
<marker>Zhang, Clark, 2008</marker>
<rawString>Yue Zhang and Stephen Clark. 2008. A tale of two parsers: Investigating and combining graph-based and transition-based dependency parsing. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 562– 571, Honolulu, Hawaii, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Joakim Nivre</author>
</authors>
<title>Transition-based dependency parsing with rich non-local features.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>188--193</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="1187" citStr="Zhang and Nivre, 2011" startWordPosition="171" endWordPosition="174">n in dependency parsing. Rather than assigning lexical heads to punctuations, we treat punctuations as properties of their neighbouring words, used as features to guide the parser to build the dependency graph. Integrating our method with an arc-standard parser yields a 93.06% unlabelled attachment score, which is the best accuracy by a single-model transition-based parser reported so far. 1 Introduction The task of dependency parsing is to identify the lexical head of each of the tokens in a string. Modern statistical parsers (McDonald et al., 2005; Nivre et al., 2007; Huang and Sagae, 2010; Zhang and Nivre, 2011) treat all the tokens equally, assigning lexical heads to punctuations as well as words. Punctuations arguably play an important role in syntactic analysis. However, there are a number of reasons that it is not necessary to parse punctuations: First, the lexical heads of punctuations are not as well defined as those of words. Consequently, punctuations are not as consistently annotated in treebanks as words, making it harder to parse punctuations. For example, modern statistical parsers achieve above 90% unlabelled attachment score (UAS) on words. However, the UAS on punctuations are generally</context>
<context position="6268" citStr="Zhang and Nivre, 2011" startWordPosition="964" endWordPosition="967">ck-knifing. Parsing accuracy is evaluated using unlabelled attachment score (UAS), which is the percentage of words that are assigned the correct lexical heads. To show that the influence of punctuations on parsing is independent of specific parsing algorithms, we conduct experiments using three parsers, each representing a different parsing methodology: the open source MSTParser1(McDonald and Pereira, 2006), our own re-implementation of an arc-standard transitionbased parser (Nivre, 2008), which is trained using global learning and beam-search (Zhang and Clark, 2008) with a rich feature set (Zhang and Nivre, 2011) 2, and our own re-implementation of the easy-first parser (Goldberg and Elhadad, 2010) with an extended feature set (Ma et al., 2013). 2.2 Punctuations and Parsing Accuracy Our first experiment is to show that, compared with words, punctuations are more difficult to parse and to learn. To see this, we evaluate the parsing accuracies of the selected parsers on words and punctuations, separately. Results are listed in Table 1, where row 2 and row 3 list the UAS of words (all excluding punctuations) on the development and test set, respectively. Row 4 and row 5 list accuracies of punctuations (a</context>
<context position="7820" citStr="Zhang and Nivre (2011)" startWordPosition="1213" endWordPosition="1216">ng of the easy-first parser3. The result is that more than 31% of the parameter updates are caused due to punctuations, though punctuations account for only 11.6% of the total tokens in the training set. The fact that parsers achieve low accuracies on punctuations is to some degree expected, because the head of a punctuation mark is linguistically less well-defined. However, a related problem is 1We trained a second order labelled parser with all the configurations set to the default value. The code is publicly available at http://sourceforge.net/projects/mstparser/ 2Some feature templates in Zhang and Nivre (2011) involve head word and head POS tags which are not available for an arc-standard parser. Interestingly, without those features our arc-standard parser still achieves 92.84% UAS which is comparable to the 92.90% UAS obtained by the arceager parser of Zhang and Nivre (2011) 3For the greedy easy-first parser, whether a parameter update is caused by punctuation error can be determined with no ambiguity. 792 Figure 1: Illustration of processing paired punctuation. The property of a word is denoted by the punctuation below that word. that parsing accuracy on words tends to drop on the sentences whic</context>
<context position="18612" citStr="Zhang and Nivre (2011)" startWordPosition="3065" endWordPosition="3068">le 4, which shows that parsing accuracies can be further improved by also processing non-paired punctuations. The overall accuracy improvement when the beam width is 1 reaches 0.91%. The extra improvements mainly come from better accuracies on the sentences with comma. However, the exact type of errors that are corrected by using non-paired punctuations is more difficult to summarize. The final results on the test set are listed in Table 55. Table 5 also lists the accuracies of stateof-the-art transition-based parsers. In particular, “Huang 10” and “Zhang 11” denote Huang and Sagae (2010) and Zhang and Nivre (2011), respectively. “Bohnet 12” and “Choi 13” denote Bohnet and Nivre (2012) and Choi and Mccallum (2013), respectively. We can see that our method achieves the best accuracy for singlemodel transition-based parsers. 5 Conclusion and Related Work In this work, we proposed to treat punctuations as properties of context words for dependency parsing. Experiments with an arc-standard parser showed that our method effectively improves parsing performance and we achieved the best accuracy for single-model transition-based parser. Regarding punctuation processing for dependency parsing, Li et al. (2010) </context>
</contexts>
<marker>Zhang, Nivre, 2011</marker>
<rawString>Yue Zhang and Joakim Nivre. 2011. Transition-based dependency parsing with rich non-local features. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 188–193, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Joakim Nivre</author>
</authors>
<title>Analyzing the effect of global learning and beam-search on transition-based dependency parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING 2012: Posters,</booktitle>
<pages>1391--1400</pages>
<location>Mumbai, India,</location>
<contexts>
<context position="17667" citStr="Zhang and Nivre, 2012" startWordPosition="2912" endWordPosition="2915">ctuations and the words outside, while the baseline parser sometimes creates more dependency arcs that cross the boundary. The second is more interesting. Our method is able to capture that the root, wh, of the sub-tree within the paired-punctuation, such as “Mechanisms” in Figure 1, generally serves as a modifier of the words outside, while the baseline parser occasionally make wh as the head of the sentence. As we increase the beam width, the improvement of our method over the baseline becomes smaller. This is as expected, since beam search also has the effect of reducing error propagation (Zhang and Nivre, 2012), thereby alleviating the errors caused by punctuations. In the last experiment, we examine the effect of incorporating all punctuations using the method introduced in Section 2. In this experiment, we use all the feature templates in Table 3 and those in the baseline parser. Results are listed in the fourth column of Table 4, which shows that parsing accuracies can be further improved by also processing non-paired punctuations. The overall accuracy improvement when the beam width is 1 reaches 0.91%. The extra improvements mainly come from better accuracies on the sentences with comma. However</context>
</contexts>
<marker>Zhang, Nivre, 2012</marker>
<rawString>Yue Zhang and Joakim Nivre. 2012. Analyzing the effect of global learning and beam-search on transition-based dependency parsing. In Proceedings of COLING 2012: Posters, pages 1391–1400, Mumbai, India, December. The COLING 2012 Organizing Committee.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>