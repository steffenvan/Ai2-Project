<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000694">
<title confidence="0.993288">
Word Segmentation for Urdu OCR System
</title>
<author confidence="0.962134">
Misbah Akram
</author>
<affiliation confidence="0.7907">
National University of Computer
and Emerging Sciences
</affiliation>
<email confidence="0.997543">
misbahakram@gmail.com
</email>
<sectionHeader confidence="0.99386" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998795">
This paper presents a technique for word
segmentation for the Urdu OCR system.
Word segmentation or word tokeniza-
tion is a preliminary task for Urdu lan-
guage processing. Several techniques
are available for word segmentation in
other languages. A methodology is pro-
posed for word segmentation in this pa-
per which determines the boundaries of
words given a sequence of ligatures,
based on collocation of ligatures and
words in the corpus. Using this tech-
nique, word identification rate of
96.10% is achieved, using trigram prob-
abilities normalized over the number of
ligatures and words in the sequence.
</bodyText>
<sectionHeader confidence="0.998985" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.978718055555555">
Urdu uses Nastalique style of Arabic script
for writing, which is cursive in nature. Charac-
ters join together to form ligatures, which end
either with a space or with a non-joining charac-
ter. A word may be composed of one of more
ligatures. In Urdu, space is not used to separate
two consecutive words in a sentence; instead
readers themselves identify the boundaries of
words, as the sequence of ligatures, as they read
along the text. Space is used to get appropriate
character shapes and thus it may even be used
within a word to break the word into constituent
ligatures (Naseem 2007, Durrani 2008). There-
fore, like other languages (Theeramunkong &amp;
Usanavasin, 2001; Wan and Liu, 2007; Khanka-
sikam &amp; Muansuwan, 2005; Haruechaiyasak et
al., 2008; Haizhou &amp; Baosheng, 1998), word
segmentation or word tokenization is a prelimi-
</bodyText>
<author confidence="0.705407">
Sarmad Hussain
</author>
<affiliation confidence="0.9174705">
Center for Language Engineering,
Al-Khawarizmi Institute of Computer
Science, University of Engineering and
Technology, Lahore, Pakistan
</affiliation>
<email confidence="0.98258">
sarmad.hussain@kics.edu.pk
</email>
<bodyText confidence="0.999313428571429">
nary task for Urdu language processing. It has
applications in many areas like spell checking,
POS tagging, speech synthesis, information re-
trieval etc. This paper focuses on the word seg-
mentation problem from the point of view of
Optical Character Recognition (OCR) System.
As space is not visible in typed and scanned text,
spacing cues are not available to the OCR for
word separation and therefore segmentation has
to be done more explicitly. This word segmenta-
tion model for Urdu OCR system takes input in
the form of a sequence of ligatures recognized
by an OCR to construct a sequence of words
from them.
</bodyText>
<sectionHeader confidence="0.968047" genericHeader="method">
2 Literature Review
</sectionHeader>
<bodyText confidence="0.9883232">
Many languages, e.g., English, French,
Hindi, Nepali, Sinhala, Bengali, Greek, Russian,
etc. segment text into a sequence of words using
delimiters such as space, comma and semi colon
etc., but on the other hand many Asian languag-
es like Urdu, Persian, Arabic, Chinese,
Dzongkha, Lao and Thai have no explicit word
boundaries. In such languages, words are seg-
mented using more advanced techniques, which
can be categorized into three methods:
</bodyText>
<listItem confidence="0.9097934">
(i) Dictionary/lexicon based approaches
(ii) Linguistic knowledge based approaches
(iii) Machine learning based approach-
es/statistical approaches
(Haruechaiyasak et al., 2008)
</listItem>
<bodyText confidence="0.998199">
Longest matching (Poowarawan, 1986; Richard
Sproat, 1996) and maximum matching (Sproat
et al., 1996; Haizhou &amp; Baosheng, 1998) are
examples of lexicon based approaches. These
techniques segment text using the lexicon. Their
</bodyText>
<page confidence="0.997282">
88
</page>
<note confidence="0.5958265">
Proceedings of the 8th Workshop on Asian Language Resources, pages 88–94,
Beijing, China, 21-22 August 2010. c�2010 Asian Federation for Natural Language Processing
</note>
<bodyText confidence="0.999918888888889">
accuracy depends on the quality and size of the
dictionary.
N-Grams (Chang et al., 1992; Li Haizhou
et al., 1997; Richard Sproat, 1996; Dai &amp; Lee,
1994; Aroonmanakun, 2002) and Maximum
collocation (Aroonmanakun, 2002) are Linguis-
tic knowledge based approaches, which also
rely very much on the lexicon. These approach-
es select most likely segmentation from the set
of possible segmentations using a probabilistic
or cost-based scoring mechanism.
Word segmentation using decision trees
(Sornlertlamvanich et al., 2000; Theeramun-
kong &amp; Usanavasin, 2001) and similar other
techniques fall in the third category of word
segmentation techniques. These approaches use
a corpus in which word boundaries are explicit-
ly marked. These approaches do not require dic-
tionaries. In these approaches ambiguity prob-
lems are handled by providing a sufficiently
large set of training examples to enable accurate
classification.
A knowledge based approach has been
adopted for earlier work on Urdu word segmen-
tation (Durrani 2007; also see Durrani and Hus-
sain 2010). In this technique word segmentation
of Urdu text is achieved by employing know-
ledge based on the Urdu linguistics and script.
The initial segmentations are ranked using min-
word, unigram and bigram techniques. It reports
95.8 % overall accuracy for word segmentation
of Urdu text. Mukund et al. (2009) propose us-
ing character model along with linguistic rules
and report 83% precision. Lehal (2009) propos-
es a two stage process, which first uses Urdu
linguistic knowledge, and then uses statistical
information of Urdu and Hindi (also using
transliteration into Hindi) in the second stage
for words not addressed in the first stage, re-
porting an accuracy of 98.57%.
These techniques use characters or words in
the input, whereas an OCR outputs a series of
ligatures. The current paper presents work done
using statistical methods as an alternative,
which works with ligatures as input.
</bodyText>
<sectionHeader confidence="0.998624" genericHeader="method">
3 Methodology
</sectionHeader>
<bodyText confidence="0.999298461538462">
Current work uses the co-occurrence in-
formation of ligatures and words to construct a
statistical model, based on manually cleaned
and segmented training corpora. Ligature and
word statistics are derived from these corpora.
In the decoding phase, first all sequences of
words are generated from input set of ligatures
and ranking of these sequences is done based on
lexical lookup. Top k sequences are selected for
further processing, based on the number of valid
words. Finally, the probability of each of the k
sequences is calculated for the final decision.
Details are described in the subsequent sections.
</bodyText>
<subsectionHeader confidence="0.999965">
3.1 Data collection and preparation
</subsectionHeader>
<bodyText confidence="0.999936157894737">
An existing lexicon of 49630 unique words
is used (derived from Ijaz et al. 2007). The cor-
pus used for building ligature grams consists of
half a million words. Of these, 300,000 words
are taken from the Sports, Consumer Informa-
tion and Culture/Entertainment domains of the
18 million word corpus (Ijaz et al. 2007),
100,000 words are obtained from Urdu-Nepali-
English Parallel Corpus (available at
www.PANL10n.net), and another 100,000
words are taken from a previously POS tagged
corpus (Sajjad, 2007; tags of this corpus are re-
moved before further processing). This corpus
is manually cleaned for word segmentation er-
rors, by adding missing spaces between words
and replacing spaces with Zero Width Non-
Joiner (ZWNJ) within words. For the computa-
tion of word grams, the 18 million word corpus
of Urdu is used (Ijaz et al. 2007).
</bodyText>
<subsectionHeader confidence="0.999523">
3.2 Count and probability calculations
</subsectionHeader>
<bodyText confidence="0.999709">
Table 1 and Table 2 below give the counts
for unigram, bigrams and trigram of the liga-
tures and the words derived from the corpora
respectively.
</bodyText>
<table confidence="0.994728">
Ligature Ligature Ligature Ligature
Tokens Unigram Bigrams Trigrams
1508078 10215 35202 65962
</table>
<tableCaption confidence="0.8760925">
Table 1. Unigram, bigram and trigram counts of
the ligature corpus
</tableCaption>
<table confidence="0.999732333333333">
Word Word Word Word
Tokens Unigrams Bigrams Trigrams
17352476 157379 1120524 8143982
</table>
<tableCaption confidence="0.8020175">
Table 2. Unigram, bigram and trigram counts of
the word corpus
</tableCaption>
<bodyText confidence="0.9997895">
After deriving word unigrams, bigrams,
and trigrams, the following cleaning of corpus is
</bodyText>
<page confidence="0.99884">
89
</page>
<bodyText confidence="0.999891020408163">
performed. In the 18 million word corpus, cer-
tain words are combined due to missing space,
but are separate words. Some of these words
occur with very high frequency in the corpus.
For example “I .P” (ho ga, “will be”) exists as
single word rather than two words due to miss-
ing space. To solve this space insertion problem,
a list of about 700 words with frequency greater
than 50 is obtained from the word unigrams.
Each word of the list is manually reviewed and
space is inserted, where required. Then these
error words are removed from the word unigram
and added to the word unigram frequency list as
two or three individual words incrementing re-
spective counts.
For the space insertion problem in word
bigrams, each error word in joined-word list
(700-word list) is checked. Where these error
words occurs in a bigram word frequency list,
for example “���� GLM” (kiya ho ga “will have
done”) exists in the bigram list and contains
&amp;quot;Tt9 &amp;quot; error word, then this bigram entry “I .P GLM”
is removed from the bigram list and counts of
“Lt _-�” and “s&apos; GLM” are increased by the count of
“��j� L�d”. If these words do not exist in the word
bigram list then they are added as a new bi-
grams with the count of “��j� L�6”. Same proce-
dure is performed for the word trigrams.
The second main issue is with word-affixes,
which are sometimes separated by spaces from
the words. Therefore, in calculations, these are
treated as separate words and exist as bigram
entries in the list rather than a unigram entry.
For example &amp;quot; (sehat+wand, “healthy”)
exists as a bigram entry but in Urdu it is a single
word. To cope with this problem, a list of
word-affixes is used. If any entry of word bi-
gram matches with an affix, then this word is
combined by removing spurious space from it
(and inserting ZWNJ, if required to maintain its
glyph shape). Then this word is inserted in the
unigram list with its original bigram count and
unigram list updated accordingly. Same proce-
dure is performed if a trigram word matches
with an affix.
After cleaning, unigram, bigram and tri-
gram counts for both words and ligatures are
calculated. To avoid data sparseness One Count
Smoothing (Chen &amp; Goodman, 1996) is applied.
</bodyText>
<subsectionHeader confidence="0.999657">
3.3 Word sequences generation from input
</subsectionHeader>
<bodyText confidence="0.999228888888889">
The input, in the form of sequence of liga-
tures is used to generate all possible words.
These sequences are then ranked based on real
words. For this purpose, a tree of these se-
quences is incrementally built. The first ligature
is added as a root of tree, and at each level two
to three additional nodes are added. For exam-
ple the second level of the tree contains the fol-
lowing tree nodes.
</bodyText>
<listItem confidence="0.993182">
• Current ligature forms a separate word, se-
parated with space, from the sequence at its
parent, l1 l2
• Current ligature concatenates, without a
space, with the sequence at its parent, l1l2
• Current ligature concatenates, without a
</listItem>
<bodyText confidence="0.983755941176471">
space, with the sequence at its parent but
with an additional, l1ZWNJl2
For each node, at each level of the tree, a nu-
meric value is assigned, which is the sum of
squares of the number of ligatures in each word
which is in the dictionary. If a word does not
exist in dictionary then it does not contribute to
the total sum. If a node-string has only one word
and this word does not occur in the dictionary as
a valid word then it is checked that this word
may occur at the start of any dictionary entry. In
this case numeric value is also assigned.
After assignment, nodes are ranked ac-
cording to these values and best k (beam value)
nodes are selected. These selected nodes are
further ranked using statistical methods dis-
cussed below.
</bodyText>
<subsectionHeader confidence="0.995359">
3.4 Best word segmentation selection
</subsectionHeader>
<bodyText confidence="0.99982725">
For selection of the most probable word
segmentation sequence word and ligature mod-
els are used. For word probabilities the follow-
ing is used.
</bodyText>
<equation confidence="0.984747">
P(W) = argmaxwi ∈ SP(wi)
</equation>
<bodyText confidence="0.9768305">
To reduce the complexity of computing, Mar-
kov assumption are taken to give bigram and
trigram approximations (e.g., see Jurafsky &amp;
Martin 2006) as given below.
</bodyText>
<figure confidence="0.398946666666667">
�
P�W� = argmax�� � ∈ S ∏ P~
� w�|w����
�
P�W� = argmax�� � ∈ S�∏ P�w�|w���w����
���
</figure>
<bodyText confidence="0.9505905">
Similarly the ligature models are built by
taking the assumption that sentences are made
</bodyText>
<page confidence="0.990573">
90
</page>
<bodyText confidence="0.99992725">
up of sequences of ligatures rather than words
and space is also a valid ligature. By taking the
Markov bigram and trigram assumption for liga-
ture grams we get the following.
</bodyText>
<equation confidence="0.93527325">
P(L) = argmax�,,i ∈ S(∏~~P �l�|l�����
�
P(L) = argmax�� � ∈ S�∏� ~P �l�|l���l�����
�
</equation>
<bodyText confidence="0.999947">
Given the ligatures, e.g. as input from and
OCR, we can formulate the decoding problem
as the following equation.
</bodyText>
<equation confidence="0.988641">
P(W|L) = argmaxwl� ∈ SP(wi |l�M)
</equation>
<bodyText confidence="0.9988">
where wi� = wl,wz,w3,w4,...wn and
lm = l1,l2,l3,l4,...lM ; n represents number of
words and m represents the number of liga-
tures. This equation also represents that m
number of ligatures can be assigned to n
number of words. By applying the Bayesian
theorem we get the following derivation.
</bodyText>
<equation confidence="0.9877885">
P&lt;= &gt;?�� ~@.P~~ ��
P�W|L� = argmax�� � ∈ S P~= &gt;�
</equation>
<bodyText confidence="0.999893">
As P (lm) is same for all wi , so the denomi-
nator does not change the equation, simplify-
ing to the following expression.
</bodyText>
<equation confidence="0.754758">
P(W|L) = argmaxwl ∈ SP(lm|wi).P(wl)
</equation>
<bodyText confidence="0.742639">
where
</bodyText>
<equation confidence="0.9781725">
P�l� ~|w~ �� =P �l�,l�,l3, ... l�|w� ��
=P�l�|w� �� ∗ P �l�|w� ~l~~ ∗ P�l3|w� �l�l�� ∗
P�l4|w� �l�l�l3� ∗ ... P�l�|w� �l�l�l3
...lm-1)
</equation>
<bodyText confidence="0.9999675">
Assuming that a ligature li depends only on
the word sequence w1 and its previous liga-
ture l1_1, and not the ligature history, the
above equation can be simplifed as follows.
</bodyText>
<equation confidence="0.994016">
P(l1&apos;|wi) = P (l1|wi) ∗ P (l2|w1l�� ∗ P03|w1l2)
∗ P04|w1l3) ∗ ... P0m|w1nlm-1)
= ∏ P �l�|w�
� � ~l~~~~
</equation>
<bodyText confidence="0.9981676">
Further, if it is assumed that l1 depends on
the word in which it appears, not whole word
sequence, the equation can be further simpli-
fied to the following (as probability of l1 with-
in a word is 1).
</bodyText>
<equation confidence="0.868556666666667">
P(l�m|wi) = ∏m P (li|li-1)
~
Thus, considering bigrams, P(W|L) =
� �
argmaxwl ∈ S EF(P (li|li-i)G (F P(wk|wk-1))
1 k=1
</equation>
<bodyText confidence="0.999874333333333">
This gives the maximum probable word se-
quence among all the alternative word se-
quences. The precision of the equation can
be taken at bigram or trigram level for both
ligature and word, giving the following pos-
sibilities. Additionally, normalization is also
done to better compare different sequences,
as each sequences has different number of
words and ligatures per word.
</bodyText>
<listItem confidence="0.877732">
• Ligature trigram and word bigram based
technique
</listItem>
<figure confidence="0.985923363636364">
�
P�W� = argmax�� � ∈ S�∏ ~P �l�|l���l����� ∗
�
(∏k=1 P(wk|wk-1))
• Ligature bigram and word trigram based
technique
P�W� = argmax�� � ∈ S�∏ � ~P �l�|l���� � ∗
�
�∏ P�w�|w���w�����
�
���
• Ligature trigram and word trigram based
technique
�
P�W� = argmax�� � ∈ S�∏ ~P �l�|l���l���� � ∗
�
�∏ P�w�|w���w�����
�
���
• Normalized ligature bigram and word bi-
gram based technique
�
P�W� = argmax�� � ∈ S�∏ ~P �l�|l���� �� NL
L ∗
�
(∏k=1 P(wk|wk-1) 1LNW
• Normalized ligature trigram and word bi-
gram based technique
�
P�W� = argmax�� � ∈ S N�∏ ~P �l�|l���l���� �� NL
L O ∗
�
�
�∏ P�w�|w�����
� L NW
���
• Normalized ligature bigram and word tri-
gram based technique
P(W) = argmax�� � ∈ S�∏ � ~P �l�|l���� �� NL
L ∗
�
�
�∏ P�w�|w���w����
� L NW
���
• Normalized ligature trigram and word tri-
gram based technique
�
P�W� = argmax�� � ∈ S�∏ ~P �l�|l���l���� �� NL
L ∗
�
�
�∏ P�w�|w���w����
� L NW
���
</figure>
<bodyText confidence="0.9998464">
In the current work, all the above tech-
niques are used and the best sequence from each
one is shortlisted. Then the word sequence
which occurs the most times in this shortlist is
finally selected.
</bodyText>
<page confidence="0.997304">
91
</page>
<bodyText confidence="0.9983765">
NL represents the number of ligature bi-
grams or trigrams and NW represents the num-
ber of word bigram or trigrams that exist in the
given sentence.
</bodyText>
<sectionHeader confidence="0.998018" genericHeader="evaluation">
4 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999931487804878">
The model is tested on a corpus of 150 sen-
tences composed of 2156 words and 6075 liga-
tures. In these sentences, 62 words are unknown,
i.e. the words that do not exist in our dictionary.
The average length of the sentence is 14 words
and 40.5 ligatures. The average length of word
is 2.81 ligatures. All the techniques are tested
with a beam value, k, of 10, 20, 30, 40, and 50.
The results can be viewed from two pers-
pectives: sentence identification rate, and word
identification rate. A sentence is considered
incorrect even if one word of the sentence is
identified wrongly. The technique gives the
sentence identification rate of 76% at the beam
value of 30. At word level, Normalized Liga-
ture Trigram Word Trigram Technique outper-
forms other techniques and gives a 96.10%
word identification rate at the beam value of 50.
The normalized data gives much better pre-
diction compared to the un-normalized data.
Sentence identification errors depend heavi-
ly on the unknown words. For example, at the
beam value of 30 we predict 38 incorrect sen-
tences, of which 25 sentence level errors are due
to unknown-words and 13 errors are due to
known word identification errors. Thus improv-
ing system vocabulary will have significant im-
pact on accuracy.
Many of the word errors are caused due to
insufficient cleaning of word the larger corpus.
Though the words with frequency greater than
50 from the 18 million word corpus have been
cleaned, the lower frequency words cause these
errors. For example word list still contains
&amp;quot;yد���?&amp;quot;(bunyad per, “depends on”), &amp;quot;�� ����� &amp;quot; (se
taqseem, “divided by”) with frequency of 40
and 5 respectively, and each should be two
words with a space between them. If low fre-
quency words are also cleaned results will fur-
ther improve, though it would take a lot of ma-
nual effort.
</bodyText>
<table confidence="0.999685625">
Bea� Total Sentences %age ° Total known ° Total unknown °
Value identified Total Words /nage words identified /nage words identified /nage
Identified
10 110/150 73.33% 2060/2156 95.55% 2024/2092 96.75% 36/64 56.25%
20 112/150 74.67% 2066/2156 95.83% 2027/2092 96.89% 39/64 60.94%
30 114/150 76% 2062/2156 95.64% 2019/2083 96.93% 43/73 58.90%
40 105/150 70% 2037/2156 94.48% 2000/2092 95.60% 37/64 57.81%
50 106/150 70.67% 2040/2156 94.62% 2000/2092 95.60% 40/64 62.50%
</table>
<tableCaption confidence="0.995744">
Table 3. Results changing beam width k of the tree
</tableCaption>
<table confidence="0.999791363636364">
Technique Total sentences %age Total words %age Total known %age Total unknown %age
identified identified words Identified words identified
Ligature Bigram 50/150 33.33% 1835/2156 85.11% 1806/2092 86.33% 29/64 45.31%
Ligature Bigram Word 68/150 45.33% 1900/2156 88.13% 1865/2092 89.15% 35/64 54.69%
Bigram
Ligature Bigram Word 83/150 55.33% 1960/2156 90.91% 1924/2092 91.97% 36/64 56.25%
Trigram
Ligature Trigram 16/150 10.67% 1637/2156 75.93% 1610/2092 76.96% 27/64 42.19%
Ligature Trigram Word 42/150 28% 1776/2156 82.38% 1746/2092 83.46% 30/64 46.88%
Bigram
Ligature Trigram Word 62/150 41.33% 1868/2156 86.64% 1835/2092 87.72% 33/64 51.56%
Trigram
Normalized Ligature 90/150 60% 2067/2156 95.87% 2024/2092 96.75% 43/64 67.19%
Bigram Word Bigram
Normalized Ligature 100/150 66.67% 2070/2156 96.01% 2028/2092 96.94% 42/64 65.63%
Bigram Word Trigram
Normalized Ligature 93/150 62% 2071/2156 96.06% 2030/2092 97.04% 41/64 64.06%
Trigram Word Bigram
Normalized Ligature 101/150 67.33% 2072/2156 96.10% 2030/2092 97.04% 42/64 65.63%
Trigram Word Trigram
Word Bigram 47/150 31.33% 1827/2156 84.74% 1796/2092 85.85% 31/64 48.44%
Word Trigram 74/150 49.33% 1937/2156 89.84% 1903/2092 90.97% 34/64 53.13%
</table>
<page confidence="0.861666">
92
</page>
<tableCaption confidence="0.986179">
Table 4. Results for all techniques for the beam value of 50
</tableCaption>
<bodyText confidence="0.999729117647059">
Errors are also caused if an alternate liga-
ture sequence exists. For example the proper
noun &amp;quot;J�5ر4&amp;quot; (kartak) is not identifiec as it does
not exist in dictionary, but the alternate two
word sequence &amp;quot;J55 رu&amp;quot; (kar tak, “till the car”)
is valid.
This work uses the knowledge of ligature
grams and word grams. It can be further en-
hanced by using the character grams. We have
tried to clean the corpus. Further cleaning and
additional corpus will improve the results as
well. Improvement can also be achieved by
handling abbreviations and English words trans-
literated in the text. The unknown word detec-
tion rate can be increased by applying POS tag-
ging to further help rank the multiple possible
sentences.
</bodyText>
<sectionHeader confidence="0.999717" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999878666666667">
This work presents an initial effort on sta-
tistical solution of word segmentation, especial-
ly for Urdu OCR systems. This work develops a
cleaned corpus of half a million Urdu words for
statistical training of ligature based data, which
is now available for the research community. In
addition, the work develops a statistical model
for word segmentation using ligature and word
statistics. Using ligature statistics improves
upon using just the word statistics. Further
normalization has significant impact on accura-
cy.
</bodyText>
<sectionHeader confidence="0.998476" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999254098360656">
Aroonmanakun, W. (2002). Collocation and Thai
Word Segmentation. In Proceedings of the Fifth
Symposium on Natural Language Processing &amp;
The Fifth Oriental COCOSDA Workshop, (pp.
68-75). Pathumthani.
Chang, Jyun-Shen, Chen, S.-D., Zhen, Y., Liu, X.-Z.,
&amp; Ke, S.-J. (1992). Large-corpus-based methods
for Chinese personal name recognition. Journal of
Chinese Information Processing , 6 (3), 7-15.
Chen, F., &amp; Goodman, T. (1996). An Empirical
Study of Smoothing Techniques for Language
Modeling. In the Proceedings of the 34th Annual
Meeting of the Association for Computational
Linguistics, (pp. 310-318).
Church, K. W., &amp; Gale, W. A. (1991). A comparison
of the enhanced Good-Turing and deleted estima-
tion methods for estimating probabilities of Eng-
lish bigrams. Computer Speech and Language , 5,
19-54.
Dai, J.-C., &amp; Lee, H.-J. (1994). Paring with Tag In-
formation in a probabilistic generalized LR pars-
er. International Conference on Chinese Compu-
ting. Singapore.
Durrani, N. (2007). Typology of word and automatic
word Segmentation in Urdu text corpus. Thesis,
National University of Computer &amp; Emerging
Sciences, Lahore , Pakistan.
Durrani, N., Hussain, S. (2010). Urdu Word Segmen-
tation, In the 11th Annual Conference of the
North American Chapter of the Association for
Computational Linguistics (NAACL HLT 2010),
Los Angeles, US.
Haizhou, L., &amp; Baosheng, Y. (1998). Chinese Word
Segmentation. In Proceedings of the 12th Pacific
Asia Conference on Language, Information and
Computation, (pp. 212-217).
Haruechaiyasak, C., Kongyoung, S., &amp; Dailey, M. N.
(2008). A Comparative Study on Thai Word Seg-
mentation Approaches.
Hussain, S. (2008). Resources for Urdu Language
Processing. In Proceedings of the Sixth Workshop
on Asian Language Resources.
Ijaz, M., Hussain, S. (2007). Corpus Based Urdu
Lexicon Development, in the Proceedings of Con-
ference on Language Technology (CLT07), Uni-
versity of Peshawar, Pakistan.
Jurafsky, D., &amp; Martin., l. J(2000). Speech and Lan-
guage Processing: An Introduction to Natural
Language Processing (1st ed.). Computational
Linguistics and Speech Recognition Prentice Hall.
Khankasikam, K., &amp; Muansuwan, N. (n.d.). Thai
Word Segmentation a Lexical Semantic Ap-
proach.
Khankasikam, K., &amp; Muansuwan, N. (2005). Thai
Word Segmentation a Lexical Semantic Approach.
In the Proceedings of the Tenth Machine Transla-
tion Summit, (pp. 331-338). Thailand.
Lehal, G. (2009). A Two Stage Word Segmentation
System for Handling Space Insertion Problem in
Urdu Script. World Academy of Science, Engi-
neering and Technology 60.
</reference>
<page confidence="0.984452">
93
</page>
<reference confidence="0.996348480769231">
Word Binding Force. In Proceedings of the 16th
conference on Computational linguistics.
MacKay, D. J., &amp; Peto, L. C. (1995). A Hierarchical
Dirichlet Language Mode. Natural Language En-
gineering , 1 (3), 1-19.
Mukund, S. &amp; Srihari, R. (2009). NE Tagging for
Urdu based on Bootstrap POS Learning. In the
Proceedings of CLIAWS3, Third International
Cross Lingual Information Access Workshop,
pages 61–69, Boulder, Colorado, USA.
Naseem, T., &amp; Hussain, S. (2007). Spelling Error
Trends in Urdu, In the Proceedings of Conference
on Language Technology (CLT07), University of
Peshawar, Pakistan.
Pascale, F., &amp; Dekai, W. (1994). Statistical augmen-
tation of a Chinese machine readable dictionary.
Proceedings of the Second Annual Workshop on
Very Large Corpora, (pp. 69-85).
Poowarawan, Y. (1986). Dictionary-based Thai Syl-
lable Separation. Proceedings of the Ninth Elec-
tronics Engineering Conference.
Richard Sproat, C. S. (1996). A Stochastic Finite-
State Word-Segmentation Algorithm for Chinese.
Computational Linguistics , 22 (3).
Sajjad, H. (2007). Statistical Part-of-Speech for Ur-
du. MS thesis, National University of Computer
and Emerging Sciencies, Centre for Research in
Urdu Language Processing, Lahore , Pakistan.
Sornlertlamvanich, V., Potipiti, T., &amp; charoenporn,
T. (2000). Automatic Corpus-Based Thai Word
Algorithm Extraction with the C4.5 Learning.
Proceedings of the 18th conference on Computa-
tional linguistics.
Sproat, R., Shih, C., Gale, W., &amp; Chang, N. (1996).
A Stochastic Finite-State Word-Segmentation Al-
gorithm for Chinese. Computational Linguistics ,
22 (3).
Theeramunkong, T., &amp; Usanavasin, S. (2001). Non-
Dictionary-Based Thai Word Segmentation Using
Decision Trees. Proceedings of the first interna-
tional conference on Human language technology
research.
Urdu-Nepali-English Parallel Corpus. (n.d.). Re-
trieved from Center for Research in Urdu Lan-
guage Processing: http://www.crulp.org/software
/ling_resources/urdunepalienglishparallelcorpus.
htm
Wang, X.-J., Liu, W., &amp; Qin, Y. (2007). A Search-
based Chinese Word Segmentation Method. 16th
International World Wide Web Conference.
Wong, P.-k., &amp; Chan, C. (1996). Chinese Word
Segmentation based on Maximum Matching and
</reference>
<page confidence="0.99954">
94
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.953200">
<title confidence="0.999593">Word Segmentation for Urdu OCR System</title>
<author confidence="0.988705">Misbah Akram</author>
<affiliation confidence="0.984441">National University of and Emerging Sciences</affiliation>
<email confidence="0.998662">misbahakram@gmail.com</email>
<abstract confidence="0.999626882352941">This paper presents a technique for word segmentation for the Urdu OCR system. Word segmentation or word tokenization is a preliminary task for Urdu language processing. Several techniques are available for word segmentation in other languages. A methodology is proposed for word segmentation in this paper which determines the boundaries of words given a sequence of ligatures, based on collocation of ligatures and words in the corpus. Using this technique, word identification rate of 96.10% is achieved, using trigram probabilities normalized over the number of ligatures and words in the sequence.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>W Aroonmanakun</author>
</authors>
<title>Collocation and Thai Word Segmentation.</title>
<date>2002</date>
<booktitle>In Proceedings of the Fifth Symposium on Natural Language Processing &amp; The Fifth Oriental COCOSDA Workshop,</booktitle>
<pages>68--75</pages>
<contexts>
<context position="3576" citStr="Aroonmanakun, 2002" startWordPosition="554" endWordPosition="555">s/statistical approaches (Haruechaiyasak et al., 2008) Longest matching (Poowarawan, 1986; Richard Sproat, 1996) and maximum matching (Sproat et al., 1996; Haizhou &amp; Baosheng, 1998) are examples of lexicon based approaches. These techniques segment text using the lexicon. Their 88 Proceedings of the 8th Workshop on Asian Language Resources, pages 88–94, Beijing, China, 21-22 August 2010. c�2010 Asian Federation for Natural Language Processing accuracy depends on the quality and size of the dictionary. N-Grams (Chang et al., 1992; Li Haizhou et al., 1997; Richard Sproat, 1996; Dai &amp; Lee, 1994; Aroonmanakun, 2002) and Maximum collocation (Aroonmanakun, 2002) are Linguistic knowledge based approaches, which also rely very much on the lexicon. These approaches select most likely segmentation from the set of possible segmentations using a probabilistic or cost-based scoring mechanism. Word segmentation using decision trees (Sornlertlamvanich et al., 2000; Theeramunkong &amp; Usanavasin, 2001) and similar other techniques fall in the third category of word segmentation techniques. These approaches use a corpus in which word boundaries are explicitly marked. These approaches do not require dictionaries. In thes</context>
</contexts>
<marker>Aroonmanakun, 2002</marker>
<rawString>Aroonmanakun, W. (2002). Collocation and Thai Word Segmentation. In Proceedings of the Fifth Symposium on Natural Language Processing &amp; The Fifth Oriental COCOSDA Workshop, (pp. 68-75). Pathumthani.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jyun-Shen Chang</author>
<author>S-D Chen</author>
<author>Y Zhen</author>
<author>X-Z Liu</author>
<author>S-J Ke</author>
</authors>
<title>Large-corpus-based methods for Chinese personal name recognition.</title>
<date>1992</date>
<journal>Journal of Chinese Information Processing ,</journal>
<volume>6</volume>
<issue>3</issue>
<pages>7--15</pages>
<contexts>
<context position="3491" citStr="Chang et al., 1992" startWordPosition="538" endWordPosition="541">es (ii) Linguistic knowledge based approaches (iii) Machine learning based approaches/statistical approaches (Haruechaiyasak et al., 2008) Longest matching (Poowarawan, 1986; Richard Sproat, 1996) and maximum matching (Sproat et al., 1996; Haizhou &amp; Baosheng, 1998) are examples of lexicon based approaches. These techniques segment text using the lexicon. Their 88 Proceedings of the 8th Workshop on Asian Language Resources, pages 88–94, Beijing, China, 21-22 August 2010. c�2010 Asian Federation for Natural Language Processing accuracy depends on the quality and size of the dictionary. N-Grams (Chang et al., 1992; Li Haizhou et al., 1997; Richard Sproat, 1996; Dai &amp; Lee, 1994; Aroonmanakun, 2002) and Maximum collocation (Aroonmanakun, 2002) are Linguistic knowledge based approaches, which also rely very much on the lexicon. These approaches select most likely segmentation from the set of possible segmentations using a probabilistic or cost-based scoring mechanism. Word segmentation using decision trees (Sornlertlamvanich et al., 2000; Theeramunkong &amp; Usanavasin, 2001) and similar other techniques fall in the third category of word segmentation techniques. These approaches use a corpus in which word bo</context>
</contexts>
<marker>Chang, Chen, Zhen, Liu, Ke, 1992</marker>
<rawString>Chang, Jyun-Shen, Chen, S.-D., Zhen, Y., Liu, X.-Z., &amp; Ke, S.-J. (1992). Large-corpus-based methods for Chinese personal name recognition. Journal of Chinese Information Processing , 6 (3), 7-15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Chen</author>
<author>T Goodman</author>
</authors>
<title>An Empirical Study of Smoothing Techniques for Language Modeling.</title>
<date>1996</date>
<booktitle>In the Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>310--318</pages>
<contexts>
<context position="9593" citStr="Chen &amp; Goodman, 1996" startWordPosition="1547" endWordPosition="1550">ntry but in Urdu it is a single word. To cope with this problem, a list of word-affixes is used. If any entry of word bigram matches with an affix, then this word is combined by removing spurious space from it (and inserting ZWNJ, if required to maintain its glyph shape). Then this word is inserted in the unigram list with its original bigram count and unigram list updated accordingly. Same procedure is performed if a trigram word matches with an affix. After cleaning, unigram, bigram and trigram counts for both words and ligatures are calculated. To avoid data sparseness One Count Smoothing (Chen &amp; Goodman, 1996) is applied. 3.3 Word sequences generation from input The input, in the form of sequence of ligatures is used to generate all possible words. These sequences are then ranked based on real words. For this purpose, a tree of these sequences is incrementally built. The first ligature is added as a root of tree, and at each level two to three additional nodes are added. For example the second level of the tree contains the following tree nodes. • Current ligature forms a separate word, separated with space, from the sequence at its parent, l1 l2 • Current ligature concatenates, without a space, wi</context>
</contexts>
<marker>Chen, Goodman, 1996</marker>
<rawString>Chen, F., &amp; Goodman, T. (1996). An Empirical Study of Smoothing Techniques for Language Modeling. In the Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics, (pp. 310-318).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K W Church</author>
<author>W A Gale</author>
</authors>
<title>A comparison of the enhanced Good-Turing and deleted estimation methods for estimating probabilities of English bigrams.</title>
<date>1991</date>
<journal>Computer Speech and Language ,</journal>
<volume>5</volume>
<marker>Church, Gale, 1991</marker>
<rawString>Church, K. W., &amp; Gale, W. A. (1991). A comparison of the enhanced Good-Turing and deleted estimation methods for estimating probabilities of English bigrams. Computer Speech and Language , 5, 19-54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-C Dai</author>
<author>H-J Lee</author>
</authors>
<title>Paring with Tag Information in a probabilistic generalized LR</title>
<date>1994</date>
<booktitle>parser. International Conference on Chinese Computing. Singapore.</booktitle>
<contexts>
<context position="3555" citStr="Dai &amp; Lee, 1994" startWordPosition="550" endWordPosition="553">g based approaches/statistical approaches (Haruechaiyasak et al., 2008) Longest matching (Poowarawan, 1986; Richard Sproat, 1996) and maximum matching (Sproat et al., 1996; Haizhou &amp; Baosheng, 1998) are examples of lexicon based approaches. These techniques segment text using the lexicon. Their 88 Proceedings of the 8th Workshop on Asian Language Resources, pages 88–94, Beijing, China, 21-22 August 2010. c�2010 Asian Federation for Natural Language Processing accuracy depends on the quality and size of the dictionary. N-Grams (Chang et al., 1992; Li Haizhou et al., 1997; Richard Sproat, 1996; Dai &amp; Lee, 1994; Aroonmanakun, 2002) and Maximum collocation (Aroonmanakun, 2002) are Linguistic knowledge based approaches, which also rely very much on the lexicon. These approaches select most likely segmentation from the set of possible segmentations using a probabilistic or cost-based scoring mechanism. Word segmentation using decision trees (Sornlertlamvanich et al., 2000; Theeramunkong &amp; Usanavasin, 2001) and similar other techniques fall in the third category of word segmentation techniques. These approaches use a corpus in which word boundaries are explicitly marked. These approaches do not require </context>
</contexts>
<marker>Dai, Lee, 1994</marker>
<rawString>Dai, J.-C., &amp; Lee, H.-J. (1994). Paring with Tag Information in a probabilistic generalized LR parser. International Conference on Chinese Computing. Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Durrani</author>
</authors>
<title>Typology of word and automatic word Segmentation in Urdu text corpus.</title>
<date>2007</date>
<tech>Thesis,</tech>
<institution>National University of Computer &amp; Emerging Sciences, Lahore , Pakistan.</institution>
<contexts>
<context position="4414" citStr="Durrani 2007" startWordPosition="678" endWordPosition="679"> probabilistic or cost-based scoring mechanism. Word segmentation using decision trees (Sornlertlamvanich et al., 2000; Theeramunkong &amp; Usanavasin, 2001) and similar other techniques fall in the third category of word segmentation techniques. These approaches use a corpus in which word boundaries are explicitly marked. These approaches do not require dictionaries. In these approaches ambiguity problems are handled by providing a sufficiently large set of training examples to enable accurate classification. A knowledge based approach has been adopted for earlier work on Urdu word segmentation (Durrani 2007; also see Durrani and Hussain 2010). In this technique word segmentation of Urdu text is achieved by employing knowledge based on the Urdu linguistics and script. The initial segmentations are ranked using minword, unigram and bigram techniques. It reports 95.8 % overall accuracy for word segmentation of Urdu text. Mukund et al. (2009) propose using character model along with linguistic rules and report 83% precision. Lehal (2009) proposes a two stage process, which first uses Urdu linguistic knowledge, and then uses statistical information of Urdu and Hindi (also using transliteration into H</context>
</contexts>
<marker>Durrani, 2007</marker>
<rawString>Durrani, N. (2007). Typology of word and automatic word Segmentation in Urdu text corpus. Thesis, National University of Computer &amp; Emerging Sciences, Lahore , Pakistan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Durrani</author>
<author>S Hussain</author>
</authors>
<title>Urdu Word Segmentation,</title>
<date>2010</date>
<booktitle>In the 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL HLT 2010),</booktitle>
<location>Los Angeles, US.</location>
<contexts>
<context position="4450" citStr="Durrani and Hussain 2010" startWordPosition="682" endWordPosition="686">based scoring mechanism. Word segmentation using decision trees (Sornlertlamvanich et al., 2000; Theeramunkong &amp; Usanavasin, 2001) and similar other techniques fall in the third category of word segmentation techniques. These approaches use a corpus in which word boundaries are explicitly marked. These approaches do not require dictionaries. In these approaches ambiguity problems are handled by providing a sufficiently large set of training examples to enable accurate classification. A knowledge based approach has been adopted for earlier work on Urdu word segmentation (Durrani 2007; also see Durrani and Hussain 2010). In this technique word segmentation of Urdu text is achieved by employing knowledge based on the Urdu linguistics and script. The initial segmentations are ranked using minword, unigram and bigram techniques. It reports 95.8 % overall accuracy for word segmentation of Urdu text. Mukund et al. (2009) propose using character model along with linguistic rules and report 83% precision. Lehal (2009) proposes a two stage process, which first uses Urdu linguistic knowledge, and then uses statistical information of Urdu and Hindi (also using transliteration into Hindi) in the second stage for words </context>
</contexts>
<marker>Durrani, Hussain, 2010</marker>
<rawString>Durrani, N., Hussain, S. (2010). Urdu Word Segmentation, In the 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL HLT 2010), Los Angeles, US.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Haizhou</author>
<author>Y Baosheng</author>
</authors>
<title>Chinese Word Segmentation.</title>
<date>1998</date>
<booktitle>In Proceedings of the 12th Pacific Asia Conference on Language, Information and Computation,</booktitle>
<pages>212--217</pages>
<contexts>
<context position="1529" citStr="Haizhou &amp; Baosheng, 1998" startWordPosition="243" endWordPosition="246"> a space or with a non-joining character. A word may be composed of one of more ligatures. In Urdu, space is not used to separate two consecutive words in a sentence; instead readers themselves identify the boundaries of words, as the sequence of ligatures, as they read along the text. Space is used to get appropriate character shapes and thus it may even be used within a word to break the word into constituent ligatures (Naseem 2007, Durrani 2008). Therefore, like other languages (Theeramunkong &amp; Usanavasin, 2001; Wan and Liu, 2007; Khankasikam &amp; Muansuwan, 2005; Haruechaiyasak et al., 2008; Haizhou &amp; Baosheng, 1998), word segmentation or word tokenization is a prelimiSarmad Hussain Center for Language Engineering, Al-Khawarizmi Institute of Computer Science, University of Engineering and Technology, Lahore, Pakistan sarmad.hussain@kics.edu.pk nary task for Urdu language processing. It has applications in many areas like spell checking, POS tagging, speech synthesis, information retrieval etc. This paper focuses on the word segmentation problem from the point of view of Optical Character Recognition (OCR) System. As space is not visible in typed and scanned text, spacing cues are not available to the OCR </context>
<context position="3138" citStr="Haizhou &amp; Baosheng, 1998" startWordPosition="485" endWordPosition="488">e of words using delimiters such as space, comma and semi colon etc., but on the other hand many Asian languages like Urdu, Persian, Arabic, Chinese, Dzongkha, Lao and Thai have no explicit word boundaries. In such languages, words are segmented using more advanced techniques, which can be categorized into three methods: (i) Dictionary/lexicon based approaches (ii) Linguistic knowledge based approaches (iii) Machine learning based approaches/statistical approaches (Haruechaiyasak et al., 2008) Longest matching (Poowarawan, 1986; Richard Sproat, 1996) and maximum matching (Sproat et al., 1996; Haizhou &amp; Baosheng, 1998) are examples of lexicon based approaches. These techniques segment text using the lexicon. Their 88 Proceedings of the 8th Workshop on Asian Language Resources, pages 88–94, Beijing, China, 21-22 August 2010. c�2010 Asian Federation for Natural Language Processing accuracy depends on the quality and size of the dictionary. N-Grams (Chang et al., 1992; Li Haizhou et al., 1997; Richard Sproat, 1996; Dai &amp; Lee, 1994; Aroonmanakun, 2002) and Maximum collocation (Aroonmanakun, 2002) are Linguistic knowledge based approaches, which also rely very much on the lexicon. These approaches select most li</context>
</contexts>
<marker>Haizhou, Baosheng, 1998</marker>
<rawString>Haizhou, L., &amp; Baosheng, Y. (1998). Chinese Word Segmentation. In Proceedings of the 12th Pacific Asia Conference on Language, Information and Computation, (pp. 212-217).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Haruechaiyasak</author>
<author>S Kongyoung</author>
<author>M N Dailey</author>
</authors>
<title>A Comparative Study on Thai Word Segmentation Approaches.</title>
<date>2008</date>
<contexts>
<context position="1502" citStr="Haruechaiyasak et al., 2008" startWordPosition="239" endWordPosition="242">atures, which end either with a space or with a non-joining character. A word may be composed of one of more ligatures. In Urdu, space is not used to separate two consecutive words in a sentence; instead readers themselves identify the boundaries of words, as the sequence of ligatures, as they read along the text. Space is used to get appropriate character shapes and thus it may even be used within a word to break the word into constituent ligatures (Naseem 2007, Durrani 2008). Therefore, like other languages (Theeramunkong &amp; Usanavasin, 2001; Wan and Liu, 2007; Khankasikam &amp; Muansuwan, 2005; Haruechaiyasak et al., 2008; Haizhou &amp; Baosheng, 1998), word segmentation or word tokenization is a prelimiSarmad Hussain Center for Language Engineering, Al-Khawarizmi Institute of Computer Science, University of Engineering and Technology, Lahore, Pakistan sarmad.hussain@kics.edu.pk nary task for Urdu language processing. It has applications in many areas like spell checking, POS tagging, speech synthesis, information retrieval etc. This paper focuses on the word segmentation problem from the point of view of Optical Character Recognition (OCR) System. As space is not visible in typed and scanned text, spacing cues ar</context>
<context position="3011" citStr="Haruechaiyasak et al., 2008" startWordPosition="467" endWordPosition="470">re Review Many languages, e.g., English, French, Hindi, Nepali, Sinhala, Bengali, Greek, Russian, etc. segment text into a sequence of words using delimiters such as space, comma and semi colon etc., but on the other hand many Asian languages like Urdu, Persian, Arabic, Chinese, Dzongkha, Lao and Thai have no explicit word boundaries. In such languages, words are segmented using more advanced techniques, which can be categorized into three methods: (i) Dictionary/lexicon based approaches (ii) Linguistic knowledge based approaches (iii) Machine learning based approaches/statistical approaches (Haruechaiyasak et al., 2008) Longest matching (Poowarawan, 1986; Richard Sproat, 1996) and maximum matching (Sproat et al., 1996; Haizhou &amp; Baosheng, 1998) are examples of lexicon based approaches. These techniques segment text using the lexicon. Their 88 Proceedings of the 8th Workshop on Asian Language Resources, pages 88–94, Beijing, China, 21-22 August 2010. c�2010 Asian Federation for Natural Language Processing accuracy depends on the quality and size of the dictionary. N-Grams (Chang et al., 1992; Li Haizhou et al., 1997; Richard Sproat, 1996; Dai &amp; Lee, 1994; Aroonmanakun, 2002) and Maximum collocation (Aroonmana</context>
</contexts>
<marker>Haruechaiyasak, Kongyoung, Dailey, 2008</marker>
<rawString>Haruechaiyasak, C., Kongyoung, S., &amp; Dailey, M. N. (2008). A Comparative Study on Thai Word Segmentation Approaches.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Hussain</author>
</authors>
<title>Resources for Urdu Language Processing.</title>
<date>2008</date>
<booktitle>In Proceedings of the Sixth Workshop on Asian Language Resources.</booktitle>
<marker>Hussain, 2008</marker>
<rawString>Hussain, S. (2008). Resources for Urdu Language Processing. In Proceedings of the Sixth Workshop on Asian Language Resources.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ijaz</author>
<author>S Hussain</author>
</authors>
<title>Corpus Based Urdu Lexicon Development,</title>
<date>2007</date>
<booktitle>in the Proceedings of Conference on Language Technology (CLT07),</booktitle>
<institution>University of Peshawar, Pakistan.</institution>
<marker>Ijaz, Hussain, 2007</marker>
<rawString>Ijaz, M., Hussain, S. (2007). Corpus Based Urdu Lexicon Development, in the Proceedings of Conference on Language Technology (CLT07), University of Peshawar, Pakistan.</rawString>
</citation>
<citation valid="false">
<authors>
<author>D Jurafsky</author>
<author>l Martin</author>
</authors>
<booktitle>J(2000). Speech and Language Processing: An Introduction to Natural Language Processing (1st ed.). Computational Linguistics and Speech Recognition</booktitle>
<publisher>Prentice Hall.</publisher>
<marker>Jurafsky, Martin, </marker>
<rawString>Jurafsky, D., &amp; Martin., l. J(2000). Speech and Language Processing: An Introduction to Natural Language Processing (1st ed.). Computational Linguistics and Speech Recognition Prentice Hall.</rawString>
</citation>
<citation valid="false">
<authors>
<author>K Khankasikam</author>
<author>N Muansuwan</author>
</authors>
<title>Thai Word Segmentation a Lexical Semantic Approach.</title>
<marker>Khankasikam, Muansuwan, </marker>
<rawString>Khankasikam, K., &amp; Muansuwan, N. (n.d.). Thai Word Segmentation a Lexical Semantic Approach.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Khankasikam</author>
<author>N Muansuwan</author>
</authors>
<title>Thai Word Segmentation a Lexical Semantic Approach.</title>
<date>2005</date>
<booktitle>In the Proceedings of the Tenth Machine Translation Summit,</booktitle>
<pages>331--338</pages>
<contexts>
<context position="1473" citStr="Khankasikam &amp; Muansuwan, 2005" startWordPosition="234" endWordPosition="238">cters join together to form ligatures, which end either with a space or with a non-joining character. A word may be composed of one of more ligatures. In Urdu, space is not used to separate two consecutive words in a sentence; instead readers themselves identify the boundaries of words, as the sequence of ligatures, as they read along the text. Space is used to get appropriate character shapes and thus it may even be used within a word to break the word into constituent ligatures (Naseem 2007, Durrani 2008). Therefore, like other languages (Theeramunkong &amp; Usanavasin, 2001; Wan and Liu, 2007; Khankasikam &amp; Muansuwan, 2005; Haruechaiyasak et al., 2008; Haizhou &amp; Baosheng, 1998), word segmentation or word tokenization is a prelimiSarmad Hussain Center for Language Engineering, Al-Khawarizmi Institute of Computer Science, University of Engineering and Technology, Lahore, Pakistan sarmad.hussain@kics.edu.pk nary task for Urdu language processing. It has applications in many areas like spell checking, POS tagging, speech synthesis, information retrieval etc. This paper focuses on the word segmentation problem from the point of view of Optical Character Recognition (OCR) System. As space is not visible in typed and </context>
</contexts>
<marker>Khankasikam, Muansuwan, 2005</marker>
<rawString>Khankasikam, K., &amp; Muansuwan, N. (2005). Thai Word Segmentation a Lexical Semantic Approach. In the Proceedings of the Tenth Machine Translation Summit, (pp. 331-338). Thailand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Lehal</author>
</authors>
<title>A Two Stage Word Segmentation System for Handling Space Insertion Problem in Urdu Script.</title>
<date>2009</date>
<journal>World Academy of Science, Engineering and Technology</journal>
<volume>60</volume>
<contexts>
<context position="4849" citStr="Lehal (2009)" startWordPosition="749" endWordPosition="750">fficiently large set of training examples to enable accurate classification. A knowledge based approach has been adopted for earlier work on Urdu word segmentation (Durrani 2007; also see Durrani and Hussain 2010). In this technique word segmentation of Urdu text is achieved by employing knowledge based on the Urdu linguistics and script. The initial segmentations are ranked using minword, unigram and bigram techniques. It reports 95.8 % overall accuracy for word segmentation of Urdu text. Mukund et al. (2009) propose using character model along with linguistic rules and report 83% precision. Lehal (2009) proposes a two stage process, which first uses Urdu linguistic knowledge, and then uses statistical information of Urdu and Hindi (also using transliteration into Hindi) in the second stage for words not addressed in the first stage, reporting an accuracy of 98.57%. These techniques use characters or words in the input, whereas an OCR outputs a series of ligatures. The current paper presents work done using statistical methods as an alternative, which works with ligatures as input. 3 Methodology Current work uses the co-occurrence information of ligatures and words to construct a statistical </context>
</contexts>
<marker>Lehal, 2009</marker>
<rawString>Lehal, G. (2009). A Two Stage Word Segmentation System for Handling Space Insertion Problem in Urdu Script. World Academy of Science, Engineering and Technology 60.</rawString>
</citation>
<citation valid="false">
<title>Word Binding Force.</title>
<booktitle>In Proceedings of the 16th conference on Computational linguistics.</booktitle>
<marker></marker>
<rawString>Word Binding Force. In Proceedings of the 16th conference on Computational linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D J MacKay</author>
<author>L C Peto</author>
</authors>
<title>A Hierarchical Dirichlet Language Mode.</title>
<date>1995</date>
<journal>Natural Language Engineering ,</journal>
<volume>1</volume>
<issue>3</issue>
<pages>1--19</pages>
<marker>MacKay, Peto, 1995</marker>
<rawString>MacKay, D. J., &amp; Peto, L. C. (1995). A Hierarchical Dirichlet Language Mode. Natural Language Engineering , 1 (3), 1-19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Mukund</author>
<author>R Srihari</author>
</authors>
<title>NE Tagging for Urdu based on Bootstrap POS Learning.</title>
<date>2009</date>
<booktitle>In the Proceedings of CLIAWS3, Third International Cross Lingual Information Access Workshop,</booktitle>
<pages>61--69</pages>
<location>Boulder, Colorado, USA.</location>
<marker>Mukund, Srihari, 2009</marker>
<rawString>Mukund, S. &amp; Srihari, R. (2009). NE Tagging for Urdu based on Bootstrap POS Learning. In the Proceedings of CLIAWS3, Third International Cross Lingual Information Access Workshop, pages 61–69, Boulder, Colorado, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Naseem</author>
<author>S Hussain</author>
</authors>
<title>Spelling Error Trends in Urdu,</title>
<date>2007</date>
<booktitle>In the Proceedings of Conference on Language Technology (CLT07),</booktitle>
<institution>University of Peshawar, Pakistan.</institution>
<marker>Naseem, Hussain, 2007</marker>
<rawString>Naseem, T., &amp; Hussain, S. (2007). Spelling Error Trends in Urdu, In the Proceedings of Conference on Language Technology (CLT07), University of Peshawar, Pakistan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pascale</author>
<author>W Dekai</author>
</authors>
<title>Statistical augmentation of a Chinese machine readable dictionary.</title>
<date>1994</date>
<booktitle>Proceedings of the Second Annual Workshop on Very Large Corpora,</booktitle>
<pages>69--85</pages>
<marker>Pascale, Dekai, 1994</marker>
<rawString>Pascale, F., &amp; Dekai, W. (1994). Statistical augmentation of a Chinese machine readable dictionary. Proceedings of the Second Annual Workshop on Very Large Corpora, (pp. 69-85).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Poowarawan</author>
</authors>
<title>Dictionary-based Thai Syllable Separation.</title>
<date>1986</date>
<booktitle>Proceedings of the Ninth Electronics Engineering Conference.</booktitle>
<contexts>
<context position="3046" citStr="Poowarawan, 1986" startWordPosition="473" endWordPosition="474">, Hindi, Nepali, Sinhala, Bengali, Greek, Russian, etc. segment text into a sequence of words using delimiters such as space, comma and semi colon etc., but on the other hand many Asian languages like Urdu, Persian, Arabic, Chinese, Dzongkha, Lao and Thai have no explicit word boundaries. In such languages, words are segmented using more advanced techniques, which can be categorized into three methods: (i) Dictionary/lexicon based approaches (ii) Linguistic knowledge based approaches (iii) Machine learning based approaches/statistical approaches (Haruechaiyasak et al., 2008) Longest matching (Poowarawan, 1986; Richard Sproat, 1996) and maximum matching (Sproat et al., 1996; Haizhou &amp; Baosheng, 1998) are examples of lexicon based approaches. These techniques segment text using the lexicon. Their 88 Proceedings of the 8th Workshop on Asian Language Resources, pages 88–94, Beijing, China, 21-22 August 2010. c�2010 Asian Federation for Natural Language Processing accuracy depends on the quality and size of the dictionary. N-Grams (Chang et al., 1992; Li Haizhou et al., 1997; Richard Sproat, 1996; Dai &amp; Lee, 1994; Aroonmanakun, 2002) and Maximum collocation (Aroonmanakun, 2002) are Linguistic knowledge</context>
</contexts>
<marker>Poowarawan, 1986</marker>
<rawString>Poowarawan, Y. (1986). Dictionary-based Thai Syllable Separation. Proceedings of the Ninth Electronics Engineering Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sproat</author>
<author>C S</author>
</authors>
<title>A Stochastic FiniteState Word-Segmentation Algorithm for Chinese.</title>
<date>1996</date>
<journal>Computational Linguistics ,</journal>
<volume>22</volume>
<issue>3</issue>
<marker>Sproat, S, 1996</marker>
<rawString>Richard Sproat, C. S. (1996). A Stochastic FiniteState Word-Segmentation Algorithm for Chinese. Computational Linguistics , 22 (3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Sajjad</author>
</authors>
<title>Statistical Part-of-Speech for Urdu.</title>
<date>2007</date>
<booktitle>Research in Urdu Language Processing,</booktitle>
<tech>MS thesis,</tech>
<institution>National University of Computer and Emerging Sciencies, Centre for</institution>
<location>Lahore ,</location>
<contexts>
<context position="6499" citStr="Sajjad, 2007" startWordPosition="1012" endWordPosition="1013"> for the final decision. Details are described in the subsequent sections. 3.1 Data collection and preparation An existing lexicon of 49630 unique words is used (derived from Ijaz et al. 2007). The corpus used for building ligature grams consists of half a million words. Of these, 300,000 words are taken from the Sports, Consumer Information and Culture/Entertainment domains of the 18 million word corpus (Ijaz et al. 2007), 100,000 words are obtained from Urdu-NepaliEnglish Parallel Corpus (available at www.PANL10n.net), and another 100,000 words are taken from a previously POS tagged corpus (Sajjad, 2007; tags of this corpus are removed before further processing). This corpus is manually cleaned for word segmentation errors, by adding missing spaces between words and replacing spaces with Zero Width NonJoiner (ZWNJ) within words. For the computation of word grams, the 18 million word corpus of Urdu is used (Ijaz et al. 2007). 3.2 Count and probability calculations Table 1 and Table 2 below give the counts for unigram, bigrams and trigram of the ligatures and the words derived from the corpora respectively. Ligature Ligature Ligature Ligature Tokens Unigram Bigrams Trigrams 1508078 10215 35202</context>
</contexts>
<marker>Sajjad, 2007</marker>
<rawString>Sajjad, H. (2007). Statistical Part-of-Speech for Urdu. MS thesis, National University of Computer and Emerging Sciencies, Centre for Research in Urdu Language Processing, Lahore , Pakistan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Sornlertlamvanich</author>
<author>T Potipiti</author>
<author>T charoenporn</author>
</authors>
<title>Automatic Corpus-Based Thai Word Algorithm Extraction with the C4.5 Learning.</title>
<date>2000</date>
<booktitle>Proceedings of the 18th conference on Computational linguistics.</booktitle>
<contexts>
<context position="3920" citStr="Sornlertlamvanich et al., 2000" startWordPosition="600" endWordPosition="603">esources, pages 88–94, Beijing, China, 21-22 August 2010. c�2010 Asian Federation for Natural Language Processing accuracy depends on the quality and size of the dictionary. N-Grams (Chang et al., 1992; Li Haizhou et al., 1997; Richard Sproat, 1996; Dai &amp; Lee, 1994; Aroonmanakun, 2002) and Maximum collocation (Aroonmanakun, 2002) are Linguistic knowledge based approaches, which also rely very much on the lexicon. These approaches select most likely segmentation from the set of possible segmentations using a probabilistic or cost-based scoring mechanism. Word segmentation using decision trees (Sornlertlamvanich et al., 2000; Theeramunkong &amp; Usanavasin, 2001) and similar other techniques fall in the third category of word segmentation techniques. These approaches use a corpus in which word boundaries are explicitly marked. These approaches do not require dictionaries. In these approaches ambiguity problems are handled by providing a sufficiently large set of training examples to enable accurate classification. A knowledge based approach has been adopted for earlier work on Urdu word segmentation (Durrani 2007; also see Durrani and Hussain 2010). In this technique word segmentation of Urdu text is achieved by empl</context>
</contexts>
<marker>Sornlertlamvanich, Potipiti, charoenporn, 2000</marker>
<rawString>Sornlertlamvanich, V., Potipiti, T., &amp; charoenporn, T. (2000). Automatic Corpus-Based Thai Word Algorithm Extraction with the C4.5 Learning. Proceedings of the 18th conference on Computational linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Sproat</author>
<author>C Shih</author>
<author>W Gale</author>
<author>N Chang</author>
</authors>
<title>A Stochastic Finite-State Word-Segmentation Algorithm for Chinese.</title>
<date>1996</date>
<journal>Computational Linguistics ,</journal>
<volume>22</volume>
<issue>3</issue>
<contexts>
<context position="3111" citStr="Sproat et al., 1996" startWordPosition="481" endWordPosition="484">t text into a sequence of words using delimiters such as space, comma and semi colon etc., but on the other hand many Asian languages like Urdu, Persian, Arabic, Chinese, Dzongkha, Lao and Thai have no explicit word boundaries. In such languages, words are segmented using more advanced techniques, which can be categorized into three methods: (i) Dictionary/lexicon based approaches (ii) Linguistic knowledge based approaches (iii) Machine learning based approaches/statistical approaches (Haruechaiyasak et al., 2008) Longest matching (Poowarawan, 1986; Richard Sproat, 1996) and maximum matching (Sproat et al., 1996; Haizhou &amp; Baosheng, 1998) are examples of lexicon based approaches. These techniques segment text using the lexicon. Their 88 Proceedings of the 8th Workshop on Asian Language Resources, pages 88–94, Beijing, China, 21-22 August 2010. c�2010 Asian Federation for Natural Language Processing accuracy depends on the quality and size of the dictionary. N-Grams (Chang et al., 1992; Li Haizhou et al., 1997; Richard Sproat, 1996; Dai &amp; Lee, 1994; Aroonmanakun, 2002) and Maximum collocation (Aroonmanakun, 2002) are Linguistic knowledge based approaches, which also rely very much on the lexicon. Thes</context>
</contexts>
<marker>Sproat, Shih, Gale, Chang, 1996</marker>
<rawString>Sproat, R., Shih, C., Gale, W., &amp; Chang, N. (1996). A Stochastic Finite-State Word-Segmentation Algorithm for Chinese. Computational Linguistics , 22 (3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Theeramunkong</author>
<author>S Usanavasin</author>
</authors>
<title>NonDictionary-Based Thai Word Segmentation Using Decision Trees.</title>
<date>2001</date>
<booktitle>Proceedings of the first international conference on Human language technology research.</booktitle>
<contexts>
<context position="1423" citStr="Theeramunkong &amp; Usanavasin, 2001" startWordPosition="226" endWordPosition="229">script for writing, which is cursive in nature. Characters join together to form ligatures, which end either with a space or with a non-joining character. A word may be composed of one of more ligatures. In Urdu, space is not used to separate two consecutive words in a sentence; instead readers themselves identify the boundaries of words, as the sequence of ligatures, as they read along the text. Space is used to get appropriate character shapes and thus it may even be used within a word to break the word into constituent ligatures (Naseem 2007, Durrani 2008). Therefore, like other languages (Theeramunkong &amp; Usanavasin, 2001; Wan and Liu, 2007; Khankasikam &amp; Muansuwan, 2005; Haruechaiyasak et al., 2008; Haizhou &amp; Baosheng, 1998), word segmentation or word tokenization is a prelimiSarmad Hussain Center for Language Engineering, Al-Khawarizmi Institute of Computer Science, University of Engineering and Technology, Lahore, Pakistan sarmad.hussain@kics.edu.pk nary task for Urdu language processing. It has applications in many areas like spell checking, POS tagging, speech synthesis, information retrieval etc. This paper focuses on the word segmentation problem from the point of view of Optical Character Recognition (</context>
<context position="3955" citStr="Theeramunkong &amp; Usanavasin, 2001" startWordPosition="604" endWordPosition="608">China, 21-22 August 2010. c�2010 Asian Federation for Natural Language Processing accuracy depends on the quality and size of the dictionary. N-Grams (Chang et al., 1992; Li Haizhou et al., 1997; Richard Sproat, 1996; Dai &amp; Lee, 1994; Aroonmanakun, 2002) and Maximum collocation (Aroonmanakun, 2002) are Linguistic knowledge based approaches, which also rely very much on the lexicon. These approaches select most likely segmentation from the set of possible segmentations using a probabilistic or cost-based scoring mechanism. Word segmentation using decision trees (Sornlertlamvanich et al., 2000; Theeramunkong &amp; Usanavasin, 2001) and similar other techniques fall in the third category of word segmentation techniques. These approaches use a corpus in which word boundaries are explicitly marked. These approaches do not require dictionaries. In these approaches ambiguity problems are handled by providing a sufficiently large set of training examples to enable accurate classification. A knowledge based approach has been adopted for earlier work on Urdu word segmentation (Durrani 2007; also see Durrani and Hussain 2010). In this technique word segmentation of Urdu text is achieved by employing knowledge based on the Urdu l</context>
</contexts>
<marker>Theeramunkong, Usanavasin, 2001</marker>
<rawString>Theeramunkong, T., &amp; Usanavasin, S. (2001). NonDictionary-Based Thai Word Segmentation Using Decision Trees. Proceedings of the first international conference on Human language technology research.</rawString>
</citation>
<citation valid="false">
<title>Retrieved from Center for Research in Urdu Language Processing: http://www.crulp.org/software /ling_resources/urdunepalienglishparallelcorpus. htm</title>
<marker></marker>
<rawString>Urdu-Nepali-English Parallel Corpus. (n.d.). Retrieved from Center for Research in Urdu Language Processing: http://www.crulp.org/software /ling_resources/urdunepalienglishparallelcorpus. htm</rawString>
</citation>
<citation valid="true">
<authors>
<author>X-J Wang</author>
<author>W Liu</author>
<author>Y Qin</author>
</authors>
<title>A Searchbased Chinese Word Segmentation Method.</title>
<date>2007</date>
<booktitle>16th International World Wide Web Conference.</booktitle>
<marker>Wang, Liu, Qin, 2007</marker>
<rawString>Wang, X.-J., Liu, W., &amp; Qin, Y. (2007). A Searchbased Chinese Word Segmentation Method. 16th International World Wide Web Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P-k Wong</author>
<author>C Chan</author>
</authors>
<title>Chinese Word Segmentation based on Maximum Matching and</title>
<date>1996</date>
<marker>Wong, Chan, 1996</marker>
<rawString>Wong, P.-k., &amp; Chan, C. (1996). Chinese Word Segmentation based on Maximum Matching and</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>