<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.040418">
<title confidence="0.996224">
USAAR-SHEFFIELD: Semantic Textual Similarity with Deep Regression
and Machine Translation Evaluation Metrics
</title>
<author confidence="0.991715">
Liling Tanα, Carolina Scartonβ, Lucia Speciaβ and Josef van Genabithγ
</author>
<affiliation confidence="0.9821795">
αUniversit¨at des Saarlandes / Campus A2.2, Saarbr¨ucken, Germany
βUniversity of Sheffield / Regent Court, 211 Portobello, Sheffield, UK
</affiliation>
<address confidence="0.57843">
γDeutsches Forschungszentrum f¨ur K¨unstliche Intelligenz / Saarbr¨ucken, Germany
</address>
<email confidence="0.951448">
alvations@gmail.com, c.scarton@sheffield.ac.uk,
l.specia@sheffield.ac.uk, josef.van genabith@dfki.de
</email>
<sectionHeader confidence="0.99557" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99988225">
This paper describes the USAAR-
SHEFFIELD systems that participated
in the Semantic Textual Similarity (STS)
English task of SemEval-2015. We extend the
work on using machine translation evaluation
metrics in the STS task. Different from
previous approaches, we regard the metrics’
robustness across different text types and
conflate the training data across different
subcorpora. In addition, we introduce a novel
deep regressor architecture and evaluated its
efficiency in the STS task.
</bodyText>
<sectionHeader confidence="0.998975" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999244888888889">
Semantic Textual Similarity (STS) is the task of
measuring the degree to which two text snippets
have the same meaning (Agirre et al., 2014). For
instance, given the two texts, ”a dog sprints across
the water” and ”a dog jumps through water”, partic-
ipating systems are required to predict a real number
similarity score on a scale of 0 (no relation) to 5 (se-
mantic equivalence).
This paper presents a collaborative submission
between Saarland University and University of
Sheffield to the STS English shared task at SemEval-
2015. We have submitted three models that use Ma-
chine Translation (MT) evaluation metrics as fea-
tures to build supervised regressors that predict the
similarity scores for the STS task. We introduce two
variants of a novel deep regressor architecture and
a classical baseline regression system that uses MT
evaluation metrics as input features.
</bodyText>
<sectionHeader confidence="0.999814" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999741333333333">
Previously, research teams have applied MT evalua-
tion metrics for the STS task with increasingly bet-
ter results (Agirre et al., 2012; Agirre et al., 2013;
Agirre et al., 2014). Rios et al. (2012) trained a
Support Vector Regressor scoring a Pearson corre-
lation mean of 0.3825 (Baseline1: 0.4356). Barr´on-
Cede˜no et al. (2013) also used a Support Vector Re-
gressor and did better than the baseline at 0.4037
mean score (Baseline: 0.3639). Huang and Chang
(2014) used a linear regressor and scored 0.792 beat-
ing the baseline system (Baseline: 0.613).
Another notable mention of MT technology in the
STS task is the use of referential translation ma-
chines to predict and derive features instead of us-
ing MT evaluation metrics (Bic¸ici and van Genabith,
2013; Bic¸ici and Way, 2014).
These previous approaches have trained a differ-
ent system for each subcorpus provided by the task
organizers. We have chosen to combine the differ-
ent subcorpora since MT evaluation metrics are ex-
pected to be robust against text types and domains
(Han et al., 2012; Pad´o et al., 2009).
Much of the previous work on using MT evalu-
ation metrics is based on improving the regressors
through algorithm choice, feature selection and pa-
rameters tuning. We introduce a novel architecture
of hybrid supervised machine learning, Deep Re-
gression, which attempts to combine different re-
gressors and automating feature selection by means
of dimensionality reduction.
</bodyText>
<footnote confidence="0.9360135">
1Refers to the token cosine baseline system
(baseline-tokencos) from the task organizers.
</footnote>
<page confidence="0.995346">
85
</page>
<note confidence="0.6119735">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 85–89,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.977632" genericHeader="method">
3 Deep Regression Architecture
</sectionHeader>
<bodyText confidence="0.993726846153846">
Ensemble learning constructs a set of models based
on different algorithms and then labels new data
points by taking a (weighted) vote from the algo-
rithms’ predictions (Dietterich, 2000). A typical sin-
gle layer feed-forward neural network creates a layer
of perceptrons that receives inputs and predicts a
series of outputs converted by means of an activa-
tion function and then the outputs will enter a final
layer of a single classifier to provide a final predic-
tion (Auer et al., 2008). We propose a deep regres-
sion architecture that is a unique way to combine
a single-layer feed-forward neural net architecture
with ensemble-like supervised learning.
</bodyText>
<figureCaption confidence="0.998918">
Figure 1: Deep Regression Architecture.
</figureCaption>
<bodyText confidence="0.98404805">
Figure 1 presents the Deep Regression architec-
ture where the inputs are fed into the different hid-
den regressors and unlike traditional neural network,
each regressor produces a discrete output with a dif-
ferent cost function unlike the consistent activation
function in neural nets. Different from ensemble
learning, the voting/selection determinant has been
replaced by a last layer of a single regressor that
takes latent layer as input to produce the final out-
put STS score.
By designing the architecture in this way, the fea-
ture space from the input is reduced to the number
of hidden regressors and the input for the last layer
regressors is a latent layer in the higher dimensional
space. Within a standard neural net, every node in
the latent layer is influenced by all the perceptrons
in the previous layer. In contrast, each latent dimen-
sion is only dependent on one regressor; in this re-
spect it resembles ensemble learning where the re-
gressors/classifiers are trained independently.
</bodyText>
<sectionHeader confidence="0.990622" genericHeader="method">
4 Feature Matrix
</sectionHeader>
<bodyText confidence="0.999538222222222">
Machine Translation evaluation metrics consider
varying degrees of information at the lexical, syn-
tactic and semantic levels. Each metric comprises
several features that compute the translation quality
by comparing every translation against one or sev-
eral reference translations. We consider three sets
of features: n-gram overlaps, Shallow Parsing met-
rics and METEOR. These metrics correspond to the
lexical, syntactic and semantic levels respectively.
</bodyText>
<subsectionHeader confidence="0.973336">
4.1 N-gram Overlaps
</subsectionHeader>
<bodyText confidence="0.987529941176471">
Gonz`alez et al. (2014) reintroduces the notion of lan-
guage independent metrics relying on n-gram over-
laps. This is similar to the BLEU metric that cal-
culates the geometric mean of n-gram precision by
comparing the translation against its reference(s)
(Papineni et al., 2002) without the brevity penalty.
Different from BLEU, the n-gram overlaps are
computed as similarity coefficients instead of taking
the crude proportion of overlap n-gram.
n-gramoverlap = sim (n -gramtrans ∩ n-gramref)
We use 16 features of n-gram overlap by consid-
ering both the cosine similarity and Jaccard Index in
calculating the n-gram overlaps for character and to-
ken n-gram from the order of bigrams to 5-grams. In
addition, we use the ratio of n-gram lengths and the
Jaccard similarity of pseudo-cognates (Simard et al.,
1992) as the 17th and 18th n-gram overlap features.
</bodyText>
<subsectionHeader confidence="0.998791">
4.2 Shallow Parsing
</subsectionHeader>
<bodyText confidence="0.999666">
The Shallow Parsing (SP) metric measures the syn-
tactic similarities by computing the overlaps be-
tween the translation and the reference translation at
the Parts-Of-Speech (POS), word lemmas and base
phrase chunks level. The purpose of the SP metric
is to capture the proportion of lexical items correctly
translated according to their shallow syntactic real-
ization.
The base phrase chunks are tagged using the
BIOS toolkit (Surdeanu et al., 2005) and POS tag-
</bodyText>
<page confidence="0.979979">
86
</page>
<bodyText confidence="0.99816">
ging and lemmatization are achieved using SVM-
Tool (Gim´enez and M`arquez, 2004). For in-
stance, given a pair of sentences in the format
(word/POS/lemma/chunk):
</bodyText>
<listItem confidence="0.995616833333333">
• NP(a/DT/a/B-NP dog/NN/dog/I-NP)
sprints/VBZ/sprint/B-VP across/IN/across/O
NP(the/DET/the/B-NP water/NN/water/I-NP)
• NP(a/DT/a/B-NP dog/NN/dog/I-NP)
jumps/VBZ/jump/B-VP through/IN/through/O
water/NN/water/B-NP
</listItem>
<bodyText confidence="0.962320416666667">
We consider the overlap proportions for the POS
features, lemma, IOB features, shallow chunks. The
Inside, Outside, Begin (IOB) features refer to the
shallow parsing tags at the lexical level, e.g. B-NP
represents the beginning of a noun phrase (Sang et
al., 2000). The IOB features are measured lexi-
cally by considering each IOB tag while the shallow
chunk features only consider the number of brack-
eted chunks.
For instance, the POS tag DT occurs twice in first
sentence one and once in second sentence, thus we
extract the feature SP-POS(DT) = 1/2 = 0.5.
</bodyText>
<listItem confidence="0.9996906">
• SP-POS(DT,NN,VBZ,IN) = [0.5,1,1,1]
• SP-LEMMA(a,dog,jump,through,water) =
[1,1,0,0,1]
• SP-IOB(B-NP,I-NP,B-VP,O) = [1,1,-0.5,1,1]
• SP-CHUNK(NP) = [0.5]
</listItem>
<bodyText confidence="0.9855045">
For SP-POS, SP-LEMMA and SP-IOB, we use
the NIST-like measure where we not only con-
sider the individual POS, LEMMA or IOB tags but
an accumulated score over a sequence of 1-5 n-
grams, e.g. SP-POS(DT+NN,DT+NN+VBZ, ...)
or SP-LEMMA(a+dog,a+dog+jump, ...).
</bodyText>
<sectionHeader confidence="0.999464" genericHeader="method">
5 METEOR
</sectionHeader>
<bodyText confidence="0.999754058823529">
METEOR aligns the translation to a reference trans-
lation first then it uses unigram mapping to match
words at their surface forms, word stems, syn-
onym matches and paraphrase matches (Banerjee
and Lavie, 2005; Denkowski and Lavie, 2010).
Different from the n-gram and shallow parsing
features, METEOR makes a distinction between
content words and function words and the precision
and recall is measured by weighing them differently.
It also accounts for word order differences by penal-
izing chunks from the translation that do not appear
in the translation.
We use the METEOR 1.5 system with tuned
weights and penalty using the WMT12 data. For
the STS experiment, we use all four variants of
METEOR: exact matches, stem matches, synonym
matches and paraphrase matches.
</bodyText>
<sectionHeader confidence="0.998204" genericHeader="evaluation">
6 Experiments and Results
</sectionHeader>
<subsectionHeader confidence="0.998678">
6.1 Training Data
</subsectionHeader>
<bodyText confidence="0.999967222222222">
We conflated all training and test data of vari-
ous text types from previous SemEval STS shared
tasks into a single training set with 10597 para-
graph/sentence/caption pairs. The MT metrics for
each text pair were computed with the Asiya toolkit
(Gim´enez and M`arquez, 2010). Tokenization and
preprocessing operations, such as lemmatization,
POS tagging, parsing and n-gram extraction, are per-
formed by the Asiya toolkit.
</bodyText>
<subsectionHeader confidence="0.999018">
6.2 Models
</subsectionHeader>
<bodyText confidence="0.9913315">
We submitted three models to the SemEval-2015
STS English Task:
</bodyText>
<listItem confidence="0.999551">
• ModelX: Deep Regression framework with the
full feature set from n-gram overlaps, Shallow
Parsing and METEOR.
• ModelY: Bayesian Ridge Regressor with the
full feature set
• ModelZ: Deep Regression framework with
only METEOR features
</listItem>
<bodyText confidence="0.999261625">
For the hidden regressors layer of the deep regres-
sion models, we have used the multivariate linear,
logistic, Bayesian ridge, elastic net, random sam-
ple consensus and support vector (radial basis func-
tion kernel) regressors.2 The final layer regressor
is a Bayesian ridge regressor. These supervised re-
gressors are implemented in scikit-learn (Pe-
dregosa et al., 2011).
</bodyText>
<footnote confidence="0.65206175">
2No comprehensive parameter tuning was at-
tempted on the models and the default parameters for
each regressor can be found on our code repository,
https://github.com/alvations/USAAR-SemEval-2015.
</footnote>
<page confidence="0.996941">
87
</page>
<table confidence="0.99968575">
Ans-Forums Ans-Student Belief Headlines Images Mean Rank
ModelX 0.3706 0.3609 0.4767 0.5183 0.5436 0.4616 68
ModelY 0.6264 0.7386 0.705 0.7927 0.8162 0.7275 21
ModelZ 0.4237 0.6757 0.6994 0.5239 0.6833 0.6111 58
</table>
<tableCaption confidence="0.999695">
Table 1: Spearman’s Results for STS English Task @ SemEval-2015.
</tableCaption>
<subsectionHeader confidence="0.866588">
6.3 Results
</subsectionHeader>
<bodyText confidence="0.999378214285714">
Table 1 presents the official results for the En-
glish STS task where our baseline model (ModelY)
strikingly outperforms the deep regressor models
(ModelX and ModelZ).
Our baseline model achieved modest results rank-
ing 24 out of 73 submissions, however our deep re-
gressors have failed to function on par with a sim-
ple baseline regressor. We note that the deep regres-
sor with the full feature set (ModelX) scored lower
than the deep regressor with only the METEOR fea-
tures (ModelZ). This reiterates the effectiveness of
semantically motivated METEOR features in deter-
mining similarity as previously indicated by Huang
and Chang (2014).
</bodyText>
<figureCaption confidence="0.970234">
Figure 2: Comparison of Results with Best and Baseline
Systems
</figureCaption>
<bodyText confidence="0.99993375">
Interestingly, the conflation of datasets has no
obvious detrimental effects on the performance for
any specific domains. Figure 2 presents a com-
parison of results between ModelY, the top sys-
tem from DLSU and the organizers’ baseline sys-
tem (TokenCos). It shows that the distribution
of Spearman’s correlation for our model is as well-
balanced as the best system.
</bodyText>
<sectionHeader confidence="0.992238" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999916230769231">
In this paper, we have described our submissions
to the STS English task for SemEval-2015. We
have introduced a novel deep regression infrastruc-
ture with MT evaluation metrics to measure seman-
tic similarity. Although our deep regressors per-
formed poorly, our baseline system have achieved
promising results amongst the participating systems
and we showed that conflating datasets of different
genres has negligible effects on a semantic similarity
system based on MT evaluation metrics.
The results also confirm the good performance of
METEOR, a traditional MT evaluation metric, for
the STS task.
</bodyText>
<sectionHeader confidence="0.979345" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999496">
The research leading to these results has received
funding from the People Programme (Marie Curie
Actions) of the European Union’s Seventh Frame-
work Programme FP7/2007-2013/ under REA grant
agreement n ◦ 317471.
</bodyText>
<sectionHeader confidence="0.994557" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996527333333333">
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. SemEval-2012 Task 6: A
Pilot on Semantic Textual Similarity. In First Joint
Conference on Lexical and Computational Semantics
(*SEM): Proceedings of the Main Conference and the
Shared Task, pages 385–393, Montr´eal, Canada.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. SEM 2013 shared
task: Semantic Textual Similarity. In Second Joint
Conference on Lexical and Computational Semantics
(*SEM), Volume 1: Proceedings of the Main Confer-
ence and the Shared Task, pages 32–43, Atlanta, Geor-
gia.
Eneko Agirre, Carmen Banea, Claire Cardic, Daniel
Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo,
Rada Mihalcea, German Rigau, and Janyce Wiebe.
2014. SemEval-2014 Task 10: Multilingual Seman-
tic Textual Similarity. In Proceedings of the 8th Inter-
</reference>
<page confidence="0.997758">
88
</page>
<reference confidence="0.998879476635514">
national Workshop on Semantic Evaluation (SemEval
2014), pages 81–91, Dublin, Ireland.
Peter Auer, Harald Burgsteiner, and Wolfgang Maass.
2008. A learning rule for very simple universal ap-
proximators consisting of a single layer of perceptrons.
Neural Networks, 21(5):786–795.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Pro-
ceedings of the ACL 2005 Workshop on Intrinsic and
Extrinsic Evaluation Measures for MT and/or Summa-
rization, pages 65–72, Ann Arbor, Michigan.
Alberto Barr´on-Cede˜no, Llu´ıs M`arquez, Maria Fuentes,
Horacio Rodr´ıguez, and Jordi Turmo. 2013. UPC-
CORE: What Can Machine Translation Evaluation
Metrics and Wikipedia Do for Estimating Semantic
Textual Similarity? In Second Joint Conference on
Lexical and Computational Semantics (*SEM), Vol-
ume 1: Proceedings of the Main Conference and the
Shared Task, pages 143–147, Atlanta, Georgia.
Ergun Bic¸ici and Josef van Genabith. 2013. CNGL-
CORE: Referential Translation Machines for Measur-
ing Semantic Similarity. In Second Joint Conference
on Lexical and Computational Semantics (*SEM), Vol-
ume 1: Proceedings of the Main Conference and the
Shared Task, pages 234–240, Atlanta, Georgia.
Ergun Bic¸ici and Andy Way. 2014. RTM-DCU: Ref-
erential Translation Machines for Semantic Similarity.
In Proceedings of the 8th International Workshop on
Semantic Evaluation (SemEval 2014), pages 487–496,
Dublin, Ireland.
Michael Denkowski and Alon Lavie. 2010. Extending
the METEOR Machine Translation Evaluation Metric
to the Phrase Level. In Proceedings of the HLT: The
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 250–253, Los Angeles, California.
Thomas G. Dietterich. 2000. Ensemble Methods in Ma-
chine Learning. In Proceedings of the First Interna-
tional Workshop on Multiple Classifier Systems, MCS
’00, pages 1–15.
Jes´us Gim´enez and Llu´ıs M`arquez. 2004. Fast and Ac-
curate Part-of-Speech Tagging: The SVM Approach
Revisited. Recent Advances in Natural Language Pro-
cessing III, pages 153–162.
Jes´us Gim´enez and Llu´ıs M`arquez. 2010. Asiya:
An Open Toolkit for Automatic Machine Translation
(Meta-)Evaluation. The Prague Bulletin of Mathemat-
ical Linguistics, (94):77–86.
Meritxell Gonz`alez, , Alberto Barr´on-Cedeo, and Llus
M`arquez. 2014. Ipa and stout: Leveraging linguis-
tic and source-based features for machine translation
evaluation. In Ninth Workshop on Statistical Machine
Translation, page 8.
Aaron L.F. Han, Derek F. Wong, and Lidia S. Chao.
2012. Lepor: A robust evaluation metric for machine
translation with augmented factors. In 24th Interna-
tional Conference on Computational Linguistics, page
441. Citeseer.
Pingping Huang and Baobao Chang. 2014. SSMT:A
Machine Translation Evaluation View To Paragraph-
to-Sentence Semantic Similarity. In Proceedings of
the 8th International Workshop on Semantic Evalua-
tion (SemEval 2014), pages 585–589, Dublin, Ireland.
Sebastian Pad´o, Michel Galley, Dan Jurafsky, and Chris
Manning. 2009. Robust machine translation evalua-
tion with entailment features. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Volume
1, pages 297–305.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei
jing Zhu. 2002. BLEU: A Method for Automatic
Evaluation of Machine Translation. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311–318, Philadelphia,
Pennsylvania.
Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier Grisel,
Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vin-
cent Dubourg, Jake Vanderplas, Alexandre Passos,
David Cournapeau, Matthieu Brucher, Matthieu Per-
rot, and ´Edouard Duchesnay. 2011. Scikit-learn: Ma-
chine Learning in Python. Journal of Machine Learn-
ing Research, 12:2825–2830.
Miguel Rios, Wilker Aziz, and Lucia Specia. 2012.
UOW: Semantically Informed Text Similarity. In First
Joint Conference on Lexical and Computational Se-
mantics (*SEM): Proceedings of the Main Confer-
ence and the Shared Task, pages 673–678, Montr´eal,
Canada.
Tjong Kim Sang, Erik F., and Sabine Buchholz. 2000.
Introduction to the conll-2000 shared task: Chunking.
In Proceedings of the 2Nd Workshop on Learning Lan-
guage in Logic and the 4th Conference on Computa-
tional Natural Language Learning - Volume 7, ConLL
’00, pages 127–132, Stroudsburg, PA, USA.
Michel Simard, George F. Foster, and Pierre Isabelle.
1992. Using Cognates to Align Sentences in Bilin-
gual Corpora. In Proceedings of the Forth Interna-
tional Conference on Theoretical and Methodological
Issues in Machine Translation, Montr´eal, Canada.
Mihai Surdeanu, Jordi Turmo, and Eli Comelles. 2005.
Named Entity Recognition from Spontaneous Open-
Domain Speech. In Proceedings of the 9th Inter-
national Conference on Speech Communication and
Technology (Interspeech), Lisbon, Portugal.
</reference>
<page confidence="0.999752">
89
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.181492">
<title confidence="0.990576">USAAR-SHEFFIELD: Semantic Textual Similarity with Deep and Machine Translation Evaluation Metrics</title>
<author confidence="0.9148695">Carolina Lucia Josef van_des Saarlandes Campus A</author>
<author confidence="0.9148695">Saarbr¨ucken</author>
<address confidence="0.5575905">of Sheffield / Regent Court, 211 Portobello, Sheffield, Forschungszentrum f¨ur K¨unstliche Intelligenz / Saarbr¨ucken,</address>
<email confidence="0.797268">alvations@gmail.com,l.specia@sheffield.ac.uk,josef.vangenabith@dfki.de</email>
<abstract confidence="0.994402384615385">This paper describes the USAAR- SHEFFIELD systems that participated in the Semantic Textual Similarity (STS) English task of SemEval-2015. We extend the work on using machine translation evaluation metrics in the STS task. Different from previous approaches, we regard the metrics’ robustness across different text types and conflate the training data across different subcorpora. In addition, we introduce a novel deep regressor architecture and evaluated its efficiency in the STS task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor Gonzalez-Agirre</author>
</authors>
<date>2012</date>
<booktitle>SemEval-2012 Task 6: A Pilot on Semantic Textual Similarity. In First Joint Conference on Lexical and Computational Semantics (*SEM): Proceedings of the Main Conference and the Shared Task,</booktitle>
<pages>385--393</pages>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="2025" citStr="Agirre et al., 2012" startWordPosition="289" endWordPosition="292">ts a collaborative submission between Saarland University and University of Sheffield to the STS English shared task at SemEval2015. We have submitted three models that use Machine Translation (MT) evaluation metrics as features to build supervised regressors that predict the similarity scores for the STS task. We introduce two variants of a novel deep regressor architecture and a classical baseline regression system that uses MT evaluation metrics as input features. 2 Related Work Previously, research teams have applied MT evaluation metrics for the STS task with increasingly better results (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014). Rios et al. (2012) trained a Support Vector Regressor scoring a Pearson correlation mean of 0.3825 (Baseline1: 0.4356). Barr´onCede˜no et al. (2013) also used a Support Vector Regressor and did better than the baseline at 0.4037 mean score (Baseline: 0.3639). Huang and Chang (2014) used a linear regressor and scored 0.792 beating the baseline system (Baseline: 0.613). Another notable mention of MT technology in the STS task is the use of referential translation machines to predict and derive features instead of using MT evaluation metrics (Bic¸ici a</context>
</contexts>
<marker>Agirre, Cer, Diab, Gonzalez-Agirre, 2012</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. 2012. SemEval-2012 Task 6: A Pilot on Semantic Textual Similarity. In First Joint Conference on Lexical and Computational Semantics (*SEM): Proceedings of the Main Conference and the Shared Task, pages 385–393, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor GonzalezAgirre</author>
<author>Weiwei Guo</author>
</authors>
<title>shared task: Semantic Textual Similarity.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task,</booktitle>
<pages>32--43</pages>
<publisher>SEM</publisher>
<location>Atlanta,</location>
<contexts>
<context position="2046" citStr="Agirre et al., 2013" startWordPosition="293" endWordPosition="296">bmission between Saarland University and University of Sheffield to the STS English shared task at SemEval2015. We have submitted three models that use Machine Translation (MT) evaluation metrics as features to build supervised regressors that predict the similarity scores for the STS task. We introduce two variants of a novel deep regressor architecture and a classical baseline regression system that uses MT evaluation metrics as input features. 2 Related Work Previously, research teams have applied MT evaluation metrics for the STS task with increasingly better results (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014). Rios et al. (2012) trained a Support Vector Regressor scoring a Pearson correlation mean of 0.3825 (Baseline1: 0.4356). Barr´onCede˜no et al. (2013) also used a Support Vector Regressor and did better than the baseline at 0.4037 mean score (Baseline: 0.3639). Huang and Chang (2014) used a linear regressor and scored 0.792 beating the baseline system (Baseline: 0.613). Another notable mention of MT technology in the STS task is the use of referential translation machines to predict and derive features instead of using MT evaluation metrics (Bic¸ici and van Genabith, 2013</context>
</contexts>
<marker>Agirre, Cer, Diab, GonzalezAgirre, Guo, 2013</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, Aitor GonzalezAgirre, and Weiwei Guo. 2013. SEM 2013 shared task: Semantic Textual Similarity. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task, pages 32–43, Atlanta, Georgia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Carmen Banea</author>
<author>Claire Cardic</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor Gonzalez-Agirre</author>
<author>Weiwei Guo</author>
<author>Rada Mihalcea</author>
<author>German Rigau</author>
<author>Janyce Wiebe</author>
</authors>
<date>2014</date>
<booktitle>SemEval-2014 Task 10: Multilingual Semantic Textual Similarity. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>81--91</pages>
<location>Dublin, Ireland.</location>
<contexts>
<context position="1150" citStr="Agirre et al., 2014" startWordPosition="147" endWordPosition="150">he USAARSHEFFIELD systems that participated in the Semantic Textual Similarity (STS) English task of SemEval-2015. We extend the work on using machine translation evaluation metrics in the STS task. Different from previous approaches, we regard the metrics’ robustness across different text types and conflate the training data across different subcorpora. In addition, we introduce a novel deep regressor architecture and evaluated its efficiency in the STS task. 1 Introduction Semantic Textual Similarity (STS) is the task of measuring the degree to which two text snippets have the same meaning (Agirre et al., 2014). For instance, given the two texts, ”a dog sprints across the water” and ”a dog jumps through water”, participating systems are required to predict a real number similarity score on a scale of 0 (no relation) to 5 (semantic equivalence). This paper presents a collaborative submission between Saarland University and University of Sheffield to the STS English shared task at SemEval2015. We have submitted three models that use Machine Translation (MT) evaluation metrics as features to build supervised regressors that predict the similarity scores for the STS task. We introduce two variants of a </context>
</contexts>
<marker>Agirre, Banea, Cardic, Cer, Diab, Gonzalez-Agirre, Guo, Mihalcea, Rigau, Wiebe, 2014</marker>
<rawString>Eneko Agirre, Carmen Banea, Claire Cardic, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Rada Mihalcea, German Rigau, and Janyce Wiebe. 2014. SemEval-2014 Task 10: Multilingual Semantic Textual Similarity. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 81–91, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Auer</author>
<author>Harald Burgsteiner</author>
<author>Wolfgang Maass</author>
</authors>
<title>A learning rule for very simple universal approximators consisting of a single layer of perceptrons.</title>
<date>2008</date>
<journal>Neural Networks,</journal>
<volume>21</volume>
<issue>5</issue>
<contexts>
<context position="4119" citStr="Auer et al., 2008" startWordPosition="623" endWordPosition="626">5), pages 85–89, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics 3 Deep Regression Architecture Ensemble learning constructs a set of models based on different algorithms and then labels new data points by taking a (weighted) vote from the algorithms’ predictions (Dietterich, 2000). A typical single layer feed-forward neural network creates a layer of perceptrons that receives inputs and predicts a series of outputs converted by means of an activation function and then the outputs will enter a final layer of a single classifier to provide a final prediction (Auer et al., 2008). We propose a deep regression architecture that is a unique way to combine a single-layer feed-forward neural net architecture with ensemble-like supervised learning. Figure 1: Deep Regression Architecture. Figure 1 presents the Deep Regression architecture where the inputs are fed into the different hidden regressors and unlike traditional neural network, each regressor produces a discrete output with a different cost function unlike the consistent activation function in neural nets. Different from ensemble learning, the voting/selection determinant has been replaced by a last layer of a sin</context>
</contexts>
<marker>Auer, Burgsteiner, Maass, 2008</marker>
<rawString>Peter Auer, Harald Burgsteiner, and Wolfgang Maass. 2008. A learning rule for very simple universal approximators consisting of a single layer of perceptrons. Neural Networks, 21(5):786–795.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL 2005 Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization,</booktitle>
<pages>65--72</pages>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="8701" citStr="Banerjee and Lavie, 2005" startWordPosition="1322" endWordPosition="1325">• SP-POS(DT,NN,VBZ,IN) = [0.5,1,1,1] • SP-LEMMA(a,dog,jump,through,water) = [1,1,0,0,1] • SP-IOB(B-NP,I-NP,B-VP,O) = [1,1,-0.5,1,1] • SP-CHUNK(NP) = [0.5] For SP-POS, SP-LEMMA and SP-IOB, we use the NIST-like measure where we not only consider the individual POS, LEMMA or IOB tags but an accumulated score over a sequence of 1-5 ngrams, e.g. SP-POS(DT+NN,DT+NN+VBZ, ...) or SP-LEMMA(a+dog,a+dog+jump, ...). 5 METEOR METEOR aligns the translation to a reference translation first then it uses unigram mapping to match words at their surface forms, word stems, synonym matches and paraphrase matches (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010). Different from the n-gram and shallow parsing features, METEOR makes a distinction between content words and function words and the precision and recall is measured by weighing them differently. It also accounts for word order differences by penalizing chunks from the translation that do not appear in the translation. We use the METEOR 1.5 system with tuned weights and penalty using the WMT12 data. For the STS experiment, we use all four variants of METEOR: exact matches, stem matches, synonym matches and paraphrase matches. 6 Experiments and Results 6.1 Training </context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments. In Proceedings of the ACL 2005 Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization, pages 65–72, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alberto Barr´on-Cede˜no</author>
<author>Llu´ıs M`arquez</author>
<author>Maria Fuentes</author>
<author>Horacio Rodr´ıguez</author>
<author>Jordi Turmo</author>
</authors>
<title>UPCCORE: What Can Machine Translation Evaluation Metrics and Wikipedia Do for Estimating Semantic Textual Similarity?</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task,</booktitle>
<pages>143--147</pages>
<location>Atlanta,</location>
<marker>Barr´on-Cede˜no, M`arquez, Fuentes, Rodr´ıguez, Turmo, 2013</marker>
<rawString>Alberto Barr´on-Cede˜no, Llu´ıs M`arquez, Maria Fuentes, Horacio Rodr´ıguez, and Jordi Turmo. 2013. UPCCORE: What Can Machine Translation Evaluation Metrics and Wikipedia Do for Estimating Semantic Textual Similarity? In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task, pages 143–147, Atlanta, Georgia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ergun Bic¸ici</author>
<author>Josef van Genabith</author>
</authors>
<title>CNGLCORE: Referential Translation Machines for Measuring Semantic Similarity.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task,</booktitle>
<pages>234--240</pages>
<location>Atlanta,</location>
<marker>Bic¸ici, van Genabith, 2013</marker>
<rawString>Ergun Bic¸ici and Josef van Genabith. 2013. CNGLCORE: Referential Translation Machines for Measuring Semantic Similarity. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task, pages 234–240, Atlanta, Georgia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ergun Bic¸ici</author>
<author>Andy Way</author>
</authors>
<title>RTM-DCU: Referential Translation Machines for Semantic Similarity.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>487--496</pages>
<location>Dublin, Ireland.</location>
<marker>Bic¸ici, Way, 2014</marker>
<rawString>Ergun Bic¸ici and Andy Way. 2014. RTM-DCU: Referential Translation Machines for Semantic Similarity. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 487–496, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Denkowski</author>
<author>Alon Lavie</author>
</authors>
<title>Extending the METEOR Machine Translation Evaluation Metric to the Phrase Level.</title>
<date>2010</date>
<booktitle>In Proceedings of the HLT: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>250--253</pages>
<location>Los Angeles, California.</location>
<contexts>
<context position="8729" citStr="Denkowski and Lavie, 2010" startWordPosition="1326" endWordPosition="1329">0.5,1,1,1] • SP-LEMMA(a,dog,jump,through,water) = [1,1,0,0,1] • SP-IOB(B-NP,I-NP,B-VP,O) = [1,1,-0.5,1,1] • SP-CHUNK(NP) = [0.5] For SP-POS, SP-LEMMA and SP-IOB, we use the NIST-like measure where we not only consider the individual POS, LEMMA or IOB tags but an accumulated score over a sequence of 1-5 ngrams, e.g. SP-POS(DT+NN,DT+NN+VBZ, ...) or SP-LEMMA(a+dog,a+dog+jump, ...). 5 METEOR METEOR aligns the translation to a reference translation first then it uses unigram mapping to match words at their surface forms, word stems, synonym matches and paraphrase matches (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010). Different from the n-gram and shallow parsing features, METEOR makes a distinction between content words and function words and the precision and recall is measured by weighing them differently. It also accounts for word order differences by penalizing chunks from the translation that do not appear in the translation. We use the METEOR 1.5 system with tuned weights and penalty using the WMT12 data. For the STS experiment, we use all four variants of METEOR: exact matches, stem matches, synonym matches and paraphrase matches. 6 Experiments and Results 6.1 Training Data We conflated all traini</context>
</contexts>
<marker>Denkowski, Lavie, 2010</marker>
<rawString>Michael Denkowski and Alon Lavie. 2010. Extending the METEOR Machine Translation Evaluation Metric to the Phrase Level. In Proceedings of the HLT: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 250–253, Los Angeles, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas G Dietterich</author>
</authors>
<title>Ensemble Methods in Machine Learning.</title>
<date>2000</date>
<booktitle>In Proceedings of the First International Workshop on Multiple Classifier Systems, MCS ’00,</booktitle>
<pages>1--15</pages>
<contexts>
<context position="3818" citStr="Dietterich, 2000" startWordPosition="572" endWordPosition="573">ession, which attempts to combine different regressors and automating feature selection by means of dimensionality reduction. 1Refers to the token cosine baseline system (baseline-tokencos) from the task organizers. 85 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 85–89, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics 3 Deep Regression Architecture Ensemble learning constructs a set of models based on different algorithms and then labels new data points by taking a (weighted) vote from the algorithms’ predictions (Dietterich, 2000). A typical single layer feed-forward neural network creates a layer of perceptrons that receives inputs and predicts a series of outputs converted by means of an activation function and then the outputs will enter a final layer of a single classifier to provide a final prediction (Auer et al., 2008). We propose a deep regression architecture that is a unique way to combine a single-layer feed-forward neural net architecture with ensemble-like supervised learning. Figure 1: Deep Regression Architecture. Figure 1 presents the Deep Regression architecture where the inputs are fed into the differ</context>
</contexts>
<marker>Dietterich, 2000</marker>
<rawString>Thomas G. Dietterich. 2000. Ensemble Methods in Machine Learning. In Proceedings of the First International Workshop on Multiple Classifier Systems, MCS ’00, pages 1–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>Fast and Accurate Part-of-Speech Tagging: The SVM Approach Revisited. Recent Advances in Natural Language Processing III,</title>
<date>2004</date>
<pages>153--162</pages>
<marker>Gim´enez, M`arquez, 2004</marker>
<rawString>Jes´us Gim´enez and Llu´ıs M`arquez. 2004. Fast and Accurate Part-of-Speech Tagging: The SVM Approach Revisited. Recent Advances in Natural Language Processing III, pages 153–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>Asiya: An Open Toolkit for Automatic Machine Translation (Meta-)Evaluation. The Prague Bulletin of Mathematical Linguistics,</title>
<date>2010</date>
<marker>Gim´enez, M`arquez, 2010</marker>
<rawString>Jes´us Gim´enez and Llu´ıs M`arquez. 2010. Asiya: An Open Toolkit for Automatic Machine Translation (Meta-)Evaluation. The Prague Bulletin of Mathematical Linguistics, (94):77–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alberto Barr´on-Cedeo</author>
<author>Llus M`arquez</author>
</authors>
<title>Ipa and stout: Leveraging linguistic and source-based features for machine translation evaluation.</title>
<date>2014</date>
<booktitle>In Ninth Workshop on Statistical Machine Translation,</booktitle>
<pages>8</pages>
<marker>Barr´on-Cedeo, M`arquez, 2014</marker>
<rawString>Meritxell Gonz`alez, , Alberto Barr´on-Cedeo, and Llus M`arquez. 2014. Ipa and stout: Leveraging linguistic and source-based features for machine translation evaluation. In Ninth Workshop on Statistical Machine Translation, page 8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aaron L F Han</author>
<author>Derek F Wong</author>
<author>Lidia S Chao</author>
</authors>
<title>Lepor: A robust evaluation metric for machine translation with augmented factors.</title>
<date>2012</date>
<booktitle>In 24th International Conference on Computational Linguistics,</booktitle>
<pages>441</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="2935" citStr="Han et al., 2012" startWordPosition="442" endWordPosition="445">: 0.3639). Huang and Chang (2014) used a linear regressor and scored 0.792 beating the baseline system (Baseline: 0.613). Another notable mention of MT technology in the STS task is the use of referential translation machines to predict and derive features instead of using MT evaluation metrics (Bic¸ici and van Genabith, 2013; Bic¸ici and Way, 2014). These previous approaches have trained a different system for each subcorpus provided by the task organizers. We have chosen to combine the different subcorpora since MT evaluation metrics are expected to be robust against text types and domains (Han et al., 2012; Pad´o et al., 2009). Much of the previous work on using MT evaluation metrics is based on improving the regressors through algorithm choice, feature selection and parameters tuning. We introduce a novel architecture of hybrid supervised machine learning, Deep Regression, which attempts to combine different regressors and automating feature selection by means of dimensionality reduction. 1Refers to the token cosine baseline system (baseline-tokencos) from the task organizers. 85 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 85–89, Denver, Colorado,</context>
</contexts>
<marker>Han, Wong, Chao, 2012</marker>
<rawString>Aaron L.F. Han, Derek F. Wong, and Lidia S. Chao. 2012. Lepor: A robust evaluation metric for machine translation with augmented factors. In 24th International Conference on Computational Linguistics, page 441. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pingping Huang</author>
<author>Baobao Chang</author>
</authors>
<title>SSMT:A Machine Translation Evaluation View To Paragraphto-Sentence Semantic Similarity.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>585--589</pages>
<location>Dublin, Ireland.</location>
<contexts>
<context position="2352" citStr="Huang and Chang (2014)" startWordPosition="344" endWordPosition="347">e two variants of a novel deep regressor architecture and a classical baseline regression system that uses MT evaluation metrics as input features. 2 Related Work Previously, research teams have applied MT evaluation metrics for the STS task with increasingly better results (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014). Rios et al. (2012) trained a Support Vector Regressor scoring a Pearson correlation mean of 0.3825 (Baseline1: 0.4356). Barr´onCede˜no et al. (2013) also used a Support Vector Regressor and did better than the baseline at 0.4037 mean score (Baseline: 0.3639). Huang and Chang (2014) used a linear regressor and scored 0.792 beating the baseline system (Baseline: 0.613). Another notable mention of MT technology in the STS task is the use of referential translation machines to predict and derive features instead of using MT evaluation metrics (Bic¸ici and van Genabith, 2013; Bic¸ici and Way, 2014). These previous approaches have trained a different system for each subcorpus provided by the task organizers. We have chosen to combine the different subcorpora since MT evaluation metrics are expected to be robust against text types and domains (Han et al., 2012; Pad´o et al., 2</context>
<context position="11526" citStr="Huang and Chang (2014)" startWordPosition="1760" endWordPosition="1763">esents the official results for the English STS task where our baseline model (ModelY) strikingly outperforms the deep regressor models (ModelX and ModelZ). Our baseline model achieved modest results ranking 24 out of 73 submissions, however our deep regressors have failed to function on par with a simple baseline regressor. We note that the deep regressor with the full feature set (ModelX) scored lower than the deep regressor with only the METEOR features (ModelZ). This reiterates the effectiveness of semantically motivated METEOR features in determining similarity as previously indicated by Huang and Chang (2014). Figure 2: Comparison of Results with Best and Baseline Systems Interestingly, the conflation of datasets has no obvious detrimental effects on the performance for any specific domains. Figure 2 presents a comparison of results between ModelY, the top system from DLSU and the organizers’ baseline system (TokenCos). It shows that the distribution of Spearman’s correlation for our model is as wellbalanced as the best system. 7 Conclusion In this paper, we have described our submissions to the STS English task for SemEval-2015. We have introduced a novel deep regression infrastructure with MT ev</context>
</contexts>
<marker>Huang, Chang, 2014</marker>
<rawString>Pingping Huang and Baobao Chang. 2014. SSMT:A Machine Translation Evaluation View To Paragraphto-Sentence Semantic Similarity. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 585–589, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
<author>Michel Galley</author>
<author>Dan Jurafsky</author>
<author>Chris Manning</author>
</authors>
<title>Robust machine translation evaluation with entailment features.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP:</booktitle>
<volume>1</volume>
<pages>297--305</pages>
<marker>Pad´o, Galley, Jurafsky, Manning, 2009</marker>
<rawString>Sebastian Pad´o, Michel Galley, Dan Jurafsky, and Chris Manning. 2009. Robust machine translation evaluation with entailment features. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1, pages 297–305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei jing Zhu</author>
</authors>
<title>BLEU: A Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, Pennsylvania.</location>
<contexts>
<context position="6087" citStr="Papineni et al., 2002" startWordPosition="928" endWordPosition="931">ach metric comprises several features that compute the translation quality by comparing every translation against one or several reference translations. We consider three sets of features: n-gram overlaps, Shallow Parsing metrics and METEOR. These metrics correspond to the lexical, syntactic and semantic levels respectively. 4.1 N-gram Overlaps Gonz`alez et al. (2014) reintroduces the notion of language independent metrics relying on n-gram overlaps. This is similar to the BLEU metric that calculates the geometric mean of n-gram precision by comparing the translation against its reference(s) (Papineni et al., 2002) without the brevity penalty. Different from BLEU, the n-gram overlaps are computed as similarity coefficients instead of taking the crude proportion of overlap n-gram. n-gramoverlap = sim (n -gramtrans ∩ n-gramref) We use 16 features of n-gram overlap by considering both the cosine similarity and Jaccard Index in calculating the n-gram overlaps for character and token n-gram from the order of bigrams to 5-grams. In addition, we use the ratio of n-gram lengths and the Jaccard similarity of pseudo-cognates (Simard et al., 1992) as the 17th and 18th n-gram overlap features. 4.2 Shallow Parsing T</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Wei jing Zhu. 2002. BLEU: A Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian Pedregosa</author>
<author>Alexandre Gramfort Ga¨el Varoquaux</author>
<author>Vincent Michel</author>
<author>Bertrand Thirion</author>
<author>Olivier Grisel</author>
<author>Mathieu Blondel</author>
<author>Peter Prettenhofer</author>
</authors>
<title>Scikit-learn: Machine Learning in Python.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2825</pages>
<location>Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David</location>
<marker>Pedregosa, Ga¨el Varoquaux, Michel, Thirion, Grisel, Blondel, Prettenhofer, 2011</marker>
<rawString>Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and ´Edouard Duchesnay. 2011. Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research, 12:2825–2830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miguel Rios</author>
<author>Wilker Aziz</author>
<author>Lucia Specia</author>
</authors>
<title>UOW: Semantically Informed Text Similarity.</title>
<date>2012</date>
<booktitle>In First Joint Conference on Lexical and Computational Semantics (*SEM): Proceedings of the Main Conference and the Shared Task,</booktitle>
<pages>673--678</pages>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="2088" citStr="Rios et al. (2012)" startWordPosition="301" endWordPosition="304">iversity of Sheffield to the STS English shared task at SemEval2015. We have submitted three models that use Machine Translation (MT) evaluation metrics as features to build supervised regressors that predict the similarity scores for the STS task. We introduce two variants of a novel deep regressor architecture and a classical baseline regression system that uses MT evaluation metrics as input features. 2 Related Work Previously, research teams have applied MT evaluation metrics for the STS task with increasingly better results (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014). Rios et al. (2012) trained a Support Vector Regressor scoring a Pearson correlation mean of 0.3825 (Baseline1: 0.4356). Barr´onCede˜no et al. (2013) also used a Support Vector Regressor and did better than the baseline at 0.4037 mean score (Baseline: 0.3639). Huang and Chang (2014) used a linear regressor and scored 0.792 beating the baseline system (Baseline: 0.613). Another notable mention of MT technology in the STS task is the use of referential translation machines to predict and derive features instead of using MT evaluation metrics (Bic¸ici and van Genabith, 2013; Bic¸ici and Way, 2014). These previous a</context>
</contexts>
<marker>Rios, Aziz, Specia, 2012</marker>
<rawString>Miguel Rios, Wilker Aziz, and Lucia Specia. 2012. UOW: Semantically Informed Text Similarity. In First Joint Conference on Lexical and Computational Semantics (*SEM): Proceedings of the Main Conference and the Shared Task, pages 673–678, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tjong Kim Sang</author>
<author>F Erik</author>
<author>Sabine Buchholz</author>
</authors>
<title>Introduction to the conll-2000 shared task: Chunking.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2Nd Workshop on Learning Language in Logic and the 4th Conference on Computational Natural Language Learning - Volume 7, ConLL ’00,</booktitle>
<pages>127--132</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="7783" citStr="Sang et al., 2000" startWordPosition="1178" endWordPosition="1181">tion are achieved using SVMTool (Gim´enez and M`arquez, 2004). For instance, given a pair of sentences in the format (word/POS/lemma/chunk): • NP(a/DT/a/B-NP dog/NN/dog/I-NP) sprints/VBZ/sprint/B-VP across/IN/across/O NP(the/DET/the/B-NP water/NN/water/I-NP) • NP(a/DT/a/B-NP dog/NN/dog/I-NP) jumps/VBZ/jump/B-VP through/IN/through/O water/NN/water/B-NP We consider the overlap proportions for the POS features, lemma, IOB features, shallow chunks. The Inside, Outside, Begin (IOB) features refer to the shallow parsing tags at the lexical level, e.g. B-NP represents the beginning of a noun phrase (Sang et al., 2000). The IOB features are measured lexically by considering each IOB tag while the shallow chunk features only consider the number of bracketed chunks. For instance, the POS tag DT occurs twice in first sentence one and once in second sentence, thus we extract the feature SP-POS(DT) = 1/2 = 0.5. • SP-POS(DT,NN,VBZ,IN) = [0.5,1,1,1] • SP-LEMMA(a,dog,jump,through,water) = [1,1,0,0,1] • SP-IOB(B-NP,I-NP,B-VP,O) = [1,1,-0.5,1,1] • SP-CHUNK(NP) = [0.5] For SP-POS, SP-LEMMA and SP-IOB, we use the NIST-like measure where we not only consider the individual POS, LEMMA or IOB tags but an accumulated score</context>
</contexts>
<marker>Sang, Erik, Buchholz, 2000</marker>
<rawString>Tjong Kim Sang, Erik F., and Sabine Buchholz. 2000. Introduction to the conll-2000 shared task: Chunking. In Proceedings of the 2Nd Workshop on Learning Language in Logic and the 4th Conference on Computational Natural Language Learning - Volume 7, ConLL ’00, pages 127–132, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Simard</author>
<author>George F Foster</author>
<author>Pierre Isabelle</author>
</authors>
<title>Using Cognates to Align Sentences in Bilingual Corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the Forth International Conference on Theoretical and Methodological Issues in Machine Translation,</booktitle>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="6619" citStr="Simard et al., 1992" startWordPosition="1013" endWordPosition="1016"> precision by comparing the translation against its reference(s) (Papineni et al., 2002) without the brevity penalty. Different from BLEU, the n-gram overlaps are computed as similarity coefficients instead of taking the crude proportion of overlap n-gram. n-gramoverlap = sim (n -gramtrans ∩ n-gramref) We use 16 features of n-gram overlap by considering both the cosine similarity and Jaccard Index in calculating the n-gram overlaps for character and token n-gram from the order of bigrams to 5-grams. In addition, we use the ratio of n-gram lengths and the Jaccard similarity of pseudo-cognates (Simard et al., 1992) as the 17th and 18th n-gram overlap features. 4.2 Shallow Parsing The Shallow Parsing (SP) metric measures the syntactic similarities by computing the overlaps between the translation and the reference translation at the Parts-Of-Speech (POS), word lemmas and base phrase chunks level. The purpose of the SP metric is to capture the proportion of lexical items correctly translated according to their shallow syntactic realization. The base phrase chunks are tagged using the BIOS toolkit (Surdeanu et al., 2005) and POS tag86 ging and lemmatization are achieved using SVMTool (Gim´enez and M`arquez</context>
</contexts>
<marker>Simard, Foster, Isabelle, 1992</marker>
<rawString>Michel Simard, George F. Foster, and Pierre Isabelle. 1992. Using Cognates to Align Sentences in Bilingual Corpora. In Proceedings of the Forth International Conference on Theoretical and Methodological Issues in Machine Translation, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Jordi Turmo</author>
<author>Eli Comelles</author>
</authors>
<title>Named Entity Recognition from Spontaneous OpenDomain Speech.</title>
<date>2005</date>
<booktitle>In Proceedings of the 9th International Conference on Speech Communication and Technology (Interspeech),</booktitle>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="7132" citStr="Surdeanu et al., 2005" startWordPosition="1094" endWordPosition="1097">ition, we use the ratio of n-gram lengths and the Jaccard similarity of pseudo-cognates (Simard et al., 1992) as the 17th and 18th n-gram overlap features. 4.2 Shallow Parsing The Shallow Parsing (SP) metric measures the syntactic similarities by computing the overlaps between the translation and the reference translation at the Parts-Of-Speech (POS), word lemmas and base phrase chunks level. The purpose of the SP metric is to capture the proportion of lexical items correctly translated according to their shallow syntactic realization. The base phrase chunks are tagged using the BIOS toolkit (Surdeanu et al., 2005) and POS tag86 ging and lemmatization are achieved using SVMTool (Gim´enez and M`arquez, 2004). For instance, given a pair of sentences in the format (word/POS/lemma/chunk): • NP(a/DT/a/B-NP dog/NN/dog/I-NP) sprints/VBZ/sprint/B-VP across/IN/across/O NP(the/DET/the/B-NP water/NN/water/I-NP) • NP(a/DT/a/B-NP dog/NN/dog/I-NP) jumps/VBZ/jump/B-VP through/IN/through/O water/NN/water/B-NP We consider the overlap proportions for the POS features, lemma, IOB features, shallow chunks. The Inside, Outside, Begin (IOB) features refer to the shallow parsing tags at the lexical level, e.g. B-NP represents</context>
</contexts>
<marker>Surdeanu, Turmo, Comelles, 2005</marker>
<rawString>Mihai Surdeanu, Jordi Turmo, and Eli Comelles. 2005. Named Entity Recognition from Spontaneous OpenDomain Speech. In Proceedings of the 9th International Conference on Speech Communication and Technology (Interspeech), Lisbon, Portugal.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>