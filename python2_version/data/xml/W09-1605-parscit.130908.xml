<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002343">
<title confidence="0.99211">
Directions for Exploiting Asymmetries in Multilingual Wikipedia
</title>
<author confidence="0.966488">
Elena Filatova
</author>
<affiliation confidence="0.932536">
Computer and Information
</affiliation>
<address confidence="0.513315333333333">
Sciences Department
Fordham University
Bronx, NY 10458, USA
</address>
<email confidence="0.997991">
filatova@cis.fordham.edu
</email>
<sectionHeader confidence="0.995599" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998839833333333">
Multilingual Wikipedia has been used exten-
sively for a variety Natural Language Pro-
cessing (NLP) tasks. Many Wikipedia entries
(people, locations, events, etc.) have descrip-
tions in several languages. These descriptions,
however, are not identical. On the contrary,
descriptions in different languages created for
the same Wikipedia entry can vary greatly in
terms of description length and information
choice. Keeping these peculiarities in mind is
necessary while using multilingual Wikipedia
as a corpus for training and testing NLP ap-
plications. In this paper we present prelimi-
nary results on quantifying Wikipedia multi-
linguality. Our results support the observation
about the substantial variation in descriptions
of Wikipedia entries created in different lan-
guages. However, we believe that asymme-
tries in multilingual Wikipedia do not make
Wikipedia an undesirable corpus for NLP ap-
plications training. On the contrary, we out-
line research directions that can utilize multi-
lingual Wikipedia asymmetries to bridge the
communication gaps in multilingual societies.
</bodyText>
<sectionHeader confidence="0.999335" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999758375">
Multilingual parallel corpora such as translations of
fiction, European parliament proceedings, Canadian
parliament proceedings, the Dutch parallel corpus
are being used for training machine translation and
paraphrase extraction systems. All of these corpora
are parallel corpora.
Parallel corpora contain the same information
translated from one language (the source language
</bodyText>
<page confidence="0.972758">
30
</page>
<bodyText confidence="0.99994790625">
of the text) into a set of pre-specified languages with
the goal of preserving the information covered in
the source language document. Translators work-
ing with fiction also carefully preserve the stylistic
details of the original text.
Parallel corpora are a valuable resource for train-
ing NLP tools. However, they exist only for a small
number of language pairs and usually in a specific
context (e.g., legal documents, parliamentary notes).
Recently NLP community expressed a lot of interest
in studying other types of multilingual corpora.
The largest multilingual corpus known at the mo-
ment is World Wide Web (WWW). One part of par-
ticular interest is the on-line encyclopedia-style site,
Wikipedia.1 Most Wikipedia entries (people, loca-
tions, events, etc.) have descriptions in different lan-
guages. However, Wikipedia is not a parallel cor-
pus as these descriptions are not translations of a
Wikipedia article from one language into another.
Rather, Wikipedia articles in different languages are
independently created by different users.
Wikipedia does not have any filtering on who can
write and edit Wikipedia articles. In contrast to pro-
fessional encyclopedias (like Encyclopedia Britan-
nica), Wikipedia authors and editors are not nec-
essarily experts in the field for which they create
and edit Wikipedia articles. The trustworthiness
of Wikipedia is questioned by many people (Keen,
2007).
The multilinguality of Wikipedia makes this situ-
ation even more convoluted as the sets of Wikipedia
contributors for different languages are not the same.
</bodyText>
<footnote confidence="0.989624">
1http://www.wikipedia.org/
</footnote>
<note confidence="0.937394">
Proceedings of CLIAWS3, Third International Cross Lingual Information Access Workshop, pages 30–37,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999955962962963">
Moreover, these sets might not even intersect. It
is unclear how similar or different descriptions of
a particular Wikipedia entry in different languages
are. Knowing that there are differences in descrip-
tions for the same entry and the ability to identify
these differences is essential for successful commu-
nication in multilingual societies.
In this paper we present a preliminary study of the
asymmetries in a subset of multilingual Wikipedia.
We analyze the number of languages in which the
Wikipedia entry descriptions are created; and the
length variation for the same entry descriptions cre-
ated in different languages. We believe that this in-
formation can be helpful for understanding asymme-
tries in multilingual Wikipedia. These asymmetries,
in turn, can be used by NLP researchers for training
summarization systems, and contradiction detection
systems.
The rest of the paper is structured as follows. In
Section 2 we describe related work, including the
work on utilizing parallel corpora. In Section 3
we provide examples of our analysis for several
Wikipedia entries. In Section 4 we describe our cor-
pus, and the systematic analysis performed on this
corpus. In Section 5 we draw conclusions based on
the collected statistics and outline avenues for our
future research.
</bodyText>
<sectionHeader confidence="0.999694" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999947166666667">
There exist several types of multilingual corpora
(e.g., parallel, comparable) that are used in the NLP
community. These corpora vary in their nature ac-
cording to the tasks for which these corpora were
created.
Corpora developed for multilingual and cross-
lingual question-answering (QA), information re-
trieval (IR), and information extraction (IE) tasks
are typically compilations of documents on related
subjects written in different languages. Documents
in such corpora rarely have counterparts in all the
languages presented in the corpus (CLEF, 2000;
Magnini et al., 2003).
Parallel multilingual corpora such as Canadian
parliament proceedings (Germann, 2001), European
parliament proceedings (Koehn, 2005), the Dutch
parallel corpus (Macken et al., 2007), JRC-ACQUIS
Multilingual Parallel Corpus (Steinberger et al.,
2006), and so on contain documents that are exact
translations of the source documents.
Understanding the corpus nature allows systems
to utilize different aspects of multilingual corpora.
For example, Barzilay et al. (2001) use several trans-
lations of the French text of Gustave Flaubert’s
novel Madame Bovary into English to mine a corpus
of English paraphrases. Thus, they utilize the cre-
ativity and language expertise of professional trans-
lators who used different wordings to convey not
only the meaning but also the stylistic peculiarities
of Flaubert’s French text into English.
Parallel corpora are a valuable resource for train-
ing NLP tools. However, they exist only for a small
number of language pairs and usually in a specific
context (e.g., legal documents, parliamentary notes).
Recently NLP community expressed a lot of inter-
est in studying comparable corpora. Workshops on
building and using comparable corpora have become
a part of NLP conferences (LREC, 2008; ACL,
2009). A comparable corpus is defined as a set of
documents in one to many languages, that are com-
parable in content and form in various degrees and
dimensions.
Wikipedia entries can have descriptions in several
languages independently created for each language.
Thus, Wikipedia can be considered a comparable
corpus.
Wikipedia is used in QA for answer extraction
and verification (Ahn et al., 2005; Buscaldi and
Rosso, 2006; Ko et al., 2007). In summarization,
Wikipedia articles structure is used to learn the fea-
tures for summary generation (Baidsy et al., 2008).
Several NLP systems utilize the Wikipedia multi-
linguality property. Adafre et al. (2006) analyze the
possibility of constructing an English-Dutch parallel
corpus by suggesting two ways of looking for sim-
ilar sentences in Wikipedia pages (using matching
translations and hyperlinks). Richman et al. (2008)
utilize multilingual characteristics of Wikipedia to
annotate a large corpus of text with Named Entity
tags. Multilingual Wikipedia has been used to fa-
cilitate cross-language IR (Sch¨onhofen et al., 2007)
and to perform cross-lingual QA (Ferrindez et al.,
2007).
One of the first attempts to analyze similarities
and differences in multilingual Wikipedia is de-
scribed in Adar et al. (2009) where the main goal
</bodyText>
<page confidence="0.999328">
31
</page>
<bodyText confidence="0.999919">
is to use self-supervised learning to align or/and cre-
ate new Wikipedia infoboxes across four languages
(English, Spanish, French, German). Wikipedia
infoboxes contain a small number of facts about
Wikipedia entries in a semi-structured format.
</bodyText>
<sectionHeader confidence="0.6633435" genericHeader="method">
3 Analysis of Multilingual Wikipedia
Entry Examples
</sectionHeader>
<bodyText confidence="0.9998801">
Wikipedia is a resource generated by collaborative
effort of those who are willing to contribute their ex-
pertise and ideas about a wide variety of subjects.
Wikipedia entries can have descriptions in one or
several languages. Currently, Wikipedia has articles
in more than 200 languages. Table 1 presents infor-
mation about the languages that have the most ar-
ticles in Wikipedia: the number of languages, the
language name, and the Internet Engineering Task
Force (IETF) standard language tag.2
English is the language having the most number
of Wikipedia descriptions, however, this does not
mean that all the Wikipedia entries have descriptions
in English. For example, entries about people, lo-
cations, events, etc. famous or/and important only
within a community speaking in a particular lan-
guage are not likely to have articles in many lan-
guages. Below, we list a few examples that illustrate
this point. Of course, more work is required to quan-
tify the frequency of such entries.
</bodyText>
<listItem confidence="0.996425666666667">
• the Wikipedia entry about Mexican singer and
actress Rocio Banquells has only one descrip-
tion: in Spanish;
• the Wikipedia entry about a mountain ski re-
sort Falakro in northern Greece has descrip-
tions in four languages: Bulgarian, English,
Greek, Nynorsk (one of the two official Nor-
wegian standard languages);
• the Wikipedia entry about Prioksko-Terrasny
Nature Biosphere Reserve, a Russia’s small-
est nature reserve, has descriptions in two lan-
guages: Russian and English;
</listItem>
<bodyText confidence="0.6645428">
2http://en.wikipedia.org/wiki/List_of_
Wikipedias
Wikipedia is changing constantly. All the quotes and examples
from Wikipedia presented and analyzed in this paper were
collected on February 10, 2009, between 14:00 and 21:00 PST.
</bodyText>
<table confidence="0.995675875">
Number or Articles Language IETF Tag
2,750,000+ English en
750,000+ German de
French fr
500,000+ Japanese jp
Polish pl
Italian it
Dutch nl
</table>
<tableCaption confidence="0.9788095">
Table 1: Language editions of Wikipedia by number of
articles.
</tableCaption>
<listItem confidence="0.8109165">
• the Wikipedia entry about a Kazakhstani fig-
ure skater Denis Ten who is of partial Korean
descent has descriptions in four languages: En-
glish, Japanese, Korean, and Russian.
</listItem>
<bodyText confidence="0.999475714285714">
At the same time, Wikipedia entries that are im-
portant or interesting for people from many commu-
nities speaking different languages have articles in
a variety of languages. For example, Newton’s law
of universal gravitation is a fundamental nature law
and has descriptions in 30 languages. Interestingly,
the Wikipedia entry about Isaac Newton who first
formulated the law of universal gravitation and who
is know all over the world has descriptions in 111
different languages.
However, even if a Wikipedia entry has arti-
cles in many languages, the information covered by
these articles can differ substantially. The two main
sources of differences are:
</bodyText>
<listItem confidence="0.9307274">
• the amount of the information covered by the
Wikipedia articles (the length of the Wikipedia
articles);
• the choice of the information covered by the
Wikipedia articles.
</listItem>
<bodyText confidence="0.9995096">
For example, Wikipedia entry about Isadora Dun-
can has descriptions in 44 languages. The length of
the descriptions about Isadora Duncan is different
for every language: 127 sentences for the article in
English; 77 - for French; 37 - for Russian, 1 - for
Greek, etc. The question arises: whether a shorter
article can be considered a summary of a longer arti-
cle, or whether a shorter article might contain infor-
mation that is either not covered in a longer article
or contradicts the information in the longer article.
</bodyText>
<page confidence="0.996449">
32
</page>
<bodyText confidence="0.999934833333333">
Isadora Duncan was a American-born dancer who
was very popular in Europe and was married to a
Russian poet, Sergey Esenin. Certain amount of in-
formation facts (i.e., major biography dates) about
Isadora Duncan are repeated in the articles in ev-
ery language. However, shorter articles are not nec-
essarily summaries of longer articles. For exam-
ple, the article in Russian that is almost four time
shorter than the articles in English, contains infor-
mation that is not covered in the articles written in
English. The same can be noted about articles in
French and Spanish.
In this paper, we analyze the distribution of lan-
guages used in Wikipedia for the list of 48 people in
the DUC 2004 biography generation task. We ana-
lyze, the number of languages that contain articles
for each of the 48 DUC 2004 people. We also ana-
lyze the distribution of the lengths for the descrip-
tions in different languages. We believe that this
statistics is important for the understanding of the
Wikipedia multilinguality nature and can be used by
many NLP applications. Several NLP applications
that can leverage this information are listed in Sec-
tion 5.
</bodyText>
<sectionHeader confidence="0.845875" genericHeader="method">
4 Analysis of Wikipedia Multilinguality
</sectionHeader>
<bodyText confidence="0.999973333333333">
In this paper, we propose a framework to quantify
the multilinguality aspect of Wikipedia. In the cur-
rent work we use a small portion of Wikipedia. Ana-
lyzing only a portion of Wikipedia allows us to com-
pare in detail the multilinguality aspect for all the
Wikipedia entries in our data set.
</bodyText>
<subsectionHeader confidence="0.995584">
4.1 Data Set
</subsectionHeader>
<bodyText confidence="0.999874166666667">
For our analysis, we used the list of people created
for the Task 5 of DUC 2004: biography generation
task (48 people).3
First, we downloaded from Wikipedia all the arti-
cles in all the languages corresponding to each per-
son from the DUC 2004 evaluation set. For our
analysis we used Wikitext, the text that is used by
Wikipedia authors and editors. Wikitext complies
with the wiki markup language and can be pro-
cessed by the Wikimedia content manager system
into HTML which can then be viewed in a browser.
This is the text that can be obtained through the
</bodyText>
<footnote confidence="0.736531">
3http://duc.nist.gov/duc2004/tasks.html/
</footnote>
<bodyText confidence="0.999981863636364">
Wikipedia dumps.4 For our analysis we removed
from the wikitext all the markup tags and tabular in-
formation (e.g., infoboxes and tables) and kept only
plain text. There is no commonly accepted standard
wikitext language, thus our final text had a certain
amount of noise which, however, does not affect the
conclusions drawn from our analysis.
For this work, for each Wikipedia entry (i.e.,
DUC 2004 person) we downloaded the correspond-
ing descriptions in all the languages, including sim-
ple English, Esperanto, Latin, etc. To facilitate the
comparison of descriptions written in different lan-
guages we used the Google machine translation sys-
tem5 to translate the downloaded descriptions into
English. The number of languages currently covered
by the Google translation system (41 language) is
smaller than the number of languages in which there
exist Wikipedia articles (265 languages). However,
we believe that using for cross-lingual analysis de-
scriptions only in those languages that can be han-
dled by the Google translation system does not af-
fect the generality of our conclusions.
</bodyText>
<subsectionHeader confidence="0.984637">
4.2 Data Processing Tools
</subsectionHeader>
<bodyText confidence="0.999942375">
After the Wikipedia descriptions for each person
from the DUC 2004 set were collected and trans-
lated, we divided the description texts into sentences
using the LingPipe sentence chunker (Alias-i, 2009).
We apply sentence splitter only to the English lan-
guage documents: either originally created in En-
glish or translated into English by the Google trans-
lation system.
</bodyText>
<subsectionHeader confidence="0.999526">
4.3 Data Analysis
</subsectionHeader>
<bodyText confidence="0.996786727272727">
As mentioned in Section 1, the goal of the analysis
described in this paper is to quantify the language
diversity in Wikipedia entry descriptions.
We chose English as our reference and, for each
DUC 2004 person, compared a description of this
person in English against the descriptions of this per-
son in other languages.
Language count: In Figure 1, we present infor-
mation about descriptions in how many languages
are created in Wikipedia for each person from the
DUC 2004 set. All the people from the DUC 2004
</bodyText>
<footnote confidence="0.9999375">
4http://download.wikimedia.org/
5http://translate.google.com/
</footnote>
<page confidence="0.998832">
33
</page>
<figureCaption confidence="0.999947">
Figure 1: Number of languages for DUC 2004 people Wikipedia entries.
</figureCaption>
<bodyText confidence="0.990889693877551">
set have descriptions in English. The results in
Figure 1 are presented in sorted order: from the
Wikipedia entries with the largest number of de-
scriptions (languages covered) to the Wikipedia en-
tries with the smallest number of descriptions (lan-
guages covered). Five people from the DUC 2004
set have only one description (English). The per-
son who has descriptions in the most number of
languages for our data set is the former Secretary-
General of the United Nations Kofi Annan (86 lan-
guages). Figure 1 also has information about de-
scriptions in how many languages were translated
into English (handled by the Google translation sys-
tem).
Despite the fact that English is the language hav-
ing descriptions for more Wikipedia entries than any
other language, it does not always provide the great-
est coverage for Wikipedia entries. To show this
we analyzed the length of Wikipedia entry descrip-
tions for the people from the DUC 2004 set. For our
analysis, the length of a description is equal to the
number of sentences in this description. To count
the number of sentences in the uniform way for as
many languages as possible we used translations of
Wikipedia description from languages that are cur-
rently handled by the Google translation system into
English. Those five people from the DUC 2004 set
that have descriptions only in English are excluded
from this analysis. Thus, in the data set for the next
analysis we have 43 data points.
Sentence count: For every Wikipedia entry (per-
son from the DUC 2004 set), we count the length
of the descriptions originally created in English or
translated into English by the Google translation
system. In Figure 2, we present information about
the length of the Wikipedia entity descriptions for
English and for the language other than English
with the maximum description length. The results
in Figure 2 are presented in sorted order: from
the Wikipedia entry with the maximal longest de-
scription in the language other than English to the
Wikipedia entry with the minimal longest descrip-
tion in the language other than English for our data
set. This sorted order does not correspond to the
sorted order from Figure 1. It is interesting so see
that the sorted order in Figure 2 does not correlate
to the length distribution of English descriptions for
our data set.
Obviously, the descriptions in English are not al-
</bodyText>
<page confidence="0.997752">
34
</page>
<figureCaption confidence="0.999708">
Figure 2: Number of sentences in the English description and the longest non-English description.
</figureCaption>
<bodyText confidence="0.999765695652174">
ways the longest ones. To be precise for 17 out of
43 people from the DUC 2004 set, the corresponding
Wikipedia description in English was not the longest
one. In several cases, the length of the description
in English is several times shorter than the length
of the longest (non-English) description. For exam-
ple, the description of G¨unter Grass in German has
251 sentences while his description in English has
74 sentences.
It is safe to assume that longer descriptions
have more information than shorter descriptions
and 17 out of 43 English language descriptions of
Wikipedia entries in our data set can be naturally
extended with the information covered in the de-
scriptions in other languages. Thus, multilingual
Wikipedia gives a straight-forward way of extend-
ing Wikipedia entry descriptions.
It must be noted that the average length of
Wikipedia descriptions (also presented on Figure 2)
is very short. Thus, many descriptions for Wikipedia
entries are quite short. The question arises how well
the information covered in short descriptions corre-
sponds to the information covered in long descrip-
tions.
Correlation Analysis: In this paper, we present
analysis for a small portion of Wikipedia. Currently,
Wikipedia has more than more than 2,750, 000 ar-
ticles in English alone. Thus, the question arises
whether our analysis can be used without loss of
generality for the complete Wikipedia (i.e., all de-
scriptions for all Wikipedia entries).6 To check
this we analyzed the correspondence of how many
Wikipedia entry descriptions are there for each lan-
guage. For the Wikipedia subset corresponding
to the people from the DUC 2004 set we simply
counted how many Wikipedia entries have descrip-
tions in each language. For the complete set of
Wikipedia descriptions we used the Wikipedia size
numbers from the List of Wikipedias page.7 Af-
ter getting the Wikipedia size numbers we kept the
data only for those languages that are used for de-
scriptions of Wikipedia entries corresponding to the
DUC 2004 people.
To compute correlation between these two lists of
numbers we ranked numbers in each of these lists.
The Rank (Spearman) Correlation Coefficient for
</bodyText>
<footnote confidence="0.99450025">
6It must be noted that the notion of complete Wikipedia is
elusive as Wikipedia is changing constantly.
7http://en.wikipedia.org/wiki/List_of_
Wikipedias
</footnote>
<page confidence="0.9992">
35
</page>
<bodyText confidence="0.9996188">
information and contradiction detection. At the
same time, information symmetries in multilingual
Wikipedia can be used for learning summarization
features.
the above two ranked lists is equal to 0.763 which
shows a high correlation between the two ranked
lists. Thus, the preliminary analysis presented in
work can be a good predictor for the descriptions’
length distribution across descriptions in the com-
plete multilingual Wikipedia.
</bodyText>
<sectionHeader confidence="0.7441465" genericHeader="method">
References
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.996673717948718">
In this papers we presented a way of quantify-
ing multilingual aspects of Wikipedia entry descrip-
tions. We showed that despite the fact that English
has descriptions for the most number of Wikipedia
entries across all languages, English descriptions
can not always be considered as the most detailed
descriptions. We showed that for many Wikipedia
entries, descriptions in the languages other than En-
glish are much longer than the corresponding de-
scriptions in English.
Our estimation is that even though Wikipedia en-
try descriptions created in different languages are
not identical, they are likely to contain informa-
tion facts that appear in descriptions in many lan-
guages. One research direction that we are inter-
ested in pursuing is investigating whether the infor-
mation repeated in multiple descriptions of a partic-
ular entry corresponds to the pyramid summariza-
tion model (Teufel and Halteren, 2004; Nenkova et
al., 2007). In case of the positive answer to this
question, multilingual Wikipedia can be used as a
reliable corpus for learning summarization features.
Also, our preliminary analysis shows that
Wikipedia entry descriptions might contain informa-
tion that contradicts information presented in the en-
try descriptions in other languages. Even the choice
of a title for a Wikipedia entry can provide inter-
esting information. For example, the title for the
Wikipedia entry about Former Yugoslav Republic of
Macedonia in English, German, Italian, and many
other languages uses the term Republic of Macedo-
nia or simply Macedonia. However, Greece does not
recognize this name, and thus, the title of the corre-
sponding description in Greek has a complete formal
name of the country: Former Yugoslav Republic of
Macedonia.
Multilingual Wikipedia is full of information
asymmetries. Studying information asymmetries in
multilingual Wikipedia can boost research in new
</bodyText>
<reference confidence="0.946304173913044">
ACL. 2009. Workshop on building and using compara-
ble corpora: from parallel to non-parallel corpora.
Sisay Fissaha Adafre and Maarten de Rijke. 2006. Find-
ing similar sentences across multiple languages in
wikipedia. In Proceedings of the Conference of the
European Chapter of the Association for Computa-
tional Linguistics, Workshop on New Text – Wikis and
blogs and other dynamic text sources, Trento, Italy,
April.
Eytan Adar, Michael Skinner, and Dan Weld. 2009.
Information arbitrage in multi-lingual Wikipedia. In
Proceedings of the Second ACM International Con-
ference on Web Search and Data Mining, Barcelona,
Spain, February.
David Ahn, Valentin Jijkoun, Gilad Mishne, Karin
M¨uller, Maarten de Rijke, and Stefan Schlobach.
2005. Using Wikipedia at the TREC QA track. In
Proceedings of the Text REtrieval Conference (TREC
2004).
Alias-i. 2009. Lingpipe 3.7.0. (accessed January 19,
2009).http://alias-i.com/lingpipe.
Fadi Baidsy, Julia Hirschberg, and Elena Filatova. 2008.
An unsupervised approach to biography production
using wikipedia. In Proceedings of the 46th Annual
Meeting of the Association for Computational Linguis-
tics (ACL-2008), Columbus, OH, USA, July.
Regina Barzilaya and Kathleen McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In Pro-
ceedings ofthe 39th Annual Meeting ofthe Association
for Computational Linguistics (ACL-2001), Toulouse,
France, July.
Davide Buscaldi and Paolo Rosso. 2006. Mining knowl-
edge from wikipedia for the question answering task.
In Proceedings of The Fifth international Conference
on Language Resources and Evaluation (LREC-2006),
Genoa, Italy, May.
CLEF. 2000. Cross-language evaluation forum (CLEF).
http://www.clef-campaign.org.
Sergio Ferrandez, Antonio Toral, ´Oscar Ferrandez, Anto-
nio Ferrandez, and Rafael Munoz. 2007. Applying
Wikipedia’s multilingual knowledge to cross-lingual
question answering. Lecture Notes in Computer Sci-
ence (LNCS): Natural Language Processing and In-
formation Systems, 4592:352–363.
Ulrich Germann. 2001. Aligned hansards of
the 36th parliament of Canada. Website.
</reference>
<page confidence="0.978671">
36
</page>
<reference confidence="0.999289666666667">
http://www.isi.edu/natural-language/
download/hansard/.
Andrew Keen. 2007. The Cult of the Amateur: How
Today’s Internet is Killing Our Culture. Doubleday
Business.
Jeongwoo Ko, Teruko Mitamura, and Eric Nyberg. 2007.
Language-independent probabilistic answer ranking
for multilingual question answering. In Proceed-
ings of the 45th Annual Meeting of the Association
for Computational Linguistics (ACL-2007), Prague,
Czech Republic, June.
Philipp Koehn. 2005. Europarl: A parallel corpus for sta-
tistical machine translation. In Proceedings of the Ma-
chine Translation Summit (MT-2005), Phuket Island,
Thailand, September.
LREC. 2008. Workshop on building and using compara-
ble corpora.
Lieve Macken, Julia Trushkina, and Lidia Rura. 2007.
Dutch Parallel Corpus: MT corpus and translator’s
aid. In Proceedings of the Eleventh Machine Transla-
tion Summit (MT-2007), pages 313–320, Copenhagen,
Denmark, September.
Bernardo Magnini, Simone Romagnoli, and Ro Vallin.
2003. Creating the DISEQuA corpus: A test set
for multilingual question answering. In Proceedings
of the Cross-Lingual Evaluation Forum (CLEF-2003),
Trondheim, Norway, August.
Ani Nenkova, Rebecca Passonneau, and Kathleen McK-
eown. 2007. The Pyramid method: Incorporating hu-
man content selection variation in summarization eval-
uation. ACM Transactions on Speech and Language
Processing, 4(2).
Alexander Richman and Patrick Schone. 2008. Mining
Wiki resources for multilingual named entity recogni-
tion. In Proceedings of the 46th Annual Meeting of
the Association for Computational Linguistics (ACL-
2008), Columbus, OH, USA, July.
P´eter Sch¨onhofen, Andras Bencz´ur, Istv´an Bir´o, and
Karoly Csalogany. 2007. Performing cross-language
retrieval with wikipedia. In Proceedings of the Work-
ing Notes for the CLEF 2007 Workshop, Budapest,
Hungary, September.
Ralf Steinberger, Bruno Pouliquen, Anna Widiger,
Camelia Ignat, Tomaˇz Erjavec, Dan Tufis, and Daniel
Varga. 2006. The JRC-Acquis: A multilingual
aligned parallel corpus with 20+ languages. In Pro-
ceedings of The Fifth international Conference on
Language Resources and Evaluation (LREC-2006),
Genoa, Italy, May.
Simone Teufel and Hans Van Halteren. 2004. Evaluating
information content by factoid analysis: Human anno-
tation and stability. In Proceedings of the 42th Annual
Meeting of the Association for Computational Linguis-
tics (ACL-2004), Barcelona, Spain, July.
</reference>
<page confidence="0.999607">
37
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.495341">
<title confidence="0.979481">Directions for Exploiting Asymmetries in Multilingual Wikipedia</title>
<author confidence="0.912309">Elena</author>
<affiliation confidence="0.728482">Computer and Sciences</affiliation>
<address confidence="0.7091245">Fordham Bronx, NY 10458,</address>
<email confidence="0.999726">filatova@cis.fordham.edu</email>
<abstract confidence="0.99964476">Multilingual Wikipedia has been used extensively for a variety Natural Language Processing (NLP) tasks. Many Wikipedia entries (people, locations, events, etc.) have descriptions in several languages. These descriptions, however, are not identical. On the contrary, descriptions in different languages created for the same Wikipedia entry can vary greatly in terms of description length and information choice. Keeping these peculiarities in mind is necessary while using multilingual Wikipedia as a corpus for training and testing NLP applications. In this paper we present preliminary results on quantifying Wikipedia multilinguality. Our results support the observation about the substantial variation in descriptions of Wikipedia entries created in different languages. However, we believe that asymmetries in multilingual Wikipedia do not make Wikipedia an undesirable corpus for NLP applications training. On the contrary, we outline research directions that can utilize multilingual Wikipedia asymmetries to bridge the communication gaps in multilingual societies.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>ACL</author>
</authors>
<title>Workshop on building and using comparable corpora: from parallel to non-parallel corpora.</title>
<date>2009</date>
<contexts>
<context position="6532" citStr="ACL, 2009" startWordPosition="962" endWordPosition="963">ilize the creativity and language expertise of professional translators who used different wordings to convey not only the meaning but also the stylistic peculiarities of Flaubert’s French text into English. Parallel corpora are a valuable resource for training NLP tools. However, they exist only for a small number of language pairs and usually in a specific context (e.g., legal documents, parliamentary notes). Recently NLP community expressed a lot of interest in studying comparable corpora. Workshops on building and using comparable corpora have become a part of NLP conferences (LREC, 2008; ACL, 2009). A comparable corpus is defined as a set of documents in one to many languages, that are comparable in content and form in various degrees and dimensions. Wikipedia entries can have descriptions in several languages independently created for each language. Thus, Wikipedia can be considered a comparable corpus. Wikipedia is used in QA for answer extraction and verification (Ahn et al., 2005; Buscaldi and Rosso, 2006; Ko et al., 2007). In summarization, Wikipedia articles structure is used to learn the features for summary generation (Baidsy et al., 2008). Several NLP systems utilize the Wikipe</context>
</contexts>
<marker>ACL, 2009</marker>
<rawString>ACL. 2009. Workshop on building and using comparable corpora: from parallel to non-parallel corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sisay Fissaha Adafre</author>
<author>Maarten de Rijke</author>
</authors>
<title>Finding similar sentences across multiple languages in wikipedia.</title>
<date>2006</date>
<booktitle>In Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics, Workshop on New Text – Wikis and</booktitle>
<location>Trento, Italy,</location>
<marker>Adafre, de Rijke, 2006</marker>
<rawString>Sisay Fissaha Adafre and Maarten de Rijke. 2006. Finding similar sentences across multiple languages in wikipedia. In Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics, Workshop on New Text – Wikis and blogs and other dynamic text sources, Trento, Italy, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eytan Adar</author>
<author>Michael Skinner</author>
<author>Dan Weld</author>
</authors>
<title>Information arbitrage in multi-lingual Wikipedia.</title>
<date>2009</date>
<booktitle>In Proceedings of the Second ACM International Conference on Web Search and Data Mining,</booktitle>
<location>Barcelona, Spain,</location>
<contexts>
<context position="7788" citStr="Adar et al. (2009)" startWordPosition="1154" endWordPosition="1157">t al. (2006) analyze the possibility of constructing an English-Dutch parallel corpus by suggesting two ways of looking for similar sentences in Wikipedia pages (using matching translations and hyperlinks). Richman et al. (2008) utilize multilingual characteristics of Wikipedia to annotate a large corpus of text with Named Entity tags. Multilingual Wikipedia has been used to facilitate cross-language IR (Sch¨onhofen et al., 2007) and to perform cross-lingual QA (Ferrindez et al., 2007). One of the first attempts to analyze similarities and differences in multilingual Wikipedia is described in Adar et al. (2009) where the main goal 31 is to use self-supervised learning to align or/and create new Wikipedia infoboxes across four languages (English, Spanish, French, German). Wikipedia infoboxes contain a small number of facts about Wikipedia entries in a semi-structured format. 3 Analysis of Multilingual Wikipedia Entry Examples Wikipedia is a resource generated by collaborative effort of those who are willing to contribute their expertise and ideas about a wide variety of subjects. Wikipedia entries can have descriptions in one or several languages. Currently, Wikipedia has articles in more than 200 la</context>
</contexts>
<marker>Adar, Skinner, Weld, 2009</marker>
<rawString>Eytan Adar, Michael Skinner, and Dan Weld. 2009. Information arbitrage in multi-lingual Wikipedia. In Proceedings of the Second ACM International Conference on Web Search and Data Mining, Barcelona, Spain, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Ahn</author>
<author>Valentin Jijkoun</author>
<author>Gilad Mishne</author>
<author>Karin M¨uller</author>
<author>Maarten de Rijke</author>
<author>Stefan Schlobach</author>
</authors>
<title>Using Wikipedia at the TREC QA track.</title>
<date>2005</date>
<booktitle>In Proceedings of the Text REtrieval Conference (TREC</booktitle>
<marker>Ahn, Jijkoun, Mishne, M¨uller, de Rijke, Schlobach, 2005</marker>
<rawString>David Ahn, Valentin Jijkoun, Gilad Mishne, Karin M¨uller, Maarten de Rijke, and Stefan Schlobach. 2005. Using Wikipedia at the TREC QA track. In Proceedings of the Text REtrieval Conference (TREC 2004).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alias-i</author>
</authors>
<title>Lingpipe 3.7.0. (accessed</title>
<date>2009</date>
<contexts>
<context position="14914" citStr="Alias-i, 2009" startWordPosition="2310" endWordPosition="2311">. The number of languages currently covered by the Google translation system (41 language) is smaller than the number of languages in which there exist Wikipedia articles (265 languages). However, we believe that using for cross-lingual analysis descriptions only in those languages that can be handled by the Google translation system does not affect the generality of our conclusions. 4.2 Data Processing Tools After the Wikipedia descriptions for each person from the DUC 2004 set were collected and translated, we divided the description texts into sentences using the LingPipe sentence chunker (Alias-i, 2009). We apply sentence splitter only to the English language documents: either originally created in English or translated into English by the Google translation system. 4.3 Data Analysis As mentioned in Section 1, the goal of the analysis described in this paper is to quantify the language diversity in Wikipedia entry descriptions. We chose English as our reference and, for each DUC 2004 person, compared a description of this person in English against the descriptions of this person in other languages. Language count: In Figure 1, we present information about descriptions in how many languages a</context>
</contexts>
<marker>Alias-i, 2009</marker>
<rawString>Alias-i. 2009. Lingpipe 3.7.0. (accessed January 19, 2009).http://alias-i.com/lingpipe.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fadi Baidsy</author>
<author>Julia Hirschberg</author>
<author>Elena Filatova</author>
</authors>
<title>An unsupervised approach to biography production using wikipedia.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL-2008),</booktitle>
<location>Columbus, OH, USA,</location>
<contexts>
<context position="7092" citStr="Baidsy et al., 2008" startWordPosition="1050" endWordPosition="1053">ave become a part of NLP conferences (LREC, 2008; ACL, 2009). A comparable corpus is defined as a set of documents in one to many languages, that are comparable in content and form in various degrees and dimensions. Wikipedia entries can have descriptions in several languages independently created for each language. Thus, Wikipedia can be considered a comparable corpus. Wikipedia is used in QA for answer extraction and verification (Ahn et al., 2005; Buscaldi and Rosso, 2006; Ko et al., 2007). In summarization, Wikipedia articles structure is used to learn the features for summary generation (Baidsy et al., 2008). Several NLP systems utilize the Wikipedia multilinguality property. Adafre et al. (2006) analyze the possibility of constructing an English-Dutch parallel corpus by suggesting two ways of looking for similar sentences in Wikipedia pages (using matching translations and hyperlinks). Richman et al. (2008) utilize multilingual characteristics of Wikipedia to annotate a large corpus of text with Named Entity tags. Multilingual Wikipedia has been used to facilitate cross-language IR (Sch¨onhofen et al., 2007) and to perform cross-lingual QA (Ferrindez et al., 2007). One of the first attempts to a</context>
</contexts>
<marker>Baidsy, Hirschberg, Filatova, 2008</marker>
<rawString>Fadi Baidsy, Julia Hirschberg, and Elena Filatova. 2008. An unsupervised approach to biography production using wikipedia. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL-2008), Columbus, OH, USA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilaya</author>
<author>Kathleen McKeown</author>
</authors>
<title>Extracting paraphrases from a parallel corpus.</title>
<date>2001</date>
<booktitle>In Proceedings ofthe 39th Annual Meeting ofthe Association for Computational Linguistics (ACL-2001),</booktitle>
<location>Toulouse, France,</location>
<marker>Barzilaya, McKeown, 2001</marker>
<rawString>Regina Barzilaya and Kathleen McKeown. 2001. Extracting paraphrases from a parallel corpus. In Proceedings ofthe 39th Annual Meeting ofthe Association for Computational Linguistics (ACL-2001), Toulouse, France, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Davide Buscaldi</author>
<author>Paolo Rosso</author>
</authors>
<title>Mining knowledge from wikipedia for the question answering task.</title>
<date>2006</date>
<booktitle>In Proceedings of The Fifth international Conference on Language Resources and Evaluation (LREC-2006),</booktitle>
<location>Genoa, Italy,</location>
<contexts>
<context position="6951" citStr="Buscaldi and Rosso, 2006" startWordPosition="1027" endWordPosition="1030">y notes). Recently NLP community expressed a lot of interest in studying comparable corpora. Workshops on building and using comparable corpora have become a part of NLP conferences (LREC, 2008; ACL, 2009). A comparable corpus is defined as a set of documents in one to many languages, that are comparable in content and form in various degrees and dimensions. Wikipedia entries can have descriptions in several languages independently created for each language. Thus, Wikipedia can be considered a comparable corpus. Wikipedia is used in QA for answer extraction and verification (Ahn et al., 2005; Buscaldi and Rosso, 2006; Ko et al., 2007). In summarization, Wikipedia articles structure is used to learn the features for summary generation (Baidsy et al., 2008). Several NLP systems utilize the Wikipedia multilinguality property. Adafre et al. (2006) analyze the possibility of constructing an English-Dutch parallel corpus by suggesting two ways of looking for similar sentences in Wikipedia pages (using matching translations and hyperlinks). Richman et al. (2008) utilize multilingual characteristics of Wikipedia to annotate a large corpus of text with Named Entity tags. Multilingual Wikipedia has been used to fac</context>
</contexts>
<marker>Buscaldi, Rosso, 2006</marker>
<rawString>Davide Buscaldi and Paolo Rosso. 2006. Mining knowledge from wikipedia for the question answering task. In Proceedings of The Fifth international Conference on Language Resources and Evaluation (LREC-2006), Genoa, Italy, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>CLEF</author>
</authors>
<title>Cross-language evaluation forum</title>
<date>2000</date>
<contexts>
<context position="5275" citStr="CLEF, 2000" startWordPosition="774" endWordPosition="775"> outline avenues for our future research. 2 Related Work There exist several types of multilingual corpora (e.g., parallel, comparable) that are used in the NLP community. These corpora vary in their nature according to the tasks for which these corpora were created. Corpora developed for multilingual and crosslingual question-answering (QA), information retrieval (IR), and information extraction (IE) tasks are typically compilations of documents on related subjects written in different languages. Documents in such corpora rarely have counterparts in all the languages presented in the corpus (CLEF, 2000; Magnini et al., 2003). Parallel multilingual corpora such as Canadian parliament proceedings (Germann, 2001), European parliament proceedings (Koehn, 2005), the Dutch parallel corpus (Macken et al., 2007), JRC-ACQUIS Multilingual Parallel Corpus (Steinberger et al., 2006), and so on contain documents that are exact translations of the source documents. Understanding the corpus nature allows systems to utilize different aspects of multilingual corpora. For example, Barzilay et al. (2001) use several translations of the French text of Gustave Flaubert’s novel Madame Bovary into English to mine</context>
</contexts>
<marker>CLEF, 2000</marker>
<rawString>CLEF. 2000. Cross-language evaluation forum (CLEF). http://www.clef-campaign.org.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergio Ferrandez</author>
<author>Antonio Toral</author>
<author>´Oscar Ferrandez</author>
<author>Antonio Ferrandez</author>
<author>Rafael Munoz</author>
</authors>
<title>Applying Wikipedia’s multilingual knowledge to cross-lingual question answering.</title>
<date>2007</date>
<booktitle>Lecture Notes in Computer Science (LNCS): Natural Language Processing and Information Systems,</booktitle>
<pages>4592--352</pages>
<marker>Ferrandez, Toral, Ferrandez, Ferrandez, Munoz, 2007</marker>
<rawString>Sergio Ferrandez, Antonio Toral, ´Oscar Ferrandez, Antonio Ferrandez, and Rafael Munoz. 2007. Applying Wikipedia’s multilingual knowledge to cross-lingual question answering. Lecture Notes in Computer Science (LNCS): Natural Language Processing and Information Systems, 4592:352–363.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulrich Germann</author>
</authors>
<title>Aligned hansards of the 36th parliament of Canada.</title>
<date>2001</date>
<note>Website. http://www.isi.edu/natural-language/ download/hansard/.</note>
<contexts>
<context position="5385" citStr="Germann, 2001" startWordPosition="788" endWordPosition="789"> (e.g., parallel, comparable) that are used in the NLP community. These corpora vary in their nature according to the tasks for which these corpora were created. Corpora developed for multilingual and crosslingual question-answering (QA), information retrieval (IR), and information extraction (IE) tasks are typically compilations of documents on related subjects written in different languages. Documents in such corpora rarely have counterparts in all the languages presented in the corpus (CLEF, 2000; Magnini et al., 2003). Parallel multilingual corpora such as Canadian parliament proceedings (Germann, 2001), European parliament proceedings (Koehn, 2005), the Dutch parallel corpus (Macken et al., 2007), JRC-ACQUIS Multilingual Parallel Corpus (Steinberger et al., 2006), and so on contain documents that are exact translations of the source documents. Understanding the corpus nature allows systems to utilize different aspects of multilingual corpora. For example, Barzilay et al. (2001) use several translations of the French text of Gustave Flaubert’s novel Madame Bovary into English to mine a corpus of English paraphrases. Thus, they utilize the creativity and language expertise of professional tra</context>
</contexts>
<marker>Germann, 2001</marker>
<rawString>Ulrich Germann. 2001. Aligned hansards of the 36th parliament of Canada. Website. http://www.isi.edu/natural-language/ download/hansard/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Keen</author>
</authors>
<title>The Cult of the Amateur: How Today’s Internet is Killing Our Culture.</title>
<date>2007</date>
<publisher>Doubleday Business.</publisher>
<contexts>
<context position="3061" citStr="Keen, 2007" startWordPosition="443" endWordPosition="444">fferent languages. However, Wikipedia is not a parallel corpus as these descriptions are not translations of a Wikipedia article from one language into another. Rather, Wikipedia articles in different languages are independently created by different users. Wikipedia does not have any filtering on who can write and edit Wikipedia articles. In contrast to professional encyclopedias (like Encyclopedia Britannica), Wikipedia authors and editors are not necessarily experts in the field for which they create and edit Wikipedia articles. The trustworthiness of Wikipedia is questioned by many people (Keen, 2007). The multilinguality of Wikipedia makes this situation even more convoluted as the sets of Wikipedia contributors for different languages are not the same. 1http://www.wikipedia.org/ Proceedings of CLIAWS3, Third International Cross Lingual Information Access Workshop, pages 30–37, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics Moreover, these sets might not even intersect. It is unclear how similar or different descriptions of a particular Wikipedia entry in different languages are. Knowing that there are differences in descriptions for the same entry and the </context>
</contexts>
<marker>Keen, 2007</marker>
<rawString>Andrew Keen. 2007. The Cult of the Amateur: How Today’s Internet is Killing Our Culture. Doubleday Business.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeongwoo Ko</author>
<author>Teruko Mitamura</author>
<author>Eric Nyberg</author>
</authors>
<title>Language-independent probabilistic answer ranking for multilingual question answering.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL-2007),</booktitle>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="6969" citStr="Ko et al., 2007" startWordPosition="1031" endWordPosition="1034">munity expressed a lot of interest in studying comparable corpora. Workshops on building and using comparable corpora have become a part of NLP conferences (LREC, 2008; ACL, 2009). A comparable corpus is defined as a set of documents in one to many languages, that are comparable in content and form in various degrees and dimensions. Wikipedia entries can have descriptions in several languages independently created for each language. Thus, Wikipedia can be considered a comparable corpus. Wikipedia is used in QA for answer extraction and verification (Ahn et al., 2005; Buscaldi and Rosso, 2006; Ko et al., 2007). In summarization, Wikipedia articles structure is used to learn the features for summary generation (Baidsy et al., 2008). Several NLP systems utilize the Wikipedia multilinguality property. Adafre et al. (2006) analyze the possibility of constructing an English-Dutch parallel corpus by suggesting two ways of looking for similar sentences in Wikipedia pages (using matching translations and hyperlinks). Richman et al. (2008) utilize multilingual characteristics of Wikipedia to annotate a large corpus of text with Named Entity tags. Multilingual Wikipedia has been used to facilitate cross-lang</context>
</contexts>
<marker>Ko, Mitamura, Nyberg, 2007</marker>
<rawString>Jeongwoo Ko, Teruko Mitamura, and Eric Nyberg. 2007. Language-independent probabilistic answer ranking for multilingual question answering. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL-2007), Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the Machine Translation Summit (MT-2005),</booktitle>
<location>Phuket Island, Thailand,</location>
<contexts>
<context position="5432" citStr="Koehn, 2005" startWordPosition="793" endWordPosition="794">e NLP community. These corpora vary in their nature according to the tasks for which these corpora were created. Corpora developed for multilingual and crosslingual question-answering (QA), information retrieval (IR), and information extraction (IE) tasks are typically compilations of documents on related subjects written in different languages. Documents in such corpora rarely have counterparts in all the languages presented in the corpus (CLEF, 2000; Magnini et al., 2003). Parallel multilingual corpora such as Canadian parliament proceedings (Germann, 2001), European parliament proceedings (Koehn, 2005), the Dutch parallel corpus (Macken et al., 2007), JRC-ACQUIS Multilingual Parallel Corpus (Steinberger et al., 2006), and so on contain documents that are exact translations of the source documents. Understanding the corpus nature allows systems to utilize different aspects of multilingual corpora. For example, Barzilay et al. (2001) use several translations of the French text of Gustave Flaubert’s novel Madame Bovary into English to mine a corpus of English paraphrases. Thus, they utilize the creativity and language expertise of professional translators who used different wordings to convey </context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Proceedings of the Machine Translation Summit (MT-2005), Phuket Island, Thailand, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>LREC</author>
</authors>
<title>Workshop on building and using comparable corpora.</title>
<date>2008</date>
<contexts>
<context position="6520" citStr="LREC, 2008" startWordPosition="960" endWordPosition="961">hus, they utilize the creativity and language expertise of professional translators who used different wordings to convey not only the meaning but also the stylistic peculiarities of Flaubert’s French text into English. Parallel corpora are a valuable resource for training NLP tools. However, they exist only for a small number of language pairs and usually in a specific context (e.g., legal documents, parliamentary notes). Recently NLP community expressed a lot of interest in studying comparable corpora. Workshops on building and using comparable corpora have become a part of NLP conferences (LREC, 2008; ACL, 2009). A comparable corpus is defined as a set of documents in one to many languages, that are comparable in content and form in various degrees and dimensions. Wikipedia entries can have descriptions in several languages independently created for each language. Thus, Wikipedia can be considered a comparable corpus. Wikipedia is used in QA for answer extraction and verification (Ahn et al., 2005; Buscaldi and Rosso, 2006; Ko et al., 2007). In summarization, Wikipedia articles structure is used to learn the features for summary generation (Baidsy et al., 2008). Several NLP systems utiliz</context>
</contexts>
<marker>LREC, 2008</marker>
<rawString>LREC. 2008. Workshop on building and using comparable corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lieve Macken</author>
<author>Julia Trushkina</author>
<author>Lidia Rura</author>
</authors>
<title>Dutch Parallel Corpus: MT corpus and translator’s aid.</title>
<date>2007</date>
<booktitle>In Proceedings of the Eleventh Machine Translation Summit (MT-2007),</booktitle>
<pages>313--320</pages>
<location>Copenhagen, Denmark,</location>
<contexts>
<context position="5481" citStr="Macken et al., 2007" startWordPosition="799" endWordPosition="802">eir nature according to the tasks for which these corpora were created. Corpora developed for multilingual and crosslingual question-answering (QA), information retrieval (IR), and information extraction (IE) tasks are typically compilations of documents on related subjects written in different languages. Documents in such corpora rarely have counterparts in all the languages presented in the corpus (CLEF, 2000; Magnini et al., 2003). Parallel multilingual corpora such as Canadian parliament proceedings (Germann, 2001), European parliament proceedings (Koehn, 2005), the Dutch parallel corpus (Macken et al., 2007), JRC-ACQUIS Multilingual Parallel Corpus (Steinberger et al., 2006), and so on contain documents that are exact translations of the source documents. Understanding the corpus nature allows systems to utilize different aspects of multilingual corpora. For example, Barzilay et al. (2001) use several translations of the French text of Gustave Flaubert’s novel Madame Bovary into English to mine a corpus of English paraphrases. Thus, they utilize the creativity and language expertise of professional translators who used different wordings to convey not only the meaning but also the stylistic pecul</context>
</contexts>
<marker>Macken, Trushkina, Rura, 2007</marker>
<rawString>Lieve Macken, Julia Trushkina, and Lidia Rura. 2007. Dutch Parallel Corpus: MT corpus and translator’s aid. In Proceedings of the Eleventh Machine Translation Summit (MT-2007), pages 313–320, Copenhagen, Denmark, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernardo Magnini</author>
<author>Simone Romagnoli</author>
<author>Ro Vallin</author>
</authors>
<title>Creating the DISEQuA corpus: A test set for multilingual question answering.</title>
<date>2003</date>
<booktitle>In Proceedings of the Cross-Lingual Evaluation Forum (CLEF-2003),</booktitle>
<location>Trondheim, Norway,</location>
<contexts>
<context position="5298" citStr="Magnini et al., 2003" startWordPosition="776" endWordPosition="779">nues for our future research. 2 Related Work There exist several types of multilingual corpora (e.g., parallel, comparable) that are used in the NLP community. These corpora vary in their nature according to the tasks for which these corpora were created. Corpora developed for multilingual and crosslingual question-answering (QA), information retrieval (IR), and information extraction (IE) tasks are typically compilations of documents on related subjects written in different languages. Documents in such corpora rarely have counterparts in all the languages presented in the corpus (CLEF, 2000; Magnini et al., 2003). Parallel multilingual corpora such as Canadian parliament proceedings (Germann, 2001), European parliament proceedings (Koehn, 2005), the Dutch parallel corpus (Macken et al., 2007), JRC-ACQUIS Multilingual Parallel Corpus (Steinberger et al., 2006), and so on contain documents that are exact translations of the source documents. Understanding the corpus nature allows systems to utilize different aspects of multilingual corpora. For example, Barzilay et al. (2001) use several translations of the French text of Gustave Flaubert’s novel Madame Bovary into English to mine a corpus of English pa</context>
</contexts>
<marker>Magnini, Romagnoli, Vallin, 2003</marker>
<rawString>Bernardo Magnini, Simone Romagnoli, and Ro Vallin. 2003. Creating the DISEQuA corpus: A test set for multilingual question answering. In Proceedings of the Cross-Lingual Evaluation Forum (CLEF-2003), Trondheim, Norway, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Rebecca Passonneau</author>
<author>Kathleen McKeown</author>
</authors>
<title>The Pyramid method: Incorporating human content selection variation in summarization evaluation.</title>
<date>2007</date>
<journal>ACM Transactions on Speech and Language Processing,</journal>
<volume>4</volume>
<issue>2</issue>
<contexts>
<context position="21924" citStr="Nenkova et al., 2007" startWordPosition="3440" endWordPosition="3443">ptions. We showed that for many Wikipedia entries, descriptions in the languages other than English are much longer than the corresponding descriptions in English. Our estimation is that even though Wikipedia entry descriptions created in different languages are not identical, they are likely to contain information facts that appear in descriptions in many languages. One research direction that we are interested in pursuing is investigating whether the information repeated in multiple descriptions of a particular entry corresponds to the pyramid summarization model (Teufel and Halteren, 2004; Nenkova et al., 2007). In case of the positive answer to this question, multilingual Wikipedia can be used as a reliable corpus for learning summarization features. Also, our preliminary analysis shows that Wikipedia entry descriptions might contain information that contradicts information presented in the entry descriptions in other languages. Even the choice of a title for a Wikipedia entry can provide interesting information. For example, the title for the Wikipedia entry about Former Yugoslav Republic of Macedonia in English, German, Italian, and many other languages uses the term Republic of Macedonia or simp</context>
</contexts>
<marker>Nenkova, Passonneau, McKeown, 2007</marker>
<rawString>Ani Nenkova, Rebecca Passonneau, and Kathleen McKeown. 2007. The Pyramid method: Incorporating human content selection variation in summarization evaluation. ACM Transactions on Speech and Language Processing, 4(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Richman</author>
<author>Patrick Schone</author>
</authors>
<title>Mining Wiki resources for multilingual named entity recognition.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL2008),</booktitle>
<location>Columbus, OH, USA,</location>
<marker>Richman, Schone, 2008</marker>
<rawString>Alexander Richman and Patrick Schone. 2008. Mining Wiki resources for multilingual named entity recognition. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL2008), Columbus, OH, USA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P´eter Sch¨onhofen</author>
<author>Andras Bencz´ur</author>
<author>Istv´an Bir´o</author>
<author>Karoly Csalogany</author>
</authors>
<title>Performing cross-language retrieval with wikipedia.</title>
<date>2007</date>
<booktitle>In Proceedings of the Working Notes for the CLEF 2007 Workshop,</booktitle>
<location>Budapest, Hungary,</location>
<marker>Sch¨onhofen, Bencz´ur, Bir´o, Csalogany, 2007</marker>
<rawString>P´eter Sch¨onhofen, Andras Bencz´ur, Istv´an Bir´o, and Karoly Csalogany. 2007. Performing cross-language retrieval with wikipedia. In Proceedings of the Working Notes for the CLEF 2007 Workshop, Budapest, Hungary, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralf Steinberger</author>
<author>Bruno Pouliquen</author>
<author>Anna Widiger</author>
<author>Camelia Ignat</author>
<author>Tomaˇz Erjavec</author>
<author>Dan Tufis</author>
<author>Daniel Varga</author>
</authors>
<title>The JRC-Acquis: A multilingual aligned parallel corpus with 20+ languages.</title>
<date>2006</date>
<booktitle>In Proceedings of The Fifth international Conference on Language Resources and Evaluation (LREC-2006),</booktitle>
<location>Genoa, Italy,</location>
<contexts>
<context position="5549" citStr="Steinberger et al., 2006" startWordPosition="807" endWordPosition="810">created. Corpora developed for multilingual and crosslingual question-answering (QA), information retrieval (IR), and information extraction (IE) tasks are typically compilations of documents on related subjects written in different languages. Documents in such corpora rarely have counterparts in all the languages presented in the corpus (CLEF, 2000; Magnini et al., 2003). Parallel multilingual corpora such as Canadian parliament proceedings (Germann, 2001), European parliament proceedings (Koehn, 2005), the Dutch parallel corpus (Macken et al., 2007), JRC-ACQUIS Multilingual Parallel Corpus (Steinberger et al., 2006), and so on contain documents that are exact translations of the source documents. Understanding the corpus nature allows systems to utilize different aspects of multilingual corpora. For example, Barzilay et al. (2001) use several translations of the French text of Gustave Flaubert’s novel Madame Bovary into English to mine a corpus of English paraphrases. Thus, they utilize the creativity and language expertise of professional translators who used different wordings to convey not only the meaning but also the stylistic peculiarities of Flaubert’s French text into English. Parallel corpora ar</context>
</contexts>
<marker>Steinberger, Pouliquen, Widiger, Ignat, Erjavec, Tufis, Varga, 2006</marker>
<rawString>Ralf Steinberger, Bruno Pouliquen, Anna Widiger, Camelia Ignat, Tomaˇz Erjavec, Dan Tufis, and Daniel Varga. 2006. The JRC-Acquis: A multilingual aligned parallel corpus with 20+ languages. In Proceedings of The Fifth international Conference on Language Resources and Evaluation (LREC-2006), Genoa, Italy, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Teufel</author>
<author>Hans Van Halteren</author>
</authors>
<title>Evaluating information content by factoid analysis: Human annotation and stability.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42th Annual Meeting of the Association for Computational Linguistics (ACL-2004),</booktitle>
<location>Barcelona, Spain,</location>
<marker>Teufel, Van Halteren, 2004</marker>
<rawString>Simone Teufel and Hans Van Halteren. 2004. Evaluating information content by factoid analysis: Human annotation and stability. In Proceedings of the 42th Annual Meeting of the Association for Computational Linguistics (ACL-2004), Barcelona, Spain, July.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>