<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<figure confidence="0.539235">
HEADING WITH A PURPOSE
Michael
Department of Computer
1. iNTRODUCTION
</figure>
<bodyText confidence="0.999413406779661">
A newspaper story about terrorism, war, politics or
footoall is not likely to be read in the same way as a
gothic novel, college catalog or physics textbook.
Similarly, tne process used to understand a casual
conversation is unlikely to be the same as the process
of understanding a biology lecture or TV situation
comedy. One of the primary differences amongst these
various types of comprehension is that the reader or
listener will have different goals in each case. The
reasons a person has for reading, or the goals he has
when engaging in conversation will nave a strong affect
on what he pays attention to, now deeply the input is
processed, and what information is incorporated into
memory. The computer model of understanding described
here addresses the problem of using a reader&apos;s purpose
to assist in natural language understanding. This
program, the Integrated Partial Parser (IPP) is designed
to model the way people read newspaper stories in a
robust, comprehensive, manner. IP? nas a set of
interests, much as a human reader does. At the moment
it concentrates on stories about international violence
and terrorism.
1PP contrasts sharply with many other techniques which
have been used in parsing. Most models of language
processing have had no purpose in reading. They pursue
all inputs with the same diligence and create the same
type of representation for all stories. me key
difference in IPP is that it maps lexical input into as
high a level representation as possible, thereby
performing the complete understanding process. Other
approaches have invariably first tried to create a
preliminary representation, often a strictly syntactic
parse tree, in preparation for real understanding.
Since high-level, semantic representations are
ultimately necessary for understanding, there is no
obvious need for creating a preliminary syntactic
representation, which can be a very difficult task. The
isolation of the lexical level processing from more
complete understanding processes makes it very difficult
for high level predictions to influence low-level
processing, which is crucial In IPP.
One very popular technique for creating a low-level
representation of sentences has been the Augmented
Transition Network (ATN). Parsers of this sort have
been discussed by Woods (11) and Kaplan [3]. An
ATN-like parser was developed by Winograd (10]. Most
ATN parsers nave dealt primarily witn syntax,
occasionally checking a&apos; few simple semantic properties
of words. A more recent parser which does an isolated
syntactic parse was created by Marcus (4]. The
important thing to note about all of these parsers is
that tney view syntactic parsing as a process to be done
prior to real understanding. Even thougn systems of
this sort at times make use of semantic information,
they are driven by syntax. Their goal of developing a
syntactic parse tree i3 not an explicit part of the
purpcse of human understanding.
The type of understanding done by 1PP is in some sense a
compromise between the very detailed understanding of
</bodyText>
<footnote confidence="0.45304525">
This work was supported in part by the Advanced Research
Projects Agency of the Department of Defense and
monitored under the Office of Naval Research under
contract N00014-75-C-1111.
</footnote>
<note confidence="0.670322">
Lebowitz
Science, Yale University
</note>
<bodyText confidence="0.98932203030303">
SAM DI and PAM [9], both of which operated in
conjunction with ELI, Riesbeck&apos;s parser (5I, and the
skimming, highly top-down, style of FRUMP [2). ELI was
a semantically driven parser wnich maps English language
sentences into the Conceptual Dependency [6]
representations of their meanings. It made extensive
use of the semantic properties of the words oeing
processed, but interacted only slightly with the rest of
tne understanding processes it was a part of. it would
pass off a completed Conceptual Dependency
representation of each sentence to SAM or PAM which
would try to incorporate it into an overall story
representation. Both these programs attempted to
understand each sentence fully, SAM in terms of scripts,
PAM in terms of plans and goals, before going onto the
next sentence. (In [8] Scnank and Abelson describe
scripts, plans and goals.) SAM and PAM model the way
people might read a story if they were expecting a
detailed test on it, or tne way a textbook might be
read. Each program&apos;s purpose was to get out of a story
every piece of information possible. iney treated each
piece of every story as being equally important, and
requiring total understanding. Both of these programs
are relatively fragile, requiring complex dictionary
entries for every word they might encounter, as well as
extensive knowledge of tne appropriate scripts and
plans.
FRUMP, in contrast to SAM and PAM, is a robust system
whicn attempts to extract the amount of information from
a newspaper story which a person gets when he skims
rapidly. It does this by selecting a script to
represent the story and then trying to fill in the
various slots which are important to understand the
story. Its purpose is simply to obtain enough
information from a story to produce a meaningful
summary. FRUMP is strongly top-down, and worries about
incoming information from the story only insofar as it
helps fill in the details of the script wnich it
selected. So wnile FRUMP is robust, simply skipping
over words it doesn&apos;t know, it does miss interesting
sections of stories which are not explained by its
initial selection of a script.
IPP attempts to model the way people normally read a
newspaper story. Unlike SAM and PAM, it does not care
if it gets every last piece of information out of a
story. Dull, mundane information is gladly ignored.
But, in contrast with FRUMP, it does not want to miss
interesting parts of stories simply because tney do not
mesh with initial expectations. It tries to create a
representation which captures the important aspects of
each story, but also tries to minimize extensive,
unnecessary processing which does not contribute to the
understanding of the story.
Thus IPP&apos;s purpose is to decide what parts of a story,
if any, are interesting (in IPP&apos;s case, that means
related to terrorism), and Incorporate the appropriate
information into its memory. The concepts used to
determine what is interesting are an extension of ideas
presented by Schenk (7).
2. tat J2Z glaja
The ultimate purpose of reading a newspaper story is to
incorporate new information into memory. In order to do
this, a number of different kinds of knowledge are
needed. The understander must know the meanings of
words, linguistic rules about how words combine into
sentences, the conventions used in writing newspaper
</bodyText>
<page confidence="0.99799">
59
</page>
<bodyText confidence="0.998641840277778">
stories, and, crucially, have extensive knowledge about
the &amp;quot;real world.&amp;quot; It is impossible to properly
understand a story without applying already existing
knowledge about the functioning of the world. This
means the use of long-term memory cannot be fruitfully
separated from other aspects of the natural
understanding problem. The management of all this
information by an understander is a critical problem in
comprehension, since the application of all potentially
relevant knowledge all the time, would seriously degrade
the understanding process, possibly to the point of
halting it altogether. In our model of understanding,
the role played by the interests of the understander is
to allow detailed processing to occur only on the parts
of the story which are important to overall
understanding, thereby conserving processing resources.
Central to any understanding system is the type of
knowledge structure used to represent stories. At the
present time, IP? represents stories in terms of scripts
similar to, although simpler than, those used by SAM and
FRUMP. Most of the common events in IPP&apos;s area of
interest, terrorism, such as hijackings, kidnappings,
and ambushes, are reasonably stereotyped, although not
necessarily with all the temporal sequencing present in
the scripts SAM uses. IPP also represents some events
directly in Conceptual Dependency. The representations
in IPP consist of two types of &apos;structures. There are
the event structures themselves, generally scripts such
as $KIDNAP and $AMBUSH, which form the backbone of the
story representations, and tokens which fill the roles
in the event structures. These tokens are basically the
Picture Producers of [6], and represent the concepts
underlying words such as &amp;quot;airliner,&amp;quot; &amp;quot;machine-gun&amp;quot; and
&amp;quot;kidnapper.&amp;quot; The final story representation can also
include links between event structures indicating
causal, temporal and script-scene relationships.
Due to IPP&apos;s limited repertoire of structures with which
to represent events, it is currently unable to fully
understand some stories which make sense only in terms
of goals and plans, or other higher level
representations. However, the understanding techniques
used in IPP should be applicable to stories which
require the Use of such knowledge structures. This is a
topic of current research.
It is worth noting that the form of a story&apos;s
representation may depend on the purpose behind its
being read. If the reader 13 only mildly interested in
the subject of the story, scriptal representation may
well be adequate. On the other hand, for an story of
great interest to the reader, additional effort may be
expended to allow the goals and plans of the actors in
the story to be Obrked out. This is generally more
complex than simply representing a story in terms of
stereotypical knowledge, and will only be attempted in
cases of great interest.
In order to achieve its purpose, IPP does extensive
&amp;quot;top-down&amp;quot; processing. That is, it makes predictions
about what it is likely to see. These predictions range
from low-level, syntactic predictions (&amp;quot;the next noun
phrase will be the person kidnapped,&amp;quot; for instance) to
quite high-level, global predictions, (&amp;quot;expect to see
demands made by the terrorist&amp;quot;). Significantly, the
program only makes predictions about things it would
like to know. It doesn&apos;t mind skipping over unimportant
parts of the text.
The top-down predictions made by IPP are implemented in
terms of requests, similar to those used by Riesbeck
(5), which are basically just test-action pairs. While
such an implementation in theory allows arbitrary
computations to be performed, the actions used in IPP
are in fact quite limited. IPP requests can build an
event structure, link event structures together, use a
token to fill a role in an event structure, activate new
requests or de-activate other active requests.
The tests in IPP requests are also limited in nature.
They can look for certain types of events or tokens,
check for words with a specified property in their
dictionary entry, or even check for specific lexical
items. The tests for lexical items are quite important
in keeping IPP&apos;s processing efficient. One advantage is
that very specific top-down predictions will often allow
an otherwise very complex word disambiguation process to
be bypassed. For example, in a story about a hijacking,
IPP expects the word &amp;quot;carrying&amp;quot; to indicate that the
passengers of the hijacked vehicle are to follow. So it
never has to consider in any detail the meaning of
&amp;quot;carrying.&amp;quot; Many function words really have no meaning
by themselves, and the type of predictive processing
used by IPP is crucial in handling them efficiently.
Despite its top-down orientation, IP? does not ignore
unexpected input. Rather, if the new information is
interesting in itself the program will concentrate on
it, making new predictions in addition to, or instead
of, the original ones. The proper integration of
top-down and bottom-up processing allows the program to
be efficient, and yet not miss interesting, unexpected
information.
The bottom-up processing of IPP is based around a
classification of words that is done strictly on the
basis of processing considerations. IPP is interested
in the traditional syntactic classifications only when
they help determine how words should be processed.
IPP&apos;s criteria for classification involve the type of
data structures words build, and when they should be
processed.
Words can build either of the main data structures used
in IPP, events and tokens. The words building events
are usually verbs, but many syntactic nouns, such as
&amp;quot;kidnapping,&amp;quot; &amp;quot;riot,&amp;quot; and &amp;quot;demonstration&amp;quot; also indicate
events, and are handled in just the same way as
traditional verbs. Some words, such as most adjectives
and adverbs, do not build structures but rather modify
structures built by other words. These words are
handled according to the type of structure they modify.
The second criteria for classifying words - wnen they
should be processed - is crucial to IPP&apos;s operation. In
order to model a rapid, normally paced reader, IP?
attempts to avoid doing any processing which will not
add to its overall understanding of a story. To do
this, it classifies words into three groups - words
which must be fully processed immediately, words which
should be saved in short-term memory, and then processed
later, if necessary, and words which should be skipped
entirely.
Words which must be processed immediately include
interesting words building either event structures or
tokens. &amp;quot;Gunmen,&amp;quot; &amp;quot;kidnapped&amp;quot; and &amp;quot;exploded&amp;quot; are
typical examples. These words give Us the overall
framework of a story, indicate how much effort should be
devoted to further analysis, and, most importantly,
generate the predictions which allow later processing to
proceed efficiently.
The save and process later words are those which may
become significant later, but are not obviously
important when they are read. This class is quite
substantial, including many dull nouns and nearly all
adjectives and adverbs. In a noun phrase such as
&amp;quot;numerous Italian gunmen,&amp;quot; there is no point in
processing to any depth &amp;quot;numerous&amp;quot; or &amp;quot;Italian&amp;quot; until we
know the word they modify is important enough to be
included in the final representation. In the cases
where further processing is necessary, IPP has the
proper information to easily incorporate the saved words
into the story representation, and in the many cases
</bodyText>
<page confidence="0.992565">
60
</page>
<bodyText confidence="0.999720217391304">
where the word is not important, no effort above saving
the word is required. The processing strategy for these
words is a key to modeling normal reading.
The final class of words are those 1PP skips altogether.
This class includes very uninteresting words which
neither contribute processing clues, nor add to the
story representation. Many function words, adjectives
and verbs irrelevant to the domain at hand, and most
pronouns fall into this category. These words can still
be significant in cases where they are predicted, but
otherwise they are ignored by IPP and take no processing
effort.
In addition to the processing techniques mentioned so
far, IPP makes use of several very pragmatic heuristics.
These are particularly important in processing noun
groups properly. An example of the type of heuristic
used is IPP&apos;s assumption that the first actor in a story
tends to be important, and is worth extra processing
effort. Other heuristics can be seen in the example in
section 4. 1PP&apos;s basic strategy is to make reasonable
guesses about the appropriate representation as quickly
as possible, facilitating later processing and fix
things later if its guesses are prove to be wrong.
</bodyText>
<sectionHeader confidence="0.78129" genericHeader="method">
3. A DETAILED EUMPLti
</sectionHeader>
<bodyText confidence="0.998448571428571">
In order to illustrate how IP? operates, and how its
purpose affects its processing, an annotated run of IPP
on a typical story, one taken from the Boston .0obe is
shown below. Tne text between the rows of stars has
been added to explain the operation of IP?. Items
beginning with a dollar sign, such as $TERRORISM,
indicate scripts used by IP? to represent events.
</bodyText>
<table confidence="0.951702916666667">
[PHOTO: Initiated Sun 24-Jun-79 3:36PM]
ORUN IPP
&apos;(PARSE Si)
Input: Si (3 14 79) IRELAND
(GUNMEN FIRING FROM AMBUSH SERIOUSLY WOUNDED AN
8-YEAR-OLD GIRL AS SHE WAS BEING TAKEN TO SCHOOL
YESTERDAY AT STEWARTSTOWN COUNTY TYRONNE)
Processing:
GUNMEN : Interesting token - GUNMEN
Predictions - SHOOTING-WILL-OCCUR ROBBERY-SCRIPT
TERRORISM-SCRIPT HIJACKING-SCRIPT
GUNMEN is marked in the dictionary as inherently
</table>
<bodyText confidence="0.994372428571429">
interesting. In humans this presumably occurs after a
reader has noted that stories involving gunmen tend to
be interesting. Since it is interesting, IPP fully
processes GUNMEN, knowing that it is important to its
purpose of extracting the significant content of the
story. It builds a token to represent the GUNMEN and
makes several predictions to facilitate later
processing. There is a strong possibility that some
verb conceptually equivalent to &amp;quot;shoot&amp;quot; will appear.
There are also a set of scripts, including $ROBBERY,
$TERRORISM and $HIJACK wnich are likely to appear, so
IPP creates predictions looking for clues indicating
that one of these scripts should be activated and used
to represent the story.
</bodyText>
<equation confidence="0.4756878">
FIRING : Word satisfies prediction
Prediction confirmed - SHOOTING-WILL-OCCUR
Instantiated $SHOOT script
Predictions - $SHOOf-HGL:,.-FINDER REASON-POH-SHOOT1NG
$SHuoT-SCENES
</equation>
<bodyText confidence="0.989598346153846">
FIRING satisfies the prediction for a &amp;quot;shoot&amp;quot; verb.
Notice that the prediction immediately disambiguates
FIRING. Other senses of the word, such as &amp;quot;terminate
employment&amp;quot; are never considered. Once IPP has
confirmed an event, it builds a structure to represent
it, in this case the $SHOOT script and the token for
GUNMEN is filled in as the actor. Predictions are made
trying to find the unknown roles of the script, VICTIM,
in particular, the reason for the shooting, and any
scenes of $SHOOT wnich might be found.
Instantiated $ATTACK-PERSON script
Predictions - $ATTACK-PERSON-ROLE-F1NDEK
$ATTACK-PERSON-SCENES
IPP does not consider the $SHOOT script to be a total
explanation of a snooting event. It requires a
representation wnich indicates the purpose of the
various actors, in the absence of any other
information, IPP assumes people who shoot are
deliberately attacking someone. So the $ATTACK-PERSON
script is inferred, and $SHOOT attacned to it as a
scene. The $ATTACK-PERSON representation allows IPP to
make inferences which are relevant to any case of a
person being attacked, not just snootings. IP? is still
not able to instantiate any of the high level scripts
predicted by GUNMEN, since the $ATTACK-PERSON script is
associated with several of them.
</bodyText>
<equation confidence="0.954986">
FROM : Function word
Predictions - FILL-FROM-SLOT
</equation>
<bodyText confidence="0.998632714285714">
FROM in 3 context such as this normally indicates the
location from which the attack was made is to follow, so
IP? makes a prediction to that effect. However, since a
word building a token does not follow, the prediction is
deactivated. The fact that AMBUSH is syntactically a
noun is not relevant, since IPP&apos;s prediction looks for a
word which identifies a place.
</bodyText>
<equation confidence="0.8513755">
AMBUSH : Scene word
Predictions - $AMBUSH-ROLE-FINDER $AMBUSH-SCENES
Prediction confirmed - TERRORISM-SCRIPT
Instantiated $TERROR1SM script
Predictions - TERRORIST-DEMANDS $TERRORISM-ROLE-FINDER
$TERROR1SM-SCENES COUNTER-MEASURES
</equation>
<bodyText confidence="0.999846705882353">
IPP Knows the word AMBUSH to indicate an instance of the
$AMBUSH script, and that $AMBUSH can be a scene of
$TERRORISM (i.e. it is an activity which can be
construed as a terrorist act). This causes the
prediction made by GUNMEN that $TERRORISM was a possible
script to be triggerred. Even if AMBUSH had other
meanings, or could be associated with other higher level
scripts, the prediction would enable quick, accurate
Identification and incorporation of the word&apos;s meaning
Into the story representation. IPP&apos;s purpose of
associating the shooting with a high level knowledge
structure which helps to explain it, has been achieved.
At this point in the processing an instance of
$TERRORISM is constructed to serve as the top level
representation of the story. The $AMBUSH and
$ATTACK-PERSON scripts are attached as scenes of
$TERNORISM.
</bodyText>
<page confidence="0.998257">
61
</page>
<table confidence="0.7946148">
SERIOUSLY : Skip and save
wOUNDED : Word satisfies prediction
Prediction confirmed - WOUND-SCENE
Predictions - $WOUND-ROLE-FINDER $WOUND-SCENES
Story Representation:
</table>
<figureCaption confidence="0.7779558">
$WOUND is a known scene of $ATTACK-PERSON, representing
a common outcome of an attack. It is instantiated and
attached to $ATTACK-PERSON. IPP infers that the actor
of $WOUND is probably the same as for SATTACK-PERSON,
i.e. the GUNMEN.
</figureCaption>
<table confidence="0.94904525">
AN : Skip and save
8-YEAR-OLD : Skip and save
GIRL : Normal token - GIRL
Prediction confirmed - SWOUND-ROLE-FINDER-VICTIM
</table>
<bodyText confidence="0.9586374">
GIRL builds a token which fills the VICTIM role of the
$WOUND script. Since IPP nas inferred that the VICTIM
of the $ATTACK-PERSON and $SHOOT scripts are the same as
the VICTIM of $WOUND, it also fills in those roles.
Identifying these roles iS integral to IPP&apos;s purpose of
understanding the story, since an attack on a person can
only be properly understood if the victim is known. As
this person is important to the understanding of the
story, IPP wants to acquire as much information as
possible about her. Therefore, it looks back at the
modifiers temporarily saved in short-term memory,
8-YEAH-OLD in this case, and uses them to modify the
token built for GIRL. The age of the girl is noted as
eight years. This information could easily be crucial
to appreciating the interesting nature of the story.
</bodyText>
<figure confidence="0.815218625">
AS Skip
SHE Skip
WAS Skip and save
BEING Dull verb - skipped
TAKEN Skip
TO Function word
SCHOOL Normal token - SCHOOL
YESTERDAY Normal token - YESTERDAY
</figure>
<bodyText confidence="0.994630555555556">
Nothing in this phrase is either inherently interesting
or fulfills expectations made earlier in the processing
of the story. So it is all processed very
superficially, adding nothing to the final
representation. It is important that IPP makes no
attempt to disambiguate words such as TAKEN, an
extremely complex process, since it knows none of the
possible meanings will add significantly to its
understanding.
</bodyText>
<figure confidence="0.98883251724138">
Ile MAIN EVENT &amp;quot;
SCRIPT $TERRORISM
ACTOR GUNMEN
PLACE STEWARTSTOWN COUNTY TYRONNE
TIME YESTERDAY
SCENES
SCRIPT $AMBUSH
ACTOR GUNMEN
SCRIPT
ACTOR
VICTIM
SCENES
SCRIPT
ACTOR
VICTIM
SCRIPT
ACTOR
VICTIM
EXTENT
$ATTACK-PERSON
GUNMEN
8 YEAR OLD GIRL
$SHOOT
GUNMEN
8 YEAR OLD GIRL
$WOUND
GUNMEN
8 YEAR OLD GIRL
GREATERTHAN-INORM*
</figure>
<figureCaption confidence="0.419305">
IPP&apos;s final representation indicates that it has
</figureCaption>
<bodyText confidence="0.91049147826087">
fulfilled its purpose in reading the story. It has
extracted roughly the SSIOS information as a person
reading the story quickly. IPP has recognized an
instance of terrorism consisting of an ambush in which
an eight year-old girl was wounded. That seems to be
about all a person would normally remember from such a
story.
(PHOTO: Terminated Sun 24-Jun-79 3:38PM]
As it processes JI story such SS this one, IPP keeps
track of how interesting it feels the story is. Novelty
and relevance tend to increase interestingness, while
redundancy and irrelevance decrease it. For example, in
the story shown above, the fact that the victim of the
shooting was an 8 year-old increases the interest of the
story, and the the incident taking place in Northern
Ireland as opposed to a more unusual site for terrorism
decreases the interest. The story&apos;s interest is used to
determine how much effort should be expended in trying
to fill in more details of the story. If the level of
interestingness decreases far enough, the program can
stop processing the story, and look for a more
interesting one, in the same way a person does when
reading through a newspaper.
</bodyText>
<sectionHeader confidence="0.995898" genericHeader="method">
4. ANOTHER LAMELE
</sectionHeader>
<bodyText confidence="0.99990325">
The following example further illustrates the
capabilities of IPP. In this example only IPP&apos;s final
story representation is shown. This story was also
taken from the Boston Globe.
</bodyText>
<sectionHeader confidence="0.9334455" genericHeader="method">
(PHOTO: Initiated Wed 27-Jun-79 1:00PM]
@RUN IPP
</sectionHeader>
<equation confidence="0.285617">
6(PARSE S2)
Input: S2 (6 3 79) GUATEMALA
</equation>
<sectionHeader confidence="0.769448" genericHeader="method">
(THE SON OF FORMER PRESIDENT EUGENIO KJELL LAUGERUD
WAS SHOT DEAD BY UNIDENTIFIED ASSAILANTS LAST WEEK
AND A BOMB EXPLODED Al&apos; THE HOME OF A GOVERNMENT
OFFICIAL POLICE SAID)
</sectionHeader>
<table confidence="0.9796772">
AT : Function word
STEW ARTSTOWN : Skip and save
COUNTY : Skip and save
TYRONNE : Normal token - TYRONNE
Prediction confirmed - $TERRORISM-ROLE-FINDER-PLACE
</table>
<bodyText confidence="0.702481666666667">
STEWARTSTOWN COUNTY TYRONNE satisfies the prediction for
the place where the terrorism took place. IPP has
inferred that all the scenes of the event took place at
the same location. IPP expends effort in identifying
this role, as location is crucial to the understanding
of most stories. It is also important in the
organization of memories about stories. A incidence of
terrorism in Northern Ireland is understood differently
from one in New York or Geneva.
</bodyText>
<page confidence="0.995871">
62
</page>
<figure confidence="0.167602">
Story Representation:
1, MAIN Evor
SCRIPT $TERHOR1SM
</figure>
<sectionHeader confidence="0.928913833333333" genericHeader="method">
ACTOR UNKNOWN ASSAILANTS
SCENES
SCRIPT $ATTACK-PERSON
ACTOR UNKNOWN ASSAILANTS
VICTIM SON OF PREVIOUS PRESIDENT
EUGENIO KJELL LAUGERUD
</sectionHeader>
<bodyText confidence="0.999778111111111">
handles sucn sentences, out also accurately processes
stories taken directly from newspapers, which often
involve extremely convoluted syntax, and in many cases
are not grammatical at all. Sentences of this type are
difficult, if not impossible for parsers relying on
syntax. IP? is able to process news stories quickly, on
the order of 2 CPU seconds, and when done, it has
achieved a complete understanding of the story, not just
a syntactic parse.
</bodyText>
<sectionHeader confidence="0.855299333333333" genericHeader="method">
SCENES $SHOOT
SCRIPT UNKNOWN ASSAILANTS
ACTOR SON OF PREVIOUS PRESIDENT
VICTIM EUGENIO KJELL LAUGERUD
SCRIPT $K1LL
ACTOR UNKNOWN ASSAILANTS
VICTIM SON OF PREVIOUS ?RESIDENT
SCRIPT EUGENIO KJELL LAUGEHUD
ACTOR $ATTACK-PLACE
PLACE UNKNOWN ASSAILANTS
SCENES HOME OF GOVERNMENT OFFICIAL
SCRIPT $BOMB
ACTOR UNKNOWN ASSAILANTS
PLACE HOME OF GOVERNMENT OFFICIAL
[PHOTO: Terminated - Wed 27-Jun-79 1:09PM]
</sectionHeader>
<bodyText confidence="0.999878304347826">
As shown in tne examples above, interest can provide a
purpose for reading newspaper stories. In other
situations, other factors might provide the purpose.
But the purpose is never simply to create a
representation - especially a representation with no
semantic content, such as a syntax tree. This is not to
say syntax is not important, obviously in many
circumstances it provides crucial information, but it
should not drive the understanding process. Preliminary
representations are needed only if they assist in the
reader&apos;s ultimate purpose - building an appropriate,
high-level representation which can be incorporated with
already existing knowledge. The results achieved by IPP
indicate that parsing directly into high-level knowledge
structures is possible, and in many situations may well
be more practical than first doing a low-level parse.
Its integrated approacn allows IPP to make use of all
the various kinds of knowledge which people use when
understanding a story.
This example makes several interesting points about the
way IPP operates. Notice that IPP has jumped to a
conclusion about the story,. which, while plausible,
could easily be wrong. It assumes that the actor of the
$BOMB and $ATTACK-PLACE scripts is the same as the actor
of the $TERROHISM script, which was in turn inferred
from the actor of the shooting incident. This is
plausible, as normally news stories are about a coherent
set of events with logical relations amongst them. So
it is reasonable for a story to oe about a series of
related acts of terrorism, committed by the same person
or group, and tnat is what IPP assumes here even though
that may not be correct. dut this kind of inference is
exactly the kind which IPP must make in order to do
efficient top-down processing, despite the possibility
of errors.
The otner interesting point about this example is the
way some of IPP&apos;s quite pragmatic heuristics for
processing give positive results. For instance, as
mentioned earlier, the first actor mentioned has a
strong tendency to be important to the understanding of
a story. In this story that means that the modifying
prepositional phrase &amp;quot;of former President Eugenio Kjell
Laugerud&amp;quot; is analyzed and attached to the token built
for &amp;quot;son,&amp;quot; usually not an interesting word. Heuristics
of this sort give IPP its power and robustness, rather
than any single rule about language understanding.
</bodyText>
<sectionHeader confidence="0.988265" genericHeader="conclusions">
5. CONCLUSION.
</sectionHeader>
<bodyText confidence="0.999939733333333">
IPP has oeen implemented on a DECsystem 20/50 at Yale.
It currently nas a vocabulary of more than 1400 words
which is oeing continually increased in an attempt to
make tne program an expert understander of newspaper
stories about terrorism. It is also planned to add
information about higher level knowledge structures such
as goals and plans and expand IPP&apos;s domain of interest.
To date, IPP nas successfully processed over 50 stories
taken directly from various newspapers, many sight
unseen.
The difference between the powers of IPP and the
syntactically driven parsers mentioned earlier can best
be seen by the kinds of sentences they handle.
Syntax-based parsers generally deal with relatively
simple, syntactically well-formed sentences. IPP
</bodyText>
<sectionHeader confidence="0.844189" genericHeader="references">
Peferences
</sectionHeader>
<reference confidence="0.999318111111111">
(1] Cullingford, R. (1978) Script application:
Computer understanding of newspaper stories.
Research Report 116, Department of Computer
Science, Yale University.
[2] DeJong, G. F. (1979) Skimming stories in real
time: An experiment in integrated understanding.
Research Report 158, Department of Computer
Science, Yale University.
[3] Kaplan, R. M. (1975) On process models for
sentence analysis. In D. A. Norman and
D. E. Rumelhart, eds., Explorations la Coanitim.
W. H. Freeman and Company, San Francisco.
[4] Marcus, M. P. (1979) A Theory of Syntactic
Recognition for Natural Language, in P H .
Winston and H. H. Brown (eds.), Artificial,
Intelligence: An MIT Presnective, MIT Press,
Cambridge, Massachusetts.
[5] Riesbeck, C. K. (1975) Conceptual analysis. In
R. C. Schenk (ed.), Conceptual Information
Processing,. North Holland, Amsterdam.
[6] Schenk, R. C. (1975) Conceptual Informatioa
processing. North Holland, Amsterdam.
[7] Schank, R. C. (1978) Interestingness: Controlling
inferences. Research Report 145, Department of
Computer Science, Yale University.
[8] Schank, R. C. and Abelson, R. P. (1977) Scripts.
plans. Goals Ana Understanding. Lawrence Erlbaum
Associates, Hillsdale, New Jersey.
(9] viiienaky, R. (1978) Understanding goal-based
stories. Research Report 140, Department of
Computer Science, Yale University.
[10] Winograd, T. (1972) Understanding, Batural
Lawman.. Academic Press, New York.
[11] Woods, W. A. (1970) Transition network grammars
for natural language analysis. Communication&apos; oL
laa BO. Vol. 13, p 591.
</reference>
<page confidence="0.999464">
63
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.99718">HEADING WITH A PURPOSE</title>
<author confidence="0.997494">Michael</author>
<affiliation confidence="0.991087">Department of Computer</affiliation>
<abstract confidence="0.999059033898305">A newspaper story about terrorism, war, politics or footoall is not likely to be read in the same way as a gothic novel, college catalog or physics textbook. Similarly, tne process used to understand a casual conversation is unlikely to be the same as the process of understanding a biology lecture or TV situation One of differences amongst these various types of comprehension is that the reader or listener will have different goals in each case. The reasons a person has for reading, or the goals he has when engaging in conversation will nave a strong affect pays attention to, now deeply the input is processed, and what information is incorporated into memory. The computer model of understanding described here addresses the problem of using a reader&apos;s purpose to assist in natural language understanding. This program, the Integrated Partial Parser (IPP) is designed to model the way people read newspaper stories in a robust, comprehensive, manner. IP? nas a set of interests, much as a human reader does. At the moment it concentrates on stories about international violence and terrorism. 1PP contrasts sharply with many other techniques which have been used in parsing. Most models of language processing have had no purpose in reading. They pursue all inputs with the same diligence and create the same type of representation for all stories. me key difference in IPP is that it maps lexical input into as high a level representation as possible, thereby performing the complete understanding process. Other approaches have invariably first tried to create a preliminary representation, often a strictly syntactic parse tree, in preparation for real understanding. Since high-level, semantic representations are ultimately necessary for understanding, there is no obvious need for creating a preliminary syntactic which can be a very difficult task. isolation of the lexical level processing from more complete understanding processes makes it very difficult for high level predictions to influence low-level processing, which is crucial In IPP. One very popular technique for creating a low-level representation of sentences has been the Augmented Transition Network (ATN). Parsers of this sort have been discussed by Woods (11) and Kaplan [3]. An ATN-like parser was developed by Winograd (10]. Most ATN parsers nave dealt primarily witn syntax, occasionally checking a&apos; few simple semantic properties of words. A more recent parser which does an isolated syntactic parse was created by Marcus (4]. The important thing to note about all of these parsers is that tney view syntactic parsing as a process to be done prior to real understanding. Even thougn systems of this sort at times make use of semantic information, they are driven by syntax. Their goal of developing a parse tree an explicit part of the purpcse of human understanding. The type of understanding done by 1PP is in some sense a compromise between the very detailed understanding of</abstract>
<note confidence="0.805857833333333">This work was supported in part by the Advanced Research Projects Agency of the Department of Defense and monitored under the Office of Naval Research under contract N00014-75-C-1111. Lebowitz Science, Yale University</note>
<abstract confidence="0.996000643442624">DI and PAM of which operated in conjunction with ELI, Riesbeck&apos;s parser (5I, and the highly top-down, style of FRUMP was a semantically driven parser wnich maps English language sentences into the Conceptual Dependency [6] representations of their meanings. It made extensive use of the semantic properties of the words oeing processed, but interacted only slightly with the rest of tne understanding processes it was a part of. it would pass off a completed Conceptual Dependency representation of each sentence to SAM or PAM which would try to incorporate it into an overall story representation. Both these programs attempted to understand each sentence fully, SAM in terms of scripts, PAM in terms of plans and goals, before going onto the sentence. (In and Abelson describe scripts, plans and goals.) SAM and PAM model the way people might read a story if they were expecting a test on it, or a textbook might be read. Each program&apos;s purpose was to get out of a story every piece of information possible. iney treated each piece of every story as being equally important, and requiring total understanding. Both of these programs are relatively fragile, requiring complex dictionary entries for every word they might encounter, as well as extensive knowledge of tne appropriate scripts and plans. FRUMP, in contrast to SAM and PAM, is a robust system whicn attempts to extract the amount of information from newspaper story person gets skims rapidly. It does this by selecting a script to represent the story and then trying to fill in the slots important to understand the story. Its purpose is simply to obtain enough information from a story to produce a meaningful summary. FRUMP is strongly top-down, and worries about incoming information from the story only insofar as it helps fill in the details of the script wnich it selected. So wnile FRUMP is robust, simply skipping over words it doesn&apos;t know, it does miss interesting sections of stories which are not explained by its initial selection of a script. IPP attempts to model the way people normally read a newspaper story. Unlike SAM and PAM, it does not care if it gets every last piece of information out of a story. Dull, mundane information is gladly ignored. in contrast with FRUMP, it does not want interesting parts of stories simply because tney do not mesh with initial expectations. It tries to create a representation which captures the important aspects of each story, but also tries to minimize extensive, unnecessary processing which does not contribute to the understanding of the story. IPP&apos;s purpose is to decide what parts story, if any, are interesting (in IPP&apos;s case, that means related to terrorism), and Incorporate the appropriate information into its memory. The concepts used to determine what is interesting are an extension of ideas presented by Schenk (7). tat The ultimate purpose of reading a newspaper story is to incorporate new information into memory. In order to do this, a number of different kinds of knowledge are needed. The understander must know the meanings of words, linguistic rules about how words combine into sentences, the conventions used in writing newspaper 59 stories, and, crucially, have extensive knowledge about the &amp;quot;real world.&amp;quot; It is impossible to properly understand a story without applying already existing knowledge about the functioning of the world. This means the use of long-term memory cannot be fruitfully separated from other aspects of the natural understanding problem. The management of all this information by an understander is a critical problem in comprehension, since the application of all potentially relevant knowledge all the time, would seriously degrade the understanding process, possibly to the point of halting it altogether. In our model of understanding, role played by the interests of the understander to allow detailed processing to occur only on the parts of the story which are important to overall understanding, thereby conserving processing resources. to any understanding system type of knowledge structure used to represent stories. At the present time, IP? represents stories in terms of scripts similar to, although simpler than, those used by SAM and FRUMP. Most of the common events in IPP&apos;s area of interest, terrorism, such as hijackings, kidnappings, and ambushes, are reasonably stereotyped, although not necessarily with all the temporal sequencing present in the scripts SAM uses. IPP also represents some events directly in Conceptual Dependency. The representations in IPP consist of two types of &apos;structures. There are the event structures themselves, generally scripts such as $KIDNAP and $AMBUSH, which form the backbone of the story representations, and tokens which fill the roles in the event structures. These tokens are basically the Producers of represent the concepts underlying words such as &amp;quot;airliner,&amp;quot; &amp;quot;machine-gun&amp;quot; and &amp;quot;kidnapper.&amp;quot; The final story representation can also include links between event structures indicating causal, temporal and script-scene relationships. Due to IPP&apos;s limited repertoire of structures with which to represent events, it is currently unable to fully understand some stories which make sense only in terms of goals and plans, or other higher level representations. However, the understanding techniques used in IPP should be applicable to stories which the such knowledge structures. This is a topic of current research. is noting that the form of a story&apos;s representation may depend on the purpose behind its being read. If the reader 13 only mildly interested in the subject of the story, scriptal representation may On the other hand, for an story of great interest to the reader, additional effort may be expended to allow the goals and plans of the actors in the story to be Obrked out. This is generally more complex than simply representing a story in terms of stereotypical knowledge, and will only be attempted in cases of great interest. In order to achieve its purpose, IPP does extensive processing. That makes predictions what it to see. These predictions range from low-level, syntactic predictions (&amp;quot;the next noun phrase will be the person kidnapped,&amp;quot; for instance) to high-level, global predictions, (&amp;quot;expect to demands made by the terrorist&amp;quot;). Significantly, the program only makes predictions about things it would like to know. It doesn&apos;t mind skipping over unimportant parts of the text. The top-down predictions made by IPP are implemented in terms of requests, similar to those used by Riesbeck (5), which are basically just test-action pairs. While such an implementation in theory allows arbitrary computations to be performed, the actions used in IPP fact limited. IPP requests can build an event structure, link event structures together, use a token to fill a role in an event structure, activate new requests or de-activate other active requests. The tests in IPP requests are also limited in nature. They can look for certain types of events or tokens, check for words with a specified property in their entry, or even check lexical items. The tests for lexical items are quite important in keeping IPP&apos;s processing efficient. One advantage is that very specific top-down predictions will often allow an otherwise very complex word disambiguation process to be bypassed. For example, in a story about a hijacking, IPP expects the word &amp;quot;carrying&amp;quot; to indicate that the passengers of the hijacked vehicle are to follow. So it never has to consider in any detail the meaning of &amp;quot;carrying.&amp;quot; Many function words really have no meaning by themselves, and the type of predictive processing by IPP in handling them efficiently. Despite its top-down orientation, IP? does not ignore unexpected input. Rather, if the new information is interesting in itself the program will concentrate on it, making new predictions in addition to, or instead of, the original ones. The proper integration of top-down and bottom-up processing allows the program to efficient, and yet not unexpected information. bottom-up processing of IPP around a classification of words that is done strictly on the of processing considerations. IPP in the traditional syntactic classifications only when they help determine how words should be processed. IPP&apos;s criteria for classification involve the type of data structures words build, and when they should be processed. Words can build either of the main data structures used in IPP, events and tokens. The words building events are usually verbs, but many syntactic nouns, such as &amp;quot;kidnapping,&amp;quot; &amp;quot;riot,&amp;quot; and &amp;quot;demonstration&amp;quot; also indicate events, and are handled in just the same way as traditional verbs. Some words, such as most adjectives and adverbs, do not build structures but rather modify structures built by other words. These words are handled according to the type of structure they modify. The second criteria for classifying words wnen they be processed to IPP&apos;s operation. In order to model a rapid, normally paced reader, IP? attempts to avoid doing any processing which will not add to its overall understanding of a story. To do this, it classifies words into three groups words which must be fully processed immediately, words which should be saved in short-term memory, and then processed later, if necessary, and words which should be skipped entirely. Words which must be processed immediately include interesting words building either event structures or tokens. &amp;quot;Gunmen,&amp;quot; &amp;quot;kidnapped&amp;quot; and &amp;quot;exploded&amp;quot; are examples. These words give overall framework of a story, indicate how much effort should be devoted to further analysis, and, most importantly, generate the predictions which allow later processing to proceed efficiently. The save and process later words are those which may become significant later, but are not obviously important when they are read. This class is quite substantial, including many dull nouns and nearly all adjectives and adverbs. In a noun phrase such as &amp;quot;numerous Italian gunmen,&amp;quot; there is no point in processing to any depth &amp;quot;numerous&amp;quot; or &amp;quot;Italian&amp;quot; until we know the word they modify is important enough to be included in the final representation. In the cases where further processing is necessary, IPP has the proper information to easily incorporate the saved words into the story representation, and in the many cases 60 where the word is not important, no effort above saving word is required. The processing strategy words is a key to modeling normal reading. The final class of words are those 1PP skips altogether. This class includes very uninteresting words which neither contribute processing clues, nor add to the story representation. Many function words, adjectives and verbs irrelevant to the domain at hand, and most pronouns fall into this category. These words can still be significant in cases where they are predicted, but otherwise they are ignored by IPP and take no processing effort. In addition to the processing techniques mentioned so far, IPP makes use of several very pragmatic heuristics. These are particularly important in processing noun properly. An example of of heuristic used is IPP&apos;s assumption that the first actor in a story tends to be important, and is worth extra processing effort. Other heuristics can be seen in the example in section 4. 1PP&apos;s basic strategy is to make reasonable guesses about the appropriate representation as quickly as possible, facilitating later processing and fix things later if its guesses are prove to be wrong. A EUMPLti In order to illustrate how IP? operates, and how its purpose affects its processing, an annotated run of IPP a typical story, one taken from the Boston .0obeis shown below. Tne text between the rows of stars has been added to explain the operation of IP?. Items beginning with a dollar sign, such as $TERRORISM, indicate scripts used by IP? to represent events. [PHOTO: Initiated Sun 24-Jun-79 3:36PM]</abstract>
<affiliation confidence="0.6792445">ORUN IPP &apos;(PARSE Si)</affiliation>
<address confidence="0.639055">Input: Si (3 14 79) IRELAND</address>
<affiliation confidence="0.852505">GUNMEN FIRING FROM AMBUSH SERIOUSLY WOUNDED AN</affiliation>
<address confidence="0.608536">8-YEAR-OLD GIRL AS SHE WAS BEING TAKEN TO SCHOOL</address>
<affiliation confidence="0.401874">YESTERDAY AT STEWARTSTOWN COUNTY TYRONNE</affiliation>
<title confidence="0.82999575">Processing: GUNMEN : Interesting token - GUNMEN Predictions - SHOOTING-WILL-OCCUR ROBBERY-SCRIPT TERRORISM-SCRIPT HIJACKING-SCRIPT</title>
<abstract confidence="0.979972358974359">GUNMEN is marked in the dictionary as inherently interesting. In humans this presumably occurs after a reader has noted that stories involving gunmen tend to be interesting. Since it is interesting, IPP fully processes GUNMEN, knowing that it is important to its purpose of extracting the significant content of the story. It builds a token to represent the GUNMEN and makes several predictions to facilitate later There is a strong possibility that verb conceptually equivalent to &amp;quot;shoot&amp;quot; will appear. There are also a set of scripts, including $ROBBERY, $TERRORISM and $HIJACK wnich are likely to appear, so IPP creates predictions looking for clues indicating that one of these scripts should be activated and used to represent the story. FIRING : Word satisfies prediction Prediction confirmed - SHOOTING-WILL-OCCUR Instantiated $SHOOT script - REASON-POH-SHOOT1NG $SHuoT-SCENES FIRING satisfies the prediction for a &amp;quot;shoot&amp;quot; verb. Notice that the prediction immediately disambiguates FIRING. Other senses of the word, such as &amp;quot;terminate employment&amp;quot; are never considered. Once IPP has confirmed an event, it builds a structure to represent it, in this case the $SHOOT script and the token for GUNMEN is filled in as the actor. Predictions are made trying to find the unknown roles of the script, VICTIM, in particular, the reason for the shooting, and any scenes of $SHOOT wnich might be found. Instantiated $ATTACK-PERSON script Predictions - $ATTACK-PERSON-ROLE-F1NDEK $ATTACK-PERSON-SCENES IPP does not consider the $SHOOT script to be a total explanation of a snooting event. It requires a representation wnich indicates the purpose of the various actors, in the absence of any other information, IPP assumes people who shoot are deliberately attacking someone. So the $ATTACK-PERSON script is inferred, and $SHOOT attacned to it as a scene. The $ATTACK-PERSON representation allows IPP to make inferences which are relevant to any case of a person being attacked, not just snootings. IP? is still not able to instantiate any of the high level scripts predicted by GUNMEN, since the $ATTACK-PERSON script is associated with several of them. FROM : Function word Predictions - FILL-FROM-SLOT in such as this normally indicates the location from which the attack was made is to follow, so IP? makes a prediction to that effect. However, since a word building a token does not follow, the prediction is deactivated. The fact that AMBUSH is syntactically a noun is not relevant, since IPP&apos;s prediction looks for a word which identifies a place. AMBUSH : Scene word Predictions - $AMBUSH-ROLE-FINDER $AMBUSH-SCENES Prediction confirmed - TERRORISM-SCRIPT Instantiated $TERROR1SM script Predictions - TERRORIST-DEMANDS $TERRORISM-ROLE-FINDER $TERROR1SM-SCENES COUNTER-MEASURES IPP Knows the word AMBUSH to indicate an instance of the $AMBUSH script, and that $AMBUSH can be a scene of $TERRORISM (i.e. it is an activity which can be construed as a terrorist act). This causes the prediction made by GUNMEN that $TERRORISM was a possible script to be triggerred. Even if AMBUSH had other or could with other higher level scripts, the prediction would enable quick, accurate Identification and incorporation of the word&apos;s meaning Into the story representation. IPP&apos;s purpose of associating the shooting with a high level knowledge structure which helps to explain it, has been achieved. At this point in the processing an instance of $TERRORISM is constructed to serve as the top level representation of the story. The $AMBUSH and $ATTACK-PERSON scripts are attached as scenes of $TERNORISM.</abstract>
<date confidence="0.213597">61</date>
<title confidence="0.7450074">SERIOUSLY : Skip and save wOUNDED : Word satisfies prediction Prediction confirmed - WOUND-SCENE Predictions - $WOUND-ROLE-FINDER $WOUND-SCENES Story Representation:</title>
<abstract confidence="0.852040170731707">known scene of $ATTACK-PERSON, representing a common outcome of an attack. It is instantiated and attached to $ATTACK-PERSON. IPP infers that the actor $WOUND is probably the same as i.e. the GUNMEN. AN : Skip and save 8-YEAR-OLD : Skip and save GIRL : Normal token - GIRL Prediction confirmed - SWOUND-ROLE-FINDER-VICTIM GIRL builds a token which fills the VICTIM role of the $WOUND script. Since IPP nas inferred that the VICTIM of the $ATTACK-PERSON and $SHOOT scripts are the same as the VICTIM of $WOUND, it also fills in those roles. these roles to IPP&apos;s purpose of understanding the story, since an attack on a person can be properly understood if the victim is known. this person is important to the understanding of the story, IPP wants to acquire as much information as possible about her. Therefore, it looks back at the modifiers temporarily saved in short-term memory, 8-YEAH-OLD in this case, and uses them to modify the built for GIRL. The age of the girl as eight years. This information could easily be crucial to appreciating the interesting nature of the story. AS Skip SHE Skip WAS Skip and save BEING Dull verb skipped TAKEN Skip TO Function word SCHOOL Normal token - SCHOOL YESTERDAY Normal token - YESTERDAY Nothing in this phrase is either inherently interesting or fulfills expectations made earlier in the processing of the story. So it is all processed very superficially, adding nothing to the final It that IPP makes no attempt to disambiguate words such as TAKEN, an extremely complex process, since it knows none of the possible meanings will add significantly to its understanding.</abstract>
<title confidence="0.92360855">Ile MAIN EVENT &amp;quot; SCRIPT $TERRORISM ACTOR GUNMEN PLACE STEWARTSTOWN COUNTY TYRONNE TIME YESTERDAY SCENES SCRIPT $AMBUSH ACTOR GUNMEN SCRIPT ACTOR VICTIM SCENES SCRIPT ACTOR VICTIM SCRIPT ACTOR VICTIM EXTENT $ATTACK-PERSON</title>
<abstract confidence="0.7939634">GUNMEN 8 YEAR OLD GIRL $SHOOT GUNMEN 8 YEAR OLD GIRL $WOUND GUNMEN 8 YEAR OLD GIRL GREATERTHAN-INORM* IPP&apos;s final representation indicates that it has fulfilled its purpose in reading the story. It has roughly the as a person reading the story quickly. IPP has recognized an instance of terrorism consisting of an ambush in which an eight year-old girl was wounded. That seems to be about all a person would normally remember from such a story. (PHOTO: Terminated Sun 24-Jun-79 3:38PM] processes JI story such this IPP keeps of how interesting it feels the story and relevance tend to increase interestingness, while redundancy and irrelevance decrease it. For example, in the story shown above, the fact that the victim of the was an increases the interest of the story, and the the incident taking place in Northern Ireland as opposed to a more unusual site for terrorism the interest. The story&apos;s interest to determine how much effort should be expended in trying to fill in more details of the story. If the level of interestingness decreases far enough, the program can stop processing the story, and look for a more interesting one, in the same way a person does when reading through a newspaper. ANOTHERLAMELE The following example further illustrates the capabilities of IPP. In this example only IPP&apos;s final representation shown. story was also from the Boston Globe. (PHOTO: Initiated Wed 27-Jun-79 1:00PM] @RUN IPP S2) S2 3 79) (THE SON OF FORMER PRESIDENT EUGENIO KJELL LAUGERUD WAS SHOT DEAD BY UNIDENTIFIED ASSAILANTS LAST WEEK AND A BOMB EXPLODED Al&apos; THE HOME OF A GOVERNMENT OFFICIAL POLICE SAID) AT : Function word STEW ARTSTOWN : Skip and save COUNTY : Skip and save TYRONNE : Normal token - TYRONNE Prediction confirmed - $TERRORISM-ROLE-FINDER-PLACE STEWARTSTOWN COUNTY TYRONNE satisfies the prediction for the place where the terrorism took place. IPP has inferred that all the scenes of the event took place at the same location. IPP expends effort in identifying this role, as location is crucial to the understanding most stories. It important in the organization of memories about stories. A incidence of in Northern Ireland differently from one in New York or Geneva.</abstract>
<note confidence="0.758844333333333">62 Story Representation: 1,MAIN</note>
<title confidence="0.856007">ACTOR UNKNOWN ASSAILANTS SCENES SCRIPT $ATTACK-PERSON ACTOR UNKNOWN ASSAILANTS VICTIM SON OF PREVIOUS PRESIDENT</title>
<author confidence="0.448426">EUGENIO KJELL LAUGERUD</author>
<abstract confidence="0.998800666666667">handles sucn sentences, out also accurately processes stories taken directly from newspapers, which often involve extremely convoluted syntax, and in many cases are not grammatical at all. Sentences of this type are difficult, if not impossible for parsers relying on syntax. IP? is able to process news stories quickly, on the order of 2 CPU seconds, and when done, it has achieved a complete understanding of the story, not just a syntactic parse.</abstract>
<title confidence="0.734414363636364">SCENES SCRIPT ACTOR VICTIM $SHOOT SCRIPT ACTOR VICTIM UNKNOWN ASSAILANTS SCRIPT ACTOR PLACE SCENES SON OF PREVIOUS PRESIDENT EUGENIO KJELL LAUGERUD SCRIPT $K1LL ACTOR UNKNOWN ASSAILANTS PLACE SON OF PREVIOUS ?RESIDENT EUGENIO KJELL LAUGEHUD $ATTACK-PLACE UNKNOWN ASSAILANTS HOME OF GOVERNMENT OFFICIAL $BOMB UNKNOWN ASSAILANTS HOME OF GOVERNMENT OFFICIAL</title>
<abstract confidence="0.969316507692307">[PHOTO: Terminated - Wed 27-Jun-79 1:09PM] As shown in tne examples above, interest can provide a purpose for reading newspaper stories. In other situations, other factors might provide the purpose. But the purpose is never simply to create a representation especially a representation with no semantic content, such as a syntax tree. This is not to say syntax is not important, obviously in many circumstances it provides crucial information, but it should not drive the understanding process. Preliminary representations are needed only if they assist in the reader&apos;s ultimate purpose building an appropriate, high-level representation which can be incorporated with already existing knowledge. The results achieved by IPP indicate that parsing directly into high-level knowledge structures is possible, and in many situations may well be more practical than first doing a low-level parse. Its integrated approacn allows IPP to make use of all the various kinds of knowledge which people use when understanding a story. This example makes several interesting points about the way IPP operates. Notice that IPP has jumped to a conclusion about the story,. which, while plausible, could easily be wrong. It assumes that the actor of the $BOMB and $ATTACK-PLACE scripts is the same as the actor of the $TERROHISM script, which was in turn inferred from the actor of the shooting incident. This is plausible, as normally news stories are about a coherent set of events with logical relations amongst them. So it is reasonable for a story to oe about a series of related acts of terrorism, committed by the same person or group, and tnat is what IPP assumes here even though that may not be correct. dut this kind of inference is exactly the kind which IPP must make in order to do efficient top-down processing, despite the possibility of errors. The otner interesting point about this example is the way some of IPP&apos;s quite pragmatic heuristics for processing give positive results. For instance, as mentioned earlier, the first actor mentioned has a strong tendency to be important to the understanding of a story. In this story that means that the modifying prepositional phrase &amp;quot;of former President Eugenio Kjell Laugerud&amp;quot; is analyzed and attached to the token built for &amp;quot;son,&amp;quot; usually not an interesting word. Heuristics of this sort give IPP its power and robustness, rather than any single rule about language understanding. IPP has oeen implemented on a DECsystem 20/50 at Yale. It currently nas a vocabulary of more than 1400 words which is oeing continually increased in an attempt to make tne program an expert understander of newspaper stories about terrorism. It is also planned to add information about higher level knowledge structures such as goals and plans and expand IPP&apos;s domain of interest. To date, IPP nas successfully processed over 50 stories taken directly from various newspapers, many sight unseen. The difference between the powers of IPP and the syntactically driven parsers mentioned earlier can best be seen by the kinds of sentences they handle. Syntax-based parsers generally deal with relatively simple, syntactically well-formed sentences. IPP Peferences (1] Cullingford, R. (1978) Script application: Computer understanding of newspaper stories.</abstract>
<note confidence="0.946783416666667">Research Report 116, Department of Computer Science, Yale University. DeJong, G. F. stories in real time: An experiment in integrated understanding. Research Report 158, Department of Computer Science, Yale University. [3] Kaplan, R. M. (1975) On process models for sentence analysis. In D. A. Norman and E. Rumelhart, eds., Explorationsla Coanitim. W. H. Freeman and Company, San Francisco. [4] Marcus, M. P. (1979) A Theory of Syntactic Recognition for Natural Language, in P H .</note>
<author confidence="0.411848">H H Brown</author>
<affiliation confidence="0.860191">Intelligence:An Presnective,MIT Press,</affiliation>
<address confidence="0.935278">Cambridge, Massachusetts.</address>
<note confidence="0.9282166">Riesbeck, C. Conceptual In C. Schenk (ed.), Information Processing,.North Holland, Amsterdam. Schenk, R. C. (1975) Informatioa processing.North Holland, Amsterdam. [7] Schank, R. C. (1978) Interestingness: Controlling inferences. Research Report 145, Department of Computer Science, Yale University. Schank, R. C. and Abelson, R. GoalsAna Understanding.Lawrence Erlbaum Associates, Hillsdale, New Jersey. viiienaky, R. Understanding goal-based stories. Research Report 140, Department of Computer Science, Yale University. Winograd, T. (1972) Batural Lawman.. Academic Press, New York. [11] Woods, W. A. (1970) Transition network grammars natural language analysis. Communication&apos;oL Vol. 13, 63</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<title>Script application: Computer understanding of newspaper stories.</title>
<date>1978</date>
<tech>Research Report 116,</tech>
<institution>Department of Computer Science, Yale University.</institution>
<marker>1978</marker>
<rawString> (1] Cullingford, R. (1978) Script application: Computer understanding of newspaper stories. Research Report 116, Department of Computer Science, Yale University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G F DeJong</author>
</authors>
<title>Skimming stories in real time: An experiment in integrated understanding.</title>
<date>1979</date>
<tech>Research Report 158,</tech>
<institution>Department of Computer Science, Yale University.</institution>
<marker>[2]</marker>
<rawString>DeJong, G. F. (1979) Skimming stories in real time: An experiment in integrated understanding. Research Report 158, Department of Computer Science, Yale University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M Kaplan</author>
</authors>
<title>On process models for sentence analysis. In</title>
<date>1975</date>
<booktitle>Explorations la Coanitim. W. H. Freeman and Company,</booktitle>
<editor>D. A. Norman and D. E. Rumelhart, eds.,</editor>
<location>San Francisco.</location>
<contexts>
<context position="2379" citStr="[3]" startWordPosition="371" endWordPosition="371">ng. Since high-level, semantic representations are ultimately necessary for understanding, there is no obvious need for creating a preliminary syntactic representation, which can be a very difficult task. The isolation of the lexical level processing from more complete understanding processes makes it very difficult for high level predictions to influence low-level processing, which is crucial In IPP. One very popular technique for creating a low-level representation of sentences has been the Augmented Transition Network (ATN). Parsers of this sort have been discussed by Woods (11) and Kaplan [3]. An ATN-like parser was developed by Winograd (10]. Most ATN parsers nave dealt primarily witn syntax, occasionally checking a&apos; few simple semantic properties of words. A more recent parser which does an isolated syntactic parse was created by Marcus (4]. The important thing to note about all of these parsers is that tney view syntactic parsing as a process to be done prior to real understanding. Even thougn systems of this sort at times make use of semantic information, they are driven by syntax. Their goal of developing a syntactic parse tree i3 not an explicit part of the purpcse of human </context>
</contexts>
<marker>[3]</marker>
<rawString>Kaplan, R. M. (1975) On process models for sentence analysis. In D. A. Norman and D. E. Rumelhart, eds., Explorations la Coanitim. W. H. Freeman and Company, San Francisco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
</authors>
<title>A Theory of Syntactic Recognition for Natural Language,</title>
<date>1979</date>
<booktitle>Artificial, Intelligence: An MIT Presnective,</booktitle>
<editor>in P H . Winston and H. H. Brown (eds.),</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<marker>[4]</marker>
<rawString>Marcus, M. P. (1979) A Theory of Syntactic Recognition for Natural Language, in P H . Winston and H. H. Brown (eds.), Artificial, Intelligence: An MIT Presnective, MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C K Riesbeck</author>
</authors>
<title>Conceptual analysis. In</title>
<date>1975</date>
<booktitle>Conceptual Information Processing,.</booktitle>
<editor>R. C. Schenk (ed.),</editor>
<publisher>North</publisher>
<location>Holland, Amsterdam.</location>
<marker>[5]</marker>
<rawString>Riesbeck, C. K. (1975) Conceptual analysis. In R. C. Schenk (ed.), Conceptual Information Processing,. North Holland, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Schenk</author>
</authors>
<title>Conceptual Informatioa processing.</title>
<date>1975</date>
<publisher>North</publisher>
<location>Holland, Amsterdam.</location>
<contexts>
<context position="3578" citStr="[6]" startWordPosition="566" endWordPosition="566">man understanding. The type of understanding done by 1PP is in some sense a compromise between the very detailed understanding of This work was supported in part by the Advanced Research Projects Agency of the Department of Defense and monitored under the Office of Naval Research under contract N00014-75-C-1111. Lebowitz Science, Yale University SAM DI and PAM [9], both of which operated in conjunction with ELI, Riesbeck&apos;s parser (5I, and the skimming, highly top-down, style of FRUMP [2). ELI was a semantically driven parser wnich maps English language sentences into the Conceptual Dependency [6] representations of their meanings. It made extensive use of the semantic properties of the words oeing processed, but interacted only slightly with the rest of tne understanding processes it was a part of. it would pass off a completed Conceptual Dependency representation of each sentence to SAM or PAM which would try to incorporate it into an overall story representation. Both these programs attempted to understand each sentence fully, SAM in terms of scripts, PAM in terms of plans and goals, before going onto the next sentence. (In [8] Scnank and Abelson describe scripts, plans and goals.) </context>
<context position="8326" citStr="[6]" startWordPosition="1334" endWordPosition="1334">mmon events in IPP&apos;s area of interest, terrorism, such as hijackings, kidnappings, and ambushes, are reasonably stereotyped, although not necessarily with all the temporal sequencing present in the scripts SAM uses. IPP also represents some events directly in Conceptual Dependency. The representations in IPP consist of two types of &apos;structures. There are the event structures themselves, generally scripts such as $KIDNAP and $AMBUSH, which form the backbone of the story representations, and tokens which fill the roles in the event structures. These tokens are basically the Picture Producers of [6], and represent the concepts underlying words such as &amp;quot;airliner,&amp;quot; &amp;quot;machine-gun&amp;quot; and &amp;quot;kidnapper.&amp;quot; The final story representation can also include links between event structures indicating causal, temporal and script-scene relationships. Due to IPP&apos;s limited repertoire of structures with which to represent events, it is currently unable to fully understand some stories which make sense only in terms of goals and plans, or other higher level representations. However, the understanding techniques used in IPP should be applicable to stories which require the Use of such knowledge structures. This i</context>
</contexts>
<marker>[6]</marker>
<rawString>Schenk, R. C. (1975) Conceptual Informatioa processing. North Holland, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Schank</author>
</authors>
<title>Interestingness: Controlling inferences.</title>
<date>1978</date>
<tech>Research Report 145,</tech>
<institution>Department of Computer Science, Yale University.</institution>
<marker>[7]</marker>
<rawString>Schank, R. C. (1978) Interestingness: Controlling inferences. Research Report 145, Department of Computer Science, Yale University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Schank</author>
<author>R P Abelson</author>
</authors>
<title>Scripts. plans. Goals Ana Understanding. Lawrence Erlbaum Associates,</title>
<date>1977</date>
<tech>Research Report 140,</tech>
<institution>Department of Computer Science, Yale University.</institution>
<location>Hillsdale, New Jersey. (9] viiienaky, R.</location>
<contexts>
<context position="4122" citStr="[8]" startWordPosition="655" endWordPosition="655">glish language sentences into the Conceptual Dependency [6] representations of their meanings. It made extensive use of the semantic properties of the words oeing processed, but interacted only slightly with the rest of tne understanding processes it was a part of. it would pass off a completed Conceptual Dependency representation of each sentence to SAM or PAM which would try to incorporate it into an overall story representation. Both these programs attempted to understand each sentence fully, SAM in terms of scripts, PAM in terms of plans and goals, before going onto the next sentence. (In [8] Scnank and Abelson describe scripts, plans and goals.) SAM and PAM model the way people might read a story if they were expecting a detailed test on it, or tne way a textbook might be read. Each program&apos;s purpose was to get out of a story every piece of information possible. iney treated each piece of every story as being equally important, and requiring total understanding. Both of these programs are relatively fragile, requiring complex dictionary entries for every word they might encounter, as well as extensive knowledge of tne appropriate scripts and plans. FRUMP, in contrast to SAM and P</context>
</contexts>
<marker>[8]</marker>
<rawString>Schank, R. C. and Abelson, R. P. (1977) Scripts. plans. Goals Ana Understanding. Lawrence Erlbaum Associates, Hillsdale, New Jersey. (9] viiienaky, R. (1978) Understanding goal-based stories. Research Report 140, Department of Computer Science, Yale University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Winograd</author>
</authors>
<title>Understanding, Batural Lawman..</title>
<date>1972</date>
<publisher>Academic Press,</publisher>
<location>New York.</location>
<marker>[10]</marker>
<rawString>Winograd, T. (1972) Understanding, Batural Lawman.. Academic Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A Woods</author>
</authors>
<title>Transition network grammars for natural language analysis.</title>
<date>1970</date>
<journal>Communication&apos; oL laa BO.</journal>
<volume>13</volume>
<pages>591</pages>
<marker>[11]</marker>
<rawString>Woods, W. A. (1970) Transition network grammars for natural language analysis. Communication&apos; oL laa BO. Vol. 13, p 591.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>