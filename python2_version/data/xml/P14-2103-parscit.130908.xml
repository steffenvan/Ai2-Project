<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.039757">
<title confidence="0.989903">
Labelling Topics using Unsupervised Graph-based Methods
</title>
<author confidence="0.987856">
Nikolaos Aletras and Mark Stevenson
</author>
<affiliation confidence="0.9979805">
Department of Computer Science
University of Sheffield
</affiliation>
<address confidence="0.947491333333333">
Regent Court, 211 Portobello
Sheffield, S1 4DP
United Kingdom
</address>
<email confidence="0.997089">
{n.aletras, m.stevenson}@dcs.shef.ac.uk
</email>
<sectionHeader confidence="0.997363" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999974923076923">
This paper introduces an unsupervised
graph-based method that selects textual
labels for automatically generated topics.
Our approach uses the topic keywords to
query a search engine and generate a graph
from the words contained in the results.
PageRank is then used to weigh the words
in the graph and score the candidate labels.
The state-of-the-art method for this task is
supervised (Lau et al., 2011). Evaluation
on a standard data set shows that the per-
formance of our approach is consistently
superior to previously reported methods.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999939153846154">
Topic models (Hofmann, 1999; Blei et al., 2003)
have proved to be a useful way to represent the
content of document collections, e.g. (Chaney and
Blei, 2012; Ganguly et al., 2013; Gretarsson et
al., 2012; Hinneburg et al., 2012; Snyder et al.,
2013). In these interfaces, topics need to be pre-
sented to users in an easily interpretable way. A
common way to represent topics is as set of key-
words generated from the n terms with the highest
marginal probabilities. For example, a topic about
the global financial crisis could be represented
by its top 10 most probable terms: FINANCIAL,
BANK, MARKET, GOVERNMENT, MORTGAGE,
BAILOUT, BILLION, STREET, WALL, CRISIS. But
interpreting such lists is not always straightfor-
ward, particularly since background knowledge
may be required (Chang et al., 2009).
Textual labels could assist with the interpre-
tations of topics and researchers have developed
methods to generate these automatically (Mei et
al., 2007; Lau et al., 2010; Lau et al., 2011). For
example, a topic which has keywords SCHOOL,
STUDENT, UNIVERSITY, COLLEGE, TEACHER,
CLASS, EDUCATION, LEARN, HIGH, PROGRAM,
could be labelled as EDUCATION and a suitable la-
bel for the topic shown above would be GLOBAL
FINANCIAL CRISIS. Approaches that make use of
alternative modalities, such as images (Aletras and
Stevenson, 2013), have also been proposed.
Mei et al. (2007) label topics using statistically
significant bigrams identified in a reference collec-
tion. Magatti et al. (2009) introduced an approach
for labelling topics that relied on two hierarchical
knowledge resources labelled by humans, while
Lau et al. (2010) proposed selecting the most rep-
resentative word from a topic as its label. Hulpus
et al. (2013) make use of structured data from DB-
pedia to label topics.
Lau et al. (2011) proposed a method for auto-
matically labelling topics using information from
Wikipedia. A set of candidate labels is gener-
ated from Wikipedia article titles by querying us-
ing topic terms. Additional labels are then gen-
erated by chunk parsing the article titles to iden-
tify n-grams that represent Wikipedia articles as
well. Outlier labels (less relevant to the topic) are
identified and removed. Finally, the top-5 topic
terms are added to the candidate set. The la-
bels are ranked using Support Vector Regression
(SVR) (Vapnik, 1998) and features extracted us-
ing word association measures (i.e. PMI, t-test, χ2
and Dice coefficient), lexical features and search
engine ranking. Lau et al. (2011) report two ver-
sions of their approach, one unsupervised (which
is used as a baseline) and another which is super-
vised. They reported that the supervised version
achieves better performance than a previously re-
ported approach (Mei et al., 2007).
This paper introduces an alternative graph-
based approach which is unsupervised and less
computationally intensive than Lau et al. (2011).
Our method uses topic keywords to form a query.
A graph is generated from the words contained in
the search results and these are then ranked using
the PageRank algorithm (Page et al., 1999; Mihal-
</bodyText>
<page confidence="0.977663">
631
</page>
<bodyText confidence="0.310983333333333">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 631–636,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
{‘Description’: ‘Microsoft will accelerate your journey to cloud computing with an
agile and responsive datacenter built from your existing technology investments.’,
‘DisplayUrl’: ‘www.microsoft.com/en-us/server-cloud/datacenter/virtualization.aspx’,
‘ID’: ‘a42b0908-174e-4f25-b59c-70bdf394a9da’,
</bodyText>
<figure confidence="0.909527333333333">
‘Title’: ‘Microsoft  |Server &amp; Cloud  |Datacenter  |Virtualization ...’,
‘Url’: ‘http://www.microsoft.com/en-us/server-cloud/datacenter/virtualization.aspx’,
... }
</figure>
<figureCaption confidence="0.999987">
Figure 1: Sample of the metadata associated with a search result.
</figureCaption>
<bodyText confidence="0.999883">
cea and Tarau, 2004). Evaluation on a standard
data set shows that our method consistently out-
performs the best performing previously reported
method, which is supervised (Lau et al., 2011).
</bodyText>
<sectionHeader confidence="0.997753" genericHeader="introduction">
2 Methodology
</sectionHeader>
<bodyText confidence="0.999975571428571">
We use the topic keywords to query a search en-
gine. We assume that the search results returned
are relevant to the topic and can be used to identify
and weigh relevant keywords. The most impor-
tant keywords can be used to generate keyphrases
for labelling the topic or weight pre-existing can-
didate labels.
</bodyText>
<subsectionHeader confidence="0.9638625">
2.1 Retrieving and Processing Text
Information
</subsectionHeader>
<bodyText confidence="0.999362647058824">
We use the approach described by Lau et al. (2011)
to generate candidate labels from Wikipedia arti-
cles. The 10 terms with the highest marginal prob-
abilities in the topic are used to query Wikipedia
and the titles of the articles retrieved used as candi-
date labels. Further candidate labels are generated
by processing the titles of these articles to identify
noun chunks and n-grams within the noun chunks
that are themselves the titles of Wikipedia arti-
cles. Outlier labels, identified using a similarity
measure (Grieser et al., 2011), are removed. This
method has been proved to produce labels which
effectively summarise a topic’s main subject.
However, it should be noted that our method is
flexible and could be applied to any set of can-
didate labels. We have experimented with various
approaches to candidate label generation but chose
to report results using the approach described by
Lau et al. (2011) to allow direct comparison of ap-
proaches.
Information obtained from web searches is used
to identify the best labels from the set of candi-
dates. The top n keywords, i.e. those with highest
marginal probability within the topic, are used to
form a query which was submitted to the Bing1
search engine. Textual information included in the
Title field2 of the search results metadata was ex-
tracted. Each title was tokenised using openNLP3
and stop words removed.
Figure 1 shows a sample of the metadata asso-
ciated with a search result for the topic: VMWARE,
SERVER, VIRTUAL, ORACLE, UPDATE, VIRTU-
ALIZATION, APPLICATION, INFRASTRUCTURE,
MANAGEMENT, MICROSOFT.
</bodyText>
<subsectionHeader confidence="0.999694">
2.2 Creating a Text Graph
</subsectionHeader>
<bodyText confidence="0.99992455">
We consider any remaining words in the search
result metadata as nodes, v ∈ V , in a graph
G = (V, E). Each node is connected to its neigh-
bouring words in a context window of ±n words.
In the previous example, the words added to the
graph from the Title of the search result are mi-
crosoft, server, cloud, datacenter and virtualiza-
tion.
We consider both unweighted and weighted
graphs. When the graph is unweighted we assume
that all the edges have a weight e = 1. In addi-
tion, we weight the edges of the graph by comput-
ing the relatedness between two nodes, vi and vj,
as their normalised Pointwise Mutual Information
(NPMI) (Bouma, 2009). Word co-occurrences are
computed using Wikipedia as a a reference cor-
pus. Pairs of words are connected with edges only
if NPMI(wi, wj) &gt; 0.2 avoiding connections be-
tween words co-occurring by chance and hence in-
troducing noise.
</bodyText>
<subsectionHeader confidence="0.990364">
2.3 Identifying Important Terms
</subsectionHeader>
<bodyText confidence="0.999736666666667">
Important terms are identified by applying the
PageRank algorithm (Page et al., 1999) in a sim-
ilar way to the approach used by Mihalcea and
</bodyText>
<footnote confidence="0.99991675">
1http://www.bing.com/
2We also experimented with using the Description field
but found that this reduced performance.
3http://opennlp.apache.org/
</footnote>
<page confidence="0.996048">
632
</page>
<bodyText confidence="0.999418833333333">
Tarau (2004) for document keyphrase extraction.
The PageRank score (Pr) over G for a word (vi)
can be computed by the following equation:
denoting the appropriateness of a label given the
topic. The full data set consists of approximately
6,000 candidate labels (27 labels per topic).
</bodyText>
<equation confidence="0.889224">
+ (1 − d)v (1)
</equation>
<bodyText confidence="0.999996666666667">
where C(vi) denotes the set of vertices which are
connected to the vertex vi. d is the damping factor
which is set to the default value of d = 0.85 (Page
et al., 1999). In standard PageRank all elements
of the vector v are the same, 1 N where N is the
number of nodes in the graph.
</bodyText>
<subsectionHeader confidence="0.998889">
2.4 Ranking Labels
</subsectionHeader>
<bodyText confidence="0.99868725">
Given a candidate label L = {w1, ..., wm} con-
taining m keywords, we compute the score of L
by simply adding the PageRank scores of its con-
stituent keywords:
</bodyText>
<equation confidence="0.996013">
Score(L) = �m Pr(wi) (2)
i=1
</equation>
<bodyText confidence="0.999948">
The label with the highest score amongst the set
of candidates is selected to represent the topic. We
also experimented with normalised versions of the
score, e.g. mean of the PageRank scores. How-
ever, this has a negative effect on performance
since it favoured short labels of one or two words
which were not sufficiently descriptive of the top-
ics. In addition, we expect that candidate labels
containing words that do not appear in the graph
(with the exception of stop words) are unlikely to
be good labels for the topic. In these cases the
score of the candidate label is set to 0. We also
experimented with removing this restriction but
found that it lowered performance.
</bodyText>
<sectionHeader confidence="0.997955" genericHeader="method">
3 Experimental Evaluation
</sectionHeader>
<subsectionHeader confidence="0.965565">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.999955888888889">
We evaluate our method on the publicly avail-
able data set published by Lau et al. (2011). The
data set consists of 228 topics generated using
text documents from four domains, i.e. blog
posts (BLOGS), books (BOOKS), news articles
(NEWS) and scientific articles from the biomedi-
cal domain (PUBMED). Each topic is represented
by its ten most probable keywords. It is also as-
sociated with candidate labels and human ratings
</bodyText>
<subsectionHeader confidence="0.996127">
3.2 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.999732375">
Our evaluation follows the framework proposed
by Lau et al. (2011) using two metrics, i.e. Top-
1 average rating and nDCG, to compare various
labelling methods.
Top-1 average rating is the average human rat-
ing (between 0 and 3) assigned to the top-ranked
label proposed by the system. This provides an in-
dication of the overall quality of the label the sys-
tem judges as the best one.
Normalised discounted cumulative gain
(nDCG) (J¨arvelin and Kek¨al¨ainen, 2002; Croft et
al., 2009) compares the label ranking proposed
by the system to the ranking provided by human
annotators. The discounted cumulative gain
at position p, DCGp, is computed using the
following equation:
</bodyText>
<equation confidence="0.9995875">
reli (3)
log2(i)
</equation>
<bodyText confidence="0.999865">
where reli is the relevance of the label to the topic
in position i. Then nDCG is computed as:
</bodyText>
<equation confidence="0.925955333333333">
DCGp
nDCGp = (4)
IDCGp
</equation>
<bodyText confidence="0.99997175">
where IDCGp is the superviseed ranking of the
image labels, in our experiments this is the rank-
ing provided by the scores in the human annotated
data set.
</bodyText>
<subsectionHeader confidence="0.998315">
3.3 Model Parameters
</subsectionHeader>
<bodyText confidence="0.999983705882353">
Our proposed model requires two parameters to
be set: the context window size when connecting
neighbouring words in the graph and the number
of the search results considered when constructing
the graph.
We experimented with different sizes of context
window, n, between ±1 words to the left and right
and all words in the title. The best results were ob-
tained when n = 2 for all of the domains. In addi-
tion, we experimented with varying the number of
search results between 10 and 300. We observed
no noticeable difference in the performance when
the number of search results is equal or greater
than 30 (see below). We choose to report results
obtained using 30 search results for each topic. In-
cluding more results did not improve performance
but required additional processing.
</bodyText>
<equation confidence="0.990818666666667">
�Pr(vi) = d ·
vj∈C(vi)
v
sim(vj,vk) Pr (j)
sim(vi, vj)
E
vk∈C(vj)
DCGp = rel1 + � p
i=2
</equation>
<page confidence="0.997408">
633
</page>
<table confidence="0.99997544">
Domain Model Top-1 Av. Rating nDCG-1 nDCG-3 nDCG-5
Lau et al. (2011)-U 1.84 0.75 0.77 0.79
Lau et al. (2011)-S 1.98 0.81 0.82 0.83
BLOGS
PR 2.05† 0.83 0.84 0.83
PR-NPMI 2.08† 0.84 0.84 0.83
Upper bound 2.45 1.00 1.00 1.00
Lau et al. (2011)-U 1.75 0.77 0.77 0.79
Lau et al. (2011)-S 1.91 0.84 0.81 0.83
BOOKS
PR 1.98† 0.86 0.88 0.87
PR-NPMI 2.01† 0.87 0.88 0.87
Upper bound 2.29 1.00 1.00 1.00
Lau et al. (2011)-U 1.96 0.80 0.79 0.78
Lau et al. (2011)-S 2.02 0.82 0.82 0.84
NEWS
PR 2.04† 0.83 0.81 0.81
PR-NPMI 2.05† 0.83 0.81 0.81
Upper bound 2.45 1.00 1.00 1.00
Lau et al. (2011)-U 1.73 0.75 0.77 0.79
Lau et al. (2011)-S 1.79 0.77 0.82 0.84
PUBMED
PR 1.88†‡ 0.80 0.80 0.80
PR-NPMI 1.90†‡ 0.81 0.80 0.80
Upper bound 2.31 1.00 1.00 1.00
</table>
<tableCaption confidence="0.8563835">
Table 1: Results for Various Approaches to Topic Labelling (†: significant difference (t-test, p &lt; 0.05)
to Lau et al. (2011)-U; ‡: significant difference (p &lt; 0.05) to Lau et al. (2011)-S).
</tableCaption>
<sectionHeader confidence="0.999596" genericHeader="evaluation">
4 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999870604651163">
Results are shown in Table 1. Performance when
PageRank is applied to the unweighted (PR) and
NPMI-weighted graphs (PR-NPMI) (see Section
2.2) is shown. Performance of the best unsuper-
vised (Lau et al. (2011)-U) and supervised (Lau
et al. (2011)-S) methods reported by Lau et al.
(2011) are shown. Lau et al. (2011)-U uses the av-
erage x2 scores between the topic keywords and
the label keywords while Lau et al. (2011)-S uses
SVR to combine evidence from all features. In
addition, upper bound figures, the maximum pos-
sible value given the scores assigned by the anno-
tators, are also shown.
The results obtained by applying PageRank
over the unweighted graph (2.05, 1.98, 2.04 and
1.88) are consistently better than the supervised
and unsupervised methods reported by Lau et al.
(2011) for the Top-1 Average scores and this im-
provement is observed in all domains. The differ-
ence is significant (t-test, p &lt; 0.05) for the un-
supervised method. A slight improvement in per-
formance is observed when the weighted graph is
used (2.08, 2.01, 2.05 and 1.90). This is expected
since the weighted graph contains additional in-
formation about word relatedness. For example,
the word hardware is more related and, therefore,
closer in the graph to the word virtualization than
to the word investments.
Results from the nDCG metric imply that our
methods provide better rankings of the candidate
labels in the majority of the cases. It is outper-
formed by the best supervised approach in two do-
mains, NEWS and PUBMED, using the nDCG-
3 and nDCG-5 metrics. However, the best label
proposed by our methods is judged to be better
(as shown by the nDCG-1 and Top-1 Av. Rat-
ing scores), demonstrating that it is only the lower
ranked labels in our approach that are not as good
as the supervised approach.
An interesting finding is that, although limited
in length, the textual information in the search re-
sult’s metadata contain enough salient terms rel-
evant to the topic to provide reliable estimates of
</bodyText>
<page confidence="0.992759">
634
</page>
<figure confidence="0.992423871794872">
Top-1 Av. Rating 2.2
2.1
2
1.9
1.8
1.7
2.2
2.1
1.9
1.8
1.7
2
50 100 150 200 250 300
Number of Search Results
(a) BLOGS
50 100 150 200 250 300
Number of Search Results
(b) BOOKS
Top-1 Av. Rating
2.2
2.1
1.9
1.8
1.7
2
50 100 150 200 250 300
2.2
2.1
1.9
1.8
1.7
2
50 100 150 200 250 300
Lau et al. (2011)-U
Lau et al. (2011)-S
PR
PR-NPMI
Number of Search Results Number of Search Results
(c) NEWS (d) PUBMED
</figure>
<figureCaption confidence="0.999873">
Figure 2: Top-1 Average Rating obtained for different number of search results.
</figureCaption>
<bodyText confidence="0.99938728">
term importance. Consequently, it is not necessary
to measure semantic similarity between topic key-
words and candidate labels as previous approaches
have done. In addition, performance improvement
gained from using the weighted graph is mod-
est, suggesting that the computation of association
scores over a large reference corpus could be omit-
ted if resources are limited.
In Figure 2, we show the scores of Top-1 av-
erage rating obtained in the different domains by
experimenting with the number of search results
used to generate the text graph. The most inter-
esting finding is that performance is stable when
30 or more search results are considered. In addi-
tion, we observe that quality of the topic labels in
the four domains remains stable, and higher than
the supervised method, when the number of search
results used is between 150 and 200. The only
domain in which performance of the supervised
method is sometimes better than the approach pro-
posed here is NEWS. The main reason is that news
topics are more fine grained and the candidate
labels of better quality (Lau et al., 2011) which
has direct impact in good performance of ranking
methods.
</bodyText>
<sectionHeader confidence="0.999449" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999917">
We described an unsupervised graph-based
method to associate textual labels with automati-
cally generated topics. Our approach uses results
retrieved from a search engine using the topic
keywords as a query. A graph is generated from
the words contained in the search results metadata
and candidate labels ranked using the PageRank
algorithm. Evaluation on a standard data set
shows that our method consistently outperforms
the supervised state-of-the-art method for the task.
</bodyText>
<sectionHeader confidence="0.998812" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9998546">
We would like to thank Jey Han Lau for providing
us with the labels selected by Lau et al. (2011)-
U and Lau et al. (2011)-S. We also thank Daniel
Preoiiuc-Pietro for his useful comments on early
drafts of this paper.
</bodyText>
<page confidence="0.998672">
635
</page>
<sectionHeader confidence="0.996075" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99931082">
Nikolaos Aletras and Mark Stevenson. 2013. Rep-
resenting topics using images. In Proceedings of
the 2013 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 158–167, At-
lanta, Georgia.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993–1022.
Gerlof Bouma. 2009. Normalized (pointwise) mutual
information in collocation extraction. In Proceed-
ings of GSCL.
Allison June-Barlow Chaney and David M. Blei. 2012.
Visualizing topic models. In Proceedings of the
Sixth International AAAI Conference on Weblogs
and Social Media, Dublin, Ireland.
Jonathan Chang, Jordan Boyd-Graber, and Sean Ger-
rish. 2009. Reading Tea Leaves: How Humans In-
terpret Topic Models. Neural Information, pages 1–
9.
Bruce W. Croft, Donald Metzler, and Trevor Strohman.
2009. Search engines: Information retrieval in
practice. Addison-Wesley.
Debasis Ganguly, Manisha Ganguly, Johannes Level-
ing, and Gareth J.F. Jones. 2013. TopicVis: A GUI
for Topic-based feedback and navigation. In Pro-
ceedings of the Thirty-Sixth Annual International
ACM SIGIR Conference on Research and Develop-
ment in Information Retrieval (SIGIR 13), Dublin,
Ireland.
Brynjar Gretarsson, John O’Donovan, Svetlin Bostand-
jiev, Tobias H¨ollerer, Arthur Asuncion, David New-
man, and Padhraic Smyth. 2012. TopicNets: Visual
analysis of large text corpora with topic modeling.
ACM Trans. Intell. Syst. Technol., 3(2):23:1–23:26.
Karl Grieser, Timothy Baldwin, Fabian Bohnert, and
Liz Sonenberg. 2011. Using Ontological and Doc-
ument Similarity to Estimate Museum Exhibit Re-
latedness. Journal on Computing and Cultural Her-
itage (JOCCH), 3(3):10:1–10:20.
Alexander Hinneburg, Rico Preiss, and Ren´e Schr¨oder.
2012. TopicExplorer: Exploring document collec-
tions with topic models. In Peter A. Flach, Tijl
Bie, and Nello Cristianini, editors, Machine Learn-
ing and Knowledge Discovery in Databases, volume
7524 of Lecture Notes in Computer Science, pages
838–841. Springer Berlin Heidelberg.
Thomas Hofmann. 1999. Probabilistic latent semantic
indexing. In Proceedings of the 22nd Annual Inter-
national ACM SIGIR Conference on Research and
Development in Information Retrieval (SIGIR ’99),
pages 50–57, Berkeley, California, United States.
Ioana Hulpus, Conor Hayes, Marcel Karnstedt, and
Derek Greene. 2013. Unsupervised graph-based
topic labelling using DBpedia. In Proceedings of the
6th ACM International Conference on Web Search
and Data Mining (WSDM ’13), pages 465–474,
Rome, Italy.
Kalervo J¨arvelin and Jaana Kek¨al¨ainen. 2002. Cumu-
lated gain-based evaluation of IR techniques. ACM
Trans. Inf. Syst., 20(4):422–446.
Jey Han Lau, David Newman, Sarvnaz Karimi, and
Timothy Baldwin. 2010. Best topic word selec-
tion for topic labelling. In The 23rd International
Conference on Computational Linguistics (COLING
’10), pages 605–613, Beijing, China.
Jey Han Lau, Karl Grieser, David Newman, and Tim-
othy Baldwin. 2011. Automatic labelling of topic
models. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1536–
1545, Portland, Oregon, USA.
Davide Magatti, Silvia Calegari, Davide Ciucci, and
Fabio Stella. 2009. Automatic Labeling of Top-
ics. In Proceedings of the 9th International Confer-
ence on Intelligent Systems Design and Applications
(ICSDA ’09), pages 1227–1232, Pisa, Italy.
Qiaozhu Mei, Xuehua Shen, and Cheng Xiang Zhai.
2007. Automatic Labeling of Multinomial Topic
Models. In Proceedings of the 13th ACM Inter-
national Conference on Knowledge Discovery and
Data Mining (SIGKDD ’07), pages 490–499, San
Jose, California.
Rada Mihalcea and Paul Tarau. 2004. TextRank:
Bringing order into texts. In Proceedings of Inter-
national Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP ’04), pages 404–
411, Barcelona, Spain.
Lawrence Page, Sergey Brin, Rajeev Motwani, and
Terry Winograd. 1999. The PageRank citation
ranking: Bringing order to the web. Technical Re-
port 1999-66, Stanford InfoLab.
Justin Snyder, Rebecca Knowles, Mark Dredze,
Matthew Gormley, and Travis Wolfe. 2013. Topic
models and metadata for visualizing text corpora. In
Proceedings of the 2013 NAACL-HLT Demonstra-
tion Session, pages 5–9, Atlanta, Georgia. Associa-
tion for Computational Linguistics.
Vladimir N Vapnik. 1998. Statistical learning theory.
Wiley, New York.
</reference>
<page confidence="0.998797">
636
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.367654">
<title confidence="0.998591">Labelling Topics using Unsupervised Graph-based Methods</title>
<author confidence="0.823926">Aletras</author>
<affiliation confidence="0.9929655">Department of Computer University of</affiliation>
<address confidence="0.757661">Regent Court, 211 Sheffield, S1</address>
<note confidence="0.79013">United</note>
<abstract confidence="0.991613857142857">This paper introduces an unsupervised graph-based method that selects textual labels for automatically generated topics. Our approach uses the topic keywords to query a search engine and generate a graph from the words contained in the results. PageRank is then used to weigh the words in the graph and score the candidate labels. The state-of-the-art method for this task is supervised (Lau et al., 2011). Evaluation on a standard data set shows that the performance of our approach is consistently superior to previously reported methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nikolaos Aletras</author>
<author>Mark Stevenson</author>
</authors>
<title>Representing topics using images.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>158--167</pages>
<location>Atlanta,</location>
<contexts>
<context position="2138" citStr="Aletras and Stevenson, 2013" startWordPosition="326" endWordPosition="329"> always straightforward, particularly since background knowledge may be required (Chang et al., 2009). Textual labels could assist with the interpretations of topics and researchers have developed methods to generate these automatically (Mei et al., 2007; Lau et al., 2010; Lau et al., 2011). For example, a topic which has keywords SCHOOL, STUDENT, UNIVERSITY, COLLEGE, TEACHER, CLASS, EDUCATION, LEARN, HIGH, PROGRAM, could be labelled as EDUCATION and a suitable label for the topic shown above would be GLOBAL FINANCIAL CRISIS. Approaches that make use of alternative modalities, such as images (Aletras and Stevenson, 2013), have also been proposed. Mei et al. (2007) label topics using statistically significant bigrams identified in a reference collection. Magatti et al. (2009) introduced an approach for labelling topics that relied on two hierarchical knowledge resources labelled by humans, while Lau et al. (2010) proposed selecting the most representative word from a topic as its label. Hulpus et al. (2013) make use of structured data from DBpedia to label topics. Lau et al. (2011) proposed a method for automatically labelling topics using information from Wikipedia. A set of candidate labels is generated from</context>
</contexts>
<marker>Aletras, Stevenson, 2013</marker>
<rawString>Nikolaos Aletras and Mark Stevenson. 2013. Representing topics using images. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 158–167, Atlanta, Georgia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet Allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="861" citStr="Blei et al., 2003" startWordPosition="122" endWordPosition="125">ac.uk Abstract This paper introduces an unsupervised graph-based method that selects textual labels for automatically generated topics. Our approach uses the topic keywords to query a search engine and generate a graph from the words contained in the results. PageRank is then used to weigh the words in the graph and score the candidate labels. The state-of-the-art method for this task is supervised (Lau et al., 2011). Evaluation on a standard data set shows that the performance of our approach is consistently superior to previously reported methods. 1 Introduction Topic models (Hofmann, 1999; Blei et al., 2003) have proved to be a useful way to represent the content of document collections, e.g. (Chaney and Blei, 2012; Ganguly et al., 2013; Gretarsson et al., 2012; Hinneburg et al., 2012; Snyder et al., 2013). In these interfaces, topics need to be presented to users in an easily interpretable way. A common way to represent topics is as set of keywords generated from the n terms with the highest marginal probabilities. For example, a topic about the global financial crisis could be represented by its top 10 most probable terms: FINANCIAL, BANK, MARKET, GOVERNMENT, MORTGAGE, BAILOUT, BILLION, STREET,</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet Allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerlof Bouma</author>
</authors>
<title>Normalized (pointwise) mutual information in collocation extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of GSCL.</booktitle>
<contexts>
<context position="7411" citStr="Bouma, 2009" startWordPosition="1159" endWordPosition="1160">words in the search result metadata as nodes, v ∈ V , in a graph G = (V, E). Each node is connected to its neighbouring words in a context window of ±n words. In the previous example, the words added to the graph from the Title of the search result are microsoft, server, cloud, datacenter and virtualization. We consider both unweighted and weighted graphs. When the graph is unweighted we assume that all the edges have a weight e = 1. In addition, we weight the edges of the graph by computing the relatedness between two nodes, vi and vj, as their normalised Pointwise Mutual Information (NPMI) (Bouma, 2009). Word co-occurrences are computed using Wikipedia as a a reference corpus. Pairs of words are connected with edges only if NPMI(wi, wj) &gt; 0.2 avoiding connections between words co-occurring by chance and hence introducing noise. 2.3 Identifying Important Terms Important terms are identified by applying the PageRank algorithm (Page et al., 1999) in a similar way to the approach used by Mihalcea and 1http://www.bing.com/ 2We also experimented with using the Description field but found that this reduced performance. 3http://opennlp.apache.org/ 632 Tarau (2004) for document keyphrase extraction. </context>
</contexts>
<marker>Bouma, 2009</marker>
<rawString>Gerlof Bouma. 2009. Normalized (pointwise) mutual information in collocation extraction. In Proceedings of GSCL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Allison June-Barlow Chaney</author>
<author>David M Blei</author>
</authors>
<title>Visualizing topic models.</title>
<date>2012</date>
<booktitle>In Proceedings of the Sixth International AAAI Conference on Weblogs and Social</booktitle>
<location>Media, Dublin, Ireland.</location>
<contexts>
<context position="970" citStr="Chaney and Blei, 2012" startWordPosition="141" endWordPosition="144">tomatically generated topics. Our approach uses the topic keywords to query a search engine and generate a graph from the words contained in the results. PageRank is then used to weigh the words in the graph and score the candidate labels. The state-of-the-art method for this task is supervised (Lau et al., 2011). Evaluation on a standard data set shows that the performance of our approach is consistently superior to previously reported methods. 1 Introduction Topic models (Hofmann, 1999; Blei et al., 2003) have proved to be a useful way to represent the content of document collections, e.g. (Chaney and Blei, 2012; Ganguly et al., 2013; Gretarsson et al., 2012; Hinneburg et al., 2012; Snyder et al., 2013). In these interfaces, topics need to be presented to users in an easily interpretable way. A common way to represent topics is as set of keywords generated from the n terms with the highest marginal probabilities. For example, a topic about the global financial crisis could be represented by its top 10 most probable terms: FINANCIAL, BANK, MARKET, GOVERNMENT, MORTGAGE, BAILOUT, BILLION, STREET, WALL, CRISIS. But interpreting such lists is not always straightforward, particularly since background knowl</context>
</contexts>
<marker>Chaney, Blei, 2012</marker>
<rawString>Allison June-Barlow Chaney and David M. Blei. 2012. Visualizing topic models. In Proceedings of the Sixth International AAAI Conference on Weblogs and Social Media, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Chang</author>
<author>Jordan Boyd-Graber</author>
<author>Sean Gerrish</author>
</authors>
<title>Reading Tea Leaves: How Humans Interpret Topic Models. Neural Information,</title>
<date>2009</date>
<pages>1--9</pages>
<contexts>
<context position="1611" citStr="Chang et al., 2009" startWordPosition="244" endWordPosition="247">; Gretarsson et al., 2012; Hinneburg et al., 2012; Snyder et al., 2013). In these interfaces, topics need to be presented to users in an easily interpretable way. A common way to represent topics is as set of keywords generated from the n terms with the highest marginal probabilities. For example, a topic about the global financial crisis could be represented by its top 10 most probable terms: FINANCIAL, BANK, MARKET, GOVERNMENT, MORTGAGE, BAILOUT, BILLION, STREET, WALL, CRISIS. But interpreting such lists is not always straightforward, particularly since background knowledge may be required (Chang et al., 2009). Textual labels could assist with the interpretations of topics and researchers have developed methods to generate these automatically (Mei et al., 2007; Lau et al., 2010; Lau et al., 2011). For example, a topic which has keywords SCHOOL, STUDENT, UNIVERSITY, COLLEGE, TEACHER, CLASS, EDUCATION, LEARN, HIGH, PROGRAM, could be labelled as EDUCATION and a suitable label for the topic shown above would be GLOBAL FINANCIAL CRISIS. Approaches that make use of alternative modalities, such as images (Aletras and Stevenson, 2013), have also been proposed. Mei et al. (2007) label topics using statistic</context>
</contexts>
<marker>Chang, Boyd-Graber, Gerrish, 2009</marker>
<rawString>Jonathan Chang, Jordan Boyd-Graber, and Sean Gerrish. 2009. Reading Tea Leaves: How Humans Interpret Topic Models. Neural Information, pages 1– 9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bruce W Croft</author>
<author>Donald Metzler</author>
<author>Trevor Strohman</author>
</authors>
<title>Search engines: Information retrieval in practice.</title>
<date>2009</date>
<publisher>Addison-Wesley.</publisher>
<contexts>
<context position="10387" citStr="Croft et al., 2009" startWordPosition="1665" endWordPosition="1668">ic is represented by its ten most probable keywords. It is also associated with candidate labels and human ratings 3.2 Evaluation Metrics Our evaluation follows the framework proposed by Lau et al. (2011) using two metrics, i.e. Top1 average rating and nDCG, to compare various labelling methods. Top-1 average rating is the average human rating (between 0 and 3) assigned to the top-ranked label proposed by the system. This provides an indication of the overall quality of the label the system judges as the best one. Normalised discounted cumulative gain (nDCG) (J¨arvelin and Kek¨al¨ainen, 2002; Croft et al., 2009) compares the label ranking proposed by the system to the ranking provided by human annotators. The discounted cumulative gain at position p, DCGp, is computed using the following equation: reli (3) log2(i) where reli is the relevance of the label to the topic in position i. Then nDCG is computed as: DCGp nDCGp = (4) IDCGp where IDCGp is the superviseed ranking of the image labels, in our experiments this is the ranking provided by the scores in the human annotated data set. 3.3 Model Parameters Our proposed model requires two parameters to be set: the context window size when connecting neigh</context>
</contexts>
<marker>Croft, Metzler, Strohman, 2009</marker>
<rawString>Bruce W. Croft, Donald Metzler, and Trevor Strohman. 2009. Search engines: Information retrieval in practice. Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Debasis Ganguly</author>
<author>Manisha Ganguly</author>
<author>Johannes Leveling</author>
<author>Gareth J F Jones</author>
</authors>
<title>TopicVis: A GUI for Topic-based feedback and navigation.</title>
<date>2013</date>
<booktitle>In Proceedings of the Thirty-Sixth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 13),</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="992" citStr="Ganguly et al., 2013" startWordPosition="145" endWordPosition="148">opics. Our approach uses the topic keywords to query a search engine and generate a graph from the words contained in the results. PageRank is then used to weigh the words in the graph and score the candidate labels. The state-of-the-art method for this task is supervised (Lau et al., 2011). Evaluation on a standard data set shows that the performance of our approach is consistently superior to previously reported methods. 1 Introduction Topic models (Hofmann, 1999; Blei et al., 2003) have proved to be a useful way to represent the content of document collections, e.g. (Chaney and Blei, 2012; Ganguly et al., 2013; Gretarsson et al., 2012; Hinneburg et al., 2012; Snyder et al., 2013). In these interfaces, topics need to be presented to users in an easily interpretable way. A common way to represent topics is as set of keywords generated from the n terms with the highest marginal probabilities. For example, a topic about the global financial crisis could be represented by its top 10 most probable terms: FINANCIAL, BANK, MARKET, GOVERNMENT, MORTGAGE, BAILOUT, BILLION, STREET, WALL, CRISIS. But interpreting such lists is not always straightforward, particularly since background knowledge may be required (</context>
</contexts>
<marker>Ganguly, Ganguly, Leveling, Jones, 2013</marker>
<rawString>Debasis Ganguly, Manisha Ganguly, Johannes Leveling, and Gareth J.F. Jones. 2013. TopicVis: A GUI for Topic-based feedback and navigation. In Proceedings of the Thirty-Sixth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 13), Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brynjar Gretarsson</author>
<author>John O’Donovan</author>
<author>Svetlin Bostandjiev</author>
<author>Tobias H¨ollerer</author>
<author>Arthur Asuncion</author>
<author>David Newman</author>
<author>Padhraic Smyth</author>
</authors>
<title>TopicNets: Visual analysis of large text corpora with topic modeling.</title>
<date>2012</date>
<journal>ACM Trans. Intell. Syst. Technol.,</journal>
<volume>3</volume>
<issue>2</issue>
<marker>Gretarsson, O’Donovan, Bostandjiev, H¨ollerer, Asuncion, Newman, Smyth, 2012</marker>
<rawString>Brynjar Gretarsson, John O’Donovan, Svetlin Bostandjiev, Tobias H¨ollerer, Arthur Asuncion, David Newman, and Padhraic Smyth. 2012. TopicNets: Visual analysis of large text corpora with topic modeling. ACM Trans. Intell. Syst. Technol., 3(2):23:1–23:26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl Grieser</author>
<author>Timothy Baldwin</author>
<author>Fabian Bohnert</author>
<author>Liz Sonenberg</author>
</authors>
<title>Using Ontological and Document Similarity to Estimate Museum Exhibit Relatedness.</title>
<date>2011</date>
<booktitle>Journal on Computing and Cultural Heritage (JOCCH),</booktitle>
<pages>3--3</pages>
<contexts>
<context position="5716" citStr="Grieser et al., 2011" startWordPosition="866" endWordPosition="869">eight pre-existing candidate labels. 2.1 Retrieving and Processing Text Information We use the approach described by Lau et al. (2011) to generate candidate labels from Wikipedia articles. The 10 terms with the highest marginal probabilities in the topic are used to query Wikipedia and the titles of the articles retrieved used as candidate labels. Further candidate labels are generated by processing the titles of these articles to identify noun chunks and n-grams within the noun chunks that are themselves the titles of Wikipedia articles. Outlier labels, identified using a similarity measure (Grieser et al., 2011), are removed. This method has been proved to produce labels which effectively summarise a topic’s main subject. However, it should be noted that our method is flexible and could be applied to any set of candidate labels. We have experimented with various approaches to candidate label generation but chose to report results using the approach described by Lau et al. (2011) to allow direct comparison of approaches. Information obtained from web searches is used to identify the best labels from the set of candidates. The top n keywords, i.e. those with highest marginal probability within the topi</context>
</contexts>
<marker>Grieser, Baldwin, Bohnert, Sonenberg, 2011</marker>
<rawString>Karl Grieser, Timothy Baldwin, Fabian Bohnert, and Liz Sonenberg. 2011. Using Ontological and Document Similarity to Estimate Museum Exhibit Relatedness. Journal on Computing and Cultural Heritage (JOCCH), 3(3):10:1–10:20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Hinneburg</author>
<author>Rico Preiss</author>
<author>Ren´e Schr¨oder</author>
</authors>
<title>TopicExplorer: Exploring document collections with topic models.</title>
<date>2012</date>
<booktitle>Machine Learning and Knowledge Discovery in Databases,</booktitle>
<volume>7524</volume>
<pages>838--841</pages>
<editor>In Peter A. Flach, Tijl Bie, and Nello Cristianini, editors,</editor>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<marker>Hinneburg, Preiss, Schr¨oder, 2012</marker>
<rawString>Alexander Hinneburg, Rico Preiss, and Ren´e Schr¨oder. 2012. TopicExplorer: Exploring document collections with topic models. In Peter A. Flach, Tijl Bie, and Nello Cristianini, editors, Machine Learning and Knowledge Discovery in Databases, volume 7524 of Lecture Notes in Computer Science, pages 838–841. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
</authors>
<title>Probabilistic latent semantic indexing.</title>
<date>1999</date>
<booktitle>In Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’99),</booktitle>
<pages>50--57</pages>
<location>Berkeley, California, United States.</location>
<contexts>
<context position="841" citStr="Hofmann, 1999" startWordPosition="120" endWordPosition="121">nson}@dcs.shef.ac.uk Abstract This paper introduces an unsupervised graph-based method that selects textual labels for automatically generated topics. Our approach uses the topic keywords to query a search engine and generate a graph from the words contained in the results. PageRank is then used to weigh the words in the graph and score the candidate labels. The state-of-the-art method for this task is supervised (Lau et al., 2011). Evaluation on a standard data set shows that the performance of our approach is consistently superior to previously reported methods. 1 Introduction Topic models (Hofmann, 1999; Blei et al., 2003) have proved to be a useful way to represent the content of document collections, e.g. (Chaney and Blei, 2012; Ganguly et al., 2013; Gretarsson et al., 2012; Hinneburg et al., 2012; Snyder et al., 2013). In these interfaces, topics need to be presented to users in an easily interpretable way. A common way to represent topics is as set of keywords generated from the n terms with the highest marginal probabilities. For example, a topic about the global financial crisis could be represented by its top 10 most probable terms: FINANCIAL, BANK, MARKET, GOVERNMENT, MORTGAGE, BAILO</context>
</contexts>
<marker>Hofmann, 1999</marker>
<rawString>Thomas Hofmann. 1999. Probabilistic latent semantic indexing. In Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’99), pages 50–57, Berkeley, California, United States.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ioana Hulpus</author>
<author>Conor Hayes</author>
<author>Marcel Karnstedt</author>
<author>Derek Greene</author>
</authors>
<title>Unsupervised graph-based topic labelling using DBpedia.</title>
<date>2013</date>
<booktitle>In Proceedings of the 6th ACM International Conference on Web Search and Data Mining (WSDM ’13),</booktitle>
<pages>465--474</pages>
<location>Rome, Italy.</location>
<contexts>
<context position="2531" citStr="Hulpus et al. (2013)" startWordPosition="388" endWordPosition="391">RN, HIGH, PROGRAM, could be labelled as EDUCATION and a suitable label for the topic shown above would be GLOBAL FINANCIAL CRISIS. Approaches that make use of alternative modalities, such as images (Aletras and Stevenson, 2013), have also been proposed. Mei et al. (2007) label topics using statistically significant bigrams identified in a reference collection. Magatti et al. (2009) introduced an approach for labelling topics that relied on two hierarchical knowledge resources labelled by humans, while Lau et al. (2010) proposed selecting the most representative word from a topic as its label. Hulpus et al. (2013) make use of structured data from DBpedia to label topics. Lau et al. (2011) proposed a method for automatically labelling topics using information from Wikipedia. A set of candidate labels is generated from Wikipedia article titles by querying using topic terms. Additional labels are then generated by chunk parsing the article titles to identify n-grams that represent Wikipedia articles as well. Outlier labels (less relevant to the topic) are identified and removed. Finally, the top-5 topic terms are added to the candidate set. The labels are ranked using Support Vector Regression (SVR) (Vapn</context>
</contexts>
<marker>Hulpus, Hayes, Karnstedt, Greene, 2013</marker>
<rawString>Ioana Hulpus, Conor Hayes, Marcel Karnstedt, and Derek Greene. 2013. Unsupervised graph-based topic labelling using DBpedia. In Proceedings of the 6th ACM International Conference on Web Search and Data Mining (WSDM ’13), pages 465–474, Rome, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kalervo J¨arvelin</author>
<author>Jaana Kek¨al¨ainen</author>
</authors>
<title>Cumulated gain-based evaluation of IR techniques.</title>
<date>2002</date>
<journal>ACM Trans. Inf. Syst.,</journal>
<volume>20</volume>
<issue>4</issue>
<marker>J¨arvelin, Kek¨al¨ainen, 2002</marker>
<rawString>Kalervo J¨arvelin and Jaana Kek¨al¨ainen. 2002. Cumulated gain-based evaluation of IR techniques. ACM Trans. Inf. Syst., 20(4):422–446.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jey Han Lau</author>
<author>David Newman</author>
<author>Sarvnaz Karimi</author>
<author>Timothy Baldwin</author>
</authors>
<title>Best topic word selection for topic labelling.</title>
<date>2010</date>
<booktitle>In The 23rd International Conference on Computational Linguistics (COLING ’10),</booktitle>
<pages>605--613</pages>
<location>Beijing, China.</location>
<contexts>
<context position="1782" citStr="Lau et al., 2010" startWordPosition="271" endWordPosition="274">y to represent topics is as set of keywords generated from the n terms with the highest marginal probabilities. For example, a topic about the global financial crisis could be represented by its top 10 most probable terms: FINANCIAL, BANK, MARKET, GOVERNMENT, MORTGAGE, BAILOUT, BILLION, STREET, WALL, CRISIS. But interpreting such lists is not always straightforward, particularly since background knowledge may be required (Chang et al., 2009). Textual labels could assist with the interpretations of topics and researchers have developed methods to generate these automatically (Mei et al., 2007; Lau et al., 2010; Lau et al., 2011). For example, a topic which has keywords SCHOOL, STUDENT, UNIVERSITY, COLLEGE, TEACHER, CLASS, EDUCATION, LEARN, HIGH, PROGRAM, could be labelled as EDUCATION and a suitable label for the topic shown above would be GLOBAL FINANCIAL CRISIS. Approaches that make use of alternative modalities, such as images (Aletras and Stevenson, 2013), have also been proposed. Mei et al. (2007) label topics using statistically significant bigrams identified in a reference collection. Magatti et al. (2009) introduced an approach for labelling topics that relied on two hierarchical knowledge </context>
</contexts>
<marker>Lau, Newman, Karimi, Baldwin, 2010</marker>
<rawString>Jey Han Lau, David Newman, Sarvnaz Karimi, and Timothy Baldwin. 2010. Best topic word selection for topic labelling. In The 23rd International Conference on Computational Linguistics (COLING ’10), pages 605–613, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jey Han Lau</author>
<author>Karl Grieser</author>
<author>David Newman</author>
<author>Timothy Baldwin</author>
</authors>
<title>Automatic labelling of topic models.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1536--1545</pages>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="663" citStr="Lau et al., 2011" startWordPosition="91" endWordPosition="94">d Methods Nikolaos Aletras and Mark Stevenson Department of Computer Science University of Sheffield Regent Court, 211 Portobello Sheffield, S1 4DP United Kingdom {n.aletras, m.stevenson}@dcs.shef.ac.uk Abstract This paper introduces an unsupervised graph-based method that selects textual labels for automatically generated topics. Our approach uses the topic keywords to query a search engine and generate a graph from the words contained in the results. PageRank is then used to weigh the words in the graph and score the candidate labels. The state-of-the-art method for this task is supervised (Lau et al., 2011). Evaluation on a standard data set shows that the performance of our approach is consistently superior to previously reported methods. 1 Introduction Topic models (Hofmann, 1999; Blei et al., 2003) have proved to be a useful way to represent the content of document collections, e.g. (Chaney and Blei, 2012; Ganguly et al., 2013; Gretarsson et al., 2012; Hinneburg et al., 2012; Snyder et al., 2013). In these interfaces, topics need to be presented to users in an easily interpretable way. A common way to represent topics is as set of keywords generated from the n terms with the highest marginal </context>
<context position="2607" citStr="Lau et al. (2011)" startWordPosition="403" endWordPosition="406">topic shown above would be GLOBAL FINANCIAL CRISIS. Approaches that make use of alternative modalities, such as images (Aletras and Stevenson, 2013), have also been proposed. Mei et al. (2007) label topics using statistically significant bigrams identified in a reference collection. Magatti et al. (2009) introduced an approach for labelling topics that relied on two hierarchical knowledge resources labelled by humans, while Lau et al. (2010) proposed selecting the most representative word from a topic as its label. Hulpus et al. (2013) make use of structured data from DBpedia to label topics. Lau et al. (2011) proposed a method for automatically labelling topics using information from Wikipedia. A set of candidate labels is generated from Wikipedia article titles by querying using topic terms. Additional labels are then generated by chunk parsing the article titles to identify n-grams that represent Wikipedia articles as well. Outlier labels (less relevant to the topic) are identified and removed. Finally, the top-5 topic terms are added to the candidate set. The labels are ranked using Support Vector Regression (SVR) (Vapnik, 1998) and features extracted using word association measures (i.e. PMI, </context>
<context position="4810" citStr="Lau et al., 2011" startWordPosition="715" endWordPosition="718">ponsive datacenter built from your existing technology investments.’, ‘DisplayUrl’: ‘www.microsoft.com/en-us/server-cloud/datacenter/virtualization.aspx’, ‘ID’: ‘a42b0908-174e-4f25-b59c-70bdf394a9da’, ‘Title’: ‘Microsoft |Server &amp; Cloud |Datacenter |Virtualization ...’, ‘Url’: ‘http://www.microsoft.com/en-us/server-cloud/datacenter/virtualization.aspx’, ... } Figure 1: Sample of the metadata associated with a search result. cea and Tarau, 2004). Evaluation on a standard data set shows that our method consistently outperforms the best performing previously reported method, which is supervised (Lau et al., 2011). 2 Methodology We use the topic keywords to query a search engine. We assume that the search results returned are relevant to the topic and can be used to identify and weigh relevant keywords. The most important keywords can be used to generate keyphrases for labelling the topic or weight pre-existing candidate labels. 2.1 Retrieving and Processing Text Information We use the approach described by Lau et al. (2011) to generate candidate labels from Wikipedia articles. The 10 terms with the highest marginal probabilities in the topic are used to query Wikipedia and the titles of the articles r</context>
<context position="6090" citStr="Lau et al. (2011)" startWordPosition="928" endWordPosition="931"> generated by processing the titles of these articles to identify noun chunks and n-grams within the noun chunks that are themselves the titles of Wikipedia articles. Outlier labels, identified using a similarity measure (Grieser et al., 2011), are removed. This method has been proved to produce labels which effectively summarise a topic’s main subject. However, it should be noted that our method is flexible and could be applied to any set of candidate labels. We have experimented with various approaches to candidate label generation but chose to report results using the approach described by Lau et al. (2011) to allow direct comparison of approaches. Information obtained from web searches is used to identify the best labels from the set of candidates. The top n keywords, i.e. those with highest marginal probability within the topic, are used to form a query which was submitted to the Bing1 search engine. Textual information included in the Title field2 of the search results metadata was extracted. Each title was tokenised using openNLP3 and stop words removed. Figure 1 shows a sample of the metadata associated with a search result for the topic: VMWARE, SERVER, VIRTUAL, ORACLE, UPDATE, VIRTUALIZAT</context>
<context position="9550" citStr="Lau et al. (2011)" startWordPosition="1528" endWordPosition="1531">k scores. However, this has a negative effect on performance since it favoured short labels of one or two words which were not sufficiently descriptive of the topics. In addition, we expect that candidate labels containing words that do not appear in the graph (with the exception of stop words) are unlikely to be good labels for the topic. In these cases the score of the candidate label is set to 0. We also experimented with removing this restriction but found that it lowered performance. 3 Experimental Evaluation 3.1 Data We evaluate our method on the publicly available data set published by Lau et al. (2011). The data set consists of 228 topics generated using text documents from four domains, i.e. blog posts (BLOGS), books (BOOKS), news articles (NEWS) and scientific articles from the biomedical domain (PUBMED). Each topic is represented by its ten most probable keywords. It is also associated with candidate labels and human ratings 3.2 Evaluation Metrics Our evaluation follows the framework proposed by Lau et al. (2011) using two metrics, i.e. Top1 average rating and nDCG, to compare various labelling methods. Top-1 average rating is the average human rating (between 0 and 3) assigned to the to</context>
<context position="11829" citStr="Lau et al. (2011)" startWordPosition="1918" endWordPosition="1921">tle. The best results were obtained when n = 2 for all of the domains. In addition, we experimented with varying the number of search results between 10 and 300. We observed no noticeable difference in the performance when the number of search results is equal or greater than 30 (see below). We choose to report results obtained using 30 search results for each topic. Including more results did not improve performance but required additional processing. �Pr(vi) = d · vj∈C(vi) v sim(vj,vk) Pr (j) sim(vi, vj) E vk∈C(vj) DCGp = rel1 + � p i=2 633 Domain Model Top-1 Av. Rating nDCG-1 nDCG-3 nDCG-5 Lau et al. (2011)-U 1.84 0.75 0.77 0.79 Lau et al. (2011)-S 1.98 0.81 0.82 0.83 BLOGS PR 2.05† 0.83 0.84 0.83 PR-NPMI 2.08† 0.84 0.84 0.83 Upper bound 2.45 1.00 1.00 1.00 Lau et al. (2011)-U 1.75 0.77 0.77 0.79 Lau et al. (2011)-S 1.91 0.84 0.81 0.83 BOOKS PR 1.98† 0.86 0.88 0.87 PR-NPMI 2.01† 0.87 0.88 0.87 Upper bound 2.29 1.00 1.00 1.00 Lau et al. (2011)-U 1.96 0.80 0.79 0.78 Lau et al. (2011)-S 2.02 0.82 0.82 0.84 NEWS PR 2.04† 0.83 0.81 0.81 PR-NPMI 2.05† 0.83 0.81 0.81 Upper bound 2.45 1.00 1.00 1.00 Lau et al. (2011)-U 1.73 0.75 0.77 0.79 Lau et al. (2011)-S 1.79 0.77 0.82 0.84 PUBMED PR 1.88†‡ 0.80 0.8</context>
<context position="13132" citStr="Lau et al. (2011)" startWordPosition="2154" endWordPosition="2157">for Various Approaches to Topic Labelling (†: significant difference (t-test, p &lt; 0.05) to Lau et al. (2011)-U; ‡: significant difference (p &lt; 0.05) to Lau et al. (2011)-S). 4 Results and Discussion Results are shown in Table 1. Performance when PageRank is applied to the unweighted (PR) and NPMI-weighted graphs (PR-NPMI) (see Section 2.2) is shown. Performance of the best unsupervised (Lau et al. (2011)-U) and supervised (Lau et al. (2011)-S) methods reported by Lau et al. (2011) are shown. Lau et al. (2011)-U uses the average x2 scores between the topic keywords and the label keywords while Lau et al. (2011)-S uses SVR to combine evidence from all features. In addition, upper bound figures, the maximum possible value given the scores assigned by the annotators, are also shown. The results obtained by applying PageRank over the unweighted graph (2.05, 1.98, 2.04 and 1.88) are consistently better than the supervised and unsupervised methods reported by Lau et al. (2011) for the Top-1 Average scores and this improvement is observed in all domains. The difference is significant (t-test, p &lt; 0.05) for the unsupervised method. A slight improvement in performance is observed when the weighted graph is u</context>
<context position="15002" citStr="Lau et al. (2011)" startWordPosition="2490" endWordPosition="2493">rating that it is only the lower ranked labels in our approach that are not as good as the supervised approach. An interesting finding is that, although limited in length, the textual information in the search result’s metadata contain enough salient terms relevant to the topic to provide reliable estimates of 634 Top-1 Av. Rating 2.2 2.1 2 1.9 1.8 1.7 2.2 2.1 1.9 1.8 1.7 2 50 100 150 200 250 300 Number of Search Results (a) BLOGS 50 100 150 200 250 300 Number of Search Results (b) BOOKS Top-1 Av. Rating 2.2 2.1 1.9 1.8 1.7 2 50 100 150 200 250 300 2.2 2.1 1.9 1.8 1.7 2 50 100 150 200 250 300 Lau et al. (2011)-U Lau et al. (2011)-S PR PR-NPMI Number of Search Results Number of Search Results (c) NEWS (d) PUBMED Figure 2: Top-1 Average Rating obtained for different number of search results. term importance. Consequently, it is not necessary to measure semantic similarity between topic keywords and candidate labels as previous approaches have done. In addition, performance improvement gained from using the weighted graph is modest, suggesting that the computation of association scores over a large reference corpus could be omitted if resources are limited. In Figure 2, we show the scores of Top-1 ave</context>
<context position="16275" citStr="Lau et al., 2011" startWordPosition="2703" endWordPosition="2706">enting with the number of search results used to generate the text graph. The most interesting finding is that performance is stable when 30 or more search results are considered. In addition, we observe that quality of the topic labels in the four domains remains stable, and higher than the supervised method, when the number of search results used is between 150 and 200. The only domain in which performance of the supervised method is sometimes better than the approach proposed here is NEWS. The main reason is that news topics are more fine grained and the candidate labels of better quality (Lau et al., 2011) which has direct impact in good performance of ranking methods. 5 Conclusion We described an unsupervised graph-based method to associate textual labels with automatically generated topics. Our approach uses results retrieved from a search engine using the topic keywords as a query. A graph is generated from the words contained in the search results metadata and candidate labels ranked using the PageRank algorithm. Evaluation on a standard data set shows that our method consistently outperforms the supervised state-of-the-art method for the task. Acknowledgments We would like to thank Jey Han</context>
</contexts>
<marker>Lau, Grieser, Newman, Baldwin, 2011</marker>
<rawString>Jey Han Lau, Karl Grieser, David Newman, and Timothy Baldwin. 2011. Automatic labelling of topic models. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 1536– 1545, Portland, Oregon, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Davide Magatti</author>
<author>Silvia Calegari</author>
<author>Davide Ciucci</author>
<author>Fabio Stella</author>
</authors>
<title>Automatic Labeling of Topics.</title>
<date>2009</date>
<booktitle>In Proceedings of the 9th International Conference on Intelligent Systems Design and Applications (ICSDA ’09),</booktitle>
<pages>1227--1232</pages>
<location>Pisa, Italy.</location>
<contexts>
<context position="2295" citStr="Magatti et al. (2009)" startWordPosition="350" endWordPosition="353">s and researchers have developed methods to generate these automatically (Mei et al., 2007; Lau et al., 2010; Lau et al., 2011). For example, a topic which has keywords SCHOOL, STUDENT, UNIVERSITY, COLLEGE, TEACHER, CLASS, EDUCATION, LEARN, HIGH, PROGRAM, could be labelled as EDUCATION and a suitable label for the topic shown above would be GLOBAL FINANCIAL CRISIS. Approaches that make use of alternative modalities, such as images (Aletras and Stevenson, 2013), have also been proposed. Mei et al. (2007) label topics using statistically significant bigrams identified in a reference collection. Magatti et al. (2009) introduced an approach for labelling topics that relied on two hierarchical knowledge resources labelled by humans, while Lau et al. (2010) proposed selecting the most representative word from a topic as its label. Hulpus et al. (2013) make use of structured data from DBpedia to label topics. Lau et al. (2011) proposed a method for automatically labelling topics using information from Wikipedia. A set of candidate labels is generated from Wikipedia article titles by querying using topic terms. Additional labels are then generated by chunk parsing the article titles to identify n-grams that re</context>
</contexts>
<marker>Magatti, Calegari, Ciucci, Stella, 2009</marker>
<rawString>Davide Magatti, Silvia Calegari, Davide Ciucci, and Fabio Stella. 2009. Automatic Labeling of Topics. In Proceedings of the 9th International Conference on Intelligent Systems Design and Applications (ICSDA ’09), pages 1227–1232, Pisa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiaozhu Mei</author>
<author>Xuehua Shen</author>
<author>Cheng Xiang Zhai</author>
</authors>
<title>Automatic Labeling of Multinomial Topic Models.</title>
<date>2007</date>
<booktitle>In Proceedings of the 13th ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD ’07),</booktitle>
<pages>490--499</pages>
<location>San Jose, California.</location>
<contexts>
<context position="1764" citStr="Mei et al., 2007" startWordPosition="267" endWordPosition="270">e way. A common way to represent topics is as set of keywords generated from the n terms with the highest marginal probabilities. For example, a topic about the global financial crisis could be represented by its top 10 most probable terms: FINANCIAL, BANK, MARKET, GOVERNMENT, MORTGAGE, BAILOUT, BILLION, STREET, WALL, CRISIS. But interpreting such lists is not always straightforward, particularly since background knowledge may be required (Chang et al., 2009). Textual labels could assist with the interpretations of topics and researchers have developed methods to generate these automatically (Mei et al., 2007; Lau et al., 2010; Lau et al., 2011). For example, a topic which has keywords SCHOOL, STUDENT, UNIVERSITY, COLLEGE, TEACHER, CLASS, EDUCATION, LEARN, HIGH, PROGRAM, could be labelled as EDUCATION and a suitable label for the topic shown above would be GLOBAL FINANCIAL CRISIS. Approaches that make use of alternative modalities, such as images (Aletras and Stevenson, 2013), have also been proposed. Mei et al. (2007) label topics using statistically significant bigrams identified in a reference collection. Magatti et al. (2009) introduced an approach for labelling topics that relied on two hiera</context>
<context position="3546" citStr="Mei et al., 2007" startWordPosition="554" endWordPosition="557">. Outlier labels (less relevant to the topic) are identified and removed. Finally, the top-5 topic terms are added to the candidate set. The labels are ranked using Support Vector Regression (SVR) (Vapnik, 1998) and features extracted using word association measures (i.e. PMI, t-test, χ2 and Dice coefficient), lexical features and search engine ranking. Lau et al. (2011) report two versions of their approach, one unsupervised (which is used as a baseline) and another which is supervised. They reported that the supervised version achieves better performance than a previously reported approach (Mei et al., 2007). This paper introduces an alternative graphbased approach which is unsupervised and less computationally intensive than Lau et al. (2011). Our method uses topic keywords to form a query. A graph is generated from the words contained in the search results and these are then ranked using the PageRank algorithm (Page et al., 1999; Mihal631 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 631–636, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics {‘Description’: ‘Microsoft will accelerate your j</context>
</contexts>
<marker>Mei, Shen, Zhai, 2007</marker>
<rawString>Qiaozhu Mei, Xuehua Shen, and Cheng Xiang Zhai. 2007. Automatic Labeling of Multinomial Topic Models. In Proceedings of the 13th ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD ’07), pages 490–499, San Jose, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Paul Tarau</author>
</authors>
<title>TextRank: Bringing order into texts.</title>
<date>2004</date>
<booktitle>In Proceedings of International Conference on Empirical Methods in Natural Language Processing (EMNLP ’04),</booktitle>
<pages>404--411</pages>
<location>Barcelona,</location>
<marker>Mihalcea, Tarau, 2004</marker>
<rawString>Rada Mihalcea and Paul Tarau. 2004. TextRank: Bringing order into texts. In Proceedings of International Conference on Empirical Methods in Natural Language Processing (EMNLP ’04), pages 404– 411, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence Page</author>
<author>Sergey Brin</author>
<author>Rajeev Motwani</author>
<author>Terry Winograd</author>
</authors>
<title>The PageRank citation ranking: Bringing order to the web.</title>
<date>1999</date>
<tech>Technical Report 1999-66,</tech>
<institution>Stanford InfoLab.</institution>
<contexts>
<context position="3875" citStr="Page et al., 1999" startWordPosition="608" endWordPosition="611">and search engine ranking. Lau et al. (2011) report two versions of their approach, one unsupervised (which is used as a baseline) and another which is supervised. They reported that the supervised version achieves better performance than a previously reported approach (Mei et al., 2007). This paper introduces an alternative graphbased approach which is unsupervised and less computationally intensive than Lau et al. (2011). Our method uses topic keywords to form a query. A graph is generated from the words contained in the search results and these are then ranked using the PageRank algorithm (Page et al., 1999; Mihal631 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 631–636, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics {‘Description’: ‘Microsoft will accelerate your journey to cloud computing with an agile and responsive datacenter built from your existing technology investments.’, ‘DisplayUrl’: ‘www.microsoft.com/en-us/server-cloud/datacenter/virtualization.aspx’, ‘ID’: ‘a42b0908-174e-4f25-b59c-70bdf394a9da’, ‘Title’: ‘Microsoft |Server &amp; Cloud |Datacenter |Virtualization ...’, ‘Url’: ‘htt</context>
<context position="7758" citStr="Page et al., 1999" startWordPosition="1212" endWordPosition="1215"> weighted graphs. When the graph is unweighted we assume that all the edges have a weight e = 1. In addition, we weight the edges of the graph by computing the relatedness between two nodes, vi and vj, as their normalised Pointwise Mutual Information (NPMI) (Bouma, 2009). Word co-occurrences are computed using Wikipedia as a a reference corpus. Pairs of words are connected with edges only if NPMI(wi, wj) &gt; 0.2 avoiding connections between words co-occurring by chance and hence introducing noise. 2.3 Identifying Important Terms Important terms are identified by applying the PageRank algorithm (Page et al., 1999) in a similar way to the approach used by Mihalcea and 1http://www.bing.com/ 2We also experimented with using the Description field but found that this reduced performance. 3http://opennlp.apache.org/ 632 Tarau (2004) for document keyphrase extraction. The PageRank score (Pr) over G for a word (vi) can be computed by the following equation: denoting the appropriateness of a label given the topic. The full data set consists of approximately 6,000 candidate labels (27 labels per topic). + (1 − d)v (1) where C(vi) denotes the set of vertices which are connected to the vertex vi. d is the damping </context>
</contexts>
<marker>Page, Brin, Motwani, Winograd, 1999</marker>
<rawString>Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. 1999. The PageRank citation ranking: Bringing order to the web. Technical Report 1999-66, Stanford InfoLab.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Justin Snyder</author>
<author>Rebecca Knowles</author>
<author>Mark Dredze</author>
<author>Matthew Gormley</author>
<author>Travis Wolfe</author>
</authors>
<title>Topic models and metadata for visualizing text corpora.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 NAACL-HLT Demonstration Session,</booktitle>
<pages>5--9</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta,</location>
<contexts>
<context position="1063" citStr="Snyder et al., 2013" startWordPosition="157" endWordPosition="160">d generate a graph from the words contained in the results. PageRank is then used to weigh the words in the graph and score the candidate labels. The state-of-the-art method for this task is supervised (Lau et al., 2011). Evaluation on a standard data set shows that the performance of our approach is consistently superior to previously reported methods. 1 Introduction Topic models (Hofmann, 1999; Blei et al., 2003) have proved to be a useful way to represent the content of document collections, e.g. (Chaney and Blei, 2012; Ganguly et al., 2013; Gretarsson et al., 2012; Hinneburg et al., 2012; Snyder et al., 2013). In these interfaces, topics need to be presented to users in an easily interpretable way. A common way to represent topics is as set of keywords generated from the n terms with the highest marginal probabilities. For example, a topic about the global financial crisis could be represented by its top 10 most probable terms: FINANCIAL, BANK, MARKET, GOVERNMENT, MORTGAGE, BAILOUT, BILLION, STREET, WALL, CRISIS. But interpreting such lists is not always straightforward, particularly since background knowledge may be required (Chang et al., 2009). Textual labels could assist with the interpretatio</context>
</contexts>
<marker>Snyder, Knowles, Dredze, Gormley, Wolfe, 2013</marker>
<rawString>Justin Snyder, Rebecca Knowles, Mark Dredze, Matthew Gormley, and Travis Wolfe. 2013. Topic models and metadata for visualizing text corpora. In Proceedings of the 2013 NAACL-HLT Demonstration Session, pages 5–9, Atlanta, Georgia. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir N Vapnik</author>
</authors>
<title>Statistical learning theory.</title>
<date>1998</date>
<publisher>Wiley,</publisher>
<location>New York.</location>
<contexts>
<context position="3140" citStr="Vapnik, 1998" startWordPosition="491" endWordPosition="492">013) make use of structured data from DBpedia to label topics. Lau et al. (2011) proposed a method for automatically labelling topics using information from Wikipedia. A set of candidate labels is generated from Wikipedia article titles by querying using topic terms. Additional labels are then generated by chunk parsing the article titles to identify n-grams that represent Wikipedia articles as well. Outlier labels (less relevant to the topic) are identified and removed. Finally, the top-5 topic terms are added to the candidate set. The labels are ranked using Support Vector Regression (SVR) (Vapnik, 1998) and features extracted using word association measures (i.e. PMI, t-test, χ2 and Dice coefficient), lexical features and search engine ranking. Lau et al. (2011) report two versions of their approach, one unsupervised (which is used as a baseline) and another which is supervised. They reported that the supervised version achieves better performance than a previously reported approach (Mei et al., 2007). This paper introduces an alternative graphbased approach which is unsupervised and less computationally intensive than Lau et al. (2011). Our method uses topic keywords to form a query. A grap</context>
</contexts>
<marker>Vapnik, 1998</marker>
<rawString>Vladimir N Vapnik. 1998. Statistical learning theory. Wiley, New York.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>