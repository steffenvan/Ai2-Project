<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.988488">
A Clustering Algorithm for Chinese Adjectives and Nouns&apos;
</title>
<author confidence="0.998578">
Yang Wen&apos;, Chunfa Yuan&apos;, Changning Huang&apos;
</author>
<affiliation confidence="0.77955225">
&apos;State Key Laboratory of Intelligent Technology and System
Deptartment of Computer Science &amp; Technology, Tsinghua University,
Beijing 100084, P.R.C.
&apos;Microsoft Research, China
</affiliation>
<keyword confidence="0.7778366">
Email- ycf@s1000e CS tsinscjina edn cn
Key Words:
bidirectional hierarchical clustering,
collocations, minimum description length,
collocational degree, revisional distance
</keyword>
<sectionHeader confidence="0.988755" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999763818181818">
This paper proposes a bidirctional
hierarchical clustering algorithm for
simultaneously clustering words of different
parts of speech based on collocations. The
algorithm is composed of cycles of two
kinds of alternate clustering processes. We
construct an objective function based on
Minimum Description Length. To partly
solve the problem caused by sparse data two
concepts of collocational degree and
revisional distance are presented.
</bodyText>
<sectionHeader confidence="0.999156" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999791642857143">
Recently research on the compositional
frames (classification and collocational
relationship of words) for Chinese words has
been described in Ji et al. (1996)[1], Ji
(1997)[2]. The objective of their work is to
obtain the clusters of words of different parts
of speech and to derive the collocational
relationship between different clusters from
the collocational relationship between words
of different categories.
There are two ways to construct the
clusters: One is to get clusters from
thesaurus classified manually by linguists.
But the fact is that words with the same
meanings do not always have the same
ability of collocating with other words. The
method isn&apos;t fit for the NLP problems under
our consideration. Another way is to get
clusters automatically by computing on the
distribution environments of words based on
statistical method. The distribution
environment of a word is the set of words of
other parts of speech that can be collocated
with it. We employ the second method in our
work.
Previous research usually gets the
clusters of words of a certain part of speech
based on their distribution environments. But
</bodyText>
<note confidence="0.505031">
Supported by National Natural Science Foundation of China (69773031) and &amp;quot;973&amp;quot; Project (G1998030507).
</note>
<page confidence="0.996865">
124
</page>
<bodyText confidence="0.999817809523809">
we accept the assumption that the clustering
processes of words of different parts of
speech are inherently related. For example,
having collocations between Chinese
adjectives and nouns and if we take on nouns
as entities and adjectives as features of
nouns&apos; distribution environments, we can
obtain clusters of nouns and vice versa. The
key of the relationship of the two clustering
processes is that they use the same
collocations. Therefore we consider the
question of clustering the nouns and
adjectives simultaneously. Li&apos;s work shows
that they optimize the clustering results
based on this viewpoint (Li et al., 1997)[3].
But they don&apos;t explain how to get initial
clusters and their scale of problem is too
small.
In this paper, we propose an algorithm
named bidirectional hierarchical clustering to
attempt answering the question.
</bodyText>
<sectionHeader confidence="0.998349" genericHeader="introduction">
2 Concepts
</sectionHeader>
<subsectionHeader confidence="0.999845">
2.1 Problem Description
</subsectionHeader>
<bodyText confidence="0.998007888888889">
Our problem can be described as follows:
given the set of adjectives A, the set of nouns
N and the collocation instances, our system
will construct a partition Pm over N and a
partition PA over A that respectively contain
sets of nouns and sets of adjectives. And
both partitions meet the condition that words
in the same set (called cluster) have similar
semantic distribution environment.
</bodyText>
<subsectionHeader confidence="0.99982">
2.2 Partitions and Clusters
</subsectionHeader>
<bodyText confidence="0.653856">
Let S be a set, Si cS(i =1,2, ---,n). If
</bodyText>
<equation confidence="0.8605118">
Ps ={ S } satisfies that
(1)U S1 S
i=1
(2) Si n S = 0, Vi, j = 1,2,- • .n,i # j
Then Ps is a partition over S.
</equation>
<bodyText confidence="0.7188656">
In this paper, we call A. EPA an
&amp;quot;adjective cluster&amp;quot; and N, EPN a &amp;quot;noun
cluster&amp;quot;. And we want to obtain the
composition of partitions &lt; PA, PN&gt; as the
clustering result.
</bodyText>
<subsectionHeader confidence="0.290416">
2.3 Distance between Clusters
</subsectionHeader>
<bodyText confidence="0.306149333333333">
• In order to measure the distance between
clusters of the same part of speech, we use
the following equations:
</bodyText>
<equation confidence="0.446953">
(Dino,
cDjuoi
and
disA(4„Ai)=1
(1)
</equation>
<bodyText confidence="0.858229454545455">
environment of 4 and is make up of nouns
which can be collocated with 4. 1-11, is the
distribution environment of N, and is
composed of adjectives which can be
collocated with Ni. (Di and &apos;Ili follow
similar definitions. This distance is a kind of
Euclidean distance.
dis,;(N„N j)= 1
where (1),
(2)
is the distribution
</bodyText>
<page confidence="0.98564">
125
</page>
<subsectionHeader confidence="0.998999">
2.4 Collocational Degree
</subsectionHeader>
<bodyText confidence="0.9974819">
Since redundant collocations might be
created during clustering, the concept
&amp;quot;collocational degree&amp;quot; is used to measure the
collocational relationship between a cluster
and its distribution environment. The
collocational degree is defmed as the ratio of
the existing collocation instances between
the cluster and its distribution environment
to all possible collocations generated by
them. Thus,
</bodyText>
<equation confidence="0.826447">
deg it — 1{4 I a € A1,Ø e (1)„ a0
lAilloil
and
1{nvineN„yET„nyeC)1
degN,—
(4)
</equation>
<bodyText confidence="0.916548">
where C is the set of all existing instances.
</bodyText>
<subsectionHeader confidence="0.998469">
2.5 Redundant Ratio
</subsectionHeader>
<bodyText confidence="0.999454">
After we get the collocational degree of
a cluster, redundant ratio (marked as r) is
calculated to measure the whole performance
of the clustering result. We define the
redundant ratio as 1 minus the ratio of all
existing instances to all possible colloca-
tions generated by all clusters (including
nouns and adjectives) and their distribution
environments. So r is calculated as
</bodyText>
<equation confidence="0.9213495">
21CI
r —1
</equation>
<sectionHeader confidence="0.970799" genericHeader="method">
3 A Bidirectional Hierarchical
</sectionHeader>
<subsectionHeader confidence="0.738214">
Clustering Algorithm
</subsectionHeader>
<bodyText confidence="0.999802111111111">
Usually a hierarchical clustering
algorithm [7] constructs a clustering &amp;quot;tree&amp;quot;
by combining small clusters into large ones
or dividing large clusters into small ones.
The bidirectional hierarchical clustering
algorithm proposed by us is composed of
two kinds of alternate clustering processes.
The algorithm flow is described as
follows:
</bodyText>
<listItem confidence="0.800367333333333">
1) Initially, regard every noun and
adjective each as a cluster. Calculate
the distances between clusters of the
same part of speech.
2) Suppose without loss of generality
that we choose to cluster nouns first.
</listItem>
<bodyText confidence="0.976407111111111">
Select two noun clusters N, &amp; Ari
of the minimum distance and
integrate them into a new one N.
.
j) Calculate the collocational degree of
the new cluster. Adjust the sequence
numbers of the original clusters and
the relational information of adjective
clusters.
</bodyText>
<listItem confidence="0.996968583333333">
4) Calculate the distances between the
new cluster and other clusters.
5) Repeat from step 2) to 4) until the
satisfaction of certain condition. For
example, the number of the clusters
has decreased to certain amount.&apos;
6) Similarly, we can follow the same
steps from 2) to 5) for constructing
adjective clusters, completing one
cycle of clustering processes of nouns
and adjectives.
7) Repeat from step 2) to 6) until the
</listItem>
<figure confidence="0.9510665">
(3)
(5)
pAi to, ± nArtv
I I
</figure>
<footnote confidence="0.838089">
2 In this paper, we set the proportion is 20%.
</footnote>
<page confidence="0.997009">
126
</page>
<bodyText confidence="0.997574153846154">
objective function&apos; reaches the
minimum value.
One advantage of this algorithm is that:
when two clusters of nouns have similar
distribution environments, they might be
classified into one cluster. This information
can be delivered to the clusters of adjectives
that respectively collocate with them by the
clustering process of nouns. Thus these
clusters of adjectives have great possibility
to be combined into one cluster, while the
ordinary hierarchical clustering algorithm
can not do it.
</bodyText>
<sectionHeader confidence="0.998531" genericHeader="method">
4 An Objective Function Based on
MDL
</sectionHeader>
<bodyText confidence="0.999960045454546">
The objective function is designed to
control the processes of clustering words
based on the Minimum Description Length
(MDL) principle. According to MDL, the
best probability model for a given set of data
is a model that uses the shortest code length
for encoding the model itself and the given
data relative to it [4] [5]. We regard the
clusters as the model for the collocations of
adjectives and nouns. The objective function
is defined as the sum of the code length for
the model (&amp;quot;model description length&amp;quot;) and
that for the data (&amp;quot;data description length&amp;quot;).
When the clustering result minimises the
objective function, the bidirectional
processes should be stopped and the result is
the best probable one. The objective function
based on MDL trade-offs between the
simplicity of a model and its accuracy in
fitting to the data, which are respectively
quantified by the model description length
and the data description length.
</bodyText>
<subsectionHeader confidence="0.341642">
Described later in section 4.
</subsectionHeader>
<bodyText confidence="0.991956">
The following are the formulas to
calculate the objective function L:
</bodyText>
<equation confidence="0.929667">
L = Lmod Lam (6)
</equation>
<bodyText confidence="0.8749085">
Loma is the model description length
calculated as
</bodyText>
<table confidence="0.825792">
-NkA 1 14 , 1 x--,k 1 1 (7)
,4:7 ,
= —10g2 — 2.3 —10a — 1
k, km 2 kr
= log2 (k ,k N) +1
</table>
<bodyText confidence="0.749519125">
Where k, and kN respectively denote
the number of clusters of adjectives and
nouns. &amp;quot;+1&amp;quot; means that the algorithm needs
one bit to indicate whether the collocational
relationship between the two clusters exists.
Lda, is composed of the data description
length of adjectives and that of nouns,
namely
</bodyText>
<equation confidence="0.894738">
Ldat = Ldat(A)± Ldat(N)
(8)
</equation>
<bodyText confidence="0.828563666666667">
And the two types of data description
length are calculated as follows
IA. 1 1
</bodyText>
<equation confidence="0.997617571428571">
i=1 kAkN 1og2 1411Nd
Ldat(A)=
€ (Di and 0; G Nk
i±
Lda,(N)= log,
1.1 k AIkNiii=i IN,114I
1
</equation>
<bodyText confidence="0.558034">
VP&amp;quot;; eV, and
</bodyText>
<sectionHeader confidence="0.991351" genericHeader="method">
5 Our Experiment
</sectionHeader>
<bodyText confidence="0.98643">
We take the words and collocations
</bodyText>
<equation confidence="0.6549245">
(9)
(10)
</equation>
<page confidence="0.986587">
127
</page>
<bodyText confidence="0.99986045">
gathered in Ni&apos;s Thesaurus [6] to test our
algorithm. From Ni&apos;s thesaurus, we obtain
2,569 adjectives, 4,536 nouns and 37,346
collocations between adjectives and nouns.
Table 1 shows results of using 5
different revisional distance formulas
discussed in the next section. Because the
length of this paper is limited, we only give
some examples (10 clusters for each part of
speech) of clusters in section 8. We can see
that the redundant ratio obviously decreases
by using the revisional distance, and the
result that has the lowest redundant ratio
corresponds of the minimum value of the
objective function. By human evaluation,
most clusters contain the words that have
similar meanings and distribution
environments. So our algorithm proves to be
effective for word clustering based on
collocations.
</bodyText>
<sectionHeader confidence="0.998901" genericHeader="method">
6 Discussions
</sectionHeader>
<subsectionHeader confidence="0.996942">
6.1 Rivisional Distance
</subsectionHeader>
<bodyText confidence="0.9999465625">
When we combine clusters into a new
cluster, their distribution environments will
be combined as well. The combination of
clusters and their distribution environments
might very likely generate redundant
collocations that are not listed in the
thesaurus. With the word clustering
processes going on, there might be more and
more redundant collocations. They will
obviously affect the accuracy of the
distances between clusters. When calculating
the distances, the redundant collocations
must be considered. So the question is how
to revise the distance equation. Notice that
the collocational degree defmed in the above
measures the collocational relationship
</bodyText>
<tableCaption confidence="0.997807">
Table 1: Results of different revisional distances
</tableCaption>
<table confidence="0.536488333333333">
Revisional distance kA km L r
Not used 409 550 20. 067 99. 01%
dis&apos; = -degln dis 397 610 20. 082 86. 96%
dis&apos; = dis/deg 383 595 20. 002 78. 78%
dis&apos; = dis I NI-deg 373 586 20.017 80.39%
dis&apos; = -disln deg 395 557 20. 007 80. 08%
</table>
<bodyText confidence="0.999959958333333">
However, the redundant ratio is still very
large. The main cause is that existing
instances are too sparse, covering only
0.32% of all possible collocations. So
another advantage of our algorithm is that
we can acquire many new reasonable
collocations not gathered in the thesaurus. If
we add the , new collocations into initial
thesaurus and execute the algorithm on new
data set, the performance will have great
potential to improve. It is further work that
can be carried out in the future.
between a cluster and its distribution
environment. Obviously under the same
distance, clusters having higher collocational
degree have more higher similarity between
each other (because they have more actual
collocations) than those having lower
collocational degree. So the collocational
degree can be used to revise the distance
equations.
There are two problems that should- be
considered when we design the revisional
distance equations. The first one is to convert
</bodyText>
<page confidence="0.995959">
128
</page>
<bodyText confidence="0.9314894">
the collocational degrees of two clusters into
one collocational degree as the revisional
factor for distance equations. It is the
average collocational degree, marked as
deg, calculated by
</bodyText>
<equation confidence="0.86459">
deg 410,1141+ deg Ailt illAjl
degA (1.4 ± IA) No, u j (11)
</equation>
<bodyText confidence="0.5955645">
and
deg N deg Ni IT/ Vri I
</bodyText>
<equation confidence="0.9950135">
degN — (12)
I
</equation>
<bodyText confidence="0.998953388888889">
In fact it is the collocational degree of
the new cluster into which if we assume
combining the two original clusters.
The second problem is that the revjsonal
distance equations should keep coherent of
monotonicity with the original distance. It
means that under the same average collo-
cational degree, the revional distance should
keep the same (or opposite) monotonicity
with the original distance, and under the
same original distance, the revional distance
should keep the same (or opposite)
monotonicity with the average collocational
degree.
In this paper, four simple revisional
distance equations are presented based on
consideration of the upper two problems.
They are:
</bodyText>
<equation confidence="0.713756714285714">
a) dis&apos; = —deg ln dis
b) dis&apos; = dis
deg
. , dis
c)dzs =
1/ deg
d) dis&apos; = —dis In deg
</equation>
<listItem confidence="0.954008666666667">
• Where dis&apos; denotes the revional
distance and dis denotes the original
distance.
</listItem>
<bodyText confidence="0.980964">
From the comparison of the upper
different results (shown in Table 1), we can
draw the conclusion that using revisonal
distance equations can increase the
clustering accuracy remarkably.
</bodyText>
<subsectionHeader confidence="0.9999775">
6.2 Determinant of Objective Function&apos;s
Minimum Value
</subsectionHeader>
<bodyText confidence="0.999862458333333">
The clustering algorithm terminates
when the objective function is minimized.
As a result it is very important to find out the
function&apos;s minimum value. After analyzing
the objective function, we find that it
normally monotonically declines with
clustering processes going on until it gets
minimized. At the beginning, there are a
large number of clusters with only one
element in each of them. So the model
description length is quite large while the
data description length is quite small.
Because the clustering process is hierarchical,
every time when the combination occurs the
number of clusters will decrease by one with
the model description length&apos;s decreasing as
well. At the same time the number of a
certain cluster&apos;s elements will increase by
one with the data description length&apos;s
increment as well. However, the decrement
is larger than the increment and it is getting
smaller while the increment is getting larger.
In this way, the objective function declines
until the objective function reach its
</bodyText>
<page confidence="0.995842">
129
</page>
<bodyText confidence="0.999562666666666">
minimum value. If we continue to execute
the algorithm, we will see that the value of
the objective function rises very fast like as
is shown in Figure 1.
addition, the clustering algorithm may help
to find new collocations that are not in the
thesaurus. This algorithm can also be
extended to other collocation models, such
as verb-noun collocations.
</bodyText>
<figureCaption confidence="0.999611">
Figure 1: Values of the Objective Functions
</figureCaption>
<figure confidence="0.970685555555556">
27
25
23
21
19
17
15
0 1000 2000 3000 4000 5000 6000 7000 8000
quantity of clusters
</figure>
<bodyText confidence="0.99988725">
Therefore we choose a fairly simple
way to avoid the appearance of the local
optimum: When there are two consecutive
increases in the objective function during
one clustering process, stop the process and
start another one. When two consecutive
clustering processes are stopped due to the
same reason, we assume that we have got the
minimum value and stop the whole
clustering process. In our future work we
will try to fmd a better way to determine the
minimum value of the objective function.
</bodyText>
<sectionHeader confidence="0.996117" genericHeader="method">
7 Conclusion &amp; Future Work
</sectionHeader>
<bodyText confidence="0.99692725">
In this paper we have presented a
bidirctional hierarchical clustering algorithm
of simultaneously clustering Chinese
adjectives and nouns based on their
collocations. Our preliminary experiments
show that it can distinguish different words
by their distribution environments. In
Our future work includes:
</bodyText>
<listItem confidence="0.826807666666667">
1) Because the sparsity of collocations
is a main factor of affecting the
word clustering accuracy, we can
use the clustering results to discover
new data and enrich the thesaurus.
2) As there are yet no adjustments to
</listItem>
<bodyText confidence="0.9276136">
the hierarchical clustering results,
we are considering using some
iterative algorithm, such as K-
means algorithm, to optimise the
clustering results.
</bodyText>
<sectionHeader confidence="0.977895" genericHeader="method">
8 Attachment (Examples)
</sectionHeader>
<bodyText confidence="0.999983666666667">
We give 10 clusters of each part of
speech clustered by our algorithm (using
revisional distance formula b) as follows:
</bodyText>
<subsectionHeader confidence="0.993584">
8.1 Chinese Adjective clusters (10 of 383)
</subsectionHeader>
<reference confidence="0.345625">
Al JA,* idtp
A2 ttitgE tfrg &amp;fh if•-al 018
*.gt wf:a ggrAl
II ME fe
311.1-V3
</reference>
<page confidence="0.951396">
130
</page>
<figure confidence="0.93998946875">
A3 g-CiNtJrAs
A4
3TIS *
A5 4- cd- 113 T Fgi
RAW Mg MA Meg
MR 33g We?. AS PiN Pia MS
7% 70 111 V* MA
A6 11Z _DA 3AR
A7 WI, lat?
A8 it ill` tcla ±Al tit API
A9 ax ao TAPA [4]
A10 13.Lt V!iJ V* MS ffi
8.2 Chinese noun clusters (10 of 595) [5]
Ni I4/111 +31 %NW
/Ng&apos; Al
N2 E itjgJr
9ZfTttiZt [6]
/.1NW&apos; /Mk MA WE
N3FM 1)1L, a A
int
N4 itAII1.-
*A tar UM A
N5 EIS t- 74 EA ti At Mill
&apos;:• :•
N6 t W1E ti4 .t1.4S t EN*
KM &apos;NCH V*
N7 t a -4 PIA ffigE At.
Akt
N8 kri 51Itt &apos;INS
tICA
N9 tA rtg N-T Pi*
N10 VIE 1913T faV 04111
</figure>
<sectionHeader confidence="0.725983" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.758084333333333">
[1] Donghong Ji &amp; Changning Huang, A
Semantic Composition Model for
Chinese Noun and Adjective,
</reference>
<figure confidence="0.983682916666667">
[7]
Hang Li &amp; Naoki Abe, Clustering
Words with the MDL Principle, cmp-
1g/9605014 v2, 1996
Wei Xu, The Study of Syntax-
Semantics Integrated Chinese Parsing,
Thesis for the Degree of Master in
Computer Science, Tsinghua University,
1997
Wenjie Ni et al., Modern Chinese
Thesaurus, China People Press, 1984
Zhaoqi Bian et al., Pattern Recognition,
Tsinghua University Press, 1997
Communications of COCIPS 6(1): P25-
P33, 1996
[2] Donghong Ji, Computational Research
on Issues of Lexical Semantics, Post-
doctoral Research Report, Tsinghua
University, P14—P26, 1997
[3] Juanzi Li et al., Two-dimensional
Clustering Based on Compositional
Examples, Language Engineering,
P164-P169, Tsinghua University Press,
1997
</figure>
<page confidence="0.958485">
131
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.065550">
<title confidence="0.999086">A Clustering Algorithm for Chinese Adjectives and Nouns&apos;</title>
<author confidence="0.617668">Yang Wen&apos;</author>
<author confidence="0.617668">Chunfa Yuan&apos;</author>
<author confidence="0.617668">Changning</author>
<affiliation confidence="0.7025315">apos;State Key Laboratory of Intelligent Technology and Deptartment of Computer Science &amp; Technology, Tsinghua</affiliation>
<address confidence="0.83163">Beijing 100084,</address>
<affiliation confidence="0.944569">apos;Microsoft Research,</affiliation>
<keyword confidence="0.6216862">ycf@s1000e edn Key Words: bidirectional hierarchical clustering, collocations, minimum description length, collocational degree, revisional distance</keyword>
<abstract confidence="0.99910425">This paper proposes a bidirctional hierarchical clustering algorithm for simultaneously clustering words of different speech based on collocations. The algorithm is composed of cycles of two kinds of alternate clustering processes. We construct an objective function based on Minimum Description Length. To partly solve the problem caused by sparse data two concepts of collocational degree and revisional distance are presented.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Al JA</author>
</authors>
<title>idtp A2 ttitgE tfrg &amp;fh if•-al 018 *.gt wf:a ggrAl II ME fe 311.1-V3</title>
<marker>JA, </marker>
<rawString>Al JA,* idtp A2 ttitgE tfrg &amp;fh if•-al 018 *.gt wf:a ggrAl II ME fe 311.1-V3</rawString>
</citation>
<citation valid="false">
<title>[1] Donghong Ji &amp; Changning Huang, A Semantic Composition Model for Chinese Noun and Adjective,</title>
<marker></marker>
<rawString>[1] Donghong Ji &amp; Changning Huang, A Semantic Composition Model for Chinese Noun and Adjective,</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>