<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.048511">
<title confidence="0.9963935">
A relatedness benchmark to test the role of determiners
in compositional distributional semantics
</title>
<author confidence="0.958166">
Raffaella Bernardi and Georgiana Dinu and Marco Marelli and Marco Baroni
</author>
<affiliation confidence="0.929098">
Center for Mind/Brain Sciences (University of Trento, Italy)
</affiliation>
<email confidence="0.97831">
first.last@unitn.it
</email>
<sectionHeader confidence="0.993309" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999506538461539">
Distributional models of semantics cap-
ture word meaning very effectively, and
they have been recently extended to ac-
count for compositionally-obtained rep-
resentations of phrases made of content
words. We explore whether compositional
distributional semantic models can also
handle a construction in which grammat-
ical terms play a crucial role, namely de-
terminer phrases (DPs). We introduce a
new publicly available dataset to test dis-
tributional representations of DPs, and we
evaluate state-of-the-art models on this set.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999922096774194">
Distributional semantics models (DSMs) approx-
imate meaning with vectors that record the dis-
tributional occurrence patterns of words in cor-
pora. DSMs have been effectively applied to in-
creasingly more sophisticated semantic tasks in
linguistics, artificial intelligence and cognitive sci-
ence, and they have been recently extended to
capture the meaning of phrases and sentences via
compositional mechanisms. However, scaling up
to larger constituents poses the issue of how to
handle grammatical words, such as determiners,
prepositions, or auxiliaries, that lack rich concep-
tual content, and operate instead as the logical
“glue” holding sentences together.
In typical DSMs, grammatical words are treated
as “stop words” to be discarded, or at best used
as context features in the representation of content
words. Similarly, current compositional DSMs
(cDSMs) focus almost entirely on phrases made
of two or more content words (e.g., adjective-noun
or verb-noun combinations) and completely ig-
nore grammatical words, to the point that even
the test set of transitive sentences proposed by
Grefenstette and Sadrzadeh (2011) contains only
Tarzan-style statements with determiner-less sub-
jects and objects: “table show result”, “priest say
mass”, etc. As these examples suggest, however,
as soon as we set our sight on modeling phrases
and sentences, grammatical words are hard to
avoid. Stripping off grammatical words has more
serious consequences than making you sound like
the Lord of the Jungle. Even if we accept the
view of, e.g., Garrette et al. (2013), that the log-
ical framework of language should be left to other
devices than distributional semantics, and the lat-
ter should be limited to similarity scoring, still ig-
noring grammatical elements is going to dramat-
ically distort the very similarity scores (c)DSMs
should provide. If we want to use a cDSM for
the classic similarity-based paraphrasing task, the
model shouldn’t conclude that “The table shows
many results” is identical to “the table shows no
results” since the two sentences contain the same
content words, or that “to kill many rats” and “to
kill few rats” are equally good paraphrases of “to
exterminate rats”.
We focus here on how cDSMs handle determin-
ers and the phrases they form with nouns (deter-
miner phrases, or DPs).1 While determiners are
only a subset of grammatical words, they are a
large and important subset, constituting the natu-
ral stepping stone towards sentential distributional
semantics: Compositional methods have already
been successfully applied to simple noun-verb and
noun-verb-noun structures (Mitchell and Lapata,
2008; Grefenstette and Sadrzadeh, 2011), and de-
terminers are just what is missing to turn these
skeletal constructions into full-fledged sentences.
Moreover, determiner-noun phrases are, in super-
ficial syntactic terms, similar to the adjective-noun
phrases that have already been extensively studied
from a cDSM perspective by Baroni and Zampar-
</bodyText>
<footnote confidence="0.99819">
1Some linguists refer to what we call DPs as noun phrases
or NPs. We say DPs simply to emphasize our focus on deter-
miners.
</footnote>
<page confidence="0.987395">
53
</page>
<bodyText confidence="0.906161466666667">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 53–57,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
elli (2010), Guevara (2010) and Mitchell and Lap-
ata (2010). Thus, we can straightforwardly extend
the methods already proposed for adjective-noun
phrases to DPs.
We introduce a new task, a similarity-based
challenge, where we consider nouns that are
strongly conceptually related to certain DPs and
test whether cDSMs can pick the most appropri-
ate related DP (e.g., monarchy is more related to
one ruler than many rulers).2 We make our new
dataset publicly available, and we hope that it will
stimulate further work on the distributional seman-
tics of grammatical elements.3
</bodyText>
<sectionHeader confidence="0.970146" genericHeader="method">
2 Composition models
</sectionHeader>
<bodyText confidence="0.984907431818182">
Interest in compositional DSMs has skyrocketed
in the last few years, particularly since the influ-
ential work of Mitchell and Lapata (2008; 2009;
2010), who proposed three simple but effective
composition models. In these models, the com-
posed vectors are obtained through component-
wise operations on the constituent vectors. Given
input vectors u and v, the multiplicative model
(mult) returns a composed vector p with: pi =
uivi. In the weighted additive model (wadd), the
composed vector is a weighted sum of the two in-
put vectors: p = αu + βv, where α and β are two
scalars. Finally, in the dilation model, the output
vector is obtained by first decomposing one of the
input vectors, say v, into a vector parallel to u and
an orthogonal vector. Following this, the parallel
vector is dilated by a factor A before re-combining.
This results in: p = (A − 1)(u, v)u + (u, u)v.
A more general form of the additive model
(fulladd) has been proposed by Guevara (2010)
(see also Zanzotto et al. (2010)). In this approach,
the two vectors to be added are pre-multiplied by
weight matrices estimated from corpus-extracted
examples: p = Au + Bv.
Baroni and Zamparelli (2010) and Coecke et
al. (2010) take inspiration from formal semantics
to characterize composition in terms of function
application. The former model adjective-noun
phrases by treating the adjective as a function from
nouns onto modified nouns. Given that linear
functions can be expressed by matrices and their
application by matrix-by-vector multiplication, a
2Baroni et al. (2012), like us, study determiner phrases
with distributional methods, but they do not model them com-
positionally.
3Dataset and code available from clic.cimec.
unitn.it/composes.
functor (such as the adjective) is represented by a
matrix U to be multiplied with the argument vec-
tor v (e.g., the noun vector): p = Uv. Adjective
matrices are estimated from corpus-extracted ex-
amples of noun vectors and corresponding output
adjective-noun phrase vectors, similarly to Gue-
vara’s approach.4
</bodyText>
<sectionHeader confidence="0.970125" genericHeader="method">
3 The noun-DP relatedness benchmark
</sectionHeader>
<bodyText confidence="0.991644186046512">
Paraphrasing a single word with a phrase is a
natural task for models of compositionality (Tur-
ney, 2012; Zanzotto et al., 2010) and determin-
ers sometimes play a crucial role in defining the
meaning of a noun. For example a trilogy is com-
posed of three works, an assemblage includes sev-
eral things and an orchestra is made of many
musicians. These examples are particularly in-
teresting, since they point to a “conceptual” use
of determiners, as components of the stable and
generic meaning of a content word (as opposed to
situation-dependent deictic and anaphoric usages):
for these determiners the boundary between con-
tent and grammatical word is somewhat blurred,
and they thus provide a good entry point for testing
DSM representations of DPs on a classic similarity
task. In other words, we can set up an experiment
in which having an effective representation of the
determiner is crucial in order to obtain the correct
result.
Using regular expressions over WordNet
glosses (Fellbaum, 1998) and complementing
them with definitions from various online dic-
tionaries, we constructed a list of more than 200
nouns that are strongly conceptually related to a
specific DP. We created a multiple-choice test set
by matching each noun with its associated DP
(target DP), two “foil” DPs sharing the same noun
as the target but combined with other determiners
(same-N foils), one DP made of the target deter-
miner combined with a random noun (same-D
foil), the target determiner (D foil), and the target
noun (Nfoil). A few examples are shown in Table
1. After the materials were checked by all authors,
two native speakers took the multiple-choice test.
We removed the cases (32) where these subjects
provided an unexpected answer. The final set,
4Other approaches to composition in DSMs have been re-
cently proposed by Socher et al. (2012) and Turney (2012).
We leave their empirical evaluation on DPs to further work,
in the first case because it is not trivial to adapt their complex
architecture to our setting; in the other because it is not clear
how Turney would extend his approach to represent DPs.
</bodyText>
<page confidence="0.992497">
54
</page>
<table confidence="0.9439078">
noun target DP same-Nfoil 1 same-Nfoil2 same-D foil D foil Nfoil
duel two opponents various opponents three opponents two engineers two opponents
homeless no home too few homes one home no incision no home
polygamy several wives most wives fewer wives several negotiators several wives
opulence too many goods some goods no goods too many abductions too many goods
</table>
<tableCaption confidence="0.99964">
Table 1: Examples from the noun-DP relatedness benchmark
</tableCaption>
<bodyText confidence="0.822348">
characterized by full subject agreement, contains
173 nouns, each matched with 6 possible answers.
The target DPs contain 23 distinct determiners.
</bodyText>
<sectionHeader confidence="0.997326" genericHeader="method">
4 Setup
</sectionHeader>
<bodyText confidence="0.999978117647059">
Our semantic space provides distributional repre-
sentations of determiners, nouns and DPs. We
considered a set of 50 determiners that include all
those in our benchmark and range from quanti-
fying determiners (every, some... ) and low nu-
merals (one to four), to multi-word units analyzed
as single determiners in the literature, such as a
few, all that, too much. We picked the 20K most
frequent nouns in our source corpus considering
singular and plural forms as separate words, since
number clearly plays an important role in DP se-
mantics. Finally, for each of the target determiners
we added to the space the 2K most frequent DPs
containing that determiner and a target noun.
Co-occurrence statistics were collected from the
concatenation of ukWaC, a mid-2009 dump of the
English Wikipedia and the British National Cor-
pus,5 with a total of 2.8 billion tokens. We use
a bag-of-words approach, counting co-occurrence
with all context words in the same sentence with
a target item. We tuned a number of parameters
on the independent MEN word-relatedness bench-
mark (Bruni et al., 2012). This led us to pick the
top 20K most frequent content word lemmas as
context items, Pointwise Mutual Information as
weighting scheme, and dimensionality reduction
by Non-negative Matrix Factorization.
Except for the parameter-free mult method, pa-
rameters of the composition methods are esti-
mated by minimizing the average Euclidean dis-
tance between the model-generated and corpus-
extracted vectors of the 20K DPs we consider.6
For the lexfunc model, we assume that the deter-
miner is the functor and the noun is the argument,
</bodyText>
<footnote confidence="0.950154428571429">
5wacky.sslmit.unibo.it; www.natcorp.ox.
ac.uk
6All vectors are normalized to unit length before compo-
sition. Note that the objective function used in estimation
minimizes the distance between model-generated and corpus-
extracted vectors. We do not use labeled evaluation data to
optimize the model parameters.
</footnote>
<table confidence="0.999301666666667">
method accuracy method accuracy
lexfunc 39.3 noun 17.3
fulladd 34.7 random 16.7
observed 34.1 mult 12.7
dilation 31.8 determiner 4.6
wadd 23.1
</table>
<tableCaption confidence="0.8629375">
Table 2: Percentage accuracy of composition
methods on the relatedness benchmark
</tableCaption>
<bodyText confidence="0.999846363636363">
and estimate separate matrices representing each
determiner using the 2K DPs in the semantic space
that contain that determiner. For dilation, we treat
direction of stretching as a parameter, finding that
it is better to stretch the noun.
Similarly to the classic TOEFL synonym detec-
tion challenge (Landauer and Dumais, 1997), our
models tackle the relatedness task by measuring
cosines between each target noun and the candi-
date answers and returning the item with the high-
est cosine.
</bodyText>
<sectionHeader confidence="0.99992" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999968260869565">
Table 2 reports the accuracy results (mean ranks
of correct answers confirm the same trend). All
models except mult and determiner outperform the
trivial random guessing baseline, although they
are all well below the 100% accuracy of the hu-
mans who took our test. For the mult method we
observe a very strong bias for choosing a single
word as answer (&gt;60% of the times), which in
the test set is always incorrect. This leads to its
accuracy being below the chance level. We sus-
pect that the highly “intersective” nature of this
model (we obtain very sparse composed DP vec-
tors, only ≈4% dense) leads to it not being a re-
liable method for comparing sequences of words
of different length: Shorter sequences will be con-
sidered more similar due to their higher density.
The determiner-only baseline (using the vector of
the component determiner as surrogate for the DP)
fails because D vectors tend to be far from N vec-
tors, thus the N foil is often preferred to the correct
response (that is represented, for this baseline, by
its D). In the noun-only baseline (use the vector
of the component noun as surrogate for the DP),
</bodyText>
<page confidence="0.996726">
55
</page>
<bodyText confidence="0.999969054054054">
the correct response is identical to the same-N and
N foils, thus forcing a random choice between
these. Not surprisingly, this approach performs
quite badly. The observed DP vectors extracted di-
rectly from the corpus compete with the top com-
positional methods, but do not surpass them.7
The lexfunc method is the best compositional
model, indicating that its added flexibility in mod-
eling composition pays off empirically. The ful-
ladd model is not as good, but also performs well.
The wadd and especially dilation models perform
relatively well, but they are penalized by the fact
that they assign more weight to the noun vectors,
making the right answer dangerously similar to the
same-N and N foils.
Taking a closer look at the performance of the
best model (lexfunc), we observe that it is not
equally distributed across determiners. Focusing
on those determiners appearing in at least 4 cor-
rect answers, they range from those where lexfunc
performance was very significantly above chance
(p&lt;0.001 of equal or higher chance performance):
too few, all, four, too much, less, several; to
those on which performance was still significant
but less impressively so (0.001 &lt; p &lt; 0.05): sev-
eral, no, various, most, two, too many, many, one;
to those where performance was not significantly
better than chance at the 0.05 level: much, more,
three, another. Given that, on the one hand, per-
formance is not constant across determiners, and
on the other no obvious groupings can account
for their performance difference (compare the ex-
cellent lexfunc performance on four to the lousy
one on three!), future research should explore the
contextual properties of specific determiners that
make them more or less amenable to be captured
by compositional DSMs.
</bodyText>
<sectionHeader confidence="0.995036" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.998285444444444">
DSMs, even when applied to phrases, are typically
seen as models of content word meaning. How-
ever, to scale up compositionally beyond the sim-
plest constructions, cDSMs must deal with gram-
matical terms such as determiners. This paper
started exploring this issue by introducing a new
and publicly available set testing DP semantics in
a similarity-based task and using it to systemati-
cally evaluate, for the first time, cDSMs on a con-
</bodyText>
<footnote confidence="0.86191525">
7The observed method is in fact at advantage in our ex-
periment because a considerable number of DP foils are not
found in the corpus and are assigned similarity 0 with the tar-
get.
</footnote>
<bodyText confidence="0.99997436">
struction involving grammatical words. The most
important take-home message is that distributional
representations are rich enough to encode infor-
mation about determiners, achieving performance
well above chance on the new benchmark.
Theoretical considerations would lead one to
expect a “functional” approach to determiner rep-
resentations along the lines of Baroni and Zampar-
elli (2010) and Coecke et al. (2010) to outperform
those approaches that combine vectors separately
representing determiners and nouns. This predic-
tion was largely borne out in the results, although
the additive models, and particularly fulladd, were
competitive rivals.
We attempted to capture the distributional se-
mantics of DPs using a fairly standard, “vanilla”
semantic space characterized by latent dimensions
that summarize patterns of co-occurrence with
content word contexts. By inspecting the con-
text words that are most associated with the var-
ious latent dimensions we obtained through Non-
negative Matrix Factorization, we notice how they
are capturing broad, “topical” aspects of meaning
(the first dimension is represented by scripture, be-
liever, resurrection, the fourth by fever, infection,
infected, and so on). Considering the sort of se-
mantic space we used (which we took to be a rea-
sonable starting point because of its effectiveness
in a standard lexical task), it is actually surpris-
ing that we obtained the significant results we ob-
tained. Thus, a top priority in future work is to ex-
plore different contextual features, such as adverbs
and grammatical terms, that might carry informa-
tion that is more directly relevant to the semantics
of determiners.
Another important line of research pertains to
improving composition methods: Although the
best model, at 40% accuracy, is well above chance,
we are still far from the 100% performance of hu-
mans. We will try, in particular, to include non-
linear transformations in the spirit of Socher et al.
(2012), and look for better ways to automatically
select training data.
Last but not least, in the near future we
would like to test if cDSMs, besides dealing with
similarity-based aspects of determiner meaning,
can also help in capturing those formal properties
of determiners, such as monotonicity or definite-
ness, that theoretical semanticists have been tradi-
tionally interested in.
</bodyText>
<page confidence="0.995882">
56
</page>
<sectionHeader confidence="0.998453" genericHeader="acknowledgments">
7 Acknowledgments
</sectionHeader>
<bodyText confidence="0.996240333333333">
This research was supported by the ERC 2011
Starting Independent Research Grant n. 283554
(COMPOSES).
</bodyText>
<sectionHeader confidence="0.990054" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999831655737705">
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of EMNLP, pages 1183–1193, Boston,
MA.
Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do,
and Chung-Chieh Shan. 2012. Entailment above
the word level in distributional semantics. In Pro-
ceedings of EACL, pages 23–32, Avignon, France.
Elia Bruni, Gemma Boleda, Marco Baroni, and
Nam Khanh Tran. 2012. Distributional semantics
in Technicolor. In Proceedings of ACL, pages 136–
145, Jeju Island, Korea.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen
Clark. 2010. Mathematical foundations for a com-
positional distributional model of meaning. Linguis-
tic Analysis, 36:345–384.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
MA.
Dan Garrette, Katrin Erk, and Ray Mooney. 2013. A
formal approach to linking logical form and vector-
space lexical semantics. In H. Bunt, J. Bos, and
S. Pulman, editors, Computing Meaning, Vol. 4. In
press.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical composi-
tional distributional model of meaning. In Proceed-
ings of EMNLP, pages 1394–1404, Edinburgh, UK.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of GEMS, pages 33–37,
Uppsala, Sweden.
Thomas Landauer and Susan Dumais. 1997. A solu-
tion to Plato’s problem: The latent semantic analysis
theory of acquisition, induction, and representation
of knowledge. Psychological Review, 104(2):211–
240.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL, pages 236–244, Columbus, OH.
Jeff Mitchell and Mirella Lapata. 2009. Language
models based on semantic composition. In Proceed-
ings of EMNLP, pages 430–439, Singapore.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388–1429.
Richard Socher, Brody Huval, Christopher Manning,
and Andrew Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceed-
ings of EMNLP, pages 1201–1211, Jeju Island, Ko-
rea.
Peter Turney. 2012. Domain and function: A dual-
space model of semantic relations and compositions.
Journal of Artificial Intelligence Research, 44:533–
585.
Fabio Zanzotto, Ioannis Korkontzelos, Francesca
Falucchi, and Suresh Manandhar. 2010. Estimat-
ing linear models for compositional distributional
semantics. In Proceedings of COLING, pages 1263–
1271, Beijing, China.
</reference>
<page confidence="0.999145">
57
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.955808">
<title confidence="0.979278">A relatedness benchmark to test the role of in compositional distributional semantics</title>
<author confidence="0.991414">Bernardi Dinu Marelli</author>
<affiliation confidence="0.999425">Center for Mind/Brain Sciences (University of Trento,</affiliation>
<email confidence="0.99932">first.last@unitn.it</email>
<abstract confidence="0.999359785714286">Distributional models of semantics capture word meaning very effectively, and they have been recently extended to account for compositionally-obtained representations of phrases made of content words. We explore whether compositional distributional semantic models can also handle a construction in which grammatical terms play a crucial role, namely determiner phrases (DPs). We introduce a new publicly available dataset to test distributional representations of DPs, and we evaluate state-of-the-art models on this set.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1183--1193</pages>
<location>Boston, MA.</location>
<contexts>
<context position="5831" citStr="Baroni and Zamparelli (2010)" startWordPosition="905" endWordPosition="908">u + βv, where α and β are two scalars. Finally, in the dilation model, the output vector is obtained by first decomposing one of the input vectors, say v, into a vector parallel to u and an orthogonal vector. Following this, the parallel vector is dilated by a factor A before re-combining. This results in: p = (A − 1)(u, v)u + (u, u)v. A more general form of the additive model (fulladd) has been proposed by Guevara (2010) (see also Zanzotto et al. (2010)). In this approach, the two vectors to be added are pre-multiplied by weight matrices estimated from corpus-extracted examples: p = Au + Bv. Baroni and Zamparelli (2010) and Coecke et al. (2010) take inspiration from formal semantics to characterize composition in terms of function application. The former model adjective-noun phrases by treating the adjective as a function from nouns onto modified nouns. Given that linear functions can be expressed by matrices and their application by matrix-by-vector multiplication, a 2Baroni et al. (2012), like us, study determiner phrases with distributional methods, but they do not model them compositionally. 3Dataset and code available from clic.cimec. unitn.it/composes. functor (such as the adjective) is represented by </context>
<context position="15933" citStr="Baroni and Zamparelli (2010)" startWordPosition="2541" endWordPosition="2545">o systematically evaluate, for the first time, cDSMs on a con7The observed method is in fact at advantage in our experiment because a considerable number of DP foils are not found in the corpus and are assigned similarity 0 with the target. struction involving grammatical words. The most important take-home message is that distributional representations are rich enough to encode information about determiners, achieving performance well above chance on the new benchmark. Theoretical considerations would lead one to expect a “functional” approach to determiner representations along the lines of Baroni and Zamparelli (2010) and Coecke et al. (2010) to outperform those approaches that combine vectors separately representing determiners and nouns. This prediction was largely borne out in the results, although the additive models, and particularly fulladd, were competitive rivals. We attempted to capture the distributional semantics of DPs using a fairly standard, “vanilla” semantic space characterized by latent dimensions that summarize patterns of co-occurrence with content word contexts. By inspecting the context words that are most associated with the various latent dimensions we obtained through Nonnegative Ma</context>
</contexts>
<marker>Baroni, Zamparelli, 2010</marker>
<rawString>Marco Baroni and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space. In Proceedings of EMNLP, pages 1183–1193, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Raffaella Bernardi</author>
<author>Ngoc-Quynh Do</author>
<author>Chung-Chieh Shan</author>
</authors>
<title>Entailment above the word level in distributional semantics.</title>
<date>2012</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>23--32</pages>
<location>Avignon, France.</location>
<contexts>
<context position="6208" citStr="Baroni et al. (2012)" startWordPosition="960" endWordPosition="963">add) has been proposed by Guevara (2010) (see also Zanzotto et al. (2010)). In this approach, the two vectors to be added are pre-multiplied by weight matrices estimated from corpus-extracted examples: p = Au + Bv. Baroni and Zamparelli (2010) and Coecke et al. (2010) take inspiration from formal semantics to characterize composition in terms of function application. The former model adjective-noun phrases by treating the adjective as a function from nouns onto modified nouns. Given that linear functions can be expressed by matrices and their application by matrix-by-vector multiplication, a 2Baroni et al. (2012), like us, study determiner phrases with distributional methods, but they do not model them compositionally. 3Dataset and code available from clic.cimec. unitn.it/composes. functor (such as the adjective) is represented by a matrix U to be multiplied with the argument vector v (e.g., the noun vector): p = Uv. Adjective matrices are estimated from corpus-extracted examples of noun vectors and corresponding output adjective-noun phrase vectors, similarly to Guevara’s approach.4 3 The noun-DP relatedness benchmark Paraphrasing a single word with a phrase is a natural task for models of compositio</context>
</contexts>
<marker>Baroni, Bernardi, Do, Shan, 2012</marker>
<rawString>Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do, and Chung-Chieh Shan. 2012. Entailment above the word level in distributional semantics. In Proceedings of EACL, pages 23–32, Avignon, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elia Bruni</author>
<author>Gemma Boleda</author>
<author>Marco Baroni</author>
<author>Nam Khanh Tran</author>
</authors>
<title>Distributional semantics in Technicolor.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>136--145</pages>
<location>Jeju Island,</location>
<contexts>
<context position="10489" citStr="Bruni et al., 2012" startWordPosition="1658" endWordPosition="1661">eparate words, since number clearly plays an important role in DP semantics. Finally, for each of the target determiners we added to the space the 2K most frequent DPs containing that determiner and a target noun. Co-occurrence statistics were collected from the concatenation of ukWaC, a mid-2009 dump of the English Wikipedia and the British National Corpus,5 with a total of 2.8 billion tokens. We use a bag-of-words approach, counting co-occurrence with all context words in the same sentence with a target item. We tuned a number of parameters on the independent MEN word-relatedness benchmark (Bruni et al., 2012). This led us to pick the top 20K most frequent content word lemmas as context items, Pointwise Mutual Information as weighting scheme, and dimensionality reduction by Non-negative Matrix Factorization. Except for the parameter-free mult method, parameters of the composition methods are estimated by minimizing the average Euclidean distance between the model-generated and corpusextracted vectors of the 20K DPs we consider.6 For the lexfunc model, we assume that the determiner is the functor and the noun is the argument, 5wacky.sslmit.unibo.it; www.natcorp.ox. ac.uk 6All vectors are normalized </context>
</contexts>
<marker>Bruni, Boleda, Baroni, Tran, 2012</marker>
<rawString>Elia Bruni, Gemma Boleda, Marco Baroni, and Nam Khanh Tran. 2012. Distributional semantics in Technicolor. In Proceedings of ACL, pages 136– 145, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Coecke</author>
<author>Mehrnoosh Sadrzadeh</author>
<author>Stephen Clark</author>
</authors>
<title>Mathematical foundations for a compositional distributional model of meaning. Linguistic Analysis,</title>
<date>2010</date>
<pages>36--345</pages>
<contexts>
<context position="5856" citStr="Coecke et al. (2010)" startWordPosition="910" endWordPosition="913">lars. Finally, in the dilation model, the output vector is obtained by first decomposing one of the input vectors, say v, into a vector parallel to u and an orthogonal vector. Following this, the parallel vector is dilated by a factor A before re-combining. This results in: p = (A − 1)(u, v)u + (u, u)v. A more general form of the additive model (fulladd) has been proposed by Guevara (2010) (see also Zanzotto et al. (2010)). In this approach, the two vectors to be added are pre-multiplied by weight matrices estimated from corpus-extracted examples: p = Au + Bv. Baroni and Zamparelli (2010) and Coecke et al. (2010) take inspiration from formal semantics to characterize composition in terms of function application. The former model adjective-noun phrases by treating the adjective as a function from nouns onto modified nouns. Given that linear functions can be expressed by matrices and their application by matrix-by-vector multiplication, a 2Baroni et al. (2012), like us, study determiner phrases with distributional methods, but they do not model them compositionally. 3Dataset and code available from clic.cimec. unitn.it/composes. functor (such as the adjective) is represented by a matrix U to be multipli</context>
<context position="15958" citStr="Coecke et al. (2010)" startWordPosition="2547" endWordPosition="2550">e first time, cDSMs on a con7The observed method is in fact at advantage in our experiment because a considerable number of DP foils are not found in the corpus and are assigned similarity 0 with the target. struction involving grammatical words. The most important take-home message is that distributional representations are rich enough to encode information about determiners, achieving performance well above chance on the new benchmark. Theoretical considerations would lead one to expect a “functional” approach to determiner representations along the lines of Baroni and Zamparelli (2010) and Coecke et al. (2010) to outperform those approaches that combine vectors separately representing determiners and nouns. This prediction was largely borne out in the results, although the additive models, and particularly fulladd, were competitive rivals. We attempted to capture the distributional semantics of DPs using a fairly standard, “vanilla” semantic space characterized by latent dimensions that summarize patterns of co-occurrence with content word contexts. By inspecting the context words that are most associated with the various latent dimensions we obtained through Nonnegative Matrix Factorization, we no</context>
</contexts>
<marker>Coecke, Sadrzadeh, Clark, 2010</marker>
<rawString>Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark. 2010. Mathematical foundations for a compositional distributional model of meaning. Linguistic Analysis, 36:345–384.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Garrette</author>
<author>Katrin Erk</author>
<author>Ray Mooney</author>
</authors>
<title>A formal approach to linking logical form and vectorspace lexical semantics.</title>
<date>2013</date>
<journal>Computing Meaning,</journal>
<volume>4</volume>
<editor>In H. Bunt, J. Bos, and S. Pulman, editors,</editor>
<publisher>In press.</publisher>
<contexts>
<context position="2356" citStr="Garrette et al. (2013)" startWordPosition="345" endWordPosition="348">djective-noun or verb-noun combinations) and completely ignore grammatical words, to the point that even the test set of transitive sentences proposed by Grefenstette and Sadrzadeh (2011) contains only Tarzan-style statements with determiner-less subjects and objects: “table show result”, “priest say mass”, etc. As these examples suggest, however, as soon as we set our sight on modeling phrases and sentences, grammatical words are hard to avoid. Stripping off grammatical words has more serious consequences than making you sound like the Lord of the Jungle. Even if we accept the view of, e.g., Garrette et al. (2013), that the logical framework of language should be left to other devices than distributional semantics, and the latter should be limited to similarity scoring, still ignoring grammatical elements is going to dramatically distort the very similarity scores (c)DSMs should provide. If we want to use a cDSM for the classic similarity-based paraphrasing task, the model shouldn’t conclude that “The table shows many results” is identical to “the table shows no results” since the two sentences contain the same content words, or that “to kill many rats” and “to kill few rats” are equally good paraphras</context>
</contexts>
<marker>Garrette, Erk, Mooney, 2013</marker>
<rawString>Dan Garrette, Katrin Erk, and Ray Mooney. 2013. A formal approach to linking logical form and vectorspace lexical semantics. In H. Bunt, J. Bos, and S. Pulman, editors, Computing Meaning, Vol. 4. In press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Grefenstette</author>
<author>Mehrnoosh Sadrzadeh</author>
</authors>
<title>Experimental support for a categorical compositional distributional model of meaning.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1394--1404</pages>
<location>Edinburgh, UK.</location>
<contexts>
<context position="1921" citStr="Grefenstette and Sadrzadeh (2011)" startWordPosition="275" endWordPosition="278">tical words, such as determiners, prepositions, or auxiliaries, that lack rich conceptual content, and operate instead as the logical “glue” holding sentences together. In typical DSMs, grammatical words are treated as “stop words” to be discarded, or at best used as context features in the representation of content words. Similarly, current compositional DSMs (cDSMs) focus almost entirely on phrases made of two or more content words (e.g., adjective-noun or verb-noun combinations) and completely ignore grammatical words, to the point that even the test set of transitive sentences proposed by Grefenstette and Sadrzadeh (2011) contains only Tarzan-style statements with determiner-less subjects and objects: “table show result”, “priest say mass”, etc. As these examples suggest, however, as soon as we set our sight on modeling phrases and sentences, grammatical words are hard to avoid. Stripping off grammatical words has more serious consequences than making you sound like the Lord of the Jungle. Even if we accept the view of, e.g., Garrette et al. (2013), that the logical framework of language should be left to other devices than distributional semantics, and the latter should be limited to similarity scoring, still</context>
<context position="3453" citStr="Grefenstette and Sadrzadeh, 2011" startWordPosition="516" endWordPosition="519">s” since the two sentences contain the same content words, or that “to kill many rats” and “to kill few rats” are equally good paraphrases of “to exterminate rats”. We focus here on how cDSMs handle determiners and the phrases they form with nouns (determiner phrases, or DPs).1 While determiners are only a subset of grammatical words, they are a large and important subset, constituting the natural stepping stone towards sentential distributional semantics: Compositional methods have already been successfully applied to simple noun-verb and noun-verb-noun structures (Mitchell and Lapata, 2008; Grefenstette and Sadrzadeh, 2011), and determiners are just what is missing to turn these skeletal constructions into full-fledged sentences. Moreover, determiner-noun phrases are, in superficial syntactic terms, similar to the adjective-noun phrases that have already been extensively studied from a cDSM perspective by Baroni and Zampar1Some linguists refer to what we call DPs as noun phrases or NPs. We say DPs simply to emphasize our focus on determiners. 53 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 53–57, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computatio</context>
</contexts>
<marker>Grefenstette, Sadrzadeh, 2011</marker>
<rawString>Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011. Experimental support for a categorical compositional distributional model of meaning. In Proceedings of EMNLP, pages 1394–1404, Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emiliano Guevara</author>
</authors>
<title>A regression model of adjective-noun compositionality in distributional semantics.</title>
<date>2010</date>
<booktitle>In Proceedings of GEMS,</booktitle>
<pages>33--37</pages>
<location>Uppsala,</location>
<contexts>
<context position="4096" citStr="Guevara (2010)" startWordPosition="615" endWordPosition="616">at is missing to turn these skeletal constructions into full-fledged sentences. Moreover, determiner-noun phrases are, in superficial syntactic terms, similar to the adjective-noun phrases that have already been extensively studied from a cDSM perspective by Baroni and Zampar1Some linguists refer to what we call DPs as noun phrases or NPs. We say DPs simply to emphasize our focus on determiners. 53 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 53–57, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics elli (2010), Guevara (2010) and Mitchell and Lapata (2010). Thus, we can straightforwardly extend the methods already proposed for adjective-noun phrases to DPs. We introduce a new task, a similarity-based challenge, where we consider nouns that are strongly conceptually related to certain DPs and test whether cDSMs can pick the most appropriate related DP (e.g., monarchy is more related to one ruler than many rulers).2 We make our new dataset publicly available, and we hope that it will stimulate further work on the distributional semantics of grammatical elements.3 2 Composition models Interest in compositional DSMs h</context>
<context position="5628" citStr="Guevara (2010)" startWordPosition="874" endWordPosition="875"> v, the multiplicative model (mult) returns a composed vector p with: pi = uivi. In the weighted additive model (wadd), the composed vector is a weighted sum of the two input vectors: p = αu + βv, where α and β are two scalars. Finally, in the dilation model, the output vector is obtained by first decomposing one of the input vectors, say v, into a vector parallel to u and an orthogonal vector. Following this, the parallel vector is dilated by a factor A before re-combining. This results in: p = (A − 1)(u, v)u + (u, u)v. A more general form of the additive model (fulladd) has been proposed by Guevara (2010) (see also Zanzotto et al. (2010)). In this approach, the two vectors to be added are pre-multiplied by weight matrices estimated from corpus-extracted examples: p = Au + Bv. Baroni and Zamparelli (2010) and Coecke et al. (2010) take inspiration from formal semantics to characterize composition in terms of function application. The former model adjective-noun phrases by treating the adjective as a function from nouns onto modified nouns. Given that linear functions can be expressed by matrices and their application by matrix-by-vector multiplication, a 2Baroni et al. (2012), like us, study det</context>
</contexts>
<marker>Guevara, 2010</marker>
<rawString>Emiliano Guevara. 2010. A regression model of adjective-noun compositionality in distributional semantics. In Proceedings of GEMS, pages 33–37, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Landauer</author>
<author>Susan Dumais</author>
</authors>
<title>A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<issue>2</issue>
<pages>240</pages>
<contexts>
<context position="11873" citStr="Landauer and Dumais, 1997" startWordPosition="1867" endWordPosition="1870">ors. We do not use labeled evaluation data to optimize the model parameters. method accuracy method accuracy lexfunc 39.3 noun 17.3 fulladd 34.7 random 16.7 observed 34.1 mult 12.7 dilation 31.8 determiner 4.6 wadd 23.1 Table 2: Percentage accuracy of composition methods on the relatedness benchmark and estimate separate matrices representing each determiner using the 2K DPs in the semantic space that contain that determiner. For dilation, we treat direction of stretching as a parameter, finding that it is better to stretch the noun. Similarly to the classic TOEFL synonym detection challenge (Landauer and Dumais, 1997), our models tackle the relatedness task by measuring cosines between each target noun and the candidate answers and returning the item with the highest cosine. 5 Results Table 2 reports the accuracy results (mean ranks of correct answers confirm the same trend). All models except mult and determiner outperform the trivial random guessing baseline, although they are all well below the 100% accuracy of the humans who took our test. For the mult method we observe a very strong bias for choosing a single word as answer (&gt;60% of the times), which in the test set is always incorrect. This leads to </context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Thomas Landauer and Susan Dumais. 1997. A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological Review, 104(2):211– 240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Vector-based models of semantic composition.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>236--244</pages>
<location>Columbus, OH.</location>
<contexts>
<context position="3418" citStr="Mitchell and Lapata, 2008" startWordPosition="512" endWordPosition="515"> “the table shows no results” since the two sentences contain the same content words, or that “to kill many rats” and “to kill few rats” are equally good paraphrases of “to exterminate rats”. We focus here on how cDSMs handle determiners and the phrases they form with nouns (determiner phrases, or DPs).1 While determiners are only a subset of grammatical words, they are a large and important subset, constituting the natural stepping stone towards sentential distributional semantics: Compositional methods have already been successfully applied to simple noun-verb and noun-verb-noun structures (Mitchell and Lapata, 2008; Grefenstette and Sadrzadeh, 2011), and determiners are just what is missing to turn these skeletal constructions into full-fledged sentences. Moreover, determiner-noun phrases are, in superficial syntactic terms, similar to the adjective-noun phrases that have already been extensively studied from a cDSM perspective by Baroni and Zampar1Some linguists refer to what we call DPs as noun phrases or NPs. We say DPs simply to emphasize our focus on determiners. 53 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 53–57, Sofia, Bulgaria, August 4-9 2013</context>
<context position="4802" citStr="Mitchell and Lapata (2008" startWordPosition="725" endWordPosition="728">s already proposed for adjective-noun phrases to DPs. We introduce a new task, a similarity-based challenge, where we consider nouns that are strongly conceptually related to certain DPs and test whether cDSMs can pick the most appropriate related DP (e.g., monarchy is more related to one ruler than many rulers).2 We make our new dataset publicly available, and we hope that it will stimulate further work on the distributional semantics of grammatical elements.3 2 Composition models Interest in compositional DSMs has skyrocketed in the last few years, particularly since the influential work of Mitchell and Lapata (2008; 2009; 2010), who proposed three simple but effective composition models. In these models, the composed vectors are obtained through componentwise operations on the constituent vectors. Given input vectors u and v, the multiplicative model (mult) returns a composed vector p with: pi = uivi. In the weighted additive model (wadd), the composed vector is a weighted sum of the two input vectors: p = αu + βv, where α and β are two scalars. Finally, in the dilation model, the output vector is obtained by first decomposing one of the input vectors, say v, into a vector parallel to u and an orthogona</context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. In Proceedings of ACL, pages 236–244, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Language models based on semantic composition.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>430--439</pages>
<marker>Mitchell, Lapata, 2009</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2009. Language models based on semantic composition. In Proceedings of EMNLP, pages 430–439, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Composition in distributional models of semantics.</title>
<date>2010</date>
<journal>Cognitive Science,</journal>
<volume>34</volume>
<issue>8</issue>
<contexts>
<context position="4127" citStr="Mitchell and Lapata (2010)" startWordPosition="618" endWordPosition="622">rn these skeletal constructions into full-fledged sentences. Moreover, determiner-noun phrases are, in superficial syntactic terms, similar to the adjective-noun phrases that have already been extensively studied from a cDSM perspective by Baroni and Zampar1Some linguists refer to what we call DPs as noun phrases or NPs. We say DPs simply to emphasize our focus on determiners. 53 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 53–57, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics elli (2010), Guevara (2010) and Mitchell and Lapata (2010). Thus, we can straightforwardly extend the methods already proposed for adjective-noun phrases to DPs. We introduce a new task, a similarity-based challenge, where we consider nouns that are strongly conceptually related to certain DPs and test whether cDSMs can pick the most appropriate related DP (e.g., monarchy is more related to one ruler than many rulers).2 We make our new dataset publicly available, and we hope that it will stimulate further work on the distributional semantics of grammatical elements.3 2 Composition models Interest in compositional DSMs has skyrocketed in the last few </context>
</contexts>
<marker>Mitchell, Lapata, 2010</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2010. Composition in distributional models of semantics. Cognitive Science, 34(8):1388–1429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher Manning</author>
<author>Andrew Ng</author>
</authors>
<title>Semantic compositionality through recursive matrix-vector spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1201--1211</pages>
<location>Jeju Island,</location>
<contexts>
<context position="8559" citStr="Socher et al. (2012)" startWordPosition="1341" endWordPosition="1344">atching each noun with its associated DP (target DP), two “foil” DPs sharing the same noun as the target but combined with other determiners (same-N foils), one DP made of the target determiner combined with a random noun (same-D foil), the target determiner (D foil), and the target noun (Nfoil). A few examples are shown in Table 1. After the materials were checked by all authors, two native speakers took the multiple-choice test. We removed the cases (32) where these subjects provided an unexpected answer. The final set, 4Other approaches to composition in DSMs have been recently proposed by Socher et al. (2012) and Turney (2012). We leave their empirical evaluation on DPs to further work, in the first case because it is not trivial to adapt their complex architecture to our setting; in the other because it is not clear how Turney would extend his approach to represent DPs. 54 noun target DP same-Nfoil 1 same-Nfoil2 same-D foil D foil Nfoil duel two opponents various opponents three opponents two engineers two opponents homeless no home too few homes one home no incision no home polygamy several wives most wives fewer wives several negotiators several wives opulence too many goods some goods no goods</context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Christopher Manning, and Andrew Ng. 2012. Semantic compositionality through recursive matrix-vector spaces. In Proceedings of EMNLP, pages 1201–1211, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
</authors>
<title>Domain and function: A dualspace model of semantic relations and compositions.</title>
<date>2012</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>44</volume>
<pages>585</pages>
<contexts>
<context position="6828" citStr="Turney, 2012" startWordPosition="1056" endWordPosition="1058">s, study determiner phrases with distributional methods, but they do not model them compositionally. 3Dataset and code available from clic.cimec. unitn.it/composes. functor (such as the adjective) is represented by a matrix U to be multiplied with the argument vector v (e.g., the noun vector): p = Uv. Adjective matrices are estimated from corpus-extracted examples of noun vectors and corresponding output adjective-noun phrase vectors, similarly to Guevara’s approach.4 3 The noun-DP relatedness benchmark Paraphrasing a single word with a phrase is a natural task for models of compositionality (Turney, 2012; Zanzotto et al., 2010) and determiners sometimes play a crucial role in defining the meaning of a noun. For example a trilogy is composed of three works, an assemblage includes several things and an orchestra is made of many musicians. These examples are particularly interesting, since they point to a “conceptual” use of determiners, as components of the stable and generic meaning of a content word (as opposed to situation-dependent deictic and anaphoric usages): for these determiners the boundary between content and grammatical word is somewhat blurred, and they thus provide a good entry po</context>
<context position="8577" citStr="Turney (2012)" startWordPosition="1346" endWordPosition="1347">s associated DP (target DP), two “foil” DPs sharing the same noun as the target but combined with other determiners (same-N foils), one DP made of the target determiner combined with a random noun (same-D foil), the target determiner (D foil), and the target noun (Nfoil). A few examples are shown in Table 1. After the materials were checked by all authors, two native speakers took the multiple-choice test. We removed the cases (32) where these subjects provided an unexpected answer. The final set, 4Other approaches to composition in DSMs have been recently proposed by Socher et al. (2012) and Turney (2012). We leave their empirical evaluation on DPs to further work, in the first case because it is not trivial to adapt their complex architecture to our setting; in the other because it is not clear how Turney would extend his approach to represent DPs. 54 noun target DP same-Nfoil 1 same-Nfoil2 same-D foil D foil Nfoil duel two opponents various opponents three opponents two engineers two opponents homeless no home too few homes one home no incision no home polygamy several wives most wives fewer wives several negotiators several wives opulence too many goods some goods no goods too many abductio</context>
</contexts>
<marker>Turney, 2012</marker>
<rawString>Peter Turney. 2012. Domain and function: A dualspace model of semantic relations and compositions. Journal of Artificial Intelligence Research, 44:533– 585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Zanzotto</author>
<author>Ioannis Korkontzelos</author>
<author>Francesca Falucchi</author>
<author>Suresh Manandhar</author>
</authors>
<title>Estimating linear models for compositional distributional semantics.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>1263--1271</pages>
<location>Beijing, China.</location>
<contexts>
<context position="5661" citStr="Zanzotto et al. (2010)" startWordPosition="878" endWordPosition="881">del (mult) returns a composed vector p with: pi = uivi. In the weighted additive model (wadd), the composed vector is a weighted sum of the two input vectors: p = αu + βv, where α and β are two scalars. Finally, in the dilation model, the output vector is obtained by first decomposing one of the input vectors, say v, into a vector parallel to u and an orthogonal vector. Following this, the parallel vector is dilated by a factor A before re-combining. This results in: p = (A − 1)(u, v)u + (u, u)v. A more general form of the additive model (fulladd) has been proposed by Guevara (2010) (see also Zanzotto et al. (2010)). In this approach, the two vectors to be added are pre-multiplied by weight matrices estimated from corpus-extracted examples: p = Au + Bv. Baroni and Zamparelli (2010) and Coecke et al. (2010) take inspiration from formal semantics to characterize composition in terms of function application. The former model adjective-noun phrases by treating the adjective as a function from nouns onto modified nouns. Given that linear functions can be expressed by matrices and their application by matrix-by-vector multiplication, a 2Baroni et al. (2012), like us, study determiner phrases with distribution</context>
</contexts>
<marker>Zanzotto, Korkontzelos, Falucchi, Manandhar, 2010</marker>
<rawString>Fabio Zanzotto, Ioannis Korkontzelos, Francesca Falucchi, and Suresh Manandhar. 2010. Estimating linear models for compositional distributional semantics. In Proceedings of COLING, pages 1263– 1271, Beijing, China.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>