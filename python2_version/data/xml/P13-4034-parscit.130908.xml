<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000958">
<title confidence="0.9485375">
Mr. MIRA: Open-Source Large-Margin Structured Learning on
MapReduce
</title>
<author confidence="0.999607">
Vladimir Eidelman&apos;, Ke Wu&apos;, Ferhan Ture&apos;, Philip Resnik&apos;, Jimmy Lin3
</author>
<affiliation confidence="0.998285333333333">
&apos; Dept. of Computer Science &apos; Dept. of Linguistics 3 The iSchool
Institute for Advanced Computer Studies
University of Maryland
</affiliation>
<email confidence="0.998526">
{eidelman,wuke,fture,resnik,jimmylin}@umd.edu
</email>
<sectionHeader confidence="0.997385" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999892785714286">
We present an open-source framework
for large-scale online structured learning.
Developed with the flexibility to handle
cost-augmented inference problems such
as statistical machine translation (SMT),
our large-margin learner can be used with
any decoder. Integration with MapReduce
using Hadoop streaming allows efficient
scaling with increasing size of training
data. Although designed with a focus
on SMT, the decoder-agnostic design of
our learner allows easy future extension to
other structured learning problems such as
sequence labeling and parsing.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999972315789474">
Structured learning problems such as sequence la-
beling or parsing, where the output has a rich in-
ternal structure, commonly arise in NLP. While
batch learning algorithms adapted for structured
learning such as CRFs (Lafferty et al., 2001)
and structural SVMs (Joachims, 1998) have re-
ceived much attention, online methods such as
the structured perceptron (Collins, 2002) and a
family of Passive-Aggressive algorithms (Cram-
mer et al., 2006) have recently gained promi-
nence across many tasks, including part-of-speech
tagging (Shen, 2007), parsing (McDonald et
al., 2005) and statistical machine translation
(SMT) (Chiang, 2012), due to their ability to deal
with large training sets and high-dimensional in-
put representations.
Unlike batch learners, which must consider all
examples when optimizing the objective, online
learners operate in rounds, optimizing using one
example or a handful of examples at a time. This
online nature offers several attractive properties,
facilitating scaling to large training sets while re-
maining simple and offering fast convergence.
Mr. MIRA, the open source system1 de-
scribed in this paper, implements an online large-
margin structured learning algorithm based on
MIRA (§2.1), for cost-augmented online large-
scale training in high-dimensional feature spaces.
Our contribution lies in providing the first pub-
lished decoder-agnostic parallelization of MIRA
with Hadoop for structured learning.
While the current demonstrated application fo-
cuses on large-scale discriminative training for
machine translation, the learning algorithm is gen-
eral with respect to the inference algorithm em-
ployed. We are able to decouple our learner en-
tirely from the MT decoder, allowing users to
specify their own inference procedure through a
simple text communication protocol (§2.2). The
learner only requires k-best output with feature
vectors, as well as the specification of a cost func-
tion. Standard automatic evaluation metrics for
MT, such as BLEU (Papineni et al., 2002) and TER
(Snover et al., 2006), have already been imple-
mented. Furthermore, our system can be extended
to other structured learning problems with a min-
imal amount of effort, simply by implementing a
task-specific cost function and specifying an ap-
propriate decoder.
Through Hadoop streaming, our system can
take advantage of commodity clusters to handle
large-scale training (§3), while also being capable
of running in environments ranging from a single
machine to a PBS-managed batch cluster. Experi-
mental results (§4) show that it scales linearly and
makes fast parameter tuning on large tuning sets
for SMT practical.
</bodyText>
<sectionHeader confidence="0.971473" genericHeader="introduction">
2 Learning and Inference
</sectionHeader>
<subsectionHeader confidence="0.996129">
2.1 Online Large-Margin Learning
</subsectionHeader>
<footnote confidence="0.681636">
MIRA is a popular online large-margin structured
learning method for NLP tasks (McDonald et al.,
2005; Chiang et al., 2009; Chiang, 2012). The
1https://github.com/kho/mr-mira
</footnote>
<page confidence="0.958747">
199
</page>
<bodyText confidence="0.8631855">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 199–204,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
main intuition is that we want our model to enforce
a margin between the correct and incorrect out-
puts of a sentence that agrees with our cost func-
tion. This is done by making the smallest update
we can to our parameters, w, on every sentence,
that will ensure that the difference in model scores
δfi(y0) = w&gt;(f(xi, y+) − f(xi, y0)) between the
correct output y+ and incorrect output y0 is at least
as large as the cost, Δi(y0), incurred by predicting
the incorrect output:2
</bodyText>
<equation confidence="0.995080666666667">
2||w − wt||2 + Cξi
1
s.t. ∀y0 ∈ Y(xi), δfi(y0) ≥ Δi(y0) − ξi
</equation>
<bodyText confidence="0.999701076923077">
where Y(xi) is the space of possible structured
outputs we are able to produce from xi, and
C is a regularization parameter that controls the
size of the update. In practice, we can de-
fine Y(xi) to be the k-best output. With a
passive-aggressive (PA) update, the ∀y0 constraint
above can be approximated by selecting the sin-
gle most violated constraint, which maximizes
y0 ← arg maxy∈Y(xi) w&gt;f(xi, y) + Δi(y). This
optimization problem is attractive because it re-
duces to a simple analytical solution, essentially
performing a subgradient descent step with the
step size adjusted based on each example:
</bodyText>
<equation confidence="0.989893">
/ Δi(y0) − δfi(y0)α←
kf(xi, y+) − f(xi, y0)k2
minl C,
</equation>
<bodyText confidence="0.999885764705882">
The user-defined cost function is a task-specific
external measure of quality that relays how bad se-
lecting y0 truly is on the task we care about. The
cost can take any form as long as it decomposes
across the local parts of the structure, just as the
feature functions. For instance, it could be the
Hamming loss for sequence labeling, F-score for
parsing, or an approximate BLEU score for SMT.
Cost-augmented Inference For most struc-
tured prediction problems in machine learning,
yi ∈ Y(xi), that is, the model is able to produce,
and thus score, the correct output structure, mean-
ing y+ = yi. However, for certain NLP prob-
lems this may not be the case. For instance in
SMT, our model may not be able to produce or
reach the correct reference translation, which pro-
hibits our model from scoring it. This problem
</bodyText>
<footnote confidence="0.930088">
2For a more formal description we refer the reader
to (Crammer et al., 2006; Chiang, 2012).
</footnote>
<bodyText confidence="0.983354481481481">
necessitates cost-augmented inference, where we
select y+ ← arg maxy∈Y(xi) w&gt;f(xi, y) − Δi(y)
from the space of structures our model can pro-
duce, to stand in for the correct output in optimiza-
tion. Our system was developed to handle both
cases, with the decoder providing the k-best list
to the learner, specifying whether to perform cost-
augmented selection.
Sparse Features While utilizing sparse features
is a primary motivation for performing large-scale
discriminative training, which features to use and
how to learn their weights can have a large im-
pact on the potential benefit. To this end, we in-
corporate `1/`2 regularization for joint feature se-
lection in order to improve efficiency and counter
overfitting effects (Simianer et al., 2012). Further-
more, the PA update has a single learning rate η
for all features, which specifies how much the fea-
ture weights can change at each update. How-
ever, since dense features (e.g., language model)
are observed far more frequently than sparse fea-
tures (e.g., rule id), we may instead want to use
a per-feature learning rate that allows larger steps
for features that do not have much support. Thus,
we allow setting an adaptive per-feature learning
rate (Green et al., 2013; Crammer et al., 2009;
Duchi et al., 2011).
</bodyText>
<equation confidence="0.601027">
w ← w + ηα (f(xi, y+) − f(xi, y0))
</equation>
<subsectionHeader confidence="0.977409">
2.2 Learner/Decoder Communication
</subsectionHeader>
<bodyText confidence="0.999898545454545">
Training requires communication between the de-
coder and the learner. The decoder needs to re-
ceive weight updates and the input sentence from
the learner; and the learner needs to receive k-best
output with feature vectors from the decoder. This
is essentially all the required communication be-
tween the learner and the decoder. Below, we de-
scribe a simple line-based text protocol.
Input sentence and weight updates Follow-
ing common practice in machine translation, the
learner encodes each input sentence as a single-
line SGML entry named seg and sends it to the
decoder. The first line of Figure 1 is an exam-
ple sentence in this format. In addition to the
required sentence ID (useful in parallel process-
ing), an optional delta field is used to encode
the weight updates, as a sparse vector indexed
by feature names. First, for each name and up-
date pair, a binary record consisting of a null-
terminated string (name) and a double-precision
floating point number in native byte order (up-
date) is created. Then, all binary records are con-
</bodyText>
<equation confidence="0.939645">
wt+1 = arg min
W
</equation>
<page confidence="0.868244">
200
</page>
<bodyText confidence="0.556534">
&lt;seg id=&amp;quot;123&amp;quot; delta=&amp;quot;TE0AexSuR+F6hD8=&amp;quot;&gt; das ist ein kleine haus &lt;/seg&gt;
</bodyText>
<figureCaption confidence="0.999244">
Figure 1: Example decoder input in SGML
</figureCaption>
<figure confidence="0.9995725">
5
123   5   this is a small house   TE0AAAAA... &lt;base64&gt;   120.3
123   5   this is the small house   &lt;base64&gt;   118.4
123   5   this was small house   &lt;base64&gt;   110.5
&lt;empty&gt;
&lt;empty&gt;
</figure>
<figureCaption confidence="0.999916">
Figure 2: Example k-best output
</figureCaption>
<bodyText confidence="0.999265705882353">
catenated and encoded in base64. In the example
above, the value of delta is the base64 encod-
ing of 0x4c 0x4d 0x00 0x7b 0x14 0xae 0x47
0xe1 0x7a 0x84 0x3f. The first 3 bytes store the
feature name (LM) and the next 8 bytes is its update
(0.01), to be added to the decoder’s current value
of the corresponding feature weight.
The learner also allows the user to pass any ad-
ditional information to the decoder, as long as it
can be encoded as a single-line text string. Such
information, if given, is appended after the seg en-
try, with a leading tab character as the delimiter.
For example, the second line of Figure 1 passes
two phrase translation rules to the decoder.
k-best output The decoder reads from standard
input and outputs the k-best output for one input
sentence before consuming the next line. For the
k-best output, the decoder first outputs to standard
output a line consisting of a single integer N. Next
the decoder outputs N lines where each line can
be either empty or an actual hypothesis. When the
line is an actual hypothesis, it consists of the fol-
lowing parts:
SID is the sentence ID of the corresponding input;
LEN is the length of source sentence;3 TOK contains
the tokens of the hypothesis sentence separated by
spaces; FEAT is the feature vector, encoded in the
same way as the weight updates, delimited by a
whitespace. Everything after FEAT until the end of
the line is discarded. See Figure 2 for an example
are discarded by the learner.
Overall workflow The learner reads lines from
standard input in the following tab-delimited for-
mat:
</bodyText>
<footnote confidence="0.8703028">
3This is used in computing the smoothed cost. Usually
this is identical for all hypotheses if the input is a plain sen-
tence. But in applications such as lattice-based translation,
each hypothesis can be produced from different source sen-
tences, resulting in different lengths.
</footnote>
<equation confidence="0.29403">
SRC&lt;tab&gt;REF&lt;tab&gt;REST
</equation>
<bodyText confidence="0.98928824">
SRC is the actual input sentence as a seg entry; REF
is the gold output for the input sentence, for ex-
ample, reference translations in MT;4 REST is the
additional information that will be appended after
the seg entry and passed to the decoder.
The learner creates a sub-process for the de-
coder and connects to the sub-process’ standard
input and output with pipes. Then it processes the
input lines one by one. For each line, it first sends
a composed input message to the decoder, combin-
ing the input sentence, weight updates, and user-
supplied information. Next it collects the k-best
output from the decoder, solves the QP problem to
obtain weight updates and repeats.
The learner produces two types of output. First,
the 1-best hypothesis for each input sentence, in
the following format:
SID&lt;tab&gt;TOK
Second, when there are no more input lines, the
learner outputs final weights and the number of
lines processed, in the following format:
The 1-best hypotheses can be scored against ref-
erences to obtain an estimate of cost. The final
weights are stored in a way convenient for averag-
ing in a parallel setting, as we shall discuss next.
</bodyText>
<sectionHeader confidence="0.985411" genericHeader="method">
3 Large-Scale Discriminative Training
</sectionHeader>
<subsectionHeader confidence="0.979603">
3.1 MapReduce
</subsectionHeader>
<bodyText confidence="0.999956222222222">
With large amounts of data available today,
distributed computations have become essen-
tial. MapReduce (Dean and Ghemawat, 2004)
has emerged as a popular distributed process-
ing framework for commodity clusters that has
gained widespread adoption in both industry and
academia, thanks to its simplicity and the avail-
ability of the Hadoop open-source implementa-
tion. MapReduce provides a higher level of
</bodyText>
<footnote confidence="0.98068">
4There can be multiple references, separated by  .
</footnote>
<page confidence="0.996215">
201
</page>
<bodyText confidence="0.9998933125">
abstraction for designing distributed algorithms
compared to, say, MPI or pthreads, by hiding
system-level details (e.g., deadlock, race condi-
tions, machine failures) from the developer.
A single MapReduce program begins with a
map phase, where mapper processes input key-
value pairs to produce an arbitrary number of in-
termediate key-value pairs. The mappers execute
in parallel, consuming data splits independently.
Following the map phase, all key-value pairs emit-
ted by the mappers are sorted by key and dis-
tributed to the reducers, such that all pairs shar-
ing the same key are guaranteed to arrive at the
same reducer. Finally, in the reduce phase, each
reducer processes the intermediate key-value pairs
it receives and emits final output key-value pairs.
</bodyText>
<subsectionHeader confidence="0.99906">
3.2 System Architecture
</subsectionHeader>
<bodyText confidence="0.999964633333333">
Algorithm design We use Hadoop streaming to
parallelize the training process. Hadoop stream-
ing allows any arbitrary executable to serve as the
mapper or reducer, as long as it handles key-value
pairs properly.5 One iteration of training is im-
plemented as a single Hadoop streaming job. In
the map step, our learner can be directly used as
the mapper. Each mapper loads the same initial
weights, processes a single split of data and pro-
duces key-value pairs: the one-best hypothesis of
each sentence is output with the sentence ID as
the key (non-negative); the final weights with re-
spect to the split are output with a special negative
key. In the reduce step, a single reducer collects all
key-value pairs, grouped and sorted by keys. The
one-best hypotheses are output to disk in the or-
der they are received, so that the order matches the
reference translation set. The reducer also com-
putes the feature selection and weighted average
of final weights received from all of the mappers.
Assuming mapper i produces the final weights wi
after processing ni sentences, the weighted aver-
aged is defined as w∗ _ �i Wi × ni i ni . Although aver-
aging yields different result from running a single
learner over the entire data, we have found the dif-
ference to be quite small in terms of convergence
and quality of tuned weights in practice.
After the reducer finishes, the averaged weights
are extracted and used as the initial weights for the
next iteration; the emitted hypotheses are scored
</bodyText>
<footnote confidence="0.620737666666667">
5By default, each line is treated as a key-value pair en-
coded in text, where the key and the value are separated by a
&lt;tab&gt;.
</footnote>
<bodyText confidence="0.999877303030303">
against the references, which allows us to track the
learning curve and the progress of convergence.
Scalability In an application such as SMT, the
decoder requires access to the translation gram-
mar and language model to produce translation hy-
potheses. For small tuning sets, which have been
typical in MT research, having these files trans-
ferred across the network to individual servers
(which then load the data into memory) is not
a problem. However, for even modest input on
the order of tens of thousands of sentences, this
creates a challenge. For example, distributing
thousands of per-sentence grammar files to all the
workers in a Hadoop cluster is time-consuming,
especially when this needs to be performed prior
to every iteration.
To benefit from MapReduce, it is essential to
avoid dependencies on “side data” as much as
possible, due to the challenges explained above
with data transfer. To address this issue, we ap-
pend the per-sentence translation grammar as user-
supplied additional information to each input sen-
tence. This results in a large input file (e.g., 75 gi-
gabytes for 50,000 sentences), but this is not an is-
sue since the data reside on the Hadoop distributed
file system and MapReduce optimizes for data lo-
cality when scheduling mappers.
Unfortunately, it is much more difficult to ob-
tain per-sentence language models that are small
enough to handle in this same manner. Currently,
the best solution we have found is to use Hadoop’s
distributed cache to ship the single large language
model to each worker.
</bodyText>
<sectionHeader confidence="0.997329" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999773928571429">
We evaluated online learning in Hadoop Map-
Reduce by applying it to German-English ma-
chine translation, using our hierarchical phrase-
based translation system with cdec as the de-
coder (Dyer et al., 2010). The parallel training
data consist of the Europarl and News Commen-
tary corpora from the WMT12 translation task,6
containing 2.08M sentences. A 5-gram language
model was trained on the English side of the bi-
text along with 750M words of news using SRILM
with modified Kneser-Ney smoothing (Chen and
Goodman, 1996).
We experimented with two feature sets: (1) a
small set with standard MT features, including
</bodyText>
<footnote confidence="0.975442">
6http://www.statmt.org/wmt12/translation-task.html
</footnote>
<page confidence="0.987789">
202
</page>
<table confidence="0.999843461538461">
Tuning set size Time/iteration # splits # features Tuning BLEU Test
(in seconds)
(corpus) (on disk, GB) BLEU TER
dev 3.3 119 120 16 22.38 22.69 60.61
5k 7.8 289 120 16 32.60 22.14 59.60
10k 15.2 432 120 16 33.16 22.06 59.43
25k 37.2 942 300 16 32.48 22.21 59.54
50k 74.5 1802 600 16 32.21 22.21 59.39
dev 3.3 232 120 85k 23.08 23.00 60.19
5k 7.8 610 120 159k 33.70 22.26 59.26
10k 15.2 1136 120 200k 34.00 22.12 59.24
25k 37.2 2395 300 200k 32.96 22.35 59.29
50k 74.5 4465 600 200k 32.86 22.40 59.15
</table>
<tableCaption confidence="0.993063">
Table 1: Evaluation of our Hadoop implementation of MIRA, showing running time as well as BLEU
</tableCaption>
<table confidence="0.9376376">
and TER values for tuning and testing data.
dev test 5k 10k 25k 50k
Sentences 3003 3003 5000 10000 25000 50000
Tokens en 75k 74k 132k 255k 634k 1258k
Tokens de 74k 73k 133k 256k 639k 1272k
</table>
<tableCaption confidence="0.998833">
Table 2: Corpus statistics
</tableCaption>
<bodyText confidence="0.999954482758621">
phrase and lexical translation probabilities in both
directions, word and arity penalties, and language
model scores; and (2) a large set containing the top
200k sparse features that might be useful to train
on large numbers of instances: rule id and shape,
target bigrams, insertions and deletions, and struc-
tural distortion features.
All experiments were conducted on a Hadoop
cluster (running Cloudera’s distribution, CDH
4.2.1) with 16 nodes, each with two quad-core 2.2
GHz Intel Nehalem Processors, 24 GB RAM, and
three 2 TB drives. In total, the cluster is configured
for a capacity of 128 parallel workers, although
we do not have direct control over the number
of simultaneous mappers, which depends on the
number of input splits. If the number of splits is
smaller than 128, then the cluster is under-utilized.
To note this, we report the number of splits for
each setting in our experimental results (Table 1).
We ran MIRA on a number of tuning sets, de-
scribed in Table 2, in order to test the effective-
ness and scalability of our system. First, we used
the standard development set from WMT12, con-
sisting of 3,003 sentences from news domain. In
order to show the scaling characteristics of our ap-
proach, we then used larger portions of the train-
ing bitext directly to tune parameters. In order to
avoid overfitting, we used a jackknifing method
to split the training data into n = 10 folds, and
built a translation system on n − 1 folds, while
adjusting the sampling rate to sample sentences
from the other fold to obtain tuning sets ranging
from 5,000 sentences to 50,000 sentences. Table 1
shows details of experimental results for each set-
ting. The second column shows the space each
tuning set takes up on disk when we include refer-
ence translations and grammar files along with the
sentences. The reported tuning BLEU is from the
iteration with best performance, and running times
are reported from the top-scoring iteration as well.
Even though our focus in this evaluation is to
show the scalability of our implementation to large
input and feature sets, it is also worthwhile to men-
tion the effectiveness aspect. As we increase the
tuning set size by sampling sentences from the
training data, we see very little improvement in
BLEU and TER with the smaller feature set. This
is not surprising, since sparse features are more
likely to gain from additional tuning instances. In-
deed, tuning scores for all sets improve substan-
tially with sparse features, accompanied by small
increases on test.
While tuning on dev data results in better BLEU
on test data than when tuning on the larger sets, it
is important to note that although we are able to
tune more features on the larger bitext tuning sets,
they are not composed of the same genre as the
dev and test sets, resulting in a domain mismatch.
</bodyText>
<page confidence="0.996131">
203
</page>
<bodyText confidence="0.999996692307692">
Therefore, we are actually comparing a smaller in-
domain tuning set with a larger out-of-domain set.
While this domain adaptation is problematic (Had-
dow and Koehn, 2012), the ability to discrimina-
tively tune on larger sets remains highly desirable.
In terms of running time, we observe that the al-
gorithm scales linearly with respect to input size,
regardless of the feature set. With more features,
running time increases due to a more complex
translation model, as well as larger intermediate
output (i.e., amount of information passed from
mappers to reducers). The scaling characteristics
point out the strength of our system: our scalable
MIRA implementation allows one to tackle learn-
ing problems where there are many parameters,
but also many training instances.
Comparing the wall clock time of paralleliza-
tion with Hadoop to the standard mode of 10–20
learner parallelization (Haddow et al., 2011; Chi-
ang et al., 2009), for the small 25k feature set-
ting, after one iteration, which takes 4625 sec-
onds using 15 learners on our PBS cluster, the tun-
ing score is 19.5 BLEU, while in approximately
the same time, we can perform five iterations
with Hadoop and obtain 30.98 BLEU. While this
is not a completely fair comparison, as the two
clusters utilize different resources and the num-
ber of learners, it suggests the practical benefits
that Hadoop can provide. Although increasing the
number of learners on our PBS cluster to the num-
ber of mappers used in Hadoop would result in
roughly equivalent performance, arbitrarily scal-
ing out learners on the PBS cluster to handle larger
training sets can be challenging since we’d have to
manually coordinate the parallel processes in an
ad-hoc manner. In contrast, Hadoop provides scal-
able parallelization in a manageable framework,
providing data distribution, synchronization, fault
tolerance, as well as other features, “for free”.
</bodyText>
<sectionHeader confidence="0.999597" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999557">
In this paper, we presented an open-source
framework that allows seamlessly scaling struc-
tured learning to large feature-rich problems with
Hadoop, which lets us take advantage of large
amounts of data as well as sparse features. The
development of Mr. MIRA has been motivated pri-
marily by application to SMT, but we are planning
to extend our system to other structured prediction
tasks in NLP such as parsing, as well as to facili-
tate its use in other domains.
</bodyText>
<sectionHeader confidence="0.9987" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.968628375">
This research was supported in part by the DARPA
BOLT program, Contract No. HR0011-12-C-
0015; NSF under awards IIS-0916043 and IIS-
1144034. Vladimir Eidelman is supported by a
NDSEG Fellowship. Any opinions, findings, con-
clusions, or recommendations expressed are those
of the authors and do not necessarily reflect views
of the sponsors.
</bodyText>
<sectionHeader confidence="0.999026" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99949293877551">
S. Chen and J. Goodman. 1996. An empirical study of
smoothing techniques for language modeling. In ACL.
D. Chiang, K. Knight, and W. Wang. 2009. 11,001 new fea-
tures for statistical machine translation. In NAACL-HLT.
D. Chiang. 2012. Hope and fear for discriminative training
of statistical translation models. JMLR, 13:1159–1187.
M. Collins. 2002. Ranking algorithms for named-entity ex-
traction: boosting and the voted perceptron. In ACL.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, and
Y. Singer. 2006. Online passive-aggressive algorithms.
JMLR, 7:551–585.
K. Crammer, A. Kulesza, and M. Dredze. 2009. Adaptive
regularization of weight vectors. In NIPS.
J. Dean and S. Ghemawat. 2004. MapReduce: Simplified
data processing on large clusters. In OSDI.
J. Duchi, E. Hazan, and Y. Singer. 2011. Adaptive subgra-
dient methods for online learning and stochastic optimiza-
tion. JMLR, 12:2121–2159.
C. Dyer, A. Lopez, J. Ganitkevitch, J. Weese, F. Ture, P. Blun-
som, H. Setiawan, V. Eidelman, and P. Resnik. 2010.
cdec: A decoder, alignment, and learning framework for
finite-state and context-free translation models. In ACL
System Demonstrations.
S. Green, S. Wang, D. Cer, and C. Manning. 2013. Fast and
adaptive online training of feature-rich translation models.
In ACL.
B. Haddow and P. Koehn. 2012. Analysing the effect of out-
of-domain data on smt systems. In WMT.
B. Haddow, A. Arun, and P. Koehn. 2011. SampleRank
training for phrase-based machine translation. In WMT.
T. Joachims. 1998. Text categorization with support vec-
tor machines: Learning with many relevant features. In
ECML.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and
labeling sequence data. In ICML.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In ACL.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In ACL.
L. Shen. 2007. Guided learning for bidirectional sequence
classification. In ACL.
P. Simianer, S. Riezler, and C. Dyer. 2012. Joint feature
selection in distributed stochastic learning for large-scale
discriminative training in SMT. In ACL.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate with
targeted human annotation. In AMTA.
</reference>
<page confidence="0.998895">
204
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.770015">
<title confidence="0.988767">Mr. MIRA: Open-Source Large-Margin Structured Learning on MapReduce</title>
<author confidence="0.999043">Ke Ferhan Philip Jimmy</author>
<affiliation confidence="0.930265666666667">of Computer Science of Linguistics 3The Institute for Advanced Computer University of</affiliation>
<abstract confidence="0.999102333333333">We present an open-source framework for large-scale online structured learning. Developed with the flexibility to handle cost-augmented inference problems such as statistical machine translation (SMT), our large-margin learner can be used with any decoder. Integration with MapReduce using Hadoop streaming allows efficient scaling with increasing size of training data. Although designed with a focus on SMT, the decoder-agnostic design of our learner allows easy future extension to other structured learning problems such as sequence labeling and parsing.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>clusions</author>
</authors>
<title>or recommendations expressed are those of the authors and do not necessarily reflect views of the sponsors.</title>
<marker>clusions, </marker>
<rawString>clusions, or recommendations expressed are those of the authors and do not necessarily reflect views of the sponsors.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Chen</author>
<author>J Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1996</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="16704" citStr="Chen and Goodman, 1996" startWordPosition="2729" endWordPosition="2732">d is to use Hadoop’s distributed cache to ship the single large language model to each worker. 4 Evaluation We evaluated online learning in Hadoop MapReduce by applying it to German-English machine translation, using our hierarchical phrasebased translation system with cdec as the decoder (Dyer et al., 2010). The parallel training data consist of the Europarl and News Commentary corpora from the WMT12 translation task,6 containing 2.08M sentences. A 5-gram language model was trained on the English side of the bitext along with 750M words of news using SRILM with modified Kneser-Ney smoothing (Chen and Goodman, 1996). We experimented with two feature sets: (1) a small set with standard MT features, including 6http://www.statmt.org/wmt12/translation-task.html 202 Tuning set size Time/iteration # splits # features Tuning BLEU Test (in seconds) (corpus) (on disk, GB) BLEU TER dev 3.3 119 120 16 22.38 22.69 60.61 5k 7.8 289 120 16 32.60 22.14 59.60 10k 15.2 432 120 16 33.16 22.06 59.43 25k 37.2 942 300 16 32.48 22.21 59.54 50k 74.5 1802 600 16 32.21 22.21 59.39 dev 3.3 232 120 85k 23.08 23.00 60.19 5k 7.8 610 120 159k 33.70 22.26 59.26 10k 15.2 1136 120 200k 34.00 22.12 59.24 25k 37.2 2395 300 200k 32.96 22.3</context>
</contexts>
<marker>Chen, Goodman, 1996</marker>
<rawString>S. Chen and J. Goodman. 1996. An empirical study of smoothing techniques for language modeling. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
<author>K Knight</author>
<author>W Wang</author>
</authors>
<title>11,001 new features for statistical machine translation. In</title>
<date>2009</date>
<journal>JMLR,</journal>
<pages>13--1159</pages>
<contexts>
<context position="3690" citStr="Chiang et al., 2009" startWordPosition="537" endWordPosition="540"> implementing a task-specific cost function and specifying an appropriate decoder. Through Hadoop streaming, our system can take advantage of commodity clusters to handle large-scale training (§3), while also being capable of running in environments ranging from a single machine to a PBS-managed batch cluster. Experimental results (§4) show that it scales linearly and makes fast parameter tuning on large tuning sets for SMT practical. 2 Learning and Inference 2.1 Online Large-Margin Learning MIRA is a popular online large-margin structured learning method for NLP tasks (McDonald et al., 2005; Chiang et al., 2009; Chiang, 2012). The 1https://github.com/kho/mr-mira 199 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 199–204, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics main intuition is that we want our model to enforce a margin between the correct and incorrect outputs of a sentence that agrees with our cost function. This is done by making the smallest update we can to our parameters, w, on every sentence, that will ensure that the difference in model scores δfi(y0) = w&gt;(f(xi, y+) − f(xi, y0)) between the correct out</context>
<context position="21410" citStr="Chiang et al., 2009" startWordPosition="3533" endWordPosition="3537">ales linearly with respect to input size, regardless of the feature set. With more features, running time increases due to a more complex translation model, as well as larger intermediate output (i.e., amount of information passed from mappers to reducers). The scaling characteristics point out the strength of our system: our scalable MIRA implementation allows one to tackle learning problems where there are many parameters, but also many training instances. Comparing the wall clock time of parallelization with Hadoop to the standard mode of 10–20 learner parallelization (Haddow et al., 2011; Chiang et al., 2009), for the small 25k feature setting, after one iteration, which takes 4625 seconds using 15 learners on our PBS cluster, the tuning score is 19.5 BLEU, while in approximately the same time, we can perform five iterations with Hadoop and obtain 30.98 BLEU. While this is not a completely fair comparison, as the two clusters utilize different resources and the number of learners, it suggests the practical benefits that Hadoop can provide. Although increasing the number of learners on our PBS cluster to the number of mappers used in Hadoop would result in roughly equivalent performance, arbitraril</context>
</contexts>
<marker>Chiang, Knight, Wang, 2009</marker>
<rawString>D. Chiang, K. Knight, and W. Wang. 2009. 11,001 new features for statistical machine translation. In NAACL-HLT. D. Chiang. 2012. Hope and fear for discriminative training of statistical translation models. JMLR, 13:1159–1187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Ranking algorithms for named-entity extraction: boosting and the voted perceptron.</title>
<date>2002</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1264" citStr="Collins, 2002" startWordPosition="174" endWordPosition="175">g with increasing size of training data. Although designed with a focus on SMT, the decoder-agnostic design of our learner allows easy future extension to other structured learning problems such as sequence labeling and parsing. 1 Introduction Structured learning problems such as sequence labeling or parsing, where the output has a rich internal structure, commonly arise in NLP. While batch learning algorithms adapted for structured learning such as CRFs (Lafferty et al., 2001) and structural SVMs (Joachims, 1998) have received much attention, online methods such as the structured perceptron (Collins, 2002) and a family of Passive-Aggressive algorithms (Crammer et al., 2006) have recently gained prominence across many tasks, including part-of-speech tagging (Shen, 2007), parsing (McDonald et al., 2005) and statistical machine translation (SMT) (Chiang, 2012), due to their ability to deal with large training sets and high-dimensional input representations. Unlike batch learners, which must consider all examples when optimizing the objective, online learners operate in rounds, optimizing using one example or a handful of examples at a time. This online nature offers several attractive properties, </context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>M. Collins. 2002. Ranking algorithms for named-entity extraction: boosting and the voted perceptron. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crammer</author>
<author>O Dekel</author>
<author>J Keshet</author>
<author>S Shalev-Shwartz</author>
<author>Y Singer</author>
</authors>
<title>Online passive-aggressive algorithms.</title>
<date>2006</date>
<journal>JMLR,</journal>
<pages>7--551</pages>
<contexts>
<context position="1333" citStr="Crammer et al., 2006" startWordPosition="182" endWordPosition="186">h a focus on SMT, the decoder-agnostic design of our learner allows easy future extension to other structured learning problems such as sequence labeling and parsing. 1 Introduction Structured learning problems such as sequence labeling or parsing, where the output has a rich internal structure, commonly arise in NLP. While batch learning algorithms adapted for structured learning such as CRFs (Lafferty et al., 2001) and structural SVMs (Joachims, 1998) have received much attention, online methods such as the structured perceptron (Collins, 2002) and a family of Passive-Aggressive algorithms (Crammer et al., 2006) have recently gained prominence across many tasks, including part-of-speech tagging (Shen, 2007), parsing (McDonald et al., 2005) and statistical machine translation (SMT) (Chiang, 2012), due to their ability to deal with large training sets and high-dimensional input representations. Unlike batch learners, which must consider all examples when optimizing the objective, online learners operate in rounds, optimizing using one example or a handful of examples at a time. This online nature offers several attractive properties, facilitating scaling to large training sets while remaining simple an</context>
<context position="6016" citStr="Crammer et al., 2006" startWordPosition="936" endWordPosition="939">. For instance, it could be the Hamming loss for sequence labeling, F-score for parsing, or an approximate BLEU score for SMT. Cost-augmented Inference For most structured prediction problems in machine learning, yi ∈ Y(xi), that is, the model is able to produce, and thus score, the correct output structure, meaning y+ = yi. However, for certain NLP problems this may not be the case. For instance in SMT, our model may not be able to produce or reach the correct reference translation, which prohibits our model from scoring it. This problem 2For a more formal description we refer the reader to (Crammer et al., 2006; Chiang, 2012). necessitates cost-augmented inference, where we select y+ ← arg maxy∈Y(xi) w&gt;f(xi, y) − Δi(y) from the space of structures our model can produce, to stand in for the correct output in optimization. Our system was developed to handle both cases, with the decoder providing the k-best list to the learner, specifying whether to perform costaugmented selection. Sparse Features While utilizing sparse features is a primary motivation for performing large-scale discriminative training, which features to use and how to learn their weights can have a large impact on the potential benefi</context>
</contexts>
<marker>Crammer, Dekel, Keshet, Shalev-Shwartz, Singer, 2006</marker>
<rawString>K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, and Y. Singer. 2006. Online passive-aggressive algorithms. JMLR, 7:551–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crammer</author>
<author>A Kulesza</author>
<author>M Dredze</author>
</authors>
<title>Adaptive regularization of weight vectors.</title>
<date>2009</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="7280" citStr="Crammer et al., 2009" startWordPosition="1144" endWordPosition="1147">ization for joint feature selection in order to improve efficiency and counter overfitting effects (Simianer et al., 2012). Furthermore, the PA update has a single learning rate η for all features, which specifies how much the feature weights can change at each update. However, since dense features (e.g., language model) are observed far more frequently than sparse features (e.g., rule id), we may instead want to use a per-feature learning rate that allows larger steps for features that do not have much support. Thus, we allow setting an adaptive per-feature learning rate (Green et al., 2013; Crammer et al., 2009; Duchi et al., 2011). w ← w + ηα (f(xi, y+) − f(xi, y0)) 2.2 Learner/Decoder Communication Training requires communication between the decoder and the learner. The decoder needs to receive weight updates and the input sentence from the learner; and the learner needs to receive k-best output with feature vectors from the decoder. This is essentially all the required communication between the learner and the decoder. Below, we describe a simple line-based text protocol. Input sentence and weight updates Following common practice in machine translation, the learner encodes each input sentence as</context>
</contexts>
<marker>Crammer, Kulesza, Dredze, 2009</marker>
<rawString>K. Crammer, A. Kulesza, and M. Dredze. 2009. Adaptive regularization of weight vectors. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Dean</author>
<author>S Ghemawat</author>
</authors>
<title>MapReduce: Simplified data processing on large clusters.</title>
<date>2004</date>
<booktitle>In OSDI.</booktitle>
<contexts>
<context position="11917" citStr="Dean and Ghemawat, 2004" startWordPosition="1936" endWordPosition="1939"> types of output. First, the 1-best hypothesis for each input sentence, in the following format: SID&lt;tab&gt;TOK Second, when there are no more input lines, the learner outputs final weights and the number of lines processed, in the following format: The 1-best hypotheses can be scored against references to obtain an estimate of cost. The final weights are stored in a way convenient for averaging in a parallel setting, as we shall discuss next. 3 Large-Scale Discriminative Training 3.1 MapReduce With large amounts of data available today, distributed computations have become essential. MapReduce (Dean and Ghemawat, 2004) has emerged as a popular distributed processing framework for commodity clusters that has gained widespread adoption in both industry and academia, thanks to its simplicity and the availability of the Hadoop open-source implementation. MapReduce provides a higher level of 4There can be multiple references, separated by . 201 abstraction for designing distributed algorithms compared to, say, MPI or pthreads, by hiding system-level details (e.g., deadlock, race conditions, machine failures) from the developer. A single MapReduce program begins with a map phase, where mapper processes input keyv</context>
</contexts>
<marker>Dean, Ghemawat, 2004</marker>
<rawString>J. Dean and S. Ghemawat. 2004. MapReduce: Simplified data processing on large clusters. In OSDI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Duchi</author>
<author>E Hazan</author>
<author>Y Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2011</date>
<journal>JMLR,</journal>
<pages>12--2121</pages>
<contexts>
<context position="7301" citStr="Duchi et al., 2011" startWordPosition="1148" endWordPosition="1151">ure selection in order to improve efficiency and counter overfitting effects (Simianer et al., 2012). Furthermore, the PA update has a single learning rate η for all features, which specifies how much the feature weights can change at each update. However, since dense features (e.g., language model) are observed far more frequently than sparse features (e.g., rule id), we may instead want to use a per-feature learning rate that allows larger steps for features that do not have much support. Thus, we allow setting an adaptive per-feature learning rate (Green et al., 2013; Crammer et al., 2009; Duchi et al., 2011). w ← w + ηα (f(xi, y+) − f(xi, y0)) 2.2 Learner/Decoder Communication Training requires communication between the decoder and the learner. The decoder needs to receive weight updates and the input sentence from the learner; and the learner needs to receive k-best output with feature vectors from the decoder. This is essentially all the required communication between the learner and the decoder. Below, we describe a simple line-based text protocol. Input sentence and weight updates Following common practice in machine translation, the learner encodes each input sentence as a singleline SGML en</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>J. Duchi, E. Hazan, and Y. Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. JMLR, 12:2121–2159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Dyer</author>
<author>A Lopez</author>
<author>J Ganitkevitch</author>
<author>J Weese</author>
<author>F Ture</author>
<author>P Blunsom</author>
<author>H Setiawan</author>
<author>V Eidelman</author>
<author>P Resnik</author>
</authors>
<title>cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models.</title>
<date>2010</date>
<booktitle>In ACL System Demonstrations.</booktitle>
<contexts>
<context position="16390" citStr="Dyer et al., 2010" startWordPosition="2678" endWordPosition="2681">n issue since the data reside on the Hadoop distributed file system and MapReduce optimizes for data locality when scheduling mappers. Unfortunately, it is much more difficult to obtain per-sentence language models that are small enough to handle in this same manner. Currently, the best solution we have found is to use Hadoop’s distributed cache to ship the single large language model to each worker. 4 Evaluation We evaluated online learning in Hadoop MapReduce by applying it to German-English machine translation, using our hierarchical phrasebased translation system with cdec as the decoder (Dyer et al., 2010). The parallel training data consist of the Europarl and News Commentary corpora from the WMT12 translation task,6 containing 2.08M sentences. A 5-gram language model was trained on the English side of the bitext along with 750M words of news using SRILM with modified Kneser-Ney smoothing (Chen and Goodman, 1996). We experimented with two feature sets: (1) a small set with standard MT features, including 6http://www.statmt.org/wmt12/translation-task.html 202 Tuning set size Time/iteration # splits # features Tuning BLEU Test (in seconds) (corpus) (on disk, GB) BLEU TER dev 3.3 119 120 16 22.38</context>
</contexts>
<marker>Dyer, Lopez, Ganitkevitch, Weese, Ture, Blunsom, Setiawan, Eidelman, Resnik, 2010</marker>
<rawString>C. Dyer, A. Lopez, J. Ganitkevitch, J. Weese, F. Ture, P. Blunsom, H. Setiawan, V. Eidelman, and P. Resnik. 2010. cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models. In ACL System Demonstrations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Green</author>
<author>S Wang</author>
<author>D Cer</author>
<author>C Manning</author>
</authors>
<title>Fast and adaptive online training of feature-rich translation models.</title>
<date>2013</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="7258" citStr="Green et al., 2013" startWordPosition="1140" endWordPosition="1143">porate `1/`2 regularization for joint feature selection in order to improve efficiency and counter overfitting effects (Simianer et al., 2012). Furthermore, the PA update has a single learning rate η for all features, which specifies how much the feature weights can change at each update. However, since dense features (e.g., language model) are observed far more frequently than sparse features (e.g., rule id), we may instead want to use a per-feature learning rate that allows larger steps for features that do not have much support. Thus, we allow setting an adaptive per-feature learning rate (Green et al., 2013; Crammer et al., 2009; Duchi et al., 2011). w ← w + ηα (f(xi, y+) − f(xi, y0)) 2.2 Learner/Decoder Communication Training requires communication between the decoder and the learner. The decoder needs to receive weight updates and the input sentence from the learner; and the learner needs to receive k-best output with feature vectors from the decoder. This is essentially all the required communication between the learner and the decoder. Below, we describe a simple line-based text protocol. Input sentence and weight updates Following common practice in machine translation, the learner encodes </context>
</contexts>
<marker>Green, Wang, Cer, Manning, 2013</marker>
<rawString>S. Green, S. Wang, D. Cer, and C. Manning. 2013. Fast and adaptive online training of feature-rich translation models. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Haddow</author>
<author>P Koehn</author>
</authors>
<title>Analysing the effect of outof-domain data on smt systems.</title>
<date>2012</date>
<booktitle>In WMT.</booktitle>
<contexts>
<context position="20652" citStr="Haddow and Koehn, 2012" startWordPosition="3412" endWordPosition="3416"> additional tuning instances. Indeed, tuning scores for all sets improve substantially with sparse features, accompanied by small increases on test. While tuning on dev data results in better BLEU on test data than when tuning on the larger sets, it is important to note that although we are able to tune more features on the larger bitext tuning sets, they are not composed of the same genre as the dev and test sets, resulting in a domain mismatch. 203 Therefore, we are actually comparing a smaller indomain tuning set with a larger out-of-domain set. While this domain adaptation is problematic (Haddow and Koehn, 2012), the ability to discriminatively tune on larger sets remains highly desirable. In terms of running time, we observe that the algorithm scales linearly with respect to input size, regardless of the feature set. With more features, running time increases due to a more complex translation model, as well as larger intermediate output (i.e., amount of information passed from mappers to reducers). The scaling characteristics point out the strength of our system: our scalable MIRA implementation allows one to tackle learning problems where there are many parameters, but also many training instances.</context>
</contexts>
<marker>Haddow, Koehn, 2012</marker>
<rawString>B. Haddow and P. Koehn. 2012. Analysing the effect of outof-domain data on smt systems. In WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Haddow</author>
<author>A Arun</author>
<author>P Koehn</author>
</authors>
<title>SampleRank training for phrase-based machine translation.</title>
<date>2011</date>
<booktitle>In WMT.</booktitle>
<contexts>
<context position="21388" citStr="Haddow et al., 2011" startWordPosition="3529" endWordPosition="3532">that the algorithm scales linearly with respect to input size, regardless of the feature set. With more features, running time increases due to a more complex translation model, as well as larger intermediate output (i.e., amount of information passed from mappers to reducers). The scaling characteristics point out the strength of our system: our scalable MIRA implementation allows one to tackle learning problems where there are many parameters, but also many training instances. Comparing the wall clock time of parallelization with Hadoop to the standard mode of 10–20 learner parallelization (Haddow et al., 2011; Chiang et al., 2009), for the small 25k feature setting, after one iteration, which takes 4625 seconds using 15 learners on our PBS cluster, the tuning score is 19.5 BLEU, while in approximately the same time, we can perform five iterations with Hadoop and obtain 30.98 BLEU. While this is not a completely fair comparison, as the two clusters utilize different resources and the number of learners, it suggests the practical benefits that Hadoop can provide. Although increasing the number of learners on our PBS cluster to the number of mappers used in Hadoop would result in roughly equivalent p</context>
</contexts>
<marker>Haddow, Arun, Koehn, 2011</marker>
<rawString>B. Haddow, A. Arun, and P. Koehn. 2011. SampleRank training for phrase-based machine translation. In WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Text categorization with support vector machines: Learning with many relevant features.</title>
<date>1998</date>
<booktitle>In ECML.</booktitle>
<contexts>
<context position="1169" citStr="Joachims, 1998" startWordPosition="160" endWordPosition="161">used with any decoder. Integration with MapReduce using Hadoop streaming allows efficient scaling with increasing size of training data. Although designed with a focus on SMT, the decoder-agnostic design of our learner allows easy future extension to other structured learning problems such as sequence labeling and parsing. 1 Introduction Structured learning problems such as sequence labeling or parsing, where the output has a rich internal structure, commonly arise in NLP. While batch learning algorithms adapted for structured learning such as CRFs (Lafferty et al., 2001) and structural SVMs (Joachims, 1998) have received much attention, online methods such as the structured perceptron (Collins, 2002) and a family of Passive-Aggressive algorithms (Crammer et al., 2006) have recently gained prominence across many tasks, including part-of-speech tagging (Shen, 2007), parsing (McDonald et al., 2005) and statistical machine translation (SMT) (Chiang, 2012), due to their ability to deal with large training sets and high-dimensional input representations. Unlike batch learners, which must consider all examples when optimizing the objective, online learners operate in rounds, optimizing using one exampl</context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>T. Joachims. 1998. Text categorization with support vector machines: Learning with many relevant features. In ECML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="1132" citStr="Lafferty et al., 2001" startWordPosition="153" endWordPosition="156">tion (SMT), our large-margin learner can be used with any decoder. Integration with MapReduce using Hadoop streaming allows efficient scaling with increasing size of training data. Although designed with a focus on SMT, the decoder-agnostic design of our learner allows easy future extension to other structured learning problems such as sequence labeling and parsing. 1 Introduction Structured learning problems such as sequence labeling or parsing, where the output has a rich internal structure, commonly arise in NLP. While batch learning algorithms adapted for structured learning such as CRFs (Lafferty et al., 2001) and structural SVMs (Joachims, 1998) have received much attention, online methods such as the structured perceptron (Collins, 2002) and a family of Passive-Aggressive algorithms (Crammer et al., 2006) have recently gained prominence across many tasks, including part-of-speech tagging (Shen, 2007), parsing (McDonald et al., 2005) and statistical machine translation (SMT) (Chiang, 2012), due to their ability to deal with large training sets and high-dimensional input representations. Unlike batch learners, which must consider all examples when optimizing the objective, online learners operate i</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Crammer</author>
<author>F Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1463" citStr="McDonald et al., 2005" startWordPosition="201" endWordPosition="204">uch as sequence labeling and parsing. 1 Introduction Structured learning problems such as sequence labeling or parsing, where the output has a rich internal structure, commonly arise in NLP. While batch learning algorithms adapted for structured learning such as CRFs (Lafferty et al., 2001) and structural SVMs (Joachims, 1998) have received much attention, online methods such as the structured perceptron (Collins, 2002) and a family of Passive-Aggressive algorithms (Crammer et al., 2006) have recently gained prominence across many tasks, including part-of-speech tagging (Shen, 2007), parsing (McDonald et al., 2005) and statistical machine translation (SMT) (Chiang, 2012), due to their ability to deal with large training sets and high-dimensional input representations. Unlike batch learners, which must consider all examples when optimizing the objective, online learners operate in rounds, optimizing using one example or a handful of examples at a time. This online nature offers several attractive properties, facilitating scaling to large training sets while remaining simple and offering fast convergence. Mr. MIRA, the open source system1 described in this paper, implements an online largemargin structure</context>
<context position="3669" citStr="McDonald et al., 2005" startWordPosition="533" endWordPosition="536">nt of effort, simply by implementing a task-specific cost function and specifying an appropriate decoder. Through Hadoop streaming, our system can take advantage of commodity clusters to handle large-scale training (§3), while also being capable of running in environments ranging from a single machine to a PBS-managed batch cluster. Experimental results (§4) show that it scales linearly and makes fast parameter tuning on large tuning sets for SMT practical. 2 Learning and Inference 2.1 Online Large-Margin Learning MIRA is a popular online large-margin structured learning method for NLP tasks (McDonald et al., 2005; Chiang et al., 2009; Chiang, 2012). The 1https://github.com/kho/mr-mira 199 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 199–204, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics main intuition is that we want our model to enforce a margin between the correct and incorrect outputs of a sentence that agrees with our cost function. This is done by making the smallest update we can to our parameters, w, on every sentence, that will ensure that the difference in model scores δfi(y0) = w&gt;(f(xi, y+) − f(xi, y0)) be</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>R. McDonald, K. Crammer, and F. Pereira. 2005. Online large-margin training of dependency parsers. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="2888" citStr="Papineni et al., 2002" startWordPosition="412" endWordPosition="415">elization of MIRA with Hadoop for structured learning. While the current demonstrated application focuses on large-scale discriminative training for machine translation, the learning algorithm is general with respect to the inference algorithm employed. We are able to decouple our learner entirely from the MT decoder, allowing users to specify their own inference procedure through a simple text communication protocol (§2.2). The learner only requires k-best output with feature vectors, as well as the specification of a cost function. Standard automatic evaluation metrics for MT, such as BLEU (Papineni et al., 2002) and TER (Snover et al., 2006), have already been implemented. Furthermore, our system can be extended to other structured learning problems with a minimal amount of effort, simply by implementing a task-specific cost function and specifying an appropriate decoder. Through Hadoop streaming, our system can take advantage of commodity clusters to handle large-scale training (§3), while also being capable of running in environments ranging from a single machine to a PBS-managed batch cluster. Experimental results (§4) show that it scales linearly and makes fast parameter tuning on large tuning se</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Shen</author>
</authors>
<title>Guided learning for bidirectional sequence classification.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1430" citStr="Shen, 2007" startWordPosition="198" endWordPosition="199">ed learning problems such as sequence labeling and parsing. 1 Introduction Structured learning problems such as sequence labeling or parsing, where the output has a rich internal structure, commonly arise in NLP. While batch learning algorithms adapted for structured learning such as CRFs (Lafferty et al., 2001) and structural SVMs (Joachims, 1998) have received much attention, online methods such as the structured perceptron (Collins, 2002) and a family of Passive-Aggressive algorithms (Crammer et al., 2006) have recently gained prominence across many tasks, including part-of-speech tagging (Shen, 2007), parsing (McDonald et al., 2005) and statistical machine translation (SMT) (Chiang, 2012), due to their ability to deal with large training sets and high-dimensional input representations. Unlike batch learners, which must consider all examples when optimizing the objective, online learners operate in rounds, optimizing using one example or a handful of examples at a time. This online nature offers several attractive properties, facilitating scaling to large training sets while remaining simple and offering fast convergence. Mr. MIRA, the open source system1 described in this paper, implement</context>
</contexts>
<marker>Shen, 2007</marker>
<rawString>L. Shen. 2007. Guided learning for bidirectional sequence classification. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Simianer</author>
<author>S Riezler</author>
<author>C Dyer</author>
</authors>
<title>Joint feature selection in distributed stochastic learning for large-scale</title>
<date>2012</date>
<contexts>
<context position="6782" citStr="Simianer et al., 2012" startWordPosition="1058" endWordPosition="1061">model can produce, to stand in for the correct output in optimization. Our system was developed to handle both cases, with the decoder providing the k-best list to the learner, specifying whether to perform costaugmented selection. Sparse Features While utilizing sparse features is a primary motivation for performing large-scale discriminative training, which features to use and how to learn their weights can have a large impact on the potential benefit. To this end, we incorporate `1/`2 regularization for joint feature selection in order to improve efficiency and counter overfitting effects (Simianer et al., 2012). Furthermore, the PA update has a single learning rate η for all features, which specifies how much the feature weights can change at each update. However, since dense features (e.g., language model) are observed far more frequently than sparse features (e.g., rule id), we may instead want to use a per-feature learning rate that allows larger steps for features that do not have much support. Thus, we allow setting an adaptive per-feature learning rate (Green et al., 2013; Crammer et al., 2009; Duchi et al., 2011). w ← w + ηα (f(xi, y+) − f(xi, y0)) 2.2 Learner/Decoder Communication Training r</context>
</contexts>
<marker>Simianer, Riezler, Dyer, 2012</marker>
<rawString>P. Simianer, S. Riezler, and C. Dyer. 2012. Joint feature selection in distributed stochastic learning for large-scale</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>