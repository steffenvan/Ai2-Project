<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.025105">
<note confidence="0.994219">
Proceedings of the Third SIGdial Workshop on Discourse and Dialogue,
Philadelphia, July 2002, pp. 46-49. Association for Computational Linguistics.
</note>
<title confidence="0.993094">
Annotating Semantic Consistency of Speech Recognition Hypotheses
</title>
<author confidence="0.87599">
Iryna Gurevych, Robert Porzel and Michael Strube
</author>
<affiliation confidence="0.805814">
European Media Laboratory GmbH
</affiliation>
<address confidence="0.784907">
SchloB-Wolfsbrunnenweg 33
D-69118 Heidelberg, Germany
</address>
<email confidence="0.999474">
e-mail: {gurevych,porzel,strube}@eml.villa-bosch.de
</email>
<sectionHeader confidence="0.993919" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999565545454545">
Recent work on natural language processing
systems is aimed at more conversational,
context-adaptive systems in multiple do-
mains. An important requirement for such a
system is the automatic detection of the do-
main and a domain consistency check of the
given speech recognition hypotheses. We
report a pilot study addressing these tasks,
the underlying data collection and investi-
gate the feasibility of annotating the data re-
liably by human annotators.
</bodyText>
<sectionHeader confidence="0.998798" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999969041666667">
The complete understanding of naturally oc-
curring discourse is still an unsolved task in
computational linguistics. Several large re-
search efforts are underway to build multi-
domain and multimodal information systems,
e.g. the DARPA Communicator Program1, the
SmartKom research framework2 (Wahlster et
al., 2001), the AT&amp;T interactive speech and
multimodal user interface program3.
Dialogue systems which deal with com-
plex dialogues require the interaction of multi-
ple knowledge sources, e.g. domain, discourse
and user model (Flycht-Eriksson, 1999). Fur-
thermore NLP systems have to adapt to differ-
ent environments and applications. This can
only be achieved if the system is able to de-
termine how well a given speech recognition
hypothesis (SRH) fits within the respective
domain model and what domain should be
considered by the system currently in focus.
The purpose of this paper is to develop
an annotation scheme for annotating a corpus
of SRH with information on semantic consis-
tency and domain specificity. We investigate
</bodyText>
<footnote confidence="0.96708725">
1http://fofoca.mitre.org
2http://www.smartkom.com
3http://www.research.att.com/news/2002/January/IS
MUI.html
</footnote>
<bodyText confidence="0.9983685">
the feasibility of an automatic solution by first
looking at how reliably human annotators can
solve the task.
The structure of the paper is as follows:
Section 2 gives an overview of the domain
modeling component in the SmartKom system.
In Section 3 we report on the data collection
underlying our study. A description of the
suggested annotation scheme is given in
Section 4. Section 5 presents the results of an
experiment in which the reliability of human
annotations is investigated.
</bodyText>
<sectionHeader confidence="0.898252" genericHeader="method">
2 Domain Modeling in SmartKom
</sectionHeader>
<bodyText confidence="0.999958888888889">
The SmartKom research project (a consortium
of twelve academic and industrial partners)
aims at developing a multi-modal and multi-
domain information system. Domains include
cinema information, home electronic device
control, etc. A central goal is the development
of new computational methods for
disambiguating different modalities on
semantic and pragmatic levels.
The information flow in SmartKom is
organized as follows: On the input side the
parser picks an N-best list of hypotheses out of
the speech recognizer’s word lattice (Oerder
and Ney, 1993). This list is sent to the media
fusion component and then handed over to the
intention recognition component.
The main task of intention recognition
in SmartKom is to select the best hypothesis
from the N-best list produced by the parser.
This is then sent to the dialogue management
component for computing an appropriate
action. In order to find the best hypothesis, the
intention recognition module consults a
number of other components involved in
language, discourse and domain analysis and
requests confidence scores to make an
appropriate decision (s. Fig. 1).
</bodyText>
<listItem confidence="0.865556166666667">
Tasks of the domain modeling
component are:
• to supply a confidence score on the
consistency of SRH with respect to the
domain model;
• to detect the domain currently in focus.
</listItem>
<figureCaption confidence="0.996977">
Figure 1. Information flow
</figureCaption>
<bodyText confidence="0.999423333333333">
These tasks are inherently related to each
other: It is possible to assign SRH to certain
domains only if they are consistent with the
domain model. On the other hand, a
consistency score can only be useful when it is
given with respect to certain domains.
</bodyText>
<sectionHeader confidence="0.99679" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.999988428571429">
We consider semantic consistency scoring and
domain detection a classification task. The
question is whether it is feasible to solve this
task automatically. As a first step towards an
answer we reformulate the problem: automatic
classification of SRH is possible only if
humans are able to do that reliably.
</bodyText>
<subsectionHeader confidence="0.99921">
3.1 Data Collection
</subsectionHeader>
<bodyText confidence="0.9999966">
In order to test the reliability of such
annotations we collected a corpus of SRH. The
data collection was conducted by means of a
hidden operator test (Rapp and Strube, 2002).
In the test the SmartKom system was
simulated. We had 29 subjects prompted to say
certain inputs in 8 dialogues. 1479 turns were
recorded. Each user-turn in the dialogue
corresponded to a single intention, e.g. route
request or sights information request.
</bodyText>
<subsectionHeader confidence="0.999271">
3.2 Data Preprocessing
</subsectionHeader>
<bodyText confidence="0.999982533333334">
The data obtained from the hidden operator
tests had to be prepared for our study to com-
pose a corpus with N-best SRH. For this pur-
pose we sent the audio files to the speech rec-
ognizer. The input for the domain modeling
component, i.e. N-best lists of SRH were re-
corded in log-files and then processed with a
couple of Perl scripts. The final corpus con-
sisted of ca. 2300 SRH. This corresponds to ca.
1.55 speech recognition hypotheses per user’s
turn.
The SRH corpus was then transformed
into a set of annotation files which could be
read into MMAX, the annotation tool adopted
for this task (Mueller and Strube, 2001).
</bodyText>
<sectionHeader confidence="0.961076" genericHeader="method">
4 Annotation Scheme
</sectionHeader>
<bodyText confidence="0.9987885">
For our study, a markable, i.e. an expression to
be annotated, is a single SRH. The annotators
as well as the domain modeling component in
SmartKom currently do not take the dialogue
context into account and do not perform
context-dependent analysis. Hence, we
presented the markables completely out of
dialogue order and thus prevented the
annotators from interpreting SRH context-
dependently.
</bodyText>
<subsectionHeader confidence="0.998028">
4.1 Semantic Consistency
</subsectionHeader>
<bodyText confidence="0.999255266666667">
In the first step, the annotators had to classify
markables with respect to semantic consis-
tency. Semantic consistency is defined as well-
formedness of an SRH on an abstract semantic
level. We differentiate three classes of seman-
tic consistency: consistent, semi-consistent, or
inconsistent. First, all nouns and verbs con-
tained in the hypothesis are extracted and cor-
responding concepts are retrieved from a
lemma-concept dictionary (lexicon) supplied
for the annotators. The decision regarding con-
sistency, semi-consistency and inconsistency
has to be done on the basis of evaluating the
set of concepts corresponding to the individual
hypothesis.
</bodyText>
<listItem confidence="0.997336555555556">
• Consistent means that all concepts are
semantically related to each other, e.g.
&amp;quot;ich moechte die kuerzeste Route&amp;quot;4 is
mapped to the concepts &amp;quot;self&amp;quot;, &amp;quot;wish&amp;quot;,
&amp;quot;route&amp;quot; all of which are related to each
other. Therefore the hypothesis is con-
sidered consistent.
• The label semi-consistent is used if at
least a fragment of the hypothesis is
</listItem>
<figure confidence="0.95544925">
4 I’d like the shortest route.
Media Fusion
Language
parsing
Intention
recognition
Discourse
model
Dialogue
Management
Domain
model
</figure>
<bodyText confidence="0.999225714285714">
meaningful. For example, the hypothe-
sis &amp;quot;ich moechte das Video sind&amp;quot;5 is
considered semi-consistent as the
fragment &amp;quot;ich moechte das Video&amp;quot;, i.e.
a set of corresponding concepts &amp;quot;self&amp;quot;,
&amp;quot;want&amp;quot;, &amp;quot;video&amp;quot; is semantically well-
formed.
</bodyText>
<listItem confidence="0.966136333333333">
• Inconsistent hypotheses are those
whose conceptual mappings are not
semantically related within the domain
model. E.g. &amp;quot;ich wuerde die Karte ja
Wiedersehen&amp;quot;6 is conceptualized as
&amp;quot;self&amp;quot;, &amp;quot;map&amp;quot;, &amp;quot;parting&amp;quot;. This set of
concepts does not semantically make
sense and the hypothesis should be re-
jected.
</listItem>
<subsectionHeader confidence="0.979671">
4.2 Domain Detection
</subsectionHeader>
<bodyText confidence="0.999936107142857">
One of our considerations was that it is princi-
pally not always feasible to detect domains
from an SRH. This is because the output of
speech recognition is often corrupt, which
may, in many cases, lead to false domain as-
signments. We argue that domain detection is
dependent on the semantic consistency score.
Therefore, according to our annotation scheme
no domain analysis should be given to the se-
mantically inconsistent SRH.
If the hypothesis is considered either
consistent or semi-consistent, certain domains
will be assigned to it. The list of SmartKom
domains for this study is finite and includes the
following: route planning, sights information,
cinema information, electronic program guide,
home electronic device control, personal assis-
tance, interaction management, small-talk and
off-talk.
In some cases multiple domains can be
assigned to a single markable. The reason is
that some domains are inherently so close to
each other, e.g. cinema information and elec-
tronic program guide, that the distinction can
only be made when the context is taken into
account. As this is not the case for our study
we allow for the specification of multiple do-
mains per SRH.
</bodyText>
<page confidence="0.5444415">
5 I’d like the video are.
6 I would the map yes good-bye.
</page>
<sectionHeader confidence="0.949934" genericHeader="method">
5 Reliability of Annotations
</sectionHeader>
<subsectionHeader confidence="0.99708">
5.1 The Kappa Statistic
</subsectionHeader>
<bodyText confidence="0.9992778">
To measure the reliability of annotations we
used the Kappa statistic (Carletta, 1996).
The value of Kappa statistic (K) for se-
mantic consistency in our experiment was
0.58, which shows that there was not a high
level of agreement between annotators7. In the
field of content analysis, where the Kappa
statistic originated, K&gt;0.8 is usually taken to
indicate good reliability, 0.68&lt;K&lt;0.8 allows to
draw tentative conclusions.
</bodyText>
<figureCaption confidence="0.730074666666667">
The distribution of semantic consistency
classes and domain assignments is given in
Fig. 2.
</figureCaption>
<table confidence="0.995173153846154">
Type %
Consistent 51
Semi-consistent 10,3
Inconsistent 38,7
Domain %
Route planning 33,1
Sights info 13,3
Cinema info 10,8
Electr. Program guide 15,9
Home device control 12,0
Personal assistance 1,1
Interaction Management 13,1
Other 0,7
</table>
<figureCaption confidence="0.997846">
Figure 2. Distribution of Classes
</figureCaption>
<subsectionHeader confidence="0.997617">
5.2 Discussion of the results
</subsectionHeader>
<bodyText confidence="0.999721166666667">
One reason for the relatively low coefficient of
agreement between annotators could be a small
number of annotators (two) as compared to
rather fine distinction between the classes in-
consistent vs. semi-consistent and semi-
consistent vs. consistent respectively.
Another reason arises from the analysis
of disagreements among annotators. We find
many annotation errors caused by the fact that
the annotators were not able to interpret the
conceptualized SRH correctly. In spite of the
fact that we emphasized the necessity of care-
</bodyText>
<footnote confidence="0.885698">
7 Results on the reliability of domain assignments
are not the subject of the present paper and will be
published elsewhere.
</footnote>
<bodyText confidence="0.9995578">
ful examination for high-quality annotations,
the annotators tended to take functional words
like prepositions into account. According to
our annotation scheme, however, they had to
be ignored during the analysis.
</bodyText>
<subsectionHeader confidence="0.999048">
5.3 Revisions to the annotation scheme
</subsectionHeader>
<bodyText confidence="0.999944041666667">
As already noted, one possible reason for dis-
agreements among annotators is a rather fine
distinction between the classes inconsistent vs.
semi-consistent and semi-consistent vs. consis-
tent. We had difficulties in defining strict crite-
ria for separating semi-consistent as a class on
its own. The percentage of its use is rather low
as compared to the other two and amounts to
10.3% on average.
A possible solution to this problem
might be to merge the class semi-consistent
with either consistent or inconsistent. We con-
ducted a corresponding experiment with the
available annotations.
In the first case we merged the classes
inconsistent and semi-consistent. We then ran
the Kappa statistic over the data and obtained
K=0.7. We found this to be a considerable
improvement as compared to earlier K=0.58.
In the second case we merged the
classes consistent and semi-consistent. The
Kappa statistic with this data amounted to
0.59, which could not be considered an im-
provement.
</bodyText>
<sectionHeader confidence="0.985763" genericHeader="method">
6 Concluding Remarks
</sectionHeader>
<bodyText confidence="0.999991848484848">
In this work we raised the question whether it
is possible to reliably annotate speech recogni-
tion hypotheses with information about seman-
tic consistency and domain specificity. The
motivation for that was to find out whether it is
feasible to develop and evaluate a computer
program addressing the same task and imple-
menting the algorithm reflected in the annota-
tion scheme.
We found that humans principally had
problems in looking solely at the conceptual-
ized speech recognition hypotheses. This,
however, should not be a problem for a ma-
chine where the word-to-concept mapping is
done automatically and all so-called function
words are discarded. In the future it would be
interesting to have humans annotate not speech
recognition hypotheses per se, but only their
automatically generated conceptual mappings.
Another finding was that the originally
proposed annotation scheme does not allow for
a high level of agreement between human an-
notators with respect to semantic consistency.
Eliminating the class semi-consistent led us,
however, to a considerably better reliability of
annotations.
We consider this study as a first attempt
to show the feasibility of determining semantic
consistency of the output of the speech recog-
nizer. We plan to integrate the results into the
domain modeling component and conduct
further experiments on semantic consistency
and domain detection.
</bodyText>
<sectionHeader confidence="0.996673" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9996744">
The work presented in this paper was con-
ducted within the SmartKom project partly
founded by the German Ministry of Research
and Technology under grant 01IL95I7 and by
the Klaus Tschira Foundation.
</bodyText>
<sectionHeader confidence="0.998974" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99984075862069">
Carletta, J. 1996. Assessing agreement on classifi-
cation tasks: The kappa statistic. Computational
Linguistics, 22 (2):249-254.
Flycht-Eriksson, A. 1999. A Survey of Knowledge
Sources in Dialogue Systems. In: Proc. of
IJCAI&apos;99 Workshop on Knowledge and Reason-
ing in Practical Dialogue Systems. Stockholm,
Sweden, pp. 41-48.
Mueller, C., Strube, M. 2001. Annotating anaphoric
and bridging expressions with MMAX. In:
Proc. of the 2nd SIGdial Workshop on Discourse
and Dialogue. Aalborg, Denmark, 2001, pp. 90-
95.
Oerder, M., Ney, H. 1993. Word Graphs: An Effi-
cient Interface between Continuous Speech
Recognition and Language Understanding.
In:Proc. of the International Conf. on Acoustics,
Speech and Signal Processing. IEEE Signal
Processing Society.
Rapp, S., Strube, M. 2002. An iterative data collec-
tion approach for multimodal dialogue sys-
tems. In: Proc. of the 3rd International Con-
ference on Language Resources and Evalua-
tion. Las Palmas, Canary Islands, Spain. To
appear.
Wahlster, W., Reithinger, N., Blocher, A. 2001.
SmartKom: Multimodal Communication with a
Life-Like Character. In: Proc. of Eurospeech
2001. Aalborg, Danemark, pp. 1547-1550.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.015052">
<note confidence="0.6776425">Proceedings of the Third SIGdial Workshop on Discourse and Dialogue, Philadelphia, July 2002, pp. 46-49. Association for Computational Linguistics.</note>
<title confidence="0.968309">Annotating Semantic Consistency of Speech Recognition Hypotheses</title>
<author confidence="0.834213">Iryna Gurevych</author>
<author confidence="0.834213">Robert Porzel</author>
<author confidence="0.834213">Michael</author>
<affiliation confidence="0.9664775">European Media Laboratory SchloB-Wolfsbrunnenweg</affiliation>
<address confidence="0.999394">D-69118 Heidelberg,</address>
<email confidence="0.999158">e-mail:{gurevych,porzel,strube}@eml.villa-bosch.de</email>
<abstract confidence="0.975775651376148">Recent work on natural language processing systems is aimed at more conversational, context-adaptive systems in multiple domains. An important requirement for such a system is the automatic detection of the domain and a domain consistency check of the given speech recognition hypotheses. We report a pilot study addressing these tasks, the underlying data collection and investigate the feasibility of annotating the data reliably by human annotators. The complete understanding of naturally occurring discourse is still an unsolved task in computational linguistics. Several large research efforts are underway to build multidomain and multimodal information systems, the DARPA Communicator the research (Wahlster et al., 2001), the AT&amp;T interactive speech and user interface Dialogue systems which deal with complex dialogues require the interaction of multiple knowledge sources, e.g. domain, discourse and user model (Flycht-Eriksson, 1999). Furthermore NLP systems have to adapt to different environments and applications. This can only be achieved if the system is able to determine how well a given speech recognition hypothesis (SRH) fits within the respective domain model and what domain should be considered by the system currently in focus. The purpose of this paper is to develop an annotation scheme for annotating a corpus of SRH with information on semantic consistency and domain specificity. We investigate MUI.html the feasibility of an automatic solution by first looking at how reliably human annotators can solve the task. The structure of the paper is as follows: Section 2 gives an overview of the domain modeling component in the SmartKom system. In Section 3 we report on the data collection underlying our study. A description of the suggested annotation scheme is given in Section 4. Section 5 presents the results of an experiment in which the reliability of human annotations is investigated. Modeling in SmartKom SmartKom research project consortium of twelve academic and industrial partners) aims at developing a multi-modal and multidomain information system. Domains include cinema information, home electronic device control, etc. A central goal is the development of new computational methods for disambiguating different modalities on semantic and pragmatic levels. The information flow in SmartKom is organized as follows: On the input side the parser picks an N-best list of hypotheses out of the speech recognizer’s word lattice (Oerder and Ney, 1993). This list is sent to the media fusion component and then handed over to the intention recognition component. The main task of intention recognition in SmartKom is to select the best hypothesis from the N-best list produced by the parser. This is then sent to the dialogue management component for computing an appropriate action. In order to find the best hypothesis, the intention recognition module consults a number of other components involved in language, discourse and domain analysis and requests confidence scores to make an appropriate decision (s. Fig. 1). Tasks of the domain modeling component are: • to supply a confidence score on the consistency of SRH with respect to the domain model; • to detect the domain currently in focus. Figure 1. Information flow These tasks are inherently related to each other: It is possible to assign SRH to certain domains only if they are consistent with the domain model. On the other hand, a consistency score can only be useful when it is given with respect to certain domains. We consider semantic consistency scoring and domain detection a classification task. The question is whether it is feasible to solve this task automatically. As a first step towards an answer we reformulate the problem: automatic classification of SRH is possible only if humans are able to do that reliably. 3.1 Data Collection In order to test the reliability of such annotations we collected a corpus of SRH. The data collection was conducted by means of a hidden operator test (Rapp and Strube, 2002). In the test the SmartKom system was simulated. We had 29 subjects prompted to say certain inputs in 8 dialogues. 1479 turns were recorded. Each user-turn in the dialogue corresponded to a single intention, e.g. route request or sights information request. 3.2 Data Preprocessing The data obtained from the hidden operator tests had to be prepared for our study to coma corpus with N-best SRH. For this purpose we sent the audio files to the speech recognizer. The input for the domain modeling component, i.e. N-best lists of SRH were recorded in log-files and then processed with a couple of Perl scripts. The final corpus consisted of ca. 2300 SRH. This corresponds to ca. 1.55 speech recognition hypotheses per user’s turn. The SRH corpus was then transformed into a set of annotation files which could be read into MMAX, the annotation tool adopted for this task (Mueller and Strube, 2001). Scheme For our study, a markable, i.e. an expression to be annotated, is a single SRH. The annotators as well as the domain modeling component in SmartKom currently do not take the dialogue context into account and do not perform context-dependent analysis. Hence, we presented the markables completely out of dialogue order and thus prevented annotators from interpreting SRH contextdependently. 4.1 Semantic Consistency In the first step, the annotators had to classify markables with respect to semantic consistency. Semantic consistency is defined as wellformedness of an SRH on an abstract semantic level. We differentiate three classes of semanconsistency: or First, all nouns and verbs contained in the hypothesis are extracted and corresponding concepts are retrieved from a lemma-concept dictionary (lexicon) supplied for the annotators. The decision regarding consistency, semi-consistency and inconsistency has to be done on the basis of evaluating the set of concepts corresponding to the individual hypothesis. Consistent that all concepts are semantically related to each other, e.g. moechte die kuerzeste is mapped to the concepts &amp;quot;self&amp;quot;, &amp;quot;wish&amp;quot;, &amp;quot;route&amp;quot; all of which are related to each other. Therefore the hypothesis is considered consistent. The label used if at least a fragment of the hypothesis is like the shortest route. Media Fusion Language parsing Intention recognition Discourse model Dialogue Management Domain model meaningful. For example, the hypothe- &amp;quot;ich moechte das Video is considered semi-consistent as the fragment &amp;quot;ich moechte das Video&amp;quot;, i.e. a set of corresponding concepts &amp;quot;self&amp;quot;, &amp;quot;want&amp;quot;, &amp;quot;video&amp;quot; is semantically wellformed. Inconsistent are those whose conceptual mappings are not semantically related within the domain model. E.g. &amp;quot;ich wuerde die Karte ja is conceptualized as &amp;quot;self&amp;quot;, &amp;quot;map&amp;quot;, &amp;quot;parting&amp;quot;. This set of concepts does not semantically make sense and the hypothesis should be rejected. 4.2 Domain Detection One of our considerations was that it is principally not always feasible to detect domains from an SRH. This is because the output of speech recognition is often corrupt, which may, in many cases, lead to false domain assignments. We argue that domain detection is dependent on the semantic consistency score. Therefore, according to our annotation scheme no domain analysis should be given to the semantically inconsistent SRH. If the hypothesis is considered either consistent or semi-consistent, certain domains will be assigned to it. The list of SmartKom domains for this study is finite and includes the following: route planning, sights information, cinema information, electronic program guide, home electronic device control, personal assistance, interaction management, small-talk and off-talk. In some cases multiple domains can be assigned to a single markable. The reason is that some domains are inherently so close to each other, e.g. cinema information and electronic program guide, that the distinction can only be made when the context is taken into account. As this is not the case for our study we allow for the specification of multiple domains per SRH. like the video are. would the map yes good-bye. of Annotations 5.1 The Kappa Statistic To measure the reliability of annotations we used the Kappa statistic (Carletta, 1996). The value of Kappa statistic (K) for semantic consistency in our experiment was 0.58, which shows that there was not a high of agreement between In the field of content analysis, where the Kappa statistic originated, K&gt;0.8 is usually taken to indicate good reliability, 0.68&lt;K&lt;0.8 allows to draw tentative conclusions. The distribution of semantic consistency classes and domain assignments is given in Fig. 2. Type % Consistent 51 Semi-consistent 10,3 Inconsistent 38,7 Domain % Route planning 33,1 Sights info 13,3 Cinema info 10,8 Electr. Program guide 15,9 Home device control 12,0 Personal assistance 1,1 Interaction Management 13,1 Other 0,7 Figure 2. Distribution of Classes 5.2 Discussion of the results One reason for the relatively low coefficient of agreement between annotators could be a small number of annotators (two) as compared to fine distinction between the classes insemi- Another reason arises from the analysis of disagreements among annotators. We find many annotation errors caused by the fact that the annotators were not able to interpret the conceptualized SRH correctly. In spite of the that we emphasized the necessity of careon the reliability of domain assignments are not the subject of the present paper and will be published elsewhere. ful examination for high-quality annotations, the annotators tended to take functional words like prepositions into account. According to our annotation scheme, however, they had to be ignored during the analysis. 5.3 Revisions to the annotation scheme As already noted, one possible reason for disagreements among annotators is a rather fine between the classes consis- We had difficulties in defining strict critefor separating a class on its own. The percentage of its use is rather low as compared to the other two and amounts to 10.3% on average. A possible solution to this problem be to merge the class either We conducted a corresponding experiment with the available annotations. In the first case we merged the classes We then ran the Kappa statistic over the data and obtained K=0.7. We found this to be a considerable improvement as compared to earlier K=0.58. In the second case we merged the The Kappa statistic with this data amounted to 0.59, which could not be considered an improvement. Remarks In this work we raised the question whether it is possible to reliably annotate speech recognition hypotheses with information about semantic consistency and domain specificity. The motivation for that was to find out whether it is feasible to develop and evaluate a computer program addressing the same task and implementing the algorithm reflected in the annotation scheme. We found that humans principally had in looking solely at the conceptualrecognition hypotheses. This, however, should not be a problem for a machine where the word-to-concept mapping is done automatically and all so-called function words are discarded. In the future it would be interesting to have humans annotate not speech hypotheses se, only their automatically generated conceptual mappings. Another finding was that the originally proposed annotation scheme does not allow for a high level of agreement between human annotators with respect to semantic consistency. the class led us, however, to a considerably better reliability of annotations. We consider this study as a first attempt to show the feasibility of determining semantic consistency of the output of the speech recognizer. We plan to integrate the results into the domain modeling component and conduct further experiments on semantic consistency and domain detection.</abstract>
<note confidence="0.836256476190476">Acknowledgements The work presented in this paper was conducted within the SmartKom project partly founded by the German Ministry of Research and Technology under grant 01IL95I7 and by the Klaus Tschira Foundation. References Carletta, J. 1996. Assessing agreement on classifitasks: The kappa statistic. 22 (2):249-254. Flycht-Eriksson, A. 1999. A Survey of Knowledge in Dialogue Systems. In: of IJCAI&apos;99 Workshop on Knowledge and Reasonin Practical Dialogue Stockholm, Sweden, pp. 41-48. Mueller, C., Strube, M. 2001. Annotating anaphoric and bridging expressions with MMAX. In: of the SIGdial Workshop on Discourse Aalborg, Denmark, 2001, pp. 90- 95. Oerder, M., Ney, H. 1993. Word Graphs: An Effi-</note>
<title confidence="0.697476333333333">cient Interface between Continuous Speech Recognition and Language Understanding. of the International Conf. on Acoustics,</title>
<author confidence="0.593655">Signal</author>
<affiliation confidence="0.72267">Processing Society.</affiliation>
<note confidence="0.9381729">Rapp, S., Strube, M. 2002. An iterative data collecapproach for multimodal dialogue sys- In: of the 3rd International Conference on Language Resources and Evalua- Las Palmas, Canary Islands, Spain. To appear. Wahlster, W., Reithinger, N., Blocher, A. 2001. SmartKom: Multimodal Communication with a Character. In: of Eurospeech 2001. Aalborg, Danemark, pp. 1547-1550.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Carletta</author>
</authors>
<title>Assessing agreement on classification tasks: The kappa statistic.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<pages>2--249</pages>
<contexts>
<context position="9013" citStr="Carletta, 1996" startWordPosition="1399" endWordPosition="1400">nteraction management, small-talk and off-talk. In some cases multiple domains can be assigned to a single markable. The reason is that some domains are inherently so close to each other, e.g. cinema information and electronic program guide, that the distinction can only be made when the context is taken into account. As this is not the case for our study we allow for the specification of multiple domains per SRH. 5 I’d like the video are. 6 I would the map yes good-bye. 5 Reliability of Annotations 5.1 The Kappa Statistic To measure the reliability of annotations we used the Kappa statistic (Carletta, 1996). The value of Kappa statistic (K) for semantic consistency in our experiment was 0.58, which shows that there was not a high level of agreement between annotators7. In the field of content analysis, where the Kappa statistic originated, K&gt;0.8 is usually taken to indicate good reliability, 0.68&lt;K&lt;0.8 allows to draw tentative conclusions. The distribution of semantic consistency classes and domain assignments is given in Fig. 2. Type % Consistent 51 Semi-consistent 10,3 Inconsistent 38,7 Domain % Route planning 33,1 Sights info 13,3 Cinema info 10,8 Electr. Program guide 15,9 Home device contro</context>
</contexts>
<marker>Carletta, 1996</marker>
<rawString>Carletta, J. 1996. Assessing agreement on classification tasks: The kappa statistic. Computational Linguistics, 22 (2):249-254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Flycht-Eriksson</author>
</authors>
<title>A Survey of Knowledge Sources in Dialogue Systems. In:</title>
<date>1999</date>
<booktitle>Proc. of IJCAI&apos;99 Workshop on Knowledge and Reasoning in Practical Dialogue Systems.</booktitle>
<pages>41--48</pages>
<location>Stockholm,</location>
<contexts>
<context position="1424" citStr="Flycht-Eriksson, 1999" startWordPosition="192" endWordPosition="193">ility of annotating the data reliably by human annotators. 1 Introduction The complete understanding of naturally occurring discourse is still an unsolved task in computational linguistics. Several large research efforts are underway to build multidomain and multimodal information systems, e.g. the DARPA Communicator Program1, the SmartKom research framework2 (Wahlster et al., 2001), the AT&amp;T interactive speech and multimodal user interface program3. Dialogue systems which deal with complex dialogues require the interaction of multiple knowledge sources, e.g. domain, discourse and user model (Flycht-Eriksson, 1999). Furthermore NLP systems have to adapt to different environments and applications. This can only be achieved if the system is able to determine how well a given speech recognition hypothesis (SRH) fits within the respective domain model and what domain should be considered by the system currently in focus. The purpose of this paper is to develop an annotation scheme for annotating a corpus of SRH with information on semantic consistency and domain specificity. We investigate 1http://fofoca.mitre.org 2http://www.smartkom.com 3http://www.research.att.com/news/2002/January/IS MUI.html the feasib</context>
</contexts>
<marker>Flycht-Eriksson, 1999</marker>
<rawString>Flycht-Eriksson, A. 1999. A Survey of Knowledge Sources in Dialogue Systems. In: Proc. of IJCAI&apos;99 Workshop on Knowledge and Reasoning in Practical Dialogue Systems. Stockholm, Sweden, pp. 41-48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Mueller</author>
<author>M Strube</author>
</authors>
<title>Annotating anaphoric and bridging expressions with MMAX. In:</title>
<date>2001</date>
<booktitle>Proc. of the 2nd SIGdial Workshop on Discourse and Dialogue.</booktitle>
<pages>90--95</pages>
<location>Aalborg, Denmark,</location>
<contexts>
<context position="5530" citStr="Mueller and Strube, 2001" startWordPosition="855" endWordPosition="858">ssing The data obtained from the hidden operator tests had to be prepared for our study to compose a corpus with N-best SRH. For this purpose we sent the audio files to the speech recognizer. The input for the domain modeling component, i.e. N-best lists of SRH were recorded in log-files and then processed with a couple of Perl scripts. The final corpus consisted of ca. 2300 SRH. This corresponds to ca. 1.55 speech recognition hypotheses per user’s turn. The SRH corpus was then transformed into a set of annotation files which could be read into MMAX, the annotation tool adopted for this task (Mueller and Strube, 2001). 4 Annotation Scheme For our study, a markable, i.e. an expression to be annotated, is a single SRH. The annotators as well as the domain modeling component in SmartKom currently do not take the dialogue context into account and do not perform context-dependent analysis. Hence, we presented the markables completely out of dialogue order and thus prevented the annotators from interpreting SRH contextdependently. 4.1 Semantic Consistency In the first step, the annotators had to classify markables with respect to semantic consistency. Semantic consistency is defined as wellformedness of an SRH o</context>
</contexts>
<marker>Mueller, Strube, 2001</marker>
<rawString>Mueller, C., Strube, M. 2001. Annotating anaphoric and bridging expressions with MMAX. In: Proc. of the 2nd SIGdial Workshop on Discourse and Dialogue. Aalborg, Denmark, 2001, pp. 90-95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Oerder</author>
<author>H Ney</author>
</authors>
<title>Word Graphs: An Efficient Interface between Continuous Speech Recognition and Language Understanding.</title>
<date>1993</date>
<booktitle>In:Proc. of the International Conf. on Acoustics, Speech and Signal Processing. IEEE Signal Processing Society.</booktitle>
<contexts>
<context position="3088" citStr="Oerder and Ney, 1993" startWordPosition="442" endWordPosition="445"> human annotations is investigated. 2 Domain Modeling in SmartKom The SmartKom research project (a consortium of twelve academic and industrial partners) aims at developing a multi-modal and multidomain information system. Domains include cinema information, home electronic device control, etc. A central goal is the development of new computational methods for disambiguating different modalities on semantic and pragmatic levels. The information flow in SmartKom is organized as follows: On the input side the parser picks an N-best list of hypotheses out of the speech recognizer’s word lattice (Oerder and Ney, 1993). This list is sent to the media fusion component and then handed over to the intention recognition component. The main task of intention recognition in SmartKom is to select the best hypothesis from the N-best list produced by the parser. This is then sent to the dialogue management component for computing an appropriate action. In order to find the best hypothesis, the intention recognition module consults a number of other components involved in language, discourse and domain analysis and requests confidence scores to make an appropriate decision (s. Fig. 1). Tasks of the domain modeling co</context>
</contexts>
<marker>Oerder, Ney, 1993</marker>
<rawString>Oerder, M., Ney, H. 1993. Word Graphs: An Efficient Interface between Continuous Speech Recognition and Language Understanding. In:Proc. of the International Conf. on Acoustics, Speech and Signal Processing. IEEE Signal Processing Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Rapp</author>
<author>M Strube</author>
</authors>
<title>An iterative data collection approach for multimodal dialogue systems. In:</title>
<date>2002</date>
<booktitle>Proc. of the 3rd International Conference on Language Resources and Evaluation. Las Palmas, Canary Islands,</booktitle>
<note>To appear.</note>
<contexts>
<context position="4630" citStr="Rapp and Strube, 2002" startWordPosition="699" endWordPosition="702">l. On the other hand, a consistency score can only be useful when it is given with respect to certain domains. 3 Data We consider semantic consistency scoring and domain detection a classification task. The question is whether it is feasible to solve this task automatically. As a first step towards an answer we reformulate the problem: automatic classification of SRH is possible only if humans are able to do that reliably. 3.1 Data Collection In order to test the reliability of such annotations we collected a corpus of SRH. The data collection was conducted by means of a hidden operator test (Rapp and Strube, 2002). In the test the SmartKom system was simulated. We had 29 subjects prompted to say certain inputs in 8 dialogues. 1479 turns were recorded. Each user-turn in the dialogue corresponded to a single intention, e.g. route request or sights information request. 3.2 Data Preprocessing The data obtained from the hidden operator tests had to be prepared for our study to compose a corpus with N-best SRH. For this purpose we sent the audio files to the speech recognizer. The input for the domain modeling component, i.e. N-best lists of SRH were recorded in log-files and then processed with a couple of </context>
</contexts>
<marker>Rapp, Strube, 2002</marker>
<rawString>Rapp, S., Strube, M. 2002. An iterative data collection approach for multimodal dialogue systems. In: Proc. of the 3rd International Conference on Language Resources and Evaluation. Las Palmas, Canary Islands, Spain. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Wahlster</author>
<author>N Reithinger</author>
<author>A Blocher</author>
</authors>
<title>SmartKom: Multimodal Communication with a Life-Like Character. In:</title>
<date>2001</date>
<booktitle>Proc. of Eurospeech</booktitle>
<pages>1547--1550</pages>
<location>Aalborg, Danemark,</location>
<contexts>
<context position="1187" citStr="Wahlster et al., 2001" startWordPosition="157" endWordPosition="160">r such a system is the automatic detection of the domain and a domain consistency check of the given speech recognition hypotheses. We report a pilot study addressing these tasks, the underlying data collection and investigate the feasibility of annotating the data reliably by human annotators. 1 Introduction The complete understanding of naturally occurring discourse is still an unsolved task in computational linguistics. Several large research efforts are underway to build multidomain and multimodal information systems, e.g. the DARPA Communicator Program1, the SmartKom research framework2 (Wahlster et al., 2001), the AT&amp;T interactive speech and multimodal user interface program3. Dialogue systems which deal with complex dialogues require the interaction of multiple knowledge sources, e.g. domain, discourse and user model (Flycht-Eriksson, 1999). Furthermore NLP systems have to adapt to different environments and applications. This can only be achieved if the system is able to determine how well a given speech recognition hypothesis (SRH) fits within the respective domain model and what domain should be considered by the system currently in focus. The purpose of this paper is to develop an annotation </context>
</contexts>
<marker>Wahlster, Reithinger, Blocher, 2001</marker>
<rawString>Wahlster, W., Reithinger, N., Blocher, A. 2001. SmartKom: Multimodal Communication with a Life-Like Character. In: Proc. of Eurospeech 2001. Aalborg, Danemark, pp. 1547-1550.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>