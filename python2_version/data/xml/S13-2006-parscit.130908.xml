<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.9898785">
SOFTCARDINALITY: Learning to Identify Directional
Cross-Lingual Entailment from Cardinalities and SMT
</title>
<author confidence="0.916179">
Sergio Jimenez, Claudia Becerra
</author>
<affiliation confidence="0.813747">
Universidad Nacional de Colombia
</affiliation>
<address confidence="0.790674">
Ciudad Universitaria,
edificio 453, oficina 114
Bogotá, Colombia
</address>
<email confidence="0.988642">
sgjimenezv@unal.edu.co
cjbecerrac@unal.edu.co
</email>
<sectionHeader confidence="0.996651" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9952018">
In this paper we describe our system submit-
ted for evaluation in the CLTE-SemEval-2013
task, which achieved the best results in two
of the four data sets, and finished third in av-
erage. This system consists of a SVM clas-
sifier with features extracted from texts (and
their translations SMT) based on a cardinality
function. Such function was the soft cardinal-
ity. Furthermore, this system was simplified
by providing a single model for the 4 pairs
of languages obtaining better (unofficial) re-
sults than separate models for each language
pair. We also evaluated the use of additional
circular-pivoting translations achieving results
6.14% above the best official results.
</bodyText>
<sectionHeader confidence="0.998767" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999683647058824">
The Cross-Lingual Textual Entailment (CLTE) task
consists in determining the type of directional en-
tailment (i.e. forward, backward, bidirectional or
no-entailment) between a pair of texts T1 and T2,
each one written in different languages (Negri et al.,
2013). The texts and reference annotations for this
task were obtained through crowdsourcing applied
to simpler sub-tasks (Negri et al., 2011). CLTE has
as main applications content synchronization and
aggregation in different languages (Mehdad et al.,
2012; Duh et al., 2013). We participated in the first
evaluation of this task in 2012 (Negri et al., 2012),
achieving third place on average among 29 partici-
pating systems (Jimenez et al., 2012).
Since in the CLTE task text pairs are in different
languages, in our system, all comparisons made be-
tween two texts imply that one of them was written
</bodyText>
<page confidence="0.983012">
34
</page>
<note confidence="0.8247508">
Alexander Gelbukh
CIC-IPN
Av. Juan Dios Bátiz, Av. Mendizábal,
Col. Nueva Industrial Vallejo
CP 07738, DF, México
</note>
<email confidence="0.967663">
gelbukh@gelbukh.com
</email>
<bodyText confidence="0.998167017857143">
by a human and the other is a translation provided by
statistical machine translation (SMT). Our approach
is based on an SVM classifier (Cortes and Vapnik,
1995) whose features were cardinalities combined
with similarity scores. That system was motivated
by the fact that most text similarity functions are
symmetric, e.g. Edit Distance (Levenshtein, 1966),
longest common sub-sequence (Hirschberg, 1977),
Jaro-Winkler similarity (Winkler, 1990), cosine sim-
ilarity (Salton et al., 1975). Thus, the use of these
functions as only resource seems counter-intuitive
since CLTE task is asymmetric for the forward and
backward entailment classes.
Moreover, cardinality is the central component of
the resemblance coefficients such as Jaccard, Dice,
overlap, etc. For instance, if T1 and T2 are texts
represented as bag of words, it is only necessary to
know the cardinalities |T1|, |T2 |and |T1 n T2 |to ob-
tain a similarity score using a resemblance coeffi-
cient such as the Dice’s coefficient (i.e. 2 · |T1 n
T2|/(|T1 |+ |T2|)). Therefore, the idea is to use the
individual cardinalities to enrich a set of features ex-
tracted from texts.
Cardinality gives a rough idea of the amount of
information in a collection of elements (i.e. words)
providing the number of different elements therein.
That is, in a collection of elements whose majority
are repetitions contains less information than a col-
lection whose elements are mostly different. How-
ever, the classical sets cardinality is a rigid mea-
sure as do not take account the degree of similarity
among the elements. Unlike the sets cardinality, soft
cardinality (Jimenez et al., 2010) uses the similari-
ties among the elements providing a more flexible
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 34–38, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics
measurement of the amount of information in a col-
lection. In the 2012 CLTE evaluation campaign, it
was noted that the soft cardinality overcame classi-
cal cardinality in the task at hand. All the models
used in our participation and proposed in this paper
are based on the soft cardinality. A brief descrip-
tion of the soft cardinality is presented in Section 2,
along with a description of the functions used to pro-
vide the similarities between words. Besides, the set
of features that are derived from all pairs of texts and
their cardinalities are presented in Section 3.
Section 4 provides a detailed description for each
of the 4 models (one for each language pair) used
to get the predictions submitted for evaluation. In
Section 5 a simplified-multilingual model is tested
with several word-similarity functions and circular-
pivoting translations.
In sections 6 and 7 a brief discussion of the results
and conclusions of our participation in this evalua-
tion campaign are presented.
</bodyText>
<sectionHeader confidence="0.906171" genericHeader="introduction">
2 Soft Cardinality
</sectionHeader>
<bodyText confidence="0.998537333333333">
The soft cardinality (Jimenez et al., 2010) of a col-
lection of words T is calculated with the following
expression:
</bodyText>
<equation confidence="0.748089722222222">
Having T
... ,
wi
0; p
0; 1 &gt;
sim(x, y)
0, x
y; and sim(x, x) = 1. The
parameter p controls the degree of &amp;quot;softness&amp;quot; of the
cardinality (the larger the
The coefficients
wi are weights associated with each word (or term)
t, which can represent the importance or informative
character of each word (e.g.
weights). The func-
tion sim is aword-similarity function. Three such
functions are considered in this paper:
Q-grams: each word ai is represented as a col-
</equation>
<bodyText confidence="0.29374">
lection of character q-grams (Kukich, 1992). In-
</bodyText>
<figure confidence="0.830994390243902">
stead of single length q-grams, a combination of
a range of lengths
to q2 was used. Next,
a couple of words are compared with the fol-
lowing resemblance coefficient: sim(ti, tj)
this word-similarity function are
q2,
an
={t1,t2,
tn};
≥
≥
≥
=6
“harder”).
idf
q1
=|ti∩tj|+biasα·max(|ti|,|tj|)+(1−α)·min(|ti|,|tj|). The parameters of
q1,
α
d bias.
#1 |T1|0 |T1∪T2|0
|T2|0 |T1−T2|0
|T1∩T2|0 |T2−T1|0
|T1∩T2|0/|T1|0 |T1∩T2|0/|T2|0
#9 |T1∩T2|0/|T1∪T2|0 #10|T1|0+|T2|0|T!∩T2|0 2·|T1∩T2|0
#11 |T1∩T2|0/√|T1|0·|T2|0 #12 m���|T1|0,|T2|0]
|T1∩T2|0+|T1|0+|T2|0 |T1|0 · |T2|0
2·|T1|0·|T2|0
Group 1: basic cardinalities
#4
#2
#5
#3
#6
Group 2: asymmetrical ratios
#7
#8
Group 3: similarity and arithmetical* scores
#13
#14*
</figure>
<equation confidence="0.71269325">
tj) = 1
Jaro-Winkler: this measure is based on the Jaro
(1989) similarity, which is given by this expression
Jaro(ti, tj) = 3 (len(ti) + len(tj) + c cm)
</equation>
<bodyText confidence="0.8855562">
here
is the number of characters in common within a slid-
ing window of length
division by 0, when c = 0 then Jaro(ti, tj) = 0. The
number of transpositions m is obtained sorting the
common characters according to their occurrence
in each of the words and counting the number of
non-matching characters. Winkler (1990) proposed
an extension to this measure taking into account
the common prefix length l through this expression:
</bodyText>
<equation confidence="0.983028">
sim(ti, tj) = Jaro(ti, tj) +
(1
Jaro(ti, tj
sim(ti,
−EditDistance(ti,tj)max[len(ti),len(tj)].
,w
c
max[len(ti),len(tj)] −1. To avoid
2
l10
−
</equation>
<bodyText confidence="0.85674">
)).
</bodyText>
<subsectionHeader confidence="0.89956">
For a pair of texts
</subsectionHeader>
<bodyText confidence="0.742694222222222">
and T2 represented as bags
of words three basic soft cardinalities can be cal-
culated:
and
The soft car-
dinality of their union is calculated using the con-
catenation of
and T2. More additional features
can be derived from these three basic features, e.g.
</bodyText>
<equation confidence="0.7664835">
T1
|T1|0,|T2|0
|T1∪T2|0.
T1
</equation>
<bodyText confidence="0.957162">
|T1∩T2|0=|T1|0+|T2|0−|T1∪T2|0 and|T1−T2|0=|T1|0−|T1∩T2|0. The complete set of features clas-
sified into three groups are shown in Table 1.
</bodyText>
<sectionHeader confidence="0.988161" genericHeader="method">
4 Submitted Runs Description
</sectionHeader>
<equation confidence="0.8937645">
� −1
�(1)
</equation>
<page confidence="0.986381">
35
</page>
<tableCaption confidence="0.865719">
Table 1: Set of features derived from texts
an
T1
d T2
Edit-Distance: a similarity score for a pair of
words can be obtained from their Edit Distance
(Levenshtein, 1966) by normalizing and converting
distance to similarity with the following expression:
</tableCaption>
<sectionHeader confidence="0.99028" genericHeader="method">
3 Features from Cardinali
</sectionHeader>
<subsectionHeader confidence="0.587876">
ties
</subsectionHeader>
<bodyText confidence="0.859324333333333">
The data for the 2013
task consists of 4 data
sets (spa-eng, ita-eng, fra-eng an
</bodyText>
<sectionHeader confidence="0.608369" genericHeader="method">
CLTE
</sectionHeader>
<bodyText confidence="0.847212">
d deu-eng) each
</bodyText>
<equation confidence="0.944241875">
�
�
wi �sim(ti, tj)p
j=1
|T|0 =
n
i=1
n
</equation>
<table confidence="0.9990808">
Data set q1 q2 a bias
deu-eng 2 2 0.5 0.0
fra-eng 2 3 0.5 0.0
ita-eng 2 4 0.6 0.0
spa-eng 1 3 0.5 0.1
</table>
<tableCaption confidence="0.9708255">
Table 2: Parameters of the q-grams word-similarity func-
tion for each language pair
</tableCaption>
<bodyText confidence="0.999284527777777">
with 1,000 pairs of texts for training and 500 for
testing. For each pair of texts T1 and T2 written
in two different languages, two translations are pro-
vided using the Google’s translator1. Thus, T1t is a
translation of T1 into the language of T2 and T2t is
a translation of T2 into the language of T1. Using
these pivoting translations, two pairs of texts can be
compared: T1 with T2t and T1t with T2.
Then all training and testing texts and their trans-
lations were pre-processed with the following se-
quence of actions: i) text strings were tokenized,
ii) uppercase characters are converted into lower-
case equivalents, iii) stop words were removed, iv)
punctuation marks were removed, and v) words were
stemmed using the Snowball2 multilingual stem-
mers provided by the NLTK Toolkit (Loper and
Bird, 2002). Then every stemmed word is tagged
with its idf weight (Jones, 2004) calculated with the
complete collection of texts and translations in the
same language.
Five instances of the soft cardinality are provided
using 1, 2, 3, 4 and 5 as values of the parameter
p. Therefore, the total number of features for each
pair of texts is the multiplication of the number of
features in the feature set (i.e. 14, see Table 1) by
the number of soft cardinality functions (5) and by 2,
corresponding to the two pairs of comparable texts.
That is, 14 x 5 x 2 = 140 features.
The sim function used was q-grams, whose pa-
rameters were adjusted for each language pair.
These parameters, which are shown in Table 2, were
obtained by manual exploration using the training
data.
Four vector data sets for training (one for each
language pair) were built by extracting the 140 fea-
tures from the 1,000 training instances and using
</bodyText>
<footnote confidence="0.999975">
1https://translate.google.com
2http://snowball.tartarus.org
</footnote>
<table confidence="0.998733625">
ECNUCS-team’s system
spa-eng ita-eng fra-eng deu-eng average
run4 0.422 0.416 0.436 0.452 0.432
run3 0.408 0.426 0.458 0.432 0.431
SOFTCARDINALITY-team’s system
spa-eng ita-eng fra-eng deu-eng average
run] 0.434 0.454 0.416 0.414 0.430
run2 0.432 0.448 0.426 0.402 0.427
</table>
<tableCaption confidence="0.994859">
Table 3: Official results for our system and the top per-
forming system ECNUCS (accuracies)
</tableCaption>
<bodyText confidence="0.999803846153846">
their gold-standard annotations as class attribute.
Predictions for the 500 test cases were obtained
through a SVM classifier trained with each data set.
For the submitted run], this SVM classifier used a
linear kernel with its complexity parameter set to its
default value C = 1. For the run2, this parameter
was adjusted for each pair of languages with the fol-
lowing values: Cspa−eng = 2.0, Cita−eng = 1.5,
Cfra−eng = 2.3 and Cdeu−eng = 2.0. The imple-
mentation of the SVM used is that which is available
in WEKA v.3.6.9 (SMO) (Hall et al., 2009). Official
results for run], run2 and best accuracies obtained
among all participant systems are shown in Table 3.
</bodyText>
<sectionHeader confidence="0.993482" genericHeader="method">
5 A Single Multilingual Model
</sectionHeader>
<bodyText confidence="0.99979465">
This section presents the results of our additional ex-
periments in search for a simplified model and in
turn to respond to the following questions: i) Can
one simplified-multilingual model overcome the ap-
proach presented in Section 4? ii) Does using addi-
tional circular-pivoting translations improve perfor-
mance? and iii) Do other word-similarity functions
work better than the q-grams measure?
First, it is important to note that the approach
described in Section 4 used only patterns discov-
ered in cardinalities. This means, that no language-
dependent features was used, with the exception of
the stemmers. Therefore, we wonder whether the
patterns discovered in a pair of languages can be use-
ful in other language pairs. To answer this question,
a single prediction model was built by aggregating
instances from each of the vector data sets into one
data set with 4,000 training instances. Afterward,
this model was used to provide predictions for the
2,000 test cases.
</bodyText>
<page confidence="0.996088">
36
</page>
<bodyText confidence="0.998932482758621">
Moreover, customization for each pair of lan-
guages in the word-similarity function, which is
show in Table 2, was set on the following unique set
of parameters: q1 = 1, q2 = 3, α = 0.5, bias = 0.0.
Thus, the words are compared using q-grams and
the Dice coefficient. In addition to the measure of
q-grams, two &amp;quot;off-the-shelf&amp;quot; measures were used as
nonparametric alternatives, namely: Edit Distance
(Levenshtein, 1966) and the Jaro-Winkler similarity
(Winkler, 1990).
In another attempt to simplify this model, we
evaluated the predictive ability of each of the three
groups of features shown in Table 1. The combi-
nation of groups 2 and 3, consistently obtained bet-
ter results when the evaluation with 10 fold cross-
validation was used in the training data. This result
was consistent with the simple training versus test
data evaluation. The sum of all previous simplifica-
tions significantly reduced the number of parameters
and features in comparison with the model described
in Section 4. That is, only one SVM and 4 parame-
ters, namely: α, bias, q1 and q2.
Besides, the additional use of circular-pivoting
translations was tested. In the original model, for
every pair of texts (T1, T2) their pivot translations
(Tt1, Tt2) were provided allowing the calculation of
|T1 ∪ Tt2 |and |T1t ∪ T2|. Translations T1t and T2t can
also be translated back to their original languages
obtaining Ttt
</bodyText>
<listItem confidence="0.830905">
1 and Ttt
2 . These additional transla-
tions in turn allows the calculation of |Ttt
1 ∪ Tt2|
and |T1t ∪ Ttt
2 |. This procedure can be repeated
again to obtain Tttt
</listItem>
<equation confidence="0.918465444444445">
1 and Tttt
2 , which in turn provides
|T1 ∪ Tttt
2 |, |Tttt
1 ∪ T2|, |Ttt
1 ∪ Tttt
2  |and |Tttt
1 ∪ Ttt
2 |.
</equation>
<bodyText confidence="0.9997075">
The original feature set is denoted as t. The extended
feature sets using double-pivoting translations and
triple-pivot translations are denoted respectively as
tt and ttt.
The results obtained with this simplified model
using single, double and triple pivot translations are
shown in Table 4. The first column indicates the
word-similarity function used by the soft cardinal-
ity and the second column indicates the number of
pivoting translations.
</bodyText>
<sectionHeader confidence="0.998602" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.99746">
In spite of the customization of the parameter C in
the run2, the run1 obtained better results than run2
</bodyText>
<table confidence="0.9995578">
Soft C. #t spa-e ita-e fra-e deu-e avg.
Ed.Dist. t 0.444 0.450 0.440 0.410 0.436
Ed.Dist. tt 0.452 0.464 0.434 0.432 0.446
Ed.Dist. ttt 0.464 0.468 0.440 0.424 0.449
Jaro-W. t 0.422 0.450 0.426 0.406 0.426
Jaro-W. tt 0.430 0.456 0.444 0.400 0.433
Jaro-W. ttt 0.426 0.458 0.430 0.430 0.436
q-grams t 0.428 0.456 0.456 0.432 0.443
q-grams tt 0.436 0.478 0.444 0.430 0.447
q-grams ttt 0.452 0.474 0.464 0.442 0.458
</table>
<tableCaption confidence="0.999793">
Table 4: Single-multilingual model results (accuracies)
</tableCaption>
<bodyText confidence="0.999974">
(see Table 3). This result indicates that the simpler
model produced better predictions in unseen data.
It is also important to note that two of the three
multilingual systems proposed in Section 5 achieved
higher scores than the best official results (see rows
containing “t” in Table 4). This indicates that the
proposed simplified model is able to discover pat-
terns in the cardinalities of a pair of languages and
project them into the other language pairs.
Regarding the use of additional circular-pivoting
translations, Table 4 shows that t was overcome on
average by tt and tt by ttt in all cases of the three
sets of results. The relative improvement obtained
by comparing t versus ttt for each group was 3.0% in
Edit Distance, 2.3% for Jaro-Winkler and 3.4% for
the q-gram measure. This same trend holds roughly
for each language pair.
</bodyText>
<sectionHeader confidence="0.999125" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999949466666667">
We described the SOFTCARDINALITY system
that participated in the SemEval CLTE evaluation
campaign in 2013, obtaining the best results in data
sets spa-eng and ita-eng, and achieving the third
place on average. This result was obtained using
separate models for each language pair. It was also
concluded that a single-multilingual model outper-
forms that approach. Besides, we found that the
use of additional pivoting translations provide bet-
ter results. Finally, the measure based on q-grams of
characters, used within the soft cardinality, resulted
to be the best option among other measures of word
similarity. In conclusion, the soft cardinality method
used in combination with SMT and SVM classifiers
is a competitive method for the CLTE task.
</bodyText>
<page confidence="0.999189">
37
</page>
<sectionHeader confidence="0.998186" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999874642857143">
This research was funded in part by the Systems
and Industrial Engineering Department, the Office
of Student Welfare of the National University of
Colombia, Bogotá, and through a grant from the
Colombian Department for Science, Technology
and Innovation, Colciencias, proj. 1101-521-28465
with funding from “El Patrimonio Autónomo Fondo
Nacional de Financiamiento para la Ciencia, la Tec-
nología y la Innovación, Francisco José de Caldas.”
The third author recognizes the support from Mexi-
can Government (SNI, COFAA-IPN, SIP 20131702,
CONACYT 50206-H) and CONACYT–DST India
(proj. 122030 “Answer Validation through Textual
Entailment”).
</bodyText>
<sectionHeader confidence="0.998337" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999662973684211">
Corinna Cortes and Vladimir N. Vapnik. 1995. Support-
vector networks. Machine Learning, 20(3):273–297.
Kevin Duh, Ching-Man Au Yeung, Tomoharu Iwata, and
Masaaki Nagata. 2013. Managing information dispar-
ity in multilingual document collections. ACM Trans.
Speech Lang. Process., 10(1):1:1–1:28, March.
Mark Hall, Frank Eibe, Geoffrey Holmes, and Bernhard
Pfahringer. 2009. The WEKA data mining software:
An update. SIGKDD Explorations, 11(1):10–18.
Daniel S. Hirschberg. 1977. Algorithms for the longest
common subsequence problem. J. ACM, 24(4):664–
675, October.
M.A. Jaro. 1989. Advances in record-linkage methodol-
ogy as applied to matching the 1985 census of tampa,
florida. Journal of the American Statistical Associa-
tion, pages 414–420, June.
Sergio Jimenez, Fabio Gonzalez, and Alexander Gel-
bukh. 2010. Text comparison using soft cardinality.
In Edgar Chavez and Stefano Lonardi, editors, String
Processing and Information Retrieval, volume 6393 of
LNCS, pages 297–302. Springer, Berlin, Heidelberg.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2012. Soft cardinality+ ML: learning adaptive
similarity functions for cross-lingual textual entail-
ment. In Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval, *SEM 2012),
Montreal, Canada. ACL.
Karen Spärck Jones. 2004. A statistical interpretation of
term specificity and its application in retrieval. Jour-
nal of Documentation, 60(5):493–502, October.
Karen Kukich. 1992. Techniques for automatically
correcting words in text. ACM Computing Surveys,
24:377–439, December.
Vladimir I. Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions, and reversals. Soviet
Physics Doklady, 10(8):707–710.
Edward Loper and Steven Bird. 2002. NLTK: the natu-
ral language toolkit. In Proceedings of the ACL Work-
shop on Effective Tools and Methodologies for Teach-
ing Natural Language Processing and Computational
Linguistics. Philadelphia. Association for Computa-
tional Linguistics.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2012. Detecting semantic equivalence and informa-
tion disparity in cross-lingual documents. In Proceed-
ings of the 50th Annual Meeting of the Association for
Computational Linguistics: Short Papers - Volume 2,
ACL ’12, page 120–124, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Matteo Negri, Luisa Bentivogli, Yashar Mehdad, Danilo
Giampiccolo, and Alessandro Marchetti. 2011. Di-
vide and conquer: crowdsourcing the creation of cross-
lingual textual entailment corpora. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, EMNLP ’11, page 670–679,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Matteo Negri, Alessandro Marchetti, Yashar Mehdad,
Luisa Bentivogli, and Danilo Giampiccolo. 2012.
2012. semeval-2012 task 8: Cross-lingual textual en-
tailment for content synchronization. In Proceedings
of the 6th International Workshop on Semantic Evalu-
ation (SemEval 2012), Montreal, Canada.
Matteo Negri, Alessandro Marchetti, Yashar Mehdad,
and Luisa Bentivogli. 2013. Semeval-2013 task
8: Cross-lingual textual entailment for content syn-
chronization. In Proceedings of the 7th International
Workshop on Semantic Evaluation (SemEval 2013).
Gerard Salton, Andrew K. C. Wong, and Chung-Shu
Yang. 1975. A vector space model for automatic in-
dexing. Commun. ACM, 18(11):613–620.
William E. Winkler. 1990. String comparator metrics
and enhanced decision rules in the fellegi-sunter model
of record linkage. In Proceedings of the Section on
Survey Research Methods, pages 354–359. American
Statistical Association.
</reference>
<page confidence="0.999339">
38
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.479761">
<title confidence="0.9962405">SOFTCARDINALITY: Learning to Identify Directional Cross-Lingual Entailment from Cardinalities and SMT</title>
<author confidence="0.980329">Sergio Jimenez</author>
<author confidence="0.980329">Claudia</author>
<affiliation confidence="0.709847">Universidad Nacional de Ciudad</affiliation>
<address confidence="0.763569">edificio 453, oficina Bogotá,</address>
<email confidence="0.987411">cjbecerrac@unal.edu.co</email>
<abstract confidence="0.998631625">In this paper we describe our system submitted for evaluation in the CLTE-SemEval-2013 task, which achieved the best results in two of the four data sets, and finished third in average. This system consists of a SVM classifier with features extracted from texts (and their translations SMT) based on a cardinality function. Such function was the soft cardinality. Furthermore, this system was simplified by providing a single model for the 4 pairs of languages obtaining better (unofficial) results than separate models for each language pair. We also evaluated the use of additional circular-pivoting translations achieving results 6.14% above the best official results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Corinna Cortes</author>
<author>Vladimir N Vapnik</author>
</authors>
<title>Supportvector networks.</title>
<date>1995</date>
<booktitle>Machine Learning,</booktitle>
<volume>20</volume>
<issue>3</issue>
<contexts>
<context position="2127" citStr="Cortes and Vapnik, 1995" startWordPosition="317" endWordPosition="320">3). We participated in the first evaluation of this task in 2012 (Negri et al., 2012), achieving third place on average among 29 participating systems (Jimenez et al., 2012). Since in the CLTE task text pairs are in different languages, in our system, all comparisons made between two texts imply that one of them was written 34 Alexander Gelbukh CIC-IPN Av. Juan Dios Bátiz, Av. Mendizábal, Col. Nueva Industrial Vallejo CP 07738, DF, México gelbukh@gelbukh.com by a human and the other is a translation provided by statistical machine translation (SMT). Our approach is based on an SVM classifier (Cortes and Vapnik, 1995) whose features were cardinalities combined with similarity scores. That system was motivated by the fact that most text similarity functions are symmetric, e.g. Edit Distance (Levenshtein, 1966), longest common sub-sequence (Hirschberg, 1977), Jaro-Winkler similarity (Winkler, 1990), cosine similarity (Salton et al., 1975). Thus, the use of these functions as only resource seems counter-intuitive since CLTE task is asymmetric for the forward and backward entailment classes. Moreover, cardinality is the central component of the resemblance coefficients such as Jaccard, Dice, overlap, etc. For </context>
</contexts>
<marker>Cortes, Vapnik, 1995</marker>
<rawString>Corinna Cortes and Vladimir N. Vapnik. 1995. Supportvector networks. Machine Learning, 20(3):273–297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Duh</author>
<author>Ching-Man Au Yeung</author>
<author>Tomoharu Iwata</author>
<author>Masaaki Nagata</author>
</authors>
<title>Managing information disparity in multilingual document collections.</title>
<date>2013</date>
<journal>ACM Trans. Speech Lang. Process.,</journal>
<volume>10</volume>
<issue>1</issue>
<contexts>
<context position="1505" citStr="Duh et al., 2013" startWordPosition="214" endWordPosition="217"> translations achieving results 6.14% above the best official results. 1 Introduction The Cross-Lingual Textual Entailment (CLTE) task consists in determining the type of directional entailment (i.e. forward, backward, bidirectional or no-entailment) between a pair of texts T1 and T2, each one written in different languages (Negri et al., 2013). The texts and reference annotations for this task were obtained through crowdsourcing applied to simpler sub-tasks (Negri et al., 2011). CLTE has as main applications content synchronization and aggregation in different languages (Mehdad et al., 2012; Duh et al., 2013). We participated in the first evaluation of this task in 2012 (Negri et al., 2012), achieving third place on average among 29 participating systems (Jimenez et al., 2012). Since in the CLTE task text pairs are in different languages, in our system, all comparisons made between two texts imply that one of them was written 34 Alexander Gelbukh CIC-IPN Av. Juan Dios Bátiz, Av. Mendizábal, Col. Nueva Industrial Vallejo CP 07738, DF, México gelbukh@gelbukh.com by a human and the other is a translation provided by statistical machine translation (SMT). Our approach is based on an SVM classifier (Co</context>
</contexts>
<marker>Duh, Yeung, Iwata, Nagata, 2013</marker>
<rawString>Kevin Duh, Ching-Man Au Yeung, Tomoharu Iwata, and Masaaki Nagata. 2013. Managing information disparity in multilingual document collections. ACM Trans. Speech Lang. Process., 10(1):1:1–1:28, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Frank Eibe</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
</authors>
<title>The WEKA data mining software: An update.</title>
<date>2009</date>
<journal>SIGKDD Explorations,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="10745" citStr="Hall et al., 2009" startWordPosition="1732" endWordPosition="1735">sults for our system and the top performing system ECNUCS (accuracies) their gold-standard annotations as class attribute. Predictions for the 500 test cases were obtained through a SVM classifier trained with each data set. For the submitted run], this SVM classifier used a linear kernel with its complexity parameter set to its default value C = 1. For the run2, this parameter was adjusted for each pair of languages with the following values: Cspa−eng = 2.0, Cita−eng = 1.5, Cfra−eng = 2.3 and Cdeu−eng = 2.0. The implementation of the SVM used is that which is available in WEKA v.3.6.9 (SMO) (Hall et al., 2009). Official results for run], run2 and best accuracies obtained among all participant systems are shown in Table 3. 5 A Single Multilingual Model This section presents the results of our additional experiments in search for a simplified model and in turn to respond to the following questions: i) Can one simplified-multilingual model overcome the approach presented in Section 4? ii) Does using additional circular-pivoting translations improve performance? and iii) Do other word-similarity functions work better than the q-grams measure? First, it is important to note that the approach described i</context>
</contexts>
<marker>Hall, Eibe, Holmes, Pfahringer, 2009</marker>
<rawString>Mark Hall, Frank Eibe, Geoffrey Holmes, and Bernhard Pfahringer. 2009. The WEKA data mining software: An update. SIGKDD Explorations, 11(1):10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel S Hirschberg</author>
</authors>
<title>Algorithms for the longest common subsequence problem.</title>
<date>1977</date>
<journal>J. ACM,</journal>
<volume>24</volume>
<issue>4</issue>
<pages>675</pages>
<contexts>
<context position="2370" citStr="Hirschberg, 1977" startWordPosition="351" endWordPosition="352">, all comparisons made between two texts imply that one of them was written 34 Alexander Gelbukh CIC-IPN Av. Juan Dios Bátiz, Av. Mendizábal, Col. Nueva Industrial Vallejo CP 07738, DF, México gelbukh@gelbukh.com by a human and the other is a translation provided by statistical machine translation (SMT). Our approach is based on an SVM classifier (Cortes and Vapnik, 1995) whose features were cardinalities combined with similarity scores. That system was motivated by the fact that most text similarity functions are symmetric, e.g. Edit Distance (Levenshtein, 1966), longest common sub-sequence (Hirschberg, 1977), Jaro-Winkler similarity (Winkler, 1990), cosine similarity (Salton et al., 1975). Thus, the use of these functions as only resource seems counter-intuitive since CLTE task is asymmetric for the forward and backward entailment classes. Moreover, cardinality is the central component of the resemblance coefficients such as Jaccard, Dice, overlap, etc. For instance, if T1 and T2 are texts represented as bag of words, it is only necessary to know the cardinalities |T1|, |T2 |and |T1 n T2 |to obtain a similarity score using a resemblance coefficient such as the Dice’s coefficient (i.e. 2 · |T1 n T</context>
</contexts>
<marker>Hirschberg, 1977</marker>
<rawString>Daniel S. Hirschberg. 1977. Algorithms for the longest common subsequence problem. J. ACM, 24(4):664– 675, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Jaro</author>
</authors>
<title>Advances in record-linkage methodology as applied to matching the 1985 census of tampa, florida.</title>
<date>1989</date>
<journal>Journal of the American Statistical Association,</journal>
<pages>414--420</pages>
<contexts>
<context position="6305" citStr="Jaro (1989)" startWordPosition="980" endWordPosition="981">ient: sim(ti, tj) this word-similarity function are q2, an ={t1,t2, tn}; ≥ ≥ ≥ =6 “harder”). idf q1 =|ti∩tj|+biasα·max(|ti|,|tj|)+(1−α)·min(|ti|,|tj|). The parameters of q1, α d bias. #1 |T1|0 |T1∪T2|0 |T2|0 |T1−T2|0 |T1∩T2|0 |T2−T1|0 |T1∩T2|0/|T1|0 |T1∩T2|0/|T2|0 #9 |T1∩T2|0/|T1∪T2|0 #10|T1|0+|T2|0|T!∩T2|0 2·|T1∩T2|0 #11 |T1∩T2|0/√|T1|0·|T2|0 #12 m���|T1|0,|T2|0] |T1∩T2|0+|T1|0+|T2|0 |T1|0 · |T2|0 2·|T1|0·|T2|0 Group 1: basic cardinalities #4 #2 #5 #3 #6 Group 2: asymmetrical ratios #7 #8 Group 3: similarity and arithmetical* scores #13 #14* tj) = 1 Jaro-Winkler: this measure is based on the Jaro (1989) similarity, which is given by this expression Jaro(ti, tj) = 3 (len(ti) + len(tj) + c cm) here is the number of characters in common within a sliding window of length division by 0, when c = 0 then Jaro(ti, tj) = 0. The number of transpositions m is obtained sorting the common characters according to their occurrence in each of the words and counting the number of non-matching characters. Winkler (1990) proposed an extension to this measure taking into account the common prefix length l through this expression: sim(ti, tj) = Jaro(ti, tj) + (1 Jaro(ti, tj sim(ti, −EditDistance(ti,tj)max[len(ti</context>
</contexts>
<marker>Jaro, 1989</marker>
<rawString>M.A. Jaro. 1989. Advances in record-linkage methodology as applied to matching the 1985 census of tampa, florida. Journal of the American Statistical Association, pages 414–420, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergio Jimenez</author>
<author>Fabio Gonzalez</author>
<author>Alexander Gelbukh</author>
</authors>
<title>Text comparison using soft cardinality.</title>
<date>2010</date>
<booktitle>String Processing and Information Retrieval,</booktitle>
<volume>6393</volume>
<pages>297--302</pages>
<editor>In Edgar Chavez and Stefano Lonardi, editors,</editor>
<publisher>Springer,</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="3595" citStr="Jimenez et al., 2010" startWordPosition="549" endWordPosition="552">|T1 |+ |T2|)). Therefore, the idea is to use the individual cardinalities to enrich a set of features extracted from texts. Cardinality gives a rough idea of the amount of information in a collection of elements (i.e. words) providing the number of different elements therein. That is, in a collection of elements whose majority are repetitions contains less information than a collection whose elements are mostly different. However, the classical sets cardinality is a rigid measure as do not take account the degree of similarity among the elements. Unlike the sets cardinality, soft cardinality (Jimenez et al., 2010) uses the similarities among the elements providing a more flexible Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 34–38, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics measurement of the amount of information in a collection. In the 2012 CLTE evaluation campaign, it was noted that the soft cardinality overcame classical cardinality in the task at hand. All the models used in our participation and proposed in this paper are based on the soft cardin</context>
<context position="4960" citStr="Jimenez et al., 2010" startWordPosition="764" endWordPosition="767">larities between words. Besides, the set of features that are derived from all pairs of texts and their cardinalities are presented in Section 3. Section 4 provides a detailed description for each of the 4 models (one for each language pair) used to get the predictions submitted for evaluation. In Section 5 a simplified-multilingual model is tested with several word-similarity functions and circularpivoting translations. In sections 6 and 7 a brief discussion of the results and conclusions of our participation in this evaluation campaign are presented. 2 Soft Cardinality The soft cardinality (Jimenez et al., 2010) of a collection of words T is calculated with the following expression: Having T ... , wi 0; p 0; 1 &gt; sim(x, y) 0, x y; and sim(x, x) = 1. The parameter p controls the degree of &amp;quot;softness&amp;quot; of the cardinality (the larger the The coefficients wi are weights associated with each word (or term) t, which can represent the importance or informative character of each word (e.g. weights). The function sim is aword-similarity function. Three such functions are considered in this paper: Q-grams: each word ai is represented as a collection of character q-grams (Kukich, 1992). Instead of single length q-</context>
</contexts>
<marker>Jimenez, Gonzalez, Gelbukh, 2010</marker>
<rawString>Sergio Jimenez, Fabio Gonzalez, and Alexander Gelbukh. 2010. Text comparison using soft cardinality. In Edgar Chavez and Stefano Lonardi, editors, String Processing and Information Retrieval, volume 6393 of LNCS, pages 297–302. Springer, Berlin, Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergio Jimenez</author>
<author>Claudia Becerra</author>
<author>Alexander Gelbukh</author>
</authors>
<title>Soft cardinality+ ML: learning adaptive similarity functions for cross-lingual textual entailment.</title>
<date>2012</date>
<booktitle>In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval, *SEM 2012),</booktitle>
<publisher>ACL.</publisher>
<location>Montreal, Canada.</location>
<contexts>
<context position="1676" citStr="Jimenez et al., 2012" startWordPosition="243" endWordPosition="246"> of directional entailment (i.e. forward, backward, bidirectional or no-entailment) between a pair of texts T1 and T2, each one written in different languages (Negri et al., 2013). The texts and reference annotations for this task were obtained through crowdsourcing applied to simpler sub-tasks (Negri et al., 2011). CLTE has as main applications content synchronization and aggregation in different languages (Mehdad et al., 2012; Duh et al., 2013). We participated in the first evaluation of this task in 2012 (Negri et al., 2012), achieving third place on average among 29 participating systems (Jimenez et al., 2012). Since in the CLTE task text pairs are in different languages, in our system, all comparisons made between two texts imply that one of them was written 34 Alexander Gelbukh CIC-IPN Av. Juan Dios Bátiz, Av. Mendizábal, Col. Nueva Industrial Vallejo CP 07738, DF, México gelbukh@gelbukh.com by a human and the other is a translation provided by statistical machine translation (SMT). Our approach is based on an SVM classifier (Cortes and Vapnik, 1995) whose features were cardinalities combined with similarity scores. That system was motivated by the fact that most text similarity functions are sym</context>
</contexts>
<marker>Jimenez, Becerra, Gelbukh, 2012</marker>
<rawString>Sergio Jimenez, Claudia Becerra, and Alexander Gelbukh. 2012. Soft cardinality+ ML: learning adaptive similarity functions for cross-lingual textual entailment. In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval, *SEM 2012), Montreal, Canada. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Spärck Jones</author>
</authors>
<title>A statistical interpretation of term specificity and its application in retrieval.</title>
<date>2004</date>
<journal>Journal of Documentation,</journal>
<volume>60</volume>
<issue>5</issue>
<contexts>
<context position="8936" citStr="Jones, 2004" startWordPosition="1434" endWordPosition="1435">anslation of T2 into the language of T1. Using these pivoting translations, two pairs of texts can be compared: T1 with T2t and T1t with T2. Then all training and testing texts and their translations were pre-processed with the following sequence of actions: i) text strings were tokenized, ii) uppercase characters are converted into lowercase equivalents, iii) stop words were removed, iv) punctuation marks were removed, and v) words were stemmed using the Snowball2 multilingual stemmers provided by the NLTK Toolkit (Loper and Bird, 2002). Then every stemmed word is tagged with its idf weight (Jones, 2004) calculated with the complete collection of texts and translations in the same language. Five instances of the soft cardinality are provided using 1, 2, 3, 4 and 5 as values of the parameter p. Therefore, the total number of features for each pair of texts is the multiplication of the number of features in the feature set (i.e. 14, see Table 1) by the number of soft cardinality functions (5) and by 2, corresponding to the two pairs of comparable texts. That is, 14 x 5 x 2 = 140 features. The sim function used was q-grams, whose parameters were adjusted for each language pair. These parameters,</context>
</contexts>
<marker>Jones, 2004</marker>
<rawString>Karen Spärck Jones. 2004. A statistical interpretation of term specificity and its application in retrieval. Journal of Documentation, 60(5):493–502, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Kukich</author>
</authors>
<title>Techniques for automatically correcting words in text.</title>
<date>1992</date>
<journal>ACM Computing Surveys,</journal>
<pages>24--377</pages>
<contexts>
<context position="5531" citStr="Kukich, 1992" startWordPosition="868" endWordPosition="869">he soft cardinality (Jimenez et al., 2010) of a collection of words T is calculated with the following expression: Having T ... , wi 0; p 0; 1 &gt; sim(x, y) 0, x y; and sim(x, x) = 1. The parameter p controls the degree of &amp;quot;softness&amp;quot; of the cardinality (the larger the The coefficients wi are weights associated with each word (or term) t, which can represent the importance or informative character of each word (e.g. weights). The function sim is aword-similarity function. Three such functions are considered in this paper: Q-grams: each word ai is represented as a collection of character q-grams (Kukich, 1992). Instead of single length q-grams, a combination of a range of lengths to q2 was used. Next, a couple of words are compared with the following resemblance coefficient: sim(ti, tj) this word-similarity function are q2, an ={t1,t2, tn}; ≥ ≥ ≥ =6 “harder”). idf q1 =|ti∩tj|+biasα·max(|ti|,|tj|)+(1−α)·min(|ti|,|tj|). The parameters of q1, α d bias. #1 |T1|0 |T1∪T2|0 |T2|0 |T1−T2|0 |T1∩T2|0 |T2−T1|0 |T1∩T2|0/|T1|0 |T1∩T2|0/|T2|0 #9 |T1∩T2|0/|T1∪T2|0 #10|T1|0+|T2|0|T!∩T2|0 2·|T1∩T2|0 #11 |T1∩T2|0/√|T1|0·|T2|0 #12 m���|T1|0,|T2|0] |T1∩T2|0+|T1|0+|T2|0 |T1|0 · |T2|0 2·|T1|0·|T2|0 Group 1: basic cardin</context>
</contexts>
<marker>Kukich, 1992</marker>
<rawString>Karen Kukich. 1992. Techniques for automatically correcting words in text. ACM Computing Surveys, 24:377–439, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir I Levenshtein</author>
</authors>
<title>Binary codes capable of correcting deletions, insertions, and reversals.</title>
<date>1966</date>
<journal>Soviet Physics Doklady,</journal>
<volume>10</volume>
<issue>8</issue>
<contexts>
<context position="2322" citStr="Levenshtein, 1966" startWordPosition="346" endWordPosition="347">t pairs are in different languages, in our system, all comparisons made between two texts imply that one of them was written 34 Alexander Gelbukh CIC-IPN Av. Juan Dios Bátiz, Av. Mendizábal, Col. Nueva Industrial Vallejo CP 07738, DF, México gelbukh@gelbukh.com by a human and the other is a translation provided by statistical machine translation (SMT). Our approach is based on an SVM classifier (Cortes and Vapnik, 1995) whose features were cardinalities combined with similarity scores. That system was motivated by the fact that most text similarity functions are symmetric, e.g. Edit Distance (Levenshtein, 1966), longest common sub-sequence (Hirschberg, 1977), Jaro-Winkler similarity (Winkler, 1990), cosine similarity (Salton et al., 1975). Thus, the use of these functions as only resource seems counter-intuitive since CLTE task is asymmetric for the forward and backward entailment classes. Moreover, cardinality is the central component of the resemblance coefficients such as Jaccard, Dice, overlap, etc. For instance, if T1 and T2 are texts represented as bag of words, it is only necessary to know the cardinalities |T1|, |T2 |and |T1 n T2 |to obtain a similarity score using a resemblance coefficient </context>
<context position="7618" citStr="Levenshtein, 1966" startWordPosition="1199" endWordPosition="1200">sented as bags of words three basic soft cardinalities can be calculated: and The soft cardinality of their union is calculated using the concatenation of and T2. More additional features can be derived from these three basic features, e.g. T1 |T1|0,|T2|0 |T1∪T2|0. T1 |T1∩T2|0=|T1|0+|T2|0−|T1∪T2|0 and|T1−T2|0=|T1|0−|T1∩T2|0. The complete set of features classified into three groups are shown in Table 1. 4 Submitted Runs Description � −1 �(1) 35 Table 1: Set of features derived from texts an T1 d T2 Edit-Distance: a similarity score for a pair of words can be obtained from their Edit Distance (Levenshtein, 1966) by normalizing and converting distance to similarity with the following expression: 3 Features from Cardinali ties The data for the 2013 task consists of 4 data sets (spa-eng, ita-eng, fra-eng an CLTE d deu-eng) each � � wi �sim(ti, tj)p j=1 |T|0 = n i=1 n Data set q1 q2 a bias deu-eng 2 2 0.5 0.0 fra-eng 2 3 0.5 0.0 ita-eng 2 4 0.6 0.0 spa-eng 1 3 0.5 0.1 Table 2: Parameters of the q-grams word-similarity function for each language pair with 1,000 pairs of texts for training and 500 for testing. For each pair of texts T1 and T2 written in two different languages, two translations are provide</context>
<context position="12282" citStr="Levenshtein, 1966" startWordPosition="1984" endWordPosition="1985">was built by aggregating instances from each of the vector data sets into one data set with 4,000 training instances. Afterward, this model was used to provide predictions for the 2,000 test cases. 36 Moreover, customization for each pair of languages in the word-similarity function, which is show in Table 2, was set on the following unique set of parameters: q1 = 1, q2 = 3, α = 0.5, bias = 0.0. Thus, the words are compared using q-grams and the Dice coefficient. In addition to the measure of q-grams, two &amp;quot;off-the-shelf&amp;quot; measures were used as nonparametric alternatives, namely: Edit Distance (Levenshtein, 1966) and the Jaro-Winkler similarity (Winkler, 1990). In another attempt to simplify this model, we evaluated the predictive ability of each of the three groups of features shown in Table 1. The combination of groups 2 and 3, consistently obtained better results when the evaluation with 10 fold crossvalidation was used in the training data. This result was consistent with the simple training versus test data evaluation. The sum of all previous simplifications significantly reduced the number of parameters and features in comparison with the model described in Section 4. That is, only one SVM and 4</context>
</contexts>
<marker>Levenshtein, 1966</marker>
<rawString>Vladimir I. Levenshtein. 1966. Binary codes capable of correcting deletions, insertions, and reversals. Soviet Physics Doklady, 10(8):707–710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Loper</author>
<author>Steven Bird</author>
</authors>
<title>NLTK: the natural language toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics. Philadelphia. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="8867" citStr="Loper and Bird, 2002" startWordPosition="1420" endWordPosition="1423">tor1. Thus, T1t is a translation of T1 into the language of T2 and T2t is a translation of T2 into the language of T1. Using these pivoting translations, two pairs of texts can be compared: T1 with T2t and T1t with T2. Then all training and testing texts and their translations were pre-processed with the following sequence of actions: i) text strings were tokenized, ii) uppercase characters are converted into lowercase equivalents, iii) stop words were removed, iv) punctuation marks were removed, and v) words were stemmed using the Snowball2 multilingual stemmers provided by the NLTK Toolkit (Loper and Bird, 2002). Then every stemmed word is tagged with its idf weight (Jones, 2004) calculated with the complete collection of texts and translations in the same language. Five instances of the soft cardinality are provided using 1, 2, 3, 4 and 5 as values of the parameter p. Therefore, the total number of features for each pair of texts is the multiplication of the number of features in the feature set (i.e. 14, see Table 1) by the number of soft cardinality functions (5) and by 2, corresponding to the two pairs of comparable texts. That is, 14 x 5 x 2 = 140 features. The sim function used was q-grams, who</context>
</contexts>
<marker>Loper, Bird, 2002</marker>
<rawString>Edward Loper and Steven Bird. 2002. NLTK: the natural language toolkit. In Proceedings of the ACL Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics. Philadelphia. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yashar Mehdad</author>
<author>Matteo Negri</author>
<author>Marcello Federico</author>
</authors>
<title>Detecting semantic equivalence and information disparity in cross-lingual documents.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers - Volume 2, ACL ’12,</booktitle>
<pages>120--124</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1486" citStr="Mehdad et al., 2012" startWordPosition="210" endWordPosition="213">nal circular-pivoting translations achieving results 6.14% above the best official results. 1 Introduction The Cross-Lingual Textual Entailment (CLTE) task consists in determining the type of directional entailment (i.e. forward, backward, bidirectional or no-entailment) between a pair of texts T1 and T2, each one written in different languages (Negri et al., 2013). The texts and reference annotations for this task were obtained through crowdsourcing applied to simpler sub-tasks (Negri et al., 2011). CLTE has as main applications content synchronization and aggregation in different languages (Mehdad et al., 2012; Duh et al., 2013). We participated in the first evaluation of this task in 2012 (Negri et al., 2012), achieving third place on average among 29 participating systems (Jimenez et al., 2012). Since in the CLTE task text pairs are in different languages, in our system, all comparisons made between two texts imply that one of them was written 34 Alexander Gelbukh CIC-IPN Av. Juan Dios Bátiz, Av. Mendizábal, Col. Nueva Industrial Vallejo CP 07738, DF, México gelbukh@gelbukh.com by a human and the other is a translation provided by statistical machine translation (SMT). Our approach is based on an</context>
</contexts>
<marker>Mehdad, Negri, Federico, 2012</marker>
<rawString>Yashar Mehdad, Matteo Negri, and Marcello Federico. 2012. Detecting semantic equivalence and information disparity in cross-lingual documents. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers - Volume 2, ACL ’12, page 120–124, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matteo Negri</author>
<author>Luisa Bentivogli</author>
<author>Yashar Mehdad</author>
<author>Danilo Giampiccolo</author>
<author>Alessandro Marchetti</author>
</authors>
<title>Divide and conquer: crowdsourcing the creation of crosslingual textual entailment corpora.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<pages>670--679</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1371" citStr="Negri et al., 2011" startWordPosition="194" endWordPosition="197">ining better (unofficial) results than separate models for each language pair. We also evaluated the use of additional circular-pivoting translations achieving results 6.14% above the best official results. 1 Introduction The Cross-Lingual Textual Entailment (CLTE) task consists in determining the type of directional entailment (i.e. forward, backward, bidirectional or no-entailment) between a pair of texts T1 and T2, each one written in different languages (Negri et al., 2013). The texts and reference annotations for this task were obtained through crowdsourcing applied to simpler sub-tasks (Negri et al., 2011). CLTE has as main applications content synchronization and aggregation in different languages (Mehdad et al., 2012; Duh et al., 2013). We participated in the first evaluation of this task in 2012 (Negri et al., 2012), achieving third place on average among 29 participating systems (Jimenez et al., 2012). Since in the CLTE task text pairs are in different languages, in our system, all comparisons made between two texts imply that one of them was written 34 Alexander Gelbukh CIC-IPN Av. Juan Dios Bátiz, Av. Mendizábal, Col. Nueva Industrial Vallejo CP 07738, DF, México gelbukh@gelbukh.com by a </context>
</contexts>
<marker>Negri, Bentivogli, Mehdad, Giampiccolo, Marchetti, 2011</marker>
<rawString>Matteo Negri, Luisa Bentivogli, Yashar Mehdad, Danilo Giampiccolo, and Alessandro Marchetti. 2011. Divide and conquer: crowdsourcing the creation of crosslingual textual entailment corpora. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, page 670–679, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matteo Negri</author>
<author>Alessandro Marchetti</author>
<author>Yashar Mehdad</author>
<author>Luisa Bentivogli</author>
<author>Danilo Giampiccolo</author>
</authors>
<title>semeval-2012 task 8: Cross-lingual textual entailment for content synchronization.</title>
<date>2012</date>
<booktitle>In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="1588" citStr="Negri et al., 2012" startWordPosition="229" endWordPosition="232">tion The Cross-Lingual Textual Entailment (CLTE) task consists in determining the type of directional entailment (i.e. forward, backward, bidirectional or no-entailment) between a pair of texts T1 and T2, each one written in different languages (Negri et al., 2013). The texts and reference annotations for this task were obtained through crowdsourcing applied to simpler sub-tasks (Negri et al., 2011). CLTE has as main applications content synchronization and aggregation in different languages (Mehdad et al., 2012; Duh et al., 2013). We participated in the first evaluation of this task in 2012 (Negri et al., 2012), achieving third place on average among 29 participating systems (Jimenez et al., 2012). Since in the CLTE task text pairs are in different languages, in our system, all comparisons made between two texts imply that one of them was written 34 Alexander Gelbukh CIC-IPN Av. Juan Dios Bátiz, Av. Mendizábal, Col. Nueva Industrial Vallejo CP 07738, DF, México gelbukh@gelbukh.com by a human and the other is a translation provided by statistical machine translation (SMT). Our approach is based on an SVM classifier (Cortes and Vapnik, 1995) whose features were cardinalities combined with similarity s</context>
</contexts>
<marker>Negri, Marchetti, Mehdad, Bentivogli, Giampiccolo, 2012</marker>
<rawString>Matteo Negri, Alessandro Marchetti, Yashar Mehdad, Luisa Bentivogli, and Danilo Giampiccolo. 2012. 2012. semeval-2012 task 8: Cross-lingual textual entailment for content synchronization. In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matteo Negri</author>
<author>Alessandro Marchetti</author>
<author>Yashar Mehdad</author>
<author>Luisa Bentivogli</author>
</authors>
<title>Semeval-2013 task 8: Cross-lingual textual entailment for content synchronization.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval</booktitle>
<contexts>
<context position="1234" citStr="Negri et al., 2013" startWordPosition="174" endWordPosition="177"> function was the soft cardinality. Furthermore, this system was simplified by providing a single model for the 4 pairs of languages obtaining better (unofficial) results than separate models for each language pair. We also evaluated the use of additional circular-pivoting translations achieving results 6.14% above the best official results. 1 Introduction The Cross-Lingual Textual Entailment (CLTE) task consists in determining the type of directional entailment (i.e. forward, backward, bidirectional or no-entailment) between a pair of texts T1 and T2, each one written in different languages (Negri et al., 2013). The texts and reference annotations for this task were obtained through crowdsourcing applied to simpler sub-tasks (Negri et al., 2011). CLTE has as main applications content synchronization and aggregation in different languages (Mehdad et al., 2012; Duh et al., 2013). We participated in the first evaluation of this task in 2012 (Negri et al., 2012), achieving third place on average among 29 participating systems (Jimenez et al., 2012). Since in the CLTE task text pairs are in different languages, in our system, all comparisons made between two texts imply that one of them was written 34 Al</context>
</contexts>
<marker>Negri, Marchetti, Mehdad, Bentivogli, 2013</marker>
<rawString>Matteo Negri, Alessandro Marchetti, Yashar Mehdad, and Luisa Bentivogli. 2013. Semeval-2013 task 8: Cross-lingual textual entailment for content synchronization. In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Andrew K C Wong</author>
<author>Chung-Shu Yang</author>
</authors>
<title>A vector space model for automatic indexing.</title>
<date>1975</date>
<journal>Commun. ACM,</journal>
<volume>18</volume>
<issue>11</issue>
<contexts>
<context position="2452" citStr="Salton et al., 1975" startWordPosition="360" endWordPosition="363">Alexander Gelbukh CIC-IPN Av. Juan Dios Bátiz, Av. Mendizábal, Col. Nueva Industrial Vallejo CP 07738, DF, México gelbukh@gelbukh.com by a human and the other is a translation provided by statistical machine translation (SMT). Our approach is based on an SVM classifier (Cortes and Vapnik, 1995) whose features were cardinalities combined with similarity scores. That system was motivated by the fact that most text similarity functions are symmetric, e.g. Edit Distance (Levenshtein, 1966), longest common sub-sequence (Hirschberg, 1977), Jaro-Winkler similarity (Winkler, 1990), cosine similarity (Salton et al., 1975). Thus, the use of these functions as only resource seems counter-intuitive since CLTE task is asymmetric for the forward and backward entailment classes. Moreover, cardinality is the central component of the resemblance coefficients such as Jaccard, Dice, overlap, etc. For instance, if T1 and T2 are texts represented as bag of words, it is only necessary to know the cardinalities |T1|, |T2 |and |T1 n T2 |to obtain a similarity score using a resemblance coefficient such as the Dice’s coefficient (i.e. 2 · |T1 n T2|/(|T1 |+ |T2|)). Therefore, the idea is to use the individual cardinalities to e</context>
</contexts>
<marker>Salton, Wong, Yang, 1975</marker>
<rawString>Gerard Salton, Andrew K. C. Wong, and Chung-Shu Yang. 1975. A vector space model for automatic indexing. Commun. ACM, 18(11):613–620.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William E Winkler</author>
</authors>
<title>String comparator metrics and enhanced decision rules in the fellegi-sunter model of record linkage.</title>
<date>1990</date>
<booktitle>In Proceedings of the Section on Survey Research Methods,</booktitle>
<pages>354--359</pages>
<publisher>American Statistical Association.</publisher>
<contexts>
<context position="2411" citStr="Winkler, 1990" startWordPosition="355" endWordPosition="356">ly that one of them was written 34 Alexander Gelbukh CIC-IPN Av. Juan Dios Bátiz, Av. Mendizábal, Col. Nueva Industrial Vallejo CP 07738, DF, México gelbukh@gelbukh.com by a human and the other is a translation provided by statistical machine translation (SMT). Our approach is based on an SVM classifier (Cortes and Vapnik, 1995) whose features were cardinalities combined with similarity scores. That system was motivated by the fact that most text similarity functions are symmetric, e.g. Edit Distance (Levenshtein, 1966), longest common sub-sequence (Hirschberg, 1977), Jaro-Winkler similarity (Winkler, 1990), cosine similarity (Salton et al., 1975). Thus, the use of these functions as only resource seems counter-intuitive since CLTE task is asymmetric for the forward and backward entailment classes. Moreover, cardinality is the central component of the resemblance coefficients such as Jaccard, Dice, overlap, etc. For instance, if T1 and T2 are texts represented as bag of words, it is only necessary to know the cardinalities |T1|, |T2 |and |T1 n T2 |to obtain a similarity score using a resemblance coefficient such as the Dice’s coefficient (i.e. 2 · |T1 n T2|/(|T1 |+ |T2|)). Therefore, the idea is</context>
<context position="6712" citStr="Winkler (1990)" startWordPosition="1053" endWordPosition="1054">|T1|0·|T2|0 Group 1: basic cardinalities #4 #2 #5 #3 #6 Group 2: asymmetrical ratios #7 #8 Group 3: similarity and arithmetical* scores #13 #14* tj) = 1 Jaro-Winkler: this measure is based on the Jaro (1989) similarity, which is given by this expression Jaro(ti, tj) = 3 (len(ti) + len(tj) + c cm) here is the number of characters in common within a sliding window of length division by 0, when c = 0 then Jaro(ti, tj) = 0. The number of transpositions m is obtained sorting the common characters according to their occurrence in each of the words and counting the number of non-matching characters. Winkler (1990) proposed an extension to this measure taking into account the common prefix length l through this expression: sim(ti, tj) = Jaro(ti, tj) + (1 Jaro(ti, tj sim(ti, −EditDistance(ti,tj)max[len(ti),len(tj)]. ,w c max[len(ti),len(tj)] −1. To avoid 2 l10 − )). For a pair of texts and T2 represented as bags of words three basic soft cardinalities can be calculated: and The soft cardinality of their union is calculated using the concatenation of and T2. More additional features can be derived from these three basic features, e.g. T1 |T1|0,|T2|0 |T1∪T2|0. T1 |T1∩T2|0=|T1|0+|T2|0−|T1∪T2|0 and|T1−T2|0=|</context>
<context position="12330" citStr="Winkler, 1990" startWordPosition="1990" endWordPosition="1991">vector data sets into one data set with 4,000 training instances. Afterward, this model was used to provide predictions for the 2,000 test cases. 36 Moreover, customization for each pair of languages in the word-similarity function, which is show in Table 2, was set on the following unique set of parameters: q1 = 1, q2 = 3, α = 0.5, bias = 0.0. Thus, the words are compared using q-grams and the Dice coefficient. In addition to the measure of q-grams, two &amp;quot;off-the-shelf&amp;quot; measures were used as nonparametric alternatives, namely: Edit Distance (Levenshtein, 1966) and the Jaro-Winkler similarity (Winkler, 1990). In another attempt to simplify this model, we evaluated the predictive ability of each of the three groups of features shown in Table 1. The combination of groups 2 and 3, consistently obtained better results when the evaluation with 10 fold crossvalidation was used in the training data. This result was consistent with the simple training versus test data evaluation. The sum of all previous simplifications significantly reduced the number of parameters and features in comparison with the model described in Section 4. That is, only one SVM and 4 parameters, namely: α, bias, q1 and q2. Besides</context>
</contexts>
<marker>Winkler, 1990</marker>
<rawString>William E. Winkler. 1990. String comparator metrics and enhanced decision rules in the fellegi-sunter model of record linkage. In Proceedings of the Section on Survey Research Methods, pages 354–359. American Statistical Association.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>