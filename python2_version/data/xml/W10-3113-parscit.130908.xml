<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000118">
<title confidence="0.999705">
Using SVMs with the Command Relation Features to
Identify Negated Events in Biomedical Literature
</title>
<author confidence="0.985503">
Farzaneh Sarafraz
</author>
<affiliation confidence="0.984788">
School of Computer Science
University of Manchester
</affiliation>
<address confidence="0.629582">
Manchester, United Kingdom
</address>
<email confidence="0.996274">
sarafraf@cs.man.ac.uk
</email>
<author confidence="0.994002">
Goran Nenadic
</author>
<affiliation confidence="0.998253">
School of Computer Science
University of Manchester
</affiliation>
<address confidence="0.625218">
Manchester, United Kingdom
</address>
<email confidence="0.995532">
g.nenadic@manchester.ac.uk
</email>
<sectionHeader confidence="0.994109" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.995752434782609">
In this paper we explore the identification of
negated molecular events (e.g. protein binding,
gene expressions, regulation, etc.) in biomedi-
cal research abstracts. We construe the prob-
lem as a classification task and apply a ma-
chine learning (ML) approach that uses lexi-
cal, syntactic, and semantic features associated
with sentences that represent events. Lexical
features include negation cues, whereas syn-
tactic features are engineered from constitu-
ency parse trees and the command relation be-
tween constituents. Semantic features include
event type and participants. We also consider a
rule-based approach that uses only the com-
mand relation. On a test dataset, the ML ap-
proach showed significantly better results
(51% F-measure) compared to the command-
based rules (35-42% F-measure). Training a
separate classifier for each event class proved
to be useful, as the micro-averaged F-score
improved to 63% (with 88% precision), dem-
onstrating the potential of task-specific ML
approaches to negation detection.
</bodyText>
<sectionHeader confidence="0.999129" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999938892857143">
With almost 2000 new papers published every
day, biomedical knowledge is mainly communi-
cated through a growing body of research papers.
As the amount of textual information increases,
the need for sophisticated information extraction
(IE) methods are becoming more than evident. IE
methods rely on a range of language processing
methods such as named entity recognition and
parsing to extract the required information in a
more structured form which can be used for
knowledge exploration and hypothesis genera-
tion (Donaldson et al. 2003; Natarajan et al.
2006).
Given the large number of publications, the
identification of conflicting or contradicting facts
is critical for systematic mining of biomedical
literature and knowledge consolidation. Detec-
tion of negations is of particular importance for
IE methods, as it often can hugely affect the
quality of the extracted information. For exam-
ple, when mining molecular events, a key piece
of information is whether the text states that the
two proteins are or are not interacting, or that a
given gene is or is not expressed. In recent years,
several challenges and shared tasks have in-
cluded the extraction of negations, typically as
part of other tasks (e.g. the BioNLP’09 Shared
Task 3 (Kim et al. 2009)).
Several systems and methods have aimed to
handle negation detection in order to improve the
quality of extracted information (Hakenberg et
al. 2009; Morante and Daelemans 2009). Prior
research on this topic has primarily focused on
finding negated concepts by negation cues and
scopes. These concepts are usually represented
by a set of predefined terms, and negation detec-
tion typically aims to determine whether a term
falls within the scope of a negation cue.
In this paper we address the task of identifi-
cation of negated events. We present a machine
learning (ML) method that combines a set of fea-
tures mainly engineered from a sentence parse
tree with lexical cues. More specifically, parse-
based features use the notion of the command
relation that models the scope affected by an
element (Langacker, 1969). We use molecular
events as a case study and experiment on the
BioNLP’09 data, which comprises a gold-
standard corpus of research abstracts manually
annotated for events and negations (Kim et al.
2009). The evaluation shows that, by using the
proposed approach, negated events can be identi-
fied with precision of 88% and recall of 49%
(63% F-measure). We compare these results with
two rule-based approaches that achieved the
maximum F-measure of 42%.
</bodyText>
<page confidence="0.991051">
78
</page>
<note confidence="0.640055">
Proceedings of the Workshop on Negation and Speculation in Natural Language Processing, pages 78–85,
Uppsala, July 2010.
</note>
<bodyText confidence="0.999758571428571">
The rest of this paper is organised as follows.
Section 2 summarises and reviews previous re-
search on negation extraction. Section 3 defines
the problem and introduces the data used for the
case study. Section 4 focuses on the ML-based
methodology for extracting negated events. The
final sections contain the results and discussions.
</bodyText>
<sectionHeader confidence="0.999725" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999941217391304">
There have been numerous contemplations of the
concept of negation (Lawler, 2010), but no gen-
eral agreement so far exists on its definition,
form, and function. We adopt here a definition of
negation as given by Cambridge Encyclopedia of
Language Sciences: “Negation is a comparison
between a ‘real’ situation lacking some element
and an ‘imaginal’ situation that does not lack it”.
The imaginal situation is affirmative compared
with the negative real situation. The element
whose polarity differs between the two situations
is the negation target.
Negations in natural language can be ex-
pressed by syntactically negative expressions, i.e.
with the use of negating words such as no, not,
never, etc. The word or phrase that makes the
sentence wholly or partially negative is the nega-
tion cue and the part of the sentence that is af-
fected by the negation cue and has become nega-
tive is the negation scope.
We briefly review two classes of approaches
to detect negations: those aiming at negated con-
cepts and those targeting negated events.
</bodyText>
<subsectionHeader confidence="0.995332">
2.1 Detecting Negated Concepts and
Phrases
</subsectionHeader>
<bodyText confidence="0.9841942">
There have been a number of approaches sug-
gested for detection of negated targets and
scopes. Most of them rely on task-specific, hand-
crafted rules of various complexities. They differ
in the size and composition of the list of negation
cues, and the way to utilise such a list. Some
methods use parse trees, whilst others use results
of shallow parsing.
Rule-based methods range from simple co-
occurrence based approaches to patterns that rely
on shallow parsing. The ‘bag-of-words’ ap-
proach, looking for proximate co-occurrences of
negation cues and terms in the same sentence, is
probably the simplest method for finding nega-
tions, and is used by many as a baseline method.
Many approaches have targeted the clinical
and biomedical domains. NegEx (Chapman et al.
2001), for example, uses two generic regular ex-
pressions that are triggered by negation phrases
such as:
&lt;negation cue&gt; * &lt;target term&gt;
&lt;target term&gt; * &lt;negation cue&gt;
where the asterisk (*) represents a string of up to
five tokens. Target terms represent domain con-
cepts that are terms from the Unified Medical
Language System (UMLS1). The cue set com-
prises 272 clinically-specific negation cues, in-
cluding those such as denial of or absence of.
Although simple, the proposed approach showed
good results on clinical data (78% sensitivity
(recall), 84% precision, and 94% specificity).
In addition to concepts that are explicitly ne-
gated by negation phrases, Patrick et al. (2006)
further consider so-called pre-coordinated nega-
tive terms (e.g. headache) that have been col-
lected from SNOMED CT2 medical terminology.
Similarly, NegFinder uses hand-crafted rules to
detect negated UMLS terms, including simple
conjunctive and disjunctive statements (Mutalik
et al. 2001). They used a list of 60 negation cues.
Tolentino et al. (2006), however, show that using
rules on a small set of only five negation cues
(no, neither/nor, ruled out, denies, without) can
still be reasonably successful in detecting nega-
tions in medical reports (F-score 91%).
Huang and Lowe (2007) introduced a negation
grammar that used regular expressions and de-
pendency parse trees to identify negation cues
and their scope in the sentence. They applied the
rules to a set of radiology reports and reported a
precision of 99% and a recall of 92%.
Not many efforts have been reported on using
machine learning to detect patterns in sentences
that contain negative expressions. Still, Morante
and Daelemans (2009), for example, used vari-
ous classifiers (Memory-based Learners, Support
Vector Machines, and Conditional Random
Fields) to detect negation cues and their scope.
An extensive list of features included the token’s
stem and part-of-speech, as well as those of the
neighbouring tokens. Separate classifiers were
used for detecting negation cues and negation
scopes. The method was applied to clinical text,
biomedical abstracts, and biomedical papers with
F-scores of 80%, 77%, and 68% respectively.
</bodyText>
<subsectionHeader confidence="0.999534">
2.2 Detecting Negated Events
</subsectionHeader>
<bodyText confidence="0.985248">
Several approaches have recently been suggested
for the extraction of negated events, particularly
</bodyText>
<footnote confidence="0.999494">
1 http://www.nlm.nih.gov/research/umls/
2 http://www.snomed.org
</footnote>
<page confidence="0.999297">
79
</page>
<bodyText confidence="0.9958796">
in the biomedical domain. Events are typically
represented via participants (biomedical entities
that take part in an event) and event triggers (to-
kens that indicate presence of the event). Van
Landeghem et al. (2008) used a rule-based ap-
proach based on token distances in sentence and
lexical information in event triggers to detect
negated molecular events. Kilicoglu and Bergler
(2009), Hakenberg et al. (2009), and Sanchez
(2007) used a number of heuristic rules concern-
ing the type of the negation cue and the type of
the dependency relation to detect negated mo-
lecular events described in text. For example, a
rule can state that if the negation cue is “lack” or
“absence”, then the trigger has to be in the
prepositional phrase of the cue; or that if the cue
is “unable” or “fail”, then the trigger has to be in
the clausal complement of the cue (Kilicoglu and
Bergler 2009). As expected, such approaches
suffer from lower recall.
MacKinlay et al. (2009), on the other hand,
use ML, assigning a vector of complex deep
parse features (including syntactic predicates to
capture negation scopes, conjunctions and se-
mantically negated verbs) to every event trigger.
The system achieved an F-score of 36% on the
same dataset as used in this paper.
We note that the methods mentioned above
mainly focus on finding negated triggers in order
to detect negated events. In this paper we explore
not only negation of triggers but also phrases in
which participants are negated (consider, for ex-
ample, “SLP-76” in the sentence “In contrast,
Grb2 can be coimmunoprecipitated with Sos1
and Sos2 but not with SLP-76.”)
</bodyText>
<sectionHeader confidence="0.993297" genericHeader="method">
3 Molecular Events
</sectionHeader>
<bodyText confidence="0.999897740740741">
As a case study, we look at identification of ne-
gated molecular events. In general, molecular
events include various types of reactions that
affect genes and protein molecules. Each event is
of a particular type (e.g. binding, phosphoryla-
tion, regulation, etc.). Depending on the type,
each event may have one or more participating
proteins (sometimes referred to as themes).
Regulatory events are particularly complex, as
they can have a cause (a protein or another
event) in addition to a theme, which can be either
a protein or another event. Table 1 shows exam-
ples of five events, where participants are bio-
medical entities (events 1-3) or other events
(events 4 and 5). Note that a sentence can ex-
press more than one molecular event.
Identification of molecular events in the litera-
ture is a challenging IE task (Kim et al. 2009;
Sarafraz et al. 2009). For the task of identifying
negated events, we assume that events have al-
ready been identified in text. Each event is repre-
sented by its type, a textual trigger, and one or
more participants or causes (see Table 1). Since
the participants of different event types can vary
in both their number and type, we consider three
classes of events to support our analysis (see
Section 5):
</bodyText>
<listItem confidence="0.999798285714286">
• Class I comprises events with exactly one
entity theme (e.g. transcription, protein ca-
tabolism, localization, gene expression,
phosphorylation).
• Class II events include binding events only,
which have one or more entity participants.
• Class III contains regulation events, which
</listItem>
<bodyText confidence="0.917986208333333">
have exactly one theme and possibly one
cause. However, the theme and the cause can
be entities or events of any type.
The corpus used in this study is provided by
the BioNLP’09 challenge (Kim et al. 2009). It
contains two sets of biomedical abstracts: a
“training” set (containing 800 abstracts used for
training and analysis purposes) and a “develop-
ment” set (containing 150 abstracts used for test-
ing purposes only). Both document sets are
manually annotated with information about en-
tity mentions (e.g. genes and proteins). Sentences
that report molecular events are further annotated
with the corresponding event type, textual trigger
and participants. In total, nine event types are
“The effect of this synergism was perceptible at the level of induction of the IL-2 gene.”
Event Trigger Type Participant (theme) Cause
Event 1 “induction” Gene expression IL-2
“Overexpression of full-length ALG-4 induced transcription of FasL and, consequently, apoptosis.”
Event Trigger Type Participant (theme) Cause
Event 2 “transcription” Transcription FasL Event 4
Event 3 “Overexpression” Gene expression ALG-4
Event 4 “Overexpression” Positive regulation Event 3
Event 5 “induced” Positive regulation Event 2
</bodyText>
<tableCaption confidence="0.998307">
Table 1: Examples of how molecular events described in text are characterised.
</tableCaption>
<page confidence="0.993954">
80
</page>
<bodyText confidence="0.998153428571428">
considered (gene expression, transcription, pro-
tein catabolism, localization, phosphorylation,
binding, regulation, positive regulation, and
negative regulation). In addition, every event has
been tagged as either affirmative (reporting a
specific interaction) or negative (reporting that a
specific interaction has not been observed).
</bodyText>
<tableCaption confidence="0.511654">
Table 2 provides an overview of the two
BioNLP’09 datasets. We note that only around
6% of events are negated.
</tableCaption>
<table confidence="0.999448571428571">
Event Training Development
class data data
total negated total negated
Class I 2,858 131 559 26
Class II 887 44 249 15
Class III 4,870 440 987 66
Total 9,685 615 1,795 107
</table>
<tableCaption confidence="0.998574">
Table 2: Overview of the total number of events and
negated event annotations in the two datasets.
</tableCaption>
<sectionHeader confidence="0.997611" genericHeader="method">
4 Methodology
</sectionHeader>
<bodyText confidence="0.999987111111111">
We consider two approaches to extract negated
events. We first discuss a rule-based approach
that uses constituency parse trees and the com-
mand relation to identify negated events. Then,
we introduce a ML method that combines lexi-
cal, syntactic and semantic features to identify
negated events. Note that in all cases, input sen-
tences have been pre-annotated for entity men-
tions, event triggers, types, and participants.
</bodyText>
<subsectionHeader confidence="0.80263">
4.1 Negation Detection Using the Command
Relation Rules
</subsectionHeader>
<bodyText confidence="0.998942375">
The question of which parts of a syntactic struc-
ture affect the other parts has been extensively
investigated. Langacker (1969) introduced the
concept of command to determine the scope
within a sentence affected by an element. More
precisely, if a and b are nodes in the constituency
parse tree of a sentence, then a X-commands b
iff the lowest ancestor of a with label X is also
an ancestor of b. Note that the command relation
is not symmetrical. Langacker observed that
when a S-commands b, then a affects the scope
containing b. For simplicity, we say “command”
when we mean S-command.
To determine whether token a commands to-
ken b, given the parse tree of a sentence, we use
a simple algorithm introduced by McCawley
(1993): trace up the branches of the constituency
parse tree from a until you hit a node that is la-
belled X. If b is reachable by tracing down the
branches of the tree from that node, then a X-
commands b; otherwise, it does not.
We hypothesise that if a negation cue com-
mands an event trigger or participant, then the
associated event is negated.
</bodyText>
<subsectionHeader confidence="0.857999">
4.2 Negation Detection Using Machine
Learning on Parse Tree Features
</subsectionHeader>
<bodyText confidence="0.998320777777778">
Given a sentence that describes an event, we fur-
ther construe the negation detection problem as a
classification task: the aim is to classify the event
as affirmative or negative. We explore both a
single SVM (support vector machine) classifier
for all events and three separate SVMs for each
of the event classes. The following features have
been engineered from an event-representing sen-
tence:
</bodyText>
<listItem confidence="0.968649565217391">
1. Event type (one of the nine types as defined
in BioNLP’09);
2. Whether the sentence contains a negation
cue from the cue list;
3. The negation cue itself (if present);
4. The part-of-speech (POS) tag of the negation
cue;
5. The POS tag of the event trigger;
6. The POS tag of the participants of the event.
If the participant is another event, the POS
tag of the trigger of that event is used;
7. The parse node type of the lowest common
ancestor of the trigger and the cue (i.e. the
type of the smallest phrase that contains both
the trigger and the cue, e.g. S, VP, PP, etc.);
8. Whether or not the negation cue commands
any of the participants; nested events (for
Class III) are treated as above (i.e. as being
represented by their triggers);
9. Whether or not the negation cue commands
the trigger;
10. The parse-tree distance between the event
trigger and the negation cue.
</listItem>
<bodyText confidence="0.999949333333333">
We use a default value (null) where none of
the other values apply (e.g. when there is no cue
in feature 3, 4, 7). These features have been used
to train four SVMs on the training dataset: one
modelled all events together, and the others
modelled the three event classes separately.
</bodyText>
<sectionHeader confidence="0.999963" genericHeader="method">
5 Results
</sectionHeader>
<bodyText confidence="0.99989975">
All the results refer to the methods applied on the
development dataset (see Table 2). If the nega-
tion detection task is regarded as an information
extraction task of finding positive instances (i.e.
</bodyText>
<page confidence="0.993478">
81
</page>
<bodyText confidence="0.998393714285714">
negated events), then precision, recall, and F-
score would be appropriate measures. If we con-
sider the classification aspect of the task, speci-
ficity is more appropriate if true negative hits are
considered as valuable as true positive ones. We
therefore use the following metrics to evaluate
the two methods:
</bodyText>
<equation confidence="0.726277">
TP
Precision=
TP+FP
TP
Recall=Sensitivity=
TP+FN
F1= 2x Precision x Recall
Precision+Recall
TN
Specificity=
TN+FP
</equation>
<bodyText confidence="0.9999924">
where TP denotes the number of true positives
(the number of correctly identified negated
events), FN is the number of false negatives (the
number of negated events that have been re-
ported as affirmative), with TN and FP defined
accordingly.
Two sets of negation cues were used in order
to compare their influence. A smaller set was
derived from related work, whereas additional
cues were semi-automatically extracted by ex-
ploring the training data. The small negation cue
set contains 14 words3, whereas the larger nega-
tion cue set contains 32 words4. As expected, the
larger set resulted in increased recall, but de-
creased precision. However, the effects on the F-
score were typically not significant. The results
are only shown using the larger cue set.
The texts were processed using the GENIA
tagger (Tsuruoka and Tsujii 2005).We used con-
stituency parse trees automatically produced by
two different constituency parsers reported in
(McClosky et al. 2006) and (Bikel 2004). No
major differences were observed in the results
using the two parsers. The data shown in the re-
sults are produced by the former.
</bodyText>
<subsectionHeader confidence="0.991519">
5.1 Baseline Results
</subsectionHeader>
<bodyText confidence="0.999928">
Our baseline method relies on an implementation
of the NegEx algorithm as explained in Section
2.1. Event triggers were used as negation targets
for the algorithm. An event is then considered to
be negated if the trigger is negated; otherwise it
</bodyText>
<construct confidence="0.388237125">
3 Negation cues in this set include: no, not, none,
negative, without, absence, fail, fails, failed, fail-
ure, cannot, lack, lacking, lacked.
4 Negation cues in this set include the smaller set and
18 task-specific words: inactive, neither, nor, in-
hibit, unable, blocks, blocking, preventing, pre-
vents, absent, never, unaffected, unchanged, im-
paired, little, independent, except, and exception.
</construct>
<bodyText confidence="0.999604">
is affirmative. The results (see Table 3) are sub-
stantially lower than those reported for NegEx on
clinical data (specificity of 94% and sensitivity
of 78%). For comparison, the table also provides
an even simpler baseline approach that tags as
negated any event whose associated sentence
contains any negation cue word.
</bodyText>
<table confidence="0.999073333333333">
Approach P R F1 Spec.
any negation cue present 20% 78% 32% 81%
NegEx 36% 37% 36% 93%
</table>
<tableCaption confidence="0.9938515">
Table 3: Baseline results.
(NegEx and a ‘bag-of-words’ approach)
</tableCaption>
<subsectionHeader confidence="0.996812">
5.2 Rules Based on the Command Relation
</subsectionHeader>
<bodyText confidence="0.978442222222222">
Table 4 shows the results of applying the S-
command relation rule for negation detection.
We experimented with three possible ap-
proaches: an event is considered negated if
- the negation cue commands any event
participant in the parse tree;
- the negation cue commands the event
trigger in the tree;
- the negation cue commands both.
</bodyText>
<table confidence="0.999634">
Approach P R F1 Spec.
negation cue commands 23% 76% 35% 84%
any participant
negation cue 23% 68% 34% 85%
commands trigger
negation cue 23% 68% 35% 86%
commands both
</table>
<tableCaption confidence="0.9781445">
Table 4: Performance when only the S-command
relation is used.
</tableCaption>
<bodyText confidence="0.9999172">
Compared with the baseline methods, the rules
based on the command relation did not improve
the performance. While precision was low
(23%), recall was high (around 70%), indicating
that in the majority of cases there is an S-
command relation in particular with the partici-
pants (the highest recall). We also note a signifi-
cant drop in specificity, as many affirmative
events have triggers/participants S-commanded
by a negation cue (not “linked” to a given event).
</bodyText>
<subsectionHeader confidence="0.98856">
5.3 Machine Learning Results
</subsectionHeader>
<bodyText confidence="0.999901714285714">
All SVM classifiers have been trained on the
training dataset using a Python implementation
of SVM Light using the linear kernel and the
default parameters (Joachims 1999). Table 5
shows the results of the single SVM classifier
that has been trained for all three event classes
together (applied on the development data).
</bodyText>
<page confidence="0.99706">
82
</page>
<bodyText confidence="0.9993422">
Compared to previous methods, there was sig-
nificant improvement in precision, while recall
was relatively low. Still, the overall F-measure
was significantly better compared with the rule-
based methods (51% vs. 35%).
</bodyText>
<table confidence="0.9956504">
Feature set P R F1 Spec.
Features 1-7 43% 8% 14% 99.2%
Features 1-8 73% 19% 30% 99.3%
Features 1-9 71% 38% 49% 99.2%
Features 1-10 76% 38% 51% 99.2%
</table>
<tableCaption confidence="0.540317666666667">
Table 5: The results of the single SVM classifier. Fea-
tures 1-7 are lexical and POS tag-based features. Fea-
ture 8 models whether the cue S-commands any of the
participants. Feature 9 is related to the cue S-
commanding the trigger. Feature 10 is the parse-tree
distance between the cue and trigger.
</tableCaption>
<bodyText confidence="0.9999712">
We first experimented with the effect of differ-
ent types of feature on the quality of the negation
prediction. Table 5 shows the results of the first
classifier with an incremental addition of lexical
features, parse tree-related features, and finally a
combination of those with the command relation
between the negation cue and event trigger and
participants. It is worth noting that both precision
and recall improved as more features are added.
We also separately trained classifiers on the
three classes of events (see Table 6). This further
increased the performance: compared with the
results of the single classifier, the F1 micro-
average improved from 51% to 63%, with simi-
lar gains for both precision and recall.
</bodyText>
<table confidence="0.999103">
Event class P R F1 Spec.
Class I 94% 65% 77% 99.8%
(559 events)
Class II 100% 33% 50% 100%
(249 events)
Class III 81% 44% 57% 99.2%
(987 events)
Micro Average 88% 49% 63% 99.4%
(1,795 events)
Macro Average 92% 47% 62% 99.7%
(3 classes)
</table>
<tableCaption confidence="0.9994285">
Table 6: The results of the separate classifiers on dif-
ferent classes using common features.
</tableCaption>
<sectionHeader confidence="0.997835" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999971051724138">
As expected, approaches that focus only on event
triggers and their surface distances from negation
cues proved inadequate for biomedical scientific
articles. Low recall was mainly caused by many
event triggers being too far from the negation cue
to be detected as within the scope.
Furthermore, compared to clinical notes, for
example, sentences that describe molecular
events are significantly more complex. For ex-
ample, the event-describing sentences in the
training data have on average 2.6 event triggers.
The number of events per sentence is even
higher, as the same trigger can indicate multiple
events, sometimes with opposite polarities. Con-
sider for example the sentence
“We also demonstrate that the IKK complex,
but not p90 (rsk), is responsible for the in vivo
phosphorylation of I-kappa-B-alpha mediated
by the co-activation of PKC and calcineurin.”
Here, the trigger (phosphorylation) is linked with
one affirmative and one negative regulatory
event by two different molecules, hence trigger-
ing two events of opposite polarities.
These findings, together with previous work,
suggested that for any method to effectively de-
tect negations, it should be able to link the nega-
tion cue to the specific token, event trigger or
entity name in question. Therefore, more com-
plex models are needed to capture the specific
structure of the sentence as well as the composi-
tion of the interaction and the arrangement of its
trigger and participants.
By combining several feature types (lexical,
syntactic and semantic), the machine learning
approach proved to provide significantly better
results. In the incremental feature addition explo-
ration process, adding the cue-commands-
participant feature had the greatest effect on the
F-score, suggesting the significance of treating
event participants. We note, however, that many
of the previous attempts focus on event triggers
only, although participants do play an important
role in the detection of negations in biomedical
events and thus should be used as negation tar-
gets instead of or in addition to triggers. It is in-
teresting that adding parse-tree distance between
the trigger and negation cue improves precision
by 5%.
Differences in event classes (in the number
and type of participants) proved to be important.
Significant improvement in performance was
observed when individual classifiers were trained
for the three event classes, suggesting that events
with different numbers or types of participants
are expressed differently in text, at least when
negations are considered. Class I events are the
simplest (one participant only), so it was ex-
pected that negated events in this class would be
</bodyText>
<page confidence="0.997359">
83
</page>
<bodyText confidence="0.999988147058824">
the easiest to detect (F-score of 77%). Class II
negated events (which can have multiple partici-
pants), demonstrated the lowest recall (33%). A
likely reason is that the feature set used is not
suitable for multi-participant events: for exam-
ple, feature 8 focuses on the negation cue com-
manding any of the participants, and not all of
them. It is surprising that negated regulation
events (Class III) were not the most difficult to
identify, given their complexity.
We applied the negation detection on the
type, trigger and participants of pre-identified
events in order to explore the complexity of ne-
gations, unaffected by automatic named entity
recognition, event trigger detection, participant
identification, etc. As these steps are typically
performed before further characterisation of
events, this assumption is not superficial and
such information can be used as input to the ne-
gation detection module. MacKinlay et al. (2009)
also used gold annotations as input for negation
detection, and reported precision, recall, and F-
score of 68%, 24%, and 36% respectively on the
same dataset (compared to 88%, 49% and 63%
in our case). The best performing negation detec-
tion approach in the BioNLP’09 shared task re-
ported recall of up to 15%, but with overall event
detection sensitivity of 33% (Kilicoglu and Ber-
gler 2009) on a ‘test’ dataset (different from that
used in this study). This makes it difficult to di-
rectly compare their results to our work, but we
can still provide some rough estimates: had all
events been correctly identified, their negation
detection approach could have reached 45% re-
call (compared to 49% in our case). With preci-
sion of around 50%, their projected F-score,
again assuming perfect event identification,
could have been in the region of 50% (compared
to 63% in our case).
The experiments with rules that were based
on the command relations have proven to be ge-
neric, providing very high recall (~70%) but with
poor precision. Although only the results with S-
command relations have been reported here (see
Table 4), we examined other types of command
relation, namely NP-, PP-, SBAR-, and VP-
command. The only variation able to improve
prediction accuracy was whether the cue VP-
commands any of the participants, with an F-
score of 42%, which is higher than the results
achieved by the S-command (F-score of 35%).
The S-command relation was used in the SVM
modules as VP-command did not make the re-
sults significantly better.
One of the issues we faced was the manage-
ment of multi-token and sub-token entities and
triggers (e.g. alpha B1 and alpha B2 in “alpha
B1/alpha B2 ratio”, which will be typically to-
kenised as “alpha”, “B1/alpha”, and “B2”). In
our approach, we considered all the entities that
are either multi-token or sub-token. However, if
we assign participants that are both multi-token
and sub-token simultaneously to events and ex-
tract similar features for the classifier from them
as from simple entities, the F-score is reduced by
about 2%. It would be probably better to assign a
new category to those participants and add a new
value for them specifically in every feature.
</bodyText>
<sectionHeader confidence="0.999573" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999964527777778">
Given the number of published articles, detection
of negations is of particular importance for bio-
medical IE. Here we explored the identification
of negated molecular events, given their triggers
(to characterise event type) and participants. We
considered two approaches:5 a rule-based ap-
proach using constituency parse trees and the
command relation to identify negation cues and
scopes, and a machine learning method that
combines a set of lexical, syntactic and semantic
features engineered from the associated sentence.
When compared with a regular-expression-based
baseline method (NegEx-like), the proposed ML
method achieved significantly better results: 63%
F-score with 88% precision. The best results
were obtained when separate classifiers were
trained for each of the three event classes, as dif-
ferences between them (in the number and type
of participants) proved to be important.
The results presented here were obtained by
using the ‘gold’ event annotations as the input. It
would be interesting to explore the impact of
typically noisy automatic event extraction on
negation identification. Furthermore, an immedi-
ate future step would be to explore class-specific
features (e.g. type of theme and cause for Class
III events, and whether the cue S-commands all
participants for Class II events). In addition, in
the current approach we used constituency parse
trees. Our previous attempts to identify molecu-
lar events (Sarafraz et al. 2009) as well as those
discussed in Section 2 use dependency parse
trees. A topic open for future research will be to
combine information from both dependency and
constituency parse trees as features for detecting
negated events.
</bodyText>
<footnote confidence="0.567808">
5 Available at http://bit.ly/bzBaUX
</footnote>
<page confidence="0.998532">
84
</page>
<sectionHeader confidence="0.998062" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999925">
We are grateful to the organisers of BioNLP’09
for providing the annotated data.
</bodyText>
<sectionHeader confidence="0.990314" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999938470588235">
Daniel Bikel. 2004. A Distributional Analysis of a
Lexicalized Statistical Parsing. Proc. Conference
on Empirical Methods in Natural Language.
Wendy Chapman. 2001. A Simple Algorithm for
Identifying Negated Findings and Diseases in Dis-
charge Summaries. Journal of Biomedical Infor-
matics, 34(5):301-310.
Ian Donaldson, Martin, J., de Bruijn, B., Wolting, C.,
Lay, V., Tuekam, B., Zhang, S., Baskin, B., Bader,
G. D., Michalickova, K., Pawson, T. and Hogue, C.
W. 2003. PreBIND and Textomy--mining the bio-
medical literature for protein-protein interactions
using a support vector machine. BMC Bioinf. 4: 11.
Jörg Hakenberg, Illés Solt, Domonkos Tikk, Luis
Tari, Astrid Rheinländer, Quang L. Ngyuen,
Graciela Gonzalez and Ulf Leser. 2009. Molecular
event extraction from link grammar parse trees.
BioNLP’09: Proceedings of the Workshop on
BioNLP. 86-94.
Yang Huang and Henry J. Lowe. 2007. A Novel Hy-
brid Approach to Automated Negation Detection in
Clinical Radiology Reports. Journal of the Ameri-
can Medical Informatics Association, 14(3):304-
311.
Thorsten Joachims. 1999. Making large-Scale SVM
Learning Practical. Advances in Kernel Methods -
Support Vector Learning, B. Schölkopf and C.
Burges and A. Smola (ed.) MIT-Press, MA.
Halil Kilicoglu, and Sabine Bergler. 2009. Syntactic
dependency based heuristics for biological event
extraction. BioNLP’09: Proceedings of the Work-
shop on BioNLP. 119-127.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yo-
shinobu Kano, Jun’ichi Tsujii. 2009. Overview of
BioNLP’09 shared task on event extraction.
BioNLP’09: Proceedings of the Workshop on
BioNLP. 1-9.
Sofie Van Landeghem, Yvan Saeys, Bernard De
Baets and Yves Van de Peer. 2008. Extracting Pro-
tein-Protein Interactions from Text using Rich Fea-
ture Vectors and Feature Selection. Proceedings of
the Third International Symposium on Semantic
Mining in Biomedicine. 77-84.
Ronald Langacker. 1969. On Pronominalization and
the Chain of Command. In D. Reibel and S. Schane
(eds.), Modern Studies in English, Prentice-Hall,
Englewood Cliffs, NJ. 160–186.
John Lawler. 2010. Negation and Negative Polarity.
The Cambridge Encyclopedia of the Language Sci-
ences. Patrick Colm Hogan (ed.) Cambridge Uni-
versity Press. Cambridge, UK.
Andrew MacKinlay, David Martinez and Timothy
Baldwin. 2009. Biomedical Event Annotation with
CRFs and Precision Grammars. BioNLP’09: Pro-
ceedings of the Workshop on BioNLP. 77-85.
James McCawley. 1993. Everything that Linguists
have Always Wanted to Know about Logic But
Were Ashamed to Ask. 2nd edition. The University
of Chicago Press. Chicago, IL.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective Self-Training for Parsing.
Proceedings of HLT/NAACL 2006. 152-159.
Roser Morante and Walter Daelemans. 2009. A
Metalearning Approach to Processing the Scope of
Negation. CoNLL ‘09: Proceedings of the 13th
Conference on Computational Natural Language
Learning. 21-29.
Pradeep Mutalik, Aniruddha Deshpande, and Prakash
M. Nadkarni. 2001. Use of General-purpose Nega-
tion Detection to Augment Concept Indexing of
Medical Documents: A Quantitative Study using
the UMLS. Journal of the American Medical In-
formatics Association : JAMIA. 8(6):598-609.
Jeyakumar Natarajan, Berrar, D., Dubitzky, W., Hack,
C., Zhang, Y., DeSesa, C., Van Brocklyn, J. R. and
Bremer, E.G. 2006. Text mining of full-text journal
articles combined with gene expression analysis
reveals a relationship between sphingosine-1-
phosphate and invasiveness of a glioblastoma cell
line. BMC Bioinformatics. 7: 373.
Jon Patrick, Yefeng Wang, and Peter Budd. 2006.
Automatic Mapping Clinical Notes to Medical
Terminologies. Proc. Of the 2006 Australian Lan-
guage Technology Workshop. 75-82.
Olivia Sanchez. 2007. Text mining applied to biologi-
cal texts: beyond the extraction of protein-protein
interactions. PhD Thesis.
Farzaneh Sarafraz, James Eales, Reza Mohammadi,
Jonathan Dickerson, David Robertson and Goran
Nenadic. 2009. Biomedical Event Detection using
Rules, Conditional Random Fields and Parse Tree
Distances. BioNLP’09: Proceedings of the Work-
shop on BioNLP.
Herman Tolentino, Michael Matters, Wikke Walop,
Barbara Law, Wesley Tong, Fang Liu, Paul Fon-
telo, Katrin Kohl, and Daniel Payne. 2006. Concept
Negation in Free Text Components of Vaccine
Safety Reports. AMIA Annual Symposium proc.
Yoshimasa Tsuruoka, and Jun’ichi Tsujii. 2005. Bidi-
rectional Inference with the Easiest-First Strategy
for Tagging Sequence Data. Proceedings of
HLT/EMNLP 2005, 467-474.
</reference>
<page confidence="0.9997">
85
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.445022">
<title confidence="0.9326725">Using SVMs with the Command Relation Features Identify Negated Events in Biomedical Literature</title>
<author confidence="0.830868">Farzaneh</author>
<affiliation confidence="0.9997635">School of Computer Science University of</affiliation>
<address confidence="0.793146">Manchester, United</address>
<email confidence="0.991894">sarafraf@cs.man.ac.uk</email>
<author confidence="0.971584">Goran</author>
<affiliation confidence="0.999788">School of Computer University of</affiliation>
<address confidence="0.803781">Manchester, United</address>
<email confidence="0.996144">g.nenadic@manchester.ac.uk</email>
<abstract confidence="0.998270083333333">In this paper we explore the identification of negated molecular events (e.g. protein binding, gene expressions, regulation, etc.) in biomedical research abstracts. We construe the problem as a classification task and apply a machine learning (ML) approach that uses lexical, syntactic, and semantic features associated with sentences that represent events. Lexical features include negation cues, whereas syntactic features are engineered from constituparse trees and the between constituents. Semantic features include event type and participants. We also consider a rule-based approach that uses only the command relation. On a test dataset, the ML approach showed significantly better results (51% F-measure) compared to the commandbased rules (35-42% F-measure). Training a separate classifier for each event class proved to be useful, as the micro-averaged F-score improved to 63% (with 88% precision), demonstrating the potential of task-specific ML approaches to negation detection.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Daniel Bikel</author>
</authors>
<title>A Distributional Analysis of a Lexicalized Statistical Parsing.</title>
<date>2004</date>
<booktitle>Proc. Conference on Empirical Methods in Natural Language.</booktitle>
<contexts>
<context position="18555" citStr="Bikel 2004" startWordPosition="2980" endWordPosition="2981">as additional cues were semi-automatically extracted by exploring the training data. The small negation cue set contains 14 words3, whereas the larger negation cue set contains 32 words4. As expected, the larger set resulted in increased recall, but decreased precision. However, the effects on the Fscore were typically not significant. The results are only shown using the larger cue set. The texts were processed using the GENIA tagger (Tsuruoka and Tsujii 2005).We used constituency parse trees automatically produced by two different constituency parsers reported in (McClosky et al. 2006) and (Bikel 2004). No major differences were observed in the results using the two parsers. The data shown in the results are produced by the former. 5.1 Baseline Results Our baseline method relies on an implementation of the NegEx algorithm as explained in Section 2.1. Event triggers were used as negation targets for the algorithm. An event is then considered to be negated if the trigger is negated; otherwise it 3 Negation cues in this set include: no, not, none, negative, without, absence, fail, fails, failed, failure, cannot, lack, lacking, lacked. 4 Negation cues in this set include the smaller set and 18 </context>
</contexts>
<marker>Bikel, 2004</marker>
<rawString>Daniel Bikel. 2004. A Distributional Analysis of a Lexicalized Statistical Parsing. Proc. Conference on Empirical Methods in Natural Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wendy Chapman</author>
</authors>
<title>A Simple Algorithm for Identifying Negated Findings and Diseases in Discharge Summaries.</title>
<date>2001</date>
<journal>Journal of Biomedical Informatics,</journal>
<pages>34--5</pages>
<marker>Chapman, 2001</marker>
<rawString>Wendy Chapman. 2001. A Simple Algorithm for Identifying Negated Findings and Diseases in Discharge Summaries. Journal of Biomedical Informatics, 34(5):301-310.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian Donaldson</author>
<author>J Martin</author>
<author>B de Bruijn</author>
<author>C Wolting</author>
<author>V Lay</author>
<author>B Tuekam</author>
<author>S Zhang</author>
<author>B Baskin</author>
<author>G D Bader</author>
<author>K Michalickova</author>
<author>T Pawson</author>
<author>C W Hogue</author>
</authors>
<title>PreBIND and Textomy--mining the biomedical literature for protein-protein interactions using a support vector machine.</title>
<date>2003</date>
<journal>BMC Bioinf.</journal>
<volume>4</volume>
<pages>11</pages>
<marker>Donaldson, Martin, de Bruijn, Wolting, Lay, Tuekam, Zhang, Baskin, Bader, Michalickova, Pawson, Hogue, 2003</marker>
<rawString>Ian Donaldson, Martin, J., de Bruijn, B., Wolting, C., Lay, V., Tuekam, B., Zhang, S., Baskin, B., Bader, G. D., Michalickova, K., Pawson, T. and Hogue, C. W. 2003. PreBIND and Textomy--mining the biomedical literature for protein-protein interactions using a support vector machine. BMC Bioinf. 4: 11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jörg Hakenberg</author>
<author>Illés Solt</author>
<author>Domonkos Tikk</author>
<author>Luis Tari</author>
<author>Astrid Rheinländer</author>
<author>Quang L Ngyuen</author>
</authors>
<title>Graciela Gonzalez and Ulf Leser.</title>
<date>2009</date>
<booktitle>BioNLP’09: Proceedings of the Workshop on BioNLP.</booktitle>
<pages>86--94</pages>
<contexts>
<context position="2782" citStr="Hakenberg et al. 2009" startWordPosition="415" endWordPosition="418">importance for IE methods, as it often can hugely affect the quality of the extracted information. For example, when mining molecular events, a key piece of information is whether the text states that the two proteins are or are not interacting, or that a given gene is or is not expressed. In recent years, several challenges and shared tasks have included the extraction of negations, typically as part of other tasks (e.g. the BioNLP’09 Shared Task 3 (Kim et al. 2009)). Several systems and methods have aimed to handle negation detection in order to improve the quality of extracted information (Hakenberg et al. 2009; Morante and Daelemans 2009). Prior research on this topic has primarily focused on finding negated concepts by negation cues and scopes. These concepts are usually represented by a set of predefined terms, and negation detection typically aims to determine whether a term falls within the scope of a negation cue. In this paper we address the task of identification of negated events. We present a machine learning (ML) method that combines a set of features mainly engineered from a sentence parse tree with lexical cues. More specifically, parsebased features use the notion of the command relati</context>
<context position="9008" citStr="Hakenberg et al. (2009)" startWordPosition="1398" endWordPosition="1401"> respectively. 2.2 Detecting Negated Events Several approaches have recently been suggested for the extraction of negated events, particularly 1 http://www.nlm.nih.gov/research/umls/ 2 http://www.snomed.org 79 in the biomedical domain. Events are typically represented via participants (biomedical entities that take part in an event) and event triggers (tokens that indicate presence of the event). Van Landeghem et al. (2008) used a rule-based approach based on token distances in sentence and lexical information in event triggers to detect negated molecular events. Kilicoglu and Bergler (2009), Hakenberg et al. (2009), and Sanchez (2007) used a number of heuristic rules concerning the type of the negation cue and the type of the dependency relation to detect negated molecular events described in text. For example, a rule can state that if the negation cue is “lack” or “absence”, then the trigger has to be in the prepositional phrase of the cue; or that if the cue is “unable” or “fail”, then the trigger has to be in the clausal complement of the cue (Kilicoglu and Bergler 2009). As expected, such approaches suffer from lower recall. MacKinlay et al. (2009), on the other hand, use ML, assigning a vector of c</context>
</contexts>
<marker>Hakenberg, Solt, Tikk, Tari, Rheinländer, Ngyuen, 2009</marker>
<rawString>Jörg Hakenberg, Illés Solt, Domonkos Tikk, Luis Tari, Astrid Rheinländer, Quang L. Ngyuen, Graciela Gonzalez and Ulf Leser. 2009. Molecular event extraction from link grammar parse trees. BioNLP’09: Proceedings of the Workshop on BioNLP. 86-94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Huang</author>
<author>Henry J Lowe</author>
</authors>
<title>A Novel Hybrid Approach to Automated Negation Detection in Clinical Radiology Reports.</title>
<date>2007</date>
<journal>Journal of the American Medical Informatics Association,</journal>
<pages>14--3</pages>
<contexts>
<context position="7487" citStr="Huang and Lowe (2007)" startWordPosition="1172" endWordPosition="1175">tion phrases, Patrick et al. (2006) further consider so-called pre-coordinated negative terms (e.g. headache) that have been collected from SNOMED CT2 medical terminology. Similarly, NegFinder uses hand-crafted rules to detect negated UMLS terms, including simple conjunctive and disjunctive statements (Mutalik et al. 2001). They used a list of 60 negation cues. Tolentino et al. (2006), however, show that using rules on a small set of only five negation cues (no, neither/nor, ruled out, denies, without) can still be reasonably successful in detecting negations in medical reports (F-score 91%). Huang and Lowe (2007) introduced a negation grammar that used regular expressions and dependency parse trees to identify negation cues and their scope in the sentence. They applied the rules to a set of radiology reports and reported a precision of 99% and a recall of 92%. Not many efforts have been reported on using machine learning to detect patterns in sentences that contain negative expressions. Still, Morante and Daelemans (2009), for example, used various classifiers (Memory-based Learners, Support Vector Machines, and Conditional Random Fields) to detect negation cues and their scope. An extensive list of f</context>
</contexts>
<marker>Huang, Lowe, 2007</marker>
<rawString>Yang Huang and Henry J. Lowe. 2007. A Novel Hybrid Approach to Automated Negation Detection in Clinical Radiology Reports. Journal of the American Medical Informatics Association, 14(3):304-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<date>1999</date>
<booktitle>Making large-Scale SVM Learning Practical. Advances in Kernel Methods -Support Vector Learning,</booktitle>
<editor>B. Schölkopf and C. Burges and A. Smola (ed.) MIT-Press, MA.</editor>
<contexts>
<context position="21085" citStr="Joachims 1999" startWordPosition="3394" endWordPosition="3395">d on the command relation did not improve the performance. While precision was low (23%), recall was high (around 70%), indicating that in the majority of cases there is an Scommand relation in particular with the participants (the highest recall). We also note a significant drop in specificity, as many affirmative events have triggers/participants S-commanded by a negation cue (not “linked” to a given event). 5.3 Machine Learning Results All SVM classifiers have been trained on the training dataset using a Python implementation of SVM Light using the linear kernel and the default parameters (Joachims 1999). Table 5 shows the results of the single SVM classifier that has been trained for all three event classes together (applied on the development data). 82 Compared to previous methods, there was significant improvement in precision, while recall was relatively low. Still, the overall F-measure was significantly better compared with the rulebased methods (51% vs. 35%). Feature set P R F1 Spec. Features 1-7 43% 8% 14% 99.2% Features 1-8 73% 19% 30% 99.3% Features 1-9 71% 38% 49% 99.2% Features 1-10 76% 38% 51% 99.2% Table 5: The results of the single SVM classifier. Features 1-7 are lexical and P</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Making large-Scale SVM Learning Practical. Advances in Kernel Methods -Support Vector Learning, B. Schölkopf and C. Burges and A. Smola (ed.) MIT-Press, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Halil Kilicoglu</author>
<author>Sabine Bergler</author>
</authors>
<title>Syntactic dependency based heuristics for biological event extraction.</title>
<date>2009</date>
<booktitle>BioNLP’09: Proceedings of the Workshop on BioNLP.</booktitle>
<pages>119--127</pages>
<contexts>
<context position="8983" citStr="Kilicoglu and Bergler (2009)" startWordPosition="1394" endWordPosition="1397"> F-scores of 80%, 77%, and 68% respectively. 2.2 Detecting Negated Events Several approaches have recently been suggested for the extraction of negated events, particularly 1 http://www.nlm.nih.gov/research/umls/ 2 http://www.snomed.org 79 in the biomedical domain. Events are typically represented via participants (biomedical entities that take part in an event) and event triggers (tokens that indicate presence of the event). Van Landeghem et al. (2008) used a rule-based approach based on token distances in sentence and lexical information in event triggers to detect negated molecular events. Kilicoglu and Bergler (2009), Hakenberg et al. (2009), and Sanchez (2007) used a number of heuristic rules concerning the type of the negation cue and the type of the dependency relation to detect negated molecular events described in text. For example, a rule can state that if the negation cue is “lack” or “absence”, then the trigger has to be in the prepositional phrase of the cue; or that if the cue is “unable” or “fail”, then the trigger has to be in the clausal complement of the cue (Kilicoglu and Bergler 2009). As expected, such approaches suffer from lower recall. MacKinlay et al. (2009), on the other hand, use ML</context>
<context position="26951" citStr="Kilicoglu and Bergler 2009" startWordPosition="4335" endWordPosition="4339"> identification, etc. As these steps are typically performed before further characterisation of events, this assumption is not superficial and such information can be used as input to the negation detection module. MacKinlay et al. (2009) also used gold annotations as input for negation detection, and reported precision, recall, and Fscore of 68%, 24%, and 36% respectively on the same dataset (compared to 88%, 49% and 63% in our case). The best performing negation detection approach in the BioNLP’09 shared task reported recall of up to 15%, but with overall event detection sensitivity of 33% (Kilicoglu and Bergler 2009) on a ‘test’ dataset (different from that used in this study). This makes it difficult to directly compare their results to our work, but we can still provide some rough estimates: had all events been correctly identified, their negation detection approach could have reached 45% recall (compared to 49% in our case). With precision of around 50%, their projected F-score, again assuming perfect event identification, could have been in the region of 50% (compared to 63% in our case). The experiments with rules that were based on the command relations have proven to be generic, providing very high</context>
</contexts>
<marker>Kilicoglu, Bergler, 2009</marker>
<rawString>Halil Kilicoglu, and Sabine Bergler. 2009. Syntactic dependency based heuristics for biological event extraction. BioNLP’09: Proceedings of the Workshop on BioNLP. 119-127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin-Dong Kim</author>
</authors>
<title>Tomoko Ohta, Sampo Pyysalo, Yoshinobu Kano, Jun’ichi Tsujii.</title>
<date>2009</date>
<booktitle>Overview of BioNLP’09 shared task on event extraction. BioNLP’09: Proceedings of the Workshop on BioNLP.</booktitle>
<pages>1--9</pages>
<marker>Kim, 2009</marker>
<rawString>Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshinobu Kano, Jun’ichi Tsujii. 2009. Overview of BioNLP’09 shared task on event extraction. BioNLP’09: Proceedings of the Workshop on BioNLP. 1-9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sofie Van Landeghem</author>
<author>Yvan Saeys</author>
<author>Bernard De Baets</author>
<author>Yves Van de Peer</author>
</authors>
<title>Extracting Protein-Protein Interactions from Text using Rich Feature Vectors and Feature Selection.</title>
<date>2008</date>
<booktitle>Proceedings of the Third International Symposium on Semantic Mining in Biomedicine.</booktitle>
<pages>77--84</pages>
<marker>Van Landeghem, Saeys, De Baets, Van de Peer, 2008</marker>
<rawString>Sofie Van Landeghem, Yvan Saeys, Bernard De Baets and Yves Van de Peer. 2008. Extracting Protein-Protein Interactions from Text using Rich Feature Vectors and Feature Selection. Proceedings of the Third International Symposium on Semantic Mining in Biomedicine. 77-84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Langacker</author>
</authors>
<title>On Pronominalization and the Chain of Command. In</title>
<date>1969</date>
<booktitle>Modern Studies in English, Prentice-Hall, Englewood Cliffs, NJ.</booktitle>
<pages>160--186</pages>
<editor>D. Reibel and S. Schane (eds.),</editor>
<contexts>
<context position="3447" citStr="Langacker, 1969" startWordPosition="527" endWordPosition="528">this topic has primarily focused on finding negated concepts by negation cues and scopes. These concepts are usually represented by a set of predefined terms, and negation detection typically aims to determine whether a term falls within the scope of a negation cue. In this paper we address the task of identification of negated events. We present a machine learning (ML) method that combines a set of features mainly engineered from a sentence parse tree with lexical cues. More specifically, parsebased features use the notion of the command relation that models the scope affected by an element (Langacker, 1969). We use molecular events as a case study and experiment on the BioNLP’09 data, which comprises a goldstandard corpus of research abstracts manually annotated for events and negations (Kim et al. 2009). The evaluation shows that, by using the proposed approach, negated events can be identified with precision of 88% and recall of 49% (63% F-measure). We compare these results with two rule-based approaches that achieved the maximum F-measure of 42%. 78 Proceedings of the Workshop on Negation and Speculation in Natural Language Processing, pages 78–85, Uppsala, July 2010. The rest of this paper i</context>
<context position="14372" citStr="Langacker (1969)" startWordPosition="2263" endWordPosition="2264">Methodology We consider two approaches to extract negated events. We first discuss a rule-based approach that uses constituency parse trees and the command relation to identify negated events. Then, we introduce a ML method that combines lexical, syntactic and semantic features to identify negated events. Note that in all cases, input sentences have been pre-annotated for entity mentions, event triggers, types, and participants. 4.1 Negation Detection Using the Command Relation Rules The question of which parts of a syntactic structure affect the other parts has been extensively investigated. Langacker (1969) introduced the concept of command to determine the scope within a sentence affected by an element. More precisely, if a and b are nodes in the constituency parse tree of a sentence, then a X-commands b iff the lowest ancestor of a with label X is also an ancestor of b. Note that the command relation is not symmetrical. Langacker observed that when a S-commands b, then a affects the scope containing b. For simplicity, we say “command” when we mean S-command. To determine whether token a commands token b, given the parse tree of a sentence, we use a simple algorithm introduced by McCawley (1993</context>
</contexts>
<marker>Langacker, 1969</marker>
<rawString>Ronald Langacker. 1969. On Pronominalization and the Chain of Command. In D. Reibel and S. Schane (eds.), Modern Studies in English, Prentice-Hall, Englewood Cliffs, NJ. 160–186.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lawler</author>
</authors>
<title>Negation and Negative Polarity. The Cambridge Encyclopedia of the Language Sciences. Patrick Colm Hogan (ed.)</title>
<date>2010</date>
<publisher>Cambridge University Press. Cambridge, UK.</publisher>
<contexts>
<context position="4454" citStr="Lawler, 2010" startWordPosition="686" endWordPosition="687">ed approaches that achieved the maximum F-measure of 42%. 78 Proceedings of the Workshop on Negation and Speculation in Natural Language Processing, pages 78–85, Uppsala, July 2010. The rest of this paper is organised as follows. Section 2 summarises and reviews previous research on negation extraction. Section 3 defines the problem and introduces the data used for the case study. Section 4 focuses on the ML-based methodology for extracting negated events. The final sections contain the results and discussions. 2 Related Work There have been numerous contemplations of the concept of negation (Lawler, 2010), but no general agreement so far exists on its definition, form, and function. We adopt here a definition of negation as given by Cambridge Encyclopedia of Language Sciences: “Negation is a comparison between a ‘real’ situation lacking some element and an ‘imaginal’ situation that does not lack it”. The imaginal situation is affirmative compared with the negative real situation. The element whose polarity differs between the two situations is the negation target. Negations in natural language can be expressed by syntactically negative expressions, i.e. with the use of negating words such as n</context>
</contexts>
<marker>Lawler, 2010</marker>
<rawString>John Lawler. 2010. Negation and Negative Polarity. The Cambridge Encyclopedia of the Language Sciences. Patrick Colm Hogan (ed.) Cambridge University Press. Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew MacKinlay</author>
<author>David Martinez</author>
<author>Timothy Baldwin</author>
</authors>
<title>Biomedical Event Annotation with CRFs and Precision Grammars.</title>
<date>2009</date>
<booktitle>BioNLP’09: Proceedings of the Workshop on BioNLP.</booktitle>
<pages>77--85</pages>
<contexts>
<context position="9556" citStr="MacKinlay et al. (2009)" startWordPosition="1497" endWordPosition="1500">ted molecular events. Kilicoglu and Bergler (2009), Hakenberg et al. (2009), and Sanchez (2007) used a number of heuristic rules concerning the type of the negation cue and the type of the dependency relation to detect negated molecular events described in text. For example, a rule can state that if the negation cue is “lack” or “absence”, then the trigger has to be in the prepositional phrase of the cue; or that if the cue is “unable” or “fail”, then the trigger has to be in the clausal complement of the cue (Kilicoglu and Bergler 2009). As expected, such approaches suffer from lower recall. MacKinlay et al. (2009), on the other hand, use ML, assigning a vector of complex deep parse features (including syntactic predicates to capture negation scopes, conjunctions and semantically negated verbs) to every event trigger. The system achieved an F-score of 36% on the same dataset as used in this paper. We note that the methods mentioned above mainly focus on finding negated triggers in order to detect negated events. In this paper we explore not only negation of triggers but also phrases in which participants are negated (consider, for example, “SLP-76” in the sentence “In contrast, Grb2 can be coimmunopreci</context>
<context position="26562" citStr="MacKinlay et al. (2009)" startWordPosition="4269" endWordPosition="4272">pants, and not all of them. It is surprising that negated regulation events (Class III) were not the most difficult to identify, given their complexity. We applied the negation detection on the type, trigger and participants of pre-identified events in order to explore the complexity of negations, unaffected by automatic named entity recognition, event trigger detection, participant identification, etc. As these steps are typically performed before further characterisation of events, this assumption is not superficial and such information can be used as input to the negation detection module. MacKinlay et al. (2009) also used gold annotations as input for negation detection, and reported precision, recall, and Fscore of 68%, 24%, and 36% respectively on the same dataset (compared to 88%, 49% and 63% in our case). The best performing negation detection approach in the BioNLP’09 shared task reported recall of up to 15%, but with overall event detection sensitivity of 33% (Kilicoglu and Bergler 2009) on a ‘test’ dataset (different from that used in this study). This makes it difficult to directly compare their results to our work, but we can still provide some rough estimates: had all events been correctly </context>
</contexts>
<marker>MacKinlay, Martinez, Baldwin, 2009</marker>
<rawString>Andrew MacKinlay, David Martinez and Timothy Baldwin. 2009. Biomedical Event Annotation with CRFs and Precision Grammars. BioNLP’09: Proceedings of the Workshop on BioNLP. 77-85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James McCawley</author>
</authors>
<title>Everything that Linguists have Always Wanted to Know about Logic But Were Ashamed to Ask. 2nd edition. The University of Chicago Press.</title>
<date>1993</date>
<location>Chicago, IL.</location>
<contexts>
<context position="14973" citStr="McCawley (1993)" startWordPosition="2370" endWordPosition="2371">ngacker (1969) introduced the concept of command to determine the scope within a sentence affected by an element. More precisely, if a and b are nodes in the constituency parse tree of a sentence, then a X-commands b iff the lowest ancestor of a with label X is also an ancestor of b. Note that the command relation is not symmetrical. Langacker observed that when a S-commands b, then a affects the scope containing b. For simplicity, we say “command” when we mean S-command. To determine whether token a commands token b, given the parse tree of a sentence, we use a simple algorithm introduced by McCawley (1993): trace up the branches of the constituency parse tree from a until you hit a node that is labelled X. If b is reachable by tracing down the branches of the tree from that node, then a Xcommands b; otherwise, it does not. We hypothesise that if a negation cue commands an event trigger or participant, then the associated event is negated. 4.2 Negation Detection Using Machine Learning on Parse Tree Features Given a sentence that describes an event, we further construe the negation detection problem as a classification task: the aim is to classify the event as affirmative or negative. We explore </context>
</contexts>
<marker>McCawley, 1993</marker>
<rawString>James McCawley. 1993. Everything that Linguists have Always Wanted to Know about Logic But Were Ashamed to Ask. 2nd edition. The University of Chicago Press. Chicago, IL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Effective Self-Training for Parsing.</title>
<date>2006</date>
<booktitle>Proceedings of HLT/NAACL</booktitle>
<pages>152--159</pages>
<contexts>
<context position="18538" citStr="McClosky et al. 2006" startWordPosition="2975" endWordPosition="2978">ed from related work, whereas additional cues were semi-automatically extracted by exploring the training data. The small negation cue set contains 14 words3, whereas the larger negation cue set contains 32 words4. As expected, the larger set resulted in increased recall, but decreased precision. However, the effects on the Fscore were typically not significant. The results are only shown using the larger cue set. The texts were processed using the GENIA tagger (Tsuruoka and Tsujii 2005).We used constituency parse trees automatically produced by two different constituency parsers reported in (McClosky et al. 2006) and (Bikel 2004). No major differences were observed in the results using the two parsers. The data shown in the results are produced by the former. 5.1 Baseline Results Our baseline method relies on an implementation of the NegEx algorithm as explained in Section 2.1. Event triggers were used as negation targets for the algorithm. An event is then considered to be negated if the trigger is negated; otherwise it 3 Negation cues in this set include: no, not, none, negative, without, absence, fail, fails, failed, failure, cannot, lack, lacking, lacked. 4 Negation cues in this set include the sm</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006. Effective Self-Training for Parsing. Proceedings of HLT/NAACL 2006. 152-159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roser Morante</author>
<author>Walter Daelemans</author>
</authors>
<title>A Metalearning Approach to Processing the Scope of Negation.</title>
<date>2009</date>
<booktitle>CoNLL ‘09: Proceedings of the 13th Conference on Computational Natural Language Learning.</booktitle>
<pages>21--29</pages>
<contexts>
<context position="2811" citStr="Morante and Daelemans 2009" startWordPosition="419" endWordPosition="422">ds, as it often can hugely affect the quality of the extracted information. For example, when mining molecular events, a key piece of information is whether the text states that the two proteins are or are not interacting, or that a given gene is or is not expressed. In recent years, several challenges and shared tasks have included the extraction of negations, typically as part of other tasks (e.g. the BioNLP’09 Shared Task 3 (Kim et al. 2009)). Several systems and methods have aimed to handle negation detection in order to improve the quality of extracted information (Hakenberg et al. 2009; Morante and Daelemans 2009). Prior research on this topic has primarily focused on finding negated concepts by negation cues and scopes. These concepts are usually represented by a set of predefined terms, and negation detection typically aims to determine whether a term falls within the scope of a negation cue. In this paper we address the task of identification of negated events. We present a machine learning (ML) method that combines a set of features mainly engineered from a sentence parse tree with lexical cues. More specifically, parsebased features use the notion of the command relation that models the scope affe</context>
<context position="7904" citStr="Morante and Daelemans (2009)" startWordPosition="1240" endWordPosition="1243">ing rules on a small set of only five negation cues (no, neither/nor, ruled out, denies, without) can still be reasonably successful in detecting negations in medical reports (F-score 91%). Huang and Lowe (2007) introduced a negation grammar that used regular expressions and dependency parse trees to identify negation cues and their scope in the sentence. They applied the rules to a set of radiology reports and reported a precision of 99% and a recall of 92%. Not many efforts have been reported on using machine learning to detect patterns in sentences that contain negative expressions. Still, Morante and Daelemans (2009), for example, used various classifiers (Memory-based Learners, Support Vector Machines, and Conditional Random Fields) to detect negation cues and their scope. An extensive list of features included the token’s stem and part-of-speech, as well as those of the neighbouring tokens. Separate classifiers were used for detecting negation cues and negation scopes. The method was applied to clinical text, biomedical abstracts, and biomedical papers with F-scores of 80%, 77%, and 68% respectively. 2.2 Detecting Negated Events Several approaches have recently been suggested for the extraction of negat</context>
</contexts>
<marker>Morante, Daelemans, 2009</marker>
<rawString>Roser Morante and Walter Daelemans. 2009. A Metalearning Approach to Processing the Scope of Negation. CoNLL ‘09: Proceedings of the 13th Conference on Computational Natural Language Learning. 21-29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pradeep Mutalik</author>
<author>Aniruddha Deshpande</author>
<author>Prakash M Nadkarni</author>
</authors>
<title>Use of General-purpose Negation Detection to Augment Concept Indexing of Medical Documents: A Quantitative Study using the UMLS.</title>
<date>2001</date>
<journal>Journal of the American Medical Informatics Association : JAMIA.</journal>
<pages>8--6</pages>
<contexts>
<context position="7190" citStr="Mutalik et al. 2001" startWordPosition="1122" endWordPosition="1125">omprises 272 clinically-specific negation cues, including those such as denial of or absence of. Although simple, the proposed approach showed good results on clinical data (78% sensitivity (recall), 84% precision, and 94% specificity). In addition to concepts that are explicitly negated by negation phrases, Patrick et al. (2006) further consider so-called pre-coordinated negative terms (e.g. headache) that have been collected from SNOMED CT2 medical terminology. Similarly, NegFinder uses hand-crafted rules to detect negated UMLS terms, including simple conjunctive and disjunctive statements (Mutalik et al. 2001). They used a list of 60 negation cues. Tolentino et al. (2006), however, show that using rules on a small set of only five negation cues (no, neither/nor, ruled out, denies, without) can still be reasonably successful in detecting negations in medical reports (F-score 91%). Huang and Lowe (2007) introduced a negation grammar that used regular expressions and dependency parse trees to identify negation cues and their scope in the sentence. They applied the rules to a set of radiology reports and reported a precision of 99% and a recall of 92%. Not many efforts have been reported on using machi</context>
</contexts>
<marker>Mutalik, Deshpande, Nadkarni, 2001</marker>
<rawString>Pradeep Mutalik, Aniruddha Deshpande, and Prakash M. Nadkarni. 2001. Use of General-purpose Negation Detection to Augment Concept Indexing of Medical Documents: A Quantitative Study using the UMLS. Journal of the American Medical Informatics Association : JAMIA. 8(6):598-609.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeyakumar Natarajan</author>
<author>D Berrar</author>
<author>W Dubitzky</author>
<author>C Hack</author>
<author>Y Zhang</author>
<author>C DeSesa</author>
<author>J R Van Brocklyn</author>
<author>E G Bremer</author>
</authors>
<title>Text mining of full-text journal articles combined with gene expression analysis reveals a relationship between sphingosine-1-phosphate and invasiveness of a glioblastoma cell line.</title>
<date>2006</date>
<journal>BMC Bioinformatics.</journal>
<volume>7</volume>
<pages>373</pages>
<marker>Natarajan, Berrar, Dubitzky, Hack, Zhang, DeSesa, Van Brocklyn, Bremer, 2006</marker>
<rawString>Jeyakumar Natarajan, Berrar, D., Dubitzky, W., Hack, C., Zhang, Y., DeSesa, C., Van Brocklyn, J. R. and Bremer, E.G. 2006. Text mining of full-text journal articles combined with gene expression analysis reveals a relationship between sphingosine-1-phosphate and invasiveness of a glioblastoma cell line. BMC Bioinformatics. 7: 373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jon Patrick</author>
<author>Yefeng Wang</author>
<author>Peter Budd</author>
</authors>
<title>Automatic Mapping Clinical Notes to Medical Terminologies.</title>
<date>2006</date>
<booktitle>Proc. Of the 2006 Australian Language Technology Workshop.</booktitle>
<pages>75--82</pages>
<contexts>
<context position="6901" citStr="Patrick et al. (2006)" startWordPosition="1083" endWordPosition="1086">at are triggered by negation phrases such as: &lt;negation cue&gt; * &lt;target term&gt; &lt;target term&gt; * &lt;negation cue&gt; where the asterisk (*) represents a string of up to five tokens. Target terms represent domain concepts that are terms from the Unified Medical Language System (UMLS1). The cue set comprises 272 clinically-specific negation cues, including those such as denial of or absence of. Although simple, the proposed approach showed good results on clinical data (78% sensitivity (recall), 84% precision, and 94% specificity). In addition to concepts that are explicitly negated by negation phrases, Patrick et al. (2006) further consider so-called pre-coordinated negative terms (e.g. headache) that have been collected from SNOMED CT2 medical terminology. Similarly, NegFinder uses hand-crafted rules to detect negated UMLS terms, including simple conjunctive and disjunctive statements (Mutalik et al. 2001). They used a list of 60 negation cues. Tolentino et al. (2006), however, show that using rules on a small set of only five negation cues (no, neither/nor, ruled out, denies, without) can still be reasonably successful in detecting negations in medical reports (F-score 91%). Huang and Lowe (2007) introduced a </context>
</contexts>
<marker>Patrick, Wang, Budd, 2006</marker>
<rawString>Jon Patrick, Yefeng Wang, and Peter Budd. 2006. Automatic Mapping Clinical Notes to Medical Terminologies. Proc. Of the 2006 Australian Language Technology Workshop. 75-82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivia Sanchez</author>
</authors>
<title>Text mining applied to biological texts: beyond the extraction of protein-protein interactions.</title>
<date>2007</date>
<tech>PhD Thesis.</tech>
<contexts>
<context position="9028" citStr="Sanchez (2007)" startWordPosition="1403" endWordPosition="1404">Negated Events Several approaches have recently been suggested for the extraction of negated events, particularly 1 http://www.nlm.nih.gov/research/umls/ 2 http://www.snomed.org 79 in the biomedical domain. Events are typically represented via participants (biomedical entities that take part in an event) and event triggers (tokens that indicate presence of the event). Van Landeghem et al. (2008) used a rule-based approach based on token distances in sentence and lexical information in event triggers to detect negated molecular events. Kilicoglu and Bergler (2009), Hakenberg et al. (2009), and Sanchez (2007) used a number of heuristic rules concerning the type of the negation cue and the type of the dependency relation to detect negated molecular events described in text. For example, a rule can state that if the negation cue is “lack” or “absence”, then the trigger has to be in the prepositional phrase of the cue; or that if the cue is “unable” or “fail”, then the trigger has to be in the clausal complement of the cue (Kilicoglu and Bergler 2009). As expected, such approaches suffer from lower recall. MacKinlay et al. (2009), on the other hand, use ML, assigning a vector of complex deep parse fe</context>
</contexts>
<marker>Sanchez, 2007</marker>
<rawString>Olivia Sanchez. 2007. Text mining applied to biological texts: beyond the extraction of protein-protein interactions. PhD Thesis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Farzaneh Sarafraz</author>
<author>James Eales</author>
<author>Reza Mohammadi</author>
<author>Jonathan Dickerson</author>
<author>David Robertson</author>
<author>Goran Nenadic</author>
</authors>
<title>Biomedical Event Detection using Rules, Conditional Random Fields and Parse Tree Distances.</title>
<date>2009</date>
<booktitle>BioNLP’09: Proceedings of the Workshop on BioNLP.</booktitle>
<contexts>
<context position="11081" citStr="Sarafraz et al. 2009" startWordPosition="1751" endWordPosition="1754">ylation, regulation, etc.). Depending on the type, each event may have one or more participating proteins (sometimes referred to as themes). Regulatory events are particularly complex, as they can have a cause (a protein or another event) in addition to a theme, which can be either a protein or another event. Table 1 shows examples of five events, where participants are biomedical entities (events 1-3) or other events (events 4 and 5). Note that a sentence can express more than one molecular event. Identification of molecular events in the literature is a challenging IE task (Kim et al. 2009; Sarafraz et al. 2009). For the task of identifying negated events, we assume that events have already been identified in text. Each event is represented by its type, a textual trigger, and one or more participants or causes (see Table 1). Since the participants of different event types can vary in both their number and type, we consider three classes of events to support our analysis (see Section 5): • Class I comprises events with exactly one entity theme (e.g. transcription, protein catabolism, localization, gene expression, phosphorylation). • Class II events include binding events only, which have one or more </context>
</contexts>
<marker>Sarafraz, Eales, Mohammadi, Dickerson, Robertson, Nenadic, 2009</marker>
<rawString>Farzaneh Sarafraz, James Eales, Reza Mohammadi, Jonathan Dickerson, David Robertson and Goran Nenadic. 2009. Biomedical Event Detection using Rules, Conditional Random Fields and Parse Tree Distances. BioNLP’09: Proceedings of the Workshop on BioNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herman Tolentino</author>
<author>Michael Matters</author>
<author>Wikke Walop</author>
<author>Barbara Law</author>
<author>Wesley Tong</author>
<author>Fang Liu</author>
<author>Paul Fontelo</author>
<author>Katrin Kohl</author>
<author>Daniel Payne</author>
</authors>
<title>Concept Negation in Free Text Components of Vaccine Safety Reports. AMIA Annual Symposium proc.</title>
<date>2006</date>
<contexts>
<context position="7253" citStr="Tolentino et al. (2006)" startWordPosition="1134" endWordPosition="1137">ose such as denial of or absence of. Although simple, the proposed approach showed good results on clinical data (78% sensitivity (recall), 84% precision, and 94% specificity). In addition to concepts that are explicitly negated by negation phrases, Patrick et al. (2006) further consider so-called pre-coordinated negative terms (e.g. headache) that have been collected from SNOMED CT2 medical terminology. Similarly, NegFinder uses hand-crafted rules to detect negated UMLS terms, including simple conjunctive and disjunctive statements (Mutalik et al. 2001). They used a list of 60 negation cues. Tolentino et al. (2006), however, show that using rules on a small set of only five negation cues (no, neither/nor, ruled out, denies, without) can still be reasonably successful in detecting negations in medical reports (F-score 91%). Huang and Lowe (2007) introduced a negation grammar that used regular expressions and dependency parse trees to identify negation cues and their scope in the sentence. They applied the rules to a set of radiology reports and reported a precision of 99% and a recall of 92%. Not many efforts have been reported on using machine learning to detect patterns in sentences that contain negati</context>
</contexts>
<marker>Tolentino, Matters, Walop, Law, Tong, Liu, Fontelo, Kohl, Payne, 2006</marker>
<rawString>Herman Tolentino, Michael Matters, Wikke Walop, Barbara Law, Wesley Tong, Fang Liu, Paul Fontelo, Katrin Kohl, and Daniel Payne. 2006. Concept Negation in Free Text Components of Vaccine Safety Reports. AMIA Annual Symposium proc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshimasa Tsuruoka</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Bidirectional Inference with the Easiest-First Strategy for Tagging Sequence Data.</title>
<date>2005</date>
<booktitle>Proceedings of HLT/EMNLP</booktitle>
<pages>467--474</pages>
<contexts>
<context position="18409" citStr="Tsuruoka and Tsujii 2005" startWordPosition="2957" endWordPosition="2960"> with TN and FP defined accordingly. Two sets of negation cues were used in order to compare their influence. A smaller set was derived from related work, whereas additional cues were semi-automatically extracted by exploring the training data. The small negation cue set contains 14 words3, whereas the larger negation cue set contains 32 words4. As expected, the larger set resulted in increased recall, but decreased precision. However, the effects on the Fscore were typically not significant. The results are only shown using the larger cue set. The texts were processed using the GENIA tagger (Tsuruoka and Tsujii 2005).We used constituency parse trees automatically produced by two different constituency parsers reported in (McClosky et al. 2006) and (Bikel 2004). No major differences were observed in the results using the two parsers. The data shown in the results are produced by the former. 5.1 Baseline Results Our baseline method relies on an implementation of the NegEx algorithm as explained in Section 2.1. Event triggers were used as negation targets for the algorithm. An event is then considered to be negated if the trigger is negated; otherwise it 3 Negation cues in this set include: no, not, none, ne</context>
</contexts>
<marker>Tsuruoka, Tsujii, 2005</marker>
<rawString>Yoshimasa Tsuruoka, and Jun’ichi Tsujii. 2005. Bidirectional Inference with the Easiest-First Strategy for Tagging Sequence Data. Proceedings of HLT/EMNLP 2005, 467-474.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>