<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003742">
<title confidence="0.99876">
A Patent Document Retrieval System Addressing
Both Semantic and Syntactic Properties
</title>
<author confidence="0.997015">
Liang Chen Naoyuki Tokuda Hisahiro Adachi
</author>
<affiliation confidence="0.997285">
Computer Science Department R &amp; D Center, Sunflare Company
University of Northern British Columbia Shinjuku-Hirose Bldg., 4-7 Yotsuya
</affiliation>
<address confidence="0.801646">
Prince George, BC, Canada V2N 4Z9 Sinjuku-ku, Tokyo, Japan 160-0004
</address>
<email confidence="0.991791">
chenl@unbc.ca {tokuda n,adachi h}@sunflare.co.jp
</email>
<sectionHeader confidence="0.998539" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999930176470588">
Combining the principle of Differential
Latent Semantic Index (DLSI) (Chen et
al., 2001) and the Template Matching
Technique (Tokuda and Chen, 2001), we
propose a new user queries-based patent
document retrieval system by NLP tech-
nology. The DLSI method first narrows
down the search space of a sought-after
patent document by content search and
the template matching technique then
pins down the documents by exploit-
ing the words-based template matching
scheme by syntactic search. Compared
with the synonymous search scheme by
thesaurus dictionaries, the new method
results in an improved overall retrieval
efficiency of patent documents.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999960877358491">
Information (document) retrieval systems resort to
two classes of approaches; the first makes use of the
form-based or words-based approach addressing
the exact syntactic properties of documents, while
the second makes use of the content-based ap-
proach which exploits the semantic connection be-
tween documents and queries. While most of com-
mercial systems adopt the form-based approach ex-
ploiting the simple string matching algorithm or the
weighted matching algorithm, the approach needs a
thesaurus dictionary to resolve the synonym-related
problem. Some research works have now been un-
derway from the content-based approach focusing
the dimension reduction scheme.
The content-based approach is motivated by
semantics-based search schemes. Assuming that
the content of a document is closely related to the
tf-idf of the words used (Zobel and Moffat, 1998),
we first represent documents as term vectors. One
of the immediate difficulties we encounter in deal-
ing with document vector spaces lies in its too
high a dimensionality of the vector spaces which
is particularly true in document analysis largely
due to a large variety of synonyms and polysemic
words used in natural language. In image recog-
nition field (Turk and Pentland, 1991; Chen and
Tokuda, 2003b), a so-called PCA (principal com-
ponent analysis) principle has been used success-
fully in facial recognition problems as a most ef-
fective scheme of dimension reduction. The LSI
(latent semantic indexing) technique (Berry et al.,
1999; Littman et al., 1998) is a counterpart of the
PCA in text document processing.
We have recently extended the LSI to a DLSI
(differential latent semantic indexing) method
(Chen et al., 2001), where in the DLSI scheme, we
improve the robustness of the LSI scheme by in-
troducing and making use of projections of, inte-
rior as well as exterior differential document vec-
tors (see Section 2 for detailed discussions). Our
present study shows how we can make use of the
characteristics in improving the IR performance in
patent document search. In patent retrieval applica-
tion, we are fortunate because all the patent docu-
ments are well structured with very precise, human
generated abstracts attached so that two interior and
exterior documents are automatically provided, fa-
cilitating the application of the DLSI method in de-
veloping a patent document retrieval system.
Despite the improved superiority of the DLSI
technique over the LSI technique (see Section 2 for
detailed discussions), the system still has a problem
of instability when used as an NLP-oriented query-
based commercial product due to content search’s
inherent poor precision and recall rate. A content-
based information retrieval system is still far be-
yond our research ability to be implemented into a
coding system. Some syntactic properties seeking
the “form” or ”word” similarity must be introduced
if the LSI/DLSI based system can be used with ro-
bustness. This is so because we have to resolve
some conflicting factors here. The content based IR
system tries to search the document in accordance
with the similarity of “meaning” of a query, which
captures the abstraction of the exact words used.
For example, we believe that the LSI/DLSI based
system should be able to retrieve a similar set of
documents to a query “Information Processing De-
vices” and “Computing Machinery”, where prob-
ably some of documents obtained might not con-
tain even the phrases “Information Processing De-
vices” or “Computing Machinery”, or even neither
of these words at all. Form based systems, on the
other hand, have to depend on the exact words used;
in other words, unless a “perfect” thesaurus dictio-
nary is used, we may not capture the correct doc-
uments. Unfortunately we know of no such com-
plete thesaurus dictionary, and even if there is such
a dictionary, the matching or collating method will
be still too complex with respect to computing re-
sources.
To solve “form” similarity problems encoun-
tered in a DLSI/LSI approach, we introduce the
template-automaton method which has been orig-
inally developed for the language tutoring system
(Tokuda and Chen, 2001). The template method
sets up a variety of expected patterns of patent doc-
ument abstracts whereby we want to match a query
against a multitude of template paths by pinning
down a path having the highest similarity mea-
sure to the query from among the documents pre-
selected by the DLSI method. All we have to do
here is to maintain the template structure contain-
ing the possible candidates of the abstracts of patent
documents in natural language, and maintain the
template structures in the database. A DP(dynamic
programming) based-template matching method is
very efficient in finding a best matched path to a
query facilitating the final location of the patent
document.
The rest of the paper is organized as follows. The
scheme of the DLSI method is introduced in Sec-
tion 2 while the template structure will be explained
in Section 3. The Flow of the entire search process
and concluding remarks will be given in Sections 4
&amp;5.
</bodyText>
<sectionHeader confidence="0.9584335" genericHeader="method">
2 Differential Latent Semantic Indexing
Method
</sectionHeader>
<bodyText confidence="0.999829981132076">
A term is defined as a word or a phrase that appears
at least in two documents. We exclude the so-called
stop words such as ”a”, ”the” in English which are
used most frequently in any topics, but remain ir-
relevant to our purpose of document search.
Suppose we select and list the terms that appear
in the documents as tl, t2, ..., t,-,,,. For each patent
document in collection, we preprocess it and assign
it with a document vector as (al, a2, ..., ar,,), where
a2 = fZ • gZ; here f2 denotes the number of times
the term t2 appears in an expression of the docu-
ment, and g2 denotes the global weight over all the
documents; the weight denotes a parameter indicat-
ing the relative importance of the term in represent-
ing the document abstracts. Local weights could
be either raw occurrence counts, boolean, or loga-
rithms of occurrence count. Global weights could
be no weighting (uniform), domain specific, or en-
tropy weighting. The document vector is normal-
ized as (b1, b2, ..., bm). Since all the patent docu-
ments are provided with a formal abstract, we sup-
pose the abstracts be equivalent to their documents
in content so that the abstract and the document
should both be retrieved as part of the similar doc-
uments to the query supplied. We will show be-
low how we can set up the DLSI technique lead-
ing to an improved robust scheme below. We have
shown how the shortcoming of a global projection-
based LSI scheme can be improved by making a
best use of differences of two vectors in adapting to
the unique characteristics of each document (Chen
et al., 2001).
A Differential Document Vector is defined as
h — 12 where h and 12 are normalized document
vectors satisfying particular types of documents.
An Exterior Differential Document Vector in par-
ticular is defined as the Differential Document Vec-
tor I = h — 12, if h and 12 constitute two nor-
malized document vectors of any two different doc-
uments. An Interior Differential Document Vec-
tor is defined by the Differential Document Vector
I = h — 12, where h and 12 constitute two differ-
ent normalized document vectors of the same doc-
ument. The different document vectors of the same
documents may be taken from parts of documents
including abstracts, or may be produced by differ-
ent schemes of summaries, or from the querries.
The Exterior Differential Term-Document Matrix
is defined as a matrix, each column of which is set
to an Exterior Differential Document Vector. The
Interior Differential Term-Document Matrix is de-
fined as a matrix, each column of which comprises
an interior Differential Document Vector.
</bodyText>
<subsectionHeader confidence="0.998193">
2.1 Details of a DLSI Model
</subsectionHeader>
<bodyText confidence="0.996798363636363">
Any differential term-document matrix, say, of m-
by-n matrix D of rank r &lt; q = min(m, n),
can be decomposed into a product of three ma-
trices, namely D = USVT, such that U and V
are an m-by-q and q-by-n unitary matrices respec-
tively, where the first r columns of U and V are
the eigenvectors of DDT and DT D respectively.
S = diag(Æ1, Æ2, • • • , Æq), where Æi are nonnega-
tive square roots of eigen values of DDT, ÆI &gt; 0
for i &lt; r and Æi = 0 for i &gt; r. By convention,
the diagonal elements of S are sorted in decreasing
order of magnitude. To obtain a new reduced ma-
trix Sk, we simply keep the h-by-h leftmost-upper
corner matrix (h &lt; r) of S , other terms being
deleted; we similarly obtain the two new matrices
Uk and Vk by keeping the leftmost h columns of
U and V respectively. The product of Uk, Sk and
VT provides a matrix Dk which is approximately
equal to D. Each of differential document vec-
tor q could find a projection on the h dimensional
differential latent semantic fact space spanned by
the k columns of Uk. The projection can easily
be obtained by UkTq. Note that, the mean x of the
exterior-(interior-)differential document vectors are
approximately 0. Thus,E = nDDT , where E is
the covariance of the distribution computed from
the training set. Assuming that the differential doc-
ument vectors formed follow a high-dimensional
Gaussian distribution, the likelihood of any differ-
ential document vector x will be given by
� ����� �(2,)n/21�11/2 ,
where d(x) = xTE—IX. Since Æ2 ~ are eigenvalues
of DDT, we have S2 = UT DDT U, and thus
</bodyText>
<equation confidence="0.979804888888889">
d(x) = nxT (DDT)-1x = nyTS-2Y
where y = UTx = (Y1, Y2, ... , Yn)T .
Because S is a diagonal matrix, d(x) _
�r 2/ 2
n i=1 �Ji cSi .
It is convenient to estimate the quantity by
d(x) = n( k �Ji lÆi2 + I r Yi2 )
i=1 P i=k+1
1 r
</equation>
<bodyText confidence="0.880251571428571">
where p = r_k k+1 Æ2i .
Because the columns of U are orthonormal vec-
tors, E2=k+1 YZ could be estimated by IXI12 —
E~ 1 �i 2.
i—
Thus, the likelihood function P (xID)
could be estimated by
</bodyText>
<equation confidence="0.999596285714286">
P(xlD) _
1/2 �_ n k Y? ( ne2(x) )
n exp �2=1 �
2 b
exp — 2p
�
(2,)n/2Flk ��� Æ~ � p(r—k)/2
</equation>
<bodyText confidence="0.99974775">
where y = Uk x, E2(x) _ I�xJ12 —Ek ~~1 Y2 , p =
r—k Ei=k+l Æ2 , r is the rank of matrix D. In prac-
tical cases, p may be approximated byÆ~~+1/2, and
rbyn.
</bodyText>
<subsectionHeader confidence="0.859932">
2.2 Algorithm
2.2.1 Setting Up Retrieval System
</subsectionHeader>
<listItem confidence="0.99220475">
1. Text preprocessing: Identify words and noun
phrases as well as stop words.
2. System term construction: Set up the term list as
well as the global weights.
3. Set up the document vectors of all the collected
documents in normalized form .
4. Construct interior differential term-document
matrix ����l
</listItem>
<equation confidence="0.850869333333333">
� , such that each of its column is an
exp�—zd(x)1
� (1)
</equation>
<bodyText confidence="0.956574">
interior differential document vector.
</bodyText>
<listItem confidence="0.858182">
5. Construct an exterior differential term-document
</listItem>
<bodyText confidence="0.776267">
matrix D�~�
� , such that each of its column is an
exterior differential document vector.
</bodyText>
<listItem confidence="0.9577578">
6. Decompose DI and DE by SVD (singular value
decomposition) algorithm into USV form. Find
proper values of k’s to define the likelihood func-
tions P(xIDI) and P(xIDE) as Equition (1).
7. P(DIlx) _
</listItem>
<equation confidence="0.99918">
P(xIDr)P(Dr)
P(xlDI)P(DI) + P(xlDE)P(DE)&apos;
</equation>
<bodyText confidence="0.999782666666667">
where P(DI) is set to an average number of re-
calls divided by the number of documents in the
data base and P (DE) is set to 1 — P (DI).
</bodyText>
<subsubsectionHeader confidence="0.706757">
2.2.2 Patent Document Search
</subsubsectionHeader>
<listItem confidence="0.998792571428571">
1. A query is treated as a document; a document
vector is set up by generating the terms as well as
their frequency of occurrence, and thus a normal-
ized document vector is obtained for the query.
Each document in the data base are processed by
the procedures in items 2-5 below.
2. Given a query, construct a differential document
vector x .
3. Calculate the interior document likelihood func-
tion P (xIDI), and calculate the exterior document
likelihood function P (xIDE) for the document.
4. Calculate the Bayesian posteriori probability
function P (DIIx) .
5. Select those documents whose P(DIIx) exceeds
</listItem>
<bodyText confidence="0.505086">
a given threshold (say, 0.5), or choose N documents
having the first N largest P(DIIx).
</bodyText>
<sectionHeader confidence="0.9869195" genericHeader="method">
3 Template Structure for Storing Patent
Abstracts
</sectionHeader>
<bodyText confidence="0.999986357142858">
Each patent document is usually provided with an
abstract. The abstract can be used for content-based
information retrieval by using DLSI method as de-
scribed above. As we have mentioned before, the
content-based information retrieval system by LSI
analysis is not robust enough to be directly applica-
ble to a real system. We will use the DLSI method
only to narrow down the search space at a first stage
of filtering in information retrieval. We will resort
to a form based searching strategy to pin down the
patent document.
Now that the content-based DLSI search scheme
has narrowed down the search space in content, the
form based search strategy we now employ need
not to pay attention to the synonymous expressions
of the searching terms or sentences.
This first stage of filtering is now implemented
without going through the tedious process of deal-
ing with the synonymous expressions by synonym
dictionaries which are hard to develop and to use.
Even if we succeeded in treating the synomyms, we
also have to realize that the polynonym of a nat-
ural language will reduce the advantage of using
synonym dictionary further, because two words are
synonymous in one situation but might not be so in
other situations, depending on context.
In view of lengthy sentences used in patent docu-
ments including their abstracts, we want to empha-
size that automaton-based template structure is an
extremely efficient way of expressing lengthy sen-
tences with their synonymous expressions.
We will demonstrate this point by way of exam-
ples below. For a sentence, “There are beautiful
parks in Japan across the nation”, we can use a tem-
plate as of figure 1 where a variety of synonymous
expressions are explicitly represented.
The problem here is, how we could get the tem-
plate for an abstract of patent document? Firstly,
we regard the original abstract of patent itself as a
simplest template. Then, we register queries into
the matched template structures by combining each
pair of matched terms into one node. This is illus-
trated by an example procedure in figures 1-3. The
original template of an abstract is indicated by fig-
ure 1, but when a query of figure 2, namely, “There
are lovely parks across Japan”, is matched to the
template of figure 1, the template could be modi-
fied to a new structure of figure3.
Suppose that the query sentence is, “There are
ugly streets in Japan”. Now although we could lo-
cate a matching pattern similar to that of figure 2,
we will have to rule it out so that we will not come
up with a template which include the above sen-
tence as a path, or part of a path . This mecha-
nism should be established from users’ response.
We will explain it in Section 4.1.
</bodyText>
<figure confidence="0.974782">
nationwide in Japan
the
country
nation
beautiful
pretty
There are
parks
in Japan
across
all over
nationwide
</figure>
<figureCaption confidence="0.999983">
Figure 1: Template Example Indicating a set of Semantically Similar Patent Abstracts
Figure 2: Query Template to be matched with Abstract Template
</figureCaption>
<figure confidence="0.999549735294117">
nationwide in
Japan
beautiful
There are
parks
pretty
in Japan
There are
lovely
parks
across
Japan
countr y
nation
across
all over
the
nationwide
Japan
nationwide
in
across
in Japan
beautiful
lovely
pretty
There are
parks
the
country
nation
nationwide
all over
across
</figure>
<figureCaption confidence="0.999909">
Figure 3: Modified Template
</figureCaption>
<sectionHeader confidence="0.998158" genericHeader="method">
4 The Flow of the Search Process
</sectionHeader>
<subsectionHeader confidence="0.989074">
4.1 The Entire Flow of the Complete Search
Process
</subsectionHeader>
<bodyText confidence="0.9988015">
Before starting the search process, we should set up
the DLSI for all the patent documents.
</bodyText>
<listItem confidence="0.840631">
1. Locate the query in the DLSI space.
2. Find and select those patent documents whose
abstracts’ vector space lie in a neighborhood of the
query vector space having semantic similarity to
sentences of figure 1 by the DLSI matching algo-
rithm.
3. For each of the abstracts obtained by step 4.1,
use the template matching algorithm of (Chen and
Tokuda, 2003a) to calculate the similarity of the
summary and the query, select the documents of
which the abstracts have a highest similarity to the
query.
4. Show the result to the user.
5. Modify the abstracts in the database by users’
responses.
</listItem>
<sectionHeader confidence="0.993268" genericHeader="conclusions">
5 Concluding Remarks
</sectionHeader>
<bodyText confidence="0.999992461538461">
We have proposed a new IR method for patent
documents addressing both semantic and syntactic
properties by combining a mixed model of content
and form based methods; the first stage of DLSI
method narrows down the search space by content
and the second template method pins down the doc-
ument by syntactic search on words. We are able to
do so, mainly because the DLSI matching in the
first stage captures those documents based on con-
tent while the template method can now pin down
the patent documents having a highest similarity in
form with the query. An experimental verification
of the present approach is now underway.
</bodyText>
<sectionHeader confidence="0.999554" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9998005">
M. W. Berry, Z. Drmac, and E. R. Jessup. 1999. Matri-
ces, vector spaces, and information retrieval. SIAM Rev.,
41(2):335–362.
L. Chen and N. Tokuda. 2003a. Bug diagnosis by string
matching: Application to ILTS for translation. CALICO
Journal, 20(2):227–244.
L. Chen and N. Tokuda. 2003b. Robustness of regional match-
ing scheme over global matching scheme. ArtificialIntelli-
gence, 144(1-2):213–232.
L. Chen, N. Tokuda, and A. Nagai. 2001. Probabilistic In-
formation Retrieval Method Based on Differential Latent
Semantic Index Space. IEICE Trans. on Information and
Systems, E84-D(7):910–914.
M. L. Littman, Fan Jiang, and Greg A. Keim. 1998. Learn-
ing a language-independent representation for terms from a
partially aligned corpus. In Proceedings of the Fifteenth In-
ternational Conference on Machine Learning, pages 314–
322.
N. Tokuda and L. Chen. 2001. An online tutoring system for
language translation. IEEEMultimedia, 8(3):46–55.
M. Turk and A. Pentland. 1991. Eigenfaces for recognition.
Journal of Cognitive Neuroscience, 3(1):71–86.
Justin Zobel and Alistair Moffat. 1998. Exploring the similar-
ity space. ACM SIGIR FORUM, 32(1):18–34.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.862201">
<title confidence="0.98108">A Patent Document Retrieval System Both Semantic and Syntactic Properties</title>
<author confidence="0.999087">Liang Chen Naoyuki Tokuda Hisahiro Adachi</author>
<affiliation confidence="0.99243">Computer Science Department R &amp; D Center, Sunflare University of Northern British Columbia Shinjuku-Hirose Bldg., 4-7 Yotsuya</affiliation>
<address confidence="0.938614">Prince George, BC, Canada V2N 4Z9 Sinjuku-ku, Tokyo, Japan 160-0004</address>
<email confidence="0.968231">n,adachi</email>
<abstract confidence="0.999555222222222">Combining the principle of Differential Latent Semantic Index (DLSI) (Chen et al., 2001) and the Template Matching Technique (Tokuda and Chen, 2001), we propose a new user queries-based patent document retrieval system by NLP technology. The DLSI method first narrows down the search space of a sought-after patent document by content search and the template matching technique then pins down the documents by exploiting the words-based template matching scheme by syntactic search. Compared with the synonymous search scheme by thesaurus dictionaries, the new method results in an improved overall retrieval efficiency of patent documents.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M W Berry</author>
<author>Z Drmac</author>
<author>E R Jessup</author>
</authors>
<title>Matrices, vector spaces, and information retrieval.</title>
<date>1999</date>
<journal>SIAM Rev.,</journal>
<volume>41</volume>
<issue>2</issue>
<contexts>
<context position="2539" citStr="Berry et al., 1999" startWordPosition="374" endWordPosition="377"> documents as term vectors. One of the immediate difficulties we encounter in dealing with document vector spaces lies in its too high a dimensionality of the vector spaces which is particularly true in document analysis largely due to a large variety of synonyms and polysemic words used in natural language. In image recognition field (Turk and Pentland, 1991; Chen and Tokuda, 2003b), a so-called PCA (principal component analysis) principle has been used successfully in facial recognition problems as a most effective scheme of dimension reduction. The LSI (latent semantic indexing) technique (Berry et al., 1999; Littman et al., 1998) is a counterpart of the PCA in text document processing. We have recently extended the LSI to a DLSI (differential latent semantic indexing) method (Chen et al., 2001), where in the DLSI scheme, we improve the robustness of the LSI scheme by introducing and making use of projections of, interior as well as exterior differential document vectors (see Section 2 for detailed discussions). Our present study shows how we can make use of the characteristics in improving the IR performance in patent document search. In patent retrieval application, we are fortunate because all</context>
</contexts>
<marker>Berry, Drmac, Jessup, 1999</marker>
<rawString>M. W. Berry, Z. Drmac, and E. R. Jessup. 1999. Matrices, vector spaces, and information retrieval. SIAM Rev., 41(2):335–362.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Chen</author>
<author>N Tokuda</author>
</authors>
<title>Bug diagnosis by string matching: Application to ILTS for translation.</title>
<date>2003</date>
<journal>CALICO Journal,</journal>
<volume>20</volume>
<issue>2</issue>
<contexts>
<context position="2305" citStr="Chen and Tokuda, 2003" startWordPosition="338" endWordPosition="341">imension reduction scheme. The content-based approach is motivated by semantics-based search schemes. Assuming that the content of a document is closely related to the tf-idf of the words used (Zobel and Moffat, 1998), we first represent documents as term vectors. One of the immediate difficulties we encounter in dealing with document vector spaces lies in its too high a dimensionality of the vector spaces which is particularly true in document analysis largely due to a large variety of synonyms and polysemic words used in natural language. In image recognition field (Turk and Pentland, 1991; Chen and Tokuda, 2003b), a so-called PCA (principal component analysis) principle has been used successfully in facial recognition problems as a most effective scheme of dimension reduction. The LSI (latent semantic indexing) technique (Berry et al., 1999; Littman et al., 1998) is a counterpart of the PCA in text document processing. We have recently extended the LSI to a DLSI (differential latent semantic indexing) method (Chen et al., 2001), where in the DLSI scheme, we improve the robustness of the LSI scheme by introducing and making use of projections of, interior as well as exterior differential document vec</context>
<context position="16433" citStr="Chen and Tokuda, 2003" startWordPosition="2815" endWordPosition="2818">There are parks the country nation nationwide all over across Figure 3: Modified Template 4 The Flow of the Search Process 4.1 The Entire Flow of the Complete Search Process Before starting the search process, we should set up the DLSI for all the patent documents. 1. Locate the query in the DLSI space. 2. Find and select those patent documents whose abstracts’ vector space lie in a neighborhood of the query vector space having semantic similarity to sentences of figure 1 by the DLSI matching algorithm. 3. For each of the abstracts obtained by step 4.1, use the template matching algorithm of (Chen and Tokuda, 2003a) to calculate the similarity of the summary and the query, select the documents of which the abstracts have a highest similarity to the query. 4. Show the result to the user. 5. Modify the abstracts in the database by users’ responses. 5 Concluding Remarks We have proposed a new IR method for patent documents addressing both semantic and syntactic properties by combining a mixed model of content and form based methods; the first stage of DLSI method narrows down the search space by content and the second template method pins down the document by syntactic search on words. We are able to do s</context>
</contexts>
<marker>Chen, Tokuda, 2003</marker>
<rawString>L. Chen and N. Tokuda. 2003a. Bug diagnosis by string matching: Application to ILTS for translation. CALICO Journal, 20(2):227–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Chen</author>
<author>N Tokuda</author>
</authors>
<title>Robustness of regional matching scheme over global matching scheme.</title>
<date>2003</date>
<journal>ArtificialIntelligence,</journal>
<pages>144--1</pages>
<contexts>
<context position="2305" citStr="Chen and Tokuda, 2003" startWordPosition="338" endWordPosition="341">imension reduction scheme. The content-based approach is motivated by semantics-based search schemes. Assuming that the content of a document is closely related to the tf-idf of the words used (Zobel and Moffat, 1998), we first represent documents as term vectors. One of the immediate difficulties we encounter in dealing with document vector spaces lies in its too high a dimensionality of the vector spaces which is particularly true in document analysis largely due to a large variety of synonyms and polysemic words used in natural language. In image recognition field (Turk and Pentland, 1991; Chen and Tokuda, 2003b), a so-called PCA (principal component analysis) principle has been used successfully in facial recognition problems as a most effective scheme of dimension reduction. The LSI (latent semantic indexing) technique (Berry et al., 1999; Littman et al., 1998) is a counterpart of the PCA in text document processing. We have recently extended the LSI to a DLSI (differential latent semantic indexing) method (Chen et al., 2001), where in the DLSI scheme, we improve the robustness of the LSI scheme by introducing and making use of projections of, interior as well as exterior differential document vec</context>
<context position="16433" citStr="Chen and Tokuda, 2003" startWordPosition="2815" endWordPosition="2818">There are parks the country nation nationwide all over across Figure 3: Modified Template 4 The Flow of the Search Process 4.1 The Entire Flow of the Complete Search Process Before starting the search process, we should set up the DLSI for all the patent documents. 1. Locate the query in the DLSI space. 2. Find and select those patent documents whose abstracts’ vector space lie in a neighborhood of the query vector space having semantic similarity to sentences of figure 1 by the DLSI matching algorithm. 3. For each of the abstracts obtained by step 4.1, use the template matching algorithm of (Chen and Tokuda, 2003a) to calculate the similarity of the summary and the query, select the documents of which the abstracts have a highest similarity to the query. 4. Show the result to the user. 5. Modify the abstracts in the database by users’ responses. 5 Concluding Remarks We have proposed a new IR method for patent documents addressing both semantic and syntactic properties by combining a mixed model of content and form based methods; the first stage of DLSI method narrows down the search space by content and the second template method pins down the document by syntactic search on words. We are able to do s</context>
</contexts>
<marker>Chen, Tokuda, 2003</marker>
<rawString>L. Chen and N. Tokuda. 2003b. Robustness of regional matching scheme over global matching scheme. ArtificialIntelligence, 144(1-2):213–232.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Chen</author>
<author>N Tokuda</author>
<author>A Nagai</author>
</authors>
<date>2001</date>
<booktitle>Probabilistic Information Retrieval Method Based on Differential Latent Semantic Index Space. IEICE Trans. on Information and Systems, E84-D(7):910–914.</booktitle>
<contexts>
<context position="2730" citStr="Chen et al., 2001" startWordPosition="406" endWordPosition="409">rly true in document analysis largely due to a large variety of synonyms and polysemic words used in natural language. In image recognition field (Turk and Pentland, 1991; Chen and Tokuda, 2003b), a so-called PCA (principal component analysis) principle has been used successfully in facial recognition problems as a most effective scheme of dimension reduction. The LSI (latent semantic indexing) technique (Berry et al., 1999; Littman et al., 1998) is a counterpart of the PCA in text document processing. We have recently extended the LSI to a DLSI (differential latent semantic indexing) method (Chen et al., 2001), where in the DLSI scheme, we improve the robustness of the LSI scheme by introducing and making use of projections of, interior as well as exterior differential document vectors (see Section 2 for detailed discussions). Our present study shows how we can make use of the characteristics in improving the IR performance in patent document search. In patent retrieval application, we are fortunate because all the patent documents are well structured with very precise, human generated abstracts attached so that two interior and exterior documents are automatically provided, facilitating the applic</context>
<context position="7686" citStr="Chen et al., 2001" startWordPosition="1251" endWordPosition="1254"> vector is normalized as (b1, b2, ..., bm). Since all the patent documents are provided with a formal abstract, we suppose the abstracts be equivalent to their documents in content so that the abstract and the document should both be retrieved as part of the similar documents to the query supplied. We will show below how we can set up the DLSI technique leading to an improved robust scheme below. We have shown how the shortcoming of a global projectionbased LSI scheme can be improved by making a best use of differences of two vectors in adapting to the unique characteristics of each document (Chen et al., 2001). A Differential Document Vector is defined as h — 12 where h and 12 are normalized document vectors satisfying particular types of documents. An Exterior Differential Document Vector in particular is defined as the Differential Document Vector I = h — 12, if h and 12 constitute two normalized document vectors of any two different documents. An Interior Differential Document Vector is defined by the Differential Document Vector I = h — 12, where h and 12 constitute two different normalized document vectors of the same document. The different document vectors of the same documents may be taken </context>
</contexts>
<marker>Chen, Tokuda, Nagai, 2001</marker>
<rawString>L. Chen, N. Tokuda, and A. Nagai. 2001. Probabilistic Information Retrieval Method Based on Differential Latent Semantic Index Space. IEICE Trans. on Information and Systems, E84-D(7):910–914.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M L Littman</author>
<author>Fan Jiang</author>
<author>Greg A Keim</author>
</authors>
<title>Learning a language-independent representation for terms from a partially aligned corpus.</title>
<date>1998</date>
<booktitle>In Proceedings of the Fifteenth International Conference on Machine Learning,</booktitle>
<pages>314--322</pages>
<contexts>
<context position="2562" citStr="Littman et al., 1998" startWordPosition="378" endWordPosition="381">ectors. One of the immediate difficulties we encounter in dealing with document vector spaces lies in its too high a dimensionality of the vector spaces which is particularly true in document analysis largely due to a large variety of synonyms and polysemic words used in natural language. In image recognition field (Turk and Pentland, 1991; Chen and Tokuda, 2003b), a so-called PCA (principal component analysis) principle has been used successfully in facial recognition problems as a most effective scheme of dimension reduction. The LSI (latent semantic indexing) technique (Berry et al., 1999; Littman et al., 1998) is a counterpart of the PCA in text document processing. We have recently extended the LSI to a DLSI (differential latent semantic indexing) method (Chen et al., 2001), where in the DLSI scheme, we improve the robustness of the LSI scheme by introducing and making use of projections of, interior as well as exterior differential document vectors (see Section 2 for detailed discussions). Our present study shows how we can make use of the characteristics in improving the IR performance in patent document search. In patent retrieval application, we are fortunate because all the patent documents a</context>
</contexts>
<marker>Littman, Jiang, Keim, 1998</marker>
<rawString>M. L. Littman, Fan Jiang, and Greg A. Keim. 1998. Learning a language-independent representation for terms from a partially aligned corpus. In Proceedings of the Fifteenth International Conference on Machine Learning, pages 314– 322.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Tokuda</author>
<author>L Chen</author>
</authors>
<title>An online tutoring system for language translation.</title>
<date>2001</date>
<journal>IEEEMultimedia,</journal>
<volume>8</volume>
<issue>3</issue>
<contexts>
<context position="5152" citStr="Tokuda and Chen, 2001" startWordPosition="802" endWordPosition="805">ither of these words at all. Form based systems, on the other hand, have to depend on the exact words used; in other words, unless a “perfect” thesaurus dictionary is used, we may not capture the correct documents. Unfortunately we know of no such complete thesaurus dictionary, and even if there is such a dictionary, the matching or collating method will be still too complex with respect to computing resources. To solve “form” similarity problems encountered in a DLSI/LSI approach, we introduce the template-automaton method which has been originally developed for the language tutoring system (Tokuda and Chen, 2001). The template method sets up a variety of expected patterns of patent document abstracts whereby we want to match a query against a multitude of template paths by pinning down a path having the highest similarity measure to the query from among the documents preselected by the DLSI method. All we have to do here is to maintain the template structure containing the possible candidates of the abstracts of patent documents in natural language, and maintain the template structures in the database. A DP(dynamic programming) based-template matching method is very efficient in finding a best matched</context>
</contexts>
<marker>Tokuda, Chen, 2001</marker>
<rawString>N. Tokuda and L. Chen. 2001. An online tutoring system for language translation. IEEEMultimedia, 8(3):46–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Turk</author>
<author>A Pentland</author>
</authors>
<title>Eigenfaces for recognition.</title>
<date>1991</date>
<journal>Journal of Cognitive Neuroscience,</journal>
<volume>3</volume>
<issue>1</issue>
<contexts>
<context position="2282" citStr="Turk and Pentland, 1991" startWordPosition="334" endWordPosition="337">d approach focusing the dimension reduction scheme. The content-based approach is motivated by semantics-based search schemes. Assuming that the content of a document is closely related to the tf-idf of the words used (Zobel and Moffat, 1998), we first represent documents as term vectors. One of the immediate difficulties we encounter in dealing with document vector spaces lies in its too high a dimensionality of the vector spaces which is particularly true in document analysis largely due to a large variety of synonyms and polysemic words used in natural language. In image recognition field (Turk and Pentland, 1991; Chen and Tokuda, 2003b), a so-called PCA (principal component analysis) principle has been used successfully in facial recognition problems as a most effective scheme of dimension reduction. The LSI (latent semantic indexing) technique (Berry et al., 1999; Littman et al., 1998) is a counterpart of the PCA in text document processing. We have recently extended the LSI to a DLSI (differential latent semantic indexing) method (Chen et al., 2001), where in the DLSI scheme, we improve the robustness of the LSI scheme by introducing and making use of projections of, interior as well as exterior di</context>
</contexts>
<marker>Turk, Pentland, 1991</marker>
<rawString>M. Turk and A. Pentland. 1991. Eigenfaces for recognition. Journal of Cognitive Neuroscience, 3(1):71–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Justin Zobel</author>
<author>Alistair Moffat</author>
</authors>
<title>Exploring the similarity space.</title>
<date>1998</date>
<journal>ACM SIGIR FORUM,</journal>
<volume>32</volume>
<issue>1</issue>
<contexts>
<context position="1901" citStr="Zobel and Moffat, 1998" startWordPosition="271" endWordPosition="274">nt-based approach which exploits the semantic connection between documents and queries. While most of commercial systems adopt the form-based approach exploiting the simple string matching algorithm or the weighted matching algorithm, the approach needs a thesaurus dictionary to resolve the synonym-related problem. Some research works have now been underway from the content-based approach focusing the dimension reduction scheme. The content-based approach is motivated by semantics-based search schemes. Assuming that the content of a document is closely related to the tf-idf of the words used (Zobel and Moffat, 1998), we first represent documents as term vectors. One of the immediate difficulties we encounter in dealing with document vector spaces lies in its too high a dimensionality of the vector spaces which is particularly true in document analysis largely due to a large variety of synonyms and polysemic words used in natural language. In image recognition field (Turk and Pentland, 1991; Chen and Tokuda, 2003b), a so-called PCA (principal component analysis) principle has been used successfully in facial recognition problems as a most effective scheme of dimension reduction. The LSI (latent semantic i</context>
</contexts>
<marker>Zobel, Moffat, 1998</marker>
<rawString>Justin Zobel and Alistair Moffat. 1998. Exploring the similarity space. ACM SIGIR FORUM, 32(1):18–34.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>