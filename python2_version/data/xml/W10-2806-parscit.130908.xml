<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000290">
<title confidence="0.993516">
Semantic Composition with Quotient Algebras
</title>
<author confidence="0.992772">
Daoud Clarke
</author>
<affiliation confidence="0.995113">
University of Hertfordshire
</affiliation>
<address confidence="0.803239">
Hatfield, UK
</address>
<email confidence="0.983807">
daoud@metrica.net
</email>
<author confidence="0.981496">
Rudi Lutz
</author>
<affiliation confidence="0.989022">
University of Sussex
</affiliation>
<address confidence="0.751263">
Brighton, UK
</address>
<email confidence="0.991281">
rudil@sussex.ac.uk
</email>
<author confidence="0.996669">
David Weir
</author>
<affiliation confidence="0.997021">
University of Sussex
</affiliation>
<address confidence="0.755407">
Brighton, UK
</address>
<email confidence="0.993151">
davidw@sussex.ac.uk
</email>
<sectionHeader confidence="0.993759" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998333">
We describe an algebraic approach for
computing with vector based semantics.
The tensor product has been proposed as
a method of composition, but has the un-
desirable property that strings of different
length are incomparable. We consider how
a quotient algebra of the tensor algebra can
allow such comparisons to be made, offer-
ing the possibility of data-driven models of
semantic composition.
</bodyText>
<sectionHeader confidence="0.998798" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999395677419355">
Vector based techniques have been exploited in a
wide array of natural language processing appli-
cations (Sch¨utze, 1998; McCarthy et al., 2004;
Grefenstette, 1994; Lin, 1998; Bellegarda, 2000;
Choi et al., 2001). Techniques such as latent se-
mantic analysis and distributional similarity anal-
yse contexts in which terms occur, building up a
vector of features which incorporate aspects of the
meaning of the term. This idea has its origins in
the distributional hypothesis of Harris (1968), that
words with similar meanings will occur in similar
contexts, and vice-versa.
However, there has been limited attention paid
to extending this idea beyond individual words,
so that the distributional meaning of phrases and
whole sentences can be represented as vectors.
While these techniques work well at the word
level, for longer strings, data becomes extremely
sparse. This has led to various proposals explor-
ing methods for composing vectors, rather than de-
riving them directly from the data (Landauer and
Dumais, 1997; Foltz et al., 1998; Kintsch, 2001;
Widdows, 2008; Clark et al., 2008; Mitchell and
Lapata, 2008; Erk and Pado, 2009; Preller and
Sadrzadeh, 2009). Many of these approaches use
a pre-defined composition operation such as ad-
dition (Landauer and Dumais, 1997; Foltz et al.,
1998) or the tensor product (Smolensky, 1990;
Clark and Pulman, 2007; Widdows, 2008) which
contrasts with the data-driven definition of com-
position developed here.
</bodyText>
<sectionHeader confidence="0.996296" genericHeader="method">
2 Tensor Algebras
</sectionHeader>
<bodyText confidence="0.983621766666667">
Following the context-theoretic semantics of
Clarke (2007), we take the meaning of strings as
being described by a multiplication on a vector
space that is bilinear with respect to the addition
of the vector space, i.e.
x(y + z) = xy + xz (x + y)z = xz + yz
It is assumed that the multiplication is associative,
but not commutative. The resulting structure is an
associative algebra over a field — or simply an
algebra when there is no ambiguity.
One commonly used bilinear multiplication op-
erator on vector spaces is the tensor product (de-
noted ⊗), whose use as a method of combining
meaning was first proposed by Smolensky (1990),
and has been considered more recently by Clark
and Pulman (2007) and Widdows (2008), who also
looked at the direct sum (which Widdows calls
the direct product, denoted ⊕).
We give a very brief account of the tensor prod-
uct and direct sum in the finite-dimensional case;
see (Halmos, 1974) for formal and complete defi-
nitions. Roughly speaking, if u1, u2,... un form
an orthonormal basis for a vector space U and
v1, v2,... vm form an orthonormal basis for vector
space V , then the space U ⊗ V has dimensionality
nm with an orthonormal basis formed by the set
of all ordered pairs (ui, vj), denoted by ui ⊗ vj,
of the individual basis elements. For arbitrary el-
ements u = Eni=1 αiui and v = Emj=1 βjvj the
tensor product of u and v is then given by
</bodyText>
<equation confidence="0.8186915">
u ⊗ v = n �m αiβj ui ⊗ vj
i j
</equation>
<page confidence="0.973492">
38
</page>
<note confidence="0.6262495">
Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics, ACL 2010, pages 38–44,
Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.94998775">
For two finite dimensional vector spaces U and
V (over a field F) of dimensionality n and m re-
spectively, the direct sum U ® V is defined as the
cartesian product U x V together with the oper-
</bodyText>
<construct confidence="0.87268425">
ations (u1, v1) + (u2, v2) = (u1 + u2, v1 + v2),
and a(u1, v1) = (au1, av1), for u1, u2 E U,
v1, v2 E V and a E F. In this case the vectors
u1, u2,... un, v1, v2,... vm form an orthonormal
</construct>
<bodyText confidence="0.982547734693877">
set of basis vectors in U ® V , which is thus of
dimensionality n + m. In this case one normally
identifies U with the set of vectors in U ® V of the
form (u, 0), and V with the set of vectors of the
form (0, v). This construction makes U ® V iso-
morphic to V ®U, and thus the direct sum is often
treated as commutative, as we do in this paper.
The motivation behind using the tensor product
to combine meanings is that it is very fine-grained.
So, if, for example, red is represented by a vector
u consisting of a feature for each noun that is mod-
ified by red, and apple is represented by a vector
v consisting of a feature for each verb that occurs
with apple as a direct object, then red apple will
be represented by u ® v with a non-zero compo-
nent for every pair of non-zero features (one from
u and one from v). So, there is a non-zero ele-
ment for each composite feature, something that
has been described as red, and something that has
been done with an apple, for example, sky and eat.
Both ® and ® are intuitively appealing as se-
mantic composition operators, since u and v are
reconstructible from each of u ® v and u ® v, and
thus no information is lost in composing u and v.
Conversely, this is not possible with ordinary vec-
tor addition, which also suffers from the fact that it
is strictly commutative (not simply up to isomor-
phism like ®), whereas natural language composi-
tion is in general manifestly non-commutative.
We make use of a construction called the tensor
algebra on a vector space V (where V is a space
of context features), defined as:
T(V ) = R ® V ® (V ® V ) ® (V ® V ® V ) ® ···
Any element of T (V ) can be described as a sum of
components with each in a different tensor power
of V . Multiplication is defined as the tensor prod-
uct on these components, and extended linearly to
the whole of T(V ). We define the degree of a vec-
tor u in T (V ) to be the tensor power of its high-
est dimensional non-zero component, and denote
it deg(u); so for example, both v®v and u®(v®v)
have degree two, for 0 =� u, v E V . We restrict
T (V ) to only contain vectors of finite degree.
A standard way to compare elements of a vector
space is to make use of an inner product, which
provides a measure of semantic distance on that
space. Assuming we have an inner product (·, ·) on
V , T (V ) can be given an inner product by defining
(α, Q) = αQ for α, Q E R, and
</bodyText>
<equation confidence="0.987848">
(x1 ® y1, x2 ® y2) = (x1, x2)(y1, y2)
</equation>
<bodyText confidence="0.994919230769231">
for x1, y1, x2, y2 E V , and then extending this in-
ductively (and by linearity) to the whole of T (V ).
We assume that words are associated with vec-
tors in V , and that the higher tensor powers repre-
sent strings of words. The problem with the tensor
product as a method of composition, given the in-
ner product as we have defined it, is that strings
of different lengths will have orthogonal vectors,
clearly a serious problem, since strings of different
lengths can have similar meanings. In our previous
example, the vector corresponding to the concept
red apple lives in the vector space U ® V , and so
we have no way to compare it to the space V of
nouns, even though red apple should clearly be re-
lated to apple.
Previous work has not made full use of the ten-
sor product space; only tensor products are used,
not sums of tensor products, giving us the equiva-
lent of the product states of quantum mechanics.
Our approach imposes relations on the vectors of
the tensor product space that causes some product
states to become equivalent to entangled states,
containing sums of tensor products of different de-
grees. This allows strings of different lengths to
share components. We achieve this by construct-
ing a quotient algebra.
</bodyText>
<sectionHeader confidence="0.982999" genericHeader="method">
3 Quotient Algebras
</sectionHeader>
<bodyText confidence="0.995604">
An ideal I of an algebra A is a sub-vector space
of A such that xa E I and ax E I for all a E A
and all x E I. An ideal introduces a congruence
- on A defined by x - y if and only if x − y E I.
For any set of elements A C_ A there is a unique
minimal ideal IA containing all elements of A; this
is called the ideal generated by A. The quotient
algebra A/I is the set of all equivalence classes
defined by this congruence. Multiplication is de-
fined on A/I by the multiplication on A, since -
is a congruence.
By adding an element x − y to the generating
set A of an ideal, we are saying that we want to
set x − y to zero in the quotient algebra, which
has the effect of setting x equal to y. Thus, if we
</bodyText>
<page confidence="0.996578">
39
</page>
<bodyText confidence="0.999592041666667">
have a set of pairs of vectors that we wish to make
equal in the quotient algebra, we put their differ-
ences in the generating set of the ideal. Note that
putting a single vector v in the generating set can
have knock-on effects, since all products of v with
elements of A will also end up in the ideal.
Although we have an inner product defined on
T(V ), we are not aware of any satisfactory method
for defining an inner product on T(V )/I, a con-
sequence of the fact that both T (V ) and I are
not complete. Instead, we define an inner prod-
uct on a space which contains the quotient algebra,
T(V )/I. Rather than considering all elements of
the ideal when computing the quotient, we con-
sider a sub-vector space of the ideal, limiting our-
selves to the space Gk generated from A by only
allowing multiplication by elements up to a certain
degree, k.
Let us denote the vector subspace generated by
linearity alone (no multiplications) from a sub-
set A of T(V ) by G(A). Also suppose B =
{e1, ... , eN} is a basis for V . We then define
the spaces Gk as follows. Define sets Ak (k =
0,1,2, ...) inductively as follows:
</bodyText>
<equation confidence="0.816864083333333">
A0 = A
Ak = Ak−1 U {(ei ® Ak−1)|ei E B}
U {(Ak−1 ® ei)|ei E B}
Define
Gk = G(Ak)
We note that
G0 C G1 C ... Gk C ... C I C T(V )
form an increasing sequence of linear vector sub-
spaces of T(V ), and that
∞
I= U Gk
k=0
</equation>
<bodyText confidence="0.99907752631579">
This means that for any x E I there exists a small-
est k such that for all k0 &gt; k we have that x E Gk0.
Lemma. Let x E I, x =� 0 and let deg(x) = d.
Then for all k &gt; d − mindeg(A) we have that
x E Gk, where mindeg(A) is defined to be the
minimum degree of the non-zero components oc-
curring in the elements of A.
Proof. We first note that for x E I it must
be the case that deg(x) &gt; mindeg(A) since I
is generated from A. Therefore we know d −
mindeg(A) &gt; 0. We only need to show that
x E Gd−mindeg(A). Let k0 be the smallest in-
teger such that x E Gk0. Since x E� Gk0−1 it
must be the case that the highest degree term of
x comes from V ® Gk0−1 U Gk0−1 ® V . There-
fore k0 + mindeg(A) &lt; d &lt; k0 + maxdeg(A).
From this it follows that the smallest k0 for which
x E Gk0 satisfies k0 &lt; d − mindeg(A), and we
know x E Gk for all k &gt; k0. In particular x E Gk
for k &gt; d − mindeg(A).
We show that T(V )/Gk (for an appropriate
choice of k) captures the essential features of
T(V )/I in terms of equivalence:
Proposition. Let deg(a − b) = d and let k &gt;
d − mindeg(A). Then a - b in T(V )/Gk if and
only if a - b in T (V )/I.
Proof. Since Gk C I, the equivalence class of an
element a in T(V )/I is a superset of the equiva-
lence class of a in T(V )/Gk, which gives the for-
ward implication. The reverse follows from the
lemma above.
In order to define an inner product on
T(V )/Gk, we make use of the result of Berbe-
rian (1961) that if M is a finite-dimensional
linear subspace of a pre-Hilbert space P, then
P = M ® M⊥, where M⊥ is the orthogonal
complement of M in P. In our case this implies
T (V ) = Gk ® G⊥k and that every element
x E T (V ) has a unique decomposition as
x = y + x0k where y E Gk and x0k E G⊥k . This
implies that T(V )/Gk is isomorphic to G⊥k , and
that for each equivalence class [x]k in T(V )/Gk
there is a unique corresponding element x0k E G⊥k
such that x0k E [x]k. This element x0k can be
thought of as the canonical representation of all
elements of [x]k in T(V )/Gk, and can be found
by projecting any element in an equivalence class
onto G⊥k . This enables us to define an inner
product on T(V )/Gk by ([x]k, [y]k)k = (x0k, y0k).
The idea behind working in the quotient algebra
T(V )/I rather than in T (V ) is that the elements
of the ideal capture differences that we wish to ig-
nore, or alternatively, equivalences that we wish to
impose. The equivalence classes in T (V )/I repre-
sent this imposition, and the canonical representa-
tives in I⊥ are elements which ignore the distinc-
tions between elements of the equivalence classes.
</bodyText>
<page confidence="0.996312">
40
</page>
<bodyText confidence="0.99984825">
However, by using Gk, for some k, instead of
the full ideal I, we do not capture some of the
equivalences implied by I. We would, therefore,
like to choose k so that no equivalences of impor-
tance to the sentences we are considering are ig-
nored. While we have not precisely established a
minimal value for k that achieves this, in the dis-
cussion that follows, we set k heuristically as
</bodyText>
<equation confidence="0.992576">
k = l − mindeg(A)
</equation>
<bodyText confidence="0.99997248275862">
where l is the maximum length of the sentences
currently under consideration, and A is the gen-
erating set for the ideal I. The intuition behind
this is that we wish all vectors occurring in A to
have some component in common with the vec-
tor representation of our sentences. Since com-
ponents in the ideal are generated by multipli-
cation (and linearity), in order to allow the ele-
ments of A containing the lowest degree compo-
nents to potentially interact with our sentences,
we will have to allow multiplication of those el-
ements (and all others) by components of degree
up to l − mindeg(A).
Given a finite set A C_ T (V ) of elements gen-
erating the ideal I, to compute canonical repre-
sentations, we first compute a generating set Ak
for Gk following the inductive definition given
earlier, and removing any elements that are not
linearly independent using a standard algorithm.
Using the Gram-Schmidt process (Trefethen and
Bau, 1997), we then calculate an orthonormal ba-
sis A&apos; for Gk, and, by a simple extension of Gram-
Schmidt, compute the projection of a vector u onto
Gk using the basis A&apos;.
We now show how A, the set of vectors gener-
ating the ideal, can be constructed on the basis of
a tree-bank, ensuring that the vectors for any two
strings of the same grammatical type are compa-
rable.
</bodyText>
<sectionHeader confidence="0.999113" genericHeader="method">
4 Data-driven Composition
</sectionHeader>
<bodyText confidence="0.999626411764706">
Suppose we have a tree-bank, its associated tree-
bank grammar G, and a way of associating a con-
text vector with every occurrence of a subtree in
the tree-bank (where the vectors indicate the pres-
ence of features occurring in that particular con-
text). The context vector associated with a spe-
cific occurrence of a subtree in the tree-bank is an
individual context vector.
We assume that for every rule, there is a distin-
guished non-terminal on the right hand side which
we call the head. We also assume that for every
production π there is a linear function φπ from the
space generated by the individual context vectors
of the head to the space generated by the individ-
ual context vectors of the left hand side. When
there is no ambiguity, we simply denote this func-
tion φ.
</bodyText>
<equation confidence="0.668864">
�
Let X be the sum over all individual vectors of
</equation>
<bodyText confidence="0.865694">
subtrees rooted with X in the tree-bank. Similarly,
for each Xj in the right-hand-side of the rule πi :
X —* X1 ... Xr(πi), where r(π) is the rank of π,
let fi,� be the sum over the individual vectors of
those subtrees rooted with Xj where the subtree
occurs as the jth daughter of a local tree involving
the production πi in the tree-bank.
For each rule π : X —* X1 ... Xr with head Xh
we add vectors
</bodyText>
<equation confidence="0.876094">
λπ,i = φ(ei)− X1®...® Xh−1®ei® Xh+1®...®�Xr
</equation>
<bodyText confidence="0.99553852">
for each basis element ei of VXh to the generating
set. The reasoning behind this is to ensure that the
meaning corresponding to a vector associated with
the head of a rule is maintained as it is mapped to
the vector space associated with the left hand side
of the rule.
It is often natural to assume that the individual
context vector of a non-terminal is the same as the
individual context vector of its head. In this case,
we can take φ to be the identity map. In particular,
for a rule of the form π : X —* X1, then λπ,i is
zero.
It is important to note at this point that we have
presented only one of many ways in which a gram-
mar could be used to generate an ideal. In partic-
ular, it is possible to add more vectors to the ideal,
allowing more fine-grained distinctions, for exam-
ple through the use of a lexicalised grammar.
For each sentence w, we compute the tensor
product w� = 61 ® 62 ® · · · ® an where the string
of words a1 ... an form w, and each ai is a vector
in V . For a sentence w we find an element wO of
the orthogonal complement of Gk in T (V ) such
that wO E [ w], where [ w] denotes the equivalence
class of w� given the subspace Gk.
</bodyText>
<sectionHeader confidence="0.996556" genericHeader="evaluation">
5 Example
</sectionHeader>
<bodyText confidence="0.999859333333333">
We show how our formalism applies in a simple
example. Assume we have a corpus which
consists of the following sentences:
</bodyText>
<page confidence="0.998258">
41
</page>
<table confidence="0.9770845">
apple big apple red apple city big city red city book big book red book
apple 1.0 0.26 0.24 0.52 0.13 0.12 0.33 0.086 0.080
big apple 1.0 0.33 0.13 0.52 0.17 0.086 0.33 0.11
red apple 1.0 0.12 0.17 0.52 0.080 0.11 0.33
city 1.0 0.26 0.24 0.0 0.0 0.0
big city 1.0 0.33 0.0 0.0 0.0
red city 1.0 0.0 0.0 0.0
book 1.0 0.26 0.24
big book 1.0 0.33
red book 1.0
</table>
<figureCaption confidence="0.992735">
Figure 1: Similarities between phrases
</figureCaption>
<bodyText confidence="0.9918815">
see red apple see big city
buy apple visit big apple
read big book modernise city
throw old small red book see modern city
buy large new book
together with the following productions.
</bodyText>
<listItem confidence="0.9642355">
1. N&apos; Adj N&apos;
2. N&apos; N
</listItem>
<bodyText confidence="0.999862533333333">
where N and Adj are terminals representing nouns
and adjectives, along with rules for the terminals.
We consider the space of adjective/noun phrases,
generated by N&apos;, and define the individual context
of a noun to be the verb it occurs with, and the in-
dividual context of an adjective to be the noun it
modifies. For each rule, we take 0 to be the iden-
tity map, so the vector spaces associated with N
and N&apos;, and the vector space generated by indi-
vidual contexts of the nouns are all the same. In
this case, the only non-zero vectors which we add
to the ideal are those for the second rule (ignoring
the first rule, since we do not consider verbs in this
example except as contexts), which has the set of
vectors
</bodyText>
<equation confidence="0.736413">
AZ = ez − Adj ® ez
</equation>
<bodyText confidence="0.9999325">
where i ranges over the basis vectors for contexts
of nouns: see, buy, visit, read, modernise, and
</bodyText>
<equation confidence="0.590447">
Adj = 2eapple + 2ebook + eczty
</equation>
<bodyText confidence="0.999699">
In order to compute canonical representations
of vectors, we take k = 1.
</bodyText>
<subsectionHeader confidence="0.93581">
5.1 Discussion
</subsectionHeader>
<bodyText confidence="0.999962185185185">
Figure 1 shows the similarity between the noun
phrases in our sample corpus. Note that the vec-
tors we have put in the generating set describe only
compositionality of meaning — thus for example
the similarity of the non-compositional phrase big
apple to city is purely due to the distributional
similarity between apple and city and composition
with the adjective big.
Our preliminary investigations indicate that the
cosine similarity values are very sensitive to the
particular corpus and features chosen; we are cur-
rently investigating other ways of measuring and
computing similarity.
One interesting feature in the results is how ad-
jectives alter the similarity between nouns. For ex-
ample, red apple and red city have the same sim-
ilarity as apple and city, which is what we would
expect from a pure tensor product. This also ex-
plains why all phrases containing book are disjoint
to those containing city, since the original vector
for book is disjoint to city.
The contribution that the quotient algebra gives
is in comparing the vectors for nouns with those
for noun-adjective phrases. For example, red ap-
ple has components in common with apple, as we
would expect, which would not be the case with
just the tensor product.
</bodyText>
<sectionHeader confidence="0.995255" genericHeader="conclusions">
6 Conclusion and Further Work
</sectionHeader>
<bodyText confidence="0.9998905">
We have presented the outline of a novel approach
to semantic composition that uses quotient alge-
bras to compare vector representations of strings
of different lengths.
</bodyText>
<page confidence="0.996994">
42
</page>
<bodyText confidence="0.999985035714286">
The dimensionality of the construction we use
increases exponentially in the length of the sen-
tence; this is a result of our use of the tensor prod-
uct. This causes a problem for computation us-
ing longer phrases; we hope to address this in fu-
ture work by looking at the representations we use.
For example, product states can be represented in
much lower dimensions by representing them as
products of lower dimensional vectors.
The example we have given would seem to in-
dicate that we intend putting abstract (syntactic)
information about meaning into the set of generat-
ing elements of the ideal. However, there is no rea-
son that more fine-grained aspects of meaning can-
not be incorporated, even to the extent of putting
in vectors for every pair of words. This would
automatically incorporate information about non-
compositionality of meaning. For example, by in-
cluding the vector big apple − big � � apple, we
would expect to capture the fact that the term big
apple is non-compositional, and more similar to
city than we would otherwise expect.
Future work will also include establishing the
implications of varying the constant k and explor-
ing different methods for choosing the set A that
generates the ideal. We are currently preparing
an experimental evaluation of our approach, using
vectors obtained from large corpora.
</bodyText>
<sectionHeader confidence="0.998945" genericHeader="acknowledgments">
7 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999972">
We are grateful to Peter Hines, Stephen Clark, Pe-
ter Lane and Paul Hender for useful discussions.
The first author also wishes to thank Metrica for
supporting this research.
</bodyText>
<sectionHeader confidence="0.990176" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999616098039215">
Jerome R. Bellegarda. 2000. Exploiting latent se-
mantic information in statistical language modeling.
Proceedings of the IEEE, 88(8):1279–1296.
Sterling K. Berberian. 1961. Introduction to Hilbert
Space. Oxford University Press.
Freddy Choi, Peter Wiemer-Hastings, and Johanna
Moore. 2001. Latent Semantic Analysis for text
segmentation. In Proceedings of the 2001 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 109–117.
Stephen Clark and Stephen Pulman. 2007. Combin-
ing symbolic and distributional models of meaning.
In Proceedings of the AAAI Spring Symposium on
Quantum Interaction, pages 52–55, Stanford, CA.
Stephen Clark, Bob Coecke, and Mehrnoosh
Sadrzadeh. 2008. A compositional distribu-
tional model of meaning. In Proceedings of the
Second Quantum Interaction Symposium (QI-2008),
pages 133–140, Oxford, UK.
Daoud Clarke. 2007. Context-theoretic Semantics
for Natural Language: an Algebraic Framework.
Ph.D. thesis, Department of Informatics, University
of Sussex.
Katrin Erk and Sebastian Pado. 2009. Paraphrase as-
sessment in structured vector space: Exploring pa-
rameters and datasets. In Proceedings of the EACL
Workshop on Geometrical Methods for Natural Lan-
guage Semantics (GEMS).
P. W. Foltz, W. Kintsch, and T. K. Landauer. 1998.
The measurement of textual coherence with latent
semantic analysis. Discourse Process, 15:285–307.
Gregory Grefenstette. 1994. Explorations in auto-
matic thesaurus discovery. Kluwer Academic Pub-
lishers, Dordrecht, NL.
Paul Halmos. 1974. Finite dimensional vector spaces.
Springer.
Zellig Harris. 1968. Mathematical Structures of Lan-
guage. Wiley, New York.
W. Kintsch. 2001. Predication. Cognitive Science,
25:173–202.
T. K. Landauer and S. T. Dumais. 1997. A solu-
tion to Plato’s problem: the latent semantic analysis
theory of acquisition, induction and representation
of knowledge. Psychological Review, 104(2):211–
240.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th An-
nual Meeting of the Association for Computational
Linguistics and the 17th International Conference
on Computational Linguistics (COLING-ACL ’98),
pages 768–774, Montreal.
</reference>
<page confidence="0.995546">
43
</page>
<reference confidence="0.999416769230769">
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant word senses in
untagged text. In ACL ’04: Proceedings of the 42nd
Annual Meeting on Association for Computational
Linguistics, page 279, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings
of ACL-08: HLT, pages 236–244, Columbus, Ohio,
June. Association for Computational Linguistics.
Anne Preller and Mehrnoosh Sadrzadeh. 2009. Bell
states and negation in natural languages. In Pro-
ceedings of Quantum Physics and Logic.
Heinrich Sch¨utze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97–
123.
Paul Smolensky. 1990. Tensor product variable bind-
ing and the representation of symbolic structures in
connectionist systems. Artificial Intelligence, 46(1-
2):159–216, November.
Lloyd N. Trefethen and David Bau. 1997. Numerical
Linear Algebra. SIAM.
Dominic Widdows. 2008. Semantic vector products:
Some initial investigations. In Proceedings of the
Second Symposium on Quantum Interaction, Ox-
ford, UK.
</reference>
<page confidence="0.999287">
44
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.383080">
<title confidence="0.999651">Semantic Composition with Quotient Algebras</title>
<author confidence="0.978423">Daoud</author>
<affiliation confidence="0.999804">University of</affiliation>
<address confidence="0.950706">Hatfield,</address>
<email confidence="0.996636">daoud@metrica.net</email>
<author confidence="0.948421">Rudi</author>
<affiliation confidence="0.999882">University of</affiliation>
<address confidence="0.647073">Brighton,</address>
<email confidence="0.997767">rudil@sussex.ac.uk</email>
<author confidence="0.775645">David</author>
<affiliation confidence="0.999717">University of</affiliation>
<address confidence="0.887139">Brighton,</address>
<email confidence="0.999709">davidw@sussex.ac.uk</email>
<abstract confidence="0.998111545454545">We describe an algebraic approach for computing with vector based semantics. The tensor product has been proposed as a method of composition, but has the undesirable property that strings of different length are incomparable. We consider how a quotient algebra of the tensor algebra can allow such comparisons to be made, offering the possibility of data-driven models of semantic composition.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jerome R Bellegarda</author>
</authors>
<title>Exploiting latent semantic information in statistical language modeling.</title>
<date>2000</date>
<booktitle>Proceedings of the IEEE,</booktitle>
<pages>88--8</pages>
<contexts>
<context position="853" citStr="Bellegarda, 2000" startWordPosition="120" endWordPosition="121">.ac.uk Abstract We describe an algebraic approach for computing with vector based semantics. The tensor product has been proposed as a method of composition, but has the undesirable property that strings of different length are incomparable. We consider how a quotient algebra of the tensor algebra can allow such comparisons to be made, offering the possibility of data-driven models of semantic composition. 1 Introduction Vector based techniques have been exploited in a wide array of natural language processing applications (Sch¨utze, 1998; McCarthy et al., 2004; Grefenstette, 1994; Lin, 1998; Bellegarda, 2000; Choi et al., 2001). Techniques such as latent semantic analysis and distributional similarity analyse contexts in which terms occur, building up a vector of features which incorporate aspects of the meaning of the term. This idea has its origins in the distributional hypothesis of Harris (1968), that words with similar meanings will occur in similar contexts, and vice-versa. However, there has been limited attention paid to extending this idea beyond individual words, so that the distributional meaning of phrases and whole sentences can be represented as vectors. While these techniques work </context>
</contexts>
<marker>Bellegarda, 2000</marker>
<rawString>Jerome R. Bellegarda. 2000. Exploiting latent semantic information in statistical language modeling. Proceedings of the IEEE, 88(8):1279–1296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sterling K Berberian</author>
</authors>
<title>Introduction to Hilbert Space.</title>
<date>1961</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="11231" citStr="Berberian (1961)" startWordPosition="2171" endWordPosition="2173">d we know x E Gk for all k &gt; k0. In particular x E Gk for k &gt; d − mindeg(A). We show that T(V )/Gk (for an appropriate choice of k) captures the essential features of T(V )/I in terms of equivalence: Proposition. Let deg(a − b) = d and let k &gt; d − mindeg(A). Then a - b in T(V )/Gk if and only if a - b in T (V )/I. Proof. Since Gk C I, the equivalence class of an element a in T(V )/I is a superset of the equivalence class of a in T(V )/Gk, which gives the forward implication. The reverse follows from the lemma above. In order to define an inner product on T(V )/Gk, we make use of the result of Berberian (1961) that if M is a finite-dimensional linear subspace of a pre-Hilbert space P, then P = M ® M⊥, where M⊥ is the orthogonal complement of M in P. In our case this implies T (V ) = Gk ® G⊥k and that every element x E T (V ) has a unique decomposition as x = y + x0k where y E Gk and x0k E G⊥k . This implies that T(V )/Gk is isomorphic to G⊥k , and that for each equivalence class [x]k in T(V )/Gk there is a unique corresponding element x0k E G⊥k such that x0k E [x]k. This element x0k can be thought of as the canonical representation of all elements of [x]k in T(V )/Gk, and can be found by projecting</context>
</contexts>
<marker>Berberian, 1961</marker>
<rawString>Sterling K. Berberian. 1961. Introduction to Hilbert Space. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Freddy Choi</author>
<author>Peter Wiemer-Hastings</author>
<author>Johanna Moore</author>
</authors>
<title>Latent Semantic Analysis for text segmentation.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>109--117</pages>
<contexts>
<context position="873" citStr="Choi et al., 2001" startWordPosition="122" endWordPosition="125"> describe an algebraic approach for computing with vector based semantics. The tensor product has been proposed as a method of composition, but has the undesirable property that strings of different length are incomparable. We consider how a quotient algebra of the tensor algebra can allow such comparisons to be made, offering the possibility of data-driven models of semantic composition. 1 Introduction Vector based techniques have been exploited in a wide array of natural language processing applications (Sch¨utze, 1998; McCarthy et al., 2004; Grefenstette, 1994; Lin, 1998; Bellegarda, 2000; Choi et al., 2001). Techniques such as latent semantic analysis and distributional similarity analyse contexts in which terms occur, building up a vector of features which incorporate aspects of the meaning of the term. This idea has its origins in the distributional hypothesis of Harris (1968), that words with similar meanings will occur in similar contexts, and vice-versa. However, there has been limited attention paid to extending this idea beyond individual words, so that the distributional meaning of phrases and whole sentences can be represented as vectors. While these techniques work well at the word lev</context>
</contexts>
<marker>Choi, Wiemer-Hastings, Moore, 2001</marker>
<rawString>Freddy Choi, Peter Wiemer-Hastings, and Johanna Moore. 2001. Latent Semantic Analysis for text segmentation. In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing, pages 109–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>Stephen Pulman</author>
</authors>
<title>Combining symbolic and distributional models of meaning.</title>
<date>2007</date>
<booktitle>In Proceedings of the AAAI Spring Symposium on Quantum Interaction,</booktitle>
<pages>52--55</pages>
<location>Stanford, CA.</location>
<contexts>
<context position="2019" citStr="Clark and Pulman, 2007" startWordPosition="303" endWordPosition="306"> be represented as vectors. While these techniques work well at the word level, for longer strings, data becomes extremely sparse. This has led to various proposals exploring methods for composing vectors, rather than deriving them directly from the data (Landauer and Dumais, 1997; Foltz et al., 1998; Kintsch, 2001; Widdows, 2008; Clark et al., 2008; Mitchell and Lapata, 2008; Erk and Pado, 2009; Preller and Sadrzadeh, 2009). Many of these approaches use a pre-defined composition operation such as addition (Landauer and Dumais, 1997; Foltz et al., 1998) or the tensor product (Smolensky, 1990; Clark and Pulman, 2007; Widdows, 2008) which contrasts with the data-driven definition of composition developed here. 2 Tensor Algebras Following the context-theoretic semantics of Clarke (2007), we take the meaning of strings as being described by a multiplication on a vector space that is bilinear with respect to the addition of the vector space, i.e. x(y + z) = xy + xz (x + y)z = xz + yz It is assumed that the multiplication is associative, but not commutative. The resulting structure is an associative algebra over a field — or simply an algebra when there is no ambiguity. One commonly used bilinear multiplicati</context>
</contexts>
<marker>Clark, Pulman, 2007</marker>
<rawString>Stephen Clark and Stephen Pulman. 2007. Combining symbolic and distributional models of meaning. In Proceedings of the AAAI Spring Symposium on Quantum Interaction, pages 52–55, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>Bob Coecke</author>
<author>Mehrnoosh Sadrzadeh</author>
</authors>
<title>A compositional distributional model of meaning.</title>
<date>2008</date>
<booktitle>In Proceedings of the Second Quantum Interaction Symposium (QI-2008),</booktitle>
<pages>133--140</pages>
<location>Oxford, UK.</location>
<contexts>
<context position="1748" citStr="Clark et al., 2008" startWordPosition="260" endWordPosition="263">sis of Harris (1968), that words with similar meanings will occur in similar contexts, and vice-versa. However, there has been limited attention paid to extending this idea beyond individual words, so that the distributional meaning of phrases and whole sentences can be represented as vectors. While these techniques work well at the word level, for longer strings, data becomes extremely sparse. This has led to various proposals exploring methods for composing vectors, rather than deriving them directly from the data (Landauer and Dumais, 1997; Foltz et al., 1998; Kintsch, 2001; Widdows, 2008; Clark et al., 2008; Mitchell and Lapata, 2008; Erk and Pado, 2009; Preller and Sadrzadeh, 2009). Many of these approaches use a pre-defined composition operation such as addition (Landauer and Dumais, 1997; Foltz et al., 1998) or the tensor product (Smolensky, 1990; Clark and Pulman, 2007; Widdows, 2008) which contrasts with the data-driven definition of composition developed here. 2 Tensor Algebras Following the context-theoretic semantics of Clarke (2007), we take the meaning of strings as being described by a multiplication on a vector space that is bilinear with respect to the addition of the vector space, </context>
</contexts>
<marker>Clark, Coecke, Sadrzadeh, 2008</marker>
<rawString>Stephen Clark, Bob Coecke, and Mehrnoosh Sadrzadeh. 2008. A compositional distributional model of meaning. In Proceedings of the Second Quantum Interaction Symposium (QI-2008), pages 133–140, Oxford, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daoud Clarke</author>
</authors>
<title>Context-theoretic Semantics for Natural Language: an Algebraic Framework.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Informatics, University of Sussex.</institution>
<contexts>
<context position="2191" citStr="Clarke (2007)" startWordPosition="328" endWordPosition="329">s for composing vectors, rather than deriving them directly from the data (Landauer and Dumais, 1997; Foltz et al., 1998; Kintsch, 2001; Widdows, 2008; Clark et al., 2008; Mitchell and Lapata, 2008; Erk and Pado, 2009; Preller and Sadrzadeh, 2009). Many of these approaches use a pre-defined composition operation such as addition (Landauer and Dumais, 1997; Foltz et al., 1998) or the tensor product (Smolensky, 1990; Clark and Pulman, 2007; Widdows, 2008) which contrasts with the data-driven definition of composition developed here. 2 Tensor Algebras Following the context-theoretic semantics of Clarke (2007), we take the meaning of strings as being described by a multiplication on a vector space that is bilinear with respect to the addition of the vector space, i.e. x(y + z) = xy + xz (x + y)z = xz + yz It is assumed that the multiplication is associative, but not commutative. The resulting structure is an associative algebra over a field — or simply an algebra when there is no ambiguity. One commonly used bilinear multiplication operator on vector spaces is the tensor product (denoted ⊗), whose use as a method of combining meaning was first proposed by Smolensky (1990), and has been considered m</context>
</contexts>
<marker>Clarke, 2007</marker>
<rawString>Daoud Clarke. 2007. Context-theoretic Semantics for Natural Language: an Algebraic Framework. Ph.D. thesis, Department of Informatics, University of Sussex.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Pado</author>
</authors>
<title>Paraphrase assessment in structured vector space: Exploring parameters and datasets.</title>
<date>2009</date>
<booktitle>In Proceedings of the EACL Workshop on Geometrical Methods for Natural Language Semantics (GEMS).</booktitle>
<contexts>
<context position="1795" citStr="Erk and Pado, 2009" startWordPosition="268" endWordPosition="271">eanings will occur in similar contexts, and vice-versa. However, there has been limited attention paid to extending this idea beyond individual words, so that the distributional meaning of phrases and whole sentences can be represented as vectors. While these techniques work well at the word level, for longer strings, data becomes extremely sparse. This has led to various proposals exploring methods for composing vectors, rather than deriving them directly from the data (Landauer and Dumais, 1997; Foltz et al., 1998; Kintsch, 2001; Widdows, 2008; Clark et al., 2008; Mitchell and Lapata, 2008; Erk and Pado, 2009; Preller and Sadrzadeh, 2009). Many of these approaches use a pre-defined composition operation such as addition (Landauer and Dumais, 1997; Foltz et al., 1998) or the tensor product (Smolensky, 1990; Clark and Pulman, 2007; Widdows, 2008) which contrasts with the data-driven definition of composition developed here. 2 Tensor Algebras Following the context-theoretic semantics of Clarke (2007), we take the meaning of strings as being described by a multiplication on a vector space that is bilinear with respect to the addition of the vector space, i.e. x(y + z) = xy + xz (x + y)z = xz + yz It i</context>
</contexts>
<marker>Erk, Pado, 2009</marker>
<rawString>Katrin Erk and Sebastian Pado. 2009. Paraphrase assessment in structured vector space: Exploring parameters and datasets. In Proceedings of the EACL Workshop on Geometrical Methods for Natural Language Semantics (GEMS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P W Foltz</author>
<author>W Kintsch</author>
<author>T K Landauer</author>
</authors>
<title>The measurement of textual coherence with latent semantic analysis.</title>
<date>1998</date>
<booktitle>Discourse Process,</booktitle>
<pages>15--285</pages>
<contexts>
<context position="1698" citStr="Foltz et al., 1998" startWordPosition="252" endWordPosition="255">idea has its origins in the distributional hypothesis of Harris (1968), that words with similar meanings will occur in similar contexts, and vice-versa. However, there has been limited attention paid to extending this idea beyond individual words, so that the distributional meaning of phrases and whole sentences can be represented as vectors. While these techniques work well at the word level, for longer strings, data becomes extremely sparse. This has led to various proposals exploring methods for composing vectors, rather than deriving them directly from the data (Landauer and Dumais, 1997; Foltz et al., 1998; Kintsch, 2001; Widdows, 2008; Clark et al., 2008; Mitchell and Lapata, 2008; Erk and Pado, 2009; Preller and Sadrzadeh, 2009). Many of these approaches use a pre-defined composition operation such as addition (Landauer and Dumais, 1997; Foltz et al., 1998) or the tensor product (Smolensky, 1990; Clark and Pulman, 2007; Widdows, 2008) which contrasts with the data-driven definition of composition developed here. 2 Tensor Algebras Following the context-theoretic semantics of Clarke (2007), we take the meaning of strings as being described by a multiplication on a vector space that is bilinear </context>
</contexts>
<marker>Foltz, Kintsch, Landauer, 1998</marker>
<rawString>P. W. Foltz, W. Kintsch, and T. K. Landauer. 1998. The measurement of textual coherence with latent semantic analysis. Discourse Process, 15:285–307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<title>Explorations in automatic thesaurus discovery.</title>
<date>1994</date>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Dordrecht, NL.</location>
<contexts>
<context position="824" citStr="Grefenstette, 1994" startWordPosition="116" endWordPosition="117">ssex Brighton, UK davidw@sussex.ac.uk Abstract We describe an algebraic approach for computing with vector based semantics. The tensor product has been proposed as a method of composition, but has the undesirable property that strings of different length are incomparable. We consider how a quotient algebra of the tensor algebra can allow such comparisons to be made, offering the possibility of data-driven models of semantic composition. 1 Introduction Vector based techniques have been exploited in a wide array of natural language processing applications (Sch¨utze, 1998; McCarthy et al., 2004; Grefenstette, 1994; Lin, 1998; Bellegarda, 2000; Choi et al., 2001). Techniques such as latent semantic analysis and distributional similarity analyse contexts in which terms occur, building up a vector of features which incorporate aspects of the meaning of the term. This idea has its origins in the distributional hypothesis of Harris (1968), that words with similar meanings will occur in similar contexts, and vice-versa. However, there has been limited attention paid to extending this idea beyond individual words, so that the distributional meaning of phrases and whole sentences can be represented as vectors.</context>
</contexts>
<marker>Grefenstette, 1994</marker>
<rawString>Gregory Grefenstette. 1994. Explorations in automatic thesaurus discovery. Kluwer Academic Publishers, Dordrecht, NL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Halmos</author>
</authors>
<title>Finite dimensional vector spaces.</title>
<date>1974</date>
<publisher>Springer.</publisher>
<contexts>
<context position="3054" citStr="Halmos, 1974" startWordPosition="483" endWordPosition="484">ve, but not commutative. The resulting structure is an associative algebra over a field — or simply an algebra when there is no ambiguity. One commonly used bilinear multiplication operator on vector spaces is the tensor product (denoted ⊗), whose use as a method of combining meaning was first proposed by Smolensky (1990), and has been considered more recently by Clark and Pulman (2007) and Widdows (2008), who also looked at the direct sum (which Widdows calls the direct product, denoted ⊕). We give a very brief account of the tensor product and direct sum in the finite-dimensional case; see (Halmos, 1974) for formal and complete definitions. Roughly speaking, if u1, u2,... un form an orthonormal basis for a vector space U and v1, v2,... vm form an orthonormal basis for vector space V , then the space U ⊗ V has dimensionality nm with an orthonormal basis formed by the set of all ordered pairs (ui, vj), denoted by ui ⊗ vj, of the individual basis elements. For arbitrary elements u = Eni=1 αiui and v = Emj=1 βjvj the tensor product of u and v is then given by u ⊗ v = n �m αiβj ui ⊗ vj i j 38 Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics, ACL 2010, pages 38–4</context>
</contexts>
<marker>Halmos, 1974</marker>
<rawString>Paul Halmos. 1974. Finite dimensional vector spaces. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig Harris</author>
</authors>
<date>1968</date>
<booktitle>Mathematical Structures of Language.</booktitle>
<publisher>Wiley,</publisher>
<location>New York.</location>
<contexts>
<context position="1150" citStr="Harris (1968)" startWordPosition="168" endWordPosition="169">an allow such comparisons to be made, offering the possibility of data-driven models of semantic composition. 1 Introduction Vector based techniques have been exploited in a wide array of natural language processing applications (Sch¨utze, 1998; McCarthy et al., 2004; Grefenstette, 1994; Lin, 1998; Bellegarda, 2000; Choi et al., 2001). Techniques such as latent semantic analysis and distributional similarity analyse contexts in which terms occur, building up a vector of features which incorporate aspects of the meaning of the term. This idea has its origins in the distributional hypothesis of Harris (1968), that words with similar meanings will occur in similar contexts, and vice-versa. However, there has been limited attention paid to extending this idea beyond individual words, so that the distributional meaning of phrases and whole sentences can be represented as vectors. While these techniques work well at the word level, for longer strings, data becomes extremely sparse. This has led to various proposals exploring methods for composing vectors, rather than deriving them directly from the data (Landauer and Dumais, 1997; Foltz et al., 1998; Kintsch, 2001; Widdows, 2008; Clark et al., 2008; </context>
</contexts>
<marker>Harris, 1968</marker>
<rawString>Zellig Harris. 1968. Mathematical Structures of Language. Wiley, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Kintsch</author>
</authors>
<date>2001</date>
<journal>Predication. Cognitive Science,</journal>
<pages>25--173</pages>
<contexts>
<context position="1713" citStr="Kintsch, 2001" startWordPosition="256" endWordPosition="257"> in the distributional hypothesis of Harris (1968), that words with similar meanings will occur in similar contexts, and vice-versa. However, there has been limited attention paid to extending this idea beyond individual words, so that the distributional meaning of phrases and whole sentences can be represented as vectors. While these techniques work well at the word level, for longer strings, data becomes extremely sparse. This has led to various proposals exploring methods for composing vectors, rather than deriving them directly from the data (Landauer and Dumais, 1997; Foltz et al., 1998; Kintsch, 2001; Widdows, 2008; Clark et al., 2008; Mitchell and Lapata, 2008; Erk and Pado, 2009; Preller and Sadrzadeh, 2009). Many of these approaches use a pre-defined composition operation such as addition (Landauer and Dumais, 1997; Foltz et al., 1998) or the tensor product (Smolensky, 1990; Clark and Pulman, 2007; Widdows, 2008) which contrasts with the data-driven definition of composition developed here. 2 Tensor Algebras Following the context-theoretic semantics of Clarke (2007), we take the meaning of strings as being described by a multiplication on a vector space that is bilinear with respect to</context>
</contexts>
<marker>Kintsch, 2001</marker>
<rawString>W. Kintsch. 2001. Predication. Cognitive Science, 25:173–202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T K Landauer</author>
<author>S T Dumais</author>
</authors>
<title>A solution to Plato’s problem: the latent semantic analysis theory of acquisition, induction and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<issue>2</issue>
<pages>240</pages>
<contexts>
<context position="1678" citStr="Landauer and Dumais, 1997" startWordPosition="248" endWordPosition="251"> meaning of the term. This idea has its origins in the distributional hypothesis of Harris (1968), that words with similar meanings will occur in similar contexts, and vice-versa. However, there has been limited attention paid to extending this idea beyond individual words, so that the distributional meaning of phrases and whole sentences can be represented as vectors. While these techniques work well at the word level, for longer strings, data becomes extremely sparse. This has led to various proposals exploring methods for composing vectors, rather than deriving them directly from the data (Landauer and Dumais, 1997; Foltz et al., 1998; Kintsch, 2001; Widdows, 2008; Clark et al., 2008; Mitchell and Lapata, 2008; Erk and Pado, 2009; Preller and Sadrzadeh, 2009). Many of these approaches use a pre-defined composition operation such as addition (Landauer and Dumais, 1997; Foltz et al., 1998) or the tensor product (Smolensky, 1990; Clark and Pulman, 2007; Widdows, 2008) which contrasts with the data-driven definition of composition developed here. 2 Tensor Algebras Following the context-theoretic semantics of Clarke (2007), we take the meaning of strings as being described by a multiplication on a vector spa</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>T. K. Landauer and S. T. Dumais. 1997. A solution to Plato’s problem: the latent semantic analysis theory of acquisition, induction and representation of knowledge. Psychological Review, 104(2):211– 240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and the 17th International Conference on Computational Linguistics (COLING-ACL ’98),</booktitle>
<pages>768--774</pages>
<location>Montreal.</location>
<contexts>
<context position="835" citStr="Lin, 1998" startWordPosition="118" endWordPosition="119">vidw@sussex.ac.uk Abstract We describe an algebraic approach for computing with vector based semantics. The tensor product has been proposed as a method of composition, but has the undesirable property that strings of different length are incomparable. We consider how a quotient algebra of the tensor algebra can allow such comparisons to be made, offering the possibility of data-driven models of semantic composition. 1 Introduction Vector based techniques have been exploited in a wide array of natural language processing applications (Sch¨utze, 1998; McCarthy et al., 2004; Grefenstette, 1994; Lin, 1998; Bellegarda, 2000; Choi et al., 2001). Techniques such as latent semantic analysis and distributional similarity analyse contexts in which terms occur, building up a vector of features which incorporate aspects of the meaning of the term. This idea has its origins in the distributional hypothesis of Harris (1968), that words with similar meanings will occur in similar contexts, and vice-versa. However, there has been limited attention paid to extending this idea beyond individual words, so that the distributional meaning of phrases and whole sentences can be represented as vectors. While thes</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and the 17th International Conference on Computational Linguistics (COLING-ACL ’98), pages 768–774, Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Rob Koeling</author>
<author>Julie Weeds</author>
<author>John Carroll</author>
</authors>
<title>Finding predominant word senses in untagged text.</title>
<date>2004</date>
<booktitle>In ACL ’04: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>279</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="804" citStr="McCarthy et al., 2004" startWordPosition="112" endWordPosition="115">d Weir University of Sussex Brighton, UK davidw@sussex.ac.uk Abstract We describe an algebraic approach for computing with vector based semantics. The tensor product has been proposed as a method of composition, but has the undesirable property that strings of different length are incomparable. We consider how a quotient algebra of the tensor algebra can allow such comparisons to be made, offering the possibility of data-driven models of semantic composition. 1 Introduction Vector based techniques have been exploited in a wide array of natural language processing applications (Sch¨utze, 1998; McCarthy et al., 2004; Grefenstette, 1994; Lin, 1998; Bellegarda, 2000; Choi et al., 2001). Techniques such as latent semantic analysis and distributional similarity analyse contexts in which terms occur, building up a vector of features which incorporate aspects of the meaning of the term. This idea has its origins in the distributional hypothesis of Harris (1968), that words with similar meanings will occur in similar contexts, and vice-versa. However, there has been limited attention paid to extending this idea beyond individual words, so that the distributional meaning of phrases and whole sentences can be rep</context>
</contexts>
<marker>McCarthy, Koeling, Weeds, Carroll, 2004</marker>
<rawString>Diana McCarthy, Rob Koeling, Julie Weeds, and John Carroll. 2004. Finding predominant word senses in untagged text. In ACL ’04: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, page 279, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Vector-based models of semantic composition.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>236--244</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="1775" citStr="Mitchell and Lapata, 2008" startWordPosition="264" endWordPosition="267">, that words with similar meanings will occur in similar contexts, and vice-versa. However, there has been limited attention paid to extending this idea beyond individual words, so that the distributional meaning of phrases and whole sentences can be represented as vectors. While these techniques work well at the word level, for longer strings, data becomes extremely sparse. This has led to various proposals exploring methods for composing vectors, rather than deriving them directly from the data (Landauer and Dumais, 1997; Foltz et al., 1998; Kintsch, 2001; Widdows, 2008; Clark et al., 2008; Mitchell and Lapata, 2008; Erk and Pado, 2009; Preller and Sadrzadeh, 2009). Many of these approaches use a pre-defined composition operation such as addition (Landauer and Dumais, 1997; Foltz et al., 1998) or the tensor product (Smolensky, 1990; Clark and Pulman, 2007; Widdows, 2008) which contrasts with the data-driven definition of composition developed here. 2 Tensor Algebras Following the context-theoretic semantics of Clarke (2007), we take the meaning of strings as being described by a multiplication on a vector space that is bilinear with respect to the addition of the vector space, i.e. x(y + z) = xy + xz (x </context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. In Proceedings of ACL-08: HLT, pages 236–244, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anne Preller</author>
<author>Mehrnoosh Sadrzadeh</author>
</authors>
<title>Bell states and negation in natural languages.</title>
<date>2009</date>
<booktitle>In Proceedings of Quantum Physics and Logic.</booktitle>
<contexts>
<context position="1825" citStr="Preller and Sadrzadeh, 2009" startWordPosition="272" endWordPosition="275">n similar contexts, and vice-versa. However, there has been limited attention paid to extending this idea beyond individual words, so that the distributional meaning of phrases and whole sentences can be represented as vectors. While these techniques work well at the word level, for longer strings, data becomes extremely sparse. This has led to various proposals exploring methods for composing vectors, rather than deriving them directly from the data (Landauer and Dumais, 1997; Foltz et al., 1998; Kintsch, 2001; Widdows, 2008; Clark et al., 2008; Mitchell and Lapata, 2008; Erk and Pado, 2009; Preller and Sadrzadeh, 2009). Many of these approaches use a pre-defined composition operation such as addition (Landauer and Dumais, 1997; Foltz et al., 1998) or the tensor product (Smolensky, 1990; Clark and Pulman, 2007; Widdows, 2008) which contrasts with the data-driven definition of composition developed here. 2 Tensor Algebras Following the context-theoretic semantics of Clarke (2007), we take the meaning of strings as being described by a multiplication on a vector space that is bilinear with respect to the addition of the vector space, i.e. x(y + z) = xy + xz (x + y)z = xz + yz It is assumed that the multiplicat</context>
</contexts>
<marker>Preller, Sadrzadeh, 2009</marker>
<rawString>Anne Preller and Mehrnoosh Sadrzadeh. 2009. Bell states and negation in natural languages. In Proceedings of Quantum Physics and Logic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heinrich Sch¨utze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<pages>123</pages>
<marker>Sch¨utze, 1998</marker>
<rawString>Heinrich Sch¨utze. 1998. Automatic word sense discrimination. Computational Linguistics, 24(1):97– 123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Smolensky</author>
</authors>
<title>Tensor product variable binding and the representation of symbolic structures in connectionist systems.</title>
<date>1990</date>
<journal>Artificial Intelligence,</journal>
<pages>46--1</pages>
<contexts>
<context position="1995" citStr="Smolensky, 1990" startWordPosition="301" endWordPosition="302">ole sentences can be represented as vectors. While these techniques work well at the word level, for longer strings, data becomes extremely sparse. This has led to various proposals exploring methods for composing vectors, rather than deriving them directly from the data (Landauer and Dumais, 1997; Foltz et al., 1998; Kintsch, 2001; Widdows, 2008; Clark et al., 2008; Mitchell and Lapata, 2008; Erk and Pado, 2009; Preller and Sadrzadeh, 2009). Many of these approaches use a pre-defined composition operation such as addition (Landauer and Dumais, 1997; Foltz et al., 1998) or the tensor product (Smolensky, 1990; Clark and Pulman, 2007; Widdows, 2008) which contrasts with the data-driven definition of composition developed here. 2 Tensor Algebras Following the context-theoretic semantics of Clarke (2007), we take the meaning of strings as being described by a multiplication on a vector space that is bilinear with respect to the addition of the vector space, i.e. x(y + z) = xy + xz (x + y)z = xz + yz It is assumed that the multiplication is associative, but not commutative. The resulting structure is an associative algebra over a field — or simply an algebra when there is no ambiguity. One commonly us</context>
</contexts>
<marker>Smolensky, 1990</marker>
<rawString>Paul Smolensky. 1990. Tensor product variable binding and the representation of symbolic structures in connectionist systems. Artificial Intelligence, 46(1-2):159–216, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lloyd N Trefethen</author>
<author>David Bau</author>
</authors>
<title>Numerical Linear Algebra.</title>
<date>1997</date>
<publisher>SIAM.</publisher>
<contexts>
<context position="13706" citStr="Trefethen and Bau, 1997" startWordPosition="2635" endWordPosition="2638">enerated by multiplication (and linearity), in order to allow the elements of A containing the lowest degree components to potentially interact with our sentences, we will have to allow multiplication of those elements (and all others) by components of degree up to l − mindeg(A). Given a finite set A C_ T (V ) of elements generating the ideal I, to compute canonical representations, we first compute a generating set Ak for Gk following the inductive definition given earlier, and removing any elements that are not linearly independent using a standard algorithm. Using the Gram-Schmidt process (Trefethen and Bau, 1997), we then calculate an orthonormal basis A&apos; for Gk, and, by a simple extension of GramSchmidt, compute the projection of a vector u onto Gk using the basis A&apos;. We now show how A, the set of vectors generating the ideal, can be constructed on the basis of a tree-bank, ensuring that the vectors for any two strings of the same grammatical type are comparable. 4 Data-driven Composition Suppose we have a tree-bank, its associated treebank grammar G, and a way of associating a context vector with every occurrence of a subtree in the tree-bank (where the vectors indicate the presence of features occu</context>
</contexts>
<marker>Trefethen, Bau, 1997</marker>
<rawString>Lloyd N. Trefethen and David Bau. 1997. Numerical Linear Algebra. SIAM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominic Widdows</author>
</authors>
<title>Semantic vector products: Some initial investigations.</title>
<date>2008</date>
<booktitle>In Proceedings of the Second Symposium on Quantum Interaction,</booktitle>
<location>Oxford, UK.</location>
<contexts>
<context position="1728" citStr="Widdows, 2008" startWordPosition="258" endWordPosition="259">utional hypothesis of Harris (1968), that words with similar meanings will occur in similar contexts, and vice-versa. However, there has been limited attention paid to extending this idea beyond individual words, so that the distributional meaning of phrases and whole sentences can be represented as vectors. While these techniques work well at the word level, for longer strings, data becomes extremely sparse. This has led to various proposals exploring methods for composing vectors, rather than deriving them directly from the data (Landauer and Dumais, 1997; Foltz et al., 1998; Kintsch, 2001; Widdows, 2008; Clark et al., 2008; Mitchell and Lapata, 2008; Erk and Pado, 2009; Preller and Sadrzadeh, 2009). Many of these approaches use a pre-defined composition operation such as addition (Landauer and Dumais, 1997; Foltz et al., 1998) or the tensor product (Smolensky, 1990; Clark and Pulman, 2007; Widdows, 2008) which contrasts with the data-driven definition of composition developed here. 2 Tensor Algebras Following the context-theoretic semantics of Clarke (2007), we take the meaning of strings as being described by a multiplication on a vector space that is bilinear with respect to the addition o</context>
</contexts>
<marker>Widdows, 2008</marker>
<rawString>Dominic Widdows. 2008. Semantic vector products: Some initial investigations. In Proceedings of the Second Symposium on Quantum Interaction, Oxford, UK.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>