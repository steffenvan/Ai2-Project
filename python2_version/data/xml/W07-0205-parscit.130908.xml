<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001624">
<title confidence="0.996073">
Unigram Language Models using Diffusion Smoothing over Graphs
</title>
<author confidence="0.997558">
Bruno Jedynak
</author>
<affiliation confidence="0.919107">
Dept. of Appl. Mathematics and Statistics
Center for Imaging Sciences
Johns Hopkins University
</affiliation>
<address confidence="0.948401">
Baltimore, MD 21218-2686
</address>
<email confidence="0.998969">
bruno.jedynak@jhu.edu
</email>
<author confidence="0.97327">
Damianos Karakos
</author>
<affiliation confidence="0.896541333333333">
Dept. of Electrical and Computer Engineering
Center for Language and Speech Processing
Johns Hopkins University
</affiliation>
<address confidence="0.948592">
Baltimore, MD 21218-2686
</address>
<email confidence="0.998459">
damianos@jhu.edu
</email>
<sectionHeader confidence="0.995629" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999433">
We propose to use graph-based diffusion
techniques with data-dependent kernels
to build unigram language models. Our
approach entails building graphs, where
each vertex corresponds uniquely to a
word from a closed vocabulary, and the
existence of an edge (with an appropri-
ate weight) between two words indicates
some form of similarity between them. In
one of our constructions, we place an edge
between two words if the number of times
these words were seen in a training set
differs by at most one count. This graph
construction results in a similarity ma-
trix with small intrinsic dimension, since
words with the same counts have the same
neighbors. Experimental results from a
benchmark task from language modeling
show that our method is competitive with
the Good-Turing estimator.
</bodyText>
<sectionHeader confidence="0.925349" genericHeader="method">
1 Diffusion over Graphs
</sectionHeader>
<subsectionHeader confidence="0.856876">
1.1 Notation
</subsectionHeader>
<bodyText confidence="0.999985476190477">
Let G = (V, E) be an undirected graph, where V
is a finite set of vertices, and E C V x V is the
set of edges. Also, let V be a vocabulary of words,
whose probabilities we want to estimate. Each ver-
tex corresponds uniquely to a word, i.e., there is a
one-to-one mapping between V and V . Without loss
of generality, we will use V to denote both the set
of words and the set of vertices. Moreover, to sim-
plify notation, we assume that the letters x, y, z will
always denote vertices of G.
The existence of an edge between x, y will be
denoted by x — y. We assume that the graph
is strongly connected (i.e., there is a path between
any two vertices). Furthermore, we define a non-
negative real valued function w over V x V , which
plays the role of the similarity between two words
(the higher the value of w(x, y), the more similar
words x, y are). In the experimental results section,
we will compare different measures of similarity be-
tween words which will result in different smooth-
ing algorithms. The degree of a vertex is defined as
</bodyText>
<equation confidence="0.973009">
d(x) = � w(x, y). (1)
yEV:x—y
</equation>
<bodyText confidence="0.999609">
We assume that for any vertex x, d(x) &gt; 0; that is,
every word is similar to at least some other word.
</bodyText>
<subsectionHeader confidence="0.999229">
1.2 Smoothing by Normalized Diffusion
</subsectionHeader>
<bodyText confidence="0.999971">
The setting described here was introduced in (Szlam
et al., 2006). First, we define a Markov chain {XtI,
which corresponds to a random walk over the graph
G. Its initial value is equal to X0, which has dis-
tribution 7r0. (Although 7r0 can be chosen arbitrar-
ily, we assume in this paper that it is equal to the
empirical, unsmoothed, distribution of words over a
training set.) We then define the transition matrix as
follows:
</bodyText>
<equation confidence="0.8338105">
T (x, y) = P(X1 = y|X0 = x) = d−1(x)w(x, y).
(2)
</equation>
<bodyText confidence="0.992942">
This transition matrix, together with 7r0, induces a
distribution over V , which is equal to the distribu-
</bodyText>
<page confidence="0.976317">
33
</page>
<subsubsectionHeader confidence="0.313918">
TextGraphs-2: Graph-Based Al orithms for Natural Language Processing, pages 33–36,
</subsubsectionHeader>
<bodyText confidence="0.402015">
Rochester, April 2007 (c 2007 Association for Computational Linguistics
</bodyText>
<equation confidence="0.921198333333333">
tion π1 of X1:
Eπ1(y) = T(x, y)π0(x). (3)
xEV
</equation>
<bodyText confidence="0.999943">
This distribution can be construed as a smoothed
version of π0, since the π1 probability of an un-
seen word will always be non-zero, if it has a non-
zero similarity to a seen word. In the same way, a
whole sequence of distributions π2, π3,... can be
computed; we only consider π1 as our smoothed es-
timate in this paper. (One may wonder whether the
stationary distribution of this Markov chain, i.e., the
limiting distribution of Xt, as t —* oc, has any sig-
nificance; we do not address this question here, as
this limiting distribution may have very little depen-
dence on π0 in the Markov chain cases under con-
sideration.)
</bodyText>
<subsectionHeader confidence="0.993911">
1.3 Smoothing by Kernel Diffusion
</subsectionHeader>
<bodyText confidence="0.9844615">
We assume here that for any vertex x, w(x, x) = 0
and that w is symmetric. Following (Kondor and
Lafferty, 2002), we define the following matrix over
VxV
</bodyText>
<equation confidence="0.936877">
H(x, y) = w(x, y)δ(x — y) — d(x)δ(x = y), (4)
</equation>
<bodyText confidence="0.998282333333333">
where δ(u) is the delta function which takes the
value 1 if property u is true, and 0 otherwise. The
negative of the matrix H is called the Laplacian of
the graph and plays a central role in spectral graph
theory (Chung, 1997). We further define the heat
equation over the graph G as
</bodyText>
<equation confidence="0.953793">
∂ Kt = HKt, t &gt; 0, (5)
∂t
</equation>
<bodyText confidence="0.98641285">
with initial condition K0 = I, where Kt is a time-
dependent square matrix of same dimension as H,
and I is the identity matrix. Kt(x, y) can be inter-
preted as the amount of heat that reaches vertex x
at time t, when starting with a unit amount of heat
concentrated at y. Using (1) and (4), the right hand
side of (5) expands to
HKt(x, y) = E w(x, z) (Kt(z, y) — Kt(x, y)) .
z:z—x
(6)
From this equation, we see that the amount of heat
at x will increase (resp. decrease) if the current
amount of heat at x (namely Kt(x, y)) is smaller
(resp. larger) than the weighted average amount of
heat at the neighbors of x, thus causing the system
to reach a steady state.
The heat equation (5) has a unique solution which
is the matrix exponential Kt = exp(tH), (see (Kon-
dor and Lafferty, 2002)) and which can be defined
equivalently as
</bodyText>
<equation confidence="0.995909">
etH = lim CI + tH 1n (7)
n→+oo \ n J
or as
etH = I + tH + t2 2!H2 + t3 3!H3 + ··· (8)
</equation>
<bodyText confidence="0.999182714285714">
Moreover, if the initial condition is replaced by
K0(x, y) = π0(x)δ(x = y) then the solution of
the heat equation is given by the matrix product
π1 = Ktπ0. In the following, π0 will be the em-
pirical distribution over the training set and t will be
chosen by trial and error. As before, π1 will provide
a smoothed version of π0.
</bodyText>
<sectionHeader confidence="0.949716" genericHeader="method">
2 Unigram Language Models
</sectionHeader>
<bodyText confidence="0.999975444444444">
Let Tr be a training set of n tokens, and T a sepa-
rate test set of m tokens. We denote by n(x), m(x)
the number of times the word x has been seen in
the training and test set, respectively. We assume a
closed vocabulary V containing K words. A uni-
gram model is a probability distribution π over the
vocabulary V. We measure its performace using
the average code length (Cover and Thomas, 1991)
measured on the test set:
</bodyText>
<equation confidence="0.9854652">
l(π) = — |1 xE m(x)log2 π(x). (9)
T|
The empirical distribution over the training set is
n(x)
π0(x) = n
</equation>
<bodyText confidence="0.999612142857143">
This estimate assigns a probability 0 to all unseen
words, which is undesirable, as it leads to zero prob-
ability of word sequences which can actually be ob-
served in practice. A simple way to smooth such
estimates is to add a small, not necessarily integer,
count to each word leading to the so-called add-β
estimate πβ, defined as
</bodyText>
<equation confidence="0.91510525">
. (10)
n(x) + β
πβ(x) =
n + βK . (11)
34
One may observe that
π,3(x) = (1− λ)π0(x) + λ K 1 , with λ = βK n + βK .
(12)
</equation>
<bodyText confidence="0.999693857142857">
Hence add-β estimators perform a linear interpola-
tion between π0 and the uniform distribution over
the entire vocabulary.
In practice, a much more efficient smoothing
method is the so-called Good-Turing (Orlitsky et al.,
2003; McAllester and Schapire, 2000). The Good-
Turing estimate is defined as
</bodyText>
<equation confidence="0.89852825">
rn(x)+1(n(x) + 1)
πGT(x) =
nrn(x)
= α π0(x), otherwise,
</equation>
<bodyText confidence="0.9999736">
where rj is the number of distinct words seen j times
in the training set, and α is such that πGT sums up
to 1 over the vocabulary. The threshold M is em-
pirically chosen, and usually lies between 5 and 10.
(Choosing a much larger M decreases the perfor-
mance considerably.)
The Good-Turing estimator is used frequently in
practice, and we will compare our results against it.
The add-β will provide a baseline, as well as an idea
of the variation between different smoothers.
</bodyText>
<sectionHeader confidence="0.958349" genericHeader="method">
3 Graphs over sets of words
</sectionHeader>
<bodyText confidence="0.999916">
Our objective, in this section, is to show how to de-
sign various graphs on words; different choices for
the edges and for the weight function w lead to dif-
ferent smoothings.
</bodyText>
<subsectionHeader confidence="0.999868">
3.1 Full Graph and add-β Smoothers
</subsectionHeader>
<bodyText confidence="0.999966666666667">
The simplest possible choice is the complete graph,
where all vertices are pair-wise connected. In the
case of normalized diffusion, choosing
</bodyText>
<equation confidence="0.982615">
w(x, y) = αδ(x = y) + 1, (13)
</equation>
<bodyText confidence="0.91266975">
with α =6 0 leads to the add-β smoother with param-
eter β = α−1n.
In the case of kernel smoothing with the complete
graph and w ≡ 1, one can show, see (Kondor and
Lafferty, 2002) that
Kt(x, y) = K−1 (1 + (K − 1)e−Kt) if x = y
= K−1 (1 − e−Kt) if x =6 y.
This leads to another add-β smoother.
</bodyText>
<subsectionHeader confidence="0.997038">
3.2 Graphs based on counts
</subsectionHeader>
<bodyText confidence="0.99998625">
A more interesting way of designing the word graph
is through a similarity function which is based on
the training set. For the normalized diffusion case,
we propose the following
</bodyText>
<equation confidence="0.996767">
w(x, y) = δ(|n(x) − n(y) |≤ 1). (14)
</equation>
<bodyText confidence="0.87068425">
That is, 2 words are “similar” if they have been seen
a number of times which differs by at most one. The
obtained estimator is denoted by πND. After some
algebraic manipulations, we obtain
</bodyText>
<equation confidence="0.9965645">
1
πND(y) = n
</equation>
<bodyText confidence="0.956212666666667">
This estimator has a Good-Turing “flavor”. For ex-
ample, the total mass associated with the unseen
words is
</bodyText>
<equation confidence="0.999139">
. (16)
1 + r1 + r2
r0 r0
</equation>
<bodyText confidence="0.999900285714286">
Note that the estimate of the unseen mass, in the case
of the Good-Turing estimator, is equal to n−1r1,
which is very close to the above when the vocabu-
lary is large compared to the size of the training set
(as is usually the case in practice).
Similarly, in the case of kernel diffusion, we
choose w ≡ 1 and
</bodyText>
<equation confidence="0.65749">
x ∼ y ⇐⇒ |n(x) − n(y) |≤ 1 (17)
</equation>
<bodyText confidence="0.99958675">
The time t is chosen to be |V |−1. The smoother can-
not be computed in closed form. We used the for-
mula (7) with n = 3 in the experiments. Larger
values of n did not improve the results.
</bodyText>
<sectionHeader confidence="0.990616" genericHeader="evaluation">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.9999894">
In our experiments, we used Sections 00-22 (con-
sisting of ∼ 106 words) of the UPenn Treebank cor-
pus for training, and Sections 23-24 (consisting of
∼ 105 words) for testing. We split the training set
into 10 subsets, leading to 10 datasets of size ∼ 105
tokens each. The first of these sets was further split
in subsets of size ∼ 104 tokens each. Averaged re-
sults are presented in the tables below for various
choices of the training set size. We show the mean
code-length, as well as the standard deviation (when
</bodyText>
<figure confidence="0.9914952">
, if n(x) &lt; M
n(y)+1
E
j=n(y)−1
(15)
rj−1 + rj + rj+1
jrj .
E 1
y•,n(y)=0 π1(y) = n
r1
</figure>
<page confidence="0.98767">
35
</page>
<table confidence="0.9971244">
mean code length std
πa, β = 1 12.94 0.05
πGT 11.40 0.08
πND 11.42 0.08
πKD 11.51 0.08
</table>
<tableCaption confidence="0.999363">
Table 1: Results with training set of size - 104.
</tableCaption>
<table confidence="0.9989914">
mean code length std
πa, β = 1 11.10 0.03
πGT 10.68 0.06
πND 10.69 0.06
πKD 10.74 0.08
</table>
<tableCaption confidence="0.999918">
Table 2: Results with training set of size - 105.
</tableCaption>
<bodyText confidence="0.992829416666667">
available). In all cases, we chose K = 105 as the
fixed size of our vocabulary.
The results show that πND, the estimate ob-
tained with the Normalized Diffusion, is competi-
tive with the Good-Turing πGT. We performed a
Kolmogorov-Smirnov test in order to determine if
the code-lengths obtained with πND and πGT in Ta-
ble 1 differ significantly. The result is negative (P-
value = .65), and the same holds for the larger train-
ing set in Table 2 (P-value=.95). On the other hand,
πKD (obtained with Kernel Diffusion) is not as effi-
cient, but still better than add-β with β = 1.
</bodyText>
<sectionHeader confidence="0.988911" genericHeader="conclusions">
5 Concluding Remarks
</sectionHeader>
<bodyText confidence="0.991980583333333">
We showed that diffusions on graphs can be useful
for language modeling. They yield naturally smooth
estimates, and, under a particular choice of the “sim-
ilarity” function between words, they are competi-
tive with the Good-Turing estimator, which is con-
sidered to be the state-of-the-art in unigram lan-
guage modeling. We plan to perform more exper-
mean code length
πa, β = 1 10.34
πGT 10.30
πND 10.30
πKD 10.31
</bodyText>
<tableCaption confidence="0.996985">
Table 3: Results with training set of size - 106.
</tableCaption>
<bodyText confidence="0.999949571428571">
iments with other definitions of similarity between
words. For example, we expect similarities based
on co-occurence in documents, or based on notions
of semantic closeness (computed, for instance, using
the WordNet hierarchy) to yield significant improve-
ments over estimators which are only based on word
counts.
</bodyText>
<sectionHeader confidence="0.999191" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999884272727273">
F. Chung. 1997. Spectral Graph Theory. Number 92
in CBMS Regional Conference Series in Mathematics.
American Mathematical Society.
Thomas M. Cover and Joy A. Thomas. 1991. Elements
ofInformation Theory. John Wiley &amp; Sons, Inc.
Risi Imre Kondor and John Lafferty. 2002. Diffusion
kernels on graphs and other discrete input spaces. In
ICML ’02: Proceedings of the Nineteenth Interna-
tional Conference on Machine Learning, pages 315–
322.
David McAllester and Robert E. Schapire. 2000. On the
convergence rate of Good-Turing estimators. In Proc.
13th Annu. Conference on Comput. Learning Theory.
Alon Orlitsky, Narayana P. Santhanam, and Junan Zhang.
2003. Always Good Turing: Asymptotically optimal
probability estimation. In FOCS ’03: Proceedings of
the 44th Annual IEEE Symposium on Foundations of
Computer Science.
Arthur D. Szlam, Mauro Maggioni, and Ronald R. Coif-
man. 2006. A general framework for adaptive regular-
ization based on diffusion processes on graphs. Tech-
nical report, YALE/DCS/TR1365.
</reference>
<page confidence="0.998941">
36
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.008940">
<title confidence="0.999923">Unigram Language Models using Diffusion Smoothing over Graphs</title>
<author confidence="0.997959">Bruno</author>
<affiliation confidence="0.9850045">Dept. of Appl. Mathematics and Center for Imaging</affiliation>
<address confidence="0.7312775">Johns Hopkins Baltimore, MD</address>
<email confidence="0.998355">bruno.jedynak@jhu.edu</email>
<affiliation confidence="0.74327725">Damianos Dept. of Electrical and Computer Center for Language and Speech Johns Hopkins</affiliation>
<address confidence="0.975848">Baltimore, MD</address>
<email confidence="0.999509">damianos@jhu.edu</email>
<abstract confidence="0.987013708487084">We propose to use graph-based diffusion techniques with data-dependent kernels to build unigram language models. Our approach entails building graphs, where each vertex corresponds uniquely to a word from a closed vocabulary, and the existence of an edge (with an appropriate weight) between two words indicates some form of similarity between them. In one of our constructions, we place an edge between two words if the number of times these words were seen in a training set differs by at most one count. This graph construction results in a similarity matrix with small intrinsic dimension, since words with the same counts have the same neighbors. Experimental results from a benchmark task from language modeling show that our method is competitive with the Good-Turing estimator. 1 Diffusion over Graphs 1.1 Notation an undirected graph, where a finite set of vertices, and the of edges. Also, let a vocabulary of words, whose probabilities we want to estimate. Each vertex corresponds uniquely to a word, i.e., there is a mapping between Without loss generality, we will use denote both the set words and the set of vertices. Moreover, to simnotation, we assume that the letters y, z denote vertices of existence of an edge between y be by We assume that the graph is strongly connected (i.e., there is a path between any two vertices). Furthermore, we define a nonreal valued function which the role of the two words higher the value of the more similar y In the experimental results section, we will compare different measures of similarity between words which will result in different smoothing algorithms. The degree of a vertex is defined as = assume that for any vertex that is, every word is similar to at least some other word. 1.2 Smoothing by Normalized Diffusion The setting described here was introduced in (Szlam al., 2006). First, we define a Markov chain which corresponds to a random walk over the graph Its initial value is equal to which has dis- (Although be chosen arbitrarily, we assume in this paper that it is equal to the empirical, unsmoothed, distribution of words over a training set.) We then define the transition matrix as follows: = = (2) transition matrix, together with induces a over which is equal to the distribu- 33 Graph-Based Al orithms for Natural Language pages 33–36, April 2007 Association for Computational Linguistics = distribution can be construed as a of since the of an unseen word will always be non-zero, if it has a nonzero similarity to a seen word. In the same way, a sequence of distributions be we only consider our smoothed estimate in this paper. (One may wonder whether the of this Markov chain, i.e., the distribution of as has any significance; we do not address this question here, as this limiting distribution may have very little depenon the Markov chain cases under consideration.) 1.3 Smoothing by Kernel Diffusion assume here that for any vertex = 0 that symmetric. Following (Kondor and Lafferty, 2002), we define the following matrix over = the delta function which takes the 1 if property true, and 0 otherwise. The negative of the matrix H is called the Laplacian of the graph and plays a central role in spectral graph theory (Chung, 1997). We further define the heat over the graph t &gt; ∂t initial condition where a timesquare matrix of same dimension as the identity matrix. be interas the amount of heat that reaches vertex time when starting with a unit amount of heat at Using (1) and (4), the right hand side of (5) expands to = . (6) From this equation, we see that the amount of heat increase (resp. decrease) if the current of heat at is smaller (resp. larger) than the weighted average amount of at the neighbors of thus causing the system to reach a steady state. The heat equation (5) has a unique solution which the matrix exponential (see (Kondor and Lafferty, 2002)) and which can be defined equivalently as lim or as Moreover, if the initial condition is replaced by = the solution of the heat equation is given by the matrix product In the following, be the emdistribution over the training set and be by trial and error. As before, provide smoothed version of 2 Unigram Language Models a training set of and sepatest set of We denote by number of times the word been seen in the training and test set, respectively. We assume a vocabulary A unimodel is a probability distribution the We measure its performace using the average code length (Cover and Thomas, 1991) measured on the test set: = The empirical distribution over the training set is = This estimate assigns a probability 0 to all unseen words, which is undesirable, as it leads to zero probability of word sequences which can actually be observed in practice. A simple way to smooth such estimates is to add a small, not necessarily integer, to each word leading to the so-called defined as = . 34 One may observe that = + K βK n . (12) perform a linear interpolabetween the uniform distribution over the entire vocabulary. In practice, a much more efficient smoothing method is the so-called Good-Turing (Orlitsky et al., 2003; McAllester and Schapire, 2000). The Good- Turing estimate is defined as + 1) = the number of distinct words seen j times the training set, and such that sums up 1 over the vocabulary. The threshold empirically chosen, and usually lies between 5 and 10. a much larger the performance considerably.) The Good-Turing estimator is used frequently in practice, and we will compare our results against it. provide a baseline, as well as an idea of the variation between different smoothers. 3 Graphs over sets of words Our objective, in this section, is to show how to design various graphs on words; different choices for edges and for the weight function to different smoothings. Full Graph and The simplest possible choice is the complete graph, where all vertices are pair-wise connected. In the case of normalized diffusion, choosing = + to the with param- In the case of kernel smoothing with the complete and one can show, see (Kondor and Lafferty, 2002) that = + leads to another 3.2 Graphs based on counts A more interesting way of designing the word graph is through a similarity function which is based on the training set. For the normalized diffusion case, we propose the following = ≤ That is, 2 words are “similar” if they have been seen a number of times which differs by at most one. The estimator is denoted by After some algebraic manipulations, we obtain 1 = This estimator has a Good-Turing “flavor”. For example, the total mass associated with the unseen words is . + + Note that the estimate of the unseen mass, in the case the Good-Turing estimator, is equal to which is very close to the above when the vocabulary is large compared to the size of the training set (as is usually the case in practice). Similarly, in the case of kernel diffusion, we x ≤ time chosen to be The smoother cannot be computed in closed form. We used the for- (7) with 3 the experiments. Larger of not improve the results. 4 Experimental Results In our experiments, we used Sections 00-22 (conof words) of the UPenn Treebank corpus for training, and Sections 23-24 (consisting of words) for testing. We split the training set 10 subsets, leading to 10 datasets of size tokens each. The first of these sets was further split subsets of size tokens each. Averaged results are presented in the tables below for various choices of the training set size. We show the mean code-length, as well as the standard deviation (when M E (15) . E 1 = 35 mean code length std β 1 12.94 0.05 11.40 0.08 11.42 0.08 11.51 0.08 1: Results with training set of size mean code length std β 1 11.10 0.03 10.68 0.06 10.69 0.06 10.74 0.08 2: Results with training set of size In all cases, we chose as the fixed size of our vocabulary. results show that the estimate obtained with the Normalized Diffusion, is competiwith the Good-Turing We performed a Kolmogorov-Smirnov test in order to determine if code-lengths obtained with andin Table 1 differ significantly. The result is negative (Pvalue = .65), and the same holds for the larger training set in Table 2 (P-value=.95). On the other hand, with Kernel Diffusion) is not as effibut still better than 5 Concluding Remarks We showed that diffusions on graphs can be useful for language modeling. They yield naturally smooth estimates, and, under a particular choice of the “similarity” function between words, they are competitive with the Good-Turing estimator, which is considered to be the state-of-the-art in unigram lanmodeling. We plan to perform more expermean code length β 1 10.34 10.30 10.30 10.31 3: Results with training set of size iments with other definitions of similarity between words. For example, we expect similarities based on co-occurence in documents, or based on notions of semantic closeness (computed, for instance, using the WordNet hierarchy) to yield significant improvements over estimators which are only based on word counts.</abstract>
<note confidence="0.962356818181818">References Chung. 1997. Graph Number 92 in CBMS Regional Conference Series in Mathematics. American Mathematical Society. M. Cover and Joy A. Thomas. 1991. John Wiley &amp; Sons, Inc. Risi Imre Kondor and John Lafferty. 2002. Diffusion kernels on graphs and other discrete input spaces. In ICML ’02: Proceedings of the Nineteenth Interna- Conference on Machine pages 315– 322. David McAllester and Robert E. Schapire. 2000. On the rate of Good-Turing estimators. In Annu. Conference on Comput. Learning Alon Orlitsky, Narayana P. Santhanam, and Junan Zhang. 2003. Always Good Turing: Asymptotically optimal estimation. In ’03: Proceedings of the 44th Annual IEEE Symposium on Foundations of Arthur D. Szlam, Mauro Maggioni, and Ronald R. Coifman. 2006. A general framework for adaptive regularization based on diffusion processes on graphs. Technical report, YALE/DCS/TR1365.</note>
<intro confidence="0.55648">36</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>F Chung</author>
</authors>
<title>Spectral Graph Theory.</title>
<date>1997</date>
<journal>Number</journal>
<booktitle>in CBMS Regional Conference Series in Mathematics.</booktitle>
<volume>92</volume>
<publisher>American Mathematical Society.</publisher>
<contexts>
<context position="4291" citStr="Chung, 1997" startWordPosition="753" endWordPosition="754"> do not address this question here, as this limiting distribution may have very little dependence on π0 in the Markov chain cases under consideration.) 1.3 Smoothing by Kernel Diffusion We assume here that for any vertex x, w(x, x) = 0 and that w is symmetric. Following (Kondor and Lafferty, 2002), we define the following matrix over VxV H(x, y) = w(x, y)δ(x — y) — d(x)δ(x = y), (4) where δ(u) is the delta function which takes the value 1 if property u is true, and 0 otherwise. The negative of the matrix H is called the Laplacian of the graph and plays a central role in spectral graph theory (Chung, 1997). We further define the heat equation over the graph G as ∂ Kt = HKt, t &gt; 0, (5) ∂t with initial condition K0 = I, where Kt is a timedependent square matrix of same dimension as H, and I is the identity matrix. Kt(x, y) can be interpreted as the amount of heat that reaches vertex x at time t, when starting with a unit amount of heat concentrated at y. Using (1) and (4), the right hand side of (5) expands to HKt(x, y) = E w(x, z) (Kt(z, y) — Kt(x, y)) . z:z—x (6) From this equation, we see that the amount of heat at x will increase (resp. decrease) if the current amount of heat at x (namely Kt(</context>
</contexts>
<marker>Chung, 1997</marker>
<rawString>F. Chung. 1997. Spectral Graph Theory. Number 92 in CBMS Regional Conference Series in Mathematics. American Mathematical Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas M Cover</author>
<author>Joy A Thomas</author>
</authors>
<title>Elements ofInformation Theory.</title>
<date>1991</date>
<publisher>John Wiley &amp; Sons, Inc.</publisher>
<contexts>
<context position="6035" citStr="Cover and Thomas, 1991" startWordPosition="1103" endWordPosition="1106">y the matrix product π1 = Ktπ0. In the following, π0 will be the empirical distribution over the training set and t will be chosen by trial and error. As before, π1 will provide a smoothed version of π0. 2 Unigram Language Models Let Tr be a training set of n tokens, and T a separate test set of m tokens. We denote by n(x), m(x) the number of times the word x has been seen in the training and test set, respectively. We assume a closed vocabulary V containing K words. A unigram model is a probability distribution π over the vocabulary V. We measure its performace using the average code length (Cover and Thomas, 1991) measured on the test set: l(π) = — |1 xE m(x)log2 π(x). (9) T| The empirical distribution over the training set is n(x) π0(x) = n This estimate assigns a probability 0 to all unseen words, which is undesirable, as it leads to zero probability of word sequences which can actually be observed in practice. A simple way to smooth such estimates is to add a small, not necessarily integer, count to each word leading to the so-called add-β estimate πβ, defined as . (10) n(x) + β πβ(x) = n + βK . (11) 34 One may observe that π,3(x) = (1− λ)π0(x) + λ K 1 , with λ = βK n + βK . (12) Hence add-β estimat</context>
</contexts>
<marker>Cover, Thomas, 1991</marker>
<rawString>Thomas M. Cover and Joy A. Thomas. 1991. Elements ofInformation Theory. John Wiley &amp; Sons, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Risi Imre Kondor</author>
<author>John Lafferty</author>
</authors>
<title>Diffusion kernels on graphs and other discrete input spaces.</title>
<date>2002</date>
<booktitle>In ICML ’02: Proceedings of the Nineteenth International Conference on Machine Learning,</booktitle>
<pages>315--322</pages>
<contexts>
<context position="3977" citStr="Kondor and Lafferty, 2002" startWordPosition="689" endWordPosition="692">as a nonzero similarity to a seen word. In the same way, a whole sequence of distributions π2, π3,... can be computed; we only consider π1 as our smoothed estimate in this paper. (One may wonder whether the stationary distribution of this Markov chain, i.e., the limiting distribution of Xt, as t —* oc, has any significance; we do not address this question here, as this limiting distribution may have very little dependence on π0 in the Markov chain cases under consideration.) 1.3 Smoothing by Kernel Diffusion We assume here that for any vertex x, w(x, x) = 0 and that w is symmetric. Following (Kondor and Lafferty, 2002), we define the following matrix over VxV H(x, y) = w(x, y)δ(x — y) — d(x)δ(x = y), (4) where δ(u) is the delta function which takes the value 1 if property u is true, and 0 otherwise. The negative of the matrix H is called the Laplacian of the graph and plays a central role in spectral graph theory (Chung, 1997). We further define the heat equation over the graph G as ∂ Kt = HKt, t &gt; 0, (5) ∂t with initial condition K0 = I, where Kt is a timedependent square matrix of same dimension as H, and I is the identity matrix. Kt(x, y) can be interpreted as the amount of heat that reaches vertex x at </context>
<context position="8030" citStr="Kondor and Lafferty, 2002" startWordPosition="1474" endWordPosition="1477"> variation between different smoothers. 3 Graphs over sets of words Our objective, in this section, is to show how to design various graphs on words; different choices for the edges and for the weight function w lead to different smoothings. 3.1 Full Graph and add-β Smoothers The simplest possible choice is the complete graph, where all vertices are pair-wise connected. In the case of normalized diffusion, choosing w(x, y) = αδ(x = y) + 1, (13) with α =6 0 leads to the add-β smoother with parameter β = α−1n. In the case of kernel smoothing with the complete graph and w ≡ 1, one can show, see (Kondor and Lafferty, 2002) that Kt(x, y) = K−1 (1 + (K − 1)e−Kt) if x = y = K−1 (1 − e−Kt) if x =6 y. This leads to another add-β smoother. 3.2 Graphs based on counts A more interesting way of designing the word graph is through a similarity function which is based on the training set. For the normalized diffusion case, we propose the following w(x, y) = δ(|n(x) − n(y) |≤ 1). (14) That is, 2 words are “similar” if they have been seen a number of times which differs by at most one. The obtained estimator is denoted by πND. After some algebraic manipulations, we obtain 1 πND(y) = n This estimator has a Good-Turing “flavo</context>
</contexts>
<marker>Kondor, Lafferty, 2002</marker>
<rawString>Risi Imre Kondor and John Lafferty. 2002. Diffusion kernels on graphs and other discrete input spaces. In ICML ’02: Proceedings of the Nineteenth International Conference on Machine Learning, pages 315– 322.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McAllester</author>
<author>Robert E Schapire</author>
</authors>
<title>On the convergence rate of Good-Turing estimators.</title>
<date>2000</date>
<booktitle>In Proc. 13th Annu. Conference on Comput. Learning Theory.</booktitle>
<contexts>
<context position="6873" citStr="McAllester and Schapire, 2000" startWordPosition="1261" endWordPosition="1264">le, as it leads to zero probability of word sequences which can actually be observed in practice. A simple way to smooth such estimates is to add a small, not necessarily integer, count to each word leading to the so-called add-β estimate πβ, defined as . (10) n(x) + β πβ(x) = n + βK . (11) 34 One may observe that π,3(x) = (1− λ)π0(x) + λ K 1 , with λ = βK n + βK . (12) Hence add-β estimators perform a linear interpolation between π0 and the uniform distribution over the entire vocabulary. In practice, a much more efficient smoothing method is the so-called Good-Turing (Orlitsky et al., 2003; McAllester and Schapire, 2000). The GoodTuring estimate is defined as rn(x)+1(n(x) + 1) πGT(x) = nrn(x) = α π0(x), otherwise, where rj is the number of distinct words seen j times in the training set, and α is such that πGT sums up to 1 over the vocabulary. The threshold M is empirically chosen, and usually lies between 5 and 10. (Choosing a much larger M decreases the performance considerably.) The Good-Turing estimator is used frequently in practice, and we will compare our results against it. The add-β will provide a baseline, as well as an idea of the variation between different smoothers. 3 Graphs over sets of words O</context>
</contexts>
<marker>McAllester, Schapire, 2000</marker>
<rawString>David McAllester and Robert E. Schapire. 2000. On the convergence rate of Good-Turing estimators. In Proc. 13th Annu. Conference on Comput. Learning Theory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alon Orlitsky</author>
<author>Narayana P Santhanam</author>
<author>Junan Zhang</author>
</authors>
<title>Always Good Turing: Asymptotically optimal probability estimation.</title>
<date>2003</date>
<booktitle>In FOCS ’03: Proceedings of the 44th Annual IEEE Symposium on Foundations of Computer Science.</booktitle>
<contexts>
<context position="6841" citStr="Orlitsky et al., 2003" startWordPosition="1257" endWordPosition="1260">rds, which is undesirable, as it leads to zero probability of word sequences which can actually be observed in practice. A simple way to smooth such estimates is to add a small, not necessarily integer, count to each word leading to the so-called add-β estimate πβ, defined as . (10) n(x) + β πβ(x) = n + βK . (11) 34 One may observe that π,3(x) = (1− λ)π0(x) + λ K 1 , with λ = βK n + βK . (12) Hence add-β estimators perform a linear interpolation between π0 and the uniform distribution over the entire vocabulary. In practice, a much more efficient smoothing method is the so-called Good-Turing (Orlitsky et al., 2003; McAllester and Schapire, 2000). The GoodTuring estimate is defined as rn(x)+1(n(x) + 1) πGT(x) = nrn(x) = α π0(x), otherwise, where rj is the number of distinct words seen j times in the training set, and α is such that πGT sums up to 1 over the vocabulary. The threshold M is empirically chosen, and usually lies between 5 and 10. (Choosing a much larger M decreases the performance considerably.) The Good-Turing estimator is used frequently in practice, and we will compare our results against it. The add-β will provide a baseline, as well as an idea of the variation between different smoother</context>
</contexts>
<marker>Orlitsky, Santhanam, Zhang, 2003</marker>
<rawString>Alon Orlitsky, Narayana P. Santhanam, and Junan Zhang. 2003. Always Good Turing: Asymptotically optimal probability estimation. In FOCS ’03: Proceedings of the 44th Annual IEEE Symposium on Foundations of Computer Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arthur D Szlam</author>
<author>Mauro Maggioni</author>
<author>Ronald R Coifman</author>
</authors>
<title>A general framework for adaptive regularization based on diffusion processes on graphs.</title>
<date>2006</date>
<tech>Technical report, YALE/DCS/TR1365.</tech>
<contexts>
<context position="2494" citStr="Szlam et al., 2006" startWordPosition="424" endWordPosition="427">ermore, we define a nonnegative real valued function w over V x V , which plays the role of the similarity between two words (the higher the value of w(x, y), the more similar words x, y are). In the experimental results section, we will compare different measures of similarity between words which will result in different smoothing algorithms. The degree of a vertex is defined as d(x) = � w(x, y). (1) yEV:x—y We assume that for any vertex x, d(x) &gt; 0; that is, every word is similar to at least some other word. 1.2 Smoothing by Normalized Diffusion The setting described here was introduced in (Szlam et al., 2006). First, we define a Markov chain {XtI, which corresponds to a random walk over the graph G. Its initial value is equal to X0, which has distribution 7r0. (Although 7r0 can be chosen arbitrarily, we assume in this paper that it is equal to the empirical, unsmoothed, distribution of words over a training set.) We then define the transition matrix as follows: T (x, y) = P(X1 = y|X0 = x) = d−1(x)w(x, y). (2) This transition matrix, together with 7r0, induces a distribution over V , which is equal to the distribu33 TextGraphs-2: Graph-Based Al orithms for Natural Language Processing, pages 33–36, </context>
</contexts>
<marker>Szlam, Maggioni, Coifman, 2006</marker>
<rawString>Arthur D. Szlam, Mauro Maggioni, and Ronald R. Coifman. 2006. A general framework for adaptive regularization based on diffusion processes on graphs. Technical report, YALE/DCS/TR1365.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>