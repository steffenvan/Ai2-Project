<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000121">
<title confidence="0.998904">
Incrementality, Speaker-Hearer Switching
and the Disambiguation Challenge
</title>
<author confidence="0.983036">
Ruth Kempson, Eleni Gregoromichelaki
</author>
<affiliation confidence="0.933251">
King’s College London
</affiliation>
<email confidence="0.994596">
{ruth.kempson, eleni.gregor}@kcl.ac.uk
</email>
<sectionHeader confidence="0.993829" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99994728125">
Taking so-called split utterances as our
point of departure, we argue that a new
perspective on the major challenge of dis-
ambiguation becomes available, given a
framework in which both parsing and gen-
eration incrementally involve the same
mechanisms for constructing trees reflect-
ing interpretation (Dynamic Syntax: (Cann
et al., 2005; Kempson et al., 2001)). With
all dependencies, syntactic, semantic and
pragmatic, defined in terms of incremental
progressive tree growth, the phenomenon
of speaker/hearer role-switch emerges as
an immediate consequence, with the po-
tential for clarification, acknowledgement,
correction, all available incrementally at
any sub-sentential point in the interpreta-
tion process. Accordingly, at all interme-
diate points where interpretation of an ut-
terance subpart is not fully determined for
the hearer in context, uncertainty can be
resolved immediately by suitable clarifica-
tion/correction/repair/extension as an ex-
change between interlocutors. The result
is a major check on the combinatorial ex-
plosion of alternative structures and inter-
pretations at each choice point, and the ba-
sis for a model of how interpretation in
context can be established without either
party having to make assumptions about
what information they and their interlocu-
tor share in resolving ambiguities.
</bodyText>
<sectionHeader confidence="0.999336" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999034571428571">
A major characteristic of dialogue is effortless
switching between the roles of hearer and speaker.
Dialogue participants seamlessly shift between
parsing and generation bi-directionally across any
syntactic dependency, without any indication of
there being any problem associated with such
shifts (examples from Howes et al. (in prep)):
</bodyText>
<author confidence="0.711608">
Yo Sato
</author>
<affiliation confidence="0.768606">
University of Hertfordshire
</affiliation>
<email confidence="0.631037">
y.sato@herts.ac.uk
</email>
<listItem confidence="0.4385036">
(1) Conversation from A and B, to C:
A: We’re going
B: to Bristol, where Jo lives.
(2) A smelling smoke comes into the kitchen:
A: Have you burnt
B the buns. Very thoroughly.
A: But did you burn
B: Myself? No. Luckily.
(3) A: Are you left or
B: Right-handed.
</listItem>
<bodyText confidence="0.999906285714286">
Furthermore, in no case is there any guarantee that
the way the shared utterance evolves is what ei-
ther party had in mind to say at the outset, indeed
obviously not, as otherwise the exchange risks be-
ing otiose. This flexibility provides a vehicle for
ongoing clarification, acknowledgement, correc-
tions, repairs etc. ((6)-(7) from (Mills, 2007)):
</bodyText>
<listItem confidence="0.953297363636364">
(4) A: I’m seeing Bill.
B: The builder?
A: Yeah, who lives with Monica.
(5) A: I saw Don
B: John?
A: Don, the guy from Bristol.
(6) A: I’m on the second switch
B: Switch?
A: Yeah, the grey thing
(7) A: I’m on the second row third on the left.
B: What?
</listItem>
<bodyText confidence="0.950619545454545">
A: on the left
The fragmental utterances that constitute such in-
cremental, joint contributions have been analysed
as falling into discrete structural types according
to their function, in all cases resolved to propo-
sitional types by combining with appropriate ab-
stractions from context (Fern´andez, 2006; Purver,
2004). However, any such fragment and their
resolution may occur as mid-turn interruptions,
well before any emergent propositional structure
is completed:
</bodyText>
<note confidence="0.897738">
Proceedings of EACL 2009 Workshop on Semantic Representation of Spoken Language - SRSL 2009, pages 74–81,
Athens, Greece, 30 March 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.997217">
74
</page>
<bodyText confidence="0.991176794117647">
(8) A: They X-rayed me, and took a urine
sample, took a blood sample.
Er, the doctor ...
B: Chorlton?
A: Chorlton, mhm, he examined me, erm,
he, he said now they were on about a slight
[shadow] on my heart. [BNC: KPY
1005-1008]
The advantage of such ongoing, incremental, joint
conversational contributions is the effective nar-
rowing down of the search space out of which
hearers select (a) interpretations to yield some
commonly shared understanding, e.g. choice
of referents for NPs, and, (b) restricted struc-
tural frames which allow (grammatical) context-
dependent fragment resolution, i.e. exact speci-
fications of what contextually available structures
resolve elliptical elements. This seems to pro-
vide an answer as to why such fragments are so
frequent and undemanding elements of dialogue,
forming the basis for the observed coordination
between participants: successive resolution at sub-
sentential stages yields a progressively jointly es-
tablished common ground, that can thereafter be
taken as a secure, albeit individual, basis for filter-
ing out interpretations inconsistent with such con-
firmed knowledge-base (see (Poesio and Rieser,
2008; Ginzburg, forthcmg) etc). All such dialogue
phenomena, illustrated in (1)-(8), jointly and in-
crementally achieved, we address with the general
term split utterances.
However, such exchanges are hard to model
within orthodox grammatical frameworks, given
that usually it is the sentence/proposition that is
taken as the unit of syntactic/semantic analysis;
and they have not been addressed in detail within
such frameworks, being set aside as deviant, given
that such grammars in principle do not specify
a concept of grammaticality that relies on a de-
scription of the context of occurrence of a certain
structure (however, see Poesio and Rieser (2008)
for German completions). In so far as fragment
utterances are now being addressed, the pressure
of compatibility with sentence-based grammars
is at least partly responsible for analyses of e.g.
clarificatory-request fragments as sentential in na-
ture (Ginzburg and Cooper, 2004). But such anal-
yses fail to provide a basis for incrementally re-
solved clarification requests such as the interrup-
tion in (8) where no sentential basis is yet avail-
able over which to define the required abstraction
of contextually provided content.
In the psycholinguistic literature, on the other
hand, there is broad agreement that incrementality
is a crucial feature of parsing with semantic inter-
pretation taking place as early as possible at the
sub-sentential level (see e.g. (Sturt and Crocker,
1996)). Nonetheless, this does not, in and of it-
self, provide a basis for explaining the ease and
frequency of split utterances in dialogue: the inter-
active coordination between the parsing and pro-
duction activities, one feeding the other, remains
as a challenge.
In NLP modelling, parsing and generation algo-
rithms are generally dissociated from the descrip-
tion of linguistic entities and rules, i.e. the gram-
mar formalisms, which are considered either to be
independent of processing (‘process-neutral’) or
to require some additional generation- or parsing-
specific mechanisms to be incorporated. However,
this point of view creates obstacles for a success-
ful account of data as in (1)-(8). Modelling those
would require that, for the current speaker, the ini-
tiated generation mechanism has to be displaced
mid-production without the propositional genera-
tion task having been completed. Then the parsing
mechanism, despite being independent of, indeed
in some sense the reverse of, the generation com-
ponent, has to take over mid-sentence as though, in
some sense there had been parsing involved up to
the point of switchover. Conversely, for the hearer-
turned-speaker, it would be necessary to somehow
connect their parse with what they are now about
to produce in order to compose the meaning of the
combined sentence. Moreover, in both directions
of switch, as (2) shows, this is not a phenomenon
of both interlocutors intending to say the same
sentence: as (3) shows, even the function of the
utterance (e.g. question/answer) can alter in the
switch of roles and such fragments can play two
roles (e.g. question/completion) at the same time
(e.g. (2)). Hence the grammatical integration of
such joint contributions must be flexible enough
to allow such switches which means that such
fragment resolutions must occur before the com-
putation of intentions at the pragmatic level. So
the ability of language users to successfully pro-
cess such utterances, even at sub-sentential levels,
means that modelling their grammar requires fine-
grained grammaticality definitions able to char-
acterise and integrate sub-sentential fragments in
turns jointly constructed by speaker and hearer.
</bodyText>
<page confidence="0.997864">
75
</page>
<bodyText confidence="0.999774321428571">
This can be achieved straightforwardly if fea-
tures like incrementality and context-dependent
processing are built into the grammar architecture
itself. The modelling of split utterances then be-
comes straightforward as each successive process-
ing step exploits solely the grammatical apparatus
to succeed or fail. Such a view notably does not in-
voke high-level decisions about speaker/hearer in-
tentions as part of the mechanism itself. That this
is the right view to take is enhanced by the fact that
as all of (1)-(8) show, neither party in such role-
exchanges can definitively know in advance what
will emerge as the eventual joint proposition. If,
to the contrary, generation decisions are modelled
as involving intentions for whole utterances, there
will be no the basis for modelling how such in-
complete strings can be integrated in suitable con-
texts, with joint propositional structures emerging
before such joint intentions have been established.
An additional puzzle, equally related to both
the challenges of disambiguation and the status
of modelling speaker’s intentions as part of the
mechanism whereby utterance interpretation takes
place, is the common occurrence of hearers NOT
being constrained by any check on consistency
with speaker intentions in determining a putative
interpretation, failing to make use of well estab-
lished shared knowledge:
</bodyText>
<listItem confidence="0.9113205">
(9) A: I’m going to cook salmon, as John’s
coming.
B: What? John’s a vegetarian.
A: Not my brother. John Smith.
(10) A: Why don’t you have cheese and noodles?
B: Beef? You KNOW I’m a vegetarian
</listItem>
<bodyText confidence="0.999895655172414">
Such examples are problematic for any account
that proposes that interpretation mechanisms for
utterance understanding solely depend on selec-
tion of interpretations which either the speaker
could have intended (Sperber and Wilson, 1986;
Carston, 2002), or ones which are compati-
ble with checking consistency with the com-
mon ground/plans established between speaker
and hearer (Poesio and Rieser, 2008; Ginzburg,
forthcmg), mutual knowledge, etc. (Clark, 1996;
Brennan and Clark, 1996). To the contrary, the
data in (9)-(10) tend to show that the full range
of interpretations computable by the grammar has
in principle to be available at all choice points for
construal, without any filter based on plausibility
measures, thus leaving the disambiguation chal-
lenge still unresolved.
In this paper we show how with speaker and
hearer in principle using the same mechanisms for
construal, equally incrementally applied, such dis-
ambiguation issues can be resolved in a timely
manner which in turn reduces the multiplication
of structural/interpretive options. As we shall see,
what connects our diverse examples, and indeed
underpins the smooth shift in the joint endeav-
our of conversation, lies in incremental, context-
dependent processing and bidirectionality, essen-
tial ingredients of the Dynamic Syntax (Cann et al.,
2005) dialogue model.
</bodyText>
<sectionHeader confidence="0.952909" genericHeader="method">
2 Incrementality in Dynamic Syntax
</sectionHeader>
<bodyText confidence="0.997535142857143">
Dynamic Syntax (DS) is a procedure-oriented
framework, involving incremental processing, i.e.
strictly sequential, word-by-word interpretation of
linguistic strings. The notion of incrementality
in DS is closely related to another of its features,
the goal-directedness of BOTH parsing and gener-
ation. At each stage of processing, structural pre-
dictions are triggered that could fulfill the goals
compatible with the input, in an underspecified
manner. For example, when a proper name like
Bob is encountered sentence-initially in English,
a semantic predicate node is predicted to follow
(?Ty(e → t)), amongst other possibilities.
By way of introducing the reader to the DS
devices, let us look at some formal details with
an example, Bob saw Mary. The ‘complete’ se-
mantic representation tree resulting after the com-
plete processing of this sentence is shown in Fig-
ure 2 below. A DS tree is formally encoded with
the tree logic LOFT (Blackburn and Meyer-Viol
(1994)), we omit these details here) and is gen-
erally binary configurational, with annotations at
every node. Important annotations here, see the
(simplified) tree below, are those which represent
semantic formulae along with their type informa-
tion (e.g. ‘Ty(x)’) based on a combination of the
epsilon and lambda calculi1.
Such complete trees are constructed, starting
from a radically underspecified annotation, the ax-
iom, the leftmost minimal tree in Figure 2, and
going through monotonic updates of partial, or
structurally underspecified, trees. The outline of
this process is illustrated schematically in Figure
2. Crucial for expressing the goal-directedness
are requirements, i.e. unrealised but expected
</bodyText>
<footnote confidence="0.994249">
1These are the adopted semantic representation languages
in DS but the computational formalism is compatible with
other semantic-representation formats
</footnote>
<page confidence="0.933693">
76
</page>
<figure confidence="0.999702291666667">
0 1 2
?Ty(t) ?Ty(t)
?Ty(t),
♦
H
?Ty(e), ♦ ?Ty(e → t)
H
Ty(e), Bob′ ?Ty(e → t),
♦
H
3
?Ty(t)
?Ty(e), Ty(e → (e → t)),
♦ See′
0(gen)/4
Ty(t), ♦
See′(Mary′)(Bob′)
Ty(e), Ty(e → t),
Bob′ See′(Mary′)
Ty(e), Ty(e → (e → t)),
Mary′ See′
T y(e), ?Ty(e → t)
Bob′
H
</figure>
<figureCaption confidence="0.93387">
Figure 2: Monotonic tree growth in DS
</figureCaption>
<figure confidence="0.998156666666667">
Ty(t),
See′(Mary′)(Bob′)
Ty(e), Ty(e → t),
Bob′ See′(Mary′)
Ty(e), Ty(e → (e → t)),
Mary′ See′
</figure>
<figureCaption confidence="0.999986">
Figure 1: A DS complete tree
</figureCaption>
<bodyText confidence="0.995050178571429">
node/tree specifications, indicated by ‘?’ in front
of annotations. The axiom says that a proposition
(of type t, Ty(t)) is expected to be constructed.
Furthermore, the pointer, notated with ‘0’ indi-
cates the ‘current’ node in processing, namely the
one to be processed next, and governs word order.
Updates are carried out by means of applying
actions, which are divided into two types. Compu-
tational actions govern general tree-constructional
processes, such as moving the pointer, introducing
and updating nodes, as well as compiling interpre-
tation for all non-terminal nodes in the tree. In our
example, the update of (1) to (2) is executed via
computational actions specific to English, expand-
ing the axiom to the subject and predicate nodes,
requiring the former to be processed next by the
position of the 0. Construction of only weakly
specified tree relations (unfixed nodes) can also be
induced, characterised only as dominance by some
current node, with subsequent update required. In-
dividual lexical items also provide procedures for
building structure in the form of lexical actions,
inducing both nodes and annotations. For exam-
ple, in the update from (2) to (3), the set of lexical
actions for the word see is applied, yielding the
predicate subtree and its annotations. Thus partial
trees grow incrementally, driven by procedures as-
sociated with particular words as they are encoun-
tered.
Requirements embody structural predictions as
mentioned earlier. Thus unlike the conven-
tional bottom-up parsing,2 the DS model takes
the parser/generator to entertain some predicted
goal(s) to be reached eventually at any stage of
processing, and this is precisely what makes the
formalism incremental. This is the characteri-
sation of incrementality adopted by some psy-
cholinguists under the appellation of connected-
ness (Sturt and Crocker, 1996; Costa et al., 2002):
an encountered word always gets ‘connected’ to a
larger, predicted, tree.
Individual DS trees consist of predicates and
their arguments. Complex structures are obtained
via a general tree-adjunction operation licensing
the construction of so-called LINKed trees, pairs
of trees where sharing of information occurs. In
its simplest form this mechanism is the same one
which provides the potential for compiling in-
2The examples in (1)-(8) also suggest the implausibility
of purely bottom-up or head-driven parsing being adopted di-
rectly, because such strategies involve waiting until all the
daughters are gathered before moving on to their projection.
In fact, the parsing strategy adopted by DS is somewhat sim-
ilar to mixed parsing strategies like the left-corner or Earley
algorithm to a degree. These parsing strategic issues are more
fully discussed in Sato (forthcmg).
</bodyText>
<page confidence="0.989129">
77
</page>
<bodyText confidence="0.815003">
A consultant, a friend of Jo’s, is retiring: Ty(t), Retire′((ǫ, x, Consultant′(x) n Friend′(Jo′)(x)))
</bodyText>
<equation confidence="0.656244333333333">
Ty(e), (ǫ, x, Consultant′(x) n Friend′(Jo′)(x)) Ty(e --+ t), Retire′
Ty(e), (ǫ, x, Friend′(Jo′)(x))
Ty(cn), (x, Friend′(Jo′)(x))
x Friend′(Jo′)
Jo′ Friend′
Ty(cn --+ e), AP.ǫ, P
</equation>
<figureCaption confidence="0.99973">
Figure 3: Apposition in DS
</figureCaption>
<bodyText confidence="0.995197685185185">
terpretation for apposition constructions as can
be seen in Figure (3)3. The assumption in the
construction of such LINKed structures is that at
any arbitrary stage of development, some type-
complete subtree may constitute the context for
the subsequent parsing of the following string as
an adjunct structure candidate for incorporation
into the primary tree, hence the obligatory sharing
of information in the resulting semantic represen-
tation.
More generally, context in DS is defined as the
storage of parse states, i.e., the storing of par-
tial tree, word sequence parsed to date, plus the
actions used in building up the partial tree. For-
mally, a parse state P is defined as a set of triples
(T, W, A), where: T is a (possibly partial) tree;
W is the associated sequence of words; A is the
associated sequence of lexical and computational
actions. At any point in the parsing process, the
context C for a particular partial tree T in the set
P can be taken to consist of: a set of triples P′ =
{..., (Ti, Wi, Ai), ...I resulting from the previ-
ous sentence(s); and the triple (T, W, A) itself,
the subtree currently being processed. Anaphora
and ellipsis construal generally involve re-use of
formulae, structures, and actions from the set C.
Grammaticality of a string of words is then de-
fined relative to its context C, a string being well-
formed iff there is a mapping from string onto
completed tree with no outstanding requirements
given the monotonic processing of that string rela-
tive to context. All fragments illustrated above are
processed by means of either extending the current
3Epsilon terms, like ǫ, x, Consultant′(x), stand for wit-
nesses of existentially quantified formulae in the epsilon cal-
culus and represent the semantic content of indefinites in DS.
Defined relative to the equivalence O(ǫ, x, O(x)) = 3xO(x),
their defining property is their reflection of their contain-
ing environment, and accordingly they are particularly well-
suited to expressing the growth of terms secured by such ap-
positional devices.
tree, or constructing LINKed structures and trans-
fer of information among them so that one tree
provides the context for another, and are licensed
as wellformed relative to that context. In particu-
lar, fragments like the doctor in (8) are licensed by
the grammar because they occur at a stage in pro-
cessing at which the context contains an appropri-
ate structure within which they can be integrated.
The definite NP is taken as an anaphoric device,
relying on a substitution process from the context
of the partial tree to which the node it decorates is
LINKed to achieve the appropriate construal and
tree-update:
</bodyText>
<listItem confidence="0.508861">
(11) The“parse” tree licensing production of the
doctor: LINK adjunction
</listItem>
<equation confidence="0.701464">
?Ty(t)
Chorlton′ ?Ty(e --+ t)
(Doctor′(Chorlton′)), Q
</equation>
<sectionHeader confidence="0.888035" genericHeader="method">
3 Bidirectionality in DS
</sectionHeader>
<bodyText confidence="0.999963066666667">
Crucially, for our current concern, this architec-
ture allows a dialogue model in which generation
and parsing function in parallel, following exactly
the same procedure in the same order. See Fig (2)
for a (simplified) display of the transitions manip-
ulated by a parse of Bob saw Mary, as each word
is processed and integrated to reach the complete
tree. Generation of this utterance from a complete
tree follows precisely the same actions and trees
from left to right, although the complete tree is
available from the start (this is why the complete
tree is marked ‘0’ for generation): in this case the
eventual message is known by the speaker, though
of course not by the hearer. What generation in-
volves in addition to the parse steps is reference
</bodyText>
<page confidence="0.995953">
78
</page>
<bodyText confidence="0.998980181818182">
to this complete tree to check whether each pu-
tative step is consistent with it in order not to be
deviated from the legitimate course of action, that
is, a subsumption check. The trees (1-3) are li-
censed because each of these subsumes (4). Each
time then the generator applies a lexical action, it
is licensed to produce the word that carries that ac-
tion under successful subsumption check: at Step
(3), for example, the generator processes the lex-
ical action which results in the annotation ‘See′’,
and upon success and subsumption of (4) license
to generate the word see at that point ensues.
For split utterances, two more assumptions are
pertinent. On the one hand, speakers may have
initially only a partial structure to convey: this is
unproblematic, as all that is required by the for-
malism is monotonicity of tree growth, the check
being one of subsumption which can be carried
out on partial trees as well. On the other hand,
the utterance plan may change, even within a sin-
gle speaker. Extensions and clarifications in DS
can be straightforwardly generated by appending
a LINKed structure projecting the added material
to be conveyed (preserving the monotonicity con-
straint)4.
(12) I’m going home, with my brother, maybe
with his wife.
Such a model under which the speaker and
hearer essentially follow the same sets of actions,
updating incrementally their semantic representa-
tions, allows the hearer to ‘mirror’ the same series
of partial trees, albeit not knowing in advance what
the content of the unspecified nodes will be.
</bodyText>
<sectionHeader confidence="0.996774" genericHeader="method">
4 Parser/generator implementation
</sectionHeader>
<bodyText confidence="0.999588583333333">
The process-integral nature of DS emphasised
thus far lends itself to the straightforward imple-
mentation of a parsing/generating system, since
the ‘actions’ defined in the grammar directly pro-
vide a major part of its implementation. By now it
should also be clear that the DS formalism is fully
bi-directional, not only in the sense that the same
grammar can be used for generation and parsing,
but also because the two sets of activities, conven-
tionally treated as ‘reverse’ processes, are mod-
elled to run in parallel. Therefore, not only can the
same sets of actions be used for both processes,
</bodyText>
<footnote confidence="0.980066">
4Revisions however will involve shifting to a previous
partial tree as the newly selected context: I’m going home,
to my brother, sorry my mother.
</footnote>
<bodyText confidence="0.995216615384616">
but also a large part of the parsing and generation
algorithms can be shared.
This design architecture and a prototype im-
plementation are outlined in (Purver and Otsuka,
2003), and the effort is under way to scale up the
DS parsing/generating system incorporating the
results in (Gargett et al., 2008; Gregoromichelaki
et al., to appear).5 The parser starts from the axiom
(step 0 in Fig.2), which ‘predicts’ a proposition to
be built, and follows the applicable actions, lexi-
cal or general, to develop a complete tree. Now,
as has been described in this paper, the genera-
tor follows exactly the same steps: the axiom is
developed through successive updates into a com-
plete tree. The only material difference from –
or rather in addition to– parsing is the complete
tree (Step 0(gen)/4), given from the very start of
the generation task, which is then referred to at
each tree update for subsumption check. The main
point is that despite the obvious difference in their
purposes –outputting a string from a meaning ver-
sus outputting a meaning from a string– parsing
and generation indeed share the direction of pro-
cessing in DS. Moreover, as no intervening level
of syntactic structure over the string is ever com-
puted, the parsing/generation tasks are more effi-
ciently incremental in that semantic interpretation
is directly imposed at each stage of lexical integra-
tion, irrespective of whether some given partially
developed constituent is complete.
To clarify, see the pseudocode in the Prolog
format below, which is a close analogue of the
implemented function that both does parsing and
generation of a word (context manipulation is
ignored here for reasons of space). The plus
and minus signs attached to a variable indicate it
must/needn’t be instantiated, respectively. In ef-
fect, the former corresponds to the input, the latter
to the output.
</bodyText>
<equation confidence="0.529752">
(13) parse gen word(
+OldMeaning,±Word,±NewMeaning):-
apply lexical actions(+OldMeaning, ±Word,
</equation>
<bodyText confidence="0.800851375">
+LexActions, −IntermediateMeaning ),
apply computational actions(
+IntermediateMeaning, +CompActions,
±NewMeaning )
OldMeaning is an obligatory input item, which
corresponds to the semantic structure con-
structed so far (which might be just structural
tree information initially before any lexical
</bodyText>
<footnote confidence="0.9907375">
5The preliminary results are described in (Sato,
forthcmg).
</footnote>
<page confidence="0.998763">
79
</page>
<bodyText confidence="0.999868576923077">
input has been processed thus advocating a
strong predictive element even compared to
(Sturt and Crocker, 1996). Now notice that
the other two variables —corresponding to the
word and the new (post-word) meaning— may
function either as the input or output. More
precisely, this is intended to be a shorthand
for either (+OldMeaning,+Word,−NewMeaning)
i.e. Word as input and NewMeaning as out-
put, or (+OldMeaning,−Word,+NewMeaning), i.e.
NewMeaning as input and Word as output, to repeat,
the former corresponding to parsing and the latter
to generation.
In either case, the same set of two sub-
procedures, the two kinds of actions described in
(13), are applied sequentially to process the input
to produce the output. These procedures corre-
spond to an incremental ‘update’ from one par-
tial tree to another, through a word. The whole
function is then recursively applied to exhaust the
words in the string, from left to right, either in
parsing or generation. Thus there is no differ-
ence between the two in the order of procedures
to be applied, or words to be processed. Thus it is
a mere switch of input/output that shifts between
parsing and generation.6
</bodyText>
<subsectionHeader confidence="0.998085">
4.1 Split utterances in Dynamic Syntax
</subsectionHeader>
<bodyText confidence="0.99478636">
Split utterances follow as an immediate conse-
quence of these assumptions. For the dialogues in
(1)-(8), therefore, while A reaches a partial tree of
what she has uttered through successive updates
as described above, B as the hearer, will follow
the same updates to reach the same representation
of what he has heard. This provides him with the
ability at any stage to become the speaker, inter-
rupting to continue A’s utterance, repair, ask for
clarification, reformulate, or provide a correction,
as and when necessary7. According to our model
of dialogue, repeating or extending a constituent
of A’s utterance by B is licensed only if B, the
hearer turned now speaker, entertains a message
6Thus the parsing procedure is dictated by the grammar to
a large extent, but importantly, not completely. More specif-
ically, the grammar formalism specifies the state paths them-
selves, but not how the paths should be searched. The DS ac-
tions are defined in conditional terms, i.e. what to do as and
when a certain condition holds. If a number of actions can be
applied at some point during a parse, i.e. locally ambiguity
is encountered, then it is up to a particular implementation
of the parser to decide which should be traversed first. The
current implementation includes suggestions of search strate-
gies.
</bodyText>
<footnote confidence="0.955864">
7The account extends the implementation reported in
(Purver et al., 2006)
</footnote>
<bodyText confidence="0.99984182">
to be conveyed that matches or extends the parse
tree of what he has heard in a monotonic fashion.
In DS, this message is a semantic representation
in tree format and its presence allows B to only ut-
ter the relevant subpart of A’s intended utterance.
Indeed, this update is what B is seeking to clarify,
extend or acknowledge. In DS, B can reuse the
already constructed (partial) parse tree in his con-
text, rather than having to rebuild an entire propo-
sitional tree or subtree.
The fact that the parsing formalism integrates
a strong element of predictivity, i.e. the parser
is always one step ahead from the lexical in-
put, allows a straightforward switch from pars-
ing to generation thus resulting in an explana-
tion of the facility with which split utterances oc-
cur (even without explicit reasoning processes).
Moreover, on the one hand, because of incremen-
tality, the issue of interpretation-selection can be
faced at any point in the process, with correc-
tions/acknowledgements etc. able to be provided
at any point; this results in the potential exponen-
tial explosion of interpretations being kept firmly
in check. And, structurally, such fragments can
be integrated in the current partial tree represen-
tation only (given the position of the pointer) so
there is no structural ambiguity multiplication. On
the other hand, for any one of these intermedi-
ate check points, bidirectionality entails that con-
sistency checking remains internal to the individ-
ual interlocutors’ system, the fact of their mir-
roring each other resulting at their being at the
same point of tree growth. This is sufficient to en-
sure that any inconsistency with their own parse
recognised by one party as grounds for correc-
tion/repair can be processed AS a correction/repair
by the other party without requiring any additional
metarepresentation of their interlocutors’ informa-
tion state (at least for these purposes). This allows
the possibility of building up apparently complex
assumptions of shared content, without any neces-
sity of constructing hypotheses of what is enter-
tained by the other, since all context-based selec-
tions are based on the context of the interlocutor
themselves. This, in its turn, opens up the possi-
bility of hearers constructing interpretations based
on selections made that transparently violate what
is knowledge shared by both parties, for no pre-
sumption of common ground is essential as input
to the interpretation process (see, e.g. (9)-(10)).
</bodyText>
<page confidence="0.996035">
80
</page>
<sectionHeader confidence="0.998566" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999855">
It is notable that, from this perspective, no pre-
sumption of common ground or hypothesis as to
what the speaker could have intended is necessary
to determine how the hearer selects interpretation.
All that is required is a concept of system-internal
consistency checking, the potential for clarifica-
tion in cases of uncertainty, and reliance at such
points on disambiguation/correction/repair by the
other party. The advantage of such a proposal, we
suggest, is the provision of a fully mechanistic ac-
count for disambiguation (cf. (Pickering and Gar-
rod, 2004)). The consequence of such an analysis
is that language use is essentially interactive (see
also (Ginzburg, forthcmg; Clark, 1996)): the only
constraint as to whether some hypothesised in-
terpretation assigned by either party is confirmed
turns on whether it is acknowledged or corrected
(see also (Healey, 2008)).
</bodyText>
<sectionHeader confidence="0.99677" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.959366">
This work was supported by grants ESRC RES-062-23-0962,
the EU ITALK project (FP7-214668) and Leverhulme F07-
04OU. We are grateful for comments to: Robin Cooper, Alex
Davies, Arash Eshghi, Jonathan Ginzburg, Pat Healey, Greg
James Mills. Normal disclaimers apply.
</bodyText>
<sectionHeader confidence="0.995809" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999773853333333">
Patrick Blackburn and Wilfried Meyer-Viol. 1994.
Linguistics, logic and finite trees. Bulletin of the
IGPL, 2:3–31.
Susan E. Brennan and Herbert H. Clark. 1996. Con-
ceptual pacts and lexical choice in conversation.
Journal of Experimental Psychology: Learning,
Memory and Cognition, 22:482–1493.
Ronnie Cann, Ruth Kempson, and Lutz Marten. 2005.
The Dynamics ofLanguage. Elsevier, Oxford.
Robyn Carston. 2002. Thoughts and Utterances: The
Pragmatics of Explicit Communication. Blackwell.
Herbert H. Clark. 1996. Using Language. Cambridge
University Press.
Fabrizio Costa, Paolo Frasconi, Vincenzo Lombardo,
Patrick Sturt, and Giovanni Soda. 2002. Enhanc-
ing first-pass attachment prediction. In ECAI 2002:
508-512.
Raquel Fern´andez. 2006. Non-Sentential Utterances
in Dialogue: Classification, Resolution and Use.
Ph.D. thesis, King’s College London, University of
London.
Andrew Gargett, Eleni Gregoromichelaki, Chris
Howes, and Yo Sato. 2008. Dialogue-grammar cor-
respondence in dynamic syntax. In Proceedings of
the 12th SEMDIAL (LONDIAL).
Jonathan Ginzburg and Robin Cooper. 2004. Clarifi-
cation, ellipsis, and the nature of contextual updates
in dialogue. Linguistics and Philosophy, 27(3):297–
365.
Jonathan Ginzburg. forthcmg. Semantics for Conver-
sation. CSLI.
Eleni Gregoromichelaki, Yo Sato, Ruth Kempson, An-
drew Gargett, and Christine Howes. to appear. Dia-
logue modelling and the remit of core grammar. In
Proceedings ofIWCS 2009.
Patrick Healey. 2008. Interactive misalignment: The
role of repair in the development of group sub-
languages. In R. Cooper and R. Kempson, editors,
Language in Flux. College Publications.
Christine Howes, Patrick G. T. Healey, and Gregory
Mills. in prep. a: An experimental investigation
into... b:... split utterances.
Ruth Kempson, Wilfried Meyer-Viol, and Dov Gabbay.
2001. Dynamic Syntax: The Flow ofLanguage Un-
derstanding. Blackwell.
Gregory J. Mills. 2007. Semantic co-ordination in di-
alogue: the role of direct interaction. Ph.D. thesis,
Queen Mary University of London.
Martin Pickering and Simon Garrod. 2004. Toward
a mechanistic psychology of dialogue. Behavioral
and Brain Sciences.
Massimo Poesio and Hannes Rieser. 2008. Comple-
tions, coordination, and alignment in dialogue. Ms.
Matthew Purver and Masayuki Otsuka. 2003. Incre-
mental generation by incremental parsing: Tactical
generation in Dynamic Syntax. In Proceedings of
the 9th European Workshop in Natural Language
Generation (ENLG), pages 79–86.
Matthew Purver, Ronnie Cann, and Ruth Kempson.
2006. Grammars as parsers: Meeting the dialogue
challenge. Research on Language and Computa-
tion, 4(2-3):289–326.
Matthew Purver. 2004. The Theory and Use of Clari-
fication Requests in Dialogue. Ph.D. thesis, Univer-
sity of London, forthcoming.
Yo Sato. forthcmg. Local ambiguity, search strate-
gies and parsing in dynamic syntax. In Eleni Gre-
goromichelaki and Ruth Kempson, editors, Dynamic
Syntax: Collected Papers. CSLI.
Dan Sperber and Deirdre Wilson. 1986. Relevance:
Communication and Cognition. Blackwell.
Patrick Sturt and Matthew Crocker. 1996. Monotonic
syntactic processing: a cross-linguistic study of at-
tachment and reanalysis. Language and Cognitive
Processes, 11:448–494.
</reference>
<page confidence="0.999264">
81
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.936137">
<title confidence="0.990828">Incrementality, Speaker-Hearer and the Disambiguation Challenge</title>
<author confidence="0.999141">Ruth Kempson</author>
<author confidence="0.999141">Eleni Gregoromichelaki</author>
<affiliation confidence="0.978379">King’s College London</affiliation>
<abstract confidence="0.999187121212121">so-called utterances our point of departure, we argue that a new perspective on the major challenge of disambiguation becomes available, given a framework in which both parsing and generation incrementally involve the same mechanisms for constructing trees reflectinterpretation (Cann et al., 2005; Kempson et al., 2001)). With all dependencies, syntactic, semantic and pragmatic, defined in terms of incremental progressive tree growth, the phenomenon of speaker/hearer role-switch emerges as an immediate consequence, with the potential for clarification, acknowledgement, correction, all available incrementally at any sub-sentential point in the interpretation process. Accordingly, at all intermediate points where interpretation of an utterance subpart is not fully determined for the hearer in context, uncertainty can be resolved immediately by suitable clarification/correction/repair/extension as an exchange between interlocutors. The result is a major check on the combinatorial explosion of alternative structures and interpretations at each choice point, and the basis for a model of how interpretation in context can be established without either party having to make assumptions about what information they and their interlocutor share in resolving ambiguities.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Patrick Blackburn</author>
<author>Wilfried Meyer-Viol</author>
</authors>
<title>Linguistics, logic and finite trees.</title>
<date>1994</date>
<journal>Bulletin of the IGPL,</journal>
<pages>2--3</pages>
<contexts>
<context position="12077" citStr="Blackburn and Meyer-Viol (1994)" startWordPosition="1853" endWordPosition="1856">ctural predictions are triggered that could fulfill the goals compatible with the input, in an underspecified manner. For example, when a proper name like Bob is encountered sentence-initially in English, a semantic predicate node is predicted to follow (?Ty(e → t)), amongst other possibilities. By way of introducing the reader to the DS devices, let us look at some formal details with an example, Bob saw Mary. The ‘complete’ semantic representation tree resulting after the complete processing of this sentence is shown in Figure 2 below. A DS tree is formally encoded with the tree logic LOFT (Blackburn and Meyer-Viol (1994)), we omit these details here) and is generally binary configurational, with annotations at every node. Important annotations here, see the (simplified) tree below, are those which represent semantic formulae along with their type information (e.g. ‘Ty(x)’) based on a combination of the epsilon and lambda calculi1. Such complete trees are constructed, starting from a radically underspecified annotation, the axiom, the leftmost minimal tree in Figure 2, and going through monotonic updates of partial, or structurally underspecified, trees. The outline of this process is illustrated schematically</context>
</contexts>
<marker>Blackburn, Meyer-Viol, 1994</marker>
<rawString>Patrick Blackburn and Wilfried Meyer-Viol. 1994. Linguistics, logic and finite trees. Bulletin of the IGPL, 2:3–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan E Brennan</author>
<author>Herbert H Clark</author>
</authors>
<title>Conceptual pacts and lexical choice in conversation.</title>
<date>1996</date>
<booktitle>Journal of Experimental Psychology: Learning, Memory and Cognition,</booktitle>
<pages>22--482</pages>
<contexts>
<context position="10222" citStr="Brennan and Clark, 1996" startWordPosition="1567" endWordPosition="1570">What? John’s a vegetarian. A: Not my brother. John Smith. (10) A: Why don’t you have cheese and noodles? B: Beef? You KNOW I’m a vegetarian Such examples are problematic for any account that proposes that interpretation mechanisms for utterance understanding solely depend on selection of interpretations which either the speaker could have intended (Sperber and Wilson, 1986; Carston, 2002), or ones which are compatible with checking consistency with the common ground/plans established between speaker and hearer (Poesio and Rieser, 2008; Ginzburg, forthcmg), mutual knowledge, etc. (Clark, 1996; Brennan and Clark, 1996). To the contrary, the data in (9)-(10) tend to show that the full range of interpretations computable by the grammar has in principle to be available at all choice points for construal, without any filter based on plausibility measures, thus leaving the disambiguation challenge still unresolved. In this paper we show how with speaker and hearer in principle using the same mechanisms for construal, equally incrementally applied, such disambiguation issues can be resolved in a timely manner which in turn reduces the multiplication of structural/interpretive options. As we shall see, what connec</context>
</contexts>
<marker>Brennan, Clark, 1996</marker>
<rawString>Susan E. Brennan and Herbert H. Clark. 1996. Conceptual pacts and lexical choice in conversation. Journal of Experimental Psychology: Learning, Memory and Cognition, 22:482–1493.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronnie Cann</author>
<author>Ruth Kempson</author>
<author>Lutz Marten</author>
</authors>
<title>The Dynamics ofLanguage.</title>
<date>2005</date>
<publisher>Elsevier,</publisher>
<location>Oxford.</location>
<contexts>
<context position="11059" citStr="Cann et al., 2005" startWordPosition="1696" endWordPosition="1699">sibility measures, thus leaving the disambiguation challenge still unresolved. In this paper we show how with speaker and hearer in principle using the same mechanisms for construal, equally incrementally applied, such disambiguation issues can be resolved in a timely manner which in turn reduces the multiplication of structural/interpretive options. As we shall see, what connects our diverse examples, and indeed underpins the smooth shift in the joint endeavour of conversation, lies in incremental, contextdependent processing and bidirectionality, essential ingredients of the Dynamic Syntax (Cann et al., 2005) dialogue model. 2 Incrementality in Dynamic Syntax Dynamic Syntax (DS) is a procedure-oriented framework, involving incremental processing, i.e. strictly sequential, word-by-word interpretation of linguistic strings. The notion of incrementality in DS is closely related to another of its features, the goal-directedness of BOTH parsing and generation. At each stage of processing, structural predictions are triggered that could fulfill the goals compatible with the input, in an underspecified manner. For example, when a proper name like Bob is encountered sentence-initially in English, a semant</context>
</contexts>
<marker>Cann, Kempson, Marten, 2005</marker>
<rawString>Ronnie Cann, Ruth Kempson, and Lutz Marten. 2005. The Dynamics ofLanguage. Elsevier, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robyn Carston</author>
</authors>
<title>Thoughts and Utterances: The Pragmatics of Explicit Communication.</title>
<date>2002</date>
<publisher>Blackwell.</publisher>
<contexts>
<context position="9989" citStr="Carston, 2002" startWordPosition="1535" endWordPosition="1536">eing constrained by any check on consistency with speaker intentions in determining a putative interpretation, failing to make use of well established shared knowledge: (9) A: I’m going to cook salmon, as John’s coming. B: What? John’s a vegetarian. A: Not my brother. John Smith. (10) A: Why don’t you have cheese and noodles? B: Beef? You KNOW I’m a vegetarian Such examples are problematic for any account that proposes that interpretation mechanisms for utterance understanding solely depend on selection of interpretations which either the speaker could have intended (Sperber and Wilson, 1986; Carston, 2002), or ones which are compatible with checking consistency with the common ground/plans established between speaker and hearer (Poesio and Rieser, 2008; Ginzburg, forthcmg), mutual knowledge, etc. (Clark, 1996; Brennan and Clark, 1996). To the contrary, the data in (9)-(10) tend to show that the full range of interpretations computable by the grammar has in principle to be available at all choice points for construal, without any filter based on plausibility measures, thus leaving the disambiguation challenge still unresolved. In this paper we show how with speaker and hearer in principle using </context>
</contexts>
<marker>Carston, 2002</marker>
<rawString>Robyn Carston. 2002. Thoughts and Utterances: The Pragmatics of Explicit Communication. Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert H Clark</author>
</authors>
<title>Using Language.</title>
<date>1996</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="10196" citStr="Clark, 1996" startWordPosition="1565" endWordPosition="1566">s coming. B: What? John’s a vegetarian. A: Not my brother. John Smith. (10) A: Why don’t you have cheese and noodles? B: Beef? You KNOW I’m a vegetarian Such examples are problematic for any account that proposes that interpretation mechanisms for utterance understanding solely depend on selection of interpretations which either the speaker could have intended (Sperber and Wilson, 1986; Carston, 2002), or ones which are compatible with checking consistency with the common ground/plans established between speaker and hearer (Poesio and Rieser, 2008; Ginzburg, forthcmg), mutual knowledge, etc. (Clark, 1996; Brennan and Clark, 1996). To the contrary, the data in (9)-(10) tend to show that the full range of interpretations computable by the grammar has in principle to be available at all choice points for construal, without any filter based on plausibility measures, thus leaving the disambiguation challenge still unresolved. In this paper we show how with speaker and hearer in principle using the same mechanisms for construal, equally incrementally applied, such disambiguation issues can be resolved in a timely manner which in turn reduces the multiplication of structural/interpretive options. As</context>
</contexts>
<marker>Clark, 1996</marker>
<rawString>Herbert H. Clark. 1996. Using Language. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabrizio Costa</author>
<author>Paolo Frasconi</author>
<author>Vincenzo Lombardo</author>
<author>Patrick Sturt</author>
<author>Giovanni Soda</author>
</authors>
<title>Enhancing first-pass attachment prediction.</title>
<date>2002</date>
<booktitle>In ECAI 2002:</booktitle>
<pages>508--512</pages>
<contexts>
<context position="15226" citStr="Costa et al., 2002" startWordPosition="2353" endWordPosition="2356">icate subtree and its annotations. Thus partial trees grow incrementally, driven by procedures associated with particular words as they are encountered. Requirements embody structural predictions as mentioned earlier. Thus unlike the conventional bottom-up parsing,2 the DS model takes the parser/generator to entertain some predicted goal(s) to be reached eventually at any stage of processing, and this is precisely what makes the formalism incremental. This is the characterisation of incrementality adopted by some psycholinguists under the appellation of connectedness (Sturt and Crocker, 1996; Costa et al., 2002): an encountered word always gets ‘connected’ to a larger, predicted, tree. Individual DS trees consist of predicates and their arguments. Complex structures are obtained via a general tree-adjunction operation licensing the construction of so-called LINKed trees, pairs of trees where sharing of information occurs. In its simplest form this mechanism is the same one which provides the potential for compiling in2The examples in (1)-(8) also suggest the implausibility of purely bottom-up or head-driven parsing being adopted directly, because such strategies involve waiting until all the daughter</context>
</contexts>
<marker>Costa, Frasconi, Lombardo, Sturt, Soda, 2002</marker>
<rawString>Fabrizio Costa, Paolo Frasconi, Vincenzo Lombardo, Patrick Sturt, and Giovanni Soda. 2002. Enhancing first-pass attachment prediction. In ECAI 2002: 508-512.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raquel Fern´andez</author>
</authors>
<title>Non-Sentential Utterances in Dialogue: Classification, Resolution and Use.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>King’s College London, University of London.</institution>
<marker>Fern´andez, 2006</marker>
<rawString>Raquel Fern´andez. 2006. Non-Sentential Utterances in Dialogue: Classification, Resolution and Use. Ph.D. thesis, King’s College London, University of London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Gargett</author>
<author>Eleni Gregoromichelaki</author>
<author>Chris Howes</author>
<author>Yo Sato</author>
</authors>
<title>Dialogue-grammar correspondence in dynamic syntax.</title>
<date>2008</date>
<booktitle>In Proceedings of the 12th SEMDIAL (LONDIAL).</booktitle>
<contexts>
<context position="22581" citStr="Gargett et al., 2008" startWordPosition="3560" endWordPosition="3563"> of activities, conventionally treated as ‘reverse’ processes, are modelled to run in parallel. Therefore, not only can the same sets of actions be used for both processes, 4Revisions however will involve shifting to a previous partial tree as the newly selected context: I’m going home, to my brother, sorry my mother. but also a large part of the parsing and generation algorithms can be shared. This design architecture and a prototype implementation are outlined in (Purver and Otsuka, 2003), and the effort is under way to scale up the DS parsing/generating system incorporating the results in (Gargett et al., 2008; Gregoromichelaki et al., to appear).5 The parser starts from the axiom (step 0 in Fig.2), which ‘predicts’ a proposition to be built, and follows the applicable actions, lexical or general, to develop a complete tree. Now, as has been described in this paper, the generator follows exactly the same steps: the axiom is developed through successive updates into a complete tree. The only material difference from – or rather in addition to– parsing is the complete tree (Step 0(gen)/4), given from the very start of the generation task, which is then referred to at each tree update for subsumption </context>
</contexts>
<marker>Gargett, Gregoromichelaki, Howes, Sato, 2008</marker>
<rawString>Andrew Gargett, Eleni Gregoromichelaki, Chris Howes, and Yo Sato. 2008. Dialogue-grammar correspondence in dynamic syntax. In Proceedings of the 12th SEMDIAL (LONDIAL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Ginzburg</author>
<author>Robin Cooper</author>
</authors>
<title>Clarification, ellipsis, and the nature of contextual updates in dialogue.</title>
<date>2004</date>
<journal>Linguistics and Philosophy,</journal>
<volume>27</volume>
<issue>3</issue>
<pages>365</pages>
<contexts>
<context position="5499" citStr="Ginzburg and Cooper, 2004" startWordPosition="832" endWordPosition="835">is taken as the unit of syntactic/semantic analysis; and they have not been addressed in detail within such frameworks, being set aside as deviant, given that such grammars in principle do not specify a concept of grammaticality that relies on a description of the context of occurrence of a certain structure (however, see Poesio and Rieser (2008) for German completions). In so far as fragment utterances are now being addressed, the pressure of compatibility with sentence-based grammars is at least partly responsible for analyses of e.g. clarificatory-request fragments as sentential in nature (Ginzburg and Cooper, 2004). But such analyses fail to provide a basis for incrementally resolved clarification requests such as the interruption in (8) where no sentential basis is yet available over which to define the required abstraction of contextually provided content. In the psycholinguistic literature, on the other hand, there is broad agreement that incrementality is a crucial feature of parsing with semantic interpretation taking place as early as possible at the sub-sentential level (see e.g. (Sturt and Crocker, 1996)). Nonetheless, this does not, in and of itself, provide a basis for explaining the ease and </context>
</contexts>
<marker>Ginzburg, Cooper, 2004</marker>
<rawString>Jonathan Ginzburg and Robin Cooper. 2004. Clarification, ellipsis, and the nature of contextual updates in dialogue. Linguistics and Philosophy, 27(3):297– 365.</rawString>
</citation>
<citation valid="false">
<authors>
<author>forthcmg</author>
</authors>
<title>Semantics for Conversation.</title>
<publisher>CSLI.</publisher>
<marker>forthcmg, </marker>
<rawString>Jonathan Ginzburg. forthcmg. Semantics for Conversation. CSLI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eleni Gregoromichelaki</author>
<author>Yo Sato</author>
<author>Ruth Kempson</author>
<author>Andrew Gargett</author>
<author>Christine Howes</author>
</authors>
<title>to appear. Dialogue modelling and the remit of core grammar.</title>
<date>2009</date>
<booktitle>In Proceedings ofIWCS</booktitle>
<marker>Gregoromichelaki, Sato, Kempson, Gargett, Howes, 2009</marker>
<rawString>Eleni Gregoromichelaki, Yo Sato, Ruth Kempson, Andrew Gargett, and Christine Howes. to appear. Dialogue modelling and the remit of core grammar. In Proceedings ofIWCS 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Healey</author>
</authors>
<title>Interactive misalignment: The role of repair in the development of group sublanguages.</title>
<date>2008</date>
<editor>In R. Cooper and R. Kempson, editors, Language in Flux.</editor>
<publisher>College Publications.</publisher>
<marker>Healey, 2008</marker>
<rawString>Patrick Healey. 2008. Interactive misalignment: The role of repair in the development of group sublanguages. In R. Cooper and R. Kempson, editors, Language in Flux. College Publications.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Christine Howes</author>
<author>Patrick G T Healey</author>
<author>Gregory Mills</author>
</authors>
<title>in prep. a: An experimental investigation into... b:... split utterances.</title>
<marker>Howes, Healey, Mills, </marker>
<rawString>Christine Howes, Patrick G. T. Healey, and Gregory Mills. in prep. a: An experimental investigation into... b:... split utterances.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruth Kempson</author>
<author>Wilfried Meyer-Viol</author>
<author>Dov Gabbay</author>
</authors>
<title>Dynamic Syntax: The Flow ofLanguage Understanding.</title>
<date>2001</date>
<publisher>Blackwell.</publisher>
<marker>Kempson, Meyer-Viol, Gabbay, 2001</marker>
<rawString>Ruth Kempson, Wilfried Meyer-Viol, and Dov Gabbay. 2001. Dynamic Syntax: The Flow ofLanguage Understanding. Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory J Mills</author>
</authors>
<title>Semantic co-ordination in dialogue: the role of direct interaction.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>Queen Mary University of London.</institution>
<contexts>
<context position="2508" citStr="Mills, 2007" startWordPosition="369" endWordPosition="370">s.ac.uk (1) Conversation from A and B, to C: A: We’re going B: to Bristol, where Jo lives. (2) A smelling smoke comes into the kitchen: A: Have you burnt B the buns. Very thoroughly. A: But did you burn B: Myself? No. Luckily. (3) A: Are you left or B: Right-handed. Furthermore, in no case is there any guarantee that the way the shared utterance evolves is what either party had in mind to say at the outset, indeed obviously not, as otherwise the exchange risks being otiose. This flexibility provides a vehicle for ongoing clarification, acknowledgement, corrections, repairs etc. ((6)-(7) from (Mills, 2007)): (4) A: I’m seeing Bill. B: The builder? A: Yeah, who lives with Monica. (5) A: I saw Don B: John? A: Don, the guy from Bristol. (6) A: I’m on the second switch B: Switch? A: Yeah, the grey thing (7) A: I’m on the second row third on the left. B: What? A: on the left The fragmental utterances that constitute such incremental, joint contributions have been analysed as falling into discrete structural types according to their function, in all cases resolved to propositional types by combining with appropriate abstractions from context (Fern´andez, 2006; Purver, 2004). However, any such fragmen</context>
</contexts>
<marker>Mills, 2007</marker>
<rawString>Gregory J. Mills. 2007. Semantic co-ordination in dialogue: the role of direct interaction. Ph.D. thesis, Queen Mary University of London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Pickering</author>
<author>Simon Garrod</author>
</authors>
<title>Toward a mechanistic psychology of dialogue. Behavioral and Brain Sciences.</title>
<date>2004</date>
<contexts>
<context position="30189" citStr="Pickering and Garrod, 2004" startWordPosition="4780" endWordPosition="4784">to the interpretation process (see, e.g. (9)-(10)). 80 5 Conclusion It is notable that, from this perspective, no presumption of common ground or hypothesis as to what the speaker could have intended is necessary to determine how the hearer selects interpretation. All that is required is a concept of system-internal consistency checking, the potential for clarification in cases of uncertainty, and reliance at such points on disambiguation/correction/repair by the other party. The advantage of such a proposal, we suggest, is the provision of a fully mechanistic account for disambiguation (cf. (Pickering and Garrod, 2004)). The consequence of such an analysis is that language use is essentially interactive (see also (Ginzburg, forthcmg; Clark, 1996)): the only constraint as to whether some hypothesised interpretation assigned by either party is confirmed turns on whether it is acknowledged or corrected (see also (Healey, 2008)). Acknowledgements This work was supported by grants ESRC RES-062-23-0962, the EU ITALK project (FP7-214668) and Leverhulme F07- 04OU. We are grateful for comments to: Robin Cooper, Alex Davies, Arash Eshghi, Jonathan Ginzburg, Pat Healey, Greg James Mills. Normal disclaimers apply. Refe</context>
</contexts>
<marker>Pickering, Garrod, 2004</marker>
<rawString>Martin Pickering and Simon Garrod. 2004. Toward a mechanistic psychology of dialogue. Behavioral and Brain Sciences.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimo Poesio</author>
<author>Hannes Rieser</author>
</authors>
<date>2008</date>
<note>Completions, coordination, and alignment in dialogue. Ms.</note>
<contexts>
<context position="4568" citStr="Poesio and Rieser, 2008" startWordPosition="692" endWordPosition="695">hich allow (grammatical) contextdependent fragment resolution, i.e. exact specifications of what contextually available structures resolve elliptical elements. This seems to provide an answer as to why such fragments are so frequent and undemanding elements of dialogue, forming the basis for the observed coordination between participants: successive resolution at subsentential stages yields a progressively jointly established common ground, that can thereafter be taken as a secure, albeit individual, basis for filtering out interpretations inconsistent with such confirmed knowledge-base (see (Poesio and Rieser, 2008; Ginzburg, forthcmg) etc). All such dialogue phenomena, illustrated in (1)-(8), jointly and incrementally achieved, we address with the general term split utterances. However, such exchanges are hard to model within orthodox grammatical frameworks, given that usually it is the sentence/proposition that is taken as the unit of syntactic/semantic analysis; and they have not been addressed in detail within such frameworks, being set aside as deviant, given that such grammars in principle do not specify a concept of grammaticality that relies on a description of the context of occurrence of a cer</context>
<context position="10138" citStr="Poesio and Rieser, 2008" startWordPosition="1556" endWordPosition="1559">stablished shared knowledge: (9) A: I’m going to cook salmon, as John’s coming. B: What? John’s a vegetarian. A: Not my brother. John Smith. (10) A: Why don’t you have cheese and noodles? B: Beef? You KNOW I’m a vegetarian Such examples are problematic for any account that proposes that interpretation mechanisms for utterance understanding solely depend on selection of interpretations which either the speaker could have intended (Sperber and Wilson, 1986; Carston, 2002), or ones which are compatible with checking consistency with the common ground/plans established between speaker and hearer (Poesio and Rieser, 2008; Ginzburg, forthcmg), mutual knowledge, etc. (Clark, 1996; Brennan and Clark, 1996). To the contrary, the data in (9)-(10) tend to show that the full range of interpretations computable by the grammar has in principle to be available at all choice points for construal, without any filter based on plausibility measures, thus leaving the disambiguation challenge still unresolved. In this paper we show how with speaker and hearer in principle using the same mechanisms for construal, equally incrementally applied, such disambiguation issues can be resolved in a timely manner which in turn reduces</context>
</contexts>
<marker>Poesio, Rieser, 2008</marker>
<rawString>Massimo Poesio and Hannes Rieser. 2008. Completions, coordination, and alignment in dialogue. Ms.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Purver</author>
<author>Masayuki Otsuka</author>
</authors>
<title>Incremental generation by incremental parsing: Tactical generation in Dynamic Syntax.</title>
<date>2003</date>
<booktitle>In Proceedings of the 9th European Workshop in Natural Language Generation (ENLG),</booktitle>
<pages>79--86</pages>
<contexts>
<context position="22456" citStr="Purver and Otsuka, 2003" startWordPosition="3539" endWordPosition="3542">bi-directional, not only in the sense that the same grammar can be used for generation and parsing, but also because the two sets of activities, conventionally treated as ‘reverse’ processes, are modelled to run in parallel. Therefore, not only can the same sets of actions be used for both processes, 4Revisions however will involve shifting to a previous partial tree as the newly selected context: I’m going home, to my brother, sorry my mother. but also a large part of the parsing and generation algorithms can be shared. This design architecture and a prototype implementation are outlined in (Purver and Otsuka, 2003), and the effort is under way to scale up the DS parsing/generating system incorporating the results in (Gargett et al., 2008; Gregoromichelaki et al., to appear).5 The parser starts from the axiom (step 0 in Fig.2), which ‘predicts’ a proposition to be built, and follows the applicable actions, lexical or general, to develop a complete tree. Now, as has been described in this paper, the generator follows exactly the same steps: the axiom is developed through successive updates into a complete tree. The only material difference from – or rather in addition to– parsing is the complete tree (Ste</context>
</contexts>
<marker>Purver, Otsuka, 2003</marker>
<rawString>Matthew Purver and Masayuki Otsuka. 2003. Incremental generation by incremental parsing: Tactical generation in Dynamic Syntax. In Proceedings of the 9th European Workshop in Natural Language Generation (ENLG), pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Purver</author>
<author>Ronnie Cann</author>
<author>Ruth Kempson</author>
</authors>
<title>Grammars as parsers: Meeting the dialogue challenge.</title>
<date>2006</date>
<journal>Research on Language and Computation,</journal>
<pages>4--2</pages>
<contexts>
<context position="27161" citStr="Purver et al., 2006" startWordPosition="4293" endWordPosition="4296">e extent, but importantly, not completely. More specifically, the grammar formalism specifies the state paths themselves, but not how the paths should be searched. The DS actions are defined in conditional terms, i.e. what to do as and when a certain condition holds. If a number of actions can be applied at some point during a parse, i.e. locally ambiguity is encountered, then it is up to a particular implementation of the parser to decide which should be traversed first. The current implementation includes suggestions of search strategies. 7The account extends the implementation reported in (Purver et al., 2006) to be conveyed that matches or extends the parse tree of what he has heard in a monotonic fashion. In DS, this message is a semantic representation in tree format and its presence allows B to only utter the relevant subpart of A’s intended utterance. Indeed, this update is what B is seeking to clarify, extend or acknowledge. In DS, B can reuse the already constructed (partial) parse tree in his context, rather than having to rebuild an entire propositional tree or subtree. The fact that the parsing formalism integrates a strong element of predictivity, i.e. the parser is always one step ahead</context>
</contexts>
<marker>Purver, Cann, Kempson, 2006</marker>
<rawString>Matthew Purver, Ronnie Cann, and Ruth Kempson. 2006. Grammars as parsers: Meeting the dialogue challenge. Research on Language and Computation, 4(2-3):289–326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Purver</author>
</authors>
<title>The Theory and Use of Clarification Requests in Dialogue.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>University of London, forthcoming.</institution>
<contexts>
<context position="3081" citStr="Purver, 2004" startWordPosition="470" endWordPosition="471">repairs etc. ((6)-(7) from (Mills, 2007)): (4) A: I’m seeing Bill. B: The builder? A: Yeah, who lives with Monica. (5) A: I saw Don B: John? A: Don, the guy from Bristol. (6) A: I’m on the second switch B: Switch? A: Yeah, the grey thing (7) A: I’m on the second row third on the left. B: What? A: on the left The fragmental utterances that constitute such incremental, joint contributions have been analysed as falling into discrete structural types according to their function, in all cases resolved to propositional types by combining with appropriate abstractions from context (Fern´andez, 2006; Purver, 2004). However, any such fragment and their resolution may occur as mid-turn interruptions, well before any emergent propositional structure is completed: Proceedings of EACL 2009 Workshop on Semantic Representation of Spoken Language - SRSL 2009, pages 74–81, Athens, Greece, 30 March 2009. c�2009 Association for Computational Linguistics 74 (8) A: They X-rayed me, and took a urine sample, took a blood sample. Er, the doctor ... B: Chorlton? A: Chorlton, mhm, he examined me, erm, he, he said now they were on about a slight [shadow] on my heart. [BNC: KPY 1005-1008] The advantage of such ongoing, in</context>
</contexts>
<marker>Purver, 2004</marker>
<rawString>Matthew Purver. 2004. The Theory and Use of Clarification Requests in Dialogue. Ph.D. thesis, University of London, forthcoming.</rawString>
</citation>
<citation valid="false">
<authors>
<author>forthcmg</author>
</authors>
<title>Local ambiguity, search strategies and parsing in dynamic syntax.</title>
<booktitle>In Eleni Gregoromichelaki</booktitle>
<editor>and Ruth Kempson, editors,</editor>
<publisher>CSLI.</publisher>
<marker>forthcmg, </marker>
<rawString>Yo Sato. forthcmg. Local ambiguity, search strategies and parsing in dynamic syntax. In Eleni Gregoromichelaki and Ruth Kempson, editors, Dynamic Syntax: Collected Papers. CSLI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Sperber</author>
<author>Deirdre Wilson</author>
</authors>
<title>Relevance: Communication and Cognition.</title>
<date>1986</date>
<publisher>Blackwell.</publisher>
<contexts>
<context position="9973" citStr="Sperber and Wilson, 1986" startWordPosition="1531" endWordPosition="1534">ccurrence of hearers NOT being constrained by any check on consistency with speaker intentions in determining a putative interpretation, failing to make use of well established shared knowledge: (9) A: I’m going to cook salmon, as John’s coming. B: What? John’s a vegetarian. A: Not my brother. John Smith. (10) A: Why don’t you have cheese and noodles? B: Beef? You KNOW I’m a vegetarian Such examples are problematic for any account that proposes that interpretation mechanisms for utterance understanding solely depend on selection of interpretations which either the speaker could have intended (Sperber and Wilson, 1986; Carston, 2002), or ones which are compatible with checking consistency with the common ground/plans established between speaker and hearer (Poesio and Rieser, 2008; Ginzburg, forthcmg), mutual knowledge, etc. (Clark, 1996; Brennan and Clark, 1996). To the contrary, the data in (9)-(10) tend to show that the full range of interpretations computable by the grammar has in principle to be available at all choice points for construal, without any filter based on plausibility measures, thus leaving the disambiguation challenge still unresolved. In this paper we show how with speaker and hearer in </context>
</contexts>
<marker>Sperber, Wilson, 1986</marker>
<rawString>Dan Sperber and Deirdre Wilson. 1986. Relevance: Communication and Cognition. Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Sturt</author>
<author>Matthew Crocker</author>
</authors>
<title>Monotonic syntactic processing: a cross-linguistic study of attachment and reanalysis.</title>
<date>1996</date>
<booktitle>Language and Cognitive Processes,</booktitle>
<pages>11--448</pages>
<contexts>
<context position="6006" citStr="Sturt and Crocker, 1996" startWordPosition="913" endWordPosition="916"> responsible for analyses of e.g. clarificatory-request fragments as sentential in nature (Ginzburg and Cooper, 2004). But such analyses fail to provide a basis for incrementally resolved clarification requests such as the interruption in (8) where no sentential basis is yet available over which to define the required abstraction of contextually provided content. In the psycholinguistic literature, on the other hand, there is broad agreement that incrementality is a crucial feature of parsing with semantic interpretation taking place as early as possible at the sub-sentential level (see e.g. (Sturt and Crocker, 1996)). Nonetheless, this does not, in and of itself, provide a basis for explaining the ease and frequency of split utterances in dialogue: the interactive coordination between the parsing and production activities, one feeding the other, remains as a challenge. In NLP modelling, parsing and generation algorithms are generally dissociated from the description of linguistic entities and rules, i.e. the grammar formalisms, which are considered either to be independent of processing (‘process-neutral’) or to require some additional generation- or parsingspecific mechanisms to be incorporated. However</context>
<context position="15205" citStr="Sturt and Crocker, 1996" startWordPosition="2349" endWordPosition="2352">pplied, yielding the predicate subtree and its annotations. Thus partial trees grow incrementally, driven by procedures associated with particular words as they are encountered. Requirements embody structural predictions as mentioned earlier. Thus unlike the conventional bottom-up parsing,2 the DS model takes the parser/generator to entertain some predicted goal(s) to be reached eventually at any stage of processing, and this is precisely what makes the formalism incremental. This is the characterisation of incrementality adopted by some psycholinguists under the appellation of connectedness (Sturt and Crocker, 1996; Costa et al., 2002): an encountered word always gets ‘connected’ to a larger, predicted, tree. Individual DS trees consist of predicates and their arguments. Complex structures are obtained via a general tree-adjunction operation licensing the construction of so-called LINKed trees, pairs of trees where sharing of information occurs. In its simplest form this mechanism is the same one which provides the potential for compiling in2The examples in (1)-(8) also suggest the implausibility of purely bottom-up or head-driven parsing being adopted directly, because such strategies involve waiting u</context>
<context position="24700" citStr="Sturt and Crocker, 1996" startWordPosition="3886" endWordPosition="3889">rmer corresponds to the input, the latter to the output. (13) parse gen word( +OldMeaning,±Word,±NewMeaning):- apply lexical actions(+OldMeaning, ±Word, +LexActions, −IntermediateMeaning ), apply computational actions( +IntermediateMeaning, +CompActions, ±NewMeaning ) OldMeaning is an obligatory input item, which corresponds to the semantic structure constructed so far (which might be just structural tree information initially before any lexical 5The preliminary results are described in (Sato, forthcmg). 79 input has been processed thus advocating a strong predictive element even compared to (Sturt and Crocker, 1996). Now notice that the other two variables —corresponding to the word and the new (post-word) meaning— may function either as the input or output. More precisely, this is intended to be a shorthand for either (+OldMeaning,+Word,−NewMeaning) i.e. Word as input and NewMeaning as output, or (+OldMeaning,−Word,+NewMeaning), i.e. NewMeaning as input and Word as output, to repeat, the former corresponding to parsing and the latter to generation. In either case, the same set of two subprocedures, the two kinds of actions described in (13), are applied sequentially to process the input to produce the o</context>
</contexts>
<marker>Sturt, Crocker, 1996</marker>
<rawString>Patrick Sturt and Matthew Crocker. 1996. Monotonic syntactic processing: a cross-linguistic study of attachment and reanalysis. Language and Cognitive Processes, 11:448–494.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>