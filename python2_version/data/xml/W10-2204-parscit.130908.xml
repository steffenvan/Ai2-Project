<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.942459">
Maximum Likelihood Estimation of Feature-based Distributions
</title>
<author confidence="0.996736">
Jeffrey Heinz and Cesar Koirala
</author>
<affiliation confidence="0.997761">
University of Delaware
</affiliation>
<address confidence="0.89977">
Newark, Delaware, USA
</address>
<email confidence="0.999311">
{heinz,koirala}@udel.edu
</email>
<sectionHeader confidence="0.994809" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997958611111111">
Motivated by recent work in phonotac-
tic learning (Hayes and Wilson 2008, Al-
bright 2009), this paper shows how to de-
fine feature-based probability distributions
whose parameters can be provably effi-
ciently estimated. The main idea is that
these distributions are defined as a prod-
uct of simpler distributions (cf. Ghahra-
mani and Jordan 1997). One advantage
of this framework is it draws attention to
what is minimally necessary to describe
and learn phonological feature interactions
in phonotactic patterns. The “bottom-up”
approach adopted here is contrasted with
the “top-down” approach in Hayes and
Wilson (2008), and it is argued that the
bottom-up approach is more analytically
transparent.
</bodyText>
<sectionHeader confidence="0.990787" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.622811357142857">
The hypothesis that the atomic units of phonology
are phonological features, and not segments, is one
of the tenets of modern phonology (Jakobson et
al., 1952; Chomsky and Halle, 1968). Accord-
ing to this hypothesis, segments are essentially
epiphenomenal and exist only by virtue of being
a shorthand description of a collection of more
primitive units—the features. Incorporating this
hypothesis into phonological learning models has
been the focus of much influential work (Gildea
and Jurafsky, 1996; Wilson, 2006; Hayes and Wil-
son, 2008; Moreton, 2008; Albright, 2009).
This paper makes three contributions. The first
contribution is a framework within which:
</bodyText>
<listItem confidence="0.99731">
1. researchers can choose which statistical in-
dependence assumptions to make regarding
phonological features;
2. feature systems can be fully integrated into
strictly local (McNaughton and Papert, 1971)
</listItem>
<bodyText confidence="0.99747975">
(i.e. n-gram models (Jurafsky and Martin,
2008)) and strictly piecewise models (Rogers
et al., 2009; Heinz and Rogers, 2010) in
order to define families of provably well-
formed, feature-based probability distribu-
tions that are provably efficiently estimable.
The main idea is to define a family of distribu-
tions as the normalized product of simpler distri-
butions. Each simpler distribution can be repre-
sented by a Probabilistic Deterministic Finite Ac-
ceptor (PDFA), and the product of these PDFAs
defines the actual distribution. When a family of
distributions F is defined in this way, F may have
many fewer parameters than if F is defined over
the product PDFA directly. This is because the pa-
rameters of the distributions are defined in terms
of the factors which combine in predictable ways
via the product. Fewer parameters means accurate
estimation occurs with less data and, relatedly, the
family contains fewer distributions.
This idea is not new. It is explicit in Facto-
rial Hidden Markov Models (FHMMs) (Ghahra-
mani and Jordan, 1997; Saul and Jordan, 1999),
and more recently underlies approaches to de-
scribing and inferring regular string transductions
(Dreyer et al., 2008; Dreyer and Eisner, 2009).
Although HMMs and probabilistic finite-state au-
tomata describe the same class of distributions
(Vidal et al., 2005a; Vidal et al., 2005b), this paper
presents these ideas in formal language-theoretic
and automata-theoretic terms because (1) there are
no hidden states and is thus simpler than FHMMs,
(2) determinstic automata have several desirable
properties crucially used here, and (3) PDFAs
add probabilities to structure whereas HMMs add
structure to probabilities and the authors are more
comfortable with the former perspective (for fur-
ther discussion, see Vidal et al. (2005a,b)).
The second contribution illustrates the main
idea with a feature-based bigram model with a
</bodyText>
<page confidence="0.968434">
28
</page>
<note confidence="0.958082">
Proceedings of the 11th Meeting of the ACL-SIGMORPHON, ACL 2010, pages 28–37,
Uppsala, Sweden, 15 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999788411764706">
strong statistical independence assumption: no
two features interact. This is shown to capture ex-
actly the intuition that sounds with like features
have like distributions. Also, the assumption of
non-interacting features is shown to be too strong
because like sounds do not have like distributions
in actual phonotactic patterns. Four kinds of fea-
tural interactions are identified and possible solu-
tions are discussed.
Finally, we compare this proposal with Hayes
and Wilson (2008). Essentially, the model here
represents a “bottom-up” approach whereas theirs
is “top-down.” “Top-down” models, which con-
sider every set of features as potentially interact-
ing in every allowable context, face the difficult
problem of searching a vast space and often re-
sort to heuristic-based methods, which are diffi-
cult to analyze. To illustrate, we suggest that the
role played by phonological features in the phono-
tactic learner in Hayes and Wilson (2008) is not
well-understood. We demonstrate that classes of
all segments but one (i.e. the complement classes
of single segments) play a significant role, which
diminishes the contribution provided by natural
classes themselves (i.e. ones made by phonologi-
cal features). In contrast, the proposed model here
is analytically transparent.
This paper is organized as follows. §2 reviews
some background. §3 discusses bigram models
and §4 defines feature systems and feature-based
distributions. §5 develops a model with a strong
independence assumption and §6 discusses feat-
ural interaction. §7 dicusses Hayes and Wilson
(2008) and §8 concludes.
</bodyText>
<sectionHeader confidence="0.991967" genericHeader="introduction">
2 Preliminaries
</sectionHeader>
<bodyText confidence="0.975848153846154">
We start with mostly standard notation. P(A) is
the powerset of A. E denotes a finite set of sym-
bols and a string over E is a finite sequence of
these symbols. E+ and E* denote all strings over
this alphabet of nonzero but finite length, and of
any finite length, respectively. A function f with
domain A and codomain B is written f : A → B.
When discussing partial functions, the notation ↑
and ↓ indicate for particular arguments whether
the function is undefined and defined, respectively.
A language L is a subset of E*. A stochastic
language D is a probability distribution over E*.
The probability p of word w with respect to D is
written PrD(w) = p. Recall that all distributions
D must satisfy �wEΣ∗ PrD(w) = 1. If L is lan-
guage then PrD(L) = EwEL PrD(w). Since all
distributions in this paper are stochastic languages,
we use the two terms interchangeably.
A Probabilistic Deterministic Finite-
state Automaton (PDFA) is a tuple
M = hQ, E, q0, δ, F, Ti where Q is the state
set, E is the alphabet, q0 is the start state, δ is
a deterministic transition function, F and T are
the final-state and transition probabilities. In
particular, T : Q × E → R+ and F : Q → R+
such that
</bodyText>
<equation confidence="0.94582">
for all q ∈ Q, F(q) + � T(q, σ) = 1. (1)
σEΣ
</equation>
<bodyText confidence="0.9992128">
PDFAs are typically represented as labeled di-
rected graphs (e.g. M′ in Figure 1).
A PDFA M generates a stochastic language
DM. If it exists, the (unique) path for a word w =
a0 ... ak belonging to E* through a PDFA is a
sequence h(q0, a0), (q1, a1), . . . , (qk, ak)i, where
qi+1 = δ(qi, ai). The probability a PDFA assigns
to w is obtained by multiplying the transition prob-
abilities with the final probability along w’s path if
it exists, and zero otherwise.
</bodyText>
<equation confidence="0.997945">
PrDM(w) = k 11O T (qi, ai) · F(qk+1) (2)
</equation>
<bodyText confidence="0.98142025">
if ˆd(q0, w)↓ and 0 otherwise
A stochastic language is regular deterministic iff
there is a PDFA which generates it.
The structural components of a PDFA M is the
deterministic finite-state automata (DFA) given by
the states Q, alphabet E, transitions δ, and initial
state q0 of M. By the structure of a PDFA, we
mean its structural components.1 Each PDFA M
defines a family of distributions given by the pos-
sible instantiations of T and F satisfying Equa-
tion 1. These distributions have at most |Q|· (|E|+
1) parameters (since for each state there are |E|
possible transitions plus the possibility of finality.)
These are, for all q ∈ Q and σ ∈ E, the proba-
bilities T(q, σ) and F(q). To make the connection
to probability theory, we sometimes write these as
Pr(σ  |q) and Pr(#  |q), respectively.
We define the product of PDFAs in terms of
co-emission probabilities (Vidal et al., 2005a).
Let M1 = hQ1,E1,q01,δ1,F1,T1i and M2 =
</bodyText>
<footnote confidence="0.993512666666667">
1This is up to the renaming of states so PDFA with iso-
morphic structural components are said to have the same
structure.
</footnote>
<page confidence="0.996549">
29
</page>
<bodyText confidence="0.741508">
(Q2, E2, q02, δ2, F2, T2) be PDFAs. The proba-
bility that σ1 is emitted from q1 E Q1 at the
same moment σ2 is emitted from q2 E Q2 is
</bodyText>
<equation confidence="0.76817075">
CT(σ1,σ2,q1,q2) = T1(q1,σ1)·T2(q2,σ2). Sim-
ilarly, the probability that a word simultaneously
ends at q1 E Q1 and at q2 E Q2 is CF(q1, q2) =
F1(q1)· F2(q2).
</equation>
<bodyText confidence="0.901213">
Definition 1 The normalized co-emission product
of PDFAs M1 and M2 is M = M1 x M2 =
(Q, E, q0, δ, F, T) where
</bodyText>
<listItem confidence="0.9461748">
1. Q, q0, and F are defined in terms of the
standard DFA product over the state space
Q1 x Q2 (Hopcroft et al., 2001).
2. E = E1 x E2
3. For all (q1, q2) E Q and (σ1, σ2) E
</listItem>
<equation confidence="0.999535230769231">
E, δ((q1, q2), (σ1, σ2)) = (q′1, q′2) iff
δ1(q1, σ1) = q′1 and δ2(q2, σ2) = q′2.2
4. For all (q1, q2) E Q,
(a) let Z((q1, q2)) = CF((q1, q2)) +
E
hσ1,σ2i∈Σ CT(σ1, σ2, q1, q2) be the
normalization term; and
(b) F((q1,q2)) = CF(q1,q2)
Z ; and
(c) for all (σ1, σ2) E E,
T ((q1, q2), (σ1, σ2)) =
CT(hσ1,σ2,q1,q2i)
Z
</equation>
<bodyText confidence="0.999583368421053">
In other words, the numerators of T and F are
defined to be the co-emission probabilities, and
division by Z ensures that M defines a well-
formed probability distribution.3 The normalized
co-emission product effectively adopts a statisti-
cal independence assumption between the states
of M1 and M2. If S is a list of PDFAs, we write
® S for their product (note order of product is ir-
relevant up to renaming of the states).
The maximum likelihood (ML) estimation of
regular deterministic distributions is a solved
problem when the structure of the PDFA is known
(Vidal et al., 2005a; Vidal et al., 2005b; de la
Higuera, 2010). Let S be a finite sample of words
drawn from a regular deterministic distribution D.
The problem is to estimate parameters T and F of
2Note that restricting 8 to cases when 0-1 = 0-2 obtains
the standard definition of 8 = 81 x 82 (Hopcroft et al., 2001).
The reason we maintain two alphabets becomes clear in §4.
</bodyText>
<equation confidence="0.992089">
3Z((q1, q2)) is less than one whenever either F1(q1) or
F2(q2) are neither zero nor one.
M so that DM approaches D using the widely-
adopted ML criterion (Equation 3).
Fˆ) = argmax PrM (w) (3)
T,F
(wi
</equation>
<bodyText confidence="0.999290862068965">
It is well-known that if D is generated by some
PDFA M′ with the same structural components as
M, then the ML estimate of S with respect to M
guarantees that DM approaches D as the size of
S goes to infinity (Vidal et al., 2005a; Vidal et al.,
2005b; de la Higuera, 2010).
Finding the ML estimate of a finite sample S
with respect to M is simple provided M is de-
terministic with known structural components. In-
formally, the corpus is passed through the PDFA,
and the paths of each word through the corpus are
tracked to obtain counts, which are then normal-
ized by state. Let M = (Q, E, δ, q0, F, T) be the
PDFA whose parameters F and T are to be esti-
mated. For all states q E Q and symbols σ E E,
The ML estimation of the probability of T (q, σ)
is obtained by dividing the number of times this
transition is used in parsing the sample S by the
number of times state q is encountered in the pars-
ing of S. Similarly, the ML estimation of F(q) is
obtained by calculating the relative frequency of
state q being final with state q being encountered
in the parsing of S. For both cases, the division is
normalizing; i.e. it guarantees that there is a well-
formed probability distribution at each state. Fig-
ure 1 illustrates the counts obtained for a machine
M with sample S = {abca}.4 Figure 1 shows
a DFA with counts and the PDFA obtained after
normalizing these counts.
</bodyText>
<sectionHeader confidence="0.956884" genericHeader="method">
3 Strictly local distributions
</sectionHeader>
<bodyText confidence="0.8932991875">
In formal language theory, strictly k-local lan-
guages occupy the bottom rung of a subregular
hierarchy which makes distinctions on the basis
of contiguous subsequences (McNaughton and Pa-
pert, 1971; Rogers and Pullum, to appear; Rogers
et al., 2009). They are also the categorical coun-
terpart to stochastic languages describable with n-
gram models (where n = k) (Garcia et al., 1990;
Jurafsky and Martin, 2008). Since stochastic lan-
guages are distributions, we refer to strictly k-
local stochastic languages as strictly k-local distri-
4Technically, M is neither a simple DFA or PDFA; rather,
it has been called a Frequency DFA. We do not formally de-
fine them here, see de la Higuera (2010).
(
Tˆ,
</bodyText>
<page confidence="0.937015">
30
</page>
<figureCaption confidence="0.922991">
Figure 1: M shows the counts obtained by parsing
it with sample S = {abca}. M′ shows the proba-
bilities obtained after normalizing those counts.
</figureCaption>
<bodyText confidence="0.925209666666667">
butions (SLDk). We illustrate with SLD2 (bigram
models) for ease of exposition.
For an alphabet E, SL2 distributions have
</bodyText>
<equation confidence="0.832609">
(|E |+ 1)2 parameters. These are, for all v, -r ∈
E ∪ {#}, the probabilities Pr(v  |-r). The proba-
bility of w = v1 ... vn is given in Equation 4.
Pr(w) def = Pr(v1  |#) × Pr(v2  |v1) (4)
× ... × Pr(#  |vn)
</equation>
<bodyText confidence="0.996760318181818">
PDFA representations of SL2 distributions have
the following structure: Q = E ∪ {#}, q0 = #,
and for all q ∈ Q and v ∈ E, it is the case that
δ(q, v) = v.
As an example, the DFA in Figure 2 provides
the structure of PDFAs which recognize SL2 dis-
tributions with E = {a, b, c}. Plainly, the param-
eters of the model are given by assigning proba-
bilities to each transition and to the ending at each
state. In fact, for all v ∈ E and -r ∈ E ∪ {#},
Pr(v  |-r) is T(-r, v) and Pr(#  |-r) is F(-r).
It follows that the probability of a particular path
through the model corresponds to Equation 4. The
structure of a SL2 distribution for alphabet E is
given by MSL2(E).
Additionally, given a finite sample S ⊂ E*, the
ML estimate of S with respect to the family of
distributions describable with MSL2(E) is given
by counting the parse of S through MSL2(E) and
then normalizing as described in §2. This is equiv-
alent to the procedure described in Jurafsky and
Martin (2008, chap. 4).
</bodyText>
<sectionHeader confidence="0.998789" genericHeader="method">
4 Feature-based distributions
</sectionHeader>
<bodyText confidence="0.99944675">
This section first introduces feature systems. Then
it defines feature-based SL2 distributions which
make the strong independence assumption that no
two features interact. It explains how to find
</bodyText>
<figureCaption confidence="0.8271235">
Figure 2: MSL2({a, b, c}) represents the structure
of SL2 distributions when E = {a, b, c}.
</figureCaption>
<equation confidence="0.96187525">
F G
+ -
+ +
- +
</equation>
<tableCaption confidence="0.6606655">
Table 1: An example of a feature system with E =
{a, b, c} and two features F and G.
</tableCaption>
<bodyText confidence="0.9999236">
the ML estimate of samples with respect to such
distributions. This section closes by identifying
kinds of featural interactions in phonotactic pat-
terns, and discusses how such interactions can be
addressed within this framework.
</bodyText>
<subsectionHeader confidence="0.997153">
4.1 Feature systems
</subsectionHeader>
<bodyText confidence="0.998957">
Assume the elements of the alphabet share prop-
erties, called features. For concreteness, let each
feature be a total function F : E → VF, where
the codomain VF is a finite set of values. A fi-
nite vector of features F = hF1, ... , Fni is called
a feature system. Table 1 provides an example
of a feature system with F = hF, Gi and values
</bodyText>
<equation confidence="0.7437954">
VF = VG = {+, −}.
We extend the domain of all features F ∈ F
to E+, so that F(v1 ... vn) = F(v1) ... F(vn).
For example, using the feature system in Table 1,
F(abc) = + + − and G(abc) = − + +. We
</equation>
<bodyText confidence="0.914699">
also extend the domain of F to all languages:
F(L) = ∪wELf(w). We also extend the notation
so that F(v) = hF1(v), ... , Fn(v)i. For example,
F(c) = h−F, +Gi (feature indices are included
for readability).
For feature F : E → VF, let F−1 be the inverse
function with domain VF and codomain P(E).
For example in Table 1, G−1(+) = {b, c}. F−1
is similarly defined, i.e. F−1(h−F,+Gi) = {c}.
</bodyText>
<figure confidence="0.997287038461539">
c:1 c:1/5
M M′
A:1
b:1
a:2
A:1/5
b:1/5
a:2/5
c
a
c
a
b
#
b
b
a
b
a
c
b
c
a
a
b
c
</figure>
<page confidence="0.999803">
31
</page>
<bodyText confidence="0.977514">
If, for all arguments v, F−1(v) is nonempty then
the feature system is exhaustive. If, for all argu-
ments v� such that F−1(v) is nonempty, it is the
case that |F−1(v) |= 1 then the feature system is
distinctive. E.g. the feature system in Table 1 in
not exhaustive since F−1(h−F, −Gi) = ∅, but itis
distinctive since where F−1 is nonempty, it picks
out exactly one element of the alphabet.
Generally, phonological feature systems for a
particular language are distinctive but not exhaus-
tive. Any feature system F can be made exhaustive
by adding finitely many symbols to the alphabet
(since F is finite). Let E′ denote an alphabet ob-
tained by adding to E the fewest symbols which
make F exhaustive.
Each feature system also defines a set of indi-
cator functions VF = U f∈F(Vf × {f}) with do-
main E such that hv, fi(σ) = 1 iff f(σ) = v and
0 otherwise. In the example in Table 1, VF =
{+F, −F, +G, −G} (omitting angle braces for
readability). For all f ∈ F, the set VFf is the
VF restricted to f. So continuing our example,
VFF = {+F, −F}.
</bodyText>
<subsectionHeader confidence="0.858054">
4.2 Feature-based distributions
</subsectionHeader>
<bodyText confidence="0.9952062">
We now define feature-based SL2 distributions un-
der the strong independence assumption that no
two features interact. For feature system F =
hF1 ... Fni, there are n PDFAs, one for each fea-
ture. The normalized co-emission product of these
PDFAs essentially defines the distribution. For
each FZ, the structure of its PDFA is given by
MSL2(VFi). For example, MF = MSL2(VF)
and MG = MSL2(VG) in figures 3 and 4 illustrate
the finite-state representation of feature-based SL2
distributions given the feature system in Table 1.5
The states of each machine make distinctions ac-
cording to features F and G, respectively. The pa-
rameters of these distributions are given by assign-
ing probabilities to each transition and to the end-
ing at each state (except for Pr(#  |#)).6
Thus there are 2|VF |+ EF∈F |VFF |2 + 1 pa-
rameters for feature-based SL2 distributions. For
example, the feature system in Table 1 defines a
distribution with 2·4 + 22 + 22 + 1 = 17 param-
</bodyText>
<footnote confidence="0.984012">
5For readability, featural information in the states and
transitions is included in these figures. By definition, the
states and transitions are only labeled with elements of VF
and VG, respectively. In this case, that makes the structures
of the two machines identical.
6It is possible to replace Pr(# I #) with two parameters,
Pr(# I #F) Pr(# I #G), but for ease of exposition we do
not pursue this further.
</footnote>
<figureCaption confidence="0.99919075">
Figure 3: MF represents a SL2 distribution with
respect to feature F.
Figure 4: MG represents a SL2 distribution with
respect to feature G.
</figureCaption>
<bodyText confidence="0.9698914">
eters, which include Pr(#  |+F), Pr(+F  |#),
Pr(+F  |+F), Pr(+F  |−F), ..., the G equiva-
lents, and Pr(#  |#). Let SLD2F be the family of
distributions given by all possible parameter set-
tings (i.e. all possible probability assignments for
each MSL2(VFi) in accordance with Equation 1.)
The normalized co-emission product defines the
feature-based distribution. For example, the struc-
ture of the product of MF and MG is shown in
Figure 5.
As defined, the normalized co-emission product
can result in states and transitions that cannot be
interpreted by non-exhaustive feature systems. An
example of this is in Figure 5 since h−F, −Gi is
not interpretable by the feature system in Table 1.
We make the system exhaustive by letting E′ =
E ∪ {d} and setting F(d) = h−F, −Gi.
What is the probability of a given b in the
feature-based model? According to the normal-
ized co-emission product (Defintion 1), it is
</bodyText>
<equation confidence="0.978083">
Pr(a  |b) = Pr(h+F, −Gi  |h+F, +Gi) =
Pr(+F  |+F)· Pr(−G  |+G)
Z
where Z = Z(h+F, +Gi) equals
E Pr(F(σ)  |+F)· Pr(G(σ)  |+G)
v∈E′
+ (Pr(#  |+F)· Pr(#  |+G)
</equation>
<bodyText confidence="0.977438">
Generally, for an exhuastive distinctive feature
system F = hF1, ... , Fni, and for all σ, τ ∈ E,
</bodyText>
<figure confidence="0.989463744186047">
+F
#
-F
+F
-F
+F
-F
+F
-F
+ G
#
-G
+ G
-G
+ G
-G
+ G
-G
32
-F,+G
#
+F,-G
+F,+G
+F,-G
+F,-G
+F,+G
+F,-G
-F,-G
+F,+G
+F,+G
+F,-G
-F,-G
-F,-G
+F,+G
+F,-G
-F,+G
+F,+G
-F,+G
-F,-G -F,+G
-F,-G
-F,-G
-F,+G
-F,+G
</figure>
<figureCaption confidence="0.999984">
Figure 5: The structure of the product of MF and MG.
</figureCaption>
<equation confidence="0.9902177">
the Pr(σ  |τ) is given by Equation 5. First, the
normalization term is provided. Let
� �
�rl Pr(Fi(σ)  |Fi(τ)) �
1&lt;i&lt;n
+ � Pr(#  |Fi(τ))
1&lt;i&lt;n
Then
( ) = f11&lt;i&lt;n Pr(Fi(σ)  |Fi(τ)) (5)
Pr (o- T Z(τ)
</equation>
<bodyText confidence="0.991231529411764">
The probabilities Pr(σ  |#) and Pr(#  |τ)
are similarly decomposed into featural parameters.
Finally, like SL2 distributions, the probability of a
word w G E* is given by Equation 4. We have
thus proved the following.
Theorem 1 The parameters of a feature-based
SL2 distribution define a well-formed probability
distribution over E*.
Proof It is sufficient to show for all τ G E U {#}
that &amp;EEu{#} Pr(σ  |τ) = 1 since in this
case, Equation 4 yields a well-formed probability
distribution over E*. This follows directly from
the definition of the normalized co-emission
product (Definition 1). ❑
The normalized co-emission product adopts a
statistical independence assumption, which here is
between features since each machine represents a
single feature. For example, consider Pr(a  |b) =
Pr((−F, +G)  |(+F, +G)). The probability
Pr((−F, +G)  |(+F, +G)) cannot be arbitrar-
ily different from the probabilities Pr(−F  |+F)
and Pr(+G  |+G); it is not an independent pa-
rameter. In fact, because Pr(a  |b) is computed
directly as the normalized product of parameters
Pr(−F  |+F) and Pr(+G  |+G), the assump-
tion is that the features F and G do not interact. In
other words, this model describes exactly the state
of affairs one expects if there is no statistical in-
teraction between phonological features. In terms
of inference, this means if one sound is observed
to occur in some context (at least contexts dis-
tinguishable by SL2 models), then similar sounds
(i.e. those that share many of its featural values)
are expected to occur in this context as well.
</bodyText>
<subsectionHeader confidence="0.993888">
4.3 ML estimation
</subsectionHeader>
<bodyText confidence="0.998741684210526">
The ML estimate of feature-based SL2 distribu-
tions is obtained by counting the parse of a sample
through each feature machine, and normalizing the
results. This is because the parameters of the dis-
tribution are the probabilities on the feature ma-
chines, whose product determines the actual dis-
tribution. The following theorem follows imme-
diately from the PDFA representation of feature-
based SL2 distributions.
Theorem 2 Let F = (F1,... Fn) and let D be de-
scribed by M = ®1&lt;i&lt;n MSL2(VF i). Consider
a finite sample S drawn from D. Then the ML es-
timate of S with respect to SLD2F is obtained by
finding, for each Fi G F, the ML estimate ofFi(S)
with respect to MSL2(VF i).
Proof The ML estimate of S with respect to
SLD2F returns the parameter values that maxi-
mize the likelihood of S within the family SLD2F.
The parameters of D GSLD2F are found on the
</bodyText>
<equation confidence="0.9870275">
Z(τ) = E
σEE
</equation>
<page confidence="0.985671">
33
</page>
<bodyText confidence="0.949377153846154">
states of each MSL2(VFi). By definition, each
MSL2(VFi) describes a probability distribution
over FZ(E∗), as well as a family of distributions.
Therefore finding the MLE of S with respect to
SLD2F means finding the MLE estimate of FZ(S)
with respect to each MSL2(VFi).
Optimizing the ML estimate of FZ(S) for
each MZ = MSL2(VFi) means that as |FZ(S)|
increases, the estimates ˆTMi and ˆFMi approach
the true values TMi and FMi. It follows that
as |S |increases, ˆTM and ˆFM approach the true
values of TM and FM and consequently DM
approaches D. ❑
</bodyText>
<subsectionHeader confidence="0.892144">
4.4 Discussion
</subsectionHeader>
<bodyText confidence="0.998351">
Feature-based models can have significantly fewer
parameters than segment-based models. Con-
sider binary feature systems, where |VF |= 2|F|.
An exhaustive feature system with 10 binary fea-
tures describes an alphabet with 1024 symbols.
Segment-based bigram models have (1024+1)2 =
1, 050, 625 parameters, but the feature-based one
only has 40 + 40 + 1 = 81 parameters! Con-
sequently, much less training data is required to
accurately estimate the parameters of the model.
Another way of describing this is in terms of ex-
pressivity. For given feature system, feature-based
SL2 distributions are a proper subset of SL2 dis-
tributions since, as the the PDFA representations
make clear, every feature-based distribution can be
described by a segmental bigram model, but not
vice versa. The fact that feature-based distribu-
tions have potentially far fewer parameters is a re-
flection of the restrictive nature of the model. The
statistical independence assumption constrains the
system in predictable ways. The next section
shows exactly what feature-based generalization
looks like under these assumptions.
</bodyText>
<sectionHeader confidence="0.992123" genericHeader="method">
5 Examples
</sectionHeader>
<bodyText confidence="0.9996324">
This section demonstrates feature-based gener-
alization by comparing it with segment-based
generalization, using a small corpus S =
{aaab, caca, acab, cbb} and the feature system
in Table 1. Tables 2 and 3 show the results of
ML estimation of S with respect to segment-based
SL2 distributions (unsmoothed bigram model)
and feature-based SL2 distributions, respectively.
Each table shows the Pr(σ  |τ) for all σ, τ E
{a, b, c, d, #} (where F(d) = (−F, −G)), for
</bodyText>
<table confidence="0.918699142857143">
P(σ  |τ) σ
a b c d #
a 0.29 0.29 0.29 0. 0.14
b 0. 0.25 0. 0. 0.75
τ c 0.75 0.25 0. 0. 0.
d 0. 0. 0. 0. 0.
# 0.5 0. 0.5 0. 0.
</table>
<tableCaption confidence="0.832017">
Table 2: ML estimates of parameters of segment-
based SL2 distributions.
</tableCaption>
<table confidence="0.974837571428571">
P(σ  |τ) σ
a b c d #
a 0.22 0.43 0.17 0.09 0.09
b 0.32 0.21 0.09 0.13 0.26
τ c 0.60 0.40 0. 0 0.
d 0.33 0.67 0 0 0
# 0.25 0.25 0.25 0.25 0.
</table>
<tableCaption confidence="0.9785565">
Table 3: ML estimates of parameters of feature-
based SL2 distributions.
</tableCaption>
<bodyText confidence="0.997523346153846">
ease of comparison.
Observe the sharp divergence between the two
models in certain cells. For example, no words be-
gin with b in the sample. Hence the segment-based
ML estimates of Pr(b  |#) is zero. Conversely,
the feature-based ML estimate is nonzero because
b, like a, is +F, and b, like c, is +G, and both a
and c begin words. Also, notice nonzero probabil-
ities are assigned to d occuring after a and b. This
is because F(d) = (−F, −G) and the following
sequences all occur in the corpus: [+F][-F] (ac),
[+G][-G] (ca), and [-G][-G] (aa). On the other
hand, zero probabilities are assigned to d ocurring
after c and d because there are no cc sequences in
the corpus and hence the probability of [-F] occur-
ing after [-F] is zero.
This simple example demonstrates exactly how
the model works. Generalizations are made on the
basis of individual features, not individual sym-
bols. In fact, segments are truly epiphenomenal in
this model, as demonstrated by the nonzero prob-
abilties assigned to segments outside the original
feature system (here, this is d). To sum up, this
model captures exactly the idea that the distribu-
tion of segments is conditioned on the distribu-
tions of its features.
</bodyText>
<page confidence="0.998565">
34
</page>
<sectionHeader confidence="0.993904" genericHeader="method">
6 Featural interaction
</sectionHeader>
<bodyText confidence="0.995673890243903">
In many empirical cases of interest, features do
interact, which suggests the strong independence
assumption is incorrect for modeling phonotactic
learning.
There are at least four kinds of featural inter-
action. First, different features may be prohib-
ited from occuring simultaneously in certain con-
texts. As an example of the first type consider
the fact that both velars and nasal sounds occur
word-initially in English, but the velar nasal may
not. Second, specific languages may prohibit dif-
ferent features from simultaneously occuring in all
contexts. In English, for example, there are syl-
labic sounds and obstruents but no syllabic obstru-
ents. Third, different features may be universally
incompatible: e.g. no vowels are both [+high] and
[+low]. The last type of interaction is that different
features may be prohibited from occuring syntag-
matically. For example, some languages prohibit
voiceless sounds from occuring after nasals.
Although the independence assumption is too
strong, it is still useful. First, it allows researchers
to quantify the extent to which data can be ex-
plained without invoking featural interaction. For
example, following Hayes and Wilson (2008), we
may be interested in how well human acceptabil-
ity judgements collected by Scholes (1966) can be
explained if different features do not interact. Af-
ter training the feature-based SL2 model on a cor-
pus of word initial onsets adapted from the CMU
pronouncing dictionary (Hayes and Wilson, 2008,
395-396) and using a standard phonological fea-
ture system (Hayes, 2009, chap. 4), it achieves
a correlation (Spearman’s r) of 0.751.7 In other
words, roughly three quarters of the acceptability
judgements are explained without relying on feat-
ural interaction (or segments).
Secondly, the incorrect predictions of the model
are in principle detectable. For example, recall
that English has word-inital velars and nasals, but
no word-inital velar nasals. A one-cell chi-squared
test can determine whether the observed number
of [#xj] is significantly below the expected number
according to the feature-based distribution, which
could lead to a new parameter being adopted to
describe the interaction of the [dorsal] and [nasal]
7We use the feature chart in Hayes (2009) because it con-
tains over 150 IPA symbols (and not just English phonemes).
Featural combinations not in the chart were assumed to be
impossible (e.g. [+high,+low]) and were zeroed out.
features word-initially. The details of these proce-
dures are left for future research and are likely to
draw from the rich literature on Bayesian networks
(Pearl, 1989; Ghahramani, 1998).
More important, however, is this framework al-
lows researchers to construct the independence as-
sumptions they want into the model in at least two
ways. First, universally incompatible features can
be excluded. For example, suppose [-F] and [-G]
in the feature system in Table 1 are anatomically
incompatible like [+low] and [+high]. If desired,
they can be excluded from the model essentially
by zeroing out any probability mass assigned to
such combinations and re-normalizing.
Second, models can be defined where multiple
features are permitted to interact. For example,
suppose features F and G from Table 1 are em-
bedded in a larger feature system. The machine
in Figure 5 can be defined to be a factor of the
model, and now interactions between F and G will
be learned, including syntagmatic ones. The flex-
ibility of the framework and the generality of the
normalized co-emission product allow researchers
to consider feature-based distributions which al-
low any two features to interact but which pro-
hibit three-feature interactions, or which allow any
three features to interact but which prohibit four-
feature interactions, or models where only certain
features are permitted to interact but not others
(perhaps because they belong to the same node in a
feature geometry (Clements, 1985; Clements and
Hume, 1995).8
</bodyText>
<sectionHeader confidence="0.955173" genericHeader="evaluation">
7 Hayes and Wilson (2008)
</sectionHeader>
<bodyText confidence="0.982303133333333">
This section introduces the Hayes and Wilson
(2008) (henceforth HW) phonotactic learner and
shows that the contribution features play in gener-
alization is not as clear as previously thought.
HW propose an inductive model which ac-
quires a maxent grammar defined by weighted
constraints. Each constraint is described as a se-
quence of natural classes using phonological fea-
tures. The constraint format also allows reference
to word boundaries and at most one complement
class. (The complement class of S ⊆ Σ is Σ/S.)
For example, the constraint
*#[ˆ -voice,+anterior,+strident][-approximant]
means that in word-initial C1C2 clusters, if C2 is a
nasal or obstruent, then C1 must be [s].
</bodyText>
<footnote confidence="0.9826265">
8Note if all features are permitted to interact, this yields
the segmental bigram model.
</footnote>
<page confidence="0.993462">
35
</page>
<table confidence="0.9766136">
Hayes and Wilson maxent models r
features &amp; complement classes 0.946
no features &amp; complement classes 0.937
features &amp; no complement classes 0.914
no features &amp; no complement classes 0.885
</table>
<tableCaption confidence="0.924831">
Table 4: Correlations of different settings versions
of HW maxent model with Scholes data.
</tableCaption>
<bodyText confidence="0.9821724">
HW report that the model obtains a correlation
(Spearman’s r) of 0.946 with blick test data from
Scholes (1966). HW and Albright (2009) attribute
this high correlation to the model’s use of natural
classes and phonological features. HW also report
that when the model is run without features, the
grammar obtained scores an r value of only 0.885,
implying that the gain in correlation is due specif-
ically to the use of phonological features.
However, there are two relevant issues. The first
is the use of complement classes. If features are
not used but complement classes are (in effect only
allowing the model to refer to single segments and
the complements of single segments, e.g. [t] and
[ˆt]) then in fact the grammar obtained scores an
r value of 0.936, a result comparable to the one
reported.9 Table 4 shows the r values obtained by
the HW learner under different conditions. Note
we replicate the main result of r = 0.946 when
using both features and complement classes.10
This exercise reveals that phonological features
play a smaller role in the HW phonotactic learner
than previously thought. Features are helpful, but
not as much as complement classes of single seg-
ments (though features with complement classes
yields the best result by this measure).
The second issue relates to the first: the question
of whether additional parameters are worth the
gain in empirical coverage. Wilson and Obdeyn
(2009) provide an excellent discussion of the
model comparison literature and provide a rigor-
ous comparative analysis of computational mod-
eleling of OCP restrictions. Here we only raise the
questions and leave the answers to future research.
Compare the HW learners in the first two rows
in Table 4. Is the ∼ 0.01 gain in r score worth
the additional parameters which refer to phono-
9Examination of the output grammar reveals heavy re-
liance on the complement class [ˆs], which is not surprising
given the discussion of [sC] clusters in HW.
</bodyText>
<footnote confidence="0.708511">
10This software is available on Bruce Hayes’ webpage:
http://www.linguistics.ucla.edu/
people/hayes/Phonotactics/index.htm.
</footnote>
<bodyText confidence="0.994698">
logically natural classes? Also, the feature-based
SL2 model in §4 only receives an r score of 0.751,
much lower than the results in Table 4. Yet this
model has far fewer parameters not only because
the maxent models in Table 4 keep track of tri-
grams, but also because of its strong independence
assumption. As mentioned, this result is infor-
mative because it reveals how much can be ex-
plained without featural interaction. In the con-
text of model comparison, this particular model
provides an inductive baseline against which the
utility of additional parameters invoking featural
interaction ought to be measured.
</bodyText>
<sectionHeader confidence="0.997329" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999953608695652">
The current proposal explicitly embeds the Jakob-
sonian hypothesis that the primitive unit of
phonology is the phonological feature into a
phonotactic learning model. While this paper
specifically shows how to integrate features into
n-gram models to describe feature-based strictly
n-local distributions, these techniques can be ap-
plied to other regular deterministic distributions,
such as strictly k-piecewise models, which de-
scribe long-distance dependencies, like the ones
found in consonant and vowel harmony (Heinz, to
appear; Heinz and Rogers, 2010).
In contrast to models which assume that all
features potentially interact, a baseline model
was specifically introduced under the assumption
that no two features interact. In this way, the
“bottom-up” approach to feature-based general-
ization shifts the focus of inquiry to the featural
interactions necessary (and ultimately sufficient)
to describe and learn phonotactic patterns. The
framework introduced here shows how researchers
can study feature interaction in phonotactic mod-
els in a systematic, transparent way.
</bodyText>
<sectionHeader confidence="0.998255" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.996182666666667">
We thank Bill Idsardi, Tim O’Neill, Jim Rogers,
Robert Wilder, Colin Wilson and the U. of
Delaware’s phonology/phonetics group for valu-
able discussion. Special thanks to Mark Ellison
for helpful comments, to Adam Albright for illu-
minating remarks on the types of featural interac-
tion in phonotactic patterns, and to Jason Eisner
for bringing to our attention FHMMs and other re-
lated work.
</bodyText>
<page confidence="0.997475">
36
</page>
<sectionHeader confidence="0.993884" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999871247524753">
Adam Albright. 2009. Feature-based generalisation
as a source of gradient acceptability. Phonology,
26(1):9–41.
Noam Chomsky and Morris Halle. 1968. The Sound
Pattern ofEnglish. Harper &amp; Row, New York.
G.N. Clements and Elizabeth V. Hume. 1995. The
internal organization of speech sounds. In John A.
Goldsmith, editor, The handbook of phonological
theory, chapter 7. Blackwell, Cambridge, MA.
George N. Clements. 1985. The geometry of phono-
logical features. Phonology Yearbook, 2:225–252.
Colin de la Higuera. 2010. Grammatical Inference:
Learning Automata and Grammars. Cambridge
University Press.
Markus Dreyer and Jason Eisner. 2009. Graphical
models over multiple strings. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 101–110, Singa-
pore, August.
Markus Dreyer, Jason R. Smith, and Jason Eisner.
2008. Latent-variable modeling of string transduc-
tions with finite-state methods. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 1080–1089,
Honolulu, October.
Pedro Garcia, Enrique Vidal, and Jos´e Oncina. 1990.
Learning locally testable languages in the strict
sense. In Proceedings of the Workshop on Algorith-
mic Learning Theory, pages 325–338.
Zoubin Ghahramani and Michael I. Jordan. 1997. Fac-
torial hidden markov models. Machine Learning,
29(2):245–273.
Zoubin Ghahramani. 1998. Learning dynamic
bayesian networks. In Adaptive Processing of
Sequences and Data Structures, pages 168–197.
Springer-Verlag.
Daniel Gildea and Daniel Jurafsky. 1996. Learn-
ing bias and phonological-rule induction. Compu-
tational Linguistics, 24(4).
Bruce Hayes and Colin Wilson. 2008. A maximum en-
tropy model of phonotactics and phonotactic learn-
ing. Linguistic Inquiry, 39:379–440.
Bruce Hayes. 2009. Introductory Phonology. Wiley-
Blackwell.
Jeffrey Heinz and James Rogers. 2010. Estimating
strictly piecewise distributions. In Proceedings of
the 48th Annual Meeting ofthe Association for Com-
putational Linguistics, Uppsala, Sweden.
Jeffrey Heinz. to appear. Learning long-distance
phonotactics. Linguistic Inquiry, 41(4).
John Hopcroft, Rajeev Motwani, and Jeffrey Ullman.
2001. Introduction to Automata Theory, Languages,
and Computation. Boston, MA: Addison-Wesley.
Roman Jakobson, C. Gunnar, M. Fant, and Morris
Halle. 1952. Preliminaries to Speech Analysis.
MIT Press.
Daniel Jurafsky and James Martin. 2008. Speech
and Language Processing: An Introduction to Nat-
ural Language Processing, Speech Recognition, and
Computational Linguistics. Prentice-Hall, Upper
Saddle River, NJ, 2nd edition.
Robert McNaughton and Seymour Papert. 1971.
Counter-Free Automata. MIT Press.
Elliot Moreton. 2008. Analytic bias and phonological
typology. Phonology, 25(1):83–127.
Judea Pearl. 1989. Probabilistic Reasoning in In-
telligent Systems: Networks of Plausible Inference.
Morgan Kauffman.
James Rogers and Geoffrey Pullum. to appear. Aural
pattern recognition experiments and the subregular
hierarchy. Journal of Logic, Language and Infor-
mation.
James Rogers, Jeffrey Heinz, Gil Bailey, Matt Edlef-
sen, Molly Visscher, David Wellcome, and Sean
Wibel. 2009. On languages piecewise testable in
the strict sense. In Proceedings of the 11th Meeting
of the Assocation for Mathematics ofLanguage.
Lawrence K. Saul and Michael I. Jordan. 1999. Mixed
memory markov models: Decomposing complex
stochastic processes as mixtures of simpler ones.
Machine Learning, 37(1):75–87.
Robert J. Scholes. 1966. Phonotactic grammaticality.
Mouton, The Hague.
Enrique Vidal, Franck Thollard, Colin de la Higuera,
Francisco Casacuberta, and Rafael C. Carrasco.
2005a. Probabilistic finite-state machines-part I.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 27(7):1013–1025.
Enrique Vidal, Frank Thollard, Colin de la Higuera,
Francisco Casacuberta, and Rafael C. Carrasco.
2005b. Probabilistic finite-state machines-part II.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 27(7):1026–1039.
Colin Wilson and Marieke Obdeyn. 2009. Simplifying
subsidiary theory: statistical evidence from arabic,
muna, shona, and wargamay. Johns Hopkins Uni-
versity.
Colin Wilson. 2006. Learning phonology with sub-
stantive bias: An experimental and computational
study of velar palatalization. Cognitive Science,
30(5):945–982.
</reference>
<page confidence="0.999608">
37
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.740933">
<title confidence="0.999755">Maximum Likelihood Estimation of Feature-based Distributions</title>
<author confidence="0.999599">Jeffrey Heinz</author>
<author confidence="0.999599">Cesar</author>
<affiliation confidence="0.999585">University of</affiliation>
<address confidence="0.819034">Newark, Delaware,</address>
<abstract confidence="0.994474315789473">Motivated by recent work in phonotactic learning (Hayes and Wilson 2008, Albright 2009), this paper shows how to define feature-based probability distributions whose parameters can be provably efficiently estimated. The main idea is that these distributions are defined as a product of simpler distributions (cf. Ghahramani and Jordan 1997). One advantage of this framework is it draws attention to what is minimally necessary to describe and learn phonological feature interactions in phonotactic patterns. The “bottom-up” approach adopted here is contrasted with the “top-down” approach in Hayes and Wilson (2008), and it is argued that the bottom-up approach is more analytically transparent.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Adam Albright</author>
</authors>
<title>Feature-based generalisation as a source of gradient acceptability.</title>
<date>2009</date>
<journal>Phonology,</journal>
<volume>26</volume>
<issue>1</issue>
<contexts>
<context position="1454" citStr="Albright, 2009" startWordPosition="213" endWordPosition="214">re analytically transparent. 1 Introduction The hypothesis that the atomic units of phonology are phonological features, and not segments, is one of the tenets of modern phonology (Jakobson et al., 1952; Chomsky and Halle, 1968). According to this hypothesis, segments are essentially epiphenomenal and exist only by virtue of being a shorthand description of a collection of more primitive units—the features. Incorporating this hypothesis into phonological learning models has been the focus of much influential work (Gildea and Jurafsky, 1996; Wilson, 2006; Hayes and Wilson, 2008; Moreton, 2008; Albright, 2009). This paper makes three contributions. The first contribution is a framework within which: 1. researchers can choose which statistical independence assumptions to make regarding phonological features; 2. feature systems can be fully integrated into strictly local (McNaughton and Papert, 1971) (i.e. n-gram models (Jurafsky and Martin, 2008)) and strictly piecewise models (Rogers et al., 2009; Heinz and Rogers, 2010) in order to define families of provably wellformed, feature-based probability distributions that are provably efficiently estimable. The main idea is to define a family of distribu</context>
<context position="30908" citStr="Albright (2009)" startWordPosition="5363" endWordPosition="5364">-approximant] means that in word-initial C1C2 clusters, if C2 is a nasal or obstruent, then C1 must be [s]. 8Note if all features are permitted to interact, this yields the segmental bigram model. 35 Hayes and Wilson maxent models r features &amp; complement classes 0.946 no features &amp; complement classes 0.937 features &amp; no complement classes 0.914 no features &amp; no complement classes 0.885 Table 4: Correlations of different settings versions of HW maxent model with Scholes data. HW report that the model obtains a correlation (Spearman’s r) of 0.946 with blick test data from Scholes (1966). HW and Albright (2009) attribute this high correlation to the model’s use of natural classes and phonological features. HW also report that when the model is run without features, the grammar obtained scores an r value of only 0.885, implying that the gain in correlation is due specifically to the use of phonological features. However, there are two relevant issues. The first is the use of complement classes. If features are not used but complement classes are (in effect only allowing the model to refer to single segments and the complements of single segments, e.g. [t] and [ˆt]) then in fact the grammar obtained s</context>
</contexts>
<marker>Albright, 2009</marker>
<rawString>Adam Albright. 2009. Feature-based generalisation as a source of gradient acceptability. Phonology, 26(1):9–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
<author>Morris Halle</author>
</authors>
<title>The Sound Pattern ofEnglish.</title>
<date>1968</date>
<publisher>Harper &amp; Row,</publisher>
<location>New York.</location>
<contexts>
<context position="1067" citStr="Chomsky and Halle, 1968" startWordPosition="154" endWordPosition="157">s a product of simpler distributions (cf. Ghahramani and Jordan 1997). One advantage of this framework is it draws attention to what is minimally necessary to describe and learn phonological feature interactions in phonotactic patterns. The “bottom-up” approach adopted here is contrasted with the “top-down” approach in Hayes and Wilson (2008), and it is argued that the bottom-up approach is more analytically transparent. 1 Introduction The hypothesis that the atomic units of phonology are phonological features, and not segments, is one of the tenets of modern phonology (Jakobson et al., 1952; Chomsky and Halle, 1968). According to this hypothesis, segments are essentially epiphenomenal and exist only by virtue of being a shorthand description of a collection of more primitive units—the features. Incorporating this hypothesis into phonological learning models has been the focus of much influential work (Gildea and Jurafsky, 1996; Wilson, 2006; Hayes and Wilson, 2008; Moreton, 2008; Albright, 2009). This paper makes three contributions. The first contribution is a framework within which: 1. researchers can choose which statistical independence assumptions to make regarding phonological features; 2. feature </context>
</contexts>
<marker>Chomsky, Halle, 1968</marker>
<rawString>Noam Chomsky and Morris Halle. 1968. The Sound Pattern ofEnglish. Harper &amp; Row, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G N Clements</author>
<author>Elizabeth V Hume</author>
</authors>
<title>The internal organization of speech sounds. In</title>
<date>1995</date>
<booktitle>The handbook of phonological theory, chapter 7.</booktitle>
<editor>John A. Goldsmith, editor,</editor>
<publisher>Blackwell,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="29689" citStr="Clements and Hume, 1995" startWordPosition="5167" endWordPosition="5170"> be a factor of the model, and now interactions between F and G will be learned, including syntagmatic ones. The flexibility of the framework and the generality of the normalized co-emission product allow researchers to consider feature-based distributions which allow any two features to interact but which prohibit three-feature interactions, or which allow any three features to interact but which prohibit fourfeature interactions, or models where only certain features are permitted to interact but not others (perhaps because they belong to the same node in a feature geometry (Clements, 1985; Clements and Hume, 1995).8 7 Hayes and Wilson (2008) This section introduces the Hayes and Wilson (2008) (henceforth HW) phonotactic learner and shows that the contribution features play in generalization is not as clear as previously thought. HW propose an inductive model which acquires a maxent grammar defined by weighted constraints. Each constraint is described as a sequence of natural classes using phonological features. The constraint format also allows reference to word boundaries and at most one complement class. (The complement class of S ⊆ Σ is Σ/S.) For example, the constraint *#[ˆ -voice,+anterior,+stride</context>
</contexts>
<marker>Clements, Hume, 1995</marker>
<rawString>G.N. Clements and Elizabeth V. Hume. 1995. The internal organization of speech sounds. In John A. Goldsmith, editor, The handbook of phonological theory, chapter 7. Blackwell, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George N Clements</author>
</authors>
<title>The geometry of phonological features.</title>
<date>1985</date>
<journal>Phonology Yearbook,</journal>
<pages>2--225</pages>
<contexts>
<context position="29663" citStr="Clements, 1985" startWordPosition="5165" endWordPosition="5166">an be defined to be a factor of the model, and now interactions between F and G will be learned, including syntagmatic ones. The flexibility of the framework and the generality of the normalized co-emission product allow researchers to consider feature-based distributions which allow any two features to interact but which prohibit three-feature interactions, or which allow any three features to interact but which prohibit fourfeature interactions, or models where only certain features are permitted to interact but not others (perhaps because they belong to the same node in a feature geometry (Clements, 1985; Clements and Hume, 1995).8 7 Hayes and Wilson (2008) This section introduces the Hayes and Wilson (2008) (henceforth HW) phonotactic learner and shows that the contribution features play in generalization is not as clear as previously thought. HW propose an inductive model which acquires a maxent grammar defined by weighted constraints. Each constraint is described as a sequence of natural classes using phonological features. The constraint format also allows reference to word boundaries and at most one complement class. (The complement class of S ⊆ Σ is Σ/S.) For example, the constraint *#[</context>
</contexts>
<marker>Clements, 1985</marker>
<rawString>George N. Clements. 1985. The geometry of phonological features. Phonology Yearbook, 2:225–252.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin de la Higuera</author>
</authors>
<title>Grammatical Inference: Learning Automata and Grammars.</title>
<date>2010</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="9683" citStr="Higuera, 2010" startWordPosition="1630" endWordPosition="1631"> words, the numerators of T and F are defined to be the co-emission probabilities, and division by Z ensures that M defines a wellformed probability distribution.3 The normalized co-emission product effectively adopts a statistical independence assumption between the states of M1 and M2. If S is a list of PDFAs, we write ® S for their product (note order of product is irrelevant up to renaming of the states). The maximum likelihood (ML) estimation of regular deterministic distributions is a solved problem when the structure of the PDFA is known (Vidal et al., 2005a; Vidal et al., 2005b; de la Higuera, 2010). Let S be a finite sample of words drawn from a regular deterministic distribution D. The problem is to estimate parameters T and F of 2Note that restricting 8 to cases when 0-1 = 0-2 obtains the standard definition of 8 = 81 x 82 (Hopcroft et al., 2001). The reason we maintain two alphabets becomes clear in §4. 3Z((q1, q2)) is less than one whenever either F1(q1) or F2(q2) are neither zero nor one. M so that DM approaches D using the widelyadopted ML criterion (Equation 3). Fˆ) = argmax PrM (w) (3) T,F (wi It is well-known that if D is generated by some PDFA M′ with the same structural compo</context>
<context position="12280" citStr="Higuera (2010)" startWordPosition="2108" endWordPosition="2109"> rung of a subregular hierarchy which makes distinctions on the basis of contiguous subsequences (McNaughton and Papert, 1971; Rogers and Pullum, to appear; Rogers et al., 2009). They are also the categorical counterpart to stochastic languages describable with ngram models (where n = k) (Garcia et al., 1990; Jurafsky and Martin, 2008). Since stochastic languages are distributions, we refer to strictly klocal stochastic languages as strictly k-local distri4Technically, M is neither a simple DFA or PDFA; rather, it has been called a Frequency DFA. We do not formally define them here, see de la Higuera (2010). ( Tˆ, 30 Figure 1: M shows the counts obtained by parsing it with sample S = {abca}. M′ shows the probabilities obtained after normalizing those counts. butions (SLDk). We illustrate with SLD2 (bigram models) for ease of exposition. For an alphabet E, SL2 distributions have (|E |+ 1)2 parameters. These are, for all v, -r ∈ E ∪ {#}, the probabilities Pr(v |-r). The probability of w = v1 ... vn is given in Equation 4. Pr(w) def = Pr(v1 |#) × Pr(v2 |v1) (4) × ... × Pr(# |vn) PDFA representations of SL2 distributions have the following structure: Q = E ∪ {#}, q0 = #, and for all q ∈ Q and v ∈ E,</context>
</contexts>
<marker>Higuera, 2010</marker>
<rawString>Colin de la Higuera. 2010. Grammatical Inference: Learning Automata and Grammars. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Dreyer</author>
<author>Jason Eisner</author>
</authors>
<title>Graphical models over multiple strings.</title>
<date>2009</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>101--110</pages>
<location>Singapore,</location>
<contexts>
<context position="2957" citStr="Dreyer and Eisner, 2009" startWordPosition="445" endWordPosition="448">, F may have many fewer parameters than if F is defined over the product PDFA directly. This is because the parameters of the distributions are defined in terms of the factors which combine in predictable ways via the product. Fewer parameters means accurate estimation occurs with less data and, relatedly, the family contains fewer distributions. This idea is not new. It is explicit in Factorial Hidden Markov Models (FHMMs) (Ghahramani and Jordan, 1997; Saul and Jordan, 1999), and more recently underlies approaches to describing and inferring regular string transductions (Dreyer et al., 2008; Dreyer and Eisner, 2009). Although HMMs and probabilistic finite-state automata describe the same class of distributions (Vidal et al., 2005a; Vidal et al., 2005b), this paper presents these ideas in formal language-theoretic and automata-theoretic terms because (1) there are no hidden states and is thus simpler than FHMMs, (2) determinstic automata have several desirable properties crucially used here, and (3) PDFAs add probabilities to structure whereas HMMs add structure to probabilities and the authors are more comfortable with the former perspective (for further discussion, see Vidal et al. (2005a,b)). The secon</context>
</contexts>
<marker>Dreyer, Eisner, 2009</marker>
<rawString>Markus Dreyer and Jason Eisner. 2009. Graphical models over multiple strings. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 101–110, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Dreyer</author>
<author>Jason R Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Latent-variable modeling of string transductions with finite-state methods.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1080--1089</pages>
<location>Honolulu,</location>
<contexts>
<context position="2931" citStr="Dreyer et al., 2008" startWordPosition="441" endWordPosition="444">s defined in this way, F may have many fewer parameters than if F is defined over the product PDFA directly. This is because the parameters of the distributions are defined in terms of the factors which combine in predictable ways via the product. Fewer parameters means accurate estimation occurs with less data and, relatedly, the family contains fewer distributions. This idea is not new. It is explicit in Factorial Hidden Markov Models (FHMMs) (Ghahramani and Jordan, 1997; Saul and Jordan, 1999), and more recently underlies approaches to describing and inferring regular string transductions (Dreyer et al., 2008; Dreyer and Eisner, 2009). Although HMMs and probabilistic finite-state automata describe the same class of distributions (Vidal et al., 2005a; Vidal et al., 2005b), this paper presents these ideas in formal language-theoretic and automata-theoretic terms because (1) there are no hidden states and is thus simpler than FHMMs, (2) determinstic automata have several desirable properties crucially used here, and (3) PDFAs add probabilities to structure whereas HMMs add structure to probabilities and the authors are more comfortable with the former perspective (for further discussion, see Vidal et</context>
</contexts>
<marker>Dreyer, Smith, Eisner, 2008</marker>
<rawString>Markus Dreyer, Jason R. Smith, and Jason Eisner. 2008. Latent-variable modeling of string transductions with finite-state methods. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1080–1089, Honolulu, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pedro Garcia</author>
<author>Enrique Vidal</author>
<author>Jos´e Oncina</author>
</authors>
<title>Learning locally testable languages in the strict sense.</title>
<date>1990</date>
<booktitle>In Proceedings of the Workshop on Algorithmic Learning Theory,</booktitle>
<pages>325--338</pages>
<contexts>
<context position="11975" citStr="Garcia et al., 1990" startWordPosition="2054" endWordPosition="2057">robability distribution at each state. Figure 1 illustrates the counts obtained for a machine M with sample S = {abca}.4 Figure 1 shows a DFA with counts and the PDFA obtained after normalizing these counts. 3 Strictly local distributions In formal language theory, strictly k-local languages occupy the bottom rung of a subregular hierarchy which makes distinctions on the basis of contiguous subsequences (McNaughton and Papert, 1971; Rogers and Pullum, to appear; Rogers et al., 2009). They are also the categorical counterpart to stochastic languages describable with ngram models (where n = k) (Garcia et al., 1990; Jurafsky and Martin, 2008). Since stochastic languages are distributions, we refer to strictly klocal stochastic languages as strictly k-local distri4Technically, M is neither a simple DFA or PDFA; rather, it has been called a Frequency DFA. We do not formally define them here, see de la Higuera (2010). ( Tˆ, 30 Figure 1: M shows the counts obtained by parsing it with sample S = {abca}. M′ shows the probabilities obtained after normalizing those counts. butions (SLDk). We illustrate with SLD2 (bigram models) for ease of exposition. For an alphabet E, SL2 distributions have (|E |+ 1)2 paramet</context>
</contexts>
<marker>Garcia, Vidal, Oncina, 1990</marker>
<rawString>Pedro Garcia, Enrique Vidal, and Jos´e Oncina. 1990. Learning locally testable languages in the strict sense. In Proceedings of the Workshop on Algorithmic Learning Theory, pages 325–338.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zoubin Ghahramani</author>
<author>Michael I Jordan</author>
</authors>
<title>Factorial hidden markov models.</title>
<date>1997</date>
<booktitle>Machine Learning,</booktitle>
<volume>29</volume>
<issue>2</issue>
<contexts>
<context position="2789" citStr="Ghahramani and Jordan, 1997" startWordPosition="419" endWordPosition="423">Probabilistic Deterministic Finite Acceptor (PDFA), and the product of these PDFAs defines the actual distribution. When a family of distributions F is defined in this way, F may have many fewer parameters than if F is defined over the product PDFA directly. This is because the parameters of the distributions are defined in terms of the factors which combine in predictable ways via the product. Fewer parameters means accurate estimation occurs with less data and, relatedly, the family contains fewer distributions. This idea is not new. It is explicit in Factorial Hidden Markov Models (FHMMs) (Ghahramani and Jordan, 1997; Saul and Jordan, 1999), and more recently underlies approaches to describing and inferring regular string transductions (Dreyer et al., 2008; Dreyer and Eisner, 2009). Although HMMs and probabilistic finite-state automata describe the same class of distributions (Vidal et al., 2005a; Vidal et al., 2005b), this paper presents these ideas in formal language-theoretic and automata-theoretic terms because (1) there are no hidden states and is thus simpler than FHMMs, (2) determinstic automata have several desirable properties crucially used here, and (3) PDFAs add probabilities to structure wher</context>
</contexts>
<marker>Ghahramani, Jordan, 1997</marker>
<rawString>Zoubin Ghahramani and Michael I. Jordan. 1997. Factorial hidden markov models. Machine Learning, 29(2):245–273.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zoubin Ghahramani</author>
</authors>
<title>Learning dynamic bayesian networks.</title>
<date>1998</date>
<booktitle>In Adaptive Processing of Sequences and Data Structures,</booktitle>
<pages>168--197</pages>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="28371" citStr="Ghahramani, 1998" startWordPosition="4958" endWordPosition="4959">r of [#xj] is significantly below the expected number according to the feature-based distribution, which could lead to a new parameter being adopted to describe the interaction of the [dorsal] and [nasal] 7We use the feature chart in Hayes (2009) because it contains over 150 IPA symbols (and not just English phonemes). Featural combinations not in the chart were assumed to be impossible (e.g. [+high,+low]) and were zeroed out. features word-initially. The details of these procedures are left for future research and are likely to draw from the rich literature on Bayesian networks (Pearl, 1989; Ghahramani, 1998). More important, however, is this framework allows researchers to construct the independence assumptions they want into the model in at least two ways. First, universally incompatible features can be excluded. For example, suppose [-F] and [-G] in the feature system in Table 1 are anatomically incompatible like [+low] and [+high]. If desired, they can be excluded from the model essentially by zeroing out any probability mass assigned to such combinations and re-normalizing. Second, models can be defined where multiple features are permitted to interact. For example, suppose features F and G f</context>
</contexts>
<marker>Ghahramani, 1998</marker>
<rawString>Zoubin Ghahramani. 1998. Learning dynamic bayesian networks. In Adaptive Processing of Sequences and Data Structures, pages 168–197. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Learning bias and phonological-rule induction.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>4</issue>
<contexts>
<context position="1384" citStr="Gildea and Jurafsky, 1996" startWordPosition="200" endWordPosition="203">h in Hayes and Wilson (2008), and it is argued that the bottom-up approach is more analytically transparent. 1 Introduction The hypothesis that the atomic units of phonology are phonological features, and not segments, is one of the tenets of modern phonology (Jakobson et al., 1952; Chomsky and Halle, 1968). According to this hypothesis, segments are essentially epiphenomenal and exist only by virtue of being a shorthand description of a collection of more primitive units—the features. Incorporating this hypothesis into phonological learning models has been the focus of much influential work (Gildea and Jurafsky, 1996; Wilson, 2006; Hayes and Wilson, 2008; Moreton, 2008; Albright, 2009). This paper makes three contributions. The first contribution is a framework within which: 1. researchers can choose which statistical independence assumptions to make regarding phonological features; 2. feature systems can be fully integrated into strictly local (McNaughton and Papert, 1971) (i.e. n-gram models (Jurafsky and Martin, 2008)) and strictly piecewise models (Rogers et al., 2009; Heinz and Rogers, 2010) in order to define families of provably wellformed, feature-based probability distributions that are provably </context>
</contexts>
<marker>Gildea, Jurafsky, 1996</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 1996. Learning bias and phonological-rule induction. Computational Linguistics, 24(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bruce Hayes</author>
<author>Colin Wilson</author>
</authors>
<title>A maximum entropy model of phonotactics and phonotactic learning. Linguistic Inquiry,</title>
<date>2008</date>
<pages>39--379</pages>
<contexts>
<context position="787" citStr="Hayes and Wilson (2008)" startWordPosition="110" endWordPosition="113">bstract Motivated by recent work in phonotactic learning (Hayes and Wilson 2008, Albright 2009), this paper shows how to define feature-based probability distributions whose parameters can be provably efficiently estimated. The main idea is that these distributions are defined as a product of simpler distributions (cf. Ghahramani and Jordan 1997). One advantage of this framework is it draws attention to what is minimally necessary to describe and learn phonological feature interactions in phonotactic patterns. The “bottom-up” approach adopted here is contrasted with the “top-down” approach in Hayes and Wilson (2008), and it is argued that the bottom-up approach is more analytically transparent. 1 Introduction The hypothesis that the atomic units of phonology are phonological features, and not segments, is one of the tenets of modern phonology (Jakobson et al., 1952; Chomsky and Halle, 1968). According to this hypothesis, segments are essentially epiphenomenal and exist only by virtue of being a shorthand description of a collection of more primitive units—the features. Incorporating this hypothesis into phonological learning models has been the focus of much influential work (Gildea and Jurafsky, 1996; W</context>
<context position="4282" citStr="Hayes and Wilson (2008)" startWordPosition="642" endWordPosition="645">e 11th Meeting of the ACL-SIGMORPHON, ACL 2010, pages 28–37, Uppsala, Sweden, 15 July 2010. c�2010 Association for Computational Linguistics strong statistical independence assumption: no two features interact. This is shown to capture exactly the intuition that sounds with like features have like distributions. Also, the assumption of non-interacting features is shown to be too strong because like sounds do not have like distributions in actual phonotactic patterns. Four kinds of featural interactions are identified and possible solutions are discussed. Finally, we compare this proposal with Hayes and Wilson (2008). Essentially, the model here represents a “bottom-up” approach whereas theirs is “top-down.” “Top-down” models, which consider every set of features as potentially interacting in every allowable context, face the difficult problem of searching a vast space and often resort to heuristic-based methods, which are difficult to analyze. To illustrate, we suggest that the role played by phonological features in the phonotactic learner in Hayes and Wilson (2008) is not well-understood. We demonstrate that classes of all segments but one (i.e. the complement classes of single segments) play a signifi</context>
<context position="26936" citStr="Hayes and Wilson (2008)" startWordPosition="4732" endWordPosition="4735">n English, for example, there are syllabic sounds and obstruents but no syllabic obstruents. Third, different features may be universally incompatible: e.g. no vowels are both [+high] and [+low]. The last type of interaction is that different features may be prohibited from occuring syntagmatically. For example, some languages prohibit voiceless sounds from occuring after nasals. Although the independence assumption is too strong, it is still useful. First, it allows researchers to quantify the extent to which data can be explained without invoking featural interaction. For example, following Hayes and Wilson (2008), we may be interested in how well human acceptability judgements collected by Scholes (1966) can be explained if different features do not interact. After training the feature-based SL2 model on a corpus of word initial onsets adapted from the CMU pronouncing dictionary (Hayes and Wilson, 2008, 395-396) and using a standard phonological feature system (Hayes, 2009, chap. 4), it achieves a correlation (Spearman’s r) of 0.751.7 In other words, roughly three quarters of the acceptability judgements are explained without relying on featural interaction (or segments). Secondly, the incorrect predi</context>
<context position="29717" citStr="Hayes and Wilson (2008)" startWordPosition="5172" endWordPosition="5175">d now interactions between F and G will be learned, including syntagmatic ones. The flexibility of the framework and the generality of the normalized co-emission product allow researchers to consider feature-based distributions which allow any two features to interact but which prohibit three-feature interactions, or which allow any three features to interact but which prohibit fourfeature interactions, or models where only certain features are permitted to interact but not others (perhaps because they belong to the same node in a feature geometry (Clements, 1985; Clements and Hume, 1995).8 7 Hayes and Wilson (2008) This section introduces the Hayes and Wilson (2008) (henceforth HW) phonotactic learner and shows that the contribution features play in generalization is not as clear as previously thought. HW propose an inductive model which acquires a maxent grammar defined by weighted constraints. Each constraint is described as a sequence of natural classes using phonological features. The constraint format also allows reference to word boundaries and at most one complement class. (The complement class of S ⊆ Σ is Σ/S.) For example, the constraint *#[ˆ -voice,+anterior,+strident][-approximant] means that</context>
</contexts>
<marker>Hayes, Wilson, 2008</marker>
<rawString>Bruce Hayes and Colin Wilson. 2008. A maximum entropy model of phonotactics and phonotactic learning. Linguistic Inquiry, 39:379–440.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bruce Hayes</author>
</authors>
<title>Introductory Phonology.</title>
<date>2009</date>
<publisher>WileyBlackwell.</publisher>
<contexts>
<context position="27303" citStr="Hayes, 2009" startWordPosition="4794" endWordPosition="4795">ls. Although the independence assumption is too strong, it is still useful. First, it allows researchers to quantify the extent to which data can be explained without invoking featural interaction. For example, following Hayes and Wilson (2008), we may be interested in how well human acceptability judgements collected by Scholes (1966) can be explained if different features do not interact. After training the feature-based SL2 model on a corpus of word initial onsets adapted from the CMU pronouncing dictionary (Hayes and Wilson, 2008, 395-396) and using a standard phonological feature system (Hayes, 2009, chap. 4), it achieves a correlation (Spearman’s r) of 0.751.7 In other words, roughly three quarters of the acceptability judgements are explained without relying on featural interaction (or segments). Secondly, the incorrect predictions of the model are in principle detectable. For example, recall that English has word-inital velars and nasals, but no word-inital velar nasals. A one-cell chi-squared test can determine whether the observed number of [#xj] is significantly below the expected number according to the feature-based distribution, which could lead to a new parameter being adopted </context>
</contexts>
<marker>Hayes, 2009</marker>
<rawString>Bruce Hayes. 2009. Introductory Phonology. WileyBlackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Heinz</author>
<author>James Rogers</author>
</authors>
<title>Estimating strictly piecewise distributions.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting ofthe Association for Computational Linguistics,</booktitle>
<location>Uppsala,</location>
<contexts>
<context position="1873" citStr="Heinz and Rogers, 2010" startWordPosition="271" endWordPosition="274">Incorporating this hypothesis into phonological learning models has been the focus of much influential work (Gildea and Jurafsky, 1996; Wilson, 2006; Hayes and Wilson, 2008; Moreton, 2008; Albright, 2009). This paper makes three contributions. The first contribution is a framework within which: 1. researchers can choose which statistical independence assumptions to make regarding phonological features; 2. feature systems can be fully integrated into strictly local (McNaughton and Papert, 1971) (i.e. n-gram models (Jurafsky and Martin, 2008)) and strictly piecewise models (Rogers et al., 2009; Heinz and Rogers, 2010) in order to define families of provably wellformed, feature-based probability distributions that are provably efficiently estimable. The main idea is to define a family of distributions as the normalized product of simpler distributions. Each simpler distribution can be represented by a Probabilistic Deterministic Finite Acceptor (PDFA), and the product of these PDFAs defines the actual distribution. When a family of distributions F is defined in this way, F may have many fewer parameters than if F is defined over the product PDFA directly. This is because the parameters of the distributions </context>
<context position="34035" citStr="Heinz and Rogers, 2010" startWordPosition="5856" endWordPosition="5859">s invoking featural interaction ought to be measured. 8 Conclusion The current proposal explicitly embeds the Jakobsonian hypothesis that the primitive unit of phonology is the phonological feature into a phonotactic learning model. While this paper specifically shows how to integrate features into n-gram models to describe feature-based strictly n-local distributions, these techniques can be applied to other regular deterministic distributions, such as strictly k-piecewise models, which describe long-distance dependencies, like the ones found in consonant and vowel harmony (Heinz, to appear; Heinz and Rogers, 2010). In contrast to models which assume that all features potentially interact, a baseline model was specifically introduced under the assumption that no two features interact. In this way, the “bottom-up” approach to feature-based generalization shifts the focus of inquiry to the featural interactions necessary (and ultimately sufficient) to describe and learn phonotactic patterns. The framework introduced here shows how researchers can study feature interaction in phonotactic models in a systematic, transparent way. Acknowledgments We thank Bill Idsardi, Tim O’Neill, Jim Rogers, Robert Wilder, </context>
</contexts>
<marker>Heinz, Rogers, 2010</marker>
<rawString>Jeffrey Heinz and James Rogers. 2010. Estimating strictly piecewise distributions. In Proceedings of the 48th Annual Meeting ofthe Association for Computational Linguistics, Uppsala, Sweden.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jeffrey Heinz</author>
</authors>
<title>to appear. Learning long-distance phonotactics.</title>
<journal>Linguistic Inquiry,</journal>
<volume>41</volume>
<issue>4</issue>
<marker>Heinz, </marker>
<rawString>Jeffrey Heinz. to appear. Learning long-distance phonotactics. Linguistic Inquiry, 41(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Hopcroft</author>
<author>Rajeev Motwani</author>
<author>Jeffrey Ullman</author>
</authors>
<title>Introduction to Automata Theory, Languages, and Computation.</title>
<date>2001</date>
<publisher>Addison-Wesley.</publisher>
<location>Boston, MA:</location>
<contexts>
<context position="8693" citStr="Hopcroft et al., 2001" startWordPosition="1438" endWordPosition="1441"> renaming of states so PDFA with isomorphic structural components are said to have the same structure. 29 (Q2, E2, q02, δ2, F2, T2) be PDFAs. The probability that σ1 is emitted from q1 E Q1 at the same moment σ2 is emitted from q2 E Q2 is CT(σ1,σ2,q1,q2) = T1(q1,σ1)·T2(q2,σ2). Similarly, the probability that a word simultaneously ends at q1 E Q1 and at q2 E Q2 is CF(q1, q2) = F1(q1)· F2(q2). Definition 1 The normalized co-emission product of PDFAs M1 and M2 is M = M1 x M2 = (Q, E, q0, δ, F, T) where 1. Q, q0, and F are defined in terms of the standard DFA product over the state space Q1 x Q2 (Hopcroft et al., 2001). 2. E = E1 x E2 3. For all (q1, q2) E Q and (σ1, σ2) E E, δ((q1, q2), (σ1, σ2)) = (q′1, q′2) iff δ1(q1, σ1) = q′1 and δ2(q2, σ2) = q′2.2 4. For all (q1, q2) E Q, (a) let Z((q1, q2)) = CF((q1, q2)) + E hσ1,σ2i∈Σ CT(σ1, σ2, q1, q2) be the normalization term; and (b) F((q1,q2)) = CF(q1,q2) Z ; and (c) for all (σ1, σ2) E E, T ((q1, q2), (σ1, σ2)) = CT(hσ1,σ2,q1,q2i) Z In other words, the numerators of T and F are defined to be the co-emission probabilities, and division by Z ensures that M defines a wellformed probability distribution.3 The normalized co-emission product effectively adopts a stat</context>
<context position="9938" citStr="Hopcroft et al., 2001" startWordPosition="1677" endWordPosition="1680">sumption between the states of M1 and M2. If S is a list of PDFAs, we write ® S for their product (note order of product is irrelevant up to renaming of the states). The maximum likelihood (ML) estimation of regular deterministic distributions is a solved problem when the structure of the PDFA is known (Vidal et al., 2005a; Vidal et al., 2005b; de la Higuera, 2010). Let S be a finite sample of words drawn from a regular deterministic distribution D. The problem is to estimate parameters T and F of 2Note that restricting 8 to cases when 0-1 = 0-2 obtains the standard definition of 8 = 81 x 82 (Hopcroft et al., 2001). The reason we maintain two alphabets becomes clear in §4. 3Z((q1, q2)) is less than one whenever either F1(q1) or F2(q2) are neither zero nor one. M so that DM approaches D using the widelyadopted ML criterion (Equation 3). Fˆ) = argmax PrM (w) (3) T,F (wi It is well-known that if D is generated by some PDFA M′ with the same structural components as M, then the ML estimate of S with respect to M guarantees that DM approaches D as the size of S goes to infinity (Vidal et al., 2005a; Vidal et al., 2005b; de la Higuera, 2010). Finding the ML estimate of a finite sample S with respect to M is si</context>
</contexts>
<marker>Hopcroft, Motwani, Ullman, 2001</marker>
<rawString>John Hopcroft, Rajeev Motwani, and Jeffrey Ullman. 2001. Introduction to Automata Theory, Languages, and Computation. Boston, MA: Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roman Jakobson</author>
<author>C Gunnar</author>
<author>M Fant</author>
<author>Morris Halle</author>
</authors>
<title>Preliminaries to Speech Analysis.</title>
<date>1952</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="1041" citStr="Jakobson et al., 1952" startWordPosition="150" endWordPosition="153">ributions are defined as a product of simpler distributions (cf. Ghahramani and Jordan 1997). One advantage of this framework is it draws attention to what is minimally necessary to describe and learn phonological feature interactions in phonotactic patterns. The “bottom-up” approach adopted here is contrasted with the “top-down” approach in Hayes and Wilson (2008), and it is argued that the bottom-up approach is more analytically transparent. 1 Introduction The hypothesis that the atomic units of phonology are phonological features, and not segments, is one of the tenets of modern phonology (Jakobson et al., 1952; Chomsky and Halle, 1968). According to this hypothesis, segments are essentially epiphenomenal and exist only by virtue of being a shorthand description of a collection of more primitive units—the features. Incorporating this hypothesis into phonological learning models has been the focus of much influential work (Gildea and Jurafsky, 1996; Wilson, 2006; Hayes and Wilson, 2008; Moreton, 2008; Albright, 2009). This paper makes three contributions. The first contribution is a framework within which: 1. researchers can choose which statistical independence assumptions to make regarding phonolog</context>
</contexts>
<marker>Jakobson, Gunnar, Fant, Halle, 1952</marker>
<rawString>Roman Jakobson, C. Gunnar, M. Fant, and Morris Halle. 1952. Preliminaries to Speech Analysis. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Jurafsky</author>
<author>James Martin</author>
</authors>
<title>Speech and Language Processing: An Introduction to Natural Language Processing, Speech Recognition, and Computational Linguistics. Prentice-Hall, Upper Saddle River, NJ, 2nd edition.</title>
<date>2008</date>
<contexts>
<context position="1796" citStr="Jurafsky and Martin, 2008" startWordPosition="259" endWordPosition="262">g a shorthand description of a collection of more primitive units—the features. Incorporating this hypothesis into phonological learning models has been the focus of much influential work (Gildea and Jurafsky, 1996; Wilson, 2006; Hayes and Wilson, 2008; Moreton, 2008; Albright, 2009). This paper makes three contributions. The first contribution is a framework within which: 1. researchers can choose which statistical independence assumptions to make regarding phonological features; 2. feature systems can be fully integrated into strictly local (McNaughton and Papert, 1971) (i.e. n-gram models (Jurafsky and Martin, 2008)) and strictly piecewise models (Rogers et al., 2009; Heinz and Rogers, 2010) in order to define families of provably wellformed, feature-based probability distributions that are provably efficiently estimable. The main idea is to define a family of distributions as the normalized product of simpler distributions. Each simpler distribution can be represented by a Probabilistic Deterministic Finite Acceptor (PDFA), and the product of these PDFAs defines the actual distribution. When a family of distributions F is defined in this way, F may have many fewer parameters than if F is defined over th</context>
<context position="12003" citStr="Jurafsky and Martin, 2008" startWordPosition="2058" endWordPosition="2061">on at each state. Figure 1 illustrates the counts obtained for a machine M with sample S = {abca}.4 Figure 1 shows a DFA with counts and the PDFA obtained after normalizing these counts. 3 Strictly local distributions In formal language theory, strictly k-local languages occupy the bottom rung of a subregular hierarchy which makes distinctions on the basis of contiguous subsequences (McNaughton and Papert, 1971; Rogers and Pullum, to appear; Rogers et al., 2009). They are also the categorical counterpart to stochastic languages describable with ngram models (where n = k) (Garcia et al., 1990; Jurafsky and Martin, 2008). Since stochastic languages are distributions, we refer to strictly klocal stochastic languages as strictly k-local distri4Technically, M is neither a simple DFA or PDFA; rather, it has been called a Frequency DFA. We do not formally define them here, see de la Higuera (2010). ( Tˆ, 30 Figure 1: M shows the counts obtained by parsing it with sample S = {abca}. M′ shows the probabilities obtained after normalizing those counts. butions (SLDk). We illustrate with SLD2 (bigram models) for ease of exposition. For an alphabet E, SL2 distributions have (|E |+ 1)2 parameters. These are, for all v, -</context>
<context position="13719" citStr="Jurafsky and Martin (2008" startWordPosition="2387" endWordPosition="2390"> probabilities to each transition and to the ending at each state. In fact, for all v ∈ E and -r ∈ E ∪ {#}, Pr(v |-r) is T(-r, v) and Pr(# |-r) is F(-r). It follows that the probability of a particular path through the model corresponds to Equation 4. The structure of a SL2 distribution for alphabet E is given by MSL2(E). Additionally, given a finite sample S ⊂ E*, the ML estimate of S with respect to the family of distributions describable with MSL2(E) is given by counting the parse of S through MSL2(E) and then normalizing as described in §2. This is equivalent to the procedure described in Jurafsky and Martin (2008, chap. 4). 4 Feature-based distributions This section first introduces feature systems. Then it defines feature-based SL2 distributions which make the strong independence assumption that no two features interact. It explains how to find Figure 2: MSL2({a, b, c}) represents the structure of SL2 distributions when E = {a, b, c}. F G + - + + - + Table 1: An example of a feature system with E = {a, b, c} and two features F and G. the ML estimate of samples with respect to such distributions. This section closes by identifying kinds of featural interactions in phonotactic patterns, and discusses h</context>
</contexts>
<marker>Jurafsky, Martin, 2008</marker>
<rawString>Daniel Jurafsky and James Martin. 2008. Speech and Language Processing: An Introduction to Natural Language Processing, Speech Recognition, and Computational Linguistics. Prentice-Hall, Upper Saddle River, NJ, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert McNaughton</author>
<author>Seymour Papert</author>
</authors>
<title>Counter-Free Automata.</title>
<date>1971</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="1748" citStr="McNaughton and Papert, 1971" startWordPosition="252" endWordPosition="255">lly epiphenomenal and exist only by virtue of being a shorthand description of a collection of more primitive units—the features. Incorporating this hypothesis into phonological learning models has been the focus of much influential work (Gildea and Jurafsky, 1996; Wilson, 2006; Hayes and Wilson, 2008; Moreton, 2008; Albright, 2009). This paper makes three contributions. The first contribution is a framework within which: 1. researchers can choose which statistical independence assumptions to make regarding phonological features; 2. feature systems can be fully integrated into strictly local (McNaughton and Papert, 1971) (i.e. n-gram models (Jurafsky and Martin, 2008)) and strictly piecewise models (Rogers et al., 2009; Heinz and Rogers, 2010) in order to define families of provably wellformed, feature-based probability distributions that are provably efficiently estimable. The main idea is to define a family of distributions as the normalized product of simpler distributions. Each simpler distribution can be represented by a Probabilistic Deterministic Finite Acceptor (PDFA), and the product of these PDFAs defines the actual distribution. When a family of distributions F is defined in this way, F may have ma</context>
<context position="11791" citStr="McNaughton and Papert, 1971" startWordPosition="2021" endWordPosition="2025">g the relative frequency of state q being final with state q being encountered in the parsing of S. For both cases, the division is normalizing; i.e. it guarantees that there is a wellformed probability distribution at each state. Figure 1 illustrates the counts obtained for a machine M with sample S = {abca}.4 Figure 1 shows a DFA with counts and the PDFA obtained after normalizing these counts. 3 Strictly local distributions In formal language theory, strictly k-local languages occupy the bottom rung of a subregular hierarchy which makes distinctions on the basis of contiguous subsequences (McNaughton and Papert, 1971; Rogers and Pullum, to appear; Rogers et al., 2009). They are also the categorical counterpart to stochastic languages describable with ngram models (where n = k) (Garcia et al., 1990; Jurafsky and Martin, 2008). Since stochastic languages are distributions, we refer to strictly klocal stochastic languages as strictly k-local distri4Technically, M is neither a simple DFA or PDFA; rather, it has been called a Frequency DFA. We do not formally define them here, see de la Higuera (2010). ( Tˆ, 30 Figure 1: M shows the counts obtained by parsing it with sample S = {abca}. M′ shows the probabiliti</context>
</contexts>
<marker>McNaughton, Papert, 1971</marker>
<rawString>Robert McNaughton and Seymour Papert. 1971. Counter-Free Automata. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elliot Moreton</author>
</authors>
<title>Analytic bias and phonological typology.</title>
<date>2008</date>
<journal>Phonology,</journal>
<volume>25</volume>
<issue>1</issue>
<contexts>
<context position="1437" citStr="Moreton, 2008" startWordPosition="211" endWordPosition="212"> approach is more analytically transparent. 1 Introduction The hypothesis that the atomic units of phonology are phonological features, and not segments, is one of the tenets of modern phonology (Jakobson et al., 1952; Chomsky and Halle, 1968). According to this hypothesis, segments are essentially epiphenomenal and exist only by virtue of being a shorthand description of a collection of more primitive units—the features. Incorporating this hypothesis into phonological learning models has been the focus of much influential work (Gildea and Jurafsky, 1996; Wilson, 2006; Hayes and Wilson, 2008; Moreton, 2008; Albright, 2009). This paper makes three contributions. The first contribution is a framework within which: 1. researchers can choose which statistical independence assumptions to make regarding phonological features; 2. feature systems can be fully integrated into strictly local (McNaughton and Papert, 1971) (i.e. n-gram models (Jurafsky and Martin, 2008)) and strictly piecewise models (Rogers et al., 2009; Heinz and Rogers, 2010) in order to define families of provably wellformed, feature-based probability distributions that are provably efficiently estimable. The main idea is to define a f</context>
</contexts>
<marker>Moreton, 2008</marker>
<rawString>Elliot Moreton. 2008. Analytic bias and phonological typology. Phonology, 25(1):83–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judea Pearl</author>
</authors>
<title>Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference.</title>
<date>1989</date>
<publisher>Morgan Kauffman.</publisher>
<contexts>
<context position="28352" citStr="Pearl, 1989" startWordPosition="4956" endWordPosition="4957">bserved number of [#xj] is significantly below the expected number according to the feature-based distribution, which could lead to a new parameter being adopted to describe the interaction of the [dorsal] and [nasal] 7We use the feature chart in Hayes (2009) because it contains over 150 IPA symbols (and not just English phonemes). Featural combinations not in the chart were assumed to be impossible (e.g. [+high,+low]) and were zeroed out. features word-initially. The details of these procedures are left for future research and are likely to draw from the rich literature on Bayesian networks (Pearl, 1989; Ghahramani, 1998). More important, however, is this framework allows researchers to construct the independence assumptions they want into the model in at least two ways. First, universally incompatible features can be excluded. For example, suppose [-F] and [-G] in the feature system in Table 1 are anatomically incompatible like [+low] and [+high]. If desired, they can be excluded from the model essentially by zeroing out any probability mass assigned to such combinations and re-normalizing. Second, models can be defined where multiple features are permitted to interact. For example, suppose</context>
</contexts>
<marker>Pearl, 1989</marker>
<rawString>Judea Pearl. 1989. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kauffman.</rawString>
</citation>
<citation valid="false">
<authors>
<author>James Rogers</author>
<author>Geoffrey Pullum</author>
</authors>
<title>to appear. Aural pattern recognition experiments and the subregular hierarchy.</title>
<journal>Journal of Logic, Language and Information.</journal>
<marker>Rogers, Pullum, </marker>
<rawString>James Rogers and Geoffrey Pullum. to appear. Aural pattern recognition experiments and the subregular hierarchy. Journal of Logic, Language and Information.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Rogers</author>
<author>Jeffrey Heinz</author>
<author>Gil Bailey</author>
<author>Matt Edlefsen</author>
<author>Molly Visscher</author>
<author>David Wellcome</author>
<author>Sean Wibel</author>
</authors>
<title>On languages piecewise testable in the strict sense.</title>
<date>2009</date>
<booktitle>In Proceedings of the 11th Meeting of the Assocation for Mathematics ofLanguage.</booktitle>
<contexts>
<context position="1848" citStr="Rogers et al., 2009" startWordPosition="267" endWordPosition="270"> units—the features. Incorporating this hypothesis into phonological learning models has been the focus of much influential work (Gildea and Jurafsky, 1996; Wilson, 2006; Hayes and Wilson, 2008; Moreton, 2008; Albright, 2009). This paper makes three contributions. The first contribution is a framework within which: 1. researchers can choose which statistical independence assumptions to make regarding phonological features; 2. feature systems can be fully integrated into strictly local (McNaughton and Papert, 1971) (i.e. n-gram models (Jurafsky and Martin, 2008)) and strictly piecewise models (Rogers et al., 2009; Heinz and Rogers, 2010) in order to define families of provably wellformed, feature-based probability distributions that are provably efficiently estimable. The main idea is to define a family of distributions as the normalized product of simpler distributions. Each simpler distribution can be represented by a Probabilistic Deterministic Finite Acceptor (PDFA), and the product of these PDFAs defines the actual distribution. When a family of distributions F is defined in this way, F may have many fewer parameters than if F is defined over the product PDFA directly. This is because the paramet</context>
<context position="11843" citStr="Rogers et al., 2009" startWordPosition="2031" endWordPosition="2034">q being encountered in the parsing of S. For both cases, the division is normalizing; i.e. it guarantees that there is a wellformed probability distribution at each state. Figure 1 illustrates the counts obtained for a machine M with sample S = {abca}.4 Figure 1 shows a DFA with counts and the PDFA obtained after normalizing these counts. 3 Strictly local distributions In formal language theory, strictly k-local languages occupy the bottom rung of a subregular hierarchy which makes distinctions on the basis of contiguous subsequences (McNaughton and Papert, 1971; Rogers and Pullum, to appear; Rogers et al., 2009). They are also the categorical counterpart to stochastic languages describable with ngram models (where n = k) (Garcia et al., 1990; Jurafsky and Martin, 2008). Since stochastic languages are distributions, we refer to strictly klocal stochastic languages as strictly k-local distri4Technically, M is neither a simple DFA or PDFA; rather, it has been called a Frequency DFA. We do not formally define them here, see de la Higuera (2010). ( Tˆ, 30 Figure 1: M shows the counts obtained by parsing it with sample S = {abca}. M′ shows the probabilities obtained after normalizing those counts. butions </context>
</contexts>
<marker>Rogers, Heinz, Bailey, Edlefsen, Visscher, Wellcome, Wibel, 2009</marker>
<rawString>James Rogers, Jeffrey Heinz, Gil Bailey, Matt Edlefsen, Molly Visscher, David Wellcome, and Sean Wibel. 2009. On languages piecewise testable in the strict sense. In Proceedings of the 11th Meeting of the Assocation for Mathematics ofLanguage.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence K Saul</author>
<author>Michael I Jordan</author>
</authors>
<title>Mixed memory markov models: Decomposing complex stochastic processes as mixtures of simpler ones.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="2813" citStr="Saul and Jordan, 1999" startWordPosition="424" endWordPosition="427">inite Acceptor (PDFA), and the product of these PDFAs defines the actual distribution. When a family of distributions F is defined in this way, F may have many fewer parameters than if F is defined over the product PDFA directly. This is because the parameters of the distributions are defined in terms of the factors which combine in predictable ways via the product. Fewer parameters means accurate estimation occurs with less data and, relatedly, the family contains fewer distributions. This idea is not new. It is explicit in Factorial Hidden Markov Models (FHMMs) (Ghahramani and Jordan, 1997; Saul and Jordan, 1999), and more recently underlies approaches to describing and inferring regular string transductions (Dreyer et al., 2008; Dreyer and Eisner, 2009). Although HMMs and probabilistic finite-state automata describe the same class of distributions (Vidal et al., 2005a; Vidal et al., 2005b), this paper presents these ideas in formal language-theoretic and automata-theoretic terms because (1) there are no hidden states and is thus simpler than FHMMs, (2) determinstic automata have several desirable properties crucially used here, and (3) PDFAs add probabilities to structure whereas HMMs add structure t</context>
</contexts>
<marker>Saul, Jordan, 1999</marker>
<rawString>Lawrence K. Saul and Michael I. Jordan. 1999. Mixed memory markov models: Decomposing complex stochastic processes as mixtures of simpler ones. Machine Learning, 37(1):75–87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert J Scholes</author>
</authors>
<title>Phonotactic grammaticality.</title>
<date>1966</date>
<publisher>Mouton, The Hague.</publisher>
<contexts>
<context position="27029" citStr="Scholes (1966)" startWordPosition="4749" endWordPosition="4750">fferent features may be universally incompatible: e.g. no vowels are both [+high] and [+low]. The last type of interaction is that different features may be prohibited from occuring syntagmatically. For example, some languages prohibit voiceless sounds from occuring after nasals. Although the independence assumption is too strong, it is still useful. First, it allows researchers to quantify the extent to which data can be explained without invoking featural interaction. For example, following Hayes and Wilson (2008), we may be interested in how well human acceptability judgements collected by Scholes (1966) can be explained if different features do not interact. After training the feature-based SL2 model on a corpus of word initial onsets adapted from the CMU pronouncing dictionary (Hayes and Wilson, 2008, 395-396) and using a standard phonological feature system (Hayes, 2009, chap. 4), it achieves a correlation (Spearman’s r) of 0.751.7 In other words, roughly three quarters of the acceptability judgements are explained without relying on featural interaction (or segments). Secondly, the incorrect predictions of the model are in principle detectable. For example, recall that English has word-in</context>
<context position="30884" citStr="Scholes (1966)" startWordPosition="5359" endWordPosition="5360">e,+anterior,+strident][-approximant] means that in word-initial C1C2 clusters, if C2 is a nasal or obstruent, then C1 must be [s]. 8Note if all features are permitted to interact, this yields the segmental bigram model. 35 Hayes and Wilson maxent models r features &amp; complement classes 0.946 no features &amp; complement classes 0.937 features &amp; no complement classes 0.914 no features &amp; no complement classes 0.885 Table 4: Correlations of different settings versions of HW maxent model with Scholes data. HW report that the model obtains a correlation (Spearman’s r) of 0.946 with blick test data from Scholes (1966). HW and Albright (2009) attribute this high correlation to the model’s use of natural classes and phonological features. HW also report that when the model is run without features, the grammar obtained scores an r value of only 0.885, implying that the gain in correlation is due specifically to the use of phonological features. However, there are two relevant issues. The first is the use of complement classes. If features are not used but complement classes are (in effect only allowing the model to refer to single segments and the complements of single segments, e.g. [t] and [ˆt]) then in fac</context>
</contexts>
<marker>Scholes, 1966</marker>
<rawString>Robert J. Scholes. 1966. Phonotactic grammaticality. Mouton, The Hague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Enrique Vidal</author>
<author>Franck Thollard</author>
<author>Colin de la Higuera</author>
<author>Francisco Casacuberta</author>
<author>Rafael C Carrasco</author>
</authors>
<title>Probabilistic finite-state machines-part I.</title>
<date>2005</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>27</volume>
<issue>7</issue>
<contexts>
<context position="3073" citStr="Vidal et al., 2005" startWordPosition="462" endWordPosition="465">the distributions are defined in terms of the factors which combine in predictable ways via the product. Fewer parameters means accurate estimation occurs with less data and, relatedly, the family contains fewer distributions. This idea is not new. It is explicit in Factorial Hidden Markov Models (FHMMs) (Ghahramani and Jordan, 1997; Saul and Jordan, 1999), and more recently underlies approaches to describing and inferring regular string transductions (Dreyer et al., 2008; Dreyer and Eisner, 2009). Although HMMs and probabilistic finite-state automata describe the same class of distributions (Vidal et al., 2005a; Vidal et al., 2005b), this paper presents these ideas in formal language-theoretic and automata-theoretic terms because (1) there are no hidden states and is thus simpler than FHMMs, (2) determinstic automata have several desirable properties crucially used here, and (3) PDFAs add probabilities to structure whereas HMMs add structure to probabilities and the authors are more comfortable with the former perspective (for further discussion, see Vidal et al. (2005a,b)). The second contribution illustrates the main idea with a feature-based bigram model with a 28 Proceedings of the 11th Meeting</context>
<context position="8010" citStr="Vidal et al., 2005" startWordPosition="1300" endWordPosition="1303"> state q0 of M. By the structure of a PDFA, we mean its structural components.1 Each PDFA M defines a family of distributions given by the possible instantiations of T and F satisfying Equation 1. These distributions have at most |Q|· (|E|+ 1) parameters (since for each state there are |E| possible transitions plus the possibility of finality.) These are, for all q ∈ Q and σ ∈ E, the probabilities T(q, σ) and F(q). To make the connection to probability theory, we sometimes write these as Pr(σ |q) and Pr(# |q), respectively. We define the product of PDFAs in terms of co-emission probabilities (Vidal et al., 2005a). Let M1 = hQ1,E1,q01,δ1,F1,T1i and M2 = 1This is up to the renaming of states so PDFA with isomorphic structural components are said to have the same structure. 29 (Q2, E2, q02, δ2, F2, T2) be PDFAs. The probability that σ1 is emitted from q1 E Q1 at the same moment σ2 is emitted from q2 E Q2 is CT(σ1,σ2,q1,q2) = T1(q1,σ1)·T2(q2,σ2). Similarly, the probability that a word simultaneously ends at q1 E Q1 and at q2 E Q2 is CF(q1, q2) = F1(q1)· F2(q2). Definition 1 The normalized co-emission product of PDFAs M1 and M2 is M = M1 x M2 = (Q, E, q0, δ, F, T) where 1. Q, q0, and F are defined in ter</context>
<context position="9639" citStr="Vidal et al., 2005" startWordPosition="1620" endWordPosition="1623">1, q2), (σ1, σ2)) = CT(hσ1,σ2,q1,q2i) Z In other words, the numerators of T and F are defined to be the co-emission probabilities, and division by Z ensures that M defines a wellformed probability distribution.3 The normalized co-emission product effectively adopts a statistical independence assumption between the states of M1 and M2. If S is a list of PDFAs, we write ® S for their product (note order of product is irrelevant up to renaming of the states). The maximum likelihood (ML) estimation of regular deterministic distributions is a solved problem when the structure of the PDFA is known (Vidal et al., 2005a; Vidal et al., 2005b; de la Higuera, 2010). Let S be a finite sample of words drawn from a regular deterministic distribution D. The problem is to estimate parameters T and F of 2Note that restricting 8 to cases when 0-1 = 0-2 obtains the standard definition of 8 = 81 x 82 (Hopcroft et al., 2001). The reason we maintain two alphabets becomes clear in §4. 3Z((q1, q2)) is less than one whenever either F1(q1) or F2(q2) are neither zero nor one. M so that DM approaches D using the widelyadopted ML criterion (Equation 3). Fˆ) = argmax PrM (w) (3) T,F (wi It is well-known that if D is generated by</context>
</contexts>
<marker>Vidal, Thollard, Higuera, Casacuberta, Carrasco, 2005</marker>
<rawString>Enrique Vidal, Franck Thollard, Colin de la Higuera, Francisco Casacuberta, and Rafael C. Carrasco. 2005a. Probabilistic finite-state machines-part I. IEEE Transactions on Pattern Analysis and Machine Intelligence, 27(7):1013–1025.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Enrique Vidal</author>
<author>Frank Thollard</author>
<author>Colin de la Higuera</author>
<author>Francisco Casacuberta</author>
<author>Rafael C Carrasco</author>
</authors>
<title>Probabilistic finite-state machines-part II.</title>
<date>2005</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>27</volume>
<issue>7</issue>
<contexts>
<context position="3073" citStr="Vidal et al., 2005" startWordPosition="462" endWordPosition="465">the distributions are defined in terms of the factors which combine in predictable ways via the product. Fewer parameters means accurate estimation occurs with less data and, relatedly, the family contains fewer distributions. This idea is not new. It is explicit in Factorial Hidden Markov Models (FHMMs) (Ghahramani and Jordan, 1997; Saul and Jordan, 1999), and more recently underlies approaches to describing and inferring regular string transductions (Dreyer et al., 2008; Dreyer and Eisner, 2009). Although HMMs and probabilistic finite-state automata describe the same class of distributions (Vidal et al., 2005a; Vidal et al., 2005b), this paper presents these ideas in formal language-theoretic and automata-theoretic terms because (1) there are no hidden states and is thus simpler than FHMMs, (2) determinstic automata have several desirable properties crucially used here, and (3) PDFAs add probabilities to structure whereas HMMs add structure to probabilities and the authors are more comfortable with the former perspective (for further discussion, see Vidal et al. (2005a,b)). The second contribution illustrates the main idea with a feature-based bigram model with a 28 Proceedings of the 11th Meeting</context>
<context position="8010" citStr="Vidal et al., 2005" startWordPosition="1300" endWordPosition="1303"> state q0 of M. By the structure of a PDFA, we mean its structural components.1 Each PDFA M defines a family of distributions given by the possible instantiations of T and F satisfying Equation 1. These distributions have at most |Q|· (|E|+ 1) parameters (since for each state there are |E| possible transitions plus the possibility of finality.) These are, for all q ∈ Q and σ ∈ E, the probabilities T(q, σ) and F(q). To make the connection to probability theory, we sometimes write these as Pr(σ |q) and Pr(# |q), respectively. We define the product of PDFAs in terms of co-emission probabilities (Vidal et al., 2005a). Let M1 = hQ1,E1,q01,δ1,F1,T1i and M2 = 1This is up to the renaming of states so PDFA with isomorphic structural components are said to have the same structure. 29 (Q2, E2, q02, δ2, F2, T2) be PDFAs. The probability that σ1 is emitted from q1 E Q1 at the same moment σ2 is emitted from q2 E Q2 is CT(σ1,σ2,q1,q2) = T1(q1,σ1)·T2(q2,σ2). Similarly, the probability that a word simultaneously ends at q1 E Q1 and at q2 E Q2 is CF(q1, q2) = F1(q1)· F2(q2). Definition 1 The normalized co-emission product of PDFAs M1 and M2 is M = M1 x M2 = (Q, E, q0, δ, F, T) where 1. Q, q0, and F are defined in ter</context>
<context position="9639" citStr="Vidal et al., 2005" startWordPosition="1620" endWordPosition="1623">1, q2), (σ1, σ2)) = CT(hσ1,σ2,q1,q2i) Z In other words, the numerators of T and F are defined to be the co-emission probabilities, and division by Z ensures that M defines a wellformed probability distribution.3 The normalized co-emission product effectively adopts a statistical independence assumption between the states of M1 and M2. If S is a list of PDFAs, we write ® S for their product (note order of product is irrelevant up to renaming of the states). The maximum likelihood (ML) estimation of regular deterministic distributions is a solved problem when the structure of the PDFA is known (Vidal et al., 2005a; Vidal et al., 2005b; de la Higuera, 2010). Let S be a finite sample of words drawn from a regular deterministic distribution D. The problem is to estimate parameters T and F of 2Note that restricting 8 to cases when 0-1 = 0-2 obtains the standard definition of 8 = 81 x 82 (Hopcroft et al., 2001). The reason we maintain two alphabets becomes clear in §4. 3Z((q1, q2)) is less than one whenever either F1(q1) or F2(q2) are neither zero nor one. M so that DM approaches D using the widelyadopted ML criterion (Equation 3). Fˆ) = argmax PrM (w) (3) T,F (wi It is well-known that if D is generated by</context>
</contexts>
<marker>Vidal, Thollard, Higuera, Casacuberta, Carrasco, 2005</marker>
<rawString>Enrique Vidal, Frank Thollard, Colin de la Higuera, Francisco Casacuberta, and Rafael C. Carrasco. 2005b. Probabilistic finite-state machines-part II. IEEE Transactions on Pattern Analysis and Machine Intelligence, 27(7):1026–1039.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Wilson</author>
<author>Marieke Obdeyn</author>
</authors>
<title>Simplifying subsidiary theory: statistical evidence from arabic, muna, shona, and wargamay.</title>
<date>2009</date>
<institution>Johns Hopkins University.</institution>
<contexts>
<context position="32193" citStr="Wilson and Obdeyn (2009)" startWordPosition="5574" endWordPosition="5577">orted.9 Table 4 shows the r values obtained by the HW learner under different conditions. Note we replicate the main result of r = 0.946 when using both features and complement classes.10 This exercise reveals that phonological features play a smaller role in the HW phonotactic learner than previously thought. Features are helpful, but not as much as complement classes of single segments (though features with complement classes yields the best result by this measure). The second issue relates to the first: the question of whether additional parameters are worth the gain in empirical coverage. Wilson and Obdeyn (2009) provide an excellent discussion of the model comparison literature and provide a rigorous comparative analysis of computational modeleling of OCP restrictions. Here we only raise the questions and leave the answers to future research. Compare the HW learners in the first two rows in Table 4. Is the ∼ 0.01 gain in r score worth the additional parameters which refer to phono9Examination of the output grammar reveals heavy reliance on the complement class [ˆs], which is not surprising given the discussion of [sC] clusters in HW. 10This software is available on Bruce Hayes’ webpage: http://www.li</context>
</contexts>
<marker>Wilson, Obdeyn, 2009</marker>
<rawString>Colin Wilson and Marieke Obdeyn. 2009. Simplifying subsidiary theory: statistical evidence from arabic, muna, shona, and wargamay. Johns Hopkins University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Wilson</author>
</authors>
<title>Learning phonology with substantive bias: An experimental and computational study of velar palatalization.</title>
<date>2006</date>
<journal>Cognitive Science,</journal>
<volume>30</volume>
<issue>5</issue>
<contexts>
<context position="1398" citStr="Wilson, 2006" startWordPosition="204" endWordPosition="205">), and it is argued that the bottom-up approach is more analytically transparent. 1 Introduction The hypothesis that the atomic units of phonology are phonological features, and not segments, is one of the tenets of modern phonology (Jakobson et al., 1952; Chomsky and Halle, 1968). According to this hypothesis, segments are essentially epiphenomenal and exist only by virtue of being a shorthand description of a collection of more primitive units—the features. Incorporating this hypothesis into phonological learning models has been the focus of much influential work (Gildea and Jurafsky, 1996; Wilson, 2006; Hayes and Wilson, 2008; Moreton, 2008; Albright, 2009). This paper makes three contributions. The first contribution is a framework within which: 1. researchers can choose which statistical independence assumptions to make regarding phonological features; 2. feature systems can be fully integrated into strictly local (McNaughton and Papert, 1971) (i.e. n-gram models (Jurafsky and Martin, 2008)) and strictly piecewise models (Rogers et al., 2009; Heinz and Rogers, 2010) in order to define families of provably wellformed, feature-based probability distributions that are provably efficiently es</context>
</contexts>
<marker>Wilson, 2006</marker>
<rawString>Colin Wilson. 2006. Learning phonology with substantive bias: An experimental and computational study of velar palatalization. Cognitive Science, 30(5):945–982.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>