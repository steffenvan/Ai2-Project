<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000920">
<title confidence="0.957946">
SCAI: Extracting drug-drug interactions using a rich feature vector
</title>
<author confidence="0.979826">
Tamara Bobi´c1,2, Juliane Fluck1, Martin Hofmann-Apitius1,2
</author>
<affiliation confidence="0.685654">
1Fraunhofer SCAI 2B-IT, Bonn Universit¨at
</affiliation>
<address confidence="0.990870333333333">
Schloss Birlinghoven Dahlmannstraße 2
53754 Sankt Augustin 53113 Bonn
Germany Germany
</address>
<email confidence="0.997816">
{tbobic, jfluck, hofmann-apitius}@scai.fraunhofer.de
</email>
<sectionHeader confidence="0.998598" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999570857142857">
Automatic relation extraction provides great
support for scientists and database curators in
dealing with the extensive amount of biomed-
ical textual data. The DDIExtraction 2013
challenge poses the task of detecting drug-
drug interactions and further categorizing
them into one of the four relation classes. We
present our machine learning system which
utilizes lexical, syntactical and semantic based
feature sets. Resampling, balancing and en-
semble learning experiments are performed to
infer the best configuration. For general drug-
drug relation extraction, the system achieves
70.4% in F, score.
</bodyText>
<sectionHeader confidence="0.999515" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9996762">
Drug-drug interactions (DDI) describe possible in-
terference between pharmacological substances and
are of critical importance in drug development and
administration (August et al., 1997). A drug may
alter the metabolism of another, thus causing an en-
hanced, reduced or even toxic effect in certain med-
ical treatments. For example: “Fluvoxamine in-
hibits the CYP2C9 catalyzed biotransformation of
tolbutamide.” Automated extraction of DDI from
biomedical literature allows for a more efficient
maintenance of the drug knowledge databases and
is beneficial for patients, health care professionals
and the pharmaceutical industry.
Having in mind their biomedical importance, the
objective of the first DDIExtraction challenge1 in
</bodyText>
<footnote confidence="0.9884855">
1http://labda.inf.uc3m.es/
DDIExtraction2011/
</footnote>
<bodyText confidence="0.99990835483871">
2011 was to motivate the development and to eval-
uate the automatic relation extraction (RE) systems
for DDI. Given annotated drug entities, the partic-
ipants addressed the task of identifying undirected
binary relations among them. The knowledge ex-
traction was performed on the sentence level and the
best system achieved 65.74% F1 score (Thomas et
al., 2011a).
The 2013 DDIExtraction challenge2 (organized
as Task 9 of SemEval 2013 (Segura-Bedmar et al.,
2013)) is based on a similar task definition, but ad-
ditionally includes the disambiguation between four
types of interaction: mechanism, effect, advise and
int. The evaluation of participating systems is two-
fold, i. e. partial and strict. Partial evaluation con-
siders that a prediction is correct when the pair la-
bel matches the gold annotation, while strict eval-
uation requires also a correct relation type to be
assigned. The train and test corpora were gener-
ated from textual resources of DrugBank (Knox et
al., 2011) database and MedLine3 abstracts, dealing
with the topic of DDI.
In the following sections we describe our super-
vised machine learning based approach for the ex-
traction of DDI, using a rich feature vector (see Sec-
tion 2.1). The base system employed LibLINEAR
classifier, generating the first run submitted to the
DDIExtraction challenge. Configurations coming
from the two ensemble strategies (Section 2.2) pro-
duced the remaining prediction runs. Furthermore,
we experimentally investigated the impact of train
</bodyText>
<footnote confidence="0.993998333333333">
2http://www.cs.york.ac.uk/semeval-2013/
task9/
3http://www.ncbi.nlm.nih.gov/pubmed/
</footnote>
<page confidence="0.966311">
675
</page>
<bodyText confidence="0.8388685">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 675–683, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics
corpora imbalance on DDI detection through resam-
pling strategies (Section 2.3). Finally, relation type
disambiguation methodology is presented in Sec-
tion 2.4.
</bodyText>
<sectionHeader confidence="0.997727" genericHeader="introduction">
2 Methods
</sectionHeader>
<bodyText confidence="0.9995965">
We formulate the task of relation extraction as
feature-based classification of co-occurring entities
in a sentence. A sentence with n entities contains at
most (� ) interacting pairs. For entity pairs that the
</bodyText>
<page confidence="0.597919">
2
</page>
<bodyText confidence="0.99975375">
classifier detects as “true”, a post-processing step is
performed where one of the four relation types is
assigned, depending on the identified type-specific
trigger words.
</bodyText>
<subsectionHeader confidence="0.806133">
2.1 Features
</subsectionHeader>
<bodyText confidence="0.999934913043478">
To improve generalization of lexical information
Porter stemming algorithm (Porter, 1980) was ap-
plied. All entities present in the sentence, which
were not a part of the investigated pair, are renamed
to a common neutral name (entity blinding).
For the generation of dependency-based features,
sentences in the provided corpora were parsed using
Charniak-Lease parser (Lease and Charniak, 2005;
Thomas et al., 2011b). The resulting constituent
parse trees were converted into Stanford dependency
graphs (Marneffe et al., 2006). Following the idea of
Thomas et al. (2011b), similar relations are treated
equally by using their common parent type (unifica-
tion of dependency types). An example is generaliz-
ing relations “subj”, “nsubj” and “csubj” to a parent
relation “subj”.
In the following subsections the three groups of
features (lexical, syntactical and semantic) with their
corresponding members are described. Table 1 gives
a more structured overview of the feature vector, or-
ganized by type. It should be noted that the listed
features are used for the generation of all three pre-
diction sets submitted to the DDI challenge.
</bodyText>
<subsectionHeader confidence="0.868645">
2.1.1 Lexical features
</subsectionHeader>
<bodyText confidence="0.999960882352941">
Lexical features capture the token information
around the inspected entity pair (EP). The sentence
text is divided into three parts: text between the EP,
text before the EP (left from the first entity) and text
after the EP (right from the second entity). It has
been observed that much of the relation information
can be extracted by only considering these three con-
texts (Bunescu and Mooney, 2005b; Giuliano et al.,
2006).
The majority of features are n-grams based, with
n E 11, 2, 3}. They encompass a narrow (win-
dow=3) and wide (window=10) surrounding con-
text, along with the area between the entities. Addi-
tionally, combinations of the tokens from the three
areas is considered, thus forming before-between,
between-after and before-after conjunct features
(narrow context).
</bodyText>
<subsectionHeader confidence="0.546656">
2.1.2 Syntactic/Dependency features
</subsectionHeader>
<bodyText confidence="0.999486458333333">
Vertices (v) in the dependency graph are analyzed
from a lexical (stemmed token text) and syntacti-
cal (POS tag) perspective, while the edges (e) are
included using the grammatical relation they repre-
sent.
The majority of dependency-based features are
constructed using the properties of edges and ver-
tices along the shortest path (SP) of an entity pair.
The shortest path subtree is conceived to encode
grammatical relations with highest information con-
tent for a specific EP (Bunescu and Mooney, 2005a).
Similarly to lexical features, n-grams of vertices
(edges) along the SP are captured. Furthermore, al-
ternating sequences of vertices and edges (v-walks
and e-walks) of length 3 are accounted for, follow-
ing previous work (Kim et al., 2010; Miwa et al.,
2010).
Apart from the SP-related features, incorporat-
ing information about the entities’ parents and their
common ancestor in the dependency graph is also
beneficial. The lexical and syntactical properties of
these vertices are encoded, along with the grammat-
ical relations on the path from the entities to their
common ancestor.
</bodyText>
<subsectionHeader confidence="0.850751">
2.1.3 Semantic features
</subsectionHeader>
<bodyText confidence="0.999680333333333">
Semantic group of features deals with understand-
ing and meaning of the context in which a particular
entity pair appears.
A feature that accounts for hypothetical state-
ments was introduced in order to reduce the num-
ber of false positives (phrases that indicate investi-
gation in progress, but not actual facts). Negation
(e. g. “not”) detected close to the entity pair (narrow
context) along with a check whether entities in the
</bodyText>
<page confidence="0.998677">
676
</page>
<bodyText confidence="0.9996271875">
pair refer to the same real-word object (abbreviation
or a repetition) represent features which also con-
tribute to the reduction of false positive predictions.
Drug entities in the corpora were annotated with
one of four classes (drug, drug n, brand, group),
which provided another layer of relation informa-
tion for the classifier. Prior knowledge about true
DDI coming from the train corpora is used as a fea-
ture, if a previously known EP is observed in the test
data. Presence of other entities (which are not part
of the inspected EP) in the sentence text is captured,
together with their position relative to the EP.
Finally, mentions of general trigger (interaction)
terms are checked in all three context areas. More-
over, interaction phrases specific to a certain DDI
type (see Section 2.4) are accounted for.
</bodyText>
<subsectionHeader confidence="0.995943">
2.2 Ensemble learning
</subsectionHeader>
<bodyText confidence="0.999923107142857">
Combining different machine learning algorithms
was proposed as a direction for improvement of the
classification accuracy (Bauer and Kohavi, 1999).
A synthesis of predictions using LibLINEAR,
Naive Bayes and Voting Perceptron classifiers is an
attempt to approach and learn the relation informa-
tion from different angles with a goal of increasing
the system’s performance. The three base models in-
cluded in the ensemble are employed through their
WEKA4 (Hall et al., 2009) implementation with de-
fault parameter values and trained on the full feature
vector described in Section 2.1.
LibLINEAR (Fan et al., 2008) is a linear support
vector machine classifier, which has shown high per-
formance (in runtime as well as model accuracy) on
large and sparse data sets. Support vector machines
(SVM, Cortes and Vapnik (1995)) have gained a lot
of popularity in the past decade and very often are
state-of-the-art approach for text mining challenges.
Naive Bayes (Domingos and Pazzani, 1996) is
a simple form of Bayesian networks which relies
on the assumption that every feature is independent
from all other features. Despite their naive design
and apparently oversimplified assumptions, Naive
Bayes can often outperform more sophisticated clas-
sification methods and has worked quite well in
many complex real-world situations. Furthermore,
it can be robust to noise features and is quite insen-
</bodyText>
<footnote confidence="0.959755">
4http://www.cs.waikato.ac.nz/ml/weka/
</footnote>
<table confidence="0.997637">
Corpus Pos Neg Total
MedLine 232 (0.13) 1,555 (0.87) 1,787
DrugBank 3,788 (0.15) 22,217 (0.85) 26,005
</table>
<tableCaption confidence="0.990249">
Table 2: Ratio of positive and negative instances in the
DrugBank and MedLine train corpora.
</tableCaption>
<bodyText confidence="0.998689214285714">
sitive to stratification (Provost, 2000), which is of
high value in class imbalance scenarios.
Voting Perceptron (Freund and Schapire, 1999)
combines a series of perceptrons, which are lin-
ear classification algorithms that process elements
in the train set one at a time (“online”). The sys-
tem stores the number of iterations the perceptron
“survives”, i. e. when the training set instances are
classified correctly. The obtained count represents a
weight used for combining the prediction vectors by
a weighted majority vote.
In the ensemble learning scenario we consider
two strategies that aim at increasing the system’s
performance by either favoring precision or recall:
</bodyText>
<listItem confidence="0.9974255">
1. “majority” – a pair represents true relation only
if majority of the classifiers support that claim
2. “union” – a pair represents true relation if at
least one of the classifiers supports that claim
</listItem>
<subsectionHeader confidence="0.972768">
2.3 Train corpora imbalance
</subsectionHeader>
<bodyText confidence="0.99997585">
Analysis of the basic train corpora statistics re-
veals an unequal ratio of positive and negative in-
stances, i. e. under-representation of true interacting
pairs (see Table 2). Class distribution imbalance of-
ten causes machine learning algorithms to perform
poorly on the minority class (Hulse et al., 2007),
thus, in this case, affecting the recall of true rela-
tions.
In order to explore the sensitivity of our system
to the positive/negative ratio, we performed random
undersampling of the data, artificially obtaining a
desirable ratio (50-50). All positive instances in the
dataset were kept, while the same number of neg-
ative instances were randomly chosen. The reverse
approach of oversampling was considered, but given
the ample train data provided by the organizers, such
strategy could pose run-time challenges.
The experimental setting is described as follows.
MedLine and DrugBank train corpora were divided
further into train (exp-train) and test (exp-test) sets,
</bodyText>
<page confidence="0.98699">
677
</page>
<subsectionHeader confidence="0.727099">
Medline DrugBank
</subsectionHeader>
<bodyText confidence="0.678574">
lex sem syn lex+sem lex+syn sem+syn lex+sem+syn
</bodyText>
<figureCaption confidence="0.993611">
Figure 1: Contribution of individual feature sets and their combinations to the system’s performance, evaluated by 10-
fold cross-validation on the train corpora. Lex is an abbreviation for lexical, sem for semantic and syn for syntactical
features.
</figureCaption>
<figure confidence="0.982148150943396">
Feature
1. n-grams of tokens between the EP
2. n-grams of tokens before the EP (narrow context, window = 3)
3. n-grams of tokens after the EP (narrow context, window = 3)
4. n-grams of tokens before the EP (wide context, window = 10)
5. n-grams of tokens after the EP (wide context, window = 10)
6. conjucted positions: before-between, between-after and before-after
7. dependency n-grams on the SP
8. syntactical n-grams on the SP
9. lexical n-grams on the SP
10. lexical and syntactical e-walks
11. lexical and syntactical v-walks
12. SP length (number of edges)
13. lexical and syntactical information of the entities’ parents
14. lexical and syntactical information of the entities’ common ancestor
15. dependency n-grams from both entities to their common ancestor
16. common ancestor represents a verb or a noun
17. hypothetical context
18. negation close to the EP
19. entities refer to the same object
20. type of entities that form the EP
21. prior knowledge (from the train data)
22. other entities present close to the EP
23. DDI trigger words (general)
24. DDI types trigger words (specific)
Semantic
Syntactical / Dependency
Lexical
Table 1: Overview of features used, stratified into groups. EP denotes an entity pair, SP represent the shortest path.
64
36.9
55.9
69.5
69.3
62
72
80
70
51.6
60
F1
50
40
41.2
36.4
41.1
43.8
46
27.5
30
20
10
0
</figure>
<page confidence="0.94285">
678
</page>
<table confidence="0.957537666666667">
Corpus Exp-train pairs Exp-test pairs
MedLine 1,259 (70.4%) 528 (29.6%)
DrugBank 18,148 (69.8%) 7,857 (30.2%)
</table>
<tableCaption confidence="0.8006245">
Table 3: Experimental train and test subsets derived from
the MedLine and DrugBank train corpora.
</tableCaption>
<table confidence="0.9997548">
Relation MedLine DrugBank
mechanism 62 (0.27) 1257 (0.33)
effect 152 (0.66) 1535 (0.41)
advise 8 (0.03) 818 (0.21)
int 10 (0.04) 178 (0.05)
</table>
<tableCaption confidence="0.994642">
Table 4: The number of positive pairs for different DDI
types in the train corpora. Ratios are given in brackets.
</tableCaption>
<bodyText confidence="0.998272333333333">
with an approximate ratio of 70-30. Instances from
a particular document were always sampled to the
same subset, in order to avoid information leakage.
Table 3 gives an overview of the number of entity
pairs each set comprises. The exp-train corpora were
used for training the model in an original (full-size)
and balanced (subsample) scenario, evaluated on the
exp-test sets.
It should be noted that undersampling experi-
ments were performed on the train corpora in order
to inspect the impact of data imbalance on our sys-
tem (results shown in Section 3.4). However, due
to the challenge limitation of submitting only three
runs, this configuration was ignored in favor of uti-
lizing the complete train corpora.
</bodyText>
<subsectionHeader confidence="0.980097">
2.4 Relation type assignment
</subsectionHeader>
<bodyText confidence="0.9998461">
The DDIExtraction challenge guidelines specify
four classes of relations: advise, mechanism, effect
and int. Table 4 illustrates the ratio of positive pairs
assigned to each type in MedLine and DrugBank
train corpora.
In Section 2.4.1, a brief outlook on the interaction
type characteristics is given, along with some of the
most common relation (trigger) phrases specific to
them. Section 2.4.2 explains the methodology be-
hind the process of relation type assignment.
</bodyText>
<subsectionHeader confidence="0.761551">
2.4.1 Relations overview
</subsectionHeader>
<bodyText confidence="0.999766">
Advise pertains to recommendations regarding co-
administration of two or more drugs. Sentences de-
scribing these relations usually contain words such
as: should, recommended, advisable, caution, avoid
etc., as seen in the following examples:
</bodyText>
<listItem confidence="0.998173142857143">
• Barbiturates and glutethimide should not be
administered to patients receiving coumarin
drugs.
• Concurrent therapy with ORENCIA and TNF
antagonists is not recommended.
• The co-administration of Fluvoxamine Tablets
and diazepam is generally not advisable.
</listItem>
<bodyText confidence="0.998510857142857">
Effect is a relation type describing the signs or
symptoms linked to the DDI, including the phar-
macodynamic effect, i. e. mechanism of interaction.
Some of the phrases often found to denote this type
of relation are: effect, cause, decrease, increase, in-
hibit, activate, modulate etc. The following exam-
ples present expressions of an effect relation:
</bodyText>
<listItem confidence="0.9916511">
• Pretreatment of megakaryocytes with extracel-
lular RR (50 microM) also inhibited InsP(3)-
induced responses.
• It is concluded that neurotensin modulates in
an opposite way the function of the enkephalin-
ergic neurons and the central action of tuftsin.
• Diazepam at doses of 0.25 mg/kg and 2.5
mg/kg injected with morphine was found to
decrease the antinociceptive effect of mor-
phine.
</listItem>
<bodyText confidence="0.997268875">
Mechanism illustrates a more detailed description
of the observed pharmacokinetic changes that in-
cludes biochemical information about metabolism,
absorption, biotransformation, excretion etc. Mech-
anism relations often include mentions of effect-
related interaction phrases, but provide an additional
knowledge layer by addressing more complex bio-
logical concepts:
</bodyText>
<listItem confidence="0.998655333333333">
• Cholestyramine, an anionic-binding resin, has
a considerable effect in lowering the rate and
extent offluvastatin bioavailability.
• Additional iron significantly inhibited the
absorption of cobalt in both dietary cobalt
treatments.
• Macrolide antibiotics inhibit the metabolism of
HMG-CoA reductase inhibitors that are me-
tabolized by CYP3A4.
</listItem>
<page confidence="0.998846">
679
</page>
<bodyText confidence="0.9995714">
Int relation implies sentences which only state
that an interaction occurs, without providing much
additional information about it. Trigger phrases that
can be found in such sentences are usually limited to
different lexical forms of “interaction”:
</bodyText>
<listItem confidence="0.9977555">
• Rifampin and warfarin: a drug interaction.
• In vitro interaction of prostaglandin F2alpha
and oxytocin in placental vessels.
• Treatment with antidepressant drugs can di-
rectly interfere with blood glucose levels or
may interact with hypoglycemic agents.
</listItem>
<subsubsectionHeader confidence="0.436155">
2.4.2 Type disambiguation methodology
</subsubsectionHeader>
<bodyText confidence="0.9958799">
We approach the problem of relation type disam-
biguation as a post-processing step, utilizing identi-
fied (sentence level) trigger words as classification
determinants. Precompiled relation trigger lists are
generated by manual inspection of the train corpora,
largely focusing on MedLine. The lists are specific
to the four interaction types and non-overlapping.
Cases when a sentence contains trigger phrases
from different relation classes are resolved by fol-
lowing a priority list:
</bodyText>
<listItem confidence="0.998374">
1. advise
2. mechanism
3. effect
4. int
</listItem>
<bodyText confidence="0.999982545454545">
The rationale behind such priority assignment are
the following observed patterns in the train corpora.
Regardless of effect or mechanism connotation, if
the sentence contains recommendation-like phrases
(e. g. “should”, “advisable”), it is almost always
classified as an advise. Likewise, even though a re-
lation might be describing an effect, if it contains
a more detailed biochemical description, it is most
likely representing mechanism. Finally, effect has
advantage over int due to the simplicity of the int
relation, along with the lowest observed frequency.
</bodyText>
<sectionHeader confidence="0.999964" genericHeader="related work">
3 Results and Discussion
</sectionHeader>
<subsectionHeader confidence="0.99939">
3.1 Baseline relation extraction performance
</subsectionHeader>
<bodyText confidence="0.999975172413793">
Performances of the submitted prediction runs are
shown in Table 5, where the first row (run1) repre-
sents a system trained on the original (unbalanced)
train corpora, using LibLINEAR classifier and a rich
feature vector (see Section 2.1). The table offers re-
sults overview on MedLine, DrugBank and joined
test corpora (“All”), using partial evaluation (general
DDI detection).
The difference in performance on MedLine and
Drugbank is apparent, measuring up to almost 25
percentage points (pp) in F1 score (46.2% for Med-
Line and 71.1% for DrugBank). Due to a consid-
erably larger size of the DrugBank corpus, overall
results are greatly influenced by this corpus (F1 =
69.0%).
The results imply system’s sensitivity towards
class imbalance, which manifests in favored preci-
sion over recall. However, this discrepancy is much
less observed on DrugBank test corpus. Despite the
similarity in class ratio, DrugBank is a more com-
pact and homogenous corpus, with a relatively uni-
fied writing style. Coming from a manually curated
database, it has a rather standardized way of describ-
ing interactions, resulting in higher performance of
the relation extraction system. MedLine corpora,
however, are derived from different journals and re-
search groups which gives rise to extremely diverse
writing styles and a more challenging task for infor-
mation extraction.
</bodyText>
<subsectionHeader confidence="0.989453">
3.2 Features contribution
</subsectionHeader>
<bodyText confidence="0.976641631578947">
Figure 1 illustrates the performance of the LibLIN-
EAR classifier, when all combinations of the three
different feature sets are explored.
It can be observed that the highest performance is
always achieved when all the features are included
during training (lex+syn+sem), resulting in 51.6%
and 72.0% F1 score for 10-fold cross-validation on
MedLine and DrugBank train corpora respectively.
Lexical features appear to be most useful for the
DrugBank corpus, achieving 88.9% of the maxi-
mum performance when used solely. MedLine, on
the other hand, benefits the most from syntactic fea-
tures that reach 79.8% of the best result, compared to
53.3% with lexical features. Semantic group of fea-
tures exhibits a uniform performance for both cor-
pora, achieving 36.4% and 36.9% of F1 score. Fi-
nally, grouping of two or all three feature sets is
always beneficial and results in higher performance
than the constituting base configurations.
</bodyText>
<page confidence="0.995278">
680
</page>
<table confidence="0.9769082">
MedLine DrugBank All
Classifier P R F1 P R F1 P R F1
run1: LibLINEAR 68.8 34.7 46.2 83.6 61.9 71.1 82.6 59.2 69.0
run2: Majority 68.6 25.3 36.9 83.7 61.7 71.0 82.9 58.1 68.3
run3: Union 43.1 52.6 47.4 79.6 68.1 73.4 74.8 66.6 70.4
</table>
<tableCaption confidence="0.993117">
Table 5: Results of the three submitted runs on the test corpora.
</tableCaption>
<table confidence="0.9969328">
Classifier DrugBank MedLine
LibLinear 654 48
Naive Bayes 854 88
V. Perceptron 608 30
MedLine DrugBank
Train set P R Fl P R Fl
original 48.4 39.6 43.6 75.1 62.4 68.2
balanced 37.2 70.4 48.7 60.8 72.7 66.2
Majority 693 35
Union 980 116
</table>
<tableCaption confidence="0.985111">
Table 6: Number of positive predictions on MedLine and
DrugBank test corpora, using different configurations.
</tableCaption>
<subsectionHeader confidence="0.999275">
3.3 Ensemble experiments
</subsectionHeader>
<bodyText confidence="0.999715652173913">
Performance of the majority and union ensemble
configurations on the test corpora is presented in Ta-
ble 5. Table 6 gives an overview of the number of
predicted positive pairs by the ensemble, as well as
those by the individual base classifiers.
Voting Perceptron behaves similarly to LibLin-
ear, while Naive Bayes demonstrates insensitivity
in terms of class imbalance, predicting the high-
est number of positive pairs for both MedLine and
DrugBank test corpora.
Union voting strategy tends to overcome the lim-
itations of poor recall, resulting in highest perfor-
mance on all test corpora (47.4% for MedLine,
73.4% for DrugBank and 70.4% for All) among the
three runs. The superior result is obtained by dimin-
ishing precision in favor or recall, which was shown
as beneficial in these use-cases. However, the F1
score difference is slight (1.2 pp, 2.3 pp and 1.4 pp),
as compared to the baseline system (run1).
Predictions using the union ensemble ranked Yd
in the general DDI extraction evaluation, achieving
5.5 pp and 9.6 pp of F1 score less than the top two
participating teams.
</bodyText>
<tableCaption confidence="0.992343333333333">
Table 7: Comparison of results on the full train set and
a balanced subsample, as evaluated on the MedLine and
DrugBank train corpora.
</tableCaption>
<subsectionHeader confidence="0.989738">
3.4 Balanced training corpora
</subsectionHeader>
<bodyText confidence="0.999366722222222">
Table 7 presents relation extraction performance for
training on a balanced subset, compared to the orig-
inal unbalanced corpus.
In case of MedLine, an increase of around 5 pp
in F1 score can be observed for the balanced sub-
sample. However, given a relatively high initial per-
formance on DrugBank and the characteristics of
that corpus, training on a subsample results in 2 pp
reduced F1 score. The raise of 30.8 pp in recall
contributes greatly to the increased performance on
MedLine, even though 11.2 pp of precision are lost.
However, in case of DrugBank, a 10.3 pp increase
in recall is not enough to compensate for the 14.3 pp
loss in precision.
It can be observed that although undersampling
approach removes information from the model train-
ing stage, the class balance plays a more significant
role for the final performance.
</bodyText>
<subsectionHeader confidence="0.963087">
3.5 Relation type disambiguation
</subsectionHeader>
<bodyText confidence="0.999850875">
Correct classification of interacting pairs into four
defined classes was evaluated using macro and mi-
cro average measures.
While micro-averaged F1 score is calculated by
constructing a global contingency table and then
calculating precision and recall, macro-averaged F1
score is obtained by first calculating precision and
recall for each relation type and then taking their
</bodyText>
<page confidence="0.994961">
681
</page>
<table confidence="0.99973675">
MedLine DrugBank P All F1
P R F1 P R F1 R
micro avg. 62.5 31.6 42.0 51.3 43.9 47.3 55.1 39.5 46.0
macro avg. 42.0 19.7 26.9 66.5 35.3 46.1 66.6 33.8 44.8
mechanism 70.0 29.2 41.2 58.0 39.2 46.8 53.2 39.1 45.0
effect 64.7 35.5 45.8 52.4 44.6 48.2 48.8 43.9 46.2
advise 18.2 28.6 22.2 50.7 65.0 57.0 50.5 63.3 56.2
int 0 0 0 100 1.1 2.1 100 1.0 2.1
</table>
<tableCaption confidence="0.999838">
Table 8: Results of DDI extraction when relation class detection is evaluated.
</tableCaption>
<bodyText confidence="0.992813326086956">
average (Segura-Bedmar et al., 2013). Therefore,
macro average takes into consideration the relative
frequency of each interaction class, while micro av-
erage treats all classes equally.
Table 8 shows an overview of performances for
DDI extraction with relation class disambiguation,
evaluated for each type separately, as well as cumu-
latively using micro and macro scores. For Med-
Line test corpus, the micro average F1 score of 42%
ranked 1&amp;quot; among all participating systems. How-
ever, the macro average score is much lower, due to
poor performance on advise and int relation classes
and occupies 5th position. Considering that our
methodology gives advantage to relations which are
observed more frequently, it is more adapted to-
wards the micro measure.
The process of manually generating type-specific
trigger lists was largely based on the MedLine train
corpus due to its size, with the assumption that
the relations in DrugBank are similarly expressed.
However, both micro and macro scores for Drug-
Bank ranked 7th, showing that adaptation of trigger
word lists needs to be done, depending on the target
corpus.
In general, lower performance for relation class
assignment is partially due to incompleteness of the
trigger lists, but also coming intrinsically from the
relation priority hierarchy. Most of classification
errors occur when a trigger word belonging to a
“higher” priority class is identified in the sentence.
In the following example the word “should” im-
plies advise relation, although guanfacine and CNS-
depressant drug express an effect relation:
The potential for increased sedation when guan-
facine is given with other CNS-depressant drug
should be appreciated.
Another example is a sentence mentioning “ef-
fect”, but actually describing a simple int relation:
Chloral hydrate and methaqualone interact
pharmacologically with orally administered antico-
agulant agents, but the effect is not clinically signif-
icant.
Furthermore, a lot of missclassifications occur in
sentences which contain pairs and triggers from dif-
ferent types, resulting in all relations being assigned
to the highest identified type.
</bodyText>
<sectionHeader confidence="0.999533" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.99999725">
We present a machine learning based system for
extraction of drug-drug interactions, using lexical,
syntactic and semantic properties of the sentence
text. The system achieves competitive performance
for the general DDI extraction, albeit demonstrat-
ing sensitivity to the train corpora class imbalance.
We show that, depending on the use case, resam-
pling, balancing and ensemble strategies are suc-
cessful in tuning the system to favor recall over pre-
cision. The post-processing step of relation type as-
signment achieves top ranked results for the Med-
Line corpus, however, needs more adaption in case
of DrugBank. Future work includes a comparison
with a multi-classifier approach, which circumvents
the manual task of trigger list generation, supporting
the fully automated scenario of relation extraction.
</bodyText>
<sectionHeader confidence="0.99935" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.95416875">
The authors would like to thank Roman Klinger for
fruitful discussions. T. Bobi´c was funded by the
Bonn-Aachen International Center for Information
Technology (B-IT) Research School.
</bodyText>
<page confidence="0.99785">
682
</page>
<sectionHeader confidence="0.998339" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999849">
J.T. August, F. Murad, W. Anders, J.T. Coyle, and A.P. Li.
1997. Drug-Drug Interactions: Scientific and Regula-
tory Perspectives: Scientific and Regulatory Perspec-
tives. Advances in pharmacology. Elsevier Science.
E. Bauer and R. Kohavi. 1999. An empirical comparison
of voting classification algorithms: Bagging, boosting,
and variants. Machine Learning, 36(1-2).
R. C. Bunescu and R. J. Mooney. 2005a. A shortest path
dependency kernel for relation extraction. In HLT and
EMNLP.
R. C. Bunescu and R. J. Mooney. 2005b. Subsequence
Kernels for Relation Extraction. NIPS.
C. Cortes and V. Vapnik. 1995. Support vector networks.
In Machine Learning.
P. Domingos and M. Pazzani. 1996. Beyond indepen-
dence: Conditions for the optimality of the simple
bayesian classifier. In ICML.
E. Fan, K. Chang, C. Hsieh, X. Wang, and C. Lin. 2008.
LIBLINEAR: A Library for Large Linear Classifica-
tion. Machine Learning Research, 9.
Y. Freund and R. E. Schapire. 1999. Large margin clas-
sification using the perceptron algorithm. Machine
Learning, 37(3).
C. Giuliano, A. Lavelli, and L. Romano. 2006. Exploit-
ing shallow linguistic information for relation extrac-
tion from biomedical literature. In Proc. of the 11st
Conf. of the European Chapter of the Association for
Computational Linguistics (EACL’06).
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I. H. Witten. 2009. The weka data mining
software: An update. SIGKDD Explorations, 11.
J. V. Hulse, T. M. Khoshgoftaar, and A. Napolitano.
2007. Experimental perspectives on learning from im-
balanced data. In ICML.
S. Kim, J. Yoon, J. Yang, and S. Park. 2010. Walk-
weighted subsequence kernels for protein-protein in-
teraction extraction. BMC Bioinformatics, 11.
C. Knox, V. Law, T. Jewison, P. Liu, S. Ly, A. Frolkis,
A. Pon, K. Banco, C. Mak, V. Neveu, Y. Djoumbou,
R. Eisner, A. Chi Guo, and D.S Wishart. 2011. Drug-
bank 3.0: a comprehensive resource for ’omics’ re-
search on drugs. Nucleic Acids Res, 39.
M. Lease and E. Charniak. 2005. Parsing biomedical
literature. In Proc. of IJCNLP’05.
M. C. De Marneffe, B. Maccartney, and C. D. Man-
ning. 2006. Generating typed dependency parses from
phrase structure parses. In LREC.
M. Miwa, R. Saetre, J. D. Kim, and J. Tsujii. 2010. Event
extraction with complex event classification using rich
features. Journal of bioinformatics and computational
biology, 8.
M. Porter. 1980. An algorithm for suffix stripping. Pro-
gram, 14.
F. Provost. 2000. Machine learning from imbalanced
data sets 101 (extended abstract).
I. Segura-Bedmar, P. Martnez, and M. Herrero-Zazo.
2013. Semeval-2013 task 9: Extraction of drug-drug
interactions from biomedical texts. In Proceedings of
the 7th International Workshop on Semantic Evalua-
tion (SemEval 2013).
P. Thomas, M. Neves, I. Solt, D. Tikk, and U. Leser.
2011a. Relation extraction for drug-drug interactions
using ensemble learning. In Proceedings of the 1st
Challenge Task on Drug-Drug Interaction Extraction
2011.
P. Thomas, S. Pietschmann, I. Solt, D. Tikk, and U. Leser.
2011b. Not all links are equal: Exploiting dependency
types for the extraction of protein-protein interactions
from text. In Proceedings of BioNLP 2011 Workshop.
</reference>
<page confidence="0.999153">
683
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.208618">
<title confidence="0.996501">SCAI: Extracting drug-drug interactions using a rich feature vector</title>
<author confidence="0.678268666666667">Juliane Martin SCAI Bonn Universit¨at Schloss Birlinghoven Dahlmannstraße</author>
<affiliation confidence="0.427491">53754 Sankt Augustin 53113 Bonn</affiliation>
<address confidence="0.860852">Germany Germany</address>
<email confidence="0.998291">jfluck,</email>
<abstract confidence="0.999308285714286">Automatic relation extraction provides great support for scientists and database curators in dealing with the extensive amount of biomedical textual data. The DDIExtraction 2013 challenge poses the task of detecting drugdrug interactions and further categorizing them into one of the four relation classes. We present our machine learning system which utilizes lexical, syntactical and semantic based feature sets. Resampling, balancing and ensemble learning experiments are performed to infer the best configuration. For general drugdrug relation extraction, the system achieves</abstract>
<intro confidence="0.732279">in</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J T August</author>
<author>F Murad</author>
<author>W Anders</author>
<author>J T Coyle</author>
<author>A P Li</author>
</authors>
<title>Drug-Drug Interactions: Scientific and Regulatory Perspectives: Scientific and Regulatory Perspectives. Advances in pharmacology.</title>
<date>1997</date>
<publisher>Elsevier Science.</publisher>
<contexts>
<context position="1118" citStr="August et al., 1997" startWordPosition="144" endWordPosition="147">allenge poses the task of detecting drugdrug interactions and further categorizing them into one of the four relation classes. We present our machine learning system which utilizes lexical, syntactical and semantic based feature sets. Resampling, balancing and ensemble learning experiments are performed to infer the best configuration. For general drugdrug relation extraction, the system achieves 70.4% in F, score. 1 Introduction Drug-drug interactions (DDI) describe possible interference between pharmacological substances and are of critical importance in drug development and administration (August et al., 1997). A drug may alter the metabolism of another, thus causing an enhanced, reduced or even toxic effect in certain medical treatments. For example: “Fluvoxamine inhibits the CYP2C9 catalyzed biotransformation of tolbutamide.” Automated extraction of DDI from biomedical literature allows for a more efficient maintenance of the drug knowledge databases and is beneficial for patients, health care professionals and the pharmaceutical industry. Having in mind their biomedical importance, the objective of the first DDIExtraction challenge1 in 1http://labda.inf.uc3m.es/ DDIExtraction2011/ 2011 was to mo</context>
</contexts>
<marker>August, Murad, Anders, Coyle, Li, 1997</marker>
<rawString>J.T. August, F. Murad, W. Anders, J.T. Coyle, and A.P. Li. 1997. Drug-Drug Interactions: Scientific and Regulatory Perspectives: Scientific and Regulatory Perspectives. Advances in pharmacology. Elsevier Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Bauer</author>
<author>R Kohavi</author>
</authors>
<title>An empirical comparison of voting classification algorithms: Bagging, boosting, and variants.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>36--1</pages>
<contexts>
<context position="8597" citStr="Bauer and Kohavi, 1999" startWordPosition="1285" endWordPosition="1288"> coming from the train corpora is used as a feature, if a previously known EP is observed in the test data. Presence of other entities (which are not part of the inspected EP) in the sentence text is captured, together with their position relative to the EP. Finally, mentions of general trigger (interaction) terms are checked in all three context areas. Moreover, interaction phrases specific to a certain DDI type (see Section 2.4) are accounted for. 2.2 Ensemble learning Combining different machine learning algorithms was proposed as a direction for improvement of the classification accuracy (Bauer and Kohavi, 1999). A synthesis of predictions using LibLINEAR, Naive Bayes and Voting Perceptron classifiers is an attempt to approach and learn the relation information from different angles with a goal of increasing the system’s performance. The three base models included in the ensemble are employed through their WEKA4 (Hall et al., 2009) implementation with default parameter values and trained on the full feature vector described in Section 2.1. LibLINEAR (Fan et al., 2008) is a linear support vector machine classifier, which has shown high performance (in runtime as well as model accuracy) on large and sp</context>
</contexts>
<marker>Bauer, Kohavi, 1999</marker>
<rawString>E. Bauer and R. Kohavi. 1999. An empirical comparison of voting classification algorithms: Bagging, boosting, and variants. Machine Learning, 36(1-2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Bunescu</author>
<author>R J Mooney</author>
</authors>
<title>A shortest path dependency kernel for relation extraction.</title>
<date>2005</date>
<booktitle>In HLT and EMNLP.</booktitle>
<contexts>
<context position="5650" citStr="Bunescu and Mooney, 2005" startWordPosition="820" endWordPosition="823"> a more structured overview of the feature vector, organized by type. It should be noted that the listed features are used for the generation of all three prediction sets submitted to the DDI challenge. 2.1.1 Lexical features Lexical features capture the token information around the inspected entity pair (EP). The sentence text is divided into three parts: text between the EP, text before the EP (left from the first entity) and text after the EP (right from the second entity). It has been observed that much of the relation information can be extracted by only considering these three contexts (Bunescu and Mooney, 2005b; Giuliano et al., 2006). The majority of features are n-grams based, with n E 11, 2, 3}. They encompass a narrow (window=3) and wide (window=10) surrounding context, along with the area between the entities. Additionally, combinations of the tokens from the three areas is considered, thus forming before-between, between-after and before-after conjunct features (narrow context). 2.1.2 Syntactic/Dependency features Vertices (v) in the dependency graph are analyzed from a lexical (stemmed token text) and syntactical (POS tag) perspective, while the edges (e) are included using the grammatical r</context>
</contexts>
<marker>Bunescu, Mooney, 2005</marker>
<rawString>R. C. Bunescu and R. J. Mooney. 2005a. A shortest path dependency kernel for relation extraction. In HLT and EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Bunescu</author>
<author>R J Mooney</author>
</authors>
<title>Subsequence Kernels for Relation Extraction.</title>
<date>2005</date>
<publisher>NIPS.</publisher>
<contexts>
<context position="5650" citStr="Bunescu and Mooney, 2005" startWordPosition="820" endWordPosition="823"> a more structured overview of the feature vector, organized by type. It should be noted that the listed features are used for the generation of all three prediction sets submitted to the DDI challenge. 2.1.1 Lexical features Lexical features capture the token information around the inspected entity pair (EP). The sentence text is divided into three parts: text between the EP, text before the EP (left from the first entity) and text after the EP (right from the second entity). It has been observed that much of the relation information can be extracted by only considering these three contexts (Bunescu and Mooney, 2005b; Giuliano et al., 2006). The majority of features are n-grams based, with n E 11, 2, 3}. They encompass a narrow (window=3) and wide (window=10) surrounding context, along with the area between the entities. Additionally, combinations of the tokens from the three areas is considered, thus forming before-between, between-after and before-after conjunct features (narrow context). 2.1.2 Syntactic/Dependency features Vertices (v) in the dependency graph are analyzed from a lexical (stemmed token text) and syntactical (POS tag) perspective, while the edges (e) are included using the grammatical r</context>
</contexts>
<marker>Bunescu, Mooney, 2005</marker>
<rawString>R. C. Bunescu and R. J. Mooney. 2005b. Subsequence Kernels for Relation Extraction. NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cortes</author>
<author>V Vapnik</author>
</authors>
<title>Support vector networks.</title>
<date>1995</date>
<booktitle>In Machine Learning.</booktitle>
<contexts>
<context position="9267" citStr="Cortes and Vapnik (1995)" startWordPosition="1393" endWordPosition="1396">Naive Bayes and Voting Perceptron classifiers is an attempt to approach and learn the relation information from different angles with a goal of increasing the system’s performance. The three base models included in the ensemble are employed through their WEKA4 (Hall et al., 2009) implementation with default parameter values and trained on the full feature vector described in Section 2.1. LibLINEAR (Fan et al., 2008) is a linear support vector machine classifier, which has shown high performance (in runtime as well as model accuracy) on large and sparse data sets. Support vector machines (SVM, Cortes and Vapnik (1995)) have gained a lot of popularity in the past decade and very often are state-of-the-art approach for text mining challenges. Naive Bayes (Domingos and Pazzani, 1996) is a simple form of Bayesian networks which relies on the assumption that every feature is independent from all other features. Despite their naive design and apparently oversimplified assumptions, Naive Bayes can often outperform more sophisticated classification methods and has worked quite well in many complex real-world situations. Furthermore, it can be robust to noise features and is quite insen4http://www.cs.waikato.ac.nz/</context>
</contexts>
<marker>Cortes, Vapnik, 1995</marker>
<rawString>C. Cortes and V. Vapnik. 1995. Support vector networks. In Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Domingos</author>
<author>M Pazzani</author>
</authors>
<title>Beyond independence: Conditions for the optimality of the simple bayesian classifier.</title>
<date>1996</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="9433" citStr="Domingos and Pazzani, 1996" startWordPosition="1419" endWordPosition="1422">em’s performance. The three base models included in the ensemble are employed through their WEKA4 (Hall et al., 2009) implementation with default parameter values and trained on the full feature vector described in Section 2.1. LibLINEAR (Fan et al., 2008) is a linear support vector machine classifier, which has shown high performance (in runtime as well as model accuracy) on large and sparse data sets. Support vector machines (SVM, Cortes and Vapnik (1995)) have gained a lot of popularity in the past decade and very often are state-of-the-art approach for text mining challenges. Naive Bayes (Domingos and Pazzani, 1996) is a simple form of Bayesian networks which relies on the assumption that every feature is independent from all other features. Despite their naive design and apparently oversimplified assumptions, Naive Bayes can often outperform more sophisticated classification methods and has worked quite well in many complex real-world situations. Furthermore, it can be robust to noise features and is quite insen4http://www.cs.waikato.ac.nz/ml/weka/ Corpus Pos Neg Total MedLine 232 (0.13) 1,555 (0.87) 1,787 DrugBank 3,788 (0.15) 22,217 (0.85) 26,005 Table 2: Ratio of positive and negative instances in th</context>
</contexts>
<marker>Domingos, Pazzani, 1996</marker>
<rawString>P. Domingos and M. Pazzani. 1996. Beyond independence: Conditions for the optimality of the simple bayesian classifier. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Fan</author>
<author>K Chang</author>
<author>C Hsieh</author>
<author>X Wang</author>
<author>C Lin</author>
</authors>
<title>LIBLINEAR: A Library for Large Linear Classification.</title>
<date>2008</date>
<journal>Machine Learning Research,</journal>
<volume>9</volume>
<contexts>
<context position="9062" citStr="Fan et al., 2008" startWordPosition="1359" endWordPosition="1362">ning Combining different machine learning algorithms was proposed as a direction for improvement of the classification accuracy (Bauer and Kohavi, 1999). A synthesis of predictions using LibLINEAR, Naive Bayes and Voting Perceptron classifiers is an attempt to approach and learn the relation information from different angles with a goal of increasing the system’s performance. The three base models included in the ensemble are employed through their WEKA4 (Hall et al., 2009) implementation with default parameter values and trained on the full feature vector described in Section 2.1. LibLINEAR (Fan et al., 2008) is a linear support vector machine classifier, which has shown high performance (in runtime as well as model accuracy) on large and sparse data sets. Support vector machines (SVM, Cortes and Vapnik (1995)) have gained a lot of popularity in the past decade and very often are state-of-the-art approach for text mining challenges. Naive Bayes (Domingos and Pazzani, 1996) is a simple form of Bayesian networks which relies on the assumption that every feature is independent from all other features. Despite their naive design and apparently oversimplified assumptions, Naive Bayes can often outperfo</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>E. Fan, K. Chang, C. Hsieh, X. Wang, and C. Lin. 2008. LIBLINEAR: A Library for Large Linear Classification. Machine Learning Research, 9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>R E Schapire</author>
</authors>
<title>Large margin classification using the perceptron algorithm.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>37</volume>
<issue>3</issue>
<contexts>
<context position="10211" citStr="Freund and Schapire, 1999" startWordPosition="1532" endWordPosition="1535">ign and apparently oversimplified assumptions, Naive Bayes can often outperform more sophisticated classification methods and has worked quite well in many complex real-world situations. Furthermore, it can be robust to noise features and is quite insen4http://www.cs.waikato.ac.nz/ml/weka/ Corpus Pos Neg Total MedLine 232 (0.13) 1,555 (0.87) 1,787 DrugBank 3,788 (0.15) 22,217 (0.85) 26,005 Table 2: Ratio of positive and negative instances in the DrugBank and MedLine train corpora. sitive to stratification (Provost, 2000), which is of high value in class imbalance scenarios. Voting Perceptron (Freund and Schapire, 1999) combines a series of perceptrons, which are linear classification algorithms that process elements in the train set one at a time (“online”). The system stores the number of iterations the perceptron “survives”, i. e. when the training set instances are classified correctly. The obtained count represents a weight used for combining the prediction vectors by a weighted majority vote. In the ensemble learning scenario we consider two strategies that aim at increasing the system’s performance by either favoring precision or recall: 1. “majority” – a pair represents true relation only if majority</context>
</contexts>
<marker>Freund, Schapire, 1999</marker>
<rawString>Y. Freund and R. E. Schapire. 1999. Large margin classification using the perceptron algorithm. Machine Learning, 37(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Giuliano</author>
<author>A Lavelli</author>
<author>L Romano</author>
</authors>
<title>Exploiting shallow linguistic information for relation extraction from biomedical literature.</title>
<date>2006</date>
<booktitle>In Proc. of the 11st Conf. of the European Chapter of the Association for Computational Linguistics (EACL’06).</booktitle>
<contexts>
<context position="5675" citStr="Giuliano et al., 2006" startWordPosition="824" endWordPosition="827"> of the feature vector, organized by type. It should be noted that the listed features are used for the generation of all three prediction sets submitted to the DDI challenge. 2.1.1 Lexical features Lexical features capture the token information around the inspected entity pair (EP). The sentence text is divided into three parts: text between the EP, text before the EP (left from the first entity) and text after the EP (right from the second entity). It has been observed that much of the relation information can be extracted by only considering these three contexts (Bunescu and Mooney, 2005b; Giuliano et al., 2006). The majority of features are n-grams based, with n E 11, 2, 3}. They encompass a narrow (window=3) and wide (window=10) surrounding context, along with the area between the entities. Additionally, combinations of the tokens from the three areas is considered, thus forming before-between, between-after and before-after conjunct features (narrow context). 2.1.2 Syntactic/Dependency features Vertices (v) in the dependency graph are analyzed from a lexical (stemmed token text) and syntactical (POS tag) perspective, while the edges (e) are included using the grammatical relation they represent. T</context>
</contexts>
<marker>Giuliano, Lavelli, Romano, 2006</marker>
<rawString>C. Giuliano, A. Lavelli, and L. Romano. 2006. Exploiting shallow linguistic information for relation extraction from biomedical literature. In Proc. of the 11st Conf. of the European Chapter of the Association for Computational Linguistics (EACL’06).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hall</author>
<author>E Frank</author>
<author>G Holmes</author>
<author>B Pfahringer</author>
<author>P Reutemann</author>
<author>I H Witten</author>
</authors>
<title>The weka data mining software: An update.</title>
<date>2009</date>
<journal>SIGKDD Explorations,</journal>
<volume>11</volume>
<contexts>
<context position="8923" citStr="Hall et al., 2009" startWordPosition="1337" endWordPosition="1340">all three context areas. Moreover, interaction phrases specific to a certain DDI type (see Section 2.4) are accounted for. 2.2 Ensemble learning Combining different machine learning algorithms was proposed as a direction for improvement of the classification accuracy (Bauer and Kohavi, 1999). A synthesis of predictions using LibLINEAR, Naive Bayes and Voting Perceptron classifiers is an attempt to approach and learn the relation information from different angles with a goal of increasing the system’s performance. The three base models included in the ensemble are employed through their WEKA4 (Hall et al., 2009) implementation with default parameter values and trained on the full feature vector described in Section 2.1. LibLINEAR (Fan et al., 2008) is a linear support vector machine classifier, which has shown high performance (in runtime as well as model accuracy) on large and sparse data sets. Support vector machines (SVM, Cortes and Vapnik (1995)) have gained a lot of popularity in the past decade and very often are state-of-the-art approach for text mining challenges. Naive Bayes (Domingos and Pazzani, 1996) is a simple form of Bayesian networks which relies on the assumption that every feature i</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and I. H. Witten. 2009. The weka data mining software: An update. SIGKDD Explorations, 11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J V Hulse</author>
<author>T M Khoshgoftaar</author>
<author>A Napolitano</author>
</authors>
<title>Experimental perspectives on learning from imbalanced data.</title>
<date>2007</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="11284" citStr="Hulse et al., 2007" startWordPosition="1702" endWordPosition="1705">im at increasing the system’s performance by either favoring precision or recall: 1. “majority” – a pair represents true relation only if majority of the classifiers support that claim 2. “union” – a pair represents true relation if at least one of the classifiers supports that claim 2.3 Train corpora imbalance Analysis of the basic train corpora statistics reveals an unequal ratio of positive and negative instances, i. e. under-representation of true interacting pairs (see Table 2). Class distribution imbalance often causes machine learning algorithms to perform poorly on the minority class (Hulse et al., 2007), thus, in this case, affecting the recall of true relations. In order to explore the sensitivity of our system to the positive/negative ratio, we performed random undersampling of the data, artificially obtaining a desirable ratio (50-50). All positive instances in the dataset were kept, while the same number of negative instances were randomly chosen. The reverse approach of oversampling was considered, but given the ample train data provided by the organizers, such strategy could pose run-time challenges. The experimental setting is described as follows. MedLine and DrugBank train corpora w</context>
</contexts>
<marker>Hulse, Khoshgoftaar, Napolitano, 2007</marker>
<rawString>J. V. Hulse, T. M. Khoshgoftaar, and A. Napolitano. 2007. Experimental perspectives on learning from imbalanced data. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kim</author>
<author>J Yoon</author>
<author>J Yang</author>
<author>S Park</author>
</authors>
<title>Walkweighted subsequence kernels for protein-protein interaction extraction.</title>
<date>2010</date>
<journal>BMC Bioinformatics,</journal>
<volume>11</volume>
<contexts>
<context position="6811" citStr="Kim et al., 2010" startWordPosition="999" endWordPosition="1002">hile the edges (e) are included using the grammatical relation they represent. The majority of dependency-based features are constructed using the properties of edges and vertices along the shortest path (SP) of an entity pair. The shortest path subtree is conceived to encode grammatical relations with highest information content for a specific EP (Bunescu and Mooney, 2005a). Similarly to lexical features, n-grams of vertices (edges) along the SP are captured. Furthermore, alternating sequences of vertices and edges (v-walks and e-walks) of length 3 are accounted for, following previous work (Kim et al., 2010; Miwa et al., 2010). Apart from the SP-related features, incorporating information about the entities’ parents and their common ancestor in the dependency graph is also beneficial. The lexical and syntactical properties of these vertices are encoded, along with the grammatical relations on the path from the entities to their common ancestor. 2.1.3 Semantic features Semantic group of features deals with understanding and meaning of the context in which a particular entity pair appears. A feature that accounts for hypothetical statements was introduced in order to reduce the number of false pos</context>
</contexts>
<marker>Kim, Yoon, Yang, Park, 2010</marker>
<rawString>S. Kim, J. Yoon, J. Yang, and S. Park. 2010. Walkweighted subsequence kernels for protein-protein interaction extraction. BMC Bioinformatics, 11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Knox</author>
<author>V Law</author>
<author>T Jewison</author>
<author>P Liu</author>
<author>S Ly</author>
<author>A Frolkis</author>
<author>A Pon</author>
<author>K Banco</author>
<author>C Mak</author>
<author>V Neveu</author>
<author>Y Djoumbou</author>
<author>R Eisner</author>
<author>A Chi Guo</author>
<author>D S Wishart</author>
</authors>
<title>Drugbank 3.0: a comprehensive resource for ’omics’ research on drugs.</title>
<date>2011</date>
<journal>Nucleic Acids Res,</journal>
<volume>39</volume>
<contexts>
<context position="2679" citStr="Knox et al., 2011" startWordPosition="380" endWordPosition="383">a). The 2013 DDIExtraction challenge2 (organized as Task 9 of SemEval 2013 (Segura-Bedmar et al., 2013)) is based on a similar task definition, but additionally includes the disambiguation between four types of interaction: mechanism, effect, advise and int. The evaluation of participating systems is twofold, i. e. partial and strict. Partial evaluation considers that a prediction is correct when the pair label matches the gold annotation, while strict evaluation requires also a correct relation type to be assigned. The train and test corpora were generated from textual resources of DrugBank (Knox et al., 2011) database and MedLine3 abstracts, dealing with the topic of DDI. In the following sections we describe our supervised machine learning based approach for the extraction of DDI, using a rich feature vector (see Section 2.1). The base system employed LibLINEAR classifier, generating the first run submitted to the DDIExtraction challenge. Configurations coming from the two ensemble strategies (Section 2.2) produced the remaining prediction runs. Furthermore, we experimentally investigated the impact of train 2http://www.cs.york.ac.uk/semeval-2013/ task9/ 3http://www.ncbi.nlm.nih.gov/pubmed/ 675 S</context>
</contexts>
<marker>Knox, Law, Jewison, Liu, Ly, Frolkis, Pon, Banco, Mak, Neveu, Djoumbou, Eisner, Guo, Wishart, 2011</marker>
<rawString>C. Knox, V. Law, T. Jewison, P. Liu, S. Ly, A. Frolkis, A. Pon, K. Banco, C. Mak, V. Neveu, Y. Djoumbou, R. Eisner, A. Chi Guo, and D.S Wishart. 2011. Drugbank 3.0: a comprehensive resource for ’omics’ research on drugs. Nucleic Acids Res, 39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lease</author>
<author>E Charniak</author>
</authors>
<title>Parsing biomedical literature.</title>
<date>2005</date>
<booktitle>In Proc. of IJCNLP’05.</booktitle>
<contexts>
<context position="4490" citStr="Lease and Charniak, 2005" startWordPosition="634" endWordPosition="637">teracting pairs. For entity pairs that the 2 classifier detects as “true”, a post-processing step is performed where one of the four relation types is assigned, depending on the identified type-specific trigger words. 2.1 Features To improve generalization of lexical information Porter stemming algorithm (Porter, 1980) was applied. All entities present in the sentence, which were not a part of the investigated pair, are renamed to a common neutral name (entity blinding). For the generation of dependency-based features, sentences in the provided corpora were parsed using Charniak-Lease parser (Lease and Charniak, 2005; Thomas et al., 2011b). The resulting constituent parse trees were converted into Stanford dependency graphs (Marneffe et al., 2006). Following the idea of Thomas et al. (2011b), similar relations are treated equally by using their common parent type (unification of dependency types). An example is generalizing relations “subj”, “nsubj” and “csubj” to a parent relation “subj”. In the following subsections the three groups of features (lexical, syntactical and semantic) with their corresponding members are described. Table 1 gives a more structured overview of the feature vector, organized by </context>
</contexts>
<marker>Lease, Charniak, 2005</marker>
<rawString>M. Lease and E. Charniak. 2005. Parsing biomedical literature. In Proc. of IJCNLP’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M C De Marneffe</author>
<author>B Maccartney</author>
<author>C D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In LREC.</booktitle>
<marker>De Marneffe, Maccartney, Manning, 2006</marker>
<rawString>M. C. De Marneffe, B. Maccartney, and C. D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Miwa</author>
<author>R Saetre</author>
<author>J D Kim</author>
<author>J Tsujii</author>
</authors>
<title>Event extraction with complex event classification using rich features.</title>
<date>2010</date>
<journal>Journal of bioinformatics and computational biology,</journal>
<volume>8</volume>
<contexts>
<context position="6831" citStr="Miwa et al., 2010" startWordPosition="1003" endWordPosition="1006"> are included using the grammatical relation they represent. The majority of dependency-based features are constructed using the properties of edges and vertices along the shortest path (SP) of an entity pair. The shortest path subtree is conceived to encode grammatical relations with highest information content for a specific EP (Bunescu and Mooney, 2005a). Similarly to lexical features, n-grams of vertices (edges) along the SP are captured. Furthermore, alternating sequences of vertices and edges (v-walks and e-walks) of length 3 are accounted for, following previous work (Kim et al., 2010; Miwa et al., 2010). Apart from the SP-related features, incorporating information about the entities’ parents and their common ancestor in the dependency graph is also beneficial. The lexical and syntactical properties of these vertices are encoded, along with the grammatical relations on the path from the entities to their common ancestor. 2.1.3 Semantic features Semantic group of features deals with understanding and meaning of the context in which a particular entity pair appears. A feature that accounts for hypothetical statements was introduced in order to reduce the number of false positives (phrases that</context>
</contexts>
<marker>Miwa, Saetre, Kim, Tsujii, 2010</marker>
<rawString>M. Miwa, R. Saetre, J. D. Kim, and J. Tsujii. 2010. Event extraction with complex event classification using rich features. Journal of bioinformatics and computational biology, 8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Porter</author>
</authors>
<title>An algorithm for suffix stripping.</title>
<date>1980</date>
<journal>Program,</journal>
<volume>14</volume>
<contexts>
<context position="4186" citStr="Porter, 1980" startWordPosition="589" endWordPosition="590">esampling strategies (Section 2.3). Finally, relation type disambiguation methodology is presented in Section 2.4. 2 Methods We formulate the task of relation extraction as feature-based classification of co-occurring entities in a sentence. A sentence with n entities contains at most (� ) interacting pairs. For entity pairs that the 2 classifier detects as “true”, a post-processing step is performed where one of the four relation types is assigned, depending on the identified type-specific trigger words. 2.1 Features To improve generalization of lexical information Porter stemming algorithm (Porter, 1980) was applied. All entities present in the sentence, which were not a part of the investigated pair, are renamed to a common neutral name (entity blinding). For the generation of dependency-based features, sentences in the provided corpora were parsed using Charniak-Lease parser (Lease and Charniak, 2005; Thomas et al., 2011b). The resulting constituent parse trees were converted into Stanford dependency graphs (Marneffe et al., 2006). Following the idea of Thomas et al. (2011b), similar relations are treated equally by using their common parent type (unification of dependency types). An exampl</context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>M. Porter. 1980. An algorithm for suffix stripping. Program, 14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Provost</author>
</authors>
<title>Machine learning from imbalanced data sets 101 (extended abstract).</title>
<date>2000</date>
<contexts>
<context position="10111" citStr="Provost, 2000" startWordPosition="1519" endWordPosition="1520">ption that every feature is independent from all other features. Despite their naive design and apparently oversimplified assumptions, Naive Bayes can often outperform more sophisticated classification methods and has worked quite well in many complex real-world situations. Furthermore, it can be robust to noise features and is quite insen4http://www.cs.waikato.ac.nz/ml/weka/ Corpus Pos Neg Total MedLine 232 (0.13) 1,555 (0.87) 1,787 DrugBank 3,788 (0.15) 22,217 (0.85) 26,005 Table 2: Ratio of positive and negative instances in the DrugBank and MedLine train corpora. sitive to stratification (Provost, 2000), which is of high value in class imbalance scenarios. Voting Perceptron (Freund and Schapire, 1999) combines a series of perceptrons, which are linear classification algorithms that process elements in the train set one at a time (“online”). The system stores the number of iterations the perceptron “survives”, i. e. when the training set instances are classified correctly. The obtained count represents a weight used for combining the prediction vectors by a weighted majority vote. In the ensemble learning scenario we consider two strategies that aim at increasing the system’s performance by e</context>
</contexts>
<marker>Provost, 2000</marker>
<rawString>F. Provost. 2000. Machine learning from imbalanced data sets 101 (extended abstract).</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Segura-Bedmar</author>
<author>P Martnez</author>
<author>M Herrero-Zazo</author>
</authors>
<title>Semeval-2013 task 9: Extraction of drug-drug interactions from biomedical texts.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval</booktitle>
<contexts>
<context position="2164" citStr="Segura-Bedmar et al., 2013" startWordPosition="296" endWordPosition="299">rmaceutical industry. Having in mind their biomedical importance, the objective of the first DDIExtraction challenge1 in 1http://labda.inf.uc3m.es/ DDIExtraction2011/ 2011 was to motivate the development and to evaluate the automatic relation extraction (RE) systems for DDI. Given annotated drug entities, the participants addressed the task of identifying undirected binary relations among them. The knowledge extraction was performed on the sentence level and the best system achieved 65.74% F1 score (Thomas et al., 2011a). The 2013 DDIExtraction challenge2 (organized as Task 9 of SemEval 2013 (Segura-Bedmar et al., 2013)) is based on a similar task definition, but additionally includes the disambiguation between four types of interaction: mechanism, effect, advise and int. The evaluation of participating systems is twofold, i. e. partial and strict. Partial evaluation considers that a prediction is correct when the pair label matches the gold annotation, while strict evaluation requires also a correct relation type to be assigned. The train and test corpora were generated from textual resources of DrugBank (Knox et al., 2011) database and MedLine3 abstracts, dealing with the topic of DDI. In the following sec</context>
<context position="24913" citStr="Segura-Bedmar et al., 2013" startWordPosition="3864" endWordPosition="3867">hen calculating precision and recall, macro-averaged F1 score is obtained by first calculating precision and recall for each relation type and then taking their 681 MedLine DrugBank P All F1 P R F1 P R F1 R micro avg. 62.5 31.6 42.0 51.3 43.9 47.3 55.1 39.5 46.0 macro avg. 42.0 19.7 26.9 66.5 35.3 46.1 66.6 33.8 44.8 mechanism 70.0 29.2 41.2 58.0 39.2 46.8 53.2 39.1 45.0 effect 64.7 35.5 45.8 52.4 44.6 48.2 48.8 43.9 46.2 advise 18.2 28.6 22.2 50.7 65.0 57.0 50.5 63.3 56.2 int 0 0 0 100 1.1 2.1 100 1.0 2.1 Table 8: Results of DDI extraction when relation class detection is evaluated. average (Segura-Bedmar et al., 2013). Therefore, macro average takes into consideration the relative frequency of each interaction class, while micro average treats all classes equally. Table 8 shows an overview of performances for DDI extraction with relation class disambiguation, evaluated for each type separately, as well as cumulatively using micro and macro scores. For MedLine test corpus, the micro average F1 score of 42% ranked 1&amp;quot; among all participating systems. However, the macro average score is much lower, due to poor performance on advise and int relation classes and occupies 5th position. Considering that our method</context>
</contexts>
<marker>Segura-Bedmar, Martnez, Herrero-Zazo, 2013</marker>
<rawString>I. Segura-Bedmar, P. Martnez, and M. Herrero-Zazo. 2013. Semeval-2013 task 9: Extraction of drug-drug interactions from biomedical texts. In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Thomas</author>
<author>M Neves</author>
<author>I Solt</author>
<author>D Tikk</author>
<author>U Leser</author>
</authors>
<title>Relation extraction for drug-drug interactions using ensemble learning.</title>
<date>2011</date>
<booktitle>In Proceedings of the 1st Challenge Task on Drug-Drug Interaction Extraction</booktitle>
<contexts>
<context position="2061" citStr="Thomas et al., 2011" startWordPosition="281" endWordPosition="284"> drug knowledge databases and is beneficial for patients, health care professionals and the pharmaceutical industry. Having in mind their biomedical importance, the objective of the first DDIExtraction challenge1 in 1http://labda.inf.uc3m.es/ DDIExtraction2011/ 2011 was to motivate the development and to evaluate the automatic relation extraction (RE) systems for DDI. Given annotated drug entities, the participants addressed the task of identifying undirected binary relations among them. The knowledge extraction was performed on the sentence level and the best system achieved 65.74% F1 score (Thomas et al., 2011a). The 2013 DDIExtraction challenge2 (organized as Task 9 of SemEval 2013 (Segura-Bedmar et al., 2013)) is based on a similar task definition, but additionally includes the disambiguation between four types of interaction: mechanism, effect, advise and int. The evaluation of participating systems is twofold, i. e. partial and strict. Partial evaluation considers that a prediction is correct when the pair label matches the gold annotation, while strict evaluation requires also a correct relation type to be assigned. The train and test corpora were generated from textual resources of DrugBank (</context>
<context position="4511" citStr="Thomas et al., 2011" startWordPosition="638" endWordPosition="641">y pairs that the 2 classifier detects as “true”, a post-processing step is performed where one of the four relation types is assigned, depending on the identified type-specific trigger words. 2.1 Features To improve generalization of lexical information Porter stemming algorithm (Porter, 1980) was applied. All entities present in the sentence, which were not a part of the investigated pair, are renamed to a common neutral name (entity blinding). For the generation of dependency-based features, sentences in the provided corpora were parsed using Charniak-Lease parser (Lease and Charniak, 2005; Thomas et al., 2011b). The resulting constituent parse trees were converted into Stanford dependency graphs (Marneffe et al., 2006). Following the idea of Thomas et al. (2011b), similar relations are treated equally by using their common parent type (unification of dependency types). An example is generalizing relations “subj”, “nsubj” and “csubj” to a parent relation “subj”. In the following subsections the three groups of features (lexical, syntactical and semantic) with their corresponding members are described. Table 1 gives a more structured overview of the feature vector, organized by type. It should be no</context>
</contexts>
<marker>Thomas, Neves, Solt, Tikk, Leser, 2011</marker>
<rawString>P. Thomas, M. Neves, I. Solt, D. Tikk, and U. Leser. 2011a. Relation extraction for drug-drug interactions using ensemble learning. In Proceedings of the 1st Challenge Task on Drug-Drug Interaction Extraction 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Thomas</author>
<author>S Pietschmann</author>
<author>I Solt</author>
<author>D Tikk</author>
<author>U Leser</author>
</authors>
<title>Not all links are equal: Exploiting dependency types for the extraction of protein-protein interactions from text.</title>
<date>2011</date>
<booktitle>In Proceedings of BioNLP 2011 Workshop.</booktitle>
<contexts>
<context position="2061" citStr="Thomas et al., 2011" startWordPosition="281" endWordPosition="284"> drug knowledge databases and is beneficial for patients, health care professionals and the pharmaceutical industry. Having in mind their biomedical importance, the objective of the first DDIExtraction challenge1 in 1http://labda.inf.uc3m.es/ DDIExtraction2011/ 2011 was to motivate the development and to evaluate the automatic relation extraction (RE) systems for DDI. Given annotated drug entities, the participants addressed the task of identifying undirected binary relations among them. The knowledge extraction was performed on the sentence level and the best system achieved 65.74% F1 score (Thomas et al., 2011a). The 2013 DDIExtraction challenge2 (organized as Task 9 of SemEval 2013 (Segura-Bedmar et al., 2013)) is based on a similar task definition, but additionally includes the disambiguation between four types of interaction: mechanism, effect, advise and int. The evaluation of participating systems is twofold, i. e. partial and strict. Partial evaluation considers that a prediction is correct when the pair label matches the gold annotation, while strict evaluation requires also a correct relation type to be assigned. The train and test corpora were generated from textual resources of DrugBank (</context>
<context position="4511" citStr="Thomas et al., 2011" startWordPosition="638" endWordPosition="641">y pairs that the 2 classifier detects as “true”, a post-processing step is performed where one of the four relation types is assigned, depending on the identified type-specific trigger words. 2.1 Features To improve generalization of lexical information Porter stemming algorithm (Porter, 1980) was applied. All entities present in the sentence, which were not a part of the investigated pair, are renamed to a common neutral name (entity blinding). For the generation of dependency-based features, sentences in the provided corpora were parsed using Charniak-Lease parser (Lease and Charniak, 2005; Thomas et al., 2011b). The resulting constituent parse trees were converted into Stanford dependency graphs (Marneffe et al., 2006). Following the idea of Thomas et al. (2011b), similar relations are treated equally by using their common parent type (unification of dependency types). An example is generalizing relations “subj”, “nsubj” and “csubj” to a parent relation “subj”. In the following subsections the three groups of features (lexical, syntactical and semantic) with their corresponding members are described. Table 1 gives a more structured overview of the feature vector, organized by type. It should be no</context>
</contexts>
<marker>Thomas, Pietschmann, Solt, Tikk, Leser, 2011</marker>
<rawString>P. Thomas, S. Pietschmann, I. Solt, D. Tikk, and U. Leser. 2011b. Not all links are equal: Exploiting dependency types for the extraction of protein-protein interactions from text. In Proceedings of BioNLP 2011 Workshop.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>