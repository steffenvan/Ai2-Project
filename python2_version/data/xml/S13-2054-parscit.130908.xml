<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001282">
<title confidence="0.9983705">
GU-MLT-LT: Sentiment Analysis of Short Messages using
Linguistic Features and Stochastic Gradient Descent
</title>
<author confidence="0.993721">
Tobias G¨unther
</author>
<affiliation confidence="0.995491">
University of Gothenburg
</affiliation>
<address confidence="0.9057885">
Olof Wijksgatan 6
41255 G¨oteborg, Sweden
</address>
<email confidence="0.998905">
email@tobias.io
</email>
<author confidence="0.975216">
Lenz Furrer
</author>
<affiliation confidence="0.994436">
University of Zurich
</affiliation>
<address confidence="0.964747">
Binzm¨uhlestrasse 14
8050 Z¨urich, Switzerland
</address>
<email confidence="0.999361">
lenz.furrer@gmail.com
</email>
<sectionHeader confidence="0.995646" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999766733333333">
This paper describes the details of our system
submitted to the SemEval-2013 shared task on
sentiment analysis in Twitter. Our approach to
predicting the sentiment of Tweets and SMS
is based on supervised machine learning tech-
niques and task-specific feature engineering.
We used a linear classifier trained by stochas-
tic gradient descent with hinge loss and elas-
tic net regularization to make our predictions,
which were ranked first or second in three of
the four experimental conditions of the shared
task. Furthermore, our system makes use of
social media specific text preprocessing and
linguistically motivated features, such as word
stems, word clusters and negation handling.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999905770833333">
Sentiment analysis, also known as opinion min-
ing, is a research field in the area of text min-
ing and natural language processing, which inves-
tigates the automated detection of opinions in lan-
guage. In written text, an opinion is a person’s atti-
tude towards some topic, pronounced by verbal (e.g.
choice of words, rhetorical figures) or non-verbal
means (e.g. emoticons, emphatic spelling). More
formally, Liu (2012) defines an opinion as the quin-
tuple (ei, aij, sijkl, hk, tl) where “ei is the name of
an entity, aij is an aspect of ei, sijkl is the sentiment
on aspect aij of entity ei, hk is the opinion holder,
and tl is the time when the opinion is expressed by
hk. The sentiment sijkl is positive, negative, or neu-
tral, or expressed with different strength/intensity
levels [...]. When an opinion is on the entity itself
as a whole, the special aspect GENERAL is used
to denote it. [...] ei and aij together represent the
opinion target” (Liu, 2012).
With the massively growing importance of social
media in everyday life, being able to automatically
find and classify attitudes in written text allows for
estimating the mood of a large group of people, e.g.
towards a certain event, service, product, matter of
fact or the like. As working with the very short and
informal texts typical for social networks poses chal-
lenges not encountered in more traditional text gen-
res, the International Workshop on Semantic Evalu-
ation (SemEval) 2013 has a shared task on sentiment
analysis in microblogging texts, which is detailed in
Wilson et al. (2013). The task requires sentiment
analysis of Twitter1 and SMS messages and com-
prises two subtasks, one of which deals with deter-
mining the sentiment of a given message fragment
depending on its context (Task A) and one on over-
all message polarity classification (Task B).
We treat both tasks as document-level senti-
ment classification tasks, which we define ac-
cording to Liu (2012) as determining the opinion
( , GENERAL, s, , ) of a given message, where
s E {positive, negative, neutral} and “the entity e,
opinion holder h, and time of opinion t are assumed
known or irrelevant” (Liu, 2012). For Task A we
only consider the marked fraction of the message to
be given.
This introduction is followed by sections dis-
cussing related work (2), details of our system (3),
experiments (4) and results and conclusion (5).
</bodyText>
<footnote confidence="0.998699">
1a popular microblogging service on the Internet, see
http://twitter.com
</footnote>
<page confidence="0.894821">
328
</page>
<bodyText confidence="0.7371085">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 328–332, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics
</bodyText>
<sectionHeader confidence="0.998957" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99982935">
Previous approaches to sentiment analysis of mi-
croblogging texts make use of a wide range of fea-
tures, including unigrams, n-grams, part-of-speech
tags and polarity values from (usually hand-crafted)
sentiment lexicons. O’Connor et al. (2010) exam-
ine tweets concerned with the 2009 US presiden-
tial elections, relying solely on the occurrence of
words from a sentiment lexicon. Nielsen (2011) in-
vestigates the impact of including internet slang and
obscene language when building a sentiment lexi-
con. Barbosa and Feng (2010) make use of three
different sentiment detection websites to label Twit-
ter data, while Davidov et al. (2010), Kouloumpis et
al. (2011) and Pak and Paroubek (2010) use Twit-
ter hashtags and emoticons as labels. Speriosu et
al. (2011) propagate information from seed labels
along a linked structure that includes Twitter’s fol-
lower graph, and Saif et al. (2012) specifically ad-
dress the data-sparsity problem by using semantic
smoothing and topic extraction.
</bodyText>
<sectionHeader confidence="0.990829" genericHeader="method">
3 System Description
</sectionHeader>
<bodyText confidence="0.99996525">
In this section we present the details of our senti-
ment analysis system, which was implemented in
the Python programming language and is publicly
available online.2 We used the same preprocessing,
feature extraction and learning algorithm for both
subtasks, only the hyperparameters of the machine
learning algorithm were adjusted to the respective
dataset.
</bodyText>
<subsectionHeader confidence="0.999143">
3.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.99993825">
Tokenization of the messages was done using a sim-
ple regular expression, which matches either URLs,
alphanumeric character sequences (plus apostrophe)
or non-alphanumeric non-whitespace character se-
quences. This way punctuation sequences like
emoticons are preserved, while still being separated
from words in case of missing whitespace. The same
happens to hashtags, so “#liiike:)” gets separated
into the three tokens #, liiike and :), which can
then be processed separately or as n-grams. While
this strategy performed reasonably well for us, more
sophisticated tokenizers for social media messages
</bodyText>
<footnote confidence="0.495376">
2http://tobias.io/semevaltweet
</footnote>
<bodyText confidence="0.9658565">
that handle more special cases like emoticons in-
cluding letters are thinkable.
To address the large variety in spelling typical for
social networks we store three different variants of
each token:
a) The raw token found in the message
</bodyText>
<listItem confidence="0.599699571428571">
b) A normalized version, in which all characters
are converted to lowercase and all digits to 0
c) A collapsed version, in which all adjacent du-
plicate characters are removed from the nor-
malized version, if it is not present in an
English word list. That way “school” stays
“school”, but “liiike” gets converted to “like”.
</listItem>
<subsectionHeader confidence="0.972443">
3.2 Features
</subsectionHeader>
<bodyText confidence="0.999993">
We explored a wide variety of linguistic and lexical
features. In our final submission we used the follow-
ing set of features for each message:
</bodyText>
<listItem confidence="0.9879794375">
• The normalized tokens [boolean]
• The stems of the collapsed tokens, which were
computed using the Porter stemming algo-
rithm (Porter, 1980) implemented in the Python
Natural Language Toolkit (Bird et al., 2009).
[boolean]
• The word cluster IDs of raw, normalized and
collapsed tokens. The clusters were obtained
via unsupervised Brown clustering (Brown et
al., 1992) of 56,345,753 Tweets by Owoputi
et al. (2013) and are available on the web.3
[boolean]
• The accumulated (summed) positive and accu-
mulated negative SentiWordNet scores (Bac-
cianella et al., 2010) of all synsets matching the
collapsed token strings. [continuous]
</listItem>
<bodyText confidence="0.999169">
Furthermore, the normalized tokens and stems
were marked with a special negation prefix, if they
occurred after a negation word or word cluster of
negation words. If a punctuation token occurred be-
fore the end of the message the marking was discon-
tinued at that point.
</bodyText>
<footnote confidence="0.925172">
3http://www.ark.cs.cmu.edu/TweetNLP
</footnote>
<page confidence="0.994639">
329
</page>
<subsectionHeader confidence="0.981147">
3.3 Machine Learning Methods
</subsectionHeader>
<bodyText confidence="0.992782869565217">
For the classification of the messages into the posi-
tive, negative and neutral classes we use three linear
models, which were trained in an one-vs.-all man-
ner. At prediction time we simply choose the label
with the highest score. All training was done us-
ing the open-source machine learning toolkit scikit-
learn,4 which provides a consistent Python API to
fast implementations of various machine learning al-
gorithms (Pedregosa et al., 2011).
The linear models were trained using stochastic
gradient descent (SGD), which is a gradient de-
scent optimization method that minimizes a given
loss function. The term “stochastic” refers to the
fact that the weights of the model are updated for
each training example, which is an approximation of
batch gradient descent, in which all training exam-
ples are considered to make a single step. This way
SGD is very fast to train, which was important to us
to be able to rapidly evaluate different feature com-
binations and hyperparameter settings using cross-
validation.
Algorithm 1 Stochastic gradient descent with hinge
loss and elastic net regularization
</bodyText>
<equation confidence="0.491143555555556">
η ← 1/(α t)
8: c ← CLASSWEIGHT(y(i))
9: u ← u + ((1 − ρ) η α)
10: for j to NFEATURES do
∂` �−y(i)x(i)
jif y(i)s &lt; 1
∂w&apos; ←0 otherwise
12: wj ← (1 − ρ η α) wj − η c ∂`
∂w�
</equation>
<listItem confidence="0.343149428571429">
13: z ← wj
14: if wj &gt; 0 then
15: wj ← max(0, wj − (u + qj))
16: else if wj &lt; 0 then
17: wj ← min(0, wj + (u − qj))
18: qj ← qj + (wj − z)
19: t ← t + 1
</listItem>
<footnote confidence="0.981382">
4Version 0.13.1, http://scikit-learn.org
</footnote>
<table confidence="0.999732">
Hyperparameter Task A Task B
NITER 1000 1000
CLASSWEIGHT(y(i)) 1 auto5
α 0.0001 0.001
ρ 0.15 0.15
</table>
<tableCaption confidence="0.999951">
Table 1: Hyperparameters used for final model training
</tableCaption>
<bodyText confidence="0.999904454545455">
The loss function we used was hinge loss, which
is a large-margin loss function known for its use in
support vector machines. To avoid overfitting the
training set we employed elastic net regularization,
which is a combination of L1 and L2 regularization.
A simplified version of the SGD learning proce-
dure implemented in scikit-learn is shown in Algo-
rithm 1, where w is the weight vector of the model,
x(i) the feature vector of sample i, y(i) ∈ {−1, +1}
the ground truth label of sample i, η the learning
rate, α the regularization factor and ρ the elastic
net mixing parameter. Be aware that we did not
pick samples at random or shuffle the data, which
is crucial in case of training data which is sorted
by classes. The initial learning rate is set heuris-
tically and updated following Shalev-Shwartz et al.
(2007).6 The way of applying the L1 penalty (lines
13 to 18) is published as “cumulative L1 penalty” in
Tsuruoka et al. (2009). The final settings for the hy-
perparameters were determined by running a cross-
validated grid search on the combined training and
development sets and can be found in Table 1.
</bodyText>
<sectionHeader confidence="0.999651" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999914166666667">
For our experiments and the final model training
we used the combined training and development set
of the shared task. For Task A we removed mes-
sages labeled “objective” prior to training, while we
merged them into the “neutral” class for Task B.
This left us with 9419 training samples (5855 pos-
itive, 457 neutral, 3107 negative) for Task A and
10368 training samples (3855 positive, 4889 neutral,
1624 negative) for Task B. As the shared task was
evaluated on average F1 of the positive and negative
class, disregarding the neutral class, we also provide
our results in these measures in the following.
</bodyText>
<footnote confidence="0.977117666666667">
5inversely proportional to class frequency
6This is achieved by choosing “optimal” as setting for the
learning rate for scikit-learn’s SGDClassifier.
</footnote>
<listItem confidence="0.555298666666667">
1: t ← 1/(η α)
2: u ← 0
3: Initialize wj and qj with 0 for all j
4: for epoch to NITER do
5: for i to NSAMPLES do
6: s ← wTx(i)
</listItem>
<page confidence="0.865949">
330
</page>
<table confidence="0.99900325">
Negative Rec Prec Positive Avg.
Prec Rec Fl
ALL 53.86 62.68 77.88 68.95 65.54
-stem -0.38 -1.10 -0.07 -0.08 -0.385
-wc -0.74 -0.30 +0.13 -2.05 -0.835
-swn -0.15 -0.73 -0.27 +0.10 -0.23
-neg +0.04 -0.92 -1.06 +0.44 -0.30
bow -4.03 -7.01 -0.44 -3.68 -3.83
</table>
<tableCaption confidence="0.975302">
Table 2: Feature ablation study (Task B)
</tableCaption>
<figure confidence="0.9939975">
Average F1
●
●
●
●
●
● SGD ALL
SGD BOW
● PER ALL
PER BOW
40 45 50 55 60 65
●
●
●
●
500 1000 2500 5000 7500
</figure>
<bodyText confidence="0.999294361111111">
During the process of preparing our submission
we used 10-fold cross-validation to evaluate differ-
ent combinations of features, machine learning algo-
rithms and their hyperparameter settings. While we
found the features described in section 3.2 to be use-
ful, we did not find further improvement by using
n-grams and part-of-speech tags, despite using the
Twitter-specific part-of-speech tagger by Owoputi et
al. (2013). Table 2 shows a cross-validated ablation
study on the features, removing one group of fea-
tures at a time to see their contribution to the model.
Using only normalized tokens is referred to as bag-
of-words (bow). One can see that word clusters are
the most important for our model, causing the high-
est overall loss in Fl performance when being re-
moved. Nevertheless, all other features contribute to
the performance of the model as well.
Further improvement can be made by carefully
picking a machine learning algorithm and tuning its
hyperparameters. For this task we found linear mod-
els to perform better than other classification meth-
ods such as naive bayes, decision tree / random for-
est and k-nearest neighbor. Figure 1 shows that
models trained with the method described in sec-
tion 3.3 (marked SGD) clearly outperforms mod-
els trained with the popular perceptron algorithm
(which could be described as stochastic gradient de-
scent with zero-one loss, no regularization and con-
stant learning rate, marked PER) with increasing
training set size. The values were obtained by train-
ing on different portions of the training set of Task
B and testing on the previously unseen Task B Twit-
ter test set (3813 samples). Starting from a cer-
tain amount of available training data, the choice of
the training algorithm becomes even more important
than the choice of features.
</bodyText>
<subsectionHeader confidence="0.654231">
Training samples used
</subsectionHeader>
<figureCaption confidence="0.998588">
Figure 1: Effect of training set size on different classifiers
</figureCaption>
<sectionHeader confidence="0.997495" genericHeader="evaluation">
5 Results and Conclusion
</sectionHeader>
<bodyText confidence="0.999931166666667">
The results of our submission for the four hidden test
sets of the shared task can be found in Table 3. Given
the relatively small deviation from the results of the
cross-validation on combined training and develop-
ment set and the good ranks obtained in the shared
task system ranking, we conclude that the method
for sentiment analysis in microblogging messages
presented in this paper yields competitive results.
We showed that the performance for this task can
be improved by using linguistically motivated fea-
tures as well as carefully choosing a learning algo-
rithm and its hyperparameter settings.
</bodyText>
<table confidence="0.9845202">
Task Prec Rec Fl (Rank)
A SMS 86.09 91.01 88.37 (1)
A Twitter 85.06 85.43 85.19 (7)
B SMS 55.83 72.55 62.15 (2)
B Twitter 70.21 61.49 65.27 (2)
</table>
<tableCaption confidence="0.997659">
Table 3: Final results of our submission
</tableCaption>
<sectionHeader confidence="0.998243" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999960333333333">
We would like to thank the organizers of the shared
task for their effort, Peter Prettenhofer for his help
with getting to the bottom of the SGD implementa-
tion in scikit-learn and Richard Johansson as well as
the anonymous reviewers for their helpful comments
on the paper.
</bodyText>
<page confidence="0.998022">
331
</page>
<sectionHeader confidence="0.989831" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999475558139535">
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining.
In Proceedings of the 7th conference on International
Language Resources and Evaluation (LREC’10), Val-
letta, Malta.
Luciano Barbosa and Junlan Feng. 2010. Robust Sen-
timent Detection on Twitter from Biased and Noisy
Data. In COLING (Posters), pages 36–44.
Steven Bird, Ewan Klein, and Edward Loper. 2009. Nat-
ural Language Processing with Python. O’Reilly Me-
dia.
Peter F Brown, Peter V Desouza, Robert L Mercer, Vin-
cent J Della Pietra, and Jenifer C Lai. 1992. Class-
based n-gram models of natural language. Computa-
tional linguistics, 18(4):467–479.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced Sentiment Learning Using Twitter Hashtags
and Smileys. In COLING (Posters), pages 241–249.
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter Sentiment Analysis: The Good
the Bad and the OMG! In Fifth International AAAI
Conference on Weblogs and Social Media, ICWSM.
Bing Liu. 2012. Sentiment Analysis and Opinion Min-
ing. Synthesis Lectures on Human Language Tech-
nologies. Morgan &amp; Claypool Publishers.
Finn ˚Arup Nielsen. 2011. A new ANEW: Evaluation of
a word list for sentiment analysis in microblogs. In
Proceedings of the ESWC2011 Workshop on ‘Making
Sense of Microposts’: Big things come in small pack-
ages, volume 718 of CEUR Workshop Proceedings,
pages 93–98.
Brendan O’Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010. From
Tweets to Polls: Linking Text Sentiment to Public
Opinion Time Series. In Fourth International AAAI
Conference on Weblogs and Social Media, ICWSM.
Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin
Gimpel, Nathan Schneider, and Noah A Smith. 2013.
Improved part-of-speech tagging for online conver-
sational text with word clusters. In Proceedings of
NAACL 2013.
Alexander Pak and Patrick Paroubek. 2010. Twitter as
a Corpus for Sentiment Analysis and Opinion Min-
ing. In Proceedings of the 7th conference on Inter-
national Language Resources and Evaluation, volume
2010, pages 1320–1326.
Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier Grisel,
Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vin-
cent Dubourg, Jake Vanderplas, Alexandre Passos,
David Cournapeau, Matthieu Brucher, Matthieu Per-
rot, and ´Edouard Duchesnay. 2011. Scikit-learn: Ma-
chine learning in Python. Journal of Machine Learn-
ing Research, 12:2825–2830.
Martin F Porter. 1980. An algorithm for suffix stripping.
Program: electronic library and information systems,
14(3):130–137.
Hassan Saif, Yulan He, and Harith Alani. 2012. Alle-
viating data sparsity for twitter sentiment analysis. In
Proceedings of the 2nd Workshop on Making Sense of
Microposts.
Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro.
2007. Pegasos: Primal estimated sub-gradient solver
for svm. In Proceedings of the 24th international con-
ference on Machine learning, pages 807–814. ACM.
Michael Speriosu, Nikita Sudan, Sid Upadhyay, and Ja-
son Baldridge. 2011. Twitter Polarity Classification
with Label Propagation over Lexical Links and the
Follower Graph. In Proceedings of the 2011 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, EMNLP ’11, page 53–63.
Yoshimasa Tsuruoka, Jun’ichi Tsujii, and Sophia Ana-
niadou. 2009. Stochastic gradient descent training
for l1-regularized log-linear models with cumulative
penalty. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP: Volume 1-Volume 1, pages 477–
485. Association for Computational Linguistics.
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Alan
Ritter, Sara Rosenthal, and Veselin Stoyanov. 2013.
Semeval-2013 task 2: Sentiment analysis in twitter.
In Proceedings of the 7th International Workshop on
Semantic Evaluation. Association for Computational
Linguistics.
</reference>
<page confidence="0.99823">
332
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.352351">
<title confidence="0.9978945">GU-MLT-LT: Sentiment Analysis of Short Messages using Linguistic Features and Stochastic Gradient Descent</title>
<author confidence="0.992223">Tobias</author>
<affiliation confidence="0.9779465">University of Olof Wijksgatan</affiliation>
<address confidence="0.991174">41255 G¨oteborg,</address>
<email confidence="0.983463">tobias.io</email>
<author confidence="0.689078">Lenz</author>
<affiliation confidence="0.783606">University of Binzm¨uhlestrasse</affiliation>
<address confidence="0.992453">8050 Z¨urich,</address>
<email confidence="0.999968">lenz.furrer@gmail.com</email>
<abstract confidence="0.99853775">This paper describes the details of our system submitted to the SemEval-2013 shared task on sentiment analysis in Twitter. Our approach to predicting the sentiment of Tweets and SMS is based on supervised machine learning techniques and task-specific feature engineering. We used a linear classifier trained by stochastic gradient descent with hinge loss and elastic net regularization to make our predictions, which were ranked first or second in three of the four experimental conditions of the shared task. Furthermore, our system makes use of social media specific text preprocessing and linguistically motivated features, such as word stems, word clusters and negation handling.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Stefano Baccianella</author>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining.</title>
<date>2010</date>
<booktitle>In Proceedings of the 7th conference on International Language Resources and Evaluation (LREC’10),</booktitle>
<location>Valletta,</location>
<contexts>
<context position="7007" citStr="Baccianella et al., 2010" startWordPosition="1096" endWordPosition="1100">sion we used the following set of features for each message: • The normalized tokens [boolean] • The stems of the collapsed tokens, which were computed using the Porter stemming algorithm (Porter, 1980) implemented in the Python Natural Language Toolkit (Bird et al., 2009). [boolean] • The word cluster IDs of raw, normalized and collapsed tokens. The clusters were obtained via unsupervised Brown clustering (Brown et al., 1992) of 56,345,753 Tweets by Owoputi et al. (2013) and are available on the web.3 [boolean] • The accumulated (summed) positive and accumulated negative SentiWordNet scores (Baccianella et al., 2010) of all synsets matching the collapsed token strings. [continuous] Furthermore, the normalized tokens and stems were marked with a special negation prefix, if they occurred after a negation word or word cluster of negation words. If a punctuation token occurred before the end of the message the marking was discontinued at that point. 3http://www.ark.cs.cmu.edu/TweetNLP 329 3.3 Machine Learning Methods For the classification of the messages into the positive, negative and neutral classes we use three linear models, which were trained in an one-vs.-all manner. At prediction time we simply choose</context>
</contexts>
<marker>Baccianella, Esuli, Sebastiani, 2010</marker>
<rawString>Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. 2010. Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining. In Proceedings of the 7th conference on International Language Resources and Evaluation (LREC’10), Valletta, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luciano Barbosa</author>
<author>Junlan Feng</author>
</authors>
<title>Robust Sentiment Detection on Twitter from Biased and Noisy Data.</title>
<date>2010</date>
<booktitle>In COLING (Posters),</booktitle>
<pages>36--44</pages>
<contexts>
<context position="4243" citStr="Barbosa and Feng (2010)" startWordPosition="668" endWordPosition="671">Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics 2 Related Work Previous approaches to sentiment analysis of microblogging texts make use of a wide range of features, including unigrams, n-grams, part-of-speech tags and polarity values from (usually hand-crafted) sentiment lexicons. O’Connor et al. (2010) examine tweets concerned with the 2009 US presidential elections, relying solely on the occurrence of words from a sentiment lexicon. Nielsen (2011) investigates the impact of including internet slang and obscene language when building a sentiment lexicon. Barbosa and Feng (2010) make use of three different sentiment detection websites to label Twitter data, while Davidov et al. (2010), Kouloumpis et al. (2011) and Pak and Paroubek (2010) use Twitter hashtags and emoticons as labels. Speriosu et al. (2011) propagate information from seed labels along a linked structure that includes Twitter’s follower graph, and Saif et al. (2012) specifically address the data-sparsity problem by using semantic smoothing and topic extraction. 3 System Description In this section we present the details of our sentiment analysis system, which was implemented in the Python programming la</context>
</contexts>
<marker>Barbosa, Feng, 2010</marker>
<rawString>Luciano Barbosa and Junlan Feng. 2010. Robust Sentiment Detection on Twitter from Biased and Noisy Data. In COLING (Posters), pages 36–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bird</author>
<author>Ewan Klein</author>
<author>Edward Loper</author>
</authors>
<date>2009</date>
<booktitle>Natural Language Processing with Python. O’Reilly Media.</booktitle>
<contexts>
<context position="6655" citStr="Bird et al., 2009" startWordPosition="1042" endWordPosition="1045">se and all digits to 0 c) A collapsed version, in which all adjacent duplicate characters are removed from the normalized version, if it is not present in an English word list. That way “school” stays “school”, but “liiike” gets converted to “like”. 3.2 Features We explored a wide variety of linguistic and lexical features. In our final submission we used the following set of features for each message: • The normalized tokens [boolean] • The stems of the collapsed tokens, which were computed using the Porter stemming algorithm (Porter, 1980) implemented in the Python Natural Language Toolkit (Bird et al., 2009). [boolean] • The word cluster IDs of raw, normalized and collapsed tokens. The clusters were obtained via unsupervised Brown clustering (Brown et al., 1992) of 56,345,753 Tweets by Owoputi et al. (2013) and are available on the web.3 [boolean] • The accumulated (summed) positive and accumulated negative SentiWordNet scores (Baccianella et al., 2010) of all synsets matching the collapsed token strings. [continuous] Furthermore, the normalized tokens and stems were marked with a special negation prefix, if they occurred after a negation word or word cluster of negation words. If a punctuation t</context>
</contexts>
<marker>Bird, Klein, Loper, 2009</marker>
<rawString>Steven Bird, Ewan Klein, and Edward Loper. 2009. Natural Language Processing with Python. O’Reilly Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Peter V Desouza</author>
<author>Robert L Mercer</author>
<author>Vincent J Della Pietra</author>
<author>Jenifer C Lai</author>
</authors>
<title>Classbased n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="6812" citStr="Brown et al., 1992" startWordPosition="1066" endWordPosition="1069">n English word list. That way “school” stays “school”, but “liiike” gets converted to “like”. 3.2 Features We explored a wide variety of linguistic and lexical features. In our final submission we used the following set of features for each message: • The normalized tokens [boolean] • The stems of the collapsed tokens, which were computed using the Porter stemming algorithm (Porter, 1980) implemented in the Python Natural Language Toolkit (Bird et al., 2009). [boolean] • The word cluster IDs of raw, normalized and collapsed tokens. The clusters were obtained via unsupervised Brown clustering (Brown et al., 1992) of 56,345,753 Tweets by Owoputi et al. (2013) and are available on the web.3 [boolean] • The accumulated (summed) positive and accumulated negative SentiWordNet scores (Baccianella et al., 2010) of all synsets matching the collapsed token strings. [continuous] Furthermore, the normalized tokens and stems were marked with a special negation prefix, if they occurred after a negation word or word cluster of negation words. If a punctuation token occurred before the end of the message the marking was discontinued at that point. 3http://www.ark.cs.cmu.edu/TweetNLP 329 3.3 Machine Learning Methods </context>
</contexts>
<marker>Brown, Desouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>Peter F Brown, Peter V Desouza, Robert L Mercer, Vincent J Della Pietra, and Jenifer C Lai. 1992. Classbased n-gram models of natural language. Computational linguistics, 18(4):467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Oren Tsur</author>
<author>Ari Rappoport</author>
</authors>
<title>Enhanced Sentiment Learning Using Twitter Hashtags and Smileys.</title>
<date>2010</date>
<booktitle>In COLING (Posters),</booktitle>
<pages>241--249</pages>
<contexts>
<context position="4351" citStr="Davidov et al. (2010)" startWordPosition="686" endWordPosition="689">pproaches to sentiment analysis of microblogging texts make use of a wide range of features, including unigrams, n-grams, part-of-speech tags and polarity values from (usually hand-crafted) sentiment lexicons. O’Connor et al. (2010) examine tweets concerned with the 2009 US presidential elections, relying solely on the occurrence of words from a sentiment lexicon. Nielsen (2011) investigates the impact of including internet slang and obscene language when building a sentiment lexicon. Barbosa and Feng (2010) make use of three different sentiment detection websites to label Twitter data, while Davidov et al. (2010), Kouloumpis et al. (2011) and Pak and Paroubek (2010) use Twitter hashtags and emoticons as labels. Speriosu et al. (2011) propagate information from seed labels along a linked structure that includes Twitter’s follower graph, and Saif et al. (2012) specifically address the data-sparsity problem by using semantic smoothing and topic extraction. 3 System Description In this section we present the details of our sentiment analysis system, which was implemented in the Python programming language and is publicly available online.2 We used the same preprocessing, feature extraction and learning al</context>
</contexts>
<marker>Davidov, Tsur, Rappoport, 2010</marker>
<rawString>Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010. Enhanced Sentiment Learning Using Twitter Hashtags and Smileys. In COLING (Posters), pages 241–249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Efthymios Kouloumpis</author>
<author>Theresa Wilson</author>
<author>Johanna Moore</author>
</authors>
<title>Twitter Sentiment Analysis: The Good the Bad and the OMG! In</title>
<date>2011</date>
<booktitle>Fifth International AAAI Conference on Weblogs and Social Media, ICWSM.</booktitle>
<contexts>
<context position="4377" citStr="Kouloumpis et al. (2011)" startWordPosition="690" endWordPosition="693">analysis of microblogging texts make use of a wide range of features, including unigrams, n-grams, part-of-speech tags and polarity values from (usually hand-crafted) sentiment lexicons. O’Connor et al. (2010) examine tweets concerned with the 2009 US presidential elections, relying solely on the occurrence of words from a sentiment lexicon. Nielsen (2011) investigates the impact of including internet slang and obscene language when building a sentiment lexicon. Barbosa and Feng (2010) make use of three different sentiment detection websites to label Twitter data, while Davidov et al. (2010), Kouloumpis et al. (2011) and Pak and Paroubek (2010) use Twitter hashtags and emoticons as labels. Speriosu et al. (2011) propagate information from seed labels along a linked structure that includes Twitter’s follower graph, and Saif et al. (2012) specifically address the data-sparsity problem by using semantic smoothing and topic extraction. 3 System Description In this section we present the details of our sentiment analysis system, which was implemented in the Python programming language and is publicly available online.2 We used the same preprocessing, feature extraction and learning algorithm for both subtasks,</context>
</contexts>
<marker>Kouloumpis, Wilson, Moore, 2011</marker>
<rawString>Efthymios Kouloumpis, Theresa Wilson, and Johanna Moore. 2011. Twitter Sentiment Analysis: The Good the Bad and the OMG! In Fifth International AAAI Conference on Weblogs and Social Media, ICWSM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
</authors>
<title>Sentiment Analysis and Opinion Mining. Synthesis Lectures on Human Language Technologies.</title>
<date>2012</date>
<publisher>Morgan &amp; Claypool Publishers.</publisher>
<contexts>
<context position="1430" citStr="Liu (2012)" startWordPosition="209" endWordPosition="210">task. Furthermore, our system makes use of social media specific text preprocessing and linguistically motivated features, such as word stems, word clusters and negation handling. 1 Introduction Sentiment analysis, also known as opinion mining, is a research field in the area of text mining and natural language processing, which investigates the automated detection of opinions in language. In written text, an opinion is a person’s attitude towards some topic, pronounced by verbal (e.g. choice of words, rhetorical figures) or non-verbal means (e.g. emoticons, emphatic spelling). More formally, Liu (2012) defines an opinion as the quintuple (ei, aij, sijkl, hk, tl) where “ei is the name of an entity, aij is an aspect of ei, sijkl is the sentiment on aspect aij of entity ei, hk is the opinion holder, and tl is the time when the opinion is expressed by hk. The sentiment sijkl is positive, negative, or neutral, or expressed with different strength/intensity levels [...]. When an opinion is on the entity itself as a whole, the special aspect GENERAL is used to denote it. [...] ei and aij together represent the opinion target” (Liu, 2012). With the massively growing importance of social media in ev</context>
<context position="2942" citStr="Liu (2012)" startWordPosition="469" endWordPosition="470">s challenges not encountered in more traditional text genres, the International Workshop on Semantic Evaluation (SemEval) 2013 has a shared task on sentiment analysis in microblogging texts, which is detailed in Wilson et al. (2013). The task requires sentiment analysis of Twitter1 and SMS messages and comprises two subtasks, one of which deals with determining the sentiment of a given message fragment depending on its context (Task A) and one on overall message polarity classification (Task B). We treat both tasks as document-level sentiment classification tasks, which we define according to Liu (2012) as determining the opinion ( , GENERAL, s, , ) of a given message, where s E {positive, negative, neutral} and “the entity e, opinion holder h, and time of opinion t are assumed known or irrelevant” (Liu, 2012). For Task A we only consider the marked fraction of the message to be given. This introduction is followed by sections discussing related work (2), details of our system (3), experiments (4) and results and conclusion (5). 1a popular microblogging service on the Internet, see http://twitter.com 328 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh</context>
</contexts>
<marker>Liu, 2012</marker>
<rawString>Bing Liu. 2012. Sentiment Analysis and Opinion Mining. Synthesis Lectures on Human Language Technologies. Morgan &amp; Claypool Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Finn ˚Arup Nielsen</author>
</authors>
<title>A new ANEW: Evaluation of a word list for sentiment analysis in microblogs.</title>
<date>2011</date>
<booktitle>In Proceedings of the ESWC2011 Workshop on ‘Making Sense of Microposts’: Big things come in small packages, volume 718 of CEUR Workshop Proceedings,</booktitle>
<pages>93--98</pages>
<contexts>
<context position="4111" citStr="Nielsen (2011)" startWordPosition="649" endWordPosition="650">utational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 328–332, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics 2 Related Work Previous approaches to sentiment analysis of microblogging texts make use of a wide range of features, including unigrams, n-grams, part-of-speech tags and polarity values from (usually hand-crafted) sentiment lexicons. O’Connor et al. (2010) examine tweets concerned with the 2009 US presidential elections, relying solely on the occurrence of words from a sentiment lexicon. Nielsen (2011) investigates the impact of including internet slang and obscene language when building a sentiment lexicon. Barbosa and Feng (2010) make use of three different sentiment detection websites to label Twitter data, while Davidov et al. (2010), Kouloumpis et al. (2011) and Pak and Paroubek (2010) use Twitter hashtags and emoticons as labels. Speriosu et al. (2011) propagate information from seed labels along a linked structure that includes Twitter’s follower graph, and Saif et al. (2012) specifically address the data-sparsity problem by using semantic smoothing and topic extraction. 3 System Des</context>
</contexts>
<marker>Nielsen, 2011</marker>
<rawString>Finn ˚Arup Nielsen. 2011. A new ANEW: Evaluation of a word list for sentiment analysis in microblogs. In Proceedings of the ESWC2011 Workshop on ‘Making Sense of Microposts’: Big things come in small packages, volume 718 of CEUR Workshop Proceedings, pages 93–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brendan O’Connor</author>
<author>Ramnath Balasubramanyan</author>
<author>Bryan R Routledge</author>
<author>Noah A Smith</author>
</authors>
<title>From Tweets to Polls: Linking Text Sentiment to Public Opinion Time Series.</title>
<date>2010</date>
<booktitle>In Fourth International AAAI Conference on Weblogs and Social Media, ICWSM.</booktitle>
<marker>O’Connor, Balasubramanyan, Routledge, Smith, 2010</marker>
<rawString>Brendan O’Connor, Ramnath Balasubramanyan, Bryan R. Routledge, and Noah A. Smith. 2010. From Tweets to Polls: Linking Text Sentiment to Public Opinion Time Series. In Fourth International AAAI Conference on Weblogs and Social Media, ICWSM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olutobi Owoputi</author>
<author>Brendan O’Connor</author>
<author>Chris Dyer</author>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Noah A Smith</author>
</authors>
<title>Improved part-of-speech tagging for online conversational text with word clusters.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL</booktitle>
<marker>Owoputi, O’Connor, Dyer, Gimpel, Schneider, Smith, 2013</marker>
<rawString>Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A Smith. 2013. Improved part-of-speech tagging for online conversational text with word clusters. In Proceedings of NAACL 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Pak</author>
<author>Patrick Paroubek</author>
</authors>
<title>Twitter as a Corpus for Sentiment Analysis and Opinion Mining.</title>
<date>2010</date>
<booktitle>In Proceedings of the 7th conference on International Language Resources and Evaluation,</booktitle>
<volume>volume</volume>
<pages>1320--1326</pages>
<contexts>
<context position="4405" citStr="Pak and Paroubek (2010)" startWordPosition="695" endWordPosition="698">ts make use of a wide range of features, including unigrams, n-grams, part-of-speech tags and polarity values from (usually hand-crafted) sentiment lexicons. O’Connor et al. (2010) examine tweets concerned with the 2009 US presidential elections, relying solely on the occurrence of words from a sentiment lexicon. Nielsen (2011) investigates the impact of including internet slang and obscene language when building a sentiment lexicon. Barbosa and Feng (2010) make use of three different sentiment detection websites to label Twitter data, while Davidov et al. (2010), Kouloumpis et al. (2011) and Pak and Paroubek (2010) use Twitter hashtags and emoticons as labels. Speriosu et al. (2011) propagate information from seed labels along a linked structure that includes Twitter’s follower graph, and Saif et al. (2012) specifically address the data-sparsity problem by using semantic smoothing and topic extraction. 3 System Description In this section we present the details of our sentiment analysis system, which was implemented in the Python programming language and is publicly available online.2 We used the same preprocessing, feature extraction and learning algorithm for both subtasks, only the hyperparameters of</context>
</contexts>
<marker>Pak, Paroubek, 2010</marker>
<rawString>Alexander Pak and Patrick Paroubek. 2010. Twitter as a Corpus for Sentiment Analysis and Opinion Mining. In Proceedings of the 7th conference on International Language Resources and Evaluation, volume 2010, pages 1320–1326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian Pedregosa</author>
<author>Alexandre Gramfort Ga¨el Varoquaux</author>
<author>Vincent Michel</author>
<author>Bertrand Thirion</author>
<author>Olivier Grisel</author>
<author>Mathieu Blondel</author>
<author>Peter Prettenhofer</author>
</authors>
<title>Scikit-learn: Machine learning in Python.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2825</pages>
<location>Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David</location>
<marker>Pedregosa, Ga¨el Varoquaux, Michel, Thirion, Grisel, Blondel, Prettenhofer, 2011</marker>
<rawString>Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and ´Edouard Duchesnay. 2011. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin F Porter</author>
</authors>
<title>An algorithm for suffix stripping. Program: electronic library and information systems,</title>
<date>1980</date>
<pages>14--3</pages>
<contexts>
<context position="6584" citStr="Porter, 1980" startWordPosition="1033" endWordPosition="1034">rmalized version, in which all characters are converted to lowercase and all digits to 0 c) A collapsed version, in which all adjacent duplicate characters are removed from the normalized version, if it is not present in an English word list. That way “school” stays “school”, but “liiike” gets converted to “like”. 3.2 Features We explored a wide variety of linguistic and lexical features. In our final submission we used the following set of features for each message: • The normalized tokens [boolean] • The stems of the collapsed tokens, which were computed using the Porter stemming algorithm (Porter, 1980) implemented in the Python Natural Language Toolkit (Bird et al., 2009). [boolean] • The word cluster IDs of raw, normalized and collapsed tokens. The clusters were obtained via unsupervised Brown clustering (Brown et al., 1992) of 56,345,753 Tweets by Owoputi et al. (2013) and are available on the web.3 [boolean] • The accumulated (summed) positive and accumulated negative SentiWordNet scores (Baccianella et al., 2010) of all synsets matching the collapsed token strings. [continuous] Furthermore, the normalized tokens and stems were marked with a special negation prefix, if they occurred afte</context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>Martin F Porter. 1980. An algorithm for suffix stripping. Program: electronic library and information systems, 14(3):130–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hassan Saif</author>
<author>Yulan He</author>
<author>Harith Alani</author>
</authors>
<title>Alleviating data sparsity for twitter sentiment analysis.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2nd Workshop on Making Sense of Microposts.</booktitle>
<contexts>
<context position="4601" citStr="Saif et al. (2012)" startWordPosition="727" endWordPosition="730">rned with the 2009 US presidential elections, relying solely on the occurrence of words from a sentiment lexicon. Nielsen (2011) investigates the impact of including internet slang and obscene language when building a sentiment lexicon. Barbosa and Feng (2010) make use of three different sentiment detection websites to label Twitter data, while Davidov et al. (2010), Kouloumpis et al. (2011) and Pak and Paroubek (2010) use Twitter hashtags and emoticons as labels. Speriosu et al. (2011) propagate information from seed labels along a linked structure that includes Twitter’s follower graph, and Saif et al. (2012) specifically address the data-sparsity problem by using semantic smoothing and topic extraction. 3 System Description In this section we present the details of our sentiment analysis system, which was implemented in the Python programming language and is publicly available online.2 We used the same preprocessing, feature extraction and learning algorithm for both subtasks, only the hyperparameters of the machine learning algorithm were adjusted to the respective dataset. 3.1 Preprocessing Tokenization of the messages was done using a simple regular expression, which matches either URLs, alpha</context>
</contexts>
<marker>Saif, He, Alani, 2012</marker>
<rawString>Hassan Saif, Yulan He, and Harith Alani. 2012. Alleviating data sparsity for twitter sentiment analysis. In Proceedings of the 2nd Workshop on Making Sense of Microposts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shai Shalev-Shwartz</author>
<author>Yoram Singer</author>
<author>Nathan Srebro</author>
</authors>
<title>Pegasos: Primal estimated sub-gradient solver for svm.</title>
<date>2007</date>
<booktitle>In Proceedings of the 24th international conference on Machine learning,</booktitle>
<pages>807--814</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="9839" citStr="Shalev-Shwartz et al. (2007)" startWordPosition="1602" endWordPosition="1605">egularization, which is a combination of L1 and L2 regularization. A simplified version of the SGD learning procedure implemented in scikit-learn is shown in Algorithm 1, where w is the weight vector of the model, x(i) the feature vector of sample i, y(i) ∈ {−1, +1} the ground truth label of sample i, η the learning rate, α the regularization factor and ρ the elastic net mixing parameter. Be aware that we did not pick samples at random or shuffle the data, which is crucial in case of training data which is sorted by classes. The initial learning rate is set heuristically and updated following Shalev-Shwartz et al. (2007).6 The way of applying the L1 penalty (lines 13 to 18) is published as “cumulative L1 penalty” in Tsuruoka et al. (2009). The final settings for the hyperparameters were determined by running a crossvalidated grid search on the combined training and development sets and can be found in Table 1. 4 Experiments For our experiments and the final model training we used the combined training and development set of the shared task. For Task A we removed messages labeled “objective” prior to training, while we merged them into the “neutral” class for Task B. This left us with 9419 training samples (58</context>
</contexts>
<marker>Shalev-Shwartz, Singer, Srebro, 2007</marker>
<rawString>Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro. 2007. Pegasos: Primal estimated sub-gradient solver for svm. In Proceedings of the 24th international conference on Machine learning, pages 807–814. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Speriosu</author>
<author>Nikita Sudan</author>
<author>Sid Upadhyay</author>
<author>Jason Baldridge</author>
</authors>
<title>Twitter Polarity Classification with Label Propagation over Lexical Links and the Follower Graph.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<pages>53--63</pages>
<contexts>
<context position="4474" citStr="Speriosu et al. (2011)" startWordPosition="707" endWordPosition="710">part-of-speech tags and polarity values from (usually hand-crafted) sentiment lexicons. O’Connor et al. (2010) examine tweets concerned with the 2009 US presidential elections, relying solely on the occurrence of words from a sentiment lexicon. Nielsen (2011) investigates the impact of including internet slang and obscene language when building a sentiment lexicon. Barbosa and Feng (2010) make use of three different sentiment detection websites to label Twitter data, while Davidov et al. (2010), Kouloumpis et al. (2011) and Pak and Paroubek (2010) use Twitter hashtags and emoticons as labels. Speriosu et al. (2011) propagate information from seed labels along a linked structure that includes Twitter’s follower graph, and Saif et al. (2012) specifically address the data-sparsity problem by using semantic smoothing and topic extraction. 3 System Description In this section we present the details of our sentiment analysis system, which was implemented in the Python programming language and is publicly available online.2 We used the same preprocessing, feature extraction and learning algorithm for both subtasks, only the hyperparameters of the machine learning algorithm were adjusted to the respective datas</context>
</contexts>
<marker>Speriosu, Sudan, Upadhyay, Baldridge, 2011</marker>
<rawString>Michael Speriosu, Nikita Sudan, Sid Upadhyay, and Jason Baldridge. 2011. Twitter Polarity Classification with Label Propagation over Lexical Links and the Follower Graph. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, page 53–63.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Yoshimasa Tsuruoka</author>
<author>Jun’ichi Tsujii</author>
<author>Sophia Ananiadou</author>
</authors>
<title>Stochastic gradient descent training for l1-regularized log-linear models with cumulative penalty.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume</booktitle>
<volume>1</volume>
<pages>477--485</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9959" citStr="Tsuruoka et al. (2009)" startWordPosition="1624" endWordPosition="1627">ted in scikit-learn is shown in Algorithm 1, where w is the weight vector of the model, x(i) the feature vector of sample i, y(i) ∈ {−1, +1} the ground truth label of sample i, η the learning rate, α the regularization factor and ρ the elastic net mixing parameter. Be aware that we did not pick samples at random or shuffle the data, which is crucial in case of training data which is sorted by classes. The initial learning rate is set heuristically and updated following Shalev-Shwartz et al. (2007).6 The way of applying the L1 penalty (lines 13 to 18) is published as “cumulative L1 penalty” in Tsuruoka et al. (2009). The final settings for the hyperparameters were determined by running a crossvalidated grid search on the combined training and development sets and can be found in Table 1. 4 Experiments For our experiments and the final model training we used the combined training and development set of the shared task. For Task A we removed messages labeled “objective” prior to training, while we merged them into the “neutral” class for Task B. This left us with 9419 training samples (5855 positive, 457 neutral, 3107 negative) for Task A and 10368 training samples (3855 positive, 4889 neutral, 1624 negati</context>
</contexts>
<marker>Tsuruoka, Tsujii, Ananiadou, 2009</marker>
<rawString>Yoshimasa Tsuruoka, Jun’ichi Tsujii, and Sophia Ananiadou. 2009. Stochastic gradient descent training for l1-regularized log-linear models with cumulative penalty. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1-Volume 1, pages 477– 485. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Zornitsa Kozareva</author>
<author>Preslav Nakov</author>
<author>Alan Ritter</author>
<author>Sara Rosenthal</author>
<author>Veselin Stoyanov</author>
</authors>
<title>Semeval-2013 task 2: Sentiment analysis in twitter.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th International Workshop on Semantic Evaluation. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2564" citStr="Wilson et al. (2013)" startWordPosition="404" endWordPosition="407">inion target” (Liu, 2012). With the massively growing importance of social media in everyday life, being able to automatically find and classify attitudes in written text allows for estimating the mood of a large group of people, e.g. towards a certain event, service, product, matter of fact or the like. As working with the very short and informal texts typical for social networks poses challenges not encountered in more traditional text genres, the International Workshop on Semantic Evaluation (SemEval) 2013 has a shared task on sentiment analysis in microblogging texts, which is detailed in Wilson et al. (2013). The task requires sentiment analysis of Twitter1 and SMS messages and comprises two subtasks, one of which deals with determining the sentiment of a given message fragment depending on its context (Task A) and one on overall message polarity classification (Task B). We treat both tasks as document-level sentiment classification tasks, which we define according to Liu (2012) as determining the opinion ( , GENERAL, s, , ) of a given message, where s E {positive, negative, neutral} and “the entity e, opinion holder h, and time of opinion t are assumed known or irrelevant” (Liu, 2012). For Task </context>
</contexts>
<marker>Wilson, Kozareva, Nakov, Ritter, Rosenthal, Stoyanov, 2013</marker>
<rawString>Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Alan Ritter, Sara Rosenthal, and Veselin Stoyanov. 2013. Semeval-2013 task 2: Sentiment analysis in twitter. In Proceedings of the 7th International Workshop on Semantic Evaluation. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>