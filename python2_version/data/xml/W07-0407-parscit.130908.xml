<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.976019">
Discriminative word alignment by learning the alignment structure
and syntactic divergence between a language pair
</title>
<author confidence="0.838589">
Sriram Venkatapathyl
</author>
<affiliation confidence="0.580503666666667">
Language Technologies Research
Centre, IIIT -Hyderabad
Hyderabad - 500019, India.
</affiliation>
<email confidence="0.990328">
sriram@research.iiit.ac.in
</email>
<author confidence="0.952671">
Aravind K. Joshi
</author>
<affiliation confidence="0.98938625">
Department of Computer and
Information Science and Institute for
Research in Cognitive Science,
University of Pennsylvania, PA, USA.
</affiliation>
<email confidence="0.998532">
joshi@linc.cis.upenn.edu
</email>
<sectionHeader confidence="0.99564" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999428074074074">
Discriminative approaches for word align-
ment have gained popularity in recent
years because of the flexibility that they
offer for using a large variety of features
and combining information from various
sources. But, the models proposed in the
past have not been able to make much use
of features that capture the likelihood of an
alignment structure (the set of alignment
links) and the syntactic divergence be-
tween sentences in the parallel text. This is
primarily because of the limitation of their
search techniques. In this paper, we pro-
pose a generic discriminative re-ranking
approach for word alignment which allows
us to make use of structural features effec-
tively. These features are particularly use-
ful for language pairs with high structural
divergence (like English-Hindi, English-
Japanese). We have shown that by us-
ing the structural features, we have ob-
tained a decrease of 2.3% in the absolute
value of alignment error rate (AER). When
we add the cooccurence probabilities ob-
tained from IBM model-4 to our features,
we achieved the best AER (50.50) for the
English-Hindi parallel corpus.
</bodyText>
<sectionHeader confidence="0.999337" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99982025">
In this paper, we propose a discriminative re-
ranking approach for word alignment which al-
lows us to make use of structural features effec-
tively. The alignment algorithm first generates
</bodyText>
<footnote confidence="0.46886725">
11Part of the work was done at Institute for Research
in Cognitive Science (IRCS), University of Pennsylvania,
Philadelphia, PA 19104, USA, when he was visiting IRCS
as a Visiting Scholar, February to December, 2006.
</footnote>
<page confidence="0.996138">
49
</page>
<bodyText confidence="0.999275875">
a list of k-best alignments using local features.
Then it re-ranks this list of k-best alignments us-
ing global features which consider the entire align-
ment structure (set of alignment links) and the syn-
tactic divergence that exists between the sentence
pair. Use of structural information associated with
the alignment can be particularly helpful for lan-
guage pairs for which a large amount of unsuper-
vised data is not available to measure accurately
the word cooccurence values but which do have a
small set of supervised data to learn the structure
and divergence across the language pair. We have
tested our model on the English-Hindi language
pair. Here is an example of an alignment between
English-Hindi which shows the complexity of the
alignment task for this language pair.
</bodyText>
<figureCaption confidence="0.6660315">
Figure 1: An example of an alignment between an
English and a Hindi sentence
</figureCaption>
<bodyText confidence="0.999895214285714">
To learn the weights associated with the param-
eters used in our model, we have used a learning
framework called MIRA (The Margin Infused Re-
laxed Algorithm) (McDonald et al., 2005; Cram-
mer and Singer, 2003). This is an online learning
algorithm which looks at one sentence pair at a
time and compares the k-best predictions of the
alignment algorithm with the gold alignment to
update the parameter weights appropriately.
In the past, popular approaches for doing word
alignment have largely been generative (Och and
Ney, 2003; Vogel et al., 1996). In the past cou-
ple of years, the discriminative models for doing
word alignment have gained popularity because of
</bodyText>
<note confidence="0.845237">
Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 49–56,
Rochester, New York, April 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999230675000001">
the flexibility they offer in using a large variety of
features and in combining information from vari-
ous sources.
(Taskar et al., 2005) cast the problem of align-
ment as a maximum weight bipartite matching
problem, where nodes correspond to the words
in the two sentences. The link between a pair
of words, (ep,hq) is associated with a score
(score(ep,hq)) reflecting the desirability of the ex-
istence of the link. The matching problem is
solved by formulating it as a linear programming
problem. The parameter estimation is done within
the framework of large margin estimation by re-
ducing the problem to a quadratic program (QP).
The main limitation of this work is that the fea-
tures considered are local to the alignment links
joining pairs of words. The score of an align-
ment is the sum of scores of individual alignment
links measured independently i.e., it is assumed
that there is no dependence between the align-
ment links. (Lacoste-Julien et al., 2006) extend
the above approach to include features for fertil-
ity and first-order correlation between alignment
links of consecutive words in the source sentence.
They solve this by formulating the problem as a
quadratic assignment problem (QAP). But, even
this algorithm cannot include more general fea-
tures over the entire alignment. In contrast to the
above two approaches, our approach does not im-
pose any constraints on the feature space except
for fertility (≤1) of words in the source language.
In our approach, we model the one-to-one and
many-to-one links between the source sentence
and target sentence. The many-to-many alignment
links are inferred in the post-processing stage us-
ing simple generic rules. Another positive aspect
of our approach is the application of MIRA. It, be-
ing an online approach, converges fast and still re-
tains the generalizing capability of the large mar-
gin approach.
(Moore, 2005) has proposed an approach which
does not impose any restrictions on the form of
model features. But, the search technique has cer-
tain heuristic procedures dependent on the types
of features used. For example, there is little vari-
ation in the alignment search between the LLR
(Log-likelihood ratio) based model and the CLP
(Conditional-Link Probability) based model. LLR
and CLP are the word association statistics used
in Moore’s work (Moore, 2005). In contrast to
the above approach, our search technique is more
general. It achieves this by breaking the search
into two steps, first by using local features to get
the k-best alignments and then by using struc-
tural features to re-rank the list. Also, by using
all the k-best alignments for updating the parame-
ters through MIRA, it is possible to model the en-
tire inference algorithm but in Moore’s work, only
the best alignment is used to update the weights
of parameters. (Fraser and Marcu, 2006) have
proposed an algorithm for doing word alignment
which applies a discriminative step at every iter-
ation of the traditional Expectation-Maximization
algorithm used in IBM models. This model still
relies on the generative story and achieves only a
limited freedom in choosing the features. (Blun-
som and Cohn, 2006) do word alignment by com-
bining features using conditional random fields.
Even though their approach allows one to include
overlapping features while training a discrimina-
tive model, it still does not allow us to use fea-
tures that capture information of the entire align-
ment structure.
In Section 2, we describe the alignment search
in detail. Section 3 describes the features that
we have considered in our paper. Section 4 talks
about the Parameter optimization. In Section 5,
we present the results of our experiments. Section
6 contains the conclusion and our proposed future
work.
</bodyText>
<sectionHeader confidence="0.976919" genericHeader="method">
2 Alignment Search
</sectionHeader>
<bodyText confidence="0.997746789473684">
The goal of the word alignment algorithm is to link
words in the source language with words in the tar-
get language to get the alignments structure. The
best alignment structure between a source sen-
tence and a target sentence can be predicted by
considering three kinds of information, (1) Prop-
erties of alignment links taken independently, (2)
Properties of the entire alignment structure taken
as a unit, and (3) The syntactic divergence between
the source sentence and the target sentence, given
the alignment structure. Using the set of alignment
links, the syntactic structure of the source sentence
is first projected onto the target language to ob-
serve the divergence.
Let ep and hq denote the source and target
words respectively. Let n be the number of words
in source sentence and m be the number of words
in target sentence. Let S be the source sentence
and T be the target sentence.
</bodyText>
<page confidence="0.972509">
50
</page>
<subsectionHeader confidence="0.995573">
2.1 Populate the Beam
</subsectionHeader>
<bodyText confidence="0.987635875">
The task in this step is to obtain the k-best candi-
date alignment structures using the local features.
The local features mainly contain the cooccurence
information between a source and a target word
and are independent of other alignment links in
the sentence pair. Let the local feature vector be
denoted as fL(ep, hq). The score of a particular
alignment link is computed by taking a dot prod-
uct of the weight vector W with the local feature
vector of the alignment link. More formally, the
local score of an alignment link is
scoreL(ep, hq) = W.fL(ep, hq)
The total score of an alignment structure is com-
puted by adding the scores of individual alignment
links present in the alignment. Hence, the score of
an alignment structure a is,
</bodyText>
<equation confidence="0.9734515">
�scoreLa(��, S, T) = scoreL(ep, hq)
(ep,hq)Ea
</equation>
<bodyText confidence="0.999673869565218">
We have proposed a dynamic programming al-
gorithm of worst case complexity O(nm2 + nk2)
to compute the k-best alignments. First, the local
score of each source word with every target word
is computed and stored in local beams associated
with the source words. The local beams corre-
sponding to all the source words are sorted and the
top-k alignment links in each beam are retained.
This operation has the worst-case complexity of
O(nm2).
Now, the goal is to get the k-best alignments in
the global beam. The global beam initially con-
tains no alignments. The k best alignment links of
the first source word e0 are added to the global
beam. To add the alignment links of the next
source word to the global beam, the k2 (if k &lt; m)
combinations of the alignments in the global beam
and alignments links in the local beam are taken
and the best k are retained in the global beam.
If k &gt; m, then the total combinations taken are
mk. This is repeated till the entries in all the lo-
cal beams are considered, the overall worst case
complexity being O(nk2) (or O(nmk) if k &gt; m).
</bodyText>
<subsectionHeader confidence="0.996071">
2.2 Reorder the beam
</subsectionHeader>
<bodyText confidence="0.998615666666666">
We now have the k-best alignments using the local
features from the last step. We then use global fea-
tures to reorder the beam. The global features look
at the properties of the entire alignment structure
instead of the alignment links locally.
Let the global feature vector be represented as
fG(a). The global score is defined as the dot prod-
uct of the weight vector and the global feature vec-
tor.
</bodyText>
<equation confidence="0.988182">
scoreG(a) = W.fG(a)
</equation>
<bodyText confidence="0.999709">
The overall score is calculated by adding the local
score and the global score.
</bodyText>
<equation confidence="0.997358">
score(a) = scoreLa(d) + scoreG(a)
</equation>
<bodyText confidence="0.99854025">
The beam is now sorted based on the overall scores
of each alignment. The alignment at the top of
the beam is the best possible alignment between
source sentence and the target sentence.
</bodyText>
<subsectionHeader confidence="0.999567">
2.3 Post-processing
</subsectionHeader>
<bodyText confidence="0.989384518518519">
The previous two steps produce alignment struc-
tures which contain one-to-one and many-to-one
links. In this step, the goal is to extend the best
alignment structure obtained in the previous step
to include the other alignments links of one-to-
many and many-to-many types.
The majority of the links between the source
sentence and the target sentence are one-to-one.
Some of the cases where this is not true are the in-
stances of idioms, alignment of verb groups where
auxiliaries do not correspond to each other, the
alignment of case-markers etc. Except for the
cases of idioms in target language, most of the
many-to-many links between a source and target
sentences can be inferred from the instances of
one-to-one and many-to-one links using three lan-
guage language specific rules (Hindi in our case)
to handle the above cases. Figure 1, Figure 2 and
Figure 3 depict the three such cases where many-
to-many alignments can be inferred. The align-
ments present at the left are those which can be
predicted by our alignment model. The alignments
on the right side are those which can be inferred in
the post-processing stage.
..... are playing ......
....... khel rahe hain ....... khel rahe hain
(play cont be)
</bodyText>
<figureCaption confidence="0.9348935">
Figure 2: Inferring the many-to-many alignments
of verb and auxiliaries
</figureCaption>
<bodyText confidence="0.914643">
After applying the language specific rules, the
dependency structure of the source sentence is tra-
versed to ensure the consistency of the alignment
are playing
</bodyText>
<page confidence="0.954263">
51
</page>
<bodyText confidence="0.9718895">
get word based on the co-occurrence information
obtained from a large sentence aligned corpora1.
</bodyText>
<figure confidence="0.99448925">
John ..........
John ..........
John ne ....
John ne ....
</figure>
<figureCaption confidence="0.926547">
Figure 3: Inferring the one-to-many alignment to
case-markers in Hindi
</figureCaption>
<figure confidence="0.703033666666667">
... kicked the bucket ... kicked the bucket
mara gaya .......... mara gaya
(die go−light verb)
</figure>
<figureCaption confidence="0.967551">
Figure 4: Inferring many-to-many alignment for
source idioms
</figureCaption>
<bodyText confidence="0.999921411764706">
structure. If there is a dependency link between
two source words eo and ep, where eo is the head
and ep is the modifier and if eo and ep are linked
to one or more common target word(s), it is log-
ical to imagine that the alignment should be ex-
tended such that both eo and ep are linked to the
same set of target words. For example, in Figure 4,
new alignment link is first formed between ‘kick’
and ‘gayA’ using the language specific rule, and
as ‘kick’ and ‘bucket’ are both linked to ‘mara’,
‘bucket’ is also now linked to ‘gayA’. Similarity,
‘the’ is linked to both ‘mara’ and ‘gayA’. Hence,
the rules are applied by traversing through the de-
pendency tree associated with the source sentence
words in depth-first order. The dependency parser
used by us was developed by (Shen, 2006). The
following summarizes this step,
</bodyText>
<listItem confidence="0.8346225">
• Let w be the next word considered in the dependency
tree, let pw be the parent of w.
</listItem>
<bodyText confidence="0.7826604">
– If w and pw are linked to one or more common
word(s) in target language, align w to all target
words which are aligned to pw.
– Else, Use the target-specific rules (if they match)
to extend the alignments of w.
</bodyText>
<listItem confidence="0.994958">
• Recursively consider all the children of w
</listItem>
<sectionHeader confidence="0.992854" genericHeader="method">
3 Parameters
</sectionHeader>
<bodyText confidence="0.9999235">
As the number of training examples is small, we
chose to use features (both local and structural)
which are generic. Some of the features which we
used in this experiment are as follows:
</bodyText>
<sectionHeader confidence="0.489242" genericHeader="method">
3.1.1 DiceWords
</sectionHeader>
<bodyText confidence="0.9926335">
Dice Coefficient of the source word and the tar-
get word (Taskar et al., 2005).
</bodyText>
<equation confidence="0.999503333333333">
2 ∗ Count(ep, hq)
DCoeff(ep, hq) =
Count(ep) + Count(hq)
</equation>
<bodyText confidence="0.999866">
where Count(ep, hq) is the number of times the
word hq was present in the translation of sentences
containing the word ep in the parallel corpus.
</bodyText>
<subsectionHeader confidence="0.477496">
3.1.2 DiceRoots
</subsectionHeader>
<bodyText confidence="0.999701">
Dice Coefficient of the lemmatized forms of the
source and target words. It is important to consider
this feature for language pairs which do not have a
large unsupervised sentence aligned corpora. Co-
occurrence information can be learnt better after
we lemmatize the words.
</bodyText>
<subsectionHeader confidence="0.529248">
3.1.3 Dict
</subsectionHeader>
<bodyText confidence="0.9894564">
This feature tests whether there exists a dictio-
nary entry from the source word ep to the target
word hq. For English-Hindi, we used a medium-
coverage dictionary (25000 words) available from
IIIT - Hyderabad, India 2.
</bodyText>
<subsectionHeader confidence="0.42996">
3.1.4 Null POS
</subsectionHeader>
<bodyText confidence="0.999969444444444">
These parameters measures the likelihood of a
source word with a particular part of speech tag3 to
be aligned to no word (Null) on the target language
side. This feature was extremely useful because
it models the cooccurence information of words
with nulls which is not captured by the features
DiceWords and DiceRoots. Here are some of the
features of this type with extreme estimated pa-
rameter weights.
</bodyText>
<subsectionHeader confidence="0.997347">
3.2 Lemmatized word pairs
</subsectionHeader>
<bodyText confidence="0.999754714285714">
The word pairs themselves are a good indicator
of whether an alignment link exists between the
word pair or not. Also, taking word-pairs as fea-
ture helps in the alignment of some of the most
common words in both the languages. A variation
of this feature was used by (Moore, 2005) in his
paper.
</bodyText>
<subsectionHeader confidence="0.99316">
3.1 Local features (FL)
</subsectionHeader>
<bodyText confidence="0.999009">
The local features which we consider are mainly
co-occurrence features. These features estimate
the likelihood of a source word aligning to a tar-
</bodyText>
<footnote confidence="0.864738166666667">
150K sentence pairs originally collected as part of TIDES
MT project and later refined at IIIT-Hyderabad, India.
2http://ltrc.iiit.ac.in/onlineServices/Dictionaries/Dict Frame.html
3We have limited the number of POS tags by considering
only the first alphabets of Penn Tags as our POS tag cate-
gories
</footnote>
<page confidence="0.99256">
52
</page>
<table confidence="0.998904">
Param. weight Param. weight
Null ‘ 0.2737 null C -0.7030
Null U 0.1969 null D -0.6914
Null L 0.1814 null V -0.6360
Null . 0.0383 null N -0.5600
Null: 0.0055 null I -0.4839
</table>
<tableCaption confidence="0.7943065">
Table 1: Top Five Features each with Maximum
and Minimum weights
</tableCaption>
<figureCaption confidence="0.9798265">
Figure 6: Alignment where the source words are
aligned to many different target words
</figureCaption>
<equation confidence="0.825608333333333">
Formally, it is defined as
� hqET�Fert(hq)&gt;1 Fert2(hq)
Overlap(��) =
</equation>
<bodyText confidence="0.999749875">
Other parameters like the relative distance be-
tween the source word ep and the target word hq,
RelDist(ep� hq) = abs(j/|e |− k/|h|), which are
mentioned as important features in the previous
literature, did not perform well for the English-
Hindi language pair. This is because of the pre-
dominant word-order variation between the sen-
tences of English and Hindi (Refer Figure 1).
</bodyText>
<subsectionHeader confidence="0.994522">
3.3 Structural Features (FG)
</subsectionHeader>
<bodyText confidence="0.999914545454546">
The global features are used to model the prop-
erties of the entire alignment structure taken as a
unit, between the source and the target sentence.
In doing so, we have attempted to exploit the syn-
tactic information available on both the source and
the target sides of the corpus. The syntactic infor-
mation on the target side is obtained by projecting
the syntactic information of the source using the
alignment links. Some of the features which we
have used in our work are in the following subsec-
tion.
</bodyText>
<subsectionHeader confidence="0.528038">
3.3.1 Overlap
</subsectionHeader>
<bodyText confidence="0.998736181818182">
This feature considers the instances in a sen-
tence pair where a source word links to a target
word which is a participant in more than one align-
ment links (has a fertility greater than one). This
feature is used to encourage the source words to
be linked to different words in the target language.
For example, we would prefer the alignment in
Figure 6 when compared to the alignment in Fig-
ure 5 even before looking at the actual words. This
parameter captures such prior information about
the alignment structure.
</bodyText>
<figureCaption confidence="0.98689">
Figure 5: Alignment where many source words are
linked to one target word
</figureCaption>
<equation confidence="0.497586">
EhET Fert(h)
</equation>
<bodyText confidence="0.999845428571429">
where T is the Hindi sentence. EFert2(hq) is
measured in the numerator so that a more uniform
distribution of target word fertilities be favored in
comparison to others. The weight of overlap as
estimated by our model is -6.1306 which indicates
the alignments having a low overlap value are pre-
ferred.
</bodyText>
<subsectionHeader confidence="0.591271">
3.3.2 NullPercent
</subsectionHeader>
<bodyText confidence="0.99972875">
This feature measures the percentage of words
in target language sentence which are not aligned
to any word in the source language sentence. It is
defined as
</bodyText>
<equation confidence="0.954586666666667">
|hq|hqET�Fertility(hq)==0
NullPercent =
|h|hET
</equation>
<subsectionHeader confidence="0.767591">
3.3.3 Direction DepPair
</subsectionHeader>
<bodyText confidence="0.999925384615385">
The following feature attempts to capture the
first order interdependence between the alignment
links of pairs of source sentence words which are
connected by dependency relations. One way in
which such an interdependence can be measured
is by noting the order of the target sentence words
linked to the child and parent of a source sentence
dependency relation. Figures 7, 8 and 9 depict
the various possibilities. The words in the source
sentence are represented using their part-of-speech
tags. These part-of-speech tags are also projected
onto the target words. In the figures p is the parent
and c is the part-of-speech of the child.
</bodyText>
<figure confidence="0.62116">
c p
</figure>
<figureCaption confidence="0.9925625">
Figure 7: Target word linked to a child precedes
the target word linked to a parent
</figureCaption>
<figure confidence="0.87555925">
p c
53
p c
p c
</figure>
<figureCaption confidence="0.941186">
Figure 8: Target word linked to a parent precedes
the target word linked to a child
</figureCaption>
<figure confidence="0.781559">
p
</figure>
<figureCaption confidence="0.9491545">
Figure 9: Parent and the child are both linked to
same target word
</figureCaption>
<bodyText confidence="0.988934652173913">
The situation in Figure 9 is an indicator that the
parent and child dependency pair might be part or
whole of a multi-word expression on the source
side. This feature thus captures the divergence be-
tween the source sentence dependency structure
and the target language dependency structure (in-
duced by taking the alignment as a constraint).
Hence, in the test data, the alignments which do
not express this divergence between the depen-
dency trees are penalized. For example, the align-
ment in Figure 10 will be heavily penalized by
the model during re-ranking step primarily for two
reasons, 1) The word aligned to the preposition
‘of’ does not precede the word aligned to the noun
‘king’ and 2) The word aligned to the preposition
‘to’ does not succeed the word aligned to the noun
‘king’.
......... to the king of Rajastan .......
...... Rajastan ke Raja ko ..........
( Rajastan of King to )
Figure 10: A simple example of an alignment
that would be penalized by the feature Direc-
tion DepPair
</bodyText>
<subsectionHeader confidence="0.888174">
3.3.4 Direction Bigram
</subsectionHeader>
<bodyText confidence="0.999969923076923">
This feature is a variation of the previous fea-
ture. In the previous feature, the dependency pair
on the source side was projected to the target side
to observe the divergence of the dependency pair.
In this feature, we take a bigram instead of a de-
pendency pair and observe its order in the target
side. This feature is equivalent to the first-order
features used in the related work.
There are three possibilities here, (1) The words
of the bigram maintain their order when projected
onto the target words, (2) The words of the bigram
are reversed when projected, (3) Both the words
are linked to the same word of the target sentence.
</bodyText>
<sectionHeader confidence="0.996324" genericHeader="method">
4 Online large margin training
</sectionHeader>
<bodyText confidence="0.997870263157895">
For parameter optimization, we have used an on-
line large margin algorithm called MIRA (Mc-
Donald et al., 2005) (Crammer and Singer, 2003).
We will briefly describe the training algorithm that
we have used. Our training set is a set of English-
Hindi word aligned parallel corpus. Let the num-
ber of sentence pairs in the training data be t. We
have {Sr, Tr, ar} for training where r ≤ t is the
index number of the sentence pair {Sr, Tr} in the
training set and ar is the gold alignment for the
pair {Sr, Tr}. Let W be the weight vector which
has to be learnt, Wi be the weight vector after the
end of ith update. To avoid over-fitting, W is ob-
tained by averaging over all the weight vectors Wi.
A generic large margin algorithm is defined
follows for the training instances {Sr, Tr, ar},
Initialize W0, W, i
for p = 1 to Iterations do
for r = 1 to t do
</bodyText>
<subsectionHeader confidence="0.35651">
Get K-Best predictions αr = {a1, a2...ak}
</subsectionHeader>
<bodyText confidence="0.93669925">
for the training example (Sr, Tr, ar)
using the current model Wi and applying
step 1 and 2 of section 4. Compute Wi+1
by updating Wi based on
</bodyText>
<equation confidence="0.9858688">
(Sr, Tr, &amp;r, αr).
i = i + 1
W = W + W i+1
W
W = Iterations*m
</equation>
<listItem confidence="0.373563">
end for
end for
</listItem>
<bodyText confidence="0.998345111111111">
The goal of MIRA is to minimize the change in
Wi such that the score of the gold alignment a� ex-
ceeds the score of each of the predictions in α by a
margin which is equal to the number of mistakes in
the predictions when compared to the gold align-
ment. One could choose a different loss function
which assigns greater penalty for certain kinds of
mistakes when compared to others.
Step 4 (Get K-Best predictions) in the algo-
</bodyText>
<figure confidence="0.70186">
p c
c
</figure>
<page confidence="0.992387">
54
</page>
<bodyText confidence="0.9910915">
rithm mentioned above can be substituted by the
following optimization problem,
</bodyText>
<construct confidence="0.433830666666667">
minimize JJ(W&apos;+&apos; − W&apos;)JJ
s.t. Vk, score(hT, ST, TT) − score(aq,k, ST, TT)
&gt;= Mistakes(ak, aT, ST, TT)
</construct>
<bodyText confidence="0.999837235294118">
For optimization of the parameters, ideally, we
need to consider all the possible predictions and
assign margin constraints based on every predic-
tion. But, here the number of such classes is ex-
ponential and therefore we restrict ourselves to the
k − best predictions.
We estimate the parameters in two steps. In the
first step, we estimate only the weights of the lo-
cal parameters. After that, we keep the weights
of local parameters constant and then estimate the
weights of global parameters. It is important to
decouple the parameter estimation to two steps.
We also experimented estimating the parameters
in one stage but as expected, it had an adverse
impact on the parameter weights of local features
which resulted in generation of poor k-best list af-
ter the first step while testing.
</bodyText>
<sectionHeader confidence="0.998489" genericHeader="evaluation">
5 Experiments and Results
</sectionHeader>
<subsectionHeader confidence="0.810256">
5.1 Data
</subsectionHeader>
<bodyText confidence="0.99998685">
We have used English-Hindi unsupervised data of
50000 sentence pairs4. This data was used to ob-
tain the cooccurence statistics such as DiceWords
and DiceRoots which we used in our model. This
data was also used to obtain the predictions of
GIZA++ (Implements the IBM models and the
HMM model). We take the alignments of GIZA++
as baseline and evaluate our model for the English-
Hindi language pair.
The supervised training data which is used to
estimate the parameters consists of 4252 sentence
pairs. The development data consists of 100 sen-
tence pairs and the test data consists of 100 sen-
tence pairs. This supervised data was obtained
from IRCS, University of Pennsylvania. For train-
ing our model, we need to convert the many-to-
many alignments in the corpus to one-to-one or
may-to-one alignments. This is done by applying
inverse operations of those performed during the
post-processing step (section 2.3).
</bodyText>
<footnote confidence="0.9451585">
4Originally collected as part of TIDES MT project and
later refined at IIIT-Hyderabad, India.
</footnote>
<subsectionHeader confidence="0.919559">
5.2 Experiments
</subsectionHeader>
<bodyText confidence="0.999665571428572">
We first obtain the predictions of GIZA++ to ob-
tain the baseline accuracies. GIZA++ was run in
four different modes 1) English to Hindi, 2) Hindi
to English, 3) English to Hindi where the words in
both the languages are lemmatized and 4) Hindi to
English where the words are lemmatized. We then
take the intersections of the predictions run from
both the directions (English to Hindi and Hindi to
English). Table 2 contains the results of experi-
ments with GIZA++. As the recall of the align-
ment links of the intersection is very low for this
dataset, further refinements of the alignments as
suggested by (Och and Ney, 2003) were not per-
formed.
</bodyText>
<table confidence="0.999685285714286">
Mode Prec. Rec. F-meas. AER
Normal: Eng-Hin 47.57 40.87 43.96 56.04
Normal: Hin-Eng 47.97 38.50 42.72 57.28
Normal: Inter. 88.71 27.52 42.01 57.99
Lemma.: Eng-Hin 53.60 44.58 48.67 51.33
Lemma.: Hin-Eng 53.83 42.68 47.61 52.39
Lemma.: Inter. 86.14 32.80 47.51 52.49
</table>
<tableCaption confidence="0.995676">
Table 2: GIZA++ Results
</tableCaption>
<bodyText confidence="0.999893125">
In Table 3, we observe that the best result
(51.33) is obtained when GIZA++ is run after lem-
matizing the words on the both sides of the unsu-
pervised corpus. The best results obtained without
lemmatizing is 56.04 when GIZA++ is run from
English to Hindi.
The table 4 summarizes the results when we
used only the local features in our model.
</bodyText>
<table confidence="0.999582166666667">
Features Prec. Rec. F-meas. AER
DiceRoots 41.49 38.71 40.05 59.95
+ DiceWords
+ Null POS 42.82 38.29 40.43 59.57
+ Dict. 43.94 39.30 41.49 58.51
+ Word pairs 46.27 41.07 43.52 56.48
</table>
<tableCaption confidence="0.99992">
Table 3: Results using local features
</tableCaption>
<bodyText confidence="0.999972125">
We now add the global features. While esti-
mating the parameter weights associated with the
global features, we keep the weights of local fea-
tures constant. We choose the appropriate beam
size as 50 after testing with several values on the
development set. We observed that the beam sizes
(between 10 and 100) did not affect the alignment
error rates very much.
</bodyText>
<page confidence="0.993669">
55
</page>
<table confidence="0.9998795">
Features Prec. Rec. F-meas. AER
Local feats. 46.27 41.07 43.52 56.48
Local feats. 48.17 42.76 45.30 54.70
+ Overlap
Local feats. 47.93 42.55 45.08 54.92
+ Direc. Deppair
Local feats. 48.31 42.89 45.44 54.56
+ Direc. Bigram
Local feats. 48.81 43.31 45.90 54.10
+ All Global feats.
</table>
<tableCaption confidence="0.999844">
Table 4: Results after adding global features
</tableCaption>
<bodyText confidence="0.999792785714286">
We see that by adding global features, we ob-
tained an absolute increase of about 2.3 AER sug-
gesting the usefulness of structural features which
we considered. Also, the new AER is much better
than that obtained by GIZA++ run without lem-
matizing the words.
We now add the IBM Model-4 parameters (co-
occurrence probabilities between source and tar-
get words) obtained using GIZA++ and our fea-
tures, and observe the results (Table 6). We can
see that structural features resulted in a significant
decrease in AER. Also, the AER that we obtained
is slightly better than the best AER obtained by the
GIZA++ models.
</bodyText>
<table confidence="0.997779">
Features Prec. Rec. F-meas. AER
IBMModel-4 Pars. 48.85 43.98 46.29 52.71
+ LocalFeats
IBMModel-4 Pars. 48.95 50.06 49.50 50.50
+ All feats.
</table>
<tableCaption confidence="0.8451815">
Table 5: Results after combining IBM model-4 pa-
rameters with our features
</tableCaption>
<sectionHeader confidence="0.996443" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999972294117647">
In this paper, we have proposed a discriminative
re-ranking approach for word alignment which al-
lows us to make use of structural features effec-
tively. We have shown that by using the structural
features, we have obtained a decrease of 2.3% in
the absolute value of alignment error rate (AER).
When we combine the prediction of IBM model-4
with our features, we have achieved an AER which
is slightly better than the best AER of GIZA++
for the English-Hindi parallel corpus (a language
pair with significant structural divergences). We
expect to get large improvements when we add
more number of relevant local and structural fea-
tures. We also plan to design an appropriate de-
pendency based decoder for machine translation
to make good use of the parameters estimated by
our model.
</bodyText>
<sectionHeader confidence="0.996406" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9998521875">
Phil Blunsom and Trevor Cohn. 2006. Discriminative
word alignment with conditional random fields. In
Proceedings of the 21st COLING and 44th Annual
Meeting of the ACL, Sydney, Australia, July. ACL.
Koby Crammer and Yoram Singer. 2003. Ultraconser-
vative online algorithms for multiclass problems. In
Journal ofMachine Learning Research.
Alexander Fraser and Daniel Marcu. 2006. Semi-
supervised training for statistical word alignment. In
Proceedings of the 21st COLING and 44th Annual
Meeting of the ACL, Sydney, Australia, July. Asso-
ciation for Computational Linguistics.
Simon Lacoste-Julien, Ben Taskar, Dan Klein, and
Michael I. Jordan. 2006. Word alignment via
quadratic assignment. In Proceedings of the Human
Language Technology Conference of the NAACL,
Main Conference, pages 112–119, New York City,
USA, June. Association for Computational Linguis-
tics.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-project dependency pars-
ing using spanning tree algorithms. In Proceed-
ings of Human Language Technology Conference
and Conference on Empirical Methods in Natural
Language Processing, pages 523–530, Vancouver,
British Columbia, Canada, October. Association of
Computational Linguistics.
Robert C. Moore. 2005. A discriminative frame-
work for bilingual word alignment. In Proceedings
of Human Language Technology Conference and
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 81–88, Vancouver, British
Columbia, Canada, October. Association of Compu-
tational Linguistics.
F. Och and H. Ney. 2003. A systematic comparisoin
of various statistical alignment models. In Compu-
tational Linguistics.
Libin Shen. 2006. Statistical LTAG Parsing. Ph.D.
thesis.
Ben Taskar, Simon Lacoste-Julien, and Dan Klein.
2005. A discriminative machine approach to word
alignment. In Proceedings of HLT-EMNLP, pages
73–80, Vancouver, British Columbia, Canada, Octo-
ber. Association of Computational Linguistics.
Stefan Vogel, Hermann Ney, and Christoph Tillmann.
1996. Hmm-based word alignment in statistical
translation. In Proceedings of the 16th International
Conference on Computational Linguistics.
</reference>
<page confidence="0.998424">
56
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.245287">
<title confidence="0.706515666666667">Discriminative word alignment by learning the alignment structure and syntactic divergence between a language pair Language Technologies</title>
<address confidence="0.732239">Centre, IIIT Hyderabad - 500019,</address>
<email confidence="0.93981">sriram@research.iiit.ac.in</email>
<author confidence="0.951084">K Aravind</author>
<affiliation confidence="0.96103575">Department of Computer Information Science and Institute Research in Cognitive University of Pennsylvania, PA,</affiliation>
<email confidence="0.999634">joshi@linc.cis.upenn.edu</email>
<abstract confidence="0.997590071428571">Discriminative approaches for word alignment have gained popularity in recent years because of the flexibility that they offer for using a large variety of features and combining information from various sources. But, the models proposed in the past have not been able to make much use of features that capture the likelihood of an alignment structure (the set of alignment links) and the syntactic divergence between sentences in the parallel text. This is primarily because of the limitation of their search techniques. In this paper, we propose a generic discriminative re-ranking approach for word alignment which allows us to make use of structural features effectively. These features are particularly useful for language pairs with high structural divergence (like English-Hindi, English- Japanese). We have shown that by using the structural features, we have obtained a decrease of 2.3% in the absolute value of alignment error rate (AER). When we add the cooccurence probabilities obtained from IBM model-4 to our features, we achieved the best AER (50.50) for the English-Hindi parallel corpus.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
</authors>
<title>Discriminative word alignment with conditional random fields.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st COLING and 44th Annual Meeting of the ACL,</booktitle>
<publisher>ACL.</publisher>
<location>Sydney, Australia,</location>
<contexts>
<context position="6812" citStr="Blunsom and Cohn, 2006" startWordPosition="1084" endWordPosition="1088">then by using structural features to re-rank the list. Also, by using all the k-best alignments for updating the parameters through MIRA, it is possible to model the entire inference algorithm but in Moore’s work, only the best alignment is used to update the weights of parameters. (Fraser and Marcu, 2006) have proposed an algorithm for doing word alignment which applies a discriminative step at every iteration of the traditional Expectation-Maximization algorithm used in IBM models. This model still relies on the generative story and achieves only a limited freedom in choosing the features. (Blunsom and Cohn, 2006) do word alignment by combining features using conditional random fields. Even though their approach allows one to include overlapping features while training a discriminative model, it still does not allow us to use features that capture information of the entire alignment structure. In Section 2, we describe the alignment search in detail. Section 3 describes the features that we have considered in our paper. Section 4 talks about the Parameter optimization. In Section 5, we present the results of our experiments. Section 6 contains the conclusion and our proposed future work. 2 Alignment Se</context>
</contexts>
<marker>Blunsom, Cohn, 2006</marker>
<rawString>Phil Blunsom and Trevor Cohn. 2006. Discriminative word alignment with conditional random fields. In Proceedings of the 21st COLING and 44th Annual Meeting of the ACL, Sydney, Australia, July. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Yoram Singer</author>
</authors>
<title>Ultraconservative online algorithms for multiclass problems.</title>
<date>2003</date>
<journal>In Journal ofMachine Learning Research.</journal>
<contexts>
<context position="3018" citStr="Crammer and Singer, 2003" startWordPosition="469" endWordPosition="473">accurately the word cooccurence values but which do have a small set of supervised data to learn the structure and divergence across the language pair. We have tested our model on the English-Hindi language pair. Here is an example of an alignment between English-Hindi which shows the complexity of the alignment task for this language pair. Figure 1: An example of an alignment between an English and a Hindi sentence To learn the weights associated with the parameters used in our model, we have used a learning framework called MIRA (The Margin Infused Relaxed Algorithm) (McDonald et al., 2005; Crammer and Singer, 2003). This is an online learning algorithm which looks at one sentence pair at a time and compares the k-best predictions of the alignment algorithm with the gold alignment to update the parameter weights appropriately. In the past, popular approaches for doing word alignment have largely been generative (Och and Ney, 2003; Vogel et al., 1996). In the past couple of years, the discriminative models for doing word alignment have gained popularity because of Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 49–56, Rochester, New York, April</context>
<context position="21357" citStr="Crammer and Singer, 2003" startWordPosition="3588" endWordPosition="3591">the dependency pair. In this feature, we take a bigram instead of a dependency pair and observe its order in the target side. This feature is equivalent to the first-order features used in the related work. There are three possibilities here, (1) The words of the bigram maintain their order when projected onto the target words, (2) The words of the bigram are reversed when projected, (3) Both the words are linked to the same word of the target sentence. 4 Online large margin training For parameter optimization, we have used an online large margin algorithm called MIRA (McDonald et al., 2005) (Crammer and Singer, 2003). We will briefly describe the training algorithm that we have used. Our training set is a set of EnglishHindi word aligned parallel corpus. Let the number of sentence pairs in the training data be t. We have {Sr, Tr, ar} for training where r ≤ t is the index number of the sentence pair {Sr, Tr} in the training set and ar is the gold alignment for the pair {Sr, Tr}. Let W be the weight vector which has to be learnt, Wi be the weight vector after the end of ith update. To avoid over-fitting, W is obtained by averaging over all the weight vectors Wi. A generic large margin algorithm is defined f</context>
</contexts>
<marker>Crammer, Singer, 2003</marker>
<rawString>Koby Crammer and Yoram Singer. 2003. Ultraconservative online algorithms for multiclass problems. In Journal ofMachine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Fraser</author>
<author>Daniel Marcu</author>
</authors>
<title>Semisupervised training for statistical word alignment.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st COLING and 44th Annual Meeting of the ACL,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="6496" citStr="Fraser and Marcu, 2006" startWordPosition="1036" endWordPosition="1039">ditional-Link Probability) based model. LLR and CLP are the word association statistics used in Moore’s work (Moore, 2005). In contrast to the above approach, our search technique is more general. It achieves this by breaking the search into two steps, first by using local features to get the k-best alignments and then by using structural features to re-rank the list. Also, by using all the k-best alignments for updating the parameters through MIRA, it is possible to model the entire inference algorithm but in Moore’s work, only the best alignment is used to update the weights of parameters. (Fraser and Marcu, 2006) have proposed an algorithm for doing word alignment which applies a discriminative step at every iteration of the traditional Expectation-Maximization algorithm used in IBM models. This model still relies on the generative story and achieves only a limited freedom in choosing the features. (Blunsom and Cohn, 2006) do word alignment by combining features using conditional random fields. Even though their approach allows one to include overlapping features while training a discriminative model, it still does not allow us to use features that capture information of the entire alignment structure</context>
</contexts>
<marker>Fraser, Marcu, 2006</marker>
<rawString>Alexander Fraser and Daniel Marcu. 2006. Semisupervised training for statistical word alignment. In Proceedings of the 21st COLING and 44th Annual Meeting of the ACL, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Lacoste-Julien</author>
<author>Ben Taskar</author>
<author>Dan Klein</author>
<author>Michael I Jordan</author>
</authors>
<title>Word alignment via quadratic assignment.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference,</booktitle>
<pages>112--119</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City, USA,</location>
<contexts>
<context position="4633" citStr="Lacoste-Julien et al., 2006" startWordPosition="733" endWordPosition="736">th a score (score(ep,hq)) reflecting the desirability of the existence of the link. The matching problem is solved by formulating it as a linear programming problem. The parameter estimation is done within the framework of large margin estimation by reducing the problem to a quadratic program (QP). The main limitation of this work is that the features considered are local to the alignment links joining pairs of words. The score of an alignment is the sum of scores of individual alignment links measured independently i.e., it is assumed that there is no dependence between the alignment links. (Lacoste-Julien et al., 2006) extend the above approach to include features for fertility and first-order correlation between alignment links of consecutive words in the source sentence. They solve this by formulating the problem as a quadratic assignment problem (QAP). But, even this algorithm cannot include more general features over the entire alignment. In contrast to the above two approaches, our approach does not impose any constraints on the feature space except for fertility (≤1) of words in the source language. In our approach, we model the one-to-one and many-to-one links between the source sentence and target s</context>
</contexts>
<marker>Lacoste-Julien, Taskar, Klein, Jordan, 2006</marker>
<rawString>Simon Lacoste-Julien, Ben Taskar, Dan Klein, and Michael I. Jordan. 2006. Word alignment via quadratic assignment. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 112–119, New York City, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
<author>Kiril Ribarov</author>
<author>Jan Hajic</author>
</authors>
<title>Non-project dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<journal>Association of Computational Linguistics.</journal>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>523--530</pages>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="2991" citStr="McDonald et al., 2005" startWordPosition="465" endWordPosition="468">t available to measure accurately the word cooccurence values but which do have a small set of supervised data to learn the structure and divergence across the language pair. We have tested our model on the English-Hindi language pair. Here is an example of an alignment between English-Hindi which shows the complexity of the alignment task for this language pair. Figure 1: An example of an alignment between an English and a Hindi sentence To learn the weights associated with the parameters used in our model, we have used a learning framework called MIRA (The Margin Infused Relaxed Algorithm) (McDonald et al., 2005; Crammer and Singer, 2003). This is an online learning algorithm which looks at one sentence pair at a time and compares the k-best predictions of the alignment algorithm with the gold alignment to update the parameter weights appropriately. In the past, popular approaches for doing word alignment have largely been generative (Och and Ney, 2003; Vogel et al., 1996). In the past couple of years, the discriminative models for doing word alignment have gained popularity because of Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 49–56,</context>
<context position="21330" citStr="McDonald et al., 2005" startWordPosition="3583" endWordPosition="3587">serve the divergence of the dependency pair. In this feature, we take a bigram instead of a dependency pair and observe its order in the target side. This feature is equivalent to the first-order features used in the related work. There are three possibilities here, (1) The words of the bigram maintain their order when projected onto the target words, (2) The words of the bigram are reversed when projected, (3) Both the words are linked to the same word of the target sentence. 4 Online large margin training For parameter optimization, we have used an online large margin algorithm called MIRA (McDonald et al., 2005) (Crammer and Singer, 2003). We will briefly describe the training algorithm that we have used. Our training set is a set of EnglishHindi word aligned parallel corpus. Let the number of sentence pairs in the training data be t. We have {Sr, Tr, ar} for training where r ≤ t is the index number of the sentence pair {Sr, Tr} in the training set and ar is the gold alignment for the pair {Sr, Tr}. Let W be the weight vector which has to be learnt, Wi be the weight vector after the end of ith update. To avoid over-fitting, W is obtained by averaging over all the weight vectors Wi. A generic large ma</context>
</contexts>
<marker>McDonald, Pereira, Ribarov, Hajic, 2005</marker>
<rawString>Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Hajic. 2005. Non-project dependency parsing using spanning tree algorithms. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 523–530, Vancouver, British Columbia, Canada, October. Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
</authors>
<title>A discriminative framework for bilingual word alignment.</title>
<date>2005</date>
<journal>Association of Computational Linguistics.</journal>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>81--88</pages>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="5547" citStr="Moore, 2005" startWordPosition="880" endWordPosition="881">s over the entire alignment. In contrast to the above two approaches, our approach does not impose any constraints on the feature space except for fertility (≤1) of words in the source language. In our approach, we model the one-to-one and many-to-one links between the source sentence and target sentence. The many-to-many alignment links are inferred in the post-processing stage using simple generic rules. Another positive aspect of our approach is the application of MIRA. It, being an online approach, converges fast and still retains the generalizing capability of the large margin approach. (Moore, 2005) has proposed an approach which does not impose any restrictions on the form of model features. But, the search technique has certain heuristic procedures dependent on the types of features used. For example, there is little variation in the alignment search between the LLR (Log-likelihood ratio) based model and the CLP (Conditional-Link Probability) based model. LLR and CLP are the word association statistics used in Moore’s work (Moore, 2005). In contrast to the above approach, our search technique is more general. It achieves this by breaking the search into two steps, first by using local </context>
<context position="15686" citStr="Moore, 2005" startWordPosition="2623" endWordPosition="2624">be aligned to no word (Null) on the target language side. This feature was extremely useful because it models the cooccurence information of words with nulls which is not captured by the features DiceWords and DiceRoots. Here are some of the features of this type with extreme estimated parameter weights. 3.2 Lemmatized word pairs The word pairs themselves are a good indicator of whether an alignment link exists between the word pair or not. Also, taking word-pairs as feature helps in the alignment of some of the most common words in both the languages. A variation of this feature was used by (Moore, 2005) in his paper. 3.1 Local features (FL) The local features which we consider are mainly co-occurrence features. These features estimate the likelihood of a source word aligning to a tar150K sentence pairs originally collected as part of TIDES MT project and later refined at IIIT-Hyderabad, India. 2http://ltrc.iiit.ac.in/onlineServices/Dictionaries/Dict Frame.html 3We have limited the number of POS tags by considering only the first alphabets of Penn Tags as our POS tag categories 52 Param. weight Param. weight Null ‘ 0.2737 null C -0.7030 Null U 0.1969 null D -0.6914 Null L 0.1814 null V -0.636</context>
</contexts>
<marker>Moore, 2005</marker>
<rawString>Robert C. Moore. 2005. A discriminative framework for bilingual word alignment. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 81–88, Vancouver, British Columbia, Canada, October. Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparisoin of various statistical alignment models.</title>
<date>2003</date>
<booktitle>In Computational Linguistics.</booktitle>
<contexts>
<context position="3338" citStr="Och and Ney, 2003" startWordPosition="521" endWordPosition="524">language pair. Figure 1: An example of an alignment between an English and a Hindi sentence To learn the weights associated with the parameters used in our model, we have used a learning framework called MIRA (The Margin Infused Relaxed Algorithm) (McDonald et al., 2005; Crammer and Singer, 2003). This is an online learning algorithm which looks at one sentence pair at a time and compares the k-best predictions of the alignment algorithm with the gold alignment to update the parameter weights appropriately. In the past, popular approaches for doing word alignment have largely been generative (Och and Ney, 2003; Vogel et al., 1996). In the past couple of years, the discriminative models for doing word alignment have gained popularity because of Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 49–56, Rochester, New York, April 2007. c�2007 Association for Computational Linguistics the flexibility they offer in using a large variety of features and in combining information from various sources. (Taskar et al., 2005) cast the problem of alignment as a maximum weight bipartite matching problem, where nodes correspond to the words in the two se</context>
<context position="25417" citStr="Och and Ney, 2003" startWordPosition="4317" endWordPosition="4320">st obtain the predictions of GIZA++ to obtain the baseline accuracies. GIZA++ was run in four different modes 1) English to Hindi, 2) Hindi to English, 3) English to Hindi where the words in both the languages are lemmatized and 4) Hindi to English where the words are lemmatized. We then take the intersections of the predictions run from both the directions (English to Hindi and Hindi to English). Table 2 contains the results of experiments with GIZA++. As the recall of the alignment links of the intersection is very low for this dataset, further refinements of the alignments as suggested by (Och and Ney, 2003) were not performed. Mode Prec. Rec. F-meas. AER Normal: Eng-Hin 47.57 40.87 43.96 56.04 Normal: Hin-Eng 47.97 38.50 42.72 57.28 Normal: Inter. 88.71 27.52 42.01 57.99 Lemma.: Eng-Hin 53.60 44.58 48.67 51.33 Lemma.: Hin-Eng 53.83 42.68 47.61 52.39 Lemma.: Inter. 86.14 32.80 47.51 52.49 Table 2: GIZA++ Results In Table 3, we observe that the best result (51.33) is obtained when GIZA++ is run after lemmatizing the words on the both sides of the unsupervised corpus. The best results obtained without lemmatizing is 56.04 when GIZA++ is run from English to Hindi. The table 4 summarizes the results </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F. Och and H. Ney. 2003. A systematic comparisoin of various statistical alignment models. In Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
</authors>
<date>2006</date>
<tech>Statistical LTAG Parsing. Ph.D. thesis.</tech>
<contexts>
<context position="13561" citStr="Shen, 2006" startWordPosition="2250" endWordPosition="2251">get word(s), it is logical to imagine that the alignment should be extended such that both eo and ep are linked to the same set of target words. For example, in Figure 4, new alignment link is first formed between ‘kick’ and ‘gayA’ using the language specific rule, and as ‘kick’ and ‘bucket’ are both linked to ‘mara’, ‘bucket’ is also now linked to ‘gayA’. Similarity, ‘the’ is linked to both ‘mara’ and ‘gayA’. Hence, the rules are applied by traversing through the dependency tree associated with the source sentence words in depth-first order. The dependency parser used by us was developed by (Shen, 2006). The following summarizes this step, • Let w be the next word considered in the dependency tree, let pw be the parent of w. – If w and pw are linked to one or more common word(s) in target language, align w to all target words which are aligned to pw. – Else, Use the target-specific rules (if they match) to extend the alignments of w. • Recursively consider all the children of w 3 Parameters As the number of training examples is small, we chose to use features (both local and structural) which are generic. Some of the features which we used in this experiment are as follows: 3.1.1 DiceWords D</context>
</contexts>
<marker>Shen, 2006</marker>
<rawString>Libin Shen. 2006. Statistical LTAG Parsing. Ph.D. thesis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Taskar</author>
<author>Simon Lacoste-Julien</author>
<author>Dan Klein</author>
</authors>
<title>A discriminative machine approach to word alignment.</title>
<date>2005</date>
<journal>Association of Computational Linguistics.</journal>
<booktitle>In Proceedings of HLT-EMNLP,</booktitle>
<pages>73--80</pages>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="3810" citStr="Taskar et al., 2005" startWordPosition="594" endWordPosition="597">update the parameter weights appropriately. In the past, popular approaches for doing word alignment have largely been generative (Och and Ney, 2003; Vogel et al., 1996). In the past couple of years, the discriminative models for doing word alignment have gained popularity because of Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 49–56, Rochester, New York, April 2007. c�2007 Association for Computational Linguistics the flexibility they offer in using a large variety of features and in combining information from various sources. (Taskar et al., 2005) cast the problem of alignment as a maximum weight bipartite matching problem, where nodes correspond to the words in the two sentences. The link between a pair of words, (ep,hq) is associated with a score (score(ep,hq)) reflecting the desirability of the existence of the link. The matching problem is solved by formulating it as a linear programming problem. The parameter estimation is done within the framework of large margin estimation by reducing the problem to a quadratic program (QP). The main limitation of this work is that the features considered are local to the alignment links joining</context>
<context position="14237" citStr="Taskar et al., 2005" startWordPosition="2374" endWordPosition="2377">t word considered in the dependency tree, let pw be the parent of w. – If w and pw are linked to one or more common word(s) in target language, align w to all target words which are aligned to pw. – Else, Use the target-specific rules (if they match) to extend the alignments of w. • Recursively consider all the children of w 3 Parameters As the number of training examples is small, we chose to use features (both local and structural) which are generic. Some of the features which we used in this experiment are as follows: 3.1.1 DiceWords Dice Coefficient of the source word and the target word (Taskar et al., 2005). 2 ∗ Count(ep, hq) DCoeff(ep, hq) = Count(ep) + Count(hq) where Count(ep, hq) is the number of times the word hq was present in the translation of sentences containing the word ep in the parallel corpus. 3.1.2 DiceRoots Dice Coefficient of the lemmatized forms of the source and target words. It is important to consider this feature for language pairs which do not have a large unsupervised sentence aligned corpora. Cooccurrence information can be learnt better after we lemmatize the words. 3.1.3 Dict This feature tests whether there exists a dictionary entry from the source word ep to the targ</context>
</contexts>
<marker>Taskar, Lacoste-Julien, Klein, 2005</marker>
<rawString>Ben Taskar, Simon Lacoste-Julien, and Dan Klein. 2005. A discriminative machine approach to word alignment. In Proceedings of HLT-EMNLP, pages 73–80, Vancouver, British Columbia, Canada, October. Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>Hmm-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th International Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="3359" citStr="Vogel et al., 1996" startWordPosition="525" endWordPosition="528">re 1: An example of an alignment between an English and a Hindi sentence To learn the weights associated with the parameters used in our model, we have used a learning framework called MIRA (The Margin Infused Relaxed Algorithm) (McDonald et al., 2005; Crammer and Singer, 2003). This is an online learning algorithm which looks at one sentence pair at a time and compares the k-best predictions of the alignment algorithm with the gold alignment to update the parameter weights appropriately. In the past, popular approaches for doing word alignment have largely been generative (Och and Ney, 2003; Vogel et al., 1996). In the past couple of years, the discriminative models for doing word alignment have gained popularity because of Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 49–56, Rochester, New York, April 2007. c�2007 Association for Computational Linguistics the flexibility they offer in using a large variety of features and in combining information from various sources. (Taskar et al., 2005) cast the problem of alignment as a maximum weight bipartite matching problem, where nodes correspond to the words in the two sentences. The link bet</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>Stefan Vogel, Hermann Ney, and Christoph Tillmann. 1996. Hmm-based word alignment in statistical translation. In Proceedings of the 16th International Conference on Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>