<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.62886">
CONTEXTUAL WORD SIMILARITY AND ESTIMATION
FROM SPARSE DATA
</title>
<author confidence="0.77182">
Ido Dagan
</author>
<affiliation confidence="0.710625">
AT&amp;T Bell Laboratories
</affiliation>
<address confidence="0.9229125">
600 Mountain Avenue
Murray Hill, NJ 07974
</address>
<email confidence="0.917246">
daganOresearch.att.com
</email>
<author confidence="0.933847">
Shaul Marcus
</author>
<affiliation confidence="0.937605">
Computer Science Department
</affiliation>
<address confidence="0.4937325">
Technion
Haifa 32000, Israel
</address>
<email confidence="0.751469">
shaulOcs.technion.ac.il
</email>
<author confidence="0.964566">
Shaul Markovitch
</author>
<affiliation confidence="0.963847">
Computer Science Department
</affiliation>
<address confidence="0.527893">
Technion
Haifa 32000, Israel
</address>
<email confidence="0.800957">
shaulmOcs.technion.ac.il
</email>
<sectionHeader confidence="0.988848" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999991357142857">
In recent years there is much interest in word
cooccurrence relations, such as n-grams, verb-
object combinations, or cooccurrence within
a limited context. This paper discusses how
to estimate the probability of cooccurrences
that do not occur in the training data. We
present a method that makes local analogies
between each specific unobserved cooccurrence
and other cooccurrences that contain simi-
lar words, as determined by an appropriate
word similarity metric. Our evaluation sug-
gests that this method performs better than
existing smoothing methods, and may provide
an alternative to class based models.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999968323529412">
Statistical data on word cooccurrence relations
play a major role in many corpus based approaches
for natural language processing. Different types
of cooccurrence relations are in use, such as cooc-
currence within a consecutive sequence of words
(n-grams), within syntactic relations (verb-object,
adjective-noun, etc.) or the cooccurrence of two
words within a limited distance in the context. Sta-
tistical data about these various cooccurrence rela-
tions is employed for a variety of applications, such
as speech recognition (Jelinek, 1990), language gen-
eration (Smadja and McKeown, 1990), lexicogra-
phy (Church and Hanks, 1990), machine transla-
tion (Brown et al., ; Sadler, 1989), information
retrieval (Maarek and Smadja, 1989) and various
disambiguation tasks (Dagan et al., 1991; Hindle
and Rooth, 1991; Grishman et al., 1986; Dagan and
Itai, 1990).
A major problem for the above applications is
how to estimate the probability of cooccurrences
that were not observed in the training corpus. Due
to data sparseness in unrestricted language, the ag-
gregate probability of such cooccurrences is large
and can easily get to 25% or more, even for a very
large training corpus (Church and Mercer, 1992).
Since applications often have to compare alterna-
tive hypothesized cooccurrences, it is important
to distinguish between those unobserved cooccur-
rences that are likely to occur in a new piece of text
and those that are not These distinctions ought to
be made using the data that do occur in the cor-
pus. Thus, beyond its own practical importance,
the sparse data problem provides an informative
touchstone for theories on generalization and anal-
ogy in linguistic data.
The literature suggests two major approaches
for solving the sparse data problem: smoothing
and class based methods. Smoothing methods es-
timate the probability of unobserved cooccurrences
using frequency information (Good, 1953; Katz,
1987; Jelinek and Mercer, 1985; Church and Gale,
1991). Church and Gale (Church and Gale, 1991)
show, that for unobserved bigrams, the estimates of
several smoothing methods closely agree with the
probability that is expected using the frequencies of
the two words and assuming that their occurrence
is independent ((Church and Gale, 1991), figure 5).
Furthermore, using held out data they show that
this is the probability that should be estimated by a
smoothing method that takes into account the fre-
quencies of the individual words. Relying on this
result, we will use frequency based estimation (using
word frequencies) as representative for smoothing
estimates of unobserved cooccurrences, for compar-
ison purposes. As will be shown later, the problem
with smoothing estimates is that they ignore the
expected degree of association between the specific
words of the cooccurrence. For example, we would
not like to estimate the same probability for two
cooccurrences like &apos;eat bread&apos; and &apos;eat cars&apos;, de-
spite the fact that both &apos;bread&apos; and &apos;cars&apos; may have
the same frequency.
Class based models (Brown et al., ; Pereira
et al., 1993; Hirschman, 1986; Resnik, 1992) dis-
tinguish between unobserved cooccurrences using
classes of &amp;quot;similar&amp;quot; words. The probability of a spe-
cific cooccurrence is determined using generalized
parameters about the probability of class cooccur-
</bodyText>
<page confidence="0.997681">
164
</page>
<bodyText confidence="0.999955761904762">
rence. This approach, which follows long traditions
in semantic classification, is very appealing, as it
attempts to capture &amp;quot;typical&amp;quot; properties of classes
of words. However, it is not clear at all that un-
restricted language is indeed structured the way it
is assumed by class based models. In particular,
it is not clear that word cooccurrence patterns can
be structured and generalized to class cooccurrence
parameters without losing too much information.
This paper suggests an alternative approach
which assumes that class based generalizations
should be avoided, and therefore eliminates the in-
termediate level of word classes. Like some of the
class based models, we use a similarity metric to
measure the similarity between cooccurrence pat-
terns of words. But then, rather than using this
metric to construct a set of word classes, we use
it to identify the most specific analogies that can
be drawn for each specific estimation. Thus, to
estimate the probability of an unobserved cooccur-
rence of words, we use data about other cooccur-
rences that were observed in the corpus, and con-
tain words that are similar to the given ones. For
example, to estimate the probability of the unob-
served cooccurrence &apos;negative results&apos;, we use cooc-
currences such as &apos;positive results&apos; and &apos;negative
numbers&apos;, that do occur in our corpus.
The analogies we make are based on the as-
sumption that similar word cooccurrences have
similar values of mutual information. Accordingly,
our similarity metric was developed to capture sim-
ilarities between vectors of mutual information val-
ues. In addition, we use an efficient search heuris-
tic to identify the most similar words for a given
word, thus making the method computationally
affordable. Figure 1 illustrates a portion of the
similarity network induced by the similarity metric
(only some of the edges, with relatively high val-
ues, are shown). This network may be found useful
for other purposes, independently of the estimation
method.
The estimation method was implemented using
the relation of cooccurrence of two words within
a limited distance in a sentence. The proposed
method, however, is general and is applicable for
any type of lexical cooccurrence. The method was
evaluated in two experiments. In the first one we
achieved a complete scenario of the use of the esti-
mation method, by implementing a variant of the
disambiguation method in (Dagan et al., 1991), for
sense selection in machine translation. The esti-
mation method was then successfully used to in-
crease the coverage of the disambiguation method
by 15%, with an increase of the overall precision
compared to a naive, frequency based, method. In
the second experiment we evaluated the estimation
method on a data recovery task. The task sim-
ulates a typical scenario in disambiguation, and
also relates to theoretical questions about redun-
dancy and idiosyncrasy in cooccurrence data. In
this evaluation, which involved 300 examples, the
performance of the estimation method was by 27%
better than frequency based estimation.
</bodyText>
<sectionHeader confidence="0.992271" genericHeader="introduction">
2 Definitions
</sectionHeader>
<bodyText confidence="0.998238947368421">
We use the term cooccurrence pair, written as
(x, y), to denote a cooccurrence of two words in a
sentence within a distance of no more than d words.
When computing the distance d, we ignore function
words such as prepositions and determiners. In the
experiments reported here d = 3.
A cooccurrence pair can be viewed as a gen-
eralization of a bigram, where a bigram is a cooc-
currence pair with d = 1 (without ignoring func-
tion words). As with bigrams, a cooccurrence pair
is directional, i.e. (x, y) (y, x). This captures
some information about the asymmetry in the lin-
ear order of linguistic relations, such as the fact
that verbs tend to precede their objects and follow
their subjects.
The mutual information of a cooccurrence pair,
which measures the degree of association between
the two words (Church and Hanks, 1990), is defined
as (Fano, 1961):
</bodyText>
<equation confidence="0.997865">
, y) = log2 p13(x(x)pl)y) -- log2 Pp(x(xIY)) (1)
P(Yix)
= log2 p(y)
</equation>
<bodyText confidence="0.999800833333333">
where P(z) and P(y) are the probabilities of the
events x and y (occurrences of words, in our case)
and P (x , y) is the probability of the joint event (a
cooccurrence pair).
We estimate mutual information values using
the Maximum Likelihood Estimator (MLE):
</bodyText>
<equation confidence="0.972123666666667">
P (x y) N f (x y)
i(x , y) = log2 P(x) &apos;P &apos;(y) = log2 ( f()f
)
</equation>
<bodyText confidence="0.9736285">
d 4 (Y)
(2)
where f denotes the frequency of an event and
N is the length of the corpus. While better es-
timates for small probabilities are available (Good,
1953; Church and Gale, 1991), MLE is the simplest
to implement and was adequate for the purpose of
this study. Due to the unreliability of measuring
negative mutual information values in corpora that
are not extremely large, we have considered in this
work any negative value to be 0. We also set i (x , y)
to 0 if f (x y) = 0. Thus, we assume in both cases
that the association between the two words is as
expected by chance.
</bodyText>
<page confidence="0.982338">
165
</page>
<figure confidence="0.987953214285714">
articles
conference,
0.13
workshop
0.102
0.11
0.106
symposium
0.132 paper
0.11
papers
0.1
book I documentation
0.137
</figure>
<figureCaption confidence="0.999924">
Figure 1: A portion of the similarity network.
</figureCaption>
<sectionHeader confidence="0.9699145" genericHeader="method">
3 Estimation for an Unobserved
Cooccurrence
</sectionHeader>
<bodyText confidence="0.999989965517241">
Assume that we have at our disposal a method for
determining similarity between cooccurrence pat-
terns of two words (as described in the next sec-
tion). We say that two cooccurrence pairs, (wi, w2)
and (01, w;), are similar if wc is similar to w1 and
74 is similar to w2. A special (and stronger) case
of similarity is when the two pairs differ only in
one of their words (e.g. (wi, w&apos;2) and (wi, wz)).
This special case is less susceptible to noise than
unrestricted similarity, as we replace only one of
the words in the pair. In our experiments, which
involved rather noisy data, we have used only this
restricted type of similarity. The mathematical for-
mulations, though, are presented in terms of the
general case.
The question that arises now is what analo-
gies can be drawn between two similar cooccur-
rence pairs, (wi, w2) and (w, w). Their proba-
bilities cannot be expected to be similar, since the
probabilities of the words in each pair can be dif-
ferent. However, since we assume that w1 and 01
have similar cooccurrence patterns, and so do w2
and 74, it is reasonable to assume that the mutual
information of the two pairs will be similar (recall
that mutual information measures the degree of as-
sociation between the words of the pair).
Consider for example the pair (chapter, de-
scribes), which does not occur in our corpusl . This
pair was found to be similar to the pairs (intro-
</bodyText>
<footnote confidence="0.830390333333333">
1We used a corpus of about 9 million words of texts
in the computer domain, taken from articles posted to
the USENET news system.
</footnote>
<bodyText confidence="0.98743284">
duction, describes), (book, describes) and (section,
describes), that do occur in the corpus. Since
these pairs occur in the corpus, we estimate their
mutual information values using equation 2, as
shown in Table 1. We then take the average of
these mutual information values as the similarity
based estimate for I(chapter, describes), denoted
as Achapter, describes)2 . This represents the as-
sumption that the word &apos;describes&apos; is associated
with the word &apos;chapter&apos; to a similar extent as it
is associated with the words &apos;introduction&apos;, &apos;book&apos;
and &apos;section&apos;. Table 2 demonstrates how the anal-
ogy is carried out also for a pair of unassociated
words, such as (chapter, knows).
In our current implementation, we compute
/(wi, w2) using up to 6 most similar words to each
of w1 and w2, and averaging the mutual informa-
tion values of similar pairs that occur in the corpus
(6 is a parameter, tuned for our corpus. In some
cases the similarity method identifies less than 6
similar words).
Having an estimate for the mutual information
of a pair, we can estimate its expected frequency
in a corpus of the given size using a variation of
equation 2:
</bodyText>
<equation confidence="0.984976">
f(wi,w2) = -`.-1,17f(w1)f(w2)21(w1,&amp;quot;)
</equation>
<bodyText confidence="0.999644666666667">
In our example, f (chapter) = 395, N = 8, 871, 126
and d = 3, getting a similarity based estimate of
I (chapter, describes) = 3.15. This value is much
</bodyText>
<footnote confidence="0.3572006">
2We use / for similarity based estimates, and reserve
I for the traditional maximum likelihood estimate. The
similarity based estimate will be used for cooccurrence
pairs that do not occur in the corpus.
(3)
</footnote>
<page confidence="0.978066">
166
</page>
<table confidence="0.9998324">
(w1, w2) /(W1 1D2) w2) f ( ) f (w 2)
(introduction, describes) 6.85 5 464 277
(book, describes) 6.27 13 1800 277
(section, describes) 6.12 6 923 277
Average: 6.41
</table>
<tableCaption confidence="0.999992">
Table 1: The similarity based estimate as an average on similar pairs: i(chapter, describes) = 6.41
Table 2: The similarity based estimate for a pair of unassociated words: *I (chapter, knows) = 0
</tableCaption>
<table confidence="0.9966768">
(wi w2) l(wi, w2) f (wi , w2) f ( ) f (w 2)
(introduction, knows) 464 928
(book, knows) 1800 928
(section, knows) 923 928
Average: 0
</table>
<bodyText confidence="0.999943222222222">
higher than the frequency based estimate (0.037),
reflecting the plausibility of the specific combina-
tion of words&apos;. On the other hand, the similar-
ity based estimate for f (chapter, knows) is 0.124,
which is identical to the frequency based estimate,
reflecting the fact that there is no expected associ-
ation between the two words (notice that the fre-
quency based estimate is higher for the second pair,
due to the higher frequency of `knows&apos;).
</bodyText>
<sectionHeader confidence="0.991431" genericHeader="method">
4 The Similarity Metric
</sectionHeader>
<bodyText confidence="0.984350333333334">
Assume that we need to determine the degree of
similarity between two words, wi and w2. Recall
that if we decide that the two words are similar,
then we may infer that they have similar mutual in-
formation with some other word, w. This inference
would be reasonable if we find that on average wi
and w2 indeed have similar mutual information val-
ues with other words in the lexicon. The similarity
metric therefore measures the degree of similarity
between these mutual information values.
We first define the similarity between the mu-
tual information values of w1 and w2 relative to a
single other word, w. Since cooccurrence pairs are
directional, we get two measures, defined by the po-
sition of w in the pair. The left context similarity of
w1 and w2 relative to w, termed simi, (wi, w2, w),
is defined as the ratio between the two mutual in-
formation values, having the larger value in the de-
nominator:
min(/(w, w1), /(w, w2))
max(/(w, w1), /(w, w2))
</bodyText>
<footnote confidence="0.485298166666667">
3The frequency based estimate for the expected fre-
quency of a cooccurrence pair, assuming independent
occurrence of the two words and using their individual
frequencies, is f (evi)f (w2). As mentioned earlier, we
use this estimate as representative for smoothing esti-
mates of unobserved cooccurrences.
</footnote>
<bodyText confidence="0.978336648648649">
This way we get a uniform scale between 0
and 1, in which higher values reflect higher similar-
ity. If both mutual information values are 0, then
simi,(wlw2, w) is defined to be 0. The right con-
text similarity, simR(wi,w2,w), is defined equiva-
lently, for /(wi , w) and /(w2, w)4.
Using definition 4 for each word w in the lex-
icon, we get 2 • 1 similarity values for w1 and w2,
where 1 is the size of the lexicon. The general sim-
ilarity between w1 and w2, termed sim(wi , w2), is
defined as a weighted average of these 2 • 1 values.
It is necessary to use some weighting mechanism,
since small values of mutual information tend to be
less significant and more vulnerable to noisy data.
We found that the maximal value involved in com-
puting the similarity relative to a specific word pro-
vides a useful weight for this word in computing the
average. Thus, the weight for a specific left context
similarity value, WL(wi, w2, w), is defined as:
WL(wi, w2, w) = max(i(w, wi), /(w, w2)) (5)
(notice that this is the same as the denominator in
definition 4). This definition provides intuitively
appropriate weights, since we would like to give
more weight to context words that have a large mu-
tual information value with at least one of w1 and
w2. The mutual information value with the other
word may then be large, providing a strong &amp;quot;vote&amp;quot;
for similarity, or may be small, providing a strong
&amp;quot;vote&amp;quot; against similarity. The weight for a spe-
cific right context similarity value is defined equiv-
alently. Using these weights, we get the weighted
average in Figure 2 as the general definition of
41n the case of cooccurrence pairs, a word may be in-
volved in two types of relations, being the left or right
argument of the pair. The definitions can be easily
adopted to cases in which there are more types of rela-
tions, such as provided by syntactic parsing.
</bodyText>
<equation confidence="0.9165204">
aimr, (wi w2, w) =
(4)
167
sim(wi w2) = (6)
Ew Elexicon SiML (W1) tV2) tV) • WL(2111)1D21 SiMR (W1) w2) WR(W1) w21
</equation>
<bodyText confidence="0.557218666666667">
EwElexicon WL(W1, 11/21 w) WR(W15 W2) 1-0)
Et, cieicon min(/(w, wi), /(w, w2)) min(/(wi, w), /(w2, w))
Ew Etexicon max(/(w, /pi), /(w, w2)) max(/(wi /(w2, w))
</bodyText>
<figureCaption confidence="0.995068">
Figure 2: The definition of the similarity metric.
</figureCaption>
<table confidence="0.970231">
Exhaustive Search Approximation
similar words sim similar words sim
aspects 1.000 aspects 1.000
topics 0.100 topics 0.100
areas 0.088 areas 0.088
expert 0.079 expert 0.079
issues 0.076 issues 0.076
approaches 0.072 concerning 0.069
</table>
<tableCaption confidence="0.988848">
Table 3: The most similar words of aspects: heuris-
</tableCaption>
<bodyText confidence="0.950338571428572">
tic and exhaustive search produce nearly the same
results.
The values produced by our metric have an in-
tuitive interpretation, as denoting a &amp;quot;typical&amp;quot; ra-
tio between the mutual information values of each
of the two words with another third word. The
metric is reflexive (sim(w, w) = 1), symmetric
(sim(wi, w2) = sim(w2, wi)), but is not transitive
(the values of sim(wi, w2) and sim(w2, w3) do not
imply anything on the value of sim(wi, w3)). The
left column of Table 3 lists the six most similar
words to the word &apos;aspects&apos; according to this met-
ric, based on our corpus. More examples of simi-
larity were shown in Figure 1.
</bodyText>
<subsectionHeader confidence="0.999548">
4.1 An efficient search heuristic
</subsectionHeader>
<bodyText confidence="0.999707421052632">
The estimation method of section 3 requires that
we identify the most similar words of a given word
w. Doing this by computing the similarity between
w and each word in the lexicon is computationally
very expensive (o(l2), where 1 is the size of the
lexicon, and 0(1 ) to do this in advance for all the
words in the lexicon). To account for this prob-
lem we developed a simple heuristic that searches
for words that are potentially similar to w, using
thresholds on mutual information values and fre-
quencies of cooccurrence pairs. The search is based
on the property that when computing sim(wi w2),
words that have high mutual information values
&apos;The nominator in our metric resembles the similar-
ity metric in (Hindle, 1990). We found, however, that
the difference between the two metrics is important, be-
cause the denominator serves as a normalization factor.
with both w1 and w2 make the largest contributions
to the value of the similarity measure. Also, high
and reliable mutual information values are typically
associated with relatively high frequencies of the in-
volved cooccurrence pairs. We therefore search first
for all the &amp;quot;strong neighbors&amp;quot; of w, which are de-
fined as words whose cooccurrence with w has high
mutual information and high frequency, and then
search for all their &amp;quot;strong neighbors&amp;quot;. The words
found this way (&amp;quot;the strong neighbors of the strong
neighbors of w&amp;quot;) are considered as candidates for
being similar words of w, and the similarity value
with w is then computed only for these words. We
thus get an approximation for the set of words that
are most similar to w. For the example given in Ta-
ble 3, the exhaustive method required 17 minutes
of CPU time on a Sun 4 workstation, while the ap-
proximation required only 7 seconds. This was
done using a data base of 1,377,653 cooccurrence
pairs that were extracted from the corpus, along
with their counts.
</bodyText>
<sectionHeader confidence="0.999604" genericHeader="method">
5 Evaluations
</sectionHeader>
<subsectionHeader confidence="0.99595">
5.1 Word sense disambiguation in
machine translation
</subsectionHeader>
<bodyText confidence="0.999978095238095">
The purpose of the first evaluation was to test
whether the similarity based estimation method
can enhance the performance of a disambiguation
technique. Typically in a disambiguation task, dif-
ferent cooccurrences correspond to alternative in-
terpretations of the ambiguous construct. It is
therefore necessary that the probability estimates
for the alternative cooccurrences will reflect the rel-
ative order between their true probabilities. How-
ever, a consistent bias in the estimate is usually not
harmful, as it still preserves the correct relative or-
der between the alternatives.
To carry out the evaluation, we implemented
a variant of the disambiguation method of (Dagan
et al., 1991), for sense disambiguation in machine
translation. We term this method as TWS, for
Target Word Selection. Consider for example the
Hebrew phrase `laxtorn xoze shalom&apos;, which trans-
lates as &apos;to sign a peace treaty&apos;. The word cla.xtom&apos;,
however, is ambiguous, and can be translated to ei-
ther &apos;sign&apos; or &apos;seal&apos;. To resolve the ambiguity, the
</bodyText>
<page confidence="0.993111">
168
</page>
<table confidence="0.999943">
Precision Applicability
TWS 85.5 64.3
Augmented TWS 83.6 79.6
Word Frequency 66.9 100
</table>
<tableCaption confidence="0.9608075">
Table 4: Results of TWS, Augmented TWS and
Word Frequency methods
</tableCaption>
<bodyText confidence="0.998940921052631">
TWS method first generates the alternative lexi-
cal cooccurrence patterns in the target language,
that correspond to alternative selections of target
words. Then, it prefers those target words that
generate more frequent patterns. In our example,
the word &apos;sign&apos; is preferred upon the word &apos;seal&apos;,
since the pattern &apos;to sign a treaty&apos; is much more fre-
quent than the pattern &apos;to seal a treaty&apos;. Similarly,
the word `xoze&apos; is translated to &apos;treaty&apos; rather than
&apos;contract&apos;, due to the high frequency of the pattern
&apos;peace treaty&apos;6. In our implementation, cooccur-
rence pairs were used instead of lexical cooccur-
rence within syntactic relations (as in the original
work), to save the need of parsing the corpus.
We randomly selected from a software manual
a set of 269 examples of ambiguous Hebrew words
in translating Hebrew sentences to English. The
expected success rate of random selection for these
examples was 23%. The similarity based estima-
tion method was used to estimate the expected fre-
quency of unobserved cooccurrence pairs, in cases
where none of the alternative pairs occurred in
the corpus (each pair corresponds to an alternative
target word). Using this method, which we term
Augmented TWS, 41 additional cases were disam-
biguated, relative to the original method. We thus
achieved an increase of about 15% in the applica-
bility (coverage) of the TWS method, with a small
decrease in the overall precision. The performance
of the Augmented TWS method on these 41 exam-
ples was about 15% higher than that of a naive,
Word Frequency method, which always selects the
most frequent translation. It should be noted that
the Word Frequency method is equivalent to us-
ing the frequency based estimate, in which higher
word frequencies entail a higher estimate for the
corresponding cooccurrence. The results of the ex-
periment are summarized in Table 4.
</bodyText>
<subsectionHeader confidence="0.999643">
5.2 A data recovery task
</subsectionHeader>
<bodyText confidence="0.984815803030303">
In the second evaluation, the estimation method
had to distinguish between members of two sets of
61t should be emphasized that the TWS method uses
only a monolingual target corpus, and not a bilingual
corpus as in other methods ((Brown et al., 1991; Gale
et al., 1992)). The alternative cooccurrence patterns
in the target language, which correspond to the alter-
native translations of the ambiguous source words, are
constructed using a bilingual lexicon.
cooccurrence pairs, one of them containing pairs
with relatively high probability and the other pairs
with low probability. To a large extent, this task
simulates a typical scenario in disambiguation, as
demonstrated in the first evaluation.
Ideally, this evaluation should be carried out
using a large set of held out data, which would
provide good estimates for the true probabilities of
the pairs in the test sets. The estimation method
should then use a much smaller training corpus,
in which none of the example pairs occur, and
then should try to recover the probabilities that are
known to us from the held out data. However, such
a setting requires that the held out corpus would
be several times larger than the training corpus,
while the latter should be large enough for robust
application of the estimation method. This was not
feasible with the size of our corpus, and the rather
noisy data we had.
To avoid this problem, we obtained the set of
pairs with high probability from the training cor-
pus, selecting pairs that occur at least 5 times.
We then deleted these pairs from the data base
that is used by the estimation method, forcing
the method to recover their probabilities using the
other pairs of the corpus. The second set, of pairs
with low probability, was obtained by constructing
pairs that do not occur in the corpus. The two sets,
each of them containing 150 pairs, were constructed
randomly and were restricted to words with indi-
vidual frequencies between 500 and 2500. We term
these two sets as the occurring and non-occurring
sets.
The task of distinguishing between members
of the two sets, without access to the deleted fre-
quency information, is by no means trivial. Trying
to use the individual word frequencies will result
in performance close to that of using random selec-
tion. This is because the individual frequencies of
all participating words are within the same range
of values.
To address the task, we used the following pro-
cedure: The frequency of each cooccurrence pair
was estimated using the similarity-based estima-
tion method. If the estimated frequency was above
2.5 (which was set arbitrarily as the average of 5
and 0), the pair was recovered as a member of the
occurring set. Otherwise, it was recovered as a
member of the non-occurring set.
Out of the 150 pairs of the occurring set, our
method correctly identified 119 (79%). For the
non-occurring set, it correctly identified 126 pairs
(84%). Thus, the method achieved an overall ac-
curacy of 81.6%. Optimal tuning of the threshold,
to a value of 2, improves the overall accuracy to
85%, where about 90% of the members of the oc-
curring set and 80% of those in the non-occurring
</bodyText>
<page confidence="0.996303">
169
</page>
<bodyText confidence="0.999617473684211">
set are identified correctly. This is contrasted with
the optimal discrimination that could be achieved
by frequency based estimation, which is 58%.
Figures 3 and 4 illustrate the results of the ex-
periment. Figure 3 shows the distributions of the
expected frequency of the pairs in the two sets, us-
ing similarity based and frequency based estima-
tion. It clearly indicates that the similarity based
method gives high estimates mainly to members of
the occurring set and low estimates mainly to mem-
bers of the non-occurring set. Frequency based es-
timation, on the other hand, makes a much poorer
distinction between the two sets. Figure 4 plots the
two types of estimation for pairs in the occurring
set as a function of their true frequency in the cor-
pus. It can be seen that while the frequency based
estimates are always low (by construction) the sim-
ilarity based estimates are in most cases closer to
the true value.
</bodyText>
<sectionHeader confidence="0.999747" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999922666666667">
In both evaluations, similarity based estimation
performs better than frequency based estimation.
This indicates that when trying to estimate cooc-
currence probabilities, it is useful to consider the
cooccurrence patterns of the specific words and
not just their frequencies, as smoothing methods
do. Comparing with class based models, our ap-
proach suggests the advantage of making the most
specific analogies for each word, instead of making
analogies with all members of a class, via general
class parameters. This raises the question whether
generalizations over word classes, which follow long
traditions in semantic classification, indeed provide
the best means for inferencing about properties of
words.
</bodyText>
<sectionHeader confidence="0.994949" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999435">
We are grateful to Alon Itai for his help in initiating
this research. We would like to thank Ken Church
and David Lewis for their helpful comments on ear-
lier drafts of this paper.
</bodyText>
<sectionHeader confidence="0.9997" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.927193125">
Peter Brown, Vincent Della Pietra, Peter deSouza,
Jenifer Lai, and Robert Mercer. Class-based
n-gram models of natural language. Computa-
tional Linguistics. (To appear).
P. Brown, S. Della Pietra, V. Della Pietra, and
R. Mercer. 1991. Word sense disambiguation
using statistical methods. In Proc. of the An-
nual Meeting of the ACL.
</reference>
<bodyText confidence="0.7266505">
Kenneth W. Church and William A. Gale. 1991.
A comparison of the enhanced Good-Turing
</bodyText>
<figure confidence="0.428903333333333">
Estimated Value: Similarity Based
optimal occurring
threshold (58%) II non-
occurring
0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 2.2
Estimated Value: Frequency Based
</figure>
<figureCaption confidence="0.947538">
Figure 3: Frequency distributions of estimated fre-
quency values for occurring and non-occurring sets.
</figureCaption>
<figure confidence="0.983791666666667">
occurring
I non-
/ occurring
optimal
threshold (85%)
0 1 2 3 4 5 6 7 8 9 10 11 12
170
6 8 10 12 14 16 18
True Frequency
</figure>
<figureCaption confidence="0.994442">
Figure 4: Similarity based estimation (`-1-&apos;) and fre-
</figureCaption>
<bodyText confidence="0.938284823529412">
quency based estimation (`0&apos;) for the expected fre-
quency of members of the occurring set, as a func-
tion of the true frequency.
and deleted estimation methods for estimat-
ing probabilities of English bigrams. Computer
Speech and Language, 5:19-54.
Kenneth W. Church and Patrick Hanks. 1990.
Word association norms, mutual information,
and lexicography. Computational Linguistics,
16(1):22-29.
Kenneth W. Church and Robert L. Mercer. 1992.
Introduction to the special issue in computa-
tional linguistics using large corpora. Compu-
tational Linguistics. (In press).
Ido Dagan and Alon Itai. 1990. Automatic ac-
quisition of constraints for the resolution of
anaphora references and syntactic ambiguities.
</bodyText>
<reference confidence="0.988117328125">
In Proc. of COLING.
Ido Dagan, Alon Itai, and Ulrike Schwall. 1991.
Two languages are more informative than one.
In Proc. of the Annual Meeting of the ACL.
R. Fano. 1961. Transmission of Information.
Cambridge,Mass:MIT Press.
William Gale, Kenneth Church, and David
Yarowsky. 1992. Using bilingual materials
to develop word sense disambiguation meth-
ods. In Proc. of the International Conference
on Theoretical and Met hodolgical Issues in Ma-
chine Translation.
I. J. Good. 1953. The population frequencies of
species and the estimation of population pa-
rameters. Biometrika, 40:237-264.
R. Grishman, L. Hirschman, and Ngo Thanh Nhan.
1986. Discovery procedures for sublanguage se-
lectional patterns — initial experiments. Com-
putational Linguistics, 12:205-214,
D. Hindle and M. Rooth. 1991. Structural am-
biguity and lexical relations. In Proc. of the
Annual Meeting of the ACL.
D. Hindle. 1990. Noun classification from
predicate-argument structures. In Proc. of the
Annual Meeting of the ACL.
L. Hirschman. 1986. Discovering sublanguage
structures. In R. Grishman and R. Kittredge,
editors, Analyzing Language in Restricted Do-
mains: Sublanguage Description and Process-
ing, pages 211-234. Lawrence Erlbaum Asso-
ciates.
F. Jelinek and R. Mercer. 1985. Probability dis-
tribution estimation from sparse data. IBM
Technical Disclosure Bulletin, 28:2591-2594.
Frederick Jelinek. 1990. Self-organized language
modeling for speech recognition. In Alex
Waibel and Kai-Fu Lee, editors, Readings in
Speech Recognition, pages 450-506. Morgan
Kaufmann Publishers, Inc., San Maeio, Cali-
fornia.
Slava M. Katz. 1987. Estimation of probabilities
from sparse data for the language model com-
ponent of a speech recognizer. IEEE Transac-
tions on Acoustics, speech, and Signal Process-
ing, 35(3):400-401.
Yoelle Maarek and Frank Smadja. 1989. Full text
indexing based on lexical relations — An appli-
cation: Software libraries. In Proc. of SIGIR.
Fernando Pereira, Naftali Tishby, and Lillian Lee.
1993. Distributional clustering of English
words. In Proc. of the Annual Meeting of the
ACL.
Philip Resnik. 1992. Wordnet and distributional
analysis: A class-based approach to lexical dis-
covery. In AAAI Workshop on Statistically-
based Natural Language Processing Techniques,
July.
V. Sadler. 1989. Working with analogical seman-
tics: Disambiguation techniques in DLT. Foris
Publications.
Frank Smadj a and Katheleen McKeown. 1990. Au-
tomatically extracting and representing collo-
cations for language generation. In Proc. of the
Annual Meeting of the ACL.
</reference>
<figure confidence="0.9731248">
0-
+
*+ /I
Z.+
+ +++
*
$ + +
+ t ++
iilipiA8 0 0 9
o 6
</figure>
<page confidence="0.958861">
171
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.738093">
<title confidence="0.991624">CONTEXTUAL WORD SIMILARITY AND ESTIMATION</title>
<author confidence="0.823334">FROM SPARSE DATA</author>
<affiliation confidence="0.999104">AT&amp;T Bell Laboratories</affiliation>
<address confidence="0.999817">600 Mountain Avenue Murray Hill, NJ 07974</address>
<email confidence="0.999838">daganOresearch.att.com</email>
<author confidence="0.998106">Shaul Marcus</author>
<affiliation confidence="0.982604">Computer Science Department Technion</affiliation>
<address confidence="0.997042">Haifa 32000, Israel</address>
<email confidence="0.998711">shaulOcs.technion.ac.il</email>
<author confidence="0.976991">Shaul Markovitch</author>
<affiliation confidence="0.98239">Computer Science Department Technion</affiliation>
<address confidence="0.997019">Haifa 32000, Israel</address>
<email confidence="0.999223">shaulmOcs.technion.ac.il</email>
<abstract confidence="0.999621066666667">In recent years there is much interest in word cooccurrence relations, such as n-grams, verbobject combinations, or cooccurrence within a limited context. This paper discusses how to estimate the probability of cooccurrences that do not occur in the training data. We present a method that makes local analogies between each specific unobserved cooccurrence and other cooccurrences that contain similar words, as determined by an appropriate word similarity metric. Our evaluation suggests that this method performs better than existing smoothing methods, and may provide an alternative to class based models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Peter Brown</author>
<author>Vincent Della Pietra</author>
<author>Peter deSouza</author>
<author>Jenifer Lai</author>
<author>Robert Mercer</author>
</authors>
<title>Class-based n-gram models of natural language. Computational Linguistics.</title>
<note>(To appear).</note>
<marker>Brown, Pietra, deSouza, Lai, Mercer, </marker>
<rawString>Peter Brown, Vincent Della Pietra, Peter deSouza, Jenifer Lai, and Robert Mercer. Class-based n-gram models of natural language. Computational Linguistics. (To appear).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>S Della Pietra</author>
<author>V Della Pietra</author>
<author>R Mercer</author>
</authors>
<title>Word sense disambiguation using statistical methods.</title>
<date>1991</date>
<booktitle>In Proc. of the Annual Meeting of the ACL. In Proc. of COLING.</booktitle>
<contexts>
<context position="23000" citStr="Brown et al., 1991" startWordPosition="3840" endWordPosition="3843">at of a naive, Word Frequency method, which always selects the most frequent translation. It should be noted that the Word Frequency method is equivalent to using the frequency based estimate, in which higher word frequencies entail a higher estimate for the corresponding cooccurrence. The results of the experiment are summarized in Table 4. 5.2 A data recovery task In the second evaluation, the estimation method had to distinguish between members of two sets of 61t should be emphasized that the TWS method uses only a monolingual target corpus, and not a bilingual corpus as in other methods ((Brown et al., 1991; Gale et al., 1992)). The alternative cooccurrence patterns in the target language, which correspond to the alternative translations of the ambiguous source words, are constructed using a bilingual lexicon. cooccurrence pairs, one of them containing pairs with relatively high probability and the other pairs with low probability. To a large extent, this task simulates a typical scenario in disambiguation, as demonstrated in the first evaluation. Ideally, this evaluation should be carried out using a large set of held out data, which would provide good estimates for the true probabilities of th</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1991</marker>
<rawString>P. Brown, S. Della Pietra, V. Della Pietra, and R. Mercer. 1991. Word sense disambiguation using statistical methods. In Proc. of the Annual Meeting of the ACL. In Proc. of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Alon Itai</author>
<author>Ulrike Schwall</author>
</authors>
<title>Two languages are more informative than one.</title>
<date>1991</date>
<booktitle>In Proc. of the Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="1764" citStr="Dagan et al., 1991" startWordPosition="247" endWordPosition="250"> relations are in use, such as cooccurrence within a consecutive sequence of words (n-grams), within syntactic relations (verb-object, adjective-noun, etc.) or the cooccurrence of two words within a limited distance in the context. Statistical data about these various cooccurrence relations is employed for a variety of applications, such as speech recognition (Jelinek, 1990), language generation (Smadja and McKeown, 1990), lexicography (Church and Hanks, 1990), machine translation (Brown et al., ; Sadler, 1989), information retrieval (Maarek and Smadja, 1989) and various disambiguation tasks (Dagan et al., 1991; Hindle and Rooth, 1991; Grishman et al., 1986; Dagan and Itai, 1990). A major problem for the above applications is how to estimate the probability of cooccurrences that were not observed in the training corpus. Due to data sparseness in unrestricted language, the aggregate probability of such cooccurrences is large and can easily get to 25% or more, even for a very large training corpus (Church and Mercer, 1992). Since applications often have to compare alternative hypothesized cooccurrences, it is important to distinguish between those unobserved cooccurrences that are likely to occur in a</context>
<context position="6671" citStr="Dagan et al., 1991" startWordPosition="1029" endWordPosition="1032">d by the similarity metric (only some of the edges, with relatively high values, are shown). This network may be found useful for other purposes, independently of the estimation method. The estimation method was implemented using the relation of cooccurrence of two words within a limited distance in a sentence. The proposed method, however, is general and is applicable for any type of lexical cooccurrence. The method was evaluated in two experiments. In the first one we achieved a complete scenario of the use of the estimation method, by implementing a variant of the disambiguation method in (Dagan et al., 1991), for sense selection in machine translation. The estimation method was then successfully used to increase the coverage of the disambiguation method by 15%, with an increase of the overall precision compared to a naive, frequency based, method. In the second experiment we evaluated the estimation method on a data recovery task. The task simulates a typical scenario in disambiguation, and also relates to theoretical questions about redundancy and idiosyncrasy in cooccurrence data. In this evaluation, which involved 300 examples, the performance of the estimation method was by 27% better than fr</context>
<context position="20378" citStr="Dagan et al., 1991" startWordPosition="3409" endWordPosition="3412"> based estimation method can enhance the performance of a disambiguation technique. Typically in a disambiguation task, different cooccurrences correspond to alternative interpretations of the ambiguous construct. It is therefore necessary that the probability estimates for the alternative cooccurrences will reflect the relative order between their true probabilities. However, a consistent bias in the estimate is usually not harmful, as it still preserves the correct relative order between the alternatives. To carry out the evaluation, we implemented a variant of the disambiguation method of (Dagan et al., 1991), for sense disambiguation in machine translation. We term this method as TWS, for Target Word Selection. Consider for example the Hebrew phrase `laxtorn xoze shalom&apos;, which translates as &apos;to sign a peace treaty&apos;. The word cla.xtom&apos;, however, is ambiguous, and can be translated to either &apos;sign&apos; or &apos;seal&apos;. To resolve the ambiguity, the 168 Precision Applicability TWS 85.5 64.3 Augmented TWS 83.6 79.6 Word Frequency 66.9 100 Table 4: Results of TWS, Augmented TWS and Word Frequency methods TWS method first generates the alternative lexical cooccurrence patterns in the target language, that corre</context>
</contexts>
<marker>Dagan, Itai, Schwall, 1991</marker>
<rawString>Ido Dagan, Alon Itai, and Ulrike Schwall. 1991. Two languages are more informative than one. In Proc. of the Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Fano</author>
</authors>
<title>Transmission of Information.</title>
<date>1961</date>
<publisher>Cambridge,Mass:MIT Press.</publisher>
<contexts>
<context position="8161" citStr="Fano, 1961" startWordPosition="1280" endWordPosition="1281">rs. In the experiments reported here d = 3. A cooccurrence pair can be viewed as a generalization of a bigram, where a bigram is a cooccurrence pair with d = 1 (without ignoring function words). As with bigrams, a cooccurrence pair is directional, i.e. (x, y) (y, x). This captures some information about the asymmetry in the linear order of linguistic relations, such as the fact that verbs tend to precede their objects and follow their subjects. The mutual information of a cooccurrence pair, which measures the degree of association between the two words (Church and Hanks, 1990), is defined as (Fano, 1961): , y) = log2 p13(x(x)pl)y) -- log2 Pp(x(xIY)) (1) P(Yix) = log2 p(y) where P(z) and P(y) are the probabilities of the events x and y (occurrences of words, in our case) and P (x , y) is the probability of the joint event (a cooccurrence pair). We estimate mutual information values using the Maximum Likelihood Estimator (MLE): P (x y) N f (x y) i(x , y) = log2 P(x) &apos;P &apos;(y) = log2 ( f()f ) d 4 (Y) (2) where f denotes the frequency of an event and N is the length of the corpus. While better estimates for small probabilities are available (Good, 1953; Church and Gale, 1991), MLE is the simplest t</context>
</contexts>
<marker>Fano, 1961</marker>
<rawString>R. Fano. 1961. Transmission of Information. Cambridge,Mass:MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Gale</author>
<author>Kenneth Church</author>
<author>David Yarowsky</author>
</authors>
<title>Using bilingual materials to develop word sense disambiguation methods.</title>
<date>1992</date>
<booktitle>In Proc. of the International Conference on Theoretical and Met hodolgical Issues in Machine Translation.</booktitle>
<contexts>
<context position="23020" citStr="Gale et al., 1992" startWordPosition="3844" endWordPosition="3847">Frequency method, which always selects the most frequent translation. It should be noted that the Word Frequency method is equivalent to using the frequency based estimate, in which higher word frequencies entail a higher estimate for the corresponding cooccurrence. The results of the experiment are summarized in Table 4. 5.2 A data recovery task In the second evaluation, the estimation method had to distinguish between members of two sets of 61t should be emphasized that the TWS method uses only a monolingual target corpus, and not a bilingual corpus as in other methods ((Brown et al., 1991; Gale et al., 1992)). The alternative cooccurrence patterns in the target language, which correspond to the alternative translations of the ambiguous source words, are constructed using a bilingual lexicon. cooccurrence pairs, one of them containing pairs with relatively high probability and the other pairs with low probability. To a large extent, this task simulates a typical scenario in disambiguation, as demonstrated in the first evaluation. Ideally, this evaluation should be carried out using a large set of held out data, which would provide good estimates for the true probabilities of the pairs in the test </context>
</contexts>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>William Gale, Kenneth Church, and David Yarowsky. 1992. Using bilingual materials to develop word sense disambiguation methods. In Proc. of the International Conference on Theoretical and Met hodolgical Issues in Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I J Good</author>
</authors>
<title>The population frequencies of species and the estimation of population parameters.</title>
<date>1953</date>
<journal>Biometrika,</journal>
<pages>40--237</pages>
<contexts>
<context position="2878" citStr="Good, 1953" startWordPosition="426" endWordPosition="427">is important to distinguish between those unobserved cooccurrences that are likely to occur in a new piece of text and those that are not These distinctions ought to be made using the data that do occur in the corpus. Thus, beyond its own practical importance, the sparse data problem provides an informative touchstone for theories on generalization and analogy in linguistic data. The literature suggests two major approaches for solving the sparse data problem: smoothing and class based methods. Smoothing methods estimate the probability of unobserved cooccurrences using frequency information (Good, 1953; Katz, 1987; Jelinek and Mercer, 1985; Church and Gale, 1991). Church and Gale (Church and Gale, 1991) show, that for unobserved bigrams, the estimates of several smoothing methods closely agree with the probability that is expected using the frequencies of the two words and assuming that their occurrence is independent ((Church and Gale, 1991), figure 5). Furthermore, using held out data they show that this is the probability that should be estimated by a smoothing method that takes into account the frequencies of the individual words. Relying on this result, we will use frequency based esti</context>
<context position="8714" citStr="Good, 1953" startWordPosition="1389" endWordPosition="1390">ords (Church and Hanks, 1990), is defined as (Fano, 1961): , y) = log2 p13(x(x)pl)y) -- log2 Pp(x(xIY)) (1) P(Yix) = log2 p(y) where P(z) and P(y) are the probabilities of the events x and y (occurrences of words, in our case) and P (x , y) is the probability of the joint event (a cooccurrence pair). We estimate mutual information values using the Maximum Likelihood Estimator (MLE): P (x y) N f (x y) i(x , y) = log2 P(x) &apos;P &apos;(y) = log2 ( f()f ) d 4 (Y) (2) where f denotes the frequency of an event and N is the length of the corpus. While better estimates for small probabilities are available (Good, 1953; Church and Gale, 1991), MLE is the simplest to implement and was adequate for the purpose of this study. Due to the unreliability of measuring negative mutual information values in corpora that are not extremely large, we have considered in this work any negative value to be 0. We also set i (x , y) to 0 if f (x y) = 0. Thus, we assume in both cases that the association between the two words is as expected by chance. 165 articles conference, 0.13 workshop 0.102 0.11 0.106 symposium 0.132 paper 0.11 papers 0.1 book I documentation 0.137 Figure 1: A portion of the similarity network. 3 Estimat</context>
</contexts>
<marker>Good, 1953</marker>
<rawString>I. J. Good. 1953. The population frequencies of species and the estimation of population parameters. Biometrika, 40:237-264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Grishman</author>
<author>L Hirschman</author>
<author>Ngo Thanh Nhan</author>
</authors>
<title>Discovery procedures for sublanguage selectional patterns — initial experiments.</title>
<date>1986</date>
<journal>Computational Linguistics,</journal>
<pages>12--205</pages>
<contexts>
<context position="1811" citStr="Grishman et al., 1986" startWordPosition="255" endWordPosition="258">within a consecutive sequence of words (n-grams), within syntactic relations (verb-object, adjective-noun, etc.) or the cooccurrence of two words within a limited distance in the context. Statistical data about these various cooccurrence relations is employed for a variety of applications, such as speech recognition (Jelinek, 1990), language generation (Smadja and McKeown, 1990), lexicography (Church and Hanks, 1990), machine translation (Brown et al., ; Sadler, 1989), information retrieval (Maarek and Smadja, 1989) and various disambiguation tasks (Dagan et al., 1991; Hindle and Rooth, 1991; Grishman et al., 1986; Dagan and Itai, 1990). A major problem for the above applications is how to estimate the probability of cooccurrences that were not observed in the training corpus. Due to data sparseness in unrestricted language, the aggregate probability of such cooccurrences is large and can easily get to 25% or more, even for a very large training corpus (Church and Mercer, 1992). Since applications often have to compare alternative hypothesized cooccurrences, it is important to distinguish between those unobserved cooccurrences that are likely to occur in a new piece of text and those that are not These</context>
</contexts>
<marker>Grishman, Hirschman, Nhan, 1986</marker>
<rawString>R. Grishman, L. Hirschman, and Ngo Thanh Nhan. 1986. Discovery procedures for sublanguage selectional patterns — initial experiments. Computational Linguistics, 12:205-214,</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hindle</author>
<author>M Rooth</author>
</authors>
<title>Structural ambiguity and lexical relations.</title>
<date>1991</date>
<booktitle>In Proc. of the Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="1788" citStr="Hindle and Rooth, 1991" startWordPosition="251" endWordPosition="254">e, such as cooccurrence within a consecutive sequence of words (n-grams), within syntactic relations (verb-object, adjective-noun, etc.) or the cooccurrence of two words within a limited distance in the context. Statistical data about these various cooccurrence relations is employed for a variety of applications, such as speech recognition (Jelinek, 1990), language generation (Smadja and McKeown, 1990), lexicography (Church and Hanks, 1990), machine translation (Brown et al., ; Sadler, 1989), information retrieval (Maarek and Smadja, 1989) and various disambiguation tasks (Dagan et al., 1991; Hindle and Rooth, 1991; Grishman et al., 1986; Dagan and Itai, 1990). A major problem for the above applications is how to estimate the probability of cooccurrences that were not observed in the training corpus. Due to data sparseness in unrestricted language, the aggregate probability of such cooccurrences is large and can easily get to 25% or more, even for a very large training corpus (Church and Mercer, 1992). Since applications often have to compare alternative hypothesized cooccurrences, it is important to distinguish between those unobserved cooccurrences that are likely to occur in a new piece of text and t</context>
</contexts>
<marker>Hindle, Rooth, 1991</marker>
<rawString>D. Hindle and M. Rooth. 1991. Structural ambiguity and lexical relations. In Proc. of the Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hindle</author>
</authors>
<title>Noun classification from predicate-argument structures.</title>
<date>1990</date>
<booktitle>In Proc. of the Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="18462" citStr="Hindle, 1990" startWordPosition="3101" endWordPosition="3102">his by computing the similarity between w and each word in the lexicon is computationally very expensive (o(l2), where 1 is the size of the lexicon, and 0(1 ) to do this in advance for all the words in the lexicon). To account for this problem we developed a simple heuristic that searches for words that are potentially similar to w, using thresholds on mutual information values and frequencies of cooccurrence pairs. The search is based on the property that when computing sim(wi w2), words that have high mutual information values &apos;The nominator in our metric resembles the similarity metric in (Hindle, 1990). We found, however, that the difference between the two metrics is important, because the denominator serves as a normalization factor. with both w1 and w2 make the largest contributions to the value of the similarity measure. Also, high and reliable mutual information values are typically associated with relatively high frequencies of the involved cooccurrence pairs. We therefore search first for all the &amp;quot;strong neighbors&amp;quot; of w, which are defined as words whose cooccurrence with w has high mutual information and high frequency, and then search for all their &amp;quot;strong neighbors&amp;quot;. The words foun</context>
</contexts>
<marker>Hindle, 1990</marker>
<rawString>D. Hindle. 1990. Noun classification from predicate-argument structures. In Proc. of the Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Hirschman</author>
</authors>
<title>Discovering sublanguage structures. In</title>
<date>1986</date>
<booktitle>Analyzing Language in Restricted Domains: Sublanguage Description and Processing,</booktitle>
<pages>211--234</pages>
<editor>R. Grishman and R. Kittredge, editors,</editor>
<contexts>
<context position="4037" citStr="Hirschman, 1986" startWordPosition="611" endWordPosition="612">Relying on this result, we will use frequency based estimation (using word frequencies) as representative for smoothing estimates of unobserved cooccurrences, for comparison purposes. As will be shown later, the problem with smoothing estimates is that they ignore the expected degree of association between the specific words of the cooccurrence. For example, we would not like to estimate the same probability for two cooccurrences like &apos;eat bread&apos; and &apos;eat cars&apos;, despite the fact that both &apos;bread&apos; and &apos;cars&apos; may have the same frequency. Class based models (Brown et al., ; Pereira et al., 1993; Hirschman, 1986; Resnik, 1992) distinguish between unobserved cooccurrences using classes of &amp;quot;similar&amp;quot; words. The probability of a specific cooccurrence is determined using generalized parameters about the probability of class cooccur164 rence. This approach, which follows long traditions in semantic classification, is very appealing, as it attempts to capture &amp;quot;typical&amp;quot; properties of classes of words. However, it is not clear at all that unrestricted language is indeed structured the way it is assumed by class based models. In particular, it is not clear that word cooccurrence patterns can be structured and </context>
</contexts>
<marker>Hirschman, 1986</marker>
<rawString>L. Hirschman. 1986. Discovering sublanguage structures. In R. Grishman and R. Kittredge, editors, Analyzing Language in Restricted Domains: Sublanguage Description and Processing, pages 211-234. Lawrence Erlbaum Associates.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
<author>R Mercer</author>
</authors>
<title>Probability distribution estimation from sparse data.</title>
<date>1985</date>
<journal>IBM Technical Disclosure Bulletin,</journal>
<pages>28--2591</pages>
<contexts>
<context position="2916" citStr="Jelinek and Mercer, 1985" startWordPosition="430" endWordPosition="433">ish between those unobserved cooccurrences that are likely to occur in a new piece of text and those that are not These distinctions ought to be made using the data that do occur in the corpus. Thus, beyond its own practical importance, the sparse data problem provides an informative touchstone for theories on generalization and analogy in linguistic data. The literature suggests two major approaches for solving the sparse data problem: smoothing and class based methods. Smoothing methods estimate the probability of unobserved cooccurrences using frequency information (Good, 1953; Katz, 1987; Jelinek and Mercer, 1985; Church and Gale, 1991). Church and Gale (Church and Gale, 1991) show, that for unobserved bigrams, the estimates of several smoothing methods closely agree with the probability that is expected using the frequencies of the two words and assuming that their occurrence is independent ((Church and Gale, 1991), figure 5). Furthermore, using held out data they show that this is the probability that should be estimated by a smoothing method that takes into account the frequencies of the individual words. Relying on this result, we will use frequency based estimation (using word frequencies) as rep</context>
</contexts>
<marker>Jelinek, Mercer, 1985</marker>
<rawString>F. Jelinek and R. Mercer. 1985. Probability distribution estimation from sparse data. IBM Technical Disclosure Bulletin, 28:2591-2594.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
</authors>
<title>Self-organized language modeling for speech recognition.</title>
<date>1990</date>
<booktitle>In Alex Waibel and Kai-Fu Lee, editors, Readings in Speech Recognition,</booktitle>
<pages>450--506</pages>
<publisher>Morgan Kaufmann Publishers, Inc.,</publisher>
<location>San Maeio, California.</location>
<contexts>
<context position="1523" citStr="Jelinek, 1990" startWordPosition="213" endWordPosition="214">hods, and may provide an alternative to class based models. 1 Introduction Statistical data on word cooccurrence relations play a major role in many corpus based approaches for natural language processing. Different types of cooccurrence relations are in use, such as cooccurrence within a consecutive sequence of words (n-grams), within syntactic relations (verb-object, adjective-noun, etc.) or the cooccurrence of two words within a limited distance in the context. Statistical data about these various cooccurrence relations is employed for a variety of applications, such as speech recognition (Jelinek, 1990), language generation (Smadja and McKeown, 1990), lexicography (Church and Hanks, 1990), machine translation (Brown et al., ; Sadler, 1989), information retrieval (Maarek and Smadja, 1989) and various disambiguation tasks (Dagan et al., 1991; Hindle and Rooth, 1991; Grishman et al., 1986; Dagan and Itai, 1990). A major problem for the above applications is how to estimate the probability of cooccurrences that were not observed in the training corpus. Due to data sparseness in unrestricted language, the aggregate probability of such cooccurrences is large and can easily get to 25% or more, even</context>
</contexts>
<marker>Jelinek, 1990</marker>
<rawString>Frederick Jelinek. 1990. Self-organized language modeling for speech recognition. In Alex Waibel and Kai-Fu Lee, editors, Readings in Speech Recognition, pages 450-506. Morgan Kaufmann Publishers, Inc., San Maeio, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slava M Katz</author>
</authors>
<title>Estimation of probabilities from sparse data for the language model component of a speech recognizer.</title>
<date>1987</date>
<journal>IEEE Transactions on Acoustics, speech, and Signal Processing,</journal>
<pages>35--3</pages>
<contexts>
<context position="2890" citStr="Katz, 1987" startWordPosition="428" endWordPosition="429"> to distinguish between those unobserved cooccurrences that are likely to occur in a new piece of text and those that are not These distinctions ought to be made using the data that do occur in the corpus. Thus, beyond its own practical importance, the sparse data problem provides an informative touchstone for theories on generalization and analogy in linguistic data. The literature suggests two major approaches for solving the sparse data problem: smoothing and class based methods. Smoothing methods estimate the probability of unobserved cooccurrences using frequency information (Good, 1953; Katz, 1987; Jelinek and Mercer, 1985; Church and Gale, 1991). Church and Gale (Church and Gale, 1991) show, that for unobserved bigrams, the estimates of several smoothing methods closely agree with the probability that is expected using the frequencies of the two words and assuming that their occurrence is independent ((Church and Gale, 1991), figure 5). Furthermore, using held out data they show that this is the probability that should be estimated by a smoothing method that takes into account the frequencies of the individual words. Relying on this result, we will use frequency based estimation (usin</context>
</contexts>
<marker>Katz, 1987</marker>
<rawString>Slava M. Katz. 1987. Estimation of probabilities from sparse data for the language model component of a speech recognizer. IEEE Transactions on Acoustics, speech, and Signal Processing, 35(3):400-401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoelle Maarek</author>
<author>Frank Smadja</author>
</authors>
<title>Full text indexing based on lexical relations — An application: Software libraries.</title>
<date>1989</date>
<booktitle>In Proc. of SIGIR.</booktitle>
<contexts>
<context position="1711" citStr="Maarek and Smadja, 1989" startWordPosition="239" endWordPosition="242">atural language processing. Different types of cooccurrence relations are in use, such as cooccurrence within a consecutive sequence of words (n-grams), within syntactic relations (verb-object, adjective-noun, etc.) or the cooccurrence of two words within a limited distance in the context. Statistical data about these various cooccurrence relations is employed for a variety of applications, such as speech recognition (Jelinek, 1990), language generation (Smadja and McKeown, 1990), lexicography (Church and Hanks, 1990), machine translation (Brown et al., ; Sadler, 1989), information retrieval (Maarek and Smadja, 1989) and various disambiguation tasks (Dagan et al., 1991; Hindle and Rooth, 1991; Grishman et al., 1986; Dagan and Itai, 1990). A major problem for the above applications is how to estimate the probability of cooccurrences that were not observed in the training corpus. Due to data sparseness in unrestricted language, the aggregate probability of such cooccurrences is large and can easily get to 25% or more, even for a very large training corpus (Church and Mercer, 1992). Since applications often have to compare alternative hypothesized cooccurrences, it is important to distinguish between those u</context>
</contexts>
<marker>Maarek, Smadja, 1989</marker>
<rawString>Yoelle Maarek and Frank Smadja. 1989. Full text indexing based on lexical relations — An application: Software libraries. In Proc. of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>Naftali Tishby</author>
<author>Lillian Lee</author>
</authors>
<title>Distributional clustering of English words.</title>
<date>1993</date>
<booktitle>In Proc. of the Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="4020" citStr="Pereira et al., 1993" startWordPosition="607" endWordPosition="610">the individual words. Relying on this result, we will use frequency based estimation (using word frequencies) as representative for smoothing estimates of unobserved cooccurrences, for comparison purposes. As will be shown later, the problem with smoothing estimates is that they ignore the expected degree of association between the specific words of the cooccurrence. For example, we would not like to estimate the same probability for two cooccurrences like &apos;eat bread&apos; and &apos;eat cars&apos;, despite the fact that both &apos;bread&apos; and &apos;cars&apos; may have the same frequency. Class based models (Brown et al., ; Pereira et al., 1993; Hirschman, 1986; Resnik, 1992) distinguish between unobserved cooccurrences using classes of &amp;quot;similar&amp;quot; words. The probability of a specific cooccurrence is determined using generalized parameters about the probability of class cooccur164 rence. This approach, which follows long traditions in semantic classification, is very appealing, as it attempts to capture &amp;quot;typical&amp;quot; properties of classes of words. However, it is not clear at all that unrestricted language is indeed structured the way it is assumed by class based models. In particular, it is not clear that word cooccurrence patterns can b</context>
</contexts>
<marker>Pereira, Tishby, Lee, 1993</marker>
<rawString>Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993. Distributional clustering of English words. In Proc. of the Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Wordnet and distributional analysis: A class-based approach to lexical discovery.</title>
<date>1992</date>
<booktitle>In AAAI Workshop on Statisticallybased Natural Language Processing Techniques,</booktitle>
<contexts>
<context position="4052" citStr="Resnik, 1992" startWordPosition="613" endWordPosition="614">esult, we will use frequency based estimation (using word frequencies) as representative for smoothing estimates of unobserved cooccurrences, for comparison purposes. As will be shown later, the problem with smoothing estimates is that they ignore the expected degree of association between the specific words of the cooccurrence. For example, we would not like to estimate the same probability for two cooccurrences like &apos;eat bread&apos; and &apos;eat cars&apos;, despite the fact that both &apos;bread&apos; and &apos;cars&apos; may have the same frequency. Class based models (Brown et al., ; Pereira et al., 1993; Hirschman, 1986; Resnik, 1992) distinguish between unobserved cooccurrences using classes of &amp;quot;similar&amp;quot; words. The probability of a specific cooccurrence is determined using generalized parameters about the probability of class cooccur164 rence. This approach, which follows long traditions in semantic classification, is very appealing, as it attempts to capture &amp;quot;typical&amp;quot; properties of classes of words. However, it is not clear at all that unrestricted language is indeed structured the way it is assumed by class based models. In particular, it is not clear that word cooccurrence patterns can be structured and generalized to </context>
</contexts>
<marker>Resnik, 1992</marker>
<rawString>Philip Resnik. 1992. Wordnet and distributional analysis: A class-based approach to lexical discovery. In AAAI Workshop on Statisticallybased Natural Language Processing Techniques, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Sadler</author>
</authors>
<title>Working with analogical semantics: Disambiguation techniques in DLT.</title>
<date>1989</date>
<publisher>Foris Publications.</publisher>
<contexts>
<context position="1662" citStr="Sadler, 1989" startWordPosition="235" endWordPosition="236"> in many corpus based approaches for natural language processing. Different types of cooccurrence relations are in use, such as cooccurrence within a consecutive sequence of words (n-grams), within syntactic relations (verb-object, adjective-noun, etc.) or the cooccurrence of two words within a limited distance in the context. Statistical data about these various cooccurrence relations is employed for a variety of applications, such as speech recognition (Jelinek, 1990), language generation (Smadja and McKeown, 1990), lexicography (Church and Hanks, 1990), machine translation (Brown et al., ; Sadler, 1989), information retrieval (Maarek and Smadja, 1989) and various disambiguation tasks (Dagan et al., 1991; Hindle and Rooth, 1991; Grishman et al., 1986; Dagan and Itai, 1990). A major problem for the above applications is how to estimate the probability of cooccurrences that were not observed in the training corpus. Due to data sparseness in unrestricted language, the aggregate probability of such cooccurrences is large and can easily get to 25% or more, even for a very large training corpus (Church and Mercer, 1992). Since applications often have to compare alternative hypothesized cooccurrence</context>
</contexts>
<marker>Sadler, 1989</marker>
<rawString>V. Sadler. 1989. Working with analogical semantics: Disambiguation techniques in DLT. Foris Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Smadj a</author>
<author>Katheleen McKeown</author>
</authors>
<title>Automatically extracting and representing collocations for language generation.</title>
<date>1990</date>
<booktitle>In Proc. of the Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="1571" citStr="a and McKeown, 1990" startWordPosition="218" endWordPosition="221">ass based models. 1 Introduction Statistical data on word cooccurrence relations play a major role in many corpus based approaches for natural language processing. Different types of cooccurrence relations are in use, such as cooccurrence within a consecutive sequence of words (n-grams), within syntactic relations (verb-object, adjective-noun, etc.) or the cooccurrence of two words within a limited distance in the context. Statistical data about these various cooccurrence relations is employed for a variety of applications, such as speech recognition (Jelinek, 1990), language generation (Smadja and McKeown, 1990), lexicography (Church and Hanks, 1990), machine translation (Brown et al., ; Sadler, 1989), information retrieval (Maarek and Smadja, 1989) and various disambiguation tasks (Dagan et al., 1991; Hindle and Rooth, 1991; Grishman et al., 1986; Dagan and Itai, 1990). A major problem for the above applications is how to estimate the probability of cooccurrences that were not observed in the training corpus. Due to data sparseness in unrestricted language, the aggregate probability of such cooccurrences is large and can easily get to 25% or more, even for a very large training corpus (Church and Me</context>
</contexts>
<marker>a, McKeown, 1990</marker>
<rawString>Frank Smadj a and Katheleen McKeown. 1990. Automatically extracting and representing collocations for language generation. In Proc. of the Annual Meeting of the ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>