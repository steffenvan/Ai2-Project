<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000008">
<title confidence="0.766113">
RNN-based Derivation Structure Prediction for SMT
</title>
<author confidence="0.886939">
Feifei Zhai, Jiajun Zhang, Yu Zhou and Chengqing Zong
</author>
<affiliation confidence="0.8868045">
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China
</affiliation>
<email confidence="0.929291">
{ffzhai, jjzhang, yzhou, cqzong}@nlpr.ia.ac.cn
</email>
<sectionHeader confidence="0.993307" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99991125">
In this paper, we propose a novel deriva-
tion structure prediction (DSP) model
for SMT using recursive neural network
(RNN). Within the model, two steps are
involved: (1) phrase-pair vector represen-
tation, to learn vector representations for
phrase pairs; (2) derivation structure pre-
diction, to generate a bilingual RNN that
aims to distinguish good derivation struc-
tures from bad ones. Final experimental
results show that our DSP model can sig-
nificantly improve the translation quality.
</bodyText>
<sectionHeader confidence="0.998424" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.954387416666667">
Derivation structure is important for SMT decod-
ing, especially for the translation model based
on nested structures of languages, such as BTG
(bracket transduction grammar) model (Wu, 1997;
Xiong et al., 2006), hierarchical phrase-based
model (Chiang, 2007), and syntax-based model
(Galley et al., 2006; Marcu et al., 2006; Liu et
al., 2006; Huang et al., 2006; Zhang et al., 2008;
Zhang et al., 2011; Zhai et al., 2013). In general,
derivation structure refers to the tuple that records
the used translation rules and their compositions
during decoding, just as Figure 1 shows.
Intuitively, a good derivation structure usually
yields a good translation, while bad derivations al-
ways result in bad translations. For example in
Figure 1, (a) and (b) are two different derivations
for Chinese sentence “� 4 t/1&apos;A *TT T �
W. Comparing the two derivations, (a) is more
reasonable and yields a better translation. How-
ever, (b) wrongly translates phrase “&apos;j t/I&apos;A” to
“and Sharon” and combines it with [--�iff;Bush]
incorrectly, leading to a bad translation.
To explore the derivation structure’s potential
on yielding good translations, in this paper, we
propose a novel derivation structure prediction
(DSP) model for SMT decoding.
Figure 1: Two different derivation structures of
BTG translation model. In the structure, leaf
nodes denote the used translation rules. For each
node, the first line is the source string, while the
second line is its corresponding translation.
The proposed DSP model is built on recur-
sive neural network (RNN). Within the model,
two steps are involved: (1) phrase-pair vector
representation, to learn vector representations for
phrase pairs; (2) derivation structure prediction,
to build a bilingual RNN that aims to distinguish
good derivation structures from bad ones. Ex-
tensive experiments show that the proposed DSP
model significantly improves the translation qual-
ity, and thus verify the effectiveness of derivation
structure on indicating good translations.
We make the following contributions in this
work:
• We propose a novel RNN-based model to do
derivation structure prediction for SMT de-
coding. To our best knowledge, this is the
first work on this issue in SMT community;
</bodyText>
<listItem confidence="0.954629714285714">
• In current work, RNN has only been verified
to be useful on monolingual structure learn-
ing (Socher et al., 2011a; Socher et al., 2013).
We go a step further, and design a bilingual
RNN to represent the derivation structure;
• To train the RNN-based DSP model, we pro-
pose a max-margin objective that prefers gold
</listItem>
<bodyText confidence="0.769958333333333">
derivations yielded by forced decoding to
n-best derivations generated by the conven-
tional BTG translation model.
</bodyText>
<figure confidence="0.997782545454545">
布什
Bush
与 沙龙
举行了 会谈
with Sharon
布什
Bush
and Sharon
held a talk
held a talk
(a) (b)
与 沙龙
举行 了 会谈
布什
Bush
与 沙龙
with Sharon
held a talk
与 沙龙
举行 了 会谈
with Sharon
举行 了 会谈
held a talk
布什
Bush
Bush and
布什
与 沙龙
与 沙龙
and Sharon
Sharon
举行 了 会谈
held a talk
</figure>
<page confidence="0.977144">
779
</page>
<bodyText confidence="0.5421045">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 779–784,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</bodyText>
<sectionHeader confidence="0.991469" genericHeader="method">
2 The DSP Model
</sectionHeader>
<bodyText confidence="0.999791571428571">
The basic idea of DSP model is to represent the
derivation structure by RNN (Figure 2). Here, we
build the DSP model for BTG translation model,
which is naturally compatible with RNN. We be-
lieve that the DSP model is also beneficial to other
translation models. We leave them as our future
work.
</bodyText>
<subsectionHeader confidence="0.998672">
2.1 Phrase-Pair Vector Representation
</subsectionHeader>
<bodyText confidence="0.99994804">
Phrase pairs, i.e., the used translation rules, are the
leaf nodes of derivation structure. Hence, to repre-
sent the derivation structure by RNN, we need first
to represent the phrase pairs. To do this, we use
two unsupervised recursive autoencoders (RAE)
(Socher et al., 2011b), one for the source phrase
and the other for the target phrase. We call the unit
of the two RAEs the Leaf Node Network (LNN).
Using n-dimension word embedding, RAE can
learn a n-dimension vector for any phrase. Mean-
while, RAE will build a binary tree for the phrase,
as Figure 2 (in box) shows, and compute a re-
construction error to evaluate the vector. We use
E(Tph) to denote the reconstruction error given by
RAE, where ph is the phrase and Tph is the corre-
sponding binary tree. In RAE, higher error corre-
sponds to worse vector. More details can be found
in (Socher et al., 2011b).
Given a phrase pair (sp, tp), we can use LNN
to generate two n-dimension vectors, representing
sp and tp respectively. Then, we concatenate the
two vectors directly, and get a vector r E R2n to
represent phrase pair (sp, tp) (shown in Figure
2). The vector r is evaluated by combining the
reconstruction error on both sides:
</bodyText>
<equation confidence="0.994294">
1 E(Tsp, Ttp) = 2 [E (Tsp) + E(Ttp) - Nt ] (1)
</equation>
<bodyText confidence="0.99983175">
where Tsp and Ttp are the binary trees for sp and
tp. Ns and Nt denote the number of nodes in Tsp
and Ttp. Note that in order to unify the errors on
the two sides, we use ratio Ns/Nt to eliminate the
influence of phrase length.
Then, according to Equation (1), we compute
an LNN score to evaluate the vector of all phrase
pairs, i.e., leaf nodes, in derivation d:
</bodyText>
<equation confidence="0.971417">
J:
LNN(d) = − (sp,tp) E(Tsp, Ttp) (2)
</equation>
<bodyText confidence="0.974624">
where (sp, tp) is the used phrase pair in derivation
d. Obviously, the derivation with better phrase-
pair representations will get a higher LNN score.
</bodyText>
<figureCaption confidence="0.939781">
Figure 2: Illustration of DSP model, based on the
derivation structure in Figure 1(a).
</figureCaption>
<bodyText confidence="0.9977755">
The LNN score will serve as part of the DSP
model for predicting good derivation structures.
</bodyText>
<subsectionHeader confidence="0.999953">
2.2 Derivation Structure Prediction
</subsectionHeader>
<bodyText confidence="0.999710111111111">
Using the vector representations of phrase pairs,
we then build a Derivation Structure Network
(DSN) for prediction (Figure 2).
In DSN, the derivation structure is repre-
sented by repeatedly applying unit neural net-
work (UNN, Figure 3) at each non-leaf node. The
UNN receives two node vectors r1 E R2n and
r2 E R2n as input, and induces a vector p E R2n
to represent the parent node.
</bodyText>
<figureCaption confidence="0.998127">
Figure 3: The unit neural network used in DSN.
</figureCaption>
<bodyText confidence="0.999291">
For example, in Figure 2, node [14 t/I&apos;A; with
Sharon] serves as the first child with vector r1,
and node [*TT _T moi ,; held a talk] as the second
child with vector r2. The parent node vector p,
representing [4 t/I&apos;A *IT T 1,�-*; held a talk
with Sharon], is computed by merging r1 and r2:
</bodyText>
<equation confidence="0.989139">
p = f(WUNN[r1; r2] + bUNN) (3)
</equation>
<bodyText confidence="0.999858166666667">
where [r1; r2] E R4nx1 is the concatenation of r1
and r2, WUNN E R2nx4n and bUNN E R2nx1 are
the network’s parameter weight matrix and bias
term respectively. We use tanh(-) as function f.
Then, we compute a local score using a simple
inner product with a row vector Wscore
</bodyText>
<equation confidence="0.997785333333333">
UNN E R1x2n:
s(p) = Wscore
UNN - p (4)
</equation>
<bodyText confidence="0.9997498">
The score measures how well the two child nodes
r1 and r2 are merged into the parent node p.
As we all know, in BTG derivations, we have
two different ways to merge translation candi-
dates, monotone or inverted, meaning that we
</bodyText>
<figure confidence="0.978592857142857">
;N*
Bush
$ ffijt with Sharon
#h 7 held a talk
score
p
r1 r2
</figure>
<page confidence="0.976327">
780
</page>
<bodyText confidence="0.999737222222222">
merge two candidates in a monotone or inverted
order. We believe that different merging or-
der (monotone or inverted) needs different UNN.
Hence, we keep two different ones in DSN, one for
monotone order (with parameter Wmono, bmono,
and W score
mono), and the other for inverted (with pa-
rameter Winv, binv, and W score
inv ). The idea is that
the merging order of the two candidates will de-
termine which UNN will be used to generate their
parent’s vector and compute the score in Equa-
tion (4). Using a set of gold derivations, we can
train the network so that correct order will receive
a high score by Equation (4) and incorrect one will
receive a low score.
Thus, when we merge the candidates of two ad-
jacent spans during BTG-based decoding, the lo-
cal score in Equation (4) is useful in two aspects:
(1) for the same merging order, it evaluates how
well the two candidates are merged; (2) for the dif-
ferent order, it compares the candidates generated
by monotone order and inverted order.
Further, to assess the entire derivation structure,
we apply UNN to each node recursively, until the
root node. The final score utilized for derivation
structure prediction is the sum of all local scores:
</bodyText>
<equation confidence="0.998342">
X
DSN(d) = p s(p) (5)
</equation>
<bodyText confidence="0.999983">
where d denotes the derivation structure and p is
the non-leaf node in d. Obviously, by this score,
we can easily assess different derivations. Good
derivations will get higher scores while bad ones
will get lower scores.
Li et al. (2013) presented a network to predict
how to merge translation candidates, in monotone
or inverted order. Our DSN differs from Li’s work
in two points. For one thing, DSN can not only
predict how to merge candidates, but also evaluate
whether two candidates should be merged. For an-
other, DSN focuses on the entire derivation struc-
ture, rather than only the two candidates for merg-
ing. Therefore, the translation decoder will pursue
good derivation structures via DSN. Actually, Li’s
work can be easily integrated into our work. We
leave it as our future work.
</bodyText>
<sectionHeader confidence="0.994024" genericHeader="method">
3 Training
</sectionHeader>
<bodyText confidence="0.9996134">
In this section, we present the method of training
the DSP model. The parameters involved in this
process include: word embedding, parameters of
the two unsupervised RAEs in LNN, and parame-
ters in DSN.
</bodyText>
<subsectionHeader confidence="0.963473">
3.1 Max-Margin Framework
</subsectionHeader>
<bodyText confidence="0.999955333333333">
In DSP model, our goal is to assign higher scores
to gold derivations, and lower scores to bad ones.
To reach this goal, we adopt a max-margin frame-
work (Socher et al., 2010; Socher et al., 2011a;
Socher et al., 2013) for training.
Specifically, suppose we have a training data
like (ui, G(ui), A(ui)), where ui is the input
source sentence, G(ui) is the gold derivation set
containing all gold derivations of ui1, and A(ui)
is the possible derivation set that contains all
possible derivations of ui. We want to minimize
the following regularized risk function:
</bodyText>
<equation confidence="0.980269285714286">
λ
Ri(θ) + 2 k θ k2, where
Ri (θ) = ^max (s(θ, ui, d� + Δ (ˆd, G(ui)))
dEA(ui)
~sθ,ui,d~~
− max
dEg(ui)
</equation>
<bodyText confidence="0.97533375">
Here, θ is the model parameter. s(θ, ui, d) is the
DSP score for sentence ui’s derivation d. It is
computed by summing LNN score (Equation (2))
and DSN score (Equation (5)):
</bodyText>
<equation confidence="0.999473">
s(θ,u,d) = LNNθ(d) + DSNθ(d) (7)
</equation>
<bodyText confidence="0.947055">
Δ(ˆd, G(ui)) is the structure loss margin, which
penalizes derivation dˆ more if it deviates more
from gold derivations. It is formulated as:
</bodyText>
<equation confidence="0.589426666666667">
X=πE d^ αsδ{π ∈6 G(ui)} + αtDist(y(
is the source span in derivation
is an
</equation>
<bodyText confidence="0.660879473684211">
indicator function. We use the first part to count
the number of source spans in derivation
but
not in gold derivations. The second part is for
target side.
ref) computes the edit-
distance between the translation result
de-
fined by derivation
and the reference translation
ref. Obviously, this margin can effectively esti-
mate the difference between derivation
and gold
derivations, both on source side and target side.
Note that
and
are only two hyperparameters
for scaling. They are independent of each other,
and we set
</bodyText>
<equation confidence="0.884898785714286">
= 0.1 an
π
ˆd,δ{·}
ˆd,
Dist(y(ˆd),
y(ˆd)
dˆ
dˆ
αs
αt
αs
d αt = 0.1 respectively.
Δ(ˆd, G(ui))
ˆd), ref) (8)
</equation>
<bodyText confidence="0.86327625">
The margin includes two parts. For the first part,
investigate the general case here and suppose that
one sentence could have several different gold derivations.In
the experiment, we only use one gold deri
</bodyText>
<footnote confidence="0.4629245">
1We
vation for simple
</footnote>
<equation confidence="0.910352571428571">
implementation.
XN
1
J(θ) =
N
i=1
(6)
</equation>
<page confidence="0.969646">
781
</page>
<subsectionHeader confidence="0.995823">
3.2 Learning
</subsectionHeader>
<bodyText confidence="0.9999425">
As the risk function, Equation (6) is not differ-
entiable. We train the model via the subgradient
method (Ratliff et al., 2007; Socher et al., 2013).
For parameter e, the subgriadient of J(e) is:
</bodyText>
<equation confidence="0.661323">
as(e, ui, dm) +Ae
ae
</equation>
<bodyText confidence="0.9522302">
where ˆdm is the derivation with the highest DSP
score, and dm denotes the gold derivation with the
highest DSP score. We adopt the diagonal vari-
ant of AdaGrad (Duchi et al., 2011; Socher et al.,
2013) to minimize the risk function for training.
</bodyText>
<subsectionHeader confidence="0.994797">
3.3 Training Instances Collection
</subsectionHeader>
<bodyText confidence="0.999988222222222">
In order to train the model, we need to collect the
gold derivation set G(ui) and possible derivation
set A(ui) for input sentence ui.
For G(ui) , we define it by force decoding
derivation (FDD). Basically, FDD refers to the
derivation that produces the exact reference trans-
lation (single reference in our training data). For
example, since “Bush held a talk with Sharon” is
the reference of test sentence “� 4 t/11A �
IT _T�”, then Figure 1(a) is one of the FDDs.
As FDD can produce reference translation, we be-
lieve that FDD is of high quality, and take them as
gold derivations for training.
For A(ui), it should contain all possible deriva-
tions of ui. However, it is too difficult to obtain
all derivations. Thus, we use n-best derivations of
SMT decoding to simulate the complete derivation
space, and take them as the derivations in A(ui).
</bodyText>
<sectionHeader confidence="0.987256" genericHeader="method">
4 Integrating the DSP Model into SMT
</sectionHeader>
<bodyText confidence="0.99998">
To integrate the DSP model into decoding, we take
it (named DSP feature) as one of the features in the
log-linear framework of SMT. During decoding,
the DSP feature is distributed to each node in the
derivation structure. For the leaf node, the score
in Equation (2), i.e., LNN score, serves as the fea-
ture. For the non-leaf node, Equation (4) plays
the role. In order to give positive feature value to
the log-linear framework (for logarithm), we nor-
malize the DSP scores to [0,1] during decoding.
Due to the length limit, we ignore the specific nor-
malization methods here. We just preform some
simple transformations (such as adding a constant,
computing reciprocal), and convert the scores pro-
portionally to [0,1] at last.
</bodyText>
<sectionHeader confidence="0.999473" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.995395">
5.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.980196323529412">
To verify the effectiveness of our DSP model, we
perform experiments on Chinese-to-English trans-
lation. The training data contains about 2.1M sen-
tence pairs with about 27.7M Chinese words and
31.9M English words2. We train a 5-gram lan-
guage model by the Xinhua portion of Gigaword
corpus and the English part of the training data.
We obtain word alignment by GIZA++, and adopt
the grow-diag-final-and strategy to generate the
symmetric alignment. We use NIST MT 2003 data
as the development set, and NIST MT04-083 as
the test set. We use MERT (Och, 2004) to tune pa-
rameters. The translation quality is evaluated by
case-insensitive BLEU-4 (Papineni et al., 2002).
The statistical significance test is performed by
the re-sampling approach (Koehn, 2004). The
baseline system is our in-house BTG system (Wu,
1997; Xiong et al., 2006; Zhang and Zong, 2009).
To train the DSP model, we first use Word2Vec4
toolkit to pre-train the word embedding on large-
scale monolingual data. The used monolingual
data contains about 1.06B words for Chinese and
1.12B words for English. The dimensionality of
our vectors is 50. The detiled training process is
as follows:
(1) Using the BTG system to perform force de-
coding on FBIS part of the bilingual training data5,
and collect the sentences succeeded in force de-
coding (86,902 sentences in total)6. We then col-
lect the corresponding force decoding derivations
as gold derivations. Here, we only use the best
force decoding derivation for simple implementa-
tion. In future, we will try to use multiple force
decoding derivations for training.
</bodyText>
<listItem confidence="0.999748">
(2) Collecting the bilingual phrases in the leaf
nodes of gold derivations. We train LNN by these
phrases via L-BFGS algorithm. Finally, we get
351,448 source phrases to train the source side
RAE and 370,948 target phrases to train the tar-
get side RAE.
</listItem>
<footnote confidence="0.9866921">
2LDC category number : LDC2000T50, LDC2002E18,
LDC2003E07, LDC2004T07, LDC2005T06, LDC2002L27,
LDC2005T10 and LDC2005T34.
3For MT06 and MT08, we only use the part of news data.
4https://code.google.com/p/word2vec/
5Here we only use the high quality corpus FBIS to guar-
antee the quality of force decoding derivation.
6Many sentence pairs fail in forced decoding due to many
reasons, such as reordering limit, noisy alignment, and phrase
length limit (Yu et al., 2013).
</footnote>
<figure confidence="0.777029">
as(e, ui, ˆdm)
ae
aJ 1 N
ae = i
</figure>
<page confidence="0.986742">
782
</page>
<bodyText confidence="0.7287334">
(3) Decoding the 86902 sentences by the BTG
system to get n-best translations and correspond-
ing derivations. The n-best derivations are used to
simulate the entire derivation space. We retain at
most 200-best derivations for each sentence.
</bodyText>
<listItem confidence="0.962874666666667">
(4) Leveraging force decoding derivations and
n-best derivations to train the DSP model. Note
that all parameters, including word embedding and
parameters in LNN and DSN, are tuned together in
this step. It takes about 15 hours to train the entire
network using a 16-core, 2.9 GHz Xeon machine.
</listItem>
<subsectionHeader confidence="0.999266">
5.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999456888888889">
We compare baseline BTG system and the DSP-
augmented BTG system in this section. The final
translation results are shown in Table 1.
After integrating the DSP model into BTG sys-
tem, we get significant improvement on all test
sets, about 1.0 BLEU points over BTG system on
average. This comparison strongly demonstrates
that our DSP model is useful and will be a good
complement to current translation models.
</bodyText>
<table confidence="0.9972175">
Systems BLEU(%)
MT04 MT05 MT06 MT08 Aver
BTG 36.91 34.69 33.83 27.17 33.15
BTG+DSP 37.41 35.77 35.08 28.42 34.17
</table>
<tableCaption confidence="0.995893">
Table 1: Final translation results. Bold numbers
</tableCaption>
<bodyText confidence="0.999365304347826">
denote that the result is significantly better than
baseline BTG system (p &lt; 0.05). Column “Aver”
gives the average BLEU points of the 4 test sets.
To have a better intuition for the effectiveness
of our DSP model, we give a case study in Figure
4. It depicts two derivations built by BTG system
and BTG+DSP system respectively.
From Figure 4(b), we can see that BTG system
yields a bad translation due to the bad derivation
structure. In the figure, BTG system makes three
mistakes. It attaches candidates [)AVG; achieve-
ments], [off �_kYi1 M; has reached] and [�tH·;
singapore] to the big candidate [T,Rr&amp;quot; R AA A
WAA,; cannot be regarded as a natural]. Conse-
quently, the noun phrase “O�· Pfi �_kY�l M A
VG” is translated separately, rather than as a whole,
leading to a bad translation.
Differently, the DSP model is designed for pre-
dicting good derivations. In Figure 4(c), the used
translation rules are actually similar to Figure 4(b).
However, under a better guidance to build good
derivation structure, BTG+DSP system generates
a much better translation result than BTG system.
</bodyText>
<figure confidence="0.994901285714286">
(a) the example test sentence and its corresponding reference.
(b) an example derivation structure generated by BTG system
新加坡 所 达到 的 成就 不 能 被 当作 理所当然
the achievements attained by singapore cannot be regarded as a natural
新加坡 所 达到 的 成就
the achievements attained by singapore
(c) an example derivation structure generated by the DSP+BTG system
</figure>
<figureCaption confidence="0.999634">
Figure 4: Different derivation structures.
</figureCaption>
<sectionHeader confidence="0.989927" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.987939333333333">
In this paper, we explored the method of derivation
structure prediction for SMT. To fulfill this task,
we have made several major efforts as follows:
</bodyText>
<listItem confidence="0.980931166666667">
(1) We propose a novel derivation structure pre-
diction model based on RNN, including two close
and interactive parts: LNN and DSN.
(2) We extend monolingual RNN to bilingual
RNN to represent the derivation structure.
(3) We train LNN and DSN by derivations from
</listItem>
<bodyText confidence="0.90654375">
force decoding. In this way, the DSP model learns
a preference to good derivation structures.
Experimental results show that the proposed
DSP model improves the translation performance
significantly. By this, we verify the effectiveness
of derivation structure on indicating good trans-
lations. We believe that our work will shed new
lights to SMT decoding.
</bodyText>
<sectionHeader confidence="0.958822" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.984044">
We would like to thank the three anonymous re-
viewers for their valuable comments and sugges-
tions. The research work has been partially funded
by the Natural Science Foundation of China under
Grant No. 61333018 and 61303181, and the Key
Project of Knowledge Innovation Program of Chi-
nese Academy of Sciences.
</bodyText>
<figure confidence="0.997926465116279">
新加坡
xinjiapo
所 达到 的
suo dadao de
成就
chengjiu
不 能 被 当作
bu neng bei dangzuo
理所当然
lisuodangran
the achievements singapore reached cannot be taken for granted
新加坡 所 达到 的 成就 不 能 被 当作 理所当然
singapore has reached achievements cannot be regarded as a natural
新加坡
singapore
所 达到 的
has reached
所 达到 的 成就 不 能 被 当作 理所当然
has reached achievements cannot be regarded as a natural
成就
achievements
成就 不 能 被 当作 理所当然
achievements cannot be regarded as a natural
不 能 被 当作
cannot be regarded as a
不 能 被 当作 理所当然
cannot be regarded as a natural
理所当然
natural
不 能 被 当作 理所当然
cannot be regarded as a natural
不 能 被 当作
cannot be regarded as a
理所当然
natural
新加坡
singapore
新加坡 所 达到 的
attained by singapore
所 达到 的
attained by
成就
the achievements
</figure>
<page confidence="0.995288">
783
</page>
<sectionHeader confidence="0.983209" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999893386792453">
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201–228.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research, 12:2121–2159.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 961–968, Sydney, Australia, July. Asso-
ciation for Computational Linguistics.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
A syntax-directed translator with extended domain
of locality. In Proceedings of AMTA.
Peng Li, Yang Liu, and Maosong Sun. 2013. Recur-
sive autoencoders for itg-based translation. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 609–616, Sydney, Aus-
tralia, July. Association for Computational Linguis-
tics.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. Spmt: Statistical machine
translation with syntactified target language phrases.
In Proceedings of the 2006 Conference on Em-
pirical Methods in Natural Language Processing,
pages 44–52, Sydney, Australia, July. Association
for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th annual meeting on association for compu-
tational linguistics, pages 311–318. Association for
Computational Linguistics.
Nathan D Ratliff, J Andrew Bagnell, and Martin A
Zinkevich. 2007. (online) subgradient methods
for structured prediction. In Eleventh International
Conference on Artificial Intelligence and Statistics
(AIStats).
Richard Socher, Christopher D Manning, and An-
drew Y Ng. 2010. Learning continuous phrase
representations and syntactic parsing with recursive
neural networks. In Proceedings of the NIPS-2010
Deep Learning and Unsupervised Feature Learning
Workshop.
Richard Socher, Cliff C Lin, Chris Manning, and An-
drew Y Ng. 2011a. Parsing natural scenes and nat-
ural language with recursive neural networks. In
Proceedings of the 28th International Conference on
Machine Learning (ICML-11), pages 129–136.
Richard Socher, Jeffrey Pennington, Eric H Huang,
Andrew Y Ng, and Christopher D Manning. 2011b.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 151–161.
Richard Socher, John Bauer, Christopher D Manning,
and Andrew Y Ng. 2013. Parsing with composi-
tional vector grammars. In Proceedings of ACL.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational linguistics, 23(3):377–403.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum entropy based phrase reordering model for sta-
tistical machine translation. In Proceedings of ACL-
COLING, pages 505–512.
Heng Yu, Liang Huang, Haitao Mi, and Kai Zhao.
2013. Max-violation perceptron and forced decod-
ing for scalable MT training. In Proceedings of
the 2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1112–1123, Seattle,
Washington, USA, October. Association for Compu-
tational Linguistics.
Feifei Zhai, Jiajun Zhang, Yu Zhou, and Chengqing
Zong. 2013. Unsupervised tree induction for tree-
based translation. Transactions of Association for
Computational Linguistics(TACL), pages 291–300.
Jiajun Zhang and Chengqing Zong. 2009. A frame-
work for effectively integrating hard and soft syn-
tactic rules into phrase based translation. In Pro-
ceedings of the 23rd Pacific Asia Conference on Lan-
guage, Information and Computation, pages 579–
588, Hong Kong, December. City University of
Hong Kong.
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,
Chew Lim Tan, and Sheng Li. 2008. A tree
sequence alignment-based tree-to-tree translation
model. In Proceedings ofACL-08: HLT, pages 559–
567, Columbus, Ohio, June. Association for Compu-
tational Linguistics.
Jiajun Zhang, Feifei Zhai, and Chengqing Zong. 2011.
Augmenting string-to-tree translation models with
fuzzy use of source-side syntax. In Proceedings of
the 2011 Conference on Empirical Methods in Nat-
ural Language Processing, pages 204–215, Edin-
burgh, Scotland, UK., July. Association for Compu-
tational Linguistics.
</reference>
<page confidence="0.998326">
784
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.375867">
<title confidence="0.999177">RNN-based Derivation Structure Prediction for SMT</title>
<author confidence="0.593567">Feifei Zhai</author>
<author confidence="0.593567">Jiajun Zhang</author>
<author confidence="0.593567">Yu Zhou</author>
<author confidence="0.593567">Chengqing</author>
<affiliation confidence="0.956759">National Laboratory of Pattern</affiliation>
<address confidence="0.660559">Institute of Automation, Chinese Academy of Sciences, Beijing, 100190,</address>
<email confidence="0.953413">jjzhang,yzhou,</email>
<abstract confidence="0.998057">In this paper, we propose a novel derivation structure prediction (DSP) model for SMT using recursive neural network (RNN). Within the model, two steps are involved: (1) phrase-pair vector representation, to learn vector representations for phrase pairs; (2) derivation structure prediction, to generate a bilingual RNN that aims to distinguish good derivation structures from bad ones. Final experimental results show that our DSP model can significantly improve the translation quality.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="1041" citStr="Chiang, 2007" startWordPosition="147" endWordPosition="148">eps are involved: (1) phrase-pair vector representation, to learn vector representations for phrase pairs; (2) derivation structure prediction, to generate a bilingual RNN that aims to distinguish good derivation structures from bad ones. Final experimental results show that our DSP model can significantly improve the translation quality. 1 Introduction Derivation structure is important for SMT decoding, especially for the translation model based on nested structures of languages, such as BTG (bracket transduction grammar) model (Wu, 1997; Xiong et al., 2006), hierarchical phrase-based model (Chiang, 2007), and syntax-based model (Galley et al., 2006; Marcu et al., 2006; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2008; Zhang et al., 2011; Zhai et al., 2013). In general, derivation structure refers to the tuple that records the used translation rules and their compositions during decoding, just as Figure 1 shows. Intuitively, a good derivation structure usually yields a good translation, while bad derivations always result in bad translations. For example in Figure 1, (a) and (b) are two different derivations for Chinese sentence “� 4 t/1&apos;A *TT T � W. Comparing the two derivations, (a) </context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2121</pages>
<contexts>
<context position="12241" citStr="Duchi et al., 2011" startWordPosition="2115" endWordPosition="2118">te the general case here and suppose that one sentence could have several different gold derivations.In the experiment, we only use one gold deri 1We vation for simple implementation. XN 1 J(θ) = N i=1 (6) 781 3.2 Learning As the risk function, Equation (6) is not differentiable. We train the model via the subgradient method (Ratliff et al., 2007; Socher et al., 2013). For parameter e, the subgriadient of J(e) is: as(e, ui, dm) +Ae ae where ˆdm is the derivation with the highest DSP score, and dm denotes the gold derivation with the highest DSP score. We adopt the diagonal variant of AdaGrad (Duchi et al., 2011; Socher et al., 2013) to minimize the risk function for training. 3.3 Training Instances Collection In order to train the model, we need to collect the gold derivation set G(ui) and possible derivation set A(ui) for input sentence ui. For G(ui) , we define it by force decoding derivation (FDD). Basically, FDD refers to the derivation that produces the exact reference translation (single reference in our training data). For example, since “Bush held a talk with Sharon” is the reference of test sentence “� 4 t/11A � IT _T�”, then Figure 1(a) is one of the FDDs. As FDD can produce reference tran</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. The Journal of Machine Learning Research, 12:2121–2159.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>961--968</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="1086" citStr="Galley et al., 2006" startWordPosition="152" endWordPosition="155">r representation, to learn vector representations for phrase pairs; (2) derivation structure prediction, to generate a bilingual RNN that aims to distinguish good derivation structures from bad ones. Final experimental results show that our DSP model can significantly improve the translation quality. 1 Introduction Derivation structure is important for SMT decoding, especially for the translation model based on nested structures of languages, such as BTG (bracket transduction grammar) model (Wu, 1997; Xiong et al., 2006), hierarchical phrase-based model (Chiang, 2007), and syntax-based model (Galley et al., 2006; Marcu et al., 2006; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2008; Zhang et al., 2011; Zhai et al., 2013). In general, derivation structure refers to the tuple that records the used translation rules and their compositions during decoding, just as Figure 1 shows. Intuitively, a good derivation structure usually yields a good translation, while bad derivations always result in bad translations. For example in Figure 1, (a) and (b) are two different derivations for Chinese sentence “� 4 t/1&apos;A *TT T � W. Comparing the two derivations, (a) is more reasonable and yields a better transl</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 961–968, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kevin Knight</author>
<author>Aravind Joshi</author>
</authors>
<title>A syntax-directed translator with extended domain of locality.</title>
<date>2006</date>
<booktitle>In Proceedings of AMTA.</booktitle>
<contexts>
<context position="1144" citStr="Huang et al., 2006" startWordPosition="164" endWordPosition="167">e pairs; (2) derivation structure prediction, to generate a bilingual RNN that aims to distinguish good derivation structures from bad ones. Final experimental results show that our DSP model can significantly improve the translation quality. 1 Introduction Derivation structure is important for SMT decoding, especially for the translation model based on nested structures of languages, such as BTG (bracket transduction grammar) model (Wu, 1997; Xiong et al., 2006), hierarchical phrase-based model (Chiang, 2007), and syntax-based model (Galley et al., 2006; Marcu et al., 2006; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2008; Zhang et al., 2011; Zhai et al., 2013). In general, derivation structure refers to the tuple that records the used translation rules and their compositions during decoding, just as Figure 1 shows. Intuitively, a good derivation structure usually yields a good translation, while bad derivations always result in bad translations. For example in Figure 1, (a) and (b) are two different derivations for Chinese sentence “� 4 t/1&apos;A *TT T � W. Comparing the two derivations, (a) is more reasonable and yields a better translation. However, (b) wrongly translates phrase “&apos;j t/I&apos;A” t</context>
</contexts>
<marker>Huang, Knight, Joshi, 2006</marker>
<rawString>Liang Huang, Kevin Knight, and Aravind Joshi. 2006. A syntax-directed translator with extended domain of locality. In Proceedings of AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peng Li</author>
<author>Yang Liu</author>
<author>Maosong Sun</author>
</authors>
<title>Recursive autoencoders for itg-based translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="9052" citStr="Li et al. (2013)" startWordPosition="1559" endWordPosition="1562">luates how well the two candidates are merged; (2) for the different order, it compares the candidates generated by monotone order and inverted order. Further, to assess the entire derivation structure, we apply UNN to each node recursively, until the root node. The final score utilized for derivation structure prediction is the sum of all local scores: X DSN(d) = p s(p) (5) where d denotes the derivation structure and p is the non-leaf node in d. Obviously, by this score, we can easily assess different derivations. Good derivations will get higher scores while bad ones will get lower scores. Li et al. (2013) presented a network to predict how to merge translation candidates, in monotone or inverted order. Our DSN differs from Li’s work in two points. For one thing, DSN can not only predict how to merge candidates, but also evaluate whether two candidates should be merged. For another, DSN focuses on the entire derivation structure, rather than only the two candidates for merging. Therefore, the translation decoder will pursue good derivation structures via DSN. Actually, Li’s work can be easily integrated into our work. We leave it as our future work. 3 Training In this section, we present the me</context>
</contexts>
<marker>Li, Liu, Sun, 2013</marker>
<rawString>Peng Li, Yang Liu, and Maosong Sun. 2013. Recursive autoencoders for itg-based translation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Treeto-string alignment template for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>609--616</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="1124" citStr="Liu et al., 2006" startWordPosition="160" endWordPosition="163">ntations for phrase pairs; (2) derivation structure prediction, to generate a bilingual RNN that aims to distinguish good derivation structures from bad ones. Final experimental results show that our DSP model can significantly improve the translation quality. 1 Introduction Derivation structure is important for SMT decoding, especially for the translation model based on nested structures of languages, such as BTG (bracket transduction grammar) model (Wu, 1997; Xiong et al., 2006), hierarchical phrase-based model (Chiang, 2007), and syntax-based model (Galley et al., 2006; Marcu et al., 2006; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2008; Zhang et al., 2011; Zhai et al., 2013). In general, derivation structure refers to the tuple that records the used translation rules and their compositions during decoding, just as Figure 1 shows. Intuitively, a good derivation structure usually yields a good translation, while bad derivations always result in bad translations. For example in Figure 1, (a) and (b) are two different derivations for Chinese sentence “� 4 t/1&apos;A *TT T � W. Comparing the two derivations, (a) is more reasonable and yields a better translation. However, (b) wrongly translates</context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Treeto-string alignment template for statistical machine translation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 609–616, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>Wei Wang</author>
<author>Abdessamad Echihabi</author>
<author>Kevin Knight</author>
</authors>
<title>Spmt: Statistical machine translation with syntactified target language phrases.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>44--52</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="1106" citStr="Marcu et al., 2006" startWordPosition="156" endWordPosition="159">learn vector representations for phrase pairs; (2) derivation structure prediction, to generate a bilingual RNN that aims to distinguish good derivation structures from bad ones. Final experimental results show that our DSP model can significantly improve the translation quality. 1 Introduction Derivation structure is important for SMT decoding, especially for the translation model based on nested structures of languages, such as BTG (bracket transduction grammar) model (Wu, 1997; Xiong et al., 2006), hierarchical phrase-based model (Chiang, 2007), and syntax-based model (Galley et al., 2006; Marcu et al., 2006; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2008; Zhang et al., 2011; Zhai et al., 2013). In general, derivation structure refers to the tuple that records the used translation rules and their compositions during decoding, just as Figure 1 shows. Intuitively, a good derivation structure usually yields a good translation, while bad derivations always result in bad translations. For example in Figure 1, (a) and (b) are two different derivations for Chinese sentence “� 4 t/1&apos;A *TT T � W. Comparing the two derivations, (a) is more reasonable and yields a better translation. However, (b) </context>
</contexts>
<marker>Marcu, Wang, Echihabi, Knight, 2006</marker>
<rawString>Daniel Marcu, Wei Wang, Abdessamad Echihabi, and Kevin Knight. 2006. Spmt: Statistical machine translation with syntactified target language phrases. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 44–52, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th annual meeting on association for computational linguistics,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="14651" citStr="Papineni et al., 2002" startWordPosition="2522" endWordPosition="2525">odel, we perform experiments on Chinese-to-English translation. The training data contains about 2.1M sentence pairs with about 27.7M Chinese words and 31.9M English words2. We train a 5-gram language model by the Xinhua portion of Gigaword corpus and the English part of the training data. We obtain word alignment by GIZA++, and adopt the grow-diag-final-and strategy to generate the symmetric alignment. We use NIST MT 2003 data as the development set, and NIST MT04-083 as the test set. We use MERT (Och, 2004) to tune parameters. The translation quality is evaluated by case-insensitive BLEU-4 (Papineni et al., 2002). The statistical significance test is performed by the re-sampling approach (Koehn, 2004). The baseline system is our in-house BTG system (Wu, 1997; Xiong et al., 2006; Zhang and Zong, 2009). To train the DSP model, we first use Word2Vec4 toolkit to pre-train the word embedding on largescale monolingual data. The used monolingual data contains about 1.06B words for Chinese and 1.12B words for English. The dimensionality of our vectors is 50. The detiled training process is as follows: (1) Using the BTG system to perform force decoding on FBIS part of the bilingual training data5, and collect </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pages 311–318. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathan D Ratliff</author>
<author>J Andrew Bagnell</author>
<author>Martin A Zinkevich</author>
</authors>
<title>(online) subgradient methods for structured prediction.</title>
<date>2007</date>
<booktitle>In Eleventh International Conference on Artificial Intelligence and Statistics (AIStats).</booktitle>
<contexts>
<context position="11971" citStr="Ratliff et al., 2007" startWordPosition="2065" endWordPosition="2068"> Note that and are only two hyperparameters for scaling. They are independent of each other, and we set = 0.1 an π ˆd,δ{·} ˆd, Dist(y(ˆd), y(ˆd) dˆ dˆ αs αt αs d αt = 0.1 respectively. Δ(ˆd, G(ui)) ˆd), ref) (8) The margin includes two parts. For the first part, investigate the general case here and suppose that one sentence could have several different gold derivations.In the experiment, we only use one gold deri 1We vation for simple implementation. XN 1 J(θ) = N i=1 (6) 781 3.2 Learning As the risk function, Equation (6) is not differentiable. We train the model via the subgradient method (Ratliff et al., 2007; Socher et al., 2013). For parameter e, the subgriadient of J(e) is: as(e, ui, dm) +Ae ae where ˆdm is the derivation with the highest DSP score, and dm denotes the gold derivation with the highest DSP score. We adopt the diagonal variant of AdaGrad (Duchi et al., 2011; Socher et al., 2013) to minimize the risk function for training. 3.3 Training Instances Collection In order to train the model, we need to collect the gold derivation set G(ui) and possible derivation set A(ui) for input sentence ui. For G(ui) , we define it by force decoding derivation (FDD). Basically, FDD refers to the deri</context>
</contexts>
<marker>Ratliff, Bagnell, Zinkevich, 2007</marker>
<rawString>Nathan D Ratliff, J Andrew Bagnell, and Martin A Zinkevich. 2007. (online) subgradient methods for structured prediction. In Eleventh International Conference on Artificial Intelligence and Statistics (AIStats).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Learning continuous phrase representations and syntactic parsing with recursive neural networks.</title>
<date>2010</date>
<booktitle>In Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop.</booktitle>
<contexts>
<context position="10018" citStr="Socher et al., 2010" startWordPosition="1724" endWordPosition="1727"> the two candidates for merging. Therefore, the translation decoder will pursue good derivation structures via DSN. Actually, Li’s work can be easily integrated into our work. We leave it as our future work. 3 Training In this section, we present the method of training the DSP model. The parameters involved in this process include: word embedding, parameters of the two unsupervised RAEs in LNN, and parameters in DSN. 3.1 Max-Margin Framework In DSP model, our goal is to assign higher scores to gold derivations, and lower scores to bad ones. To reach this goal, we adopt a max-margin framework (Socher et al., 2010; Socher et al., 2011a; Socher et al., 2013) for training. Specifically, suppose we have a training data like (ui, G(ui), A(ui)), where ui is the input source sentence, G(ui) is the gold derivation set containing all gold derivations of ui1, and A(ui) is the possible derivation set that contains all possible derivations of ui. We want to minimize the following regularized risk function: λ Ri(θ) + 2 k θ k2, where Ri (θ) = ^max (s(θ, ui, d� + Δ (ˆd, G(ui))) dEA(ui) ~sθ,ui,d~~ − max dEg(ui) Here, θ is the model parameter. s(θ, ui, d) is the DSP score for sentence ui’s derivation d. It is computed</context>
</contexts>
<marker>Socher, Manning, Ng, 2010</marker>
<rawString>Richard Socher, Christopher D Manning, and Andrew Y Ng. 2010. Learning continuous phrase representations and syntactic parsing with recursive neural networks. In Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Cliff C Lin</author>
<author>Chris Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Parsing natural scenes and natural language with recursive neural networks.</title>
<date>2011</date>
<booktitle>In Proceedings of the 28th International Conference on Machine Learning (ICML-11),</booktitle>
<pages>129--136</pages>
<contexts>
<context position="3111" citStr="Socher et al., 2011" startWordPosition="475" endWordPosition="478">o build a bilingual RNN that aims to distinguish good derivation structures from bad ones. Extensive experiments show that the proposed DSP model significantly improves the translation quality, and thus verify the effectiveness of derivation structure on indicating good translations. We make the following contributions in this work: • We propose a novel RNN-based model to do derivation structure prediction for SMT decoding. To our best knowledge, this is the first work on this issue in SMT community; • In current work, RNN has only been verified to be useful on monolingual structure learning (Socher et al., 2011a; Socher et al., 2013). We go a step further, and design a bilingual RNN to represent the derivation structure; • To train the RNN-based DSP model, we propose a max-margin objective that prefers gold derivations yielded by forced decoding to n-best derivations generated by the conventional BTG translation model. 布什 Bush 与 沙龙 举行了 会谈 with Sharon 布什 Bush and Sharon held a talk held a talk (a) (b) 与 沙龙 举行 了 会谈 布什 Bush 与 沙龙 with Sharon held a talk 与 沙龙 举行 了 会谈 with Sharon 举行 了 会谈 held a talk 布什 Bush Bush and 布什 与 沙龙 与 沙龙 and Sharon Sharon 举行 了 会谈 held a talk 779 Proceedings of the 52nd Annual Meet</context>
<context position="4512" citStr="Socher et al., 2011" startWordPosition="725" endWordPosition="728">DSP Model The basic idea of DSP model is to represent the derivation structure by RNN (Figure 2). Here, we build the DSP model for BTG translation model, which is naturally compatible with RNN. We believe that the DSP model is also beneficial to other translation models. We leave them as our future work. 2.1 Phrase-Pair Vector Representation Phrase pairs, i.e., the used translation rules, are the leaf nodes of derivation structure. Hence, to represent the derivation structure by RNN, we need first to represent the phrase pairs. To do this, we use two unsupervised recursive autoencoders (RAE) (Socher et al., 2011b), one for the source phrase and the other for the target phrase. We call the unit of the two RAEs the Leaf Node Network (LNN). Using n-dimension word embedding, RAE can learn a n-dimension vector for any phrase. Meanwhile, RAE will build a binary tree for the phrase, as Figure 2 (in box) shows, and compute a reconstruction error to evaluate the vector. We use E(Tph) to denote the reconstruction error given by RAE, where ph is the phrase and Tph is the corresponding binary tree. In RAE, higher error corresponds to worse vector. More details can be found in (Socher et al., 2011b). Given a phra</context>
<context position="10039" citStr="Socher et al., 2011" startWordPosition="1728" endWordPosition="1731">or merging. Therefore, the translation decoder will pursue good derivation structures via DSN. Actually, Li’s work can be easily integrated into our work. We leave it as our future work. 3 Training In this section, we present the method of training the DSP model. The parameters involved in this process include: word embedding, parameters of the two unsupervised RAEs in LNN, and parameters in DSN. 3.1 Max-Margin Framework In DSP model, our goal is to assign higher scores to gold derivations, and lower scores to bad ones. To reach this goal, we adopt a max-margin framework (Socher et al., 2010; Socher et al., 2011a; Socher et al., 2013) for training. Specifically, suppose we have a training data like (ui, G(ui), A(ui)), where ui is the input source sentence, G(ui) is the gold derivation set containing all gold derivations of ui1, and A(ui) is the possible derivation set that contains all possible derivations of ui. We want to minimize the following regularized risk function: λ Ri(θ) + 2 k θ k2, where Ri (θ) = ^max (s(θ, ui, d� + Δ (ˆd, G(ui))) dEA(ui) ~sθ,ui,d~~ − max dEg(ui) Here, θ is the model parameter. s(θ, ui, d) is the DSP score for sentence ui’s derivation d. It is computed by summing LNN score</context>
</contexts>
<marker>Socher, Lin, Manning, Ng, 2011</marker>
<rawString>Richard Socher, Cliff C Lin, Chris Manning, and Andrew Y Ng. 2011a. Parsing natural scenes and natural language with recursive neural networks. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 129–136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Jeffrey Pennington</author>
<author>Eric H Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Semi-supervised recursive autoencoders for predicting sentiment distributions.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>151--161</pages>
<contexts>
<context position="3111" citStr="Socher et al., 2011" startWordPosition="475" endWordPosition="478">o build a bilingual RNN that aims to distinguish good derivation structures from bad ones. Extensive experiments show that the proposed DSP model significantly improves the translation quality, and thus verify the effectiveness of derivation structure on indicating good translations. We make the following contributions in this work: • We propose a novel RNN-based model to do derivation structure prediction for SMT decoding. To our best knowledge, this is the first work on this issue in SMT community; • In current work, RNN has only been verified to be useful on monolingual structure learning (Socher et al., 2011a; Socher et al., 2013). We go a step further, and design a bilingual RNN to represent the derivation structure; • To train the RNN-based DSP model, we propose a max-margin objective that prefers gold derivations yielded by forced decoding to n-best derivations generated by the conventional BTG translation model. 布什 Bush 与 沙龙 举行了 会谈 with Sharon 布什 Bush and Sharon held a talk held a talk (a) (b) 与 沙龙 举行 了 会谈 布什 Bush 与 沙龙 with Sharon held a talk 与 沙龙 举行 了 会谈 with Sharon 举行 了 会谈 held a talk 布什 Bush Bush and 布什 与 沙龙 与 沙龙 and Sharon Sharon 举行 了 会谈 held a talk 779 Proceedings of the 52nd Annual Meet</context>
<context position="4512" citStr="Socher et al., 2011" startWordPosition="725" endWordPosition="728">DSP Model The basic idea of DSP model is to represent the derivation structure by RNN (Figure 2). Here, we build the DSP model for BTG translation model, which is naturally compatible with RNN. We believe that the DSP model is also beneficial to other translation models. We leave them as our future work. 2.1 Phrase-Pair Vector Representation Phrase pairs, i.e., the used translation rules, are the leaf nodes of derivation structure. Hence, to represent the derivation structure by RNN, we need first to represent the phrase pairs. To do this, we use two unsupervised recursive autoencoders (RAE) (Socher et al., 2011b), one for the source phrase and the other for the target phrase. We call the unit of the two RAEs the Leaf Node Network (LNN). Using n-dimension word embedding, RAE can learn a n-dimension vector for any phrase. Meanwhile, RAE will build a binary tree for the phrase, as Figure 2 (in box) shows, and compute a reconstruction error to evaluate the vector. We use E(Tph) to denote the reconstruction error given by RAE, where ph is the phrase and Tph is the corresponding binary tree. In RAE, higher error corresponds to worse vector. More details can be found in (Socher et al., 2011b). Given a phra</context>
<context position="10039" citStr="Socher et al., 2011" startWordPosition="1728" endWordPosition="1731">or merging. Therefore, the translation decoder will pursue good derivation structures via DSN. Actually, Li’s work can be easily integrated into our work. We leave it as our future work. 3 Training In this section, we present the method of training the DSP model. The parameters involved in this process include: word embedding, parameters of the two unsupervised RAEs in LNN, and parameters in DSN. 3.1 Max-Margin Framework In DSP model, our goal is to assign higher scores to gold derivations, and lower scores to bad ones. To reach this goal, we adopt a max-margin framework (Socher et al., 2010; Socher et al., 2011a; Socher et al., 2013) for training. Specifically, suppose we have a training data like (ui, G(ui), A(ui)), where ui is the input source sentence, G(ui) is the gold derivation set containing all gold derivations of ui1, and A(ui) is the possible derivation set that contains all possible derivations of ui. We want to minimize the following regularized risk function: λ Ri(θ) + 2 k θ k2, where Ri (θ) = ^max (s(θ, ui, d� + Δ (ˆd, G(ui))) dEA(ui) ~sθ,ui,d~~ − max dEg(ui) Here, θ is the model parameter. s(θ, ui, d) is the DSP score for sentence ui’s derivation d. It is computed by summing LNN score</context>
</contexts>
<marker>Socher, Pennington, Huang, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Jeffrey Pennington, Eric H Huang, Andrew Y Ng, and Christopher D Manning. 2011b. Semi-supervised recursive autoencoders for predicting sentiment distributions. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 151–161.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>John Bauer</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Parsing with compositional vector grammars.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="3134" citStr="Socher et al., 2013" startWordPosition="479" endWordPosition="482">N that aims to distinguish good derivation structures from bad ones. Extensive experiments show that the proposed DSP model significantly improves the translation quality, and thus verify the effectiveness of derivation structure on indicating good translations. We make the following contributions in this work: • We propose a novel RNN-based model to do derivation structure prediction for SMT decoding. To our best knowledge, this is the first work on this issue in SMT community; • In current work, RNN has only been verified to be useful on monolingual structure learning (Socher et al., 2011a; Socher et al., 2013). We go a step further, and design a bilingual RNN to represent the derivation structure; • To train the RNN-based DSP model, we propose a max-margin objective that prefers gold derivations yielded by forced decoding to n-best derivations generated by the conventional BTG translation model. 布什 Bush 与 沙龙 举行了 会谈 with Sharon 布什 Bush and Sharon held a talk held a talk (a) (b) 与 沙龙 举行 了 会谈 布什 Bush 与 沙龙 with Sharon held a talk 与 沙龙 举行 了 会谈 with Sharon 举行 了 会谈 held a talk 布什 Bush Bush and 布什 与 沙龙 与 沙龙 and Sharon Sharon 举行 了 会谈 held a talk 779 Proceedings of the 52nd Annual Meeting of the Association </context>
<context position="10062" citStr="Socher et al., 2013" startWordPosition="1732" endWordPosition="1735"> the translation decoder will pursue good derivation structures via DSN. Actually, Li’s work can be easily integrated into our work. We leave it as our future work. 3 Training In this section, we present the method of training the DSP model. The parameters involved in this process include: word embedding, parameters of the two unsupervised RAEs in LNN, and parameters in DSN. 3.1 Max-Margin Framework In DSP model, our goal is to assign higher scores to gold derivations, and lower scores to bad ones. To reach this goal, we adopt a max-margin framework (Socher et al., 2010; Socher et al., 2011a; Socher et al., 2013) for training. Specifically, suppose we have a training data like (ui, G(ui), A(ui)), where ui is the input source sentence, G(ui) is the gold derivation set containing all gold derivations of ui1, and A(ui) is the possible derivation set that contains all possible derivations of ui. We want to minimize the following regularized risk function: λ Ri(θ) + 2 k θ k2, where Ri (θ) = ^max (s(θ, ui, d� + Δ (ˆd, G(ui))) dEA(ui) ~sθ,ui,d~~ − max dEg(ui) Here, θ is the model parameter. s(θ, ui, d) is the DSP score for sentence ui’s derivation d. It is computed by summing LNN score (Equation (2)) and DSN</context>
<context position="11993" citStr="Socher et al., 2013" startWordPosition="2069" endWordPosition="2072">y two hyperparameters for scaling. They are independent of each other, and we set = 0.1 an π ˆd,δ{·} ˆd, Dist(y(ˆd), y(ˆd) dˆ dˆ αs αt αs d αt = 0.1 respectively. Δ(ˆd, G(ui)) ˆd), ref) (8) The margin includes two parts. For the first part, investigate the general case here and suppose that one sentence could have several different gold derivations.In the experiment, we only use one gold deri 1We vation for simple implementation. XN 1 J(θ) = N i=1 (6) 781 3.2 Learning As the risk function, Equation (6) is not differentiable. We train the model via the subgradient method (Ratliff et al., 2007; Socher et al., 2013). For parameter e, the subgriadient of J(e) is: as(e, ui, dm) +Ae ae where ˆdm is the derivation with the highest DSP score, and dm denotes the gold derivation with the highest DSP score. We adopt the diagonal variant of AdaGrad (Duchi et al., 2011; Socher et al., 2013) to minimize the risk function for training. 3.3 Training Instances Collection In order to train the model, we need to collect the gold derivation set G(ui) and possible derivation set A(ui) for input sentence ui. For G(ui) , we define it by force decoding derivation (FDD). Basically, FDD refers to the derivation that produces t</context>
</contexts>
<marker>Socher, Bauer, Manning, Ng, 2013</marker>
<rawString>Richard Socher, John Bauer, Christopher D Manning, and Andrew Y Ng. 2013. Parsing with compositional vector grammars. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational linguistics,</journal>
<pages>23--3</pages>
<contexts>
<context position="972" citStr="Wu, 1997" startWordPosition="138" endWordPosition="139">T using recursive neural network (RNN). Within the model, two steps are involved: (1) phrase-pair vector representation, to learn vector representations for phrase pairs; (2) derivation structure prediction, to generate a bilingual RNN that aims to distinguish good derivation structures from bad ones. Final experimental results show that our DSP model can significantly improve the translation quality. 1 Introduction Derivation structure is important for SMT decoding, especially for the translation model based on nested structures of languages, such as BTG (bracket transduction grammar) model (Wu, 1997; Xiong et al., 2006), hierarchical phrase-based model (Chiang, 2007), and syntax-based model (Galley et al., 2006; Marcu et al., 2006; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2008; Zhang et al., 2011; Zhai et al., 2013). In general, derivation structure refers to the tuple that records the used translation rules and their compositions during decoding, just as Figure 1 shows. Intuitively, a good derivation structure usually yields a good translation, while bad derivations always result in bad translations. For example in Figure 1, (a) and (b) are two different derivations for Chine</context>
<context position="14799" citStr="Wu, 1997" startWordPosition="2546" endWordPosition="2547">ish words2. We train a 5-gram language model by the Xinhua portion of Gigaword corpus and the English part of the training data. We obtain word alignment by GIZA++, and adopt the grow-diag-final-and strategy to generate the symmetric alignment. We use NIST MT 2003 data as the development set, and NIST MT04-083 as the test set. We use MERT (Och, 2004) to tune parameters. The translation quality is evaluated by case-insensitive BLEU-4 (Papineni et al., 2002). The statistical significance test is performed by the re-sampling approach (Koehn, 2004). The baseline system is our in-house BTG system (Wu, 1997; Xiong et al., 2006; Zhang and Zong, 2009). To train the DSP model, we first use Word2Vec4 toolkit to pre-train the word embedding on largescale monolingual data. The used monolingual data contains about 1.06B words for Chinese and 1.12B words for English. The dimensionality of our vectors is 50. The detiled training process is as follows: (1) Using the BTG system to perform force decoding on FBIS part of the bilingual training data5, and collect the sentences succeeded in force decoding (86,902 sentences in total)6. We then collect the corresponding force decoding derivations as gold derivat</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational linguistics, 23(3):377–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Maximum entropy based phrase reordering model for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of ACLCOLING,</booktitle>
<pages>505--512</pages>
<contexts>
<context position="993" citStr="Xiong et al., 2006" startWordPosition="140" endWordPosition="143">cursive neural network (RNN). Within the model, two steps are involved: (1) phrase-pair vector representation, to learn vector representations for phrase pairs; (2) derivation structure prediction, to generate a bilingual RNN that aims to distinguish good derivation structures from bad ones. Final experimental results show that our DSP model can significantly improve the translation quality. 1 Introduction Derivation structure is important for SMT decoding, especially for the translation model based on nested structures of languages, such as BTG (bracket transduction grammar) model (Wu, 1997; Xiong et al., 2006), hierarchical phrase-based model (Chiang, 2007), and syntax-based model (Galley et al., 2006; Marcu et al., 2006; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2008; Zhang et al., 2011; Zhai et al., 2013). In general, derivation structure refers to the tuple that records the used translation rules and their compositions during decoding, just as Figure 1 shows. Intuitively, a good derivation structure usually yields a good translation, while bad derivations always result in bad translations. For example in Figure 1, (a) and (b) are two different derivations for Chinese sentence “� 4 t/1&apos;</context>
<context position="14819" citStr="Xiong et al., 2006" startWordPosition="2548" endWordPosition="2551">. We train a 5-gram language model by the Xinhua portion of Gigaword corpus and the English part of the training data. We obtain word alignment by GIZA++, and adopt the grow-diag-final-and strategy to generate the symmetric alignment. We use NIST MT 2003 data as the development set, and NIST MT04-083 as the test set. We use MERT (Och, 2004) to tune parameters. The translation quality is evaluated by case-insensitive BLEU-4 (Papineni et al., 2002). The statistical significance test is performed by the re-sampling approach (Koehn, 2004). The baseline system is our in-house BTG system (Wu, 1997; Xiong et al., 2006; Zhang and Zong, 2009). To train the DSP model, we first use Word2Vec4 toolkit to pre-train the word embedding on largescale monolingual data. The used monolingual data contains about 1.06B words for Chinese and 1.12B words for English. The dimensionality of our vectors is 50. The detiled training process is as follows: (1) Using the BTG system to perform force decoding on FBIS part of the bilingual training data5, and collect the sentences succeeded in force decoding (86,902 sentences in total)6. We then collect the corresponding force decoding derivations as gold derivations. Here, we only </context>
</contexts>
<marker>Xiong, Liu, Lin, 2006</marker>
<rawString>Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maximum entropy based phrase reordering model for statistical machine translation. In Proceedings of ACLCOLING, pages 505–512.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heng Yu</author>
<author>Liang Huang</author>
<author>Haitao Mi</author>
<author>Kai Zhao</author>
</authors>
<title>Max-violation perceptron and forced decoding for scalable MT training.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1112--1123</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="16284" citStr="Yu et al., 2013" startWordPosition="2783" endWordPosition="2786">es via L-BFGS algorithm. Finally, we get 351,448 source phrases to train the source side RAE and 370,948 target phrases to train the target side RAE. 2LDC category number : LDC2000T50, LDC2002E18, LDC2003E07, LDC2004T07, LDC2005T06, LDC2002L27, LDC2005T10 and LDC2005T34. 3For MT06 and MT08, we only use the part of news data. 4https://code.google.com/p/word2vec/ 5Here we only use the high quality corpus FBIS to guarantee the quality of force decoding derivation. 6Many sentence pairs fail in forced decoding due to many reasons, such as reordering limit, noisy alignment, and phrase length limit (Yu et al., 2013). as(e, ui, ˆdm) ae aJ 1 N ae = i 782 (3) Decoding the 86902 sentences by the BTG system to get n-best translations and corresponding derivations. The n-best derivations are used to simulate the entire derivation space. We retain at most 200-best derivations for each sentence. (4) Leveraging force decoding derivations and n-best derivations to train the DSP model. Note that all parameters, including word embedding and parameters in LNN and DSN, are tuned together in this step. It takes about 15 hours to train the entire network using a 16-core, 2.9 GHz Xeon machine. 5.2 Experimental Results We</context>
</contexts>
<marker>Yu, Huang, Mi, Zhao, 2013</marker>
<rawString>Heng Yu, Liang Huang, Haitao Mi, and Kai Zhao. 2013. Max-violation perceptron and forced decoding for scalable MT training. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1112–1123, Seattle, Washington, USA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Feifei Zhai</author>
<author>Jiajun Zhang</author>
<author>Yu Zhou</author>
<author>Chengqing Zong</author>
</authors>
<title>Unsupervised tree induction for treebased translation. Transactions of Association for Computational Linguistics(TACL),</title>
<date>2013</date>
<pages>291--300</pages>
<contexts>
<context position="1204" citStr="Zhai et al., 2013" startWordPosition="176" endWordPosition="179">bilingual RNN that aims to distinguish good derivation structures from bad ones. Final experimental results show that our DSP model can significantly improve the translation quality. 1 Introduction Derivation structure is important for SMT decoding, especially for the translation model based on nested structures of languages, such as BTG (bracket transduction grammar) model (Wu, 1997; Xiong et al., 2006), hierarchical phrase-based model (Chiang, 2007), and syntax-based model (Galley et al., 2006; Marcu et al., 2006; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2008; Zhang et al., 2011; Zhai et al., 2013). In general, derivation structure refers to the tuple that records the used translation rules and their compositions during decoding, just as Figure 1 shows. Intuitively, a good derivation structure usually yields a good translation, while bad derivations always result in bad translations. For example in Figure 1, (a) and (b) are two different derivations for Chinese sentence “� 4 t/1&apos;A *TT T � W. Comparing the two derivations, (a) is more reasonable and yields a better translation. However, (b) wrongly translates phrase “&apos;j t/I&apos;A” to “and Sharon” and combines it with [--�iff;Bush] incorrectl</context>
</contexts>
<marker>Zhai, Zhang, Zhou, Zong, 2013</marker>
<rawString>Feifei Zhai, Jiajun Zhang, Yu Zhou, and Chengqing Zong. 2013. Unsupervised tree induction for treebased translation. Transactions of Association for Computational Linguistics(TACL), pages 291–300.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiajun Zhang</author>
<author>Chengqing Zong</author>
</authors>
<title>A framework for effectively integrating hard and soft syntactic rules into phrase based translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation,</booktitle>
<pages>579--588</pages>
<institution>Hong Kong, December. City University of Hong Kong.</institution>
<contexts>
<context position="14842" citStr="Zhang and Zong, 2009" startWordPosition="2552" endWordPosition="2555">language model by the Xinhua portion of Gigaword corpus and the English part of the training data. We obtain word alignment by GIZA++, and adopt the grow-diag-final-and strategy to generate the symmetric alignment. We use NIST MT 2003 data as the development set, and NIST MT04-083 as the test set. We use MERT (Och, 2004) to tune parameters. The translation quality is evaluated by case-insensitive BLEU-4 (Papineni et al., 2002). The statistical significance test is performed by the re-sampling approach (Koehn, 2004). The baseline system is our in-house BTG system (Wu, 1997; Xiong et al., 2006; Zhang and Zong, 2009). To train the DSP model, we first use Word2Vec4 toolkit to pre-train the word embedding on largescale monolingual data. The used monolingual data contains about 1.06B words for Chinese and 1.12B words for English. The dimensionality of our vectors is 50. The detiled training process is as follows: (1) Using the BTG system to perform force decoding on FBIS part of the bilingual training data5, and collect the sentences succeeded in force decoding (86,902 sentences in total)6. We then collect the corresponding force decoding derivations as gold derivations. Here, we only use the best force deco</context>
</contexts>
<marker>Zhang, Zong, 2009</marker>
<rawString>Jiajun Zhang and Chengqing Zong. 2009. A framework for effectively integrating hard and soft syntactic rules into phrase based translation. In Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation, pages 579– 588, Hong Kong, December. City University of Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Hongfei Jiang</author>
<author>Aiti Aw</author>
<author>Haizhou Li</author>
<author>Chew Lim Tan</author>
<author>Sheng Li</author>
</authors>
<title>A tree sequence alignment-based tree-to-tree translation model.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-08: HLT,</booktitle>
<pages>559--567</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="1164" citStr="Zhang et al., 2008" startWordPosition="168" endWordPosition="171">ion structure prediction, to generate a bilingual RNN that aims to distinguish good derivation structures from bad ones. Final experimental results show that our DSP model can significantly improve the translation quality. 1 Introduction Derivation structure is important for SMT decoding, especially for the translation model based on nested structures of languages, such as BTG (bracket transduction grammar) model (Wu, 1997; Xiong et al., 2006), hierarchical phrase-based model (Chiang, 2007), and syntax-based model (Galley et al., 2006; Marcu et al., 2006; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2008; Zhang et al., 2011; Zhai et al., 2013). In general, derivation structure refers to the tuple that records the used translation rules and their compositions during decoding, just as Figure 1 shows. Intuitively, a good derivation structure usually yields a good translation, while bad derivations always result in bad translations. For example in Figure 1, (a) and (b) are two different derivations for Chinese sentence “� 4 t/1&apos;A *TT T � W. Comparing the two derivations, (a) is more reasonable and yields a better translation. However, (b) wrongly translates phrase “&apos;j t/I&apos;A” to “and Sharon” and c</context>
</contexts>
<marker>Zhang, Jiang, Aw, Li, Tan, Li, 2008</marker>
<rawString>Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, Chew Lim Tan, and Sheng Li. 2008. A tree sequence alignment-based tree-to-tree translation model. In Proceedings ofACL-08: HLT, pages 559– 567, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiajun Zhang</author>
<author>Feifei Zhai</author>
<author>Chengqing Zong</author>
</authors>
<title>Augmenting string-to-tree translation models with fuzzy use of source-side syntax.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>204--215</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="1184" citStr="Zhang et al., 2011" startWordPosition="172" endWordPosition="175">tion, to generate a bilingual RNN that aims to distinguish good derivation structures from bad ones. Final experimental results show that our DSP model can significantly improve the translation quality. 1 Introduction Derivation structure is important for SMT decoding, especially for the translation model based on nested structures of languages, such as BTG (bracket transduction grammar) model (Wu, 1997; Xiong et al., 2006), hierarchical phrase-based model (Chiang, 2007), and syntax-based model (Galley et al., 2006; Marcu et al., 2006; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2008; Zhang et al., 2011; Zhai et al., 2013). In general, derivation structure refers to the tuple that records the used translation rules and their compositions during decoding, just as Figure 1 shows. Intuitively, a good derivation structure usually yields a good translation, while bad derivations always result in bad translations. For example in Figure 1, (a) and (b) are two different derivations for Chinese sentence “� 4 t/1&apos;A *TT T � W. Comparing the two derivations, (a) is more reasonable and yields a better translation. However, (b) wrongly translates phrase “&apos;j t/I&apos;A” to “and Sharon” and combines it with [--�</context>
</contexts>
<marker>Zhang, Zhai, Zong, 2011</marker>
<rawString>Jiajun Zhang, Feifei Zhai, and Chengqing Zong. 2011. Augmenting string-to-tree translation models with fuzzy use of source-side syntax. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 204–215, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>