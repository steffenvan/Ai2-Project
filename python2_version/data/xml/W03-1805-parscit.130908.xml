<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000023">
<title confidence="0.996715">
A Language Model Approach to Keyphrase Extraction
</title>
<author confidence="0.955316">
Takashi Tomokiyo and Matthew Hurst
</author>
<affiliation confidence="0.8501175">
Applied Research Center
Intelliseek, Inc.
</affiliation>
<address confidence="0.64237">
Pittsburgh, PA 15213
</address>
<email confidence="0.954063">
ttomokiyo,mhurst @intelliseek.com
</email>
<figure confidence="0.80439455">
1 civic hybrid
2 honda civic hybrid
3 toyota prius
4 electric motor
5 honda civic
6 fuel cell
7 hybrid cars
8 honda insight
9 battery pack
10 sports car
11 civic si
12 hybrid car
13 civic lx
14 focusfcv
15 fuel cells
16 hybrid vehicles
17 tour de sol
18 years ago
19 daily driver
20 jetta tdi
</figure>
<address confidence="0.93460295">
21 mustang gt
22 ford escape
23 steering wheel
24 toyota prius today
25 electric motors
26 gasoline engine
27 internal combustion engine
28 gas engine
29 front wheels
30 key sense wire
31 civic type r
32 test drive
33 street race
34 united states
35 hybrid powertrain
36 rear bumper
37 ford focus
38 detroit auto show
39 parking lot
40 rear wheels
</address>
<figureCaption confidence="0.994451">
Figure 1: Top 40 keyphrases automatically extracted from
</figureCaption>
<bodyText confidence="0.990265210526316">
messages relevant to “civic hybrid” using our system
In order to capture domain-specific terms effi-
ciently in limited time, the extraction result should
be ranked with more indicative and good phrase first,
as shown in this example.
2 Phraseness and informativeness
The word keyphrase implies two features: phrase-
ness and informativeness.
Phraseness is a somewhat abstract notion which
describes the degree to which a given word sequence
is considered to be a phrase. In general, phraseness
is defined by the user, who has his own criteria for
the target application. For instance, one user might
want only noun phrases while another user might be
interested only in phrases describing a certain set of
products. Although there is no single definition of
the term phrase, in this paper, we focus on colloca-
tion or cohesion of consecutive words.
Informativeness refers to how well a phrase cap-
</bodyText>
<sectionHeader confidence="0.964439" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999811625">
We present a new approach to extract-
ing keyphrases based on statistical lan-
guage models. Our approach is to use
pointwise KL-divergence between mul-
tiple language models for scoring both
phraseness and informativeness, which
can be unified into a single score to rank
extracted phrases.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999859928571429">
In many real world deployments of text mining tech-
nologies, analysts are required to deal with large col-
lections of documents from unfamiliar domains. Fa-
miliarity with the domain is necessary in order to
get full leverage from text analysis tools. However,
browsing data is not an efficient way to get an un-
derstanding of the topics and events which are par-
ticular to a domain.
For example, an analyst concerned with the area
of hybrid cars may harvest messages from online fo-
rums. They may then want to rapidly construct a hi-
erarchy of topics based on the content of these mes-
sages. In addition, in cases where these messages
are harvested via a search of some sort, there is a re-
quirement to obtain a rich and effective set of search
terms.
The technology described in this paper is an ex-
ample of a phrase finder capable of delivering a set
of indicative phrases given a particular set of docu-
ments from a target domain.
In the hybrid car example, the result of this pro-
cess is a set of phrases like that shown in Figure 1.
tures or illustrates the key ideas in a set of docu-
ments. Because informativeness is defined with re-
spect to background information and new knowl-
edge, users will have different perceptions of infor-
mativeness. In our calculations, we make use of
the relationship between foreground and background
corpora to formalize the notion of informativeness.
The target document set from which representa-
tive keyphrases are extracted is called the foreground
corpus. The document set to which this target set
is compared is called the background corpus. For
example, a foreground corpus of the current week’s
news would be compared to a background corpus of
an entire news article archive to determine that cer-
tain phrases, like “press conference” are typical of
news stories in general and do not capture the par-
ticulars of current events in the way that “national
museum of antiquities” does.
Other examples of foreground and background
corpora include: a web site for a certain company
and web data in general; a newsgroup and the whole
Usenet archive; and research papers of a certain con-
ference and research papers in general.
In order to get a ranked keyphrase list, we need to
combine both phraseness and informativeness into a
single score. A sequence of words can be a good
phrase but not an informative one, like the expres-
sion “in spite of.” A word sequence can be informa-
tive for a particular domain but not a phrase; “toy-
ota, honda, ford” is an example of a non-phrase se-
quence of informative words in a hybrid car domain.
The algorithm we propose for keyphrase finding re-
quires that the keyphrase score well for both phrase-
ness and informativeness.
</bodyText>
<sectionHeader confidence="0.999966" genericHeader="introduction">
3 Related work
</sectionHeader>
<bodyText confidence="0.996101641025641">
Word collocation Various collocation metrics
have been proposed, including mean and variance
(Smadja, 1994), the t-test (Church et al., 1991),
the chi-square test, pointwise mutual information
(MI) (Church and Hanks, 1990), and binomial log-
likelihood ratio test (BLRT) (Dunning, 1993).
According to (Manning and Sch¨utze, 1999),
BLRT is one of the most stable methods for col-
location discovery. (Pantel and Lin, 2001) reports,
however, that BLRT score can be also high for two
frequent terms that are rarely adjacent, such as the
word pair “the the,” and uses a hybrid of MI and
BLRT.
Keyphrase extraction Damerau (1993) uses the
relative frequency ratio between two corpora to ex-
tract domain-specific keyphrases. One problem of
using relative frequency is that it tends to assign too
high a score for words whose frequency in the back-
ground corpus is small (or even zero).
Some work has been done in extracting
keyphrases from technical documents treating
keyphrase extraction as a supervised learning
problem (Frank et al., 1999; Turney, 2000). The
portability of a learned classifier across various
unstructured/structured text is not clear, however,
and the agreement between classifier and human
judges is not high.&apos;
We would like to have the ability to extract
keyphrases from a totally new domain of text with-
out building a training corpus.
Combining keyphrase and collocation Ya-
mamoto and Church (2001) compare two metrics,
MI and Residual IDF (RIDF), and observed that
MI is suitable for finding collocation and RIDF
is suitable for finding informative phrases. They
took the intersection of each top 10% of phrases
identified by MI and RIDF, but did not extend
the approach to combining the two metrics into a
unified score.
</bodyText>
<sectionHeader confidence="0.882833" genericHeader="method">
4 Baseline method based on binomial
log-likelihood ratio test
</sectionHeader>
<bodyText confidence="0.997232210526316">
We can use various statistics as a measure for
phraseness and informativeness. For our baseline,
we have selected the method based on binomial log-
likelihood ratio test (BLRT) described in (Dunning,
1993).
The basic idea of using BLRT for text analysis is
to consider a word sequence as a repeated sequence
of binary trials comparing each word in a corpus to
a target word, and use the likelihood ratio of two
hypotheses that (i) two events, observed times out
of total tokens and times out of total tokens
respectively, are drawn from different distributions
and (ii) from the same distribution.
&apos;e.g. Turney reports 62% “good”, 18% “bad”, 20% “no
opinion” from human judges.
The BLRT score is calculated with
In the case of calculating the phraseness score
of an adjacent word pair ( ), the null hypothesis
is that and are independent, which can be ex-
</bodyText>
<listItem confidence="0.6587495">
pressed as . We can use Equation
(1) to calculate phraseness by setting:
</listItem>
<bodyText confidence="0.999440863636364">
where is the frequency of the word and
is the frequency of following .
For calculating informativeness of a word ,
where and are the frequency of in
the foreground and background corpus, respectively.
Combining a phraseness score and an infor-
mativeness score into a single score value is not a
trivial task since the the BLRT scores vary a lot be-
tween phraseness and informativeness and also de-
pending on data (c.f. Figure 6 (a)).
One way to combine those scores is to use an ex-
ponential model. We experimented with the follow-
ing logistic function:
whose parameters , , and are estimated on a held-
out data set, given feedback from users (i.e. super-
vised).
Figure 2 shows some example phrases extracted
with this method from the data set described in Sec-
tion 6.1, where the parameters, , ,, are manually
optimized on the test data.
Although it is possible to rank keyphrases using
this approach, there are a couple of drawbacks.
</bodyText>
<figure confidence="0.9980912">
1 message news 16 sixth sense
2 minority report 17 hey kids
3 star wars 18 gaza man
4 johnharkness 19 lee harrison
5 derekjanssen 20 years ago
6 robert frenchu 21 julia roberts
7 sean o’hara 22 national guard
8 box office 23 bourne identity
9 dawn taylor 24 metrotoday www.zap2it.com
10 anthony gaza 25 starweek magazine
11 star trek 26 eric chomko
12 ancient race 27 wilner starweek
13 scooby doo 28 tim gueguen
14 austin powers 29 jodie foster
15 home.attbi.com hey 30 johnnie kendricks
</figure>
<figureCaption confidence="0.9841645">
Figure 2: Keyphrases extracted with BLRT (a=0.0003,
l=0.000005, c=8)
</figureCaption>
<bodyText confidence="0.999965095238095">
Necessity of tuning parameters the existence of
parameters in the combining function requires
human labeling, which is sometimes an expen-
sive task to do, and the robustness of learned
weight across domains is unknown. We would
like to have a parameter-free and robust way of
combining scores.
Inappropriate symmetry BLRT tests to see if two
random variables are independent or not. This
sometimes leads to unwanted phrases getting a
high score. For example, when the background
corpus happens to have many occurrences of
phrase al jazeera which is an unusual phrase in
the foreground corpus, then the phrase still gets
high score of informativeness because the dis-
tribution is so different. What we would like to
have instead is asymmetric scoring function to
test the loss of the action of not taking the target
phrase as a keyphrase.
In the next section, we propose a new method try-
ing to address these issues.
</bodyText>
<sectionHeader confidence="0.998076" genericHeader="method">
5 Proposed method
</sectionHeader>
<subsectionHeader confidence="0.999733">
5.1 Language models and expected loss
</subsectionHeader>
<bodyText confidence="0.982040210526316">
A language model assigns a probability value to ev-
ery sequence of words . The prob-
ability can be decomposed as
Assuming only depends on the previous
words, N-gram language models are commonly
where ,, and
Here each word only depends on the previous two
words. Please refer to (Jelinek, 1990) and (Chen and
Goodman, 1996) for more about N-gram models and
associated smoothing methods.
Now suppose we have a foreground corpus and
a background corpus and have created a language
model for each corpus. The simplest language
model is a unigram model, which assumes each
word of a given word sequence is drawn indepen-
dently. We denote the unigram model for the fore-
ground corpus as fg and for the background cor-
pus as ✁bg. We can also train higher order models
fg and ✁bg for each corpus, each of which is
</bodyText>
<figure confidence="0.9990785">
a -gram model, where is the order.
informativeness ✁
fg bg
phraseness
✁ bg
fg
</figure>
<figureCaption confidence="0.998829">
Figure 3: Phraseness and informativeness as loss between lan-
guage models.
</figureCaption>
<bodyText confidence="0.998691181818182">
Among those four models, fg will be the best
model to describe the foreground corpus in the sense
that it has the smallest cross-entropy or perplexity
value over the corpus.
If we use one of the other three models instead,
then we have some inefficiency or loss to describe
the corpus. We expect the amount of loss between
using fg and fg is related to phraseness and
the loss between fg and ✁bg is related to in-
formativeness. Figure 3 illustrates these relation-
ships.
</bodyText>
<subsectionHeader confidence="0.924258">
5.2 Pointwise KL-divergence between models
</subsectionHeader>
<bodyText confidence="0.998942727272727">
One natural metric to measure the loss between two
language models is the Kullback-Leibler (KL) diver-
gence. The KL divergence (also called relative en-
tropy) between two probability mass function
KL divergence is “a measure of the inefficiency
of assuming that the distribution is when the true
distribution is .” (Cover and Thomas, 1991)
You can see this by the following relationship:
The first term is the cross entropy
and the second term is the entropy of the ran-
dom variable , which is how much we could com-
press symbols if we know the true distribution .
We define pointwise KL divergence to
be the term inside of the summation of Equation (6):
Intuitively, this is the contribution of the phrase to
the expected loss of the entire distribution.
We can now quantify phraseness and informative-
ness as follows:
Phraseness of is how much we lose information
by assuming independence of each word by ap-
plying the unigram model, instead of the -
gram model.
</bodyText>
<equation confidence="0.531875">
fg fg (8)
</equation>
<bodyText confidence="0.988853">
Informativeness of is how much we lose in-
formation by assuming the phrase is drawn
from the background model instead of the fore-
ground model.
</bodyText>
<equation confidence="0.895254333333333">
✁ bg
fg ✁ or (9)
fg bg (10)
</equation>
<bodyText confidence="0.927387">
Combined The following is considered to be a mix-
ture of phraseness and informativeness.
fg bg (11)
used. The following is the trigram language model
case.
and is defined as
Note that the KL divergence is always non-
negative2, but the pointwise KL divergence can be
a negative value. An example is the phraseness of
the bigram “the the”.
the the
the the
the the
since the the the the .
Also note that in the case of phraseness of a bi-
gram, the equation looks similar to pointwise mutual
information (Church and Hanks, 1990) , but they are
different. Their relationship is as follows.
pointwise MI
The pointwise KL divergence does not assign a high
score to a rare phrase, whose contribution of loss is
small by definition, unlike pointwise mutual infor-
mation, which is known to have problems (as de-
scribed in (Manning and Sch¨utze, 1999), e.g.).
</bodyText>
<subsectionHeader confidence="0.999763">
5.3 Combining phraseness and informativeness
</subsectionHeader>
<bodyText confidence="0.912318727272727">
One way of getting a unified score of phraseness and
informativeness is using equation (11). We can also
calculate phraseness and informativeness separately
and then combine them.
We combine the phraseness score and infor-
mativeness score by simply adding them into a
single score .
(12)
Intuitively, this can be thought of as the total loss.
We will show some empirical results to justify this
scoring in the next section.
</bodyText>
<sectionHeader confidence="0.997875" genericHeader="method">
6 Experimental results
</sectionHeader>
<bodyText confidence="0.999748">
In this section, we show some preliminary experi-
mental results of applying our method on real data.
</bodyText>
<subsectionHeader confidence="0.997689">
6.1 Data set
</subsectionHeader>
<bodyText confidence="0.993753">
We used the 20 newsgroups data set3, which con-
tains 20,000 messages (7.4 million words) be-
tween February and June 1993 taken from 20
</bodyText>
<footnote confidence="0.997664333333333">
2from Jensen’s inequality.
3http://www-2.cs.cmu.edu/afs/cs.cmu.edu/
project/theo-20/www/data/news20.html
</footnote>
<bodyText confidence="0.999642625">
Usenet newsgroups, as the background data set,
and another 20,000 messages (4 million words)
between June and September 2002 taken from
rec.arts.movies.current-films newsgroup as
the foreground data set. Each message’s subject
header and the body of the message (including
quoted text) is tokenized into lowercase tokens on
both data set. No stemming is applied.
</bodyText>
<subsectionHeader confidence="0.99891">
6.2 Finding key-bigrams
</subsectionHeader>
<bodyText confidence="0.965487">
The first experiment we show is to find key-bigrams,
which is the simplest case requiring combination
of phraseness and informativeness scores. Figure 4
outlines the extraction procedure.
Inputs: foreground and background corpus.
</bodyText>
<listItem confidence="0.996527666666667">
1. create background language model from the back-
ground corpus.
2. count all adjacent word pairs in the foreground cor-
pus, skipping pre-annotated boundaries (such as
HTML tag boundaries) and stopwords.
3. for each pair of words (x,y) in the count, calculate
</listItem>
<bodyText confidence="0.6180624">
phraseness from fg and fg fg and in-
formativeness from fg and bg. Add
the two score values as the unified score.
4. sort the results by the unified score.
Output: a list of key-bigrams ranked by unified score.
</bodyText>
<figureCaption confidence="0.998622">
Figure 4: Procedure to find key-bigrams
</figureCaption>
<bodyText confidence="0.999631705882353">
For this experiment we used unsmoothed count
for calculating phraseness ,
where
, and used the unigram model for
calculating informativeness with Katz smoothing
(Chen and Goodman, 1996)4 to handle zero occur-
rences.
Figure 5 shows the extracted key-bigrams us-
ing this method. Comparing to Figure 2, you can
see that those two methods extract almost identical
ranked phrases. Note that we needed to tune three
parameters to combine phraseness and informative-
ness in BLRT, but no parameter tuning was required
in this method.
The reason why “message news” becomes the
top phrase in both methods is that it appears fre-
quently enough in message citation headers such
</bodyText>
<footnote confidence="0.795609">
4with cutoff
</footnote>
<figure confidence="0.995149833333333">
1 message news
2 minority report
3 star wars
4 johnharkness
5 robert frenchu
6 derekjanssen
7 box office
8 sean o’hara
9 dawn taylor
10 anthony gaza
11 star trek
12 ancient race
13 home.attbi.com hey
14 scooby doo
15 austin powers
16 hey kids
17 years ago
18 gaza man
19 sixth sense
20 lee harrison
21 julia roberts
22 national guard
23 bourne identity
24 metrotoday www.zap2it.com
25 starweek magazine
26 eric chomko
27 wilner starweek
28 tim gueguen
29 jodie foster
30 kevin filmnutboy
</figure>
<figureCaption confidence="0.999769">
Figure 5: Key-bigrams extracted with pointwise KL
</figureCaption>
<bodyText confidence="0.999171642857143">
as John Smith js@foo.com wrote in message
news:1pk0a@foo.com, which was not common in
the 20 newsgroup dataset.5 A more sophisticated
document analysis tool to remove citation headers
is required to improve the quality further.
Figure 6 shows the distribution of phraseness and
informativeness scores of bigrams extracted using
the BLRT and pointwise KL methods. One can
see that there is little correlation between phraseness
and informativeness in both ranking methods. Also
note that the range of x and y axis is very differ-
ent in BLRT, but in the pointwise KL method they
are comparable ranges. That makes combining two
scores easy in the pointwise KL approach.
</bodyText>
<subsectionHeader confidence="0.999973">
6.3 Ranking n-length phrases
</subsectionHeader>
<bodyText confidence="0.996261666666667">
The next example is ranking -length phrases. We
applied a phrase extension algorithm based on the
APriori algorithm (Agrawal and Srikant, 1994) to
the output of the key-bigram finder in the previous
example to generate -length candidates whose fre-
quency is greater than 5, then applied a linguistic
filter which rejects phrases that do not occur in valid
noun-phrase contexts (e.g. following articles or pos-
sessives) at least once in the corpus. We ranked re-
sulting phrases using pointwise KL score, using the
same smoothing method as in the bigram case.
Figure 7 shows the result of re-ranking keyphrases
extracted from the same movie corpus. We can see
that bigrams and trigrams are interleaved in natural
order (although not many long phrases are extracted
from the dataset, since longer NP did not occur more
than five times). Figure 1 was another example of
the result of the same pipeline of methods.
</bodyText>
<footnote confidence="0.999246">
5a popular citation pattern in 1993 was “In article
1pk0a@foo.com , js@foo.com (John Smith) writes:”
</footnote>
<bodyText confidence="0.999443285714286">
One question that might be asked is “what if we
just sort by frequency?”. If we sort by frequency,
“blair witch project” is 92nd and “empire strikes
back” is 110th on the ranked list. Since the longer
the phrase becomes, the lower the frequency of the
phrase is, frequency is not an appropriate method for
ranking phrases.
</bodyText>
<figure confidence="0.98681115">
1 minority report 21 bad guy
2 box office 22 country bears
3 scooby doo 23 man’s man
4 sixth sense 24 long time
5 national guard 25 spoiler space
6 bourne identity 26 empire strikes back
7 air national guard 27 top ten
8 united states 28 politically correct
9 phantom menace 29 white people
10 special effects 30 tv show
11 hotel room 31 bad guys
12 comic book 32 freddie prinze jr
13 blair witch project 33 monster’s ball
14 short story 34 good thing
15 real life 35 evil minions
16 jude law 36 big screen
17 iron giant 37 political correctness
18 bin laden 38 martial arts
19 black people 39 supreme court
20 opening weekend 40 beautiful mind
</figure>
<figureCaption confidence="0.9839065">
Figure 7: Result of re-ranking output from the phrase exten-
sion module
</figureCaption>
<subsectionHeader confidence="0.998399">
6.4 Revisiting unigram informativeness
</subsectionHeader>
<bodyText confidence="0.998287086956522">
An alternative approach to calculate informative-
ness from the foreground LM and the background
LM is just to take the ratio of likelihood scores,
fg☛bg . This is a smoothed version of rela-
tive frequency ratio which is commonly used to find
subject-specific terms (Damerau, 1993).
Figure 8 compares extracted keywords ranked
with pointwise KL and likelihood ratio scores, both
of which use the same foreground and background
unigram language model. We used messages re-
trieved from the query Infiniti G35 as the foreground
corpus and the same 20 newsgroup data as the back-
ground corpus. Katz smoothing is applied to both
language models.
As we can see, those two methods return very dif-
ferent ranked lists. We think the pointwise KL re-
turns a set of keywords closer to human judgment.
One example is the word “infiniti”, which we ex-
pected to be one of the informative words since it
is the query word. The pointwise KL score picked
the word as the third informative word, but the like-
lihood score missed it. Whereas “6mt”, picked up
by the likelihood ratio, which occurs 37 times in the
</bodyText>
<figure confidence="0.998012909090909">
blrt pointKR
informativeness
800000
700000
600000
500000
400000
300000
200000
100000
0
informativeness
-0.0005
0.0015
0.0005
-0.001
0.002
0.001
0
0 2000 4000 6000 8000 10000 12000 14000 16000 18000 20000 -0.0005 0 0.0005 0.001 0.0015 0.002
phraseness phraseness
(a) BLRT (b) LM-pointKL
</figure>
<figureCaption confidence="0.987782">
Figure 6: Phraseness and informativeness score of bigrams extracted with BLRT (a) and pointwise KL divergence between LMs
(b).
</figureCaption>
<table confidence="0.992845590909091">
point KL likelihood ratio
rank freq term freq term
1 1599 g35 1599 g35
2 1145 car 156 330i
3 450 infiniti 117 350z
4 299 coupe 113 doo
5 299 nissan 90 wrx
6 383 bmw 76 is300
7 156 330i 47 willow
8 441 cars 39 rsx
9 248 sedan 37 6mt
10 331 originally 35 scooby
11 201 altima 35 s2000
12 117 350z 33 gt-r
13 113 doo 32 lol
14 235 sport 30 heatwave
15 172 maxima 28 g22
16 90 wrx 26 gtr
17 111 skyline 23 g21
18 76 is300 23 g17
19 186 honda 23 nsx
20 221 engine 22 tl-s
</table>
<figureCaption confidence="0.996261666666667">
Figure 8: Top 20 keywords extracted using pointwise-KL and
likelihood ratio (after stopwords removed) from messages re-
trieved from the query “Infiniti G35”
</figureCaption>
<bodyText confidence="0.99687325">
foreground corpus and none in the background cor-
pus does not seem to be a good keyword.
The following table shows statistics of those two
words:6
</bodyText>
<table confidence="0.460409">
token fg bg PKL LR
6mt 1.837E-4 8.705E-8 0.0020 2110
infiniti 2.269E-3 4.475E-6 0.0204 506
</table>
<footnote confidence="0.639900166666667">
Since the likelihood of “6mt” with respect to the
background LM is so small, the likelihood ratio of
the word becomes very large. But the pointwise KL
score discounts the score appropriately by consider-
6“infiniti” occurs 34 times in the “rec.autos” section of the
20 newsgroup data set.
</footnote>
<bodyText confidence="0.999941428571429">
ing that the frequency of the word is low. Likelihood
ratio (or relative frequency ratio) has a tendency to
pick up rare words as informative. Pointwise KL
seems more robust in sparse data situations.
One disadvantage of the pointwise KL statistic
might be that it also picks up stopwords or punctu-
ation, when there is a significant difference in style
of writing, etc., since these words have significantly
high frequency. But stopwords are easy to define
or can be generated automatically from corpora, and
we don’t consider this to be a significant drawback.
We also expect a better background model and better
smoothing mechanism could reduce the necessity of
the stopword list.
</bodyText>
<sectionHeader confidence="0.999536" genericHeader="method">
7 Discussion
</sectionHeader>
<bodyText confidence="0.998303674418605">
Necessity of both phraseness and informativeness
Although phraseness itself is domain-dependent to
some extent (Smadja, 1994), we have shown that
there is little correlation between informativeness
and phraseness scores.
Combining method One way to calculate a com-
bined score is directly comparing fg and✁bg
in Figure 3. We have tried both approaches and got
a better result from combining separate phraseness
and informativeness scores. We think this is due
to data sparseness of the higher order ngram in the
background corpus. Further investigation is required
to make a conclusion.
We have used the simplest method of combining
two scores by adding them. We have also tried har-
monic mean and geometric mean but they did not
improve the result. We could also apply linear inter-
polation to put more weight on one score value, or
use an exponential model to combine score, but this
will require tuning parameters.
Benefits of using a language model One bene-
fit of using a language model approach is that one
can take advantage of various smoothing techniques.
For example, by interpolating with a character-based
n-gram model, we can make the LM more robust
with respect to spelling errors and variations. Con-
sider the following variations, which we need to treat
as a single entity: al-Qaida, al Qaida, al Qaeda,
al Queda, al-Qaeda, al-Qa’ida, al Qa’ida (found
in online sources). Since these are such unique
spellings in English, character n-gram is expected to
be able to give enough likelihood score to different
spellings as well.
It is also easy to incorporate other models such as
topic or discourse model, use a cache LM to capture
local context, and a class-based LM for the shared
concept. It is also possible to add a phrase length
prior probability in the model for better likelihood
estimation.
Another useful smoothing technique is linear in-
terpolation of the foreground and background lan-
guage models, when the foreground and background
corpus are disjoint.
</bodyText>
<sectionHeader confidence="0.997808" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999939384615385">
We have explained that phraseness and informative-
ness should be unified into a single score to return
useful ranked keyphrases for analysts. Our proposed
approach calculates both scores based on language
models and unified into a single score. The phrases
generated by this method are intuitively very useful,
but the results are difficult to evaluate quantitatively.
In future work we would like to further explore
evaluation of keyphrases, as well as investigate dif-
ferent smoothing techniques. Further extensions in-
clude developing a phrase boundary segmentation
algorithm based on this framework and exploring
applicability to other languages.
</bodyText>
<sectionHeader confidence="0.999256" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99050834">
Rakesh Agrawal and Ramakrishnan Srikant. 1994. Fast
algorithms for mining association rules. In Jorge B.
Bocca, Matthias Jarke, and Carlo Zaniolo, editors,
Proc. 20th Int. Conf. Very Large Data Bases, VLDB,
pages 487–499. Morgan Kaufmann, 12–15 .
Stanley F. Chen and Joshua T. Goodman. 1996. An
empirical study of smoothing techniques for language
modeling. In Proceedings of the 34th Annual Meeting
of the ACL, pages 310–318, Santa Cruz, California,
June.
Kenneth W. Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. In Computational Linguistics, volume 16.
K. Church, P. Hanks, D. Hindle, and W. Gale, 1991.
Using Statistics in Lexical Analysis, pages 115–164.
Lawrence Erlbaum.
Thomas M. Cover and Joy A. Thomas. 1991. Elements
ofInformation Theory. John Wiley.
Fred J. Damerau. 1993. Generating and evaluating
domain-oriented multi-word terms from texts. Infor-
mation Processing and Management, 29(4):433–447.
Ted E. Dunning. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Computational Lin-
guistics, 19(1):61–74.
Eibe Frank, Gordon W. Paynter, Ian H. Witten, Carl
Gutwin, and Craig G. Nevill-Manning. 1999.
Domain-specific keyphrase extraction. In IJCAI,
pages 668–673.
Frederick Jelinek. 1990. Self-organized language mod-
eling for speech recognition. In Alex Waibel and
Kai-Fu Lee, editors, Readings in Speech Recognition,
pages 450–506. Morgan Kaufmann Publishers, Inc.,
San Maeio, California.
Christopher D. Manning and Hinrich Sch¨utze. 1999.
Foundations of Statistical Natural Language Process-
ing. The MIT Press, Cambridge, Massachusetts.
Patrick Pantel and Dekang Lin. 2001. A statisti-
cal corpus-based term extractor. In E. Stroulia and
S. Matwin, editors, Lecture Notes in Artificial Intel-
ligence, pages 36–46. Springer-Verlag.
Frank Z. Smadja. 1994. Retrieving collocations from
text: Xtract. Computational Linguistics, 19(1):143–
177.
Peter D. Turney. 2000. Learning algorithms
for keyphrase extraction. Information Retrieval,
2(4):303–336.
Mikio Yamamoto and Kenneth W. Church. 2001. Us-
ing suffix arrays to compute term frequency and docu-
ment frequency for all substrings in a corpus. Compu-
tational Linguistics, 27(1):1–30.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.780640">
<title confidence="0.999973">A Language Model Approach to Keyphrase Extraction</title>
<author confidence="0.97162">Takashi Tomokiyo</author>
<author confidence="0.97162">Matthew</author>
<affiliation confidence="0.9865215">Applied Research Intelliseek,</affiliation>
<address confidence="0.99702">Pittsburgh, PA</address>
<abstract confidence="0.995536231884058">ttomokiyo,mhurst @intelliseek.com 1 civic hybrid 2 honda civic hybrid 3 toyota prius 4 electric motor 5 honda civic 6 fuel cell 7 hybrid cars 8 honda insight 9 battery pack 10 sports car 11 civic si 12 hybrid car 13 civic lx 14 focusfcv 15 fuel cells 16 hybrid vehicles 17 tour de sol 18 years ago 19 daily driver 20 jetta tdi 21 mustang gt 22 ford escape 23 steering wheel 24 toyota prius today 25 electric motors 26 gasoline engine 27 internal combustion engine 28 gas engine 29 front wheels 30 key sense wire 31 civic type r 32 test drive 33 street race 34 united states 35 hybrid powertrain 36 rear bumper 37 ford focus 38 detroit auto show 39 parking lot 40 rear wheels 1: 40 keyphrases automatically extracted from relevant to using our system In order to capture domain-specific terms efficiently in limited time, the extraction result should be ranked with more indicative and good phrase first, as shown in this example. 2 Phraseness and informativeness word two features: phrase- Phraseness is a somewhat abstract notion which describes the degree to which a given word sequence is considered to be a phrase. In general, phraseness is defined by the user, who has his own criteria for the target application. For instance, one user might want only noun phrases while another user might be interested only in phrases describing a certain set of products. Although there is no single definition of term in this paper, we focus on collocation or cohesion of consecutive words. refers to how well a phrase cap- Abstract We present a new approach to extracting keyphrases based on statistical language models. Our approach is to use pointwise KL-divergence between multiple language models for scoring both which can be unified into a single score to rank extracted phrases.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rakesh Agrawal</author>
<author>Ramakrishnan Srikant</author>
</authors>
<title>Fast algorithms for mining association rules.</title>
<date>1994</date>
<booktitle>Proc. 20th Int. Conf. Very Large Data Bases, VLDB,</booktitle>
<pages>487--499</pages>
<editor>In Jorge B. Bocca, Matthias Jarke, and Carlo Zaniolo, editors,</editor>
<publisher>Morgan Kaufmann,</publisher>
<contexts>
<context position="17417" citStr="Agrawal and Srikant, 1994" startWordPosition="2936" endWordPosition="2939">uality further. Figure 6 shows the distribution of phraseness and informativeness scores of bigrams extracted using the BLRT and pointwise KL methods. One can see that there is little correlation between phraseness and informativeness in both ranking methods. Also note that the range of x and y axis is very different in BLRT, but in the pointwise KL method they are comparable ranges. That makes combining two scores easy in the pointwise KL approach. 6.3 Ranking n-length phrases The next example is ranking -length phrases. We applied a phrase extension algorithm based on the APriori algorithm (Agrawal and Srikant, 1994) to the output of the key-bigram finder in the previous example to generate -length candidates whose frequency is greater than 5, then applied a linguistic filter which rejects phrases that do not occur in valid noun-phrase contexts (e.g. following articles or possessives) at least once in the corpus. We ranked resulting phrases using pointwise KL score, using the same smoothing method as in the bigram case. Figure 7 shows the result of re-ranking keyphrases extracted from the same movie corpus. We can see that bigrams and trigrams are interleaved in natural order (although not many long phras</context>
</contexts>
<marker>Agrawal, Srikant, 1994</marker>
<rawString>Rakesh Agrawal and Ramakrishnan Srikant. 1994. Fast algorithms for mining association rules. In Jorge B. Bocca, Matthias Jarke, and Carlo Zaniolo, editors, Proc. 20th Int. Conf. Very Large Data Bases, VLDB, pages 487–499. Morgan Kaufmann, 12–15 .</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua T Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the ACL,</booktitle>
<pages>310--318</pages>
<location>Santa Cruz, California,</location>
<contexts>
<context position="10272" citStr="Chen and Goodman, 1996" startWordPosition="1741" endWordPosition="1744">ibution is so different. What we would like to have instead is asymmetric scoring function to test the loss of the action of not taking the target phrase as a keyphrase. In the next section, we propose a new method trying to address these issues. 5 Proposed method 5.1 Language models and expected loss A language model assigns a probability value to every sequence of words . The probability can be decomposed as Assuming only depends on the previous words, N-gram language models are commonly where ,, and Here each word only depends on the previous two words. Please refer to (Jelinek, 1990) and (Chen and Goodman, 1996) for more about N-gram models and associated smoothing methods. Now suppose we have a foreground corpus and a background corpus and have created a language model for each corpus. The simplest language model is a unigram model, which assumes each word of a given word sequence is drawn independently. We denote the unigram model for the foreground corpus as fg and for the background corpus as ✁bg. We can also train higher order models fg and ✁bg for each corpus, each of which is a -gram model, where is the order. informativeness ✁ fg bg phraseness ✁ bg fg Figure 3: Phraseness and informativeness </context>
<context position="15551" citStr="Chen and Goodman, 1996" startWordPosition="2626" endWordPosition="2629">unt all adjacent word pairs in the foreground corpus, skipping pre-annotated boundaries (such as HTML tag boundaries) and stopwords. 3. for each pair of words (x,y) in the count, calculate phraseness from fg and fg fg and informativeness from fg and bg. Add the two score values as the unified score. 4. sort the results by the unified score. Output: a list of key-bigrams ranked by unified score. Figure 4: Procedure to find key-bigrams For this experiment we used unsmoothed count for calculating phraseness , where , and used the unigram model for calculating informativeness with Katz smoothing (Chen and Goodman, 1996)4 to handle zero occurrences. Figure 5 shows the extracted key-bigrams using this method. Comparing to Figure 2, you can see that those two methods extract almost identical ranked phrases. Note that we needed to tune three parameters to combine phraseness and informativeness in BLRT, but no parameter tuning was required in this method. The reason why “message news” becomes the top phrase in both methods is that it appears frequently enough in message citation headers such 4with cutoff 1 message news 2 minority report 3 star wars 4 johnharkness 5 robert frenchu 6 derekjanssen 7 box office 8 sea</context>
</contexts>
<marker>Chen, Goodman, 1996</marker>
<rawString>Stanley F. Chen and Joshua T. Goodman. 1996. An empirical study of smoothing techniques for language modeling. In Proceedings of the 34th Annual Meeting of the ACL, pages 310–318, Santa Cruz, California, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth W Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1990</date>
<booktitle>In Computational Linguistics,</booktitle>
<volume>16</volume>
<contexts>
<context position="4997" citStr="Church and Hanks, 1990" startWordPosition="847" endWordPosition="850"> good phrase but not an informative one, like the expression “in spite of.” A word sequence can be informative for a particular domain but not a phrase; “toyota, honda, ford” is an example of a non-phrase sequence of informative words in a hybrid car domain. The algorithm we propose for keyphrase finding requires that the keyphrase score well for both phraseness and informativeness. 3 Related work Word collocation Various collocation metrics have been proposed, including mean and variance (Smadja, 1994), the t-test (Church et al., 1991), the chi-square test, pointwise mutual information (MI) (Church and Hanks, 1990), and binomial loglikelihood ratio test (BLRT) (Dunning, 1993). According to (Manning and Sch¨utze, 1999), BLRT is one of the most stable methods for collocation discovery. (Pantel and Lin, 2001) reports, however, that BLRT score can be also high for two frequent terms that are rarely adjacent, such as the word pair “the the,” and uses a hybrid of MI and BLRT. Keyphrase extraction Damerau (1993) uses the relative frequency ratio between two corpora to extract domain-specific keyphrases. One problem of using relative frequency is that it tends to assign too high a score for words whose frequenc</context>
<context position="13077" citStr="Church and Hanks, 1990" startWordPosition="2241" endWordPosition="2244">hrase is drawn from the background model instead of the foreground model. ✁ bg fg ✁ or (9) fg bg (10) Combined The following is considered to be a mixture of phraseness and informativeness. fg bg (11) used. The following is the trigram language model case. and is defined as Note that the KL divergence is always nonnegative2, but the pointwise KL divergence can be a negative value. An example is the phraseness of the bigram “the the”. the the the the the the since the the the the . Also note that in the case of phraseness of a bigram, the equation looks similar to pointwise mutual information (Church and Hanks, 1990) , but they are different. Their relationship is as follows. pointwise MI The pointwise KL divergence does not assign a high score to a rare phrase, whose contribution of loss is small by definition, unlike pointwise mutual information, which is known to have problems (as described in (Manning and Sch¨utze, 1999), e.g.). 5.3 Combining phraseness and informativeness One way of getting a unified score of phraseness and informativeness is using equation (11). We can also calculate phraseness and informativeness separately and then combine them. We combine the phraseness score and informativeness </context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>Kenneth W. Church and Patrick Hanks. 1990. Word association norms, mutual information, and lexicography. In Computational Linguistics, volume 16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Church</author>
<author>P Hanks</author>
<author>D Hindle</author>
<author>W Gale</author>
</authors>
<title>Using Statistics in Lexical Analysis,</title>
<date>1991</date>
<pages>115--164</pages>
<institution>Lawrence Erlbaum.</institution>
<contexts>
<context position="4916" citStr="Church et al., 1991" startWordPosition="836" endWordPosition="839">raseness and informativeness into a single score. A sequence of words can be a good phrase but not an informative one, like the expression “in spite of.” A word sequence can be informative for a particular domain but not a phrase; “toyota, honda, ford” is an example of a non-phrase sequence of informative words in a hybrid car domain. The algorithm we propose for keyphrase finding requires that the keyphrase score well for both phraseness and informativeness. 3 Related work Word collocation Various collocation metrics have been proposed, including mean and variance (Smadja, 1994), the t-test (Church et al., 1991), the chi-square test, pointwise mutual information (MI) (Church and Hanks, 1990), and binomial loglikelihood ratio test (BLRT) (Dunning, 1993). According to (Manning and Sch¨utze, 1999), BLRT is one of the most stable methods for collocation discovery. (Pantel and Lin, 2001) reports, however, that BLRT score can be also high for two frequent terms that are rarely adjacent, such as the word pair “the the,” and uses a hybrid of MI and BLRT. Keyphrase extraction Damerau (1993) uses the relative frequency ratio between two corpora to extract domain-specific keyphrases. One problem of using relati</context>
</contexts>
<marker>Church, Hanks, Hindle, Gale, 1991</marker>
<rawString>K. Church, P. Hanks, D. Hindle, and W. Gale, 1991. Using Statistics in Lexical Analysis, pages 115–164. Lawrence Erlbaum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas M Cover</author>
<author>Joy A Thomas</author>
</authors>
<title>Elements ofInformation Theory.</title>
<date>1991</date>
<publisher>John Wiley.</publisher>
<contexts>
<context position="11754" citStr="Cover and Thomas, 1991" startWordPosition="1997" endWordPosition="2000">n we have some inefficiency or loss to describe the corpus. We expect the amount of loss between using fg and fg is related to phraseness and the loss between fg and ✁bg is related to informativeness. Figure 3 illustrates these relationships. 5.2 Pointwise KL-divergence between models One natural metric to measure the loss between two language models is the Kullback-Leibler (KL) divergence. The KL divergence (also called relative entropy) between two probability mass function KL divergence is “a measure of the inefficiency of assuming that the distribution is when the true distribution is .” (Cover and Thomas, 1991) You can see this by the following relationship: The first term is the cross entropy and the second term is the entropy of the random variable , which is how much we could compress symbols if we know the true distribution . We define pointwise KL divergence to be the term inside of the summation of Equation (6): Intuitively, this is the contribution of the phrase to the expected loss of the entire distribution. We can now quantify phraseness and informativeness as follows: Phraseness of is how much we lose information by assuming independence of each word by applying the unigram model, instead</context>
</contexts>
<marker>Cover, Thomas, 1991</marker>
<rawString>Thomas M. Cover and Joy A. Thomas. 1991. Elements ofInformation Theory. John Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fred J Damerau</author>
</authors>
<title>Generating and evaluating domain-oriented multi-word terms from texts.</title>
<date>1993</date>
<booktitle>Information Processing and Management,</booktitle>
<pages>29--4</pages>
<contexts>
<context position="5395" citStr="Damerau (1993)" startWordPosition="916" endWordPosition="917">ollocation Various collocation metrics have been proposed, including mean and variance (Smadja, 1994), the t-test (Church et al., 1991), the chi-square test, pointwise mutual information (MI) (Church and Hanks, 1990), and binomial loglikelihood ratio test (BLRT) (Dunning, 1993). According to (Manning and Sch¨utze, 1999), BLRT is one of the most stable methods for collocation discovery. (Pantel and Lin, 2001) reports, however, that BLRT score can be also high for two frequent terms that are rarely adjacent, such as the word pair “the the,” and uses a hybrid of MI and BLRT. Keyphrase extraction Damerau (1993) uses the relative frequency ratio between two corpora to extract domain-specific keyphrases. One problem of using relative frequency is that it tends to assign too high a score for words whose frequency in the background corpus is small (or even zero). Some work has been done in extracting keyphrases from technical documents treating keyphrase extraction as a supervised learning problem (Frank et al., 1999; Turney, 2000). The portability of a learned classifier across various unstructured/structured text is not clear, however, and the agreement between classifier and human judges is not high.</context>
<context position="19638" citStr="Damerau, 1993" startWordPosition="3325" endWordPosition="3326">all 14 short story 34 good thing 15 real life 35 evil minions 16 jude law 36 big screen 17 iron giant 37 political correctness 18 bin laden 38 martial arts 19 black people 39 supreme court 20 opening weekend 40 beautiful mind Figure 7: Result of re-ranking output from the phrase extension module 6.4 Revisiting unigram informativeness An alternative approach to calculate informativeness from the foreground LM and the background LM is just to take the ratio of likelihood scores, fg☛bg . This is a smoothed version of relative frequency ratio which is commonly used to find subject-specific terms (Damerau, 1993). Figure 8 compares extracted keywords ranked with pointwise KL and likelihood ratio scores, both of which use the same foreground and background unigram language model. We used messages retrieved from the query Infiniti G35 as the foreground corpus and the same 20 newsgroup data as the background corpus. Katz smoothing is applied to both language models. As we can see, those two methods return very different ranked lists. We think the pointwise KL returns a set of keywords closer to human judgment. One example is the word “infiniti”, which we expected to be one of the informative words since </context>
</contexts>
<marker>Damerau, 1993</marker>
<rawString>Fred J. Damerau. 1993. Generating and evaluating domain-oriented multi-word terms from texts. Information Processing and Management, 29(4):433–447.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted E Dunning</author>
</authors>
<title>Accurate methods for the statistics of surprise and coincidence.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="5059" citStr="Dunning, 1993" startWordPosition="858" endWordPosition="859">of.” A word sequence can be informative for a particular domain but not a phrase; “toyota, honda, ford” is an example of a non-phrase sequence of informative words in a hybrid car domain. The algorithm we propose for keyphrase finding requires that the keyphrase score well for both phraseness and informativeness. 3 Related work Word collocation Various collocation metrics have been proposed, including mean and variance (Smadja, 1994), the t-test (Church et al., 1991), the chi-square test, pointwise mutual information (MI) (Church and Hanks, 1990), and binomial loglikelihood ratio test (BLRT) (Dunning, 1993). According to (Manning and Sch¨utze, 1999), BLRT is one of the most stable methods for collocation discovery. (Pantel and Lin, 2001) reports, however, that BLRT score can be also high for two frequent terms that are rarely adjacent, such as the word pair “the the,” and uses a hybrid of MI and BLRT. Keyphrase extraction Damerau (1993) uses the relative frequency ratio between two corpora to extract domain-specific keyphrases. One problem of using relative frequency is that it tends to assign too high a score for words whose frequency in the background corpus is small (or even zero). Some work </context>
<context position="6775" citStr="Dunning, 1993" startWordPosition="1137" endWordPosition="1138">mamoto and Church (2001) compare two metrics, MI and Residual IDF (RIDF), and observed that MI is suitable for finding collocation and RIDF is suitable for finding informative phrases. They took the intersection of each top 10% of phrases identified by MI and RIDF, but did not extend the approach to combining the two metrics into a unified score. 4 Baseline method based on binomial log-likelihood ratio test We can use various statistics as a measure for phraseness and informativeness. For our baseline, we have selected the method based on binomial loglikelihood ratio test (BLRT) described in (Dunning, 1993). The basic idea of using BLRT for text analysis is to consider a word sequence as a repeated sequence of binary trials comparing each word in a corpus to a target word, and use the likelihood ratio of two hypotheses that (i) two events, observed times out of total tokens and times out of total tokens respectively, are drawn from different distributions and (ii) from the same distribution. &apos;e.g. Turney reports 62% “good”, 18% “bad”, 20% “no opinion” from human judges. The BLRT score is calculated with In the case of calculating the phraseness score of an adjacent word pair ( ), the null hypoth</context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>Ted E. Dunning. 1993. Accurate methods for the statistics of surprise and coincidence. Computational Linguistics, 19(1):61–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eibe Frank</author>
<author>Gordon W Paynter</author>
<author>Ian H Witten</author>
<author>Carl Gutwin</author>
<author>Craig G Nevill-Manning</author>
</authors>
<title>Domain-specific keyphrase extraction.</title>
<date>1999</date>
<booktitle>In IJCAI,</booktitle>
<pages>668--673</pages>
<contexts>
<context position="5805" citStr="Frank et al., 1999" startWordPosition="980" endWordPosition="983">2001) reports, however, that BLRT score can be also high for two frequent terms that are rarely adjacent, such as the word pair “the the,” and uses a hybrid of MI and BLRT. Keyphrase extraction Damerau (1993) uses the relative frequency ratio between two corpora to extract domain-specific keyphrases. One problem of using relative frequency is that it tends to assign too high a score for words whose frequency in the background corpus is small (or even zero). Some work has been done in extracting keyphrases from technical documents treating keyphrase extraction as a supervised learning problem (Frank et al., 1999; Turney, 2000). The portability of a learned classifier across various unstructured/structured text is not clear, however, and the agreement between classifier and human judges is not high.&apos; We would like to have the ability to extract keyphrases from a totally new domain of text without building a training corpus. Combining keyphrase and collocation Yamamoto and Church (2001) compare two metrics, MI and Residual IDF (RIDF), and observed that MI is suitable for finding collocation and RIDF is suitable for finding informative phrases. They took the intersection of each top 10% of phrases ident</context>
</contexts>
<marker>Frank, Paynter, Witten, Gutwin, Nevill-Manning, 1999</marker>
<rawString>Eibe Frank, Gordon W. Paynter, Ian H. Witten, Carl Gutwin, and Craig G. Nevill-Manning. 1999. Domain-specific keyphrase extraction. In IJCAI, pages 668–673.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
</authors>
<title>Self-organized language modeling for speech recognition.</title>
<date>1990</date>
<booktitle>In Alex Waibel and Kai-Fu Lee, editors, Readings in Speech Recognition,</booktitle>
<pages>450--506</pages>
<publisher>Morgan Kaufmann Publishers, Inc.,</publisher>
<location>San Maeio, California.</location>
<contexts>
<context position="10243" citStr="Jelinek, 1990" startWordPosition="1738" endWordPosition="1739">ss because the distribution is so different. What we would like to have instead is asymmetric scoring function to test the loss of the action of not taking the target phrase as a keyphrase. In the next section, we propose a new method trying to address these issues. 5 Proposed method 5.1 Language models and expected loss A language model assigns a probability value to every sequence of words . The probability can be decomposed as Assuming only depends on the previous words, N-gram language models are commonly where ,, and Here each word only depends on the previous two words. Please refer to (Jelinek, 1990) and (Chen and Goodman, 1996) for more about N-gram models and associated smoothing methods. Now suppose we have a foreground corpus and a background corpus and have created a language model for each corpus. The simplest language model is a unigram model, which assumes each word of a given word sequence is drawn independently. We denote the unigram model for the foreground corpus as fg and for the background corpus as ✁bg. We can also train higher order models fg and ✁bg for each corpus, each of which is a -gram model, where is the order. informativeness ✁ fg bg phraseness ✁ bg fg Figure 3: Ph</context>
</contexts>
<marker>Jelinek, 1990</marker>
<rawString>Frederick Jelinek. 1990. Self-organized language modeling for speech recognition. In Alex Waibel and Kai-Fu Lee, editors, Readings in Speech Recognition, pages 450–506. Morgan Kaufmann Publishers, Inc., San Maeio, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Foundations of Statistical Natural Language Processing.</title>
<date>1999</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>Christopher D. Manning and Hinrich Sch¨utze. 1999. Foundations of Statistical Natural Language Processing. The MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Dekang Lin</author>
</authors>
<title>A statistical corpus-based term extractor.</title>
<date>2001</date>
<booktitle>Lecture Notes in Artificial Intelligence,</booktitle>
<pages>36--46</pages>
<editor>In E. Stroulia and S. Matwin, editors,</editor>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="5192" citStr="Pantel and Lin, 2001" startWordPosition="878" endWordPosition="881">-phrase sequence of informative words in a hybrid car domain. The algorithm we propose for keyphrase finding requires that the keyphrase score well for both phraseness and informativeness. 3 Related work Word collocation Various collocation metrics have been proposed, including mean and variance (Smadja, 1994), the t-test (Church et al., 1991), the chi-square test, pointwise mutual information (MI) (Church and Hanks, 1990), and binomial loglikelihood ratio test (BLRT) (Dunning, 1993). According to (Manning and Sch¨utze, 1999), BLRT is one of the most stable methods for collocation discovery. (Pantel and Lin, 2001) reports, however, that BLRT score can be also high for two frequent terms that are rarely adjacent, such as the word pair “the the,” and uses a hybrid of MI and BLRT. Keyphrase extraction Damerau (1993) uses the relative frequency ratio between two corpora to extract domain-specific keyphrases. One problem of using relative frequency is that it tends to assign too high a score for words whose frequency in the background corpus is small (or even zero). Some work has been done in extracting keyphrases from technical documents treating keyphrase extraction as a supervised learning problem (Frank</context>
</contexts>
<marker>Pantel, Lin, 2001</marker>
<rawString>Patrick Pantel and Dekang Lin. 2001. A statistical corpus-based term extractor. In E. Stroulia and S. Matwin, editors, Lecture Notes in Artificial Intelligence, pages 36–46. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Z Smadja</author>
</authors>
<title>Retrieving collocations from text:</title>
<date>1994</date>
<journal>Xtract. Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<pages>177</pages>
<contexts>
<context position="4882" citStr="Smadja, 1994" startWordPosition="832" endWordPosition="833"> we need to combine both phraseness and informativeness into a single score. A sequence of words can be a good phrase but not an informative one, like the expression “in spite of.” A word sequence can be informative for a particular domain but not a phrase; “toyota, honda, ford” is an example of a non-phrase sequence of informative words in a hybrid car domain. The algorithm we propose for keyphrase finding requires that the keyphrase score well for both phraseness and informativeness. 3 Related work Word collocation Various collocation metrics have been proposed, including mean and variance (Smadja, 1994), the t-test (Church et al., 1991), the chi-square test, pointwise mutual information (MI) (Church and Hanks, 1990), and binomial loglikelihood ratio test (BLRT) (Dunning, 1993). According to (Manning and Sch¨utze, 1999), BLRT is one of the most stable methods for collocation discovery. (Pantel and Lin, 2001) reports, however, that BLRT score can be also high for two frequent terms that are rarely adjacent, such as the word pair “the the,” and uses a hybrid of MI and BLRT. Keyphrase extraction Damerau (1993) uses the relative frequency ratio between two corpora to extract domain-specific keyph</context>
<context position="22826" citStr="Smadja, 1994" startWordPosition="3880" endWordPosition="3881">advantage of the pointwise KL statistic might be that it also picks up stopwords or punctuation, when there is a significant difference in style of writing, etc., since these words have significantly high frequency. But stopwords are easy to define or can be generated automatically from corpora, and we don’t consider this to be a significant drawback. We also expect a better background model and better smoothing mechanism could reduce the necessity of the stopword list. 7 Discussion Necessity of both phraseness and informativeness Although phraseness itself is domain-dependent to some extent (Smadja, 1994), we have shown that there is little correlation between informativeness and phraseness scores. Combining method One way to calculate a combined score is directly comparing fg and✁bg in Figure 3. We have tried both approaches and got a better result from combining separate phraseness and informativeness scores. We think this is due to data sparseness of the higher order ngram in the background corpus. Further investigation is required to make a conclusion. We have used the simplest method of combining two scores by adding them. We have also tried harmonic mean and geometric mean but they did n</context>
</contexts>
<marker>Smadja, 1994</marker>
<rawString>Frank Z. Smadja. 1994. Retrieving collocations from text: Xtract. Computational Linguistics, 19(1):143– 177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Learning algorithms for keyphrase extraction.</title>
<date>2000</date>
<journal>Information Retrieval,</journal>
<volume>2</volume>
<issue>4</issue>
<contexts>
<context position="5820" citStr="Turney, 2000" startWordPosition="984" endWordPosition="985">er, that BLRT score can be also high for two frequent terms that are rarely adjacent, such as the word pair “the the,” and uses a hybrid of MI and BLRT. Keyphrase extraction Damerau (1993) uses the relative frequency ratio between two corpora to extract domain-specific keyphrases. One problem of using relative frequency is that it tends to assign too high a score for words whose frequency in the background corpus is small (or even zero). Some work has been done in extracting keyphrases from technical documents treating keyphrase extraction as a supervised learning problem (Frank et al., 1999; Turney, 2000). The portability of a learned classifier across various unstructured/structured text is not clear, however, and the agreement between classifier and human judges is not high.&apos; We would like to have the ability to extract keyphrases from a totally new domain of text without building a training corpus. Combining keyphrase and collocation Yamamoto and Church (2001) compare two metrics, MI and Residual IDF (RIDF), and observed that MI is suitable for finding collocation and RIDF is suitable for finding informative phrases. They took the intersection of each top 10% of phrases identified by MI and</context>
</contexts>
<marker>Turney, 2000</marker>
<rawString>Peter D. Turney. 2000. Learning algorithms for keyphrase extraction. Information Retrieval, 2(4):303–336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikio Yamamoto</author>
<author>Kenneth W Church</author>
</authors>
<title>Using suffix arrays to compute term frequency and document frequency for all substrings in a corpus.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>1</issue>
<contexts>
<context position="6185" citStr="Yamamoto and Church (2001)" startWordPosition="1038" endWordPosition="1042"> high a score for words whose frequency in the background corpus is small (or even zero). Some work has been done in extracting keyphrases from technical documents treating keyphrase extraction as a supervised learning problem (Frank et al., 1999; Turney, 2000). The portability of a learned classifier across various unstructured/structured text is not clear, however, and the agreement between classifier and human judges is not high.&apos; We would like to have the ability to extract keyphrases from a totally new domain of text without building a training corpus. Combining keyphrase and collocation Yamamoto and Church (2001) compare two metrics, MI and Residual IDF (RIDF), and observed that MI is suitable for finding collocation and RIDF is suitable for finding informative phrases. They took the intersection of each top 10% of phrases identified by MI and RIDF, but did not extend the approach to combining the two metrics into a unified score. 4 Baseline method based on binomial log-likelihood ratio test We can use various statistics as a measure for phraseness and informativeness. For our baseline, we have selected the method based on binomial loglikelihood ratio test (BLRT) described in (Dunning, 1993). The basi</context>
</contexts>
<marker>Yamamoto, Church, 2001</marker>
<rawString>Mikio Yamamoto and Kenneth W. Church. 2001. Using suffix arrays to compute term frequency and document frequency for all substrings in a corpus. Computational Linguistics, 27(1):1–30.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>