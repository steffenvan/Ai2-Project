<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000053">
<title confidence="0.957626">
Implicatures and Nested Beliefs in Approximate Decentralized-POMDPs
</title>
<author confidence="0.999214">
Adam Vogel, Christopher Potts, and Dan Jurafsky
</author>
<affiliation confidence="0.993363">
Stanford University
</affiliation>
<address confidence="0.91008">
Stanford, CA, USA
</address>
<email confidence="0.998299">
{acvogel,cgpotts,jurafsky}@stanford.edu
</email>
<sectionHeader confidence="0.993874" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999379642857143">
Conversational implicatures involve rea-
soning about multiply nested belief struc-
tures. This complexity poses significant
challenges for computational models of
conversation and cognition. We show that
agents in the multi-agent Decentralized-
POMDP reach implicature-rich interpreta-
tions simply as a by-product of the way
they reason about each other to maxi-
mize joint utility. Our simulations involve
a reference game of the sort studied in
psychology and linguistics as well as a
dynamic, interactional scenario involving
implemented artificial agents.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999943288888889">
Gricean conversational implicatures (Grice, 1975)
are inferences that listeners make in order to
reconcile the speaker’s linguistic behavior with
the assumption that the speaker is cooperative.
As Grice conceived of them, implicatures cru-
cially involve reasoning about multiply-nested be-
lief structures: roughly, for p to count as an impli-
cature, the speaker must believe that the listener
will infer that the speaker believes p. This com-
plexity makes implicatures an important testing
ground for models of conversation and cognition.
Implicatures have received considerable atten-
tion in the context of simple reference games in
which the listener uses the speaker’s utterance
to try to identify the speaker’s intended referent
(Rosenberg and Cohen, 1964; Clark and Wilkes-
Gibbs, 1986; Dale and Reiter, 1995; DeVault and
Stone, 2007; Krahmer and van Deemter, 2012).
Many implicature patterns can be embedded in
these games using specific combinations of poten-
tial referents and message sets. The paradigm has
proven fruitful not only for evaluating computa-
tional models (Golland et al., 2010; Degen and
Franke, 2012; Frank and Goodman, 2012; Rohde
et al., 2012; Bergen et al., 2012) but also for study-
ing children’s pragmatic abilities without implic-
itly assuming they have mastered challenging lin-
guistic structures (Stiller et al., 2011).
In this paper, we extend these results beyond
simple reference games to full decision-problems
in which the agents reason about language and ac-
tion together over time. To do this, we use the De-
centralized Partially Observable Markov Decision
Process (Dec-POMDP) to implement agents that
are capable of manipulating the multiply-nested
belief structures required for implicature calcula-
tion. Optimal decision making in Dec-POMDPs
is NEXP complete, so we employ the single-agent
POMDP approximation of Vogel et al. (2013).
We show that agents in the Dec-POMDP reach
implicature-rich interpretations simply as a by-
product of the way they reason about each other
to maximize joint utility. Our simulations involve
a reference game and a dynamic, interactional sce-
nario involving implemented artificial agents.
</bodyText>
<sectionHeader confidence="0.998261" genericHeader="method">
2 Decision-Theoretic Communication
</sectionHeader>
<bodyText confidence="0.998634647058824">
The Decentralized Partially Observable Markov
Decision Process (Dec-POMDP) (Bernstein et
al., 2002) is a multi-agent generalization of the
POMDP, where agents act to maximize a shared
utility function. Formally, a Dec-POMDP con-
sists of a tuple (S, A, O, R, T, Q, b0, γ). S is a
finite set of states, A is the set of actions, O is
the set of observations, and T(s&apos;|a1, a2, s) is the
transition distribution which determines what ef-
fect the joint action (a1, a2) has on the state of the
world. The true state s E S is not observable to
the agents, who must utilize observations o E O,
which are emitted after each action according to
the observation distribution Q(o1, o2|s&apos;, a). The
reward function R(s, a1, a2) represents the goal of
the agents, who act to maximize expected reward.
Lastly, b0 E o(S) is the initial belief state and
</bodyText>
<page confidence="0.984246">
74
</page>
<bodyText confidence="0.956690782608696">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 74–80,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
γ E [0, 1) is the discount factor.
The true state of the world s E S is not ob-
servable to either agent. In single-agent POMDPs,
agents maintain a belief state b(s) E Δ(S), which
is a distribution over states. Agents acting in Dec-
POMDPs must take into account not only their
beliefs about the state of the world, but also the
beliefs of their partners, leading to nested belief
states. In the model presented here, our agent
models the other agent’s beliefs about the state of
the world, and assumes that the other agent does
not take into account our own beliefs, a common
approach (Gmytrasiewicz and Doshi, 2005).
Agents make decisions according to
a policy πz : Δ(S) -+ A which max-
imizes the discounted expected reward
E∞t=0 γt E[R(st, at1, at2)|b0, π1, π2]. Using
the assumption that the other agent tracks one less
level of belief, we can solve for the other agent’s
policy ¯π, which allows us to estimate his actions
and beliefs over time. To construct policies,
we use Perseus (Spaan and Vlassis, 2005), a
point-based value iteration algorithm.
Even tracking just one level of nested beliefs
quickly leads to a combinatorial explosion in the
number of belief states the other agent might have.
This causes decision making in Dec-POMDPs to
be NEXP complete, limiting their application to
problems with only a handful of states (Bernstein
et al., 2002). To ameliorate this difficulty, we
use the method of Vogel et al. (2013), which cre-
ates a single-agent approximation to the full Dec-
POMDP. To form this single-agent POMDP, we
augment the state space to be S x S, where the
second set of state variables allows us to model
the other agent’s beliefs. We maintain a point
estimate ¯b of the other agent’s beliefs, which
is formed by summing out observations O that
the other player might have received. To ac-
complish this, we factor the transition distribu-
tion into two terms: T((s0, ¯s0)|a, ¯π(¯s), (s, ¯s)) =
T¯ (¯s0|s0, a, ¯π(¯s), (s, ¯s))T (s0|a, ¯π(¯s), (s, ¯s)).This
observation marginalization can be folded into the
transition distribution T¯(¯s0|s0, a, ¯π(¯s), (s, ¯s)):
</bodyText>
<equation confidence="0.995628">
T¯(¯s0 |s0, a, ¯π(¯s), (s, ¯s)) = Pr(¯s0|s0, a, ¯π(¯s), (s, ¯s))
� = Ω(¯o|¯s0, a, ¯π(¯s))T(¯s0 |a, ¯π(¯s), ¯s)
o C Es&apos;&apos; Ω(¯o |¯s00, a, ¯π(¯s))T (¯s00  |a, ¯π(¯s), ¯s)
)x Ω(¯o|s0, a, ¯π(¯s)) (1)
</equation>
<bodyText confidence="0.984325466666667">
Communication is treated as another type of ob-
servation, with messages coming from a finite set
M. Each message m E M has the semantics
Pr(s|m), which represents the probability that the
world is in state s E S given that m is true. Mes-
sages m received from a partner are combined
with perceptual observations o E O, to form a
joint observation (m, o).
A literal listener, denoted L, interprets mes-
sages according to this semantics, without taking
into account the beliefs of the speaker. L assumes
that the perceptual observations and messages are
conditionally independent given the state of the
world. Using Bayes’ rule, the literal listener’s joint
observation/message distribution is
</bodyText>
<equation confidence="0.997956333333333">
Pr((o, m)|s, s0, a) = Ω(o|s0, a) Pr(m|s)
Pr(s |m) Pr(m)
= Ω(o|s0, a) EM&apos;∈M Pr(s|m0) Pr(m0) (2)
</equation>
<bodyText confidence="0.9996178">
The Pr(m) prior over messages can be estimated
from corpus data, but we use a uniform prior for
simplicity.
A literal speaker, denoted S, produces mes-
sages according to the most descriptive term:
</bodyText>
<equation confidence="0.9827005">
πS(s) = arg max p(s|m). (3)
M∈M
</equation>
<bodyText confidence="0.9972624">
The literal speaker does not model the beliefs of
the listener.
To interpret implicatures, a level-one lis-
tener, denoted L(S), models the beliefs a literal
speaker must have had to produce an utterance:
Pr(m|s) = 1[¯πS(s) = m], where ¯πS is the level-
one listener’s estimate of the speaker’s policy. In
this setting, we denote the level-one listener’s es-
timate of the speaker’s belief as ¯s, yielding the be-
lief update equation
</bodyText>
<equation confidence="0.955936">
Pr((o, m)|(s, ¯s), (s0, ¯s0), a, ¯πS(¯s)) =
Ω(o|s0, a)1[¯πS(¯s) = m] (4)
</equation>
<bodyText confidence="0.999968333333333">
The literal semantics of messages is not explicitly
included in the level-one listener’s belief update.
Instead, when he solves for the literal speaker’s
policy ¯πS, the meaning of a message is the set of
beliefs that would lead the literal speaker to pro-
duce the utterance.
A level-one speaker, S(L), produces utterances
to influence a literal listener, and a level-two lis-
tener, L(S(L)), uses two levels of belief nesting to
interpret utterances as the beliefs that a level-one
speaker might have to produce that utterance. At
each level of nesting, we apply the marginalized
</bodyText>
<page confidence="0.870319">
75
</page>
<equation confidence="0.808557733333333">
r1 r2 r3
(a) Scenario.
Message r1 r2 r3
1 1
moustache 0
2 2
1 1
glasses 0 2 2
hat 0 0 1
(b) Literal interpretations.
Message r1 r2 r3
moustache 1 0 0
glasses 0 1 0
hat 0 0 1
(c) Implicature-rich interpretations.
</equation>
<figureCaption confidence="0.8920125">
Figure 1: A simple reference game. The matrices
give distributions Pr(t = ri|utterance)
</figureCaption>
<bodyText confidence="0.999947909090909">
belief-state approach of (Vogel et al., 2013), aug-
menting the state space with another copy of the
underlying world state space, where the new copy
represents the next level of belief. For instance, the
L(S(L)) agent will make decisions in the 5×5×5
space. For an L(S(L)) state (s, ¯s, ˆs), s is the true
state of the world, s¯ is the speaker’s belief of the
state of the world, and sˆ is the speaker’s belief of
the listener’s beliefs. In the next two sections we
show how a level-one and level-two listener infer
implicatures.
</bodyText>
<sectionHeader confidence="0.989101" genericHeader="method">
3 Reference Game Implicatures
</sectionHeader>
<bodyText confidence="0.997530079365079">
Fig. 1a is the scenario for a reference game of the
sort pioneered by Rosenberg and Cohen (1964)
and Dale and Reiter (1995). The potential refer-
ents are r1, r2, and r3. Speakers use a restricted
vocabulary consisting of three messages: ‘mous-
tache’, ‘glasses’, and ‘hat’. The speaker is as-
signed a referent ri (hidden from the listener) and
produces a message on that basis. The speaker and
listener share the goal of having the listener iden-
tify the speaker’s intended referent ri.
Fig. 1b depicts the literal interpretations for
this game. It looks like the listener’s chances
of success are low. Only ‘hat’ refers unambigu-
ously. However, the language and scenario fa-
cilitate scalar implicature (Horn, 1972; Harnish,
1979; Gazdar, 1979). Briefly, the scalar implica-
ture pattern is that a speaker who is knowledgeable
about the relevant domain will choose a commu-
nicatively weak utterance U over a communica-
tively stronger utterance U&apos; iff U&apos; is false (assum-
ing U and U&apos; are relevant). The required sense of
communicative strength encompasses logical en-
tailments as well as more particularized pragmatic
partial orders (Hirschberg, 1985).
In our scenario, ‘hat’ is stronger than ‘glasses’:
the referents wearing a hat are a proper subset
of those wearing glasses. Thus, given the play-
ers’ goal, if the speaker says ‘glasses’, the lis-
tener should draw the scalar implicature that ‘hat’
is false. Thus, ‘glasses’ comes to unambiguously
refer to r2 (Fig. 1c, line 2). Similarly, though
‘moustache’ and ‘glasses’ do not literally stand in
the specific–general relationship needed for scalar
implicature, they do with ‘glasses’ pragmatically
associated with r2 (Fig. 1c, line 1).
Our implementation of these games as Dec-
POMDPs mirrors their intuitive description and
their treatment in iterated best response models
(Jager, 2007; Jager, 2012; Franke, 2009; Frank
and Goodman, 2012). The state space 5 encodes
the attributes of the referents (e.g., hat(r2) = T,
glasses(r1) = F) and includes a target variable t
identifying the speaker’s referent (hidden from the
listener). The speaker has three speech actions,
identified with the three messages. The listener
has four actions: ‘listen’ plus a ‘choose’ action ci
for each referent ri. The set of observations O is
just the set of messages (construed as utterances).
The agents receive a positive reward iff the listener
action ci corresponds to the speaker’s target t. Be-
cause this is a one-step reference game, the transi-
tion distribution T is the identity distribution.
The literal listener L interprets utterances as
a truth-conditional speaker would produce them
(Fig. 1b). The level-one speaker S(L) augments
the state space with ¯a variable ‘listener target’ and
models L’s beliefs b using the approximate meth-
ods of Sec. 2. Crucially, the optimal speaker pol-
icy 7rS(L) is such that 7rS(L)(t=r3) = ‘hat’ and
7rS(L)(t=r1) = ‘moustache’. The level-two lis-
tener L(S(L)) models S(L) via an estimate of the
‘listener target’ variable. For each speech action
m, L(S(L)) considers all values of t and the likeli-
</bodyText>
<page confidence="0.819393">
76
</page>
<bodyText confidence="0.687">
hood that S(L) would have produced m: top right top left bottom right bottom left
</bodyText>
<equation confidence="0.928362">
Pr(t=ri|m) a ✶[¯irS(L)(t=ri) = m]
</equation>
<bodyText confidence="0.9995572">
Since S(L) uses ‘hat’ to describe r3 and
‘moustache’ to describe r1, L(S(L)) correctly in-
fers that ‘glasses’ refers to r2, completing Fig. 1c’s
full implicature-rich pattern of mutual exclusivity
(Clark, 1987; Frank et al., 2009).
This basic pattern is robustly attested empiri-
cally in human data. The experimental data are,
of course, invariably less crisp than our idealized
model predicts, but many important sources of
variation could be brought into our model, with
the addition of strong salience priors (Frank and
Goodman, 2012; Stiller et al., 2011), assumptions
about bounded rationality (Camerer et al., 2004;
Franke, 2009), and a ‘soft-max’ view of the lis-
tener (Frank et al., 2009).
</bodyText>
<sectionHeader confidence="0.995012" genericHeader="method">
4 Cards World Implicatures
</sectionHeader>
<bodyText confidence="0.999901862068966">
The Cards corpus1 contains 1266 metadata-rich
transcripts from a two-player chat-based game.
The world is a simple maze in which a deck of
cards has been distributed. The players’ goal is to
find specific subsets of the cards, subject to a vari-
ety of constraints on what they can see and do. The
Dec-POMDP-based agents of Vogel et al. (2013)
play a simplified version in which the goal is to be
co-located with a single card. Vogel et al. show
that their agents’ linguistic behavior is broadly
Gricean. However, their agents’ language is too
simple to reveal implicatures. The present section
remedies this shortcoming. Implicature-rich inter-
pretations are an immediate consequence.
We implement the simplified Cards tasks as fol-
lows. The state space S is composed of the loca-
tion of each player and the location of the card.
The transition distribution T (s�|s, a1, a2) encodes
the outcome of movement actions. Agents receive
one of two sensor observations, indicating whether
the card is at their current location. The players are
rewarded when they are both located on the card.
Each player begins knowing his own location, but
not the location of the other player nor of the card.
The players have four movement actions (‘up’,
‘down’, ‘left’, ‘right’) and nine speech actions in-
terpreted as identifying card locations. Fig. 2 de-
picts these utterances as a partial order determined
by entailment. These general-to-specific relation-
</bodyText>
<footnote confidence="0.667818">
1http://cardscorpus.christopherpotts.net
</footnote>
<figure confidence="0.509345">
xx&apos;z ❙ ❙
top right left bottom middle
</figure>
<figureCaption confidence="0.927914">
Figure 2: Cards world utterance actions.
</figureCaption>
<figure confidence="0.485850333333333">
top left (5.75) top (6.68) top right (5.57)
left (6.81) middle (7.16) right (6.86)
bottom left (6.11) bottom (6.37) bottom right (5.42)
</figure>
<figureCaption confidence="0.9832295">
Figure 3: Literal interpretations derived from the
Cards corpus. The entropy of each distribution is
included in parentheses. Each term is estimated
from all tokens that contain it, which washes
out implicature-rich usage, thereby providing our
model with an empirically-grounded literal start.
ships show that the language can support scalar
conversational implicatures.2
Fig. 2 is not entirely appropriate in our setting,
however. Our expressions are vague; there is no
</figureCaption>
<bodyText confidence="0.987066714285714">
sharp boundary between, e.g., ‘top’ and ‘bottom’,
nor is it clear where ‘top right’ begins. To model
this vagueness, we analyze each message m as
denoting a conditional distribution Pr(x|m) over
grid squares x in the gameboard. These distribu-
tions are derived from human–human Cards inter-
actions using the data and methods of Potts (2012).
Of course, there is a tension here: our model as-
sumes that we begin with literal interpretations,
but human–human data will reflect pragmatically-
enriched usage. To get around this, we approxi-
mate literal interpretations by deriving each term’s
distribution from all the corpus tokens that con-
tain it. For example, the distribution for ‘top’ is
</bodyText>
<footnote confidence="0.9063018">
2Our agents cannot produce modified versions of ‘mid-
dle’ like ‘middle right’. These would be synonymous with
implicature-enriched general terms. We work with a simple
cost-function that treats all forms alike, but future versions of
this work will incorporate more realistic form-based costs.
</footnote>
<page confidence="0.997756">
77
</page>
<figure confidence="0.4864575">
top left (5.17) top (3.46) top right (5.04)
top left (5.82) top (5.74) top right (5.49)
bottom left (4.81) bottom (3.70) bottom right (5.04)
left (3.91) middle (2.35) right (3.58)
bottom left (5.29) bottom (5.43) bottom right (5.44)
left (6.15) middle (6.14) right (6.57)
</figure>
<figureCaption confidence="0.992239">
Figure 4: Implicature-rich interpretations, derived
using the level-one listener L(S).
</figureCaption>
<bodyText confidence="0.99432974">
estimated not only from ‘top’ but also from ‘top
right’, ‘middle right’, and so forth. The denotation
for ‘top right’ excludes simple ‘top’ and ‘right’
utterances but includes expressions like ‘very top
right’. This semantics washes out any implicature
patterns, thereby giving us a proper literal starting
point. Fig. 3 shows these denotations for the full
set of expressions. The entailment relations from
Fig. 2 are (fuzzily) evident. For example, the areas
of high probability for ‘right’ properly contain the
areas of high probability for ‘top right’.
To show how the Dec-POMDP model delivers
implicatures, we begin with a literal speaker S
who does not consider the location of the other
player and instead searches the board until he finds
the card. After finding it, he communicates the re-
ferring expression with highest literal probability
for his location, using the distributions from Fig. 3.
We denote the literal speaker’s policy by πS. The
level-one listener L(S) tracks an estimate of S’s lo-
cation and beliefs about the card location. Using
the approximation defined in Sec. 2, L(S) inter-
prets an utterance m as Pr(m|s) = ✶l¯πS(s) = m].
Thus, the meaning of each m is the set of be-
liefs that S might have to produce this utterance.
Fig. 4 shows how L(S) interprets each message.
The meaning of general terms like ‘top’ and ‘right’
now exclude their modified counterparts. This
is evident in the lack of overlap between high-
probability areas and in the lower entropy values.
Direct evaluation of this result against the cor-
pus data is not possible, because the corpus does
not encode interpretations. However, we expect
Figure 5: Distributions reflecting human speakers’
aggregate referential intentions . Each term is es-
timated only from tokens that exactly match it.
listener interpretations to align with speaker in-
tentions, and we can gain insight into (aggregate)
speaker intentions using our method for ground-
ing referential terms. Whereas the literal inter-
pretation for message m is obtained from all the
tokens that contain it (Fig. 3), the speaker’s in-
tended interpretation for m is obtained from all
of the tokens that exactly match it. For instance,
the meaning of ‘top’ now excludes tokens like ‘top
left’. Fig. 5 shows these denotations, which mirror
the distributions predicted by our model (Fig. 4).
Thus, the L(S) model correctly infers the prag-
matic meaning of referring expressions as used by
human speakers, albeit in an idealized manner.
</bodyText>
<sectionHeader confidence="0.999704" genericHeader="conclusions">
5 Future Work
</sectionHeader>
<bodyText confidence="0.999961384615384">
We showed that implicatures arise in cooperative
contexts from nested belief models. Our listener-
centric implicatures must be combined with ratio-
nal speaker behavior (Vogel et al., 2013) to pro-
duce general dialog agents. The computational
complexity of Dec-POMDPs is prohibitive, and
our approximations can be problematic for deep
belief nesting. Future work will explore sampling-
based approaches to belief update and decision
making (Doshi and Gmytrasiewicz, 2009) to over-
come these problems. These steps will move us
closer to a computationally effective, unified the-
ory of pragmatic enrichment and decision making.
</bodyText>
<footnote confidence="0.858440333333333">
Acknowledgements This research was supported in
part by ONR grants N00014-10-1-0109 and N00014-13-1-
0287 and ARO grant W911NF-07-1-0216.
</footnote>
<page confidence="0.997019">
78
</page>
<sectionHeader confidence="0.990289" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999778504761905">
Leon Bergen, Noah D. Goodman, and Roger Levy.
2012. That’s what she (could have) said: How alter-
native utterances affect language use. In Proceed-
ings of the Thirty-Fourth Annual Conference of the
Cognitive Science Society.
Daniel S. Bernstein, Robert Givan, Neil Immerman,
and Shlomo Zilberstein. 2002. The complexity of
decentralized control of Markov decision processes.
Mathematics of Operations Research, 27(4):819–
840.
Colin F. Camerer, Teck-Hua Ho, and Juin-Kuan Chong.
2004. A cognitive hierarchy model of games. The
Quarterly Journal of Economics, 119(3):861–898,
August.
Herbert H. Clark and Deanna Wilkes-Gibbs. 1986.
Referring as a collaborative process. Cognition,
22(1):1–39.
Eve V. Clark. 1987. The principle of contrast: A con-
straint on language acquisition. In Brian MacWhin-
ney, editor, Mechanisms of Language Acquisition,
pages 1–33. Erlbaum, Hillsdale, NJ.
Robert Dale and Ehud Reiter. 1995. Computational
interpretations of the Gricean maxims in the gener-
ation of referring expressions. Cognitive Science,
19(2):233–263.
Judith Degen and Michael Franke. 2012. Optimal rea-
soning about referential expressions. In Proceed-
ings of SemDIAL 2012, Paris, September.
David DeVault and Matthew Stone. 2007. Manag-
ing ambiguities across utterances in dialogue. In
Ron Artstein and Laure Vieu, editors, Proceedings
of DECALOG 2007: Workshop on the Semantics
and Pragmatics of Dialogue.
Prashant Doshi and Piotr J. Gmytrasiewicz. 2009.
Monte carlo sampling methods for approximating
interactive pomdps. J. Artif. Int. Res., 34(1):297–
337, March.
Michael C. Frank and Noah D. Goodman. 2012. Pre-
dicting pragmatic reasoning in language games. Sci-
ence, 336(6084):998.
Michael C. Frank, Noah D. Goodman, and Joshua B.
Tenenbaum. 2009. Using speakers’ referential in-
tentions to model early cross-situational word learn-
ing. Psychological Science, 20(5):579–585.
Michael Franke. 2009. Signal to Act: Game Theory
in Pragmatics. ILLC Dissertation Series. Institute
for Logic, Language and Computation, University
of Amsterdam.
Gerald Gazdar. 1979. Pragmatics: Implicature, Pre-
supposition and Logical Form. Academic Press,
New York.
Piotr J. Gmytrasiewicz and Prashant Doshi. 2005. A
framework for sequential planning in multi-agent
settings. Journal of Artificial Intelligence Research,
24:24–49.
Dave Golland, Percy Liang, and Dan Klein. 2010.
A game-theoretic approach to generating spatial de-
scriptions. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 410–419, Cambridge, MA, October.
ACL.
H. Paul Grice. 1975. Logic and conversation. In Pe-
ter Cole and Jerry Morgan, editors, Syntax and Se-
mantics, volume 3: Speech Acts, pages 43–58. Aca-
demic Press, New York.
Robert M. Harnish. 1979. Logical form and implica-
ture. In Linguistic Communication and Speech Acts,
pages 313–391. MIT Press, Cambridge, MA.
Julia Hirschberg. 1985. A Theory of Scalar Implica-
ture. Ph.D. thesis, University of Pennsylvania.
Laurence R Horn. 1972. On the Semantic Properties of
Logical Operators in English. Ph.D. thesis, UCLA,
Los Angeles.
Gerhard J¨ager. 2007. Game dynamics connects se-
mantics and pragmatics. In Ahti-Veikko Pietarinen,
editor, Game Theory and Linguistic Meaning, pages
89–102. Elsevier, Amsterdam.
Gerhard J¨ager. 2012. Game theory in semantics and
pragmatics. In Maienborn et al. (Maienborn et al.,
2012).
Emiel Krahmer and Kees van Deemter. 2012. Compu-
tational generation of referring expressions: A sur-
vey. Computational Linguistics, 38(1):173–218.
Claudia Maienborn, Klaus von Heusinger, and Paul
Portner, editors. 2012. Semantics: An International
Handbook of Natural Language Meaning, volume 3.
Mouton de Gruyter, Berlin.
Christopher Potts. 2012. Goal-driven answers in the
Cards dialogue corpus. In Nathan Arnett and Ryan
Bennett, editors, Proceedings of the 30th West Coast
Conference on Formal Linguistics, Somerville, MA.
Cascadilla Press.
Hannah Rohde, Scott Seyfarth, Brady Clark, Gerhard
J¨ager, and Stefan Kaufmann. 2012. Communicat-
ing with cost-based implicature: A game-theoretic
approach to ambiguity. In The 16th Workshop on
the Semantics and Pragmatics of Dialogue, Paris,
September.
Seymour Rosenberg and Bertram D. Cohen. 1964.
Speakers’ and listeners’ processes in a word com-
munication task. Science, 145:1201–1203.
Matthijs T. J. Spaan and Nikos Vlassis. 2005.
Perseus: Randomized point-based value iteration
for POMDPs. Journal of Artificial Intelligence Re-
search, 24(1):195–220, August.
</reference>
<page confidence="0.977316">
79
</page>
<reference confidence="0.999596916666667">
Alex Stiller, Noah D. Goodman, and Michael C. Frank.
2011. Ad-hoc scalar implicature in adults and chil-
dren. In Proceedings of the 33rd Annual Meeting of
the Cognitive Science Society, Boston, July.
Adam Vogel, Max Bodoia, Dan Jurafsky, and Christo-
pher Potts. 2013. Emergence of Gricean max-
ims from multi-agent decision theory. In Human
Language Technologies: The 2013 Annual Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics, Atlanta, Geor-
gia, June. Association for Computational Linguis-
tics.
</reference>
<page confidence="0.998221">
80
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.480930">
<title confidence="0.997039">Implicatures and Nested Beliefs in Approximate Decentralized-POMDPs</title>
<author confidence="0.924916">Christopher Potts Vogel</author>
<affiliation confidence="0.592702">Stanford</affiliation>
<address confidence="0.735663">Stanford, CA,</address>
<abstract confidence="0.997162133333333">Conversational implicatures involve reasoning about multiply nested belief structures. This complexity poses significant challenges for computational models of conversation and cognition. We show that agents in the multi-agent Decentralized- POMDP reach implicature-rich interpretations simply as a by-product of the way they reason about each other to maximize joint utility. Our simulations involve a reference game of the sort studied in psychology and linguistics as well as a dynamic, interactional scenario involving implemented artificial agents.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Leon Bergen</author>
<author>Noah D Goodman</author>
<author>Roger Levy</author>
</authors>
<title>That’s what she (could have) said: How alternative utterances affect language use.</title>
<date>2012</date>
<booktitle>In Proceedings of the Thirty-Fourth Annual Conference of the Cognitive Science Society.</booktitle>
<contexts>
<context position="1951" citStr="Bergen et al., 2012" startWordPosition="278" endWordPosition="281"> considerable attention in the context of simple reference games in which the listener uses the speaker’s utterance to try to identify the speaker’s intended referent (Rosenberg and Cohen, 1964; Clark and WilkesGibbs, 1986; Dale and Reiter, 1995; DeVault and Stone, 2007; Krahmer and van Deemter, 2012). Many implicature patterns can be embedded in these games using specific combinations of potential referents and message sets. The paradigm has proven fruitful not only for evaluating computational models (Golland et al., 2010; Degen and Franke, 2012; Frank and Goodman, 2012; Rohde et al., 2012; Bergen et al., 2012) but also for studying children’s pragmatic abilities without implicitly assuming they have mastered challenging linguistic structures (Stiller et al., 2011). In this paper, we extend these results beyond simple reference games to full decision-problems in which the agents reason about language and action together over time. To do this, we use the Decentralized Partially Observable Markov Decision Process (Dec-POMDP) to implement agents that are capable of manipulating the multiply-nested belief structures required for implicature calculation. Optimal decision making in Dec-POMDPs is NEXP comp</context>
</contexts>
<marker>Bergen, Goodman, Levy, 2012</marker>
<rawString>Leon Bergen, Noah D. Goodman, and Roger Levy. 2012. That’s what she (could have) said: How alternative utterances affect language use. In Proceedings of the Thirty-Fourth Annual Conference of the Cognitive Science Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel S Bernstein</author>
<author>Robert Givan</author>
<author>Neil Immerman</author>
<author>Shlomo Zilberstein</author>
</authors>
<title>The complexity of decentralized control of Markov decision processes.</title>
<date>2002</date>
<journal>Mathematics of Operations Research,</journal>
<volume>27</volume>
<issue>4</issue>
<pages>840</pages>
<contexts>
<context position="3050" citStr="Bernstein et al., 2002" startWordPosition="437" endWordPosition="440">e multiply-nested belief structures required for implicature calculation. Optimal decision making in Dec-POMDPs is NEXP complete, so we employ the single-agent POMDP approximation of Vogel et al. (2013). We show that agents in the Dec-POMDP reach implicature-rich interpretations simply as a byproduct of the way they reason about each other to maximize joint utility. Our simulations involve a reference game and a dynamic, interactional scenario involving implemented artificial agents. 2 Decision-Theoretic Communication The Decentralized Partially Observable Markov Decision Process (Dec-POMDP) (Bernstein et al., 2002) is a multi-agent generalization of the POMDP, where agents act to maximize a shared utility function. Formally, a Dec-POMDP consists of a tuple (S, A, O, R, T, Q, b0, γ). S is a finite set of states, A is the set of actions, O is the set of observations, and T(s&apos;|a1, a2, s) is the transition distribution which determines what effect the joint action (a1, a2) has on the state of the world. The true state s E S is not observable to the agents, who must utilize observations o E O, which are emitted after each action according to the observation distribution Q(o1, o2|s&apos;, a). The reward function R</context>
<context position="5328" citStr="Bernstein et al., 2002" startWordPosition="834" endWordPosition="837">(st, at1, at2)|b0, π1, π2]. Using the assumption that the other agent tracks one less level of belief, we can solve for the other agent’s policy ¯π, which allows us to estimate his actions and beliefs over time. To construct policies, we use Perseus (Spaan and Vlassis, 2005), a point-based value iteration algorithm. Even tracking just one level of nested beliefs quickly leads to a combinatorial explosion in the number of belief states the other agent might have. This causes decision making in Dec-POMDPs to be NEXP complete, limiting their application to problems with only a handful of states (Bernstein et al., 2002). To ameliorate this difficulty, we use the method of Vogel et al. (2013), which creates a single-agent approximation to the full DecPOMDP. To form this single-agent POMDP, we augment the state space to be S x S, where the second set of state variables allows us to model the other agent’s beliefs. We maintain a point estimate ¯b of the other agent’s beliefs, which is formed by summing out observations O that the other player might have received. To accomplish this, we factor the transition distribution into two terms: T((s0, ¯s0)|a, ¯π(¯s), (s, ¯s)) = T¯ (¯s0|s0, a, ¯π(¯s), (s, ¯s))T (s0|a, ¯π</context>
</contexts>
<marker>Bernstein, Givan, Immerman, Zilberstein, 2002</marker>
<rawString>Daniel S. Bernstein, Robert Givan, Neil Immerman, and Shlomo Zilberstein. 2002. The complexity of decentralized control of Markov decision processes. Mathematics of Operations Research, 27(4):819– 840.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin F Camerer</author>
<author>Teck-Hua Ho</author>
<author>Juin-Kuan Chong</author>
</authors>
<title>A cognitive hierarchy model of games.</title>
<date>2004</date>
<journal>The Quarterly Journal of Economics,</journal>
<volume>119</volume>
<issue>3</issue>
<contexts>
<context position="12993" citStr="Camerer et al., 2004" startWordPosition="2112" endWordPosition="2115">] Since S(L) uses ‘hat’ to describe r3 and ‘moustache’ to describe r1, L(S(L)) correctly infers that ‘glasses’ refers to r2, completing Fig. 1c’s full implicature-rich pattern of mutual exclusivity (Clark, 1987; Frank et al., 2009). This basic pattern is robustly attested empirically in human data. The experimental data are, of course, invariably less crisp than our idealized model predicts, but many important sources of variation could be brought into our model, with the addition of strong salience priors (Frank and Goodman, 2012; Stiller et al., 2011), assumptions about bounded rationality (Camerer et al., 2004; Franke, 2009), and a ‘soft-max’ view of the listener (Frank et al., 2009). 4 Cards World Implicatures The Cards corpus1 contains 1266 metadata-rich transcripts from a two-player chat-based game. The world is a simple maze in which a deck of cards has been distributed. The players’ goal is to find specific subsets of the cards, subject to a variety of constraints on what they can see and do. The Dec-POMDP-based agents of Vogel et al. (2013) play a simplified version in which the goal is to be co-located with a single card. Vogel et al. show that their agents’ linguistic behavior is broadly Gr</context>
</contexts>
<marker>Camerer, Ho, Chong, 2004</marker>
<rawString>Colin F. Camerer, Teck-Hua Ho, and Juin-Kuan Chong. 2004. A cognitive hierarchy model of games. The Quarterly Journal of Economics, 119(3):861–898, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert H Clark</author>
<author>Deanna Wilkes-Gibbs</author>
</authors>
<title>Referring as a collaborative process.</title>
<date>1986</date>
<journal>Cognition,</journal>
<volume>22</volume>
<issue>1</issue>
<marker>Clark, Wilkes-Gibbs, 1986</marker>
<rawString>Herbert H. Clark and Deanna Wilkes-Gibbs. 1986. Referring as a collaborative process. Cognition, 22(1):1–39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eve V Clark</author>
</authors>
<title>The principle of contrast: A constraint on language acquisition.</title>
<date>1987</date>
<booktitle>In Brian MacWhinney, editor, Mechanisms of Language Acquisition,</booktitle>
<pages>1--33</pages>
<publisher>Erlbaum,</publisher>
<location>Hillsdale, NJ.</location>
<contexts>
<context position="12583" citStr="Clark, 1987" startWordPosition="2050" endWordPosition="2051">. 2. Crucially, the optimal speaker policy 7rS(L) is such that 7rS(L)(t=r3) = ‘hat’ and 7rS(L)(t=r1) = ‘moustache’. The level-two listener L(S(L)) models S(L) via an estimate of the ‘listener target’ variable. For each speech action m, L(S(L)) considers all values of t and the likeli76 hood that S(L) would have produced m: top right top left bottom right bottom left Pr(t=ri|m) a ✶[¯irS(L)(t=ri) = m] Since S(L) uses ‘hat’ to describe r3 and ‘moustache’ to describe r1, L(S(L)) correctly infers that ‘glasses’ refers to r2, completing Fig. 1c’s full implicature-rich pattern of mutual exclusivity (Clark, 1987; Frank et al., 2009). This basic pattern is robustly attested empirically in human data. The experimental data are, of course, invariably less crisp than our idealized model predicts, but many important sources of variation could be brought into our model, with the addition of strong salience priors (Frank and Goodman, 2012; Stiller et al., 2011), assumptions about bounded rationality (Camerer et al., 2004; Franke, 2009), and a ‘soft-max’ view of the listener (Frank et al., 2009). 4 Cards World Implicatures The Cards corpus1 contains 1266 metadata-rich transcripts from a two-player chat-based</context>
</contexts>
<marker>Clark, 1987</marker>
<rawString>Eve V. Clark. 1987. The principle of contrast: A constraint on language acquisition. In Brian MacWhinney, editor, Mechanisms of Language Acquisition, pages 1–33. Erlbaum, Hillsdale, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
<author>Ehud Reiter</author>
</authors>
<title>Computational interpretations of the Gricean maxims in the generation of referring expressions.</title>
<date>1995</date>
<journal>Cognitive Science,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1576" citStr="Dale and Reiter, 1995" startWordPosition="218" endWordPosition="221">erative. As Grice conceived of them, implicatures crucially involve reasoning about multiply-nested belief structures: roughly, for p to count as an implicature, the speaker must believe that the listener will infer that the speaker believes p. This complexity makes implicatures an important testing ground for models of conversation and cognition. Implicatures have received considerable attention in the context of simple reference games in which the listener uses the speaker’s utterance to try to identify the speaker’s intended referent (Rosenberg and Cohen, 1964; Clark and WilkesGibbs, 1986; Dale and Reiter, 1995; DeVault and Stone, 2007; Krahmer and van Deemter, 2012). Many implicature patterns can be embedded in these games using specific combinations of potential referents and message sets. The paradigm has proven fruitful not only for evaluating computational models (Golland et al., 2010; Degen and Franke, 2012; Frank and Goodman, 2012; Rohde et al., 2012; Bergen et al., 2012) but also for studying children’s pragmatic abilities without implicitly assuming they have mastered challenging linguistic structures (Stiller et al., 2011). In this paper, we extend these results beyond simple reference gam</context>
<context position="9323" citStr="Dale and Reiter (1995)" startWordPosition="1526" endWordPosition="1529">e state space with another copy of the underlying world state space, where the new copy represents the next level of belief. For instance, the L(S(L)) agent will make decisions in the 5×5×5 space. For an L(S(L)) state (s, ¯s, ˆs), s is the true state of the world, s¯ is the speaker’s belief of the state of the world, and sˆ is the speaker’s belief of the listener’s beliefs. In the next two sections we show how a level-one and level-two listener infer implicatures. 3 Reference Game Implicatures Fig. 1a is the scenario for a reference game of the sort pioneered by Rosenberg and Cohen (1964) and Dale and Reiter (1995). The potential referents are r1, r2, and r3. Speakers use a restricted vocabulary consisting of three messages: ‘moustache’, ‘glasses’, and ‘hat’. The speaker is assigned a referent ri (hidden from the listener) and produces a message on that basis. The speaker and listener share the goal of having the listener identify the speaker’s intended referent ri. Fig. 1b depicts the literal interpretations for this game. It looks like the listener’s chances of success are low. Only ‘hat’ refers unambiguously. However, the language and scenario facilitate scalar implicature (Horn, 1972; Harnish, 1979;</context>
</contexts>
<marker>Dale, Reiter, 1995</marker>
<rawString>Robert Dale and Ehud Reiter. 1995. Computational interpretations of the Gricean maxims in the generation of referring expressions. Cognitive Science, 19(2):233–263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judith Degen</author>
<author>Michael Franke</author>
</authors>
<title>Optimal reasoning about referential expressions.</title>
<date>2012</date>
<booktitle>In Proceedings of SemDIAL 2012,</booktitle>
<location>Paris,</location>
<contexts>
<context position="1884" citStr="Degen and Franke, 2012" startWordPosition="266" endWordPosition="269"> for models of conversation and cognition. Implicatures have received considerable attention in the context of simple reference games in which the listener uses the speaker’s utterance to try to identify the speaker’s intended referent (Rosenberg and Cohen, 1964; Clark and WilkesGibbs, 1986; Dale and Reiter, 1995; DeVault and Stone, 2007; Krahmer and van Deemter, 2012). Many implicature patterns can be embedded in these games using specific combinations of potential referents and message sets. The paradigm has proven fruitful not only for evaluating computational models (Golland et al., 2010; Degen and Franke, 2012; Frank and Goodman, 2012; Rohde et al., 2012; Bergen et al., 2012) but also for studying children’s pragmatic abilities without implicitly assuming they have mastered challenging linguistic structures (Stiller et al., 2011). In this paper, we extend these results beyond simple reference games to full decision-problems in which the agents reason about language and action together over time. To do this, we use the Decentralized Partially Observable Markov Decision Process (Dec-POMDP) to implement agents that are capable of manipulating the multiply-nested belief structures required for implicat</context>
</contexts>
<marker>Degen, Franke, 2012</marker>
<rawString>Judith Degen and Michael Franke. 2012. Optimal reasoning about referential expressions. In Proceedings of SemDIAL 2012, Paris, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David DeVault</author>
<author>Matthew Stone</author>
</authors>
<title>Managing ambiguities across utterances in dialogue.</title>
<date>2007</date>
<booktitle>In Ron Artstein and Laure Vieu, editors, Proceedings of DECALOG 2007: Workshop on the Semantics and Pragmatics of Dialogue.</booktitle>
<contexts>
<context position="1601" citStr="DeVault and Stone, 2007" startWordPosition="222" endWordPosition="225">ived of them, implicatures crucially involve reasoning about multiply-nested belief structures: roughly, for p to count as an implicature, the speaker must believe that the listener will infer that the speaker believes p. This complexity makes implicatures an important testing ground for models of conversation and cognition. Implicatures have received considerable attention in the context of simple reference games in which the listener uses the speaker’s utterance to try to identify the speaker’s intended referent (Rosenberg and Cohen, 1964; Clark and WilkesGibbs, 1986; Dale and Reiter, 1995; DeVault and Stone, 2007; Krahmer and van Deemter, 2012). Many implicature patterns can be embedded in these games using specific combinations of potential referents and message sets. The paradigm has proven fruitful not only for evaluating computational models (Golland et al., 2010; Degen and Franke, 2012; Frank and Goodman, 2012; Rohde et al., 2012; Bergen et al., 2012) but also for studying children’s pragmatic abilities without implicitly assuming they have mastered challenging linguistic structures (Stiller et al., 2011). In this paper, we extend these results beyond simple reference games to full decision-probl</context>
</contexts>
<marker>DeVault, Stone, 2007</marker>
<rawString>David DeVault and Matthew Stone. 2007. Managing ambiguities across utterances in dialogue. In Ron Artstein and Laure Vieu, editors, Proceedings of DECALOG 2007: Workshop on the Semantics and Pragmatics of Dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Prashant Doshi</author>
<author>Piotr J Gmytrasiewicz</author>
</authors>
<title>Monte carlo sampling methods for approximating interactive pomdps.</title>
<date>2009</date>
<journal>J. Artif. Int. Res.,</journal>
<volume>34</volume>
<issue>1</issue>
<pages>337</pages>
<marker>Doshi, Gmytrasiewicz, 2009</marker>
<rawString>Prashant Doshi and Piotr J. Gmytrasiewicz. 2009. Monte carlo sampling methods for approximating interactive pomdps. J. Artif. Int. Res., 34(1):297– 337, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael C Frank</author>
<author>Noah D Goodman</author>
</authors>
<title>Predicting pragmatic reasoning in language games.</title>
<date>2012</date>
<journal>Science,</journal>
<volume>336</volume>
<issue>6084</issue>
<contexts>
<context position="1909" citStr="Frank and Goodman, 2012" startWordPosition="270" endWordPosition="273">ion and cognition. Implicatures have received considerable attention in the context of simple reference games in which the listener uses the speaker’s utterance to try to identify the speaker’s intended referent (Rosenberg and Cohen, 1964; Clark and WilkesGibbs, 1986; Dale and Reiter, 1995; DeVault and Stone, 2007; Krahmer and van Deemter, 2012). Many implicature patterns can be embedded in these games using specific combinations of potential referents and message sets. The paradigm has proven fruitful not only for evaluating computational models (Golland et al., 2010; Degen and Franke, 2012; Frank and Goodman, 2012; Rohde et al., 2012; Bergen et al., 2012) but also for studying children’s pragmatic abilities without implicitly assuming they have mastered challenging linguistic structures (Stiller et al., 2011). In this paper, we extend these results beyond simple reference games to full decision-problems in which the agents reason about language and action together over time. To do this, we use the Decentralized Partially Observable Markov Decision Process (Dec-POMDP) to implement agents that are capable of manipulating the multiply-nested belief structures required for implicature calculation. Optimal </context>
<context position="11076" citStr="Frank and Goodman, 2012" startWordPosition="1803" endWordPosition="1806">ng glasses. Thus, given the players’ goal, if the speaker says ‘glasses’, the listener should draw the scalar implicature that ‘hat’ is false. Thus, ‘glasses’ comes to unambiguously refer to r2 (Fig. 1c, line 2). Similarly, though ‘moustache’ and ‘glasses’ do not literally stand in the specific–general relationship needed for scalar implicature, they do with ‘glasses’ pragmatically associated with r2 (Fig. 1c, line 1). Our implementation of these games as DecPOMDPs mirrors their intuitive description and their treatment in iterated best response models (Jager, 2007; Jager, 2012; Franke, 2009; Frank and Goodman, 2012). The state space 5 encodes the attributes of the referents (e.g., hat(r2) = T, glasses(r1) = F) and includes a target variable t identifying the speaker’s referent (hidden from the listener). The speaker has three speech actions, identified with the three messages. The listener has four actions: ‘listen’ plus a ‘choose’ action ci for each referent ri. The set of observations O is just the set of messages (construed as utterances). The agents receive a positive reward iff the listener action ci corresponds to the speaker’s target t. Because this is a one-step reference game, the transition dis</context>
<context position="12909" citStr="Frank and Goodman, 2012" startWordPosition="2100" endWordPosition="2103">roduced m: top right top left bottom right bottom left Pr(t=ri|m) a ✶[¯irS(L)(t=ri) = m] Since S(L) uses ‘hat’ to describe r3 and ‘moustache’ to describe r1, L(S(L)) correctly infers that ‘glasses’ refers to r2, completing Fig. 1c’s full implicature-rich pattern of mutual exclusivity (Clark, 1987; Frank et al., 2009). This basic pattern is robustly attested empirically in human data. The experimental data are, of course, invariably less crisp than our idealized model predicts, but many important sources of variation could be brought into our model, with the addition of strong salience priors (Frank and Goodman, 2012; Stiller et al., 2011), assumptions about bounded rationality (Camerer et al., 2004; Franke, 2009), and a ‘soft-max’ view of the listener (Frank et al., 2009). 4 Cards World Implicatures The Cards corpus1 contains 1266 metadata-rich transcripts from a two-player chat-based game. The world is a simple maze in which a deck of cards has been distributed. The players’ goal is to find specific subsets of the cards, subject to a variety of constraints on what they can see and do. The Dec-POMDP-based agents of Vogel et al. (2013) play a simplified version in which the goal is to be co-located with a</context>
</contexts>
<marker>Frank, Goodman, 2012</marker>
<rawString>Michael C. Frank and Noah D. Goodman. 2012. Predicting pragmatic reasoning in language games. Science, 336(6084):998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael C Frank</author>
<author>Noah D Goodman</author>
<author>Joshua B Tenenbaum</author>
</authors>
<title>Using speakers’ referential intentions to model early cross-situational word learning.</title>
<date>2009</date>
<journal>Psychological Science,</journal>
<volume>20</volume>
<issue>5</issue>
<contexts>
<context position="12604" citStr="Frank et al., 2009" startWordPosition="2052" endWordPosition="2055">y, the optimal speaker policy 7rS(L) is such that 7rS(L)(t=r3) = ‘hat’ and 7rS(L)(t=r1) = ‘moustache’. The level-two listener L(S(L)) models S(L) via an estimate of the ‘listener target’ variable. For each speech action m, L(S(L)) considers all values of t and the likeli76 hood that S(L) would have produced m: top right top left bottom right bottom left Pr(t=ri|m) a ✶[¯irS(L)(t=ri) = m] Since S(L) uses ‘hat’ to describe r3 and ‘moustache’ to describe r1, L(S(L)) correctly infers that ‘glasses’ refers to r2, completing Fig. 1c’s full implicature-rich pattern of mutual exclusivity (Clark, 1987; Frank et al., 2009). This basic pattern is robustly attested empirically in human data. The experimental data are, of course, invariably less crisp than our idealized model predicts, but many important sources of variation could be brought into our model, with the addition of strong salience priors (Frank and Goodman, 2012; Stiller et al., 2011), assumptions about bounded rationality (Camerer et al., 2004; Franke, 2009), and a ‘soft-max’ view of the listener (Frank et al., 2009). 4 Cards World Implicatures The Cards corpus1 contains 1266 metadata-rich transcripts from a two-player chat-based game. The world is a</context>
</contexts>
<marker>Frank, Goodman, Tenenbaum, 2009</marker>
<rawString>Michael C. Frank, Noah D. Goodman, and Joshua B. Tenenbaum. 2009. Using speakers’ referential intentions to model early cross-situational word learning. Psychological Science, 20(5):579–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Franke</author>
</authors>
<title>Signal to Act: Game Theory in Pragmatics. ILLC Dissertation Series. Institute for Logic, Language and Computation,</title>
<date>2009</date>
<location>University of Amsterdam.</location>
<contexts>
<context position="11050" citStr="Franke, 2009" startWordPosition="1801" endWordPosition="1802">of those wearing glasses. Thus, given the players’ goal, if the speaker says ‘glasses’, the listener should draw the scalar implicature that ‘hat’ is false. Thus, ‘glasses’ comes to unambiguously refer to r2 (Fig. 1c, line 2). Similarly, though ‘moustache’ and ‘glasses’ do not literally stand in the specific–general relationship needed for scalar implicature, they do with ‘glasses’ pragmatically associated with r2 (Fig. 1c, line 1). Our implementation of these games as DecPOMDPs mirrors their intuitive description and their treatment in iterated best response models (Jager, 2007; Jager, 2012; Franke, 2009; Frank and Goodman, 2012). The state space 5 encodes the attributes of the referents (e.g., hat(r2) = T, glasses(r1) = F) and includes a target variable t identifying the speaker’s referent (hidden from the listener). The speaker has three speech actions, identified with the three messages. The listener has four actions: ‘listen’ plus a ‘choose’ action ci for each referent ri. The set of observations O is just the set of messages (construed as utterances). The agents receive a positive reward iff the listener action ci corresponds to the speaker’s target t. Because this is a one-step referenc</context>
<context position="13008" citStr="Franke, 2009" startWordPosition="2116" endWordPosition="2117">’ to describe r3 and ‘moustache’ to describe r1, L(S(L)) correctly infers that ‘glasses’ refers to r2, completing Fig. 1c’s full implicature-rich pattern of mutual exclusivity (Clark, 1987; Frank et al., 2009). This basic pattern is robustly attested empirically in human data. The experimental data are, of course, invariably less crisp than our idealized model predicts, but many important sources of variation could be brought into our model, with the addition of strong salience priors (Frank and Goodman, 2012; Stiller et al., 2011), assumptions about bounded rationality (Camerer et al., 2004; Franke, 2009), and a ‘soft-max’ view of the listener (Frank et al., 2009). 4 Cards World Implicatures The Cards corpus1 contains 1266 metadata-rich transcripts from a two-player chat-based game. The world is a simple maze in which a deck of cards has been distributed. The players’ goal is to find specific subsets of the cards, subject to a variety of constraints on what they can see and do. The Dec-POMDP-based agents of Vogel et al. (2013) play a simplified version in which the goal is to be co-located with a single card. Vogel et al. show that their agents’ linguistic behavior is broadly Gricean. However,</context>
</contexts>
<marker>Franke, 2009</marker>
<rawString>Michael Franke. 2009. Signal to Act: Game Theory in Pragmatics. ILLC Dissertation Series. Institute for Logic, Language and Computation, University of Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerald Gazdar</author>
</authors>
<title>Pragmatics: Implicature, Presupposition and Logical Form.</title>
<date>1979</date>
<publisher>Academic Press,</publisher>
<location>New York.</location>
<contexts>
<context position="9937" citStr="Gazdar, 1979" startWordPosition="1627" endWordPosition="1628"> The potential referents are r1, r2, and r3. Speakers use a restricted vocabulary consisting of three messages: ‘moustache’, ‘glasses’, and ‘hat’. The speaker is assigned a referent ri (hidden from the listener) and produces a message on that basis. The speaker and listener share the goal of having the listener identify the speaker’s intended referent ri. Fig. 1b depicts the literal interpretations for this game. It looks like the listener’s chances of success are low. Only ‘hat’ refers unambiguously. However, the language and scenario facilitate scalar implicature (Horn, 1972; Harnish, 1979; Gazdar, 1979). Briefly, the scalar implicature pattern is that a speaker who is knowledgeable about the relevant domain will choose a communicatively weak utterance U over a communicatively stronger utterance U&apos; iff U&apos; is false (assuming U and U&apos; are relevant). The required sense of communicative strength encompasses logical entailments as well as more particularized pragmatic partial orders (Hirschberg, 1985). In our scenario, ‘hat’ is stronger than ‘glasses’: the referents wearing a hat are a proper subset of those wearing glasses. Thus, given the players’ goal, if the speaker says ‘glasses’, the listene</context>
</contexts>
<marker>Gazdar, 1979</marker>
<rawString>Gerald Gazdar. 1979. Pragmatics: Implicature, Presupposition and Logical Form. Academic Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Piotr J Gmytrasiewicz</author>
<author>Prashant Doshi</author>
</authors>
<title>A framework for sequential planning in multi-agent settings.</title>
<date>2005</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>24--24</pages>
<contexts>
<context position="4585" citStr="Gmytrasiewicz and Doshi, 2005" startWordPosition="710" endWordPosition="713">onal Linguistics γ E [0, 1) is the discount factor. The true state of the world s E S is not observable to either agent. In single-agent POMDPs, agents maintain a belief state b(s) E Δ(S), which is a distribution over states. Agents acting in DecPOMDPs must take into account not only their beliefs about the state of the world, but also the beliefs of their partners, leading to nested belief states. In the model presented here, our agent models the other agent’s beliefs about the state of the world, and assumes that the other agent does not take into account our own beliefs, a common approach (Gmytrasiewicz and Doshi, 2005). Agents make decisions according to a policy πz : Δ(S) -+ A which maximizes the discounted expected reward E∞t=0 γt E[R(st, at1, at2)|b0, π1, π2]. Using the assumption that the other agent tracks one less level of belief, we can solve for the other agent’s policy ¯π, which allows us to estimate his actions and beliefs over time. To construct policies, we use Perseus (Spaan and Vlassis, 2005), a point-based value iteration algorithm. Even tracking just one level of nested beliefs quickly leads to a combinatorial explosion in the number of belief states the other agent might have. This causes d</context>
</contexts>
<marker>Gmytrasiewicz, Doshi, 2005</marker>
<rawString>Piotr J. Gmytrasiewicz and Prashant Doshi. 2005. A framework for sequential planning in multi-agent settings. Journal of Artificial Intelligence Research, 24:24–49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dave Golland</author>
<author>Percy Liang</author>
<author>Dan Klein</author>
</authors>
<title>A game-theoretic approach to generating spatial descriptions.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>410--419</pages>
<publisher>ACL.</publisher>
<location>Cambridge, MA,</location>
<contexts>
<context position="1860" citStr="Golland et al., 2010" startWordPosition="262" endWordPosition="265">portant testing ground for models of conversation and cognition. Implicatures have received considerable attention in the context of simple reference games in which the listener uses the speaker’s utterance to try to identify the speaker’s intended referent (Rosenberg and Cohen, 1964; Clark and WilkesGibbs, 1986; Dale and Reiter, 1995; DeVault and Stone, 2007; Krahmer and van Deemter, 2012). Many implicature patterns can be embedded in these games using specific combinations of potential referents and message sets. The paradigm has proven fruitful not only for evaluating computational models (Golland et al., 2010; Degen and Franke, 2012; Frank and Goodman, 2012; Rohde et al., 2012; Bergen et al., 2012) but also for studying children’s pragmatic abilities without implicitly assuming they have mastered challenging linguistic structures (Stiller et al., 2011). In this paper, we extend these results beyond simple reference games to full decision-problems in which the agents reason about language and action together over time. To do this, we use the Decentralized Partially Observable Markov Decision Process (Dec-POMDP) to implement agents that are capable of manipulating the multiply-nested belief structur</context>
</contexts>
<marker>Golland, Liang, Klein, 2010</marker>
<rawString>Dave Golland, Percy Liang, and Dan Klein. 2010. A game-theoretic approach to generating spatial descriptions. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 410–419, Cambridge, MA, October. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Paul Grice</author>
</authors>
<title>Logic and conversation.</title>
<date>1975</date>
<booktitle>Syntax and Semantics,</booktitle>
<volume>volume</volume>
<pages>43--58</pages>
<editor>In Peter Cole and Jerry Morgan, editors,</editor>
<publisher>Academic Press,</publisher>
<location>New York.</location>
<contexts>
<context position="819" citStr="Grice, 1975" startWordPosition="104" endWordPosition="105">ersational implicatures involve reasoning about multiply nested belief structures. This complexity poses significant challenges for computational models of conversation and cognition. We show that agents in the multi-agent DecentralizedPOMDP reach implicature-rich interpretations simply as a by-product of the way they reason about each other to maximize joint utility. Our simulations involve a reference game of the sort studied in psychology and linguistics as well as a dynamic, interactional scenario involving implemented artificial agents. 1 Introduction Gricean conversational implicatures (Grice, 1975) are inferences that listeners make in order to reconcile the speaker’s linguistic behavior with the assumption that the speaker is cooperative. As Grice conceived of them, implicatures crucially involve reasoning about multiply-nested belief structures: roughly, for p to count as an implicature, the speaker must believe that the listener will infer that the speaker believes p. This complexity makes implicatures an important testing ground for models of conversation and cognition. Implicatures have received considerable attention in the context of simple reference games in which the listener u</context>
</contexts>
<marker>Grice, 1975</marker>
<rawString>H. Paul Grice. 1975. Logic and conversation. In Peter Cole and Jerry Morgan, editors, Syntax and Semantics, volume 3: Speech Acts, pages 43–58. Academic Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert M Harnish</author>
</authors>
<title>Logical form and implicature.</title>
<date>1979</date>
<booktitle>In Linguistic Communication and Speech Acts,</booktitle>
<pages>313--391</pages>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="9922" citStr="Harnish, 1979" startWordPosition="1625" endWordPosition="1626"> Reiter (1995). The potential referents are r1, r2, and r3. Speakers use a restricted vocabulary consisting of three messages: ‘moustache’, ‘glasses’, and ‘hat’. The speaker is assigned a referent ri (hidden from the listener) and produces a message on that basis. The speaker and listener share the goal of having the listener identify the speaker’s intended referent ri. Fig. 1b depicts the literal interpretations for this game. It looks like the listener’s chances of success are low. Only ‘hat’ refers unambiguously. However, the language and scenario facilitate scalar implicature (Horn, 1972; Harnish, 1979; Gazdar, 1979). Briefly, the scalar implicature pattern is that a speaker who is knowledgeable about the relevant domain will choose a communicatively weak utterance U over a communicatively stronger utterance U&apos; iff U&apos; is false (assuming U and U&apos; are relevant). The required sense of communicative strength encompasses logical entailments as well as more particularized pragmatic partial orders (Hirschberg, 1985). In our scenario, ‘hat’ is stronger than ‘glasses’: the referents wearing a hat are a proper subset of those wearing glasses. Thus, given the players’ goal, if the speaker says ‘glasse</context>
</contexts>
<marker>Harnish, 1979</marker>
<rawString>Robert M. Harnish. 1979. Logical form and implicature. In Linguistic Communication and Speech Acts, pages 313–391. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hirschberg</author>
</authors>
<title>A Theory of Scalar Implicature.</title>
<date>1985</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="10337" citStr="Hirschberg, 1985" startWordPosition="1690" endWordPosition="1691">tions for this game. It looks like the listener’s chances of success are low. Only ‘hat’ refers unambiguously. However, the language and scenario facilitate scalar implicature (Horn, 1972; Harnish, 1979; Gazdar, 1979). Briefly, the scalar implicature pattern is that a speaker who is knowledgeable about the relevant domain will choose a communicatively weak utterance U over a communicatively stronger utterance U&apos; iff U&apos; is false (assuming U and U&apos; are relevant). The required sense of communicative strength encompasses logical entailments as well as more particularized pragmatic partial orders (Hirschberg, 1985). In our scenario, ‘hat’ is stronger than ‘glasses’: the referents wearing a hat are a proper subset of those wearing glasses. Thus, given the players’ goal, if the speaker says ‘glasses’, the listener should draw the scalar implicature that ‘hat’ is false. Thus, ‘glasses’ comes to unambiguously refer to r2 (Fig. 1c, line 2). Similarly, though ‘moustache’ and ‘glasses’ do not literally stand in the specific–general relationship needed for scalar implicature, they do with ‘glasses’ pragmatically associated with r2 (Fig. 1c, line 1). Our implementation of these games as DecPOMDPs mirrors their i</context>
</contexts>
<marker>Hirschberg, 1985</marker>
<rawString>Julia Hirschberg. 1985. A Theory of Scalar Implicature. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laurence R Horn</author>
</authors>
<date>1972</date>
<booktitle>On the Semantic Properties of Logical Operators in English. Ph.D. thesis,</booktitle>
<location>UCLA, Los Angeles.</location>
<contexts>
<context position="9907" citStr="Horn, 1972" startWordPosition="1623" endWordPosition="1624">and Dale and Reiter (1995). The potential referents are r1, r2, and r3. Speakers use a restricted vocabulary consisting of three messages: ‘moustache’, ‘glasses’, and ‘hat’. The speaker is assigned a referent ri (hidden from the listener) and produces a message on that basis. The speaker and listener share the goal of having the listener identify the speaker’s intended referent ri. Fig. 1b depicts the literal interpretations for this game. It looks like the listener’s chances of success are low. Only ‘hat’ refers unambiguously. However, the language and scenario facilitate scalar implicature (Horn, 1972; Harnish, 1979; Gazdar, 1979). Briefly, the scalar implicature pattern is that a speaker who is knowledgeable about the relevant domain will choose a communicatively weak utterance U over a communicatively stronger utterance U&apos; iff U&apos; is false (assuming U and U&apos; are relevant). The required sense of communicative strength encompasses logical entailments as well as more particularized pragmatic partial orders (Hirschberg, 1985). In our scenario, ‘hat’ is stronger than ‘glasses’: the referents wearing a hat are a proper subset of those wearing glasses. Thus, given the players’ goal, if the speak</context>
</contexts>
<marker>Horn, 1972</marker>
<rawString>Laurence R Horn. 1972. On the Semantic Properties of Logical Operators in English. Ph.D. thesis, UCLA, Los Angeles.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerhard J¨ager</author>
</authors>
<title>Game dynamics connects semantics and pragmatics.</title>
<date>2007</date>
<booktitle>In Ahti-Veikko Pietarinen, editor, Game Theory and Linguistic Meaning,</booktitle>
<pages>89--102</pages>
<publisher>Elsevier,</publisher>
<location>Amsterdam.</location>
<marker>J¨ager, 2007</marker>
<rawString>Gerhard J¨ager. 2007. Game dynamics connects semantics and pragmatics. In Ahti-Veikko Pietarinen, editor, Game Theory and Linguistic Meaning, pages 89–102. Elsevier, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerhard J¨ager</author>
</authors>
<title>Game theory in semantics and pragmatics.</title>
<date>2012</date>
<booktitle>In Maienborn et</booktitle>
<marker>J¨ager, 2012</marker>
<rawString>Gerhard J¨ager. 2012. Game theory in semantics and pragmatics. In Maienborn et al. (Maienborn et al., 2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emiel Krahmer</author>
<author>Kees van Deemter</author>
</authors>
<title>Computational generation of referring expressions: A survey.</title>
<date>2012</date>
<journal>Computational Linguistics,</journal>
<volume>38</volume>
<issue>1</issue>
<marker>Krahmer, van Deemter, 2012</marker>
<rawString>Emiel Krahmer and Kees van Deemter. 2012. Computational generation of referring expressions: A survey. Computational Linguistics, 38(1):173–218.</rawString>
</citation>
<citation valid="true">
<date>2012</date>
<booktitle>Semantics: An International Handbook of Natural Language Meaning, volume 3. Mouton de Gruyter,</booktitle>
<editor>Claudia Maienborn, Klaus von Heusinger, and Paul Portner, editors.</editor>
<location>Berlin.</location>
<contexts>
<context position="15597" citStr="(2012)" startWordPosition="2529" endWordPosition="2529">cature-rich usage, thereby providing our model with an empirically-grounded literal start. ships show that the language can support scalar conversational implicatures.2 Fig. 2 is not entirely appropriate in our setting, however. Our expressions are vague; there is no sharp boundary between, e.g., ‘top’ and ‘bottom’, nor is it clear where ‘top right’ begins. To model this vagueness, we analyze each message m as denoting a conditional distribution Pr(x|m) over grid squares x in the gameboard. These distributions are derived from human–human Cards interactions using the data and methods of Potts (2012). Of course, there is a tension here: our model assumes that we begin with literal interpretations, but human–human data will reflect pragmaticallyenriched usage. To get around this, we approximate literal interpretations by deriving each term’s distribution from all the corpus tokens that contain it. For example, the distribution for ‘top’ is 2Our agents cannot produce modified versions of ‘middle’ like ‘middle right’. These would be synonymous with implicature-enriched general terms. We work with a simple cost-function that treats all forms alike, but future versions of this work will incorp</context>
</contexts>
<marker>2012</marker>
<rawString>Claudia Maienborn, Klaus von Heusinger, and Paul Portner, editors. 2012. Semantics: An International Handbook of Natural Language Meaning, volume 3. Mouton de Gruyter, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Potts</author>
</authors>
<title>Goal-driven answers in the Cards dialogue corpus.</title>
<date>2012</date>
<booktitle>Proceedings of the 30th West Coast Conference on Formal Linguistics,</booktitle>
<editor>In Nathan Arnett and Ryan Bennett, editors,</editor>
<publisher>Cascadilla Press.</publisher>
<location>Somerville, MA.</location>
<contexts>
<context position="15597" citStr="Potts (2012)" startWordPosition="2528" endWordPosition="2529"> implicature-rich usage, thereby providing our model with an empirically-grounded literal start. ships show that the language can support scalar conversational implicatures.2 Fig. 2 is not entirely appropriate in our setting, however. Our expressions are vague; there is no sharp boundary between, e.g., ‘top’ and ‘bottom’, nor is it clear where ‘top right’ begins. To model this vagueness, we analyze each message m as denoting a conditional distribution Pr(x|m) over grid squares x in the gameboard. These distributions are derived from human–human Cards interactions using the data and methods of Potts (2012). Of course, there is a tension here: our model assumes that we begin with literal interpretations, but human–human data will reflect pragmaticallyenriched usage. To get around this, we approximate literal interpretations by deriving each term’s distribution from all the corpus tokens that contain it. For example, the distribution for ‘top’ is 2Our agents cannot produce modified versions of ‘middle’ like ‘middle right’. These would be synonymous with implicature-enriched general terms. We work with a simple cost-function that treats all forms alike, but future versions of this work will incorp</context>
</contexts>
<marker>Potts, 2012</marker>
<rawString>Christopher Potts. 2012. Goal-driven answers in the Cards dialogue corpus. In Nathan Arnett and Ryan Bennett, editors, Proceedings of the 30th West Coast Conference on Formal Linguistics, Somerville, MA. Cascadilla Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hannah Rohde</author>
<author>Scott Seyfarth</author>
<author>Brady Clark</author>
<author>Gerhard J¨ager</author>
<author>Stefan Kaufmann</author>
</authors>
<title>Communicating with cost-based implicature: A game-theoretic approach to ambiguity.</title>
<date>2012</date>
<booktitle>In The 16th Workshop on the Semantics and Pragmatics of Dialogue,</booktitle>
<location>Paris,</location>
<marker>Rohde, Seyfarth, Clark, J¨ager, Kaufmann, 2012</marker>
<rawString>Hannah Rohde, Scott Seyfarth, Brady Clark, Gerhard J¨ager, and Stefan Kaufmann. 2012. Communicating with cost-based implicature: A game-theoretic approach to ambiguity. In The 16th Workshop on the Semantics and Pragmatics of Dialogue, Paris, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Seymour Rosenberg</author>
<author>Bertram D Cohen</author>
</authors>
<title>Speakers’ and listeners’ processes in a word communication task.</title>
<date>1964</date>
<journal>Science,</journal>
<pages>145--1201</pages>
<contexts>
<context position="1524" citStr="Rosenberg and Cohen, 1964" startWordPosition="209" endWordPosition="212">ic behavior with the assumption that the speaker is cooperative. As Grice conceived of them, implicatures crucially involve reasoning about multiply-nested belief structures: roughly, for p to count as an implicature, the speaker must believe that the listener will infer that the speaker believes p. This complexity makes implicatures an important testing ground for models of conversation and cognition. Implicatures have received considerable attention in the context of simple reference games in which the listener uses the speaker’s utterance to try to identify the speaker’s intended referent (Rosenberg and Cohen, 1964; Clark and WilkesGibbs, 1986; Dale and Reiter, 1995; DeVault and Stone, 2007; Krahmer and van Deemter, 2012). Many implicature patterns can be embedded in these games using specific combinations of potential referents and message sets. The paradigm has proven fruitful not only for evaluating computational models (Golland et al., 2010; Degen and Franke, 2012; Frank and Goodman, 2012; Rohde et al., 2012; Bergen et al., 2012) but also for studying children’s pragmatic abilities without implicitly assuming they have mastered challenging linguistic structures (Stiller et al., 2011). In this paper,</context>
<context position="9296" citStr="Rosenberg and Cohen (1964)" startWordPosition="1521" endWordPosition="1524">el et al., 2013), augmenting the state space with another copy of the underlying world state space, where the new copy represents the next level of belief. For instance, the L(S(L)) agent will make decisions in the 5×5×5 space. For an L(S(L)) state (s, ¯s, ˆs), s is the true state of the world, s¯ is the speaker’s belief of the state of the world, and sˆ is the speaker’s belief of the listener’s beliefs. In the next two sections we show how a level-one and level-two listener infer implicatures. 3 Reference Game Implicatures Fig. 1a is the scenario for a reference game of the sort pioneered by Rosenberg and Cohen (1964) and Dale and Reiter (1995). The potential referents are r1, r2, and r3. Speakers use a restricted vocabulary consisting of three messages: ‘moustache’, ‘glasses’, and ‘hat’. The speaker is assigned a referent ri (hidden from the listener) and produces a message on that basis. The speaker and listener share the goal of having the listener identify the speaker’s intended referent ri. Fig. 1b depicts the literal interpretations for this game. It looks like the listener’s chances of success are low. Only ‘hat’ refers unambiguously. However, the language and scenario facilitate scalar implicature </context>
</contexts>
<marker>Rosenberg, Cohen, 1964</marker>
<rawString>Seymour Rosenberg and Bertram D. Cohen. 1964. Speakers’ and listeners’ processes in a word communication task. Science, 145:1201–1203.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthijs T J Spaan</author>
<author>Nikos Vlassis</author>
</authors>
<title>Perseus: Randomized point-based value iteration for POMDPs.</title>
<date>2005</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>24</volume>
<issue>1</issue>
<contexts>
<context position="4980" citStr="Spaan and Vlassis, 2005" startWordPosition="779" endWordPosition="782"> In the model presented here, our agent models the other agent’s beliefs about the state of the world, and assumes that the other agent does not take into account our own beliefs, a common approach (Gmytrasiewicz and Doshi, 2005). Agents make decisions according to a policy πz : Δ(S) -+ A which maximizes the discounted expected reward E∞t=0 γt E[R(st, at1, at2)|b0, π1, π2]. Using the assumption that the other agent tracks one less level of belief, we can solve for the other agent’s policy ¯π, which allows us to estimate his actions and beliefs over time. To construct policies, we use Perseus (Spaan and Vlassis, 2005), a point-based value iteration algorithm. Even tracking just one level of nested beliefs quickly leads to a combinatorial explosion in the number of belief states the other agent might have. This causes decision making in Dec-POMDPs to be NEXP complete, limiting their application to problems with only a handful of states (Bernstein et al., 2002). To ameliorate this difficulty, we use the method of Vogel et al. (2013), which creates a single-agent approximation to the full DecPOMDP. To form this single-agent POMDP, we augment the state space to be S x S, where the second set of state variables</context>
</contexts>
<marker>Spaan, Vlassis, 2005</marker>
<rawString>Matthijs T. J. Spaan and Nikos Vlassis. 2005. Perseus: Randomized point-based value iteration for POMDPs. Journal of Artificial Intelligence Research, 24(1):195–220, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Stiller</author>
<author>Noah D Goodman</author>
<author>Michael C Frank</author>
</authors>
<title>Ad-hoc scalar implicature in adults and children.</title>
<date>2011</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the Cognitive Science Society,</booktitle>
<location>Boston,</location>
<contexts>
<context position="2108" citStr="Stiller et al., 2011" startWordPosition="301" endWordPosition="304">d referent (Rosenberg and Cohen, 1964; Clark and WilkesGibbs, 1986; Dale and Reiter, 1995; DeVault and Stone, 2007; Krahmer and van Deemter, 2012). Many implicature patterns can be embedded in these games using specific combinations of potential referents and message sets. The paradigm has proven fruitful not only for evaluating computational models (Golland et al., 2010; Degen and Franke, 2012; Frank and Goodman, 2012; Rohde et al., 2012; Bergen et al., 2012) but also for studying children’s pragmatic abilities without implicitly assuming they have mastered challenging linguistic structures (Stiller et al., 2011). In this paper, we extend these results beyond simple reference games to full decision-problems in which the agents reason about language and action together over time. To do this, we use the Decentralized Partially Observable Markov Decision Process (Dec-POMDP) to implement agents that are capable of manipulating the multiply-nested belief structures required for implicature calculation. Optimal decision making in Dec-POMDPs is NEXP complete, so we employ the single-agent POMDP approximation of Vogel et al. (2013). We show that agents in the Dec-POMDP reach implicature-rich interpretations s</context>
<context position="12932" citStr="Stiller et al., 2011" startWordPosition="2104" endWordPosition="2107">left bottom right bottom left Pr(t=ri|m) a ✶[¯irS(L)(t=ri) = m] Since S(L) uses ‘hat’ to describe r3 and ‘moustache’ to describe r1, L(S(L)) correctly infers that ‘glasses’ refers to r2, completing Fig. 1c’s full implicature-rich pattern of mutual exclusivity (Clark, 1987; Frank et al., 2009). This basic pattern is robustly attested empirically in human data. The experimental data are, of course, invariably less crisp than our idealized model predicts, but many important sources of variation could be brought into our model, with the addition of strong salience priors (Frank and Goodman, 2012; Stiller et al., 2011), assumptions about bounded rationality (Camerer et al., 2004; Franke, 2009), and a ‘soft-max’ view of the listener (Frank et al., 2009). 4 Cards World Implicatures The Cards corpus1 contains 1266 metadata-rich transcripts from a two-player chat-based game. The world is a simple maze in which a deck of cards has been distributed. The players’ goal is to find specific subsets of the cards, subject to a variety of constraints on what they can see and do. The Dec-POMDP-based agents of Vogel et al. (2013) play a simplified version in which the goal is to be co-located with a single card. Vogel et </context>
</contexts>
<marker>Stiller, Goodman, Frank, 2011</marker>
<rawString>Alex Stiller, Noah D. Goodman, and Michael C. Frank. 2011. Ad-hoc scalar implicature in adults and children. In Proceedings of the 33rd Annual Meeting of the Cognitive Science Society, Boston, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Vogel</author>
<author>Max Bodoia</author>
<author>Dan Jurafsky</author>
<author>Christopher Potts</author>
</authors>
<title>Emergence of Gricean maxims from multi-agent decision theory.</title>
<date>2013</date>
<booktitle>In Human Language Technologies: The 2013 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="2629" citStr="Vogel et al. (2013)" startWordPosition="379" endWordPosition="382">ut implicitly assuming they have mastered challenging linguistic structures (Stiller et al., 2011). In this paper, we extend these results beyond simple reference games to full decision-problems in which the agents reason about language and action together over time. To do this, we use the Decentralized Partially Observable Markov Decision Process (Dec-POMDP) to implement agents that are capable of manipulating the multiply-nested belief structures required for implicature calculation. Optimal decision making in Dec-POMDPs is NEXP complete, so we employ the single-agent POMDP approximation of Vogel et al. (2013). We show that agents in the Dec-POMDP reach implicature-rich interpretations simply as a byproduct of the way they reason about each other to maximize joint utility. Our simulations involve a reference game and a dynamic, interactional scenario involving implemented artificial agents. 2 Decision-Theoretic Communication The Decentralized Partially Observable Markov Decision Process (Dec-POMDP) (Bernstein et al., 2002) is a multi-agent generalization of the POMDP, where agents act to maximize a shared utility function. Formally, a Dec-POMDP consists of a tuple (S, A, O, R, T, Q, b0, γ). S is a </context>
<context position="5401" citStr="Vogel et al. (2013)" startWordPosition="847" endWordPosition="850">one less level of belief, we can solve for the other agent’s policy ¯π, which allows us to estimate his actions and beliefs over time. To construct policies, we use Perseus (Spaan and Vlassis, 2005), a point-based value iteration algorithm. Even tracking just one level of nested beliefs quickly leads to a combinatorial explosion in the number of belief states the other agent might have. This causes decision making in Dec-POMDPs to be NEXP complete, limiting their application to problems with only a handful of states (Bernstein et al., 2002). To ameliorate this difficulty, we use the method of Vogel et al. (2013), which creates a single-agent approximation to the full DecPOMDP. To form this single-agent POMDP, we augment the state space to be S x S, where the second set of state variables allows us to model the other agent’s beliefs. We maintain a point estimate ¯b of the other agent’s beliefs, which is formed by summing out observations O that the other player might have received. To accomplish this, we factor the transition distribution into two terms: T((s0, ¯s0)|a, ¯π(¯s), (s, ¯s)) = T¯ (¯s0|s0, a, ¯π(¯s), (s, ¯s))T (s0|a, ¯π(¯s), (s, ¯s)).This observation marginalization can be folded into the tr</context>
<context position="8686" citStr="Vogel et al., 2013" startWordPosition="1412" endWordPosition="1415">ces utterances to influence a literal listener, and a level-two listener, L(S(L)), uses two levels of belief nesting to interpret utterances as the beliefs that a level-one speaker might have to produce that utterance. At each level of nesting, we apply the marginalized 75 r1 r2 r3 (a) Scenario. Message r1 r2 r3 1 1 moustache 0 2 2 1 1 glasses 0 2 2 hat 0 0 1 (b) Literal interpretations. Message r1 r2 r3 moustache 1 0 0 glasses 0 1 0 hat 0 0 1 (c) Implicature-rich interpretations. Figure 1: A simple reference game. The matrices give distributions Pr(t = ri|utterance) belief-state approach of (Vogel et al., 2013), augmenting the state space with another copy of the underlying world state space, where the new copy represents the next level of belief. For instance, the L(S(L)) agent will make decisions in the 5×5×5 space. For an L(S(L)) state (s, ¯s, ˆs), s is the true state of the world, s¯ is the speaker’s belief of the state of the world, and sˆ is the speaker’s belief of the listener’s beliefs. In the next two sections we show how a level-one and level-two listener infer implicatures. 3 Reference Game Implicatures Fig. 1a is the scenario for a reference game of the sort pioneered by Rosenberg and Co</context>
<context position="13438" citStr="Vogel et al. (2013)" startWordPosition="2190" endWordPosition="2193">ought into our model, with the addition of strong salience priors (Frank and Goodman, 2012; Stiller et al., 2011), assumptions about bounded rationality (Camerer et al., 2004; Franke, 2009), and a ‘soft-max’ view of the listener (Frank et al., 2009). 4 Cards World Implicatures The Cards corpus1 contains 1266 metadata-rich transcripts from a two-player chat-based game. The world is a simple maze in which a deck of cards has been distributed. The players’ goal is to find specific subsets of the cards, subject to a variety of constraints on what they can see and do. The Dec-POMDP-based agents of Vogel et al. (2013) play a simplified version in which the goal is to be co-located with a single card. Vogel et al. show that their agents’ linguistic behavior is broadly Gricean. However, their agents’ language is too simple to reveal implicatures. The present section remedies this shortcoming. Implicature-rich interpretations are an immediate consequence. We implement the simplified Cards tasks as follows. The state space S is composed of the location of each player and the location of the card. The transition distribution T (s�|s, a1, a2) encodes the outcome of movement actions. Agents receive one of two sen</context>
</contexts>
<marker>Vogel, Bodoia, Jurafsky, Potts, 2013</marker>
<rawString>Adam Vogel, Max Bodoia, Dan Jurafsky, and Christopher Potts. 2013. Emergence of Gricean maxims from multi-agent decision theory. In Human Language Technologies: The 2013 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>