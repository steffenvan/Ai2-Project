<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009996">
<title confidence="0.9987575">
Vertex Degree Distribution for the Graph of Word Co-Occurrences
in Russian
</title>
<author confidence="0.998531">
Victor Kapustin
</author>
<affiliation confidence="0.989153">
Faculty of Philology
Saint-Petersburg State University
</affiliation>
<address confidence="0.832905">
Saint-Petersburg, Russia 199178
</address>
<email confidence="0.99798">
vak@icape.nw.ru
</email>
<author confidence="0.984881">
Anna Jamsen
</author>
<affiliation confidence="0.988505">
Faculty of Philology
Saint-Petersburg State University
</affiliation>
<address confidence="0.832828">
Saint-Petersburg, Russia 199178
</address>
<email confidence="0.998294">
anna_zheleznova@mail.ru
</email>
<sectionHeader confidence="0.998595" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999916588235294">
Degree distributions for word forms co-
occurrences for large Russian text collec-
tions are obtained. Two power laws fit the
distributions pretty good, thus supporting
Dorogovtsev-Mendes model for Russian.
Few different Russian text collections
were studied, and statistical errors are
shown to be negligible. The model expo-
nents for Russian are found to differ from
those for English, the difference probably
being due to the difference in the collec-
tions structure. On the contrary, the esti-
mated size of the supposed kernel lexicon
appeared to be almost the same for the
both languages, thus supporting the idea
of importance of word forms for a percep-
tual lexicon of a human.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999673727272727">
Few years ago Ferrer and Solé (2001a) draw the
attention of researchers to the fact that the lexicon of
a big corpus (British National Corpus – BNC –in
the case) most probably consists of two major com-
ponents: a compact kernel lexicon of about 103 –
104 words, and a cloud of all other words. Ferrer
and Solé studied word co-occurrence in BNC in
(2001b). Two word forms1 in BNC were considered
as “interacting” when they appeared in the same
sentence and the words’ distance didn’t exceed 2.
Ferrer and Solé (2001b) treated also some other no-
</bodyText>
<footnote confidence="0.454322">
1 Strictly speaking, word forms, not words.
</footnote>
<bodyText confidence="0.999961916666667">
tions of word interaction, but the results obtained
don’t differ qualitatively. The interacting words
form a graph, where the vertices are the words
themselves, and the edges are the words’ co-
occurrences. The fact of the collocation considered
to be important, not the number of collocations of
the same pair of words. Ferrer and Solé (2001b)
studied vertices degree distribution and found two
power laws for that distribution with a crossover at a
degree approximately corresponding to the previ-
ously found size of the supposed kernel lexicon of
about 103 – 104 words. In (Solé et al, 2005) word
co-occurrence networks were studied for small
(about 104 lines of text) corpora of English, Basque,
and Russian. The authors claim the same two-
regime word degree distribution behavior for all the
languages.
Dorogovtsev and Mendes (2001, 2003: 151-156)
offered an abstract model of language evolution,
which provides for two power laws for word degree
distribution with almost no fitting, and also explains
that the volume of the region of large degrees (the
kernel lexicon) is almost independent of the corpus
volume. Difference between word (lemma) and
word form for an analytic language (e.g. English)
seems to be small. Dorogovtsev-Mendes model cer-
tainly treats word forms, not lemmas, as vertices in
a corpus graph. Is it really true for inflecting lan-
guages like Russian? Many researchers consider a
word form, not a word (lemma) be a perceptual
lexicon unit (Zasorina, 1977; Ventsov and Kas-
sevich, 1998; Verbitskaya et. al., 2003; Ventsov et.
al., 2003). So a hypothesis that word forms in a cor-
pus of an inflecting language should exhibit degree
distribution similar to that of BNC looks appealing.
An attempt to investigate word frequency rank sta-
</bodyText>
<page confidence="0.996328">
89
</page>
<subsubsectionHeader confidence="0.403911">
TextGraphs-2: Graph-Based Al orithms for Natural Language Processing, pages 89–92,
</subsubsectionHeader>
<bodyText confidence="0.981169275">
Rochester, April 2007 (c 2007 Association for Computational Linguistics
tistics for Russian was made by Gelbukh and Si-
dorov (2001), but they studied only Zipf law on too
small texts to reveal the kernel lexicon effects. To
study the hypothesis one needs a corpus or a collec-
tion2 of texts comparable in volume with the BNC
part that was examined in (Ferrer and Solé, 2001b),
i.e. about 4.107 word occurrences. Certainly, texts
that were analyzed in (Solé et al, 2005) were much
smaller.
Recently Kapustin and Jamsen (2006) and Ka-
pustin (2006) studied a big (~5.107 word occur-
rences) collection of Russian. The collection
exhibited power law behavior similar to that of
BNC except that the vertex degree at the crossover
point and the average degree were about 4-5 times
less than that of BNC. These differences could be
assigned either to a collection nature (legislation
texts specifics) or to the properties of the (Russian)
language itself. We shall reference the collection
studied in (Kapustin and Jamsen, 2006; Kapustin,
2006) as “RuLegal”.
In this paper we present a study of another big
collection of Russian texts. We have found that
degree distributions (for different big sub-collec-
tions) are similar to those of BNC and of RuLegal.
While the exponents and the kernel lexicon size are
also similar to those of BNC, the average degree
for these collections are almost twice less than the
average degree of BNC, and the nature of this dif-
ference is unclear still.
The rest of the paper has the following struc-
ture. Technology section briefly describes the col-
lection and the procedures of building of co-
occurrence graph and of calculation of exponents
of power laws. In Discussion section we compare
the results obtained with those of Kapustin and
Jamsen (2006), Kapustin (2006), and (Ferrer and
Solé, 2001b). In Conclusion some considerations
for future research are discussed.
</bodyText>
<sectionHeader confidence="0.994423" genericHeader="introduction">
2 Technology
</sectionHeader>
<bodyText confidence="0.999612451612903">
At present Russian National Corpus is unavailable
for bulk statistical research due to copyright con-
siderations. So we bought a CD (“World Literature
in Russian”) in a bookstore – a collection of fiction
translations to Russian. We’ll call the collection
2 We consider a corpus to be a special type of a text collection,
which comprises text samples chosen for language research
purposes, while a more general term “collection “ refers to a
set of full texts brought together for some other purpose.
WLR. The size of the whole collection is more
than 108 word occurrences. The source format of
the collection is HTML, but its files contain essen-
tially no formatting, just plain paragraphs. We
made three non-overlapping samples from WLR
(WLR1–3). The samples were approximately of
the same size. Each sample was processed the
same way. The idea behind using more than one
sample was to estimate statistical errors.
We used open source Russian grapheme analy-
sis module (Sokirko, 2001) to strip HTML and to
split the texts into words and sentences. Word co-
occurrences were defined as in (Ferrer and Solé,
2001b): two words are “interacting” if and only if
they: (a) appear in the same sentence, and (b) the
word distance is either 1 (adjacent words) or 2 (one
word or a number or a date in-between). A found
co-occurred pair of words was tried out against
MySQL database of recorded word pairs, and if it
wasn’t found in the database, it was put there.
Then we use a simple SQL query to get a table of
count of vertices p(k) vs. vertex degree k.
</bodyText>
<figureCaption confidence="0.970196">
Figure 1. Raw degree distribution for WLR1.
</figureCaption>
<bodyText confidence="0.998683">
The raw results for one of the samples are
shown on Fig. 1. For the two other samples the
distributions are similar. All distributions are al-
most linear (in log-log coordinates, that means that
they obey power law), but fitting is impossible due
to high fluctuations. As noted by Dorogovtsev and
Mendes (2003: 222-223), cumulative distribution
</bodyText>
<figure confidence="0.9626033">
-1
-2
-3
-4
-5
-6
0
log(p(k))
0 1 2 3 4 5 6
log(k)
</figure>
<page confidence="0.769479">
90
</page>
<equation confidence="0.875707">
P(k) = ΣK≥k p(K) fluctuates much less, so we
</equation>
<bodyText confidence="0.958321666666667">
calculated the cumulative degree distributions
(Fig.2). Cumulative degree distributions for all
three WLR samples are very similar.
</bodyText>
<figureCaption confidence="0.969013">
Figure 2. Cumulative degree distributions for
WLR1 (lower curve) and RuLegal (upper curve).
</figureCaption>
<sectionHeader confidence="0.999576" genericHeader="method">
3 Discussion
</sectionHeader>
<bodyText confidence="0.986176387096774">
To estimate statistical errors we have normalized
the distributions to make them comparable: the
degree ranges were reduced to the largest one, then
the cumulative degree distribution was sampled
with the step of 1/3, as in (Ferrer and Solé, 2001a,
Dorogovtsev and Mendes, 2003: 222-223). When
we use WLR samples only, the statistical errors are
less than 7% in the middle of the curves and reach
a margin of 77% in small degrees region. With the
inclusion of RuLegal sample, difference between
samples becomes larger – up to 13% in the middle
of the curves), but are still small enough.
In both cases (with and without RuLegal) we
attempted to fit either a single power law (a
straight line in log-log coordinates) or two/three
power laws with one/two crossover points. Strong
changes and large statistical errors of the distribu-
tions in the low degree region prevent meaningful
usage of these points for fitting. We have made
attempts to fit all three approximations for all
points, and omitting one or two points with the
lowest degrees. To choose between the hypotheses
we minimized Schwarz information criterion
(Schwarz, 1978):
SIC=N*ln(Σi(pi-pˆi)2/N)-m*ln(N)
where pi – cumulative distribution at i-th point;
pˆi – fitting law at the same point;
N – number of sampling points (13–15,
depending on the number of
omitted points);
m – number of fitting parameters (2, 4 or 6
</bodyText>
<table confidence="0.997714285714286">
Omitted SIC (1/2/3 power laws)
points
WLR1–3 WLR1–3 +
RuLegal
0 –44 / –85 / –68 –42 / –93 / –77
1 –46 / –85 / –65 –43 / –93 / –73
2 –47 / –80 / –60 –44 / –86 / –67
</table>
<tableCaption confidence="0.972414">
Table 1. Fitting power laws to averaged degree
distributions – Schwarz information criterion
</tableCaption>
<table confidence="0.999834555555556">
WLR1–3 WLR1–3 RuLegal BNC
+ RuLegal
Y1 –0.95 –0.95 –0.95 –0.5
Y2 –1.44 –1.46 –1.75 –1.7
kcross 670 670 510 2000
Vkernel 4.103 4.103 4.103 5.103
kaverage 36 31 15 72
Collection 3.107 14.107 5.107 4.107
size
</table>
<tableCaption confidence="0.915636">
Table 2. Parameters of the best fit two power laws
for the cumulative distributions
</tableCaption>
<bodyText confidence="0.962942636363636">
Clearly two power laws fit the curves better.
The exponents, the crossover degree and estimated
size of the kernel lexicon (number of vertices with
high degrees above the crossover) for the best fits
(two powers, zero/one omitted point) are shown in
Table 2. The exponents for the raw distributions
are Y1 and Y2 minus 1.
Disagreement between English and Russian
seems to exist. Probably, the differences are still
due to the collections’ nature (the difference be-
tween different Russian collections is noticeable).
</bodyText>
<sectionHeader confidence="0.997718" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.998883666666667">
We found that ergodic hypothesis for word form
degree distribution seems to work for large text
collections – differences between the distributions
</bodyText>
<figure confidence="0.956755">
log(P(k))
log(k)
-1
-2
-3
-4
-5
-6
0 1 2 3 4 5 6
0
</figure>
<page confidence="0.996316">
91
</page>
<bodyText confidence="0.999941636363636">
are small (except for the few smallest degrees). At
least, a single big enough sample permits reliable
calculation of degree distribution parameters.
Dorogovtsev-Mendes model, which yields two
power laws for the degree distribution for the word
forms graph, gives pretty good explanation both
for an analytic language (English) and for an in-
flecting one (Russian), though numeric parameters
for both languages differ. The estimated sizes of
the supposed kernel lexicons for the both lan-
guages are almost the same, the fact supports the
point that word form is a perceptual lexicon unit.
To make more rigorous statements concerning
statistical properties of various languages, we plan
to calculate other important characteristics of the
co-occurrence graph for Russian: clustering coeffi-
cient and average shortest path. Also we hope that
legal obstacles to Russian National Corpus usage
will have been overcome. Other statistical lan-
guage graph studies are also interesting; among
them are investigation of networks of lemmas, and
statistical research of agglutinated languages.
</bodyText>
<sectionHeader confidence="0.996922" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999404">
The authors are grateful to the anonymous review-
ers, the comments of whom were of much help.
The work is supported in part by Russian Foun-
dation for Basic Research, grants 06-06-80434 and
06-06-80251.
</bodyText>
<sectionHeader confidence="0.998305" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998771082191781">
Sergey N. Dorogovtsev., José F. Mendes, 2001. Lan-
guage as an evolving word web. Proceedings of the
Royal Society of London B, 268(1485): 2603-2606
Sergey N. Dorogovtsev., José F. Mendes, 2003. Evolu-
tion of Networks: From Biological Nets to the Inter-
net and WWW. Oxford University Press, Oxford.
Ramon Ferrer and Ricard V. Solé. 2001a. Two regimes in
the frequency of words and the origin of complex lexi-
cons. Journal of Quantitative Linguistics 8: 165-173.
Ramon Ferrer and Ricard V. Solé. 2001b. The Small-
World of Human Language. Proceedings of the
Royal Society of London B, 268(1485): 2261-2266
Victor Kapustin, 2006. Капустин В.А. Ранговые ста-
тистики совместной встречаемости словоформ в
большой монотематической коллекции. Труды
третьей международной конференции «Корпус-
ная лингвистика», 11–13 октября 2006 г., – СПб.:
Изд-во С. Петерб. ун-та, 2006. – С. 135-142 (Rank
Statistics of Word Co-Occurrences in a Big Mono-
thematic Collection. Proc. 3rd International Conf.
“Corpus Linguistics”, Oct. 11-13, 2006. Saint-
Petersburg State Publishing: 135-142).
Alexander Gelbukh and Grigory Sidorov, 2001. Zipf
and Heaps Laws’ Coefficients Depend on Language..
Proc. CICLing-2001, Conference on Intelligent Text
Processing and Computational Linguistics (February
18–24, 2001, Mexico City), Lecture Notes in Com-
puter Science, Springer-Verlag. (2004): 332-335.
(ISSN 0302-9743, ISBN 3-540-41687-0)
Victor Kapustin and Anna Jamsen. 2006. Ранговая ста-
тистика встречаемости слов в большой текстовой
коллекции. Труды 8ой Всероссийской научной
конференции «Электронные библиотеки: пер-
спективные методы и технологии, электронные
коллекции» – RCDL’2006, Суздаль, Россия, 2006.
– C. 245–251 (Rank Statistics of Word Occurrence in
a Big Text Collection. Proc. 8th National Russian Re-
search Conference “Digital libraries: advanced meth-
ods and technologies, digital collections”, Oct. 17-19,
2006: 245-251).
Alexey Sokirko, 2001. A short description of Dialing
Project.
http://www.aot.ru/docs/sokirko/sokirko-candid-eng.html
Ricard V. Solé, Bernat Corominas, Sergi Valverde and
Luc Steels. 2005. Language Networks: their struc-
ture, function and evolution. SFI-WP 05-12-042, SFI
Working Papers
Gideon Schwarz, 1978. Estimating the dimension of a
model. Annals of Statistics 6(2): 461-464.
Anatoly V. Ventsov and Vadim B. Kassevich, 1998.
Венцов А.В., Касевич В.Б. Cnoeapb d�A Modemu
eocnpuAmuA peau. Вестник Санкт-Петербургского
университета, сер. 2, вып. 3, с. 32-39 (A Dictionary
for a Speech Perception Model. Vestnik Sankt-
Peterbugskogo Universiteta, 2(3): 32-39).
Anatoly V. Ventsov, Vadim B. Kassevich and Elena V.
Yagounova, 2003. Венцов А.В., Касевич В.Б., Ягу-
нова Е.В. Kopnyc pyccxozo A3bwa u eocnpuAmue pe-
au. Научно-техническая информация.– Серия 2, №
6, с.25-32 (Russian Corpus and Speech Perception.
Research and Technical Information, 2(6): 25-32).
Liudmila A. Verbitskaya, Nikolay N. Kazansky and
Vadim B. Kassevich, 2003. Вербицкая Л.А., Казанс-
кий Н.Н., Касевич В.Б. Некоторые проблемы созда-
ния национального корпуса русского языка.
Научно-техническая информация.– Серия 2, № 5,
с.2-8 (On Some Problems of Russian National Corpus
Development. Research and Technical Information,
2(5): 2-8).
Lidia N. Zasorina, ed., 1977. Vacmomxbau c�oeapb pyc-
cxozo A3bwa. Под ред. Л.Н. Засориной. М.: Русск.
яз. (Frequency Dictionary of Russian. Russian Lan-
guage, Moscow, 1977).
</reference>
<page confidence="0.996031">
92
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.100857">
<title confidence="0.9298995">Vertex Degree Distribution for the Graph of Word Co-Occurrences in Russian</title>
<author confidence="0.582448">Victor</author>
<affiliation confidence="0.529174">Faculty of Saint-Petersburg State</affiliation>
<address confidence="0.696058">Saint-Petersburg, Russia 199178</address>
<email confidence="0.870708">vak@icape.nw.ru</email>
<affiliation confidence="0.529825">Anna Faculty of Saint-Petersburg State</affiliation>
<address confidence="0.747235">Saint-Petersburg, Russia 199178</address>
<email confidence="0.958173">anna_zheleznova@mail.ru</email>
<abstract confidence="0.998845722222222">Degree distributions for word forms cooccurrences for large Russian text collections are obtained. Two power laws fit the distributions pretty good, thus supporting Dorogovtsev-Mendes model for Russian. Few different Russian text collections were studied, and statistical errors are shown to be negligible. The model exponents for Russian are found to differ from those for English, the difference probably being due to the difference in the collections structure. On the contrary, the estimated size of the supposed kernel lexicon appeared to be almost the same for the both languages, thus supporting the idea of importance of word forms for a perceptual lexicon of a human.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sergey N Dorogovtsev</author>
<author>José F Mendes</author>
</authors>
<title>Language as an evolving word web.</title>
<date>2001</date>
<journal>Proceedings of the Royal Society of London B,</journal>
<volume>268</volume>
<issue>1485</issue>
<pages>2603--2606</pages>
<contexts>
<context position="2436" citStr="Dorogovtsev and Mendes (2001" startWordPosition="383" endWordPosition="386"> The fact of the collocation considered to be important, not the number of collocations of the same pair of words. Ferrer and Solé (2001b) studied vertices degree distribution and found two power laws for that distribution with a crossover at a degree approximately corresponding to the previously found size of the supposed kernel lexicon of about 103 – 104 words. In (Solé et al, 2005) word co-occurrence networks were studied for small (about 104 lines of text) corpora of English, Basque, and Russian. The authors claim the same tworegime word degree distribution behavior for all the languages. Dorogovtsev and Mendes (2001, 2003: 151-156) offered an abstract model of language evolution, which provides for two power laws for word degree distribution with almost no fitting, and also explains that the volume of the region of large degrees (the kernel lexicon) is almost independent of the corpus volume. Difference between word (lemma) and word form for an analytic language (e.g. English) seems to be small. Dorogovtsev-Mendes model certainly treats word forms, not lemmas, as vertices in a corpus graph. Is it really true for inflecting languages like Russian? Many researchers consider a word form, not a word (lemma) </context>
</contexts>
<marker>Dorogovtsev, Mendes, 2001</marker>
<rawString>Sergey N. Dorogovtsev., José F. Mendes, 2001. Language as an evolving word web. Proceedings of the Royal Society of London B, 268(1485): 2603-2606</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergey N Dorogovtsev</author>
<author>José F Mendes</author>
</authors>
<title>Evolution of Networks: From Biological Nets to the Internet and WWW.</title>
<date>2003</date>
<publisher>Oxford University Press,</publisher>
<location>Oxford.</location>
<contexts>
<context position="7239" citStr="Dorogovtsev and Mendes (2003" startWordPosition="1187" endWordPosition="1190">number or a date in-between). A found co-occurred pair of words was tried out against MySQL database of recorded word pairs, and if it wasn’t found in the database, it was put there. Then we use a simple SQL query to get a table of count of vertices p(k) vs. vertex degree k. Figure 1. Raw degree distribution for WLR1. The raw results for one of the samples are shown on Fig. 1. For the two other samples the distributions are similar. All distributions are almost linear (in log-log coordinates, that means that they obey power law), but fitting is impossible due to high fluctuations. As noted by Dorogovtsev and Mendes (2003: 222-223), cumulative distribution -1 -2 -3 -4 -5 -6 0 log(p(k)) 0 1 2 3 4 5 6 log(k) 90 P(k) = ΣK≥k p(K) fluctuates much less, so we calculated the cumulative degree distributions (Fig.2). Cumulative degree distributions for all three WLR samples are very similar. Figure 2. Cumulative degree distributions for WLR1 (lower curve) and RuLegal (upper curve). 3 Discussion To estimate statistical errors we have normalized the distributions to make them comparable: the degree ranges were reduced to the largest one, then the cumulative degree distribution was sampled with the step of 1/3, as in (Fer</context>
</contexts>
<marker>Dorogovtsev, Mendes, 2003</marker>
<rawString>Sergey N. Dorogovtsev., José F. Mendes, 2003. Evolution of Networks: From Biological Nets to the Internet and WWW. Oxford University Press, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ramon Ferrer</author>
<author>Ricard V Solé</author>
</authors>
<title>Two regimes in the frequency of words and the origin of complex lexicons.</title>
<date>2001</date>
<journal>Journal of Quantitative Linguistics</journal>
<volume>8</volume>
<pages>165--173</pages>
<contexts>
<context position="1053" citStr="Ferrer and Solé (2001" startWordPosition="151" endWordPosition="154">er laws fit the distributions pretty good, thus supporting Dorogovtsev-Mendes model for Russian. Few different Russian text collections were studied, and statistical errors are shown to be negligible. The model exponents for Russian are found to differ from those for English, the difference probably being due to the difference in the collections structure. On the contrary, the estimated size of the supposed kernel lexicon appeared to be almost the same for the both languages, thus supporting the idea of importance of word forms for a perceptual lexicon of a human. 1 Introduction Few years ago Ferrer and Solé (2001a) draw the attention of researchers to the fact that the lexicon of a big corpus (British National Corpus – BNC –in the case) most probably consists of two major components: a compact kernel lexicon of about 103 – 104 words, and a cloud of all other words. Ferrer and Solé studied word co-occurrence in BNC in (2001b). Two word forms1 in BNC were considered as “interacting” when they appeared in the same sentence and the words’ distance didn’t exceed 2. Ferrer and Solé (2001b) treated also some other no1 Strictly speaking, word forms, not words. tions of word interaction, but the results obtain</context>
<context position="3813" citStr="Ferrer and Solé, 2001" startWordPosition="609" endWordPosition="612">s in a corpus of an inflecting language should exhibit degree distribution similar to that of BNC looks appealing. An attempt to investigate word frequency rank sta89 TextGraphs-2: Graph-Based Al orithms for Natural Language Processing, pages 89–92, Rochester, April 2007 (c 2007 Association for Computational Linguistics tistics for Russian was made by Gelbukh and Sidorov (2001), but they studied only Zipf law on too small texts to reveal the kernel lexicon effects. To study the hypothesis one needs a corpus or a collection2 of texts comparable in volume with the BNC part that was examined in (Ferrer and Solé, 2001b), i.e. about 4.107 word occurrences. Certainly, texts that were analyzed in (Solé et al, 2005) were much smaller. Recently Kapustin and Jamsen (2006) and Kapustin (2006) studied a big (~5.107 word occurrences) collection of Russian. The collection exhibited power law behavior similar to that of BNC except that the vertex degree at the crossover point and the average degree were about 4-5 times less than that of BNC. These differences could be assigned either to a collection nature (legislation texts specifics) or to the properties of the (Russian) language itself. We shall reference the coll</context>
<context position="5257" citStr="Ferrer and Solé, 2001" startWordPosition="847" endWordPosition="850">ions) are similar to those of BNC and of RuLegal. While the exponents and the kernel lexicon size are also similar to those of BNC, the average degree for these collections are almost twice less than the average degree of BNC, and the nature of this difference is unclear still. The rest of the paper has the following structure. Technology section briefly describes the collection and the procedures of building of cooccurrence graph and of calculation of exponents of power laws. In Discussion section we compare the results obtained with those of Kapustin and Jamsen (2006), Kapustin (2006), and (Ferrer and Solé, 2001b). In Conclusion some considerations for future research are discussed. 2 Technology At present Russian National Corpus is unavailable for bulk statistical research due to copyright considerations. So we bought a CD (“World Literature in Russian”) in a bookstore – a collection of fiction translations to Russian. We’ll call the collection 2 We consider a corpus to be a special type of a text collection, which comprises text samples chosen for language research purposes, while a more general term “collection “ refers to a set of full texts brought together for some other purpose. WLR. The size </context>
<context position="7857" citStr="Ferrer and Solé, 2001" startWordPosition="1288" endWordPosition="1291">003: 222-223), cumulative distribution -1 -2 -3 -4 -5 -6 0 log(p(k)) 0 1 2 3 4 5 6 log(k) 90 P(k) = ΣK≥k p(K) fluctuates much less, so we calculated the cumulative degree distributions (Fig.2). Cumulative degree distributions for all three WLR samples are very similar. Figure 2. Cumulative degree distributions for WLR1 (lower curve) and RuLegal (upper curve). 3 Discussion To estimate statistical errors we have normalized the distributions to make them comparable: the degree ranges were reduced to the largest one, then the cumulative degree distribution was sampled with the step of 1/3, as in (Ferrer and Solé, 2001a, Dorogovtsev and Mendes, 2003: 222-223). When we use WLR samples only, the statistical errors are less than 7% in the middle of the curves and reach a margin of 77% in small degrees region. With the inclusion of RuLegal sample, difference between samples becomes larger – up to 13% in the middle of the curves), but are still small enough. In both cases (with and without RuLegal) we attempted to fit either a single power law (a straight line in log-log coordinates) or two/three power laws with one/two crossover points. Strong changes and large statistical errors of the distributions in the low</context>
</contexts>
<marker>Ferrer, Solé, 2001</marker>
<rawString>Ramon Ferrer and Ricard V. Solé. 2001a. Two regimes in the frequency of words and the origin of complex lexicons. Journal of Quantitative Linguistics 8: 165-173.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ramon Ferrer</author>
<author>Ricard V Solé</author>
</authors>
<title>The SmallWorld of Human Language.</title>
<date>2001</date>
<journal>Proceedings of the Royal Society of London B,</journal>
<volume>268</volume>
<issue>1485</issue>
<pages>2261--2266</pages>
<contexts>
<context position="1053" citStr="Ferrer and Solé (2001" startWordPosition="151" endWordPosition="154">er laws fit the distributions pretty good, thus supporting Dorogovtsev-Mendes model for Russian. Few different Russian text collections were studied, and statistical errors are shown to be negligible. The model exponents for Russian are found to differ from those for English, the difference probably being due to the difference in the collections structure. On the contrary, the estimated size of the supposed kernel lexicon appeared to be almost the same for the both languages, thus supporting the idea of importance of word forms for a perceptual lexicon of a human. 1 Introduction Few years ago Ferrer and Solé (2001a) draw the attention of researchers to the fact that the lexicon of a big corpus (British National Corpus – BNC –in the case) most probably consists of two major components: a compact kernel lexicon of about 103 – 104 words, and a cloud of all other words. Ferrer and Solé studied word co-occurrence in BNC in (2001b). Two word forms1 in BNC were considered as “interacting” when they appeared in the same sentence and the words’ distance didn’t exceed 2. Ferrer and Solé (2001b) treated also some other no1 Strictly speaking, word forms, not words. tions of word interaction, but the results obtain</context>
<context position="3813" citStr="Ferrer and Solé, 2001" startWordPosition="609" endWordPosition="612">s in a corpus of an inflecting language should exhibit degree distribution similar to that of BNC looks appealing. An attempt to investigate word frequency rank sta89 TextGraphs-2: Graph-Based Al orithms for Natural Language Processing, pages 89–92, Rochester, April 2007 (c 2007 Association for Computational Linguistics tistics for Russian was made by Gelbukh and Sidorov (2001), but they studied only Zipf law on too small texts to reveal the kernel lexicon effects. To study the hypothesis one needs a corpus or a collection2 of texts comparable in volume with the BNC part that was examined in (Ferrer and Solé, 2001b), i.e. about 4.107 word occurrences. Certainly, texts that were analyzed in (Solé et al, 2005) were much smaller. Recently Kapustin and Jamsen (2006) and Kapustin (2006) studied a big (~5.107 word occurrences) collection of Russian. The collection exhibited power law behavior similar to that of BNC except that the vertex degree at the crossover point and the average degree were about 4-5 times less than that of BNC. These differences could be assigned either to a collection nature (legislation texts specifics) or to the properties of the (Russian) language itself. We shall reference the coll</context>
<context position="5257" citStr="Ferrer and Solé, 2001" startWordPosition="847" endWordPosition="850">ions) are similar to those of BNC and of RuLegal. While the exponents and the kernel lexicon size are also similar to those of BNC, the average degree for these collections are almost twice less than the average degree of BNC, and the nature of this difference is unclear still. The rest of the paper has the following structure. Technology section briefly describes the collection and the procedures of building of cooccurrence graph and of calculation of exponents of power laws. In Discussion section we compare the results obtained with those of Kapustin and Jamsen (2006), Kapustin (2006), and (Ferrer and Solé, 2001b). In Conclusion some considerations for future research are discussed. 2 Technology At present Russian National Corpus is unavailable for bulk statistical research due to copyright considerations. So we bought a CD (“World Literature in Russian”) in a bookstore – a collection of fiction translations to Russian. We’ll call the collection 2 We consider a corpus to be a special type of a text collection, which comprises text samples chosen for language research purposes, while a more general term “collection “ refers to a set of full texts brought together for some other purpose. WLR. The size </context>
<context position="7857" citStr="Ferrer and Solé, 2001" startWordPosition="1288" endWordPosition="1291">003: 222-223), cumulative distribution -1 -2 -3 -4 -5 -6 0 log(p(k)) 0 1 2 3 4 5 6 log(k) 90 P(k) = ΣK≥k p(K) fluctuates much less, so we calculated the cumulative degree distributions (Fig.2). Cumulative degree distributions for all three WLR samples are very similar. Figure 2. Cumulative degree distributions for WLR1 (lower curve) and RuLegal (upper curve). 3 Discussion To estimate statistical errors we have normalized the distributions to make them comparable: the degree ranges were reduced to the largest one, then the cumulative degree distribution was sampled with the step of 1/3, as in (Ferrer and Solé, 2001a, Dorogovtsev and Mendes, 2003: 222-223). When we use WLR samples only, the statistical errors are less than 7% in the middle of the curves and reach a margin of 77% in small degrees region. With the inclusion of RuLegal sample, difference between samples becomes larger – up to 13% in the middle of the curves), but are still small enough. In both cases (with and without RuLegal) we attempted to fit either a single power law (a straight line in log-log coordinates) or two/three power laws with one/two crossover points. Strong changes and large statistical errors of the distributions in the low</context>
</contexts>
<marker>Ferrer, Solé, 2001</marker>
<rawString>Ramon Ferrer and Ricard V. Solé. 2001b. The SmallWorld of Human Language. Proceedings of the Royal Society of London B, 268(1485): 2261-2266</rawString>
</citation>
<citation valid="false">
<authors>
<author>Victor Kapustin</author>
</authors>
<title>Капустин В.А. Ранговые ста-тистики совместной встречаемости словоформ в большой монотематической коллекции. Труды третьей международной конференции «Корпус-ная лингвистика», 11–13 октября</title>
<date>2006</date>
<booktitle>Proc. 3rd International Conf. “Corpus Linguistics”,</booktitle>
<pages>135--142</pages>
<contexts>
<context position="3984" citStr="Kapustin (2006)" startWordPosition="637" endWordPosition="639">-2: Graph-Based Al orithms for Natural Language Processing, pages 89–92, Rochester, April 2007 (c 2007 Association for Computational Linguistics tistics for Russian was made by Gelbukh and Sidorov (2001), but they studied only Zipf law on too small texts to reveal the kernel lexicon effects. To study the hypothesis one needs a corpus or a collection2 of texts comparable in volume with the BNC part that was examined in (Ferrer and Solé, 2001b), i.e. about 4.107 word occurrences. Certainly, texts that were analyzed in (Solé et al, 2005) were much smaller. Recently Kapustin and Jamsen (2006) and Kapustin (2006) studied a big (~5.107 word occurrences) collection of Russian. The collection exhibited power law behavior similar to that of BNC except that the vertex degree at the crossover point and the average degree were about 4-5 times less than that of BNC. These differences could be assigned either to a collection nature (legislation texts specifics) or to the properties of the (Russian) language itself. We shall reference the collection studied in (Kapustin and Jamsen, 2006; Kapustin, 2006) as “RuLegal”. In this paper we present a study of another big collection of Russian texts. We have found that</context>
<context position="5229" citStr="Kapustin (2006)" startWordPosition="844" endWordPosition="845">ferent big sub-collections) are similar to those of BNC and of RuLegal. While the exponents and the kernel lexicon size are also similar to those of BNC, the average degree for these collections are almost twice less than the average degree of BNC, and the nature of this difference is unclear still. The rest of the paper has the following structure. Technology section briefly describes the collection and the procedures of building of cooccurrence graph and of calculation of exponents of power laws. In Discussion section we compare the results obtained with those of Kapustin and Jamsen (2006), Kapustin (2006), and (Ferrer and Solé, 2001b). In Conclusion some considerations for future research are discussed. 2 Technology At present Russian National Corpus is unavailable for bulk statistical research due to copyright considerations. So we bought a CD (“World Literature in Russian”) in a bookstore – a collection of fiction translations to Russian. We’ll call the collection 2 We consider a corpus to be a special type of a text collection, which comprises text samples chosen for language research purposes, while a more general term “collection “ refers to a set of full texts brought together for some o</context>
</contexts>
<marker>Kapustin, 2006</marker>
<rawString>Victor Kapustin, 2006. Капустин В.А. Ранговые ста-тистики совместной встречаемости словоформ в большой монотематической коллекции. Труды третьей международной конференции «Корпус-ная лингвистика», 11–13 октября 2006 г., – СПб.: Изд-во С. Петерб. ун-та, 2006. – С. 135-142 (Rank Statistics of Word Co-Occurrences in a Big Monothematic Collection. Proc. 3rd International Conf. “Corpus Linguistics”, Oct. 11-13, 2006. SaintPetersburg State Publishing: 135-142).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Gelbukh</author>
<author>Grigory Sidorov</author>
</authors>
<title>Zipf and Heaps Laws’ Coefficients Depend on Language..</title>
<date>2001</date>
<booktitle>Proc. CICLing-2001, Conference on Intelligent Text Processing and Computational Linguistics</booktitle>
<pages>332--335</pages>
<contexts>
<context position="3572" citStr="Gelbukh and Sidorov (2001)" startWordPosition="563" endWordPosition="567">lecting languages like Russian? Many researchers consider a word form, not a word (lemma) be a perceptual lexicon unit (Zasorina, 1977; Ventsov and Kassevich, 1998; Verbitskaya et. al., 2003; Ventsov et. al., 2003). So a hypothesis that word forms in a corpus of an inflecting language should exhibit degree distribution similar to that of BNC looks appealing. An attempt to investigate word frequency rank sta89 TextGraphs-2: Graph-Based Al orithms for Natural Language Processing, pages 89–92, Rochester, April 2007 (c 2007 Association for Computational Linguistics tistics for Russian was made by Gelbukh and Sidorov (2001), but they studied only Zipf law on too small texts to reveal the kernel lexicon effects. To study the hypothesis one needs a corpus or a collection2 of texts comparable in volume with the BNC part that was examined in (Ferrer and Solé, 2001b), i.e. about 4.107 word occurrences. Certainly, texts that were analyzed in (Solé et al, 2005) were much smaller. Recently Kapustin and Jamsen (2006) and Kapustin (2006) studied a big (~5.107 word occurrences) collection of Russian. The collection exhibited power law behavior similar to that of BNC except that the vertex degree at the crossover point and </context>
</contexts>
<marker>Gelbukh, Sidorov, 2001</marker>
<rawString>Alexander Gelbukh and Grigory Sidorov, 2001. Zipf and Heaps Laws’ Coefficients Depend on Language.. Proc. CICLing-2001, Conference on Intelligent Text Processing and Computational Linguistics (February 18–24, 2001, Mexico City), Lecture Notes in Computer Science, Springer-Verlag. (2004): 332-335. (ISSN 0302-9743, ISBN 3-540-41687-0)</rawString>
</citation>
<citation valid="false">
<authors>
<author>Victor Kapustin</author>
<author>Anna Jamsen</author>
</authors>
<title>Ранговая ста-тистика встречаемости слов в большой текстовой коллекции. Труды 8ой Всероссийской научной конференции «Электронные библиотеки: пер-спективные методы и технологии, электронные коллекции»</title>
<date>2006</date>
<booktitle>RCDL’2006,</booktitle>
<pages>245--251</pages>
<location>Суздаль, Россия,</location>
<contexts>
<context position="3964" citStr="Kapustin and Jamsen (2006)" startWordPosition="632" endWordPosition="635">frequency rank sta89 TextGraphs-2: Graph-Based Al orithms for Natural Language Processing, pages 89–92, Rochester, April 2007 (c 2007 Association for Computational Linguistics tistics for Russian was made by Gelbukh and Sidorov (2001), but they studied only Zipf law on too small texts to reveal the kernel lexicon effects. To study the hypothesis one needs a corpus or a collection2 of texts comparable in volume with the BNC part that was examined in (Ferrer and Solé, 2001b), i.e. about 4.107 word occurrences. Certainly, texts that were analyzed in (Solé et al, 2005) were much smaller. Recently Kapustin and Jamsen (2006) and Kapustin (2006) studied a big (~5.107 word occurrences) collection of Russian. The collection exhibited power law behavior similar to that of BNC except that the vertex degree at the crossover point and the average degree were about 4-5 times less than that of BNC. These differences could be assigned either to a collection nature (legislation texts specifics) or to the properties of the (Russian) language itself. We shall reference the collection studied in (Kapustin and Jamsen, 2006; Kapustin, 2006) as “RuLegal”. In this paper we present a study of another big collection of Russian texts</context>
<context position="5212" citStr="Kapustin and Jamsen (2006)" startWordPosition="840" endWordPosition="843">egree distributions (for different big sub-collections) are similar to those of BNC and of RuLegal. While the exponents and the kernel lexicon size are also similar to those of BNC, the average degree for these collections are almost twice less than the average degree of BNC, and the nature of this difference is unclear still. The rest of the paper has the following structure. Technology section briefly describes the collection and the procedures of building of cooccurrence graph and of calculation of exponents of power laws. In Discussion section we compare the results obtained with those of Kapustin and Jamsen (2006), Kapustin (2006), and (Ferrer and Solé, 2001b). In Conclusion some considerations for future research are discussed. 2 Technology At present Russian National Corpus is unavailable for bulk statistical research due to copyright considerations. So we bought a CD (“World Literature in Russian”) in a bookstore – a collection of fiction translations to Russian. We’ll call the collection 2 We consider a corpus to be a special type of a text collection, which comprises text samples chosen for language research purposes, while a more general term “collection “ refers to a set of full texts brought to</context>
</contexts>
<marker>Kapustin, Jamsen, 2006</marker>
<rawString>Victor Kapustin and Anna Jamsen. 2006. Ранговая ста-тистика встречаемости слов в большой текстовой коллекции. Труды 8ой Всероссийской научной конференции «Электронные библиотеки: пер-спективные методы и технологии, электронные коллекции» – RCDL’2006, Суздаль, Россия, 2006. – C. 245–251 (Rank Statistics of Word Occurrence in a Big Text Collection. Proc. 8th National Russian Research Conference “Digital libraries: advanced methods and technologies, digital collections”, Oct. 17-19, 2006: 245-251).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexey Sokirko</author>
</authors>
<title>A short description of Dialing Project.</title>
<date>2001</date>
<contexts>
<context position="6326" citStr="Sokirko, 2001" startWordPosition="1022" endWordPosition="1023"> research purposes, while a more general term “collection “ refers to a set of full texts brought together for some other purpose. WLR. The size of the whole collection is more than 108 word occurrences. The source format of the collection is HTML, but its files contain essentially no formatting, just plain paragraphs. We made three non-overlapping samples from WLR (WLR1–3). The samples were approximately of the same size. Each sample was processed the same way. The idea behind using more than one sample was to estimate statistical errors. We used open source Russian grapheme analysis module (Sokirko, 2001) to strip HTML and to split the texts into words and sentences. Word cooccurrences were defined as in (Ferrer and Solé, 2001b): two words are “interacting” if and only if they: (a) appear in the same sentence, and (b) the word distance is either 1 (adjacent words) or 2 (one word or a number or a date in-between). A found co-occurred pair of words was tried out against MySQL database of recorded word pairs, and if it wasn’t found in the database, it was put there. Then we use a simple SQL query to get a table of count of vertices p(k) vs. vertex degree k. Figure 1. Raw degree distribution for W</context>
</contexts>
<marker>Sokirko, 2001</marker>
<rawString>Alexey Sokirko, 2001. A short description of Dialing Project.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ricard V Solé</author>
<author>Bernat Corominas</author>
<author>Sergi Valverde</author>
<author>Luc Steels</author>
</authors>
<title>Language Networks: their structure, function and evolution. SFI-WP 05-12-042, SFI Working Papers</title>
<date>2005</date>
<contexts>
<context position="2195" citStr="Solé et al, 2005" startWordPosition="346" endWordPosition="349">g, word forms, not words. tions of word interaction, but the results obtained don’t differ qualitatively. The interacting words form a graph, where the vertices are the words themselves, and the edges are the words’ cooccurrences. The fact of the collocation considered to be important, not the number of collocations of the same pair of words. Ferrer and Solé (2001b) studied vertices degree distribution and found two power laws for that distribution with a crossover at a degree approximately corresponding to the previously found size of the supposed kernel lexicon of about 103 – 104 words. In (Solé et al, 2005) word co-occurrence networks were studied for small (about 104 lines of text) corpora of English, Basque, and Russian. The authors claim the same tworegime word degree distribution behavior for all the languages. Dorogovtsev and Mendes (2001, 2003: 151-156) offered an abstract model of language evolution, which provides for two power laws for word degree distribution with almost no fitting, and also explains that the volume of the region of large degrees (the kernel lexicon) is almost independent of the corpus volume. Difference between word (lemma) and word form for an analytic language (e.g.</context>
<context position="3909" citStr="Solé et al, 2005" startWordPosition="624" endWordPosition="627">oks appealing. An attempt to investigate word frequency rank sta89 TextGraphs-2: Graph-Based Al orithms for Natural Language Processing, pages 89–92, Rochester, April 2007 (c 2007 Association for Computational Linguistics tistics for Russian was made by Gelbukh and Sidorov (2001), but they studied only Zipf law on too small texts to reveal the kernel lexicon effects. To study the hypothesis one needs a corpus or a collection2 of texts comparable in volume with the BNC part that was examined in (Ferrer and Solé, 2001b), i.e. about 4.107 word occurrences. Certainly, texts that were analyzed in (Solé et al, 2005) were much smaller. Recently Kapustin and Jamsen (2006) and Kapustin (2006) studied a big (~5.107 word occurrences) collection of Russian. The collection exhibited power law behavior similar to that of BNC except that the vertex degree at the crossover point and the average degree were about 4-5 times less than that of BNC. These differences could be assigned either to a collection nature (legislation texts specifics) or to the properties of the (Russian) language itself. We shall reference the collection studied in (Kapustin and Jamsen, 2006; Kapustin, 2006) as “RuLegal”. In this paper we pre</context>
</contexts>
<marker>Solé, Corominas, Valverde, Steels, 2005</marker>
<rawString>Ricard V. Solé, Bernat Corominas, Sergi Valverde and Luc Steels. 2005. Language Networks: their structure, function and evolution. SFI-WP 05-12-042, SFI Working Papers</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gideon Schwarz</author>
</authors>
<title>Estimating the dimension of a model.</title>
<date>1978</date>
<journal>Annals of Statistics</journal>
<volume>6</volume>
<issue>2</issue>
<pages>461--464</pages>
<contexts>
<context position="8743" citStr="Schwarz, 1978" startWordPosition="1437" endWordPosition="1438">– up to 13% in the middle of the curves), but are still small enough. In both cases (with and without RuLegal) we attempted to fit either a single power law (a straight line in log-log coordinates) or two/three power laws with one/two crossover points. Strong changes and large statistical errors of the distributions in the low degree region prevent meaningful usage of these points for fitting. We have made attempts to fit all three approximations for all points, and omitting one or two points with the lowest degrees. To choose between the hypotheses we minimized Schwarz information criterion (Schwarz, 1978): SIC=N*ln(Σi(pi-pˆi)2/N)-m*ln(N) where pi – cumulative distribution at i-th point; pˆi – fitting law at the same point; N – number of sampling points (13–15, depending on the number of omitted points); m – number of fitting parameters (2, 4 or 6 Omitted SIC (1/2/3 power laws) points WLR1–3 WLR1–3 + RuLegal 0 –44 / –85 / –68 –42 / –93 / –77 1 –46 / –85 / –65 –43 / –93 / –73 2 –47 / –80 / –60 –44 / –86 / –67 Table 1. Fitting power laws to averaged degree distributions – Schwarz information criterion WLR1–3 WLR1–3 RuLegal BNC + RuLegal Y1 –0.95 –0.95 –0.95 –0.5 Y2 –1.44 –1.46 –1.75 –1.7 kcross 6</context>
</contexts>
<marker>Schwarz, 1978</marker>
<rawString>Gideon Schwarz, 1978. Estimating the dimension of a model. Annals of Statistics 6(2): 461-464.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anatoly V Ventsov</author>
<author>Vadim B Kassevich</author>
</authors>
<title>Венцов А.В., Касевич В.Б. Cnoeapb d�A Modemu eocnpuAmuA peau. Вестник Санкт-Петербургского университета,</title>
<date>1998</date>
<volume>2</volume>
<pages>32--39</pages>
<contexts>
<context position="3109" citStr="Ventsov and Kassevich, 1998" startWordPosition="491" endWordPosition="495">language evolution, which provides for two power laws for word degree distribution with almost no fitting, and also explains that the volume of the region of large degrees (the kernel lexicon) is almost independent of the corpus volume. Difference between word (lemma) and word form for an analytic language (e.g. English) seems to be small. Dorogovtsev-Mendes model certainly treats word forms, not lemmas, as vertices in a corpus graph. Is it really true for inflecting languages like Russian? Many researchers consider a word form, not a word (lemma) be a perceptual lexicon unit (Zasorina, 1977; Ventsov and Kassevich, 1998; Verbitskaya et. al., 2003; Ventsov et. al., 2003). So a hypothesis that word forms in a corpus of an inflecting language should exhibit degree distribution similar to that of BNC looks appealing. An attempt to investigate word frequency rank sta89 TextGraphs-2: Graph-Based Al orithms for Natural Language Processing, pages 89–92, Rochester, April 2007 (c 2007 Association for Computational Linguistics tistics for Russian was made by Gelbukh and Sidorov (2001), but they studied only Zipf law on too small texts to reveal the kernel lexicon effects. To study the hypothesis one needs a corpus or a</context>
</contexts>
<marker>Ventsov, Kassevich, 1998</marker>
<rawString>Anatoly V. Ventsov and Vadim B. Kassevich, 1998. Венцов А.В., Касевич В.Б. Cnoeapb d�A Modemu eocnpuAmuA peau. Вестник Санкт-Петербургского университета, сер. 2, вып. 3, с. 32-39 (A Dictionary for a Speech Perception Model. Vestnik SanktPeterbugskogo Universiteta, 2(3): 32-39).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anatoly V Ventsov</author>
<author>Vadim B Kassevich</author>
<author>Elena V Yagounova</author>
</authors>
<title>Венцов А.В., Касевич В.Б., Ягу-нова Е.В. Kopnyc pyccxozo A3bwa u eocnpuAmue peau.</title>
<date>2003</date>
<journal>Научно-техническая информация.– Серия</journal>
<volume>2</volume>
<pages>25--32</pages>
<marker>Ventsov, Kassevich, Yagounova, 2003</marker>
<rawString>Anatoly V. Ventsov, Vadim B. Kassevich and Elena V. Yagounova, 2003. Венцов А.В., Касевич В.Б., Ягу-нова Е.В. Kopnyc pyccxozo A3bwa u eocnpuAmue peau. Научно-техническая информация.– Серия 2, № 6, с.25-32 (Russian Corpus and Speech Perception. Research and Technical Information, 2(6): 25-32).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liudmila A Verbitskaya</author>
<author>Nikolay N Kazansky</author>
<author>Vadim B Kassevich</author>
</authors>
<title>Вербицкая Л.А., Казанс-кий Н.Н., Касевич В.Б. Некоторые проблемы созда-ния национального корпуса русского языка.</title>
<date>2003</date>
<journal>Научно-техническая информация.– Серия</journal>
<booktitle>On Some Problems of Russian National Corpus Development. Research and Technical Information,</booktitle>
<volume>2</volume>
<pages>2--8</pages>
<marker>Verbitskaya, Kazansky, Kassevich, 2003</marker>
<rawString>Liudmila A. Verbitskaya, Nikolay N. Kazansky and Vadim B. Kassevich, 2003. Вербицкая Л.А., Казанс-кий Н.Н., Касевич В.Б. Некоторые проблемы созда-ния национального корпуса русского языка. Научно-техническая информация.– Серия 2, № 5, с.2-8 (On Some Problems of Russian National Corpus Development. Research and Technical Information, 2(5): 2-8).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lidia N Zasorina</author>
<author>ed</author>
</authors>
<title>Vacmomxbau c�oeapb pyccxozo A3bwa. Под</title>
<date>1977</date>
<booktitle>Frequency Dictionary of Russian. Russian Language,</booktitle>
<editor>ред. Л.Н. Засориной. М.: Русск. яз.</editor>
<location>Moscow,</location>
<marker>Zasorina, ed, 1977</marker>
<rawString>Lidia N. Zasorina, ed., 1977. Vacmomxbau c�oeapb pyccxozo A3bwa. Под ред. Л.Н. Засориной. М.: Русск. яз. (Frequency Dictionary of Russian. Russian Language, Moscow, 1977).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>