<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000355">
<title confidence="0.998439">
Extracting Latent Attributes from Video Scenes Using Text as
Background Knowledge
</title>
<author confidence="0.994288">
Anh Tran, Mihai Surdeanu, Paul Cohen
</author>
<affiliation confidence="0.996891">
University of Arizona
</affiliation>
<email confidence="0.973112">
{trananh, msurdeanu, prcohen}@email.arizona.edu
</email>
<sectionHeader confidence="0.994566" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999973352941177">
We explore the novel task of identify-
ing latent attributes in video scenes, such
as the mental states of actors, using
only large text collections as background
knowledge and minimal information about
the videos, such as activity and actor types.
We formalize the task and a measure of
merit that accounts for the semantic re-
latedness of mental state terms. We de-
velop and test several largely unsupervised
information extraction models that iden-
tify the mental states of human partici-
pants in video scenes. We show that these
models produce complementary informa-
tion and their combination significantly
outperforms the individual models as well
as other baseline methods.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999951733333333">
“Labeling a narrowly avoided vehicular
manslaughter as approach(car, person) is
missing something.”1 The recognition of ac-
tivities, participants, and objects in videos has
advanced considerably in recent years (Li et al.,
2010; Poppe, 2010; Weinland et al., 2011; Yang
and Ramanan, 2011; Ng et al., 2012). However,
identifying latent attributes of scenes, such as the
mental states of human participants, has not been
addressed. Latent attributes matter: If a video
surveillance system detects one person chasing
another, the response from law enforcement
should be radically different if the people are
happy (e.g., children playing) or afraid and angry
(e.g., a person running from an assailant).
</bodyText>
<footnote confidence="0.967907333333333">
This work is licenced under a Creative Commons Attribution
4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http:
//creativecommons.org/licenses/by/4.0/
1James Donlon, former manager of DARPA’s Mind’s Eye
program, personal communication.
</footnote>
<bodyText confidence="0.986750076923077">
Attributes that are latent in visual representa-
tions are often explicit in textual representations.
This suggests a novel method for inferring latent
attributes: Use explicit features of videos to query
text corpora, and from the resulting texts extract
attributes that are latent in the videos, such as men-
tal states. The contributions of this work are:
1: We formalize the novel task of latent attribute
identification from video scenes, focusing on the
identification of actors’ mental states. The input
for the task is contextual information about the
scene, such as detections about the activity (e.g.,
chase) and actor types (e.g., policeman or child),
and the output is a distribution over mental state
labels. We show that gold standard annotations
for this task can be reliably generated using crowd
sourcing. We define a novel evaluation measure,
called constrained weighted similarity-aligned F1
score, that accounts for both the differences be-
tween mental state distributions and the seman-
tic relatedness of mental state terms (e.g., partial
credit is given for irate when the target is angry).
2: We propose several robust and largely unsuper-
vised information extraction (IE) models for iden-
tifying the mental state labels of human partici-
pants in a scene, given solely the activity and actor
types: a lexical semantic (LS) model that extracts
mental state labels that are highly similar to the
context of the scene in a latent, conceptual vector
space; and an information retrieval (IR) model that
identifies labels commonly appearing in sentences
related to the explicit scene context. We show that
these models are complementary and their combi-
nation performs better than either model, alone.
3: Furthermore, we show that an event-centric
model that focuses on the mental state labels of
the participants in the relevant event (identified us-
ing syntactic patterns and coreference resolution)
outperforms the above shallower models.
</bodyText>
<page confidence="0.972343">
121
</page>
<note confidence="0.9846275">
Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 121–131,
Dublin, Ireland, August 23-24 2014.
</note>
<sectionHeader confidence="0.997492" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999983742857143">
As far as we know, the task proposed here is novel.
We can, however, review work relevant to each
part of the problem and our solution. Mental
state inference is often formulated as a classifica-
tion problem, where the goal is to predict target
mental state labels based on low-level sensory in-
put data. Most solutions try to learn classification
models based on large amounts of training data,
while some require human engineering of domain
knowledge. Hidden Markov Models (HMMs) and
Dynamic Bayesian Networks (DBNs) are popular
representations because they can model the tem-
poral evolution of mental states. For instance, the
mental states of students can be inferred from un-
intentional body gestures using a DBN (Abbasi et
al., 2009). Likewise, an HMM can also be used
to model the emotional states of humans (Liu and
Wang, 2011). Some solutions combine HMMs
and DBNs in a Bayesian inference framework to
yield a multi-layer representation that can do real-
time inference of complex mental and emotional
states (El Kaliouby and Robinson, 2004; Baltru-
saitis et al., 2011). Our work differs from these
approaches in several ways: It is mostly unsuper-
vised, multi-modal, and requires little training.
Relevant video processing technology includes
object detection (e.g., (Felzenszwalb et al., 2008)),
person detection, and pose detection (e.g., (Yang
and Ramanan, 2011)). Many tracking algo-
rithms have been developed, such as group track-
ing (McKenna et al., 2000), tracking by learn-
ing appearances (Ramanan et al., 2007), and
tracking in 3D space (Giebel et al., 2004; Brau
et al., 2013). For human action recognition,
current state-of-the-art techniques are capable of
achieving near perfect performance on the com-
monly used KTH Actions dataset (Schuldt et al.,
2004) and high performance rates on other more
challenging datasets (O’Hara and Draper, 2012;
Sadanand and Corso, 2012).
To extract mental state information from texts,
one might use any or all of the technologies of
natural language processing, so a complete review
of relevant technologies is impossible, here. Of
immediate relevance is the work of de Marneffe
et al. (2010), which identified the latent meaning
behind scalar adjectives (e.g., which ages people
have in mind when talking about “little kids”).
The authors learned these meanings by extract-
ing scalars, such as children’s ages, that were
commonly collocated with phrases, such as “lit-
tle kids,” in web documents. Mohtarami et al.
(2011) tried to infer yes/no answers from indirect
yes/no question-answer pairs (IQAPs) by predict-
ing the uncertainty of sentiment adjectives in in-
direct answers. Their method employs antonyms,
synonyms, word sense disambiguation as well as
the semantic association between the sentiment
adjectives that appear in the IQAP to assign a de-
gree of certainty to each answer. Sokolova and La-
palme (2011) further showed how to learn a model
for predicting the opinions of users based on their
written contents, such as reviews and product de-
scriptions, on the Web. Gabbard et al. (2011)
found that coreference resolution can significantly
improve the recall rate of relations extraction with-
out much expense to the precision rate.
Our work builds on these efforts by combining
information retrieval, lexical semantics, and event
extraction to extract latent scene attributes.
</bodyText>
<sectionHeader confidence="0.995349" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.999916">
For the experiments in this paper, we focus solely
on videos containing chase scenes. Chases often
invoke clear mental state inferences, and depend-
ing on context can suggest very different mental
state distributions for the actors involved.
</bodyText>
<subsectionHeader confidence="0.996313">
3.1 Video Corpus
</subsectionHeader>
<bodyText confidence="0.999981590909091">
We compiled a video dataset of 26 chase videos
found on the Web. Of these, five involve police
officers, seven involve children, four show sports-
related scenes, and twelve describe different chase
scenarios involving civilian adults (two videos in-
volve children playing sports). The average dura-
tion of the dataset is 8.8 seconds with a range of
[4,18]. Most videos involve a single chaser and a
single chasee (a person being chased) while a few
have several chasers and/or chasees.
For each video, we used Amazon Mechanical
Turk (MTurk) to identify both the actors and their
mental states. Each worker was asked to view a
video in its entirety before answering some ques-
tions about the scene. We give no prior training to
the workers. The questions were carefully phrased
to apply to all participants of a particular role, for
example all chasers (if there are more than one).
We also ask obvious validation questions about the
participants in each role (e.g., are the chasers run-
ning towards the camera?) and use the answers to
these questions to filter out poor responses. In gen-
</bodyText>
<page confidence="0.996456">
122
</page>
<bodyText confidence="0.999979446428572">
eral, we found that most responses were good and
only a few incomplete submissions were rejected.
In the first experiment, we asked MTurk work-
ers to select the actor types and various other de-
tections from a predefined list of tags. This label-
ing task is a proxy for a computer vision detection
system that functions at a human level of perfor-
mance. Indeed, we restricted the actor type labels
to a set that can be reasonably expected from auto-
matic detection algorithms: person, police officer,
child, and (non-human) object. For instance, po-
lice officers often wear distinctive color uniforms
that can be learned using the Felzenszwalb detec-
tor (Felzenszwalb et al., 2008), whereas children
can be reliably differentiated by their heights un-
der a 3D-tracking model (Brau et al., 2013). Each
video was annotated by three different workers
and the union of their annotations is produced.
The overall accuracy of the annotation was excel-
lent. The MTurk workers correctly identified the
important actors in every video.
Next, we collected a gold standard list of mental
state labels for each video by asking MTurk work-
ers to identify all applicable mental state adjec-
tives for the actors involved. We used a text-box
to allow for free-form input. Studies have shown
that people of different cultures can perceive emo-
tions very differently, and having forced choice
options cannot always capture their true percep-
tion (Gendron et al., 2014). Therefore, we did not
restrict the response of the workers in any way.
Workers could abstain from answering if they felt
the video was too ambiguous. Each video was
evaluated by ten different workers. We converted
each term provided to the closest adjective form
if possible. Terms with no equivalent adjective
forms were left in place. On rare occasions, work-
ers provided sentence descriptions despite being
asked for single-word adjectives. These sentences
were either removed, or collapsed into a single
word if appropriate. The overall quality of the an-
notations was good and generally followed com-
mon intuition. Asides from the frequently used
terms, we also received some colorful (yet infor-
mative) descriptions, like incredulous and vindic-
tive. In general, chases involving police scenar-
ios often contained violent and angry states while
chases involving children received more cheerful
labels. There were unexpected descriptions, such
as annoy for a playful chase between two children.
Upon review of the video, we agreed that one child
did indeed look annoyed. Thus, the resulting de-
scriptions were subjective, but very few were hard
to rationalize. By aggregating the answers from
the workers, we generated a gold standard distri-
bution of mental state terms for each video.2
</bodyText>
<subsectionHeader confidence="0.999515">
3.2 Text Corpus
</subsectionHeader>
<bodyText confidence="0.999783125">
The text corpus used for our models is the En-
glish Gigaword 5th Edition corpus3, made avail-
able by the Linguistics Data Consortium and in-
dexed by Lucene4. It is a comprehensive archive
of newswire text data (approximately 26 GB), ac-
quired over several years. It is in this corpus that
we expect to find mental state terms cued by con-
textual information from videos.
</bodyText>
<sectionHeader confidence="0.998227" genericHeader="method">
4 Neighborhood Models
</sectionHeader>
<bodyText confidence="0.999964967741935">
We developed several individual models based on
the neighborhood paradigm, that is, the hypoth-
esis that relevant mental state labels will appear
“near” text cued by the visual features of a scene.
The models take as input the context extracted
from a video scene, defined simply as a list of “ac-
tivity and actor-type” tuples (e.g., (chase, police)).
Multiple actor types will result in multiple tuples
for a video. The actors can be either a person, a
policeman, a child, or a (non-human) object. If
the detections describe the actor as both a person
and a child, or a person and a policeman, we auto-
matically remove the person label as it is a Word-
Net (Miller, 1995) hypernym of both child and po-
liceman. For each human actor type, we further
increase our coverage by retrieving the synonym
set (synset) of its most frequent sense (i.e., sense
#1) from WordNet. For example, a chase involv-
ing a policeman would generate the following tu-
ples: (chase, policeman) and (chase, officer).
We call these query tuples because they are used
to query text for sentences that – if all goes well –
will contain relevant mental state labels.
Given query tuples, our models use an initial
seed set of 160 mental state adjectives to produce
a single distribution over mental state labels, re-
ferred to as the response distribution, for each
video. The seed set is compiled from popular
mental and emotional state dictionaries, includ-
ing the Profile of Mood States (POMS) (McNair
et al., 1971) and Plutchik’s wheel of emotion. We
</bodyText>
<footnote confidence="0.9981775">
2All videos and annotations are available at:
http://trananh.github.io/vlsa
3Linguistics Data Consortium catalog no. LDC2011T07
4Apache Lucene: http://lucene.apache.org
</footnote>
<page confidence="0.995738">
123
</page>
<subsectionHeader confidence="0.898028">
Source Example Mental State Labels
</subsectionHeader>
<bodyText confidence="0.491914285714286">
POMS alert, annoyed, energetic, exhausted, helpful,
sad, terrified, unworthy, weary, etc.
Plutchik angry, disgusted, fearful, joyful/joyous,
sad, surprised, trusting, etc.
agitated, competitive, cynical, disappointed,
Others
excited, giddy, happy, inebriated, violent, etc.
</bodyText>
<tableCaption confidence="0.894202">
Table 1: The initial seed set contains 160 mental
</tableCaption>
<bodyText confidence="0.9319255">
state labels, compiled from different sources like
the popular Profile of Mood States dictionary and
Plutchik’s wheel of emotion.
also included frequently used labels gathered from
synsets found in WordNet (see Table 1 for exam-
ples). Note that the gold standard annotations pro-
duced by MTurk workers (Sec. 3) was not a source
for this set, nor was it restricted to these terms.
</bodyText>
<subsectionHeader confidence="0.99504">
4.1 Back-off Interpolation in Vector Space
</subsectionHeader>
<bodyText confidence="0.999990368421053">
Our first model uses the recurrent neural net-
work language model (RNNLM) of Mikolov et
al. (2013) to project both mental state labels and
query tuples into a latent conceptual space. Simi-
larity is then trivially computed as the cosine sim-
ilarity between these vectors. In all of our experi-
ments, we used a RNNLM computed over the Gi-
gaword corpus with 600-dimensional vectors.
For this vector space (vec) model, we separate
the query tuples into different levels of back-off
context. The first level includes the set of activ-
ity types as singleton context tuples, e.g., (chase),
while the second level includes all (activity, actor)
context tuples. Hence, each query tuple will yield
two different context tuples, one for each back-off
level. For each context tuple with multiple terms,
such as (chase, policeman), we find the vector rep-
resentation for the context by aggregating the vec-
tors representing the search terms:
</bodyText>
<equation confidence="0.978715">
vec(chase, policeman) = vec(chase) +
vec(policeman) .
</equation>
<bodyText confidence="0.9749423125">
The vector representation for a singleton con-
text tuple is just the vector of the single search
term. We then calculate the distance of each men-
tal state label m to the normalized vector represen-
tation of the context tuple by computing the cosine
similarity score between the two vectors:
JJvec(m)JJ JJvec(context tuple)JJ .
The hypothesis here is that mental state labels
that are related to the search context will have a
RNNLM vector that is closer to the context tuple
vector, resulting in a high cosine similarity score.
Because the number of latent dimensions is rela-
tively small (when compared to vocabulary size),
cosine similarity scores in this latent space tend to
be close. To further separate these scores, we raise
them to an exponential power:
</bodyText>
<equation confidence="0.989671">
score(m) = ecos(O-,)+1 − 1 .
</equation>
<bodyText confidence="0.999995789473684">
The processing of each context tuple yields 160
different scores, one for each mental state label.
We normalize these scores to form a single distri-
bution of scores for each context tuple. The distri-
butions are then integrated into a single distribu-
tion representative of the complete activity as fol-
lows: (a) the distributions at each context back-off
level are averaged to generate a single distribution
per level – for the second level (which includes
activity and actor types), it means distributions for
all (activity, actor) tuples are averaged, whereas
the first level only has a single distribution from
the singleton activity tuple (chase); and (b) distri-
butions for the different levels are linearly interpo-
lated, similar to the back-off strategy of (Collins,
1997). Let e1 and e2 represent the weights of some
mental state label m from the average distribution
at the first and second level, respectively. Then the
interpolated distribution score e for m is:
</bodyText>
<equation confidence="0.984213">
e = λe1 + (1 − λ)e2 .
</equation>
<bodyText confidence="0.999942125">
Compiling the distribution scores for each m
produces the final distribution representing the ac-
tivity modeled. We prune this final distribution by
taking the top ranked items that make up some γ
proportion of the distribution. We delay the dis-
cussion of how γ is tuned to Section 6. The final
pruned distribution is normalized to produce the
response distribution.
</bodyText>
<subsectionHeader confidence="0.878541">
4.2 Sentence Co-occurrence with Deleted
Interpolation
</subsectionHeader>
<bodyText confidence="0.999995272727273">
Our second model, the sent model, extracts mental
state labels based on the likelihood that they ap-
pear in sentences cued by query tuples. For each
tuple, we estimate the conditional probability that
we will see a mental state label m in a sentence,
where m is from the seed set, given that we al-
ready observed the desired activity and actor type
in the same sentence: P(mJactivity, actor). In this
case, we refer to the sentence length as the neigh-
borhood window. Furthermore, all terms must ap-
pear as the correct part-of-speech (POS): m must
</bodyText>
<equation confidence="0.9981845">
vec(m) · vec(context tuple)
cos(Om) =
</equation>
<page confidence="0.966037">
124
</page>
<bodyText confidence="0.999991210526316">
appear as an adjective or verb, the activity as a
verb, and the actor as a noun. (Mental state adjec-
tives are allowed to appear as verbs because some
are often mis-tagged as verbs; e.g., agitated, deter-
mined, welcoming.) We used Stanford’s CoreNLP
toolkit for tokenization and POS tagging.5
Note that this probability is similar to a trigram
probability in POS tagging, except the triples need
not form an ordered sequence but must appear in
the same sentence and under the correct POS tag.
Unfortunately, we cannot always compute this tri-
gram probability directly from the corpus because
there might be too few instances of each trigram
to compute a probability reliably. As is common,
we instead estimate it as a linear interpolation of
unigrams, bigrams, and trigrams. We define the
maximum likelihood probabilities Pˆ, derived from
relative frequencies f, for the unigrams, bigrams,
and trigrams as follows:
</bodyText>
<equation confidence="0.991123857142857">
Pˆ(m) = f(m)
N
Pˆ(m|activity) = f(m, activity)
f(activity)
f (m, activity, actor)
Pˆ(m|activity, actor) =
f(activity, actor)
</equation>
<bodyText confidence="0.9990005">
for all mental state labels m, activities, and actor
types in our queries. N is the total number of to-
kens in the corpus. The aforementioned POS re-
quirement is enforced: f(m) is the number of oc-
currences of m as an adjective or verb. We define
Pˆ = 0 if the corresponding numerator and denom-
inator are zero. The desired trigram probability is
then estimated as:
</bodyText>
<equation confidence="0.99985">
P (m|activity, actor) = λ1 Pˆ(m) +
λ2 Pˆ(m|activity) + λ3 Pˆ(m|activity, actor) .
</equation>
<bodyText confidence="0.9997315">
As λ1 +λ2 +λ3 = 1, P represents a probability
distribution. We use the deleted interpolation algo-
rithm (Brants, 2000) to estimate one set of lambda
values for the model, based on all trigrams.
For each query tuple generated in a video, 160
different trigrams are computed, one for each men-
tal state label in the seed set, resulting in 160 con-
ditional probability scores. We normalize these
scores into a single distribution – the mental state
distribution for that query tuple. We then combine
</bodyText>
<footnote confidence="0.763457">
5http://nlp.stanford.edu/software/
corenlp.shtml.
</footnote>
<bodyText confidence="0.999833142857143">
all resulting distributions, one from each query tu-
ple, and take the average to produce a single dis-
tribution over mental state labels for the video. As
before, we prune this distribution by taking the
top-ranked items that cover a large fraction γ of
total probability. The pruned distribution is renor-
malized to yield the final response distribution.
</bodyText>
<subsectionHeader confidence="0.999331">
4.3 Event-centric with Deleted Interpolation
</subsectionHeader>
<bodyText confidence="0.928297896551724">
The sent model has two limitations. On one hand,
it is too sparse: the single sentence neighborhood
window is too small to reliably estimate the fre-
quencies of trigrams for the probabilities of men-
tal state terms. On the other hand, it may be too
lenient, as it extracts all mental state mentions ap-
pearing in the same sentence with the activity, or
event, under consideration, regardless if they ap-
ply to this event or not. We address these limita-
tions next with an event-centric model (event).
Intuitively, the event model focuses on the men-
tal state labels of event participants. Formally,
these mental state terms are extracted as follows:
1: We identify event participants (or actors). We
do this by analyzing the syntactic dependencies of
sentences containing the target verb (e.g., chase)
to find the subject and object. In most cases, the
nominal subject of the verb chase is the chaser and
the direct object is the person being chased. We
implemented additional patterns to model passive
voice and other exceptions. We used Stanford’s
CoreNLP toolkit for syntactic dependency parsing
and the downstream coreference resolution.
2: Once the phrases that point to actors are iden-
tified, we identify all mentions of these actors in
the entire document by traversing the coreference
chains containing the phrases extracted in the pre-
vious step. The sentences traversed in the chains
define the neighborhood area for this model.
</bodyText>
<listItem confidence="0.753263769230769">
3: Lastly, we identify the mental state terms of
event participants using a second set of syntac-
tic patterns. First, we inspect several copulative
verbs, such as to be and feel, and extract men-
tal state labels from these structures if the corre-
sponding subject is one of the mentions detected
above. Second, we search for mental states along
adjectival modifier relations, where the head is an
actor mention. For all patterns, we make sure to
filter for only mental state complements belong-
ing to the initial seed list. The same POS restric-
tion as in the other models also applies. We incre-
ment the joint frequency f for the n-gram once for
</listItem>
<page confidence="0.994972">
125
</page>
<bodyText confidence="0.96563644">
each neighborhood that properly contain all search G (irate, 0.8), (afraid, 0.2)
terms from the n-gram in the correct POS.
The event model addresses both limitations of
the sent model: it avoids the lenient extraction of
mental state labels by focusing on labels associ-
ated with event participants; it addresses sparsity
by considering all mentions of event participants
in a document.
To understand the impact of this model, we
compare it against two additional baselines. The
first baseline investigates the importance of focus-
ing on mental state terms associated with event
participants. This model, called coref, implements
the first two steps of the above algorithm, but in-
stead of extracting only mental state terms associ-
ated with event actors (last step), it considers all
mentions appearing anywhere in the coreference
neighborhood. That is, all unique sentences tra-
versed by the relevant coreference chains are first
pieced together to define a single neighborhood for
a given document; then the relative joint frequen-
cies of n-grams are computed by incrementing f
once for each neighborhood that contains all terms
with correct POS tags.
The second baseline analyzes the importance of
coreference resolution to our problem. This model
is similar to sent, with the modification that it in-
creases the size of the neighborhood window to in-
clude the immediate neighbors of target sentences
that contain activity labels. We call this the win-n
model: The window around a target verb contains
2n + 1 sentences. We build the context neigh-
borhood by concatenating all target sentences and
their windows together for a given document. This
defines a single neighborhood for each document.
This contrasts with the sent model, in which the
neighborhood is defined for each sentence con-
taining the activity label in the document, resulting
in several possible neighborhoods in a document.
The joint frequency f for each n-gram – where
n &gt; 1 – is computed similarly with the coref
model: it is incremented once for each neighbor-
hood that contains all the terms from the n-gram
in the correct POS. Frequencies for unigrams are
computed similar to sent.
As before, 160 different trigrams are generated
for each query tuple, one for each mental state la-
bel in the seed set, resulting in 160 conditional
probability scores. We similarly combine these
scores and generate a single pruned distribution as
the response for each of the model above.
126
R1 (angry, 0.6), (mad, 0.4)
R2 (irate, 0.2), (afraid, 0.8)
R3 (mad, 0.4), (irate, 0.4), (scared, 0.2)
Table 2: We show an example gold standard dis-
tribution G and several candidate response distri-
butions to be matched against G. Here, R3 best
matches the shape and meaning of G, because
(irate, mad) and (afraid, scared) are close syn-
onyms. R2 appears to match G semantically, but
matches its shape poorly. R1 misses one of the
mental state labels, afraid, but contains labels that
are semantically close to the weightiest term in G.
4.4 Ensemble Model
We combined the results from the event and
vec models to produce an ensemble model (ens)
which, for a mental state label m, returns the aver-
age of m’s scores according to the response distri-
butions of the two individual models.
5 Evaluation Measures
Let R denote the response distribution over mental
state labels produced for a single video by one of
the models described in the previous section, and
let G denote the gold standard distribution pro-
duced for the same video by MTurk workers. If
R is similar to G then our models produce simi-
lar mental state terms as the workers. There are
many ways to compare distributions (e.g., KL dis-
tance, chi-square statistics) but these give bad re-
sults when distributions are sparse. More impor-
tantly, for our purposes, the measures that compare
the shapes of distributions do not allow semantic
comparisons at the level of distribution elements.
Suppose R assigns high scores to angry and mad,
only, while G assigns a high score to happy, only.
Clearly, R is wrong. But if instead G had assigned
a high score to irate, only, then R would be more
right than wrong because, at the level of the indi-
vidual elements, angry and mad are similar to irate
but not similar to happy.
We describe a series of measures, starting with
the familiar F1 score, and discuss their applicabil-
ity. To illustrate the effectiveness of each measure,
we will use the examples shown in Table 2.
5.1 Fl Score
The F1 score measures the similarity between two
sets of elements, R and G. F1 = 1 when R = G
and F1 = 0 when R and G share no elements. F1
is the harmonic mean of precision and recall:
</bodyText>
<equation confidence="0.99987275">
precision = |R ∩ G ||R ∩ G|
|R |, recall = |G |,
(1)
F1 = 2 · precision · recall precision + recall .(2)
</equation>
<bodyText confidence="0.95977975">
The F1 score penalizes the responses in Table 3
that include semantically similar labels to those in
G, and fails to reflect the weights of the labels in
G and R.
</bodyText>
<subsectionHeader confidence="0.997789">
5.2 Similarity-Aligned Fl Score
</subsectionHeader>
<bodyText confidence="0.9999796">
Although the standard F1 does not immediately fit
our needs, it is a good starting point. We can in-
corporate the semantic similarity of distribution el-
ements by generalizing the formulas for precision
and recall as follows:
</bodyText>
<equation confidence="0.993222333333333">
precision =
1 � recall = |G|
g∈G
</equation>
<bodyText confidence="0.9989875">
where σ ∈ [0, 1] is a function that yields the simi-
larity between two elements. The standard F1 has:
</bodyText>
<equation confidence="0.786068">
� 1 , if r = g
σ(r, g) = 0 , otherwise ,
</equation>
<bodyText confidence="0.990210545454546">
but clearly σ can be defined to take values pro-
portional to the similarity of r and g. We can
choose from a wide range of semantic similarity
and relatedness measures that are based on Word-
Net (Pedersen et al., 2004). The recent RNNLM
of Mikolov opens the door to even more similar-
ity measures based on vector space representations
of words (Mikolov et al., 2013). After experi-
mentations, we settled on one proposed by Hirst
and St-Onge (1998). It represents two lexicalized
concepts as semantically close if their WordNet
synsets are connected by a path that is not too
long and that “does not change direction too of-
ten” (Hirst and St-Onge, 1998). We chose this
metric because it has a finite range, accommodates
numerous POS pairs, and works well in practice.
Given the generalized precision and recall for-
mulas in Eq 3, our similarity-aligned (SA) F1
score can be computed in the usual way, as the
harmonic mean of precision and recall (Eq 2).
SA-F1 is inspired by the Constrained Entity-
Aligned F-Measure (CEAF) metric proposed
</bodyText>
<table confidence="0.9863384">
F1 SA-F1 CWSA-F1
p r f1 p r f1 p r f1
R1 0 0 0 1 .5 23 1 .8 .89
R2 1 1 1 1 1 1 .4 .4 .4
R3 13 .5 .4 1 1 1 1 1 1
</table>
<tableCaption confidence="0.992">
Table 3: The precision (p), recall (r), and F1
</tableCaption>
<bodyText confidence="0.9738054">
(f1) scores under various evaluation models are
presented for the examples from Table 2. Sup-
pose that σ(irate, angry) = σ(irate, mad) =
σ(afraid, scared) = 1, with σ of any two identi-
cal strings being 1, and σ of all other pairs are 0.
by (Luo, 2005) for coreference resolution. CEAF
computes an optimal one-to-one mapping between
subsets of reference and system entities before it
computes recall, precision and F. Similarly, SA-F1
finds optimal mappings between the labels of the
two sets based on σ (this is what the max terms in
Eq 3 do). Table 3 shows that SA-F1 correctly re-
wards the use of synonyms. The high scores given
to R2, however, indicate that it does not measure
the similarity between distribution shapes.
</bodyText>
<subsectionHeader confidence="0.9932235">
5.3 Constrained Weighted Similarity-Aligned
Fl Score
</subsectionHeader>
<bodyText confidence="0.9996122">
Let R(r) and G(r) be the probabilities of label
r in the R and G distributions, respectively. Let
σ∗ S(`) denote the best similarity score achievable
when comparing elements from set S to ` us-
ing the similarity function σ. That is, σ∗S(`) =
maxe∈S σ(`, e). We can easily weight σ∗ S(`) by
the probability of `. For example, we might re-
define precision as Er∈R R(r) · σ∗G(r). However,
this would not account for the probability of r in
the gold standard distribution, G.
An analogy might help here: Suppose we have
an unknown “mystery bag” of 100 colored pen-
cils that we will try to match with a “response
bag” of pencils. If we fill our response bag with
100 crimson pencils, while the mystery bag con-
tains only 25 crimson pencils, then our precision
score should get points only for the first 25 pen-
cils, while the remaining 75 in the response bag
should not be rewarded. For recall, the reward
given for each color in the mystery bag is capped
by the number of pencils of that color in the re-
sponse bag. The analogy is complete when we
consider that crimson pencils should perhaps be
partially rewarded when matched by cardinal, rose
or cerise pencils. In other words, a similarity mea-
</bodyText>
<figure confidence="0.959275666666667">
1 max σ(r, g) ,
|R |r∈R g∈G
(3)
max
r∈R
σ(r, g) ,
</figure>
<page confidence="0.987995">
127
</page>
<bodyText confidence="0.9955252">
sure should account for an accumulated mass of
synonyms. Let MS(E) denote the subset of terms
from S that have the best similarity score to E:
MS(E) = {e  |Q(E, e) = Q∗ S(E), be E S} .
We define new forms of precision and recall as:
</bodyText>
<equation confidence="0.985212333333334">
⎛ ⎞
�
min ⎝R(r), G(e) ⎠Q∗G(r) ,
e∈MG(r)
⎛ ⎞
�
min ⎝G(g), R(e) ⎠Q∗R(g) .
e∈MR(g)
(4)
</equation>
<bodyText confidence="0.999609">
The resulting constrained weighted similarity-
aligned (CWSA) F1 score is the harmonic mean
of these new precision and recall scores. Table 3
shows that CWSA-F1 yields the most intuitive
evaluation of the response distributions, down-
weighting R2 in favor of R3 and R1.
</bodyText>
<sectionHeader confidence="0.995156" genericHeader="method">
6 Experimental Procedure
</sectionHeader>
<bodyText confidence="0.999968736842105">
As described in Section 3, MTurk workers anno-
tated 26 videos by identifying the actor types and
mental state labels for each video. The actor types
become query tuples of the form (activity, actor)
and the mental state labels are compiled into one
probability distribution over labels for each video,
designated G. The query tuples were provided to
our neighborhood models (Sec. 4), which returned
a response distribution over mental state labels for
each video, designated R.
We selected four videos of the 26 to calibrate
the prune parameters γ and the interpolation pa-
rameters A (Sec. 4). One of these videos contains
children, one has police involvement, and two con-
tain adults. We asked additional MTurk workers to
annotate these videos, yielding an independent set
of annotations to be used solely for calibration.
The experimental question is, how well does G
match R for each video?
</bodyText>
<sectionHeader confidence="0.998409" genericHeader="evaluation">
7 Results &amp; Discussions
</sectionHeader>
<bodyText confidence="0.999734">
We report the average performance of our mod-
els along with two additional baseline methods in
Table 4. The naive baseline method unif simply
binds R to the initial seed set of 160 mental state
labels with uniform probability, while the stronger
freq baseline uses the occurrence frequency dis-
tribution of the labels from the Gigaword corpus
(note that only occurrences tagged as adjectives or
</bodyText>
<table confidence="0.997143222222222">
F1 CWSA-F1
p r f1 p r f1
unif .107 .750 .187 .284 .289 .286
freq .107 .750 .187 .362 .352 .355
sent .194 .293 .227 .366 .376 .368
vec .226 .145 .175 .399 .392 .393
coref .264 .251 .253 .382 .461 .416
event .231 .303 .256 .446 .488 .463
ens .259 .296 .274 .488 .517 .500
</table>
<tableCaption confidence="0.945202">
Table 4: The average evaluation performance
</tableCaption>
<bodyText confidence="0.995518857142857">
across 26 different chase videos are shown against
2 different baselines for all proposed models. Bold
font indicates the best score in a given column.
verbs were counted). All average improvements
of the ensemble model over the baseline models
are significant (p &lt; 0.01). Significance tests were
one-tailed and were based on nonparametric boot-
strap resampling with 10, 000 iterations.
Using the classical F1 measure, the coref model
scored highest on precision, while the ensemble
method did best on F1. Not surprisingly, no model
can top the baseline methods on recall as both
baselines use the entire seed set of 160 terms.
Even so, the average recall for the baselines were
only .750, which means that the initial seed set did
not include words that were used by the MTurk an-
notators. As we’ve mentioned, the classical F1 is
misleading because it does not credit synonyms.
For example, in one movie, one of our models
was rewarded once for matching the label angry
and penalized six times for also reporting irate,
enraged, raging, upset, furious, and mad. Fre-
quently, our models were penalized for using the
terms scared and afraid instead of fearful.
Under the CWSA-F1 evaluation measure,
which correctly accounts for both synonyms and
label probabilities, our ensemble model performed
best. The average CWSA-F1 score of the ensem-
ble model improves upon the simple uniform base-
line unif by almost 75%, and over the stronger
freq baseline by over 40%. The ensemble method
also outperforms each individual method in all
measured scores. These improvements were also
found to be significant. This strongly suggests
that the vec and event models are complementary,
and not entirely redundant. Furthermore, Table 4
shows that the event model performs considerably
better than coref. This result emphasizes the im-
portance of focusing on the mental state labels of
event participants rather than considering all men-
tal state terms collocated in the same sentence with
an actor or action verb.
</bodyText>
<equation confidence="0.76516125">
�p =
r∈R
�r =
g∈G
</equation>
<page confidence="0.985995">
128
</page>
<table confidence="0.9991092">
Models CWSA-F1 Versus coref p-value
win-0 0.388682 −0.027512 0.0067
win-1 0.415328 −0.000866 0.4629
win-2 0.399777 −0.016417 0.0311
win-3 0.392832 −0.023362 0.0029
</table>
<tableCaption confidence="0.802617">
Table 5: The average CWSA-Fi scores for the
</tableCaption>
<bodyText confidence="0.998696209302325">
win-n model with different window parameters are
shown in comparison to the coref model. The
coref model outperformed all tested configura-
tions, though the difference is not significant for
n = 1. The p-value based on the average differ-
ences were obtained using one-tailed nonparamet-
ric bootstrap resampling with 10, 000 iterations.
Table 5 explores the effectiveness of corefer-
ence resolution in expanding the neighborhood
area. The coref model outperformed the simple
windowing method under every tested configura-
tion. However, the improvement over windowing
with n = 1 is not significant. This can be ex-
plained by fact that immediately neighboring sen-
tences are more likely to be related. Moreover,
since newswire articles tend to be short, the neigh-
borhoods generated by win-1 tend to be similar to
those generated by coref. In general, coref does
not do worse than a simple windowing method and
has the bonus advantage of providing references to
the actors of interest for downstream processes.
In Table 6, we show the performance results
based on the types of chase scenarios happening in
the videos. The average scores under the uniform
baseline unif for chase videos involving children
and sporting events are lower than for police and
other chases. This suggests that our seed set of
160 mental state labels is biased towards the latter
types of events, and is not as fit to describe chases
involving children.
On average, videos involving police officers
show the biggest improvement in the CWSA-Fi
scores over the unif baseline (+0.2693), whereas
videos involving children received the lowest gain
(+0.1517). We believe this is the effect of the
Gigaword text corpus, which is a comprehensive
archive of newswire text, and thus is heavily bi-
ased towards high-speed and violent chases in-
volving the police. The Gigaword corpus is not
the place to find children happily chasing each
other. Similarly, sports-related chases, which are
also news-worthy, have a higher gain than chil-
dren’s videos on average.
</bodyText>
<table confidence="0.9538214">
Categories Unif Ensemble Gain
children 0.2082 0.3599 +0.1517
police 0.3313 0.6006 +0.2693
sports 0.2318 0.4126 +0.1808
others 0.3157 0.5457 +0.2300
</table>
<tableCaption confidence="0.761751333333333">
Table 6: The average CWSA-F, scores for the en-
semble model are shown in comparison to the uni-
form baseline method, categorize by video types.
</tableCaption>
<sectionHeader confidence="0.977709" genericHeader="conclusions">
8 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999984825">
We introduced the novel task of identifying latent
attributes in video scenes, specifically the men-
tal states of actors in chase scenes. We showed
that these attributes can be identified by using ex-
plicit features of videos to query text corpora, and
from the resulting texts extract attributes that are
latent in the videos. We presented several largely
unsupervised methods for identifying distributions
of actors’ mental states in video scenes. We de-
fined a similarity measure, CWSA-Fl, for com-
paring distributions of mental state labels that ac-
counts for both semantic relatedness of the labels
and their probabilities in the corresponding distri-
butions. We showed that very little information
from videos is needed to produce good results that
significantly outperform baseline methods.
In the future, we plan to add more detection
types. Additional contextual information from
videos (e.g., scene locations) should help improve
performance, especially on tougher videos (e.g.,
videos involving children chases). Moreover, we
believe that the initial seed set of mental state la-
bels can be learned simultaneously with the ex-
traction patterns of the event model using a mutual
bootstrapping method, similar to that of (Riloff
and Jones, 1999).
Currently, our experiments assume one distri-
bution of mental state labels for each video. They
do not distinguish between the mental states of the
chaser and chasee, while in reality these partici-
pants may be in very different states of mind. Our
event model is capable of making this distinction
and we will test its performance on this task in the
future. We also plan to test the effectiveness of our
models with actual computer vision detectors. As
a first approximation, we will simulate the noisy
nature of detectors by degrading the quality of an-
notated data. Using artificial noise on ground-truth
data, we can simulate the performance of real de-
tectors and test the robustness of our models.
</bodyText>
<page confidence="0.998116">
129
</page>
<sectionHeader confidence="0.983598" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999885287037037">
Abdul Rehman Abbasi, Matthew N. Dailey, Nitin V.
Afzulpurkar, and Takeaki Uno. 2009. Student men-
tal state inference from unintentional body gestures
using dynamic Bayesian networks. Journal on Mul-
timodal User Interfaces, 3(1-2):21–31, December.
Tadas Baltrusaitis, Daniel McDuff, Ntombikayise
Banda, Marwa Mahmoud, Rana el Kaliouby, Peter
Robinson, and Rosalind Picard. 2011. Real-time
inference of mental states from facial expressions
and upper body gestures. In Face and Gesture 2011,
pages 909–914. IEEE, March.
Thorsten Brants. 2000. TnT: A statistical part-of-
speech tagger. In Proceedings of the sixth confer-
ence on Applied natural language processing, pages
224–231, Morristown, NJ, USA. Association for
Computational Linguistics.
Ernesto Brau, Jinyan Guan, Kyle Simek, Luca Del
Pero, Colin Reimer Dawson, and Kobus Barnard.
2013. Bayesian 3D Tracking from monocular video.
In The IEEE International Conference on Computer
Vision (ICCV), December.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of the
35th annual meeting on Association for Computa-
tional Linguistics -, pages 16–23, Morristown, NJ,
USA. Association for Computational Linguistics.
R. El Kaliouby and P. Robinson. 2004. Real-Time In-
ference of Complex Mental States from Facial Ex-
pressions and Head Gestures. In 2004 Conference
on Computer Vision and Pattern Recognition Work-
shop, pages 154–154. IEEE.
Pedro Felzenszwalb, David McAllester, and Deva Ra-
manan. 2008. A discriminatively trained, multi-
scale, deformable part model. In 2008 IEEE Confer-
ence on Computer Vision and Pattern Recognition,
pages 1–8. IEEE, June.
Ryan Gabbard, Marjorie Freedman, and
RM Weischedel. 2011. Coreference for learn-
ing to extract relations: yes, Virginia, coreference
matters. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics:
Human Language Technologies: short papers -
Volume 2, pages 288–293.
Maria Gendron, Debi Roberson, Jacoba Marieta
van der Vyver, and Lisa Feldman Barrett. 2014.
Cultural relativity in perceiving emotion from vo-
calizations. Psychological science, 25(4):911–20,
April.
J Giebel, DM Gavrila, and C Schn¨orr. 2004. A
bayesian framework for multi-cue 3d object track-
ing. In Computer Vision-ECCV 2004, pages 241–
252.
Graeme Hirst and D St-Onge. 1998. Lexical chains as
representations of context for the detection and cor-
rection of malapropisms. In Christiane Fellbaum,
editor, WordNet: An Electronic Lexical Database
(Language, Speech, and Communication), pages
305–332. The MIT Press.
LJ Li, Hao Su, L Fei-Fei, and EP Xing. 2010. Ob-
ject bank: A high-level image representation for
scene classification &amp; semantic feature sparsifica-
tion. In Advances in Neural Information Processing
Systems.
Zhilei Liu and Shangfei Wang. 2011. Emotion recog-
nition using hidden Markov models from facial tem-
perature sequence. In ACII’11 Proceedings of the
4th international conference on Affective computing
and intelligent interaction - Volume Part II, pages
240–247.
Xiaoqiang Luo. 2005. On coreference resolution per-
formance metrics. In Proceedings of the confer-
ence on Human Language Technology and Empiri-
cal Methods in Natural Language Processing - HLT
’05, pages 25–32, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
MC De Marneffe, CD Manning, and Christopher Potts.
2010. ”Was it good? It was provocative.” Learning
the meaning of scalar adjectives. In Proceedings of
the 48th Annual Meeting of the Association for Com-
putational Linguistics, pages 167–176.
Stephen J. McKenna, Sumer Jabri, Zoran Duric, Azriel
Rosenfeld, and Harry Wechsler. 2000. Tracking
Groups of People. Computer Vision and Image Un-
derstanding, 80(1):42–56, October.
D M McNair, M Lorr, and L F Droppleman. 1971.
Profile of Mood States (POMS).
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781, pages 1–12.
George A. Miller. 1995. WordNet: a lexical
database for English. Communications of the ACM,
38(11):39–41, November.
Mitra Mohtarami, Hadi Amiri, Man Lan, and
Chew Lim Tan. 2011. Predicting the uncertainty
of sentiment adjectives in indirect answers. In Pro-
ceedings of the 20th ACM international conference
on Information and knowledge management - CIKM
’11, page 2485, New York, New York, USA. ACM
Press.
CB Ng, YH Tay, and BM Goi. 2012. Recognizing hu-
man gender in computer vision: a survey. PRICAI
2012: Trends in Artificial Intelligence, 7458:335–
346.
S O’Hara and B. A. Draper. 2012. Scalable action
recognition with a subspace forest. In 2012 IEEE
Conference on Computer Vision and Pattern Recog-
nition, pages 1210–1217. IEEE, June.
</reference>
<page confidence="0.970993">
130
</page>
<reference confidence="0.999716648648648">
Ted Pedersen, S Patwardhan, and J Michelizzi. 2004.
WordNet::Similarity: measuring the relatedness of
concepts. In Proceedings of the Nineteenth Na-
tional Conference on Artificial Intelligence (AAAI-
04), pages 1024–1025, San Jose, CA.
Ronald Poppe. 2010. A survey on vision-based human
action recognition. Image and Vision Computing,
28(6):976–990, June.
Deva Ramanan, David a Forsyth, and Andrew Zisser-
man. 2007. Tracking people by learning their ap-
pearance. IEEE transactions on pattern analysis
and machine intelligence, 29(1):65–81, January.
E Riloff and R Jones. 1999. Learning dictionaries for
information extraction by multi-level bootstrapping.
In Proceedings of the sixteenth national conference
on Artificial intelligence (AAAI-1999), pages 474–
479.
S. Sadanand and J. J. Corso. 2012. Action bank: A
high-level representation of activity in video. In
2012 IEEE Conference on Computer Vision and Pat-
tern Recognition, pages 1234–1241. IEEE, June.
C Schuldt, I Laptev, and B Caputo. 2004. Recognizing
human actions: a local SVM approach. In Proceed-
ings of the 17th International Conference on Pattern
Recognition, 2004. ICPR 2004., pages 32–36 Vol.3.
IEEE.
M. Sokolova and G. Lapalme. 2011. Learning opin-
ions in user-generated web content. Natural Lan-
guage Engineering, 17(04):541–567, March.
Daniel Weinland, Remi Ronfard, and Edmond Boyer.
2011. A survey of vision-based methods for action
representation, segmentation and recognition. Com-
puter Vision and Image Understanding, 115(2):224–
241, February.
Yi Yang and Deva Ramanan. 2011. Articulated pose
estimation with flexible mixtures-of-parts. In CVPR
2011, pages 1385–1392. IEEE, June.
</reference>
<page confidence="0.998309">
131
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.930910">
<title confidence="0.9825305">Extracting Latent Attributes from Video Scenes Using Text Background Knowledge</title>
<author confidence="0.977671">Anh Tran</author>
<author confidence="0.977671">Mihai Surdeanu</author>
<author confidence="0.977671">Paul</author>
<affiliation confidence="0.999826">University of Arizona</affiliation>
<email confidence="0.989127">msurdeanu,</email>
<abstract confidence="0.999479611111111">We explore the novel task of identifying latent attributes in video scenes, such as the mental states of actors, using only large text collections as background knowledge and minimal information about the videos, such as activity and actor types. We formalize the task and a measure of merit that accounts for the semantic relatedness of mental state terms. We develop and test several largely unsupervised information extraction models that identify the mental states of human participants in video scenes. We show that these models produce complementary information and their combination significantly outperforms the individual models as well as other baseline methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Abdul Rehman Abbasi</author>
<author>Matthew N Dailey</author>
<author>Nitin V Afzulpurkar</author>
<author>Takeaki Uno</author>
</authors>
<title>Student mental state inference from unintentional body gestures using dynamic Bayesian networks.</title>
<date>2009</date>
<journal>Journal on Multimodal User Interfaces,</journal>
<pages>3--1</pages>
<contexts>
<context position="4730" citStr="Abbasi et al., 2009" startWordPosition="720" endWordPosition="723">em and our solution. Mental state inference is often formulated as a classification problem, where the goal is to predict target mental state labels based on low-level sensory input data. Most solutions try to learn classification models based on large amounts of training data, while some require human engineering of domain knowledge. Hidden Markov Models (HMMs) and Dynamic Bayesian Networks (DBNs) are popular representations because they can model the temporal evolution of mental states. For instance, the mental states of students can be inferred from unintentional body gestures using a DBN (Abbasi et al., 2009). Likewise, an HMM can also be used to model the emotional states of humans (Liu and Wang, 2011). Some solutions combine HMMs and DBNs in a Bayesian inference framework to yield a multi-layer representation that can do realtime inference of complex mental and emotional states (El Kaliouby and Robinson, 2004; Baltrusaitis et al., 2011). Our work differs from these approaches in several ways: It is mostly unsupervised, multi-modal, and requires little training. Relevant video processing technology includes object detection (e.g., (Felzenszwalb et al., 2008)), person detection, and pose detection</context>
</contexts>
<marker>Abbasi, Dailey, Afzulpurkar, Uno, 2009</marker>
<rawString>Abdul Rehman Abbasi, Matthew N. Dailey, Nitin V. Afzulpurkar, and Takeaki Uno. 2009. Student mental state inference from unintentional body gestures using dynamic Bayesian networks. Journal on Multimodal User Interfaces, 3(1-2):21–31, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tadas Baltrusaitis</author>
<author>Daniel McDuff</author>
<author>Ntombikayise Banda</author>
<author>Marwa Mahmoud</author>
<author>Rana el Kaliouby</author>
<author>Peter Robinson</author>
<author>Rosalind Picard</author>
</authors>
<title>Real-time inference of mental states from facial expressions and upper body gestures.</title>
<date>2011</date>
<booktitle>In Face and Gesture</booktitle>
<pages>909--914</pages>
<publisher>IEEE,</publisher>
<marker>Baltrusaitis, McDuff, Banda, Mahmoud, el Kaliouby, Robinson, Picard, 2011</marker>
<rawString>Tadas Baltrusaitis, Daniel McDuff, Ntombikayise Banda, Marwa Mahmoud, Rana el Kaliouby, Peter Robinson, and Rosalind Picard. 2011. Real-time inference of mental states from facial expressions and upper body gestures. In Face and Gesture 2011, pages 909–914. IEEE, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
</authors>
<title>TnT: A statistical part-ofspeech tagger.</title>
<date>2000</date>
<booktitle>In Proceedings of the sixth conference on Applied natural language processing,</booktitle>
<pages>224--231</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="19629" citStr="Brants, 2000" startWordPosition="3149" endWordPosition="3150">, actor) Pˆ(m|activity, actor) = f(activity, actor) for all mental state labels m, activities, and actor types in our queries. N is the total number of tokens in the corpus. The aforementioned POS requirement is enforced: f(m) is the number of occurrences of m as an adjective or verb. We define Pˆ = 0 if the corresponding numerator and denominator are zero. The desired trigram probability is then estimated as: P (m|activity, actor) = λ1 Pˆ(m) + λ2 Pˆ(m|activity) + λ3 Pˆ(m|activity, actor) . As λ1 +λ2 +λ3 = 1, P represents a probability distribution. We use the deleted interpolation algorithm (Brants, 2000) to estimate one set of lambda values for the model, based on all trigrams. For each query tuple generated in a video, 160 different trigrams are computed, one for each mental state label in the seed set, resulting in 160 conditional probability scores. We normalize these scores into a single distribution – the mental state distribution for that query tuple. We then combine 5http://nlp.stanford.edu/software/ corenlp.shtml. all resulting distributions, one from each query tuple, and take the average to produce a single distribution over mental state labels for the video. As before, we prune thi</context>
</contexts>
<marker>Brants, 2000</marker>
<rawString>Thorsten Brants. 2000. TnT: A statistical part-ofspeech tagger. In Proceedings of the sixth conference on Applied natural language processing, pages 224–231, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ernesto Brau</author>
<author>Jinyan Guan</author>
<author>Kyle Simek</author>
<author>Luca Del Pero</author>
<author>Colin Reimer Dawson</author>
<author>Kobus Barnard</author>
</authors>
<title>Bayesian 3D Tracking from monocular video.</title>
<date>2013</date>
<booktitle>In The IEEE International Conference on Computer Vision (ICCV),</booktitle>
<contexts>
<context position="5580" citStr="Brau et al., 2013" startWordPosition="855" endWordPosition="858">rence of complex mental and emotional states (El Kaliouby and Robinson, 2004; Baltrusaitis et al., 2011). Our work differs from these approaches in several ways: It is mostly unsupervised, multi-modal, and requires little training. Relevant video processing technology includes object detection (e.g., (Felzenszwalb et al., 2008)), person detection, and pose detection (e.g., (Yang and Ramanan, 2011)). Many tracking algorithms have been developed, such as group tracking (McKenna et al., 2000), tracking by learning appearances (Ramanan et al., 2007), and tracking in 3D space (Giebel et al., 2004; Brau et al., 2013). For human action recognition, current state-of-the-art techniques are capable of achieving near perfect performance on the commonly used KTH Actions dataset (Schuldt et al., 2004) and high performance rates on other more challenging datasets (O’Hara and Draper, 2012; Sadanand and Corso, 2012). To extract mental state information from texts, one might use any or all of the technologies of natural language processing, so a complete review of relevant technologies is impossible, here. Of immediate relevance is the work of de Marneffe et al. (2010), which identified the latent meaning behind sca</context>
<context position="9460" citStr="Brau et al., 2013" startWordPosition="1483" endWordPosition="1486">pes and various other detections from a predefined list of tags. This labeling task is a proxy for a computer vision detection system that functions at a human level of performance. Indeed, we restricted the actor type labels to a set that can be reasonably expected from automatic detection algorithms: person, police officer, child, and (non-human) object. For instance, police officers often wear distinctive color uniforms that can be learned using the Felzenszwalb detector (Felzenszwalb et al., 2008), whereas children can be reliably differentiated by their heights under a 3D-tracking model (Brau et al., 2013). Each video was annotated by three different workers and the union of their annotations is produced. The overall accuracy of the annotation was excellent. The MTurk workers correctly identified the important actors in every video. Next, we collected a gold standard list of mental state labels for each video by asking MTurk workers to identify all applicable mental state adjectives for the actors involved. We used a text-box to allow for free-form input. Studies have shown that people of different cultures can perceive emotions very differently, and having forced choice options cannot always c</context>
</contexts>
<marker>Brau, Guan, Simek, Pero, Dawson, Barnard, 2013</marker>
<rawString>Ernesto Brau, Jinyan Guan, Kyle Simek, Luca Del Pero, Colin Reimer Dawson, and Kobus Barnard. 2013. Bayesian 3D Tracking from monocular video. In The IEEE International Conference on Computer Vision (ICCV), December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th annual meeting on Association for Computational Linguistics -,</booktitle>
<pages>16--23</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="16808" citStr="Collins, 1997" startWordPosition="2672" endWordPosition="2673">res for each context tuple. The distributions are then integrated into a single distribution representative of the complete activity as follows: (a) the distributions at each context back-off level are averaged to generate a single distribution per level – for the second level (which includes activity and actor types), it means distributions for all (activity, actor) tuples are averaged, whereas the first level only has a single distribution from the singleton activity tuple (chase); and (b) distributions for the different levels are linearly interpolated, similar to the back-off strategy of (Collins, 1997). Let e1 and e2 represent the weights of some mental state label m from the average distribution at the first and second level, respectively. Then the interpolated distribution score e for m is: e = λe1 + (1 − λ)e2 . Compiling the distribution scores for each m produces the final distribution representing the activity modeled. We prune this final distribution by taking the top ranked items that make up some γ proportion of the distribution. We delay the discussion of how γ is tuned to Section 6. The final pruned distribution is normalized to produce the response distribution. 4.2 Sentence Co-o</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proceedings of the 35th annual meeting on Association for Computational Linguistics -, pages 16–23, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R El Kaliouby</author>
<author>P Robinson</author>
</authors>
<title>Real-Time Inference of Complex Mental States from Facial Expressions and Head Gestures.</title>
<date>2004</date>
<booktitle>In 2004 Conference on Computer Vision and Pattern Recognition Workshop,</booktitle>
<pages>154--154</pages>
<publisher>IEEE.</publisher>
<marker>El Kaliouby, Robinson, 2004</marker>
<rawString>R. El Kaliouby and P. Robinson. 2004. Real-Time Inference of Complex Mental States from Facial Expressions and Head Gestures. In 2004 Conference on Computer Vision and Pattern Recognition Workshop, pages 154–154. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pedro Felzenszwalb</author>
<author>David McAllester</author>
<author>Deva Ramanan</author>
</authors>
<title>A discriminatively trained, multiscale, deformable part model.</title>
<date>2008</date>
<booktitle>In 2008 IEEE Conference on Computer Vision and Pattern Recognition,</booktitle>
<pages>1--8</pages>
<publisher>IEEE,</publisher>
<contexts>
<context position="5291" citStr="Felzenszwalb et al., 2008" startWordPosition="807" endWordPosition="810">rom unintentional body gestures using a DBN (Abbasi et al., 2009). Likewise, an HMM can also be used to model the emotional states of humans (Liu and Wang, 2011). Some solutions combine HMMs and DBNs in a Bayesian inference framework to yield a multi-layer representation that can do realtime inference of complex mental and emotional states (El Kaliouby and Robinson, 2004; Baltrusaitis et al., 2011). Our work differs from these approaches in several ways: It is mostly unsupervised, multi-modal, and requires little training. Relevant video processing technology includes object detection (e.g., (Felzenszwalb et al., 2008)), person detection, and pose detection (e.g., (Yang and Ramanan, 2011)). Many tracking algorithms have been developed, such as group tracking (McKenna et al., 2000), tracking by learning appearances (Ramanan et al., 2007), and tracking in 3D space (Giebel et al., 2004; Brau et al., 2013). For human action recognition, current state-of-the-art techniques are capable of achieving near perfect performance on the commonly used KTH Actions dataset (Schuldt et al., 2004) and high performance rates on other more challenging datasets (O’Hara and Draper, 2012; Sadanand and Corso, 2012). To extract men</context>
<context position="9348" citStr="Felzenszwalb et al., 2008" startWordPosition="1465" endWordPosition="1468"> only a few incomplete submissions were rejected. In the first experiment, we asked MTurk workers to select the actor types and various other detections from a predefined list of tags. This labeling task is a proxy for a computer vision detection system that functions at a human level of performance. Indeed, we restricted the actor type labels to a set that can be reasonably expected from automatic detection algorithms: person, police officer, child, and (non-human) object. For instance, police officers often wear distinctive color uniforms that can be learned using the Felzenszwalb detector (Felzenszwalb et al., 2008), whereas children can be reliably differentiated by their heights under a 3D-tracking model (Brau et al., 2013). Each video was annotated by three different workers and the union of their annotations is produced. The overall accuracy of the annotation was excellent. The MTurk workers correctly identified the important actors in every video. Next, we collected a gold standard list of mental state labels for each video by asking MTurk workers to identify all applicable mental state adjectives for the actors involved. We used a text-box to allow for free-form input. Studies have shown that peopl</context>
</contexts>
<marker>Felzenszwalb, McAllester, Ramanan, 2008</marker>
<rawString>Pedro Felzenszwalb, David McAllester, and Deva Ramanan. 2008. A discriminatively trained, multiscale, deformable part model. In 2008 IEEE Conference on Computer Vision and Pattern Recognition, pages 1–8. IEEE, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan Gabbard</author>
<author>Marjorie Freedman</author>
<author>RM Weischedel</author>
</authors>
<title>Coreference for learning to extract relations: yes, Virginia, coreference matters.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association</booktitle>
<pages>288--293</pages>
<contexts>
<context position="7033" citStr="Gabbard et al. (2011)" startWordPosition="1084" endWordPosition="1087">ids,” in web documents. Mohtarami et al. (2011) tried to infer yes/no answers from indirect yes/no question-answer pairs (IQAPs) by predicting the uncertainty of sentiment adjectives in indirect answers. Their method employs antonyms, synonyms, word sense disambiguation as well as the semantic association between the sentiment adjectives that appear in the IQAP to assign a degree of certainty to each answer. Sokolova and Lapalme (2011) further showed how to learn a model for predicting the opinions of users based on their written contents, such as reviews and product descriptions, on the Web. Gabbard et al. (2011) found that coreference resolution can significantly improve the recall rate of relations extraction without much expense to the precision rate. Our work builds on these efforts by combining information retrieval, lexical semantics, and event extraction to extract latent scene attributes. 3 Data For the experiments in this paper, we focus solely on videos containing chase scenes. Chases often invoke clear mental state inferences, and depending on context can suggest very different mental state distributions for the actors involved. 3.1 Video Corpus We compiled a video dataset of 26 chase video</context>
</contexts>
<marker>Gabbard, Freedman, Weischedel, 2011</marker>
<rawString>Ryan Gabbard, Marjorie Freedman, and RM Weischedel. 2011. Coreference for learning to extract relations: yes, Virginia, coreference matters. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers -Volume 2, pages 288–293.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Gendron</author>
<author>Debi Roberson</author>
<author>Jacoba Marieta van der Vyver</author>
<author>Lisa Feldman Barrett</author>
</authors>
<title>Cultural relativity in perceiving emotion from vocalizations.</title>
<date>2014</date>
<journal>Psychological science,</journal>
<volume>25</volume>
<issue>4</issue>
<marker>Gendron, Roberson, van der Vyver, Barrett, 2014</marker>
<rawString>Maria Gendron, Debi Roberson, Jacoba Marieta van der Vyver, and Lisa Feldman Barrett. 2014. Cultural relativity in perceiving emotion from vocalizations. Psychological science, 25(4):911–20, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Giebel</author>
<author>DM Gavrila</author>
<author>C Schn¨orr</author>
</authors>
<title>A bayesian framework for multi-cue 3d object tracking.</title>
<date>2004</date>
<booktitle>In Computer Vision-ECCV</booktitle>
<pages>241--252</pages>
<marker>Giebel, Gavrila, Schn¨orr, 2004</marker>
<rawString>J Giebel, DM Gavrila, and C Schn¨orr. 2004. A bayesian framework for multi-cue 3d object tracking. In Computer Vision-ECCV 2004, pages 241– 252.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graeme Hirst</author>
<author>D St-Onge</author>
</authors>
<title>Lexical chains as representations of context for the detection and correction of malapropisms.</title>
<date>1998</date>
<booktitle>In Christiane Fellbaum, editor, WordNet: An Electronic Lexical Database (Language, Speech, and Communication),</booktitle>
<pages>305--332</pages>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="28281" citStr="Hirst and St-Onge (1998)" startWordPosition="4630" endWordPosition="4633">lows: precision = 1 � recall = |G| g∈G where σ ∈ [0, 1] is a function that yields the similarity between two elements. The standard F1 has: � 1 , if r = g σ(r, g) = 0 , otherwise , but clearly σ can be defined to take values proportional to the similarity of r and g. We can choose from a wide range of semantic similarity and relatedness measures that are based on WordNet (Pedersen et al., 2004). The recent RNNLM of Mikolov opens the door to even more similarity measures based on vector space representations of words (Mikolov et al., 2013). After experimentations, we settled on one proposed by Hirst and St-Onge (1998). It represents two lexicalized concepts as semantically close if their WordNet synsets are connected by a path that is not too long and that “does not change direction too often” (Hirst and St-Onge, 1998). We chose this metric because it has a finite range, accommodates numerous POS pairs, and works well in practice. Given the generalized precision and recall formulas in Eq 3, our similarity-aligned (SA) F1 score can be computed in the usual way, as the harmonic mean of precision and recall (Eq 2). SA-F1 is inspired by the Constrained EntityAligned F-Measure (CEAF) metric proposed F1 SA-F1 CW</context>
</contexts>
<marker>Hirst, St-Onge, 1998</marker>
<rawString>Graeme Hirst and D St-Onge. 1998. Lexical chains as representations of context for the detection and correction of malapropisms. In Christiane Fellbaum, editor, WordNet: An Electronic Lexical Database (Language, Speech, and Communication), pages 305–332. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>LJ Li</author>
<author>Hao Su</author>
<author>L Fei-Fei</author>
<author>EP Xing</author>
</authors>
<title>Object bank: A high-level image representation for scene classification &amp; semantic feature sparsification.</title>
<date>2010</date>
<booktitle>In Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="1112" citStr="Li et al., 2010" startWordPosition="161" endWordPosition="164">counts for the semantic relatedness of mental state terms. We develop and test several largely unsupervised information extraction models that identify the mental states of human participants in video scenes. We show that these models produce complementary information and their combination significantly outperforms the individual models as well as other baseline methods. 1 Introduction “Labeling a narrowly avoided vehicular manslaughter as approach(car, person) is missing something.”1 The recognition of activities, participants, and objects in videos has advanced considerably in recent years (Li et al., 2010; Poppe, 2010; Weinland et al., 2011; Yang and Ramanan, 2011; Ng et al., 2012). However, identifying latent attributes of scenes, such as the mental states of human participants, has not been addressed. Latent attributes matter: If a video surveillance system detects one person chasing another, the response from law enforcement should be radically different if the people are happy (e.g., children playing) or afraid and angry (e.g., a person running from an assailant). This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are a</context>
</contexts>
<marker>Li, Su, Fei-Fei, Xing, 2010</marker>
<rawString>LJ Li, Hao Su, L Fei-Fei, and EP Xing. 2010. Object bank: A high-level image representation for scene classification &amp; semantic feature sparsification. In Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhilei Liu</author>
<author>Shangfei Wang</author>
</authors>
<title>Emotion recognition using hidden Markov models from facial temperature sequence.</title>
<date>2011</date>
<booktitle>In ACII’11 Proceedings of the 4th international conference on Affective computing and intelligent interaction - Volume Part II,</booktitle>
<pages>240--247</pages>
<contexts>
<context position="4826" citStr="Liu and Wang, 2011" startWordPosition="738" endWordPosition="741">e the goal is to predict target mental state labels based on low-level sensory input data. Most solutions try to learn classification models based on large amounts of training data, while some require human engineering of domain knowledge. Hidden Markov Models (HMMs) and Dynamic Bayesian Networks (DBNs) are popular representations because they can model the temporal evolution of mental states. For instance, the mental states of students can be inferred from unintentional body gestures using a DBN (Abbasi et al., 2009). Likewise, an HMM can also be used to model the emotional states of humans (Liu and Wang, 2011). Some solutions combine HMMs and DBNs in a Bayesian inference framework to yield a multi-layer representation that can do realtime inference of complex mental and emotional states (El Kaliouby and Robinson, 2004; Baltrusaitis et al., 2011). Our work differs from these approaches in several ways: It is mostly unsupervised, multi-modal, and requires little training. Relevant video processing technology includes object detection (e.g., (Felzenszwalb et al., 2008)), person detection, and pose detection (e.g., (Yang and Ramanan, 2011)). Many tracking algorithms have been developed, such as group t</context>
</contexts>
<marker>Liu, Wang, 2011</marker>
<rawString>Zhilei Liu and Shangfei Wang. 2011. Emotion recognition using hidden Markov models from facial temperature sequence. In ACII’11 Proceedings of the 4th international conference on Affective computing and intelligent interaction - Volume Part II, pages 240–247.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Luo</author>
</authors>
<title>On coreference resolution performance metrics.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing - HLT ’05,</booktitle>
<pages>25--32</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="29279" citStr="Luo, 2005" startWordPosition="4830" endWordPosition="4831">ilarity-aligned (SA) F1 score can be computed in the usual way, as the harmonic mean of precision and recall (Eq 2). SA-F1 is inspired by the Constrained EntityAligned F-Measure (CEAF) metric proposed F1 SA-F1 CWSA-F1 p r f1 p r f1 p r f1 R1 0 0 0 1 .5 23 1 .8 .89 R2 1 1 1 1 1 1 .4 .4 .4 R3 13 .5 .4 1 1 1 1 1 1 Table 3: The precision (p), recall (r), and F1 (f1) scores under various evaluation models are presented for the examples from Table 2. Suppose that σ(irate, angry) = σ(irate, mad) = σ(afraid, scared) = 1, with σ of any two identical strings being 1, and σ of all other pairs are 0. by (Luo, 2005) for coreference resolution. CEAF computes an optimal one-to-one mapping between subsets of reference and system entities before it computes recall, precision and F. Similarly, SA-F1 finds optimal mappings between the labels of the two sets based on σ (this is what the max terms in Eq 3 do). Table 3 shows that SA-F1 correctly rewards the use of synonyms. The high scores given to R2, however, indicate that it does not measure the similarity between distribution shapes. 5.3 Constrained Weighted Similarity-Aligned Fl Score Let R(r) and G(r) be the probabilities of label r in the R and G distribut</context>
</contexts>
<marker>Luo, 2005</marker>
<rawString>Xiaoqiang Luo. 2005. On coreference resolution performance metrics. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing - HLT ’05, pages 25–32, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>MC De Marneffe</author>
<author>CD Manning</author>
<author>Christopher Potts</author>
</authors>
<title>Was it good? It was provocative.” Learning the meaning of scalar adjectives.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>167--176</pages>
<marker>De Marneffe, Manning, Potts, 2010</marker>
<rawString>MC De Marneffe, CD Manning, and Christopher Potts. 2010. ”Was it good? It was provocative.” Learning the meaning of scalar adjectives. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 167–176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen J McKenna</author>
<author>Sumer Jabri</author>
<author>Zoran Duric</author>
<author>Azriel Rosenfeld</author>
<author>Harry Wechsler</author>
</authors>
<date>2000</date>
<journal>Tracking Groups of People. Computer Vision and Image Understanding,</journal>
<volume>80</volume>
<issue>1</issue>
<contexts>
<context position="5456" citStr="McKenna et al., 2000" startWordPosition="833" endWordPosition="836">lutions combine HMMs and DBNs in a Bayesian inference framework to yield a multi-layer representation that can do realtime inference of complex mental and emotional states (El Kaliouby and Robinson, 2004; Baltrusaitis et al., 2011). Our work differs from these approaches in several ways: It is mostly unsupervised, multi-modal, and requires little training. Relevant video processing technology includes object detection (e.g., (Felzenszwalb et al., 2008)), person detection, and pose detection (e.g., (Yang and Ramanan, 2011)). Many tracking algorithms have been developed, such as group tracking (McKenna et al., 2000), tracking by learning appearances (Ramanan et al., 2007), and tracking in 3D space (Giebel et al., 2004; Brau et al., 2013). For human action recognition, current state-of-the-art techniques are capable of achieving near perfect performance on the commonly used KTH Actions dataset (Schuldt et al., 2004) and high performance rates on other more challenging datasets (O’Hara and Draper, 2012; Sadanand and Corso, 2012). To extract mental state information from texts, one might use any or all of the technologies of natural language processing, so a complete review of relevant technologies is impos</context>
</contexts>
<marker>McKenna, Jabri, Duric, Rosenfeld, Wechsler, 2000</marker>
<rawString>Stephen J. McKenna, Sumer Jabri, Zoran Duric, Azriel Rosenfeld, and Harry Wechsler. 2000. Tracking Groups of People. Computer Vision and Image Understanding, 80(1):42–56, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M McNair</author>
<author>M Lorr</author>
<author>L F Droppleman</author>
</authors>
<date>1971</date>
<note>Profile of Mood States (POMS).</note>
<contexts>
<context position="13279" citStr="McNair et al., 1971" startWordPosition="2117" endWordPosition="2120">dNet. For example, a chase involving a policeman would generate the following tuples: (chase, policeman) and (chase, officer). We call these query tuples because they are used to query text for sentences that – if all goes well – will contain relevant mental state labels. Given query tuples, our models use an initial seed set of 160 mental state adjectives to produce a single distribution over mental state labels, referred to as the response distribution, for each video. The seed set is compiled from popular mental and emotional state dictionaries, including the Profile of Mood States (POMS) (McNair et al., 1971) and Plutchik’s wheel of emotion. We 2All videos and annotations are available at: http://trananh.github.io/vlsa 3Linguistics Data Consortium catalog no. LDC2011T07 4Apache Lucene: http://lucene.apache.org 123 Source Example Mental State Labels POMS alert, annoyed, energetic, exhausted, helpful, sad, terrified, unworthy, weary, etc. Plutchik angry, disgusted, fearful, joyful/joyous, sad, surprised, trusting, etc. agitated, competitive, cynical, disappointed, Others excited, giddy, happy, inebriated, violent, etc. Table 1: The initial seed set contains 160 mental state labels, compiled from dif</context>
</contexts>
<marker>McNair, Lorr, Droppleman, 1971</marker>
<rawString>D M McNair, M Lorr, and L F Droppleman. 1971. Profile of Mood States (POMS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781,</title>
<date>2013</date>
<pages>1--12</pages>
<contexts>
<context position="14366" citStr="Mikolov et al. (2013)" startWordPosition="2272" endWordPosition="2275">ers excited, giddy, happy, inebriated, violent, etc. Table 1: The initial seed set contains 160 mental state labels, compiled from different sources like the popular Profile of Mood States dictionary and Plutchik’s wheel of emotion. also included frequently used labels gathered from synsets found in WordNet (see Table 1 for examples). Note that the gold standard annotations produced by MTurk workers (Sec. 3) was not a source for this set, nor was it restricted to these terms. 4.1 Back-off Interpolation in Vector Space Our first model uses the recurrent neural network language model (RNNLM) of Mikolov et al. (2013) to project both mental state labels and query tuples into a latent conceptual space. Similarity is then trivially computed as the cosine similarity between these vectors. In all of our experiments, we used a RNNLM computed over the Gigaword corpus with 600-dimensional vectors. For this vector space (vec) model, we separate the query tuples into different levels of back-off context. The first level includes the set of activity types as singleton context tuples, e.g., (chase), while the second level includes all (activity, actor) context tuples. Hence, each query tuple will yield two different </context>
<context position="28201" citStr="Mikolov et al., 2013" startWordPosition="4617" endWordPosition="4620">ibution elements by generalizing the formulas for precision and recall as follows: precision = 1 � recall = |G| g∈G where σ ∈ [0, 1] is a function that yields the similarity between two elements. The standard F1 has: � 1 , if r = g σ(r, g) = 0 , otherwise , but clearly σ can be defined to take values proportional to the similarity of r and g. We can choose from a wide range of semantic similarity and relatedness measures that are based on WordNet (Pedersen et al., 2004). The recent RNNLM of Mikolov opens the door to even more similarity measures based on vector space representations of words (Mikolov et al., 2013). After experimentations, we settled on one proposed by Hirst and St-Onge (1998). It represents two lexicalized concepts as semantically close if their WordNet synsets are connected by a path that is not too long and that “does not change direction too often” (Hirst and St-Onge, 1998). We chose this metric because it has a finite range, accommodates numerous POS pairs, and works well in practice. Given the generalized precision and recall formulas in Eq 3, our similarity-aligned (SA) F1 score can be computed in the usual way, as the harmonic mean of precision and recall (Eq 2). SA-F1 is inspir</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, pages 1–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>WordNet: a lexical database for English.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="12469" citStr="Miller, 1995" startWordPosition="1982" endWordPosition="1983">ighborhood paradigm, that is, the hypothesis that relevant mental state labels will appear “near” text cued by the visual features of a scene. The models take as input the context extracted from a video scene, defined simply as a list of “activity and actor-type” tuples (e.g., (chase, police)). Multiple actor types will result in multiple tuples for a video. The actors can be either a person, a policeman, a child, or a (non-human) object. If the detections describe the actor as both a person and a child, or a person and a policeman, we automatically remove the person label as it is a WordNet (Miller, 1995) hypernym of both child and policeman. For each human actor type, we further increase our coverage by retrieving the synonym set (synset) of its most frequent sense (i.e., sense #1) from WordNet. For example, a chase involving a policeman would generate the following tuples: (chase, policeman) and (chase, officer). We call these query tuples because they are used to query text for sentences that – if all goes well – will contain relevant mental state labels. Given query tuples, our models use an initial seed set of 160 mental state adjectives to produce a single distribution over mental state </context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. WordNet: a lexical database for English. Communications of the ACM, 38(11):39–41, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitra Mohtarami</author>
<author>Hadi Amiri</author>
<author>Man Lan</author>
<author>Chew Lim Tan</author>
</authors>
<title>Predicting the uncertainty of sentiment adjectives in indirect answers.</title>
<date>2011</date>
<booktitle>In Proceedings of the 20th ACM international conference on Information and knowledge management - CIKM ’11,</booktitle>
<pages>2485</pages>
<publisher>ACM Press.</publisher>
<location>New York, New York, USA.</location>
<contexts>
<context position="6459" citStr="Mohtarami et al. (2011)" startWordPosition="991" endWordPosition="994">Draper, 2012; Sadanand and Corso, 2012). To extract mental state information from texts, one might use any or all of the technologies of natural language processing, so a complete review of relevant technologies is impossible, here. Of immediate relevance is the work of de Marneffe et al. (2010), which identified the latent meaning behind scalar adjectives (e.g., which ages people have in mind when talking about “little kids”). The authors learned these meanings by extracting scalars, such as children’s ages, that were commonly collocated with phrases, such as “little kids,” in web documents. Mohtarami et al. (2011) tried to infer yes/no answers from indirect yes/no question-answer pairs (IQAPs) by predicting the uncertainty of sentiment adjectives in indirect answers. Their method employs antonyms, synonyms, word sense disambiguation as well as the semantic association between the sentiment adjectives that appear in the IQAP to assign a degree of certainty to each answer. Sokolova and Lapalme (2011) further showed how to learn a model for predicting the opinions of users based on their written contents, such as reviews and product descriptions, on the Web. Gabbard et al. (2011) found that coreference re</context>
</contexts>
<marker>Mohtarami, Amiri, Lan, Tan, 2011</marker>
<rawString>Mitra Mohtarami, Hadi Amiri, Man Lan, and Chew Lim Tan. 2011. Predicting the uncertainty of sentiment adjectives in indirect answers. In Proceedings of the 20th ACM international conference on Information and knowledge management - CIKM ’11, page 2485, New York, New York, USA. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>CB Ng</author>
<author>YH Tay</author>
<author>BM Goi</author>
</authors>
<title>Recognizing human gender in computer vision: a survey. PRICAI 2012: Trends in</title>
<date>2012</date>
<journal>Artificial Intelligence,</journal>
<volume>7458</volume>
<pages>346</pages>
<contexts>
<context position="1190" citStr="Ng et al., 2012" startWordPosition="175" endWordPosition="178">t several largely unsupervised information extraction models that identify the mental states of human participants in video scenes. We show that these models produce complementary information and their combination significantly outperforms the individual models as well as other baseline methods. 1 Introduction “Labeling a narrowly avoided vehicular manslaughter as approach(car, person) is missing something.”1 The recognition of activities, participants, and objects in videos has advanced considerably in recent years (Li et al., 2010; Poppe, 2010; Weinland et al., 2011; Yang and Ramanan, 2011; Ng et al., 2012). However, identifying latent attributes of scenes, such as the mental states of human participants, has not been addressed. Latent attributes matter: If a video surveillance system detects one person chasing another, the response from law enforcement should be radically different if the people are happy (e.g., children playing) or afraid and angry (e.g., a person running from an assailant). This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http: //creativecommons.org/licenses/</context>
</contexts>
<marker>Ng, Tay, Goi, 2012</marker>
<rawString>CB Ng, YH Tay, and BM Goi. 2012. Recognizing human gender in computer vision: a survey. PRICAI 2012: Trends in Artificial Intelligence, 7458:335– 346.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S O’Hara</author>
<author>B A Draper</author>
</authors>
<title>Scalable action recognition with a subspace forest.</title>
<date>2012</date>
<booktitle>In 2012 IEEE Conference on Computer Vision and Pattern Recognition,</booktitle>
<pages>1210--1217</pages>
<publisher>IEEE,</publisher>
<marker>O’Hara, Draper, 2012</marker>
<rawString>S O’Hara and B. A. Draper. 2012. Scalable action recognition with a subspace forest. In 2012 IEEE Conference on Computer Vision and Pattern Recognition, pages 1210–1217. IEEE, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>S Patwardhan</author>
<author>J Michelizzi</author>
</authors>
<title>WordNet::Similarity: measuring the relatedness of concepts.</title>
<date>2004</date>
<booktitle>In Proceedings of the Nineteenth National Conference on Artificial Intelligence (AAAI04),</booktitle>
<pages>1024--1025</pages>
<location>San Jose, CA.</location>
<contexts>
<context position="28054" citStr="Pedersen et al., 2004" startWordPosition="4592" endWordPosition="4595"> Score Although the standard F1 does not immediately fit our needs, it is a good starting point. We can incorporate the semantic similarity of distribution elements by generalizing the formulas for precision and recall as follows: precision = 1 � recall = |G| g∈G where σ ∈ [0, 1] is a function that yields the similarity between two elements. The standard F1 has: � 1 , if r = g σ(r, g) = 0 , otherwise , but clearly σ can be defined to take values proportional to the similarity of r and g. We can choose from a wide range of semantic similarity and relatedness measures that are based on WordNet (Pedersen et al., 2004). The recent RNNLM of Mikolov opens the door to even more similarity measures based on vector space representations of words (Mikolov et al., 2013). After experimentations, we settled on one proposed by Hirst and St-Onge (1998). It represents two lexicalized concepts as semantically close if their WordNet synsets are connected by a path that is not too long and that “does not change direction too often” (Hirst and St-Onge, 1998). We chose this metric because it has a finite range, accommodates numerous POS pairs, and works well in practice. Given the generalized precision and recall formulas i</context>
</contexts>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>Ted Pedersen, S Patwardhan, and J Michelizzi. 2004. WordNet::Similarity: measuring the relatedness of concepts. In Proceedings of the Nineteenth National Conference on Artificial Intelligence (AAAI04), pages 1024–1025, San Jose, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Poppe</author>
</authors>
<title>A survey on vision-based human action recognition.</title>
<date>2010</date>
<journal>Image and Vision Computing,</journal>
<volume>28</volume>
<issue>6</issue>
<contexts>
<context position="1125" citStr="Poppe, 2010" startWordPosition="165" endWordPosition="166">mantic relatedness of mental state terms. We develop and test several largely unsupervised information extraction models that identify the mental states of human participants in video scenes. We show that these models produce complementary information and their combination significantly outperforms the individual models as well as other baseline methods. 1 Introduction “Labeling a narrowly avoided vehicular manslaughter as approach(car, person) is missing something.”1 The recognition of activities, participants, and objects in videos has advanced considerably in recent years (Li et al., 2010; Poppe, 2010; Weinland et al., 2011; Yang and Ramanan, 2011; Ng et al., 2012). However, identifying latent attributes of scenes, such as the mental states of human participants, has not been addressed. Latent attributes matter: If a video surveillance system detects one person chasing another, the response from law enforcement should be radically different if the people are happy (e.g., children playing) or afraid and angry (e.g., a person running from an assailant). This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the o</context>
</contexts>
<marker>Poppe, 2010</marker>
<rawString>Ronald Poppe. 2010. A survey on vision-based human action recognition. Image and Vision Computing, 28(6):976–990, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deva Ramanan</author>
<author>David a Forsyth</author>
<author>Andrew Zisserman</author>
</authors>
<title>Tracking people by learning their appearance.</title>
<date>2007</date>
<booktitle>IEEE transactions on pattern analysis and machine intelligence,</booktitle>
<pages>29--1</pages>
<contexts>
<context position="5513" citStr="Ramanan et al., 2007" startWordPosition="842" endWordPosition="845">mework to yield a multi-layer representation that can do realtime inference of complex mental and emotional states (El Kaliouby and Robinson, 2004; Baltrusaitis et al., 2011). Our work differs from these approaches in several ways: It is mostly unsupervised, multi-modal, and requires little training. Relevant video processing technology includes object detection (e.g., (Felzenszwalb et al., 2008)), person detection, and pose detection (e.g., (Yang and Ramanan, 2011)). Many tracking algorithms have been developed, such as group tracking (McKenna et al., 2000), tracking by learning appearances (Ramanan et al., 2007), and tracking in 3D space (Giebel et al., 2004; Brau et al., 2013). For human action recognition, current state-of-the-art techniques are capable of achieving near perfect performance on the commonly used KTH Actions dataset (Schuldt et al., 2004) and high performance rates on other more challenging datasets (O’Hara and Draper, 2012; Sadanand and Corso, 2012). To extract mental state information from texts, one might use any or all of the technologies of natural language processing, so a complete review of relevant technologies is impossible, here. Of immediate relevance is the work of de Mar</context>
</contexts>
<marker>Ramanan, Forsyth, Zisserman, 2007</marker>
<rawString>Deva Ramanan, David a Forsyth, and Andrew Zisserman. 2007. Tracking people by learning their appearance. IEEE transactions on pattern analysis and machine intelligence, 29(1):65–81, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>R Jones</author>
</authors>
<title>Learning dictionaries for information extraction by multi-level bootstrapping.</title>
<date>1999</date>
<booktitle>In Proceedings of the sixteenth national conference on Artificial intelligence (AAAI-1999),</booktitle>
<pages>474--479</pages>
<contexts>
<context position="39078" citStr="Riloff and Jones, 1999" startWordPosition="6462" endWordPosition="6465">in the corresponding distributions. We showed that very little information from videos is needed to produce good results that significantly outperform baseline methods. In the future, we plan to add more detection types. Additional contextual information from videos (e.g., scene locations) should help improve performance, especially on tougher videos (e.g., videos involving children chases). Moreover, we believe that the initial seed set of mental state labels can be learned simultaneously with the extraction patterns of the event model using a mutual bootstrapping method, similar to that of (Riloff and Jones, 1999). Currently, our experiments assume one distribution of mental state labels for each video. They do not distinguish between the mental states of the chaser and chasee, while in reality these participants may be in very different states of mind. Our event model is capable of making this distinction and we will test its performance on this task in the future. We also plan to test the effectiveness of our models with actual computer vision detectors. As a first approximation, we will simulate the noisy nature of detectors by degrading the quality of annotated data. Using artificial noise on groun</context>
</contexts>
<marker>Riloff, Jones, 1999</marker>
<rawString>E Riloff and R Jones. 1999. Learning dictionaries for information extraction by multi-level bootstrapping. In Proceedings of the sixteenth national conference on Artificial intelligence (AAAI-1999), pages 474– 479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sadanand</author>
<author>J J Corso</author>
</authors>
<title>Action bank: A high-level representation of activity in video.</title>
<date>2012</date>
<booktitle>In 2012 IEEE Conference on Computer Vision and Pattern Recognition,</booktitle>
<pages>1234--1241</pages>
<publisher>IEEE,</publisher>
<contexts>
<context position="5875" citStr="Sadanand and Corso, 2012" startWordPosition="898" endWordPosition="901">ection (e.g., (Felzenszwalb et al., 2008)), person detection, and pose detection (e.g., (Yang and Ramanan, 2011)). Many tracking algorithms have been developed, such as group tracking (McKenna et al., 2000), tracking by learning appearances (Ramanan et al., 2007), and tracking in 3D space (Giebel et al., 2004; Brau et al., 2013). For human action recognition, current state-of-the-art techniques are capable of achieving near perfect performance on the commonly used KTH Actions dataset (Schuldt et al., 2004) and high performance rates on other more challenging datasets (O’Hara and Draper, 2012; Sadanand and Corso, 2012). To extract mental state information from texts, one might use any or all of the technologies of natural language processing, so a complete review of relevant technologies is impossible, here. Of immediate relevance is the work of de Marneffe et al. (2010), which identified the latent meaning behind scalar adjectives (e.g., which ages people have in mind when talking about “little kids”). The authors learned these meanings by extracting scalars, such as children’s ages, that were commonly collocated with phrases, such as “little kids,” in web documents. Mohtarami et al. (2011) tried to infer </context>
</contexts>
<marker>Sadanand, Corso, 2012</marker>
<rawString>S. Sadanand and J. J. Corso. 2012. Action bank: A high-level representation of activity in video. In 2012 IEEE Conference on Computer Vision and Pattern Recognition, pages 1234–1241. IEEE, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Schuldt</author>
<author>I Laptev</author>
<author>B Caputo</author>
</authors>
<title>Recognizing human actions: a local SVM approach.</title>
<date>2004</date>
<booktitle>In Proceedings of the 17th International Conference on Pattern Recognition,</booktitle>
<pages>32--36</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="5761" citStr="Schuldt et al., 2004" startWordPosition="881" endWordPosition="884">upervised, multi-modal, and requires little training. Relevant video processing technology includes object detection (e.g., (Felzenszwalb et al., 2008)), person detection, and pose detection (e.g., (Yang and Ramanan, 2011)). Many tracking algorithms have been developed, such as group tracking (McKenna et al., 2000), tracking by learning appearances (Ramanan et al., 2007), and tracking in 3D space (Giebel et al., 2004; Brau et al., 2013). For human action recognition, current state-of-the-art techniques are capable of achieving near perfect performance on the commonly used KTH Actions dataset (Schuldt et al., 2004) and high performance rates on other more challenging datasets (O’Hara and Draper, 2012; Sadanand and Corso, 2012). To extract mental state information from texts, one might use any or all of the technologies of natural language processing, so a complete review of relevant technologies is impossible, here. Of immediate relevance is the work of de Marneffe et al. (2010), which identified the latent meaning behind scalar adjectives (e.g., which ages people have in mind when talking about “little kids”). The authors learned these meanings by extracting scalars, such as children’s ages, that were </context>
</contexts>
<marker>Schuldt, Laptev, Caputo, 2004</marker>
<rawString>C Schuldt, I Laptev, and B Caputo. 2004. Recognizing human actions: a local SVM approach. In Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004., pages 32–36 Vol.3. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Sokolova</author>
<author>G Lapalme</author>
</authors>
<title>Learning opinions in user-generated web content.</title>
<date>2011</date>
<journal>Natural Language Engineering,</journal>
<volume>17</volume>
<issue>04</issue>
<contexts>
<context position="6851" citStr="Sokolova and Lapalme (2011)" startWordPosition="1051" endWordPosition="1055"> in mind when talking about “little kids”). The authors learned these meanings by extracting scalars, such as children’s ages, that were commonly collocated with phrases, such as “little kids,” in web documents. Mohtarami et al. (2011) tried to infer yes/no answers from indirect yes/no question-answer pairs (IQAPs) by predicting the uncertainty of sentiment adjectives in indirect answers. Their method employs antonyms, synonyms, word sense disambiguation as well as the semantic association between the sentiment adjectives that appear in the IQAP to assign a degree of certainty to each answer. Sokolova and Lapalme (2011) further showed how to learn a model for predicting the opinions of users based on their written contents, such as reviews and product descriptions, on the Web. Gabbard et al. (2011) found that coreference resolution can significantly improve the recall rate of relations extraction without much expense to the precision rate. Our work builds on these efforts by combining information retrieval, lexical semantics, and event extraction to extract latent scene attributes. 3 Data For the experiments in this paper, we focus solely on videos containing chase scenes. Chases often invoke clear mental st</context>
</contexts>
<marker>Sokolova, Lapalme, 2011</marker>
<rawString>M. Sokolova and G. Lapalme. 2011. Learning opinions in user-generated web content. Natural Language Engineering, 17(04):541–567, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Weinland</author>
<author>Remi Ronfard</author>
<author>Edmond Boyer</author>
</authors>
<title>A survey of vision-based methods for action representation, segmentation and recognition.</title>
<date>2011</date>
<journal>Computer Vision and Image Understanding,</journal>
<volume>115</volume>
<issue>2</issue>
<pages>241</pages>
<contexts>
<context position="1148" citStr="Weinland et al., 2011" startWordPosition="167" endWordPosition="170">dness of mental state terms. We develop and test several largely unsupervised information extraction models that identify the mental states of human participants in video scenes. We show that these models produce complementary information and their combination significantly outperforms the individual models as well as other baseline methods. 1 Introduction “Labeling a narrowly avoided vehicular manslaughter as approach(car, person) is missing something.”1 The recognition of activities, participants, and objects in videos has advanced considerably in recent years (Li et al., 2010; Poppe, 2010; Weinland et al., 2011; Yang and Ramanan, 2011; Ng et al., 2012). However, identifying latent attributes of scenes, such as the mental states of human participants, has not been addressed. Latent attributes matter: If a video surveillance system detects one person chasing another, the response from law enforcement should be radically different if the people are happy (e.g., children playing) or afraid and angry (e.g., a person running from an assailant). This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License deta</context>
</contexts>
<marker>Weinland, Ronfard, Boyer, 2011</marker>
<rawString>Daniel Weinland, Remi Ronfard, and Edmond Boyer. 2011. A survey of vision-based methods for action representation, segmentation and recognition. Computer Vision and Image Understanding, 115(2):224– 241, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yi Yang</author>
<author>Deva Ramanan</author>
</authors>
<title>Articulated pose estimation with flexible mixtures-of-parts.</title>
<date>2011</date>
<booktitle>In CVPR 2011,</booktitle>
<pages>1385--1392</pages>
<publisher>IEEE,</publisher>
<contexts>
<context position="1172" citStr="Yang and Ramanan, 2011" startWordPosition="171" endWordPosition="174">erms. We develop and test several largely unsupervised information extraction models that identify the mental states of human participants in video scenes. We show that these models produce complementary information and their combination significantly outperforms the individual models as well as other baseline methods. 1 Introduction “Labeling a narrowly avoided vehicular manslaughter as approach(car, person) is missing something.”1 The recognition of activities, participants, and objects in videos has advanced considerably in recent years (Li et al., 2010; Poppe, 2010; Weinland et al., 2011; Yang and Ramanan, 2011; Ng et al., 2012). However, identifying latent attributes of scenes, such as the mental states of human participants, has not been addressed. Latent attributes matter: If a video surveillance system detects one person chasing another, the response from law enforcement should be radically different if the people are happy (e.g., children playing) or afraid and angry (e.g., a person running from an assailant). This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http: //creativecom</context>
<context position="5362" citStr="Yang and Ramanan, 2011" startWordPosition="817" endWordPosition="820">e, an HMM can also be used to model the emotional states of humans (Liu and Wang, 2011). Some solutions combine HMMs and DBNs in a Bayesian inference framework to yield a multi-layer representation that can do realtime inference of complex mental and emotional states (El Kaliouby and Robinson, 2004; Baltrusaitis et al., 2011). Our work differs from these approaches in several ways: It is mostly unsupervised, multi-modal, and requires little training. Relevant video processing technology includes object detection (e.g., (Felzenszwalb et al., 2008)), person detection, and pose detection (e.g., (Yang and Ramanan, 2011)). Many tracking algorithms have been developed, such as group tracking (McKenna et al., 2000), tracking by learning appearances (Ramanan et al., 2007), and tracking in 3D space (Giebel et al., 2004; Brau et al., 2013). For human action recognition, current state-of-the-art techniques are capable of achieving near perfect performance on the commonly used KTH Actions dataset (Schuldt et al., 2004) and high performance rates on other more challenging datasets (O’Hara and Draper, 2012; Sadanand and Corso, 2012). To extract mental state information from texts, one might use any or all of the techn</context>
</contexts>
<marker>Yang, Ramanan, 2011</marker>
<rawString>Yi Yang and Deva Ramanan. 2011. Articulated pose estimation with flexible mixtures-of-parts. In CVPR 2011, pages 1385–1392. IEEE, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>