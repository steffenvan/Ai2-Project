<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000399">
<title confidence="0.50001">
CONVEYING IMPLICIT CONTENT IN NARRATIVE SUMMARIES
</title>
<author confidence="0.676587">
Malcolm E. Cook, Wendy G. Lehnert, David D. McDonald
</author>
<affiliation confidence="0.826559666666667">
Department of Computer and Information Science
University of Massachusetts
Amherst, Massachusetts 01003
</affiliation>
<sectionHeader confidence="0.696127" genericHeader="abstract">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.9995176">
One of the key characteristics of any summary is that it
must be concise. To achieve this the content of the
summary (1) must be focused on the key events, and (2)
should leave out any information that the audience can
infer on their own. We have recently begun a project on
summarizing simple narrative stories. In our approach, we
assume that the focus of the story has already been
determined and is explicitly given in the story&apos;s long-term
representation; we concentrate instead on how one can plan
what inferences an audience will be able to make when
they read a summary. Our conclusion is that one should
think about inferences as following from the audience&apos;s
recognition of the central concepts in the story&apos;s plot, and
then plan the textual structure of the summary so as to
reinforce that recognition.
</bodyText>
<sectionHeader confidence="0.765526" genericHeader="background">
BACKGROUND
</sectionHeader>
<bodyText confidence="0.978722178571429">
This research builds on our previous work on narrative
structure and generation. We are using Plot Units [Lehnert
1981] to represent the structure of the original narrative,
and use Mumble [McDonald 1983] to do the linguistic
realization. To connect these two facilities we have a new
interface and a new text planning component named
&amp;quot;Precis&amp;quot;.
Plot units are a technique for organizing the conceptual
representation of a narrative in such a way that the
topological structure of the representation directly indicates
which events are central to the story and which are
peripheral. A graph of connected plot units is constructed
for a story as it is understood, based on the recognition of
goal-oriented behavior by the characters and their affective
reactions to events. Plot units summarize larger-scale
relationships among explicit and implicit events in the story,
and are oriented toward long term recall rather than
appreciation of story style or specific wording.
Mumble is a &amp;quot;realization&amp;quot; module for language
generation; it takes a stream of output from a text planner
and incrementally produces fluent, cohesive English text in
accordance with the planner&apos;s specifications. The planner
decides what information should be imparted and most of
its rhetorical features; Mumble filters those decisions in
accordance with grammatical constraints, handles syntax and
morphology, and performs the &amp;quot;smoothing&amp;quot; operations that
are required by the discourse context in which the
information appears.
1. This research was supported in part by the National
Science Foundation under contracts IST-8217502 and
IST-8104984, and in part by the Office of Naval Research
under contract N00014-83-K-0580.
Precis stands between the plot unit graph and Mumble.
It has been under development for a only short time and
the ultimate form that its architecture will take is not yet
fixed. We have so far been working bottom up,
experimenting with different ways to combine the partial
texts contributed by individual units and affect states, and
trying to understand the consequences of the alternatives.
We report here on one key &amp;quot;tactical&amp;quot; problem in narrative
summarization which we refer to as conceptual ellipsis,
omitting those events from a summary that we expect an
audience to be able to infer on their own, and reinforcing
that inference through a judicious choice of textual form.
DIE um fQ, CONCEPTUAL MAIMS
Ever since the original work by Bartlett, researchers have
appreciated that people who are remembering a story some
time after they have heard it typically fail to distinguish
between events that were explicitly stated in the story and
those that they only inferred while reading it. Present day
story understanding systems act in a similar way by
maintaining only a single conceptual record of what they
have understood regardless of its source [Joshi &amp;
Weischedel 1977, Graesser 1980, Dyer 1983]. Since our
summarization process starts from the conceptual
representation of the story rather than the text itself, it too
will be unable to make this distinction.
This theory of memory has two consequences. One is
that any decisions about what constituted the crux or point
of the story must have been made at comprehension time
rather than summarization time. This is one of the
purposes of a plot unit representation. The other is that
we now need to deliberately recalculate what information
should be explicit in our summary and what should be left
for the audience to infer; were this not done, the
superfluous information in the summary would make it
sound quite unnatural—as though it were being told by a
person from a different society who did not have any
commonsense understanding of the social context in which
the story was set. How the explicit versus left-to-inference
calculation turns out will vary with the summary: the same
story can be summarized or retold in different ways
depending on which character&apos;s point of view is taken or
which events are emphasized. The plot unit graph is
neutral on this question, and it will be an important part
of what we do next in this research.
Decisions about conceptual ellipsis are made prior to any
of the linguistic decisions about form; they are however
linked to those later decisions since some linguistic forms
will be more effective than others in indicating to the
audience that an inference is intended. Certain marked
choices of form will suggest to the reader that particular
implications were &amp;quot;in the mind of the writer&amp;quot; at the time
of generation. The conceptual decisions are thus the source
</bodyText>
<page confidence="0.965016">
5
</page>
<bodyText confidence="0.999980125">
of -dependencies that must be carried forward to the point
where the text-form decisions will be made in order that
the right realizations are choosen. By the same token there
will also be dependencies percolating back to the conceptual
ellipsis decisions indicating what alternative realizations are
actually available in a given case and thus whether a
particular implication can be adequately supported by the
information that is included and the way it is phrased.
</bodyText>
<sectionHeader confidence="0.931088" genericHeader="method">
AN EXAMPLE
</sectionHeader>
<bodyText confidence="0.998387">
The following simple story will demonstrate the general
phenomenon.
</bodyText>
<sectionHeader confidence="0.977755" genericHeader="method">
THE COMSYS STORY
</sectionHeader>
<construct confidence="0.858646166666667">
John and Mike were competing for the some job at
IBM. John got the Job and Mike decided to start his
own consulting firm, COMSYS. Within three years.
COMSYS was flourishing. By that time. John had
become dissatisfied with IBM so he asked Mike for a
job. Mike spitefully turned him down.
</construct>
<bodyText confidence="0.999674">
A analysis of this text in terms of plot units has
&amp;quot;Competition&amp;quot; as a central unit in the graph, which would
make it a candidate basis for a summary of the story. All
competition units have this pattern:
</bodyText>
<sectionHeader confidence="0.665615" genericHeader="method">
COMPETITION
Agentl Agent2
MI
</sectionHeader>
<bodyText confidence="0.688770294117647">
M2
Underlying this level of representation are the actual
goals and events experienced by the two charaters. In any
competition unit, we have:
MI : goal(agentl,goall)
M2 : goal(agent2,goal2)
+ : success(goall,eventl)
- : failure(goal2,event2)
with the additional constraints:
Cl : eventl = event2
C2 : goall and goal2 cannot both be realized.
(Note that in Cl the positive and negative actualization.s
are actually the same event but from the point of view of
two different characters.)
In the COMSYS story the competition is between John
and Mike over who will get a particular job at IBM. The
instantiation of the Competition unit in this story is:
</bodyText>
<equation confidence="0.266478666666667">
MI : A-goall (John has-role #employee in M-jobl
(where #employer = IBM)
M2 : A-goal2 (Mike has-role #employee in M-jobl
</equation>
<bodyText confidence="0.98186475">
where Oemployer = IBM)
+ : success(A-goall , Shire(MM,John))
- : failtue(A-goal2 , not(Shire(IBM,Mike)))
where
cl : eventl = event2 = hire(IBM,Tohn)
c2 : A-goall and A-goal2 cannot both be realized.
At the time of this writing, Precis can specify any of
the following texts for this instantiation of the Competition
unit, preferences dictated by conceptual ellipsis aside.
(Discourse fluency effects such as verb phrase deletion or
pronominalization are put in by Mumble as it is realizing
Precis&apos; specification.)
</bodyText>
<listItem confidence="0.9897468">
(a) &amp;quot;John wanted to work for IBM and so did Mike. They
hired John and did not hire Mike.&amp;quot;
(b) &amp;quot;Both John and Mike wanted to work for IBM, but
they hired John.&amp;quot;
(c) &amp;quot;Mike wanted to work for IBM, but they hired John.&amp;quot;
</listItem>
<bodyText confidence="0.999386913043479">
These three choices vary according to how much of the
content of the Competition unit they explicitly express.
Choice A includes each of the four affect states (MI, M2,
+, smoothed somewhat by the recognition that MI and
M2 share the same predicate. The very simplest choice—one
that did not express that commonality in its textual
structure, e.g. &amp;quot;John wanted to work for IBM. Mike wanted
to work for IBM. They hired John and did not hire Mike.&amp;quot;—is
completely unnatural; people wouldn&apos;t say it. This minimal
level of implicit information that the textual structure must
carry is accordingly not even made Precis responsibility, but
is instead carried out automatically within Mumble. The
alternative realization of this commonality, using a conjoined
subject rather than verb phrase deletion, is taken to be a
stylistic decision and is not deliberated over by Precis.
If we begin to include the constraints that accompany
the Competition unit (Cl and C2) explicitly in the summary
then we can leave out more of the affect states as
inferable. In choice B we make use of the first constraint,
i.e. that the positive and the negative actualizations are
consequences of the same event, to enable the omission of
event2, not(hire(MIke,IBM)), from the text of the summary
by dropping the phrase &amp;quot;they did not hire Mike&amp;quot;.
In our present version of choice B there is. no structural
indicator of the constraint. It is probably no coincidence
then that the text for B sounds a little odd—readers
unfamiliar with the original story will not really understand
what the but is supposed to be communicating until they go
further and make the deduction that there must only have
been one job available. A better version of B would
probably be: &amp;quot;Both John and Mike wanted to work for IBM,
but they gsU hired John&amp;quot;, with the only acting as an explicit
structural indicator of the information in the constraints.
This addition can probably be licensed as a consequence of
the second constraint that only one of the two goals can be
realized. At the time of this writing we do not yet have
an adequately general mechanism for making this
observation and incorporating the only, so we have not
included it among Precis&apos; choices.
It is intriguing that choice c, &amp;quot;Mike wanted to work for
IBM, but they hired John&amp;quot;, is probably the best of the three
choices even though it requires the audience to do the most
inferencing. In c we have omitted state MI—that John
wanted to work for IBM—yet the audience is able to
recover this information quite easily given the presence of
the but. Given the ease with which choice c is understood,
</bodyText>
<page confidence="0.99856">
6
</page>
<bodyText confidence="0.9970318125">
we are led to the suggestion that there may be a very
general &amp;quot;template&amp;quot; being recognized here—that choice c is
seen by an audience as an instance of the pattern:
&lt;expression of agent A&apos;s goal&gt;,
but &lt;realization of agent Be goal&gt;
and that this template always carries with it the inference
that the two goals must be incompatible and therefore As
goal has not be satisfied.
Note that here again the choice would be improved by
including an explicit lexical indication of the constraint:
&amp;quot;Mike wanted to work for IBM, but they hired John instead&amp;quot;.
We expect that most instances of these &amp;quot;rhetorical markers&amp;quot;
in texts will turn out to be indicators of constraint-level
information akin to our present cases, which raises the
intriguing possibility that a general theory of how they are
used might arise out of this kind of work in generation.
</bodyText>
<sectionHeader confidence="0.889674" genericHeader="conclusions">
gThIMARY
</sectionHeader>
<bodyText confidence="0.999415483870968">
Currently, we are working with two programs. PUGG
(Plot Unit Graph Generator) operates on an affect-state
representation of a story, and produces a graph or network
of plot units that act as pointers to the core of the
conceptual representation of the input story and organizes
how it will be &amp;quot;presented&amp;quot; to the program that plans the
text of the summary, Precis. Precis is in the early stages
of its development and so far can only use a single, core
plot unit from the graph as the basis of the summary of
the story.
Precis works at the interface between purely conceptual
and purely linguistic conciderations as it makes its planning
decisions. It chooses from a set of alternative specifications
for the summary that vary according to which of the
elements of the plot unit are included and which left to be
inferred by the audience once they recognize the story as a
case of competition. Precis can state the three alternative
choices described above (and a few other sets like them),
and Mumble can take those specifications and produce the
indicated texts. However we do not as yet have any
general mechanism for deciding which choice to prefer over
the others. Perhaps such a decision mechanism will become
apparent once these single unit summaries are embedded in
a larger context, or possibly there is no reasonable basis for
decision without more knowledge of the purpose of the
summary or the ability of a particular audience to make
these kinds of inferences (one might have to talk quite
differently to young children for example). In future work
we also hope to be able to work out a specific basis for
planning the use of inference-directing words like only or
instead.
</bodyText>
<sectionHeader confidence="0.999013" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.998966833333333">
Dyer, M. (1983) In-depth Understanding: A Computer
Model of Integrated Processing for Narrative
Comprehension, Cambridge, Mass.: MIT Press. •
Graesser, All c. (1981) Prose Comprehension Beyond the
Word New York, N.Y.: Springer-Verlag.
Joshi, AX., and Weischedel (1977) Computation of a
subclass of inferences: Presupposition and Entailment, in
Am J. of Comp. Linguistics
Lehnert, W. (1982) Plot Units: A Narrative Summarization
Strategy, in Lehnert, W. and Ringle, M. (Eds.), Strategies
for Natural Language Processing, Hillsdale, NJ.:
Lawrence Erlbaum Associates.
Lehnert, W. (1983) &amp;quot;Narrative Complexity Based on
Summarization Algorithms,&amp;quot; Proceedings of the Seventh
International Joint Conference on Artifical Intelligence,
Karlsruhe, Germany.
McDonald, D. (1983) &amp;quot;National Language Generation as a
Computational Problem - an Introduction&amp;quot; in Brady, M.
and Berwick, R. (Eds.) Computational Models of
Discourse, Cambridge, Mass.: MIT Press.
McDonald, D. (1982) &amp;quot;Description Directed Control: its
Implications for Natural Language Generation&amp;quot;, in
Cercone (ed.) Computational Linguistics, Dublin:
Pergamon Press.
</reference>
<page confidence="0.999518">
7
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000584">
<title confidence="0.999048">CONVEYING IMPLICIT CONTENT IN NARRATIVE SUMMARIES</title>
<author confidence="0.999992">Malcolm E Cook</author>
<author confidence="0.999992">Wendy G Lehnert</author>
<author confidence="0.999992">David D McDonald</author>
<affiliation confidence="0.999987">Department of Computer and Information Science University of Massachusetts</affiliation>
<address confidence="0.999919">Amherst, Massachusetts 01003</address>
<abstract confidence="0.999697288888889">One of the key characteristics of any summary is that it must be concise. To achieve this the content of the summary (1) must be focused on the key events, and (2) should leave out any information that the audience can infer on their own. We have recently begun a project on summarizing simple narrative stories. In our approach, we assume that the focus of the story has already been determined and is explicitly given in the story&apos;s long-term representation; we concentrate instead on how one can plan what inferences an audience will be able to make when they read a summary. Our conclusion is that one should think about inferences as following from the audience&apos;s recognition of the central concepts in the story&apos;s plot, and then plan the textual structure of the summary so as to reinforce that recognition. BACKGROUND This research builds on our previous work on narrative structure and generation. We are using Plot Units [Lehnert 1981] to represent the structure of the original narrative, and use Mumble [McDonald 1983] to do the linguistic realization. To connect these two facilities we have a new interface and a new text planning component named &amp;quot;Precis&amp;quot;. Plot units are a technique for organizing the conceptual representation of a narrative in such a way that the topological structure of the representation directly indicates which events are central to the story and which are peripheral. A graph of connected plot units is constructed for a story as it is understood, based on the recognition of goal-oriented behavior by the characters and their affective reactions to events. Plot units summarize larger-scale relationships among explicit and implicit events in the story, and are oriented toward long term recall rather than appreciation of story style or specific wording. Mumble is a &amp;quot;realization&amp;quot; module for language generation; it takes a stream of output from a text planner and incrementally produces fluent, cohesive English text in accordance with the planner&apos;s specifications. The planner decides what information should be imparted and most of its rhetorical features; Mumble filters those decisions in accordance with grammatical constraints, handles syntax and morphology, and performs the &amp;quot;smoothing&amp;quot; operations that are required by the discourse context in which the information appears.</abstract>
<note confidence="0.6573">1. This research was supported in part by the National Science Foundation under contracts IST-8217502 and IST-8104984, and in part by the Office of Naval Research under contract N00014-83-K-0580. Precis stands between the plot unit graph and Mumble.</note>
<abstract confidence="0.976316956730769">It has been under development for a only short time and the ultimate form that its architecture will take is not yet fixed. We have so far been working bottom up, experimenting with different ways to combine the partial texts contributed by individual units and affect states, and trying to understand the consequences of the alternatives. We report here on one key &amp;quot;tactical&amp;quot; problem in narrative which we refer to as ellipsis, omitting those events from a summary that we expect an audience to be able to infer on their own, and reinforcing that inference through a judicious choice of textual form. um CONCEPTUALMAIMS Ever since the original work by Bartlett, researchers have appreciated that people who are remembering a story some time after they have heard it typically fail to distinguish between events that were explicitly stated in the story and those that they only inferred while reading it. Present day story understanding systems act in a similar way by maintaining only a single conceptual record of what they have understood regardless of its source [Joshi &amp; Weischedel 1977, Graesser 1980, Dyer 1983]. Since our summarization process starts from the conceptual representation of the story rather than the text itself, it too will be unable to make this distinction. This theory of memory has two consequences. One is that any decisions about what constituted the crux or point of the story must have been made at comprehension time rather than summarization time. This is one of the purposes of a plot unit representation. The other is that we now need to deliberately recalculate what information should be explicit in our summary and what should be left for the audience to infer; were this not done, the superfluous information in the summary would make it sound quite unnatural—as though it were being told by a person from a different society who did not have any commonsense understanding of the social context in which the story was set. How the explicit versus left-to-inference calculation turns out will vary with the summary: the same can or retold in different ways depending on which character&apos;s point of view is taken or which events are emphasized. The plot unit graph is neutral on this question, and it will be an important part of what we do next in this research. Decisions about conceptual ellipsis are made prior to any of the linguistic decisions about form; they are however linked to those later decisions since some linguistic forms will be more effective than others in indicating to the audience that an inference is intended. Certain marked choices of form will suggest to the reader that particular implications were &amp;quot;in the mind of the writer&amp;quot; at the time of generation. The conceptual decisions are thus the source 5 that must be carried forward to the point where the text-form decisions will be made in order that the right realizations are choosen. By the same token there will also be dependencies percolating back to the conceptual ellipsis decisions indicating what alternative realizations are actually available in a given case and thus whether a particular implication can be adequately supported by the information that is included and the way it is phrased. The following simple story will demonstrate the general phenomenon. THE COMSYS STORY John and Mike were competing for the some job at IBM. John got the Job and Mike decided to start his own consulting firm, COMSYS. Within three years. COMSYS was flourishing. By that time. John had become dissatisfied with IBM so he asked Mike for a job. Mike spitefully turned him down. A analysis of this text in terms of plot units has &amp;quot;Competition&amp;quot; as a central unit in the graph, which would make it a candidate basis for a summary of the story. All competition units have this pattern: COMPETITION Agentl Agent2 MI M2 Underlying this level of representation are the actual goals and events experienced by the two charaters. In any competition unit, we have: MI : goal(agentl,goall) M2 : goal(agent2,goal2) + : success(goall,eventl) - : failure(goal2,event2) with the additional constraints: Cl : eventl = event2 C2 : goall and goal2 cannot both be realized. (Note that in Cl the positive and negative actualization.s are actually the same event but from the point of view of two different characters.) In the COMSYS story the competition is between John and Mike over who will get a particular job at IBM. The instantiation of the Competition unit in this story is: MI : A-goall (John has-role #employee in M-jobl (where #employer = IBM) M2 : A-goal2 (Mike has-role #employee in M-jobl where Oemployer = IBM) + : success(A-goall , Shire(MM,John)) - : failtue(A-goal2 , not(Shire(IBM,Mike))) where cl : eventl = event2 = hire(IBM,Tohn) c2 : A-goall and A-goal2 cannot both be realized. At the time of this writing, Precis can specify any of the following texts for this instantiation of the Competition unit, preferences dictated by conceptual ellipsis aside. (Discourse fluency effects such as verb phrase deletion or pronominalization are put in by Mumble as it is realizing Precis&apos; specification.) (a) &amp;quot;John wanted to work for IBM and so did Mike. They hired John and did not hire Mike.&amp;quot; (b) &amp;quot;Both John and Mike wanted to work for IBM, but they hired John.&amp;quot; (c) &amp;quot;Mike wanted to work for IBM, but they hired John.&amp;quot; These three choices vary according to how much of the content of the Competition unit they explicitly express. Choice A includes each of the four affect states (MI, M2, +, smoothed somewhat by the recognition that MI and M2 share the same predicate. The very simplest choice—one that did not express that commonality in its textual e.g. wanted to work for IBM. Mike wanted to work for IBM. They hired John and did not hire Mike.&amp;quot;—is completely unnatural; people wouldn&apos;t say it. This minimal level of implicit information that the textual structure must carry is accordingly not even made Precis responsibility, but is instead carried out automatically within Mumble. The alternative realization of this commonality, using a conjoined subject rather than verb phrase deletion, is taken to be a stylistic decision and is not deliberated over by Precis. If we begin to include the constraints that accompany the Competition unit (Cl and C2) explicitly in the summary then we can leave out more of the affect states as inferable. In choice B we make use of the first constraint, i.e. that the positive and the negative actualizations are consequences of the same event, to enable the omission of event2, not(hire(MIke,IBM)), from the text of the summary dropping the phrase did not hire Mike&amp;quot;. In our present version of choice B there is. no structural indicator of the constraint. It is probably no coincidence then that the text for B sounds a little odd—readers unfamiliar with the original story will not really understand but supposed to be communicating until they go further and make the deduction that there must only have been one job available. A better version of B would be: John and Mike wanted to work for IBM, they gsU hired John&amp;quot;, with the only as an explicit structural indicator of the information in the constraints. This addition can probably be licensed as a consequence of the second constraint that only one of the two goals can be realized. At the time of this writing we do not yet have an adequately general mechanism for making this and incorporating the we have not included it among Precis&apos; choices. is intriguing that choice c, wanted to work for but they hired John&amp;quot;, probably the best of the three choices even though it requires the audience to do the most inferencing. In c we have omitted state MI—that John wanted to work for IBM—yet the audience is able to recover this information quite easily given the presence of the ease with which choice c is understood, 6 we are led to the suggestion that there may be a very general &amp;quot;template&amp;quot; being recognized here—that choice c is seen by an audience as an instance of the pattern: &lt;expression of agent A&apos;s goal&gt;, of agent Be goal&gt; that this template alwayscarries with it the inference that the two goals must be incompatible and therefore As goal has not be satisfied. Note that here again the choice would be improved by including an explicit lexical indication of the constraint: wanted to work for IBM, but they hired John instead&amp;quot;. We expect that most instances of these &amp;quot;rhetorical markers&amp;quot; in texts will turn out to be indicators of constraint-level information akin to our present cases, which raises the intriguing possibility that a general theory of how they are used might arise out of this kind of work in generation. gThIMARY Currently, we are working with two programs. PUGG (Plot Unit Graph Generator) operates on an affect-state representation of a story, and produces a graph or network of plot units that act as pointers to the core of the conceptual representation of the input story and organizes how it will be &amp;quot;presented&amp;quot; to the program that plans the text of the summary, Precis. Precis is in the early stages of its development and so far can only use a single, core plot unit from the graph as the basis of the summary of the story. Precis works at the interface between purely conceptual and purely linguistic conciderations as it makes its planning decisions. It chooses from a set of alternative specifications for the summary that vary according to which of the elements of the plot unit are included and which left to be inferred by the audience once they recognize the story as a case of competition. Precis can state the three alternative choices described above (and a few other sets like them), and Mumble can take those specifications and produce the indicated texts. However we do not as yet have any general mechanism for deciding which choice to prefer over the others. Perhaps such a decision mechanism will become apparent once these single unit summaries are embedded in a larger context, or possibly there is no reasonable basis for decision without more knowledge of the purpose of the summary or the ability of a particular audience to make these kinds of inferences (one might have to talk quite differently to young children for example). In future work we also hope to be able to work out a specific basis for the use of inference-directing words like instead.</abstract>
<note confidence="0.465957777777778">REFERENCES Dyer, M. (1983) In-depth Understanding: A Computer Model of Integrated Processing for Narrative Comprehension, Cambridge, Mass.: MIT Press. • Graesser, All c. (1981) Prose Comprehension Beyond the Word New York, N.Y.: Springer-Verlag. Joshi, AX., and Weischedel (1977) Computation of a subclass of inferences: Presupposition and Entailment, in Am J. of Comp. Linguistics</note>
<degree confidence="0.924724333333333">Lehnert, W. (1982) Plot Units: A Narrative Summarization Strategy, in Lehnert, W. and Ringle, M. (Eds.), Strategies for Natural Language Processing, Hillsdale, NJ.:</degree>
<affiliation confidence="0.471812">Lawrence Erlbaum Associates.</affiliation>
<address confidence="0.438077">Lehnert, W. (1983) &amp;quot;Narrative Complexity Based on</address>
<author confidence="0.369354">Summarization Algorithms</author>
<author confidence="0.369354">Proceedings of the Seventh</author>
<affiliation confidence="0.942046">International Joint Conference on Artifical Intelligence,</affiliation>
<address confidence="0.8180585">Karlsruhe, Germany. McDonald, D. (1983) &amp;quot;National Language Generation as a</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Dyer</author>
</authors>
<title>In-depth Understanding: A Computer Model of Integrated Processing for Narrative Comprehension,</title>
<date>1983</date>
<publisher>MIT Press. •</publisher>
<location>Cambridge, Mass.:</location>
<contexts>
<context position="3941" citStr="Dyer 1983" startWordPosition="621" endWordPosition="622">heir own, and reinforcing that inference through a judicious choice of textual form. DIE um fQ, CONCEPTUAL MAIMS Ever since the original work by Bartlett, researchers have appreciated that people who are remembering a story some time after they have heard it typically fail to distinguish between events that were explicitly stated in the story and those that they only inferred while reading it. Present day story understanding systems act in a similar way by maintaining only a single conceptual record of what they have understood regardless of its source [Joshi &amp; Weischedel 1977, Graesser 1980, Dyer 1983]. Since our summarization process starts from the conceptual representation of the story rather than the text itself, it too will be unable to make this distinction. This theory of memory has two consequences. One is that any decisions about what constituted the crux or point of the story must have been made at comprehension time rather than summarization time. This is one of the purposes of a plot unit representation. The other is that we now need to deliberately recalculate what information should be explicit in our summary and what should be left for the audience to infer; were this not do</context>
</contexts>
<marker>Dyer, 1983</marker>
<rawString>Dyer, M. (1983) In-depth Understanding: A Computer Model of Integrated Processing for Narrative Comprehension, Cambridge, Mass.: MIT Press. •</rawString>
</citation>
<citation valid="true">
<authors>
<author>All c Graesser</author>
</authors>
<date>1981</date>
<booktitle>Prose Comprehension Beyond the Word</booktitle>
<publisher>Springer-Verlag.</publisher>
<location>New York, N.Y.:</location>
<marker>Graesser, 1981</marker>
<rawString>Graesser, All c. (1981) Prose Comprehension Beyond the Word New York, N.Y.: Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>AX Joshi</author>
<author>Weischedel</author>
</authors>
<title>Computation of a subclass of inferences: Presupposition and Entailment,</title>
<date>1977</date>
<journal>in Am J. of Comp. Linguistics</journal>
<contexts>
<context position="3915" citStr="Joshi &amp; Weischedel 1977" startWordPosition="615" endWordPosition="618">ect an audience to be able to infer on their own, and reinforcing that inference through a judicious choice of textual form. DIE um fQ, CONCEPTUAL MAIMS Ever since the original work by Bartlett, researchers have appreciated that people who are remembering a story some time after they have heard it typically fail to distinguish between events that were explicitly stated in the story and those that they only inferred while reading it. Present day story understanding systems act in a similar way by maintaining only a single conceptual record of what they have understood regardless of its source [Joshi &amp; Weischedel 1977, Graesser 1980, Dyer 1983]. Since our summarization process starts from the conceptual representation of the story rather than the text itself, it too will be unable to make this distinction. This theory of memory has two consequences. One is that any decisions about what constituted the crux or point of the story must have been made at comprehension time rather than summarization time. This is one of the purposes of a plot unit representation. The other is that we now need to deliberately recalculate what information should be explicit in our summary and what should be left for the audience </context>
</contexts>
<marker>Joshi, Weischedel, 1977</marker>
<rawString>Joshi, AX., and Weischedel (1977) Computation of a subclass of inferences: Presupposition and Entailment, in Am J. of Comp. Linguistics</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Lehnert</author>
</authors>
<title>Plot Units: A Narrative Summarization Strategy,</title>
<date>1982</date>
<note>in</note>
<marker>Lehnert, 1982</marker>
<rawString>Lehnert, W. (1982) Plot Units: A Narrative Summarization Strategy, in Lehnert, W. and Ringle, M. (Eds.), Strategies for Natural Language Processing, Hillsdale, NJ.: Lawrence Erlbaum Associates.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Lehnert</author>
</authors>
<title>Narrative Complexity Based on Summarization Algorithms,&amp;quot;</title>
<date>1983</date>
<booktitle>Proceedings of the Seventh International Joint Conference on Artifical Intelligence,</booktitle>
<location>Karlsruhe, Germany.</location>
<marker>Lehnert, 1983</marker>
<rawString>Lehnert, W. (1983) &amp;quot;Narrative Complexity Based on Summarization Algorithms,&amp;quot; Proceedings of the Seventh International Joint Conference on Artifical Intelligence, Karlsruhe, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McDonald</author>
</authors>
<title>National Language Generation as a Computational Problem - an Introduction&amp;quot;</title>
<date>1983</date>
<publisher>MIT Press.</publisher>
<location>Cambridge, Mass.:</location>
<note>in</note>
<contexts>
<context position="1244" citStr="McDonald 1983" startWordPosition="199" endWordPosition="200">iven in the story&apos;s long-term representation; we concentrate instead on how one can plan what inferences an audience will be able to make when they read a summary. Our conclusion is that one should think about inferences as following from the audience&apos;s recognition of the central concepts in the story&apos;s plot, and then plan the textual structure of the summary so as to reinforce that recognition. BACKGROUND This research builds on our previous work on narrative structure and generation. We are using Plot Units [Lehnert 1981] to represent the structure of the original narrative, and use Mumble [McDonald 1983] to do the linguistic realization. To connect these two facilities we have a new interface and a new text planning component named &amp;quot;Precis&amp;quot;. Plot units are a technique for organizing the conceptual representation of a narrative in such a way that the topological structure of the representation directly indicates which events are central to the story and which are peripheral. A graph of connected plot units is constructed for a story as it is understood, based on the recognition of goal-oriented behavior by the characters and their affective reactions to events. Plot units summarize larger-sca</context>
</contexts>
<marker>McDonald, 1983</marker>
<rawString>McDonald, D. (1983) &amp;quot;National Language Generation as a Computational Problem - an Introduction&amp;quot; in Brady, M. and Berwick, R. (Eds.) Computational Models of Discourse, Cambridge, Mass.: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McDonald</author>
</authors>
<title>Description Directed Control: its Implications for Natural Language Generation&amp;quot;,</title>
<date>1982</date>
<booktitle>Computational Linguistics,</booktitle>
<editor>in Cercone (ed.)</editor>
<publisher>Pergamon Press.</publisher>
<location>Dublin:</location>
<marker>McDonald, 1982</marker>
<rawString>McDonald, D. (1982) &amp;quot;Description Directed Control: its Implications for Natural Language Generation&amp;quot;, in Cercone (ed.) Computational Linguistics, Dublin: Pergamon Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>