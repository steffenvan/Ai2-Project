<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000467">
<title confidence="0.997463">
Minimizing Manual Annotation Cost
In Supervised Training From Corpora
</title>
<author confidence="0.968956">
Sean P. Engelson and Ido Dagan
</author>
<affiliation confidence="0.8959575">
Department of Mathematics and Computer Science
Bar-Ilan University
</affiliation>
<address confidence="0.894357">
52900 Ramat Gan, Israel
</address>
<email confidence="0.849132">
fengelson,daganlObimacs.cs.biu.ac.il
</email>
<sectionHeader confidence="0.995213" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999755703703704">
Corpus-based methods for natural lan-
guage processing often use supervised
training, requiring expensive manual an-
notation of training corpora. This paper
investigates methods for reducing annota-
tion cost by sample selection. In this ap-
proach, during training the learning pro-
gram examines many unlabeled examples
and selects for labeling (annotation) only
those that are most informative at each
stage. This avoids redundantly annotating
examples that contribute little new infor-
mation. This paper extends our previous
work on committee-based sample selection
for probabilistic classifiers. We describe
a family of methods for committee-based
sample selection, and report experimental
results for the task of stochastic part-of-
speech tagging. We find that all variants
achieve a significant reduction in annota-
tion cost, though their computational effi-
ciency differs. In particular, the simplest
method, which has no parameters to tune,
gives excellent results. We also show that
sample selection yields a significant reduc-
tion in the size of the model used by the
tagger.
</bodyText>
<sectionHeader confidence="0.999474" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999774578947369">
Many corpus-based methods for natural language
processing (NLP) are based on supervised training—
acquiring information from a manually annotated
corpus. Therefore, reducing annotation cost is an
important research goal for statistical NLP. The ul-
timate reduction in annotation cost is achieved by
unsupervised training methods, which do not require
an annotated corpus at all (Kupiec, 1992; Merialdo,
1994; Elworthy, 1994). It has been shown, how-
ever, that some supervised training prior to the un-
supervised phase is often beneficial. Indeed, fully
unsupervised training may not be feasible for cer-
tain tasks. This paper investigates an approach
for optimizing the supervised training (learning)
phase, which reduces the annotation effort required
to achieve a desired level of accuracy of the trained
model.
In this paper, we investigate and extend the
committee-based sample selection approach to min-
imizing training cost (Dagan and Engelson, 1995).
When using sample selection, a learning program ex-
amines many unlabeled (not annotated) examples,
selecting for labeling only those that are most in-
formative for the learner at each stage of training
(Seung, Opper, and Sompolinsky, 1992; Freund et
al., 1993; Lewis and Gale, 1994; Cohn, Atlas, and
Ladner, 1994). This avoids redundantly annotating
many examples that contribute roughly the same in-
formation to the learner.
Our work focuses on sample selection for training
probabilistic classifiers. In statistical NLP, prob-
abilistic classifiers are often used to select a pre-
ferred analysis of the linguistic structure of a text
(for example, its syntactic structure (Black et al.,
1993), word categories (Church, 1988), or word
senses (Gale, Church, and Yarowsky, 1993)). As a
representative task for probabilistic classification in
NLP, we experiment in this paper with sample se-
lection for the popular and well-understood method
of stochastic part-of-speech tagging using Hidden
Markov Models.
We first review the basic approach of committee-
based sample selection and its application to part-
of-speech tagging. This basic approach gives rise
to a family of algorithms (including the original al-
gorithm described in (Dagan and Engelson, 1995))
which we then describe. First, we describe the &apos;sim-
plest&apos; committee-based selection algorithm, which
has no parameters to tune. We then generalize the
selection scheme, allowing more options to adapt
and tune the approach for specific tasks. The paper
compares the performance of several instantiations
of the general scheme, including a batch selection
method similar to that of Lewis and Gale (1994).
In particular, we found that the simplest version of
the method achieves a significant reduction in an-
notation cost, comparable to that of other versions.
</bodyText>
<page confidence="0.998551">
319
</page>
<bodyText confidence="0.9998778">
We also evaluate the computational efficiency of the
different variants, and the number of unlabeled ex-
amples they consume. Finally, we study the effect
of sample selection on the size of the model acquired
by the learner.
</bodyText>
<sectionHeader confidence="0.86821" genericHeader="method">
2 Probabilistic Classification
</sectionHeader>
<bodyText confidence="0.995338390243902">
This section presents the framework and terminol-
ogy assumed for probabilistic classification, as well
as its instantiation for stochastic bigram part-of-
speech tagging.
A probabilistic classifier classifies input examples
e by classes c E C, where C is a known set of pos-
sible classes. Classification is based on a score func-
tion, Fm (c, e), which assigns a score to each possible
class of an example. The classifier then assigns the
example to the class with the highest score. Fm is
determined by a probabilistic model M. In many
applications, Fm is the conditional probability func-
tion, Pm (cle), specifying the probability of each class
given the example, but other score functions that
correlate with the likelihood of the class are often
used.
In stochastic part-of-speech tagging, the model as-
sumed is a Hidden Markov Model (HMM), and input
examples are sentences. The class c, to which a sen-
tence is assigned is a sequence of the parts of speech
(tags) for the words in the sentence. The score func-
tion is typically the joint (or conditional) probability
of the sentence and the tag sequence&apos;. The tagger
then assigns the sentence to the tag sequence which
is most probable according to the HMM.
The probabilistic model M, and thus the score
function Fm, are defined by a set of parameters,
{a}. During training, the values of the parameters
are estimated from a set of statistics, S, extracted
from a training set of annotated examples. We de-
note a particular model by M = {ai}, where each ai
is a specific value for the corresponding ai.
In bigram part-of-speech tagging the HMM model
M contains three types of parameters: transition
probabilities P(ti—t) giving the probability of tag
tj occuring after tag ti, lexical probabilities P(t1w)
giving the probability of tag t labeling word w, and
tag probabilities P(t) giving the marginal probability
of a tag occurring.2 The values of these parameters
are estimated from a tagged corpus which provides
a training set of labeled examples (see Section 4.1).
</bodyText>
<sectionHeader confidence="0.979879" genericHeader="method">
3 Evaluating Example Uncertainty
</sectionHeader>
<bodyText confidence="0.961021">
A sample selection method needs to evaluate the
expected usefulness, or information gain, of learn-
ing from a given example. The methods we investi-
</bodyText>
<footnote confidence="0.925849">
&apos;This gives the Viterbi model (Merialdo, 1994), which
we use here.
2This version of the method uses Bayes&apos; theorem
(P(wslt,) cc P(4It7;)) (Church, 1988).
</footnote>
<bodyText confidence="0.99987206779661">
gate approach this evaluation implicitly, measuring
an example&apos;s informativeness as the uncertainty in
its classification given the current training data (Se-
ung, Opper, and Sompolinsky, 1992; Lewis and Gale,
1994; MacKay, 1992). The reasoning is that if an
example&apos;s classification is uncertain given current
training data then the example is likely to contain
unknown information useful for classifying similar
examples in the future.
We investigate the committee-based method,
where the learning algorithm evaluates an example
by giving it to a committee containing several vari-
ant models, all &apos;consistent&apos; with the training data
seen so far. The more the committee members agree
on the classification of the example, the greater our
certainty in its classification. This is because when
the training data entails a specific classification with
high certainty, most (in a probabilistic sense) classi-
fiers consistent with the data will produce that clas-
sification.
The committee-based approach was first proposed
in a theoretical context for learning binary non-
probabilistic classifiers (Seung, Opper, and Som-
polinsky, 1992; Freund et al., 1993). In this pa-
per, we extend our previous work (Dagan and En-
gelson, 1995) where we applied the basic idea of the
committee-based approach to probabilistic classifi-
cation. Taking a Bayesian perspective, the posterior
probability of a model, P(MIS), is determined given
statistics S from the training set (and some prior dis-
tribution for the models). Committee members are
then generated by drawing models randomly from
POI IS). An example is selected for labeling if the
committee members largely disagree on its classifi-
cation. This procedure assumes that one can sample
from the models&apos; posterior distribution, at least ap-
proximately.
To illustrate the generation of committee-
members, consider a model containing a single bi-
nomial parameter a (the probability of a success),
with estimated value a. The statistics S for such a
model are given by N, the number of trials, and x,
the number of successes in those trials.
Given N and x, the &apos;best&apos; parameter value may
be estimated by one of several estimation methods.
For example, the maximum likelihood estimate for a
is a = k, giving the model M = {a} = {kJ. When
generating a committee of models, however, we are
not interested in the &apos;best&apos; model, but rather in sam-
pling the distribution of models given the statistics.
For our example, we need to sample the posterior
density of estimates for a, namely P(a = a I S)• Sam-
pling this distribution yields a set of estimates scat-
tered around ff (assuming a uniform prior), whose
variance decreases as N increases. In other words,
the more statistics there are for estimating the pa-
rameter, the more similar are the parameter values
used by different committee members.
For models with multiple parameters, parame-
</bodyText>
<page confidence="0.995067">
320
</page>
<bodyText confidence="0.999963142857143">
ter estimates for different committee members differ
more when they are based on low training counts,
and they agree more when based on high counts. Se-
lecting examples on which the committee members
disagree contributes statistics to currently uncertain
parameters whose uncertainty also affects classifica-
tion.
It may sometimes be difficult to sample P(MIS)
due to parameter interdependence. Fortunately,
models used in natural language processing often
assume independence between most model parame-
ters. In such cases it is possible to generate commit-
tee members by sampling the posterior distribution
for each independent group of parameters separately.
</bodyText>
<sectionHeader confidence="0.968326" genericHeader="method">
4 Bigram Part-Of-Speech Tagging
</sectionHeader>
<subsectionHeader confidence="0.996811">
4.1 Sampling model parameters
</subsectionHeader>
<bodyText confidence="0.999967970588235">
In order to generate committee members for bigram
tagging, we sample the posterior distributions for
transition probabilities, P(t2-4i), and for lexical
probabilities, P(tlw) (as described in Section 2).
Both types of the parameters we sample have the
form of multinomial distributions. Each multinomial
random variable corresponds to a conditioning event
and its values are given by the corresponding set of
conditioned events. For example, a transition prob-
ability parameter P(ti—+t3) has conditioning event
ti and conditioned event ti.
Let { ui} denote the set of possible values of a
given multinomial variable, and let S = Ind de-
note a set of statistics extracted from the training
set for that variable, where ni is the number of times
that the value ui appears in the training set for
the variable, defining N = Ei ni. The parameters
whose posterior distributions we wish to estimate
are ai = P(ui).
The maximum likelihood estimate for each of the
multinomial&apos;s distribution parameters, ai, is &amp; =
. In practice, this estimator is usually smoothed in
some way to compensate for data sparseness. Such
smoothing typically reduces slightly the estimates
for values with positive counts and gives small pos-
itive estimates for values with a zero count. For
simplicity, we describe here the approximation of
P(ai = aiIS) for the unsmoothed estimator3.
We approximate the posterior P(ai = ai IS) by
first assuming that the multinomial is a collection of
independent binomials, each of which corresponds to
a single value ui of the multinomial; we then normal-
ize the values so that they sum to 1. For each such
binomial, we approximate P(ai = ai IS) as a trun-
</bodyText>
<footnote confidence="0.6743952">
3In the implementation we smooth the MLE by in-
terpolation with a uniform probability distribution, fol-
lowing Merialdo (1994). Approximate adaptation of
P(a, = a,IS) to the smoothed version of the estimator
is simple.
</footnote>
<bodyText confidence="0.998952666666667">
cated normal distribution (restricted to [0,1]), with
estimated mean /./. =1*- and variance o-2 =
To generate a particular multinomial distribution,
we randomly choose values for the binomial param-
eters ai from their approximated posterior distribu-
tions (using the simple sampling method given in
(Press et al., 1988, p. 214)), and renormalize them
so that they sum to 1. Finally, to generate a random
HMM given statistics S, we choose values indepen-
dently for the parameters of each multinomial, since
all the different multinomials in an HMM are inde-
pendent.
</bodyText>
<subsectionHeader confidence="0.997364">
4.2 Examples in bigram training
</subsectionHeader>
<bodyText confidence="0.999993230769231">
Typically, concept learning problems are formulated
such that there is a set of training examples that are
independent of each other. When training a bigram
model (indeed, any HMM), this is not true, as each
word is dependent on that before it. This problem
is solved by considering each sentence as an individ-
ual example. More generally, it is possible to break
the text at any point where tagging is unambiguous.
We thus use unambiguous words (those with only
one possible part of speech) as example boundaries
in bigram tagging. This allows us to train on smaller
examples, focusing training more on the truly infor-
mative parts of the corpus.
</bodyText>
<sectionHeader confidence="0.94605" genericHeader="method">
5 Selection Algorithms
</sectionHeader>
<bodyText confidence="0.999792444444444">
Within the committee-based paradigm there exist
different methods for selecting informative examples.
Previous research in sample selection has used either
sequential selection (Seung, Opper, and Sompolin-
sky, 1992; Freund et al., 1993; Dagan and Engelson,
1995), or batch selection (Lewis and Catlett, 1994;
Lewis and Gale, 1994). We describe here general
algorithms for both sequential and batch selection.
Sequential selection examines unlabeled examples
as they are supplied, one by one, and measures the
disagreement in their classification by the commit-
tee. Those examples determined to be sufficiently
informative are selected for training. Most simply,
we can use a committee of size two and select an
example when the two models disagree on its clas-
sification. This gives the following, parameter-free,
two member sequential selection algorithm,
executed for each unlabeled input example e:
</bodyText>
<listItem confidence="0.959661666666667">
1. Draw 2 models randomly from P(MIS), where
S are statistics acquired from previously labeled
examples;
</listItem>
<bodyText confidence="0.996829">
&apos;The normal approximation, while easy to imple-
ment, can be avoided. The posterior probability P(a, =
adS) for the multinomial is given exactly by the Dirich-
let distribution (Johnson, 1972) (which reduces to the
Beta distribution in the binomial case). In this work we
assumed a uniform prior distribution for each model pa-
rameter; we have not addressed the question of how to
best choose a prior for this problem.
</bodyText>
<page confidence="0.985822">
321
</page>
<listItem confidence="0.9907">
2. Classify e by each model, giving classifications
c1 and c2;
3. If c1 c2, select e for annotation;
4. If e is selected, get its correct label and update
S accordingly.
</listItem>
<bodyText confidence="0.915841866666667">
This basic algorithm needs no parameters. If de-
sired, it is possible to tune the frequency of selection,
by changing the variance of P(MIS) (or the variance
of P(ai = a IS) for each parameter), where larger
variances increase the rate of disagreement among
the committee members. We implemented this ef-
fect by employing a temperature parameter t, used
as a multiplier of the variance of the posterior pa-
rameter distribution.
A more general algorithm results from allowing
(i) a larger number of committee members, k, in or-
der to sample P(MIS) more precisely, and (ii) more
refined example selection criteria. This gives the fol-
lowing general sequential selection algorithm,
executed for each unlabeled input example e:
</bodyText>
<listItem confidence="0.995116777777778">
1. Draw k models {Mi} randomly from P(MIS)
(possibly using a temperature 1);
2. Classify e by each model Mi giving classifica-
tions {ci};
3. Measure the disagreement D over {ci};
4. Decide whether to select e for annotation, based
on the value of D;
5. If e is selected, get its correct label and update
S accordingly.
</listItem>
<bodyText confidence="0.999801692307692">
It is easy to see that two member sequential selec-
tion is a special case of general sequential selection,
where any disagreement is considered sufficient for
selection. In order to instantiate the general algo-
rithm for larger committees, we need to define (i) a
measure for disagreement (Step 3), and (ii) a selec-
tion criterion (Step 4).
Our approach to measuring disagreement is to use
the vote entropy, the entropy of the distribution of
classifications assigned to an example (&apos;voted for&apos;)
by the committee members. Denoting the number
of committee members assigning c to e by V(c, e),
the vote entropy is:
</bodyText>
<equation confidence="0.8498525">
D 1 V(c, e) V(c, e)
log k L-sc k g k
</equation>
<bodyText confidence="0.970634131578947">
(Dividing by log k normalizes the scale for the num-
ber of committee members.) Vote entropy is maxi-
mized when all committee members disagree, and is
zero when they all agree.
In bigram tagging, each example consists of a se-
quence of several words. In our system, we measure
D separately for each word, and use the average en-
tropy over the word sequence as a measurement of
disagreement for the example. We use the average
entropy rather than the entropy over the entire se-
quence, because the number of committee members
is small with respect to the total number of possible
tag sequences. Note that we do not look at the en-
tropy of the distribution given by each single model
to the possible tags (classes), since we are only in-
terested in the uncertainty of the final classification
(see the discussion in Section 7).
We consider two alternative selection criteria (for
Step 4). The simplest is thresholded selection, in
which an example is selected for annotation if its
vote entropy exceeds some threshold O. The other
alternative is randomized selection, in which an ex-
ample is selected for annotation based on the flip
of a coin biased according to the vote entropy—a
higher vote entropy entailing a higher probability of
selection. We define the selection probability as a
linear function of vote entropy: p = gD, where g is
an entropy gain parameter. The selection method
we used in our earlier work (Dagan and Engelson,
1995) is randomized sequential selection using this
linear selection probability model, with parameters
k, t and g.
An alternative to sequential selection is batch se-
lection. Rather than evaluating examples individ-
ually for their informativeness a large batch of ex-
amples is examined, and the m best are selected for
annotation. The batch selection algorithm, exe-
cuted for each batch B of N examples, is as follows:
</bodyText>
<listItem confidence="0.9859393">
1. For each example e in B:
(a) Draw k models randomly from P(MIS);
(b) Classify e by each model, giving classifica-
tions {ci};
(c) Measure the disagreement De for e over
{c i};
2. Select for annotation the m examples from B
with the highest De;
3. Update S by the statistics of the selected exam-
ples.
</listItem>
<bodyText confidence="0.999171375">
This procedure is repeated sequentially for succes-
sive batches of N examples, returning to the start of
the corpus at the end. If N is equal to the size of the
corpus, batch selection selects the m globally best
examples in the corpus at each stage (as in (Lewis
and Catlett, 1994)). On the other hand, as N de-
creases, batch selection becomes closer to sequential
selection.
</bodyText>
<sectionHeader confidence="0.998692" genericHeader="method">
6 Experimental Results
</sectionHeader>
<bodyText confidence="0.9988515">
This section presents results of applying committee-
based sample selection to bigram part-of-speech tag-
ging, as compared with complete training on all ex-
amples in the corpus. Evaluation was performed
using the University of Pennsylvania tagged corpus
from the ACL/DCI CD-ROM I. For ease of im-
plementation, we used a complete (closed) lexicon
which contains all the words in the corpus.
The committee-based sampling algorithm was ini-
tialized using the first 1,000 words from the corpus,
</bodyText>
<page confidence="0.980594">
322
</page>
<figure confidence="0.999788666666666">
35000
30030
to
-9 25000
20000
15030
10000
5000
0
0.85 0.86 0.87 0.88 0.89 0.9
Accuracy
(a)
0.91 0.92 0.93 0 100 200 300 400 500 600 700 800 900 1030
Batch size
(a)
0.925
0.92
0.915
1 3640 words selected -
Batch selection 4=5; N=100) - 6640 words selected
Thresholded selection (t1-0.2) 9660 words selected
Randomized selection (g.0.5) 12660 words selected --
Two meMber selection ---- •
Complete training --- •
•
......
............
......
..............
0.91
&lt;`4)&apos; 0.905
0.9
0.895
0.89
Batch selection (m=5; N=100) - -
Thresholded selection (th=0.3)
Randomized selection (g=0.5)
Two umber selection -- _
Complete training ---
.... ........................................................
0.96
0.94
0.92
:t 0.9
0.88
0.86
0.98
0.96
0.94
?1, 0.92
0.9
0.88
0.86
Two member selection -
Batch selection (n5; N=50)
Batch selection (ni=5; N=100)
Batch selection (m=5; N=500) --
Batch selection (n5; N=1000) _
Complete training. -
.....
0 50000 100000 1500 200000 250030 300000
Examined training
(b)
</figure>
<figureCaption confidence="0.978039666666667">
Figure 1: Training versus accuracy. In batch, random,
and thresholded runs, k = 5 and t = 50. (a) Number
of ambiguous words selected for labeling versus classifi-
cation accuracy achieved. (b) Accuracy versus number
of words examined from the corpus (both labeled and
unlabeled).
</figureCaption>
<bodyText confidence="0.943978294117647">
and then sequentially examined the following exam-
ples in the corpus for possible labeling. The training
set consisted of the first million words in the cor-
pus, with sentence ordering randomized to compen-
sate for inhomogeneity in corpus composition. The
test set was a separate portion of the corpus, con-
sisting of 20,000 words. We compare the amount
of training required by different selection methods
to achieve a given tagging accuracy on the test set,
where both the amount of training and tagging ac-
curacy are measured over ambiguous words.&apos;
The effectiveness of randomized committee-based
5Note that most other work on tagging has measured
accuracy over all words, not just ambiguous ones. Com-
plete training of our system on 1,000,000 words gave us
an accuracy of 93.5% over ambiguous words, which cor-
responds to an accuracy of 95.9% over all words in the
</bodyText>
<figure confidence="0.998537333333333">
0 50000 100000 150000 2001300 250000 300000 350000 400000
Examined training
(b)
</figure>
<figureCaption confidence="0.9920765">
Figure 2: Evaluating batch selection, for m =-. 5. (a) Ac-
curacy achieved versus batch size at different numbers of
selected training words. (b) Accuracy versus number of
words examined from the corpus for different batch sizes.
</figureCaption>
<bodyText confidence="0.948622666666667">
selection for part-of-speech tagging, with 5 and 10
committee members, was demonstrated in (Dagan
and Engelson, 1995). Here we present and compare
results for batch, randomized, thresholded, and two
member committee-based selection.
Figure 1 presents the results of comparing the sev-
eral selection methods against each other. The plots
shown are for the best parameter settings that we
found through manual tuning for each method. Fig-
ure 1(a) shows the advantage that sample selection
gives with regard to annotation cost. For example,
complete training requires annotated examples con-
taining 98,000 ambiguous words to achieve a 92.6%
accuracy (beyond the scale of the graph), while the
selective methods require only 18,000-25,000 am-
biguous words to achieve this accuracy. We also find
test set, comparable to other published results on bigram
tagging.
</bodyText>
<page confidence="0.998092">
323
</page>
<figure confidence="0.938917">
0.85 0.86 0.87 0.88 0.89 0.9 0.91 0.92 0.93 0.94
Accuracy
</figure>
<figureCaption confidence="0.9992625">
Figure 3: The size of the trained model, measured by
the number of frequency counts &gt; 0, plotted (y-axis) ver-
sus classification accuracy achieved (x-axis). (a) Lexical
counts (freq(2, w)) (b) Bigram counts (freq(ti -42 )).
</figureCaption>
<bodyText confidence="0.9998645">
that, to a first approximation, all selection methods
considered give similar results. Thus, it seems that a
refined choice of the selection method is not crucial
for achieving large reductions in annotation cost.
This equivalence of the different methods also
largely holds with respect to computational effi-
ciency. Figure 1(b) plots classification accuracy ver-
sus number of words examined, instead of those
selected. We see that while all selective methods
are less efficient in terms of examples examined
than complete training, they are comparable to each
other. Two member selection seems to have a clear,
though small, advantage.
In Figure 2 we investigate further the properties
of batch selection. Figure 2(a) shows that accuracy
increases with batch size only up to a point, and
then starts to decrease. This result is in line with
theoretical difficulties with batch selection (Freund
et al., 1993) in that batch selection does not account
for the distribution of input examples. Hence, once
batch size increases past a point, the input distribu-
tion has too little influence on which examples are
selected, and hence classification accuracy decreases.
Furthermore, as batch size increases, computational
efficiency, in terms of the number of examples exam-
ined to attain a given accuracy, decreases tremen-
dously (Figure 2(b)).
The ability of committee-based selection to fo-
cus on the more informative parts of the training
corpus is analyzed in Figure 3. Here we examined
the number of lexical and bigram counts that were
stored (i.e, were non-zero) during training, using
the two member selection algorithm and complete
training. As the graphs show, the sample selec-
tion method achieves the same accuracy as complete
training with fewer lexical and bigram counts. This
means that many counts in the data are less useful
for correct tagging, as replacing them with smoothed
estimates works just as well.&apos; Committee-based se-
lection ignores such counts, focusing on parameters
which improve the model. This behavior has the
practical advantage of reducing the size of the model
significantly (by a factor of three here). Also, the
average count is lower in a model constructed by
selective training than in a fully trained model, sug-
gesting that the selection method avoids using ex-
amples which increase the counts for already known
parameters.
</bodyText>
<sectionHeader confidence="0.993674" genericHeader="method">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999811363636364">
Why does committee-based sample selection work?
Consider the properties of those examples that are
selected for training. In general, a selected train-
ing example will contribute data to several statistics,
which in turn will improve the estimates of several
parameter values. An informative example is there-
fore one whose contribution to the statistics leads to
a significantly useful improvement of model parame-
ter estimates. Model parameters for which acquiring
additional statistics is most beneficial can be char-
acterized by the following three properties:
</bodyText>
<listItem confidence="0.65938625">
1. The current estimate of the parameter is uncer-
tain due to insufficient statistics in the training
set. Additional statistics would bring the esti-
mate closer to the true value.
2. Classification of examples is sensitive to changes
in the current estimate of the parameter. Oth-
erwise, even if the current value of the pa-
rameter is very uncertain, acquiring additional
statistics will not change the resulting classifi-
cations.
3. The parameter affects classification for a large
proportion of examples in the input. Parame-
</listItem>
<footnote confidence="0.868713333333333">
6As noted above, we smooth the MLE estimates
by interpolation with a uniform probability distribution
(Merialdo, 1994).
</footnote>
<figure confidence="0.991788740740741">
7,1
0.86 0.87 0.88 0.89 0.9 0.91 0.92 0.93
Accuracy
1600
1400
1200
7). 1000
800
600
400
200
20030
18000
16000
14030
12000
10000
8000
6000
4000
2000
0
0.85
;
Two member selection
Complete training /----
, -
</figure>
<page confidence="0.994655">
324
</page>
<bodyText confidence="0.99745676">
ters that affect only few examples have low over-
all utility.
The committee-based selection algorithms work
because they tend to select examples that affect pa-
rameters with the above three properties. Prop-
erty 1 is addressed by randomly drawing the parame-
ter values for committee members from the posterior
distribution given the current statistics. When the
statistics for a parameter are insufficient, the vari-
ance of the posterior distribution of the estimates is
large, and hence there will be large differences in the
values of the parameter chosen for different commit-
tee members. Note that property 1 is not addressed
when uncertainty in classification is only judged rel-
ative to a single model7 (as in, eg, (Lewis and Gale,
1994)).
Property 2 is addressed by selecting examples for
which committee members highly disagree in clas-
sification (rather than measuring disagreement in
parameter estimates). Committee-based selection
thus addresses properties 1 and 2 simultaneously:
it acquires statistics just when uncertainty in cur-
rent parameter estimates entails uncertainty regard-
ing the appropriate classification of the example.
Our results show that this effect is achieved even
when using only two committee members to sample
the space of likely classifications. By appropriate
classification we mean the classification given by a
perfectly-trained model, that is, one with accurate
parameter values.
Note that this type of uncertainty regarding the
identity of the appropriate classification, is differ-
ent than uncertainty regarding the correctness of the
classification itself. For example, sufficient statistics
may yield an accurate 0.51 probability estimate for
a class c in a given example, making it certain that
c is the appropriate classification. However, the cer-
tainty that c is the correct classification is low, since
there is a 0.49 chance that c is the wrong class for
the example. A single model can be used to estimate
only the second type of uncertainty, which does not
correlate directly with the utility of additional train-
ing.
Finally, property 3 is addressed by independently
examining input examples which are drawn from the
input distribution. In this way, we implicitly model
the distribution of model parameters used for clas-
sifying input examples. Such modeling is absent in
batch selection, and we hypothesize that this is the
reason for its lower effectiveness.
</bodyText>
<sectionHeader confidence="0.998886" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.992752102564103">
Annotating large textual corpora for training natu-
ral language models is a costly process. We propose
reducing this cost significantly using committee-
7The use of a single model is also criticized in (Cohn,
Atlas, and Ladner, 1994).
based sample selection, which reduces redundant an-
notation of examples that contribute little new in-
formation. The method can be applied in a semi-
interactive process, in which the system selects sev-
eral new examples for annotation at a time and up-
dates its statistics after receiving their labels from
the user. The implicit modeling of uncertainty
makes the selection system generally applicable and
quite simple to implement.
Our experimental study of variants of the selec-
tion method suggests several practical conclusions.
First, it was found that the simplest version of the
committee-based method, using a two-member com-
mittee, yields reduction in annotation cost compa-
rable to that of the multi-member committee. The
two-member version is simpler to implement, has no
parameters to tune and is computationally more ef-
ficient. Second, we generalized the selection scheme
giving several alternatives for optimizing the method
for a specific task. For bigram tagging, comparative
evaluation of the different variants of the method
showed similar large reductions in annotation cost,
suggesting the robustness of the committee-based
approach. Third, sequential selection, which im-
plicitly models the expected utility of an example
relative to the example distribution, worked in gen-
eral better than batch selection. The latter was
found to work well only for small batch sizes, where
the method mimics sequential selection. Increas-
ing batch size (approaching &apos;pure&apos; batch selection)
reduces both accuracy and efficiency. Finally, we
studied the effect of sample selection on the size of
the trained model, showing a significant reduction
in model size.
</bodyText>
<subsectionHeader confidence="0.897642">
8.1 Further research
</subsectionHeader>
<bodyText confidence="0.897795409090909">
Our results suggest applying committee-based sam-
ple selection to other statistical NLP tasks which
rely on estimating probabilistic parameters from an
annotated corpus. Statistical methods for these
tasks typically assign a probability estimate, or some
other statistical score, to each alternative analysis
(a word sense, a category label, a parse tree, etc.),
and then select the analysis with the highest score.
The score is usually computed as a function of the
estimates of several &apos;atomic&apos; parameters, often bino-
mials or multinomials, such as:
• In word sense disambiguation (Hearst, 1991;
Gale, Church, and Yarowsky, 1993): P(slf),
where s is a specific sense of the ambiguous word
in question w, and f is a feature of occurrences
of w. Common features are words in the context
of w or morphological attributes of it.
• In prepositional-phrase (PP) attachment (Hin-
dle and Rooth, 1993): P(alf), where a is a pos-
sible attachment, such as an attachment to a
head verb or noun, and f is a feature, or a com-
bination of features, of the attachment. Corn-
</bodyText>
<page confidence="0.997344">
325
</page>
<bodyText confidence="0.999967666666667">
mon features are the words involved in the at-
tachment, such as the head verb or noun, the
preposition, and the head word of the PP.
</bodyText>
<listItem confidence="0.651511666666667">
• In statistical parsing (Black et al., 1993):
P(rlh), the probability of applying the rule r
at a certain stage of the top down derivation of
the parse tree given the history h of the deriva-
tion process.
• In text categorization (Lewis and Gale, 1994;
Iwayama and Tokunaga, 1994): P(tIC), where
t is a term in the document to be categorized,
and C is a candidate category label.
</listItem>
<bodyText confidence="0.999905090909091">
Applying committee-based selection to supervised
training for such tasks can be done analogously to
its application in the current papers. Furthermore,
committee-based selection may be attempted also
for training non-probabilistic classifiers, where ex-
plicit modeling of information gain is typically im-
possible. In such contexts, committee members
might be generated by randomly varying some of
the decisions made in the learning algorithm.
Another important area for future work is in de-
veloping sample selection methods which are inde-
pendent of the eventual learning method to be ap-
plied. This would be of considerable advantage in
developing selectively annotated corpora for general
research use. Recent work on heterogeneous uncer-
tainty sampling (Lewis and Catlett, 1994) supports
this idea, using one type of model for example selec-
tion and a different type for classification.
Acknowledgments. We thank Yoav Freund and
Yishay Mansour for helpful discussions. The first
author gratefully acknowledges the support of the
Fulbright Foundation.
</bodyText>
<sectionHeader confidence="0.999269" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9540899">
Black, Ezra, Fred Jelinek, John Lafferty, David
Magerman, Robert Mercer, and Salim Roukos.
1993. Towards history-based grammars: using
richer models for probabilistic parsing. In Proc.
of the Annual Meeting of the ACL, pages 31-37.
Church, Kenneth W. 1988. A stochastic parts pro-
gram and noun phrase parser for unrestricted text.
In Proc. of ACL Conference on Applied Natural
Language Processing.
Cohn, David, Les Atlas, and Richard Ladner. 1994.
Improving generalization with active learning.
Machine Learning, 15:201-221.
8Measuring disagreement in full syntactic parsing is
complicated. It may be approached by similar methods
to those used for parsing evaluation, which measure the
disagreement between the parser&apos;s output and the cor-
rect parse.
Dagan, Ido and Sean Engelson. 1995. Committee-
based sampling for training probabilistic classi-
fiers. In Proc. Intl Conference on Machine Learn-
ing, July.
Elworthy, David. 1994. Does Baum-Welch re-
estimation improve taggers? In Proc. of ACL
Conference on Applied Natural Language Process-
ing, pages 53-58.
Freund, Y., H. S. Seung, E. Shamir, and N. Tishby.
1993. Information, prediction, and query by com-
mittee. In Advances in Neural Information Pro-
cessing, volume 5. Morgan Kaufmann.
Gale, William, Kenneth Church, and David
Yarowsky. 1993. A method for disambiguating
word senses in a large corpus. Computers and the
Humanities, 26:415-439.
Hearst, Marti. 1991. Noun homograph disambigua-
tion using local context in large text corpora. In
Proc. of the Annual Conference of the UW Center
for the New OED and Text Research, pages 1-22.
Hindle, Donald and Mats Rooth. 1993. Structural
ambiguity and lexical relations. Computational
Linguistics, 19(1):103-120.
Iwayama, M. and T. Tokunaga. 1994. A probabilis-
tic model for text categorization based on a sin-
gle random variable with multiple values. In Pro-
ceedings of the 4th Conference on Applied Natural
Language Processing.
Johnson, Norman L. 1972. Continuous Multivariate
Distributions. John Wiley &amp; Sons, New York.
Kupiec, Julian. 1992. Robust part-of-speech tagging
using a hidden makov model. Computer Speech
and Language, 6:225-242.
Lewis, David D. and Jason Catlett. 1994. Heteroge-
neous uncertainty sampling for supervised learn-
ing. In Proc. Inel Conference on Machine Learn-
ing.
Lewis, David D. and William A. Gale. 1994. A
sequential algorithm for training text classifiers.
In Proc. of the ACM SIGIR Conference.
MacKay, David J. C. 1992. Information-based ob-
jective functions for active data selection. Neural
Computation, 4.
Merialdo, Bernard. 1994. Tagging text with a
probabilistic model. Computational Linguistics,
20(2):155-172.
Press, William H., Brian P. Flannery, Saul A.
Teukolsky, and William T. Vetterling. 1988.
Numerical Recipes in C. Cambridge University
Press.
Seung, H. S., M. Opper, and H. Sompolinsky. 1992.
Query by committee. In Proc. ACM Workshop on
Computational Learning Theory.
</reference>
<page confidence="0.999113">
326
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.915119">
<title confidence="0.991194">Minimizing Manual Annotation Cost In Supervised Training From Corpora</title>
<author confidence="0.99978">P Engelson Dagan</author>
<affiliation confidence="0.99091">Department of Mathematics and Computer Science University</affiliation>
<address confidence="0.999771">52900 Ramat Gan, Israel</address>
<email confidence="0.99958">fengelson,daganlObimacs.cs.biu.ac.il</email>
<abstract confidence="0.99801575">Corpus-based methods for natural language processing often use supervised training, requiring expensive manual annotation of training corpora. This paper investigates methods for reducing annotacost by selection. this approach, during training the learning program examines many unlabeled examples and selects for labeling (annotation) only those that are most informative at each stage. This avoids redundantly annotating examples that contribute little new information. This paper extends our previous on sample selection for probabilistic classifiers. We describe a family of methods for committee-based sample selection, and report experimental results for the task of stochastic part-ofspeech tagging. We find that all variants achieve a significant reduction in annotation cost, though their computational efficiency differs. In particular, the simplest method, which has no parameters to tune, gives excellent results. We also show that sample selection yields a significant reduction in the size of the model used by the tagger.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ezra Black</author>
<author>Fred Jelinek</author>
<author>John Lafferty</author>
<author>David Magerman</author>
<author>Robert Mercer</author>
<author>Salim Roukos</author>
</authors>
<title>Towards history-based grammars: using richer models for probabilistic parsing.</title>
<date>1993</date>
<booktitle>In Proc. of the Annual Meeting of the ACL,</booktitle>
<pages>31--37</pages>
<contexts>
<context position="2967" citStr="Black et al., 1993" startWordPosition="434" endWordPosition="437">ed (not annotated) examples, selecting for labeling only those that are most informative for the learner at each stage of training (Seung, Opper, and Sompolinsky, 1992; Freund et al., 1993; Lewis and Gale, 1994; Cohn, Atlas, and Ladner, 1994). This avoids redundantly annotating many examples that contribute roughly the same information to the learner. Our work focuses on sample selection for training probabilistic classifiers. In statistical NLP, probabilistic classifiers are often used to select a preferred analysis of the linguistic structure of a text (for example, its syntactic structure (Black et al., 1993), word categories (Church, 1988), or word senses (Gale, Church, and Yarowsky, 1993)). As a representative task for probabilistic classification in NLP, we experiment in this paper with sample selection for the popular and well-understood method of stochastic part-of-speech tagging using Hidden Markov Models. We first review the basic approach of committeebased sample selection and its application to partof-speech tagging. This basic approach gives rise to a family of algorithms (including the original algorithm described in (Dagan and Engelson, 1995)) which we then describe. First, we describe</context>
<context position="32625" citStr="Black et al., 1993" startWordPosition="5223" endWordPosition="5226">rowsky, 1993): P(slf), where s is a specific sense of the ambiguous word in question w, and f is a feature of occurrences of w. Common features are words in the context of w or morphological attributes of it. • In prepositional-phrase (PP) attachment (Hindle and Rooth, 1993): P(alf), where a is a possible attachment, such as an attachment to a head verb or noun, and f is a feature, or a combination of features, of the attachment. Corn325 mon features are the words involved in the attachment, such as the head verb or noun, the preposition, and the head word of the PP. • In statistical parsing (Black et al., 1993): P(rlh), the probability of applying the rule r at a certain stage of the top down derivation of the parse tree given the history h of the derivation process. • In text categorization (Lewis and Gale, 1994; Iwayama and Tokunaga, 1994): P(tIC), where t is a term in the document to be categorized, and C is a candidate category label. Applying committee-based selection to supervised training for such tasks can be done analogously to its application in the current papers. Furthermore, committee-based selection may be attempted also for training non-probabilistic classifiers, where explicit modeli</context>
</contexts>
<marker>Black, Jelinek, Lafferty, Magerman, Mercer, Roukos, 1993</marker>
<rawString>Black, Ezra, Fred Jelinek, John Lafferty, David Magerman, Robert Mercer, and Salim Roukos. 1993. Towards history-based grammars: using richer models for probabilistic parsing. In Proc. of the Annual Meeting of the ACL, pages 31-37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth W Church</author>
</authors>
<title>A stochastic parts program and noun phrase parser for unrestricted text.</title>
<date>1988</date>
<booktitle>In Proc. of ACL Conference on Applied Natural Language Processing.</booktitle>
<contexts>
<context position="2999" citStr="Church, 1988" startWordPosition="440" endWordPosition="441"> for labeling only those that are most informative for the learner at each stage of training (Seung, Opper, and Sompolinsky, 1992; Freund et al., 1993; Lewis and Gale, 1994; Cohn, Atlas, and Ladner, 1994). This avoids redundantly annotating many examples that contribute roughly the same information to the learner. Our work focuses on sample selection for training probabilistic classifiers. In statistical NLP, probabilistic classifiers are often used to select a preferred analysis of the linguistic structure of a text (for example, its syntactic structure (Black et al., 1993), word categories (Church, 1988), or word senses (Gale, Church, and Yarowsky, 1993)). As a representative task for probabilistic classification in NLP, we experiment in this paper with sample selection for the popular and well-understood method of stochastic part-of-speech tagging using Hidden Markov Models. We first review the basic approach of committeebased sample selection and its application to partof-speech tagging. This basic approach gives rise to a family of algorithms (including the original algorithm described in (Dagan and Engelson, 1995)) which we then describe. First, we describe the &apos;simplest&apos; committee-based </context>
<context position="6686" citStr="Church, 1988" startWordPosition="1037" endWordPosition="1038">babilities P(t1w) giving the probability of tag t labeling word w, and tag probabilities P(t) giving the marginal probability of a tag occurring.2 The values of these parameters are estimated from a tagged corpus which provides a training set of labeled examples (see Section 4.1). 3 Evaluating Example Uncertainty A sample selection method needs to evaluate the expected usefulness, or information gain, of learning from a given example. The methods we investi&apos;This gives the Viterbi model (Merialdo, 1994), which we use here. 2This version of the method uses Bayes&apos; theorem (P(wslt,) cc P(4It7;)) (Church, 1988). gate approach this evaluation implicitly, measuring an example&apos;s informativeness as the uncertainty in its classification given the current training data (Seung, Opper, and Sompolinsky, 1992; Lewis and Gale, 1994; MacKay, 1992). The reasoning is that if an example&apos;s classification is uncertain given current training data then the example is likely to contain unknown information useful for classifying similar examples in the future. We investigate the committee-based method, where the learning algorithm evaluates an example by giving it to a committee containing several variant models, all &apos;c</context>
</contexts>
<marker>Church, 1988</marker>
<rawString>Church, Kenneth W. 1988. A stochastic parts program and noun phrase parser for unrestricted text. In Proc. of ACL Conference on Applied Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Cohn</author>
<author>Les Atlas</author>
<author>Richard Ladner</author>
</authors>
<title>Improving generalization with active learning.</title>
<date>1994</date>
<booktitle>Machine Learning,</booktitle>
<pages>15--201</pages>
<contexts>
<context position="2589" citStr="Cohn, Atlas, and Ladner, 1994" startWordPosition="376" endWordPosition="380"> an approach for optimizing the supervised training (learning) phase, which reduces the annotation effort required to achieve a desired level of accuracy of the trained model. In this paper, we investigate and extend the committee-based sample selection approach to minimizing training cost (Dagan and Engelson, 1995). When using sample selection, a learning program examines many unlabeled (not annotated) examples, selecting for labeling only those that are most informative for the learner at each stage of training (Seung, Opper, and Sompolinsky, 1992; Freund et al., 1993; Lewis and Gale, 1994; Cohn, Atlas, and Ladner, 1994). This avoids redundantly annotating many examples that contribute roughly the same information to the learner. Our work focuses on sample selection for training probabilistic classifiers. In statistical NLP, probabilistic classifiers are often used to select a preferred analysis of the linguistic structure of a text (for example, its syntactic structure (Black et al., 1993), word categories (Church, 1988), or word senses (Gale, Church, and Yarowsky, 1993)). As a representative task for probabilistic classification in NLP, we experiment in this paper with sample selection for the popular and </context>
<context position="29710" citStr="Cohn, Atlas, and Ladner, 1994" startWordPosition="4757" endWordPosition="4761">y with the utility of additional training. Finally, property 3 is addressed by independently examining input examples which are drawn from the input distribution. In this way, we implicitly model the distribution of model parameters used for classifying input examples. Such modeling is absent in batch selection, and we hypothesize that this is the reason for its lower effectiveness. 8 Conclusions Annotating large textual corpora for training natural language models is a costly process. We propose reducing this cost significantly using committee7The use of a single model is also criticized in (Cohn, Atlas, and Ladner, 1994). based sample selection, which reduces redundant annotation of examples that contribute little new information. The method can be applied in a semiinteractive process, in which the system selects several new examples for annotation at a time and updates its statistics after receiving their labels from the user. The implicit modeling of uncertainty makes the selection system generally applicable and quite simple to implement. Our experimental study of variants of the selection method suggests several practical conclusions. First, it was found that the simplest version of the committee-based m</context>
</contexts>
<marker>Cohn, Atlas, Ladner, 1994</marker>
<rawString>Cohn, David, Les Atlas, and Richard Ladner. 1994. Improving generalization with active learning. Machine Learning, 15:201-221.</rawString>
</citation>
<citation valid="false">
<title>8Measuring disagreement in full syntactic parsing is complicated. It may be approached by similar methods to those used for parsing evaluation, which measure the disagreement between the parser&apos;s output and the correct parse.</title>
<marker></marker>
<rawString>8Measuring disagreement in full syntactic parsing is complicated. It may be approached by similar methods to those used for parsing evaluation, which measure the disagreement between the parser&apos;s output and the correct parse.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Sean Engelson</author>
</authors>
<title>Committeebased sampling for training probabilistic classifiers.</title>
<date>1995</date>
<booktitle>In Proc. Intl Conference on Machine Learning,</booktitle>
<contexts>
<context position="2277" citStr="Dagan and Engelson, 1995" startWordPosition="327" endWordPosition="330">s, which do not require an annotated corpus at all (Kupiec, 1992; Merialdo, 1994; Elworthy, 1994). It has been shown, however, that some supervised training prior to the unsupervised phase is often beneficial. Indeed, fully unsupervised training may not be feasible for certain tasks. This paper investigates an approach for optimizing the supervised training (learning) phase, which reduces the annotation effort required to achieve a desired level of accuracy of the trained model. In this paper, we investigate and extend the committee-based sample selection approach to minimizing training cost (Dagan and Engelson, 1995). When using sample selection, a learning program examines many unlabeled (not annotated) examples, selecting for labeling only those that are most informative for the learner at each stage of training (Seung, Opper, and Sompolinsky, 1992; Freund et al., 1993; Lewis and Gale, 1994; Cohn, Atlas, and Ladner, 1994). This avoids redundantly annotating many examples that contribute roughly the same information to the learner. Our work focuses on sample selection for training probabilistic classifiers. In statistical NLP, probabilistic classifiers are often used to select a preferred analysis of the</context>
<context position="3523" citStr="Dagan and Engelson, 1995" startWordPosition="517" endWordPosition="520"> of a text (for example, its syntactic structure (Black et al., 1993), word categories (Church, 1988), or word senses (Gale, Church, and Yarowsky, 1993)). As a representative task for probabilistic classification in NLP, we experiment in this paper with sample selection for the popular and well-understood method of stochastic part-of-speech tagging using Hidden Markov Models. We first review the basic approach of committeebased sample selection and its application to partof-speech tagging. This basic approach gives rise to a family of algorithms (including the original algorithm described in (Dagan and Engelson, 1995)) which we then describe. First, we describe the &apos;simplest&apos; committee-based selection algorithm, which has no parameters to tune. We then generalize the selection scheme, allowing more options to adapt and tune the approach for specific tasks. The paper compares the performance of several instantiations of the general scheme, including a batch selection method similar to that of Lewis and Gale (1994). In particular, we found that the simplest version of the method achieves a significant reduction in annotation cost, comparable to that of other versions. 319 We also evaluate the computational e</context>
<context position="7905" citStr="Dagan and Engelson, 1995" startWordPosition="1218" endWordPosition="1222">all &apos;consistent&apos; with the training data seen so far. The more the committee members agree on the classification of the example, the greater our certainty in its classification. This is because when the training data entails a specific classification with high certainty, most (in a probabilistic sense) classifiers consistent with the data will produce that classification. The committee-based approach was first proposed in a theoretical context for learning binary nonprobabilistic classifiers (Seung, Opper, and Sompolinsky, 1992; Freund et al., 1993). In this paper, we extend our previous work (Dagan and Engelson, 1995) where we applied the basic idea of the committee-based approach to probabilistic classification. Taking a Bayesian perspective, the posterior probability of a model, P(MIS), is determined given statistics S from the training set (and some prior distribution for the models). Committee members are then generated by drawing models randomly from POI IS). An example is selected for labeling if the committee members largely disagree on its classification. This procedure assumes that one can sample from the models&apos; posterior distribution, at least approximately. To illustrate the generation of commi</context>
<context position="13664" citStr="Dagan and Engelson, 1995" startWordPosition="2151" endWordPosition="2154">dividual example. More generally, it is possible to break the text at any point where tagging is unambiguous. We thus use unambiguous words (those with only one possible part of speech) as example boundaries in bigram tagging. This allows us to train on smaller examples, focusing training more on the truly informative parts of the corpus. 5 Selection Algorithms Within the committee-based paradigm there exist different methods for selecting informative examples. Previous research in sample selection has used either sequential selection (Seung, Opper, and Sompolinsky, 1992; Freund et al., 1993; Dagan and Engelson, 1995), or batch selection (Lewis and Catlett, 1994; Lewis and Gale, 1994). We describe here general algorithms for both sequential and batch selection. Sequential selection examines unlabeled examples as they are supplied, one by one, and measures the disagreement in their classification by the committee. Those examples determined to be sufficiently informative are selected for training. Most simply, we can use a committee of size two and select an example when the two models disagree on its classification. This gives the following, parameter-free, two member sequential selection algorithm, execute</context>
<context position="18111" citStr="Dagan and Engelson, 1995" startWordPosition="2903" endWordPosition="2906">ection 7). We consider two alternative selection criteria (for Step 4). The simplest is thresholded selection, in which an example is selected for annotation if its vote entropy exceeds some threshold O. The other alternative is randomized selection, in which an example is selected for annotation based on the flip of a coin biased according to the vote entropy—a higher vote entropy entailing a higher probability of selection. We define the selection probability as a linear function of vote entropy: p = gD, where g is an entropy gain parameter. The selection method we used in our earlier work (Dagan and Engelson, 1995) is randomized sequential selection using this linear selection probability model, with parameters k, t and g. An alternative to sequential selection is batch selection. Rather than evaluating examples individually for their informativeness a large batch of examples is examined, and the m best are selected for annotation. The batch selection algorithm, executed for each batch B of N examples, is as follows: 1. For each example e in B: (a) Draw k models randomly from P(MIS); (b) Classify e by each model, giving classifications {ci}; (c) Measure the disagreement De for e over {c i}; 2. Select fo</context>
<context position="22266" citStr="Dagan and Engelson, 1995" startWordPosition="3585" endWordPosition="3588">l words, not just ambiguous ones. Complete training of our system on 1,000,000 words gave us an accuracy of 93.5% over ambiguous words, which corresponds to an accuracy of 95.9% over all words in the 0 50000 100000 150000 2001300 250000 300000 350000 400000 Examined training (b) Figure 2: Evaluating batch selection, for m =-. 5. (a) Accuracy achieved versus batch size at different numbers of selected training words. (b) Accuracy versus number of words examined from the corpus for different batch sizes. selection for part-of-speech tagging, with 5 and 10 committee members, was demonstrated in (Dagan and Engelson, 1995). Here we present and compare results for batch, randomized, thresholded, and two member committee-based selection. Figure 1 presents the results of comparing the several selection methods against each other. The plots shown are for the best parameter settings that we found through manual tuning for each method. Figure 1(a) shows the advantage that sample selection gives with regard to annotation cost. For example, complete training requires annotated examples containing 98,000 ambiguous words to achieve a 92.6% accuracy (beyond the scale of the graph), while the selective methods require only</context>
</contexts>
<marker>Dagan, Engelson, 1995</marker>
<rawString>Dagan, Ido and Sean Engelson. 1995. Committeebased sampling for training probabilistic classifiers. In Proc. Intl Conference on Machine Learning, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Elworthy</author>
</authors>
<title>Does Baum-Welch reestimation improve taggers?</title>
<date>1994</date>
<booktitle>In Proc. of ACL Conference on Applied Natural Language Processing,</booktitle>
<pages>53--58</pages>
<contexts>
<context position="1749" citStr="Elworthy, 1994" startWordPosition="248" endWordPosition="249">method, which has no parameters to tune, gives excellent results. We also show that sample selection yields a significant reduction in the size of the model used by the tagger. 1 Introduction Many corpus-based methods for natural language processing (NLP) are based on supervised training— acquiring information from a manually annotated corpus. Therefore, reducing annotation cost is an important research goal for statistical NLP. The ultimate reduction in annotation cost is achieved by unsupervised training methods, which do not require an annotated corpus at all (Kupiec, 1992; Merialdo, 1994; Elworthy, 1994). It has been shown, however, that some supervised training prior to the unsupervised phase is often beneficial. Indeed, fully unsupervised training may not be feasible for certain tasks. This paper investigates an approach for optimizing the supervised training (learning) phase, which reduces the annotation effort required to achieve a desired level of accuracy of the trained model. In this paper, we investigate and extend the committee-based sample selection approach to minimizing training cost (Dagan and Engelson, 1995). When using sample selection, a learning program examines many unlabele</context>
</contexts>
<marker>Elworthy, 1994</marker>
<rawString>Elworthy, David. 1994. Does Baum-Welch reestimation improve taggers? In Proc. of ACL Conference on Applied Natural Language Processing, pages 53-58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>H S Seung</author>
<author>E Shamir</author>
<author>N Tishby</author>
</authors>
<title>Information, prediction, and query by committee.</title>
<date>1993</date>
<booktitle>In Advances in Neural Information Processing,</booktitle>
<volume>5</volume>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="2536" citStr="Freund et al., 1993" startWordPosition="368" endWordPosition="371"> for certain tasks. This paper investigates an approach for optimizing the supervised training (learning) phase, which reduces the annotation effort required to achieve a desired level of accuracy of the trained model. In this paper, we investigate and extend the committee-based sample selection approach to minimizing training cost (Dagan and Engelson, 1995). When using sample selection, a learning program examines many unlabeled (not annotated) examples, selecting for labeling only those that are most informative for the learner at each stage of training (Seung, Opper, and Sompolinsky, 1992; Freund et al., 1993; Lewis and Gale, 1994; Cohn, Atlas, and Ladner, 1994). This avoids redundantly annotating many examples that contribute roughly the same information to the learner. Our work focuses on sample selection for training probabilistic classifiers. In statistical NLP, probabilistic classifiers are often used to select a preferred analysis of the linguistic structure of a text (for example, its syntactic structure (Black et al., 1993), word categories (Church, 1988), or word senses (Gale, Church, and Yarowsky, 1993)). As a representative task for probabilistic classification in NLP, we experiment in </context>
<context position="7834" citStr="Freund et al., 1993" startWordPosition="1205" endWordPosition="1208">le by giving it to a committee containing several variant models, all &apos;consistent&apos; with the training data seen so far. The more the committee members agree on the classification of the example, the greater our certainty in its classification. This is because when the training data entails a specific classification with high certainty, most (in a probabilistic sense) classifiers consistent with the data will produce that classification. The committee-based approach was first proposed in a theoretical context for learning binary nonprobabilistic classifiers (Seung, Opper, and Sompolinsky, 1992; Freund et al., 1993). In this paper, we extend our previous work (Dagan and Engelson, 1995) where we applied the basic idea of the committee-based approach to probabilistic classification. Taking a Bayesian perspective, the posterior probability of a model, P(MIS), is determined given statistics S from the training set (and some prior distribution for the models). Committee members are then generated by drawing models randomly from POI IS). An example is selected for labeling if the committee members largely disagree on its classification. This procedure assumes that one can sample from the models&apos; posterior dist</context>
<context position="13637" citStr="Freund et al., 1993" startWordPosition="2147" endWordPosition="2150">ach sentence as an individual example. More generally, it is possible to break the text at any point where tagging is unambiguous. We thus use unambiguous words (those with only one possible part of speech) as example boundaries in bigram tagging. This allows us to train on smaller examples, focusing training more on the truly informative parts of the corpus. 5 Selection Algorithms Within the committee-based paradigm there exist different methods for selecting informative examples. Previous research in sample selection has used either sequential selection (Seung, Opper, and Sompolinsky, 1992; Freund et al., 1993; Dagan and Engelson, 1995), or batch selection (Lewis and Catlett, 1994; Lewis and Gale, 1994). We describe here general algorithms for both sequential and batch selection. Sequential selection examines unlabeled examples as they are supplied, one by one, and measures the disagreement in their classification by the committee. Those examples determined to be sufficiently informative are selected for training. Most simply, we can use a committee of size two and select an example when the two models disagree on its classification. This gives the following, parameter-free, two member sequential s</context>
<context position="24196" citStr="Freund et al., 1993" startWordPosition="3889" endWordPosition="3892">to computational efficiency. Figure 1(b) plots classification accuracy versus number of words examined, instead of those selected. We see that while all selective methods are less efficient in terms of examples examined than complete training, they are comparable to each other. Two member selection seems to have a clear, though small, advantage. In Figure 2 we investigate further the properties of batch selection. Figure 2(a) shows that accuracy increases with batch size only up to a point, and then starts to decrease. This result is in line with theoretical difficulties with batch selection (Freund et al., 1993) in that batch selection does not account for the distribution of input examples. Hence, once batch size increases past a point, the input distribution has too little influence on which examples are selected, and hence classification accuracy decreases. Furthermore, as batch size increases, computational efficiency, in terms of the number of examples examined to attain a given accuracy, decreases tremendously (Figure 2(b)). The ability of committee-based selection to focus on the more informative parts of the training corpus is analyzed in Figure 3. Here we examined the number of lexical and b</context>
</contexts>
<marker>Freund, Seung, Shamir, Tishby, 1993</marker>
<rawString>Freund, Y., H. S. Seung, E. Shamir, and N. Tishby. 1993. Information, prediction, and query by committee. In Advances in Neural Information Processing, volume 5. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Gale</author>
<author>Kenneth Church</author>
<author>David Yarowsky</author>
</authors>
<title>A method for disambiguating word senses in a large corpus. Computers and the Humanities,</title>
<date>1993</date>
<pages>26--415</pages>
<contexts>
<context position="3049" citStr="Gale, Church, and Yarowsky, 1993" startWordPosition="445" endWordPosition="449">re most informative for the learner at each stage of training (Seung, Opper, and Sompolinsky, 1992; Freund et al., 1993; Lewis and Gale, 1994; Cohn, Atlas, and Ladner, 1994). This avoids redundantly annotating many examples that contribute roughly the same information to the learner. Our work focuses on sample selection for training probabilistic classifiers. In statistical NLP, probabilistic classifiers are often used to select a preferred analysis of the linguistic structure of a text (for example, its syntactic structure (Black et al., 1993), word categories (Church, 1988), or word senses (Gale, Church, and Yarowsky, 1993)). As a representative task for probabilistic classification in NLP, we experiment in this paper with sample selection for the popular and well-understood method of stochastic part-of-speech tagging using Hidden Markov Models. We first review the basic approach of committeebased sample selection and its application to partof-speech tagging. This basic approach gives rise to a family of algorithms (including the original algorithm described in (Dagan and Engelson, 1995)) which we then describe. First, we describe the &apos;simplest&apos; committee-based selection algorithm, which has no parameters to tu</context>
<context position="32018" citStr="Gale, Church, and Yarowsky, 1993" startWordPosition="5107" endWordPosition="5111">rch Our results suggest applying committee-based sample selection to other statistical NLP tasks which rely on estimating probabilistic parameters from an annotated corpus. Statistical methods for these tasks typically assign a probability estimate, or some other statistical score, to each alternative analysis (a word sense, a category label, a parse tree, etc.), and then select the analysis with the highest score. The score is usually computed as a function of the estimates of several &apos;atomic&apos; parameters, often binomials or multinomials, such as: • In word sense disambiguation (Hearst, 1991; Gale, Church, and Yarowsky, 1993): P(slf), where s is a specific sense of the ambiguous word in question w, and f is a feature of occurrences of w. Common features are words in the context of w or morphological attributes of it. • In prepositional-phrase (PP) attachment (Hindle and Rooth, 1993): P(alf), where a is a possible attachment, such as an attachment to a head verb or noun, and f is a feature, or a combination of features, of the attachment. Corn325 mon features are the words involved in the attachment, such as the head verb or noun, the preposition, and the head word of the PP. • In statistical parsing (Black et al.</context>
</contexts>
<marker>Gale, Church, Yarowsky, 1993</marker>
<rawString>Gale, William, Kenneth Church, and David Yarowsky. 1993. A method for disambiguating word senses in a large corpus. Computers and the Humanities, 26:415-439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti Hearst</author>
</authors>
<title>Noun homograph disambiguation using local context in large text corpora.</title>
<date>1991</date>
<booktitle>In Proc. of the Annual Conference of the UW Center for the New OED and Text Research,</booktitle>
<pages>1--22</pages>
<contexts>
<context position="31984" citStr="Hearst, 1991" startWordPosition="5105" endWordPosition="5106"> Further research Our results suggest applying committee-based sample selection to other statistical NLP tasks which rely on estimating probabilistic parameters from an annotated corpus. Statistical methods for these tasks typically assign a probability estimate, or some other statistical score, to each alternative analysis (a word sense, a category label, a parse tree, etc.), and then select the analysis with the highest score. The score is usually computed as a function of the estimates of several &apos;atomic&apos; parameters, often binomials or multinomials, such as: • In word sense disambiguation (Hearst, 1991; Gale, Church, and Yarowsky, 1993): P(slf), where s is a specific sense of the ambiguous word in question w, and f is a feature of occurrences of w. Common features are words in the context of w or morphological attributes of it. • In prepositional-phrase (PP) attachment (Hindle and Rooth, 1993): P(alf), where a is a possible attachment, such as an attachment to a head verb or noun, and f is a feature, or a combination of features, of the attachment. Corn325 mon features are the words involved in the attachment, such as the head verb or noun, the preposition, and the head word of the PP. • In</context>
</contexts>
<marker>Hearst, 1991</marker>
<rawString>Hearst, Marti. 1991. Noun homograph disambiguation using local context in large text corpora. In Proc. of the Annual Conference of the UW Center for the New OED and Text Research, pages 1-22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Hindle</author>
<author>Mats Rooth</author>
</authors>
<title>Structural ambiguity and lexical relations.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--1</pages>
<contexts>
<context position="32281" citStr="Hindle and Rooth, 1993" startWordPosition="5154" endWordPosition="5158">cal score, to each alternative analysis (a word sense, a category label, a parse tree, etc.), and then select the analysis with the highest score. The score is usually computed as a function of the estimates of several &apos;atomic&apos; parameters, often binomials or multinomials, such as: • In word sense disambiguation (Hearst, 1991; Gale, Church, and Yarowsky, 1993): P(slf), where s is a specific sense of the ambiguous word in question w, and f is a feature of occurrences of w. Common features are words in the context of w or morphological attributes of it. • In prepositional-phrase (PP) attachment (Hindle and Rooth, 1993): P(alf), where a is a possible attachment, such as an attachment to a head verb or noun, and f is a feature, or a combination of features, of the attachment. Corn325 mon features are the words involved in the attachment, such as the head verb or noun, the preposition, and the head word of the PP. • In statistical parsing (Black et al., 1993): P(rlh), the probability of applying the rule r at a certain stage of the top down derivation of the parse tree given the history h of the derivation process. • In text categorization (Lewis and Gale, 1994; Iwayama and Tokunaga, 1994): P(tIC), where t is </context>
</contexts>
<marker>Hindle, Rooth, 1993</marker>
<rawString>Hindle, Donald and Mats Rooth. 1993. Structural ambiguity and lexical relations. Computational Linguistics, 19(1):103-120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Iwayama</author>
<author>T Tokunaga</author>
</authors>
<title>A probabilistic model for text categorization based on a single random variable with multiple values.</title>
<date>1994</date>
<booktitle>In Proceedings of the 4th Conference on Applied Natural Language Processing.</booktitle>
<contexts>
<context position="32860" citStr="Iwayama and Tokunaga, 1994" startWordPosition="5265" endWordPosition="5268">-phrase (PP) attachment (Hindle and Rooth, 1993): P(alf), where a is a possible attachment, such as an attachment to a head verb or noun, and f is a feature, or a combination of features, of the attachment. Corn325 mon features are the words involved in the attachment, such as the head verb or noun, the preposition, and the head word of the PP. • In statistical parsing (Black et al., 1993): P(rlh), the probability of applying the rule r at a certain stage of the top down derivation of the parse tree given the history h of the derivation process. • In text categorization (Lewis and Gale, 1994; Iwayama and Tokunaga, 1994): P(tIC), where t is a term in the document to be categorized, and C is a candidate category label. Applying committee-based selection to supervised training for such tasks can be done analogously to its application in the current papers. Furthermore, committee-based selection may be attempted also for training non-probabilistic classifiers, where explicit modeling of information gain is typically impossible. In such contexts, committee members might be generated by randomly varying some of the decisions made in the learning algorithm. Another important area for future work is in developing sa</context>
</contexts>
<marker>Iwayama, Tokunaga, 1994</marker>
<rawString>Iwayama, M. and T. Tokunaga. 1994. A probabilistic model for text categorization based on a single random variable with multiple values. In Proceedings of the 4th Conference on Applied Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Norman L Johnson</author>
</authors>
<title>Continuous Multivariate Distributions.</title>
<date>1972</date>
<publisher>John Wiley &amp; Sons,</publisher>
<location>New York.</location>
<contexts>
<context position="14595" citStr="Johnson, 1972" startWordPosition="2295" endWordPosition="2296">determined to be sufficiently informative are selected for training. Most simply, we can use a committee of size two and select an example when the two models disagree on its classification. This gives the following, parameter-free, two member sequential selection algorithm, executed for each unlabeled input example e: 1. Draw 2 models randomly from P(MIS), where S are statistics acquired from previously labeled examples; &apos;The normal approximation, while easy to implement, can be avoided. The posterior probability P(a, = adS) for the multinomial is given exactly by the Dirichlet distribution (Johnson, 1972) (which reduces to the Beta distribution in the binomial case). In this work we assumed a uniform prior distribution for each model parameter; we have not addressed the question of how to best choose a prior for this problem. 321 2. Classify e by each model, giving classifications c1 and c2; 3. If c1 c2, select e for annotation; 4. If e is selected, get its correct label and update S accordingly. This basic algorithm needs no parameters. If desired, it is possible to tune the frequency of selection, by changing the variance of P(MIS) (or the variance of P(ai = a IS) for each parameter), where </context>
</contexts>
<marker>Johnson, 1972</marker>
<rawString>Johnson, Norman L. 1972. Continuous Multivariate Distributions. John Wiley &amp; Sons, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian Kupiec</author>
</authors>
<title>Robust part-of-speech tagging using a hidden makov model.</title>
<date>1992</date>
<journal>Computer Speech and Language,</journal>
<pages>6--225</pages>
<contexts>
<context position="1716" citStr="Kupiec, 1992" startWordPosition="244" endWordPosition="245">. In particular, the simplest method, which has no parameters to tune, gives excellent results. We also show that sample selection yields a significant reduction in the size of the model used by the tagger. 1 Introduction Many corpus-based methods for natural language processing (NLP) are based on supervised training— acquiring information from a manually annotated corpus. Therefore, reducing annotation cost is an important research goal for statistical NLP. The ultimate reduction in annotation cost is achieved by unsupervised training methods, which do not require an annotated corpus at all (Kupiec, 1992; Merialdo, 1994; Elworthy, 1994). It has been shown, however, that some supervised training prior to the unsupervised phase is often beneficial. Indeed, fully unsupervised training may not be feasible for certain tasks. This paper investigates an approach for optimizing the supervised training (learning) phase, which reduces the annotation effort required to achieve a desired level of accuracy of the trained model. In this paper, we investigate and extend the committee-based sample selection approach to minimizing training cost (Dagan and Engelson, 1995). When using sample selection, a learni</context>
</contexts>
<marker>Kupiec, 1992</marker>
<rawString>Kupiec, Julian. 1992. Robust part-of-speech tagging using a hidden makov model. Computer Speech and Language, 6:225-242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Lewis</author>
<author>Jason Catlett</author>
</authors>
<title>Heterogeneous uncertainty sampling for supervised learning.</title>
<date>1994</date>
<booktitle>In Proc. Inel Conference on Machine Learning.</booktitle>
<contexts>
<context position="13709" citStr="Lewis and Catlett, 1994" startWordPosition="2158" endWordPosition="2161">e to break the text at any point where tagging is unambiguous. We thus use unambiguous words (those with only one possible part of speech) as example boundaries in bigram tagging. This allows us to train on smaller examples, focusing training more on the truly informative parts of the corpus. 5 Selection Algorithms Within the committee-based paradigm there exist different methods for selecting informative examples. Previous research in sample selection has used either sequential selection (Seung, Opper, and Sompolinsky, 1992; Freund et al., 1993; Dagan and Engelson, 1995), or batch selection (Lewis and Catlett, 1994; Lewis and Gale, 1994). We describe here general algorithms for both sequential and batch selection. Sequential selection examines unlabeled examples as they are supplied, one by one, and measures the disagreement in their classification by the committee. Those examples determined to be sufficiently informative are selected for training. Most simply, we can use a committee of size two and select an example when the two models disagree on its classification. This gives the following, parameter-free, two member sequential selection algorithm, executed for each unlabeled input example e: 1. Draw</context>
<context position="19104" citStr="Lewis and Catlett, 1994" startWordPosition="3079" endWordPosition="3082">ach batch B of N examples, is as follows: 1. For each example e in B: (a) Draw k models randomly from P(MIS); (b) Classify e by each model, giving classifications {ci}; (c) Measure the disagreement De for e over {c i}; 2. Select for annotation the m examples from B with the highest De; 3. Update S by the statistics of the selected examples. This procedure is repeated sequentially for successive batches of N examples, returning to the start of the corpus at the end. If N is equal to the size of the corpus, batch selection selects the m globally best examples in the corpus at each stage (as in (Lewis and Catlett, 1994)). On the other hand, as N decreases, batch selection becomes closer to sequential selection. 6 Experimental Results This section presents results of applying committeebased sample selection to bigram part-of-speech tagging, as compared with complete training on all examples in the corpus. Evaluation was performed using the University of Pennsylvania tagged corpus from the ACL/DCI CD-ROM I. For ease of implementation, we used a complete (closed) lexicon which contains all the words in the corpus. The committee-based sampling algorithm was initialized using the first 1,000 words from the corpus</context>
</contexts>
<marker>Lewis, Catlett, 1994</marker>
<rawString>Lewis, David D. and Jason Catlett. 1994. Heterogeneous uncertainty sampling for supervised learning. In Proc. Inel Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Lewis</author>
<author>William A Gale</author>
</authors>
<title>A sequential algorithm for training text classifiers.</title>
<date>1994</date>
<booktitle>In Proc. of the ACM SIGIR Conference.</booktitle>
<contexts>
<context position="2558" citStr="Lewis and Gale, 1994" startWordPosition="372" endWordPosition="375">his paper investigates an approach for optimizing the supervised training (learning) phase, which reduces the annotation effort required to achieve a desired level of accuracy of the trained model. In this paper, we investigate and extend the committee-based sample selection approach to minimizing training cost (Dagan and Engelson, 1995). When using sample selection, a learning program examines many unlabeled (not annotated) examples, selecting for labeling only those that are most informative for the learner at each stage of training (Seung, Opper, and Sompolinsky, 1992; Freund et al., 1993; Lewis and Gale, 1994; Cohn, Atlas, and Ladner, 1994). This avoids redundantly annotating many examples that contribute roughly the same information to the learner. Our work focuses on sample selection for training probabilistic classifiers. In statistical NLP, probabilistic classifiers are often used to select a preferred analysis of the linguistic structure of a text (for example, its syntactic structure (Black et al., 1993), word categories (Church, 1988), or word senses (Gale, Church, and Yarowsky, 1993)). As a representative task for probabilistic classification in NLP, we experiment in this paper with sample</context>
<context position="3926" citStr="Lewis and Gale (1994)" startWordPosition="579" endWordPosition="582">proach of committeebased sample selection and its application to partof-speech tagging. This basic approach gives rise to a family of algorithms (including the original algorithm described in (Dagan and Engelson, 1995)) which we then describe. First, we describe the &apos;simplest&apos; committee-based selection algorithm, which has no parameters to tune. We then generalize the selection scheme, allowing more options to adapt and tune the approach for specific tasks. The paper compares the performance of several instantiations of the general scheme, including a batch selection method similar to that of Lewis and Gale (1994). In particular, we found that the simplest version of the method achieves a significant reduction in annotation cost, comparable to that of other versions. 319 We also evaluate the computational efficiency of the different variants, and the number of unlabeled examples they consume. Finally, we study the effect of sample selection on the size of the model acquired by the learner. 2 Probabilistic Classification This section presents the framework and terminology assumed for probabilistic classification, as well as its instantiation for stochastic bigram part-ofspeech tagging. A probabilistic c</context>
<context position="6900" citStr="Lewis and Gale, 1994" startWordPosition="1065" endWordPosition="1068">orpus which provides a training set of labeled examples (see Section 4.1). 3 Evaluating Example Uncertainty A sample selection method needs to evaluate the expected usefulness, or information gain, of learning from a given example. The methods we investi&apos;This gives the Viterbi model (Merialdo, 1994), which we use here. 2This version of the method uses Bayes&apos; theorem (P(wslt,) cc P(4It7;)) (Church, 1988). gate approach this evaluation implicitly, measuring an example&apos;s informativeness as the uncertainty in its classification given the current training data (Seung, Opper, and Sompolinsky, 1992; Lewis and Gale, 1994; MacKay, 1992). The reasoning is that if an example&apos;s classification is uncertain given current training data then the example is likely to contain unknown information useful for classifying similar examples in the future. We investigate the committee-based method, where the learning algorithm evaluates an example by giving it to a committee containing several variant models, all &apos;consistent&apos; with the training data seen so far. The more the committee members agree on the classification of the example, the greater our certainty in its classification. This is because when the training data enta</context>
<context position="13732" citStr="Lewis and Gale, 1994" startWordPosition="2162" endWordPosition="2165">y point where tagging is unambiguous. We thus use unambiguous words (those with only one possible part of speech) as example boundaries in bigram tagging. This allows us to train on smaller examples, focusing training more on the truly informative parts of the corpus. 5 Selection Algorithms Within the committee-based paradigm there exist different methods for selecting informative examples. Previous research in sample selection has used either sequential selection (Seung, Opper, and Sompolinsky, 1992; Freund et al., 1993; Dagan and Engelson, 1995), or batch selection (Lewis and Catlett, 1994; Lewis and Gale, 1994). We describe here general algorithms for both sequential and batch selection. Sequential selection examines unlabeled examples as they are supplied, one by one, and measures the disagreement in their classification by the committee. Those examples determined to be sufficiently informative are selected for training. Most simply, we can use a committee of size two and select an example when the two models disagree on its classification. This gives the following, parameter-free, two member sequential selection algorithm, executed for each unlabeled input example e: 1. Draw 2 models randomly from</context>
<context position="27802" citStr="Lewis and Gale, 1994" startWordPosition="4466" endWordPosition="4469">they tend to select examples that affect parameters with the above three properties. Property 1 is addressed by randomly drawing the parameter values for committee members from the posterior distribution given the current statistics. When the statistics for a parameter are insufficient, the variance of the posterior distribution of the estimates is large, and hence there will be large differences in the values of the parameter chosen for different committee members. Note that property 1 is not addressed when uncertainty in classification is only judged relative to a single model7 (as in, eg, (Lewis and Gale, 1994)). Property 2 is addressed by selecting examples for which committee members highly disagree in classification (rather than measuring disagreement in parameter estimates). Committee-based selection thus addresses properties 1 and 2 simultaneously: it acquires statistics just when uncertainty in current parameter estimates entails uncertainty regarding the appropriate classification of the example. Our results show that this effect is achieved even when using only two committee members to sample the space of likely classifications. By appropriate classification we mean the classification given </context>
<context position="32831" citStr="Lewis and Gale, 1994" startWordPosition="5261" endWordPosition="5264">it. • In prepositional-phrase (PP) attachment (Hindle and Rooth, 1993): P(alf), where a is a possible attachment, such as an attachment to a head verb or noun, and f is a feature, or a combination of features, of the attachment. Corn325 mon features are the words involved in the attachment, such as the head verb or noun, the preposition, and the head word of the PP. • In statistical parsing (Black et al., 1993): P(rlh), the probability of applying the rule r at a certain stage of the top down derivation of the parse tree given the history h of the derivation process. • In text categorization (Lewis and Gale, 1994; Iwayama and Tokunaga, 1994): P(tIC), where t is a term in the document to be categorized, and C is a candidate category label. Applying committee-based selection to supervised training for such tasks can be done analogously to its application in the current papers. Furthermore, committee-based selection may be attempted also for training non-probabilistic classifiers, where explicit modeling of information gain is typically impossible. In such contexts, committee members might be generated by randomly varying some of the decisions made in the learning algorithm. Another important area for fu</context>
</contexts>
<marker>Lewis, Gale, 1994</marker>
<rawString>Lewis, David D. and William A. Gale. 1994. A sequential algorithm for training text classifiers. In Proc. of the ACM SIGIR Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David J C MacKay</author>
</authors>
<title>Information-based objective functions for active data selection.</title>
<date>1992</date>
<journal>Neural Computation,</journal>
<volume>4</volume>
<contexts>
<context position="6915" citStr="MacKay, 1992" startWordPosition="1069" endWordPosition="1070"> training set of labeled examples (see Section 4.1). 3 Evaluating Example Uncertainty A sample selection method needs to evaluate the expected usefulness, or information gain, of learning from a given example. The methods we investi&apos;This gives the Viterbi model (Merialdo, 1994), which we use here. 2This version of the method uses Bayes&apos; theorem (P(wslt,) cc P(4It7;)) (Church, 1988). gate approach this evaluation implicitly, measuring an example&apos;s informativeness as the uncertainty in its classification given the current training data (Seung, Opper, and Sompolinsky, 1992; Lewis and Gale, 1994; MacKay, 1992). The reasoning is that if an example&apos;s classification is uncertain given current training data then the example is likely to contain unknown information useful for classifying similar examples in the future. We investigate the committee-based method, where the learning algorithm evaluates an example by giving it to a committee containing several variant models, all &apos;consistent&apos; with the training data seen so far. The more the committee members agree on the classification of the example, the greater our certainty in its classification. This is because when the training data entails a specific </context>
</contexts>
<marker>MacKay, 1992</marker>
<rawString>MacKay, David J. C. 1992. Information-based objective functions for active data selection. Neural Computation, 4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Merialdo</author>
</authors>
<title>Tagging text with a probabilistic model.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<pages>20--2</pages>
<contexts>
<context position="1732" citStr="Merialdo, 1994" startWordPosition="246" endWordPosition="247">r, the simplest method, which has no parameters to tune, gives excellent results. We also show that sample selection yields a significant reduction in the size of the model used by the tagger. 1 Introduction Many corpus-based methods for natural language processing (NLP) are based on supervised training— acquiring information from a manually annotated corpus. Therefore, reducing annotation cost is an important research goal for statistical NLP. The ultimate reduction in annotation cost is achieved by unsupervised training methods, which do not require an annotated corpus at all (Kupiec, 1992; Merialdo, 1994; Elworthy, 1994). It has been shown, however, that some supervised training prior to the unsupervised phase is often beneficial. Indeed, fully unsupervised training may not be feasible for certain tasks. This paper investigates an approach for optimizing the supervised training (learning) phase, which reduces the annotation effort required to achieve a desired level of accuracy of the trained model. In this paper, we investigate and extend the committee-based sample selection approach to minimizing training cost (Dagan and Engelson, 1995). When using sample selection, a learning program exami</context>
<context position="6580" citStr="Merialdo, 1994" startWordPosition="1020" endWordPosition="1021">meters: transition probabilities P(ti—t) giving the probability of tag tj occuring after tag ti, lexical probabilities P(t1w) giving the probability of tag t labeling word w, and tag probabilities P(t) giving the marginal probability of a tag occurring.2 The values of these parameters are estimated from a tagged corpus which provides a training set of labeled examples (see Section 4.1). 3 Evaluating Example Uncertainty A sample selection method needs to evaluate the expected usefulness, or information gain, of learning from a given example. The methods we investi&apos;This gives the Viterbi model (Merialdo, 1994), which we use here. 2This version of the method uses Bayes&apos; theorem (P(wslt,) cc P(4It7;)) (Church, 1988). gate approach this evaluation implicitly, measuring an example&apos;s informativeness as the uncertainty in its classification given the current training data (Seung, Opper, and Sompolinsky, 1992; Lewis and Gale, 1994; MacKay, 1992). The reasoning is that if an example&apos;s classification is uncertain given current training data then the example is likely to contain unknown information useful for classifying similar examples in the future. We investigate the committee-based method, where the lea</context>
<context position="12048" citStr="Merialdo (1994)" startWordPosition="1894" endWordPosition="1895">s with positive counts and gives small positive estimates for values with a zero count. For simplicity, we describe here the approximation of P(ai = aiIS) for the unsmoothed estimator3. We approximate the posterior P(ai = ai IS) by first assuming that the multinomial is a collection of independent binomials, each of which corresponds to a single value ui of the multinomial; we then normalize the values so that they sum to 1. For each such binomial, we approximate P(ai = ai IS) as a trun3In the implementation we smooth the MLE by interpolation with a uniform probability distribution, following Merialdo (1994). Approximate adaptation of P(a, = a,IS) to the smoothed version of the estimator is simple. cated normal distribution (restricted to [0,1]), with estimated mean /./. =1*- and variance o-2 = To generate a particular multinomial distribution, we randomly choose values for the binomial parameters ai from their approximated posterior distributions (using the simple sampling method given in (Press et al., 1988, p. 214)), and renormalize them so that they sum to 1. Finally, to generate a random HMM given statistics S, we choose values independently for the parameters of each multinomial, since all </context>
<context position="26854" citStr="Merialdo, 1994" startWordPosition="4307" endWordPosition="4308">f the parameter is uncertain due to insufficient statistics in the training set. Additional statistics would bring the estimate closer to the true value. 2. Classification of examples is sensitive to changes in the current estimate of the parameter. Otherwise, even if the current value of the parameter is very uncertain, acquiring additional statistics will not change the resulting classifications. 3. The parameter affects classification for a large proportion of examples in the input. Parame6As noted above, we smooth the MLE estimates by interpolation with a uniform probability distribution (Merialdo, 1994). 7,1 0.86 0.87 0.88 0.89 0.9 0.91 0.92 0.93 Accuracy 1600 1400 1200 7). 1000 800 600 400 200 20030 18000 16000 14030 12000 10000 8000 6000 4000 2000 0 0.85 ; Two member selection Complete training /---- , - 324 ters that affect only few examples have low overall utility. The committee-based selection algorithms work because they tend to select examples that affect parameters with the above three properties. Property 1 is addressed by randomly drawing the parameter values for committee members from the posterior distribution given the current statistics. When the statistics for a parameter are</context>
</contexts>
<marker>Merialdo, 1994</marker>
<rawString>Merialdo, Bernard. 1994. Tagging text with a probabilistic model. Computational Linguistics, 20(2):155-172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William H Press</author>
<author>Brian P Flannery</author>
<author>Saul A Teukolsky</author>
<author>William T Vetterling</author>
</authors>
<title>Numerical Recipes in C.</title>
<date>1988</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="12457" citStr="Press et al., 1988" startWordPosition="1955" endWordPosition="1958">so that they sum to 1. For each such binomial, we approximate P(ai = ai IS) as a trun3In the implementation we smooth the MLE by interpolation with a uniform probability distribution, following Merialdo (1994). Approximate adaptation of P(a, = a,IS) to the smoothed version of the estimator is simple. cated normal distribution (restricted to [0,1]), with estimated mean /./. =1*- and variance o-2 = To generate a particular multinomial distribution, we randomly choose values for the binomial parameters ai from their approximated posterior distributions (using the simple sampling method given in (Press et al., 1988, p. 214)), and renormalize them so that they sum to 1. Finally, to generate a random HMM given statistics S, we choose values independently for the parameters of each multinomial, since all the different multinomials in an HMM are independent. 4.2 Examples in bigram training Typically, concept learning problems are formulated such that there is a set of training examples that are independent of each other. When training a bigram model (indeed, any HMM), this is not true, as each word is dependent on that before it. This problem is solved by considering each sentence as an individual example. </context>
</contexts>
<marker>Press, Flannery, Teukolsky, Vetterling, 1988</marker>
<rawString>Press, William H., Brian P. Flannery, Saul A. Teukolsky, and William T. Vetterling. 1988. Numerical Recipes in C. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H S Seung</author>
<author>M Opper</author>
<author>H Sompolinsky</author>
</authors>
<title>Query by committee.</title>
<date>1992</date>
<booktitle>In Proc. ACM Workshop on Computational Learning Theory.</booktitle>
<contexts>
<context position="2515" citStr="Seung, Opper, and Sompolinsky, 1992" startWordPosition="363" endWordPosition="367">pervised training may not be feasible for certain tasks. This paper investigates an approach for optimizing the supervised training (learning) phase, which reduces the annotation effort required to achieve a desired level of accuracy of the trained model. In this paper, we investigate and extend the committee-based sample selection approach to minimizing training cost (Dagan and Engelson, 1995). When using sample selection, a learning program examines many unlabeled (not annotated) examples, selecting for labeling only those that are most informative for the learner at each stage of training (Seung, Opper, and Sompolinsky, 1992; Freund et al., 1993; Lewis and Gale, 1994; Cohn, Atlas, and Ladner, 1994). This avoids redundantly annotating many examples that contribute roughly the same information to the learner. Our work focuses on sample selection for training probabilistic classifiers. In statistical NLP, probabilistic classifiers are often used to select a preferred analysis of the linguistic structure of a text (for example, its syntactic structure (Black et al., 1993), word categories (Church, 1988), or word senses (Gale, Church, and Yarowsky, 1993)). As a representative task for probabilistic classification in N</context>
<context position="6878" citStr="Seung, Opper, and Sompolinsky, 1992" startWordPosition="1059" endWordPosition="1064">ameters are estimated from a tagged corpus which provides a training set of labeled examples (see Section 4.1). 3 Evaluating Example Uncertainty A sample selection method needs to evaluate the expected usefulness, or information gain, of learning from a given example. The methods we investi&apos;This gives the Viterbi model (Merialdo, 1994), which we use here. 2This version of the method uses Bayes&apos; theorem (P(wslt,) cc P(4It7;)) (Church, 1988). gate approach this evaluation implicitly, measuring an example&apos;s informativeness as the uncertainty in its classification given the current training data (Seung, Opper, and Sompolinsky, 1992; Lewis and Gale, 1994; MacKay, 1992). The reasoning is that if an example&apos;s classification is uncertain given current training data then the example is likely to contain unknown information useful for classifying similar examples in the future. We investigate the committee-based method, where the learning algorithm evaluates an example by giving it to a committee containing several variant models, all &apos;consistent&apos; with the training data seen so far. The more the committee members agree on the classification of the example, the greater our certainty in its classification. This is because when </context>
<context position="13616" citStr="Seung, Opper, and Sompolinsky, 1992" startWordPosition="2141" endWordPosition="2146">is problem is solved by considering each sentence as an individual example. More generally, it is possible to break the text at any point where tagging is unambiguous. We thus use unambiguous words (those with only one possible part of speech) as example boundaries in bigram tagging. This allows us to train on smaller examples, focusing training more on the truly informative parts of the corpus. 5 Selection Algorithms Within the committee-based paradigm there exist different methods for selecting informative examples. Previous research in sample selection has used either sequential selection (Seung, Opper, and Sompolinsky, 1992; Freund et al., 1993; Dagan and Engelson, 1995), or batch selection (Lewis and Catlett, 1994; Lewis and Gale, 1994). We describe here general algorithms for both sequential and batch selection. Sequential selection examines unlabeled examples as they are supplied, one by one, and measures the disagreement in their classification by the committee. Those examples determined to be sufficiently informative are selected for training. Most simply, we can use a committee of size two and select an example when the two models disagree on its classification. This gives the following, parameter-free, tw</context>
</contexts>
<marker>Seung, Opper, Sompolinsky, 1992</marker>
<rawString>Seung, H. S., M. Opper, and H. Sompolinsky. 1992. Query by committee. In Proc. ACM Workshop on Computational Learning Theory.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>