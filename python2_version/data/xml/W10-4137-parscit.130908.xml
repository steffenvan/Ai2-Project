<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001623">
<title confidence="0.99791">
CRF-based Experiments for Cross-Domain Chinese
Word Segmentation at CIPS-SIGHAN-2010
</title>
<author confidence="0.999624">
Xiao Qin, Liang Zong, Yuqian Wu, Xiaojun Wan and Jianwu Yang
</author>
<affiliation confidence="0.9283125">
Institute of Computer Science and Technology
Peking University, China, 100871
</affiliation>
<email confidence="0.8576145">
{qinxiao,zongliang,wuyuqian,wanxiaojun,yangjianwu}
@cist.pku.edu.cn
</email>
<sectionHeader confidence="0.995274" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999261">
This paper describes our experiments on
the cross-domain Chinese word segmen-
tation task at the first CIPS-SIGHAN
Joint Conference on Chinese Language
Processing. Our system is based on the
Conditional Random Fields (CRFs)
model. Considering the particular prop-
erties of the out-of-domain data, we pro-
pose some novel steps to get some im-
provements for the special task.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999952545454545">
Chinese word segmentation is one of the most
important tasks in the field of Chinese informa-
tion processing and it is meaningful to intelligent
information processing technologies. After a lot
of researches, Chinese word segmentation has
achieved a high accuracy. Many methods have
been presented, among which the CRFs model
has attracted more and more attention. Zhao’s
group used the CRFs model in the task of Chi-
nese word segmentation in Bakeoff-4 and they
ranked at the top in all closed tests of word seg-
mentation (Zhao and Kit, 2008). The CRFs
model has been widely used because of its excel-
lent performance. However, finding a better
segmentation algorithm for the out-of-domain
text is the focus of CIP-SIGHAN-2010 bakeoff.
We still consider word segmentation as a se-
quence labeling problem. What we concern is
how to use the unlabeled corpora to enrich the
supervised CRFs learning. So we take some
strategies to make use of the information of the
texts in the unlabeled corpora.
</bodyText>
<sectionHeader confidence="0.990141" genericHeader="method">
2 System Description
</sectionHeader>
<bodyText confidence="0.9997315">
In this section, we will describe our system in
details. The system is based on the CRFs model
and we propose some novel steps for some im-
provements. It mainly consists of three steps:
preprocessing, CRF-based labeling, and re-
labeling.
</bodyText>
<subsectionHeader confidence="0.959393">
2.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.995924">
This step mainly includes two operations. First,
we should cut the whole text into a series of sen-
tences. We regard ‘o’, ‘?’, ‘!’ and ‘;’ as the
symbols of the boundary between sentences.
Then we do atomic segmentation to all the sen-
tences. Here Atomic segmentation represents
that we should regard the continuous non-
Chinese characters as a whole. Take the word
‘computer’ as an example, we should regard
‘computer’ as a whole, but not treat it as 8 sepa-
rate letters of ‘c’, ‘o’, ‘m’, ‘p’, ‘u’, ‘t’, ‘e’, and
‘r’.
</bodyText>
<subsectionHeader confidence="0.999198">
2.2 CRF-based Labeling
</subsectionHeader>
<bodyText confidence="0.998459625">
Conditional random field (CRF) is an extension
of both Maximum Entropy Model (MEMs) and
Hidden Markov Models (HMMs), which was
firstly introduced by Lafferty (Lafferty et al.,
2001). It is an undirected graphical model
trained to maximize the conditional probability
of the desired outputs given the corresponding
inputs. This model has achieved great successes
in word segmentation.
In the CRFs model, the conditional distribu-
tion P(y|x) of the labels Y givens observations X
directly is defined:
y is the label sequence, x is observation sequence,
Zx is a normalization term that makes the proba-
bility of all state sequences sum to one; fk(yt-1, yt,
t) is often a binary-valued feature function and
λ k is the weight of fk.
In our system, we choose six types of tags ac-
cording to character position in a word. Accord-
ing to Zhao’s work (Zhao et al., 2006a), the 6-
tag set enables our system to generate a better
CRF model than the 4-tag set. In our experi-
ments, we test both the 6-tag set and the 4-tag
set, and the 6-tag set truly has a better result. The
6-tag set is defined as below:
T = {B, B2, B3, M, E, S}
Here B, B2, B3, M, E represent the first,
second, third, continuing and end character posi-
tions in a multi-character word, and S is the sin-
gle-character word tag.
We adopt 6 n-gram feature templates as fea-
tures. Some researches have proved that the
combination of 6-tag set and 6 n-gram feature
template can achieve a better performance (Zhao
et al., 2006a; Zhao et al., 2006b; Zhao and Kit,
2007).
The 6 n-gram feature templates used in our
system are C-1, C0, C1, C-1C0, C0C1, C-1C1. Here
C stands for a character and the subscripts -1, 0
and 1 stand for the previous, current and next
character, respectively.
Furthermore, we try to take advantage of the
types for the characters. For example, in our sys-
tem D stands for the date, N stands for the num-
ber, L stands for the letter, P stands for the punc-
tuation and C stands for the other characters.
Introducing these features is beneficial to the
CRFs learning.
</bodyText>
<subsectionHeader confidence="0.997563">
2.3 Re-labeling step
</subsectionHeader>
<bodyText confidence="0.999953740740741">
Since the unlabeled corpora belong to different
domains, traditional methods have some limita-
tions. In this section, we propose an additional
step to make good use of the unlabelled data for
this special task. This step is based on the out-
puts of the CRFs model in the previous step.
After CRFs learning, we get a training mod-
el. With this model, we can label the literature,
computer, medicine and finance corpora. Ac-
cording to the outputs of the CRFs model, we
choose some labeled sentences with high confi-
dence and add them to the training corpus. Here
the selection of high confidence must guarantee
that the probability of the sentences selected be-
ing correct segmentations is rather high and the
number of the sentences selected is not too little
or they will make no difference to the generation
of the new CRF model. Since the existing train-
ing model does not contain the information in
the out-of-domain data, we treat the labeled sen-
tences with high confidence as additional train-
ing corpus. Then we re-train the CRFs model
with the new training data. With the training da-
ta extracted from different domains, the training
model incorporates more cross-domain informa-
tion and it can work better in the corresponding
cross-domain prediction task.
</bodyText>
<sectionHeader confidence="0.999884" genericHeader="method">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999783">
3.1 Experiment Setup
</subsectionHeader>
<bodyText confidence="0.999948631578947">
There are two sources for the corpora: the train-
ing corpora and the test corpus. And in the train-
ing corpora, there exist two types of corpus in
this task. The labeled corpus is Chinese text
which has been segmented into words while the
unlabelled corpus covers two domains: literature
and computer science. The test corpus contains 4
domains, which are literature, computer science,
medicine and finance.
There are four evaluation metrics used in
this bake-off task: Precision, Recall, F1 measure
(F1 = 2RP/(R+P)) and OOV measure, where R
and P are the recall and precision of the segmen-
tation and OOV (Out-Of-Vocabulary Word) is a
word which occurs in the reference corpus but
does not occur in the labeled training corpus.
Our system uses the CRF++ package Ver-
sion 0.49 implemented by Taku Kudo 1 from
sourceforge.
</bodyText>
<subsectionHeader confidence="0.889495">
3.2 Results and Discussions
</subsectionHeader>
<bodyText confidence="0.999957666666667">
We test the techniques described in section 2
with the given data. Now we will show the re-
sults of each operation.
</bodyText>
<subsectionHeader confidence="0.599056">
3.2.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.9905905">
As we have mentioned in section 2.1, the first
step is to cut the text into a series of sentences.
</bodyText>
<footnote confidence="0.833576">
1 http://crfpp.sourceforge.net/
</footnote>
<bodyText confidence="0.998812625">
Then we should give each character in one sen-
tence a label. Before this step, it is necessary to
do atomic segmentation. And we will regard the
continuous non-Chinese characters as a whole
and give the whole part a single label. This is
meaningful to those corpora containing a lot of
English words. Due to the diversity of the Eng-
lish words, segmenting the sentences with a lot
non-Chinese characters correctly is rather diffi-
cult only through CRF learning. We should do
atomic segmentation to all training and test cor-
pora. This may achieve a higher accuracy in a
certain degree.
The results of word segmentation are re-
ported in Table 1. ‘Clouse+/-’ indicates whether
text clause has been done.
</bodyText>
<tableCaption confidence="0.999908">
Table 1: Results with clause and without clause
</tableCaption>
<table confidence="0.999694444444444">
corpus Precision Recall F
Literature Clause+ 0.922 0.916 0.919
Clause- 0.921 0.915 0.918
Computer Clause+ 0.934 0.939 0.937
Clause- 0.934 0.939 0.936
Medicine Clause+ 0.911 0.917 0.914
Clause- 0.509 0.511 0.510
Finance Clause+ 0.940 0.943 0.941
Clause- 0.933 0.940 0.937
</table>
<bodyText confidence="0.9955998">
From Table 1, we can see there is some im-
provement in different degree and the effect in
the medicine corpus is the most obvious. So we
can conclude that our preprocessing is useful to
the word segmentation.
</bodyText>
<subsectionHeader confidence="0.504039">
3.2.2 CRF-based labeling
</subsectionHeader>
<bodyText confidence="0.999300714285714">
After preprocessing, we can use CRF++ package
to learn and test.
The selection of feature template is also an
important factor. For the purpose of comparison,
we test two kinds of feature templates in our sys-
tem. The one is showed in Table 2 and the other
one is showed in Table 3.
</bodyText>
<tableCaption confidence="0.878116">
Table 2: Template 1
</tableCaption>
<figure confidence="0.8831276">
# Unigram
U00:%x[-1,0]
U01:%x[0,0]
U02:%x[1,0]
U03:%x[-1,0]/%x[0,0]
U04:%x[0,0]/%x[1,0]
U05:%x[-1,0]/%x[1,0]
# Bigram
B
Table 3: Template 2
# Unigram
U00:%x[-1,0]
U01:%x[0,0]
U02:%x[1,0]
U03:%x[-1,0]/%x[0,0]
U04:%x[0,0]/%x[1,0]
U05:%x[-1,0]/%x[1,0]
U10:%x[-1,1]
U11:%x[0,1]
U12:%x[1,1]
U13:%x[-1,1]/%x[0,1]
U14:%x[0,1]/%x[1,1]
U15:%x[-1,1]/%x[1,1]
# Bigram
B
</figure>
<bodyText confidence="0.999654">
Now we will explain the meanings of the
templates. Here is an example. In table 4, we
show the format of the input file. The first col-
umn represents the word itself and the second
represents the feature of the word, where there
are five kinds of features: date (D), number (N),
letter (L), punctuation (P) and others (C). The
meanings of the templates are showed in table 5.
</bodyText>
<tableCaption confidence="0.866407">
Table 4: the format of the input file for CRF
</tableCaption>
<equation confidence="0.94522125">
Mfr C
_ D
i4 C
iii C
( P
ffff, C
Q C
J_ C
</equation>
<footnote confidence="0.648292666666667">
1 N
4K C
� P
</footnote>
<tableCaption confidence="0.993215">
Table 5: the example of the templates
</tableCaption>
<table confidence="0.867899857142857">
template Expanded feature
%x[0,0] Q
%x[0,1] C
%x[1,0] J`r
%x[-1,0] ffj
%x[-1,0]/ %x[0,0] ffj/Q
%x[0,0]/ %x[0,1] Q/C
</table>
<bodyText confidence="0.8800238">
With two different feature templates, we con-
tinue our experiments in the four different do-
mains. The segmentation performances of our
system on test corpora using different feature
templates are presented in Table 6.
</bodyText>
<tableCaption confidence="0.998813">
Table 6: Results with different feature templates
</tableCaption>
<table confidence="0.999167555555555">
corpus Precision Recall F
Literature T1 0.917 0.909 0.913
T2 0.922 0.916 0.919
Computer T1 0.914 0.902 0.908
T2 0.934 0.939 0.937
Medicine T1 0.906 0.905 0.905
T2 0.911 0.917 0.914
Finance T1 0.937 0.925 0.931
T2 0.940 0.943 0.941
</table>
<bodyText confidence="0.993487875">
Here T1 stands for Template 1 while T2
stands for Template 2.
From the Table 4 we can see the second fea-
ture templates make the results of the segmenta-
tion improved more significantly.
At the same time we need get the outputs with
confidence measure by setting some parameters
in CRF test.
</bodyText>
<subsectionHeader confidence="0.992637">
3.2.3 Re-labeling
</subsectionHeader>
<bodyText confidence="0.999673941176471">
As for the outputs with confidence measure
generated by previous step, we should do some
special processes. Here we set a particular value
as our standard and choose the sentences with
confidence above the value. As we know, the
test corpora are limited, the higher confidence
may cause the corpora meeting our requirements
are less. The lower confidence may not guaran-
tee the reliability. So the setting of the confi-
dence value is very significant. In our experi-
ments, we set the parameter at 0.8.
Then we add the sentences whose confidence
is above 0.8 to the training corpus. We should
re-learn with new corpora, generate the new
model and re-test the corpora related with 4 do-
mains. The segmentation performances after re-
labeling are represented in Table 7.
</bodyText>
<tableCaption confidence="0.7575675">
Table 7: Results with re-labeling and without re-
labeling
</tableCaption>
<table confidence="0.999748555555555">
corpus Precision Recall F
Literature Re + 0.922 0.916 0.919
Re - 0.921 0.916 0.918
Computer Re + 0.934 0.939 0.937
Re - 0.932 0.934 0.933
Medicine Re + 0.911 0.917 0.914
Re - 0.912 0.918 0.915
Finance Re + 0.940 0.943 0.941
Re - 0.937 0.941 0.939
</table>
<bodyText confidence="0.927708">
Here Re+/- indicates whether the re-labeling
step is to be done.
From the results we know, even though the re-
labeling step makes the results in the medicine
corpus a little worse, it has much better effect in
the other corpora. Overall, the operation of re-
labeling is necessary.
</bodyText>
<subsectionHeader confidence="0.998638">
3.3 Our results in this bakeoff
</subsectionHeader>
<bodyText confidence="0.996324">
In this task, our results are showed in Table 8.
</bodyText>
<tableCaption confidence="0.984511">
Table 8: our results in this bakeoff
</tableCaption>
<table confidence="0.996883">
Precision Recall F
Literature 0.922 0.916 0.919
Computer 0.934 0.939 0.937
Medicine 0.911 0.917 0.914
Finance 0.940 0.943 0.941
</table>
<bodyText confidence="0.997204">
From Table 6, we can see our system can
achieve a high precision, especially in the do-
mains of computer and finance. This proves our
methods are fairly effective.
</bodyText>
<sectionHeader confidence="0.999943" genericHeader="method">
4 Discussion
</sectionHeader>
<subsectionHeader confidence="0.998469">
4.1 Segmentation Features
</subsectionHeader>
<bodyText confidence="0.9999826">
In our system, we only take advantage of the
features of the words. We try to add other fea-
tures to our experiments such as AV feature
(Feng et al., 2004a; Feng et al., 2004b; Hai Zhao
et al., 2007) with the expectation of improving
the results. But the results are not satisfying. We
believe that the feature of words frequency may
be an important factor, but how to use it is worth
studying. So finding some meaningful and effec-
tive features is the crucial point.
</bodyText>
<subsectionHeader confidence="0.741907">
4.2 OOV
</subsectionHeader>
<bodyText confidence="0.999975">
In our system, we do not process the words
out of vocabulary in the special way. The recog-
nition of OOV is still a problem. In a word, there
is still much to be done to improve our system.
In the present work, we make use of some sur-
face features, and further study should be con-
tinued to find more effective features.
</bodyText>
<sectionHeader confidence="0.999448" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999973454545455">
In this paper, we have briefly described the
Chinese word segmentation for out-of-domain
texts. The CRFs model is implemented. In order
to make the best use of the test corpora, some
special strategies are introduced. Further im-
provement is made with these strategies. How-
ever, there is still much to do to achieve more
improvement. From the results, we got good ex-
perience and knew the weaknesses of our system.
These all help to improve the performance of our
system in the future.
</bodyText>
<sectionHeader confidence="0.996624" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9719605">
The research described in this paper was sup-
ported by NSFC (Grant No. 60875033).
</bodyText>
<sectionHeader confidence="0.99918" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9997524375">
Hai Zhao, Changning Huang, and Mu Li. 2006. An
improved Chinese Word Segmentation System
with Conditional Random Field. Proceedings of
the Fifth SIGHAN Workshop on Chinese Language
Processing, Sydney, Australia.
Hai Zhao and Chunyu Kit. 2007. Incorporating global
information into supervised for Chinese word
segmentation. In PACALING-2007, Melbourne,
Australia.
Hai Zhao, Changning Huang, Mu Li and Bao-Liang
Lu. 2006b. Effective tag set selection in Chinese
word segmentation via conditional random field
modeling. In PACLIC-20, Wuhan, China.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling se-
quence data. In Proceeding of ICML 2001, Mor-
gan Kaufmann, San Francisco, CA
Hai Zhao and Chunyu Kit. 2008. Unsupervised Seg-
mentation Helps Supervised Learning of Character
Tagging for Word Segmentation and Named Enti-
ty Recognition. Processing of the Sixth SIGHAN
Workshop on Chinese Language Processing, Hy-
derabad, India.
Haodi Feng, Kang Chen, Xiaotie Deng, and Weimin
Zheng. 2004a. Accessor variety criteria for Chi-
nese word extraction. Computational Linguistics.
Haodi Feng, Kang Chen, Chunyu Kit, and Xiaotie
Deng. 2004b. Unsupervised segmentation of Chi-
nese corpus using accessor variety. In First Inter-
national Joint Conference on Natural Language
Processing. Sanya, Hainan Island, China.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.222695">
<title confidence="0.655103">CRF-based Experiments for Cross-Domain Word Segmentation at CIPS-SIGHAN-2010</title>
<author confidence="0.550307">Xiao Qin</author>
<author confidence="0.550307">Liang Zong</author>
<author confidence="0.550307">Yuqian Wu</author>
<author confidence="0.550307">Xiaojun Wan</author>
<author confidence="0.550307">Jianwu</author>
<affiliation confidence="0.747652">Institute of Computer Science and</affiliation>
<address confidence="0.62066">Peking University, China, 100871</address>
<email confidence="0.817311">@cist.pku.edu.cn</email>
<abstract confidence="0.904333454545454">This paper describes our experiments on the cross-domain Chinese word segmentation task at the first CIPS-SIGHAN Joint Conference on Chinese Language Processing. Our system is based on the Conditional Random Fields (CRFs) model. Considering the particular properties of the out-of-domain data, we propose some novel steps to get some improvements for the special task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Changning Huang</author>
<author>Mu Li</author>
</authors>
<title>An improved Chinese Word Segmentation System with Conditional Random Field.</title>
<date>2006</date>
<booktitle>Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="3341" citStr="Zhao et al., 2006" startWordPosition="539" endWordPosition="542"> the conditional probability of the desired outputs given the corresponding inputs. This model has achieved great successes in word segmentation. In the CRFs model, the conditional distribution P(y|x) of the labels Y givens observations X directly is defined: y is the label sequence, x is observation sequence, Zx is a normalization term that makes the probability of all state sequences sum to one; fk(yt-1, yt, t) is often a binary-valued feature function and λ k is the weight of fk. In our system, we choose six types of tags according to character position in a word. According to Zhao’s work (Zhao et al., 2006a), the 6- tag set enables our system to generate a better CRF model than the 4-tag set. In our experiments, we test both the 6-tag set and the 4-tag set, and the 6-tag set truly has a better result. The 6-tag set is defined as below: T = {B, B2, B3, M, E, S} Here B, B2, B3, M, E represent the first, second, third, continuing and end character positions in a multi-character word, and S is the single-character word tag. We adopt 6 n-gram feature templates as features. Some researches have proved that the combination of 6-tag set and 6 n-gram feature template can achieve a better performance (Zh</context>
</contexts>
<marker>Zhao, Huang, Li, 2006</marker>
<rawString>Hai Zhao, Changning Huang, and Mu Li. 2006. An improved Chinese Word Segmentation System with Conditional Random Field. Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chunyu Kit</author>
</authors>
<title>Incorporating global information into supervised for Chinese word segmentation.</title>
<date>2007</date>
<booktitle>In PACALING-2007,</booktitle>
<location>Melbourne, Australia.</location>
<contexts>
<context position="3998" citStr="Zhao and Kit, 2007" startWordPosition="664" endWordPosition="667">to generate a better CRF model than the 4-tag set. In our experiments, we test both the 6-tag set and the 4-tag set, and the 6-tag set truly has a better result. The 6-tag set is defined as below: T = {B, B2, B3, M, E, S} Here B, B2, B3, M, E represent the first, second, third, continuing and end character positions in a multi-character word, and S is the single-character word tag. We adopt 6 n-gram feature templates as features. Some researches have proved that the combination of 6-tag set and 6 n-gram feature template can achieve a better performance (Zhao et al., 2006a; Zhao et al., 2006b; Zhao and Kit, 2007). The 6 n-gram feature templates used in our system are C-1, C0, C1, C-1C0, C0C1, C-1C1. Here C stands for a character and the subscripts -1, 0 and 1 stand for the previous, current and next character, respectively. Furthermore, we try to take advantage of the types for the characters. For example, in our system D stands for the date, N stands for the number, L stands for the letter, P stands for the punctuation and C stands for the other characters. Introducing these features is beneficial to the CRFs learning. 2.3 Re-labeling step Since the unlabeled corpora belong to different domains, trad</context>
</contexts>
<marker>Zhao, Kit, 2007</marker>
<rawString>Hai Zhao and Chunyu Kit. 2007. Incorporating global information into supervised for Chinese word segmentation. In PACALING-2007, Melbourne, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Changning Huang</author>
<author>Mu Li</author>
<author>Bao-Liang Lu</author>
</authors>
<title>Effective tag set selection in Chinese word segmentation via conditional random field modeling.</title>
<date>2006</date>
<booktitle>In PACLIC-20,</booktitle>
<location>Wuhan, China.</location>
<contexts>
<context position="3341" citStr="Zhao et al., 2006" startWordPosition="539" endWordPosition="542"> the conditional probability of the desired outputs given the corresponding inputs. This model has achieved great successes in word segmentation. In the CRFs model, the conditional distribution P(y|x) of the labels Y givens observations X directly is defined: y is the label sequence, x is observation sequence, Zx is a normalization term that makes the probability of all state sequences sum to one; fk(yt-1, yt, t) is often a binary-valued feature function and λ k is the weight of fk. In our system, we choose six types of tags according to character position in a word. According to Zhao’s work (Zhao et al., 2006a), the 6- tag set enables our system to generate a better CRF model than the 4-tag set. In our experiments, we test both the 6-tag set and the 4-tag set, and the 6-tag set truly has a better result. The 6-tag set is defined as below: T = {B, B2, B3, M, E, S} Here B, B2, B3, M, E represent the first, second, third, continuing and end character positions in a multi-character word, and S is the single-character word tag. We adopt 6 n-gram feature templates as features. Some researches have proved that the combination of 6-tag set and 6 n-gram feature template can achieve a better performance (Zh</context>
</contexts>
<marker>Zhao, Huang, Li, Lu, 2006</marker>
<rawString>Hai Zhao, Changning Huang, Mu Li and Bao-Liang Lu. 2006b. Effective tag set selection in Chinese word segmentation via conditional random field modeling. In PACLIC-20, Wuhan, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceeding of ICML 2001,</booktitle>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco, CA</location>
<contexts>
<context position="2667" citStr="Lafferty et al., 2001" startWordPosition="422" endWordPosition="425">entences. We regard ‘o’, ‘?’, ‘!’ and ‘;’ as the symbols of the boundary between sentences. Then we do atomic segmentation to all the sentences. Here Atomic segmentation represents that we should regard the continuous nonChinese characters as a whole. Take the word ‘computer’ as an example, we should regard ‘computer’ as a whole, but not treat it as 8 separate letters of ‘c’, ‘o’, ‘m’, ‘p’, ‘u’, ‘t’, ‘e’, and ‘r’. 2.2 CRF-based Labeling Conditional random field (CRF) is an extension of both Maximum Entropy Model (MEMs) and Hidden Markov Models (HMMs), which was firstly introduced by Lafferty (Lafferty et al., 2001). It is an undirected graphical model trained to maximize the conditional probability of the desired outputs given the corresponding inputs. This model has achieved great successes in word segmentation. In the CRFs model, the conditional distribution P(y|x) of the labels Y givens observations X directly is defined: y is the label sequence, x is observation sequence, Zx is a normalization term that makes the probability of all state sequences sum to one; fk(yt-1, yt, t) is often a binary-valued feature function and λ k is the weight of fk. In our system, we choose six types of tags according to</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceeding of ICML 2001, Morgan Kaufmann, San Francisco, CA</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chunyu Kit</author>
</authors>
<title>Unsupervised Segmentation Helps Supervised Learning of Character Tagging for Word Segmentation and Named Entity Recognition.</title>
<date>2008</date>
<booktitle>Processing of the Sixth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<location>Hyderabad, India.</location>
<contexts>
<context position="1224" citStr="Zhao and Kit, 2008" startWordPosition="179" endWordPosition="182"> some novel steps to get some improvements for the special task. 1 Introduction Chinese word segmentation is one of the most important tasks in the field of Chinese information processing and it is meaningful to intelligent information processing technologies. After a lot of researches, Chinese word segmentation has achieved a high accuracy. Many methods have been presented, among which the CRFs model has attracted more and more attention. Zhao’s group used the CRFs model in the task of Chinese word segmentation in Bakeoff-4 and they ranked at the top in all closed tests of word segmentation (Zhao and Kit, 2008). The CRFs model has been widely used because of its excellent performance. However, finding a better segmentation algorithm for the out-of-domain text is the focus of CIP-SIGHAN-2010 bakeoff. We still consider word segmentation as a sequence labeling problem. What we concern is how to use the unlabeled corpora to enrich the supervised CRFs learning. So we take some strategies to make use of the information of the texts in the unlabeled corpora. 2 System Description In this section, we will describe our system in details. The system is based on the CRFs model and we propose some novel steps fo</context>
</contexts>
<marker>Zhao, Kit, 2008</marker>
<rawString>Hai Zhao and Chunyu Kit. 2008. Unsupervised Segmentation Helps Supervised Learning of Character Tagging for Word Segmentation and Named Entity Recognition. Processing of the Sixth SIGHAN Workshop on Chinese Language Processing, Hyderabad, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haodi Feng</author>
<author>Kang Chen</author>
<author>Xiaotie Deng</author>
<author>Weimin Zheng</author>
</authors>
<title>Accessor variety criteria for Chinese word extraction. Computational Linguistics.</title>
<date>2004</date>
<contexts>
<context position="12231" citStr="Feng et al., 2004" startWordPosition="2067" endWordPosition="2070">g is necessary. 3.3 Our results in this bakeoff In this task, our results are showed in Table 8. Table 8: our results in this bakeoff Precision Recall F Literature 0.922 0.916 0.919 Computer 0.934 0.939 0.937 Medicine 0.911 0.917 0.914 Finance 0.940 0.943 0.941 From Table 6, we can see our system can achieve a high precision, especially in the domains of computer and finance. This proves our methods are fairly effective. 4 Discussion 4.1 Segmentation Features In our system, we only take advantage of the features of the words. We try to add other features to our experiments such as AV feature (Feng et al., 2004a; Feng et al., 2004b; Hai Zhao et al., 2007) with the expectation of improving the results. But the results are not satisfying. We believe that the feature of words frequency may be an important factor, but how to use it is worth studying. So finding some meaningful and effective features is the crucial point. 4.2 OOV In our system, we do not process the words out of vocabulary in the special way. The recognition of OOV is still a problem. In a word, there is still much to be done to improve our system. In the present work, we make use of some surface features, and further study should be con</context>
</contexts>
<marker>Feng, Chen, Deng, Zheng, 2004</marker>
<rawString>Haodi Feng, Kang Chen, Xiaotie Deng, and Weimin Zheng. 2004a. Accessor variety criteria for Chinese word extraction. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haodi Feng</author>
<author>Kang Chen</author>
<author>Chunyu Kit</author>
<author>Xiaotie Deng</author>
</authors>
<title>Unsupervised segmentation of Chinese corpus using accessor variety.</title>
<date>2004</date>
<booktitle>In First International Joint Conference on Natural Language Processing.</booktitle>
<location>Sanya, Hainan Island, China.</location>
<contexts>
<context position="12231" citStr="Feng et al., 2004" startWordPosition="2067" endWordPosition="2070">g is necessary. 3.3 Our results in this bakeoff In this task, our results are showed in Table 8. Table 8: our results in this bakeoff Precision Recall F Literature 0.922 0.916 0.919 Computer 0.934 0.939 0.937 Medicine 0.911 0.917 0.914 Finance 0.940 0.943 0.941 From Table 6, we can see our system can achieve a high precision, especially in the domains of computer and finance. This proves our methods are fairly effective. 4 Discussion 4.1 Segmentation Features In our system, we only take advantage of the features of the words. We try to add other features to our experiments such as AV feature (Feng et al., 2004a; Feng et al., 2004b; Hai Zhao et al., 2007) with the expectation of improving the results. But the results are not satisfying. We believe that the feature of words frequency may be an important factor, but how to use it is worth studying. So finding some meaningful and effective features is the crucial point. 4.2 OOV In our system, we do not process the words out of vocabulary in the special way. The recognition of OOV is still a problem. In a word, there is still much to be done to improve our system. In the present work, we make use of some surface features, and further study should be con</context>
</contexts>
<marker>Feng, Chen, Kit, Deng, 2004</marker>
<rawString>Haodi Feng, Kang Chen, Chunyu Kit, and Xiaotie Deng. 2004b. Unsupervised segmentation of Chinese corpus using accessor variety. In First International Joint Conference on Natural Language Processing. Sanya, Hainan Island, China.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>