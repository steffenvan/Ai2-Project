<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.684052">
On Reverse Feature Engineering of Syntactic Tree Kernels
</title>
<author confidence="0.784875">
Daniele Pighin
</author>
<affiliation confidence="0.746916">
FBK-irst, DISI, University of Trento
</affiliation>
<address confidence="0.470929">
Via di Sommarive, 14
I-38123 Povo (TN) Italy
</address>
<email confidence="0.993934">
daniele.pighin@gmail.com
</email>
<sectionHeader confidence="0.994704" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9996804">
In this paper, we provide a theoretical
framework for feature selection in tree ker-
nel spaces based on gradient-vector com-
ponents of kernel-based machines. We
show that a huge number of features can
be discarded without a significant decrease
in accuracy. Our selection algorithm is as
accurate as and much more efficient than
those proposed in previous work. Com-
parative experiments on three interesting
and very diverse classification tasks, i.e.
Question Classification, Relation Extrac-
tion and Semantic Role Labeling, support
our theoretical findings and demonstrate
the algorithm performance.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999969318181818">
Kernel functions are very effective at modeling
diverse linguistic phenomena by implicitly rep-
resenting data in high dimensional spaces, e.g.
(Cumby and Roth, 2003; Culotta and Sorensen,
2004; Kudo et al., 2005; Moschitti et al., 2008).
However, the implicit nature of the kernel space
causes two major drawbacks: (1) high computa-
tional costs for learning and classification, and (2)
the impossibility to identify the most important
features. A solution to both problems is the ap-
plication of feature selection techniques.
In particular, the problem of feature selection
in Tree Kernel (TK) spaces has already been ad-
dressed by previous work in NLP, e.g. (Kudo
and Matsumoto, 2003; Suzuki and Isozaki, 2005).
However, these approaches lack a theoretical char-
acterization of the problem that could support and
justify the design of more effective algorithms.
In (Pighin and Moschitti, 2009a) and (Pighin
and Moschitti, 2009b) (P&amp;M), we presented a
heuristic framework for feature selection in kernel
spaces that selects features based on the compo-
</bodyText>
<note confidence="0.98983375">
Alessandro Moschitti
DISI, University of Trento
Via di Sommarive, 14
I-38123 Povo (TN) Italy
</note>
<email confidence="0.976628">
moschitti@disi.unitn.it
</email>
<bodyText confidence="0.9999019">
nents of the weight vector, iV, optimized by Sup-
port Vector Machines (SVMs). This method ap-
pears to be very effective, as the model accuracy
does not significantly decrease even when a large
number of features are filtered out. Unfortunately,
we could not provide theoretical or intuitive moti-
vations to justify our proposed apporach.
In this paper, we present and empirically val-
idate a theory which aims at filling the above-
mentioned gaps. In particular we provide: (i) a
proof of the equation for the exact computation of
feature weights induced by TK functions (Collins
and Duffy, 2002); (ii) a theoretical characteriza-
tion of feature selection based on kiVk. We show
that if feature selection does not sensibly reduces
kiVk, the margin associated with Vi does not sen-
sibly decrease as well. Consequently, the theoret-
ical upperbound to the probability error does not
sensibly increases; (iii) a proof that the convolu-
tive nature of TK allows for filtering out an expo-
nential number of features with a small kiVk de-
crease. The combination of (ii) with (iii) suggests
that an extremely aggressive feature selection can
be applied. We describe a greedy algorithm that
exploits these results. Compared to the one pro-
posed in P&amp;M, the new version of the algorithm
has only one parameter (instead of 3), it is more
efficient and can be more easily connected with the
amount of gradient norm that is lost after feature
selection.
In the remainder: Section 2 briefly reviews
SVMs and TK functions; Section 3 describes the
problem of selecting and projecting features from
very high onto lower dimensional spaces, and pro-
vides the theoretical foundation to our approach;
Section 4 presents a selection of related work; Sec-
tion 5 describes our approach to tree fragment se-
lection; Section 6 details the outcome of our ex-
periments; finally, in Section 7 we draw our con-
clusions.
</bodyText>
<page confidence="0.986659">
223
</page>
<note confidence="0.973169">
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 223–233,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.699884" genericHeader="method">
2 Fragment Weights in TK Spaces
</sectionHeader>
<bodyText confidence="0.9998098">
The critical step for feature selection in tree ker-
nel spaces is the computation of the weights of
features (tree fragments) in the kernel machines’
gradient. The basic parameters are the fragment
frequencies which are combined with a decay fac-
tor used to downscale the weight of large sub-
trees (Collins and Duffy, 2002). In this section, af-
ter introducing basic kernel concepts, we describe
a theorem that establishes the correct weight1 of
features in the STK space.
</bodyText>
<subsectionHeader confidence="0.835708">
2.1 Kernel Based-Machines
</subsectionHeader>
<bodyText confidence="0.9999715">
Typically, a kernel machine is a linear classifier
whose decision function can be expressed as:
</bodyText>
<equation confidence="0.989819">
c(x) = w· x+ b =
</equation>
<bodyText confidence="0.995526142857143">
where x E RN is a classifying example and
IV E RN and b E R are the separating hyper-
plane’s gradient and its bias, respectively. The
gradient is a linear combination of E training
points xz E RN multiplied by their labels
yi E {−1, +1} and their weights αi E R+.
Different optimizers use different strategies to
learn the gradient. For instance, an SVM learns
to maximize the distance between positive and
negative examples, i.e. the margin -y. Applying
the so-called kernel trick, it is possible to replace
the scalar product with a kernel function defined
over pairs of objects, which can more efficiently
compute it:
</bodyText>
<equation confidence="0.997863">
c(o) = XP αiyik(oi, o) + b,
i=1
</equation>
<bodyText confidence="0.9990396">
where k(oi, o) = O(oi) · O(o), with the advantage
that we do not need to provide an explicit mapping
O : O —* RN of our example objects O in a vec-
tor space. In the next section, we show a kernel
directly working on syntactic trees.
</bodyText>
<subsectionHeader confidence="0.996888">
2.2 Syntactic Tree Kernel (STK)
</subsectionHeader>
<bodyText confidence="0.999459125">
Tree Kernel (TK) functions are convolution ker-
nels (Haussler, 1999) defined over pairs of trees.
Different TKs are characterized by alternative
fragment definitions, e.g. (Collins and Duffy,
2002; Kashima and Koyanagi, 2002; Moschitti,
2006). We will focus on the syntactic tree kernel
described in (Collins and Duffy, 2002), which re-
lies on a fragment definition that does not allow to
</bodyText>
<footnote confidence="0.8275275">
1In P&amp;M we provided an approximation of the real
weight.
</footnote>
<bodyText confidence="0.994594928571429">
break production rules (i.e. if any child of a node is
included in a fragment, then also all the other chil-
dren have to). As such, it is especially indicated
for tasks involving constituency parsed texts.
Tree kernels compute the number of common
substructures between two trees T1 and T2
without explicitly considering the whole feature
(fragment) space. Let F = {f1, f2,. . . , f|F|}
be the set of tree fragments, i.e. the explicit
representation for the components of the fragment
space, and Xi(n) be an indicator function2, equal
to 1 if the target fi is rooted at node n and equal
to 0 otherwise. A tree kernel function over T1 and
T2 is defined as
</bodyText>
<equation confidence="0.999333">
TK(T1,T2) = X X A(n1, n2), (2)
n1∈NT1 n2∈NT2
</equation>
<bodyText confidence="0.997978">
where NT1 and NT2 are the sets of nodes in T1 and
T2, respectively and
</bodyText>
<equation confidence="0.983411">
Xi(n1)Xi(n2). (3)
</equation>
<bodyText confidence="0.99102325">
The A function counts the number of common
subtrees rooted in n1 and n2 and weighs them
according to their size. It can be evaluated as
follows (Collins and Duffy, 2002):
</bodyText>
<listItem confidence="0.997913857142857">
1. if the productions at n1 and n2 are different,
then A(n1, n2) = 0;
2. if the productions at n1 and n2 are the same,
and n1 and n2 have only leaf children (i.e. they
are pre-terminal symbols) then A(n1, n2) = A;
3. if the productions at n1 and n2 are the same,
and n1 and n2 are not pre-terminals then
</listItem>
<equation confidence="0.897419">
l(n1)
A(n1, n2) = A Y (1 + A(c�n1, c�n2)), (4)
=1
</equation>
<bodyText confidence="0.999493666666667">
where l(n1) is the number of children of n1, c�n
is the j-th child of node n and A is a decay factor
penalizing larger structures.
</bodyText>
<subsectionHeader confidence="0.998596">
2.3 Tree Fragment Weights
</subsectionHeader>
<bodyText confidence="0.999934">
Eq. 3 shows that A counts the shared fragments
rooted in n1 and n2 in the form of scalar product,
as evaluated by Eq. 2. However, when A is used in
A as in Eq. 4, it changes the weight of the product
Xi(n1)Xi(n2). As A multiplies A in each recur-
sion step, we may be induced to assume3 that the
</bodyText>
<footnote confidence="0.9843104">
2We will consider it as a weighting function.
3In (Collins and Duffy, 2002), there is a short note about
the correct value weight of lambda for each product compo-
nents (i.e. pairs of fragments). This is in line with the formu-
lation we provide.
</footnote>
<equation confidence="0.998270142857143">
XP
i=1
x+b (1)
αiyiei ·
A(n1, n2) =
X |F|
i=1
</equation>
<page confidence="0.994826">
224
</page>
<bodyText confidence="0.950935617021276">
weight of a fragment is λd, where d is the depth of
the fragment. On the contrary, we show the actual
weight by providing the following:
Theorem 1. Let T and f be a tree and one of
its fragments, respectively, induced by STK. The
weight of f accounted by STK is λ
lf(n) is the number of children of n in f and
s(f) = |{n ∈ T : lf(n) &gt; 0} |is the number
of nodes that have active productions in the frag-
ment, i.e. the size of the fragment.
In other words, the exponent of λ is the number
of fragment nodes that have at least one child (i.e.
active productions), divided by 2.
Proof. The thesis can be proven by induction on
the depth d of f. The base case is f of depth
1. Fragments of depth 1 are matched by step 2
of Δ(n1, n2) computation, which assigns a value
λ = χi(n1)χi(n2) independently of the number of
children (where fi = f). It follows that the weight
off is χi(n1) = χi(n2) = λ1/2.
Suppose that the thesis is valid for depth d and
let us consider a fragment f of depth d + 1, rooted
in r. Without loss of generality, we can assume
that f is in the set of the fragments rooted in n1
and n2, as evaluated by Eq. 4. It follows that
the production rules associated with n1 and n2 are
identical to the production rule in r. Let us con-
sider M = {i ∈ {1,..,l(n1)} : l(cir) &gt; 0},
i.e. the set of child indices of r which have at
least a child. Thus, for j ∈ M, cir has a pro-
duction shared by cjn1 and cjn2. Conversely, for
j ∈/ M, there is no match and Δ(cjn1, cjn2) = 0.
Therefore, the product in Eq. 4 can be rewrit-
ten as λ 11j�M Δ(cjn1, cjn2), where the term 1 in
(1 + Δ(cjn1, cjn2)) is not considered since it ac-
counts for those cases where there are no common
productions in the children, i.e. cjn1 =6 cjn2∀j ∈
M.
We can now substitute Δ(cjn1, cjn2) with the
weight of the subtree tj of f rooted in cjr (and ex-
tended until its leaves), which is λs(tj) by induc-
tive hypothesis (since tj has depth lower than d).
Thus, the weight of f is s(f) = λ 11jcM λs(tj) =
λ1+EjEM s(tj), where EjEM s(tj) is the num-
ber of nodes in f’s subtrees rooted in r’s chil-
dren and having at least one child; by adding
1, i.e. the root of f, we obtain s(f). Finally,
</bodyText>
<equation confidence="0.968579333333333">
λs(f) = χi(n1)χi(n2), which satisfies our thesis:
s(f)
χi(n1) = χi(n2) = λ2 .
</equation>
<subsectionHeader confidence="0.951474">
2.4 Weights in Feature Vectors
</subsectionHeader>
<bodyText confidence="0.9030328">
In the light of this result, we can use the definition
of a TK function to project a tree t onto a linear
space by recognizing that t can be represented as a
vector ~xi = [x(1)
i , ... , x(N)
i ] whose attributes are
the counts of the occurrences for each fragment,
weighed with respect to the decay factor λ.
For a normalized STK kernel, the value of the
j-th attribute of the example ~xi is therefore:
</bodyText>
<equation confidence="0.93403175">
s(fj )
2
ti,jλ (5)
&apos; ENk=1 t2i kλs(fk)
</equation>
<bodyText confidence="0.9985184">
where: ti,j is the number of occurrences of the
fragment fj, associated with the j-th dimension
of the feature space, in the tree ti. It follows that
the components of w~ (see Eq. 1) can be rewritten
as:
</bodyText>
<equation confidence="0.9467655">
s(fj )
2
αiyiti,jλ (6)
&apos; ENk=1 t2i kλs(fk)
</equation>
<sectionHeader confidence="0.867014" genericHeader="method">
3 Projecting Exponentially Large Spaces
</sectionHeader>
<bodyText confidence="0.995269785714286">
In order to provide a theoretical background to our
feature selection technique and to develop effec-
tive algorithms, we want to relate our approach to
statistical learning and, in particular, support vec-
tor classification theory. Since we select features
with respect to their weight w(j), we can use the
following theorem that establishes a general bound
for margin-based classifiers.
Theorem 2. (Bartlett and Shawe-Taylor, 1998)
Let C = {~x → w~ · x~ : k~wk ≤ 1, k~xk ≤ R}
be the class of real-valued functions defined in a
ball of radius R in &lt;N. Then there is a con-
stant k such that ∀c ∈ C having a margin γ, i.e.
|~w · ~x |≥ γ, ∀~x ∈ X (training set), the error of c
is bounded by b/` + t ( 72log2` + log a) with
a probability 1 − δ, where ` = |X  |and b is the
number of examples with margin less than γ.
In other words, if X is separated with a margin
γ by a linear classifier, then the error has a bound
depending on γ. Another conclusion is that a fea-
ture selection algorithm that wants to preserve the
accuracy of the original space should not affect the
margin.
Since we would like to exploit the availability
of the initial gradient w~ derived by the applica-
tion of SVMs, it makes sense to try to quantify the
percentage of γ reduction after feature selection,
which we indicate by ρ. We found out that γ is
</bodyText>
<equation confidence="0.995757125">
s(f)
2
, where
s(fj)
x(j) = ti,jλ 2
i k~xik
w(j) = �` αiyixi = �`
i=1 (j) i=1
</equation>
<page confidence="0.99101">
225
</page>
<bodyText confidence="0.831781357142857">
linked to the reduction of ||~w||, as illustrated by
the next lemma.
Lemma 1. Let X be a set of points in a vector
space and w~ be the gradient vector which sepa-
rates them with a margin γ. If the selection de-
creases ||~w ||of a ρ rate, then the resulting hyper-
plane separates X by a margin larger than γin =
γ − ρR||~w||.
Proof. Let w~ = ~win+~wout, where ~win and ~wout ∈
&lt;N are constituted by the components of w~ that
are selected in and out, respectively, and have zero
values in the remaining positions. By hypothesis
|~w · ~x |≥ γ; without loss of generality, we can
consider just the case w~ · x~ ≥ γ, and write w~ ·
</bodyText>
<equation confidence="0.8021755">
x~ = ~win · x~ + ~wout · x~ ≥ γ ⇒ ~win · x~ ≥ γ −
~wout · x~ ≥ γ − |~wout · ~x |≥ γ − ||~wout ||× ||~x||,
</equation>
<bodyText confidence="0.816827925925926">
where the last inequality holds owing to Cauchy-
Schwarz inequality. The margin associated with
~win, i.e. γin, is therefore γ − ||~wout ||× ||~x ||≥
γ − ||~wout||R = γ − ρR||~w||.
Remark 1. The lemma suggests that, even in case
of very aggressive feature selection, if a small per-
centage ρ of ||~w ||is lost, the margin reduction is
small. Consequently, through Theorem 2, we can
conclude that the accuracy of the model is by and
large preserved.
Remark 2. We prefer to show the lemma in the
more general form, but if we use normalized x~ and
classifiers with ||~w ||≤ 1, then γin = γ − ||~w||ρ &gt;
γ − ρ.
The last result that we present justifies our se-
lection approach as it demonstrates that most of
the gradient norm is concentrated in relatively few
features, with respect to the huge space induced
by tree kernels. The selection of these few fea-
tures allows us to preserve most of the norm and
the margin.
Lemma 2. Let w~ be a linear separator of a set of
points X, where each ~xi ∈ X is an explicit vector
representations of a tree ti in the space induced by
STK and let ν be the largest s(ti), i.e. the max-
imum tree size. Then, if we discard fragments of
size greater than η, ||~wout ||≤ ν q(λν)η−(λν)ν
</bodyText>
<equation confidence="0.9557265">
γ2 1−λν .
Proof. By applying simple norm proper-
� �
� P` � ≤ P`
ties, ||~wout ||= i=1 αiyi~xouti � i=1
||αiyi~xouti ||= P`i=1 αi||~xouti||. To evaluate
</equation>
<bodyText confidence="0.911280285714286">
the latter, we first re-organize the summation in
Eq. 5 (with no normalization) such that k~xik2
= Pν Pj:s(fj)=k t2 i,jλs(fj). Since a fragment
k=1
fj can be at maximum rooted in ν nodes, then
ti,j ≤ ν. Therefore, by replacing the number of
trees of size k with the upperbound νk, we have
</bodyText>
<equation confidence="0.905266333333333">
qPν qPν
k~xik &lt; k=1 ν2λkνk = k=1 ν2(νλ)k =
qν2 1−µν
</equation>
<bodyText confidence="0.810312333333333">
1−µ , where we applied geometric series
summation. Now if we assume that our algorithm
selects out (i.e. discards) fragments with size
</bodyText>
<equation confidence="0.981715">
s(f) &gt; η, k~xoutik &lt; qν2 µ1−µ
. It follows that
q
ν2 µη−µν
||~wout ||&lt; P` i=1 αi 1−µ . In case of hard-
margin SVMs, we have P`i=1 αi = 1/γ2. Thus,
||~wout ||&lt; ν qµη−µν
1−µ = ν q(λν)η−(λν)ν
γ2 γ2 1−λν .
</equation>
<construct confidence="0.525560428571429">
Remark 3. The lemma shows that for an enough
large η and λ &lt; 1/ν, ||~wout ||can be very small,
even though it includes an exponential number of
features, i.e. all the subtrees whose size ranges
from η to ν. Therefore, according to Lemma 1 and
Theorem 2, we can discard an exponential number
offeatures with a limited loss in accuracy.
Remark 4. Regarding the proposed norm bound,
we observe that νk is a rough overestimation of the
the real number offragments having size k rooted
in the nodes of the target tree t. This suggests that
we don’t really need λ &lt; 1/ν. Moreover, in case
of soft-margin SVMs, we can bound αi with the
value of the trade-offparameter C.
</construct>
<sectionHeader confidence="0.995037" genericHeader="method">
4 Previous Work
</sectionHeader>
<bodyText confidence="0.999980952380952">
Initial work on feature selection for text, e.g.
(Yang and Pedersen, 1997), has shown that it may
improve the accuracy or, at least, improve effi-
ciency while preserving accuracy. Our context for
feature selection is different for several important
reasons: (i) we focus on structured features with
a syntactic nature, which show different behaviour
from lexical ones, e.g. they tend to be more sparse;
(ii) in the TK space, the a-priori weights are very
skewed, and large fragments receive exponentially
lower scores than small ones; (iii) there is high re-
dundancy and inter-dependency between such fea-
tures; (iv) we want to be able to observe the most
relevant features automatically generated by TKs;
and (v) the huge number of features makes it im-
possible to evaluate the weight of each feature in-
dividually.
Guyon and Elisseeff (2003) carries out a very
informative survey of feature selection techniques.
Non-filter approaches for SVMs and kernel ma-
chines are often concerned with polynomial and
</bodyText>
<page confidence="0.990202">
226
</page>
<bodyText confidence="0.997556615384615">
Gaussian kernels, e.g. (Weston et al., 2001; Neu-
mann et al., 2005). In (Kudo and Matsumoto,
2003), an extension of the PrefixSpan algo-
rithm (Pei et al., 2001) is used to efficiently mine
the features in a low degree polynomial kernel
space. The authors discuss an approximation
of their method that allows them to handle high
degree polynomial kernels. Suzuki and Isozaki
(2005) present an embedded approach to feature
selection for convolution kernels based on x2-
driven relevance assessment. With respect to their
work, the main differences in the approach that we
propose are that we want to exploit the SVM op-
timizer to select the most relevant features, and to
be able to observe the relevant fragments.
Regarding work that may directly benefit from
reverse kernel engineering is worthwhile mention-
ing: (Cancedda et al., 2003; Shen et al., 2003;
Daum´e III and Marcu, 2004; Giuglea and Mos-
chitti, 2004; Toutanova et al., 2004; Kazama and
Torisawa, 2005; Titov and Henderson, 2006; Kate
and Mooney, 2006; Zhang et al., 2006; Bloehdorn
et al., 2006; Bloehdorn and Moschitti, 2007; Mos-
chitti and Zanzotto, 2007; Surdeanu et al., 2008;
Moschitti, 2008; Moschitti and Quarteroni, 2008;
Martins et al., 2009; Nguyen et al., 2009a)
</bodyText>
<sectionHeader confidence="0.984908" genericHeader="method">
5 Mining Fragments Efficiently
</sectionHeader>
<bodyText confidence="0.999982347826087">
The high-level description of our feature selection
technique is as follows: we start by learning an
STK model and we greedily explore the support
vectors in search for the most relevant fragments.
We store them in an index, and then we decode (or
linearize) all the trees in the dataset, i.e. we repre-
sent them as vectors in a linear space where only a
very small subset of the fragments in the original
space are accounted for. These vectors are then
employed for learning and classification in the lin-
ear space.
To explore the fragment space defined by a set
of support vectors, we adopt the greedy strategy
described in Algorithm 5.1. Its arguments are a
model M, and the threshold factor L. The greedy
algorithm explores the fragment space in a small to
large fashion. The first step is the generation of the
all base fragments F encoded in each tree, i.e. the
smallest possible fragments according to the defi-
nition of the kernel function. For STK, such frag-
ments are all those consisting of a node and all its
direct children (i.e. production rules of the gram-
mar). We assess the cumulative relevance of each
</bodyText>
<figure confidence="0.976448391304348">
Algorithm 5.1: GREEDY MODEL MINER(M, L)
B BASE FRAGS(model)
B REL(BEST(B))
v B/L
Dprev FILTER(B, v)
UPDATE(Dprev)
while Dprev # 0
Dnext 0
T 1/ * widthfactor * /
Wprev Dprev
while Wprev 54 0
Wnext 0
for each f E Wprev
Ef EXPAND(f, T)
F FILTER(Ef, v)
ifF#0
Wnext Wnext U lf}
then Dnext Dnext U F
UPDATE(F)
T T + 1
Wprev Wnext
Dprev Dnext
return (result)
</figure>
<bodyText confidence="0.999952636363636">
base fragment according to Eq. 6 and then use the
relevance B of the heaviest fragment, i.e. the frag-
ment with the highest relevance in absolute value,
as a criterion to set our fragment mining threshold
Q to B/L. We then apply the FILTER(-) operator
which discards all the fragments whose cumula-
tive score is less than Q. Then, the UPDATE(-) op-
erator stores the ramaining fragments in the index.
The exploration of the kernel space is carried
out via the process of fragment expansion, by
which each fragment retained at the previous step
is incrementally grown to span more levels of the
tree and to include more nodes at each level. These
two directions of growth are controlled by the
outer and the inner while loops, respectively. Frag-
ment expansion is realized by the EXPAND(f, n)
operator, that grows the fragment f by including
the children of n expandable nodes in the frag-
ment. Expandable nodes are nodes which are
leaves in f but that have children in the tree that
originated f.
After each expansion, the FILTER(-) operator is
invoked on the set of generated fragments. If the
filtered set is empty, i.e. no fragments more rele-
vant than Q have been found during the previous
iteration, then the loop is terminated.
Unlike previous attempts, this algorithm relies
on just one parameter, i.e. L. As it revolves around
the weight of the most relevant fragment, it oper-
ates according to the norm-preservation principle
described in the previous sections. In fact, if we
call N the number of fragments mined for a given
value of L, the norm after feature selection can be
</bodyText>
<figure confidence="0.994901">
do
�
������������������� �
��������������������
�
i
do
�
���� �
�����
do
</figure>
<page confidence="0.619248">
227
</page>
<bodyText confidence="0.954891294117647">
bounded by BIN &lt; I I win I I &lt; BIN .
The choice of B, i.e. the highest relevance of
a base fragment, as an upper bound for fragment
relevance is motivated as follows. In Eq. 6, we can
identify a term Ti = αiyi/I ItiI I that is the same for
all the fragments in the tree ti. For 0 &lt; A &lt; 1,
if fj is an expansion of fk, then from our defini-
tion of fragment expansion it follows that A s(fj )
2 &lt;
. It can also be observed that ti,j &lt; ti,k. In-
deed, if ti,k is a subset of ti,j, then it will occur at
least as many times as its expansion ti,k, possibly
occurring as a seed fragment for different expan-
sions in other parts of the tree as well. Therefore,
if £f is the set of expansions of f, for every two
fragments fi,j, fi,k coming from the same tree ti,
we can conclude that x�j)
</bodyText>
<equation confidence="0.775025">
i &lt; x�k)
</equation>
<bodyText confidence="0.9644314">
i bfi,j E £fi,k . In
other words, for each tree in the model, base frag-
ments are the most relevant, and we can assume
that the relevance of the heaviest fragment is an
upper bound for the relevance of any fragment 4.
</bodyText>
<sectionHeader confidence="0.999524" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.970438344827586">
We ran a set of thorough experiments to sup-
port our claims with empirical evidence. We
show our results on three very different bench-
marks: Question Classification (QC) using TREC
10 data (Voorhees, 2001), Relation Extraction
(RE) based on the newswire and broadcast news
domain of the ACE 2004 English corpus (Dod-
dington et al., 2004) and Semantic Role Labeling
(SRL) on the CoNLL 2005 shared task data (Car-
reras and M`arquez, 2005). In the next sections we
elaborate on the setup and outcome of each set
of experiments. As a supervised learning frame-
work we used SVM-Light-TK5, which extends the
SVM-Light optimizer (Joachims, 2000) with sup-
port for tree kernel functions.
Unless differently stated, all the classifiers are
parametrized for optimal Precision and Recall on
a development set, obtained by selecting one ex-
ample in ten from the training set with the same
positive-to-negative example ratio. The results
that we show are obtained on the test sets by using
all the available data for training. For multi-class
scenarios, the classifiers are arranged in a one vs.
4In principle, the weight of some fragment encoded in the
model M may be greater than B. However, as an empirical
justification, we report that in all our experiments we have
never been able to observe such case. Thus, with a certain
probability, we can assume that the highest weight will be
obtained from the heaviest of the base fragments.
</bodyText>
<footnote confidence="0.825742">
5http://disi.unitn.it/˜moschitt/Tree-Kernel.htm
</footnote>
<bodyText confidence="0.999311142857143">
all configuration, where each sentence is a positive
example for one of the classes, and negative for
the others. While binary classifiers are evaluated
in terms of F1 measure, for multi-class classifiers
we show the final accuracy.
The next paragraphs describe the datasets used
for the experiments.
Question Classification (QC) Given a question,
the task consists in selecting the most appropriate
expected answer type from a given set of possibil-
ities. We adopted the question taxonomy known
as coarse grained, which has been described
in (Zhang and Lee, 2003) and (Li and Roth, 2006),
consisting of six non overlapping classes: Abbre-
viations (ABBR), Descriptions (DESC, e.g. def-
initions or explanations), Entity (ENTY, e.g. an-
imal, body or color), Human (HUM, e.g. group
or individual), Location (LOC, e.g. cities or coun-
tries) and Numeric (NUM, e.g. amounts or dates).
The TREC 10 QA data set accounts for 6,000
questions. For each question, we generate the
full parse of the sentence and use it to train our
models. Automatic parses are obtained with the
Stanford parser6 (Klein and Manning, 2003), and
we actually have only 5,953 sentences in our data
set due to parsing issues. During preliminary ex-
periments, we observed an uneven distribution of
examples in the traditional training/test split (the
same used in P&amp;M). Therefore, we used a ran-
dom selection to generate an unbiased split, with
5,468 sentences for training and 485 for testing.
The resulting data set is available for download
at http://danielepighin.net/cms/research/
QC_dataset.tgz.
Relation Extraction (RE) The corpus
consists of 348 documents, and contains
seven relation classes defined over pairs of
mentions: Physical, Person/Social, Employ-
ment/Membership/Subsidiary, Agent-Artifact,
PER/ORG Affiliation, GPE Affiliation, and
Discourse. There are 4,400 positive and 38,696
negative examples when the potential relations
are generated using all the entity/mention pairs in
the same sentence.
Documents are parsed using the Stanford
Parser, where the nodes of the entities are enriched
with information about the entity type. Overall,
we used the setting and data defined in (Nguyen et
al., 2009b).
</bodyText>
<equation confidence="0.420605">
6http://nlp.stanford.edu/software/lex-parser.shtml
s(fk)
A2
</equation>
<page confidence="0.994623">
228
</page>
<bodyText confidence="0.999846071428571">
Semantic Role Labeling (SRL) SRL can be de-
composed into two tasks: boundary detection,
where the word sequences that are arguments of
a predicate word w are identified, and role clas-
sification, where each argument is assigned the
proper role. For these experiments we concen-
trated on this latter task and used exactly the same
setup as P&amp;M. We considered all the argument
nodes of any of the six PropBank (Palmer et al.,
2005) core roles7 (i.e. A0, ... , A5) from all the
available training sections, i.e. 2 through 21, for a
total of 179,091 training instances. Similarly, we
collected 9,277 test instances from the annotations
of Section 23.
</bodyText>
<subsectionHeader confidence="0.997075">
6.1 Model Comparison
</subsectionHeader>
<bodyText confidence="0.999971090909091">
To show the validity of Lemma 1 in practical sce-
narios, we compare the accuracy of our linearized
models against vanilla STK classifiers. We de-
signed two types of classifiers:
LIN, a linearized STK model, which uses the
weights estimated by the learner in the STK space
and linearized examples; in other words LIN uses
wIN. It allows us to measure exactly the loss in
accuracy with respect to the reduction of ||i9||.
OPT, a linearized STK model that is re-
optimized in the linear space, i.e. for which we
retrained an SVM using the linearized training ex-
amples as input data. Since the LIN solution is
part of the candidate solutions from which OPT is
selected, we always expect higher accuracy from
it.
Additionally, we compare selection based on
gradient w� (as detailed in Section 2.4) against to
x2 selection, which evaluates the relevance of fea-
tures, in a similar way to (Suzuki and Isozaki,
2005). The relevance of a fragment is calculated
as
</bodyText>
<equation confidence="0.9809505">
2 N(yN − Mx)2
X x(N − x)M(N − M) ,
</equation>
<bodyText confidence="0.9999863">
where N is the number of support vectors, M is
the number of positive vectors (i.e. αi &gt; 0), and x
and y are the fractions of N and M where the frag-
ment is instantiated, respectively. We specify the
selection models by means of Grad for the former
and Chi for the latter. For example, a model called
OPT/Grad is a re-trained model using the features
selected according the highest gradient weights,
while LIN/Chi would be a linearized tree kernel
model using x2 for feature selection.
</bodyText>
<footnote confidence="0.9704965">
7We do not consider adjuncts because we preferred the
number of classes to be similar across the three benchmarks.
</footnote>
<figureCaption confidence="0.988924333333333">
Figure 1: Percentage of gradient Norm, i.e. 1 − p,
according to the number of selected fragments, for
different QC classifiers.
</figureCaption>
<table confidence="0.934894666666667">
STK Linearized
LIN OPT
Fl ||79 ||Frags Fl ||794- ||Fl
A 80.00 11.77 566 66.67 7.13 90.91
D 86.26 41.33 5161 81.87 25.10 83.72
E 76.86 51.71 5,702 73.03 31.06 75.56
H 84.92 43.61 5,232 80.47 26.20 77.08
L 81.69 38.73 1,732 78.87 24.27 82.89
N 92.31 37.65 1,015 85.07 24.53 87.07
</table>
<tableCaption confidence="0.998737">
Table 1: Per-class comparison between STK and
</tableCaption>
<bodyText confidence="0.993468666666667">
the LIN/Grad and OPT/Grad models on the QC
task. Each class is identified by its initial (e.g.
A=ABBR). For each class, we considered a value
of the threshold factor parameter L so as to retain
at least 60% of the gradient norm after feature se-
lection.
</bodyText>
<subsectionHeader confidence="0.929366">
6.2 Results
</subsectionHeader>
<bodyText confidence="0.999749352941177">
The plots in Figure 1 show, for each class, the per-
centage of the gradient norm (i.e. 1 − p, see Sec-
tion 3) retained when including a different num-
ber of fragments. This graph empirically validates
Lemma 2 since it clearly demonstrates that after
1,000-10,000 features the percentage of the norm
reaches a plateau (around 60-65%). This means
that after such threshold, which interestingly gen-
eralizes across all classifiers, a huge number of
features is needed for a small increase of the norm.
We recall that the maximum reachable norm is
around 70% since we apriori filter out fragments
of frequency lower than three.
Table 1 shows the F1 of the binary question clas-
sifiers learned with STK, LIN/Grad and OPT/Grad
models. It also shows the norm of the gradi-
ent before, ||w||, and after, ||win||, feature selec-
</bodyText>
<figure confidence="0.974039681818182">
1 10 100 1000 10000
Number of fragments (log)
1 − p
0.65
0.55
0.45
0.35
0.25
0.7
0.6
0.5
0.4
0.3
0.2
ABBR
DESC
ENTY
HUM
LOC
NUM
229
1—p
</figure>
<figureCaption confidence="0.973191">
Figure 2: F1-measure of LOC and DESC wrt dif-
ferent 1 — p values.
</figureCaption>
<bodyText confidence="0.999778869565217">
tion along with the number of selected fragments,
Frags. Instead of selecting an optimal number of
fragments on a validation set, we investigated the
60% value suggested by the previous plot. Thus,
for each category we selected the feature set reach-
ing approximately 60% of ||i9||. The table shows
that the accuracy of the OPT/Grad model is in
line with STK. In some cases, e.g. ABBR, the
projected model is more accurate, i.e. 90.91 vs.
80.00, whereas in others, e.g. HUM, STK per-
forms better, i.e. 84.92 vs. 77.08. It is interesting
to see how the empirical results clearly comple-
ment the theoretical findings of the previous sec-
tions. For example, the LOC classifier uses only
1,732 of the — 1012 features encoded by the cor-
responding STK model, but since only 40% of the
norm of 9i is lost, classification accuracy is af-
fected only marginally.
As mentioned above, the selected number of
features is not optimal for every class. Fig-
ure 2 plots the accuracy of the LIN/Grad and
OPT/Grad for different numbers of fragments on
two classes 8. These show that the former, with
</bodyText>
<footnote confidence="0.947049">
8The other classes, which show similar behaviour, are
omitted due to lack of space.
</footnote>
<figure confidence="0.8929305">
100 1000 10000 100000
Number of fragments (log)
</figure>
<figureCaption confidence="0.981569">
Figure 3: Multiclass accuracy obtained by includ-
ing a growing number of fragments.
</figureCaption>
<bodyText confidence="0.99988585">
more than 60% of the norm, approaches STK
whereas the latter requires less fragments. The
plots also show the comparison against the same
fragment mining algorithm and learning frame-
work when using x2-based selection. This also
provides similar good results, as far as the reduc-
tion of ||i9 ||is kept under control, i.e. as far as we
select the components of the gradient that mostly
affect its norm.
To concretely assess the benefits of our models
for QC, Figure 3 plots the accuracy of OPT/Grad
and OPT/Chi on the multiclass QC problem wrt
the number of fragments employed. The results
for the multi-class classifier are less biased by the
binary Precision/Recall classifiers thus they are
more stable and clearly show how, after selecting
the optimal number of fragments (1,000-10,000
i.e. 60-65% of the norm), the accuracy of the OPT
and CHI classifiers stabilize around levels of accu-
racy which are in line with STK.
</bodyText>
<table confidence="0.993913">
STK OPT/Grad
F1 F1 Frags
QC 83.70 84.12 —2k
RE 67.53 66.31 —10k
SRL 87.56 88.17 —300k
</table>
<tableCaption confidence="0.9735545">
Table 2: Multiclass classification accuracy on
three benchmarks.
</tableCaption>
<bodyText confidence="0.99954825">
Finally, Table 2 shows the best results that we
achieved on the three multi-class classification
tasks, i.e. QC, RE9 and SRL, and compares them
against the STK 10. For all the tasks OPT/Grad
</bodyText>
<footnote confidence="0.926479">
9For RE, we show lower accuracy than in (Nguyen et al.,
2009b) since, to have a closer comparison with STK, we do
not combine structural features with manual designed fea-
tures.
10We should point out that this models are only partially
</footnote>
<figure confidence="0.984795152173913">
90
85
80
75
70
65
60
55
50
LIN/Grad
OPT/Grad
LIN/Chi
OPT/Chi
STK
0.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6 0.65 0.7
1—p
0.1 0.2 0.3 0.4 0.5 0.6
40
20
90
80
70
60
50
30
10
0
LIN/Grad
OPT/Grad
LIN/Chi
OPT/Chi
STK
90
80
70
60
50
40
30
20
OPT/Grad
OPT/Chi
STK
F1 (LOC)
F1 (DESC)
Multiclass accuracy
</figure>
<page confidence="0.988122">
230
</page>
<bodyText confidence="0.999272857142857">
produces the best results for all the tests, even
though the difference with OPT/Chi is generally
not statistically significant. Out of three tasks,
OPT/Grad manages to slightly improve two of
them, i.e. QC (84.12 vs. 83.7) and SRL (88.17
vs. 87.56), while STK is more accurate on RE, i.e.
67.53 vs. 66.31.
</bodyText>
<subsectionHeader confidence="0.999805">
6.3 Comparison with P&amp;M
</subsectionHeader>
<bodyText confidence="0.99992595">
The results on SRL can be compared against
those that we presented in (Pighin and Moschitti,
2009a), where we measured an accuracy of 87.13
exactly on the same benchmark. As we can see in
Table 2, our model improves the classification ac-
curacy of about 1 point, i.e. 88.17. On the other
hand, such comparison is not really fair since the
algorithms rely on different parameter sets, and it
is almost impossible to find matching configura-
tions for the different versions of the algorithms
that would result in exactly the same number of
fragments. In a projected space with approxi-
mately 103 or 104 fragments, including a few hun-
dred more features can produce noticeably differ-
ent accuracy readings.
Generally speaking, the current model can
achieve comparable accuracy with P&amp;M while
considering a smaller number of fragments. For
example, in (Pighin and Moschitti, 2009b) the
best model for the A1 binary classifier of the
SRL benchmark was obtained by including 50,000
fragments, achieving an F1 score of 89.04. With
the new algorithm, using approximately half the
fragments the accuracy of the linearized A1 clas-
sifier is 90.09. In P&amp;M, the algorithm would only
consider expansions of a fragment f where at most
m nodes are expanded. Consequently, the set of
mined fragments may include some small struc-
tures which can be less relevant than larger ones.
Conversely, the new algorithm (see Alg. 5.1) may
include larger but more relevant structures, thus
accounting for a larger fraction of the gradient
norm with a smaller number of fragments.
Concerning efficiency, the complexity of both
mining algorithms is proportional to the number
of fragments that they generate. Therefore, we can
conclude that the new implementation is more effi-
cient by considering that we can achieve the same
accuracy with less fragments. As for the complex-
optimized, as we evaluated them by using the same threshold
factor parameter L for all the classes. Better performances
could be achieved by selecting an optimal value of L for in-
dividual classes when building the multi-class classifier.
ity of decoding, i.e. providing explicit vector rep-
resentations of the input trees, in P&amp;M, we used
a very naive approach, i.e. the generation of all
the fragments encoded in the tree and then look up
each fragment in the index. This solution has ex-
ponential complexity with the number of nodes in
the tree. Conversely, the new implementation has
approximately linear complexity. The approach is
based on the idea of an FST-like index, that we
can query with a tree node. Every time the tree
matches one of the fragments, the index increases
the count of that fragment for the tree. The reduc-
tion in time complexity is made possible by en-
coding in the index the sequence of expansion op-
erations that produced each indexed fragment, and
by considering only those expansions at decoding
time.
</bodyText>
<sectionHeader confidence="0.999427" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.9999735">
Available feature selection frameworks for very
high dimensional kernel families, such as tree ker-
nels, suffer from the lack of a theory that could
justify the very aggressive selection strategies nec-
essary to cope with the exceptionally high dimen-
sional feature space.
In this paper, we have provided a theoretical
foundation in the context of margin classifiers by
(i) linking the reduction of the gradient norm to the
theoretical error bound and (ii) by proving that the
norm is mostly concentrated in a relatively small
number of features. The two properties suggest
that we can apply an extremely aggressive fea-
ture selection by keeping the same accuracy. We
described a very efficient algorithm to carry out
such strategy in the fragment space. Our experi-
ments empirically support our theoretical findings
on three very different NLP tasks.
</bodyText>
<sectionHeader confidence="0.994207" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999886777777778">
We would like to thank Truc-Vien T. Nguyen for
providing us with the SVM learning and test files
of the Relation Extraction dataset. Many thanks to
the anonymous reviewers for their valuable sug-
gestions.
This research has been partially supported by the
EC project, EternalS: “Trustworthy Eternal Sys-
tems via Evolving Software, Data and Knowl-
edge”, project number FP7 247758.
</bodyText>
<page confidence="0.996097">
231
</page>
<note confidence="0.938119090909091">
References
P. Bartlett and J. Shawe-Taylor, 1998. Advances in Kernel
Methods — Support Vector Learning, chapter Generaliza-
tion Performance of Support Vector Machines and other
Pattern Classifiers. MIT Press.
Jun’ichi Kazama and Kentaro Torisawa. 2005. Speeding up
training with tree kernels for node relation labeling. In
Proceedings of HLT-EMNLP’05.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL’03, pages
423–430.
</note>
<reference confidence="0.997747340000001">
Stephan Bloehdorn and Alessandro Moschitti. 2007. Struc-
ture and semantics for expressive text kernels. In In Pro-
ceedings of CIKM ’07.
Stephan Bloehdorn, Roberto Basili, Marco Cammisa, and
Alessandro Moschitti. 2006. Semantic kernels for text
classification based on topological measures of feature
similarity. In Proceedings of ICDM 06, Hong Kong, 2006.
Nicola Cancedda, Eric Gaussier, Cyril Goutte, and
Jean Michel Renders. 2003. Word sequence kernels.
Journal of Machine Learning Research, 3:1059–1082.
Xavier Carreras and Llu´ıs M`arquez. 2005. Introduction to
the CoNLL-2005 Shared Task: Semantic Role Labeling.
In Proceedings of CoNLL’05.
Michael Collins and Nigel Duffy. 2002. New Ranking Al-
gorithms for Parsing and Tagging: Kernels over Discrete
Structures, and the Voted Perceptron. In Proceedings of
ACL’02.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
Tree Kernels for Relation Extraction. In Proceedings of
ACL’04.
Chad Cumby and Dan Roth. 2003. Kernel Methods for Re-
lational Learning. In Proceedings of ICML 2003.
Hal Daum´e III and Daniel Marcu. 2004. Np bracketing by
maximum entropy tagging and SVM reranking. In Pro-
ceedings of EMNLP’04.
G. Doddington, A. Mitchell, M. Przybocki, L. Ramshaw,
S. Strassel, and R. Weischedel. 2004. The Auto-
matic Content Extraction (ACE) Program–Tasks, Data,
and Evaluation. Proceedings of LREC 2004, pages 837–
840.
Ana-Maria Giuglea and Alessandro Moschitti. 2004.
Knowledge Discovering using FrameNet, VerbNet and
PropBank. In In Proceedings of the Workshop on On-
tology and Knowledge Discovering at ECML 2004, Pisa,
Italy.
Isabelle Guyon and Andr´e Elisseeff. 2003. An introduc-
tion to variable and feature selection. Journal of Machine
Learning Research, 3:1157–1182.
David Haussler. 1999. Convolution kernels on discrete struc-
tures. Technical report, Dept. of Computer Science, Uni-
versity of California at Santa Cruz.
T. Joachims. 2000. Estimating the generalization perfor-
mance of a SVM efficiently. In Proceedings of ICML’00.
Hisashi Kashima and Teruo Koyanagi. 2002. Kernels for
semi-structured data. In Proceedings of ICML’02.
Rohit J. Kate and Raymond J. Mooney. 2006. Using string-
kernels for learning semantic parsers. In Proceedings of
the 21st ICCL and 44th Annual Meeting of the ACL, pages
913–920, Sydney, Australia, July. Association for Compu-
tational Linguistics.
Taku Kudo and Yuji Matsumoto. 2003. Fast methods for
kernel-based text analysis. In Proceedings of ACL’03.
Taku Kudo, Jun Suzuki, and Hideki Isozaki. 2005. Boosting-
based parse reranking with subtree features. In Proceed-
ings of ACL’05.
Xin Li and Dan Roth. 2006. Learning question classifiers:
the role of semantic information. Natural Language En-
gineering, 12(3):229–249.
Andr´e F. T. Martins, Noah A. Smith, Eric P. Xing, Pedro
M. Q. Aguiar, and M´ario A. T. Figueiredo. 2009. Nonex-
tensive information theoretic kernels on measures. J.
Mach. Learn. Res., 10:935–975.
Alessandro Moschitti and Silvia Quarteroni. 2008. Kernels
on linguistic structures for answer extraction. In Proceed-
ings of ACL-08: HLT, Short Papers, Columbus, Ohio.
Alessandro Moschitti and Fabio Massimo Zanzotto. 2007.
Fast and effective kernels for relational learning from
texts. In Zoubin Ghahramani, editor, Proceedings of the
24th Annual International Conference on Machine Learn-
ing (ICML 2007).
Alessandro Moschitti, Daniele Pighin, and Roberto Basili.
2008. Tree kernels for semantic role labeling. Compu-
tational Linguistics, 34(2):193–224.
Alessandro Moschitti. 2006. Efficient convolution kernels
for dependency and constituent syntactic trees. In Pro-
ceedings of ECML’06, pages 318–329.
Alessandro Moschitti. 2008. Kernel methods, syntax and
semantics for relational text categorization. In Proceeding
of CIKM ’08, NY, USA.
Julia Neumann, Christoph Schnorr, and Gabriele Steidl.
2005. Combined SVM-Based Feature Selection and Clas-
sification. Machine Learning, 61(1-3):129–150.
Truc-Vien T. Nguyen, Alessandro Moschitti, and Giuseppe
Riccardi. 2009a. Convolution kernels on constituent, de-
pendency and sequential structures for relation extraction.
In Proceedings of EMNLP.
Truc-Vien T. Nguyen, Alessandro Moschitti, and Giuseppe
Riccardi. 2009b. Convolution kernels on constituent,
dependency and sequential structures for relation extrac-
tion. In EMNLP ’09: Proceedings of the 2009 Conference
on Empirical Methods in Natural Language Processing,
pages 1378–1387, Morristown, NJ, USA. Association for
Computational Linguistics.
Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005.
The proposition bank: An annotated corpus of semantic
roles. Comput. Linguist., 31(1):71–106.
J. Pei, J. Han, Mortazavi B. Asl, H. Pinto, Q. Chen, U. Dayal,
and M. C. Hsu. 2001. PrefixSpan Mining Sequential Pat-
terns Efficiently by Prefix Projected Pattern Growth. In
Proceedings of ICDE’01.
</reference>
<page confidence="0.931008">
232
</page>
<reference confidence="0.999849595238095">
Daniele Pighin and Alessandro Moschitti. 2009a. Efficient
linearization of tree kernel functions. In Proceedings of
CoNLL’09.
Daniele Pighin and Alessandro Moschitti. 2009b. Reverse
engineering of tree kernel feature spaces. In Proceedings
of EMNLP, pages 111–120, Singapore, August. Associa-
tion for Computational Linguistics.
Libin Shen, Anoop Sarkar, and Aravind k. Joshi. 2003. Us-
ing LTAG Based Features in Parse Reranking. In Proceed-
ings of EMNLP’06.
Mihai Surdeanu, Massimiliano Ciaramita, and Hugo
Zaragoza. 2008. Learning to rank answers on large online
QA collections. In Proceedings of ACL-08: HLT, Colum-
bus, Ohio.
Jun Suzuki and Hideki Isozaki. 2005. Sequence and Tree
Kernels with Statistical Feature Mining. In Proceedings
of NIPS’05.
Ivan Titov and James Henderson. 2006. Porting statisti-
cal parsers with data-defined kernels. In Proceedings of
CoNLL-X.
Kristina Toutanova, Penka Markova, and Christopher Man-
ning. 2004. The Leaf Path Projection View of Parse
Trees: Exploring String Kernels for HPSG Parse Selec-
tion. In Proceedings of EMNLP 2004.
Ellen M. Voorhees. 2001. Overview of the trec 2001 ques-
tion answering track. In In Proceedings of the Tenth Text
REtrieval Conference (TREC, pages 42–51.
Jason Weston, Sayan Mukherjee, Olivier Chapelle, Massimil-
iano Pontil, Tomaso Poggio, and Vladimir Vapnik. 2001.
Feature Selection for SVMs. In Proceedings of NIPS’01.
Yiming Yang and Jan O. Pedersen. 1997. A comparative
study on feature selection in text categorization. In Dou-
glas H. Fisher, editor, Proceedings of ICML-97, 14th In-
ternational Conference on Machine Learning, pages 412–
420, Nashville, US. Morgan Kaufmann Publishers, San
Francisco, US.
Dell Zhang and Wee Sun Lee. 2003. Question classifica-
tion using support vector machines. In Proceedings of SI-
GIR’03, pages 26–32.
Min Zhang, Jie Zhang, and Jian Su. 2006. Exploring Syntac-
tic Features for Relation Extraction using a Convolution
tree kernel. In Proceedings of NAACL.
</reference>
<page confidence="0.998956">
233
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.764561">
<title confidence="0.999939">On Reverse Feature Engineering of Syntactic Tree Kernels</title>
<author confidence="0.973278">Daniele</author>
<affiliation confidence="0.949747">FBK-irst, DISI, University of Via di Sommarive,</affiliation>
<address confidence="0.908693">I-38123 Povo (TN)</address>
<email confidence="0.999717">daniele.pighin@gmail.com</email>
<abstract confidence="0.9950628125">In this paper, we provide a theoretical framework for feature selection in tree kernel spaces based on gradient-vector components of kernel-based machines. We show that a huge number of features can be discarded without a significant decrease in accuracy. Our selection algorithm is as accurate as and much more efficient than those proposed in previous work. Comparative experiments on three interesting and very diverse classification tasks, i.e. Question Classification, Relation Extraction and Semantic Role Labeling, support our theoretical findings and demonstrate the algorithm performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Stephan Bloehdorn</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Structure and semantics for expressive text kernels. In</title>
<date>2007</date>
<booktitle>In Proceedings of CIKM ’07.</booktitle>
<contexts>
<context position="18007" citStr="Bloehdorn and Moschitti, 2007" startWordPosition="3259" endWordPosition="3262">ased on x2- driven relevance assessment. With respect to their work, the main differences in the approach that we propose are that we want to exploit the SVM optimizer to select the most relevant features, and to be able to observe the relevant fragments. Regarding work that may directly benefit from reverse kernel engineering is worthwhile mentioning: (Cancedda et al., 2003; Shen et al., 2003; Daum´e III and Marcu, 2004; Giuglea and Moschitti, 2004; Toutanova et al., 2004; Kazama and Torisawa, 2005; Titov and Henderson, 2006; Kate and Mooney, 2006; Zhang et al., 2006; Bloehdorn et al., 2006; Bloehdorn and Moschitti, 2007; Moschitti and Zanzotto, 2007; Surdeanu et al., 2008; Moschitti, 2008; Moschitti and Quarteroni, 2008; Martins et al., 2009; Nguyen et al., 2009a) 5 Mining Fragments Efficiently The high-level description of our feature selection technique is as follows: we start by learning an STK model and we greedily explore the support vectors in search for the most relevant fragments. We store them in an index, and then we decode (or linearize) all the trees in the dataset, i.e. we represent them as vectors in a linear space where only a very small subset of the fragments in the original space are accoun</context>
</contexts>
<marker>Bloehdorn, Moschitti, 2007</marker>
<rawString>Stephan Bloehdorn and Alessandro Moschitti. 2007. Structure and semantics for expressive text kernels. In In Proceedings of CIKM ’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Bloehdorn</author>
<author>Roberto Basili</author>
<author>Marco Cammisa</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Semantic kernels for text classification based on topological measures of feature similarity.</title>
<date>2006</date>
<booktitle>In Proceedings of ICDM 06,</booktitle>
<location>Hong Kong,</location>
<contexts>
<context position="17976" citStr="Bloehdorn et al., 2006" startWordPosition="3255" endWordPosition="3258">or convolution kernels based on x2- driven relevance assessment. With respect to their work, the main differences in the approach that we propose are that we want to exploit the SVM optimizer to select the most relevant features, and to be able to observe the relevant fragments. Regarding work that may directly benefit from reverse kernel engineering is worthwhile mentioning: (Cancedda et al., 2003; Shen et al., 2003; Daum´e III and Marcu, 2004; Giuglea and Moschitti, 2004; Toutanova et al., 2004; Kazama and Torisawa, 2005; Titov and Henderson, 2006; Kate and Mooney, 2006; Zhang et al., 2006; Bloehdorn et al., 2006; Bloehdorn and Moschitti, 2007; Moschitti and Zanzotto, 2007; Surdeanu et al., 2008; Moschitti, 2008; Moschitti and Quarteroni, 2008; Martins et al., 2009; Nguyen et al., 2009a) 5 Mining Fragments Efficiently The high-level description of our feature selection technique is as follows: we start by learning an STK model and we greedily explore the support vectors in search for the most relevant fragments. We store them in an index, and then we decode (or linearize) all the trees in the dataset, i.e. we represent them as vectors in a linear space where only a very small subset of the fragments i</context>
</contexts>
<marker>Bloehdorn, Basili, Cammisa, Moschitti, 2006</marker>
<rawString>Stephan Bloehdorn, Roberto Basili, Marco Cammisa, and Alessandro Moschitti. 2006. Semantic kernels for text classification based on topological measures of feature similarity. In Proceedings of ICDM 06, Hong Kong, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Cancedda</author>
<author>Eric Gaussier</author>
<author>Cyril Goutte</author>
<author>Jean Michel Renders</author>
</authors>
<title>Word sequence kernels.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1059</pages>
<contexts>
<context position="17755" citStr="Cancedda et al., 2003" startWordPosition="3217" endWordPosition="3220">egree polynomial kernel space. The authors discuss an approximation of their method that allows them to handle high degree polynomial kernels. Suzuki and Isozaki (2005) present an embedded approach to feature selection for convolution kernels based on x2- driven relevance assessment. With respect to their work, the main differences in the approach that we propose are that we want to exploit the SVM optimizer to select the most relevant features, and to be able to observe the relevant fragments. Regarding work that may directly benefit from reverse kernel engineering is worthwhile mentioning: (Cancedda et al., 2003; Shen et al., 2003; Daum´e III and Marcu, 2004; Giuglea and Moschitti, 2004; Toutanova et al., 2004; Kazama and Torisawa, 2005; Titov and Henderson, 2006; Kate and Mooney, 2006; Zhang et al., 2006; Bloehdorn et al., 2006; Bloehdorn and Moschitti, 2007; Moschitti and Zanzotto, 2007; Surdeanu et al., 2008; Moschitti, 2008; Moschitti and Quarteroni, 2008; Martins et al., 2009; Nguyen et al., 2009a) 5 Mining Fragments Efficiently The high-level description of our feature selection technique is as follows: we start by learning an STK model and we greedily explore the support vectors in search for </context>
</contexts>
<marker>Cancedda, Gaussier, Goutte, Renders, 2003</marker>
<rawString>Nicola Cancedda, Eric Gaussier, Cyril Goutte, and Jean Michel Renders. 2003. Word sequence kernels. Journal of Machine Learning Research, 3:1059–1082.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>Introduction to the CoNLL-2005 Shared Task: Semantic Role Labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of CoNLL’05.</booktitle>
<marker>Carreras, M`arquez, 2005</marker>
<rawString>Xavier Carreras and Llu´ıs M`arquez. 2005. Introduction to the CoNLL-2005 Shared Task: Semantic Role Labeling. In Proceedings of CoNLL’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>New Ranking Algorithms for Parsing and Tagging: Kernels over Discrete Structures, and the Voted Perceptron.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL’02.</booktitle>
<contexts>
<context position="2554" citStr="Collins and Duffy, 2002" startWordPosition="386" endWordPosition="389">TN) Italy moschitti@disi.unitn.it nents of the weight vector, iV, optimized by Support Vector Machines (SVMs). This method appears to be very effective, as the model accuracy does not significantly decrease even when a large number of features are filtered out. Unfortunately, we could not provide theoretical or intuitive motivations to justify our proposed apporach. In this paper, we present and empirically validate a theory which aims at filling the abovementioned gaps. In particular we provide: (i) a proof of the equation for the exact computation of feature weights induced by TK functions (Collins and Duffy, 2002); (ii) a theoretical characterization of feature selection based on kiVk. We show that if feature selection does not sensibly reduces kiVk, the margin associated with Vi does not sensibly decrease as well. Consequently, the theoretical upperbound to the probability error does not sensibly increases; (iii) a proof that the convolutive nature of TK allows for filtering out an exponential number of features with a small kiVk decrease. The combination of (ii) with (iii) suggests that an extremely aggressive feature selection can be applied. We describe a greedy algorithm that exploits these result</context>
<context position="4375" citStr="Collins and Duffy, 2002" startWordPosition="684" endWordPosition="687">tails the outcome of our experiments; finally, in Section 7 we draw our conclusions. 223 Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 223–233, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics 2 Fragment Weights in TK Spaces The critical step for feature selection in tree kernel spaces is the computation of the weights of features (tree fragments) in the kernel machines’ gradient. The basic parameters are the fragment frequencies which are combined with a decay factor used to downscale the weight of large subtrees (Collins and Duffy, 2002). In this section, after introducing basic kernel concepts, we describe a theorem that establishes the correct weight1 of features in the STK space. 2.1 Kernel Based-Machines Typically, a kernel machine is a linear classifier whose decision function can be expressed as: c(x) = w· x+ b = where x E RN is a classifying example and IV E RN and b E R are the separating hyperplane’s gradient and its bias, respectively. The gradient is a linear combination of E training points xz E RN multiplied by their labels yi E {−1, +1} and their weights αi E R+. Different optimizers use different strategies to </context>
<context position="5774" citStr="Collins and Duffy, 2002" startWordPosition="929" endWordPosition="932">possible to replace the scalar product with a kernel function defined over pairs of objects, which can more efficiently compute it: c(o) = XP αiyik(oi, o) + b, i=1 where k(oi, o) = O(oi) · O(o), with the advantage that we do not need to provide an explicit mapping O : O —* RN of our example objects O in a vector space. In the next section, we show a kernel directly working on syntactic trees. 2.2 Syntactic Tree Kernel (STK) Tree Kernel (TK) functions are convolution kernels (Haussler, 1999) defined over pairs of trees. Different TKs are characterized by alternative fragment definitions, e.g. (Collins and Duffy, 2002; Kashima and Koyanagi, 2002; Moschitti, 2006). We will focus on the syntactic tree kernel described in (Collins and Duffy, 2002), which relies on a fragment definition that does not allow to 1In P&amp;M we provided an approximation of the real weight. break production rules (i.e. if any child of a node is included in a fragment, then also all the other children have to). As such, it is especially indicated for tasks involving constituency parsed texts. Tree kernels compute the number of common substructures between two trees T1 and T2 without explicitly considering the whole feature (fragment) sp</context>
<context position="7862" citStr="Collins and Duffy, 2002" startWordPosition="1325" endWordPosition="1328"> same, and n1 and n2 are not pre-terminals then l(n1) A(n1, n2) = A Y (1 + A(c�n1, c�n2)), (4) =1 where l(n1) is the number of children of n1, c�n is the j-th child of node n and A is a decay factor penalizing larger structures. 2.3 Tree Fragment Weights Eq. 3 shows that A counts the shared fragments rooted in n1 and n2 in the form of scalar product, as evaluated by Eq. 2. However, when A is used in A as in Eq. 4, it changes the weight of the product Xi(n1)Xi(n2). As A multiplies A in each recursion step, we may be induced to assume3 that the 2We will consider it as a weighting function. 3In (Collins and Duffy, 2002), there is a short note about the correct value weight of lambda for each product components (i.e. pairs of fragments). This is in line with the formulation we provide. XP i=1 x+b (1) αiyiei · A(n1, n2) = X |F| i=1 224 weight of a fragment is λd, where d is the depth of the fragment. On the contrary, we show the actual weight by providing the following: Theorem 1. Let T and f be a tree and one of its fragments, respectively, induced by STK. The weight of f accounted by STK is λ lf(n) is the number of children of n in f and s(f) = |{n ∈ T : lf(n) &gt; 0} |is the number of nodes that have active pr</context>
</contexts>
<marker>Collins, Duffy, 2002</marker>
<rawString>Michael Collins and Nigel Duffy. 2002. New Ranking Algorithms for Parsing and Tagging: Kernels over Discrete Structures, and the Voted Perceptron. In Proceedings of ACL’02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Jeffrey Sorensen</author>
</authors>
<title>Dependency Tree Kernels for Relation Extraction.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL’04.</booktitle>
<contexts>
<context position="992" citStr="Culotta and Sorensen, 2004" startWordPosition="139" endWordPosition="142">mber of features can be discarded without a significant decrease in accuracy. Our selection algorithm is as accurate as and much more efficient than those proposed in previous work. Comparative experiments on three interesting and very diverse classification tasks, i.e. Question Classification, Relation Extraction and Semantic Role Labeling, support our theoretical findings and demonstrate the algorithm performance. 1 Introduction Kernel functions are very effective at modeling diverse linguistic phenomena by implicitly representing data in high dimensional spaces, e.g. (Cumby and Roth, 2003; Culotta and Sorensen, 2004; Kudo et al., 2005; Moschitti et al., 2008). However, the implicit nature of the kernel space causes two major drawbacks: (1) high computational costs for learning and classification, and (2) the impossibility to identify the most important features. A solution to both problems is the application of feature selection techniques. In particular, the problem of feature selection in Tree Kernel (TK) spaces has already been addressed by previous work in NLP, e.g. (Kudo and Matsumoto, 2003; Suzuki and Isozaki, 2005). However, these approaches lack a theoretical characterization of the problem that </context>
</contexts>
<marker>Culotta, Sorensen, 2004</marker>
<rawString>Aron Culotta and Jeffrey Sorensen. 2004. Dependency Tree Kernels for Relation Extraction. In Proceedings of ACL’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chad Cumby</author>
<author>Dan Roth</author>
</authors>
<title>Kernel Methods for Relational Learning.</title>
<date>2003</date>
<booktitle>In Proceedings of ICML</booktitle>
<contexts>
<context position="964" citStr="Cumby and Roth, 2003" startWordPosition="135" endWordPosition="138">We show that a huge number of features can be discarded without a significant decrease in accuracy. Our selection algorithm is as accurate as and much more efficient than those proposed in previous work. Comparative experiments on three interesting and very diverse classification tasks, i.e. Question Classification, Relation Extraction and Semantic Role Labeling, support our theoretical findings and demonstrate the algorithm performance. 1 Introduction Kernel functions are very effective at modeling diverse linguistic phenomena by implicitly representing data in high dimensional spaces, e.g. (Cumby and Roth, 2003; Culotta and Sorensen, 2004; Kudo et al., 2005; Moschitti et al., 2008). However, the implicit nature of the kernel space causes two major drawbacks: (1) high computational costs for learning and classification, and (2) the impossibility to identify the most important features. A solution to both problems is the application of feature selection techniques. In particular, the problem of feature selection in Tree Kernel (TK) spaces has already been addressed by previous work in NLP, e.g. (Kudo and Matsumoto, 2003; Suzuki and Isozaki, 2005). However, these approaches lack a theoretical character</context>
</contexts>
<marker>Cumby, Roth, 2003</marker>
<rawString>Chad Cumby and Dan Roth. 2003. Kernel Methods for Relational Learning. In Proceedings of ICML 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>Np bracketing by maximum entropy tagging and SVM reranking.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP’04.</booktitle>
<marker>Daum´e, Marcu, 2004</marker>
<rawString>Hal Daum´e III and Daniel Marcu. 2004. Np bracketing by maximum entropy tagging and SVM reranking. In Proceedings of EMNLP’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Doddington</author>
<author>A Mitchell</author>
<author>M Przybocki</author>
<author>L Ramshaw</author>
<author>S Strassel</author>
<author>R Weischedel</author>
</authors>
<title>The Automatic Content Extraction (ACE) Program–Tasks, Data, and Evaluation.</title>
<date>2004</date>
<booktitle>Proceedings of LREC 2004,</booktitle>
<pages>837--840</pages>
<contexts>
<context position="22663" citStr="Doddington et al., 2004" startWordPosition="4109" endWordPosition="4113">from the same tree ti, we can conclude that x�j) i &lt; x�k) i bfi,j E £fi,k . In other words, for each tree in the model, base fragments are the most relevant, and we can assume that the relevance of the heaviest fragment is an upper bound for the relevance of any fragment 4. 6 Experiments We ran a set of thorough experiments to support our claims with empirical evidence. We show our results on three very different benchmarks: Question Classification (QC) using TREC 10 data (Voorhees, 2001), Relation Extraction (RE) based on the newswire and broadcast news domain of the ACE 2004 English corpus (Doddington et al., 2004) and Semantic Role Labeling (SRL) on the CoNLL 2005 shared task data (Carreras and M`arquez, 2005). In the next sections we elaborate on the setup and outcome of each set of experiments. As a supervised learning framework we used SVM-Light-TK5, which extends the SVM-Light optimizer (Joachims, 2000) with support for tree kernel functions. Unless differently stated, all the classifiers are parametrized for optimal Precision and Recall on a development set, obtained by selecting one example in ten from the training set with the same positive-to-negative example ratio. The results that we show are</context>
</contexts>
<marker>Doddington, Mitchell, Przybocki, Ramshaw, Strassel, Weischedel, 2004</marker>
<rawString>G. Doddington, A. Mitchell, M. Przybocki, L. Ramshaw, S. Strassel, and R. Weischedel. 2004. The Automatic Content Extraction (ACE) Program–Tasks, Data, and Evaluation. Proceedings of LREC 2004, pages 837– 840.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana-Maria Giuglea</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Knowledge Discovering using FrameNet, VerbNet and PropBank. In</title>
<date>2004</date>
<booktitle>In Proceedings of the Workshop on Ontology and Knowledge Discovering at ECML 2004,</booktitle>
<location>Pisa, Italy.</location>
<contexts>
<context position="17831" citStr="Giuglea and Moschitti, 2004" startWordPosition="3230" endWordPosition="3234"> their method that allows them to handle high degree polynomial kernels. Suzuki and Isozaki (2005) present an embedded approach to feature selection for convolution kernels based on x2- driven relevance assessment. With respect to their work, the main differences in the approach that we propose are that we want to exploit the SVM optimizer to select the most relevant features, and to be able to observe the relevant fragments. Regarding work that may directly benefit from reverse kernel engineering is worthwhile mentioning: (Cancedda et al., 2003; Shen et al., 2003; Daum´e III and Marcu, 2004; Giuglea and Moschitti, 2004; Toutanova et al., 2004; Kazama and Torisawa, 2005; Titov and Henderson, 2006; Kate and Mooney, 2006; Zhang et al., 2006; Bloehdorn et al., 2006; Bloehdorn and Moschitti, 2007; Moschitti and Zanzotto, 2007; Surdeanu et al., 2008; Moschitti, 2008; Moschitti and Quarteroni, 2008; Martins et al., 2009; Nguyen et al., 2009a) 5 Mining Fragments Efficiently The high-level description of our feature selection technique is as follows: we start by learning an STK model and we greedily explore the support vectors in search for the most relevant fragments. We store them in an index, and then we decode (</context>
</contexts>
<marker>Giuglea, Moschitti, 2004</marker>
<rawString>Ana-Maria Giuglea and Alessandro Moschitti. 2004. Knowledge Discovering using FrameNet, VerbNet and PropBank. In In Proceedings of the Workshop on Ontology and Knowledge Discovering at ECML 2004, Pisa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isabelle Guyon</author>
<author>Andr´e Elisseeff</author>
</authors>
<title>An introduction to variable and feature selection.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1157</pages>
<contexts>
<context position="16757" citStr="Guyon and Elisseeff (2003)" startWordPosition="3056" endWordPosition="3059"> is different for several important reasons: (i) we focus on structured features with a syntactic nature, which show different behaviour from lexical ones, e.g. they tend to be more sparse; (ii) in the TK space, the a-priori weights are very skewed, and large fragments receive exponentially lower scores than small ones; (iii) there is high redundancy and inter-dependency between such features; (iv) we want to be able to observe the most relevant features automatically generated by TKs; and (v) the huge number of features makes it impossible to evaluate the weight of each feature individually. Guyon and Elisseeff (2003) carries out a very informative survey of feature selection techniques. Non-filter approaches for SVMs and kernel machines are often concerned with polynomial and 226 Gaussian kernels, e.g. (Weston et al., 2001; Neumann et al., 2005). In (Kudo and Matsumoto, 2003), an extension of the PrefixSpan algorithm (Pei et al., 2001) is used to efficiently mine the features in a low degree polynomial kernel space. The authors discuss an approximation of their method that allows them to handle high degree polynomial kernels. Suzuki and Isozaki (2005) present an embedded approach to feature selection for </context>
</contexts>
<marker>Guyon, Elisseeff, 2003</marker>
<rawString>Isabelle Guyon and Andr´e Elisseeff. 2003. An introduction to variable and feature selection. Journal of Machine Learning Research, 3:1157–1182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Haussler</author>
</authors>
<title>Convolution kernels on discrete structures.</title>
<date>1999</date>
<tech>Technical report,</tech>
<institution>Dept. of Computer Science, University of California at Santa Cruz.</institution>
<contexts>
<context position="5646" citStr="Haussler, 1999" startWordPosition="913" endWordPosition="914">ize the distance between positive and negative examples, i.e. the margin -y. Applying the so-called kernel trick, it is possible to replace the scalar product with a kernel function defined over pairs of objects, which can more efficiently compute it: c(o) = XP αiyik(oi, o) + b, i=1 where k(oi, o) = O(oi) · O(o), with the advantage that we do not need to provide an explicit mapping O : O —* RN of our example objects O in a vector space. In the next section, we show a kernel directly working on syntactic trees. 2.2 Syntactic Tree Kernel (STK) Tree Kernel (TK) functions are convolution kernels (Haussler, 1999) defined over pairs of trees. Different TKs are characterized by alternative fragment definitions, e.g. (Collins and Duffy, 2002; Kashima and Koyanagi, 2002; Moschitti, 2006). We will focus on the syntactic tree kernel described in (Collins and Duffy, 2002), which relies on a fragment definition that does not allow to 1In P&amp;M we provided an approximation of the real weight. break production rules (i.e. if any child of a node is included in a fragment, then also all the other children have to). As such, it is especially indicated for tasks involving constituency parsed texts. Tree kernels compu</context>
</contexts>
<marker>Haussler, 1999</marker>
<rawString>David Haussler. 1999. Convolution kernels on discrete structures. Technical report, Dept. of Computer Science, University of California at Santa Cruz.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Estimating the generalization performance of a SVM efficiently.</title>
<date>2000</date>
<booktitle>In Proceedings of ICML’00.</booktitle>
<contexts>
<context position="22962" citStr="Joachims, 2000" startWordPosition="4161" endWordPosition="4162">rough experiments to support our claims with empirical evidence. We show our results on three very different benchmarks: Question Classification (QC) using TREC 10 data (Voorhees, 2001), Relation Extraction (RE) based on the newswire and broadcast news domain of the ACE 2004 English corpus (Doddington et al., 2004) and Semantic Role Labeling (SRL) on the CoNLL 2005 shared task data (Carreras and M`arquez, 2005). In the next sections we elaborate on the setup and outcome of each set of experiments. As a supervised learning framework we used SVM-Light-TK5, which extends the SVM-Light optimizer (Joachims, 2000) with support for tree kernel functions. Unless differently stated, all the classifiers are parametrized for optimal Precision and Recall on a development set, obtained by selecting one example in ten from the training set with the same positive-to-negative example ratio. The results that we show are obtained on the test sets by using all the available data for training. For multi-class scenarios, the classifiers are arranged in a one vs. 4In principle, the weight of some fragment encoded in the model M may be greater than B. However, as an empirical justification, we report that in all our ex</context>
</contexts>
<marker>Joachims, 2000</marker>
<rawString>T. Joachims. 2000. Estimating the generalization performance of a SVM efficiently. In Proceedings of ICML’00.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hisashi Kashima</author>
<author>Teruo Koyanagi</author>
</authors>
<title>Kernels for semi-structured data.</title>
<date>2002</date>
<booktitle>In Proceedings of ICML’02.</booktitle>
<contexts>
<context position="5802" citStr="Kashima and Koyanagi, 2002" startWordPosition="933" endWordPosition="936">calar product with a kernel function defined over pairs of objects, which can more efficiently compute it: c(o) = XP αiyik(oi, o) + b, i=1 where k(oi, o) = O(oi) · O(o), with the advantage that we do not need to provide an explicit mapping O : O —* RN of our example objects O in a vector space. In the next section, we show a kernel directly working on syntactic trees. 2.2 Syntactic Tree Kernel (STK) Tree Kernel (TK) functions are convolution kernels (Haussler, 1999) defined over pairs of trees. Different TKs are characterized by alternative fragment definitions, e.g. (Collins and Duffy, 2002; Kashima and Koyanagi, 2002; Moschitti, 2006). We will focus on the syntactic tree kernel described in (Collins and Duffy, 2002), which relies on a fragment definition that does not allow to 1In P&amp;M we provided an approximation of the real weight. break production rules (i.e. if any child of a node is included in a fragment, then also all the other children have to). As such, it is especially indicated for tasks involving constituency parsed texts. Tree kernels compute the number of common substructures between two trees T1 and T2 without explicitly considering the whole feature (fragment) space. Let F = {f1, f2,. . . ,</context>
</contexts>
<marker>Kashima, Koyanagi, 2002</marker>
<rawString>Hisashi Kashima and Teruo Koyanagi. 2002. Kernels for semi-structured data. In Proceedings of ICML’02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rohit J Kate</author>
<author>Raymond J Mooney</author>
</authors>
<title>Using stringkernels for learning semantic parsers.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st ICCL and 44th Annual Meeting of the ACL,</booktitle>
<pages>913--920</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="17932" citStr="Kate and Mooney, 2006" startWordPosition="3247" endWordPosition="3250">an embedded approach to feature selection for convolution kernels based on x2- driven relevance assessment. With respect to their work, the main differences in the approach that we propose are that we want to exploit the SVM optimizer to select the most relevant features, and to be able to observe the relevant fragments. Regarding work that may directly benefit from reverse kernel engineering is worthwhile mentioning: (Cancedda et al., 2003; Shen et al., 2003; Daum´e III and Marcu, 2004; Giuglea and Moschitti, 2004; Toutanova et al., 2004; Kazama and Torisawa, 2005; Titov and Henderson, 2006; Kate and Mooney, 2006; Zhang et al., 2006; Bloehdorn et al., 2006; Bloehdorn and Moschitti, 2007; Moschitti and Zanzotto, 2007; Surdeanu et al., 2008; Moschitti, 2008; Moschitti and Quarteroni, 2008; Martins et al., 2009; Nguyen et al., 2009a) 5 Mining Fragments Efficiently The high-level description of our feature selection technique is as follows: we start by learning an STK model and we greedily explore the support vectors in search for the most relevant fragments. We store them in an index, and then we decode (or linearize) all the trees in the dataset, i.e. we represent them as vectors in a linear space where</context>
</contexts>
<marker>Kate, Mooney, 2006</marker>
<rawString>Rohit J. Kate and Raymond J. Mooney. 2006. Using stringkernels for learning semantic parsers. In Proceedings of the 21st ICCL and 44th Annual Meeting of the ACL, pages 913–920, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Fast methods for kernel-based text analysis.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL’03.</booktitle>
<contexts>
<context position="1481" citStr="Kudo and Matsumoto, 2003" startWordPosition="218" endWordPosition="221">linguistic phenomena by implicitly representing data in high dimensional spaces, e.g. (Cumby and Roth, 2003; Culotta and Sorensen, 2004; Kudo et al., 2005; Moschitti et al., 2008). However, the implicit nature of the kernel space causes two major drawbacks: (1) high computational costs for learning and classification, and (2) the impossibility to identify the most important features. A solution to both problems is the application of feature selection techniques. In particular, the problem of feature selection in Tree Kernel (TK) spaces has already been addressed by previous work in NLP, e.g. (Kudo and Matsumoto, 2003; Suzuki and Isozaki, 2005). However, these approaches lack a theoretical characterization of the problem that could support and justify the design of more effective algorithms. In (Pighin and Moschitti, 2009a) and (Pighin and Moschitti, 2009b) (P&amp;M), we presented a heuristic framework for feature selection in kernel spaces that selects features based on the compoAlessandro Moschitti DISI, University of Trento Via di Sommarive, 14 I-38123 Povo (TN) Italy moschitti@disi.unitn.it nents of the weight vector, iV, optimized by Support Vector Machines (SVMs). This method appears to be very effective</context>
<context position="17021" citStr="Kudo and Matsumoto, 2003" startWordPosition="3098" endWordPosition="3101">ents receive exponentially lower scores than small ones; (iii) there is high redundancy and inter-dependency between such features; (iv) we want to be able to observe the most relevant features automatically generated by TKs; and (v) the huge number of features makes it impossible to evaluate the weight of each feature individually. Guyon and Elisseeff (2003) carries out a very informative survey of feature selection techniques. Non-filter approaches for SVMs and kernel machines are often concerned with polynomial and 226 Gaussian kernels, e.g. (Weston et al., 2001; Neumann et al., 2005). In (Kudo and Matsumoto, 2003), an extension of the PrefixSpan algorithm (Pei et al., 2001) is used to efficiently mine the features in a low degree polynomial kernel space. The authors discuss an approximation of their method that allows them to handle high degree polynomial kernels. Suzuki and Isozaki (2005) present an embedded approach to feature selection for convolution kernels based on x2- driven relevance assessment. With respect to their work, the main differences in the approach that we propose are that we want to exploit the SVM optimizer to select the most relevant features, and to be able to observe the relevan</context>
</contexts>
<marker>Kudo, Matsumoto, 2003</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2003. Fast methods for kernel-based text analysis. In Proceedings of ACL’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Jun Suzuki</author>
<author>Hideki Isozaki</author>
</authors>
<title>Boostingbased parse reranking with subtree features.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL’05.</booktitle>
<contexts>
<context position="1011" citStr="Kudo et al., 2005" startWordPosition="143" endWordPosition="146">arded without a significant decrease in accuracy. Our selection algorithm is as accurate as and much more efficient than those proposed in previous work. Comparative experiments on three interesting and very diverse classification tasks, i.e. Question Classification, Relation Extraction and Semantic Role Labeling, support our theoretical findings and demonstrate the algorithm performance. 1 Introduction Kernel functions are very effective at modeling diverse linguistic phenomena by implicitly representing data in high dimensional spaces, e.g. (Cumby and Roth, 2003; Culotta and Sorensen, 2004; Kudo et al., 2005; Moschitti et al., 2008). However, the implicit nature of the kernel space causes two major drawbacks: (1) high computational costs for learning and classification, and (2) the impossibility to identify the most important features. A solution to both problems is the application of feature selection techniques. In particular, the problem of feature selection in Tree Kernel (TK) spaces has already been addressed by previous work in NLP, e.g. (Kudo and Matsumoto, 2003; Suzuki and Isozaki, 2005). However, these approaches lack a theoretical characterization of the problem that could support and j</context>
</contexts>
<marker>Kudo, Suzuki, Isozaki, 2005</marker>
<rawString>Taku Kudo, Jun Suzuki, and Hideki Isozaki. 2005. Boostingbased parse reranking with subtree features. In Proceedings of ACL’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xin Li</author>
<author>Dan Roth</author>
</authors>
<title>Learning question classifiers: the role of semantic information.</title>
<date>2006</date>
<journal>Natural Language Engineering,</journal>
<volume>12</volume>
<issue>3</issue>
<contexts>
<context position="24383" citStr="Li and Roth, 2006" startWordPosition="4386" endWordPosition="4389">n.it/˜moschitt/Tree-Kernel.htm all configuration, where each sentence is a positive example for one of the classes, and negative for the others. While binary classifiers are evaluated in terms of F1 measure, for multi-class classifiers we show the final accuracy. The next paragraphs describe the datasets used for the experiments. Question Classification (QC) Given a question, the task consists in selecting the most appropriate expected answer type from a given set of possibilities. We adopted the question taxonomy known as coarse grained, which has been described in (Zhang and Lee, 2003) and (Li and Roth, 2006), consisting of six non overlapping classes: Abbreviations (ABBR), Descriptions (DESC, e.g. definitions or explanations), Entity (ENTY, e.g. animal, body or color), Human (HUM, e.g. group or individual), Location (LOC, e.g. cities or countries) and Numeric (NUM, e.g. amounts or dates). The TREC 10 QA data set accounts for 6,000 questions. For each question, we generate the full parse of the sentence and use it to train our models. Automatic parses are obtained with the Stanford parser6 (Klein and Manning, 2003), and we actually have only 5,953 sentences in our data set due to parsing issues. D</context>
</contexts>
<marker>Li, Roth, 2006</marker>
<rawString>Xin Li and Dan Roth. 2006. Learning question classifiers: the role of semantic information. Natural Language Engineering, 12(3):229–249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e F T Martins</author>
<author>Noah A Smith</author>
<author>Eric P Xing</author>
<author>Pedro M Q Aguiar</author>
<author>M´ario A T Figueiredo</author>
</authors>
<title>Nonextensive information theoretic kernels on measures.</title>
<date>2009</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>10--935</pages>
<contexts>
<context position="18131" citStr="Martins et al., 2009" startWordPosition="3278" endWordPosition="3281"> want to exploit the SVM optimizer to select the most relevant features, and to be able to observe the relevant fragments. Regarding work that may directly benefit from reverse kernel engineering is worthwhile mentioning: (Cancedda et al., 2003; Shen et al., 2003; Daum´e III and Marcu, 2004; Giuglea and Moschitti, 2004; Toutanova et al., 2004; Kazama and Torisawa, 2005; Titov and Henderson, 2006; Kate and Mooney, 2006; Zhang et al., 2006; Bloehdorn et al., 2006; Bloehdorn and Moschitti, 2007; Moschitti and Zanzotto, 2007; Surdeanu et al., 2008; Moschitti, 2008; Moschitti and Quarteroni, 2008; Martins et al., 2009; Nguyen et al., 2009a) 5 Mining Fragments Efficiently The high-level description of our feature selection technique is as follows: we start by learning an STK model and we greedily explore the support vectors in search for the most relevant fragments. We store them in an index, and then we decode (or linearize) all the trees in the dataset, i.e. we represent them as vectors in a linear space where only a very small subset of the fragments in the original space are accounted for. These vectors are then employed for learning and classification in the linear space. To explore the fragment space </context>
</contexts>
<marker>Martins, Smith, Xing, Aguiar, Figueiredo, 2009</marker>
<rawString>Andr´e F. T. Martins, Noah A. Smith, Eric P. Xing, Pedro M. Q. Aguiar, and M´ario A. T. Figueiredo. 2009. Nonextensive information theoretic kernels on measures. J. Mach. Learn. Res., 10:935–975.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
<author>Silvia Quarteroni</author>
</authors>
<title>Kernels on linguistic structures for answer extraction.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT, Short Papers,</booktitle>
<location>Columbus, Ohio.</location>
<contexts>
<context position="18109" citStr="Moschitti and Quarteroni, 2008" startWordPosition="3274" endWordPosition="3277">oach that we propose are that we want to exploit the SVM optimizer to select the most relevant features, and to be able to observe the relevant fragments. Regarding work that may directly benefit from reverse kernel engineering is worthwhile mentioning: (Cancedda et al., 2003; Shen et al., 2003; Daum´e III and Marcu, 2004; Giuglea and Moschitti, 2004; Toutanova et al., 2004; Kazama and Torisawa, 2005; Titov and Henderson, 2006; Kate and Mooney, 2006; Zhang et al., 2006; Bloehdorn et al., 2006; Bloehdorn and Moschitti, 2007; Moschitti and Zanzotto, 2007; Surdeanu et al., 2008; Moschitti, 2008; Moschitti and Quarteroni, 2008; Martins et al., 2009; Nguyen et al., 2009a) 5 Mining Fragments Efficiently The high-level description of our feature selection technique is as follows: we start by learning an STK model and we greedily explore the support vectors in search for the most relevant fragments. We store them in an index, and then we decode (or linearize) all the trees in the dataset, i.e. we represent them as vectors in a linear space where only a very small subset of the fragments in the original space are accounted for. These vectors are then employed for learning and classification in the linear space. To explo</context>
</contexts>
<marker>Moschitti, Quarteroni, 2008</marker>
<rawString>Alessandro Moschitti and Silvia Quarteroni. 2008. Kernels on linguistic structures for answer extraction. In Proceedings of ACL-08: HLT, Short Papers, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
<author>Fabio Massimo Zanzotto</author>
</authors>
<title>Fast and effective kernels for relational learning from texts.</title>
<date>2007</date>
<booktitle>In Zoubin Ghahramani, editor, Proceedings of the 24th Annual International Conference on Machine Learning (ICML</booktitle>
<contexts>
<context position="18037" citStr="Moschitti and Zanzotto, 2007" startWordPosition="3263" endWordPosition="3267">sessment. With respect to their work, the main differences in the approach that we propose are that we want to exploit the SVM optimizer to select the most relevant features, and to be able to observe the relevant fragments. Regarding work that may directly benefit from reverse kernel engineering is worthwhile mentioning: (Cancedda et al., 2003; Shen et al., 2003; Daum´e III and Marcu, 2004; Giuglea and Moschitti, 2004; Toutanova et al., 2004; Kazama and Torisawa, 2005; Titov and Henderson, 2006; Kate and Mooney, 2006; Zhang et al., 2006; Bloehdorn et al., 2006; Bloehdorn and Moschitti, 2007; Moschitti and Zanzotto, 2007; Surdeanu et al., 2008; Moschitti, 2008; Moschitti and Quarteroni, 2008; Martins et al., 2009; Nguyen et al., 2009a) 5 Mining Fragments Efficiently The high-level description of our feature selection technique is as follows: we start by learning an STK model and we greedily explore the support vectors in search for the most relevant fragments. We store them in an index, and then we decode (or linearize) all the trees in the dataset, i.e. we represent them as vectors in a linear space where only a very small subset of the fragments in the original space are accounted for. These vectors are the</context>
</contexts>
<marker>Moschitti, Zanzotto, 2007</marker>
<rawString>Alessandro Moschitti and Fabio Massimo Zanzotto. 2007. Fast and effective kernels for relational learning from texts. In Zoubin Ghahramani, editor, Proceedings of the 24th Annual International Conference on Machine Learning (ICML 2007).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
<author>Daniele Pighin</author>
<author>Roberto Basili</author>
</authors>
<title>Tree kernels for semantic role labeling.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="1036" citStr="Moschitti et al., 2008" startWordPosition="147" endWordPosition="150">nificant decrease in accuracy. Our selection algorithm is as accurate as and much more efficient than those proposed in previous work. Comparative experiments on three interesting and very diverse classification tasks, i.e. Question Classification, Relation Extraction and Semantic Role Labeling, support our theoretical findings and demonstrate the algorithm performance. 1 Introduction Kernel functions are very effective at modeling diverse linguistic phenomena by implicitly representing data in high dimensional spaces, e.g. (Cumby and Roth, 2003; Culotta and Sorensen, 2004; Kudo et al., 2005; Moschitti et al., 2008). However, the implicit nature of the kernel space causes two major drawbacks: (1) high computational costs for learning and classification, and (2) the impossibility to identify the most important features. A solution to both problems is the application of feature selection techniques. In particular, the problem of feature selection in Tree Kernel (TK) spaces has already been addressed by previous work in NLP, e.g. (Kudo and Matsumoto, 2003; Suzuki and Isozaki, 2005). However, these approaches lack a theoretical characterization of the problem that could support and justify the design of more</context>
</contexts>
<marker>Moschitti, Pighin, Basili, 2008</marker>
<rawString>Alessandro Moschitti, Daniele Pighin, and Roberto Basili. 2008. Tree kernels for semantic role labeling. Computational Linguistics, 34(2):193–224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Efficient convolution kernels for dependency and constituent syntactic trees.</title>
<date>2006</date>
<booktitle>In Proceedings of ECML’06,</booktitle>
<pages>318--329</pages>
<contexts>
<context position="5820" citStr="Moschitti, 2006" startWordPosition="937" endWordPosition="938">function defined over pairs of objects, which can more efficiently compute it: c(o) = XP αiyik(oi, o) + b, i=1 where k(oi, o) = O(oi) · O(o), with the advantage that we do not need to provide an explicit mapping O : O —* RN of our example objects O in a vector space. In the next section, we show a kernel directly working on syntactic trees. 2.2 Syntactic Tree Kernel (STK) Tree Kernel (TK) functions are convolution kernels (Haussler, 1999) defined over pairs of trees. Different TKs are characterized by alternative fragment definitions, e.g. (Collins and Duffy, 2002; Kashima and Koyanagi, 2002; Moschitti, 2006). We will focus on the syntactic tree kernel described in (Collins and Duffy, 2002), which relies on a fragment definition that does not allow to 1In P&amp;M we provided an approximation of the real weight. break production rules (i.e. if any child of a node is included in a fragment, then also all the other children have to). As such, it is especially indicated for tasks involving constituency parsed texts. Tree kernels compute the number of common substructures between two trees T1 and T2 without explicitly considering the whole feature (fragment) space. Let F = {f1, f2,. . . , f|F|} be the set </context>
</contexts>
<marker>Moschitti, 2006</marker>
<rawString>Alessandro Moschitti. 2006. Efficient convolution kernels for dependency and constituent syntactic trees. In Proceedings of ECML’06, pages 318–329.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Kernel methods, syntax and semantics for relational text categorization.</title>
<date>2008</date>
<booktitle>In Proceeding of CIKM ’08,</booktitle>
<location>NY, USA.</location>
<contexts>
<context position="18077" citStr="Moschitti, 2008" startWordPosition="3272" endWordPosition="3273">ences in the approach that we propose are that we want to exploit the SVM optimizer to select the most relevant features, and to be able to observe the relevant fragments. Regarding work that may directly benefit from reverse kernel engineering is worthwhile mentioning: (Cancedda et al., 2003; Shen et al., 2003; Daum´e III and Marcu, 2004; Giuglea and Moschitti, 2004; Toutanova et al., 2004; Kazama and Torisawa, 2005; Titov and Henderson, 2006; Kate and Mooney, 2006; Zhang et al., 2006; Bloehdorn et al., 2006; Bloehdorn and Moschitti, 2007; Moschitti and Zanzotto, 2007; Surdeanu et al., 2008; Moschitti, 2008; Moschitti and Quarteroni, 2008; Martins et al., 2009; Nguyen et al., 2009a) 5 Mining Fragments Efficiently The high-level description of our feature selection technique is as follows: we start by learning an STK model and we greedily explore the support vectors in search for the most relevant fragments. We store them in an index, and then we decode (or linearize) all the trees in the dataset, i.e. we represent them as vectors in a linear space where only a very small subset of the fragments in the original space are accounted for. These vectors are then employed for learning and classificati</context>
</contexts>
<marker>Moschitti, 2008</marker>
<rawString>Alessandro Moschitti. 2008. Kernel methods, syntax and semantics for relational text categorization. In Proceeding of CIKM ’08, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Neumann</author>
<author>Christoph Schnorr</author>
<author>Gabriele Steidl</author>
</authors>
<date>2005</date>
<booktitle>Combined SVM-Based Feature Selection and Classification. Machine Learning,</booktitle>
<pages>61--1</pages>
<contexts>
<context position="16990" citStr="Neumann et al., 2005" startWordPosition="3092" endWordPosition="3096">ery skewed, and large fragments receive exponentially lower scores than small ones; (iii) there is high redundancy and inter-dependency between such features; (iv) we want to be able to observe the most relevant features automatically generated by TKs; and (v) the huge number of features makes it impossible to evaluate the weight of each feature individually. Guyon and Elisseeff (2003) carries out a very informative survey of feature selection techniques. Non-filter approaches for SVMs and kernel machines are often concerned with polynomial and 226 Gaussian kernels, e.g. (Weston et al., 2001; Neumann et al., 2005). In (Kudo and Matsumoto, 2003), an extension of the PrefixSpan algorithm (Pei et al., 2001) is used to efficiently mine the features in a low degree polynomial kernel space. The authors discuss an approximation of their method that allows them to handle high degree polynomial kernels. Suzuki and Isozaki (2005) present an embedded approach to feature selection for convolution kernels based on x2- driven relevance assessment. With respect to their work, the main differences in the approach that we propose are that we want to exploit the SVM optimizer to select the most relevant features, and to</context>
</contexts>
<marker>Neumann, Schnorr, Steidl, 2005</marker>
<rawString>Julia Neumann, Christoph Schnorr, and Gabriele Steidl. 2005. Combined SVM-Based Feature Selection and Classification. Machine Learning, 61(1-3):129–150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Truc-Vien T Nguyen</author>
<author>Alessandro Moschitti</author>
<author>Giuseppe Riccardi</author>
</authors>
<title>Convolution kernels on constituent, dependency and sequential structures for relation extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="18152" citStr="Nguyen et al., 2009" startWordPosition="3282" endWordPosition="3285">VM optimizer to select the most relevant features, and to be able to observe the relevant fragments. Regarding work that may directly benefit from reverse kernel engineering is worthwhile mentioning: (Cancedda et al., 2003; Shen et al., 2003; Daum´e III and Marcu, 2004; Giuglea and Moschitti, 2004; Toutanova et al., 2004; Kazama and Torisawa, 2005; Titov and Henderson, 2006; Kate and Mooney, 2006; Zhang et al., 2006; Bloehdorn et al., 2006; Bloehdorn and Moschitti, 2007; Moschitti and Zanzotto, 2007; Surdeanu et al., 2008; Moschitti, 2008; Moschitti and Quarteroni, 2008; Martins et al., 2009; Nguyen et al., 2009a) 5 Mining Fragments Efficiently The high-level description of our feature selection technique is as follows: we start by learning an STK model and we greedily explore the support vectors in search for the most relevant fragments. We store them in an index, and then we decode (or linearize) all the trees in the dataset, i.e. we represent them as vectors in a linear space where only a very small subset of the fragments in the original space are accounted for. These vectors are then employed for learning and classification in the linear space. To explore the fragment space defined by a set of s</context>
<context position="25970" citStr="Nguyen et al., 2009" startWordPosition="4625" endWordPosition="4628">gz. Relation Extraction (RE) The corpus consists of 348 documents, and contains seven relation classes defined over pairs of mentions: Physical, Person/Social, Employment/Membership/Subsidiary, Agent-Artifact, PER/ORG Affiliation, GPE Affiliation, and Discourse. There are 4,400 positive and 38,696 negative examples when the potential relations are generated using all the entity/mention pairs in the same sentence. Documents are parsed using the Stanford Parser, where the nodes of the entities are enriched with information about the entity type. Overall, we used the setting and data defined in (Nguyen et al., 2009b). 6http://nlp.stanford.edu/software/lex-parser.shtml s(fk) A2 228 Semantic Role Labeling (SRL) SRL can be decomposed into two tasks: boundary detection, where the word sequences that are arguments of a predicate word w are identified, and role classification, where each argument is assigned the proper role. For these experiments we concentrated on this latter task and used exactly the same setup as P&amp;M. We considered all the argument nodes of any of the six PropBank (Palmer et al., 2005) core roles7 (i.e. A0, ... , A5) from all the available training sections, i.e. 2 through 21, for a total </context>
<context position="32631" citStr="Nguyen et al., 2009" startWordPosition="5782" endWordPosition="5785">nd clearly show how, after selecting the optimal number of fragments (1,000-10,000 i.e. 60-65% of the norm), the accuracy of the OPT and CHI classifiers stabilize around levels of accuracy which are in line with STK. STK OPT/Grad F1 F1 Frags QC 83.70 84.12 —2k RE 67.53 66.31 —10k SRL 87.56 88.17 —300k Table 2: Multiclass classification accuracy on three benchmarks. Finally, Table 2 shows the best results that we achieved on the three multi-class classification tasks, i.e. QC, RE9 and SRL, and compares them against the STK 10. For all the tasks OPT/Grad 9For RE, we show lower accuracy than in (Nguyen et al., 2009b) since, to have a closer comparison with STK, we do not combine structural features with manual designed features. 10We should point out that this models are only partially 90 85 80 75 70 65 60 55 50 LIN/Grad OPT/Grad LIN/Chi OPT/Chi STK 0.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6 0.65 0.7 1—p 0.1 0.2 0.3 0.4 0.5 0.6 40 20 90 80 70 60 50 30 10 0 LIN/Grad OPT/Grad LIN/Chi OPT/Chi STK 90 80 70 60 50 40 30 20 OPT/Grad OPT/Chi STK F1 (LOC) F1 (DESC) Multiclass accuracy 230 produces the best results for all the tests, even though the difference with OPT/Chi is generally not statistically significant. Out</context>
</contexts>
<marker>Nguyen, Moschitti, Riccardi, 2009</marker>
<rawString>Truc-Vien T. Nguyen, Alessandro Moschitti, and Giuseppe Riccardi. 2009a. Convolution kernels on constituent, dependency and sequential structures for relation extraction. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Truc-Vien T Nguyen</author>
<author>Alessandro Moschitti</author>
<author>Giuseppe Riccardi</author>
</authors>
<title>Convolution kernels on constituent, dependency and sequential structures for relation extraction.</title>
<date>2009</date>
<booktitle>In EMNLP ’09: Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1378--1387</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="18152" citStr="Nguyen et al., 2009" startWordPosition="3282" endWordPosition="3285">VM optimizer to select the most relevant features, and to be able to observe the relevant fragments. Regarding work that may directly benefit from reverse kernel engineering is worthwhile mentioning: (Cancedda et al., 2003; Shen et al., 2003; Daum´e III and Marcu, 2004; Giuglea and Moschitti, 2004; Toutanova et al., 2004; Kazama and Torisawa, 2005; Titov and Henderson, 2006; Kate and Mooney, 2006; Zhang et al., 2006; Bloehdorn et al., 2006; Bloehdorn and Moschitti, 2007; Moschitti and Zanzotto, 2007; Surdeanu et al., 2008; Moschitti, 2008; Moschitti and Quarteroni, 2008; Martins et al., 2009; Nguyen et al., 2009a) 5 Mining Fragments Efficiently The high-level description of our feature selection technique is as follows: we start by learning an STK model and we greedily explore the support vectors in search for the most relevant fragments. We store them in an index, and then we decode (or linearize) all the trees in the dataset, i.e. we represent them as vectors in a linear space where only a very small subset of the fragments in the original space are accounted for. These vectors are then employed for learning and classification in the linear space. To explore the fragment space defined by a set of s</context>
<context position="25970" citStr="Nguyen et al., 2009" startWordPosition="4625" endWordPosition="4628">gz. Relation Extraction (RE) The corpus consists of 348 documents, and contains seven relation classes defined over pairs of mentions: Physical, Person/Social, Employment/Membership/Subsidiary, Agent-Artifact, PER/ORG Affiliation, GPE Affiliation, and Discourse. There are 4,400 positive and 38,696 negative examples when the potential relations are generated using all the entity/mention pairs in the same sentence. Documents are parsed using the Stanford Parser, where the nodes of the entities are enriched with information about the entity type. Overall, we used the setting and data defined in (Nguyen et al., 2009b). 6http://nlp.stanford.edu/software/lex-parser.shtml s(fk) A2 228 Semantic Role Labeling (SRL) SRL can be decomposed into two tasks: boundary detection, where the word sequences that are arguments of a predicate word w are identified, and role classification, where each argument is assigned the proper role. For these experiments we concentrated on this latter task and used exactly the same setup as P&amp;M. We considered all the argument nodes of any of the six PropBank (Palmer et al., 2005) core roles7 (i.e. A0, ... , A5) from all the available training sections, i.e. 2 through 21, for a total </context>
<context position="32631" citStr="Nguyen et al., 2009" startWordPosition="5782" endWordPosition="5785">nd clearly show how, after selecting the optimal number of fragments (1,000-10,000 i.e. 60-65% of the norm), the accuracy of the OPT and CHI classifiers stabilize around levels of accuracy which are in line with STK. STK OPT/Grad F1 F1 Frags QC 83.70 84.12 —2k RE 67.53 66.31 —10k SRL 87.56 88.17 —300k Table 2: Multiclass classification accuracy on three benchmarks. Finally, Table 2 shows the best results that we achieved on the three multi-class classification tasks, i.e. QC, RE9 and SRL, and compares them against the STK 10. For all the tasks OPT/Grad 9For RE, we show lower accuracy than in (Nguyen et al., 2009b) since, to have a closer comparison with STK, we do not combine structural features with manual designed features. 10We should point out that this models are only partially 90 85 80 75 70 65 60 55 50 LIN/Grad OPT/Grad LIN/Chi OPT/Chi STK 0.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6 0.65 0.7 1—p 0.1 0.2 0.3 0.4 0.5 0.6 40 20 90 80 70 60 50 30 10 0 LIN/Grad OPT/Grad LIN/Chi OPT/Chi STK 90 80 70 60 50 40 30 20 OPT/Grad OPT/Chi STK F1 (LOC) F1 (DESC) Multiclass accuracy 230 produces the best results for all the tests, even though the difference with OPT/Chi is generally not statistically significant. Out</context>
</contexts>
<marker>Nguyen, Moschitti, Riccardi, 2009</marker>
<rawString>Truc-Vien T. Nguyen, Alessandro Moschitti, and Giuseppe Riccardi. 2009b. Convolution kernels on constituent, dependency and sequential structures for relation extraction. In EMNLP ’09: Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1378–1387, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The proposition bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Comput. Linguist.,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="26464" citStr="Palmer et al., 2005" startWordPosition="4703" endWordPosition="4706">ies are enriched with information about the entity type. Overall, we used the setting and data defined in (Nguyen et al., 2009b). 6http://nlp.stanford.edu/software/lex-parser.shtml s(fk) A2 228 Semantic Role Labeling (SRL) SRL can be decomposed into two tasks: boundary detection, where the word sequences that are arguments of a predicate word w are identified, and role classification, where each argument is assigned the proper role. For these experiments we concentrated on this latter task and used exactly the same setup as P&amp;M. We considered all the argument nodes of any of the six PropBank (Palmer et al., 2005) core roles7 (i.e. A0, ... , A5) from all the available training sections, i.e. 2 through 21, for a total of 179,091 training instances. Similarly, we collected 9,277 test instances from the annotations of Section 23. 6.1 Model Comparison To show the validity of Lemma 1 in practical scenarios, we compare the accuracy of our linearized models against vanilla STK classifiers. We designed two types of classifiers: LIN, a linearized STK model, which uses the weights estimated by the learner in the STK space and linearized examples; in other words LIN uses wIN. It allows us to measure exactly the l</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The proposition bank: An annotated corpus of semantic roles. Comput. Linguist., 31(1):71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pei</author>
<author>J Han</author>
<author>Mortazavi B Asl</author>
<author>H Pinto</author>
<author>Q Chen</author>
<author>U Dayal</author>
<author>M C Hsu</author>
</authors>
<title>PrefixSpan Mining Sequential Patterns Efficiently by Prefix Projected Pattern Growth.</title>
<date>2001</date>
<booktitle>In Proceedings of ICDE’01.</booktitle>
<contexts>
<context position="17082" citStr="Pei et al., 2001" startWordPosition="3109" endWordPosition="3112">is high redundancy and inter-dependency between such features; (iv) we want to be able to observe the most relevant features automatically generated by TKs; and (v) the huge number of features makes it impossible to evaluate the weight of each feature individually. Guyon and Elisseeff (2003) carries out a very informative survey of feature selection techniques. Non-filter approaches for SVMs and kernel machines are often concerned with polynomial and 226 Gaussian kernels, e.g. (Weston et al., 2001; Neumann et al., 2005). In (Kudo and Matsumoto, 2003), an extension of the PrefixSpan algorithm (Pei et al., 2001) is used to efficiently mine the features in a low degree polynomial kernel space. The authors discuss an approximation of their method that allows them to handle high degree polynomial kernels. Suzuki and Isozaki (2005) present an embedded approach to feature selection for convolution kernels based on x2- driven relevance assessment. With respect to their work, the main differences in the approach that we propose are that we want to exploit the SVM optimizer to select the most relevant features, and to be able to observe the relevant fragments. Regarding work that may directly benefit from re</context>
</contexts>
<marker>Pei, Han, Asl, Pinto, Chen, Dayal, Hsu, 2001</marker>
<rawString>J. Pei, J. Han, Mortazavi B. Asl, H. Pinto, Q. Chen, U. Dayal, and M. C. Hsu. 2001. PrefixSpan Mining Sequential Patterns Efficiently by Prefix Projected Pattern Growth. In Proceedings of ICDE’01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniele Pighin</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Efficient linearization of tree kernel functions.</title>
<date>2009</date>
<booktitle>In Proceedings of CoNLL’09.</booktitle>
<contexts>
<context position="1689" citStr="Pighin and Moschitti, 2009" startWordPosition="249" endWordPosition="252">ure of the kernel space causes two major drawbacks: (1) high computational costs for learning and classification, and (2) the impossibility to identify the most important features. A solution to both problems is the application of feature selection techniques. In particular, the problem of feature selection in Tree Kernel (TK) spaces has already been addressed by previous work in NLP, e.g. (Kudo and Matsumoto, 2003; Suzuki and Isozaki, 2005). However, these approaches lack a theoretical characterization of the problem that could support and justify the design of more effective algorithms. In (Pighin and Moschitti, 2009a) and (Pighin and Moschitti, 2009b) (P&amp;M), we presented a heuristic framework for feature selection in kernel spaces that selects features based on the compoAlessandro Moschitti DISI, University of Trento Via di Sommarive, 14 I-38123 Povo (TN) Italy moschitti@disi.unitn.it nents of the weight vector, iV, optimized by Support Vector Machines (SVMs). This method appears to be very effective, as the model accuracy does not significantly decrease even when a large number of features are filtered out. Unfortunately, we could not provide theoretical or intuitive motivations to justify our proposed </context>
<context position="33527" citStr="Pighin and Moschitti, 2009" startWordPosition="5948" endWordPosition="5951">5 0.6 0.65 0.7 1—p 0.1 0.2 0.3 0.4 0.5 0.6 40 20 90 80 70 60 50 30 10 0 LIN/Grad OPT/Grad LIN/Chi OPT/Chi STK 90 80 70 60 50 40 30 20 OPT/Grad OPT/Chi STK F1 (LOC) F1 (DESC) Multiclass accuracy 230 produces the best results for all the tests, even though the difference with OPT/Chi is generally not statistically significant. Out of three tasks, OPT/Grad manages to slightly improve two of them, i.e. QC (84.12 vs. 83.7) and SRL (88.17 vs. 87.56), while STK is more accurate on RE, i.e. 67.53 vs. 66.31. 6.3 Comparison with P&amp;M The results on SRL can be compared against those that we presented in (Pighin and Moschitti, 2009a), where we measured an accuracy of 87.13 exactly on the same benchmark. As we can see in Table 2, our model improves the classification accuracy of about 1 point, i.e. 88.17. On the other hand, such comparison is not really fair since the algorithms rely on different parameter sets, and it is almost impossible to find matching configurations for the different versions of the algorithms that would result in exactly the same number of fragments. In a projected space with approximately 103 or 104 fragments, including a few hundred more features can produce noticeably different accuracy readings</context>
</contexts>
<marker>Pighin, Moschitti, 2009</marker>
<rawString>Daniele Pighin and Alessandro Moschitti. 2009a. Efficient linearization of tree kernel functions. In Proceedings of CoNLL’09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniele Pighin</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Reverse engineering of tree kernel feature spaces.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>111--120</pages>
<contexts>
<context position="1689" citStr="Pighin and Moschitti, 2009" startWordPosition="249" endWordPosition="252">ure of the kernel space causes two major drawbacks: (1) high computational costs for learning and classification, and (2) the impossibility to identify the most important features. A solution to both problems is the application of feature selection techniques. In particular, the problem of feature selection in Tree Kernel (TK) spaces has already been addressed by previous work in NLP, e.g. (Kudo and Matsumoto, 2003; Suzuki and Isozaki, 2005). However, these approaches lack a theoretical characterization of the problem that could support and justify the design of more effective algorithms. In (Pighin and Moschitti, 2009a) and (Pighin and Moschitti, 2009b) (P&amp;M), we presented a heuristic framework for feature selection in kernel spaces that selects features based on the compoAlessandro Moschitti DISI, University of Trento Via di Sommarive, 14 I-38123 Povo (TN) Italy moschitti@disi.unitn.it nents of the weight vector, iV, optimized by Support Vector Machines (SVMs). This method appears to be very effective, as the model accuracy does not significantly decrease even when a large number of features are filtered out. Unfortunately, we could not provide theoretical or intuitive motivations to justify our proposed </context>
<context position="33527" citStr="Pighin and Moschitti, 2009" startWordPosition="5948" endWordPosition="5951">5 0.6 0.65 0.7 1—p 0.1 0.2 0.3 0.4 0.5 0.6 40 20 90 80 70 60 50 30 10 0 LIN/Grad OPT/Grad LIN/Chi OPT/Chi STK 90 80 70 60 50 40 30 20 OPT/Grad OPT/Chi STK F1 (LOC) F1 (DESC) Multiclass accuracy 230 produces the best results for all the tests, even though the difference with OPT/Chi is generally not statistically significant. Out of three tasks, OPT/Grad manages to slightly improve two of them, i.e. QC (84.12 vs. 83.7) and SRL (88.17 vs. 87.56), while STK is more accurate on RE, i.e. 67.53 vs. 66.31. 6.3 Comparison with P&amp;M The results on SRL can be compared against those that we presented in (Pighin and Moschitti, 2009a), where we measured an accuracy of 87.13 exactly on the same benchmark. As we can see in Table 2, our model improves the classification accuracy of about 1 point, i.e. 88.17. On the other hand, such comparison is not really fair since the algorithms rely on different parameter sets, and it is almost impossible to find matching configurations for the different versions of the algorithms that would result in exactly the same number of fragments. In a projected space with approximately 103 or 104 fragments, including a few hundred more features can produce noticeably different accuracy readings</context>
</contexts>
<marker>Pighin, Moschitti, 2009</marker>
<rawString>Daniele Pighin and Alessandro Moschitti. 2009b. Reverse engineering of tree kernel feature spaces. In Proceedings of EMNLP, pages 111–120, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Anoop Sarkar</author>
<author>Aravind k Joshi</author>
</authors>
<title>Using LTAG Based Features in Parse Reranking.</title>
<date>2003</date>
<booktitle>In Proceedings of EMNLP’06.</booktitle>
<contexts>
<context position="17774" citStr="Shen et al., 2003" startWordPosition="3221" endWordPosition="3224"> space. The authors discuss an approximation of their method that allows them to handle high degree polynomial kernels. Suzuki and Isozaki (2005) present an embedded approach to feature selection for convolution kernels based on x2- driven relevance assessment. With respect to their work, the main differences in the approach that we propose are that we want to exploit the SVM optimizer to select the most relevant features, and to be able to observe the relevant fragments. Regarding work that may directly benefit from reverse kernel engineering is worthwhile mentioning: (Cancedda et al., 2003; Shen et al., 2003; Daum´e III and Marcu, 2004; Giuglea and Moschitti, 2004; Toutanova et al., 2004; Kazama and Torisawa, 2005; Titov and Henderson, 2006; Kate and Mooney, 2006; Zhang et al., 2006; Bloehdorn et al., 2006; Bloehdorn and Moschitti, 2007; Moschitti and Zanzotto, 2007; Surdeanu et al., 2008; Moschitti, 2008; Moschitti and Quarteroni, 2008; Martins et al., 2009; Nguyen et al., 2009a) 5 Mining Fragments Efficiently The high-level description of our feature selection technique is as follows: we start by learning an STK model and we greedily explore the support vectors in search for the most relevant f</context>
</contexts>
<marker>Shen, Sarkar, Joshi, 2003</marker>
<rawString>Libin Shen, Anoop Sarkar, and Aravind k. Joshi. 2003. Using LTAG Based Features in Parse Reranking. In Proceedings of EMNLP’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Massimiliano Ciaramita</author>
<author>Hugo Zaragoza</author>
</authors>
<title>Learning to rank answers on large online QA collections.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<location>Columbus, Ohio.</location>
<contexts>
<context position="18060" citStr="Surdeanu et al., 2008" startWordPosition="3268" endWordPosition="3271">r work, the main differences in the approach that we propose are that we want to exploit the SVM optimizer to select the most relevant features, and to be able to observe the relevant fragments. Regarding work that may directly benefit from reverse kernel engineering is worthwhile mentioning: (Cancedda et al., 2003; Shen et al., 2003; Daum´e III and Marcu, 2004; Giuglea and Moschitti, 2004; Toutanova et al., 2004; Kazama and Torisawa, 2005; Titov and Henderson, 2006; Kate and Mooney, 2006; Zhang et al., 2006; Bloehdorn et al., 2006; Bloehdorn and Moschitti, 2007; Moschitti and Zanzotto, 2007; Surdeanu et al., 2008; Moschitti, 2008; Moschitti and Quarteroni, 2008; Martins et al., 2009; Nguyen et al., 2009a) 5 Mining Fragments Efficiently The high-level description of our feature selection technique is as follows: we start by learning an STK model and we greedily explore the support vectors in search for the most relevant fragments. We store them in an index, and then we decode (or linearize) all the trees in the dataset, i.e. we represent them as vectors in a linear space where only a very small subset of the fragments in the original space are accounted for. These vectors are then employed for learning</context>
</contexts>
<marker>Surdeanu, Ciaramita, Zaragoza, 2008</marker>
<rawString>Mihai Surdeanu, Massimiliano Ciaramita, and Hugo Zaragoza. 2008. Learning to rank answers on large online QA collections. In Proceedings of ACL-08: HLT, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Suzuki</author>
<author>Hideki Isozaki</author>
</authors>
<title>Sequence and Tree Kernels with Statistical Feature Mining.</title>
<date>2005</date>
<booktitle>In Proceedings of NIPS’05.</booktitle>
<contexts>
<context position="1508" citStr="Suzuki and Isozaki, 2005" startWordPosition="222" endWordPosition="225">plicitly representing data in high dimensional spaces, e.g. (Cumby and Roth, 2003; Culotta and Sorensen, 2004; Kudo et al., 2005; Moschitti et al., 2008). However, the implicit nature of the kernel space causes two major drawbacks: (1) high computational costs for learning and classification, and (2) the impossibility to identify the most important features. A solution to both problems is the application of feature selection techniques. In particular, the problem of feature selection in Tree Kernel (TK) spaces has already been addressed by previous work in NLP, e.g. (Kudo and Matsumoto, 2003; Suzuki and Isozaki, 2005). However, these approaches lack a theoretical characterization of the problem that could support and justify the design of more effective algorithms. In (Pighin and Moschitti, 2009a) and (Pighin and Moschitti, 2009b) (P&amp;M), we presented a heuristic framework for feature selection in kernel spaces that selects features based on the compoAlessandro Moschitti DISI, University of Trento Via di Sommarive, 14 I-38123 Povo (TN) Italy moschitti@disi.unitn.it nents of the weight vector, iV, optimized by Support Vector Machines (SVMs). This method appears to be very effective, as the model accuracy doe</context>
<context position="17302" citStr="Suzuki and Isozaki (2005)" startWordPosition="3144" endWordPosition="3147"> to evaluate the weight of each feature individually. Guyon and Elisseeff (2003) carries out a very informative survey of feature selection techniques. Non-filter approaches for SVMs and kernel machines are often concerned with polynomial and 226 Gaussian kernels, e.g. (Weston et al., 2001; Neumann et al., 2005). In (Kudo and Matsumoto, 2003), an extension of the PrefixSpan algorithm (Pei et al., 2001) is used to efficiently mine the features in a low degree polynomial kernel space. The authors discuss an approximation of their method that allows them to handle high degree polynomial kernels. Suzuki and Isozaki (2005) present an embedded approach to feature selection for convolution kernels based on x2- driven relevance assessment. With respect to their work, the main differences in the approach that we propose are that we want to exploit the SVM optimizer to select the most relevant features, and to be able to observe the relevant fragments. Regarding work that may directly benefit from reverse kernel engineering is worthwhile mentioning: (Cancedda et al., 2003; Shen et al., 2003; Daum´e III and Marcu, 2004; Giuglea and Moschitti, 2004; Toutanova et al., 2004; Kazama and Torisawa, 2005; Titov and Henderso</context>
<context position="27606" citStr="Suzuki and Isozaki, 2005" startWordPosition="4899" endWordPosition="4902">arized examples; in other words LIN uses wIN. It allows us to measure exactly the loss in accuracy with respect to the reduction of ||i9||. OPT, a linearized STK model that is reoptimized in the linear space, i.e. for which we retrained an SVM using the linearized training examples as input data. Since the LIN solution is part of the candidate solutions from which OPT is selected, we always expect higher accuracy from it. Additionally, we compare selection based on gradient w� (as detailed in Section 2.4) against to x2 selection, which evaluates the relevance of features, in a similar way to (Suzuki and Isozaki, 2005). The relevance of a fragment is calculated as 2 N(yN − Mx)2 X x(N − x)M(N − M) , where N is the number of support vectors, M is the number of positive vectors (i.e. αi &gt; 0), and x and y are the fractions of N and M where the fragment is instantiated, respectively. We specify the selection models by means of Grad for the former and Chi for the latter. For example, a model called OPT/Grad is a re-trained model using the features selected according the highest gradient weights, while LIN/Chi would be a linearized tree kernel model using x2 for feature selection. 7We do not consider adjuncts beca</context>
</contexts>
<marker>Suzuki, Isozaki, 2005</marker>
<rawString>Jun Suzuki and Hideki Isozaki. 2005. Sequence and Tree Kernels with Statistical Feature Mining. In Proceedings of NIPS’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>James Henderson</author>
</authors>
<title>Porting statistical parsers with data-defined kernels.</title>
<date>2006</date>
<booktitle>In Proceedings of CoNLL-X.</booktitle>
<contexts>
<context position="17909" citStr="Titov and Henderson, 2006" startWordPosition="3243" endWordPosition="3246">and Isozaki (2005) present an embedded approach to feature selection for convolution kernels based on x2- driven relevance assessment. With respect to their work, the main differences in the approach that we propose are that we want to exploit the SVM optimizer to select the most relevant features, and to be able to observe the relevant fragments. Regarding work that may directly benefit from reverse kernel engineering is worthwhile mentioning: (Cancedda et al., 2003; Shen et al., 2003; Daum´e III and Marcu, 2004; Giuglea and Moschitti, 2004; Toutanova et al., 2004; Kazama and Torisawa, 2005; Titov and Henderson, 2006; Kate and Mooney, 2006; Zhang et al., 2006; Bloehdorn et al., 2006; Bloehdorn and Moschitti, 2007; Moschitti and Zanzotto, 2007; Surdeanu et al., 2008; Moschitti, 2008; Moschitti and Quarteroni, 2008; Martins et al., 2009; Nguyen et al., 2009a) 5 Mining Fragments Efficiently The high-level description of our feature selection technique is as follows: we start by learning an STK model and we greedily explore the support vectors in search for the most relevant fragments. We store them in an index, and then we decode (or linearize) all the trees in the dataset, i.e. we represent them as vectors </context>
</contexts>
<marker>Titov, Henderson, 2006</marker>
<rawString>Ivan Titov and James Henderson. 2006. Porting statistical parsers with data-defined kernels. In Proceedings of CoNLL-X.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Penka Markova</author>
<author>Christopher Manning</author>
</authors>
<title>The Leaf Path Projection View of Parse Trees: Exploring String Kernels for HPSG Parse Selection.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<contexts>
<context position="17855" citStr="Toutanova et al., 2004" startWordPosition="3235" endWordPosition="3238">m to handle high degree polynomial kernels. Suzuki and Isozaki (2005) present an embedded approach to feature selection for convolution kernels based on x2- driven relevance assessment. With respect to their work, the main differences in the approach that we propose are that we want to exploit the SVM optimizer to select the most relevant features, and to be able to observe the relevant fragments. Regarding work that may directly benefit from reverse kernel engineering is worthwhile mentioning: (Cancedda et al., 2003; Shen et al., 2003; Daum´e III and Marcu, 2004; Giuglea and Moschitti, 2004; Toutanova et al., 2004; Kazama and Torisawa, 2005; Titov and Henderson, 2006; Kate and Mooney, 2006; Zhang et al., 2006; Bloehdorn et al., 2006; Bloehdorn and Moschitti, 2007; Moschitti and Zanzotto, 2007; Surdeanu et al., 2008; Moschitti, 2008; Moschitti and Quarteroni, 2008; Martins et al., 2009; Nguyen et al., 2009a) 5 Mining Fragments Efficiently The high-level description of our feature selection technique is as follows: we start by learning an STK model and we greedily explore the support vectors in search for the most relevant fragments. We store them in an index, and then we decode (or linearize) all the tr</context>
</contexts>
<marker>Toutanova, Markova, Manning, 2004</marker>
<rawString>Kristina Toutanova, Penka Markova, and Christopher Manning. 2004. The Leaf Path Projection View of Parse Trees: Exploring String Kernels for HPSG Parse Selection. In Proceedings of EMNLP 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Voorhees</author>
</authors>
<title>Overview of the trec</title>
<date>2001</date>
<booktitle>In Proceedings of the Tenth Text REtrieval Conference (TREC,</booktitle>
<pages>42--51</pages>
<contexts>
<context position="22532" citStr="Voorhees, 2001" startWordPosition="4090" endWordPosition="4091">ther parts of the tree as well. Therefore, if £f is the set of expansions of f, for every two fragments fi,j, fi,k coming from the same tree ti, we can conclude that x�j) i &lt; x�k) i bfi,j E £fi,k . In other words, for each tree in the model, base fragments are the most relevant, and we can assume that the relevance of the heaviest fragment is an upper bound for the relevance of any fragment 4. 6 Experiments We ran a set of thorough experiments to support our claims with empirical evidence. We show our results on three very different benchmarks: Question Classification (QC) using TREC 10 data (Voorhees, 2001), Relation Extraction (RE) based on the newswire and broadcast news domain of the ACE 2004 English corpus (Doddington et al., 2004) and Semantic Role Labeling (SRL) on the CoNLL 2005 shared task data (Carreras and M`arquez, 2005). In the next sections we elaborate on the setup and outcome of each set of experiments. As a supervised learning framework we used SVM-Light-TK5, which extends the SVM-Light optimizer (Joachims, 2000) with support for tree kernel functions. Unless differently stated, all the classifiers are parametrized for optimal Precision and Recall on a development set, obtained b</context>
</contexts>
<marker>Voorhees, 2001</marker>
<rawString>Ellen M. Voorhees. 2001. Overview of the trec 2001 question answering track. In In Proceedings of the Tenth Text REtrieval Conference (TREC, pages 42–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Weston</author>
<author>Sayan Mukherjee</author>
<author>Olivier Chapelle</author>
<author>Massimiliano Pontil</author>
<author>Tomaso Poggio</author>
<author>Vladimir Vapnik</author>
</authors>
<title>Feature Selection for SVMs.</title>
<date>2001</date>
<booktitle>In Proceedings of NIPS’01.</booktitle>
<contexts>
<context position="16967" citStr="Weston et al., 2001" startWordPosition="3088" endWordPosition="3091">-priori weights are very skewed, and large fragments receive exponentially lower scores than small ones; (iii) there is high redundancy and inter-dependency between such features; (iv) we want to be able to observe the most relevant features automatically generated by TKs; and (v) the huge number of features makes it impossible to evaluate the weight of each feature individually. Guyon and Elisseeff (2003) carries out a very informative survey of feature selection techniques. Non-filter approaches for SVMs and kernel machines are often concerned with polynomial and 226 Gaussian kernels, e.g. (Weston et al., 2001; Neumann et al., 2005). In (Kudo and Matsumoto, 2003), an extension of the PrefixSpan algorithm (Pei et al., 2001) is used to efficiently mine the features in a low degree polynomial kernel space. The authors discuss an approximation of their method that allows them to handle high degree polynomial kernels. Suzuki and Isozaki (2005) present an embedded approach to feature selection for convolution kernels based on x2- driven relevance assessment. With respect to their work, the main differences in the approach that we propose are that we want to exploit the SVM optimizer to select the most re</context>
</contexts>
<marker>Weston, Mukherjee, Chapelle, Pontil, Poggio, Vapnik, 2001</marker>
<rawString>Jason Weston, Sayan Mukherjee, Olivier Chapelle, Massimiliano Pontil, Tomaso Poggio, and Vladimir Vapnik. 2001. Feature Selection for SVMs. In Proceedings of NIPS’01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yiming Yang</author>
<author>Jan O Pedersen</author>
</authors>
<title>A comparative study on feature selection in text categorization.</title>
<date>1997</date>
<booktitle>Proceedings of ICML-97, 14th International Conference on Machine Learning,</booktitle>
<pages>412--420</pages>
<editor>In Douglas H. Fisher, editor,</editor>
<publisher>Morgan Kaufmann Publishers,</publisher>
<location>Nashville, US.</location>
<contexts>
<context position="15993" citStr="Yang and Pedersen, 1997" startWordPosition="2931" endWordPosition="2934">ial number of features, i.e. all the subtrees whose size ranges from η to ν. Therefore, according to Lemma 1 and Theorem 2, we can discard an exponential number offeatures with a limited loss in accuracy. Remark 4. Regarding the proposed norm bound, we observe that νk is a rough overestimation of the the real number offragments having size k rooted in the nodes of the target tree t. This suggests that we don’t really need λ &lt; 1/ν. Moreover, in case of soft-margin SVMs, we can bound αi with the value of the trade-offparameter C. 4 Previous Work Initial work on feature selection for text, e.g. (Yang and Pedersen, 1997), has shown that it may improve the accuracy or, at least, improve efficiency while preserving accuracy. Our context for feature selection is different for several important reasons: (i) we focus on structured features with a syntactic nature, which show different behaviour from lexical ones, e.g. they tend to be more sparse; (ii) in the TK space, the a-priori weights are very skewed, and large fragments receive exponentially lower scores than small ones; (iii) there is high redundancy and inter-dependency between such features; (iv) we want to be able to observe the most relevant features aut</context>
</contexts>
<marker>Yang, Pedersen, 1997</marker>
<rawString>Yiming Yang and Jan O. Pedersen. 1997. A comparative study on feature selection in text categorization. In Douglas H. Fisher, editor, Proceedings of ICML-97, 14th International Conference on Machine Learning, pages 412– 420, Nashville, US. Morgan Kaufmann Publishers, San Francisco, US.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dell Zhang</author>
<author>Wee Sun Lee</author>
</authors>
<title>Question classification using support vector machines.</title>
<date>2003</date>
<booktitle>In Proceedings of SIGIR’03,</booktitle>
<pages>26--32</pages>
<contexts>
<context position="24359" citStr="Zhang and Lee, 2003" startWordPosition="4381" endWordPosition="4384">agments. 5http://disi.unitn.it/˜moschitt/Tree-Kernel.htm all configuration, where each sentence is a positive example for one of the classes, and negative for the others. While binary classifiers are evaluated in terms of F1 measure, for multi-class classifiers we show the final accuracy. The next paragraphs describe the datasets used for the experiments. Question Classification (QC) Given a question, the task consists in selecting the most appropriate expected answer type from a given set of possibilities. We adopted the question taxonomy known as coarse grained, which has been described in (Zhang and Lee, 2003) and (Li and Roth, 2006), consisting of six non overlapping classes: Abbreviations (ABBR), Descriptions (DESC, e.g. definitions or explanations), Entity (ENTY, e.g. animal, body or color), Human (HUM, e.g. group or individual), Location (LOC, e.g. cities or countries) and Numeric (NUM, e.g. amounts or dates). The TREC 10 QA data set accounts for 6,000 questions. For each question, we generate the full parse of the sentence and use it to train our models. Automatic parses are obtained with the Stanford parser6 (Klein and Manning, 2003), and we actually have only 5,953 sentences in our data set </context>
</contexts>
<marker>Zhang, Lee, 2003</marker>
<rawString>Dell Zhang and Wee Sun Lee. 2003. Question classification using support vector machines. In Proceedings of SIGIR’03, pages 26–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Jie Zhang</author>
<author>Jian Su</author>
</authors>
<title>Exploring Syntactic Features for Relation Extraction using a Convolution tree kernel.</title>
<date>2006</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="17952" citStr="Zhang et al., 2006" startWordPosition="3251" endWordPosition="3254"> feature selection for convolution kernels based on x2- driven relevance assessment. With respect to their work, the main differences in the approach that we propose are that we want to exploit the SVM optimizer to select the most relevant features, and to be able to observe the relevant fragments. Regarding work that may directly benefit from reverse kernel engineering is worthwhile mentioning: (Cancedda et al., 2003; Shen et al., 2003; Daum´e III and Marcu, 2004; Giuglea and Moschitti, 2004; Toutanova et al., 2004; Kazama and Torisawa, 2005; Titov and Henderson, 2006; Kate and Mooney, 2006; Zhang et al., 2006; Bloehdorn et al., 2006; Bloehdorn and Moschitti, 2007; Moschitti and Zanzotto, 2007; Surdeanu et al., 2008; Moschitti, 2008; Moschitti and Quarteroni, 2008; Martins et al., 2009; Nguyen et al., 2009a) 5 Mining Fragments Efficiently The high-level description of our feature selection technique is as follows: we start by learning an STK model and we greedily explore the support vectors in search for the most relevant fragments. We store them in an index, and then we decode (or linearize) all the trees in the dataset, i.e. we represent them as vectors in a linear space where only a very small s</context>
</contexts>
<marker>Zhang, Zhang, Su, 2006</marker>
<rawString>Min Zhang, Jie Zhang, and Jian Su. 2006. Exploring Syntactic Features for Relation Extraction using a Convolution tree kernel. In Proceedings of NAACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>