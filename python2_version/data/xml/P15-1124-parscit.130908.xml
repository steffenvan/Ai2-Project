<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.823158">
Model-based Word Embeddings from Decompositions of Count Matrices
</title>
<author confidence="0.983548">
Karl Stratos Michael Collins Daniel Hsu
</author>
<affiliation confidence="0.977654">
Columbia University, New York, NY 10027, USA
</affiliation>
<email confidence="0.966338">
{stratos, mcollins, djhsu}@cs.columbia.edu
</email>
<sectionHeader confidence="0.997211" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998555">
This work develops a new statistical un-
derstanding of word embeddings induced
from transformed count data. Using the
class of hidden Markov models (HMMs)
underlying Brown clustering as a genera-
tive model, we demonstrate how canoni-
cal correlation analysis (CCA) and certain
count transformations permit efficient and
effective recovery of model parameters
with lexical semantics. We further show in
experiments that these techniques empir-
ically outperform existing spectral meth-
ods on word similarity and analogy tasks,
and are also competitive with other pop-
ular methods such as WORD2VEC and
GLOVE.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99997404">
The recent spike of interest in dense, low-
dimensional lexical representations—i.e., word
embeddings—is largely due to their ability to cap-
ture subtle syntactic and semantic patterns that
are useful in a variety of natural language tasks.
A successful method for deriving such embed-
dings is the negative sampling training of the
skip-gram model suggested by Mikolov et al.
(2013b) and implemented in the popular software
WORD2VEC. The form of its training objective
was motivated by efficiency considerations, but
has subsequently been interpreted by Levy and
Goldberg (2014b) as seeking a low-rank factor-
ization of a matrix whose entries are word-context
co-occurrence counts, scaled and transformed in
a certain way. This observation sheds new light
on WORD2VEC, yet also raises several new ques-
tions about word embeddings based on decompos-
ing count data. What is the right matrix to de-
compose? Are there rigorous justifications for the
choice of matrix and count transformations?
In this paper, we answer some of these ques-
tions by investigating the decomposition specified
by CCA (Hotelling, 1936), a powerful technique
for inducing generic representations whose com-
putation is efficiently and exactly reduced to that
of a matrix singular value decomposition (SVD).
We build on and strengthen the work of Stratos et
al. (2014) which uses CCA for learning the class
of HMMs underlying Brown clustering. We show
that certain count transformations enhance the ac-
curacy of the estimation method and significantly
improve the empirical performance of word rep-
resentations derived from these model parameters
(Table 1).
In addition to providing a rigorous justifica-
tion for CCA-based word embeddings, we also
supply a general template that encompasses a
range of spectral methods (algorithms employing
SVD) for inducing word embeddings in the lit-
erature, including the method of Levy and Gold-
berg (2014b). In experiments, we demonstrate that
CCA combined with the square-root transforma-
tion achieves the best result among spectral meth-
ods and performs competitively with other popu-
lar methods such as WORD2VEC and GLOVE on
word similarity and analogy tasks. We addition-
ally demonstrate that CCA embeddings provide
the most competitive improvement when used as
features in named-entity recognition (NER).
</bodyText>
<sectionHeader confidence="0.995369" genericHeader="introduction">
2 Notation
</sectionHeader>
<bodyText confidence="0.9998472">
We use [n] to denote the set of integers {1, ... , n}.
We denote the m x m diagonal matrix with values
vi ... vm along the diagonal by diag(vi ... vm).
We write [al ... am] to denote a matrix whose i-
th column is ai. The expected value of a random
variable X is denoted by E[X]. Given a matrix Q
and an exponent a, we distinguish the entrywise
power operation Q(a) (i.e., Q(a) i,j= (Qi,j)a) from
the matrix power operation Qa (defined only for
square Q).
</bodyText>
<page confidence="0.941072">
1282
</page>
<note confidence="0.983228666666667">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1282–1291,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.878292" genericHeader="method">
3 Background in CCA
</sectionHeader>
<bodyText confidence="0.998368833333333">
In this section, we review the variational charac-
terization of CCA. This provides a flexible frame-
work for a wide variety of tasks. CCA seeks to
maximize a statistical quantity known as the Pear-
son correlation coefficient between random vari-
ables L, R E R:
</bodyText>
<equation confidence="0.529458">
E[LR] − E[L]E[R]
Cor(L, R) :=V/E[L2] − E[L]2- /E[R2] − E[R]2
</equation>
<bodyText confidence="0.9995325">
This is a value in [−1, 1] indicating the degree of
linear dependence between L and R.
</bodyText>
<subsectionHeader confidence="0.992835">
3.1 CCA objective
</subsectionHeader>
<bodyText confidence="0.998884166666667">
Let X E Rn and Y E Rn&apos; be two random vectors.
Without loss of generality, we will assume that X
and Y have zero mean.1 Let m &lt; min(n, n0).
CCA can be cast as finding a set of projection vec-
tors (called canonical directions) a1 ... am E Rn
and b1 ... bm E Rn&apos; such that for i = 1... m:
</bodyText>
<equation confidence="0.99473125">
(ai, bi) = arg max Cor(a&gt;X, b&gt;Y ) (1)
a∈Rn, b∈Rn&apos;
Cor(a&gt;X, a&gt;j X) = 0 bj &lt; i
Cor(b&gt;Y, b&gt;j Y ) = 0 bj &lt; i
</equation>
<bodyText confidence="0.991005428571429">
That is, at each i we simultaneously optimize vec-
tors a, b so that the projected random variables
a&gt;X, b&gt;Y E R are maximally correlated, subject
to the constraint that the projections are uncorre-
lated to all previous projections.
Let A := [a1 ... am] and B := [b1 ... bm].
Then we can think of the joint projections
</bodyText>
<equation confidence="0.999037">
X = A&gt;X Y = B&gt;Y (2)
</equation>
<bodyText confidence="0.9999666">
as new m-dimensional representations of the orig-
inal variables that are transformed to be as corre-
lated as possible with each other. Furthermore, of-
ten m « min(n, n0), leading to a dramatic reduc-
tion in dimensionality.
</bodyText>
<subsectionHeader confidence="0.999305">
3.2 Exact solution via SVD
</subsectionHeader>
<bodyText confidence="0.997214666666667">
Eq. (1) is non-convex due to the terms a and b that
interact with each other, so it cannot be solved
exactly using a standard optimization technique.
However, a method based on SVD provides an
efficient and exact solution. See Hardoon et al.
(2004) for a detailed discussion.
</bodyText>
<footnote confidence="0.700159">
1This can be always achieved through data preprocessing
(“centering”).
</footnote>
<equation confidence="0.956797285714286">
Lemma 3.1 (Hotelling (1936)). Assume X and
Y have zero mean. The solution (A, B) to (1)
is given by A = E[XX&gt;]−1/2U and B =
E[Y Y &gt;]−1/2V where the i-th column of U E
Rn×m (V E Rn&apos;×m) is the left (right) singular
vector of
Ω := E[XX&gt;]−1/2E[XY &gt;]E[Y Y &gt;]−1/2 (3)
</equation>
<bodyText confidence="0.975768">
corresponding to the i-th largest singular value σi.
Furthermore, σi = Cor(a&gt;i X, b&gt;iY ).
</bodyText>
<subsectionHeader confidence="0.999704">
3.3 Using CCA for word representations
</subsectionHeader>
<bodyText confidence="0.95931792">
As presented in Section 3.1, CCA is a general
framework that operates on a pair of random vari-
ables. Adapting CCA specifically to inducing
word representations results in a simple recipe for
calculating (3).
A natural approach is to set X to represent a
word and Y to represent the relevant “context”
information about a word. We can use CCA to
project X and Y to a low-dimensional space in
which they are maximally correlated: see Eq. (2).
The projected X can be considered as a new word
representation.
Denote the set of distinct word types by [n]. We
set X, Y E Rn to be one-hot encodings of words
and their associated context words. We define a
context word to be a word occurring within ρ po-
sitions to the left and right (excluding the current
word). For example, with ρ = 1, the following
snippet of text where the current word is “souls”:
Whatever our souls are made of
will generate two samples of X x Y : a pair of
indicator vectors for “souls” and “our”, and a pair
of indicator vectors for “souls” and “are”.
CCA requires performing SVD on the following
matrix Ω E Rn×n:
</bodyText>
<equation confidence="0.998467333333333">
Ω =(E[XX&gt;] − E[X]E[X]&gt;)−1/2
(E[XY &gt;] − E[X]E[Y ]&gt;)
(E[Y Y &gt;] − E[Y ]E[Y ]&gt;)−1/2
</equation>
<bodyText confidence="0.996972571428571">
At a quick glance, this expression looks daunting:
we need to perform matrix inversion and multipli-
cation on potentially large dense matrices. How-
ever, Ω is easily computable with the following
observations:
Observation 1. We can ignore the centering oper-
ation when the sample size is large (Dhillon et al.,
</bodyText>
<page confidence="0.970489">
1283
</page>
<bodyText confidence="0.860812333333333">
2011). To see why, let I(x(i), y(i))}Ni=1 be N sam-
ples of X and Y . Consider the sample estimate of
the term E[XY &gt;] − E[X]E[Y ]&gt;:
</bodyText>
<equation confidence="0.99438225">
XN
x(i)(y(i))&gt; − 1
N2
i=1
</equation>
<bodyText confidence="0.965465538461538">
The first term dominates the expression when N is
large. This is indeed the setting in this task where
the number of samples (word-context pairs in a
corpus) easily tends to billions.
Observation 2. The (uncentered) covariance
matrices E[XX&gt;] and E[Y Y &gt;] are diagonal.
This follows from our definition of the word
and context variables as one-hot encodings since
E[XwXw,] = 0 for w =� w0 and E[YcYc,] = 0 for
c =�c0.
With these observations and the binary definition
of (X, Y ), each entry in Q now has a simple
closed-form solution:
</bodyText>
<equation confidence="0.936917333333333">
P(Xw = 1, Yc = 1) Qw,c = (4)
pP(Xw = 1)P(Yc = 1)
which can be readily estimated from a corpus.
</equation>
<sectionHeader confidence="0.960662" genericHeader="method">
4 Using CCA for parameter estimation
</sectionHeader>
<bodyText confidence="0.999906444444444">
In a less well-known interpretation of Eq. (4),
CCA is seen as a parameter estimation algorithm
for a language model (Stratos et al., 2014). This
model is a restricted class of HMMs introduced by
Brown et al. (1992), henceforth called the Brown
model. In this section, we extend the result of
Stratos et al. (2014) and show that its correctness
is preserved under certain element-wise data trans-
formations.
</bodyText>
<subsectionHeader confidence="0.999823">
4.1 Clustering under a Brown model
</subsectionHeader>
<bodyText confidence="0.996509">
A Brown model is a 5-tuple (n, m, 7r, t, o) for
n, m E N and functions 7r, t, o where
</bodyText>
<listItem confidence="0.97567375">
• [n] is a set of word types.
• [m] is a set of hidden states.
• 7r(h) is the probability of generating h E [m]
in the first position of a sequence.
• t(h0|h) is the probability of generating h0 E
[m] given h E [m].
• o(w|h) is the probability of generating w E
[n] given h E [m].
</listItem>
<bodyText confidence="0.8926573">
Importantly, the model makes the following addi-
tional assumption:
Assumption 4.1 (Brown assumption). For each
word type w E [n], there is a unique hidden state
H(w) E [m] such that o(w|H(w)) &gt; 0 and
o(w|h) = 0 for all h =� H(w).
In other words, this model is an HMM in which
observation states are partitioned by hidden states.
Thus a sequence of N words w1 ... wN E [n]N
has probability 7r(H(w1)) x QN 1 o(wi|H(wi)) x
</bodyText>
<equation confidence="0.694961333333333">
=
QN−1
i=1 t(H(wi+1)|H(wi)).
</equation>
<bodyText confidence="0.886148666666667">
An equivalent definition of a Brown model is
given by organizing the parameters in matrix form.
Under this definition, a Brown model has param-
eters (7r, T, O) where 7r E Rm is a vector and
T E Rm×m, O E Rn×m are matrices whose en-
tries are set to:
</bodyText>
<equation confidence="0.997468333333333">
7rh = 7r(h) h E [m]
Th&apos;,h = t(h0|h) h, h0 E [m]
Ow,h = o(w|h) h E [m], w E [n]
</equation>
<bodyText confidence="0.999225736842105">
Our main interest is in obtaining some represen-
tations of word types that allow us to identify their
associated hidden states under the model. For this
purpose, representing a word by the correspond-
ing row of O is sufficient. To see this, note that
each row of O must have a single nonzero entry
by Assumption 4.1. Let v(w) E Rm be the w-
th row of O normalized to have unit 2-norm: then
v(w) = v(w0) iff H(w) = H(w0). See Figure 1(a)
for illustration.
A crucial aspect of this representational scheme
is that its correctness is invariant to scaling and
rotation. In particular, clustering the normalized
rows of diag(s)Ohaidiag(s2)Q&gt; where Ohai is
any element-wise power of O with any a =� 0,
Q E Rm×m is any orthogonal transformation, and
s1 E Rn and s2 E Rm are any positive vectors
yields the correct clusters under the model. See
Figure 1(b) for illustration.
</bodyText>
<subsectionHeader confidence="0.994283">
4.2 Spectral estimation
</subsectionHeader>
<bodyText confidence="0.999985">
Thus we would like to estimate O and use its rows
for representing word types. But the likelihood
function under the Brown model is non-convex,
making an MLE estimation of the model param-
eters difficult. However, the hard-clustering as-
sumption (Assumption 4.1) allows for a simple
</bodyText>
<equation confidence="0.955115272727273">
1
N
XN
i=1
! NX
x(i)
i=1
!&gt;
y(
)
i
</equation>
<page confidence="0.949385">
1284
</page>
<figure confidence="0.999620526315789">
1
1
1
smile
grin
frown
cringe
smile
grin
frown
cringe
cringe
frown
smile grin
frown
cringe
smile grin
1
(a) (b)
</figure>
<figureCaption confidence="0.8913595">
Figure 1: Visualization of the representational scheme under a Brown model with 2 hidden states. (a)
Normalizing the original rows of O. (b) Normalizing the scaled and rotated rows of O.
</figureCaption>
<bodyText confidence="0.9518962">
spectral method for consistent parameter estima-
tion of O.
To state the theorem, we define an additional
quantity. Let ρ be the number of left/right context
words to consider in CCA. Let (H1,..., HN) E
[m]N be a random sequence of hidden states
drawn from the Brown model where N &gt; 2ρ + 1.
Independently, pick a position I E [ρ + 1, N − ρ]
uniformly at random. Define π˜ E Rm where
˜πh := P(HI = h) for each h E [m].
</bodyText>
<construct confidence="0.9811785">
Theorem 4.1. Assume π˜ &gt; 0 and rank(O) =
rank(T) = m. Assume that a Brown model
(π, T, O) generates a sequence of words. Let
X, Y E Rn be one-hot encodings of words and
</construct>
<bodyText confidence="0.88667675">
their associated context words. Let U E Rnxm
be the matrix of m left singular vectors of Q(a) E
Rnxn corresponding to nonzero singular values
where Q is defined in Eq. (4) and a =� 0:
</bodyText>
<equation confidence="0.999303">
P(Xw = 1, Yc = 1)a
p
P(Xw = 1)aP(Yc = 1)a
</equation>
<bodyText confidence="0.998028666666667">
Then there exists an orthogonal matrix Q E
Rmxm and a positive s E Rm such that U =
O(a/2)diag(s)QT.
This theorem states that the CCA projection of
words in Section 3.3 is the rows of O up to scaling
and rotation even if we raise each element of Q in
Eq. (4) to an arbitrary (nonzero) power. The proof
is a variant of the proof in Stratos et al. (2014) and
is given in Appendix A.
</bodyText>
<subsectionHeader confidence="0.999988">
4.3 Choice of data transformation
</subsectionHeader>
<bodyText confidence="0.997982">
Given a corpus, the sample estimate of Q(a) is
given by:
</bodyText>
<equation confidence="0.85181">
p(5)
#(w)a#(c)a
</equation>
<bodyText confidence="0.999940428571429">
where #(w, c) denotes the co-occurrence count of
word w and context c in the corpus, #(w) :=
Pc #(w, c), and #(c) := Pw #(w, c). What
choice of a is beneficial and why? We use a = 1/2
for the following reason: it stabilizes the variance
of the term and thereby gives a more statistically
stable solution.
</bodyText>
<subsubsectionHeader confidence="0.899315">
4.3.1 Variance stabilization for word counts
</subsubsectionHeader>
<bodyText confidence="0.994508545454546">
The square-root transformation is a variance-
stabilizing transformation for Poisson random
variables (Bartlett, 1936; Anscombe, 1948). In
particular, the square-root of a Poisson variable
has variance close to 1/4, independent of its mean.
Lemma 4.1 (Bartlett (1936)). Let X be a random
variable with distribution Poisson(n x p) for any
p E (0, 1) and positive integer n. Define Y :=
-/X. Then the variance of Y approaches 1/4 as
n — *oc.
This transformation is relevant for word counts
because they can be naturally modeled as Pois-
son variables. Indeed, if word counts in a corpus
of length N are drawn from a multinomial distri-
bution over [n] with N observations, then these
counts have the same distribution as n indepen-
dent Poisson variables (whose rate parameters are
related to the multinomial probabilities), condi-
tioned on their sum equaling N (Steel, 1953). Em-
pirically, the peaky concentration of a Poisson dis-
tribution is well-suited for modeling word occur-
rences.
</bodyText>
<subsectionHeader confidence="0.491073">
4.3.2 Variance-weighted squared-error
minimization
</subsectionHeader>
<bodyText confidence="0.99851475">
At the heart of CCA is computing the SVD of the
Q(a) matrix: this can be interpreted as solving the
following (non-convex) squared-error minimiza-
tion problem:
</bodyText>
<figure confidence="0.476037">
�Q(a) T \ 2
w,c —uwvcJl
Q(a)
w,c
ˆQ(a)
w,c
#(w, c)a
Xmin
uw,vcERm
w,c
</figure>
<page confidence="0.940674">
1285
</page>
<bodyText confidence="0.998500222222222">
But we note that minimizing unweighted squared-
error objectives is generally suboptimal when the
target values are heteroscedastic. For instance, in
linear regression, it is well-known that a weighted
least squares estimator dominates ordinary least
squares in terms of statistical efficiency (Aitken,
1936; Lehmann and Casella, 1998). For our set-
ting, the analogous weighted least squares opti-
mization is:
</bodyText>
<equation confidence="0.975106666666667">
12
l Q(aic − u&gt;wvc/ (6)
Q(a) ((a)1 l
</equation>
<bodyText confidence="0.995783">
where Var(X) := E[X2] − E[X]2. This optimiza-
tion is, unfortunately, generally intractable (Sre-
bro et al., 2003). The square-root transformation,
nevertheless, obviates the variance-based weight-
ing since the target values have approximately the
same variance of 1/4.
</bodyText>
<sectionHeader confidence="0.975588" genericHeader="method">
5 A template for spectral methods
</sectionHeader>
<bodyText confidence="0.989557580645161">
Figure 2 gives a generic template that encom-
passes a range of spectral methods for deriving
word embeddings. All of them operate on co-
occurrence counts #(w, c) and share the low-rank
SVD step, but they can differ in the data transfor-
mation method (t) and the definition of the matrix
of scaled counts for SVD (s).
We introduce two additional parameters α, β ≤
1 to account for the following details. Mikolov et
al. (2013b) proposed smoothing the empirical con-
text distribution as ˆpα(c) := #(c)α/ Pc #(c)α
and found α = 0.75 to work well in practice. We
also found that setting α = 0.75 gave a small but
consistent improvement over setting α = 1. Note
that the choice of α only affects methods that make
use of the context distribution (s E {ppmi, cca}).
The parameter β controls the role of singular
values in word embeddings. This is always 0
for CCA as it does not require singular values.
But for other methods, one can consider setting
β &gt; 0 since the best-fit subspace for the rows
of Q is given by UE. For example, Deerwester
et al. (1990) use β = 1 and Levy and Goldberg
(2014b) use β = 0.5. However, it has been found
by many (including ourselves) that setting β = 1
yields substantially worse representations than set-
ting β E {0, 0.5} (Levy et al., 2015).
Different combinations of these aspects repro-
duce various spectral embeddings explored in the
literature. We enumerate some meaningful combi-
nations:
</bodyText>
<figureCaption confidence="0.9648085">
Figure 2: A template for spectral word embedding
methods.
</figureCaption>
<bodyText confidence="0.976500916666667">
No scaling [t E {—, log, sqrt}, s = —]. This is
a commonly considered setting (e.g., in Penning-
ton et al. (2014)) where no scaling is applied to the
co-occurrence counts. It is however typically ac-
companied with some kind of data transformation.
Positive point-wise mutual information (PPMI)
[t = —, s = ppmi]. Mutual information is a pop-
ular metric in many natural language tasks (Brown
et al., 1992; Pantel and Lin, 2002). In this setting,
each term in the matrix for SVD is set as the point-
wise mutual information between word w and con-
text c:
</bodyText>
<equation confidence="0.477056">
#(w, c) &amp; #(c)αlog ˆp( ˆp(w, c) w)ˆpα(c) =log #(w)#(c)α
</equation>
<bodyText confidence="0.997601833333333">
Typically negative values are thresholded to 0 to
keep Q sparse. Levy and Goldberg (2014b) ob-
served that the negative sampling objective of the
skip-gram model of Mikolov et al. (2013b) is im-
plicitly factorizing a shifted version of this ma-
trix.2
</bodyText>
<footnote confidence="0.808962">
2This is not equivalent to applying SVD on this matrix,
however, since the loss function is different.
</footnote>
<figure confidence="0.922334107142857">
SPECTRAL-TEMPLATE
Input: word-context co-occurrence counts #(w, c), dimen-
sion m, transformation method t, scaling method s, context
smoothing exponent α &lt; 1, singular value exponent β &lt; 1
Output: vector v(w) E Rm for each word w E [n]
Definitions: #(w) := Ec #(w, c), #(c) := Ew #(w, c),
N(α) := Ec #(c)α
1. Transform all #(w, c), #(w), and #(c):
#(·) *­ I #(·) ift=—
log(1 + #(·)) if t = log
#(·)2/3 if t = two-thirds
�
#(·) if t = sqrt
2. Scale statistics to construct a matrix Ω E Rn×n:
Ωw,c *- I #(w, c) if s = —
#(w,c)
#(w) if s = reg
max (log #( w,c)((a) , 0) if s = ppmi
N(α) if s = cca
V #(w)#(c)α N(1)
3. Perform rank-m SVD on Ω Pz� UEVT where E =
diag(σ1, ... , σm) is a diagonal matrix of ordered sin-
gular values σ1 &gt; · · · &gt; σm &gt; 0.
4. Define v(w) E Rm to be the w-th row of UEβ normal-
ized to have unit 2-norm.
Xmin
uw,vc∈Rm
w,c
</figure>
<page confidence="0.968337">
1286
</page>
<bodyText confidence="0.99294525">
Regression [t E {—, sqrt}, s = reg]. An-
other novelty of our work is considering a low-
rank approximation of a linear regressor that pre-
dicts the context from words. Denoting the word
sample matrix by X E RNxn and the context
sample matrix by Y E RNxn, we seek U* =
arg minUERn×n ||Y − XU||2 whose closed-from
solution is given by:
</bodyText>
<equation confidence="0.999506">
U* = (XTX)−1XTY (7)
</equation>
<bodyText confidence="0.9930231875">
Thus we aim to compute a low-rank approxima-
tion of U* with SVD. This is inspired by other pre-
dictive models in the representation learning lit-
erature (Ando and Zhang, 2005; Mikolov et al.,
2013a). We consider applying the square-root
transformation for the same variance stabilizing
effect discussed in Section 4.3.
CCA [t E {—, two-thirds, sqrt}, s = cca].
This is the focus of our work. As shown in The-
orem 4.1, we can take the element-wise power
transformation on counts (such as the power of
1, 2/3,1/2 in this template) while preserving the
representational meaning of word embeddings un-
der the Brown model interpretation. If there is no
data transformation (t = —), then we recover the
original spectral algorithm of Stratos et al. (2014).
</bodyText>
<sectionHeader confidence="0.999975" genericHeader="method">
6 Related work
</sectionHeader>
<bodyText confidence="0.999956476190476">
We make a few remarks on related works not al-
ready discussed earlier. Dhillon et al. (2011) and
(2012) propose novel modifications of CCA (LR-
MVL and two-step CCA) to derive word embed-
dings, but do not establish any explicit connection
to learning HMM parameters or justify the square-
root transformation. Pennington et al. (2014) pro-
pose a weighted factorization of log-transformed
co-occurrence counts, which is generally an in-
tractable problem (Srebro et al., 2003). In contrast,
our method requires only efficiently computable
matrix decompositions. Finally, word embeddings
have also been used as features to improve per-
formance in a variety of supervised tasks such as
sequence labeling (Dhillon et al., 2011; Collobert
et al., 2011) and dependency parsing (Lei et al.,
2014; Chen and Manning, 2014). Here, we focus
on understanding word embeddings in the context
of a generative word class model, as well as in em-
pirical tasks that directly evaluate the word embed-
dings themselves.
</bodyText>
<sectionHeader confidence="0.999762" genericHeader="evaluation">
7 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999692">
7.1 Word similarity and analogy
</subsectionHeader>
<bodyText confidence="0.9999344">
We first consider word similarity and analogy
tasks for evaluating the quality of word embed-
dings. Word similarity measures the Spearman’s
correlation coefficient between the human scores
and the embeddings’ cosine similarities for word
pairs. Word analogy measures the accuracy on
syntactic and semantic analogy questions. We re-
fer to Levy and Goldberg (2014a) for a detailed
description of these tasks. We use the multiplica-
tive technique of Levy and Goldberg (2014a) for
answering analogy questions.
For the choice of corpus, we use a pre-
processed English Wikipedia dump (http://
dumps.wikimedia.org/). The corpus con-
tains around 1.4 billion words. We only preserve
word types that appear more than 100 times and
replace all others with a special symbol, resulting
in a vocabulary of size around 188k. We define
context words to be 5 words to the left/right for all
considered methods.
We use three word similarity datasets each con-
taining 353, 3000, and 2034 word pairs.3 We
report the average similarity score across these
datasets under the label AVG-SIM. We use two
word analogy datasets that we call SYN (8000
syntactic analogy questions) and MIXED (19544
syntactic and semantic analogy questions).4
We implemented the template in Figure 2 in
C++.5 We compared against the public implemen-
tation of WORD2VEC by Mikolov et al. (2013b)
and GLOVE by Pennington et al. (2014). These
external implementations have numerous hyperpa-
rameters that are not part of the core algorithm,
such as random subsampling in WORD2VEC and
the word-context averaging in GLOVE. We refer
to Levy et al. (2015) for a discussion of the effect
of these features. In our experiments, we enable
all these features with the recommended default
settings.
We reserve a half of each dataset (by category)
</bodyText>
<footnote confidence="0.998625818181818">
3WordSim-353: http://www.cs.technion.ac.
il/˜gabr/resources/data/wordsim353/; MEN:
http://clic.cimec.unitn.it/˜elia.bruni/
MEN.html; Stanford Rare Word: http://www-nlp.
stanford.edu/˜lmthang/morphoNLM/.
4http://research.microsoft.com/en-us/
um/people/gzweig/Pubs/myz_naacl13_
test_set.tgz; http://www.fit.vutbr.cz/
˜imikolov/rnnlm/word-test.v1.txt
5The code is available at https://github.com/
karlstratos/singular.
</footnote>
<page confidence="0.946999">
1287
</page>
<table confidence="0.9999651">
Configuration 500 dimensions 1000 dimensions
AVG-SIM SYN MIXED AVG-SIM SYN MIXED
Transform (t) Scale (s)
— — 0.514 31.58 28.39 0.522 29.84 32.15
sqrt — 0.656 60.77 65.84 0.646 57.46 64.97
log — 0.669 59.28 66.86 0.672 55.66 68.62
— reg 0.530 29.61 36.90 0.562 32.78 37.65
sqrt reg 0.625 63.97 67.30 0.638 65.98 70.04
— ppmi 0.638 41.62 58.80 0.665 47.11 65.34
sqrt cca 0.678 66.40 74.73 0.690 65.14 77.70
</table>
<tableCaption confidence="0.976281">
Table 2: Performance of various spectral methods on the development portion of data.
</tableCaption>
<table confidence="0.9999144">
Transform (t) AVG-SIM SYN MIXED
— 0.572 39.68 57.64
log 0.675 55.61 69.26
two-thirds 0.650 60.52 74.00
sqrt 0.690 65.14 77.70
</table>
<tableCaption confidence="0.999134">
Table 1: Performance of CCA (1000 dimensions)
</tableCaption>
<bodyText confidence="0.958772">
on the development portion of data with different
data transformation methods (α = 0.75, Q = 0).
as a held-out portion for development and use the
other half for final evaluation.
</bodyText>
<subsectionHeader confidence="0.99393">
7.1.1 Effect of data transformation for CCA
</subsectionHeader>
<bodyText confidence="0.999974416666667">
We first look at the effect of different data trans-
formations on the performance of CCA. Table 1
shows the result on the development portion with
1000-dimensional embeddings. We see that with-
out any transformation, the performance can be
quite bad—especially in word analogy. But there
is a marked improvement upon transforming the
data. Moreover, the square-root transformation
gives the best result, improving the accuracy on
the two analogy datasets by 25.46% and 20.06%
in absolute magnitude. This aligns with the dis-
cussion in Section 4.3.
</bodyText>
<subsectionHeader confidence="0.9118705">
7.1.2 Comparison among different spectral
embeddings
</subsectionHeader>
<bodyText confidence="0.999909090909091">
Next, we look at the performance of various com-
binations in the template in Figure 2. We smooth
the context distribution with α = 0.75 for PPMI
and CCA. We use Q = 0.5 for PPMI (which has
a minor improvement over Q = 0) and Q = 0 for
all other methods. We generally find that using
Q = 0 is critical to obtaining good performance
for s E {—, reg}.
Table 2 shows the result on the development
portion for both 500 and 1000 dimensions. Even
without any scaling, SVD performs reasonably
well with the square-root and log transformations.
The regression scaling performs very poorly with-
out data transformation, but once the square-root
transformation is applied it performs quite well
(especially in analogy questions). The PPMI scal-
ing achieves good performance in word similarity
but not in word analogy. The CCA scaling, com-
bined with the square-root transformation, gives
the best overall performance. In particular, it per-
forms better than all other methods in mixed anal-
ogy questions by a significant margin.
</bodyText>
<subsectionHeader confidence="0.819163">
7.1.3 Comparison with other embedding
methods
</subsectionHeader>
<bodyText confidence="0.99992175">
We compare spectral embedding methods against
WORD2VEC and GLOVE on the test portion. We
use the following combinations based on their per-
formance on the development portion:
</bodyText>
<listItem confidence="0.99999525">
• LOG: log transform, — scaling
• REG: sqrt transform, reg scaling
• PPMI: — transform, ppmi scaling
• CCA: sqrt transform, cca scaling
</listItem>
<bodyText confidence="0.999595769230769">
For WORD2VEC, there are two model options:
continuous bag-of-words (CBOW) and skip-gram
(SKIP). Table 3 shows the result for both 500 and
1000 dimensions.
In word similarity, spectral methods generally
excel, with CCA consistently performing the best.
SKIP is the only external package that performs
comparably, with GLOVE and CBOW falling be-
hind. In word analogy, REG and CCA are signifi-
cantly better than other spectral methods. They are
also competitive to GLOVE and CBOW, but SKIP
does perform the best among all compared meth-
ods on (especially syntactic) analogy questions.
</bodyText>
<page confidence="0.961555">
1288
</page>
<table confidence="0.999839333333333">
Method 500 dimensions 1000 dimensions
AVG-SIM SYN MIXED AVG-SIM SYN MIXED
Spectral LOG 0.652 59.52 67.27 0.635 56.53 68.67
REG 0.602 65.51 67.88 0.609 66.47 70.48
PPMI 0.628 43.81 58.38 0.637 48.99 63.82
CCA 0.655 68.38 74.17 0.650 66.08 76.38
Others GLOVE 0.576 68.30 78.08 0.586 67.40 78.73
CBOW 0.597 75.79 73.60 0.509 70.97 60.12
SKIP 0.642 81.08 78.73 0.641 79.98 83.35
</table>
<tableCaption confidence="0.9874285">
Table 3: Performance of different word embedding methods on the test portion of data. See the main text
for the configuration details of spectral methods.
</tableCaption>
<subsectionHeader confidence="0.999268">
7.2 As features in a supervised task
</subsectionHeader>
<bodyText confidence="0.999992384615385">
Finally, we use word embeddings as features in
NER and compare the subsequent improvements
between various embedding methods. The ex-
perimental setting is identical to that of Stratos
et al. (2014). We use the Reuters RCV1 cor-
pus which contains 205 million words. With fre-
quency thresholding, we end up with a vocabu-
lary of size around 301k. We derive LOG, REG,
PPMI, and CCA embeddings as described in Sec-
tion 7.1.3, and GLOVE, CBOW, and SKIP em-
beddings again with the recommended default set-
tings. The number of left/right contexts is 2 for all
methods. For comparison, we also derived 1000
Brown clusters (BROWN) on the same vocabu-
lary and used the resulting bit strings as features
(Brown et al., 1992).
Table 4 shows the result for both 30 and 50 di-
mensions. In general, using any of these lexical
features provides substantial improvements over
the baseline.6 In particular, the 30-dimensional
CCA embeddings improve the F1 score by 2.84
on the development portion and by 4.88 on the
test portion. All spectral methods perform com-
petitively with external packages, with CCA and
SKIP consistently delivering the biggest improve-
ments on the development portion.
</bodyText>
<sectionHeader confidence="0.998583" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999972833333333">
In this work, we revisited SVD-based methods
for inducing word embeddings. We examined
a framework provided by CCA and showed that
the resulting word embeddings can be viewed as
cluster-revealing parameters of a certain model
and that this result is robust to data transformation.
</bodyText>
<footnote confidence="0.981856333333333">
6We mention that the well-known dev/test discrepancy in
the CoNLL 2003 dataset makes the results on the test portion
less reliable.
</footnote>
<table confidence="0.999738636363636">
Features 30 dimensions 50 dimensions
Dev Test Dev Test
— 90.04 84.40 90.04 84.40
BROWN 92.49 88.75 92.49 88.75
LOG 92.27 88.87 92.91 89.67
REG 92.51 88.08 92.73 88.88
PPMI 92.25 89.27 92.53 89.37
CCA 92.88 89.28 92.94 89.01
GLOVE 91.49 87.16 91.58 86.80
CBOW 92.44 88.34 92.83 89.21
SKIP 92.63 88.78 93.11 89.32
</table>
<tableCaption confidence="0.952809">
Table 4: NER F1 scores when word embeddings
</tableCaption>
<bodyText confidence="0.98837775">
are added as features to the baseline (—).
Our proposed method gives the best result among
spectral methods and is competitive to other pop-
ular word embedding techniques.
This work suggests many directions for fu-
ture work. Past spectral methods that involved
CCA without data transformation (e.g., Cohen et
al. (2013)) may be revisited with the square-root
transformation. Using CCA to induce representa-
tions other than word embeddings is another im-
portant future work. It would also be interesting
to formally investigate the theoretical merits and
algorithmic possibility of solving the variance-
weighted objective in Eq. (6). Even though the
objective is hard to optimize in the worst case, it
may be tractable under natural conditions.
</bodyText>
<sectionHeader confidence="0.998813" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99662675">
We thank Omer Levy, Yoav Goldberg, and David
Belanger for helpful discussions. This work
was made possible by a research grant from
Bloomberg’s Knowledge Engineering team.
</bodyText>
<page confidence="0.984636">
1289
</page>
<subsectionHeader confidence="0.437049">
A Proof of Theorem 4.1
</subsectionHeader>
<bodyText confidence="0.973542">
We first define some random variables. Let ρ be
the number of left/right context words to consider
in CCA. Let (W1, ... , WN) E [n]N be a random
sequence of words drawn from the Brown model
where N &gt; 2ρ + 1, along with the correspond-
ing sequence of hidden states (H1, ... , HN) E
[m]N. Independently, pick a position I E [ρ +
1, N − ρ] uniformly at random; pick an integer
J E [−ρ, ρ]\{0} uniformly at random. Define
B E Rn×n, u, v E Rn, π˜ E Rm, and T˜ E Rm×m
as follows:
</bodyText>
<equation confidence="0.974566">
Bw,c := P(WI = w, WI+J = c) bw, c E [n]
uw := P(WI = w) bw E [n]
vc := P(WI+J = c) bc E [n]
˜πh := P(HI = h) bh E [m]
˜Th&apos;,h := P(HI+J = h0|HI = h) bh, h0 E [m]
</equation>
<bodyText confidence="0.992827636363636">
First, we show that Qhai has a particular structure
under the Brown assumption. For the choice of
positive vector s E Rm in the theorem, we define
sh := (Ew o(w|h)a)−1/2 for all h E [m].
Lemma A.1. Qhai = AO&gt; where O E Rn×m has
rank m and A E Rn×m is defined as:
A := diag(O˜π)−a/2Ohaidiag(˜π)a/2diag(s)
Proof. Let O˜ := O T˜. It can be algebraically
verified that B = Odiag(˜π)˜O&gt;, u = O˜π, and
v = ˜O˜π. By Assumption 4.1, each entry of Bhai
has the form
</bodyText>
<equation confidence="0.984606363636364">
Bhai
w,c = (1: Ow,h X ˜πh X ˜Oc,h
h∈[m] � a
�
(= Ow,H(w) X˜πH(w) X ˜Oc,H(w) )a
= Oaw,H(w) X
1: =
h∈[m]
Thus Bhai = Ohaidiag(˜π)a(˜Ohai)&gt;. Therefore,
(diag(u)−1/2Bdiag(v)−1/2)hai
Qhai =
</equation>
<bodyText confidence="0.909499166666667">
= diag(u)−a/2Bhaidiag(v)−a/2
= diag(O˜π)−a/2Ohaidiag(˜π)a/2diag(s)
diag(s)−1diag(˜π)a/2( ˜Ohai)&gt;diag( ˜O˜π)−a/2
This gives the desired result.
Next, we show that the left component of Qhai
is in fact the emission matrix O up to (nonzero)
scaling and is furthermore orthonormal.
Lemma A.2. The matrix A in Lemma A.1 has the
expression A = Oha/2idiag(s) and has orthonor-
mal columns.
Proof. By Assumption 4.1, each entry of A is sim-
plified as follows:
</bodyText>
<equation confidence="0.992668">
Aw,h = o(w|H(w))a/2 X ˜πa/2
o(w|h)a X ˜πa/2
h X sh
H(w)
= o(w|h)a/2 X sh
</equation>
<bodyText confidence="0.99962675">
This proves the first part of the lemma. Note that:
� s2 h X Ew o(w|h)a if h = h0
[A&gt;A]h,h&apos; =
0 otherwise
Thus our choice of s gives A&gt;A = Im×m.
Proof of Theorem 4.1. With Lemma A.1 and A.2,
the proof is similar to the proof of Theorem 5.1 in
Stratos et al. (2014).
</bodyText>
<sectionHeader confidence="0.99966" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998529888888889">
Alexander C Aitken. 1936. On least squares and lin-
ear combination of observations. Proceedings of the
Royal Society of Edinburgh, 55:42–48.
Rie Kubota Ando and Tong Zhang. 2005. A frame-
work for learning predictive structures from multiple
tasks and unlabeled data. The Journal of Machine
Learning Research, 6:1817–1853.
Francis J Anscombe. 1948. The transformation
of poisson, binomial and negative-binomial data.
Biometrika, pages 246–254.
MSo Bartlett. 1936. The square root transformation in
analysis of variance. Supplement to the Journal of
the Royal Statistical Society, pages 68–78.
Peter F Brown, Peter V Desouza, Robert L Mercer,
Vincent J Della Pietra, and Jenifer C Lai. 1992.
Class-based n-gram models of natural language.
Computational Linguistics, 18(4):467–479.
Danqi Chen and Christopher D Manning. 2014. A fast
and accurate dependency parser using neural net-
works. In Proceedings of the Empirical Methods in
Natural Language Processing, pages 740–750.
Shay B Cohen, Karl Stratos, Michael Collins, Dean P
Foster, and Lyle H Ungar. 2013. Experiments with
spectral learning of latent-variable pcfgs. In Pro-
ceedings of the North American Chapter of the As-
sociation of Computational Linguistics, pages 148–
157.
</reference>
<figure confidence="0.572525333333333">
˜Oa
c,H(w)
˜πaH(w) X
Oaw,h X˜πah X
Oa
c,h
</figure>
<page confidence="0.905158">
1290
</page>
<reference confidence="0.999774207792208">
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.
Scott C. Deerwester, Susan T Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society for Information Science,
41(6):391–407.
Paramveer Dhillon, Dean P Foster, and Lyle H Ungar.
2011. Multi-view learning of word embeddings via
cca. In Proceedings of the Advances in Neural In-
formation Processing Systems, pages 199–207.
Paramveer S. Dhillon, Jordan Rodu, Dean P. Foster,
and Lyle H. Ungar. 2012. Two step cca: A
new spectral method for estimating vector models
of words. In Proceedings of the International Con-
ference on Machine learning.
David Hardoon, Sandor Szedmak, and John Shawe-
Taylor. 2004. Canonical correlation analysis:
An overview with application to learning methods.
Neural Computation, 16(12):2639–2664.
Harold Hotelling. 1936. Relations between two sets of
variates. Biometrika, 28(3/4):321–377.
Erich Leo Lehmann and George Casella. 1998. Theory
of point estimation, volume 31. Springer Science &amp;
Business Media.
Tao Lei, Yu Xin, Yuan Zhang, Regina Barzilay, and
Tommi Jaakkola. 2014. Low-rank tensors for scor-
ing dependency structures. In Proceedings of the As-
sociation for Computational Linguistics, volume 1,
pages 1381–1391.
Omer Levy and Yoav Goldberg. 2014a. Linguistic reg-
ularities in sparse and explicit word representations.
In Proceedings of the Computational Natural Lan-
guage Learning, page 171.
Omer Levy and Yoav Goldberg. 2014b. Neural word
embedding as implicit matrix factorization. In Pro-
ceedings of theAdvances in Neural Information Pro-
cessing Systems, pages 2177–2185.
Omer Levy, Yoav Goldberg, Ido Dagan, and Israel
Ramat-Gan. 2015. Improving distributional simi-
larity with lessons learned from word embeddings.
Transactions of the Association for Computational
Linguistics, 3.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Proceedings of the Advances in Neural Infor-
mation Processing Systems, pages 3111–3119.
Patrick Pantel and Dekang Lin. 2002. Discover-
ing word senses from text. In Proceedings of the
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 613–619.
ACM.
Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for
word representation. In Proceedings of the Empiri-
cial Methods in Natural Language Processing, vol-
ume 12.
Nathan Srebro, Tommi Jaakkola, et al. 2003. Weighted
low-rank approximations. In Proceedings of the In-
ternational Conference on Machine learning, vol-
ume 3, pages 720–727.
Robert G. D. Steel. 1953. Relation between pois-
son and multinomial distributions. Technical Report
BU-39-M, Cornell University.
Karl Stratos, Do-kyum Kim, Michael Collins, and
Daniel Hsu. 2014. A spectral algorithm for learn-
ing class-based n-gram models of natural language.
In Proceedings of the Association for Uncertainty in
Artificial Intelligence.
</reference>
<page confidence="0.991018">
1291
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.582705">
<title confidence="0.999914">Model-based Word Embeddings from Decompositions of Count Matrices</title>
<author confidence="0.999957">Karl Stratos Michael Collins Daniel Hsu</author>
<affiliation confidence="0.995047">Columbia University, New York, NY 10027,</affiliation>
<email confidence="0.911179">mcollins,</email>
<abstract confidence="0.977932764705883">This work develops a new statistical understanding of word embeddings induced from transformed count data. Using the class of hidden Markov models (HMMs) underlying Brown clustering as a generative model, we demonstrate how canonical correlation analysis (CCA) and certain count transformations permit efficient and effective recovery of model parameters with lexical semantics. We further show in experiments that these techniques empirically outperform existing spectral methods on word similarity and analogy tasks, and are also competitive with other popular methods such as WORD2VEC and GLOVE.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alexander C Aitken</author>
</authors>
<title>On least squares and linear combination of observations.</title>
<date>1936</date>
<booktitle>Proceedings of the Royal Society of Edinburgh,</booktitle>
<pages>55--42</pages>
<contexts>
<context position="14607" citStr="Aitken, 1936" startWordPosition="2591" endWordPosition="2592">ing word occurrences. 4.3.2 Variance-weighted squared-error minimization At the heart of CCA is computing the SVD of the Q(a) matrix: this can be interpreted as solving the following (non-convex) squared-error minimization problem: �Q(a) T \ 2 w,c —uwvcJl Q(a) w,c ˆQ(a) w,c #(w, c)a Xmin uw,vcERm w,c 1285 But we note that minimizing unweighted squarederror objectives is generally suboptimal when the target values are heteroscedastic. For instance, in linear regression, it is well-known that a weighted least squares estimator dominates ordinary least squares in terms of statistical efficiency (Aitken, 1936; Lehmann and Casella, 1998). For our setting, the analogous weighted least squares optimization is: 12 l Q(aic − u&gt;wvc/ (6) Q(a) ((a)1 l where Var(X) := E[X2] − E[X]2. This optimization is, unfortunately, generally intractable (Srebro et al., 2003). The square-root transformation, nevertheless, obviates the variance-based weighting since the target values have approximately the same variance of 1/4. 5 A template for spectral methods Figure 2 gives a generic template that encompasses a range of spectral methods for deriving word embeddings. All of them operate on cooccurrence counts #(w, c) an</context>
</contexts>
<marker>Aitken, 1936</marker>
<rawString>Alexander C Aitken. 1936. On least squares and linear combination of observations. Proceedings of the Royal Society of Edinburgh, 55:42–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rie Kubota Ando</author>
<author>Tong Zhang</author>
</authors>
<title>A framework for learning predictive structures from multiple tasks and unlabeled data.</title>
<date>2005</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>6--1817</pages>
<contexts>
<context position="18838" citStr="Ando and Zhang, 2005" startWordPosition="3373" endWordPosition="3376"> Define v(w) E Rm to be the w-th row of UEβ normalized to have unit 2-norm. Xmin uw,vc∈Rm w,c 1286 Regression [t E {—, sqrt}, s = reg]. Another novelty of our work is considering a lowrank approximation of a linear regressor that predicts the context from words. Denoting the word sample matrix by X E RNxn and the context sample matrix by Y E RNxn, we seek U* = arg minUERn×n ||Y − XU||2 whose closed-from solution is given by: U* = (XTX)−1XTY (7) Thus we aim to compute a low-rank approximation of U* with SVD. This is inspired by other predictive models in the representation learning literature (Ando and Zhang, 2005; Mikolov et al., 2013a). We consider applying the square-root transformation for the same variance stabilizing effect discussed in Section 4.3. CCA [t E {—, two-thirds, sqrt}, s = cca]. This is the focus of our work. As shown in Theorem 4.1, we can take the element-wise power transformation on counts (such as the power of 1, 2/3,1/2 in this template) while preserving the representational meaning of word embeddings under the Brown model interpretation. If there is no data transformation (t = —), then we recover the original spectral algorithm of Stratos et al. (2014). 6 Related work We make a </context>
</contexts>
<marker>Ando, Zhang, 2005</marker>
<rawString>Rie Kubota Ando and Tong Zhang. 2005. A framework for learning predictive structures from multiple tasks and unlabeled data. The Journal of Machine Learning Research, 6:1817–1853.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francis J Anscombe</author>
</authors>
<title>The transformation of poisson, binomial and negative-binomial data.</title>
<date>1948</date>
<journal>Biometrika,</journal>
<pages>246--254</pages>
<contexts>
<context position="13174" citStr="Anscombe, 1948" startWordPosition="2358" endWordPosition="2359">d is given in Appendix A. 4.3 Choice of data transformation Given a corpus, the sample estimate of Q(a) is given by: p(5) #(w)a#(c)a where #(w, c) denotes the co-occurrence count of word w and context c in the corpus, #(w) := Pc #(w, c), and #(c) := Pw #(w, c). What choice of a is beneficial and why? We use a = 1/2 for the following reason: it stabilizes the variance of the term and thereby gives a more statistically stable solution. 4.3.1 Variance stabilization for word counts The square-root transformation is a variancestabilizing transformation for Poisson random variables (Bartlett, 1936; Anscombe, 1948). In particular, the square-root of a Poisson variable has variance close to 1/4, independent of its mean. Lemma 4.1 (Bartlett (1936)). Let X be a random variable with distribution Poisson(n x p) for any p E (0, 1) and positive integer n. Define Y := -/X. Then the variance of Y approaches 1/4 as n — *oc. This transformation is relevant for word counts because they can be naturally modeled as Poisson variables. Indeed, if word counts in a corpus of length N are drawn from a multinomial distribution over [n] with N observations, then these counts have the same distribution as n independent Poiss</context>
</contexts>
<marker>Anscombe, 1948</marker>
<rawString>Francis J Anscombe. 1948. The transformation of poisson, binomial and negative-binomial data. Biometrika, pages 246–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>MSo Bartlett</author>
</authors>
<title>The square root transformation in analysis of variance. Supplement to the</title>
<date>1936</date>
<journal>Journal of the Royal Statistical Society,</journal>
<pages>68--78</pages>
<contexts>
<context position="13157" citStr="Bartlett, 1936" startWordPosition="2356" endWordPosition="2357">et al. (2014) and is given in Appendix A. 4.3 Choice of data transformation Given a corpus, the sample estimate of Q(a) is given by: p(5) #(w)a#(c)a where #(w, c) denotes the co-occurrence count of word w and context c in the corpus, #(w) := Pc #(w, c), and #(c) := Pw #(w, c). What choice of a is beneficial and why? We use a = 1/2 for the following reason: it stabilizes the variance of the term and thereby gives a more statistically stable solution. 4.3.1 Variance stabilization for word counts The square-root transformation is a variancestabilizing transformation for Poisson random variables (Bartlett, 1936; Anscombe, 1948). In particular, the square-root of a Poisson variable has variance close to 1/4, independent of its mean. Lemma 4.1 (Bartlett (1936)). Let X be a random variable with distribution Poisson(n x p) for any p E (0, 1) and positive integer n. Define Y := -/X. Then the variance of Y approaches 1/4 as n — *oc. This transformation is relevant for word counts because they can be naturally modeled as Poisson variables. Indeed, if word counts in a corpus of length N are drawn from a multinomial distribution over [n] with N observations, then these counts have the same distribution as n </context>
</contexts>
<marker>Bartlett, 1936</marker>
<rawString>MSo Bartlett. 1936. The square root transformation in analysis of variance. Supplement to the Journal of the Royal Statistical Society, pages 68–78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Peter V Desouza</author>
<author>Robert L Mercer</author>
<author>Vincent J Della Pietra</author>
<author>Jenifer C Lai</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="8518" citStr="Brown et al. (1992)" startWordPosition="1468" endWordPosition="1471">gonal. This follows from our definition of the word and context variables as one-hot encodings since E[XwXw,] = 0 for w =� w0 and E[YcYc,] = 0 for c =�c0. With these observations and the binary definition of (X, Y ), each entry in Q now has a simple closed-form solution: P(Xw = 1, Yc = 1) Qw,c = (4) pP(Xw = 1)P(Yc = 1) which can be readily estimated from a corpus. 4 Using CCA for parameter estimation In a less well-known interpretation of Eq. (4), CCA is seen as a parameter estimation algorithm for a language model (Stratos et al., 2014). This model is a restricted class of HMMs introduced by Brown et al. (1992), henceforth called the Brown model. In this section, we extend the result of Stratos et al. (2014) and show that its correctness is preserved under certain element-wise data transformations. 4.1 Clustering under a Brown model A Brown model is a 5-tuple (n, m, 7r, t, o) for n, m E N and functions 7r, t, o where • [n] is a set of word types. • [m] is a set of hidden states. • 7r(h) is the probability of generating h E [m] in the first position of a sequence. • t(h0|h) is the probability of generating h0 E [m] given h E [m]. • o(w|h) is the probability of generating w E [n] given h E [m]. Import</context>
<context position="16918" citStr="Brown et al., 1992" startWordPosition="2993" endWordPosition="2996">l., 2015). Different combinations of these aspects reproduce various spectral embeddings explored in the literature. We enumerate some meaningful combinations: Figure 2: A template for spectral word embedding methods. No scaling [t E {—, log, sqrt}, s = —]. This is a commonly considered setting (e.g., in Pennington et al. (2014)) where no scaling is applied to the co-occurrence counts. It is however typically accompanied with some kind of data transformation. Positive point-wise mutual information (PPMI) [t = —, s = ppmi]. Mutual information is a popular metric in many natural language tasks (Brown et al., 1992; Pantel and Lin, 2002). In this setting, each term in the matrix for SVD is set as the pointwise mutual information between word w and context c: #(w, c) &amp; #(c)αlog ˆp( ˆp(w, c) w)ˆpα(c) =log #(w)#(c)α Typically negative values are thresholded to 0 to keep Q sparse. Levy and Goldberg (2014b) observed that the negative sampling objective of the skip-gram model of Mikolov et al. (2013b) is implicitly factorizing a shifted version of this matrix.2 2This is not equivalent to applying SVD on this matrix, however, since the loss function is different. SPECTRAL-TEMPLATE Input: word-context co-occurr</context>
<context position="27370" citStr="Brown et al., 1992" startWordPosition="4728" endWordPosition="4731">ents between various embedding methods. The experimental setting is identical to that of Stratos et al. (2014). We use the Reuters RCV1 corpus which contains 205 million words. With frequency thresholding, we end up with a vocabulary of size around 301k. We derive LOG, REG, PPMI, and CCA embeddings as described in Section 7.1.3, and GLOVE, CBOW, and SKIP embeddings again with the recommended default settings. The number of left/right contexts is 2 for all methods. For comparison, we also derived 1000 Brown clusters (BROWN) on the same vocabulary and used the resulting bit strings as features (Brown et al., 1992). Table 4 shows the result for both 30 and 50 dimensions. In general, using any of these lexical features provides substantial improvements over the baseline.6 In particular, the 30-dimensional CCA embeddings improve the F1 score by 2.84 on the development portion and by 4.88 on the test portion. All spectral methods perform competitively with external packages, with CCA and SKIP consistently delivering the biggest improvements on the development portion. 8 Conclusion In this work, we revisited SVD-based methods for inducing word embeddings. We examined a framework provided by CCA and showed t</context>
</contexts>
<marker>Brown, Desouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>Peter F Brown, Peter V Desouza, Robert L Mercer, Vincent J Della Pietra, and Jenifer C Lai. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18(4):467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danqi Chen</author>
<author>Christopher D Manning</author>
</authors>
<title>A fast and accurate dependency parser using neural networks.</title>
<date>2014</date>
<booktitle>In Proceedings of the Empirical Methods in Natural Language Processing,</booktitle>
<pages>740--750</pages>
<contexts>
<context position="20231" citStr="Chen and Manning, 2014" startWordPosition="3599" endWordPosition="3602">beddings, but do not establish any explicit connection to learning HMM parameters or justify the squareroot transformation. Pennington et al. (2014) propose a weighted factorization of log-transformed co-occurrence counts, which is generally an intractable problem (Srebro et al., 2003). In contrast, our method requires only efficiently computable matrix decompositions. Finally, word embeddings have also been used as features to improve performance in a variety of supervised tasks such as sequence labeling (Dhillon et al., 2011; Collobert et al., 2011) and dependency parsing (Lei et al., 2014; Chen and Manning, 2014). Here, we focus on understanding word embeddings in the context of a generative word class model, as well as in empirical tasks that directly evaluate the word embeddings themselves. 7 Experiments 7.1 Word similarity and analogy We first consider word similarity and analogy tasks for evaluating the quality of word embeddings. Word similarity measures the Spearman’s correlation coefficient between the human scores and the embeddings’ cosine similarities for word pairs. Word analogy measures the accuracy on syntactic and semantic analogy questions. We refer to Levy and Goldberg (2014a) for a de</context>
</contexts>
<marker>Chen, Manning, 2014</marker>
<rawString>Danqi Chen and Christopher D Manning. 2014. A fast and accurate dependency parser using neural networks. In Proceedings of the Empirical Methods in Natural Language Processing, pages 740–750.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shay B Cohen</author>
<author>Karl Stratos</author>
<author>Michael Collins</author>
<author>Dean P Foster</author>
<author>Lyle H Ungar</author>
</authors>
<title>Experiments with spectral learning of latent-variable pcfgs.</title>
<date>2013</date>
<booktitle>In Proceedings of the North American Chapter of the Association of Computational Linguistics,</booktitle>
<pages>148--157</pages>
<contexts>
<context position="28929" citStr="Cohen et al. (2013)" startWordPosition="4980" endWordPosition="4983"> — 90.04 84.40 90.04 84.40 BROWN 92.49 88.75 92.49 88.75 LOG 92.27 88.87 92.91 89.67 REG 92.51 88.08 92.73 88.88 PPMI 92.25 89.27 92.53 89.37 CCA 92.88 89.28 92.94 89.01 GLOVE 91.49 87.16 91.58 86.80 CBOW 92.44 88.34 92.83 89.21 SKIP 92.63 88.78 93.11 89.32 Table 4: NER F1 scores when word embeddings are added as features to the baseline (—). Our proposed method gives the best result among spectral methods and is competitive to other popular word embedding techniques. This work suggests many directions for future work. Past spectral methods that involved CCA without data transformation (e.g., Cohen et al. (2013)) may be revisited with the square-root transformation. Using CCA to induce representations other than word embeddings is another important future work. It would also be interesting to formally investigate the theoretical merits and algorithmic possibility of solving the varianceweighted objective in Eq. (6). Even though the objective is hard to optimize in the worst case, it may be tractable under natural conditions. Acknowledgments We thank Omer Levy, Yoav Goldberg, and David Belanger for helpful discussions. This work was made possible by a research grant from Bloomberg’s Knowledge Engineer</context>
</contexts>
<marker>Cohen, Stratos, Collins, Foster, Ungar, 2013</marker>
<rawString>Shay B Cohen, Karl Stratos, Michael Collins, Dean P Foster, and Lyle H Ungar. 2013. Experiments with spectral learning of latent-variable pcfgs. In Proceedings of the North American Chapter of the Association of Computational Linguistics, pages 148– 157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="20165" citStr="Collobert et al., 2011" startWordPosition="3588" endWordPosition="3591">el modifications of CCA (LRMVL and two-step CCA) to derive word embeddings, but do not establish any explicit connection to learning HMM parameters or justify the squareroot transformation. Pennington et al. (2014) propose a weighted factorization of log-transformed co-occurrence counts, which is generally an intractable problem (Srebro et al., 2003). In contrast, our method requires only efficiently computable matrix decompositions. Finally, word embeddings have also been used as features to improve performance in a variety of supervised tasks such as sequence labeling (Dhillon et al., 2011; Collobert et al., 2011) and dependency parsing (Lei et al., 2014; Chen and Manning, 2014). Here, we focus on understanding word embeddings in the context of a generative word class model, as well as in empirical tasks that directly evaluate the word embeddings themselves. 7 Experiments 7.1 Word similarity and analogy We first consider word similarity and analogy tasks for evaluating the quality of word embeddings. Word similarity measures the Spearman’s correlation coefficient between the human scores and the embeddings’ cosine similarities for word pairs. Word analogy measures the accuracy on syntactic and semantic</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott C Deerwester</author>
<author>Susan T Dumais</author>
<author>Thomas K Landauer</author>
<author>George W Furnas</author>
<author>Richard A Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>41</volume>
<issue>6</issue>
<contexts>
<context position="16091" citStr="Deerwester et al. (1990)" startWordPosition="2851" endWordPosition="2854"> proposed smoothing the empirical context distribution as ˆpα(c) := #(c)α/ Pc #(c)α and found α = 0.75 to work well in practice. We also found that setting α = 0.75 gave a small but consistent improvement over setting α = 1. Note that the choice of α only affects methods that make use of the context distribution (s E {ppmi, cca}). The parameter β controls the role of singular values in word embeddings. This is always 0 for CCA as it does not require singular values. But for other methods, one can consider setting β &gt; 0 since the best-fit subspace for the rows of Q is given by UE. For example, Deerwester et al. (1990) use β = 1 and Levy and Goldberg (2014b) use β = 0.5. However, it has been found by many (including ourselves) that setting β = 1 yields substantially worse representations than setting β E {0, 0.5} (Levy et al., 2015). Different combinations of these aspects reproduce various spectral embeddings explored in the literature. We enumerate some meaningful combinations: Figure 2: A template for spectral word embedding methods. No scaling [t E {—, log, sqrt}, s = —]. This is a commonly considered setting (e.g., in Pennington et al. (2014)) where no scaling is applied to the co-occurrence counts. It</context>
</contexts>
<marker>Deerwester, Dumais, Landauer, Furnas, Harshman, 1990</marker>
<rawString>Scott C. Deerwester, Susan T Dumais, Thomas K. Landauer, George W. Furnas, and Richard A. Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society for Information Science, 41(6):391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paramveer Dhillon</author>
<author>Dean P Foster</author>
<author>Lyle H Ungar</author>
</authors>
<title>Multi-view learning of word embeddings via cca.</title>
<date>2011</date>
<booktitle>In Proceedings of the Advances in Neural Information Processing Systems,</booktitle>
<pages>199--207</pages>
<contexts>
<context position="19519" citStr="Dhillon et al. (2011)" startWordPosition="3489" endWordPosition="3492">ot transformation for the same variance stabilizing effect discussed in Section 4.3. CCA [t E {—, two-thirds, sqrt}, s = cca]. This is the focus of our work. As shown in Theorem 4.1, we can take the element-wise power transformation on counts (such as the power of 1, 2/3,1/2 in this template) while preserving the representational meaning of word embeddings under the Brown model interpretation. If there is no data transformation (t = —), then we recover the original spectral algorithm of Stratos et al. (2014). 6 Related work We make a few remarks on related works not already discussed earlier. Dhillon et al. (2011) and (2012) propose novel modifications of CCA (LRMVL and two-step CCA) to derive word embeddings, but do not establish any explicit connection to learning HMM parameters or justify the squareroot transformation. Pennington et al. (2014) propose a weighted factorization of log-transformed co-occurrence counts, which is generally an intractable problem (Srebro et al., 2003). In contrast, our method requires only efficiently computable matrix decompositions. Finally, word embeddings have also been used as features to improve performance in a variety of supervised tasks such as sequence labeling </context>
</contexts>
<marker>Dhillon, Foster, Ungar, 2011</marker>
<rawString>Paramveer Dhillon, Dean P Foster, and Lyle H Ungar. 2011. Multi-view learning of word embeddings via cca. In Proceedings of the Advances in Neural Information Processing Systems, pages 199–207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paramveer S Dhillon</author>
<author>Jordan Rodu</author>
<author>Dean P Foster</author>
<author>Lyle H Ungar</author>
</authors>
<title>Two step cca: A new spectral method for estimating vector models of words.</title>
<date>2012</date>
<booktitle>In Proceedings of the International Conference on Machine learning.</booktitle>
<marker>Dhillon, Rodu, Foster, Ungar, 2012</marker>
<rawString>Paramveer S. Dhillon, Jordan Rodu, Dean P. Foster, and Lyle H. Ungar. 2012. Two step cca: A new spectral method for estimating vector models of words. In Proceedings of the International Conference on Machine learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Hardoon</author>
<author>Sandor Szedmak</author>
<author>John ShaweTaylor</author>
</authors>
<title>Canonical correlation analysis: An overview with application to learning methods.</title>
<date>2004</date>
<journal>Neural Computation,</journal>
<volume>16</volume>
<issue>12</issue>
<contexts>
<context position="5514" citStr="Hardoon et al. (2004)" startWordPosition="918" endWordPosition="921">revious projections. Let A := [a1 ... am] and B := [b1 ... bm]. Then we can think of the joint projections X = A&gt;X Y = B&gt;Y (2) as new m-dimensional representations of the original variables that are transformed to be as correlated as possible with each other. Furthermore, often m « min(n, n0), leading to a dramatic reduction in dimensionality. 3.2 Exact solution via SVD Eq. (1) is non-convex due to the terms a and b that interact with each other, so it cannot be solved exactly using a standard optimization technique. However, a method based on SVD provides an efficient and exact solution. See Hardoon et al. (2004) for a detailed discussion. 1This can be always achieved through data preprocessing (“centering”). Lemma 3.1 (Hotelling (1936)). Assume X and Y have zero mean. The solution (A, B) to (1) is given by A = E[XX&gt;]−1/2U and B = E[Y Y &gt;]−1/2V where the i-th column of U E Rn×m (V E Rn&apos;×m) is the left (right) singular vector of Ω := E[XX&gt;]−1/2E[XY &gt;]E[Y Y &gt;]−1/2 (3) corresponding to the i-th largest singular value σi. Furthermore, σi = Cor(a&gt;i X, b&gt;iY ). 3.3 Using CCA for word representations As presented in Section 3.1, CCA is a general framework that operates on a pair of random variables. Adapting </context>
</contexts>
<marker>Hardoon, Szedmak, ShaweTaylor, 2004</marker>
<rawString>David Hardoon, Sandor Szedmak, and John ShaweTaylor. 2004. Canonical correlation analysis: An overview with application to learning methods. Neural Computation, 16(12):2639–2664.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harold Hotelling</author>
</authors>
<title>Relations between two sets of variates.</title>
<date>1936</date>
<journal>Biometrika,</journal>
<pages>28--3</pages>
<contexts>
<context position="1917" citStr="Hotelling, 1936" startWordPosition="286" endWordPosition="287">ficiency considerations, but has subsequently been interpreted by Levy and Goldberg (2014b) as seeking a low-rank factorization of a matrix whose entries are word-context co-occurrence counts, scaled and transformed in a certain way. This observation sheds new light on WORD2VEC, yet also raises several new questions about word embeddings based on decomposing count data. What is the right matrix to decompose? Are there rigorous justifications for the choice of matrix and count transformations? In this paper, we answer some of these questions by investigating the decomposition specified by CCA (Hotelling, 1936), a powerful technique for inducing generic representations whose computation is efficiently and exactly reduced to that of a matrix singular value decomposition (SVD). We build on and strengthen the work of Stratos et al. (2014) which uses CCA for learning the class of HMMs underlying Brown clustering. We show that certain count transformations enhance the accuracy of the estimation method and significantly improve the empirical performance of word representations derived from these model parameters (Table 1). In addition to providing a rigorous justification for CCA-based word embeddings, we</context>
<context position="5640" citStr="Hotelling (1936)" startWordPosition="937" endWordPosition="938">ew m-dimensional representations of the original variables that are transformed to be as correlated as possible with each other. Furthermore, often m « min(n, n0), leading to a dramatic reduction in dimensionality. 3.2 Exact solution via SVD Eq. (1) is non-convex due to the terms a and b that interact with each other, so it cannot be solved exactly using a standard optimization technique. However, a method based on SVD provides an efficient and exact solution. See Hardoon et al. (2004) for a detailed discussion. 1This can be always achieved through data preprocessing (“centering”). Lemma 3.1 (Hotelling (1936)). Assume X and Y have zero mean. The solution (A, B) to (1) is given by A = E[XX&gt;]−1/2U and B = E[Y Y &gt;]−1/2V where the i-th column of U E Rn×m (V E Rn&apos;×m) is the left (right) singular vector of Ω := E[XX&gt;]−1/2E[XY &gt;]E[Y Y &gt;]−1/2 (3) corresponding to the i-th largest singular value σi. Furthermore, σi = Cor(a&gt;i X, b&gt;iY ). 3.3 Using CCA for word representations As presented in Section 3.1, CCA is a general framework that operates on a pair of random variables. Adapting CCA specifically to inducing word representations results in a simple recipe for calculating (3). A natural approach is to set</context>
</contexts>
<marker>Hotelling, 1936</marker>
<rawString>Harold Hotelling. 1936. Relations between two sets of variates. Biometrika, 28(3/4):321–377.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erich Leo Lehmann</author>
<author>George Casella</author>
</authors>
<title>Theory of point estimation,</title>
<date>1998</date>
<volume>31</volume>
<publisher>Springer Science &amp; Business Media.</publisher>
<contexts>
<context position="14635" citStr="Lehmann and Casella, 1998" startWordPosition="2593" endWordPosition="2596">rences. 4.3.2 Variance-weighted squared-error minimization At the heart of CCA is computing the SVD of the Q(a) matrix: this can be interpreted as solving the following (non-convex) squared-error minimization problem: �Q(a) T \ 2 w,c —uwvcJl Q(a) w,c ˆQ(a) w,c #(w, c)a Xmin uw,vcERm w,c 1285 But we note that minimizing unweighted squarederror objectives is generally suboptimal when the target values are heteroscedastic. For instance, in linear regression, it is well-known that a weighted least squares estimator dominates ordinary least squares in terms of statistical efficiency (Aitken, 1936; Lehmann and Casella, 1998). For our setting, the analogous weighted least squares optimization is: 12 l Q(aic − u&gt;wvc/ (6) Q(a) ((a)1 l where Var(X) := E[X2] − E[X]2. This optimization is, unfortunately, generally intractable (Srebro et al., 2003). The square-root transformation, nevertheless, obviates the variance-based weighting since the target values have approximately the same variance of 1/4. 5 A template for spectral methods Figure 2 gives a generic template that encompasses a range of spectral methods for deriving word embeddings. All of them operate on cooccurrence counts #(w, c) and share the low-rank SVD ste</context>
</contexts>
<marker>Lehmann, Casella, 1998</marker>
<rawString>Erich Leo Lehmann and George Casella. 1998. Theory of point estimation, volume 31. Springer Science &amp; Business Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tao Lei</author>
<author>Yu Xin</author>
<author>Yuan Zhang</author>
<author>Regina Barzilay</author>
<author>Tommi Jaakkola</author>
</authors>
<title>Low-rank tensors for scoring dependency structures.</title>
<date>2014</date>
<booktitle>In Proceedings of the Association for Computational Linguistics,</booktitle>
<volume>1</volume>
<pages>1381--1391</pages>
<contexts>
<context position="20206" citStr="Lei et al., 2014" startWordPosition="3595" endWordPosition="3598"> to derive word embeddings, but do not establish any explicit connection to learning HMM parameters or justify the squareroot transformation. Pennington et al. (2014) propose a weighted factorization of log-transformed co-occurrence counts, which is generally an intractable problem (Srebro et al., 2003). In contrast, our method requires only efficiently computable matrix decompositions. Finally, word embeddings have also been used as features to improve performance in a variety of supervised tasks such as sequence labeling (Dhillon et al., 2011; Collobert et al., 2011) and dependency parsing (Lei et al., 2014; Chen and Manning, 2014). Here, we focus on understanding word embeddings in the context of a generative word class model, as well as in empirical tasks that directly evaluate the word embeddings themselves. 7 Experiments 7.1 Word similarity and analogy We first consider word similarity and analogy tasks for evaluating the quality of word embeddings. Word similarity measures the Spearman’s correlation coefficient between the human scores and the embeddings’ cosine similarities for word pairs. Word analogy measures the accuracy on syntactic and semantic analogy questions. We refer to Levy and </context>
</contexts>
<marker>Lei, Xin, Zhang, Barzilay, Jaakkola, 2014</marker>
<rawString>Tao Lei, Yu Xin, Yuan Zhang, Regina Barzilay, and Tommi Jaakkola. 2014. Low-rank tensors for scoring dependency structures. In Proceedings of the Association for Computational Linguistics, volume 1, pages 1381–1391.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
</authors>
<title>Linguistic regularities in sparse and explicit word representations.</title>
<date>2014</date>
<booktitle>In Proceedings of the Computational Natural Language Learning,</booktitle>
<pages>171</pages>
<contexts>
<context position="1390" citStr="Levy and Goldberg (2014" startWordPosition="200" endWordPosition="203">ethods such as WORD2VEC and GLOVE. 1 Introduction The recent spike of interest in dense, lowdimensional lexical representations—i.e., word embeddings—is largely due to their ability to capture subtle syntactic and semantic patterns that are useful in a variety of natural language tasks. A successful method for deriving such embeddings is the negative sampling training of the skip-gram model suggested by Mikolov et al. (2013b) and implemented in the popular software WORD2VEC. The form of its training objective was motivated by efficiency considerations, but has subsequently been interpreted by Levy and Goldberg (2014b) as seeking a low-rank factorization of a matrix whose entries are word-context co-occurrence counts, scaled and transformed in a certain way. This observation sheds new light on WORD2VEC, yet also raises several new questions about word embeddings based on decomposing count data. What is the right matrix to decompose? Are there rigorous justifications for the choice of matrix and count transformations? In this paper, we answer some of these questions by investigating the decomposition specified by CCA (Hotelling, 1936), a powerful technique for inducing generic representations whose computa</context>
<context position="2716" citStr="Levy and Goldberg (2014" startWordPosition="407" endWordPosition="411">d on and strengthen the work of Stratos et al. (2014) which uses CCA for learning the class of HMMs underlying Brown clustering. We show that certain count transformations enhance the accuracy of the estimation method and significantly improve the empirical performance of word representations derived from these model parameters (Table 1). In addition to providing a rigorous justification for CCA-based word embeddings, we also supply a general template that encompasses a range of spectral methods (algorithms employing SVD) for inducing word embeddings in the literature, including the method of Levy and Goldberg (2014b). In experiments, we demonstrate that CCA combined with the square-root transformation achieves the best result among spectral methods and performs competitively with other popular methods such as WORD2VEC and GLOVE on word similarity and analogy tasks. We additionally demonstrate that CCA embeddings provide the most competitive improvement when used as features in named-entity recognition (NER). 2 Notation We use [n] to denote the set of integers {1, ... , n}. We denote the m x m diagonal matrix with values vi ... vm along the diagonal by diag(vi ... vm). We write [al ... am] to denote a ma</context>
<context position="16129" citStr="Levy and Goldberg (2014" startWordPosition="2860" endWordPosition="2863">xt distribution as ˆpα(c) := #(c)α/ Pc #(c)α and found α = 0.75 to work well in practice. We also found that setting α = 0.75 gave a small but consistent improvement over setting α = 1. Note that the choice of α only affects methods that make use of the context distribution (s E {ppmi, cca}). The parameter β controls the role of singular values in word embeddings. This is always 0 for CCA as it does not require singular values. But for other methods, one can consider setting β &gt; 0 since the best-fit subspace for the rows of Q is given by UE. For example, Deerwester et al. (1990) use β = 1 and Levy and Goldberg (2014b) use β = 0.5. However, it has been found by many (including ourselves) that setting β = 1 yields substantially worse representations than setting β E {0, 0.5} (Levy et al., 2015). Different combinations of these aspects reproduce various spectral embeddings explored in the literature. We enumerate some meaningful combinations: Figure 2: A template for spectral word embedding methods. No scaling [t E {—, log, sqrt}, s = —]. This is a commonly considered setting (e.g., in Pennington et al. (2014)) where no scaling is applied to the co-occurrence counts. It is however typically accompanied with</context>
<context position="20820" citStr="Levy and Goldberg (2014" startWordPosition="3691" endWordPosition="3694">al., 2014; Chen and Manning, 2014). Here, we focus on understanding word embeddings in the context of a generative word class model, as well as in empirical tasks that directly evaluate the word embeddings themselves. 7 Experiments 7.1 Word similarity and analogy We first consider word similarity and analogy tasks for evaluating the quality of word embeddings. Word similarity measures the Spearman’s correlation coefficient between the human scores and the embeddings’ cosine similarities for word pairs. Word analogy measures the accuracy on syntactic and semantic analogy questions. We refer to Levy and Goldberg (2014a) for a detailed description of these tasks. We use the multiplicative technique of Levy and Goldberg (2014a) for answering analogy questions. For the choice of corpus, we use a preprocessed English Wikipedia dump (http:// dumps.wikimedia.org/). The corpus contains around 1.4 billion words. We only preserve word types that appear more than 100 times and replace all others with a special symbol, resulting in a vocabulary of size around 188k. We define context words to be 5 words to the left/right for all considered methods. We use three word similarity datasets each containing 353, 3000, and 2</context>
</contexts>
<marker>Levy, Goldberg, 2014</marker>
<rawString>Omer Levy and Yoav Goldberg. 2014a. Linguistic regularities in sparse and explicit word representations. In Proceedings of the Computational Natural Language Learning, page 171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
</authors>
<title>Neural word embedding as implicit matrix factorization.</title>
<date>2014</date>
<booktitle>In Proceedings of theAdvances in Neural Information Processing Systems,</booktitle>
<pages>2177--2185</pages>
<contexts>
<context position="1390" citStr="Levy and Goldberg (2014" startWordPosition="200" endWordPosition="203">ethods such as WORD2VEC and GLOVE. 1 Introduction The recent spike of interest in dense, lowdimensional lexical representations—i.e., word embeddings—is largely due to their ability to capture subtle syntactic and semantic patterns that are useful in a variety of natural language tasks. A successful method for deriving such embeddings is the negative sampling training of the skip-gram model suggested by Mikolov et al. (2013b) and implemented in the popular software WORD2VEC. The form of its training objective was motivated by efficiency considerations, but has subsequently been interpreted by Levy and Goldberg (2014b) as seeking a low-rank factorization of a matrix whose entries are word-context co-occurrence counts, scaled and transformed in a certain way. This observation sheds new light on WORD2VEC, yet also raises several new questions about word embeddings based on decomposing count data. What is the right matrix to decompose? Are there rigorous justifications for the choice of matrix and count transformations? In this paper, we answer some of these questions by investigating the decomposition specified by CCA (Hotelling, 1936), a powerful technique for inducing generic representations whose computa</context>
<context position="2716" citStr="Levy and Goldberg (2014" startWordPosition="407" endWordPosition="411">d on and strengthen the work of Stratos et al. (2014) which uses CCA for learning the class of HMMs underlying Brown clustering. We show that certain count transformations enhance the accuracy of the estimation method and significantly improve the empirical performance of word representations derived from these model parameters (Table 1). In addition to providing a rigorous justification for CCA-based word embeddings, we also supply a general template that encompasses a range of spectral methods (algorithms employing SVD) for inducing word embeddings in the literature, including the method of Levy and Goldberg (2014b). In experiments, we demonstrate that CCA combined with the square-root transformation achieves the best result among spectral methods and performs competitively with other popular methods such as WORD2VEC and GLOVE on word similarity and analogy tasks. We additionally demonstrate that CCA embeddings provide the most competitive improvement when used as features in named-entity recognition (NER). 2 Notation We use [n] to denote the set of integers {1, ... , n}. We denote the m x m diagonal matrix with values vi ... vm along the diagonal by diag(vi ... vm). We write [al ... am] to denote a ma</context>
<context position="16129" citStr="Levy and Goldberg (2014" startWordPosition="2860" endWordPosition="2863">xt distribution as ˆpα(c) := #(c)α/ Pc #(c)α and found α = 0.75 to work well in practice. We also found that setting α = 0.75 gave a small but consistent improvement over setting α = 1. Note that the choice of α only affects methods that make use of the context distribution (s E {ppmi, cca}). The parameter β controls the role of singular values in word embeddings. This is always 0 for CCA as it does not require singular values. But for other methods, one can consider setting β &gt; 0 since the best-fit subspace for the rows of Q is given by UE. For example, Deerwester et al. (1990) use β = 1 and Levy and Goldberg (2014b) use β = 0.5. However, it has been found by many (including ourselves) that setting β = 1 yields substantially worse representations than setting β E {0, 0.5} (Levy et al., 2015). Different combinations of these aspects reproduce various spectral embeddings explored in the literature. We enumerate some meaningful combinations: Figure 2: A template for spectral word embedding methods. No scaling [t E {—, log, sqrt}, s = —]. This is a commonly considered setting (e.g., in Pennington et al. (2014)) where no scaling is applied to the co-occurrence counts. It is however typically accompanied with</context>
<context position="20820" citStr="Levy and Goldberg (2014" startWordPosition="3691" endWordPosition="3694">al., 2014; Chen and Manning, 2014). Here, we focus on understanding word embeddings in the context of a generative word class model, as well as in empirical tasks that directly evaluate the word embeddings themselves. 7 Experiments 7.1 Word similarity and analogy We first consider word similarity and analogy tasks for evaluating the quality of word embeddings. Word similarity measures the Spearman’s correlation coefficient between the human scores and the embeddings’ cosine similarities for word pairs. Word analogy measures the accuracy on syntactic and semantic analogy questions. We refer to Levy and Goldberg (2014a) for a detailed description of these tasks. We use the multiplicative technique of Levy and Goldberg (2014a) for answering analogy questions. For the choice of corpus, we use a preprocessed English Wikipedia dump (http:// dumps.wikimedia.org/). The corpus contains around 1.4 billion words. We only preserve word types that appear more than 100 times and replace all others with a special symbol, resulting in a vocabulary of size around 188k. We define context words to be 5 words to the left/right for all considered methods. We use three word similarity datasets each containing 353, 3000, and 2</context>
</contexts>
<marker>Levy, Goldberg, 2014</marker>
<rawString>Omer Levy and Yoav Goldberg. 2014b. Neural word embedding as implicit matrix factorization. In Proceedings of theAdvances in Neural Information Processing Systems, pages 2177–2185.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
<author>Ido Dagan</author>
<author>Israel Ramat-Gan</author>
</authors>
<title>Improving distributional similarity with lessons learned from word embeddings.</title>
<date>2015</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<volume>3</volume>
<contexts>
<context position="16309" citStr="Levy et al., 2015" startWordPosition="2893" endWordPosition="2896"> Note that the choice of α only affects methods that make use of the context distribution (s E {ppmi, cca}). The parameter β controls the role of singular values in word embeddings. This is always 0 for CCA as it does not require singular values. But for other methods, one can consider setting β &gt; 0 since the best-fit subspace for the rows of Q is given by UE. For example, Deerwester et al. (1990) use β = 1 and Levy and Goldberg (2014b) use β = 0.5. However, it has been found by many (including ourselves) that setting β = 1 yields substantially worse representations than setting β E {0, 0.5} (Levy et al., 2015). Different combinations of these aspects reproduce various spectral embeddings explored in the literature. We enumerate some meaningful combinations: Figure 2: A template for spectral word embedding methods. No scaling [t E {—, log, sqrt}, s = —]. This is a commonly considered setting (e.g., in Pennington et al. (2014)) where no scaling is applied to the co-occurrence counts. It is however typically accompanied with some kind of data transformation. Positive point-wise mutual information (PPMI) [t = —, s = ppmi]. Mutual information is a popular metric in many natural language tasks (Brown et </context>
<context position="22053" citStr="Levy et al. (2015)" startWordPosition="3892" endWordPosition="3895">3 We report the average similarity score across these datasets under the label AVG-SIM. We use two word analogy datasets that we call SYN (8000 syntactic analogy questions) and MIXED (19544 syntactic and semantic analogy questions).4 We implemented the template in Figure 2 in C++.5 We compared against the public implementation of WORD2VEC by Mikolov et al. (2013b) and GLOVE by Pennington et al. (2014). These external implementations have numerous hyperparameters that are not part of the core algorithm, such as random subsampling in WORD2VEC and the word-context averaging in GLOVE. We refer to Levy et al. (2015) for a discussion of the effect of these features. In our experiments, we enable all these features with the recommended default settings. We reserve a half of each dataset (by category) 3WordSim-353: http://www.cs.technion.ac. il/˜gabr/resources/data/wordsim353/; MEN: http://clic.cimec.unitn.it/˜elia.bruni/ MEN.html; Stanford Rare Word: http://www-nlp. stanford.edu/˜lmthang/morphoNLM/. 4http://research.microsoft.com/en-us/ um/people/gzweig/Pubs/myz_naacl13_ test_set.tgz; http://www.fit.vutbr.cz/ ˜imikolov/rnnlm/word-test.v1.txt 5The code is available at https://github.com/ karlstratos/singula</context>
</contexts>
<marker>Levy, Goldberg, Dagan, Ramat-Gan, 2015</marker>
<rawString>Omer Levy, Yoav Goldberg, Ido Dagan, and Israel Ramat-Gan. 2015. Improving distributional similarity with lessons learned from word embeddings. Transactions of the Association for Computational Linguistics, 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</title>
<date>2013</date>
<contexts>
<context position="1194" citStr="Mikolov et al. (2013" startWordPosition="172" endWordPosition="175">antics. We further show in experiments that these techniques empirically outperform existing spectral methods on word similarity and analogy tasks, and are also competitive with other popular methods such as WORD2VEC and GLOVE. 1 Introduction The recent spike of interest in dense, lowdimensional lexical representations—i.e., word embeddings—is largely due to their ability to capture subtle syntactic and semantic patterns that are useful in a variety of natural language tasks. A successful method for deriving such embeddings is the negative sampling training of the skip-gram model suggested by Mikolov et al. (2013b) and implemented in the popular software WORD2VEC. The form of its training objective was motivated by efficiency considerations, but has subsequently been interpreted by Levy and Goldberg (2014b) as seeking a low-rank factorization of a matrix whose entries are word-context co-occurrence counts, scaled and transformed in a certain way. This observation sheds new light on WORD2VEC, yet also raises several new questions about word embeddings based on decomposing count data. What is the right matrix to decompose? Are there rigorous justifications for the choice of matrix and count transformati</context>
<context position="15465" citStr="Mikolov et al. (2013" startWordPosition="2734" endWordPosition="2737"> al., 2003). The square-root transformation, nevertheless, obviates the variance-based weighting since the target values have approximately the same variance of 1/4. 5 A template for spectral methods Figure 2 gives a generic template that encompasses a range of spectral methods for deriving word embeddings. All of them operate on cooccurrence counts #(w, c) and share the low-rank SVD step, but they can differ in the data transformation method (t) and the definition of the matrix of scaled counts for SVD (s). We introduce two additional parameters α, β ≤ 1 to account for the following details. Mikolov et al. (2013b) proposed smoothing the empirical context distribution as ˆpα(c) := #(c)α/ Pc #(c)α and found α = 0.75 to work well in practice. We also found that setting α = 0.75 gave a small but consistent improvement over setting α = 1. Note that the choice of α only affects methods that make use of the context distribution (s E {ppmi, cca}). The parameter β controls the role of singular values in word embeddings. This is always 0 for CCA as it does not require singular values. But for other methods, one can consider setting β &gt; 0 since the best-fit subspace for the rows of Q is given by UE. For example</context>
<context position="17304" citStr="Mikolov et al. (2013" startWordPosition="3063" endWordPosition="3066">nts. It is however typically accompanied with some kind of data transformation. Positive point-wise mutual information (PPMI) [t = —, s = ppmi]. Mutual information is a popular metric in many natural language tasks (Brown et al., 1992; Pantel and Lin, 2002). In this setting, each term in the matrix for SVD is set as the pointwise mutual information between word w and context c: #(w, c) &amp; #(c)αlog ˆp( ˆp(w, c) w)ˆpα(c) =log #(w)#(c)α Typically negative values are thresholded to 0 to keep Q sparse. Levy and Goldberg (2014b) observed that the negative sampling objective of the skip-gram model of Mikolov et al. (2013b) is implicitly factorizing a shifted version of this matrix.2 2This is not equivalent to applying SVD on this matrix, however, since the loss function is different. SPECTRAL-TEMPLATE Input: word-context co-occurrence counts #(w, c), dimension m, transformation method t, scaling method s, context smoothing exponent α &lt; 1, singular value exponent β &lt; 1 Output: vector v(w) E Rm for each word w E [n] Definitions: #(w) := Ec #(w, c), #(c) := Ew #(w, c), N(α) := Ec #(c)α 1. Transform all #(w, c), #(w), and #(c): #(·) * I #(·) ift=— log(1 + #(·)) if t = log #(·)2/3 if t = two-thirds � #(·) if t = </context>
<context position="18860" citStr="Mikolov et al., 2013" startWordPosition="3377" endWordPosition="3380">e the w-th row of UEβ normalized to have unit 2-norm. Xmin uw,vc∈Rm w,c 1286 Regression [t E {—, sqrt}, s = reg]. Another novelty of our work is considering a lowrank approximation of a linear regressor that predicts the context from words. Denoting the word sample matrix by X E RNxn and the context sample matrix by Y E RNxn, we seek U* = arg minUERn×n ||Y − XU||2 whose closed-from solution is given by: U* = (XTX)−1XTY (7) Thus we aim to compute a low-rank approximation of U* with SVD. This is inspired by other predictive models in the representation learning literature (Ando and Zhang, 2005; Mikolov et al., 2013a). We consider applying the square-root transformation for the same variance stabilizing effect discussed in Section 4.3. CCA [t E {—, two-thirds, sqrt}, s = cca]. This is the focus of our work. As shown in Theorem 4.1, we can take the element-wise power transformation on counts (such as the power of 1, 2/3,1/2 in this template) while preserving the representational meaning of word embeddings under the Brown model interpretation. If there is no data transformation (t = —), then we recover the original spectral algorithm of Stratos et al. (2014). 6 Related work We make a few remarks on related</context>
<context position="21799" citStr="Mikolov et al. (2013" startWordPosition="3851" endWordPosition="3854">place all others with a special symbol, resulting in a vocabulary of size around 188k. We define context words to be 5 words to the left/right for all considered methods. We use three word similarity datasets each containing 353, 3000, and 2034 word pairs.3 We report the average similarity score across these datasets under the label AVG-SIM. We use two word analogy datasets that we call SYN (8000 syntactic analogy questions) and MIXED (19544 syntactic and semantic analogy questions).4 We implemented the template in Figure 2 in C++.5 We compared against the public implementation of WORD2VEC by Mikolov et al. (2013b) and GLOVE by Pennington et al. (2014). These external implementations have numerous hyperparameters that are not part of the core algorithm, such as random subsampling in WORD2VEC and the word-context averaging in GLOVE. We refer to Levy et al. (2015) for a discussion of the effect of these features. In our experiments, we enable all these features with the recommended default settings. We reserve a half of each dataset (by category) 3WordSim-353: http://www.cs.technion.ac. il/˜gabr/resources/data/wordsim353/; MEN: http://clic.cimec.unitn.it/˜elia.bruni/ MEN.html; Stanford Rare Word: http:/</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Proceedings of the Advances in Neural Information Processing Systems,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="1194" citStr="Mikolov et al. (2013" startWordPosition="172" endWordPosition="175">antics. We further show in experiments that these techniques empirically outperform existing spectral methods on word similarity and analogy tasks, and are also competitive with other popular methods such as WORD2VEC and GLOVE. 1 Introduction The recent spike of interest in dense, lowdimensional lexical representations—i.e., word embeddings—is largely due to their ability to capture subtle syntactic and semantic patterns that are useful in a variety of natural language tasks. A successful method for deriving such embeddings is the negative sampling training of the skip-gram model suggested by Mikolov et al. (2013b) and implemented in the popular software WORD2VEC. The form of its training objective was motivated by efficiency considerations, but has subsequently been interpreted by Levy and Goldberg (2014b) as seeking a low-rank factorization of a matrix whose entries are word-context co-occurrence counts, scaled and transformed in a certain way. This observation sheds new light on WORD2VEC, yet also raises several new questions about word embeddings based on decomposing count data. What is the right matrix to decompose? Are there rigorous justifications for the choice of matrix and count transformati</context>
<context position="15465" citStr="Mikolov et al. (2013" startWordPosition="2734" endWordPosition="2737"> al., 2003). The square-root transformation, nevertheless, obviates the variance-based weighting since the target values have approximately the same variance of 1/4. 5 A template for spectral methods Figure 2 gives a generic template that encompasses a range of spectral methods for deriving word embeddings. All of them operate on cooccurrence counts #(w, c) and share the low-rank SVD step, but they can differ in the data transformation method (t) and the definition of the matrix of scaled counts for SVD (s). We introduce two additional parameters α, β ≤ 1 to account for the following details. Mikolov et al. (2013b) proposed smoothing the empirical context distribution as ˆpα(c) := #(c)α/ Pc #(c)α and found α = 0.75 to work well in practice. We also found that setting α = 0.75 gave a small but consistent improvement over setting α = 1. Note that the choice of α only affects methods that make use of the context distribution (s E {ppmi, cca}). The parameter β controls the role of singular values in word embeddings. This is always 0 for CCA as it does not require singular values. But for other methods, one can consider setting β &gt; 0 since the best-fit subspace for the rows of Q is given by UE. For example</context>
<context position="17304" citStr="Mikolov et al. (2013" startWordPosition="3063" endWordPosition="3066">nts. It is however typically accompanied with some kind of data transformation. Positive point-wise mutual information (PPMI) [t = —, s = ppmi]. Mutual information is a popular metric in many natural language tasks (Brown et al., 1992; Pantel and Lin, 2002). In this setting, each term in the matrix for SVD is set as the pointwise mutual information between word w and context c: #(w, c) &amp; #(c)αlog ˆp( ˆp(w, c) w)ˆpα(c) =log #(w)#(c)α Typically negative values are thresholded to 0 to keep Q sparse. Levy and Goldberg (2014b) observed that the negative sampling objective of the skip-gram model of Mikolov et al. (2013b) is implicitly factorizing a shifted version of this matrix.2 2This is not equivalent to applying SVD on this matrix, however, since the loss function is different. SPECTRAL-TEMPLATE Input: word-context co-occurrence counts #(w, c), dimension m, transformation method t, scaling method s, context smoothing exponent α &lt; 1, singular value exponent β &lt; 1 Output: vector v(w) E Rm for each word w E [n] Definitions: #(w) := Ec #(w, c), #(c) := Ew #(w, c), N(α) := Ec #(c)α 1. Transform all #(w, c), #(w), and #(c): #(·) * I #(·) ift=— log(1 + #(·)) if t = log #(·)2/3 if t = two-thirds � #(·) if t = </context>
<context position="18860" citStr="Mikolov et al., 2013" startWordPosition="3377" endWordPosition="3380">e the w-th row of UEβ normalized to have unit 2-norm. Xmin uw,vc∈Rm w,c 1286 Regression [t E {—, sqrt}, s = reg]. Another novelty of our work is considering a lowrank approximation of a linear regressor that predicts the context from words. Denoting the word sample matrix by X E RNxn and the context sample matrix by Y E RNxn, we seek U* = arg minUERn×n ||Y − XU||2 whose closed-from solution is given by: U* = (XTX)−1XTY (7) Thus we aim to compute a low-rank approximation of U* with SVD. This is inspired by other predictive models in the representation learning literature (Ando and Zhang, 2005; Mikolov et al., 2013a). We consider applying the square-root transformation for the same variance stabilizing effect discussed in Section 4.3. CCA [t E {—, two-thirds, sqrt}, s = cca]. This is the focus of our work. As shown in Theorem 4.1, we can take the element-wise power transformation on counts (such as the power of 1, 2/3,1/2 in this template) while preserving the representational meaning of word embeddings under the Brown model interpretation. If there is no data transformation (t = —), then we recover the original spectral algorithm of Stratos et al. (2014). 6 Related work We make a few remarks on related</context>
<context position="21799" citStr="Mikolov et al. (2013" startWordPosition="3851" endWordPosition="3854">place all others with a special symbol, resulting in a vocabulary of size around 188k. We define context words to be 5 words to the left/right for all considered methods. We use three word similarity datasets each containing 353, 3000, and 2034 word pairs.3 We report the average similarity score across these datasets under the label AVG-SIM. We use two word analogy datasets that we call SYN (8000 syntactic analogy questions) and MIXED (19544 syntactic and semantic analogy questions).4 We implemented the template in Figure 2 in C++.5 We compared against the public implementation of WORD2VEC by Mikolov et al. (2013b) and GLOVE by Pennington et al. (2014). These external implementations have numerous hyperparameters that are not part of the core algorithm, such as random subsampling in WORD2VEC and the word-context averaging in GLOVE. We refer to Levy et al. (2015) for a discussion of the effect of these features. In our experiments, we enable all these features with the recommended default settings. We reserve a half of each dataset (by category) 3WordSim-353: http://www.cs.technion.ac. il/˜gabr/resources/data/wordsim353/; MEN: http://clic.cimec.unitn.it/˜elia.bruni/ MEN.html; Stanford Rare Word: http:/</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013b. Distributed representations of words and phrases and their compositionality. In Proceedings of the Advances in Neural Information Processing Systems, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Dekang Lin</author>
</authors>
<title>Discovering word senses from text.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>613--619</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="16941" citStr="Pantel and Lin, 2002" startWordPosition="2997" endWordPosition="3000"> combinations of these aspects reproduce various spectral embeddings explored in the literature. We enumerate some meaningful combinations: Figure 2: A template for spectral word embedding methods. No scaling [t E {—, log, sqrt}, s = —]. This is a commonly considered setting (e.g., in Pennington et al. (2014)) where no scaling is applied to the co-occurrence counts. It is however typically accompanied with some kind of data transformation. Positive point-wise mutual information (PPMI) [t = —, s = ppmi]. Mutual information is a popular metric in many natural language tasks (Brown et al., 1992; Pantel and Lin, 2002). In this setting, each term in the matrix for SVD is set as the pointwise mutual information between word w and context c: #(w, c) &amp; #(c)αlog ˆp( ˆp(w, c) w)ˆpα(c) =log #(w)#(c)α Typically negative values are thresholded to 0 to keep Q sparse. Levy and Goldberg (2014b) observed that the negative sampling objective of the skip-gram model of Mikolov et al. (2013b) is implicitly factorizing a shifted version of this matrix.2 2This is not equivalent to applying SVD on this matrix, however, since the loss function is different. SPECTRAL-TEMPLATE Input: word-context co-occurrence counts #(w, c), di</context>
</contexts>
<marker>Pantel, Lin, 2002</marker>
<rawString>Patrick Pantel and Dekang Lin. 2002. Discovering word senses from text. In Proceedings of the ACM SIGKDD international conference on Knowledge discovery and data mining, pages 613–619. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Pennington</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Glove: Global vectors for word representation.</title>
<date>2014</date>
<booktitle>In Proceedings of the Empiricial Methods in Natural Language Processing,</booktitle>
<volume>12</volume>
<contexts>
<context position="16630" citStr="Pennington et al. (2014)" startWordPosition="2944" endWordPosition="2948">-fit subspace for the rows of Q is given by UE. For example, Deerwester et al. (1990) use β = 1 and Levy and Goldberg (2014b) use β = 0.5. However, it has been found by many (including ourselves) that setting β = 1 yields substantially worse representations than setting β E {0, 0.5} (Levy et al., 2015). Different combinations of these aspects reproduce various spectral embeddings explored in the literature. We enumerate some meaningful combinations: Figure 2: A template for spectral word embedding methods. No scaling [t E {—, log, sqrt}, s = —]. This is a commonly considered setting (e.g., in Pennington et al. (2014)) where no scaling is applied to the co-occurrence counts. It is however typically accompanied with some kind of data transformation. Positive point-wise mutual information (PPMI) [t = —, s = ppmi]. Mutual information is a popular metric in many natural language tasks (Brown et al., 1992; Pantel and Lin, 2002). In this setting, each term in the matrix for SVD is set as the pointwise mutual information between word w and context c: #(w, c) &amp; #(c)αlog ˆp( ˆp(w, c) w)ˆpα(c) =log #(w)#(c)α Typically negative values are thresholded to 0 to keep Q sparse. Levy and Goldberg (2014b) observed that the </context>
<context position="19756" citStr="Pennington et al. (2014)" startWordPosition="3527" endWordPosition="3530">n counts (such as the power of 1, 2/3,1/2 in this template) while preserving the representational meaning of word embeddings under the Brown model interpretation. If there is no data transformation (t = —), then we recover the original spectral algorithm of Stratos et al. (2014). 6 Related work We make a few remarks on related works not already discussed earlier. Dhillon et al. (2011) and (2012) propose novel modifications of CCA (LRMVL and two-step CCA) to derive word embeddings, but do not establish any explicit connection to learning HMM parameters or justify the squareroot transformation. Pennington et al. (2014) propose a weighted factorization of log-transformed co-occurrence counts, which is generally an intractable problem (Srebro et al., 2003). In contrast, our method requires only efficiently computable matrix decompositions. Finally, word embeddings have also been used as features to improve performance in a variety of supervised tasks such as sequence labeling (Dhillon et al., 2011; Collobert et al., 2011) and dependency parsing (Lei et al., 2014; Chen and Manning, 2014). Here, we focus on understanding word embeddings in the context of a generative word class model, as well as in empirical ta</context>
<context position="21839" citStr="Pennington et al. (2014)" startWordPosition="3858" endWordPosition="3861">ol, resulting in a vocabulary of size around 188k. We define context words to be 5 words to the left/right for all considered methods. We use three word similarity datasets each containing 353, 3000, and 2034 word pairs.3 We report the average similarity score across these datasets under the label AVG-SIM. We use two word analogy datasets that we call SYN (8000 syntactic analogy questions) and MIXED (19544 syntactic and semantic analogy questions).4 We implemented the template in Figure 2 in C++.5 We compared against the public implementation of WORD2VEC by Mikolov et al. (2013b) and GLOVE by Pennington et al. (2014). These external implementations have numerous hyperparameters that are not part of the core algorithm, such as random subsampling in WORD2VEC and the word-context averaging in GLOVE. We refer to Levy et al. (2015) for a discussion of the effect of these features. In our experiments, we enable all these features with the recommended default settings. We reserve a half of each dataset (by category) 3WordSim-353: http://www.cs.technion.ac. il/˜gabr/resources/data/wordsim353/; MEN: http://clic.cimec.unitn.it/˜elia.bruni/ MEN.html; Stanford Rare Word: http://www-nlp. stanford.edu/˜lmthang/morphoNL</context>
</contexts>
<marker>Pennington, Socher, Manning, 2014</marker>
<rawString>Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the Empiricial Methods in Natural Language Processing, volume 12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathan Srebro</author>
<author>Tommi Jaakkola</author>
</authors>
<title>Weighted low-rank approximations.</title>
<date>2003</date>
<booktitle>In Proceedings of the International Conference on Machine learning,</booktitle>
<volume>3</volume>
<pages>720--727</pages>
<marker>Srebro, Jaakkola, 2003</marker>
<rawString>Nathan Srebro, Tommi Jaakkola, et al. 2003. Weighted low-rank approximations. In Proceedings of the International Conference on Machine learning, volume 3, pages 720–727.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert G D Steel</author>
</authors>
<title>Relation between poisson and multinomial distributions.</title>
<date>1953</date>
<tech>Technical Report BU-39-M,</tech>
<institution>Cornell University.</institution>
<contexts>
<context position="13906" citStr="Steel, 1953" startWordPosition="2485" endWordPosition="2486">rtlett (1936)). Let X be a random variable with distribution Poisson(n x p) for any p E (0, 1) and positive integer n. Define Y := -/X. Then the variance of Y approaches 1/4 as n — *oc. This transformation is relevant for word counts because they can be naturally modeled as Poisson variables. Indeed, if word counts in a corpus of length N are drawn from a multinomial distribution over [n] with N observations, then these counts have the same distribution as n independent Poisson variables (whose rate parameters are related to the multinomial probabilities), conditioned on their sum equaling N (Steel, 1953). Empirically, the peaky concentration of a Poisson distribution is well-suited for modeling word occurrences. 4.3.2 Variance-weighted squared-error minimization At the heart of CCA is computing the SVD of the Q(a) matrix: this can be interpreted as solving the following (non-convex) squared-error minimization problem: �Q(a) T \ 2 w,c —uwvcJl Q(a) w,c ˆQ(a) w,c #(w, c)a Xmin uw,vcERm w,c 1285 But we note that minimizing unweighted squarederror objectives is generally suboptimal when the target values are heteroscedastic. For instance, in linear regression, it is well-known that a weighted leas</context>
</contexts>
<marker>Steel, 1953</marker>
<rawString>Robert G. D. Steel. 1953. Relation between poisson and multinomial distributions. Technical Report BU-39-M, Cornell University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl Stratos</author>
<author>Do-kyum Kim</author>
<author>Michael Collins</author>
<author>Daniel Hsu</author>
</authors>
<title>A spectral algorithm for learning class-based n-gram models of natural language.</title>
<date>2014</date>
<booktitle>In Proceedings of the Association for Uncertainty in Artificial Intelligence.</booktitle>
<contexts>
<context position="2146" citStr="Stratos et al. (2014)" startWordPosition="320" endWordPosition="323">tain way. This observation sheds new light on WORD2VEC, yet also raises several new questions about word embeddings based on decomposing count data. What is the right matrix to decompose? Are there rigorous justifications for the choice of matrix and count transformations? In this paper, we answer some of these questions by investigating the decomposition specified by CCA (Hotelling, 1936), a powerful technique for inducing generic representations whose computation is efficiently and exactly reduced to that of a matrix singular value decomposition (SVD). We build on and strengthen the work of Stratos et al. (2014) which uses CCA for learning the class of HMMs underlying Brown clustering. We show that certain count transformations enhance the accuracy of the estimation method and significantly improve the empirical performance of word representations derived from these model parameters (Table 1). In addition to providing a rigorous justification for CCA-based word embeddings, we also supply a general template that encompasses a range of spectral methods (algorithms employing SVD) for inducing word embeddings in the literature, including the method of Levy and Goldberg (2014b). In experiments, we demonst</context>
<context position="8442" citStr="Stratos et al., 2014" startWordPosition="1454" endWordPosition="1457">bservation 2. The (uncentered) covariance matrices E[XX&gt;] and E[Y Y &gt;] are diagonal. This follows from our definition of the word and context variables as one-hot encodings since E[XwXw,] = 0 for w =� w0 and E[YcYc,] = 0 for c =�c0. With these observations and the binary definition of (X, Y ), each entry in Q now has a simple closed-form solution: P(Xw = 1, Yc = 1) Qw,c = (4) pP(Xw = 1)P(Yc = 1) which can be readily estimated from a corpus. 4 Using CCA for parameter estimation In a less well-known interpretation of Eq. (4), CCA is seen as a parameter estimation algorithm for a language model (Stratos et al., 2014). This model is a restricted class of HMMs introduced by Brown et al. (1992), henceforth called the Brown model. In this section, we extend the result of Stratos et al. (2014) and show that its correctness is preserved under certain element-wise data transformations. 4.1 Clustering under a Brown model A Brown model is a 5-tuple (n, m, 7r, t, o) for n, m E N and functions 7r, t, o where • [n] is a set of word types. • [m] is a set of hidden states. • 7r(h) is the probability of generating h E [m] in the first position of a sequence. • t(h0|h) is the probability of generating h0 E [m] given h E </context>
<context position="12556" citStr="Stratos et al. (2014)" startWordPosition="2251" endWordPosition="2254">, Y E Rn be one-hot encodings of words and their associated context words. Let U E Rnxm be the matrix of m left singular vectors of Q(a) E Rnxn corresponding to nonzero singular values where Q is defined in Eq. (4) and a =� 0: P(Xw = 1, Yc = 1)a p P(Xw = 1)aP(Yc = 1)a Then there exists an orthogonal matrix Q E Rmxm and a positive s E Rm such that U = O(a/2)diag(s)QT. This theorem states that the CCA projection of words in Section 3.3 is the rows of O up to scaling and rotation even if we raise each element of Q in Eq. (4) to an arbitrary (nonzero) power. The proof is a variant of the proof in Stratos et al. (2014) and is given in Appendix A. 4.3 Choice of data transformation Given a corpus, the sample estimate of Q(a) is given by: p(5) #(w)a#(c)a where #(w, c) denotes the co-occurrence count of word w and context c in the corpus, #(w) := Pc #(w, c), and #(c) := Pw #(w, c). What choice of a is beneficial and why? We use a = 1/2 for the following reason: it stabilizes the variance of the term and thereby gives a more statistically stable solution. 4.3.1 Variance stabilization for word counts The square-root transformation is a variancestabilizing transformation for Poisson random variables (Bartlett, 193</context>
<context position="19411" citStr="Stratos et al. (2014)" startWordPosition="3469" endWordPosition="3472">tation learning literature (Ando and Zhang, 2005; Mikolov et al., 2013a). We consider applying the square-root transformation for the same variance stabilizing effect discussed in Section 4.3. CCA [t E {—, two-thirds, sqrt}, s = cca]. This is the focus of our work. As shown in Theorem 4.1, we can take the element-wise power transformation on counts (such as the power of 1, 2/3,1/2 in this template) while preserving the representational meaning of word embeddings under the Brown model interpretation. If there is no data transformation (t = —), then we recover the original spectral algorithm of Stratos et al. (2014). 6 Related work We make a few remarks on related works not already discussed earlier. Dhillon et al. (2011) and (2012) propose novel modifications of CCA (LRMVL and two-step CCA) to derive word embeddings, but do not establish any explicit connection to learning HMM parameters or justify the squareroot transformation. Pennington et al. (2014) propose a weighted factorization of log-transformed co-occurrence counts, which is generally an intractable problem (Srebro et al., 2003). In contrast, our method requires only efficiently computable matrix decompositions. Finally, word embeddings have a</context>
<context position="26861" citStr="Stratos et al. (2014)" startWordPosition="4637" endWordPosition="4640">88 0.609 66.47 70.48 PPMI 0.628 43.81 58.38 0.637 48.99 63.82 CCA 0.655 68.38 74.17 0.650 66.08 76.38 Others GLOVE 0.576 68.30 78.08 0.586 67.40 78.73 CBOW 0.597 75.79 73.60 0.509 70.97 60.12 SKIP 0.642 81.08 78.73 0.641 79.98 83.35 Table 3: Performance of different word embedding methods on the test portion of data. See the main text for the configuration details of spectral methods. 7.2 As features in a supervised task Finally, we use word embeddings as features in NER and compare the subsequent improvements between various embedding methods. The experimental setting is identical to that of Stratos et al. (2014). We use the Reuters RCV1 corpus which contains 205 million words. With frequency thresholding, we end up with a vocabulary of size around 301k. We derive LOG, REG, PPMI, and CCA embeddings as described in Section 7.1.3, and GLOVE, CBOW, and SKIP embeddings again with the recommended default settings. The number of left/right contexts is 2 for all methods. For comparison, we also derived 1000 Brown clusters (BROWN) on the same vocabulary and used the resulting bit strings as features (Brown et al., 1992). Table 4 shows the result for both 30 and 50 dimensions. In general, using any of these le</context>
</contexts>
<marker>Stratos, Kim, Collins, Hsu, 2014</marker>
<rawString>Karl Stratos, Do-kyum Kim, Michael Collins, and Daniel Hsu. 2014. A spectral algorithm for learning class-based n-gram models of natural language. In Proceedings of the Association for Uncertainty in Artificial Intelligence.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>