<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000371">
<title confidence="0.56830725">
Making Grammar-Based Generation Easier to Deploy in Dialogue Systems
David DeVault and David Traum and Ron Artstein
USC Institute for Creative Technologies
13274 Fiji Way
</title>
<author confidence="0.375042">
Marina del Rey, CA 90292
</author>
<email confidence="0.998182">
{devault,traum,artstein}@ict.usc.edu
</email>
<sectionHeader confidence="0.995632" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999749117647059">
We present a development pipeline and asso-
ciated algorithms designed to make grammar-
based generation easier to deploy in imple-
mented dialogue systems. Our approach real-
izes a practical trade-off between the capabili-
ties of a system’s generation component and
the authoring and maintenance burdens im-
posed on the generation content author for a
deployed system. To evaluate our approach,
we performed a human rating study with sys-
tem builders who work on a common large-
scale spoken dialogue system. Our results
demonstrate the viability of our approach and
illustrate authoring/performance trade-offs be-
tween hand-authored text, our grammar-based
approach, and a competing shallow statistical
NLG technique.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999699125">
This paper gives an overview of a new example-
based generation technique that is designed to make
grammar-based generation easier to deploy in dia-
logue systems. Dialogue systems present several
specific requirements for a practical generation com-
ponent. First, the generator needs to be fast enough
to support real-time interaction with a human user.
Second, the generator must provide adequate cover-
age for the meanings the dialogue system needs to
express. What counts as “adequate” can vary be-
tween systems, since the high-level purpose of a di-
alogue system can affect priorities regarding output
fluency, fidelity to the requested meaning, variety
of alternative outputs, and tolerance for generation
failures. Third, developing the necessary resources
for the generation component should be relatively
straightforward in terms of time and expertise re-
quired. This is especially important since dialogue
systems are complex systems with significant devel-
opment costs. Finally, it should be relatively easy
for the dialogue manager to formulate a generation
request in the format required by the generator.
Together, these requirements can reduce the at-
tractiveness of grammar-based generation when
compared to simpler template-based or canned text
output solutions. In terms of speed, off-the-
shelf, wide-coverage grammar-based realizers such
as FUF/SURGE (Elhadad, 1991) can be too slow for
real-time interaction (Callaway, 2003).
In terms of adequacy of coverage, in principle,
grammar-based generation offers significant advan-
tages over template-based or canned text output by
providing productive coverage and greater variety.
However, realizing these advantages can require sig-
nificant development costs. Specifying the neces-
sary connections between lexico-syntactic resources
and the flat, domain-specific semantic representa-
tions that are typically available in implemented sys-
tems is a subtle, labor-intensive, and knowledge-
intensive process for which attractive methodologies
do not yet exist (Reiter et al., 2003).
One strategy is to hand-build an application-
specific grammar. However, in our experience,
this process requires a painstaking, time-consuming
effort by a developer who has detailed linguistic
knowledge as well as detailed domain knowledge,
and the resulting coverage is inevitably limited.
Wide-coverage generators that aim for applicabil-
</bodyText>
<page confidence="0.972692">
198
</page>
<note confidence="0.9072115">
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 198–207,
Columbus, June 2008. c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.975891815789474">
ity across application domains (White et al., 2007;
Zhong and Stent, 2005; Langkilde-Geary, 2002;
Langkilde and Knight, 1998; Elhadad, 1991) pro-
vide a grammar (or language model) for free. How-
ever, it is harder to tailor output to the desired word-
ing and style for a specific dialogue system, and
these generators demand a specific input format that
is otherwise foreign to an existing dialogue system.
Unfortunately, in our experience, the development
burden of implementing the translation between the
system’s available meaning representations and the
generator’s required input format is quite substan-
tial. Indeed, implementing the translation might re-
quire as much effort as would be required to build a
simple custom generator; cf. (Callaway, 2003; Buse-
mann and Horacek, 1998). This development cost is
exacerbated when a dialogue system’s native mean-
ing representation scheme is under revision.
In this paper, we survey a new example-based ap-
proach (DeVault et al., 2008) that we have devel-
oped in order to mitigate these difficulties, so that
grammar-based generation can be deployed more
widely in implemented dialogue systems. Our de-
velopment pipeline requires a system developer to
create a set of training examples which directly
connect desired output texts to available applica-
tion semantic forms. This is achieved through a
streamlined authoring task that does not require de-
tailed linguistic knowledge. Our approach then
processes these training examples to automatically
construct all the resources needed for a fast, high-
quality, run-time grammar-based generation compo-
nent. We evaluate this approach using a pre-existing
spoken dialogue system. Our results demonstrate
the viability of the approach and illustrate author-
ing/performance trade-offs between hand-authored
text, our grammar-based approach, and a competing
shallow statistical NLG technique.
</bodyText>
<sectionHeader confidence="0.794842" genericHeader="introduction">
2 Background and Motivation
</sectionHeader>
<bodyText confidence="0.997668142857143">
The generation approach set out in this paper has
been developed in the context of a research pro-
gram aimed at creating interactive virtual humans
for social training purposes (Swartout et al., 2006).
Virtual humans are embodied conversational agents
that play the role of people in simulations or games.
They interact with human users and other virtual hu-
</bodyText>
<figureCaption confidence="0.997356">
Figure 1: Doctor Perez.
</figureCaption>
<bodyText confidence="0.999853411764706">
mans using spoken language and non-verbal behav-
ior such as eye gaze, gesture, and facial displays.
The case study we present here is the genera-
tion of output utterances for a particular virtual hu-
man, Doctor Perez (see Figure 1), who is designed
to teach negotiation skills in a multi-modal, multi-
party, non-team dialogue setting (Traum et al., 2005;
Traum et al., 2008). The human trainee who talks
to the doctor plays the role of a U.S. Army captain
named Captain Kirk. We summarize Doctor Perez’s
generation requirements as follows.
In order to support compelling real-time conver-
sation and effective training, the generator must be
able to identify an utterance for Doctor Perez to use
within approximately 200ms on modern hardware.
Doctor Perez has a relatively rich internal men-
tal state including beliefs, goals, plans, and emo-
tions. As Doctor Perez attempts to achieve his con-
versational goals, his utterances need to take a va-
riety of syntactic forms, including simple declar-
ative sentences, various modal constructions relat-
ing to hypothetical actions or plans, yes/no and wh-
questions, and abbreviated dialogue forms such as
elliptical clarification and repair requests, ground-
ing, and turn-taking utterances. Doctor Perez cur-
rently uses about 200 distinct output utterances in
the course of his dialogues.
Doctor Perez is designed to simulate a non-native
English speaker, so highly fluent output is not a ne-
cessity; indeed, a small degree of disfluency is even
desirable in order to increase the realism of talking
to a non-native speaker.
Finally, in reasoning about user utterances, dia-
logue management, and generation, Doctor Perez
</bodyText>
<page confidence="0.994791">
199
</page>
<figure confidence="0.999076840909091">
2 3
addressee captain-kirk
6 2 3 7
6addressee captain-kirk 7
6 7
6dialogue-act type assign-turn
[actor doctor-perez 7
6 6 2 3 7 7
6actor doctor-perez 7
6 6 6 7 7
addressee captain-kirk
6 6 7 7 7
6 6 7
6 6action assert 7 7
6 6 2 3 7
7 7
6 6 7
type state
6 6 7 7 7
66 6 7 7
6 speech-act 6 6polarity negative 7 7 7
6 6 6 7 7 7
6 6content6 6time present 7 7
7 7 7
6
6 6 6attribute resourceAttribute 7 7 7
7 7 7
6 6 6 7 7 7
4 4 4value medical-supplies 5 5 5 7
object-id market
addressee captain-kirk
dialogue-act.addressee captain-kirk
dialogue-act.type assign-turn
dialogue-act.actor doctor-perez
speech-act.actor doctor-perez
speech-act.addressee captain-kirk
speech-act.action assert
speech-act.content.type state
speech-act.content.polarity negative
speech-act.content.time present
speech-act.content.attribute resourceAttribute
speech-act.content.value medical-supplies
speech-act.content.object-id market
(a) Attribute-value matrix (b) Corresponding frame
</figure>
<figureCaption confidence="0.9903555">
Figure 2: An example of Doctor Perez’s representations for utterance semantics: Doctor Perez tells the captain that
there are no medical supplies at the market.
</figureCaption>
<bodyText confidence="0.999975529411765">
exploits an existing semantic representation scheme
that has been utilized in a family of virtual humans.
This scheme uses an attribute-value matrix (AVM)
representation to describe an utterance as a set of
core speech acts and other dialogue acts. Speech
acts generally have semantic contents that describe
propositions and questions about states and actions
in the domain, as well as other features such as po-
larity and modality. See (Traum, 2003) for some
more details and examples of this representation.
For ease of interprocess communication, and certain
kinds of statistical processing, this AVM structure is
linearized so that each non-recursive terminal value
is paired with a path from the root to the final at-
tribute. Thus, the AVM in Figure 2(a) is represented
as the “frame” in Figure 2(b).
Because the internal representations that make up
Doctor Perez’s mental state are under constant de-
velopment, the exact frames that are sent to the gen-
eration component change frequently as new rea-
soning capabilities are added and existing capabil-
ities are reorganized. Additionally, while only hun-
dreds of frames currently arise in actual dialogues,
the number of potential frames is orders of magni-
tude larger, and it is difficult to predict in advance
which frames might occur.
In this setting, over a period of years, a number
of different approaches to natural language gener-
ation have been implemented and tested, including
hand-authored canned text, domain specific hand-
built grammar-based generators (e.g., (Traum et al.,
2003)), shallow statistical generation techniques,
and the grammar-based approach presented in this
paper. We now turn to the details of our approach.
</bodyText>
<sectionHeader confidence="0.973157" genericHeader="method">
3 Technical Approach
</sectionHeader>
<bodyText confidence="0.999974357142857">
Our approach builds on recently developed tech-
niques in statistical parsing, lexicalized syntax mod-
eling, generation with lexicalized grammars, and
search optimization to automatically construct all
the resources needed for a high-quality run-time
generation component.
The approach involves three primary steps: spec-
ification of training examples, grammar induction,
and search optimization. In this section, we present
the format that training examples take and then sum-
marize the subsequent automatic processing steps.
Due to space limitations, we omit the full details
of these automatic processing steps, and refer the
reader to (DeVault et al., 2008) for additional details.
</bodyText>
<subsectionHeader confidence="0.999901">
3.1 Specification of Training Examples
</subsectionHeader>
<bodyText confidence="0.999828">
Each training example in our approach speci-
fies a target output utterance (string), its syn-
tax, and a set of links between substrings within
the utterance and system semantic representa-
tions. Formally, a training example takes the form
(u, syntax(u), semantics(u)). We will illustrate
this format using the training example in Figure 3.
In this example, the generation content author
</bodyText>
<page confidence="0.955106">
200
</page>
<figure confidence="0.970595361111111">
Utterance we don’t have medical supplies here captain
cat: SA
cat: NP
pos: PRP
we
pos: JJ
medical
pos: NNS
supplies
cat: NP
pos: NN
captain
pos: AUX
do
pos: AUX
have
cat: ADVP
pos: RB
here
cat: VP
cat: NP
cat: S
cat: VP
pos: RB
n’t
Syntax
Semantics
we do n’t
speech-act.action = assert
�speech-act.content.polarity = negative
have speech-act.content.attribute = resourceAttribute
medical supplies speech-act.content.value = medical-supplies
here speech-act.content.object-id = market
captain { addressee = captain-kirk
dialogue-act.addressee = captain-kirk
speech-act.addressee = captain-kirk
</figure>
<figureCaption confidence="0.999352">
Figure 3: A generation training example for Doctor Perez.
</figureCaption>
<bodyText confidence="0.99855172">
suggests the output utterance u = we don’t have
medical supplies here captain. Each utterance u is
accompanied by syntax(u), a syntactic analysis in
Penn Treebank format (Marcus et al., 1994). In this
example, the syntax is a hand-corrected version of
the output of the Charniak parser (Charniak, 2001;
Charniak, 2005) on this sentence; we discuss this
hand correction in Section 4.
To represent the meaning of utterances, our ap-
proach assumes that the system provides some set
M = {m1, ..., mj} of semantic representations.
The meaning of any individual utterance is then
identified with some subset of M. For Doctor Perez,
M comprises the 232 distinct key-value pairs that
appear in the system’s various generation frames. In
this example, the utterance’s meaning is captured by
the 8 key-value pairs indicated in the figure.
Our approach requires the generation content
author to link these 8 key-value pairs to con-
tiguous surface expressions within the utterance.
The technique is flexible about which surface ex-
pressions are chosen (e.g. they need not corre-
spond to constituent boundaries); however, they do
need to be compatible with the way the syntactic
analysis tokenizes the utterance, as follows. Let
</bodyText>
<equation confidence="0.89121975">
t(u) = (t1,..., t,) be the terminals in the syn-
tactic analysis, in left-to-right order. Formally,
semantics(u) = {(s1, M1), ..., (sk, Mk)}, where
t(u) = s1@ · · · @sk (with @ denoting concatena-
</equation>
<bodyText confidence="0.99997208">
tion), and where Mi C M for all i E L.k. In this
example, the surface expression we don’t, which to-
kenizes as (we, do, n′t), is connected to key-values
that indicate a negative polarity assertion.
This training example format has two features that
are crucial to our approach. First, the semantics of
an utterance is specified independently of its syntax.
This greatly reduces the amount of linguistic exper-
tise a generation content author needs to have. It
also allows making changes to the underlying syn-
tax without having to re-author the semantic links.
Second, the assignment of semantic representa-
tions to surface expressions must span the entire ut-
terance. No words or expressions can be viewed as
“meaningless”. This is essential because, otherwise,
the semantically motivated search algorithm used in
generation has no basis on which to include those
particular expressions when it constructs its output
utterance. Many systems, including Doctor Perez,
lack some of the internal representations that would
be necessary to specify semantics down to the lex-
ical level. An important feature of our approach is
that it allows an arbitrary semantic granularity to be
employed, by mapping the representations available
in the system to appropriate multi-word chunks.
</bodyText>
<page confidence="0.990147">
201
</page>
<subsectionHeader confidence="0.809946">
3.2 Automatic Grammar Induction and Search
Optimization
</subsectionHeader>
<bodyText confidence="0.999974024390244">
The first processing step is to induce a productive
grammar from the training examples. We adopt the
probabilistic tree-adjoining grammar (PTAG) for-
malism and grammar induction technique of (Chi-
ang, 2003). We induce our grammar from training
examples such as Figure 3 using heuristic rules to
assign derivations to the examples, as in (Chiang,
2003). Once derivations have been assigned, sub-
trees within the training example syntax are incre-
mentally detached. This process yields the reusable
linguistic resources in the grammar, as well as the
statistical model needed to compute operation prob-
abilities when the grammar is later used in genera-
tion. Figure 5 in the Appendix illustrates this pro-
cess by presenting the linguistic resources inferred
from the training example of Figure 3.
Our approach uses this induced grammar to treat
generation as a search problem: given a desired se-
mantic representation M′ ⊆ M, use the grammar
to incrementally construct an output utterance u that
expresses M′. We treat generation as anytime search
by accruing multiple goal states up until a specified
timeout (200ms for Doctor Perez) and returning a
list of alternative outputs ranked by their derivation
probabilities.
The search space created by a grammar induced
in this way is too large to be searched exhaustively
in most applications. The second step of automated
processing, then, uses the training examples to learn
an effective search policy so that good output sen-
tences can be found in a reasonable time frame. The
solution we have developed employs a beam search
strategy that uses weighted features to rank alterna-
tive grammatical expansions at each step. Our al-
gorithm for selecting features and weights is based
on the search optimization algorithm of (Daumé
and Marcu, 2005), which decides to update feature
weights when mistakes are made during search on
training examples. We use the boosting approach of
(Collins and Koo, 2005) to perform feature selection
and identify good weight values.
</bodyText>
<sectionHeader confidence="0.997761" genericHeader="method">
4 Empirical Evaluation
</sectionHeader>
<bodyText confidence="0.99996798">
In the introduction, we identified run-time speed, ad-
equacy of coverage, authoring burdens, and NLG re-
quest specification as important factors in the selec-
tion of a technology for a dialogue system’s NLG
component. In this section, we evaluate our tech-
nique along these four dimensions.
Hand-authored utterances. We collected a sam-
ple of 220 instances of frames that Doctor Perez’s
dialogue manager had requested of the generation
component in previous dialogues with users. Some
frames occurred more than once in this sample.
Each frame was associated with a single hand-
authored utterance. Some of these utterances arose
in human role plays for Doctor Perez; some were
written by a script writer; others were authored
by system builders to provide coverage for specific
frames. All were reviewed by a system builder for
appropriateness to the corresponding frame.
Training. We used these 220 (frame, utterance)
examples to evaluate both our approach and a shal-
low statistical method called sentence retriever (dis-
cussed below). We randomly split the examples
into 198 training and 22 test examples; we used the
same train/test split for our approach and sentence
retriever.
To train our approach, we constructed training ex-
amples in the format specified in Section 3.1. Syntax
posed an interesting problem, because the Charniak
parser frequently produces erroneous syntactic anal-
yses for utterances in Doctor Perez’s domain, but it
was not obvious how detrimental these errors would
be to overall generated output. We therefore con-
structed two alternative sets of training examples –
one where the syntax of each utterance was the un-
corrected output of the Charniak parser, and another
where the parser output was corrected by hand (the
syntax in Figure 3 above is the corrected version).
Hand correction of parser output requires consider-
able linguistic expertise, so uncorrected output rep-
resents a substantial reduction in authoring burden.
The connections between surface expressions and
frame key-value pairs were identical in both uncor-
rected and corrected training sets, since they are in-
dependent of the syntax. For each training set, we
trained our generator on the 198 training examples.
We then generated a single (highest-ranked) utter-
ance for each example in both the test and training
sets. The generator sometimes failed to find a suc-
cessful utterance within the 200ms timeout; the suc-
cess rate of our generator was 95% for training ex-
</bodyText>
<page confidence="0.995817">
202
</page>
<bodyText confidence="0.989811812500001">
amples and 80% for test examples. The successful
utterances were rated by our judges.
Sentence retriever is based on the cross-
language information retrieval techniques described
in (Leuski et al., 2006), and is currently in use for
Doctor Perez’s NLG problem. Sentence retriever
does not exploit any hierarchical syntactic analy-
sis of utterances. Instead, sentence retriever views
NLG as an information retrieval task in which a set
of training utterances are the “documents” to be re-
trieved, and the frame to be expressed is the query.
At run-time, the algorithm functions essentially as a
classifier: it uses a relative entropy metric to select
the highest ranking training utterance for the frame
that Doctor Perez wishes to express. This approach
has been used because it is to some extent robust
against changes in internal semantic representations,
and against minor deficiencies in the training corpus,
but as with a canned text approach, it requires each
utterance to be hand-authored before it can be used
in dialogue. We trained sentence retriever on the 198
training examples, and used it to generate a single
(highest-ranked) utterance for each example in both
the test and training sets. Sentence retriever’s suc-
cess rate was 96% for training examples and 90%
for test examples. The successful utterances were
rated by our judges.
Figure 7 in the Appendix illustrates the alternative
utterances that were produced for a frame present in
the test data but not in the training data.
Run-time speed. Both our approach and sentence
retriever run within the available 200ms window.
Adequacy of Coverage. To assess output quality,
we conducted a study in which 5 human judges gave
overall quality ratings for various utterances Doctor
Perez might use to express specific semantic frames.
In total, judges rated 494 different utterances which
were produced in several conditions: hand-authored
(for the relevant frame), generated by our approach,
and sentence retriever.
We asked our 5 judges to rate each of the 494 ut-
terances, in relation to the specific frame for which
it was produced, on a single 1 (“very bad”) to 5
(“very good”) scale. Since ratings need to incorpo-
rate accuracy with respect to the frame, our judges
had to be able to read the raw system semantic rep-
resentations. This meant we could only use judges
who were deeply familiar with the dialogue system;
however, the main developer of the new generation
algorithms (the first author) did not participate as
a judge. Judges were blind to the conditions un-
der which utterances were produced. The judges
rated the utterances using a custom-built application
which presented a single frame together with 1 to 6
candidate utterances for that frame. The rating inter-
face is shown in Figure 6 in the Appendix. The order
of candidate utterances for each frame was random-
ized, and the order in which frames appeared was
randomized for each judge.
The judges were instructed to incorporate both
fluency and accuracy with respect to the frame into
a single overall rating for each utterance. While it
is possible to have human judges rate fluency and
accuracy independently, ratings of fluency alone are
not particularly helpful in evaluating Doctor Perez’s
generation component, since for Doctor Perez, a cer-
tain degree of disfluency can contribute to believ-
ability (as noted in Section 2). We therefore asked
judges to make an overall assessment of output qual-
ity for the Doctor Perez character.
The judges achieved a reliability of α = 0.708
(Krippendorff, 1980); this value shows that agree-
ment is well above chance, and allows for tentative
conclusions. Agreement between subsets of judges
ranged from α = 0.802 for the most concordant pair
of judges to α = 0.593 for the most discordant pair.
We also performed an ANOVA comparing three
conditions (generated, retrieved and hand-authored
utterances) across the five judges; we found sig-
nificant main effects of condition (F(2,3107) =
55, p &lt; 0.001) and judge (F(4, 3107) = 17,p &lt;
0.001), but no significant interaction (F(8, 3107) =
0.55, p &gt; 0.8). We therefore conclude that the indi-
vidual differences among the judges do not affect the
comparison of utterances across the different condi-
tions, so we will report the rest of the evaluation on
the mean ratings per utterance.
Due to the large number of factors and the dif-
ferences in the number of utterances correspond-
ing to each condition, we ran a small number
of planned comparisons. The distribution of rat-
ings across utterances is not normal; to validate
our results we accompanied each t-test by a non-
parametric Wilcoxon rank sum test, and signifi-
cance always fell in the same general range. We
found a significant difference between generated
</bodyText>
<page confidence="0.997092">
203
</page>
<figure confidence="0.976813333333333">
40
Generated (N = 90)
Sentence retriever (N = 100)
30
20
10
0
1 2 3 4 5
Rating
</figure>
<figureCaption confidence="0.99943">
Figure 4: Observed ratings of generated (uncorrected
syntax) vs. retrieved sentences for test examples.
</figureCaption>
<bodyText confidence="0.999321666666666">
output for all examples, retrieved output for all ex-
amples, and hand-authored utterances (F(2, 622) =
16,p &lt; 0.001); however, subsequent t-tests show
that all of this difference is due to the fact that hand-
authored utterances (mean rating 4.4) are better than
retrieved (t(376) = 3.7,p &lt; 0.001) and gener-
ated (t(388) = 5.9,p &lt; 0.001) utterances, whereas
the difference between generated (mean rating 3.8)
and retrieved (mean rating 4.0) is non-significant
</bodyText>
<equation confidence="0.826534">
(t(385) = 1.6,p &gt; 0.1).
</equation>
<bodyText confidence="0.999493152542373">
Figure 4 shows the observed rating frequencies
of sentence retriever (mean 3.0) and our approach
(mean 3.6) on the test examples. While this data
does not show a significant difference, it suggests
that retriever’s selected sentences are most fre-
quently either very bad or very good; this reflects
the fact that the classification algorithm retrieves
highly fluent hand-authored text which is sometimes
semantically very incorrect. (Figure 7 in the Ap-
pendix provides such an example, in which a re-
trieved sentence has the wrong polarity.) The qual-
ity of our generated output, by comparison, appears
more graded, with very good quality the most fre-
quent outcome and lower qualities less frequent. In
a system where there is a low tolerance for very
bad quality output, generated output would likely be
considered preferable to retrieved output.
In terms of generation failures, our approach had
poorer coverage of test examples than sentence re-
triever (80% vs. 90%). Note however that in this
study, our approach only delivered an output if it
could completely cover the requested frame. In the
future, we believe coverage could be improved, with
perhaps some reduction in quality, by allowing out-
puts that only partially cover requested frames.
In terms of output variety, in this initial study our
judges rated only the highest ranked output gener-
ated or retrieved for each frame. However, we ob-
served that our generator frequently finds several al-
ternative utterances of relatively high quality (see
Figure 7); thus our approach offers another poten-
tial advantage in output variety.
Authoring burdens. Both canned text and sen-
tence retriever require only frames and correspond-
ing output sentences as input. In our approach, syn-
tax and semantic links are additionally needed. We
compared the use of corrected vs. uncorrected syn-
tax in training. Surprisingly, we found no significant
difference between generated output trained on cor-
rected and uncorrected syntax (t(29) = 0.056,p &gt;
0.9 on test items, t(498) = −1.1,p &gt; 0.2 on all
items). This is a substantial win in terms of reduced
authoring burden for our approach.
If uncorrected syntax is used, the additional bur-
den of our approach lies only in specifying the se-
mantic links. For the 220 examples in this study,
one system builder specified these links in about 6
hours. We present a detailed cost/benefit analysis of
this effort in (DeVault et al., 2008).
NLG request specification. Both our approach
and sentence retriever accept the dialogue manager’s
native semantic representation for NLG as input.
Summary. In exchange for a slightly increased
authoring burden, our approach yields a generation
component that generalizes to unseen test problems
relatively gracefully, and does not suffer from the
frequent very bad output or the necessity to author
every utterance that comes with canned text or a
competing statistical classification technique.
</bodyText>
<sectionHeader confidence="0.994037" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.988279777777778">
In this paper we have presented an approach to spec-
ifying domain-specific, grammar-based generation
by example. The method reduces the authoring bur-
den associated with developing a grammar-based
NLG component for an existing dialogue system.
We have argued that the method delivers relatively
high-quality, domain-specific output without requir-
ing that content authors possess detailed linguistic
knowledge. In future work, we will study the perfor-
</bodyText>
<equation confidence="0.47126">
Frequency (%)
</equation>
<page confidence="0.994844">
204
</page>
<bodyText confidence="0.999963428571429">
mance of our approach as the size of the training set
grows, and assess what specific weaknesses or prob-
lematic disfluencies, if any, our human rating study
identifies in output generated by our technique. Fi-
nally, we intend to evaluate the performance of our
generation approach within the context of the com-
plete, running Doctor Perez agent.
</bodyText>
<sectionHeader confidence="0.9974" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.997571857142857">
Thanks to Arno Hartholt, Susan Robinson, Thomas
Russ, Chung-chieh Shan, and Matthew Stone. This
work was sponsored by the U.S. Army Research,
Development, and Engineering Command (RDE-
COM), and the content does not necessarily reflect
the position or the policy of the Government, and no
official endorsement should be inferred.
</bodyText>
<sectionHeader confidence="0.996796" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.990482344444444">
Stephen Busemann and Helmut Horacek. 1998. A flex-
ible shallow approach to text generation. In Proceed-
ings of INLG, pages 238–247.
Charles B. Callaway. 2003. Evaluating coverage for
large symbolic NLG grammars. Proceedings of the
International Joint Conferences on Artificial Intelli-
gence.
Eugene Charniak. 2001. Immediate-head parsing for
language models. In ACL ’01: Proceedings of the
39th Annual Meeting on Association for Computa-
tional Linguistics, pages 124–131, Morristown, NJ,
USA. Association for Computational Linguistics.
Eugene Charniak. 2005.
ftp://ftp.cs.brown.edu/pub/nlparser/
parser05Aug16.tar.gz.
David Chiang. 2003. Statistical parsing with an auto-
matically extracted tree adjoining grammar. In Rens
Bod, Remko Scha, and Khalil Sima’an, editors, Data
Oriented Parsing, pages 299–316. CSLI Publications,
Stanford.
Michael Collins and Terry Koo. 2005. Discrimina-
tive reranking for natural language parsing. Compu-
tational Linguistics, 31(1):25–70.
Hal Daumé, III and Daniel Marcu. 2005. Learning as
search optimization: approximate large margin meth-
ods for structured prediction. In ICML ’05: Proceed-
ings of the 22nd international conference on Machine
learning, pages 169–176, New York, NY, USA. ACM.
David DeVault, David Traum, and Ron Artstein. 2008.
Practical grammar-based NLG from examples. In
Fifth International Natural Language Generation
Conference (INLG).
Michael Elhadad. 1991. FUF: the universal unifier user
manual version 5.0. Technical Report CUCS-038-91.
Klaus Krippendorff, 1980. Content Analysis: An Intro-
duction to Its Methodology, chapter 12, pages 129–
154. Sage, Beverly Hills, CA.
Irene Langkilde and Kevin Knight. 1998. Generation
that exploits corpus-based statistical knowledge. In
COLING-ACL, pages 704–710.
I. Langkilde-Geary. 2002. An empirical verification of
coverage and correctness for a general-purpose sen-
tence generator.
Anton Leuski, Ronakkumar Patel, David Traum, and
Brandon Kennedy. 2006. Building effective question
answering characters. In The 7th SIGdial Workshop
on Discourse and Dialogue.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated cor-
pus of english: The penn treebank. Computational
Linguistics, 19(2):313–330.
E. Reiter, S. Sripada, and R. Robertson. 2003. Acquir-
ing correct knowledge for natural language generation.
Journal of Artificial Intelligence Research, 18:491–
516.
William Swartout, Jonathan Gratch, Randall W. Hill, Ed-
uard Hovy, Stacy Marsella, Jeff Rickel, and David
Traum. 2006. Toward virtual humans. AI Mag.,
27(2):96–108.
David Traum, Michael Fleischman, and Eduard Hovy.
2003. Nl generation for virtual humans in a complex
social environment. In Working Notes AAAI Spring
Symposium on Natural Language Generation in Spo-
ken and Written Dialogue, March.
David Traum, William Swartout, Jonathan Gratch,
Stacy Marsella, Patrick Kenny, Eduard Hovy, Shri
Narayanan, Ed Fast, Bilyana Martinovski, Rahul
Baghat, Susan Robinson, Andrew Marshall, Dagen
Wang, Sudeep Gandhe, and Anton Leuski. 2005.
Dealing with doctors: A virtual human for non-team
interaction. In SIGdial.
D. R. Traum, W. Swartout, J Gratch, and S Marsella.
2008. A virtual human dialogue model for non-team
interaction. In Laila Dybkjaer and Wolfgang Minker,
editors, Recent Trends in Discourse and Dialogue.
Springer.
David Traum. 2003. Semantics and pragmatics of ques-
tions and answers for dialogue agents. In proceedings
of the International Workshop on Computational Se-
mantics, pages 380–394, January.
Michael White, Rajakrishnan Rajkumar, and Scott Mar-
tin. 2007. Towards broad coverage surface realiza-
tion with CCG. In Proc. of the Workshop on Using
Corpora for NLG: Language Generation and Machine
Translation (UCNLG+MT).
Huayan Zhong and Amanda Stent. 2005. Building
surface realizers automatically from corpora using
general-purpose tools. In Proc. Corpus Linguistics ’05
Workshop on Using Corpora for Natural Language
Generation.
</reference>
<page confidence="0.993958">
205
</page>
<figure confidence="0.985387523809524">
cat: SA
fin:other, cat: S
pos: VBP
have
fin:yes, cat: VP,
gra: obj1
syntax:
cat: NP, gra: obj1
fin:yes, cat: VP
cat: NP, apr: VBP,
apn: other
pos: RB
n’t
fin: yes, cat: VP,
gra: obj1
pos: PRP
we
apn:other, pos:VBP
do
operations: initial tree comp
semantics: speech-act.action = assert speech-act.content.attribute = resourceAttribute
speech-act.content.polarity = negative
cat: NP, apr: VBP,
gra: obj1, apn: other
cat:ADVP, gra:adj
pos: RB
here
cat: NP, apr: VBZ,
gra: adj, apn:3ps
pos: NN
captain
pos: JJ
medical
pos: NNS
supplies
syntax:
operations: comp left/right adjunction left/right adjunction
semantics: speech-act.content.value = speech-act.content.object-id = addressee = captain-kirk
medical-supplies market dialogue-act.addressee =
captain-kirk
speech-act.addressee =
captain-kirk
</figure>
<figureCaption confidence="0.999968">
Figure 5: The linguistic resources automatically inferred from the training example in Figure 3.
Figure 6: Human rating interface.
</figureCaption>
<page confidence="0.953463">
206
</page>
<figure confidence="0.824015285714286">
Input semantic form
addressee captain-kirk
dialogue-act.actor doctor-perez
dialogue-act.addressee captain-kirk
dialogue-act.type assign-turn
speech-act.action assert
speech-act.actor doctor-perez
speech-act.addressee captain-kirk
speech-act.content.attribute acceptableAttribute
speech-act.content.object-id clinic
speech-act.content.time present
speech-act.content.type state
speech-act.content.value yes
Outputs
Hand-authored
the clinic is acceptable captain
Generated (uncorrected syntax)
the clinic is up to standard captain
the clinic is acceptable captain
the clinic should be in acceptable condition captain
the clinic downtown is currently acceptable captain
the clinic should agree in an acceptable condition captain
Generated (corrected syntax)
it is necessary that the clinic be in good condition captain
i think that the clinic be in good condition captain
captain this wont work unless the clinic be in good condition
Sentence retriever
the clinic downtown is not in an acceptable condition captain
</figure>
<figureCaption confidence="0.870432">
Figure 7: The utterances generated for a single test example by different evaluation conditions. Generated outputs
whose rank (determined by derivation probability) was higher than 1 were not rated in the evaluation reported in this
paper, but are included here to suggest the potential of our approach to provide a variety of alternative outputs for the
same requested semantic form. Note how the output of sentence retriever has the opposite meaning to that of the input
frame.
</figureCaption>
<figure confidence="0.9995047">
Rank Time (ms)
1 16
2 94
3 78
4 16
5 78
Rank Time (ms)
1 47
2 31
3 62
</figure>
<page confidence="0.931487">
207
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.818181">
<title confidence="0.999996">Making Grammar-Based Generation Easier to Deploy in Dialogue Systems</title>
<author confidence="0.996438">David DeVault</author>
<author confidence="0.996438">David Traum</author>
<author confidence="0.996438">Ron</author>
<affiliation confidence="0.998265">USC Institute for Creative</affiliation>
<address confidence="0.947187">13274 Fiji</address>
<author confidence="0.938644">Marina del Rey</author>
<author confidence="0.938644">CA</author>
<email confidence="0.998978">devault@ict.usc.edu</email>
<email confidence="0.998978">traum@ict.usc.edu</email>
<email confidence="0.998978">artstein@ict.usc.edu</email>
<abstract confidence="0.995532944444444">We present a development pipeline and associated algorithms designed to make grammarbased generation easier to deploy in implemented dialogue systems. Our approach realizes a practical trade-off between the capabilities of a system’s generation component and the authoring and maintenance burdens imposed on the generation content author for a deployed system. To evaluate our approach, we performed a human rating study with system builders who work on a common largescale spoken dialogue system. Our results demonstrate the viability of our approach and illustrate authoring/performance trade-offs between hand-authored text, our grammar-based approach, and a competing shallow statistical NLG technique.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Stephen Busemann</author>
<author>Helmut Horacek</author>
</authors>
<title>A flexible shallow approach to text generation.</title>
<date>1998</date>
<booktitle>In Proceedings of INLG,</booktitle>
<pages>238--247</pages>
<contexts>
<context position="4267" citStr="Busemann and Horacek, 1998" startWordPosition="615" endWordPosition="619">mar (or language model) for free. However, it is harder to tailor output to the desired wording and style for a specific dialogue system, and these generators demand a specific input format that is otherwise foreign to an existing dialogue system. Unfortunately, in our experience, the development burden of implementing the translation between the system’s available meaning representations and the generator’s required input format is quite substantial. Indeed, implementing the translation might require as much effort as would be required to build a simple custom generator; cf. (Callaway, 2003; Busemann and Horacek, 1998). This development cost is exacerbated when a dialogue system’s native meaning representation scheme is under revision. In this paper, we survey a new example-based approach (DeVault et al., 2008) that we have developed in order to mitigate these difficulties, so that grammar-based generation can be deployed more widely in implemented dialogue systems. Our development pipeline requires a system developer to create a set of training examples which directly connect desired output texts to available application semantic forms. This is achieved through a streamlined authoring task that does not re</context>
</contexts>
<marker>Busemann, Horacek, 1998</marker>
<rawString>Stephen Busemann and Helmut Horacek. 1998. A flexible shallow approach to text generation. In Proceedings of INLG, pages 238–247.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles B Callaway</author>
</authors>
<title>Evaluating coverage for large symbolic NLG grammars.</title>
<date>2003</date>
<booktitle>Proceedings of the International Joint Conferences on Artificial Intelligence.</booktitle>
<contexts>
<context position="2396" citStr="Callaway, 2003" startWordPosition="349" endWordPosition="350">ward in terms of time and expertise required. This is especially important since dialogue systems are complex systems with significant development costs. Finally, it should be relatively easy for the dialogue manager to formulate a generation request in the format required by the generator. Together, these requirements can reduce the attractiveness of grammar-based generation when compared to simpler template-based or canned text output solutions. In terms of speed, off-theshelf, wide-coverage grammar-based realizers such as FUF/SURGE (Elhadad, 1991) can be too slow for real-time interaction (Callaway, 2003). In terms of adequacy of coverage, in principle, grammar-based generation offers significant advantages over template-based or canned text output by providing productive coverage and greater variety. However, realizing these advantages can require significant development costs. Specifying the necessary connections between lexico-syntactic resources and the flat, domain-specific semantic representations that are typically available in implemented systems is a subtle, labor-intensive, and knowledgeintensive process for which attractive methodologies do not yet exist (Reiter et al., 2003). One s</context>
<context position="4238" citStr="Callaway, 2003" startWordPosition="613" endWordPosition="614">) provide a grammar (or language model) for free. However, it is harder to tailor output to the desired wording and style for a specific dialogue system, and these generators demand a specific input format that is otherwise foreign to an existing dialogue system. Unfortunately, in our experience, the development burden of implementing the translation between the system’s available meaning representations and the generator’s required input format is quite substantial. Indeed, implementing the translation might require as much effort as would be required to build a simple custom generator; cf. (Callaway, 2003; Busemann and Horacek, 1998). This development cost is exacerbated when a dialogue system’s native meaning representation scheme is under revision. In this paper, we survey a new example-based approach (DeVault et al., 2008) that we have developed in order to mitigate these difficulties, so that grammar-based generation can be deployed more widely in implemented dialogue systems. Our development pipeline requires a system developer to create a set of training examples which directly connect desired output texts to available application semantic forms. This is achieved through a streamlined au</context>
</contexts>
<marker>Callaway, 2003</marker>
<rawString>Charles B. Callaway. 2003. Evaluating coverage for large symbolic NLG grammars. Proceedings of the International Joint Conferences on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Immediate-head parsing for language models.</title>
<date>2001</date>
<booktitle>In ACL ’01: Proceedings of the 39th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>124--131</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="12301" citStr="Charniak, 2001" startWordPosition="1854" endWordPosition="1855">act.content.attribute = resourceAttribute medical supplies speech-act.content.value = medical-supplies here speech-act.content.object-id = market captain { addressee = captain-kirk dialogue-act.addressee = captain-kirk speech-act.addressee = captain-kirk Figure 3: A generation training example for Doctor Perez. suggests the output utterance u = we don’t have medical supplies here captain. Each utterance u is accompanied by syntax(u), a syntactic analysis in Penn Treebank format (Marcus et al., 1994). In this example, the syntax is a hand-corrected version of the output of the Charniak parser (Charniak, 2001; Charniak, 2005) on this sentence; we discuss this hand correction in Section 4. To represent the meaning of utterances, our approach assumes that the system provides some set M = {m1, ..., mj} of semantic representations. The meaning of any individual utterance is then identified with some subset of M. For Doctor Perez, M comprises the 232 distinct key-value pairs that appear in the system’s various generation frames. In this example, the utterance’s meaning is captured by the 8 key-value pairs indicated in the figure. Our approach requires the generation content author to link these 8 key-v</context>
</contexts>
<marker>Charniak, 2001</marker>
<rawString>Eugene Charniak. 2001. Immediate-head parsing for language models. In ACL ’01: Proceedings of the 39th Annual Meeting on Association for Computational Linguistics, pages 124–131, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<date>2005</date>
<pages>05--16</pages>
<contexts>
<context position="12318" citStr="Charniak, 2005" startWordPosition="1856" endWordPosition="1857">ibute = resourceAttribute medical supplies speech-act.content.value = medical-supplies here speech-act.content.object-id = market captain { addressee = captain-kirk dialogue-act.addressee = captain-kirk speech-act.addressee = captain-kirk Figure 3: A generation training example for Doctor Perez. suggests the output utterance u = we don’t have medical supplies here captain. Each utterance u is accompanied by syntax(u), a syntactic analysis in Penn Treebank format (Marcus et al., 1994). In this example, the syntax is a hand-corrected version of the output of the Charniak parser (Charniak, 2001; Charniak, 2005) on this sentence; we discuss this hand correction in Section 4. To represent the meaning of utterances, our approach assumes that the system provides some set M = {m1, ..., mj} of semantic representations. The meaning of any individual utterance is then identified with some subset of M. For Doctor Perez, M comprises the 232 distinct key-value pairs that appear in the system’s various generation frames. In this example, the utterance’s meaning is captured by the 8 key-value pairs indicated in the figure. Our approach requires the generation content author to link these 8 key-value pairs to con</context>
</contexts>
<marker>Charniak, 2005</marker>
<rawString>Eugene Charniak. 2005. ftp://ftp.cs.brown.edu/pub/nlparser/ parser05Aug16.tar.gz.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Statistical parsing with an automatically extracted tree adjoining grammar.</title>
<date>2003</date>
<booktitle>In Rens Bod, Remko Scha, and Khalil Sima’an, editors, Data Oriented Parsing,</booktitle>
<pages>299--316</pages>
<publisher>CSLI Publications, Stanford.</publisher>
<contexts>
<context position="14939" citStr="Chiang, 2003" startWordPosition="2274" endWordPosition="2276">e. Many systems, including Doctor Perez, lack some of the internal representations that would be necessary to specify semantics down to the lexical level. An important feature of our approach is that it allows an arbitrary semantic granularity to be employed, by mapping the representations available in the system to appropriate multi-word chunks. 201 3.2 Automatic Grammar Induction and Search Optimization The first processing step is to induce a productive grammar from the training examples. We adopt the probabilistic tree-adjoining grammar (PTAG) formalism and grammar induction technique of (Chiang, 2003). We induce our grammar from training examples such as Figure 3 using heuristic rules to assign derivations to the examples, as in (Chiang, 2003). Once derivations have been assigned, subtrees within the training example syntax are incrementally detached. This process yields the reusable linguistic resources in the grammar, as well as the statistical model needed to compute operation probabilities when the grammar is later used in generation. Figure 5 in the Appendix illustrates this process by presenting the linguistic resources inferred from the training example of Figure 3. Our approach use</context>
</contexts>
<marker>Chiang, 2003</marker>
<rawString>David Chiang. 2003. Statistical parsing with an automatically extracted tree adjoining grammar. In Rens Bod, Remko Scha, and Khalil Sima’an, editors, Data Oriented Parsing, pages 299–316. CSLI Publications, Stanford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Terry Koo</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="16672" citStr="Collins and Koo, 2005" startWordPosition="2553" endWordPosition="2556">st applications. The second step of automated processing, then, uses the training examples to learn an effective search policy so that good output sentences can be found in a reasonable time frame. The solution we have developed employs a beam search strategy that uses weighted features to rank alternative grammatical expansions at each step. Our algorithm for selecting features and weights is based on the search optimization algorithm of (Daumé and Marcu, 2005), which decides to update feature weights when mistakes are made during search on training examples. We use the boosting approach of (Collins and Koo, 2005) to perform feature selection and identify good weight values. 4 Empirical Evaluation In the introduction, we identified run-time speed, adequacy of coverage, authoring burdens, and NLG request specification as important factors in the selection of a technology for a dialogue system’s NLG component. In this section, we evaluate our technique along these four dimensions. Hand-authored utterances. We collected a sample of 220 instances of frames that Doctor Perez’s dialogue manager had requested of the generation component in previous dialogues with users. Some frames occurred more than once in </context>
</contexts>
<marker>Collins, Koo, 2005</marker>
<rawString>Michael Collins and Terry Koo. 2005. Discriminative reranking for natural language parsing. Computational Linguistics, 31(1):25–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daumé</author>
<author>Daniel Marcu</author>
</authors>
<title>Learning as search optimization: approximate large margin methods for structured prediction.</title>
<date>2005</date>
<booktitle>In ICML ’05: Proceedings of the 22nd international conference on Machine learning,</booktitle>
<pages>169--176</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="16516" citStr="Daumé and Marcu, 2005" startWordPosition="2528" endWordPosition="2531">e outputs ranked by their derivation probabilities. The search space created by a grammar induced in this way is too large to be searched exhaustively in most applications. The second step of automated processing, then, uses the training examples to learn an effective search policy so that good output sentences can be found in a reasonable time frame. The solution we have developed employs a beam search strategy that uses weighted features to rank alternative grammatical expansions at each step. Our algorithm for selecting features and weights is based on the search optimization algorithm of (Daumé and Marcu, 2005), which decides to update feature weights when mistakes are made during search on training examples. We use the boosting approach of (Collins and Koo, 2005) to perform feature selection and identify good weight values. 4 Empirical Evaluation In the introduction, we identified run-time speed, adequacy of coverage, authoring burdens, and NLG request specification as important factors in the selection of a technology for a dialogue system’s NLG component. In this section, we evaluate our technique along these four dimensions. Hand-authored utterances. We collected a sample of 220 instances of fra</context>
</contexts>
<marker>Daumé, Marcu, 2005</marker>
<rawString>Hal Daumé, III and Daniel Marcu. 2005. Learning as search optimization: approximate large margin methods for structured prediction. In ICML ’05: Proceedings of the 22nd international conference on Machine learning, pages 169–176, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David DeVault</author>
<author>David Traum</author>
<author>Ron Artstein</author>
</authors>
<title>Practical grammar-based NLG from examples.</title>
<date>2008</date>
<booktitle>In Fifth International Natural Language Generation Conference (INLG).</booktitle>
<contexts>
<context position="4463" citStr="DeVault et al., 2008" startWordPosition="647" endWordPosition="650">rwise foreign to an existing dialogue system. Unfortunately, in our experience, the development burden of implementing the translation between the system’s available meaning representations and the generator’s required input format is quite substantial. Indeed, implementing the translation might require as much effort as would be required to build a simple custom generator; cf. (Callaway, 2003; Busemann and Horacek, 1998). This development cost is exacerbated when a dialogue system’s native meaning representation scheme is under revision. In this paper, we survey a new example-based approach (DeVault et al., 2008) that we have developed in order to mitigate these difficulties, so that grammar-based generation can be deployed more widely in implemented dialogue systems. Our development pipeline requires a system developer to create a set of training examples which directly connect desired output texts to available application semantic forms. This is achieved through a streamlined authoring task that does not require detailed linguistic knowledge. Our approach then processes these training examples to automatically construct all the resources needed for a fast, highquality, run-time grammar-based generat</context>
<context position="10897" citStr="DeVault et al., 2008" startWordPosition="1642" endWordPosition="1645">eloped techniques in statistical parsing, lexicalized syntax modeling, generation with lexicalized grammars, and search optimization to automatically construct all the resources needed for a high-quality run-time generation component. The approach involves three primary steps: specification of training examples, grammar induction, and search optimization. In this section, we present the format that training examples take and then summarize the subsequent automatic processing steps. Due to space limitations, we omit the full details of these automatic processing steps, and refer the reader to (DeVault et al., 2008) for additional details. 3.1 Specification of Training Examples Each training example in our approach specifies a target output utterance (string), its syntax, and a set of links between substrings within the utterance and system semantic representations. Formally, a training example takes the form (u, syntax(u), semantics(u)). We will illustrate this format using the training example in Figure 3. In this example, the generation content author 200 Utterance we don’t have medical supplies here captain cat: SA cat: NP pos: PRP we pos: JJ medical pos: NNS supplies cat: NP pos: NN captain pos: AUX</context>
<context position="27014" citStr="DeVault et al., 2008" startWordPosition="4240" endWordPosition="4243">e use of corrected vs. uncorrected syntax in training. Surprisingly, we found no significant difference between generated output trained on corrected and uncorrected syntax (t(29) = 0.056,p &gt; 0.9 on test items, t(498) = −1.1,p &gt; 0.2 on all items). This is a substantial win in terms of reduced authoring burden for our approach. If uncorrected syntax is used, the additional burden of our approach lies only in specifying the semantic links. For the 220 examples in this study, one system builder specified these links in about 6 hours. We present a detailed cost/benefit analysis of this effort in (DeVault et al., 2008). NLG request specification. Both our approach and sentence retriever accept the dialogue manager’s native semantic representation for NLG as input. Summary. In exchange for a slightly increased authoring burden, our approach yields a generation component that generalizes to unseen test problems relatively gracefully, and does not suffer from the frequent very bad output or the necessity to author every utterance that comes with canned text or a competing statistical classification technique. 5 Conclusion and Future Work In this paper we have presented an approach to specifying domain-specific</context>
</contexts>
<marker>DeVault, Traum, Artstein, 2008</marker>
<rawString>David DeVault, David Traum, and Ron Artstein. 2008. Practical grammar-based NLG from examples. In Fifth International Natural Language Generation Conference (INLG).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Elhadad</author>
</authors>
<title>FUF: the universal unifier user manual version 5.0.</title>
<date>1991</date>
<tech>Technical Report CUCS-038-91.</tech>
<contexts>
<context position="2337" citStr="Elhadad, 1991" startWordPosition="340" endWordPosition="341"> the generation component should be relatively straightforward in terms of time and expertise required. This is especially important since dialogue systems are complex systems with significant development costs. Finally, it should be relatively easy for the dialogue manager to formulate a generation request in the format required by the generator. Together, these requirements can reduce the attractiveness of grammar-based generation when compared to simpler template-based or canned text output solutions. In terms of speed, off-theshelf, wide-coverage grammar-based realizers such as FUF/SURGE (Elhadad, 1991) can be too slow for real-time interaction (Callaway, 2003). In terms of adequacy of coverage, in principle, grammar-based generation offers significant advantages over template-based or canned text output by providing productive coverage and greater variety. However, realizing these advantages can require significant development costs. Specifying the necessary connections between lexico-syntactic resources and the flat, domain-specific semantic representations that are typically available in implemented systems is a subtle, labor-intensive, and knowledgeintensive process for which attractive </context>
<context position="3625" citStr="Elhadad, 1991" startWordPosition="517" endWordPosition="518">and-build an applicationspecific grammar. However, in our experience, this process requires a painstaking, time-consuming effort by a developer who has detailed linguistic knowledge as well as detailed domain knowledge, and the resulting coverage is inevitably limited. Wide-coverage generators that aim for applicabil198 Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 198–207, Columbus, June 2008. c�2008 Association for Computational Linguistics ity across application domains (White et al., 2007; Zhong and Stent, 2005; Langkilde-Geary, 2002; Langkilde and Knight, 1998; Elhadad, 1991) provide a grammar (or language model) for free. However, it is harder to tailor output to the desired wording and style for a specific dialogue system, and these generators demand a specific input format that is otherwise foreign to an existing dialogue system. Unfortunately, in our experience, the development burden of implementing the translation between the system’s available meaning representations and the generator’s required input format is quite substantial. Indeed, implementing the translation might require as much effort as would be required to build a simple custom generator; cf. (C</context>
</contexts>
<marker>Elhadad, 1991</marker>
<rawString>Michael Elhadad. 1991. FUF: the universal unifier user manual version 5.0. Technical Report CUCS-038-91.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Krippendorff</author>
</authors>
<title>Content Analysis: An Introduction to Its Methodology, chapter 12,</title>
<date>1980</date>
<pages>129--154</pages>
<location>Sage, Beverly Hills, CA.</location>
<contexts>
<context position="22744" citStr="Krippendorff, 1980" startWordPosition="3534" endWordPosition="3535">e. The judges were instructed to incorporate both fluency and accuracy with respect to the frame into a single overall rating for each utterance. While it is possible to have human judges rate fluency and accuracy independently, ratings of fluency alone are not particularly helpful in evaluating Doctor Perez’s generation component, since for Doctor Perez, a certain degree of disfluency can contribute to believability (as noted in Section 2). We therefore asked judges to make an overall assessment of output quality for the Doctor Perez character. The judges achieved a reliability of α = 0.708 (Krippendorff, 1980); this value shows that agreement is well above chance, and allows for tentative conclusions. Agreement between subsets of judges ranged from α = 0.802 for the most concordant pair of judges to α = 0.593 for the most discordant pair. We also performed an ANOVA comparing three conditions (generated, retrieved and hand-authored utterances) across the five judges; we found significant main effects of condition (F(2,3107) = 55, p &lt; 0.001) and judge (F(4, 3107) = 17,p &lt; 0.001), but no significant interaction (F(8, 3107) = 0.55, p &gt; 0.8). We therefore conclude that the individual differences among t</context>
</contexts>
<marker>Krippendorff, 1980</marker>
<rawString>Klaus Krippendorff, 1980. Content Analysis: An Introduction to Its Methodology, chapter 12, pages 129– 154. Sage, Beverly Hills, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irene Langkilde</author>
<author>Kevin Knight</author>
</authors>
<title>Generation that exploits corpus-based statistical knowledge.</title>
<date>1998</date>
<booktitle>In COLING-ACL,</booktitle>
<pages>704--710</pages>
<contexts>
<context position="3609" citStr="Langkilde and Knight, 1998" startWordPosition="513" endWordPosition="516"> 2003). One strategy is to hand-build an applicationspecific grammar. However, in our experience, this process requires a painstaking, time-consuming effort by a developer who has detailed linguistic knowledge as well as detailed domain knowledge, and the resulting coverage is inevitably limited. Wide-coverage generators that aim for applicabil198 Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 198–207, Columbus, June 2008. c�2008 Association for Computational Linguistics ity across application domains (White et al., 2007; Zhong and Stent, 2005; Langkilde-Geary, 2002; Langkilde and Knight, 1998; Elhadad, 1991) provide a grammar (or language model) for free. However, it is harder to tailor output to the desired wording and style for a specific dialogue system, and these generators demand a specific input format that is otherwise foreign to an existing dialogue system. Unfortunately, in our experience, the development burden of implementing the translation between the system’s available meaning representations and the generator’s required input format is quite substantial. Indeed, implementing the translation might require as much effort as would be required to build a simple custom g</context>
</contexts>
<marker>Langkilde, Knight, 1998</marker>
<rawString>Irene Langkilde and Kevin Knight. 1998. Generation that exploits corpus-based statistical knowledge. In COLING-ACL, pages 704–710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Langkilde-Geary</author>
</authors>
<title>An empirical verification of coverage and correctness for a general-purpose sentence generator.</title>
<date>2002</date>
<contexts>
<context position="3581" citStr="Langkilde-Geary, 2002" startWordPosition="511" endWordPosition="512">t exist (Reiter et al., 2003). One strategy is to hand-build an applicationspecific grammar. However, in our experience, this process requires a painstaking, time-consuming effort by a developer who has detailed linguistic knowledge as well as detailed domain knowledge, and the resulting coverage is inevitably limited. Wide-coverage generators that aim for applicabil198 Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 198–207, Columbus, June 2008. c�2008 Association for Computational Linguistics ity across application domains (White et al., 2007; Zhong and Stent, 2005; Langkilde-Geary, 2002; Langkilde and Knight, 1998; Elhadad, 1991) provide a grammar (or language model) for free. However, it is harder to tailor output to the desired wording and style for a specific dialogue system, and these generators demand a specific input format that is otherwise foreign to an existing dialogue system. Unfortunately, in our experience, the development burden of implementing the translation between the system’s available meaning representations and the generator’s required input format is quite substantial. Indeed, implementing the translation might require as much effort as would be require</context>
</contexts>
<marker>Langkilde-Geary, 2002</marker>
<rawString>I. Langkilde-Geary. 2002. An empirical verification of coverage and correctness for a general-purpose sentence generator.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Anton Leuski</author>
</authors>
<location>Ronakkumar Patel, David Traum, and</location>
<marker>Leuski, </marker>
<rawString>Anton Leuski, Ronakkumar Patel, David Traum, and</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brandon Kennedy</author>
</authors>
<title>Building effective question answering characters.</title>
<date>2006</date>
<booktitle>In The 7th SIGdial Workshop on Discourse and Dialogue.</booktitle>
<marker>Kennedy, 2006</marker>
<rawString>Brandon Kennedy. 2006. Building effective question answering characters. In The 7th SIGdial Workshop on Discourse and Dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="12191" citStr="Marcus et al., 1994" startWordPosition="1834" endWordPosition="1837">s: RB n’t Syntax Semantics we do n’t speech-act.action = assert �speech-act.content.polarity = negative have speech-act.content.attribute = resourceAttribute medical supplies speech-act.content.value = medical-supplies here speech-act.content.object-id = market captain { addressee = captain-kirk dialogue-act.addressee = captain-kirk speech-act.addressee = captain-kirk Figure 3: A generation training example for Doctor Perez. suggests the output utterance u = we don’t have medical supplies here captain. Each utterance u is accompanied by syntax(u), a syntactic analysis in Penn Treebank format (Marcus et al., 1994). In this example, the syntax is a hand-corrected version of the output of the Charniak parser (Charniak, 2001; Charniak, 2005) on this sentence; we discuss this hand correction in Section 4. To represent the meaning of utterances, our approach assumes that the system provides some set M = {m1, ..., mj} of semantic representations. The meaning of any individual utterance is then identified with some subset of M. For Doctor Perez, M comprises the 232 distinct key-value pairs that appear in the system’s various generation frames. In this example, the utterance’s meaning is captured by the 8 key-</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1994</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1994. Building a large annotated corpus of english: The penn treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Reiter</author>
<author>S Sripada</author>
<author>R Robertson</author>
</authors>
<title>Acquiring correct knowledge for natural language generation.</title>
<date>2003</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>18</volume>
<pages>516</pages>
<contexts>
<context position="2989" citStr="Reiter et al., 2003" startWordPosition="427" endWordPosition="430">nteraction (Callaway, 2003). In terms of adequacy of coverage, in principle, grammar-based generation offers significant advantages over template-based or canned text output by providing productive coverage and greater variety. However, realizing these advantages can require significant development costs. Specifying the necessary connections between lexico-syntactic resources and the flat, domain-specific semantic representations that are typically available in implemented systems is a subtle, labor-intensive, and knowledgeintensive process for which attractive methodologies do not yet exist (Reiter et al., 2003). One strategy is to hand-build an applicationspecific grammar. However, in our experience, this process requires a painstaking, time-consuming effort by a developer who has detailed linguistic knowledge as well as detailed domain knowledge, and the resulting coverage is inevitably limited. Wide-coverage generators that aim for applicabil198 Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 198–207, Columbus, June 2008. c�2008 Association for Computational Linguistics ity across application domains (White et al., 2007; Zhong and Stent, 2005; Langkilde-Geary, 2002; Langki</context>
</contexts>
<marker>Reiter, Sripada, Robertson, 2003</marker>
<rawString>E. Reiter, S. Sripada, and R. Robertson. 2003. Acquiring correct knowledge for natural language generation. Journal of Artificial Intelligence Research, 18:491– 516.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Swartout</author>
<author>Jonathan Gratch</author>
<author>Randall W Hill</author>
<author>Eduard Hovy</author>
<author>Stacy Marsella</author>
<author>Jeff Rickel</author>
<author>David Traum</author>
</authors>
<title>Toward virtual humans.</title>
<date>2006</date>
<journal>AI Mag.,</journal>
<volume>27</volume>
<issue>2</issue>
<contexts>
<context position="5585" citStr="Swartout et al., 2006" startWordPosition="811" endWordPosition="814">matically construct all the resources needed for a fast, highquality, run-time grammar-based generation component. We evaluate this approach using a pre-existing spoken dialogue system. Our results demonstrate the viability of the approach and illustrate authoring/performance trade-offs between hand-authored text, our grammar-based approach, and a competing shallow statistical NLG technique. 2 Background and Motivation The generation approach set out in this paper has been developed in the context of a research program aimed at creating interactive virtual humans for social training purposes (Swartout et al., 2006). Virtual humans are embodied conversational agents that play the role of people in simulations or games. They interact with human users and other virtual huFigure 1: Doctor Perez. mans using spoken language and non-verbal behavior such as eye gaze, gesture, and facial displays. The case study we present here is the generation of output utterances for a particular virtual human, Doctor Perez (see Figure 1), who is designed to teach negotiation skills in a multi-modal, multiparty, non-team dialogue setting (Traum et al., 2005; Traum et al., 2008). The human trainee who talks to the doctor plays</context>
</contexts>
<marker>Swartout, Gratch, Hill, Hovy, Marsella, Rickel, Traum, 2006</marker>
<rawString>William Swartout, Jonathan Gratch, Randall W. Hill, Eduard Hovy, Stacy Marsella, Jeff Rickel, and David Traum. 2006. Toward virtual humans. AI Mag., 27(2):96–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Traum</author>
<author>Michael Fleischman</author>
<author>Eduard Hovy</author>
</authors>
<title>Nl generation for virtual humans in a complex social environment.</title>
<date>2003</date>
<booktitle>In Working Notes AAAI Spring Symposium on Natural Language Generation in Spoken and Written Dialogue,</booktitle>
<contexts>
<context position="10074" citStr="Traum et al., 2003" startWordPosition="1524" endWordPosition="1527">act frames that are sent to the generation component change frequently as new reasoning capabilities are added and existing capabilities are reorganized. Additionally, while only hundreds of frames currently arise in actual dialogues, the number of potential frames is orders of magnitude larger, and it is difficult to predict in advance which frames might occur. In this setting, over a period of years, a number of different approaches to natural language generation have been implemented and tested, including hand-authored canned text, domain specific handbuilt grammar-based generators (e.g., (Traum et al., 2003)), shallow statistical generation techniques, and the grammar-based approach presented in this paper. We now turn to the details of our approach. 3 Technical Approach Our approach builds on recently developed techniques in statistical parsing, lexicalized syntax modeling, generation with lexicalized grammars, and search optimization to automatically construct all the resources needed for a high-quality run-time generation component. The approach involves three primary steps: specification of training examples, grammar induction, and search optimization. In this section, we present the format t</context>
</contexts>
<marker>Traum, Fleischman, Hovy, 2003</marker>
<rawString>David Traum, Michael Fleischman, and Eduard Hovy. 2003. Nl generation for virtual humans in a complex social environment. In Working Notes AAAI Spring Symposium on Natural Language Generation in Spoken and Written Dialogue, March.</rawString>
</citation>
<citation valid="false">
<authors>
<author>David Traum</author>
<author>William Swartout</author>
<author>Jonathan Gratch</author>
<author>Stacy Marsella</author>
</authors>
<title>Dealing with doctors: A virtual human for non-team interaction.</title>
<date>2005</date>
<booktitle>In Laila Dybkjaer and Wolfgang Minker, editors, Recent Trends in Discourse and Dialogue.</booktitle>
<editor>Patrick Kenny, Eduard Hovy, Shri Narayanan, Ed Fast, Bilyana Martinovski, Rahul Baghat, Susan Robinson, Andrew Marshall, Dagen Wang, Sudeep Gandhe, and Anton</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="6115" citStr="Traum et al., 2005" startWordPosition="899" endWordPosition="902">creating interactive virtual humans for social training purposes (Swartout et al., 2006). Virtual humans are embodied conversational agents that play the role of people in simulations or games. They interact with human users and other virtual huFigure 1: Doctor Perez. mans using spoken language and non-verbal behavior such as eye gaze, gesture, and facial displays. The case study we present here is the generation of output utterances for a particular virtual human, Doctor Perez (see Figure 1), who is designed to teach negotiation skills in a multi-modal, multiparty, non-team dialogue setting (Traum et al., 2005; Traum et al., 2008). The human trainee who talks to the doctor plays the role of a U.S. Army captain named Captain Kirk. We summarize Doctor Perez’s generation requirements as follows. In order to support compelling real-time conversation and effective training, the generator must be able to identify an utterance for Doctor Perez to use within approximately 200ms on modern hardware. Doctor Perez has a relatively rich internal mental state including beliefs, goals, plans, and emotions. As Doctor Perez attempts to achieve his conversational goals, his utterances need to take a variety of synta</context>
</contexts>
<marker>Traum, Swartout, Gratch, Marsella, 2005</marker>
<rawString>David Traum, William Swartout, Jonathan Gratch, Stacy Marsella, Patrick Kenny, Eduard Hovy, Shri Narayanan, Ed Fast, Bilyana Martinovski, Rahul Baghat, Susan Robinson, Andrew Marshall, Dagen Wang, Sudeep Gandhe, and Anton Leuski. 2005. Dealing with doctors: A virtual human for non-team interaction. In SIGdial. D. R. Traum, W. Swartout, J Gratch, and S Marsella. 2008. A virtual human dialogue model for non-team interaction. In Laila Dybkjaer and Wolfgang Minker, editors, Recent Trends in Discourse and Dialogue. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Traum</author>
</authors>
<title>Semantics and pragmatics of questions and answers for dialogue agents.</title>
<date>2003</date>
<booktitle>In proceedings of the International Workshop on Computational Semantics,</booktitle>
<pages>380--394</pages>
<contexts>
<context position="8984" citStr="Traum, 2003" startWordPosition="1353" endWordPosition="1354">e Figure 2: An example of Doctor Perez’s representations for utterance semantics: Doctor Perez tells the captain that there are no medical supplies at the market. exploits an existing semantic representation scheme that has been utilized in a family of virtual humans. This scheme uses an attribute-value matrix (AVM) representation to describe an utterance as a set of core speech acts and other dialogue acts. Speech acts generally have semantic contents that describe propositions and questions about states and actions in the domain, as well as other features such as polarity and modality. See (Traum, 2003) for some more details and examples of this representation. For ease of interprocess communication, and certain kinds of statistical processing, this AVM structure is linearized so that each non-recursive terminal value is paired with a path from the root to the final attribute. Thus, the AVM in Figure 2(a) is represented as the “frame” in Figure 2(b). Because the internal representations that make up Doctor Perez’s mental state are under constant development, the exact frames that are sent to the generation component change frequently as new reasoning capabilities are added and existing capab</context>
</contexts>
<marker>Traum, 2003</marker>
<rawString>David Traum. 2003. Semantics and pragmatics of questions and answers for dialogue agents. In proceedings of the International Workshop on Computational Semantics, pages 380–394, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael White</author>
<author>Rajakrishnan Rajkumar</author>
<author>Scott Martin</author>
</authors>
<title>Towards broad coverage surface realization with CCG.</title>
<date>2007</date>
<booktitle>In Proc. of the Workshop on Using Corpora for NLG: Language Generation and Machine Translation (UCNLG+MT).</booktitle>
<contexts>
<context position="3535" citStr="White et al., 2007" startWordPosition="503" endWordPosition="506">or which attractive methodologies do not yet exist (Reiter et al., 2003). One strategy is to hand-build an applicationspecific grammar. However, in our experience, this process requires a painstaking, time-consuming effort by a developer who has detailed linguistic knowledge as well as detailed domain knowledge, and the resulting coverage is inevitably limited. Wide-coverage generators that aim for applicabil198 Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 198–207, Columbus, June 2008. c�2008 Association for Computational Linguistics ity across application domains (White et al., 2007; Zhong and Stent, 2005; Langkilde-Geary, 2002; Langkilde and Knight, 1998; Elhadad, 1991) provide a grammar (or language model) for free. However, it is harder to tailor output to the desired wording and style for a specific dialogue system, and these generators demand a specific input format that is otherwise foreign to an existing dialogue system. Unfortunately, in our experience, the development burden of implementing the translation between the system’s available meaning representations and the generator’s required input format is quite substantial. Indeed, implementing the translation mi</context>
</contexts>
<marker>White, Rajkumar, Martin, 2007</marker>
<rawString>Michael White, Rajakrishnan Rajkumar, and Scott Martin. 2007. Towards broad coverage surface realization with CCG. In Proc. of the Workshop on Using Corpora for NLG: Language Generation and Machine Translation (UCNLG+MT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huayan Zhong</author>
<author>Amanda Stent</author>
</authors>
<title>Building surface realizers automatically from corpora using general-purpose tools.</title>
<date>2005</date>
<booktitle>In Proc. Corpus Linguistics ’05 Workshop on Using Corpora for Natural Language Generation.</booktitle>
<contexts>
<context position="3558" citStr="Zhong and Stent, 2005" startWordPosition="507" endWordPosition="510">methodologies do not yet exist (Reiter et al., 2003). One strategy is to hand-build an applicationspecific grammar. However, in our experience, this process requires a painstaking, time-consuming effort by a developer who has detailed linguistic knowledge as well as detailed domain knowledge, and the resulting coverage is inevitably limited. Wide-coverage generators that aim for applicabil198 Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 198–207, Columbus, June 2008. c�2008 Association for Computational Linguistics ity across application domains (White et al., 2007; Zhong and Stent, 2005; Langkilde-Geary, 2002; Langkilde and Knight, 1998; Elhadad, 1991) provide a grammar (or language model) for free. However, it is harder to tailor output to the desired wording and style for a specific dialogue system, and these generators demand a specific input format that is otherwise foreign to an existing dialogue system. Unfortunately, in our experience, the development burden of implementing the translation between the system’s available meaning representations and the generator’s required input format is quite substantial. Indeed, implementing the translation might require as much eff</context>
</contexts>
<marker>Zhong, Stent, 2005</marker>
<rawString>Huayan Zhong and Amanda Stent. 2005. Building surface realizers automatically from corpora using general-purpose tools. In Proc. Corpus Linguistics ’05 Workshop on Using Corpora for Natural Language Generation.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>