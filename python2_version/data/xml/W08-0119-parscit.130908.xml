<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007760">
<title confidence="0.967173">
Training and Evaluation of the HIS POMDP Dialogue System in Noise
</title>
<author confidence="0.911167">
M. Gaˇsi´c, S. Keizer, F. Mairesse, J. Schatzmann, B. Thomson, K. Yu, S. Young
</author>
<affiliation confidence="0.7747145">
Machine Intelligence Laboratory
Engineering Department
Cambridge University
United Kingdom
</affiliation>
<sectionHeader confidence="0.990477" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998586421052632">
This paper investigates the claim that a di-
alogue manager modelled as a Partially Ob-
servable Markov Decision Process (POMDP)
can achieve improved robustness to noise
compared to conventional state-based dia-
logue managers. Using the Hidden Infor-
mation State (HIS) POMDP dialogue man-
ager as an exemplar, and an MDP-based dia-
logue manager as a baseline, evaluation results
are presented for both simulated and real dia-
logues in a Tourist Information Domain. The
results on the simulated data show that the
inherent ability to model uncertainty, allows
the POMDP model to exploit alternative hy-
potheses from the speech understanding sys-
tem. The results obtained from a user trial
show that the HIS system with a trained policy
performed significantly better than the MDP
baseline.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999924130434783">
Conventional spoken dialogue systems operate by
finding the most likely interpretation of each user
input, updating some internal representation of the
dialogue state and then outputting an appropriate re-
sponse. Error tolerance depends on using confidence
thresholds and where they fail, the dialogue manager
must resort to quite complex recovery procedures.
Such a system has no explicit mechanisms for rep-
resenting the inevitable uncertainties associated with
speech understanding or the ambiguities which natu-
rally arise in interpreting a user’s intentions. The re-
sult is a system that is inherently fragile, especially
in noisy conditions or where the user is unsure of
how to use the system.
It has been suggested that Partially Observable
Markov Decision Processes (POMDPs) offer a nat-
ural framework for building spoken dialogue sys-
tems which can both model these uncertainties
and support policies which are robust to their ef-
fects (Young, 2002; Williams and Young, 2007a).
The key idea of the POMDP is that the underlying
dialogue state is hidden and dialogue management
policies must therefore be based not on a single state
estimate but on a distribution over all states.
Whilst POMDPs are attractive theoretically, in
practice, they are notoriously intractable for any-
thing other than small state/action spaces. Hence,
practical examples of their use were initially re-
stricted to very simple domains (Roy et al., 2000;
Zhang et al., 2001). More recently, however, a num-
ber of techniques have been suggested which do al-
low POMDPs to be scaled to handle real world tasks.
The two generic mechanisms which facilitate this
scaling are factoring the state space and perform-
ing policy optimisation in a reduced summary state
space (Williams and Young, 2007a; Williams and
Young, 2007b).
Based on these ideas, a number of real-world
POMDP-based systems have recently emerged. The
most complex entity which must be represented in
the state space is the user’s goal. In the Bayesian
Update ofDialogue State (BUDS) system, the user’s
goal is further factored into conditionally indepen-
dent slots. The resulting system is then modelled
as a dynamic Bayesian network (Thomson et al.,
2008). A similar approach is also developed in
</bodyText>
<page confidence="0.970905">
112
</page>
<note confidence="0.904892">
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 112–119,
Columbus, June 2008. c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.98766484">
(Bui et al., 2007a; Bui et al., 2007b). An alterna-
tive approach taken in the Hidden Information State
(HIS) system is to retain a complete representation
of the user’s goal, but partition states into equiva-
lence classes and prune away very low probability
partitions (Young et al., 2007; Thomson et al., 2007;
Williams and Young, 2007b).
Whichever approach is taken, a key issue in a real
POMDP-based dialogue system is its ability to be
robust to noise and that is the issue that is addressed
in this paper. Using the HIS system as an exem-
plar, evaluation results are presented for a real-world
tourist information task using both simulated and
real users. The results show that a POMDP system
can learn noise robust policies and that N-best out-
puts from the speech understanding component can
be exploited to further improve robustness.
The paper is structured as follows. Firstly, in Sec-
tion 2 a brief overview of the HIS system is given.
Then in Section 3, various POMDP training regimes
are described and evaluated using a simulated user at
differing noise levels. Section 4 then presents results
from a trial in which users conducted various tasks
over a range of noise levels. Finally, in Section 5,
we discuss our results and present our conclusions.
</bodyText>
<sectionHeader confidence="0.949736" genericHeader="method">
2 The HIS System
</sectionHeader>
<subsectionHeader confidence="0.984858">
2.1 Basic Principles
</subsectionHeader>
<bodyText confidence="0.99726875">
A POMDP-based dialogue system is shown in Fig-
ure 1 where sm denotes the (unobserved or hidden)
machine state which is factored into three compo-
nents: the last user act a,,, the user’s goal s,, and
the dialogue history sd. Since sm is unknown, at
each time-step the system computes a belief state
such that the probability of being in state sm given
belief state b is b(sm). Based on this current belief
state b, the machine selects an action am, receives
a reward r(sm, am), and transitions to a new (un-
observed) state s′ , where s′ depends only on sm
mm
and am. The machine then receives an observation
o′ consisting of an N-best list of hypothesised user
actions. Finally, the belief distribution b is updated
based on o′ and am as follows:
</bodyText>
<equation confidence="0.995043666666667">
�b′(s′m) = kP(o′|s′m, am) P(s′m|am, sm)b(sm)
s,ES,
(1)
</equation>
<bodyText confidence="0.998915666666667">
where k is a normalisation constant (Kaelbling et al.,
1998). The first term on the RHS of (1) is called the
observation model and the term inside the summa-
tion is called the transition model. Maintaining this
belief state as the dialogue evolves is called belief
monitoring.
</bodyText>
<equation confidence="0.896335">
s�m� =� &lt;a�u�,�s� u�,�s� d�&gt;�
</equation>
<figureCaption confidence="0.9898835">
Figure 1: Abstract view of a POMDP-based spoken dia-
logue system
</figureCaption>
<bodyText confidence="0.999124833333333">
At each time step t, the machine receives a reward
r(bt, am,t) based on the current belief state bt and the
selected action am,t. Each action am,t is determined
by a policy 7r(bt) and building a POMDP system in-
volves finding the policy 7r* which maximises the
discounted sum R of the rewards
</bodyText>
<equation confidence="0.829909">
Atr(bt, am,t) (2)
</equation>
<bodyText confidence="0.908639">
where At is a discount coefficient.
</bodyText>
<subsectionHeader confidence="0.997101">
2.2 Probability Models
</subsectionHeader>
<bodyText confidence="0.9995281">
In the HIS system, user goals are partitioned and
initially, all states s,, E S,, are regarded as being
equally likely and they are placed in a single par-
tition p0. As the dialogue progresses, user inputs
result in changing beliefs and this root partition is
repeatedly split into smaller partitions. This split-
ting is binary, i.e. p —* {p′, p − p′} with probability
P (p′|p). By replacing sm by its factors (s, a, sd)
and making reasonable independence assumptions,
it can be shown (Young et al., 2007) that in parti-
</bodyText>
<figure confidence="0.962250208333333">
a�m�
~�
aj
User❑
Speech❑
Understanding[]
s�,
Speech❑
Generation❑
a�u�
1�
�t
a&amp;,
ap,
Belief❑
Estimator❑
Dialog[]
Policy[]
b( )❑s�m�
sw,
00
E
t=0
R =
</figure>
<page confidence="0.926602">
113
</page>
<bodyText confidence="0.735333">
tioned form (1) becomes
</bodyText>
<equation confidence="0.875372875">
b′(p′, a′&apos;, s′d) = k · P(o′|a′&apos;)
 |{z }
observation
model
P(s′d|p′, a′ &apos;, sd, am)
 |{z }
dialogue
model
</equation>
<bodyText confidence="0.999814307692308">
where p is the parent of p′.
In this equation, the observation model is approx-
imated by the normalised distribution of confidence
measures output by the speech recognition system.
The user action model allows the observation prob-
ability that is conditioned on a′&apos; to be scaled by the
probability that the user would speak a′&apos; given the
partition p′ and the last system prompt am. In the
current implementation of the HIS system, user dia-
logue acts take the form act(a = v) where act is the
dialogue type, a is an attribute and v is its value [for
example, request(food=Chinese)]. The user action
model is then approximated by
</bodyText>
<equation confidence="0.8683675">
P(a′&apos;|p′,am) ≈ P(T (a′&apos;)|T (am))P(M(a′&apos;)|p′)
(4)
</equation>
<bodyText confidence="0.999880125">
where T (·) denotes the type of the dialogue act
and M(·) denotes whether or not the dialogue act
matches the current partition p′. The dialogue
model is a deterministic encoding based on a simple
grounding model. It yields probability one when the
updated dialogue hypothesis (i.e., a specific combi-
nation of p′, a′&apos;, sd and am) is consistent with the
history and zero otherwise.
</bodyText>
<subsectionHeader confidence="0.999332">
2.3 Policy Representation
</subsectionHeader>
<bodyText confidence="0.999892142857143">
Policy representation in POMDP-systems is non-
trivial since each action depends on a complex prob-
ability distribution. One of the simplest approaches
to dealing with this problem is to discretise the state
space and then associate an action with each dis-
crete grid point. To reduce quantisation errors, the
HIS model first maps belief distributions into a re-
duced summary space before quantising. This sum-
mary space consists of the probability of the top
two hypotheses plus some status variables and the
user act type associated with the top distribution.
Quantisation is then performed using a simple dis-
tance metric to find the nearest grid point. Ac-
tions in summary space refer specifically to the top
two hypotheses, and unlike actions in master space,
they are limited to a small finite set: greet, ask, ex-
plicit confirm, implicit confirm, select confirm, of-
fer, inform, find alternative, query more, goodbye.
A simple heuristic is then used to map the selected
next system action back into the full master belief
space.
</bodyText>
<figureCaption confidence="0.996913">
Figure 2: Overview of the HIS system dialogue cycle
</figureCaption>
<bodyText confidence="0.99996552">
The dialogue manager is able to support nega-
tions, denials and requests for alternatives. When the
selected summary action is to offer the user a venue,
the summary-to-master space mapping heuristics
will normally offer a venue consistent with the most
likely user goal hypothesis. If this hypothesis is then
rejected its belief is substantially reduced and it will
no longer be the top-ranking hypothesis. If the next
system action is to make an alternative offer, then
the new top-ranking hypothesis may not be appro-
priate. For example, if an expensive French restau-
rant near the river had been offered and the user asks
for one nearer the centre of town, any alternative of-
fered should still include the user’s confirmed de-
sire for an expensive French restaurant. To ensure
this, all of the grounded features from the rejected
hypothesis are extracted and all user goal hypothe-
ses are scanned starting at the most likely until an
alternative is found that matches the grounded fea-
tures. For the current turn only, the summary-to-
master space heuristics then treat this hypothesis as
if it was the top-ranking one. If the system then of-
fers a venue based on this hypothesis, and the user
accepts it, then, since system outputs are appended
to user inputs for the purpose of belief updating, the
</bodyText>
<figure confidence="0.993476455882353">
P(a′&apos;|p′, am)
 |{z }
user action
model
X·
sd
P (p′|p)b(p, sd)
 |{z }
partition
splitting
(3)
From❑
User[]
Fromo
System❑
Observation❑
Specific[]
Action[]
~� 1�
a�uo
a�u�
~� 2�
~� u�a� N
a; []
a; []
Summary Space❑
Action❑
Refinement[]
(heuristic)[]
Ontology Rules❑
u�p�,
u�p�,
u�p�,
Strategic[]
Action[]
m[]a�^�
Application Database❑
a�u�
~� 1�
a�u�
~� 1�
2�
a�u�
~�
aU❑
~� 2�
a�u�
~� 2�
POMDPD
Policy[]
^�
b
d�s�1�
d�s�2�
1�
d�s�
d�s�2�
d�s�3�
1�h�
2�h�
3�h�
4�h�
5�h�
Map too
Summary[]
Space[]
Belief$[]
Statedb
</figure>
<page confidence="0.991039">
114
</page>
<bodyText confidence="0.99978625">
alternative hypothesis will move to the top, or near
the top, of the ranked hypothesis list. The dialogue
then typically continues with its focus on the newly
offered alternative venue.
</bodyText>
<subsectionHeader confidence="0.997228">
2.4 Summary of Operation
</subsectionHeader>
<bodyText confidence="0.99999375">
To summarise, the overall processing performed by
the HIS system in a single dialogue turn (i.e. one cy-
cle of system output and user response) is as shown
in Figure 2. Each user utterance is decoded into an
N-best list of dialogue acts. Each incoming act plus
the previous system act are matched against the for-
est of user goals and partitions are split as needed.
Each user act a,, is then duplicated and bound to
each partition p. Each partition will also have a
set of dialogue histories sd associated with it. The
combination of each p, a,, and updated sd forms a
new dialogue hypothesis hk whose beliefs are eval-
uated using (3). Once all dialogue hypotheses have
been evaluated and any duplicates merged, the mas-
ter belief state b is mapped into summary space b�
and the nearest policy belief point is found. The as-
sociated summary space machine action a„t is then
heuristically mapped back to master space and the
machine’s actual response a„t is output. The cycle
then repeats until the user’s goal is satisfied.
</bodyText>
<sectionHeader confidence="0.9731855" genericHeader="method">
3 Training and Evaluation with a
Simulated User
</sectionHeader>
<subsectionHeader confidence="0.999672">
3.1 Policy optimisation
</subsectionHeader>
<bodyText confidence="0.999049678571428">
Policy optimisation is performed in the discrete
summary space described in the previous section us-
ing on-line batch E-greedy policy iteration. Given
an existing policy 7r, dialogs are executed and ma-
chine actions generated according to 7r except that
with probability E a random action is generated. The
system maintains a set of belief points 16i}. At each
turn in training, the nearest stored belief point bk to
b is located using a distance measure. If the distance
is greater than some threshold, b� is added to the set
of stored belief points. The sequence of points bk
traversed in each dialogue is stored in a list. As-
sociated with each �bi is a function Q(�bi, a„t) whose
value is the expected total reward obtained by choos-
ing summary action a„t from state �bi. At the end
of each dialogue, the total reward is calculated and
added to an accumulator for each point in the list,
discounted by A at each step. On completion of a
batch of dialogs, the Q values are updated accord-
ing to the accumulated rewards, and the policy up-
dated by choosing the action which maximises each
Q value. The whole process is then repeated until
the policy stabilises.
In our experiments, E was fixed at 0.1 and A was
fixed at 0.95. The reward function used attempted
to encourage short successful dialogues by assign-
ing +20 for a successful dialogue and −1 for each
dialogue turn.
</bodyText>
<subsectionHeader confidence="0.998814">
3.2 User Simulation
</subsectionHeader>
<bodyText confidence="0.999994636363636">
To train a policy, a user simulator is used to gen-
erate responses to system actions. It has two main
components: a User Goal and a User Agenda. At
the start of each dialogue, the goal is randomly
initialised with requests such as “name”, “addr”,
“phone” and constraints such as “type=restaurant”,
“food=Chinese”, etc. The agenda stores the di-
alogue acts needed to elicit this information in a
stack-like structure which enables it to temporarily
store actions when another action of higher priority
needs to be issued first. This enables the simulator
to refer to previous dialogue turns at a later point. To
generate a wide spread of realistic dialogs, the sim-
ulator reacts wherever possible with varying levels
of patience and arbitrariness. In addition, the sim-
ulator will relax its constraints when its initial goal
cannot be satisfied. This allows the dialogue man-
ager to learn negotiation-type dialogues where only
an approximate solution to the user’s goal exists.
Speech understanding errors are simulated at the di-
alogue act level using confusion matrices trained on
labelled dialogue data (Schatzmann et al., 2007).
</bodyText>
<subsectionHeader confidence="0.998755">
3.3 Training and Evaluation
</subsectionHeader>
<bodyText confidence="0.999965181818182">
When training a system to operate robustly in noisy
conditions, a variety of strategies are possible. For
example, the system can be trained only on noise-
free interactions, it can be trained on increasing lev-
els of noise or it can be trained on a high noise level
from the outset. A related issue concerns the gener-
ation of grid points and the number of training itera-
tions to perform. For example, allowing a very large
number of points leads to poor performance due to
over-fitting of the training data. Conversely, having
too few point leads to poor performance due to a lack
</bodyText>
<page confidence="0.998031">
115
</page>
<bodyText confidence="0.995525387755103">
of discrimination in its dialogue strategies.
After some experimentation, the following train-
ing schedule was adopted. Training starts in a
noise free environment using a small number of grid
points and it continues until the performance of the
policy levels off. The resulting policy is then taken
as an initial policy for the next stage where the noise
level is increased, the number of grid points is ex-
panded and the number of iterations is increased.
This process is repeated until the highest noise level
is reached. This approach was motivated by the ob-
servation that a key factor in effective reinforcement
learning is the balance between exploration and ex-
ploitation. In POMDP policy optimisation which
uses dynamically allocated grid points, maintaining
this balance is crucial. In our case, the noise intro-
duced by the simulator is used as an implicit mech-
anism for increasing the exploration. Each time ex-
ploration is increased, the areas of state-space that
will be visited will also increase and hence the num-
ber of available grid points must also be increased.
At the same time, the number of iterations must be
increased to ensure that all points are visited a suf-
ficient number of times. In practice we found that
around 750 to 1000 grid points was sufficient and
the total number of simulated dialogues needed for
training was around 100,000.
A second issue when training in noisy conditions
is whether to train on just the 1-best output from the
simulator or train on the N-best outputs. A limit-
ing factor here is that the computation required for
N-best training is significantly increased since the
rate of partition generation in the HIS model in-
creases exponentially with N. In preliminary tests,
it was found that when training with 1-best outputs,
there was little difference between policies trained
entirely in no noise and policies trained on increas-
ing noise as described above. However, policies
trained on 2-best using the incremental strategy did
exhibit increased robustness to noise. To illustrate
this, Figures 3 and 4 show the average dialogue suc-
cess rates and rewards for 3 different policies, all
trained on 2-best: a hand-crafted policy (hdc), a pol-
icy trained on noise-free conditions (noise free) and
a policy trained using the incremental scheme de-
scribed above (increm). Each policy was tested us-
ing 2-best output from the simulator across a range
of error rates. In addition, the noise-free policy was
also tested on 1-best output.
</bodyText>
<figureCaption confidence="0.998772166666667">
Figure 3: Average simulated dialogue success rate as a
function of error rate for a hand-crafted (hdc), noise-free
and incrementally trained (increm) policy.
Figure 4: Average simulated dialogue reward as a func-
tion of error rate for a hand-crafted (hdc), noise-free and
incrementally trained (increm) policy.
</figureCaption>
<bodyText confidence="0.999956714285714">
As can be seen, both the trained policies improve
significantly on the hand-crafted policies. Further-
more, although the average rewards are all broadly
similar, the success rate of the incrementally trained
policy is significantly better at higher error rates.
Hence, this latter policy was selected for the user
trial described next.
</bodyText>
<sectionHeader confidence="0.96285" genericHeader="evaluation">
4 Evaluation via a User Trial
</sectionHeader>
<bodyText confidence="0.998872">
The HIS-POMDP policy (HIS-TRA) that was incre-
mentally trained on the simulated user using 2-best
lists was tested in a user trial together with a hand-
crafted HIS-POMDP policy (HIS-HDC). The strat-
egy used by the latter was to first check the most
likely hypothesis. If it contains sufficient grounded
</bodyText>
<page confidence="0.997725">
116
</page>
<bodyText confidence="0.99991325">
keys to match 1 to 3 database entities, then offer is
selected. If any part of the hypothesis is inconsis-
tent or the user has explicitly asked for another sug-
gestion, then find alternative action is selected. If
the user has asked for information about an offered
entity then inform is selected. Otherwise, an un-
grounded component of the top hypothesis is identi-
fied and depending on the belief, one of the confirm
actions is selected.
In addition, an MDP-based dialogue manager de-
veloped for earlier trials (Schatzmann, 2008) was
also tested. Since considerable effort has been put in
optimising this system, it serves as a strong baseline
for comparison. Again, both a trained policy (MDP-
TRA) and a hand-crafted policy (MDP-HDC) were
tested.
</bodyText>
<subsectionHeader confidence="0.999484">
4.1 System setup and confidence scoring
</subsectionHeader>
<bodyText confidence="0.999997333333333">
The dialogue system consisted of an ATK-based
speech recogniser, a Phoenix-based semantic parser,
the dialogue manager and a diphone based speech
synthesiser. The semantic parser uses simple phrasal
grammar rules to extract the dialogue act type and a
list of attribute/value pairs from each utterance.
In a POMDP-based dialogue system, accurate
belief-updating is very sensitive to the confidence
scores assigned to each user dialogue act. Ideally
these should provide a measure of the probability of
the decoded act given the true user act. In the evalu-
ation system, the recogniser generates a 10-best list
of hypotheses at each turn along with a compact con-
fusion network which is used to compute the infer-
ence evidence for each hypothesis. The latter is de-
fined as the sum of the log-likelihoods of each arc
in the confusion network and when exponentiated
and renormalised this gives a simple estimate of the
probability of each hypothesised utterance. Each ut-
terance in the 10-best list is passed to the semantic
parser. Equivalent dialogue acts output by the parser
are then grouped together and the dialogue act for
each group is then assigned the sum of the sentence-
level probabilities as its confidence score.
</bodyText>
<subsectionHeader confidence="0.99311">
4.2 Trial setup
</subsectionHeader>
<bodyText confidence="0.999975794871795">
For the trial itself, 36 subjects were recruited (all
British native speakers, 18 male, 18 female). Each
subject was asked to imagine himself to be a tourist
in a fictitious town called Jasonville and try to find
particular hotels, bars, or restaurants in that town.
Each subject was asked to complete a set of pre-
defined tasks where each task involved finding the
name of a venue satisfying a set of constraints such
as food type is Chinese, price-range is cheap, etc.,
and getting the value of one or more additional at-
tributes of that venue such as the address or the
phone number.
For each task, subjects were given a scenario to
read and were then asked to solve the task via a di-
alogue with the system. The tasks set could either
have one solution, several solutions, or no solution
at all in the database. In cases where a subject found
that there was no matching venue for the given task,
he/she was allowed to try and find an alternative
venue by relaxing one or more of the constraints.
In addition, subjects had to perform each task at
one of three possible noise levels. These levels cor-
respond to signal/noise ratios (SNRs) of 35.3 dB
(low noise), 10.2 dB (medium noise), or 3.3 dB
(high noise). The noise was artificially generated
and mixed with the microphone signal, in addition
it was fed into the subject’s headphones so that they
were aware of the noisy conditions.
An instructor was present at all times to indicate
to the subject which task description to follow, and
to start the right system with the appropriate noise-
level. Each subject performed an equal number of
tasks for each system (3 tasks), noise level (6 tasks)
and solution type (6 tasks for each of the types 0, 1,
or multiple solutions). Also, each subject performed
one task for all combinations of system and noise
level. Overall, each combination of system, noise
level, and solution type was used in an equal number
of dialogues.
</bodyText>
<subsectionHeader confidence="0.949198">
4.3 Results
</subsectionHeader>
<bodyText confidence="0.999987363636364">
In Table 1, some general statistics of the corpus re-
sulting from the trial are given. The semantic error
rate is based on substitutions, insertions and dele-
tions errors on semantic items. When tested after the
trial on the transcribed user utterances, the semantic
error rate was 4.1% whereas the semantic error rate
on the ASR input was 25.2%. This means that 84%
of the error rate was due to the ASR.
Tables 2 and 3 present success rates (Succ.) and
average performance scores (Perf.), comparing the
two HIS dialogue managers with the two MDP base-
</bodyText>
<page confidence="0.9918">
117
</page>
<table confidence="0.999764285714286">
Number of dialogues 432
Number of dialogue turns 3972
Number of words (transcriptions) 18239
Words per utterance 4.58
Word Error Rate 32.9
Semantic Error Rate 25.2
Semantic Error Rate transcriptions 4.1
</table>
<tableCaption confidence="0.999918">
Table 1: General corpus statistics.
</tableCaption>
<bodyText confidence="0.999108888888889">
line systems. For the success rates, also the stan-
dard deviation (std.dev) is given, assuming a bino-
mial distribution. The success rate is the percentage
of successfully completed dialogues. A task is con-
sidered to be fully completed when the user is able to
find the venue he is looking for and get all the addi-
tional information he asked for; if the task has no so-
lution and the system indicates to the user no venue
could be found, this also counts as full completion.
A task is considered to be partially completed when
only the correct venue has been given. The results on
partial completion are given in Table 2, and the re-
sults on full completion in Table 3. To mirror the re-
ward function used in training, the performance for
each dialogue is computed by assigning a reward of
20 points for full completion and subtracting 1 point
for the number of turns up until a successful recom-
mendation (i.e., partial completion).
</bodyText>
<table confidence="0.999247333333333">
Partial Task Completion statistics
System Succ. (std.dev) #turns Perf.
MDP-HDC 68.52 (4.83) 4.80 8.91
MDP-TRA 70.37 (4.75) 4.75 9.32
HIS-HDC 74.07 (4.55) 7.04 7.78
HIS-TRA 84.26 (3.78) 4.63 12.22
</table>
<tableCaption confidence="0.9567945">
Table 2: Success rates and performance results on partial
completion.
</tableCaption>
<table confidence="0.999577166666667">
Full Task Completion statistics
System Succ. (std.dev) #turns Perf.
MDP-HDC 64.81 (4.96) 5.86 7.10
MDP-TRA 65.74 (4.93) 6.18 6.97
HIS-HDC 63.89 (4.99) 8.57 4.20
HIS-TRA 78.70 (4.25) 6.36 9.38
</table>
<tableCaption confidence="0.991147">
Table 3: Success rates and performance results on full
completion.
</tableCaption>
<bodyText confidence="0.9996092">
The results show that the trained HIS dialogue
manager significantly outperforms both MDP based
dialogue managers. For success rate on partial com-
pletion, both HIS systems perform better than the
MDP systems.
</bodyText>
<subsectionHeader confidence="0.79768">
4.3.1 Subjective Results
</subsectionHeader>
<bodyText confidence="0.996988545454546">
In the user trial, the subjects were also asked for
a subjective judgement of the systems. After com-
pleting each task, the subjects were asked whether
they had found the information they were looking
for (yes/no). They were also asked to give a score
on a scale from 1 to 5 (best) on how natural/intuitive
they thought the dialogue was. Table 4 shows the
results for the 4 systems used. The performance of
the HIS systems is similar to the MDP systems, with
a slightly higher success rate for the trained one and
a slightly lower score for the handcrafted one.
</bodyText>
<table confidence="0.9996868">
System Succ. Rate (std.dev) Score
MDP-HDC 78 (4.30) 3.52
MDP-TRA 78 (4.30) 3.42
HIS-HDC 71 (4.72) 3.05
HIS-TRA 83 (3.90) 3.41
</table>
<tableCaption confidence="0.9706285">
Table 4: Subjective performance results from the user
trial.
</tableCaption>
<sectionHeader confidence="0.9993" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999909578947369">
This paper has described recent work in training a
POMDP-based dialogue manager to exploit the ad-
ditional information available from a speech under-
standing system which can generate ranked lists of
hypotheses. Following a brief overview of the Hid-
den Information State dialogue manager and pol-
icy optimisation using a user simulator, results have
been given for both simulated user and real user di-
alogues conducted at a variety of noise levels.
The user simulation results have shown that al-
though the rewards are similar, training with 2-best
rather than 1-best outputs from the user simulator
yields better success rates at high noise levels. In
view of this result, we would have liked to inves-
tigate training on longer N-best lists, but currently
computational constraints prevent this. We hope in
the future to address this issue by developing more
efficient state partitioning strategies for the HIS sys-
tem.
</bodyText>
<page confidence="0.994647">
118
</page>
<bodyText confidence="0.999986472222222">
The overall results on real data collected from the
user trial clearly indicate increased robustness by the
HIS system. We would have liked to be able to
plot performance and success scores as a function
of noise level or speech understanding error rate,
but there is great variability in these kinds of com-
plex real-world dialogues and it transpired that the
trial data was insufficient to enable any statistically
meaningful presentation of this form. We estimate
that we need at least an order of magnitude more
trial data to properly investigate the behaviour of
such systems as a function of noise level. The trial
described here, including transcription and analysis
consumed about 30 man-days of effort. Increasing
this by a factor of 10 or more is not therefore an
option for us, and clearly an alternative approach is
needed.
We have also reported results of subjective suc-
cess rate and opinion scores based on data obtained
from subjects after each trial. The results were only
weakly correlated with the measured performance
and success rates. We believe that this is partly due
to confusion as to what constituted success in the
minds of the subjects. This suggests that for subjec-
tive results to be meaningful, measurements such as
these will only be really useful if made on live sys-
tems where users have a real rather than imagined
information need. The use of live systems would
also alleviate the data sparsity problem noted earlier.
Finally and in conclusion, we believe that despite
the difficulties noted above, the results reported in
this paper represent a first step towards establish-
ing the POMDP as a viable framework for develop-
ing spoken dialogue systems which are significantly
more robust to noisy operating conditions than con-
ventional state-based systems.
</bodyText>
<sectionHeader confidence="0.998022" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.998843">
This research was partly funded by the UK EPSRC
under grant agreement EP/F013930/1 and by the
EU FP7 Programme under grant agreement 216594
(CLASSIC project: www.classic-project.org).
</bodyText>
<sectionHeader confidence="0.999613" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999753425531915">
TH Bui, M Poel, A Nijholt, and J Zwiers. 2007a. A
tractable DDN-POMDP Approach to Affective Dia-
logue Modeling for General Probabilistic Frame-based
Dialogue Systems. In Proc 5th Workshop on Knowl-
edge and Reasoning in Practical Dialogue Systems,
pages 34–57.
TH Bui, B van Schooten, and D Hofs. 2007b. Practi-
cal dialogue manager development using POMDPs .
In 8th SIGdial Workshop on Discourse and Dialogue,
Antwerp.
LP Kaelbling, ML Littman, and AR Cassandra. 1998.
Planning and Acting in Partially Observable Stochastic
Domains. Artificial Intelligence, 101:99–134.
N Roy, J Pineau, and S Thrun. 2000. Spoken Dialogue
Management Using Probabilistic Reasoning. In Proc
ACL.
J Schatzmann, B Thomson, and SJ Young. 2007. Error
Simulation for Training Statistical Dialogue Systems.
In ASRU 07, Kyoto, Japan.
J Schatzmann. 2008. Statistical User and Error Mod-
ellingfor Spoken Dialogue Systems. Ph.D. thesis, Uni-
versity of Cambridge.
B Thomson, J Schatzmann, K Weilhammer, H Ye, and
SJ Young. 2007. Training a real-world POMDP-based
Dialog System. In HLT/NAACL Workshop ”Bridging
the Gap: Academic and Industrial Research in Dialog
Technologies”, Rochester.
B Thomson, J Schatzmann, and SJ Young. 2008.
Bayesian Update of Dialogue State for Robust Dia-
logue Systems. In Int ConfAcoustics Speech and Sig-
nal Processing ICASSP, Las Vegas.
JD Williams and SJ Young. 2007a. Partially Observable
Markov Decision Processes for Spoken Dialog Sys-
tems. Computer Speech and Language, 21(2):393–
422.
JD Williams and SJ Young. 2007b. Scaling POMDPs
for Spoken Dialog Management. IEEE Audio, Speech
and Language Processing, 15(7):2116–2129.
SJ Young, J Schatzmann, K Weilhammer, and H Ye.
2007. The Hidden Information State Approach to Dia-
log Management. In ICASSP 2007, Honolulu, Hawaii.
SJ Young. 2002. Talking to Machines (Statistically
Speaking). In Int Conf Spoken Language Processing,
Denver, Colorado.
B Zhang, Q Cai, J Mao, E Chang, and B Guo. 2001.
Spoken Dialogue Management as Planning and Acting
under Uncertainty. In Eurospeech, Aalborg, Denmark.
</reference>
<page confidence="0.999093">
119
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.370282">
<title confidence="0.999419">Training and Evaluation of the HIS POMDP Dialogue System in Noise</title>
<author confidence="0.896495">M Gaˇsi´c</author>
<author confidence="0.896495">S Keizer</author>
<author confidence="0.896495">F Mairesse</author>
<author confidence="0.896495">J Schatzmann</author>
<author confidence="0.896495">B Thomson</author>
<author confidence="0.896495">K Yu</author>
<author confidence="0.896495">S</author>
<affiliation confidence="0.756116">Machine Intelligence Engineering</affiliation>
<address confidence="0.7769255">Cambridge United Kingdom</address>
<abstract confidence="0.99565145">This paper investigates the claim that a dialogue manager modelled as a Partially Observable Markov Decision Process (POMDP) can achieve improved robustness to noise compared to conventional state-based dialogue managers. Using the Hidden Information State (HIS) POMDP dialogue manager as an exemplar, and an MDP-based dialogue manager as a baseline, evaluation results are presented for both simulated and real dialogues in a Tourist Information Domain. The results on the simulated data show that the inherent ability to model uncertainty, allows the POMDP model to exploit alternative hypotheses from the speech understanding system. The results obtained from a user trial show that the HIS system with a trained policy performed significantly better than the MDP baseline.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>TH Bui</author>
<author>M Poel</author>
<author>A Nijholt</author>
<author>J Zwiers</author>
</authors>
<title>A tractable DDN-POMDP Approach to Affective Dialogue Modeling for General Probabilistic Frame-based Dialogue Systems.</title>
<date>2007</date>
<booktitle>In Proc 5th Workshop on Knowledge and Reasoning in Practical Dialogue Systems,</booktitle>
<pages>34--57</pages>
<contexts>
<context position="3440" citStr="Bui et al., 2007" startWordPosition="534" endWordPosition="537">7b). Based on these ideas, a number of real-world POMDP-based systems have recently emerged. The most complex entity which must be represented in the state space is the user’s goal. In the Bayesian Update ofDialogue State (BUDS) system, the user’s goal is further factored into conditionally independent slots. The resulting system is then modelled as a dynamic Bayesian network (Thomson et al., 2008). A similar approach is also developed in 112 Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 112–119, Columbus, June 2008. c�2008 Association for Computational Linguistics (Bui et al., 2007a; Bui et al., 2007b). An alternative approach taken in the Hidden Information State (HIS) system is to retain a complete representation of the user’s goal, but partition states into equivalence classes and prune away very low probability partitions (Young et al., 2007; Thomson et al., 2007; Williams and Young, 2007b). Whichever approach is taken, a key issue in a real POMDP-based dialogue system is its ability to be robust to noise and that is the issue that is addressed in this paper. Using the HIS system as an exemplar, evaluation results are presented for a real-world tourist information t</context>
</contexts>
<marker>Bui, Poel, Nijholt, Zwiers, 2007</marker>
<rawString>TH Bui, M Poel, A Nijholt, and J Zwiers. 2007a. A tractable DDN-POMDP Approach to Affective Dialogue Modeling for General Probabilistic Frame-based Dialogue Systems. In Proc 5th Workshop on Knowledge and Reasoning in Practical Dialogue Systems, pages 34–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>TH Bui</author>
<author>B van Schooten</author>
<author>D Hofs</author>
</authors>
<title>Practical dialogue manager development using POMDPs .</title>
<date>2007</date>
<booktitle>In 8th SIGdial Workshop on Discourse and Dialogue,</booktitle>
<location>Antwerp.</location>
<marker>Bui, van Schooten, Hofs, 2007</marker>
<rawString>TH Bui, B van Schooten, and D Hofs. 2007b. Practical dialogue manager development using POMDPs . In 8th SIGdial Workshop on Discourse and Dialogue, Antwerp.</rawString>
</citation>
<citation valid="true">
<authors>
<author>LP Kaelbling</author>
<author>ML Littman</author>
<author>AR Cassandra</author>
</authors>
<title>Planning and Acting in Partially Observable Stochastic Domains.</title>
<date>1998</date>
<journal>Artificial Intelligence,</journal>
<pages>101--99</pages>
<contexts>
<context position="5578" citStr="Kaelbling et al., 1998" startWordPosition="903" endWordPosition="906">nknown, at each time-step the system computes a belief state such that the probability of being in state sm given belief state b is b(sm). Based on this current belief state b, the machine selects an action am, receives a reward r(sm, am), and transitions to a new (unobserved) state s′ , where s′ depends only on sm mm and am. The machine then receives an observation o′ consisting of an N-best list of hypothesised user actions. Finally, the belief distribution b is updated based on o′ and am as follows: �b′(s′m) = kP(o′|s′m, am) P(s′m|am, sm)b(sm) s,ES, (1) where k is a normalisation constant (Kaelbling et al., 1998). The first term on the RHS of (1) is called the observation model and the term inside the summation is called the transition model. Maintaining this belief state as the dialogue evolves is called belief monitoring. s�m� =� &lt;a�u�,�s� u�,�s� d�&gt;� Figure 1: Abstract view of a POMDP-based spoken dialogue system At each time step t, the machine receives a reward r(bt, am,t) based on the current belief state bt and the selected action am,t. Each action am,t is determined by a policy 7r(bt) and building a POMDP system involves finding the policy 7r* which maximises the discounted sum R of the reward</context>
</contexts>
<marker>Kaelbling, Littman, Cassandra, 1998</marker>
<rawString>LP Kaelbling, ML Littman, and AR Cassandra. 1998. Planning and Acting in Partially Observable Stochastic Domains. Artificial Intelligence, 101:99–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Roy</author>
<author>J Pineau</author>
<author>S Thrun</author>
</authors>
<title>Spoken Dialogue Management Using Probabilistic Reasoning. In</title>
<date>2000</date>
<booktitle>Proc ACL.</booktitle>
<contexts>
<context position="2465" citStr="Roy et al., 2000" startWordPosition="379" endWordPosition="382">ilding spoken dialogue systems which can both model these uncertainties and support policies which are robust to their effects (Young, 2002; Williams and Young, 2007a). The key idea of the POMDP is that the underlying dialogue state is hidden and dialogue management policies must therefore be based not on a single state estimate but on a distribution over all states. Whilst POMDPs are attractive theoretically, in practice, they are notoriously intractable for anything other than small state/action spaces. Hence, practical examples of their use were initially restricted to very simple domains (Roy et al., 2000; Zhang et al., 2001). More recently, however, a number of techniques have been suggested which do allow POMDPs to be scaled to handle real world tasks. The two generic mechanisms which facilitate this scaling are factoring the state space and performing policy optimisation in a reduced summary state space (Williams and Young, 2007a; Williams and Young, 2007b). Based on these ideas, a number of real-world POMDP-based systems have recently emerged. The most complex entity which must be represented in the state space is the user’s goal. In the Bayesian Update ofDialogue State (BUDS) system, the </context>
</contexts>
<marker>Roy, Pineau, Thrun, 2000</marker>
<rawString>N Roy, J Pineau, and S Thrun. 2000. Spoken Dialogue Management Using Probabilistic Reasoning. In Proc ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Schatzmann</author>
<author>B Thomson</author>
<author>SJ Young</author>
</authors>
<title>Error Simulation for Training Statistical Dialogue Systems.</title>
<date>2007</date>
<booktitle>In ASRU 07,</booktitle>
<location>Kyoto, Japan.</location>
<contexts>
<context position="14840" citStr="Schatzmann et al., 2007" startWordPosition="2484" endWordPosition="2487">eeds to be issued first. This enables the simulator to refer to previous dialogue turns at a later point. To generate a wide spread of realistic dialogs, the simulator reacts wherever possible with varying levels of patience and arbitrariness. In addition, the simulator will relax its constraints when its initial goal cannot be satisfied. This allows the dialogue manager to learn negotiation-type dialogues where only an approximate solution to the user’s goal exists. Speech understanding errors are simulated at the dialogue act level using confusion matrices trained on labelled dialogue data (Schatzmann et al., 2007). 3.3 Training and Evaluation When training a system to operate robustly in noisy conditions, a variety of strategies are possible. For example, the system can be trained only on noisefree interactions, it can be trained on increasing levels of noise or it can be trained on a high noise level from the outset. A related issue concerns the generation of grid points and the number of training iterations to perform. For example, allowing a very large number of points leads to poor performance due to over-fitting of the training data. Conversely, having too few point leads to poor performance due t</context>
</contexts>
<marker>Schatzmann, Thomson, Young, 2007</marker>
<rawString>J Schatzmann, B Thomson, and SJ Young. 2007. Error Simulation for Training Statistical Dialogue Systems. In ASRU 07, Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Schatzmann</author>
</authors>
<title>Statistical User and Error Modellingfor Spoken Dialogue Systems.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Cambridge.</institution>
<contexts>
<context position="19435" citStr="Schatzmann, 2008" startWordPosition="3246" endWordPosition="3247">tter was to first check the most likely hypothesis. If it contains sufficient grounded 116 keys to match 1 to 3 database entities, then offer is selected. If any part of the hypothesis is inconsistent or the user has explicitly asked for another suggestion, then find alternative action is selected. If the user has asked for information about an offered entity then inform is selected. Otherwise, an ungrounded component of the top hypothesis is identified and depending on the belief, one of the confirm actions is selected. In addition, an MDP-based dialogue manager developed for earlier trials (Schatzmann, 2008) was also tested. Since considerable effort has been put in optimising this system, it serves as a strong baseline for comparison. Again, both a trained policy (MDPTRA) and a hand-crafted policy (MDP-HDC) were tested. 4.1 System setup and confidence scoring The dialogue system consisted of an ATK-based speech recogniser, a Phoenix-based semantic parser, the dialogue manager and a diphone based speech synthesiser. The semantic parser uses simple phrasal grammar rules to extract the dialogue act type and a list of attribute/value pairs from each utterance. In a POMDP-based dialogue system, accur</context>
</contexts>
<marker>Schatzmann, 2008</marker>
<rawString>J Schatzmann. 2008. Statistical User and Error Modellingfor Spoken Dialogue Systems. Ph.D. thesis, University of Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Thomson</author>
<author>J Schatzmann</author>
<author>K Weilhammer</author>
<author>H Ye</author>
<author>SJ Young</author>
</authors>
<title>Training a real-world POMDP-based Dialog System. In</title>
<date>2007</date>
<booktitle>HLT/NAACL Workshop ”Bridging the Gap: Academic and Industrial Research in Dialog Technologies”,</booktitle>
<location>Rochester.</location>
<contexts>
<context position="3731" citStr="Thomson et al., 2007" startWordPosition="582" endWordPosition="585">ally independent slots. The resulting system is then modelled as a dynamic Bayesian network (Thomson et al., 2008). A similar approach is also developed in 112 Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 112–119, Columbus, June 2008. c�2008 Association for Computational Linguistics (Bui et al., 2007a; Bui et al., 2007b). An alternative approach taken in the Hidden Information State (HIS) system is to retain a complete representation of the user’s goal, but partition states into equivalence classes and prune away very low probability partitions (Young et al., 2007; Thomson et al., 2007; Williams and Young, 2007b). Whichever approach is taken, a key issue in a real POMDP-based dialogue system is its ability to be robust to noise and that is the issue that is addressed in this paper. Using the HIS system as an exemplar, evaluation results are presented for a real-world tourist information task using both simulated and real users. The results show that a POMDP system can learn noise robust policies and that N-best outputs from the speech understanding component can be exploited to further improve robustness. The paper is structured as follows. Firstly, in Section 2 a brief ove</context>
</contexts>
<marker>Thomson, Schatzmann, Weilhammer, Ye, Young, 2007</marker>
<rawString>B Thomson, J Schatzmann, K Weilhammer, H Ye, and SJ Young. 2007. Training a real-world POMDP-based Dialog System. In HLT/NAACL Workshop ”Bridging the Gap: Academic and Industrial Research in Dialog Technologies”, Rochester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Thomson</author>
<author>J Schatzmann</author>
<author>SJ Young</author>
</authors>
<title>Bayesian Update of Dialogue State for Robust Dialogue Systems.</title>
<date>2008</date>
<booktitle>In Int ConfAcoustics Speech and Signal Processing ICASSP,</booktitle>
<location>Las Vegas.</location>
<contexts>
<context position="3225" citStr="Thomson et al., 2008" startWordPosition="502" endWordPosition="505"> world tasks. The two generic mechanisms which facilitate this scaling are factoring the state space and performing policy optimisation in a reduced summary state space (Williams and Young, 2007a; Williams and Young, 2007b). Based on these ideas, a number of real-world POMDP-based systems have recently emerged. The most complex entity which must be represented in the state space is the user’s goal. In the Bayesian Update ofDialogue State (BUDS) system, the user’s goal is further factored into conditionally independent slots. The resulting system is then modelled as a dynamic Bayesian network (Thomson et al., 2008). A similar approach is also developed in 112 Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 112–119, Columbus, June 2008. c�2008 Association for Computational Linguistics (Bui et al., 2007a; Bui et al., 2007b). An alternative approach taken in the Hidden Information State (HIS) system is to retain a complete representation of the user’s goal, but partition states into equivalence classes and prune away very low probability partitions (Young et al., 2007; Thomson et al., 2007; Williams and Young, 2007b). Whichever approach is taken, a key issue in a real POMDP-based d</context>
</contexts>
<marker>Thomson, Schatzmann, Young, 2008</marker>
<rawString>B Thomson, J Schatzmann, and SJ Young. 2008. Bayesian Update of Dialogue State for Robust Dialogue Systems. In Int ConfAcoustics Speech and Signal Processing ICASSP, Las Vegas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>JD Williams</author>
<author>SJ Young</author>
</authors>
<title>Partially Observable Markov Decision Processes for Spoken Dialog Systems.</title>
<date>2007</date>
<journal>Computer Speech and Language,</journal>
<volume>21</volume>
<issue>2</issue>
<pages>422</pages>
<contexts>
<context position="2014" citStr="Williams and Young, 2007" startWordPosition="307" endWordPosition="310">ures. Such a system has no explicit mechanisms for representing the inevitable uncertainties associated with speech understanding or the ambiguities which naturally arise in interpreting a user’s intentions. The result is a system that is inherently fragile, especially in noisy conditions or where the user is unsure of how to use the system. It has been suggested that Partially Observable Markov Decision Processes (POMDPs) offer a natural framework for building spoken dialogue systems which can both model these uncertainties and support policies which are robust to their effects (Young, 2002; Williams and Young, 2007a). The key idea of the POMDP is that the underlying dialogue state is hidden and dialogue management policies must therefore be based not on a single state estimate but on a distribution over all states. Whilst POMDPs are attractive theoretically, in practice, they are notoriously intractable for anything other than small state/action spaces. Hence, practical examples of their use were initially restricted to very simple domains (Roy et al., 2000; Zhang et al., 2001). More recently, however, a number of techniques have been suggested which do allow POMDPs to be scaled to handle real world tas</context>
<context position="3757" citStr="Williams and Young, 2007" startWordPosition="586" endWordPosition="589">. The resulting system is then modelled as a dynamic Bayesian network (Thomson et al., 2008). A similar approach is also developed in 112 Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 112–119, Columbus, June 2008. c�2008 Association for Computational Linguistics (Bui et al., 2007a; Bui et al., 2007b). An alternative approach taken in the Hidden Information State (HIS) system is to retain a complete representation of the user’s goal, but partition states into equivalence classes and prune away very low probability partitions (Young et al., 2007; Thomson et al., 2007; Williams and Young, 2007b). Whichever approach is taken, a key issue in a real POMDP-based dialogue system is its ability to be robust to noise and that is the issue that is addressed in this paper. Using the HIS system as an exemplar, evaluation results are presented for a real-world tourist information task using both simulated and real users. The results show that a POMDP system can learn noise robust policies and that N-best outputs from the speech understanding component can be exploited to further improve robustness. The paper is structured as follows. Firstly, in Section 2 a brief overview of the HIS system is</context>
</contexts>
<marker>Williams, Young, 2007</marker>
<rawString>JD Williams and SJ Young. 2007a. Partially Observable Markov Decision Processes for Spoken Dialog Systems. Computer Speech and Language, 21(2):393– 422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>JD Williams</author>
<author>SJ Young</author>
</authors>
<title>Scaling POMDPs for Spoken Dialog Management.</title>
<date>2007</date>
<journal>IEEE Audio, Speech and Language Processing,</journal>
<volume>15</volume>
<issue>7</issue>
<contexts>
<context position="2014" citStr="Williams and Young, 2007" startWordPosition="307" endWordPosition="310">ures. Such a system has no explicit mechanisms for representing the inevitable uncertainties associated with speech understanding or the ambiguities which naturally arise in interpreting a user’s intentions. The result is a system that is inherently fragile, especially in noisy conditions or where the user is unsure of how to use the system. It has been suggested that Partially Observable Markov Decision Processes (POMDPs) offer a natural framework for building spoken dialogue systems which can both model these uncertainties and support policies which are robust to their effects (Young, 2002; Williams and Young, 2007a). The key idea of the POMDP is that the underlying dialogue state is hidden and dialogue management policies must therefore be based not on a single state estimate but on a distribution over all states. Whilst POMDPs are attractive theoretically, in practice, they are notoriously intractable for anything other than small state/action spaces. Hence, practical examples of their use were initially restricted to very simple domains (Roy et al., 2000; Zhang et al., 2001). More recently, however, a number of techniques have been suggested which do allow POMDPs to be scaled to handle real world tas</context>
<context position="3757" citStr="Williams and Young, 2007" startWordPosition="586" endWordPosition="589">. The resulting system is then modelled as a dynamic Bayesian network (Thomson et al., 2008). A similar approach is also developed in 112 Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 112–119, Columbus, June 2008. c�2008 Association for Computational Linguistics (Bui et al., 2007a; Bui et al., 2007b). An alternative approach taken in the Hidden Information State (HIS) system is to retain a complete representation of the user’s goal, but partition states into equivalence classes and prune away very low probability partitions (Young et al., 2007; Thomson et al., 2007; Williams and Young, 2007b). Whichever approach is taken, a key issue in a real POMDP-based dialogue system is its ability to be robust to noise and that is the issue that is addressed in this paper. Using the HIS system as an exemplar, evaluation results are presented for a real-world tourist information task using both simulated and real users. The results show that a POMDP system can learn noise robust policies and that N-best outputs from the speech understanding component can be exploited to further improve robustness. The paper is structured as follows. Firstly, in Section 2 a brief overview of the HIS system is</context>
</contexts>
<marker>Williams, Young, 2007</marker>
<rawString>JD Williams and SJ Young. 2007b. Scaling POMDPs for Spoken Dialog Management. IEEE Audio, Speech and Language Processing, 15(7):2116–2129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>SJ Young</author>
<author>J Schatzmann</author>
<author>K Weilhammer</author>
<author>H Ye</author>
</authors>
<title>The Hidden Information State Approach to Dialog Management.</title>
<date>2007</date>
<booktitle>In ICASSP 2007,</booktitle>
<location>Honolulu, Hawaii.</location>
<contexts>
<context position="3709" citStr="Young et al., 2007" startWordPosition="578" endWordPosition="581">tored into conditionally independent slots. The resulting system is then modelled as a dynamic Bayesian network (Thomson et al., 2008). A similar approach is also developed in 112 Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 112–119, Columbus, June 2008. c�2008 Association for Computational Linguistics (Bui et al., 2007a; Bui et al., 2007b). An alternative approach taken in the Hidden Information State (HIS) system is to retain a complete representation of the user’s goal, but partition states into equivalence classes and prune away very low probability partitions (Young et al., 2007; Thomson et al., 2007; Williams and Young, 2007b). Whichever approach is taken, a key issue in a real POMDP-based dialogue system is its ability to be robust to noise and that is the issue that is addressed in this paper. Using the HIS system as an exemplar, evaluation results are presented for a real-world tourist information task using both simulated and real users. The results show that a POMDP system can learn noise robust policies and that N-best outputs from the speech understanding component can be exploited to further improve robustness. The paper is structured as follows. Firstly, in</context>
<context position="6760" citStr="Young et al., 2007" startWordPosition="1110" endWordPosition="1113">es the discounted sum R of the rewards Atr(bt, am,t) (2) where At is a discount coefficient. 2.2 Probability Models In the HIS system, user goals are partitioned and initially, all states s,, E S,, are regarded as being equally likely and they are placed in a single partition p0. As the dialogue progresses, user inputs result in changing beliefs and this root partition is repeatedly split into smaller partitions. This splitting is binary, i.e. p —* {p′, p − p′} with probability P (p′|p). By replacing sm by its factors (s, a, sd) and making reasonable independence assumptions, it can be shown (Young et al., 2007) that in partia�m� ~� aj User❑ Speech❑ Understanding[] s�, Speech❑ Generation❑ a�u� 1� �t a&amp;, ap, Belief❑ Estimator❑ Dialog[] Policy[] b( )❑s�m� sw, 00 E t=0 R = 113 tioned form (1) becomes b′(p′, a′&apos;, s′d) = k · P(o′|a′&apos;) |{z } observation model P(s′d|p′, a′ &apos;, sd, am) |{z } dialogue model where p is the parent of p′. In this equation, the observation model is approximated by the normalised distribution of confidence measures output by the speech recognition system. The user action model allows the observation probability that is conditioned on a′&apos; to be scaled by the probability that the use</context>
</contexts>
<marker>Young, Schatzmann, Weilhammer, Ye, 2007</marker>
<rawString>SJ Young, J Schatzmann, K Weilhammer, and H Ye. 2007. The Hidden Information State Approach to Dialog Management. In ICASSP 2007, Honolulu, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>SJ Young</author>
</authors>
<title>Talking to Machines (Statistically Speaking).</title>
<date>2002</date>
<booktitle>In Int Conf Spoken Language Processing,</booktitle>
<location>Denver, Colorado.</location>
<contexts>
<context position="1988" citStr="Young, 2002" startWordPosition="305" endWordPosition="306">covery procedures. Such a system has no explicit mechanisms for representing the inevitable uncertainties associated with speech understanding or the ambiguities which naturally arise in interpreting a user’s intentions. The result is a system that is inherently fragile, especially in noisy conditions or where the user is unsure of how to use the system. It has been suggested that Partially Observable Markov Decision Processes (POMDPs) offer a natural framework for building spoken dialogue systems which can both model these uncertainties and support policies which are robust to their effects (Young, 2002; Williams and Young, 2007a). The key idea of the POMDP is that the underlying dialogue state is hidden and dialogue management policies must therefore be based not on a single state estimate but on a distribution over all states. Whilst POMDPs are attractive theoretically, in practice, they are notoriously intractable for anything other than small state/action spaces. Hence, practical examples of their use were initially restricted to very simple domains (Roy et al., 2000; Zhang et al., 2001). More recently, however, a number of techniques have been suggested which do allow POMDPs to be scale</context>
</contexts>
<marker>Young, 2002</marker>
<rawString>SJ Young. 2002. Talking to Machines (Statistically Speaking). In Int Conf Spoken Language Processing, Denver, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Zhang</author>
<author>Q Cai</author>
<author>J Mao</author>
<author>E Chang</author>
<author>B Guo</author>
</authors>
<date>2001</date>
<booktitle>Spoken Dialogue Management as Planning and Acting under Uncertainty. In Eurospeech,</booktitle>
<location>Aalborg, Denmark.</location>
<contexts>
<context position="2486" citStr="Zhang et al., 2001" startWordPosition="383" endWordPosition="386">ogue systems which can both model these uncertainties and support policies which are robust to their effects (Young, 2002; Williams and Young, 2007a). The key idea of the POMDP is that the underlying dialogue state is hidden and dialogue management policies must therefore be based not on a single state estimate but on a distribution over all states. Whilst POMDPs are attractive theoretically, in practice, they are notoriously intractable for anything other than small state/action spaces. Hence, practical examples of their use were initially restricted to very simple domains (Roy et al., 2000; Zhang et al., 2001). More recently, however, a number of techniques have been suggested which do allow POMDPs to be scaled to handle real world tasks. The two generic mechanisms which facilitate this scaling are factoring the state space and performing policy optimisation in a reduced summary state space (Williams and Young, 2007a; Williams and Young, 2007b). Based on these ideas, a number of real-world POMDP-based systems have recently emerged. The most complex entity which must be represented in the state space is the user’s goal. In the Bayesian Update ofDialogue State (BUDS) system, the user’s goal is furthe</context>
</contexts>
<marker>Zhang, Cai, Mao, Chang, Guo, 2001</marker>
<rawString>B Zhang, Q Cai, J Mao, E Chang, and B Guo. 2001. Spoken Dialogue Management as Planning and Acting under Uncertainty. In Eurospeech, Aalborg, Denmark.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>