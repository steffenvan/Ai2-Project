<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000011">
<note confidence="0.579889">
Similarity-Based Estimation of Word Cooccurrence
</note>
<title confidence="0.898083">
Probabilities
</title>
<author confidence="0.917316">
Ido Dagan Fernando Pereira
</author>
<affiliation confidence="0.786013">
AT&amp;T Bell Laboratories
</affiliation>
<address confidence="0.855566">
600 Mountain Ave.
Murray Hill, NJ 07974, USA
</address>
<email confidence="0.658467">
daganOresearch.att.com
pereiraOresearch.att.com
</email>
<sectionHeader confidence="0.994027" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999831380952381">
In many applications of natural language processing it
is necessary to determine the likelihood of a given word
combination. For example, a speech recognizer may
need to determine which of the two word combinations
&amp;quot;eat a peach&amp;quot; and &amp;quot;eat a beach&amp;quot; is more likely. Statis-
tical NLP methods determine the likelihood of a word
combination according to its frequency in a training cor-
pus. However, the nature of language is such that many
word combinations are infrequent and do not occur in a
given corpus. In this work we propose a method for es-
timating the probability of such previously unseen word
combinations using available information on &amp;quot;most sim-
ilar&amp;quot; words.
We describe a probabilistic word association model
based on distributional word similarity, and apply it
to improving probability estimates for unseen word bi-
grams in a variant of Katz&apos;s back-off model. The
similarity-based method yields a 20% perplexity im-
provement in the prediction of unseen bigrams and sta-
tistically significant reductions in speech-recognition er-
ror.
</bodyText>
<sectionHeader confidence="0.979058" genericHeader="introduction">
Introduction
</sectionHeader>
<bodyText confidence="0.999817894736842">
Data sparseness is an inherent problem in statistical
methods for natural language processing. Such meth-
ods use statistics on the relative frequencies of config-
urations of elements in a training corpus to evaluate
alternative analyses or interpretations of new samples
of text or speech. The most likely analysis will be taken
to be the one that contains the most frequent config-
urations. The problem of data sparseness arises when
analyses contain configurations that never occurred in
the training corpus. Then it is not possible to estimate
probabilities from observed frequencies, and some other
estimation scheme has to be used.
We focus here on a particular kind of configuration,
word cooccurrence. Examples of such cooccurrences
include relationships between head words in syntactic
constructions (verb-object or adjective-noun, for exam-
ple) and word sequences (n-grams). In commonly used
models, the probability estimate for a previously un-
seen cooccurrence is a function of the probability esti-
</bodyText>
<author confidence="0.672352">
Lillian Lee
</author>
<affiliation confidence="0.9747555">
Division of Applied Sciences
Harvard University
</affiliation>
<address confidence="0.726355">
33 Oxford St. Cambridge MA 02138, USA
</address>
<email confidence="0.808499">
lleadas.harvard.edu
</email>
<bodyText confidence="0.999781930232558">
mates for the words in the cooccurrence. For example,
in the bigram models that we study here, the probabil-
ity P(w2lwi) of a conditioned word w2 that has never
occurred in training following the conditioning word w1
is calculated from the probability of w2, as estimated
by w2&apos;s frequency in the corpus (Jelinek, Mercer, and
Roukos, 1992; Katz, 1987). This method depends on
an independence assumption on the cooccurrence of w1
and w2: the more frequent w2 is, the higher will be the
estimate of P(w2lwi), regardless of
Class-based and similarity-based models provide an
alternative to the independence assumption. In those
models, the relationship between given words is mod-
ele.d by analogy with other words that are in some sense
similar to the given ones.
Brown et al. (1992) suggest a class-based n-gram
model in which words with similar cooccurrence distri-
butions are clustered in word classes. The cooccurrence
probability of a given pair of words then is estimated ac-
cording to an averaged cooccurrence probability of the
two corresponding classes. Pereira, Tishby, and Lee
(1993) propose a &amp;quot;soft&amp;quot; clustering scheme for certain
grammatical cooccurrences in which membership of a
word in a class is probabilistic. Cooccurrence probabil-
ities of words are then modeled by averaged cooccur-
rence probabilities of word clusters.
Dagan, Markus, and Markovitch (1993) argue that
reduction to a relatively small number of predetermined
word classes or clusters may cause a substantial loss of
information. Their similarity-based model avoids clus-
tering altogether. Instead, each word is modeled by its
own specific class, a set of words which are most simi-
lar to it (as in k-nearest neighbor approaches in pattern
recognition). Using this scheme, they predict which
unobserved cooccurrences are more likely than others.
Their model, however, is not probabilistic, that is, it
does not provide a probability estimate for unobserved
cooccurrences. It cannot therefore be used in a com-
plete probabilistic framework, such as n-gram language
models or probabilistic lexicalized grammars (Schabes,
1992; Lafferty, Sleator, and Temperley, 1992).
We now give a similarity-based method for estimating
the probabilities of cooccurrences unseen in training.
</bodyText>
<page confidence="0.994226">
272
</page>
<bodyText confidence="0.999986259259259">
Similarity-based estimation was first used for language
modeling in the cooccurrence smoothing method of Es-
sen and Steinbiss (1992), derived from work on acous-
tic model smoothing by Sugawara et al. (1985). We
present a different method that takes as starting point
the back-off scheme of Katz (1987). We first allocate an
appropriate probability mass for unseen cooccurrences
following the back-off method. Then we redistribute
that mass to unseen cooccurrences according to an av-
eraged cooccurrence distribution of a set of most similar
conditioning words, using relative entropy as our sim-
ilarity measure. This second step replaces the use of
the independence assumption in the original back-off
model.
We applied our method to estimate unseen bigram
probabilities for Wall Street Journal text and compared
it to the standard back-off model. Testing on a held-out
sample, the similarity model achieved a 20% reduction
in perplexity for unseen bigrams. These constituted
just 10.6% of the test sample, leading to an overall re-
duction in test-set perplexity of 2.4%. We also exper-
imented with an application to language modeling for
speech recognition, which yielded a statistically signifi-
cant reduction in recognition error.
The remainder of the discussion is presented in terms
of bigrams, but it is valid for other types of word cooc-
currence as well.
</bodyText>
<subsectionHeader confidence="0.504846">
Discounting and Redistribution
</subsectionHeader>
<bodyText confidence="0.941790375">
Many low-probability bigrams will be missing from any
finite sample. Yet, the aggregate probability of all these
unseen bigrams is fairly high; any new sample is very
likely to contain some.
Because of data sparseness, we cannot reliably use a
maximum likelihood estimator (MLE) for bigram prob-
abilities. The MLE for the probability of a bigram
(w1, w2) is simply:
</bodyText>
<equation confidence="0.9992135">
c(wi,w2)
PAIL(tvi,w2)=- (1)
</equation>
<bodyText confidence="0.983031666666667">
where c(wi , w2) is the frequency of (wi, w2) in the train-
ing corpus and N is the total number of bigrams. How-
ever, this estimates the probability of any unseen bi-
gram to be zero, which is clearly undesirable.
Previous proposals to circumvent the above problem
(Good, 1953; Jelinek, Mercer, and Roukos, 1992; Katz,
1987; Church and Gale, 1991) take the MLE as an ini-
tial estimate and adjust it so that the total probability
of seen bigrams is less than one, leaving some probabil-
ity mass for unseen bigrams. Typically, the adjustment
involves either interpolation, in which the new estimator
is a weighted combination of the MLE and an estimator
that is guaranteed to be nonzero for unseen bigrams, or
discounting, in which the MLE is decreased according to
a model of the unreliability of small frequency counts,
leaving some probability mass for unseen bigrams.
The back-off model of Katz (1987) provides a clear
separation between frequent events, for which observed
frequencies are reliable probability estimators, and low-
frequency events, whose prediction must involve addi-
tional information sources. In addition, the back-off
model does not require complex estimations for inter-
polation parameters.
A back-off model requires methods for (a) discounting
the estimates of previously observed events to leave out
some positive probability mass for unseen events, and
(b) redistributing among the unseen events the probabil-
ity mass freed by discounting. For bigrams the resulting
estimator has the general form
if e(wi , w2) &gt;0 , (2)
a(w1)Pr(w2lw1) otherwise
where Pd represents the discounted estimate for seen
bigrams, Pr the model for probability redistribution
among the unseen bigrams, and a(w) is a normalization
factor. Since the overall mass left for unseen bigrams
starting with w1 is given by
</bodyText>
<equation confidence="0.982086285714286">
Pd(w21/01)
W 2:441.11 ,W 2)›. 0
the normalization factor required to ensure
W2 P(wdwo = 1 is
a(wi) =
Etv2:c
1 — c(o,,.2)&gt;0 Pr(w2lw1)
</equation>
<bodyText confidence="0.9992575">
The second formulation of the normalization is compu-
tationally preferable because the total number of pos-
sible bigram types far exceeds the number of observed
types. Equation (2) modifies slightly Katz&apos;s presenta-
tion to include the placeholder Pr for alternative models
of the distribution of unseen bigrams.
Katz uses the Good-Turing formula to replace the
actual frequency c(wi , w2) of a bigram (or an event, in
general) with a discounted frequency, c*(wi, w2), de-
fined by
</bodyText>
<equation confidence="0.987519">
cn (wi,w2)+1
c*(wi, w2) = (c(wi, w2) + 1) (3)
nc(w1,e.2)
</equation>
<bodyText confidence="0.99645125">
where ?lc is the number of different bigrams in the cor-
pus that have frequency c. He then uses the discounted
frequency in the conditional probability calculation for
a bigram:
</bodyText>
<equation confidence="0.975933333333333">
c*(wi, w2)
Pd(w21w1) = (4)
e(wi)
</equation>
<bodyText confidence="0.999924666666667">
In the original Good-Turing method (Good, 1953)
the free probability mass is redistributed uniformly
among all unseen events. Instead, Katz&apos;s back-off
scheme redistributes the free probability mass non-
uniformly in proportion to the frequency of w2, by set-
ting
</bodyText>
<equation confidence="0.999567">
Pr(w21w1) = P(w2) (5)
p(tv2iw1) = {Pd(w2lwi)
wi,.2)=0 Pr(w2 wi)
13(w1)
</equation>
<page confidence="0.989902">
273
</page>
<bodyText confidence="0.99998275">
Katz thus assumes that for a given conditioning word
w1 the probability of an unseen following word w2 is
proportional to its unconditional probability. However,
the overall form of the model (2) does not depend on
this assumption, and we will next investigate an esti-
mate for Pr (W2 I WI) derived by averaging estimates for
the conditional probabilities that w2 follows words that
are distributionally similar to wi.
</bodyText>
<subsectionHeader confidence="0.911957">
The Similarity Model
</subsectionHeader>
<bodyText confidence="0.999985">
Our scheme is based on the assumption that words that
are &amp;quot;similar&amp;quot; to w1 can provide good predictions for
the distribution of w1 in unseen bigrams Let S(wi)
denote a set of words which are most similar to 11)1,
as determined by some similarity metric. We define
Psim (w2 wi), the similarity-based model for the condi-
tional distribution of wi, as a weighted average of the
conditional distributions of the words in S(wi):
</bodyText>
<equation confidence="0.9963155">
Psim (w2 wi ) =
E„ olEs(w P(w21114)7 , (6)
</equation>
<bodyText confidence="0.999943">
where W(wl, wi) is the (unnormalized) weight given to
wc, determined by its degree of similarity to w1. Ac-
cording to this scheme, w2 is more likely to follow w1 if
it tends to follow words that are most similar to w1. To
complete the scheme, it is necessary to define the simi-
larity metric and, accordingly, S(wi) and W(wc., wi).
Following Pereira, Tishby, and Lee (1993), we
measure word similarity by the relative entropy, or
Kullback-Leibler (KL) distance, between the corre-
sponding conditional distributions
</bodyText>
<equation confidence="0.984506333333333">
Dcwi 11 wio P(w2lwi) log (P wzIwi)
P(wzIwp (7)
w2
</equation>
<bodyText confidence="0.999668285714286">
The KL distance is 0 when w1 = OD and it increases
as the two distribution are less similar.
To compute (6) and (7) we must have nonzero esti-
mates of P(w2lwi) whenever necessary for (7) to be de-
fined. We use the estimates given by the standard back-
off model, which satisfy that requirement. Thus our
application of the similarity model averages together
standard back-off estimates for a set of similar condi-
tioning words.
We define S(wi) as the set of at most k nearest
words to iv]. (excluding w1 itself), that also satisfy
D(wi &lt; t. k and t are parameters that control
the contents of S(wi) and are tuned experimentally, as
we will see below.
</bodyText>
<equation confidence="0.731281">
117(wc., wi) is defined as
wl) = exp —0D(wi II wi) -
</equation>
<bodyText confidence="0.936010357142857">
The weight is larger for words that are more similar
(closer) to wi. The parameter controls the relative
contribution of words in different distances from wi: as
the value of # increases, the nearest words to wi get rel-
atively more weight. As 13 decreases, remote words get
a larger effect. Like k and t, [3 is tuned experimentally.
Having a definition for Psim(w2lwi), we could use it
directly asPr(w in the back-off scheme (2). We
found that it isb2lw
better r to smooth Psim(w21w1) by inter-
polating it with the unigram probability P(w2) (recall
P(w2) as Pr(wziwi))•
that Katz used ( Using linear in-
terpolation we get
</bodyText>
<equation confidence="0.999558">
Pr(w21w1) = 7P(w2) + (1 — 7)Psim(w21wi) , (8)
</equation>
<bodyText confidence="0.999983470588235">
where &apos;y is an experimentally-determined interpolation
parameter. This smoothing appears to compensate
for inaccuracies in Psim(w21101), mainly for infrequent
conditioning words. However, as the evaluation be-
low shows, good values for -y are small, that is, the
similarity-based model plays a stronger role than the
independence assumption.
To summarize, we construct a similarity-based model
for P(w2lwi) and then interpolate it with P(w2). The
interpolated model (8) is used in the back-off scheme
as Pr(w2lwi), to obtain better estimates for unseen bi-
grams. Four parameters, to be tuned experimentally,
are relevant for this process: k and t, which determine
the set of similar words to be considered, 13, which deter-
mines the relative effect of these words, and 7, which de-
termines the overall importance of the similarity-based
model.
</bodyText>
<sectionHeader confidence="0.602416" genericHeader="evaluation">
Evaluation
</sectionHeader>
<bodyText confidence="0.998577714285714">
We evaluated our method by comparing its perplexity&apos;
and effect on speech-recognition accuracy with the base-
line bigram back-off model developed by MIT Lincoln
Laboratories for the Wall Street Journal (WSJ) text
and dictation corpora provided by ARPA&apos;s HLT pro-
gram (Paul, 1991).2 The baseline back-off model follows
closely the Katz design, except that for compactness all
frequency one bigrams are ignored. The counts used in
this model and in ours were obtained from 40.5 million
words of WSJ text from the years 1987-89.
For perplexity evaluation, we tuned the similarity
model parameters by minimizing perplexity on an ad-
ditional sample of 57.5 thousand words of WSJ text,
drawn from the ARPA HLT development test set. The
best parameter values found were k = 60, t = 2.5, # = 4
and -y = 0.15. For these values, the improvement in
perplexity for unseen bigrams in a held-out 18 thou-
sand word sample, in which 10.6% of the bigrams are
unseen, is just over 20%. This improvement on unseen
&apos;Theo, perplexity of a conditional bigram probability
model P with respect to the true bigram distribution is
an information-theoretic measure of model quality (Jelinek.
Mercer, and Roukos, 1992) that can be empirically esti-
mated by exp — k E, log P(w,Ity,_1) for a test set of length
N. Intuitively, the lower the perplexity of a model the more
likely the model is to assign high probability to bigrams that
actually occur. In our task, lower perplexity will indicate
better prediction of unseen bigrams.
</bodyText>
<footnote confidence="0.962488">
2The ARPA WSJ development corpora come in two ver-
sions, one with verbalized punctuation and the other with-
out. We used the latter in all our experiments.
</footnote>
<page confidence="0.96879">
274
</page>
<table confidence="0.999113">
k t # training reduction (%) test reduction (%)
60 2.5 4 0.15 18.4 20.51
50 2.5 4 0.15 18.38 20.45
40 2.5 4 0.2 18.34 20.03
30 2.5 4 0.25 18.33 19.76
70 2.5 4 0.1 18.3 20.53
80 2.5 4.5 0.1 18.25 20.55
100 2.5 4.5 0.1 18.23 20.54
90 2.5 4.5 0.1 18.23 20.59
20 1.5 4 0.3 18.04 18.7
10 1.5 3.5 0.3 16.64 16.94
</table>
<tableCaption confidence="0.999851">
Table 1: Perplexity Reduction on Unseen Bigrams for Different Model Parameters
</tableCaption>
<bodyText confidence="0.9999257">
bigrams corresponds to an overall test set perplexity
improvement of 2.4% (from 237.4 to 231.7). Table 1
shows reductions in training and test perplexity, sorted
by training reduction, for different choices in the num-
ber k of closest neighbors used. The values of 7 and
I are the best ones found for each k.3
From equation (6), it is clear that the computational
cost of applying the similarity model to an unseen bi-
gram is 0(k). Therefore, lower values for k (and also
for t) are computationally preferable. From the table,
we can see that reducing k to 30 incurs a penalty of less
than 1% in the perplexity improvement, so relatively
low values of k appear to be sufficient to achieve most
of the benefit of the similarity model. As the table also
shows, the best value of 7 increases as k decreases, that
is, for lower k a greater weight is given to the condi-
tioned word&apos;s frequency. This suggests that the predic-
tive power of neighbors beyond the closest 30 or so can
be modeled fairly well by the overall frequency of the
conditioned word.
The bigram similarity model was also tested as a lan-
guage model in speech recognition. The test data for
this experiment were pruned word lattices for 403 WSJ
closed-vocabulary test sentences. Arc scores in those
lattices are sums of an acoustic score (negative log like-
lihood) and a language-model score, in this case the
negative log probability provided by the baseline bi-
gram model.
From the given lattices, we constructed new lattices
in which the arc scores were modified to use the similar-
ity model instead of the baseline model. We compared
the best sentence hypothesis in each original lattice and
in the modified one, and counted the word disagree-
ments in which one of the hypotheses is correct. There
were a total of 96 such disagreements. The similarity
model was correct in 64 cases, and the back-off model in
32. This advantage for the similarity model is statisti-
cally significant at the 0.01 level. The overall reduction
in error rate is small, from 21.4% to 20.9%, because
the number of disagreements is small compared with
</bodyText>
<figureCaption confidence="0.507251">
3Values of /3 and t refer to base 10 logarithms and expo-
nentials in all calculations.
</figureCaption>
<bodyText confidence="0.991835333333333">
the overall number of errors in our current recognition
setup.
Table 2 shows some examples of speech recognition
disagreements between the two models. The hypotheses
are labeled &apos;B&apos; for back-off and &apos;S&apos; for similarity, and the
bold-face words are errors. The similarity model seems
to be able to model better regularities such as semantic
parallelism in lists and avoiding a past tense form after
&amp;quot;to.&amp;quot; On the other hand, the similarity model makes
several mistakes in which a function word is inserted in
a place where punctuation would be found in written
text.
</bodyText>
<subsectionHeader confidence="0.788498">
Related Work
</subsectionHeader>
<bodyText confidence="0.999991">
The cooccurrence smoothing technique (Essen and
Steinbiss, 1992), based on earlier stochastic speech
modeling work by Sugawara et al. (1985), is the main
previous attempt to use similarity to estimate the prob-
ability of unseen events in language modeling. In addi-
tion to its original use in language modeling for speech
recognition, Grishman and Sterling (1993) applied the
cooccurrence smoothing technique to estimate the like-
lihood of selectional patterns. We will outline here
the main parallels and differences between our method
and cooccurrence smoothing. A more detailed analy-
sis would require an empirical comparison of the two
methods on the same corpus and task.
In cooccurrence smoothing, as in our method, a base-
line model is combined with a similarity-based model
that refines some of its probability estimates. The sim-
ilarity model in cooccurrence smoothing is based on
the intuition that the similarity between two words w
and w&apos; can be measured by the confusion probability
Pc(w&apos;lw) that w&apos; can be substituted for w in an arbi-
trary context in the training corpus. Given a baseline
probability model P, which is taken to be the MLE, the
confusion probability Pc (wc. I wi) between conditioning
words w and w1 is defined as
</bodyText>
<equation confidence="0.9464975">
Pc(wilwl) =
P(wi I w2)P(wi w2)P(w2) &apos; (9
P(wi) E )
w2
</equation>
<bodyText confidence="0.9578715">
the probability that w1 is followed by the same context
words as w&apos;1. Then the bigram estimate derived by
</bodyText>
<page confidence="0.994486">
275
</page>
<figure confidence="0.7750206">
1
B commitments ...from leaders felt the three point six billion dollars
S commitments ...from leaders fell to three point six billion dollars
followed by France the US agreed in Italy
followed by France the US Greece ...Italy
</figure>
<bodyText confidence="0.72367">
he whispers to made a
he whispers to an aide
the necessity for change exist
the necessity for change exists
without ...additional reserves Centrust would have reported
without ...additional reserves of Centrust would have reported
</bodyText>
<figure confidence="0.774312">
S in the darkness passed the church
B in the darkness past the church 1
</figure>
<tableCaption confidence="0.922876">
Table 2: Speech Recognition Disagreements between Models
</tableCaption>
<equation confidence="0.9076665">
cooccurrence smoothing is given by
Ps(wzIwi) = EP(w2114)Pc(wliwi)
</equation>
<bodyText confidence="0.995213714285714">
Notice that this formula has the same form as our sim-
ilarity model (6), except that it uses confusion proba-
bilities where we use normalized weights.4 In addition,
we restrict the summation to sufficiently similar words,
whereas the cooccurrence smoothing method sums over
all words in the lexicon.
The similarity measure (9) is symmetric in the sense
that Pc(wilw) and Pc (w 0) are identical up to fre-
quency normalization, that is pPccZtvlw,)) = pP(V). In
contrast, D(w II w&apos;) (7) is asymmetric in that it weighs
each context in proportion to its probability of occur-
rence with w, but not with w&apos;. In this way, if w and
w&apos; have comparable frequencies but w&apos; has a sharper
context distribution than w, then D(w&apos; II w) is greater
than D(w w&apos;). Therefore, in our similarity model
w&apos; will play a stronger role in estimating w than vice
versa. These properties motivated our choice of relative
entropy for similarity measure, because of the intuition
that words with sharper distributions are more infor-
mative about other words than words with flat distri-
butions.
4This presentation corresponds to model 2-8 in Essen
and Steinbiss (1992). Their presentation follows the equiv-
alent model 1-A, which averages over similar conditioned
words, with the similarity defined with the preceding word
as context. In fact, these equivalent models are symmetric
in their treatment of conditioning and conditioned word, as
they can both be rewritten as
</bodyText>
<equation confidence="0.913002">
p.,(.21wo= E P(w21.1)P(.11w)P(wiwi) .
</equation>
<bodyText confidence="0.9997072">
They also consider other definitions of confusion probabil-
ity and smoothed probability estimate, but the one above
yielded the best experimental results.
Finally, while we have used our similarity model only
for missing bigrams in a back-off scheme, Essen and
Steinbiss (1992) used linear interpolation for all bi-
grams to combine the cooccurrence smoothing model
with MLE models of bigrams and unigrams. Notice,
however, that the choice of back-off or interpolation is
independent from the similarity model used.
</bodyText>
<subsectionHeader confidence="0.984687">
Further Research
</subsectionHeader>
<bodyText confidence="0.999965967741936">
Our model provides a basic scheme for probabilistic
similarity-based estimation that can be developed in
several directions. First, variations of (6) may be tried,
such as different similarity metrics and different weight-
ing schemes. Also, some simplification of the current
model parameters may be possible, especially with re-
spect to the parameters t and k used to select the near-
est neighbors of a word. A more substantial variation
would be to base the model on similarity between con-
ditioned words rather than on similarity between con-
ditioning words.
Other evidence may be combined with the similarity-
based estimate. For instance, it may be advantageous
to weigh those estimates by some measure of the re-
liability of the similarity metric and of the neighbor
distributions. A second possibility is to take into ac-
count negative evidence: if wi is frequent, but w2 never
followed it, there may be enough statistical evidence
to put an upper bound on the estimate of P(w2
This may require an adjustment of the similarity based
estimate, possibly along the lines of (Rosenfeld and
Huang, 1992). Third, the similarity-based estimate can
be used to smooth the maximum likelihood estimate
for small nonzero frequencies. If the similarity-based
estimate is relatively high, a bigram would receive a
higher estimate than predicted by the uniform discount-
ing method.
Finally, the similarity-based model may be applied
to configurations other than bigrams. For trigrams,
it is necessary to measure similarity between differ-
ent conditioning bigrams. This can be done directly,
</bodyText>
<page confidence="0.99233">
276
</page>
<bodyText confidence="0.999944846153846">
by measuring the distance between distributions of the
form P(w3lwi, w2), corresponding to different bigrams
(wi, w2). Alternatively, and more practically, it would
be possible to define a similarity measure between bi-
grams as a function of similarities between correspond-
ing words in them. Other types of conditional cooccur-
rence probabilities have been used in probabilistic pars-
ing (Black et al., 1993). If the configuration in question
includes only two words, such as P(objectiverb), then it
is possible to use the model we have used for bigrams.
If the configuration includes more elements, it is nec-
essary to adjust the method, along the lines discussed
above for trigrams.
</bodyText>
<sectionHeader confidence="0.554521" genericHeader="conclusions">
Conclusions
</sectionHeader>
<bodyText confidence="0.999992685714286">
Similarity-based models suggest an appealing approach
for dealing with data sparseness. Based on corpus
statistics, they provide analogies between words that of-
ten agree with our linguistic and domain intuitions. In
this paper we presented a new model that implements
the similarity-based approach to provide estimates for
the conditional probabilities of unseen word cooccur-
rences.
Our method combines similarity-based estimates
with Katz&apos;s back-off scheme, which is widely used for
language modeling in speech recognition. Although the
scheme was originally proposed as a preferred way of
implementing the independence assumption, we suggest
that it is also appropriate for implementing similarity-
based models, as well as class-based models. It enables
us to rely on direct maximum likelihood estimates when
reliable statistics are available, and only otherwise re-
sort to the estimates of an &amp;quot;indirect&amp;quot; model.
The improvement we achieved for a bigram model is
statistically significant, though modest in its overall ef-
fect because of the small proportion of unseen events.
While we have used bigrams as an easily-accessible plat-
form to develop and test the model, more substantial
improvements might be obtainable for more informa-
tive configurations. An obvious case is that of tri-
grams, for which the sparse data problem is much more
severe.&apos; Our longer-term goal, however, is to apply
similarity techniques to linguistically motivated word
cooccurrence configurations, as suggested by lexical-
ized approaches to parsing (Schabes, 1992; Lafferty,
Sleator, and Temperley, 1992). In configurations like
verb-object and adjective-noun, there is some evidence
(Pereira, Tishby, and Lee, 1993) that sharper word
cooccurrence distributions are obtainable, leading to
improved predictions by similarity techniques.
</bodyText>
<sectionHeader confidence="0.997698" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.891691375">
We thank Slava Katz for discussions on the topic of this
paper, Doug McIlroy for detailed comments, Doug Paul
&apos;For WSJ trigrams, only 58.6% of test set trigrams
occur in 40M of words of training (Doug Paul, personal
communication).
for help with his baseline back-off model, and Andre
Ljolje and Michael Riley for providing the word lattices
for our experiments.
</bodyText>
<sectionHeader confidence="0.988056" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9743416875">
Black, Ezra, Fred Jelinek, John Lafferty, David M.
Magerman, David Mercer, and Salim Roukos. 1993.
Towards history-based grammars: Using richer mod-
els for probabilistic parsing. In 30th Annual Meet-
ing of the Association for Computational Linguistics,
pages 31-37, Columbus, Ohio. Ohio State University,
Association for Computational Linguistics, Morris-
town, New Jersey.
Brown, Peter F., Vincent J. Della Pietra, Peter V.
deSouza, Jenifer C. Lai, and Robert L. Mercer.
1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics, 18(4):467-479.
Church, Kenneth W. and William A. Gale. 1991. A
comparison of the enhanced Good-Turing and deleted
estimation methods for estimating probabilities of
English bigrams. Computer Speech and Language,
5:19-54.
Dagan, Ido, Shaul Markus, and Shaul Markovitch.
1993. Contextual word similarity and estimation
from sparse data. In 30th Annual Meeting of the As-
sociation for Computational Linguistics, pages 164-
171, Columbus, Ohio. Ohio State University, Asso-
ciation for Computational Linguistics, Morristown,
New Jersey.
Essen, Ute and Volker Steinbiss. 1992. Coocurrence
smoothing for stochastic language modeling. In Pro-
ceedings of ICASSP, volume I, pages 161-164. IEEE.
Good, I. J. 1953. The population frequencies of
species and the estimation of population parameters.
Biometrika, 40(3):237-264.
Grishman, Ralph and John Sterling. 1993. Smoothing
of automatically generated selectional constraints. In
Human Language Technology, pages 254-259, San
Francisco, California. Advanced Research Projects
Agency, Software and Intelligent Systems Technology
Office, Morgan Kaufmann.
Jelinek, Frederick, Robert L. Mercer, and Salim
Roukos. 1992. Principles of lexical language mod-
eling for speech recognition. In Sadaoki Furui and
M. Mohan Sondhi, editors, Advances in Speech Sig-
nal Processing. Mercer Dekker, Inc., pages 651-699.
Katz, Slava M. 1987. Estimation of probabilities from
sparse data for the language model component of a
speech recognizer. IEEE Transactions on Acoustics,
Speeech and Signal Processing, 35(4400-401.
Lafferty, John, Daniel Sleator, and Davey Temperley.
1992. Grammatical trigrams: aa probabilistic model
of link grammar. In Robert Goldman, editor, AAAI
</reference>
<page confidence="0.959098">
277
</page>
<reference confidence="0.994580413793103">
Fall Symposium on Probabilistic Approaches to Natu-
ral Language Processing, Cambridge, Massachusetts.
American Association for Artificial Intelligence.
Paul, Douglas B. 1991. Experience with a stack
decoder-based HMM CSR and back-off n-gram lan-
guage models. In Proceedings of The Speech and Nat-
ural Language Workshop, pages 284-288, Palo Alto,
California, February. Defense Advanced Research
Projects Agency, Information Science and Technol-
ogy Office, Morgan Kaufmann.
Pereira, Fernando C. N., Naftali Z. Tishby, and Lil-
lian Lee. 1993. Distributional clustering of English
words. In 30th Annual Meeting of the Association for
Computational Linguistics, pages 183-190, Colum-
bus, Ohio. Ohio State University, Association for
Computational Linguistics, Morristown, New Jersey.
Rosenfeld, Ronald and Xuedong Huang. 1992. Im-
provements in stochastic language modeling. In
DARPA Speech and Natural Language Workshop,
pages 107-111, Harriman, New York, February. Mor-
gan Kaufmann, San Mateo, California.
Schabes, Yves. 1992. Stochastic lexicalized tree-
adjoining grammars. In Proceeedings of the 14th
International Conference on Computational Linguis-
tics, Nantes, France.
Sugawara, K., M. Nishimura, K. Toshioka, M. Okochi,
and T. Kaneko. 1985. Isolated word recognition
using hidden Markov models. In Proceedings of
ICASSP, pages 1-4, Tampa, Florida. IEEE.
</reference>
<page confidence="0.996878">
278
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.775608">
<title confidence="0.997835">Similarity-Based Estimation of Word Cooccurrence Probabilities</title>
<author confidence="0.999692">Ido Dagan Fernando Pereira</author>
<affiliation confidence="0.999853">AT&amp;T Bell Laboratories</affiliation>
<address confidence="0.999816">600 Mountain Ave. Murray Hill, NJ 07974, USA</address>
<email confidence="0.998615">daganOresearch.att.compereiraOresearch.att.com</email>
<abstract confidence="0.990019545454546">In many applications of natural language processing it is necessary to determine the likelihood of a given word combination. For example, a speech recognizer may need to determine which of the two word combinations &amp;quot;eat a peach&amp;quot; and &amp;quot;eat a beach&amp;quot; is more likely. Statistical NLP methods determine the likelihood of a word combination according to its frequency in a training corpus. However, the nature of language is such that many word combinations are infrequent and do not occur in a given corpus. In this work we propose a method for estimating the probability of such previously unseen word combinations using available information on &amp;quot;most similar&amp;quot; words. We describe a probabilistic word association model based on distributional word similarity, and apply it to improving probability estimates for unseen word bigrams in a variant of Katz&apos;s back-off model. The similarity-based method yields a 20% perplexity improvement in the prediction of unseen bigrams and statistically significant reductions in speech-recognition error.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ezra Black</author>
<author>Fred Jelinek</author>
<author>John Lafferty</author>
<author>David M Magerman</author>
<author>David Mercer</author>
<author>Salim Roukos</author>
</authors>
<title>Towards history-based grammars: Using richer models for probabilistic parsing.</title>
<date>1993</date>
<booktitle>In 30th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>31--37</pages>
<institution>State University, Association for Computational Linguistics,</institution>
<location>Columbus, Ohio. Ohio</location>
<contexts>
<context position="23941" citStr="Black et al., 1993" startWordPosition="3903" endWordPosition="3906">. Finally, the similarity-based model may be applied to configurations other than bigrams. For trigrams, it is necessary to measure similarity between different conditioning bigrams. This can be done directly, 276 by measuring the distance between distributions of the form P(w3lwi, w2), corresponding to different bigrams (wi, w2). Alternatively, and more practically, it would be possible to define a similarity measure between bigrams as a function of similarities between corresponding words in them. Other types of conditional cooccurrence probabilities have been used in probabilistic parsing (Black et al., 1993). If the configuration in question includes only two words, such as P(objectiverb), then it is possible to use the model we have used for bigrams. If the configuration includes more elements, it is necessary to adjust the method, along the lines discussed above for trigrams. Conclusions Similarity-based models suggest an appealing approach for dealing with data sparseness. Based on corpus statistics, they provide analogies between words that often agree with our linguistic and domain intuitions. In this paper we presented a new model that implements the similarity-based approach to provide est</context>
</contexts>
<marker>Black, Jelinek, Lafferty, Magerman, Mercer, Roukos, 1993</marker>
<rawString>Black, Ezra, Fred Jelinek, John Lafferty, David M. Magerman, David Mercer, and Salim Roukos. 1993. Towards history-based grammars: Using richer models for probabilistic parsing. In 30th Annual Meeting of the Association for Computational Linguistics, pages 31-37, Columbus, Ohio. Ohio State University, Association for Computational Linguistics, Morristown, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Peter V deSouza</author>
<author>Jenifer C Lai</author>
<author>Robert L Mercer</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<pages>18--4</pages>
<contexts>
<context position="3164" citStr="Brown et al. (1992)" startWordPosition="484" endWordPosition="487"> occurred in training following the conditioning word w1 is calculated from the probability of w2, as estimated by w2&apos;s frequency in the corpus (Jelinek, Mercer, and Roukos, 1992; Katz, 1987). This method depends on an independence assumption on the cooccurrence of w1 and w2: the more frequent w2 is, the higher will be the estimate of P(w2lwi), regardless of Class-based and similarity-based models provide an alternative to the independence assumption. In those models, the relationship between given words is modele.d by analogy with other words that are in some sense similar to the given ones. Brown et al. (1992) suggest a class-based n-gram model in which words with similar cooccurrence distributions are clustered in word classes. The cooccurrence probability of a given pair of words then is estimated according to an averaged cooccurrence probability of the two corresponding classes. Pereira, Tishby, and Lee (1993) propose a &amp;quot;soft&amp;quot; clustering scheme for certain grammatical cooccurrences in which membership of a word in a class is probabilistic. Cooccurrence probabilities of words are then modeled by averaged cooccurrence probabilities of word clusters. Dagan, Markus, and Markovitch (1993) argue that </context>
</contexts>
<marker>Brown, Pietra, deSouza, Lai, Mercer, 1992</marker>
<rawString>Brown, Peter F., Vincent J. Della Pietra, Peter V. deSouza, Jenifer C. Lai, and Robert L. Mercer. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18(4):467-479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth W Church</author>
<author>William A Gale</author>
</authors>
<title>A comparison of the enhanced Good-Turing and deleted estimation methods for estimating probabilities of English bigrams.</title>
<date>1991</date>
<journal>Computer Speech and Language,</journal>
<pages>5--19</pages>
<contexts>
<context position="6757" citStr="Church and Gale, 1991" startWordPosition="1043" endWordPosition="1046">nseen bigrams is fairly high; any new sample is very likely to contain some. Because of data sparseness, we cannot reliably use a maximum likelihood estimator (MLE) for bigram probabilities. The MLE for the probability of a bigram (w1, w2) is simply: c(wi,w2) PAIL(tvi,w2)=- (1) where c(wi , w2) is the frequency of (wi, w2) in the training corpus and N is the total number of bigrams. However, this estimates the probability of any unseen bigram to be zero, which is clearly undesirable. Previous proposals to circumvent the above problem (Good, 1953; Jelinek, Mercer, and Roukos, 1992; Katz, 1987; Church and Gale, 1991) take the MLE as an initial estimate and adjust it so that the total probability of seen bigrams is less than one, leaving some probability mass for unseen bigrams. Typically, the adjustment involves either interpolation, in which the new estimator is a weighted combination of the MLE and an estimator that is guaranteed to be nonzero for unseen bigrams, or discounting, in which the MLE is decreased according to a model of the unreliability of small frequency counts, leaving some probability mass for unseen bigrams. The back-off model of Katz (1987) provides a clear separation between frequent </context>
</contexts>
<marker>Church, Gale, 1991</marker>
<rawString>Church, Kenneth W. and William A. Gale. 1991. A comparison of the enhanced Good-Turing and deleted estimation methods for estimating probabilities of English bigrams. Computer Speech and Language, 5:19-54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Shaul Markus</author>
<author>Shaul Markovitch</author>
</authors>
<title>Contextual word similarity and estimation from sparse data.</title>
<date>1993</date>
<booktitle>In 30th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>164--171</pages>
<institution>State University, Association for Computational Linguistics,</institution>
<location>Columbus, Ohio. Ohio</location>
<marker>Dagan, Markus, Markovitch, 1993</marker>
<rawString>Dagan, Ido, Shaul Markus, and Shaul Markovitch. 1993. Contextual word similarity and estimation from sparse data. In 30th Annual Meeting of the Association for Computational Linguistics, pages 164-171, Columbus, Ohio. Ohio State University, Association for Computational Linguistics, Morristown, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ute Essen</author>
<author>Volker Steinbiss</author>
</authors>
<title>Coocurrence smoothing for stochastic language modeling.</title>
<date>1992</date>
<booktitle>In Proceedings of ICASSP, volume I,</booktitle>
<pages>161--164</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="4766" citStr="Essen and Steinbiss (1992)" startWordPosition="721" endWordPosition="725">ict which unobserved cooccurrences are more likely than others. Their model, however, is not probabilistic, that is, it does not provide a probability estimate for unobserved cooccurrences. It cannot therefore be used in a complete probabilistic framework, such as n-gram language models or probabilistic lexicalized grammars (Schabes, 1992; Lafferty, Sleator, and Temperley, 1992). We now give a similarity-based method for estimating the probabilities of cooccurrences unseen in training. 272 Similarity-based estimation was first used for language modeling in the cooccurrence smoothing method of Essen and Steinbiss (1992), derived from work on acoustic model smoothing by Sugawara et al. (1985). We present a different method that takes as starting point the back-off scheme of Katz (1987). We first allocate an appropriate probability mass for unseen cooccurrences following the back-off method. Then we redistribute that mass to unseen cooccurrences according to an averaged cooccurrence distribution of a set of most similar conditioning words, using relative entropy as our similarity measure. This second step replaces the use of the independence assumption in the original back-off model. We applied our method to e</context>
<context position="17966" citStr="Essen and Steinbiss, 1992" startWordPosition="2940" endWordPosition="2943">ber of errors in our current recognition setup. Table 2 shows some examples of speech recognition disagreements between the two models. The hypotheses are labeled &apos;B&apos; for back-off and &apos;S&apos; for similarity, and the bold-face words are errors. The similarity model seems to be able to model better regularities such as semantic parallelism in lists and avoiding a past tense form after &amp;quot;to.&amp;quot; On the other hand, the similarity model makes several mistakes in which a function word is inserted in a place where punctuation would be found in written text. Related Work The cooccurrence smoothing technique (Essen and Steinbiss, 1992), based on earlier stochastic speech modeling work by Sugawara et al. (1985), is the main previous attempt to use similarity to estimate the probability of unseen events in language modeling. In addition to its original use in language modeling for speech recognition, Grishman and Sterling (1993) applied the cooccurrence smoothing technique to estimate the likelihood of selectional patterns. We will outline here the main parallels and differences between our method and cooccurrence smoothing. A more detailed analysis would require an empirical comparison of the two methods on the same corpus a</context>
<context position="21089" citStr="Essen and Steinbiss (1992)" startWordPosition="3461" endWordPosition="3464">weighs each context in proportion to its probability of occurrence with w, but not with w&apos;. In this way, if w and w&apos; have comparable frequencies but w&apos; has a sharper context distribution than w, then D(w&apos; II w) is greater than D(w w&apos;). Therefore, in our similarity model w&apos; will play a stronger role in estimating w than vice versa. These properties motivated our choice of relative entropy for similarity measure, because of the intuition that words with sharper distributions are more informative about other words than words with flat distributions. 4This presentation corresponds to model 2-8 in Essen and Steinbiss (1992). Their presentation follows the equivalent model 1-A, which averages over similar conditioned words, with the similarity defined with the preceding word as context. In fact, these equivalent models are symmetric in their treatment of conditioning and conditioned word, as they can both be rewritten as p.,(.21wo= E P(w21.1)P(.11w)P(wiwi) . They also consider other definitions of confusion probability and smoothed probability estimate, but the one above yielded the best experimental results. Finally, while we have used our similarity model only for missing bigrams in a back-off scheme, Essen and</context>
</contexts>
<marker>Essen, Steinbiss, 1992</marker>
<rawString>Essen, Ute and Volker Steinbiss. 1992. Coocurrence smoothing for stochastic language modeling. In Proceedings of ICASSP, volume I, pages 161-164. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I J Good</author>
</authors>
<title>The population frequencies of species and the estimation of population parameters.</title>
<date>1953</date>
<journal>Biometrika,</journal>
<pages>40--3</pages>
<contexts>
<context position="6686" citStr="Good, 1953" startWordPosition="1034" endWordPosition="1035">inite sample. Yet, the aggregate probability of all these unseen bigrams is fairly high; any new sample is very likely to contain some. Because of data sparseness, we cannot reliably use a maximum likelihood estimator (MLE) for bigram probabilities. The MLE for the probability of a bigram (w1, w2) is simply: c(wi,w2) PAIL(tvi,w2)=- (1) where c(wi , w2) is the frequency of (wi, w2) in the training corpus and N is the total number of bigrams. However, this estimates the probability of any unseen bigram to be zero, which is clearly undesirable. Previous proposals to circumvent the above problem (Good, 1953; Jelinek, Mercer, and Roukos, 1992; Katz, 1987; Church and Gale, 1991) take the MLE as an initial estimate and adjust it so that the total probability of seen bigrams is less than one, leaving some probability mass for unseen bigrams. Typically, the adjustment involves either interpolation, in which the new estimator is a weighted combination of the MLE and an estimator that is guaranteed to be nonzero for unseen bigrams, or discounting, in which the MLE is decreased according to a model of the unreliability of small frequency counts, leaving some probability mass for unseen bigrams. The back</context>
<context position="9142" citStr="Good, 1953" startWordPosition="1429" endWordPosition="1430">modifies slightly Katz&apos;s presentation to include the placeholder Pr for alternative models of the distribution of unseen bigrams. Katz uses the Good-Turing formula to replace the actual frequency c(wi , w2) of a bigram (or an event, in general) with a discounted frequency, c*(wi, w2), defined by cn (wi,w2)+1 c*(wi, w2) = (c(wi, w2) + 1) (3) nc(w1,e.2) where ?lc is the number of different bigrams in the corpus that have frequency c. He then uses the discounted frequency in the conditional probability calculation for a bigram: c*(wi, w2) Pd(w21w1) = (4) e(wi) In the original Good-Turing method (Good, 1953) the free probability mass is redistributed uniformly among all unseen events. Instead, Katz&apos;s back-off scheme redistributes the free probability mass nonuniformly in proportion to the frequency of w2, by setting Pr(w21w1) = P(w2) (5) p(tv2iw1) = {Pd(w2lwi) wi,.2)=0 Pr(w2 wi) 13(w1) 273 Katz thus assumes that for a given conditioning word w1 the probability of an unseen following word w2 is proportional to its unconditional probability. However, the overall form of the model (2) does not depend on this assumption, and we will next investigate an estimate for Pr (W2 I WI) derived by averaging e</context>
</contexts>
<marker>Good, 1953</marker>
<rawString>Good, I. J. 1953. The population frequencies of species and the estimation of population parameters. Biometrika, 40(3):237-264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Grishman</author>
<author>John Sterling</author>
</authors>
<title>Smoothing of automatically generated selectional constraints.</title>
<date>1993</date>
<booktitle>In Human Language Technology,</booktitle>
<pages>254--259</pages>
<publisher>Morgan Kaufmann.</publisher>
<location>San Francisco, California.</location>
<contexts>
<context position="18263" citStr="Grishman and Sterling (1993)" startWordPosition="2988" endWordPosition="2991">egularities such as semantic parallelism in lists and avoiding a past tense form after &amp;quot;to.&amp;quot; On the other hand, the similarity model makes several mistakes in which a function word is inserted in a place where punctuation would be found in written text. Related Work The cooccurrence smoothing technique (Essen and Steinbiss, 1992), based on earlier stochastic speech modeling work by Sugawara et al. (1985), is the main previous attempt to use similarity to estimate the probability of unseen events in language modeling. In addition to its original use in language modeling for speech recognition, Grishman and Sterling (1993) applied the cooccurrence smoothing technique to estimate the likelihood of selectional patterns. We will outline here the main parallels and differences between our method and cooccurrence smoothing. A more detailed analysis would require an empirical comparison of the two methods on the same corpus and task. In cooccurrence smoothing, as in our method, a baseline model is combined with a similarity-based model that refines some of its probability estimates. The similarity model in cooccurrence smoothing is based on the intuition that the similarity between two words w and w&apos; can be measured </context>
</contexts>
<marker>Grishman, Sterling, 1993</marker>
<rawString>Grishman, Ralph and John Sterling. 1993. Smoothing of automatically generated selectional constraints. In Human Language Technology, pages 254-259, San Francisco, California. Advanced Research Projects Agency, Software and Intelligent Systems Technology Office, Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
<author>Robert L Mercer</author>
<author>Salim Roukos</author>
</authors>
<title>Principles of lexical language modeling for speech recognition.</title>
<date>1992</date>
<booktitle>In Sadaoki Furui</booktitle>
<pages>651--699</pages>
<editor>and M. Mohan Sondhi, editors,</editor>
<publisher>Mercer Dekker, Inc.,</publisher>
<contexts>
<context position="2723" citStr="Jelinek, Mercer, and Roukos, 1992" startWordPosition="411" endWordPosition="415">tive-noun, for example) and word sequences (n-grams). In commonly used models, the probability estimate for a previously unseen cooccurrence is a function of the probability estiLillian Lee Division of Applied Sciences Harvard University 33 Oxford St. Cambridge MA 02138, USA lleadas.harvard.edu mates for the words in the cooccurrence. For example, in the bigram models that we study here, the probability P(w2lwi) of a conditioned word w2 that has never occurred in training following the conditioning word w1 is calculated from the probability of w2, as estimated by w2&apos;s frequency in the corpus (Jelinek, Mercer, and Roukos, 1992; Katz, 1987). This method depends on an independence assumption on the cooccurrence of w1 and w2: the more frequent w2 is, the higher will be the estimate of P(w2lwi), regardless of Class-based and similarity-based models provide an alternative to the independence assumption. In those models, the relationship between given words is modele.d by analogy with other words that are in some sense similar to the given ones. Brown et al. (1992) suggest a class-based n-gram model in which words with similar cooccurrence distributions are clustered in word classes. The cooccurrence probability of a giv</context>
<context position="6721" citStr="Jelinek, Mercer, and Roukos, 1992" startWordPosition="1036" endWordPosition="1040">. Yet, the aggregate probability of all these unseen bigrams is fairly high; any new sample is very likely to contain some. Because of data sparseness, we cannot reliably use a maximum likelihood estimator (MLE) for bigram probabilities. The MLE for the probability of a bigram (w1, w2) is simply: c(wi,w2) PAIL(tvi,w2)=- (1) where c(wi , w2) is the frequency of (wi, w2) in the training corpus and N is the total number of bigrams. However, this estimates the probability of any unseen bigram to be zero, which is clearly undesirable. Previous proposals to circumvent the above problem (Good, 1953; Jelinek, Mercer, and Roukos, 1992; Katz, 1987; Church and Gale, 1991) take the MLE as an initial estimate and adjust it so that the total probability of seen bigrams is less than one, leaving some probability mass for unseen bigrams. Typically, the adjustment involves either interpolation, in which the new estimator is a weighted combination of the MLE and an estimator that is guaranteed to be nonzero for unseen bigrams, or discounting, in which the MLE is decreased according to a model of the unreliability of small frequency counts, leaving some probability mass for unseen bigrams. The back-off model of Katz (1987) provides </context>
</contexts>
<marker>Jelinek, Mercer, Roukos, 1992</marker>
<rawString>Jelinek, Frederick, Robert L. Mercer, and Salim Roukos. 1992. Principles of lexical language modeling for speech recognition. In Sadaoki Furui and M. Mohan Sondhi, editors, Advances in Speech Signal Processing. Mercer Dekker, Inc., pages 651-699.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slava M Katz</author>
</authors>
<title>Estimation of probabilities from sparse data for the language model component of a speech recognizer.</title>
<date>1987</date>
<journal>IEEE Transactions on Acoustics, Speeech and Signal Processing,</journal>
<pages>35--4400</pages>
<contexts>
<context position="2736" citStr="Katz, 1987" startWordPosition="416" endWordPosition="417">quences (n-grams). In commonly used models, the probability estimate for a previously unseen cooccurrence is a function of the probability estiLillian Lee Division of Applied Sciences Harvard University 33 Oxford St. Cambridge MA 02138, USA lleadas.harvard.edu mates for the words in the cooccurrence. For example, in the bigram models that we study here, the probability P(w2lwi) of a conditioned word w2 that has never occurred in training following the conditioning word w1 is calculated from the probability of w2, as estimated by w2&apos;s frequency in the corpus (Jelinek, Mercer, and Roukos, 1992; Katz, 1987). This method depends on an independence assumption on the cooccurrence of w1 and w2: the more frequent w2 is, the higher will be the estimate of P(w2lwi), regardless of Class-based and similarity-based models provide an alternative to the independence assumption. In those models, the relationship between given words is modele.d by analogy with other words that are in some sense similar to the given ones. Brown et al. (1992) suggest a class-based n-gram model in which words with similar cooccurrence distributions are clustered in word classes. The cooccurrence probability of a given pair of wo</context>
<context position="4934" citStr="Katz (1987)" startWordPosition="753" endWordPosition="754">ences. It cannot therefore be used in a complete probabilistic framework, such as n-gram language models or probabilistic lexicalized grammars (Schabes, 1992; Lafferty, Sleator, and Temperley, 1992). We now give a similarity-based method for estimating the probabilities of cooccurrences unseen in training. 272 Similarity-based estimation was first used for language modeling in the cooccurrence smoothing method of Essen and Steinbiss (1992), derived from work on acoustic model smoothing by Sugawara et al. (1985). We present a different method that takes as starting point the back-off scheme of Katz (1987). We first allocate an appropriate probability mass for unseen cooccurrences following the back-off method. Then we redistribute that mass to unseen cooccurrences according to an averaged cooccurrence distribution of a set of most similar conditioning words, using relative entropy as our similarity measure. This second step replaces the use of the independence assumption in the original back-off model. We applied our method to estimate unseen bigram probabilities for Wall Street Journal text and compared it to the standard back-off model. Testing on a held-out sample, the similarity model achi</context>
<context position="6733" citStr="Katz, 1987" startWordPosition="1041" endWordPosition="1042"> all these unseen bigrams is fairly high; any new sample is very likely to contain some. Because of data sparseness, we cannot reliably use a maximum likelihood estimator (MLE) for bigram probabilities. The MLE for the probability of a bigram (w1, w2) is simply: c(wi,w2) PAIL(tvi,w2)=- (1) where c(wi , w2) is the frequency of (wi, w2) in the training corpus and N is the total number of bigrams. However, this estimates the probability of any unseen bigram to be zero, which is clearly undesirable. Previous proposals to circumvent the above problem (Good, 1953; Jelinek, Mercer, and Roukos, 1992; Katz, 1987; Church and Gale, 1991) take the MLE as an initial estimate and adjust it so that the total probability of seen bigrams is less than one, leaving some probability mass for unseen bigrams. Typically, the adjustment involves either interpolation, in which the new estimator is a weighted combination of the MLE and an estimator that is guaranteed to be nonzero for unseen bigrams, or discounting, in which the MLE is decreased according to a model of the unreliability of small frequency counts, leaving some probability mass for unseen bigrams. The back-off model of Katz (1987) provides a clear sepa</context>
</contexts>
<marker>Katz, 1987</marker>
<rawString>Katz, Slava M. 1987. Estimation of probabilities from sparse data for the language model component of a speech recognizer. IEEE Transactions on Acoustics, Speeech and Signal Processing, 35(4400-401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Daniel Sleator</author>
<author>Davey Temperley</author>
</authors>
<title>Grammatical trigrams: aa probabilistic model of link grammar.</title>
<date>1992</date>
<editor>In Robert Goldman, editor,</editor>
<publisher>AAAI</publisher>
<contexts>
<context position="4520" citStr="Lafferty, Sleator, and Temperley, 1992" startWordPosition="686" endWordPosition="690">mation. Their similarity-based model avoids clustering altogether. Instead, each word is modeled by its own specific class, a set of words which are most similar to it (as in k-nearest neighbor approaches in pattern recognition). Using this scheme, they predict which unobserved cooccurrences are more likely than others. Their model, however, is not probabilistic, that is, it does not provide a probability estimate for unobserved cooccurrences. It cannot therefore be used in a complete probabilistic framework, such as n-gram language models or probabilistic lexicalized grammars (Schabes, 1992; Lafferty, Sleator, and Temperley, 1992). We now give a similarity-based method for estimating the probabilities of cooccurrences unseen in training. 272 Similarity-based estimation was first used for language modeling in the cooccurrence smoothing method of Essen and Steinbiss (1992), derived from work on acoustic model smoothing by Sugawara et al. (1985). We present a different method that takes as starting point the back-off scheme of Katz (1987). We first allocate an appropriate probability mass for unseen cooccurrences following the back-off method. Then we redistribute that mass to unseen cooccurrences according to an average</context>
<context position="25806" citStr="Lafferty, Sleator, and Temperley, 1992" startWordPosition="4183" endWordPosition="4187"> for a bigram model is statistically significant, though modest in its overall effect because of the small proportion of unseen events. While we have used bigrams as an easily-accessible platform to develop and test the model, more substantial improvements might be obtainable for more informative configurations. An obvious case is that of trigrams, for which the sparse data problem is much more severe.&apos; Our longer-term goal, however, is to apply similarity techniques to linguistically motivated word cooccurrence configurations, as suggested by lexicalized approaches to parsing (Schabes, 1992; Lafferty, Sleator, and Temperley, 1992). In configurations like verb-object and adjective-noun, there is some evidence (Pereira, Tishby, and Lee, 1993) that sharper word cooccurrence distributions are obtainable, leading to improved predictions by similarity techniques. Acknowledgments We thank Slava Katz for discussions on the topic of this paper, Doug McIlroy for detailed comments, Doug Paul &apos;For WSJ trigrams, only 58.6% of test set trigrams occur in 40M of words of training (Doug Paul, personal communication). for help with his baseline back-off model, and Andre Ljolje and Michael Riley for providing the word lattices for our e</context>
</contexts>
<marker>Lafferty, Sleator, Temperley, 1992</marker>
<rawString>Lafferty, John, Daniel Sleator, and Davey Temperley. 1992. Grammatical trigrams: aa probabilistic model of link grammar. In Robert Goldman, editor, AAAI</rawString>
</citation>
<citation valid="false">
<title>Fall Symposium on Probabilistic Approaches to Natural Language Processing,</title>
<journal>Artificial Intelligence.</journal>
<publisher>American Association for</publisher>
<location>Cambridge, Massachusetts.</location>
<marker></marker>
<rawString>Fall Symposium on Probabilistic Approaches to Natural Language Processing, Cambridge, Massachusetts. American Association for Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas B Paul</author>
</authors>
<title>Experience with a stack decoder-based HMM CSR and back-off n-gram language models.</title>
<date>1991</date>
<booktitle>In Proceedings of The Speech and Natural Language Workshop,</booktitle>
<pages>284--288</pages>
<publisher>Morgan Kaufmann.</publisher>
<location>Palo Alto, California,</location>
<contexts>
<context position="13389" citStr="Paul, 1991" startWordPosition="2142" endWordPosition="2143">obtain better estimates for unseen bigrams. Four parameters, to be tuned experimentally, are relevant for this process: k and t, which determine the set of similar words to be considered, 13, which determines the relative effect of these words, and 7, which determines the overall importance of the similarity-based model. Evaluation We evaluated our method by comparing its perplexity&apos; and effect on speech-recognition accuracy with the baseline bigram back-off model developed by MIT Lincoln Laboratories for the Wall Street Journal (WSJ) text and dictation corpora provided by ARPA&apos;s HLT program (Paul, 1991).2 The baseline back-off model follows closely the Katz design, except that for compactness all frequency one bigrams are ignored. The counts used in this model and in ours were obtained from 40.5 million words of WSJ text from the years 1987-89. For perplexity evaluation, we tuned the similarity model parameters by minimizing perplexity on an additional sample of 57.5 thousand words of WSJ text, drawn from the ARPA HLT development test set. The best parameter values found were k = 60, t = 2.5, # = 4 and -y = 0.15. For these values, the improvement in perplexity for unseen bigrams in a held-ou</context>
</contexts>
<marker>Paul, 1991</marker>
<rawString>Paul, Douglas B. 1991. Experience with a stack decoder-based HMM CSR and back-off n-gram language models. In Proceedings of The Speech and Natural Language Workshop, pages 284-288, Palo Alto, California, February. Defense Advanced Research Projects Agency, Information Science and Technology Office, Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando C N Pereira</author>
<author>Naftali Z Tishby</author>
<author>Lillian Lee</author>
</authors>
<title>Distributional clustering of English words.</title>
<date>1993</date>
<booktitle>In 30th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>183--190</pages>
<institution>State University, Association for Computational Linguistics,</institution>
<location>Columbus, Ohio. Ohio</location>
<marker>Pereira, Tishby, Lee, 1993</marker>
<rawString>Pereira, Fernando C. N., Naftali Z. Tishby, and Lillian Lee. 1993. Distributional clustering of English words. In 30th Annual Meeting of the Association for Computational Linguistics, pages 183-190, Columbus, Ohio. Ohio State University, Association for Computational Linguistics, Morristown, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Rosenfeld</author>
<author>Xuedong Huang</author>
</authors>
<title>Improvements in stochastic language modeling.</title>
<date>1992</date>
<booktitle>In DARPA Speech and Natural Language Workshop,</booktitle>
<pages>107--111</pages>
<publisher>Morgan Kaufmann,</publisher>
<location>Harriman, New York, February.</location>
<contexts>
<context position="23056" citStr="Rosenfeld and Huang, 1992" startWordPosition="3771" endWordPosition="3774">larity between conditioned words rather than on similarity between conditioning words. Other evidence may be combined with the similaritybased estimate. For instance, it may be advantageous to weigh those estimates by some measure of the reliability of the similarity metric and of the neighbor distributions. A second possibility is to take into account negative evidence: if wi is frequent, but w2 never followed it, there may be enough statistical evidence to put an upper bound on the estimate of P(w2 This may require an adjustment of the similarity based estimate, possibly along the lines of (Rosenfeld and Huang, 1992). Third, the similarity-based estimate can be used to smooth the maximum likelihood estimate for small nonzero frequencies. If the similarity-based estimate is relatively high, a bigram would receive a higher estimate than predicted by the uniform discounting method. Finally, the similarity-based model may be applied to configurations other than bigrams. For trigrams, it is necessary to measure similarity between different conditioning bigrams. This can be done directly, 276 by measuring the distance between distributions of the form P(w3lwi, w2), corresponding to different bigrams (wi, w2). A</context>
</contexts>
<marker>Rosenfeld, Huang, 1992</marker>
<rawString>Rosenfeld, Ronald and Xuedong Huang. 1992. Improvements in stochastic language modeling. In DARPA Speech and Natural Language Workshop, pages 107-111, Harriman, New York, February. Morgan Kaufmann, San Mateo, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Schabes</author>
</authors>
<title>Stochastic lexicalized treeadjoining grammars.</title>
<date>1992</date>
<booktitle>In Proceeedings of the 14th International Conference on Computational Linguistics,</booktitle>
<location>Nantes, France.</location>
<contexts>
<context position="4480" citStr="Schabes, 1992" startWordPosition="684" endWordPosition="685">l loss of information. Their similarity-based model avoids clustering altogether. Instead, each word is modeled by its own specific class, a set of words which are most similar to it (as in k-nearest neighbor approaches in pattern recognition). Using this scheme, they predict which unobserved cooccurrences are more likely than others. Their model, however, is not probabilistic, that is, it does not provide a probability estimate for unobserved cooccurrences. It cannot therefore be used in a complete probabilistic framework, such as n-gram language models or probabilistic lexicalized grammars (Schabes, 1992; Lafferty, Sleator, and Temperley, 1992). We now give a similarity-based method for estimating the probabilities of cooccurrences unseen in training. 272 Similarity-based estimation was first used for language modeling in the cooccurrence smoothing method of Essen and Steinbiss (1992), derived from work on acoustic model smoothing by Sugawara et al. (1985). We present a different method that takes as starting point the back-off scheme of Katz (1987). We first allocate an appropriate probability mass for unseen cooccurrences following the back-off method. Then we redistribute that mass to unse</context>
<context position="25766" citStr="Schabes, 1992" startWordPosition="4181" endWordPosition="4182">ent we achieved for a bigram model is statistically significant, though modest in its overall effect because of the small proportion of unseen events. While we have used bigrams as an easily-accessible platform to develop and test the model, more substantial improvements might be obtainable for more informative configurations. An obvious case is that of trigrams, for which the sparse data problem is much more severe.&apos; Our longer-term goal, however, is to apply similarity techniques to linguistically motivated word cooccurrence configurations, as suggested by lexicalized approaches to parsing (Schabes, 1992; Lafferty, Sleator, and Temperley, 1992). In configurations like verb-object and adjective-noun, there is some evidence (Pereira, Tishby, and Lee, 1993) that sharper word cooccurrence distributions are obtainable, leading to improved predictions by similarity techniques. Acknowledgments We thank Slava Katz for discussions on the topic of this paper, Doug McIlroy for detailed comments, Doug Paul &apos;For WSJ trigrams, only 58.6% of test set trigrams occur in 40M of words of training (Doug Paul, personal communication). for help with his baseline back-off model, and Andre Ljolje and Michael Riley f</context>
</contexts>
<marker>Schabes, 1992</marker>
<rawString>Schabes, Yves. 1992. Stochastic lexicalized treeadjoining grammars. In Proceeedings of the 14th International Conference on Computational Linguistics, Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sugawara</author>
<author>M Nishimura</author>
<author>K Toshioka</author>
<author>M Okochi</author>
<author>T Kaneko</author>
</authors>
<title>Isolated word recognition using hidden Markov models.</title>
<date>1985</date>
<booktitle>In Proceedings of ICASSP,</booktitle>
<pages>1--4</pages>
<publisher>IEEE.</publisher>
<location>Tampa, Florida.</location>
<contexts>
<context position="4839" citStr="Sugawara et al. (1985)" startWordPosition="735" endWordPosition="738">however, is not probabilistic, that is, it does not provide a probability estimate for unobserved cooccurrences. It cannot therefore be used in a complete probabilistic framework, such as n-gram language models or probabilistic lexicalized grammars (Schabes, 1992; Lafferty, Sleator, and Temperley, 1992). We now give a similarity-based method for estimating the probabilities of cooccurrences unseen in training. 272 Similarity-based estimation was first used for language modeling in the cooccurrence smoothing method of Essen and Steinbiss (1992), derived from work on acoustic model smoothing by Sugawara et al. (1985). We present a different method that takes as starting point the back-off scheme of Katz (1987). We first allocate an appropriate probability mass for unseen cooccurrences following the back-off method. Then we redistribute that mass to unseen cooccurrences according to an averaged cooccurrence distribution of a set of most similar conditioning words, using relative entropy as our similarity measure. This second step replaces the use of the independence assumption in the original back-off model. We applied our method to estimate unseen bigram probabilities for Wall Street Journal text and comp</context>
<context position="18042" citStr="Sugawara et al. (1985)" startWordPosition="2952" endWordPosition="2955">peech recognition disagreements between the two models. The hypotheses are labeled &apos;B&apos; for back-off and &apos;S&apos; for similarity, and the bold-face words are errors. The similarity model seems to be able to model better regularities such as semantic parallelism in lists and avoiding a past tense form after &amp;quot;to.&amp;quot; On the other hand, the similarity model makes several mistakes in which a function word is inserted in a place where punctuation would be found in written text. Related Work The cooccurrence smoothing technique (Essen and Steinbiss, 1992), based on earlier stochastic speech modeling work by Sugawara et al. (1985), is the main previous attempt to use similarity to estimate the probability of unseen events in language modeling. In addition to its original use in language modeling for speech recognition, Grishman and Sterling (1993) applied the cooccurrence smoothing technique to estimate the likelihood of selectional patterns. We will outline here the main parallels and differences between our method and cooccurrence smoothing. A more detailed analysis would require an empirical comparison of the two methods on the same corpus and task. In cooccurrence smoothing, as in our method, a baseline model is co</context>
</contexts>
<marker>Sugawara, Nishimura, Toshioka, Okochi, Kaneko, 1985</marker>
<rawString>Sugawara, K., M. Nishimura, K. Toshioka, M. Okochi, and T. Kaneko. 1985. Isolated word recognition using hidden Markov models. In Proceedings of ICASSP, pages 1-4, Tampa, Florida. IEEE.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>