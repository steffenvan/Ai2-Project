<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.019543">
<title confidence="0.724900833333333">
Analysis of statistical and morphological classes to generate weighted
reordering hypotheses on a Statistical Machine Translation system
Marta R. Costa-juss`a and Jos´e A. R. Fonollosa
Department of Signal Theory and Communications
TALP Research Center (UPC)
Barcelona 08034, Spain
</title>
<email confidence="0.994229">
(mruiz,adrian)@gps.tsc.upc.edu
</email>
<sectionHeader confidence="0.998426" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999863666666667">
One main challenge of statistical machine trans-
lation (SMT) is dealing with word order. The
main idea of the statistical machine reordering
(SMR) approach is to use the powerful tech-
niques of SMT systems to generate a weighted
reordering graph for SMT systems. This tech-
nique supplies reordering constraints to an SMT
system, using statistical criteria.
In this paper, we experiment with different graph
pruning which guarantees the translation quality
improvement due to reordering at a very low in-
crease of computational cost.
The SMR approach is capable of generalizing re-
orderings, which have been learned during train-
ing, by using word classes instead of words
themselves. We experiment with statistical and
morphological classes in order to choose those
which capture the most probable reorderings.
Satisfactory results are reported in the WMT07
Es/En task. Our system outperforms in terms of
BLEU the WMT07 Official baseline system.
</bodyText>
<sectionHeader confidence="0.999468" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99988714">
Nowadays, statistical machine translation is mainly based
on phrases (Koehn et al., 2003). In parallel to this phrase-
based approach, the use of bilingual n-grams gives com-
parable results, as shown by Crego et al. (2005). Two
basic issues differentiate the n-gram-based system from
the phrase-based: training data is monotonically seg-
mented into bilingual units; and, the model considers n-
gram probabilities rather than relative frequencies. The
n-gram-based system follows a maximum entropy ap-
proach, in which a log-linear combination of multiple
models is implemented (Mari˜no et al., 2006), as an al-
ternative to the source-channel approach.
Introducing reordering capabilities is important in both
systems. Recently, new reordering strategies have been
proposed such as the reordering of each source sentence
to match the word order in the corresponding target sen-
tence, see Kanthak et al. (2005) and Mari˜no et al. (2006).
These approaches are applied in the training set and they
lack of reordering generalization.
Applied both in the training and decoding step, Collins
et al. (2005) describe a method for introducing syntac-
tic information for reordering in SMT. This approach is
applied as a pre-processing step.
Differently, Crego et al. (2006) presents a reordering
approach based on reordering patterns which is coupled
with decoding. The reordering patterns are learned di-
rectly from word alignment and all reorderings have the
same probability.
In our previous work (Costa-juss`a and Fonollosa,
2006) we presented the SMR approach which is based
on using the powerful SMT techniques to generate a re-
ordered source input for an SMT system both in train-
ing and decoding steps. One step further, (Costa-juss`a
et al., 2007) shows how the SMR system can generate a
weighted reordering graph, allowing the SMT system to
make the final reordering decision.
In this paper, the SMR approach is used to train the
SMT system and to generate a weighted reordering graph
for the decoding step. The SMR system uses word classes
instead of words themselves and we analyze both statisti-
cal and morphological classes. Moreover, we present ex-
periments regarding the reordering graph efficiency: we
analyze different graph pruning and we show the very low
increase in computational cost (compared to a monotonic
translation). Finally, we compare the performance our
system in terms of BLEU with the WMT07 baseline sys-
tem.
This paper is organized as follows. The first two sec-
tions explain the SMT and the SMR baseline systems,
respectively. Section 4 reports the study of statistical and
</bodyText>
<page confidence="0.984182">
171
</page>
<bodyText confidence="0.8352074">
Proceedings of the Second Workshop on Statistical Machine Translation, pages 171–176,
Prague, June 2007. c�2007 Association for Computational Linguistics
morphological classes. Section 5 describes the experi-
mental framework and discusses the results. Finally, Sec-
tion 6 presents the conclusions and some further work.
</bodyText>
<sectionHeader confidence="0.91277" genericHeader="method">
2 Ngram-based SMT System
</sectionHeader>
<bodyText confidence="0.998524111111111">
This section briefly describes the Ngram-based SMT (for
further details see (Mari˜no et al., 2006)). The Ngram-
based SMT system uses a translation model based on
bilingual n-grams. It is actually a language model of
bilingual units, referred to as tuples, which approxi-
mates the joint probability between source and target lan-
guages by using bilingual n-grams. Tuples are extracted
from any word alignment according to the following con-
straints:
</bodyText>
<listItem confidence="0.998018666666667">
1. a monotonic segmentation of each bilingual sen-
tence pairs is produced,
2. no word inside the tuple is aligned to words outside
the tuple, and
3. no smaller tuples can be extracted without violating
the previous constraints.
</listItem>
<bodyText confidence="0.9988764">
As a result of these constraints, only one segmentation
is possible for a given sentence pair.
In addition to the bilingual n-gram translation model,
the baseline system implements a log-linear combination
of feature functions, which are described as follows:
</bodyText>
<listItem confidence="0.979326782608696">
• A target language model. This feature consists of
a 4-gram model of words, which is trained from the
target side of the bilingual corpus.
• A class target language model. This feature con-
sists of a 5-gram model of words classes, which is
trained from the target side of the bilingual corpus
using the statistical classes from (Och, 1999).
• A word bonus function. This feature introduces
a bonus based on the number of target words con-
tained in the partial-translation hypothesis. It is used
to compensate for the system’s preference for short
output sentences.
• A source-to-target lexicon model. This feature,
which is based on the lexical parameters of the IBM
Model 1 (Brown et al., 1993), provides a comple-
mentary probability for each tuple in the translation
table. These lexicon parameters are obtained from
the source-to-target alignments.
• A target-to-source lexicon model. Similarly to the
previous feature, this feature is based on the lexical
parameters of the IBM Model 1 but, in this case,
these parameters are obtained from target-to-source
alignments.
</listItem>
<figureCaption confidence="0.994789">
Figure 1: SMR block diagram.
</figureCaption>
<sectionHeader confidence="0.960618" genericHeader="method">
3 SMR Baseline System
</sectionHeader>
<bodyText confidence="0.9999675">
As mentioned in the introduction, SMR and SMT are
based on the same principles.
</bodyText>
<subsectionHeader confidence="0.99227">
3.1 Concept
</subsectionHeader>
<bodyText confidence="0.9999388">
The aim of SMR consists in using an SMT system to deal
with reordering problems. Therefore, the SMR system
can be seen as an SMT system which translates from an
original source language (S) to a reordered source lan-
guage (S’), given a target language (T).
</bodyText>
<subsectionHeader confidence="0.997669">
3.2 Description
</subsectionHeader>
<bodyText confidence="0.999975214285714">
Figure 1 shows the SMR block diagram and an exam-
ple of the input and output of each block inside the
SMR system. The input is the initial source sentence
(S) and the output is the reordered source sentence (S’).
There are three blocks inside SMR: (1) the class replac-
ing block; (2) the decoder, which requires an Ngram
model containing the reordering information; and, (3) the
post-processing block which either reorders the source
sentence given the indexes of the decoder output 1-best
(training step) or transforms the decoder output graph to
an input graph for the SMT system (decoding step).
The decoder in Figure 1 requires a translation model
which is an Ngram model. Given a training parallel cor-
pus this model has been built following the next steps:
</bodyText>
<listItem confidence="0.999085111111111">
1. Select source and target word classes.
2. Align parallel training sentences at the word level in
both translation directions. Compute the union of
the two alignments to obtain a symmetrized many-
to-many word alignment.
3. Use the IBM1 Model to obtain a many-to-one word
alignment from the many-to-many word alignment.
4. Extract translation units from the computed many-
to-one alignment. Replace source words by their
</listItem>
<page confidence="0.99442">
172
</page>
<figureCaption confidence="0.9930905">
Figure 2: SMR approach in the (A) training step (B) in
the test step (the weight of each arch is in brackets).
</figureCaption>
<bodyText confidence="0.92602975">
classes and target words by the index of the linked
source word. An example of a translation unit here
is: C61 C28 C63#2 0 1, where # divides source
(word classes) and target (positions).
5. Compute the sequence of the above units and learn
the language model
For further information about the SMR training proce-
dure see (Costa-juss`a and Fonollosa, 2006).
</bodyText>
<subsectionHeader confidence="0.998562">
3.3 Improving SMT training
</subsectionHeader>
<bodyText confidence="0.99913305882353">
Figure 2 (A) shows the corresponding block diagram
for the training corpus: first, the given training corpus
S is translated into the reordered training source corpus
S’ with the SMR system. Then, this reordered training
source corpus S’ and the given training target corpus T
are used to build the SMT system
The main difference here is that the training is com-
puted with the S’2T task instead of the S2T given task.
Figure 3 (A) shows an example of the word alignment
computed on the given training parallel corpus S2T. Fig-
ure 3 (B) shows the same links but with the reordered
source training corpus S’. Although the quality in align-
ment is the same, the tuples that can be extracted change
(notice that tuple extraction is monotonic). We now are
able to extract smaller tuples which reduce the transla-
tion vocabulary sparseness. These new tuples are used to
build the SMT system.
</bodyText>
<subsectionHeader confidence="0.947559">
3.4 Generation of multiple weighted reordering
hypotheses
</subsectionHeader>
<bodyText confidence="0.9998243">
The SMR system, having its own search, can generate ei-
ther an output 1-best or an output graph. In decoding, the
SMR technique generates an output graph which is used
as an input graph by the SMT system. Figure 2 (B) shows
the corresponding block diagram in decoding: the SMR
output graph is given as an input graph to the SMT sys-
tem. Hereinafter, this either SMR output graph or SMT
input graph will be referred to as (weighted) reordering
graph. The monotonic search in the SMT system is ex-
tended with reorderings following this reordering graph.
</bodyText>
<figureCaption confidence="0.996619666666667">
Figure 3: Alignment and tuple extraction (A) original
training source corpus (B) reordered training source cor-
pus.
</figureCaption>
<bodyText confidence="0.999871666666667">
This reordering graph has multiple paths and each path
has its own weight. This weight is added as a feature
function in the log-linear model.
</bodyText>
<sectionHeader confidence="0.994496" genericHeader="method">
4 Morphological vs Statistical Classes
</sectionHeader>
<bodyText confidence="0.9998194375">
Previous SMR studies (Costa-juss`a and Fonollosa,
2006) (Costa-juss`a et al., 2007) considered only statisti-
cal classes. On the one hand, these statistical classes per-
formed fairly well and had the advantage of being suit-
able for any language. On the other hand, it should be
taken into account the fact of training them in the train-
ing set allows for unknown words in the development or
in the test set. Additionally, they do not have any reorder-
ing information because they are trained on a monolin-
gual set.
The first problem, unknown words which appear in
the development or in the test set, may be solved by us-
ing a disambiguation technique. Unknown words can be
assigned to one class by taking into account their own
context. The second problem, incorporating information
about order, might be solved by training classes in the
reordered training source corpus. In other words, we
monotonized the training corpus with the alignment in-
formation (i.e. reorder the source corpus in the way that
matches the target corpus under the alignment links cri-
terion). After that, we train the statistical classes, here-
inafter, called statistical reordered classes.
In some pair of languages, as for example En-
glish/Spanish, the reordering that may be performed is
related to word’s morphology (i.e. TAGS). Some TAGS
rules (with some lexical exceptions) can be extracted as
in (Popovic and Ney, 2006) where they were applied
with reordering purposes as a preprocessing step. An-
other approach that has related TAGS and reordering was
presented in (Crego and Mari˜no, 2006) where instead of
rules, they learned reordering patterns based on TAGS as
named in this paper’s introduction. Hence, the SMR tech-
</bodyText>
<page confidence="0.997443">
173
</page>
<table confidence="0.9996587">
Spanish English
Train Sentences 1,3M
Words 37,9M 35,5M
Vocabulary 138,9k 133k
Dev Sentences 2000 2000
Words 60.5k 58.7k
Vocabulary 8.1k 6.5k
Test Sentences 2000 2000
Words 60,2k 58k
Vocabulary 8,2k 6,5k
</table>
<tableCaption confidence="0.999833">
Table 1: Corpus Statistics.
</tableCaption>
<bodyText confidence="0.998393333333333">
nique may take advantage of the morphological informa-
tion. Notice that an advantage is that there is a TAG for
each word, hence there are not unknown words.
</bodyText>
<sectionHeader confidence="0.998514" genericHeader="method">
5 Evaluation Framework
</sectionHeader>
<subsectionHeader confidence="0.990913">
5.1 Corpus Statistics
</subsectionHeader>
<bodyText confidence="0.999874555555555">
Experiments were carried out using the data in the second
evaluation campaign of the WMT07 1.
This corpus consists in the official version of the
speeches held in the European Parliament Plenary Ses-
sions (EPPS), as available on the web page of the Eu-
ropean Parliament. Additionally, there was available a
smaller corpus (News-Commentary). Our training cor-
pus was the catenation of both. Table 1 shows the corpus
statistics.
</bodyText>
<subsectionHeader confidence="0.996262">
5.2 Tools and preprocessing
</subsectionHeader>
<bodyText confidence="0.999964058823529">
The system was built similarly to (Costa-juss`a et al.,
2007). The SMT baseline system uses the Ngram-
based approach, which has been explained in Section 2.
Tools used are defined as follows: word alignments were
computed using GIZA++ 2; language model was esti-
mated using SRILM 3; decoding was carried out with
MARIE4; an n-best re-ranking strategy is implemented
which is used for optimization purposes just as pro-
posed in http://www.statmt.org/jhuws/ using the simplex
method (Nelder and Mead, 1965) and BLEU as a loss
function.
The SMT system we use a 4gram translation language
model, a 5gram target language model and a 5gram class
target language model.
Spanish data have been processed so that the pronouns
which are attached to verbs are split up. Additionally,
several article and prepositions words are separated (i.e.
</bodyText>
<footnote confidence="0.999954">
1http://www.statmt.org/wmt07/
2http://www.fjoch.com/GIZA++.html
3http://www.speech.sri.com/projects/srilm/
4http://gps-tsc.upc.es/veu/soft/soft/marie/
</footnote>
<figureCaption confidence="0.9956265">
Figure 5: Perplexity over the manually aligned test set
given the SMR Ngram length.
</figureCaption>
<bodyText confidence="0.998399">
del goes into de el). This preprocessing was performed
using Freeling software (Atserias et al., 2006). Training
and evaluation were both true-case.
</bodyText>
<subsectionHeader confidence="0.995474">
5.3 Classes and Ngram length Study for the
SMR-Graph generation
</subsectionHeader>
<bodyText confidence="0.9998315">
This section evaluates several types of classes and n-gram
lengths in the SMR model in order to choose the SMR
configuration which provides the best results in trans-
lation in terms of quality. To accomplish this evalua-
tion, we have designed the following experiment. Given
500 manually aligned parallel sentences of the EPPS cor-
pora (Lambert et al., 2006), we order the source test in
the way that better matches the target set. This ordered
source set is considered our reference as it is based on
manual alignments. On the other hand, the 500 sen-
tences set is translated using the SMR configurations to
be tested. Finally, the Word Error Rate (WER) is used as
quality measure.
Figure 4 shows the WER behavior given different types
of classes. As statistical classes (cl50,cl100,cl200) we
used the Och monolingual classes (Och, 1999), which
can be performed using ’mkcls’ (a tool available with
GIZA). Also we used the statistical reordered classes
(cl100mono) which were explained in Section 4. Both
statistical and statistical reordered classes used the dis-
amb tool of SRILM in order to classify unknown words.
As morphological classes we used the TAGS provided by
Freeling. Clearly, statistical classes perform better than
TAGS and best results can be achieved with 100 and 200
classes and an n-gram length of 5.
For the sake of completeness, we have evaluated the
perplexity of the SMR Ngram model over the aligned test
set above and choosing 200 classes. Figure 5 is coherent
with the WER results above and it shows that perplexity
is not reduced for an n-gram length greater than 5.
</bodyText>
<page confidence="0.997847">
174
</page>
<figureCaption confidence="0.999354">
Figure 4: WER over the reference given various sets of classes and Ngram lengths.
</figureCaption>
<subsectionHeader confidence="0.997256">
5.4 Graph pruning
</subsectionHeader>
<bodyText confidence="0.9999466">
The more complex is the reordering graph, the less effi-
cient is the decoding. That is why, in this section, we ex-
periment with several ways of graph pruning. Addition-
ally, for each pruning we see the influence of considering
the graph weights (i.e. reordering feature importance).
Given that the reordering graph is the output of a beam
search decoder, we can consider pruning the reordering
graph by limiting the SMR beam, i.e. limiting the size of
hypothesis stacks.
Given a reordering graph, another option is to prune
states and arches only used in paths s times worse than
the best path.
Table 2 gives the results of the proposed pruning. Note
that computational time is given in terms of the mono-
tonic translation time (and it is the same for both direc-
tions). It is shown that graph pruning guarantees the effi-
ciency of the system and even increases the translation’s
quality. Similar results are obtained in terms of BLEU for
both types of pruning. In this task and for both translation
directions, it seems more appropriate to limit directly the
beam search in the SMR step to 5.
As expected, the influence of the reordering feature,
which takes into account the graph weights, tends to be
more important as pruning decreases (i.e. when the graph
has more paths).
</bodyText>
<table confidence="0.999799285714286">
Pruning WT BLEUE.2E3 BLEUE32E,,, TIME
b5 yes 31.32 32.64 2.4T..
b5 no 31.25 31.82 2.5T..
b50 yes 30.95 32.28 5.3T..
b50 no 30.90 27.44 4.8T..
b50 s10 yes 31.19 32.20 1.5T..
b50 s10 no 31.07 32.41 1.4T..
</table>
<tableCaption confidence="0.99836">
Table 2: Performance in BLEU in the test set of different
</tableCaption>
<bodyText confidence="0.951047">
graph pruning (b stands for beam and s for states); the
use of reordering feature function (WT indicates its use);
and the time increase related to T.. (monotonic transla-
tion time).
</bodyText>
<sectionHeader confidence="0.531129" genericHeader="evaluation">
5.5 Results and discussion
</sectionHeader>
<bodyText confidence="0.993399454545455">
Table 3 shows the performance of our Ngram-
based system using the SMR technique. First
row is the WMT07 baseline system which can
be reproduced following the instructions in
http://www.statmt.org/wmt07/baseline.html. This
baseline system uses a non-monotonic search. Second
row shows the results of the Ngram-based system
presented in section 2 using the weighted reordering
graph trained with the best configuration found in the
above section (200 statistical classes and an Ngram of
length 5).
</bodyText>
<page confidence="0.994022">
175
</page>
<table confidence="0.959189666666667">
System BLEU,�2,. BLEU,.2,�
WMT07 Of. Baseline 31.21 30.74
Ngram-based 32.64 31.32
</table>
<tableCaption confidence="0.99913">
Table 3: BLEU Results.
</tableCaption>
<sectionHeader confidence="0.967488" genericHeader="conclusions">
6 Conclusions and further work
</sectionHeader>
<bodyText confidence="0.99942265">
The proposed SMR technique can be used both in training
and test steps in a SMT system. Applying the SMR tech-
nique in the training step reduces the sparseness in the
translation vocabulary. Applying SMR technique in the
test step allows to generate a weighted reordering graph
for SMT system.
The use of classes plays an important role in the SMR
technique, and experiments have shown that statistical
classes are better than morphological ones.
Moreover, we have experimented with different graph
pruning showing that best translation results can be
achieved at a very low increase of computational cost
when comparing to the monotonic translation computa-
tional cost.
Finally, we have shown that our translation system us-
ing the SMR technique outperforms the WMT07 Official
baseline system (which uses a non-monotonic search) in
terms of BLEU.
As further work, we want to introduce the SMR tech-
nique in a state-of-the-art phrase-based system.
</bodyText>
<sectionHeader confidence="0.998946" genericHeader="acknowledgments">
7 Acknowledgments
</sectionHeader>
<bodyText confidence="0.9452665">
This work has been funded by the European Union under
the TC-STAR project (IST- 2002-FP6-506738) and the
Spanish Government under grant TEC2006-13964-C03
(AVIVAVOZ project).
</bodyText>
<sectionHeader confidence="0.998534" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999289389830509">
J. Atserias, B. Casas, E. Comelles, M. Gonz´alez,
L. Padr´o, and M. Padr´o. 2006. Freeling 1.3: Syntactic
and semantic services in an open-source nlp library. In
5th Int. Conf. on Language Resource and Evaluation
(LREC), pages 184–187.
P. Brown, S. Della Pietra, V. Della Pietra, and R. Mercer.
1993. The mathematics of statistical machine transla-
tion. Computational Linguistics, 19(2):263–311.
M. Collins, P. Koehn, and I. Kucerov´a. 2005. Clause
restructuring for statistical machine translation. In 43st
Annual Meeting of the Association for Computational
Linguistics (ACL’05), pages 531 – 540, Michigan.
M.R. Costa-juss`a and J.A.R. Fonollosa. 2006. Statistical
machine reordering. In Empirical Methods in Natural
Language Processing (EMNLP), pages 71–77, Sydney.
M. R. Costa-juss`a, P. Lambert, J.M. Crego, M. Khalilov,
J.A.R. Fonollosa, J.B. Mari˜no, and R. Banchs. 2007.
Ngram-based statistical machine translation enhanced
with multiple weighted reordering hypotheses. In
ACL: Workshop of Statistical Machine Translation
(WMT07), Prague.
J.M. Crego and J.B. Mari˜no. 2006. Reordering exper-
iments for n-gram-based smt. Ist IEEE/ACL Inter-
national Workshop on Spoken Language Technology
(SLT’06), pages 242–245.
J. M. Crego, M. R. Costa-juss`a, J. Mari˜no, and J. A.
Fonollosa. 2005. Ngram-based versus phrase-
based statistical machine translation. In Proc. of
the Int. Workshop on Spoken Language Translation,
IWSLT’05, pages 177–184, Pittsburgh, October.
S. Kanthak, D. Vilar, E. Matusov, R. Zens, and H. Ney.
2005. Novel reordering approaches in phrase-based
statistical machine translation. In Proceedings of the
ACL Workshop on Building and Using Parallel Texts:
Data-Driven Machine Translation and Beyond, pages
167–174, Ann Arbor, MI, June.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statisti-
cal phrase-based translation. In Proc. of the Human
Language Technology Conference, HLT-NAACL’2003,
pages 48 – 54, Edmonton, Canada, May.
P. Lambert, A. de Gispert, R. Banchs, and J. Mari˜no.
2006. Guidelines for word alignment and man-
ual alignment. Language Resources and Evaluation,
39(4):267–285.
J.B. Mari˜no, R.E. Banchs, J.M. Crego, A. de Gispert,
P. Lambert, J.A.R. Fonollosa, and M.R. Costa-juss`a.
2006. N-gram based machine translation. Computa-
tional Linguistics, 32(4):527–549.
J.A. Nelder and R. Mead. 1965. A simplex method for
function minimization. The Computer Journal, 7:308–
313.
F.J. Och. 1999. An efficient method for determining
bilingual word classes. In 9th Conf. of the European
Chapter of the Association for Computational Linguis-
tics (EACL), pages 71–76, June.
M. Popovic and H. Ney. 2006. Pos-based word reorder-
ings for statistical machine translation. In 5th Interna-
tional Conference on Language Resources and Evalu-
ation (LREC), pages 1278–1283, Genova, May.
</reference>
<page confidence="0.998728">
176
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.823712">
<title confidence="0.9550685">Analysis of statistical and morphological classes to generate reordering hypotheses on a Statistical Machine Translation system</title>
<author confidence="0.999976">Marta R Costa-juss`a</author>
<author confidence="0.999976">A R Jos´e</author>
<affiliation confidence="0.9994275">Department of Signal Theory and TALP Research Center</affiliation>
<address confidence="0.970531">Barcelona 08034,</address>
<email confidence="0.999774">(mruiz,adrian)@gps.tsc.upc.edu</email>
<abstract confidence="0.991118136363636">One main challenge of statistical machine translation (SMT) is dealing with word order. The main idea of the statistical machine reordering (SMR) approach is to use the powerful techniques of SMT systems to generate a weighted reordering graph for SMT systems. This technique supplies reordering constraints to an SMT system, using statistical criteria. In this paper, we experiment with different graph pruning which guarantees the translation quality improvement due to reordering at a very low increase of computational cost. The SMR approach is capable of generalizing reorderings, which have been learned during training, by using word classes instead of words themselves. We experiment with statistical and morphological classes in order to choose those which capture the most probable reorderings. Satisfactory results are reported in the WMT07 Es/En task. Our system outperforms in terms of BLEU the WMT07 Official baseline system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Atserias</author>
<author>B Casas</author>
<author>E Comelles</author>
<author>M Gonz´alez</author>
<author>L Padr´o</author>
<author>M Padr´o</author>
</authors>
<title>Freeling 1.3: Syntactic and semantic services in an open-source nlp library.</title>
<date>2006</date>
<booktitle>In 5th Int. Conf. on Language Resource and Evaluation (LREC),</booktitle>
<pages>184--187</pages>
<marker>Atserias, Casas, Comelles, Gonz´alez, Padr´o, Padr´o, 2006</marker>
<rawString>J. Atserias, B. Casas, E. Comelles, M. Gonz´alez, L. Padr´o, and M. Padr´o. 2006. Freeling 1.3: Syntactic and semantic services in an open-source nlp library. In 5th Int. Conf. on Language Resource and Evaluation (LREC), pages 184–187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>S Della Pietra</author>
<author>V Della Pietra</author>
<author>R Mercer</author>
</authors>
<title>The mathematics of statistical machine translation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="5831" citStr="Brown et al., 1993" startWordPosition="906" endWordPosition="909">, which is trained from the target side of the bilingual corpus. • A class target language model. This feature consists of a 5-gram model of words classes, which is trained from the target side of the bilingual corpus using the statistical classes from (Och, 1999). • A word bonus function. This feature introduces a bonus based on the number of target words contained in the partial-translation hypothesis. It is used to compensate for the system’s preference for short output sentences. • A source-to-target lexicon model. This feature, which is based on the lexical parameters of the IBM Model 1 (Brown et al., 1993), provides a complementary probability for each tuple in the translation table. These lexicon parameters are obtained from the source-to-target alignments. • A target-to-source lexicon model. Similarly to the previous feature, this feature is based on the lexical parameters of the IBM Model 1 but, in this case, these parameters are obtained from target-to-source alignments. Figure 1: SMR block diagram. 3 SMR Baseline System As mentioned in the introduction, SMR and SMT are based on the same principles. 3.1 Concept The aim of SMR consists in using an SMT system to deal with reordering problems.</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. Brown, S. Della Pietra, V. Della Pietra, and R. Mercer. 1993. The mathematics of statistical machine translation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>P Koehn</author>
<author>I Kucerov´a</author>
</authors>
<title>Clause restructuring for statistical machine translation.</title>
<date>2005</date>
<booktitle>In 43st Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>531--540</pages>
<location>Michigan.</location>
<marker>Collins, Koehn, Kucerov´a, 2005</marker>
<rawString>M. Collins, P. Koehn, and I. Kucerov´a. 2005. Clause restructuring for statistical machine translation. In 43st Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 531 – 540, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M R Costa-juss`a</author>
<author>J A R Fonollosa</author>
</authors>
<title>Statistical machine reordering.</title>
<date>2006</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>71--77</pages>
<location>Sydney.</location>
<marker>Costa-juss`a, Fonollosa, 2006</marker>
<rawString>M.R. Costa-juss`a and J.A.R. Fonollosa. 2006. Statistical machine reordering. In Empirical Methods in Natural Language Processing (EMNLP), pages 71–77, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M R Costa-juss`a</author>
<author>P Lambert</author>
<author>J M Crego</author>
<author>M Khalilov</author>
<author>J A R Fonollosa</author>
<author>J B Mari˜no</author>
<author>R Banchs</author>
</authors>
<title>Ngram-based statistical machine translation enhanced with multiple weighted reordering hypotheses.</title>
<date>2007</date>
<booktitle>In ACL: Workshop of Statistical Machine Translation (WMT07),</booktitle>
<location>Prague.</location>
<marker>Costa-juss`a, Lambert, Crego, Khalilov, Fonollosa, Mari˜no, Banchs, 2007</marker>
<rawString>M. R. Costa-juss`a, P. Lambert, J.M. Crego, M. Khalilov, J.A.R. Fonollosa, J.B. Mari˜no, and R. Banchs. 2007. Ngram-based statistical machine translation enhanced with multiple weighted reordering hypotheses. In ACL: Workshop of Statistical Machine Translation (WMT07), Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Crego</author>
<author>J B Mari˜no</author>
</authors>
<title>Reordering experiments for n-gram-based smt.</title>
<date>2006</date>
<booktitle>Ist IEEE/ACL International Workshop on Spoken Language Technology (SLT’06),</booktitle>
<pages>242--245</pages>
<marker>Crego, Mari˜no, 2006</marker>
<rawString>J.M. Crego and J.B. Mari˜no. 2006. Reordering experiments for n-gram-based smt. Ist IEEE/ACL International Workshop on Spoken Language Technology (SLT’06), pages 242–245.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Crego</author>
<author>M R Costa-juss`a</author>
<author>J Mari˜no</author>
<author>J A Fonollosa</author>
</authors>
<title>Ngram-based versus phrasebased statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. of the Int. Workshop on Spoken Language Translation, IWSLT’05,</booktitle>
<pages>177--184</pages>
<location>Pittsburgh,</location>
<marker>Crego, Costa-juss`a, Mari˜no, Fonollosa, 2005</marker>
<rawString>J. M. Crego, M. R. Costa-juss`a, J. Mari˜no, and J. A. Fonollosa. 2005. Ngram-based versus phrasebased statistical machine translation. In Proc. of the Int. Workshop on Spoken Language Translation, IWSLT’05, pages 177–184, Pittsburgh, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kanthak</author>
<author>D Vilar</author>
<author>E Matusov</author>
<author>R Zens</author>
<author>H Ney</author>
</authors>
<title>Novel reordering approaches in phrase-based statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Building and Using Parallel Texts: Data-Driven Machine Translation and Beyond,</booktitle>
<pages>167--174</pages>
<location>Ann Arbor, MI,</location>
<contexts>
<context position="2175" citStr="Kanthak et al. (2005)" startWordPosition="320" endWordPosition="323">rom the phrase-based: training data is monotonically segmented into bilingual units; and, the model considers ngram probabilities rather than relative frequencies. The n-gram-based system follows a maximum entropy approach, in which a log-linear combination of multiple models is implemented (Mari˜no et al., 2006), as an alternative to the source-channel approach. Introducing reordering capabilities is important in both systems. Recently, new reordering strategies have been proposed such as the reordering of each source sentence to match the word order in the corresponding target sentence, see Kanthak et al. (2005) and Mari˜no et al. (2006). These approaches are applied in the training set and they lack of reordering generalization. Applied both in the training and decoding step, Collins et al. (2005) describe a method for introducing syntactic information for reordering in SMT. This approach is applied as a pre-processing step. Differently, Crego et al. (2006) presents a reordering approach based on reordering patterns which is coupled with decoding. The reordering patterns are learned directly from word alignment and all reorderings have the same probability. In our previous work (Costa-juss`a and Fon</context>
</contexts>
<marker>Kanthak, Vilar, Matusov, Zens, Ney, 2005</marker>
<rawString>S. Kanthak, D. Vilar, E. Matusov, R. Zens, and H. Ney. 2005. Novel reordering approaches in phrase-based statistical machine translation. In Proceedings of the ACL Workshop on Building and Using Parallel Texts: Data-Driven Machine Translation and Beyond, pages 167–174, Ann Arbor, MI, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proc. of the Human Language Technology Conference, HLT-NAACL’2003, pages 48 – 54,</booktitle>
<location>Edmonton, Canada,</location>
<contexts>
<context position="1366" citStr="Koehn et al., 2003" startWordPosition="197" endWordPosition="200">ees the translation quality improvement due to reordering at a very low increase of computational cost. The SMR approach is capable of generalizing reorderings, which have been learned during training, by using word classes instead of words themselves. We experiment with statistical and morphological classes in order to choose those which capture the most probable reorderings. Satisfactory results are reported in the WMT07 Es/En task. Our system outperforms in terms of BLEU the WMT07 Official baseline system. 1 Introduction Nowadays, statistical machine translation is mainly based on phrases (Koehn et al., 2003). In parallel to this phrasebased approach, the use of bilingual n-grams gives comparable results, as shown by Crego et al. (2005). Two basic issues differentiate the n-gram-based system from the phrase-based: training data is monotonically segmented into bilingual units; and, the model considers ngram probabilities rather than relative frequencies. The n-gram-based system follows a maximum entropy approach, in which a log-linear combination of multiple models is implemented (Mari˜no et al., 2006), as an alternative to the source-channel approach. Introducing reordering capabilities is importa</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn, F.J. Och, and D. Marcu. 2003. Statistical phrase-based translation. In Proc. of the Human Language Technology Conference, HLT-NAACL’2003, pages 48 – 54, Edmonton, Canada, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Lambert</author>
<author>A de Gispert</author>
<author>R Banchs</author>
<author>J Mari˜no</author>
</authors>
<title>Guidelines for word alignment and manual alignment.</title>
<date>2006</date>
<journal>Language Resources and Evaluation,</journal>
<volume>39</volume>
<issue>4</issue>
<marker>Lambert, de Gispert, Banchs, Mari˜no, 2006</marker>
<rawString>P. Lambert, A. de Gispert, R. Banchs, and J. Mari˜no. 2006. Guidelines for word alignment and manual alignment. Language Resources and Evaluation, 39(4):267–285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J B Mari˜no</author>
<author>R E Banchs</author>
<author>J M Crego</author>
<author>A de Gispert</author>
<author>P Lambert</author>
<author>J A R Fonollosa</author>
<author>M R Costa-juss`a</author>
</authors>
<title>N-gram based machine translation.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>4</issue>
<marker>Mari˜no, Banchs, Crego, de Gispert, Lambert, Fonollosa, Costa-juss`a, 2006</marker>
<rawString>J.B. Mari˜no, R.E. Banchs, J.M. Crego, A. de Gispert, P. Lambert, J.A.R. Fonollosa, and M.R. Costa-juss`a. 2006. N-gram based machine translation. Computational Linguistics, 32(4):527–549.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A Nelder</author>
<author>R Mead</author>
</authors>
<title>A simplex method for function minimization.</title>
<date>1965</date>
<journal>The Computer Journal,</journal>
<volume>7</volume>
<pages>313</pages>
<contexts>
<context position="13171" citStr="Nelder and Mead, 1965" startWordPosition="2123" endWordPosition="2126">ntary). Our training corpus was the catenation of both. Table 1 shows the corpus statistics. 5.2 Tools and preprocessing The system was built similarly to (Costa-juss`a et al., 2007). The SMT baseline system uses the Ngrambased approach, which has been explained in Section 2. Tools used are defined as follows: word alignments were computed using GIZA++ 2; language model was estimated using SRILM 3; decoding was carried out with MARIE4; an n-best re-ranking strategy is implemented which is used for optimization purposes just as proposed in http://www.statmt.org/jhuws/ using the simplex method (Nelder and Mead, 1965) and BLEU as a loss function. The SMT system we use a 4gram translation language model, a 5gram target language model and a 5gram class target language model. Spanish data have been processed so that the pronouns which are attached to verbs are split up. Additionally, several article and prepositions words are separated (i.e. 1http://www.statmt.org/wmt07/ 2http://www.fjoch.com/GIZA++.html 3http://www.speech.sri.com/projects/srilm/ 4http://gps-tsc.upc.es/veu/soft/soft/marie/ Figure 5: Perplexity over the manually aligned test set given the SMR Ngram length. del goes into de el). This preprocess</context>
</contexts>
<marker>Nelder, Mead, 1965</marker>
<rawString>J.A. Nelder and R. Mead. 1965. A simplex method for function minimization. The Computer Journal, 7:308– 313.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>An efficient method for determining bilingual word classes.</title>
<date>1999</date>
<booktitle>In 9th Conf. of the European Chapter of the Association for Computational Linguistics (EACL),</booktitle>
<pages>71--76</pages>
<contexts>
<context position="5476" citStr="Och, 1999" startWordPosition="849" endWordPosition="850">nstraints. As a result of these constraints, only one segmentation is possible for a given sentence pair. In addition to the bilingual n-gram translation model, the baseline system implements a log-linear combination of feature functions, which are described as follows: • A target language model. This feature consists of a 4-gram model of words, which is trained from the target side of the bilingual corpus. • A class target language model. This feature consists of a 5-gram model of words classes, which is trained from the target side of the bilingual corpus using the statistical classes from (Och, 1999). • A word bonus function. This feature introduces a bonus based on the number of target words contained in the partial-translation hypothesis. It is used to compensate for the system’s preference for short output sentences. • A source-to-target lexicon model. This feature, which is based on the lexical parameters of the IBM Model 1 (Brown et al., 1993), provides a complementary probability for each tuple in the translation table. These lexicon parameters are obtained from the source-to-target alignments. • A target-to-source lexicon model. Similarly to the previous feature, this feature is ba</context>
<context position="14781" citStr="Och, 1999" startWordPosition="2369" endWordPosition="2370">we have designed the following experiment. Given 500 manually aligned parallel sentences of the EPPS corpora (Lambert et al., 2006), we order the source test in the way that better matches the target set. This ordered source set is considered our reference as it is based on manual alignments. On the other hand, the 500 sentences set is translated using the SMR configurations to be tested. Finally, the Word Error Rate (WER) is used as quality measure. Figure 4 shows the WER behavior given different types of classes. As statistical classes (cl50,cl100,cl200) we used the Och monolingual classes (Och, 1999), which can be performed using ’mkcls’ (a tool available with GIZA). Also we used the statistical reordered classes (cl100mono) which were explained in Section 4. Both statistical and statistical reordered classes used the disamb tool of SRILM in order to classify unknown words. As morphological classes we used the TAGS provided by Freeling. Clearly, statistical classes perform better than TAGS and best results can be achieved with 100 and 200 classes and an n-gram length of 5. For the sake of completeness, we have evaluated the perplexity of the SMR Ngram model over the aligned test set above</context>
</contexts>
<marker>Och, 1999</marker>
<rawString>F.J. Och. 1999. An efficient method for determining bilingual word classes. In 9th Conf. of the European Chapter of the Association for Computational Linguistics (EACL), pages 71–76, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Popovic</author>
<author>H Ney</author>
</authors>
<title>Pos-based word reorderings for statistical machine translation.</title>
<date>2006</date>
<booktitle>In 5th International Conference on Language Resources and Evaluation (LREC),</booktitle>
<pages>1278--1283</pages>
<location>Genova,</location>
<contexts>
<context position="11480" citStr="Popovic and Ney, 2006" startWordPosition="1853" endWordPosition="1856">mation about order, might be solved by training classes in the reordered training source corpus. In other words, we monotonized the training corpus with the alignment information (i.e. reorder the source corpus in the way that matches the target corpus under the alignment links criterion). After that, we train the statistical classes, hereinafter, called statistical reordered classes. In some pair of languages, as for example English/Spanish, the reordering that may be performed is related to word’s morphology (i.e. TAGS). Some TAGS rules (with some lexical exceptions) can be extracted as in (Popovic and Ney, 2006) where they were applied with reordering purposes as a preprocessing step. Another approach that has related TAGS and reordering was presented in (Crego and Mari˜no, 2006) where instead of rules, they learned reordering patterns based on TAGS as named in this paper’s introduction. Hence, the SMR tech173 Spanish English Train Sentences 1,3M Words 37,9M 35,5M Vocabulary 138,9k 133k Dev Sentences 2000 2000 Words 60.5k 58.7k Vocabulary 8.1k 6.5k Test Sentences 2000 2000 Words 60,2k 58k Vocabulary 8,2k 6,5k Table 1: Corpus Statistics. nique may take advantage of the morphological information. Notic</context>
</contexts>
<marker>Popovic, Ney, 2006</marker>
<rawString>M. Popovic and H. Ney. 2006. Pos-based word reorderings for statistical machine translation. In 5th International Conference on Language Resources and Evaluation (LREC), pages 1278–1283, Genova, May.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>