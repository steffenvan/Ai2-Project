<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000312">
<title confidence="0.9928835">
Reliability and Type of Consumer Health Documents on the World Wide
Web: an Annotation Study
</title>
<author confidence="0.98907">
Melanie J. Martin
</author>
<affiliation confidence="0.997895">
California State University, Stanislaus
One University Circle
</affiliation>
<address confidence="0.602414">
Turlock, CA 95382
</address>
<email confidence="0.990321">
mmartin@cs.csustan.edu
</email>
<sectionHeader confidence="0.99548" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999327266666667">
In this paper we present a detailed scheme
for annotating medical web pages de-
signed for health care consumers. The
annotation is along two axes: first, by re-
liability or the extent to which the medical
information on the page can be trusted,
second, by the type of page (patient leaf-
let, commercial, link, medical article, tes-
timonial, or support). We analyze inter-
rater agreement among three judges for
each category. Inter-rater agreement was
moderate (0.77 accuracy, 0.62 F-measure,
0.49 Kappa) on the reliability axis and
good (0.81 accuracy, 0.72 F-measure,
0.73 Kappa) along the type axis.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999942370370371">
With the explosive growth of the World Wide Web
has come, not just an explosion of information, but
also the explosion of false, misleading and unsup-
ported information. At the same time, the web is
increasingly used for tasks where information qual-
ity and reliability are vital, from legal and medical
research by both professionals and lay people, to
fact checking by journalists and research by gov-
ernment policy makers.
In particular, there has been a proliferation of
web pages in the medical domain for health care
consumers. At the first sign of illness or injury
more and more people go to the web before con-
sulting medical professionals. The quality and reli-
ability of the information on consumer medical
web pages has been of concern for some time to
medical professionals and policy makers. (For ex-
ample see Eysenbach et al., 2002, Impicciatore et
al., 1997.)
Our goal is to create a system that can automati-
cally measure the reliability of web pages in the
medical domain (Martin, 2004). More specifically,
given a web page resulting from a user query on a
medical topic, we would like to automatically pro-
vide an estimate of the extent to which the infor-
mation on the page can be trusted. In order to make
use of supervised natural language processing and
machine learning algorithms to create such a sys-
tem, and to ultimately evaluate the performance of
the system, it is necessary to have human anno-
tated data.
It is important to note the varied uses of the term
“reliability” in the computer and information sci-
ences. In the current context we use it to refer to an
intrinsic property of a web page: essentially the
trustworthiness of the information it contains. This
sense of reliability is distinct from its meaning in
measurement theory as an indicator of repeatabil-
ity. It also excludes measures such as credibility
that are based on user beliefs or understanding.
In this paper we report results of an annotation
study of medical web pages designed for health
care consumers. Three humans annotated a corpus
of web pages along two axes. The first axis is the
reliability of the information contained in the page.
The second axis is the type, or kind, of page. Inter-
coder agreement was moderate (0.77 accuracy,
0.62 F-measure, 0.49 Kappa) on the reliability axis
and good (0.81 accuracy, 0.72 F-measure, 0.73
Kappa) along the type axis.
In our materials and methods section we discuss
the data, definitions, annotation study and the re-
sults. We follow with a discussion section and a
conclusion.
</bodyText>
<page confidence="0.994571">
38
</page>
<note confidence="0.81119975">
Proceedings of the NAACL HLT 2010 Second Louhi Workshop on Text and Data Mining of Health Documents, pages 38–45,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
2 Materials and Methods from Google the top 100 search results for each of
the following 10 queries:
</note>
<bodyText confidence="0.995740666666667">
In this section we will discuss the data and definitions
for the annotation task. We also describe the annotation
study and the testing and analysis.
</bodyText>
<subsectionHeader confidence="0.888931">
2.1 Data
</subsectionHeader>
<bodyText confidence="0.9996486">
The data to be annotated consists of two corpora of
web pages created by the author: IBS70 and
MMED100. The MMED100 corpus is a subset of a
larger corpus (MMED1000). Both corpora are de-
scribed below.
</bodyText>
<sectionHeader confidence="0.501536" genericHeader="method">
2.1.1 IBS70 Corpus
</sectionHeader>
<bodyText confidence="0.999977466666667">
The IBS70 corpus was created as an exploratory
corpus for use in system development. It was
originally the top 50 Google hits for &amp;quot;irritable
bowel syndrome&amp;quot; downloaded automatically
though the Google API on July 1, 2004. The query
was chosen to provide a range of quality and types
of pages which one would expect to see more gen-
erally in the medical domain on the web: patient
information from both traditional and alternative
sources, support groups, medical articles, commer-
cial pages from drug companies and quacks. Dur-
ing system development we determined that it
would be useful to have additional pages at both
ends of the reliability spectrum, possibly to use as
seeds for clustering.
On September 15, 2004, twenty documents were
added to the corpus to create the IBS70. Ten highly
reliable documents were added based on web
searches to find documents judged as meeting the
standards of Evidence Based Medicine. Ten docu-
ments judged unreliable were added by taking the
first ten relevant “Sponsored Links” resulting from
a Google search on “irritable bowel syndrome”.
There are two important things to note about this
process: first, the high quality pages added were
disproportionately from the U.K.; second, the low
quality pages tend toward the crassly commercial
and are more extreme than one would likely find in
this proportion of the top 100 (or even 200) of the
results of a Google query for a medical condition.
</bodyText>
<subsectionHeader confidence="0.454056">
2.1.2 MMED100 Corpus
</subsectionHeader>
<bodyText confidence="0.9467015">
The MMED1000 corpus was created on November
5th and 8th, 2004 by automatically downloading
</bodyText>
<listItem confidence="0.9999794">
• Adrenoleukodystrophy
• Alzheimer&apos;s
• Endometriosis
• Fibromyalgia
• Obesity
• Pancreatic cancer
• Colloidal Silver
• Irritable Bowel Syndrome
• Late Lyme Disease
• Lower Back Pain
</listItem>
<bodyText confidence="0.999950387096774">
The queries were chosen to provide a broad range
of what might be typical queries for health con-
sumers on the web and the types of pages that
would result from these queries.
Colloidal Silver was chosen in the hopes of pro-
viding a sufficient number of pages of questionable
reliability. Adrenoleukodystrophy, Pancreatic Can-
cer, Alzheimer’s and Obesity were chosen because
there is general agreement in the medical commu-
nity that these are diseases or health issues, and on
diagnostic techniques. They also cover a spectrum
of occurrence rates, with Adrenoleukodystrophy
being relatively rare and Obesity being relatively
common. The other five queries were chosen be-
cause there is less agreement in both the medical
community and the general population about the
existence, frequency, severity and treatment of
these conditions. In particular, Fibromyalgia and
IBS can be exclusionary diagnoses without clear
and successful treatment options, which can open
the door to web pages with a range of questionable
treatments.
For annotation purposes a subset of this corpus,
MMED100, with 100 pages, was created by ran-
domly selecting ten documents from each of the
ten queries.
At this time neither corpus is publicly available.
However they can be provided on request and it is
anticipated that they will be made publicly avail-
able once a viable standard is established for the
annotations.
</bodyText>
<subsectionHeader confidence="0.993735">
2.2 Definitions
</subsectionHeader>
<bodyText confidence="0.999971333333333">
The primary classification task is to classify pages
based on their reliability (quality or trustworthiness
of the information they contain). The secondary
</bodyText>
<page confidence="0.998883">
39
</page>
<bodyText confidence="0.999941588235294">
classification task is to classify pages based on
their type (e.g. commercial, patient leaflet, link).
The classification by type emerged from the hy-
pothesis that different types of pages may need to
be treated differently to classify them based on
their reliability. For example, if the primary pur-
pose of a page is to provide links to information,
determining the reliability of the page may require
determining the reliability of the pages to which it
links. However, in the current study, annotators are
provided only the given web page and not allowed
to follow links, so their reliability determination
was made based on the apparent balance and ob-
jectivity of the links on the page.
For both tasks, only one tag was allowed, so an-
notators were instructed to consider the main pur-
pose or intent of the page.
</bodyText>
<subsectionHeader confidence="0.452077">
2.2.1 Reliability
</subsectionHeader>
<bodyText confidence="0.9967955">
Reliability of web pages is annotated based on a
five level scale.
</bodyText>
<subsectionHeader confidence="0.799499">
Probably Reliable (PrR)
</subsectionHeader>
<bodyText confidence="0.999915461538462">
The information on these pages appears to be com-
plete and correct, meeting the standards of Evi-
dence-Based Medicine where appropriate.
Information is presented in a balanced and objec-
tive manner, with the full range of options dis-
cussed (where appropriate). The page and author
appear reputable, with no obvious conflicts of in-
terest. The appropriate disclaimers, policies, and
contact information are present. Where appropri-
ate, sources are cited. An example of a page in this
category would be a patient leaflet from a reputa-
ble source that adheres to the standards of Evi-
dence-Based Medicine.
</bodyText>
<subsectionHeader confidence="0.853244">
Possibly Reliable (PoR)
</subsectionHeader>
<bodyText confidence="0.997202533333333">
The information on the page is generally good and
without obvious false or outdated statements, but
may not be sufficiently complete and balanced or
may not conform to evidence-based standards. An
example of a page in this category would be a pa-
tient leaflet that contains only a brief description of
diagnostic procedures or suggests a treatment op-
tion that is generally accepted, but not supported
by evidence.
Unable to determine (N)
For these pages it is difficult or impossible to de-
termine the reliability, generally because there is
not enough information. For example, the page
may be blank, only contain login information, or
be the front page of a medical journal.
</bodyText>
<subsectionHeader confidence="0.873977">
Possibly Unreliable (PoU)
</subsectionHeader>
<bodyText confidence="0.999987625">
These pages may contain some reliable informa-
tion, but either have some that is outdated, false or
misleading, or the information is sufficiently un-
balanced so as to be somewhat misleading. An ex-
ample of a page that might fall into this category is
a practitioner commercial pages, which has valid
information about an illness, but only discuss the
preferred treatment offered by the practitioner.
</bodyText>
<subsectionHeader confidence="0.782444">
Probably Unreliable (PrU)
</subsectionHeader>
<bodyText confidence="0.99997">
These pages contain false or misleading informa-
tion, or present an unbalanced or biased viewpoint
on the topic. Examples of pages in this category
would include: testimonials (unsupported view-
points or opinions of a single individual) or pages
that are clearly promoting and selling a single
treatment option.
</bodyText>
<subsectionHeader confidence="0.990465">
2.2.2 Type of Page
</subsectionHeader>
<bodyText confidence="0.999997888888889">
We found six types of pages that frequently come
up in search results for queries in the medical do-
main: Commercial, Patient Leaflet, Link, Medical
Articles, Support, and Testimonials. There are also
pages which are not relevant, or do not contain
sufficient information to make a determination.
Below we discuss each of these types. When a
page seems to overlap categories the annotation is
based on the primary purpose of the page.
</bodyText>
<subsectionHeader confidence="0.913694">
Commercial (C)
</subsectionHeader>
<bodyText confidence="0.99998725">
The primary purpose of these pages is to sell some-
thing, for example, pages about an ailment spon-
sored by a drug (also more general treatment or
equipment) company, which sells a drug to treat it.
Given the desire to sell, these pages might not pre-
sent complete or balanced information (making
them less likely to be reliable). Practitioner pages
with no real (substantial) information, which are
designed to get people to make an appointment, as
opposed to patient leaflets (designed to supplement
information that patients receive in the office or
clinic), might also fall into this category
</bodyText>
<page confidence="0.962373">
40
</page>
<subsectionHeader confidence="0.334317">
Link (L)
</subsectionHeader>
<bodyText confidence="0.99652276">
The primary purpose of these pages is to provide
links to other pages or sites (external), which will
provide information about a certain illness or
medical condition. These links may or may not be
annotated, and the degree of annotation may vary
considerably. Since the reliability of these pages
depends on the reliability of the pages they link to
(possibly also on the text in the annotations), with-
out following the links a reliability estimate can be
based on the range and apparent objectivity of the
links.
Patient Leaflet, Brochure, Fact Sheet or FAQ
(P)
The primary purpose of these pages is to provide
information to patients about a specific illness or
medical condition. Generally, these pages will be
produced by a clinic, medical center, physician, or
government agency, etc. The primary purpose is to
provide information. This class needs to be distin-
guished from medical articles, especially in ency-
clopedias or the Merck Manual, etc. These pages
will tend to have headings like: symptoms, diagno-
sis, treatment, etc. These headings can take the
form of links to specific parts of the same page or
to other pages on the same site (internal). The reli-
ability of these pages is based on their content and
determined by factors including Evidence-Based
Medicine, completeness, and the presence of incor-
rect or outdated information.
Medical Article (practitioner or consumer)
(MA)
The primary purpose of these pages is to discuss an
aspect of a specific illness or medical condition, or
a specific illness or medical condition. These can
be divided into two main categories: articles aimed
at consumers and articles aimed at health practitio-
ners.
Articles aimed at health practitioners, particu-
larly doctors, may be scientific research articles.
The reliability of these pages is based on their con-
tent and determined by factors including Evidence
Based Medicine, completeness, and the presence of
incorrect or outdated information. Note: Medline
search results may be considered a links page to
medical articles.
Articles aimed at consumers may come from a
variety of sources including mainstream and alter-
native media sources. Reliability is determined
based on the content as with articles for practitio-
ners.
</bodyText>
<subsectionHeader confidence="0.67787">
Testimonial (T)
</subsectionHeader>
<bodyText confidence="0.999757">
The primary purpose of these pages is to provide
testimonial(s) of individuals about their experience
with an illness, condition, or treatment. While in-
dividuals may be considered reliable when discuss-
ing their own personal experiences, these pages
tend to be unreliable, because they are generally
not objective or balanced. There is a tendency for
readers to generalize from very specific informa-
tion or experiences provided by the testimonial,
which can be misleading.
</bodyText>
<subsectionHeader confidence="0.779196">
Support (S)
</subsectionHeader>
<bodyText confidence="0.9999934">
The primary purpose of these pages is to provide
support of sufferers (or their loved ones or care-
givers) of a particular illness or condition. The
pages may contain information, similar to that
found in a patient leaflet; links to other sites, simi-
lar to a links page; and testimonials. In addition
they may contain facilities such as chat rooms,
newsletters, and email lists. Activities may include
lobbying for funding for research, generally put up
by individuals or non-profit organizations. For re-
liability, one may need to look at the agenda of the
authors or group. It may be in their interest (politi-
cally) to overstate the problem or make things out
to be worse then they are to secure increased fund-
ing or sympathy for their cause.
</bodyText>
<subsectionHeader confidence="0.668428">
Not Relevant (N)
</subsectionHeader>
<bodyText confidence="0.999491666666667">
These pages are blank or not relevant and include:
login pages, conditions of use pages, and medical
journal front pages.
</bodyText>
<subsectionHeader confidence="0.999795">
2.3 Annotation Study
</subsectionHeader>
<bodyText confidence="0.999935444444444">
In order to get started with system development, a
single annotator, M, who was involved with devel-
opment of both the classifications and the system,
tagged the IBS70 and MMED100. Then in Spring
2008 two senior undergraduate science majors
(chemistry and biology), L and E, were hired for
the annotation study. The annotation study con-
sisted to two primary phases: training and testing.
Each phase is described below.
</bodyText>
<page confidence="0.998634">
41
</page>
<subsectionHeader confidence="0.661902">
2.3.1 Training Phase
</subsectionHeader>
<bodyText confidence="0.999885866666667">
The two student annotators, L and E, received cop-
ies of the draft annotation instructions. They each
met individually with M to discuss the instructions
and any questions they had.
For each of three training runs, ten randomly
chosen web pages from the IBS70 corpus were
posted on a private web site. The students anno-
tated the pages for reliability and type and then met
individually to discuss their annotations with M.
As questions and issues arose, the instructions
were amended to reflect clarifications. For exam-
ple, L needed additional instructions on the distinc-
tion between Link and Patient Leaflet pages; a
separate category for FAQs was collapsed into the
Patient Leaflet category.
</bodyText>
<subsectionHeader confidence="0.858633">
2.3.2 Testing Phase
</subsectionHeader>
<bodyText confidence="0.999755666666667">
Once the student annotators seemed to be achiev-
ing reasonable levels of agreement (Cohen’s
Kappa above 0.4) on each task, there was a three-
part testing phase. The remaining 40 pages in the
IBS70 corpus were randomly divided into two test
corpora and finally the MMED100 corpus was an-
notated.
During the testing phase, one of the students, L,
seemed to annotate less carefully. (Possibly be-
cause the timing coincided with graduation and
summer vacation.) For example, on the MMED100
corpus L tagged 30% as N (unable to determine the
reliability, compared to 12% for E and 10% for M.
L was asked to go back and reconsider the web
pages tagged as N. We report results with L’s re-
considered tags here for completeness, but further
discussion will focus on agreement between M and
E.
</bodyText>
<subsectionHeader confidence="0.999023">
2.4 Testing and Analysis
</subsectionHeader>
<bodyText confidence="0.9997318">
We report inter-rater agreement using accuracy,
Cohen’s Kappa statistic (Cohen, 1960) for chance
corrected agreement and F-Measure (Hripcsak and
Rothschild, 2005). We consider each annotation
axis separately.
</bodyText>
<subsectionHeader confidence="0.845932">
2.4.1 Page Reliability
</subsectionHeader>
<bodyText confidence="0.999747733333333">
We can estimate a baseline distribution of the cate-
gories R (reliable), N (unable to determine), and U
(unreliable) based on an average of the tags across
all training and test sets to be approximately: 68%
R; 13% N; 19% U.
Table 1 shows the results for the Accuracy (per-
cent agreement) and Kappa statistic on the five
reliability classes across all the corpora. It became
immediately clear the annotators were not able to
make the more fine-grained distinctions between
“probably” and “possibly” for either the reliable or
unreliable classes, given the current instructions
and timeline. The classes were then collapsed to
three: R (reliable), N (unable to determine) and U
(unreliable) and the results are shown in Table 2.
</bodyText>
<table confidence="0.999086833333333">
Accuracy/ 5 Classes Reliability
Kappa
Set\Raters M-E M-L E-L
IBS train 0.47/0.30 0.33/0.12 0.40/0.19
IBS test 0.33/0.11 0.40/0.25 0.43/0.28
MMed100 0.51/0.32 0.35/0.12 0.38/0.14
</table>
<tableCaption confidence="0.997891">
Table 1. Inter-rater agreement for 5-class reliability.
</tableCaption>
<table confidence="0.999820333333333">
Accuracy/ 3 Classes Reliability
Kappa
Set\Raters M-E M-L E-L
IBS train 0.70/0.44 0.60/0.25 0.67/0.33
IBS test 0.70/0.43 0.65/0.42 0.75/0.59
MMed100 0.77/0.49 0.66/0.30 0.62/0.22
</table>
<tableCaption confidence="0.999864">
Table 2. Inter-rater agreement for 3-classs reliability.
</tableCaption>
<bodyText confidence="0.99982015">
The results in Table 2 for M-E show improved
agreement after training and consistent moderate
agreement on the test corpora based on the Kappa
statistic. Accuracy (percent agreement) for M-E is
70% for both IBS testing and training and 77% for
the MMED100.
Further analysis of L’s reliability tags showed a
bias toward the “U” tag. For example, in the
MMED100 corpus, L tagged 28% as U, compared
to 19% and 17% for M and E, respectively.
Hripcsak and Rothschild (2005) suggest use of
the F-measure (harmonic average of precision –
equivalent to positive predictive value - and recall
– equivalent to sensitivity - commonly used in In-
formation Retrieval) to calculate inter-rater agree-
ment in the absence of a gold standard. In Table 3
we report the average F-measure between each pair
of raters and the F-measure by class. A higher F-
measure indicates better agreement, so these re-
sults show that the “Can’t Tell” class is the most
</bodyText>
<page confidence="0.997988">
42
</page>
<bodyText confidence="0.8019995">
difficult to agree on, followed by the “Unreliable”
class.
</bodyText>
<table confidence="0.998656857142857">
MMED100 3 Classes Reliability
F-Measure
Class\Raters M-E M-L E-L
Reliable 0.87 0.78 0.76
Can’t Tell 0.45 0.22 0.30
Unreliable 0.55 0.46 0.36
Average 0.62 0.49 0.47
</table>
<tableCaption confidence="0.999799">
Table 3. F-measure by class for 3-classs reliability.
</tableCaption>
<bodyText confidence="0.99956725">
In order to look for patterns of agreement be-
tween the raters we looked at agreement by query
in the MMED100 corpus. In Table 4 we show the
agreement for M and E by query. Although it ap-
pears that some queries were easier to annotate
than others, since there are only 10 pages per
query, the sample may be too small to draw defi-
nite conclusions.
</bodyText>
<table confidence="0.999261333333333">
Query Accuracy Kappa
Endometriosis 1 1
Pancreatic Cancer 1 1
Late Lyme 1 1
Adrenoleukodystrophy 0.8 0.412
Obesity 0.8 0.655
Alzheimer’s 0.7 -0.154
Fibromyalgia 0.7 0.444
Lower Back Pain 0.7 -0.154
Colloidal Silver 0.6 0.13
Irritable Bowel Syn- 0.4 -0.053
drome
</table>
<tableCaption confidence="0.9840865">
Table 4. Inter-rater reliability agreement for M-E by
query.
</tableCaption>
<bodyText confidence="0.957718">
Possible ways to improve these results are pre-
sented in the “Discussion” section.
</bodyText>
<subsectionHeader confidence="0.987159">
2.4.2 Page Type
</subsectionHeader>
<bodyText confidence="0.9991045625">
The dominant page types are P (patient leaflets), L
(link), C (commercial) and MA (medical article).
The baseline distribution based on averages across
the training and test sets is approximately: 39% P;
15% L; 18% C; and 13% MA. The other three
classes S (support), T (testimonial), and N (unable
to determine) making up only 15% of the pages in
the corpus.
Table 5 shows the results for Accuracy and the
Kappa statistic on the seven type classes across all
the corpora. Collapsing categories for the type an-
notation task did not appreciably increase Kappa
scores (M-E Kappa was 0.742 on the MMED100
corpus when the P and MA classes were col-
lapsed), so it seems preferable to keep the original
classes.
</bodyText>
<table confidence="0.998761333333333">
Accuracy/ Type
Kappa
Set\Raters M-E M-L E-L
IBS train 0.57/0.42 0.83/0.78 0.47/0.28
IBS test 0.73/0.64 0.65/0.55 0.73/0.64
MMed100 0.81/0.73 0.48/0.29 0.50/0.31
</table>
<tableCaption confidence="0.999863">
Table 5. Inter-rater agreement for type annotation.
</tableCaption>
<bodyText confidence="0.9993994">
Again we see with annotators M and E, the im-
proved agreement from training to testing, as dis-
tinctions between classes were clarified (for
example, between Link and Patient Leaflets, and
between Patient Leaflets and Medical Articles).
We also computed F-measure by type for the
MMED100 corpus, as shown in Table 6. Of the
three most common types of pages (Patient Leaflet,
Link, Commercial), the Link type was the most
difficult for M-E to agree on.
</bodyText>
<table confidence="0.996983636363636">
MMED100 Type
F-Measure
Class\Raters M-E M-L E-L
P 0.893 0.593 0.625
L 0.625 0.480 0.435
C 0.727 0.323 0.414
S 0.769 0.222 0.250
T 0.500 0.000 0.800
MA 0.667 0.593 0.455
N 0.857 0.143 0.118
Average 0.720 0.336 0.442
</table>
<tableCaption confidence="0.987183">
Table 6. F-measure by class for page type.
</tableCaption>
<bodyText confidence="0.9999706">
We further analyzed the page type annotations
by query for raters M and E (Table 7). We found a
negative correlation between the variance of the
types in a query to the Kappa statistic of agreement
for the query (r2 = -0.62).
</bodyText>
<page confidence="0.998926">
43
</page>
<table confidence="0.999444">
Query Accuracy Kappa
Endometriosis 0.9 0.851
Fibromyalgia 0.9 0.846
Alzheimer’s 0.8 0.75
Irritable Bowel Syn- 0.8 0.73
drome
Obesity 0.8 0.697
Pancreatic Cancer 0.8 0.63
Colloidal Silver 0.8 0.63
Adrenoleukodystrophy 0.8 0.512
Lower Back Pain 0.8 0.512
Late Lyme 0.7 0.483
</table>
<tableCaption confidence="0.998528">
Table 7. Inter-rater type agreement for M-E by query.
</tableCaption>
<sectionHeader confidence="0.999403" genericHeader="method">
3 Discussion
</sectionHeader>
<bodyText confidence="0.999970465517242">
Librarians, scholars, and information scientists
have done significant work on the quality (reliabil-
ity) of print, and more recently, web information
(for example, see Cook 2001, Alexander and Tate
1999). It is important to distinguish quality (reli-
ability) from credibility (e.g. Danielson 2005),
which is based on the users view of the informa-
tion. Here we are interested in the quality of the
information itself.
In a relatively early study, Impicciatore et al.
(1997) sampled web documents relating to fever in
children and found the quality of the information
provided to be very low. In 2002, Eysenbach et al.
conducted a review of studies assessing the quality
of consumer health information on the web. Of the
79 studies meeting their inclusion criteria (essen-
tially appropriate scope and quantitative analysis),
they found that 70% of the studies concluded that
reliability of medical information on the Web is a
problem.
To address the question of how to determine the
quality of medical information on the web, Fallis
and Frick6 (2002) empirically tested several pro-
posed indicators and found that the standard indi-
cators of quality for print media could not be
directly translated to consumer medical informa-
tion on the Web. Price and Hersh (1999) developed
a semi-automated system to filter out low quality
consumer medical web pages based on approxi-
mately 30 criteria.
Annotation studies have been discussed and
conducted in the computational linguistics com-
munity for a variety of annotation tasks, including
subjectivity (e.g. Weibe et al. 1999) and opinion
(e.g. Somasundaran et al. 2008). Artstein and Poe-
sio (2008) surveyed inter-coder agreement in com-
putational linguistics, including Cohen’s Kappa.
To ensure a “gold standard” for training ma-
chine learning algorithms to do automatic classifi-
cation a number of approaches could be pursued:
the production of bias-corrected tags as described
by Weibe et al. (1999); a new study with “expert”
annotators – having a stronger medical background
– and additional training; ask annotators to use ex-
isting web tools (e.g. American Accreditation
HealthCare Commission) to assess the page qual-
ity; systematically assess whether the noise intro-
duced by moderate agreement levels will create
problems for machine learning with this data
(Beigman Klebanov and Beigman 2009).
The agreement on the type annotation task could
still be improved, possibly by additional clarifica-
tion to the definitions. However, it is still to be de-
termined if noise levels are low enough and
sufficiently random to be used successfully in su-
pervised learning. This task is easier than the reli-
ability task and requires less expertise of the
annotators.
</bodyText>
<sectionHeader confidence="0.999296" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999949133333333">
There is a demonstrated need to provide tools to
health care consumers to automatically filter web
pages by the reliability, quality, or trustworthiness
of the medical information the pages contain. We
have shown promising results in this study that
appropriate classes of pages can be developed.
These classes can be used by human annotators to
annotate web pages with reasonable to good
agreement.
Thus we have laid a foundation for future anno-
tation studies to create a gold standard data set of
consumer medical web pages. The corpora in this
study are currently being used to create an auto-
mated system to estimate the reliability of medical
web pages.
</bodyText>
<sectionHeader confidence="0.998844" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999275">
This work was supported in part by a CSU Stanis-
laus Naraghi Faculty Research Enhancement
Grant. I am grateful to Elizabeth Jimenez and Luis
Adalco for participating in the annotation study
and to the anonymous reviews for their comments
and suggestions. I would also like to thank Roger
Hartley my dissertation advisor and Peter Foltz for
discussions during the formulation and develop-
</bodyText>
<page confidence="0.996536">
44
</page>
<bodyText confidence="0.99984">
ment of the system, and Tom Carter for helpful and
insightful comments leading to the improvement of
this paper.
</bodyText>
<sectionHeader confidence="0.996286" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999880015384615">
Janet E. Alexander and Marsha Ann Tate. 1999. Web
Wisdom: How to Evaluate and Create Information
Quality on the Web. Lawrence Erlbaum and Associ-
ates, New Jersey.
American Accreditation HealthCare Commission.
Health information on the internet: A checklist to
help you judge which websites to trust. Retrieved
February 28, 2010, from http://www.urac.org
R. Artstein and M. Poesio. 2008. Inter-coder agreement
for computational linguistics. Comput. Linguist. 34, 4
(Dec. 2008), 555-596.
B. Beigman Klebanov, and E. Beigman. 2009. From
annotator agreement to noise models. Comput. Lin-
guist. 35, 4 (Dec. 2009), 495-503.
J. Cohen. 1960. A coefficient of agreement for nominal
scales. Educational and Psychological Measurement,
20, pages 34-46.
Alison Cooke. 2001. A Guide to Finding Quality Infor-
mation on the Internet: Selection and Evaluation
Strategies, Second Edition. Library Association Pub-
lishing, London.
D.R. Danielson. 2005. Web credibility. C. Ghaoui (Ed.),
Encyclopedia of Human-Computer Interaction. Her-
shey, PA: Idea Group, 713-721.
Gunther Eysenbach, John Powell, Oliver Kuss, and
Eun-Ryoung Sa. 2002. Empirical Studies Assessing
the Quality of Health Information for Consumers on
the World Wide Web: A Systematic Review. JAMA,
May 22, 2002; 287(20): 2691 - 2700.
Don Fallis and Martin Frick6. 2002. Indicators of Accu-
racy of Consumer Health Information on the Internet.
Journal of the American Medical Informatics Asso-
ciation, 9, 1, (2002): 73-79.
George Hripcsak and Adam S. Rothschild. 2005.
Agreement, the F-Measure, and Reliability in Infor-
mation Retrieval. J Am Med Inform Assoc. 2005
May–Jun; 12(3): 296–298.
Piero Impicciatore, Chiara Pandolfini, Nicola Casella,
and Maurizio Bonati. 1997. Reliability of Health In-
formation for the Public on the World Wide Web:
Systematic Survey of Advice on Managing Fever in
Children at Home. BMJ 1997; 314:1875 (28 June).
Melanie J. Martin. 2004. Reliability and Verification of
Natural Language Text on the World Wide Web. Pa-
per at ACM-SIGIR Doctoral Consortium, July 25,
2004, Sheffield, England.
Susan L. Price and William R. Hersh. 1999. Filtering
Web Pages for Quality Indicators: An Empirical Ap-
proach to Finding High Quality Consumer Health In-
formation. American Medical Informatics
Association 1999.
Swapna Somasundaran, Josef Ruppenhofer and Janyce
Wiebe. 2008. Discourse Level Opinion Relations: An
Annotation Study. Proceedings of the 9th SIGdial
Workshop on Discourse and Dialogue Columbus,
Ohio, June 19-20, 2008, pp. 129–137.
Janyce M. Wiebe, Rebecca F. Bruce, and Thomas P.
O&apos;Hara. 1999. Development and use of a gold-
standard data set for subjectivity classifications. In
Proceedings of the 37th Annual Meeting of the Asso-
ciation For Computational Linguistics on Computa-
tional Linguistics (College Park, Maryland, June 20 -
26, 1999). Annual Meeting of the ACL. Association
for Computational Linguistics, Morristown, NJ, 246-
253.
</reference>
<page confidence="0.999388">
45
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.311943">
<title confidence="0.8758715">Reliability and Type of Consumer Health Documents on the World Wide Web: an Annotation Study</title>
<author confidence="0.993063">J Melanie</author>
<affiliation confidence="0.999405">California State University, One University</affiliation>
<address confidence="0.938265">Turlock, CA</address>
<email confidence="0.99921">mmartin@cs.csustan.edu</email>
<abstract confidence="0.94431475">In this paper we present a detailed scheme for annotating medical web pages designed for health care consumers. The annotation is along two axes: first, by reliability or the extent to which the medical information on the page can be trusted, second, by the type of page (patient leaflet, commercial, link, medical article, testimonial, or support). We analyze interrater agreement among three judges for each category. Inter-rater agreement was moderate (0.77 accuracy, 0.62 F-measure, 0.49 Kappa) on the reliability axis and good (0.81 accuracy, 0.72 F-measure, 0.73 Kappa) along the type axis.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Janet E Alexander</author>
<author>Marsha Ann Tate</author>
</authors>
<title>Web Wisdom: How to Evaluate and Create Information Quality on the Web. Lawrence Erlbaum and Associates,</title>
<date>1999</date>
<location>New Jersey.</location>
<contexts>
<context position="22755" citStr="Alexander and Tate 1999" startWordPosition="3720" endWordPosition="3723">es in a query to the Kappa statistic of agreement for the query (r2 = -0.62). 43 Query Accuracy Kappa Endometriosis 0.9 0.851 Fibromyalgia 0.9 0.846 Alzheimer’s 0.8 0.75 Irritable Bowel Syn- 0.8 0.73 drome Obesity 0.8 0.697 Pancreatic Cancer 0.8 0.63 Colloidal Silver 0.8 0.63 Adrenoleukodystrophy 0.8 0.512 Lower Back Pain 0.8 0.512 Late Lyme 0.7 0.483 Table 7. Inter-rater type agreement for M-E by query. 3 Discussion Librarians, scholars, and information scientists have done significant work on the quality (reliability) of print, and more recently, web information (for example, see Cook 2001, Alexander and Tate 1999). It is important to distinguish quality (reliability) from credibility (e.g. Danielson 2005), which is based on the users view of the information. Here we are interested in the quality of the information itself. In a relatively early study, Impicciatore et al. (1997) sampled web documents relating to fever in children and found the quality of the information provided to be very low. In 2002, Eysenbach et al. conducted a review of studies assessing the quality of consumer health information on the web. Of the 79 studies meeting their inclusion criteria (essentially appropriate scope and quanti</context>
</contexts>
<marker>Alexander, Tate, 1999</marker>
<rawString>Janet E. Alexander and Marsha Ann Tate. 1999. Web Wisdom: How to Evaluate and Create Information Quality on the Web. Lawrence Erlbaum and Associates, New Jersey.</rawString>
</citation>
<citation valid="true">
<title>American Accreditation HealthCare Commission. Health information on the internet: A checklist to help you judge which websites to trust. Retrieved</title>
<date>2010</date>
<note>from http://www.urac.org</note>
<marker>2010</marker>
<rawString>American Accreditation HealthCare Commission. Health information on the internet: A checklist to help you judge which websites to trust. Retrieved February 28, 2010, from http://www.urac.org</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Artstein</author>
<author>M Poesio</author>
</authors>
<title>Inter-coder agreement for computational linguistics.</title>
<date>2008</date>
<journal>Comput. Linguist.</journal>
<volume>34</volume>
<pages>555--596</pages>
<contexts>
<context position="24185" citStr="Artstein and Poesio (2008)" startWordPosition="3949" endWordPosition="3953">on on the web, Fallis and Frick6 (2002) empirically tested several proposed indicators and found that the standard indicators of quality for print media could not be directly translated to consumer medical information on the Web. Price and Hersh (1999) developed a semi-automated system to filter out low quality consumer medical web pages based on approximately 30 criteria. Annotation studies have been discussed and conducted in the computational linguistics community for a variety of annotation tasks, including subjectivity (e.g. Weibe et al. 1999) and opinion (e.g. Somasundaran et al. 2008). Artstein and Poesio (2008) surveyed inter-coder agreement in computational linguistics, including Cohen’s Kappa. To ensure a “gold standard” for training machine learning algorithms to do automatic classification a number of approaches could be pursued: the production of bias-corrected tags as described by Weibe et al. (1999); a new study with “expert” annotators – having a stronger medical background – and additional training; ask annotators to use existing web tools (e.g. American Accreditation HealthCare Commission) to assess the page quality; systematically assess whether the noise introduced by moderate agreement </context>
</contexts>
<marker>Artstein, Poesio, 2008</marker>
<rawString>R. Artstein and M. Poesio. 2008. Inter-coder agreement for computational linguistics. Comput. Linguist. 34, 4 (Dec. 2008), 555-596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Beigman Klebanov</author>
<author>E Beigman</author>
</authors>
<title>From annotator agreement to noise models.</title>
<date>2009</date>
<journal>Comput. Linguist.</journal>
<volume>35</volume>
<pages>495--503</pages>
<contexts>
<context position="24884" citStr="Klebanov and Beigman 2009" startWordPosition="4055" endWordPosition="4058">ohen’s Kappa. To ensure a “gold standard” for training machine learning algorithms to do automatic classification a number of approaches could be pursued: the production of bias-corrected tags as described by Weibe et al. (1999); a new study with “expert” annotators – having a stronger medical background – and additional training; ask annotators to use existing web tools (e.g. American Accreditation HealthCare Commission) to assess the page quality; systematically assess whether the noise introduced by moderate agreement levels will create problems for machine learning with this data (Beigman Klebanov and Beigman 2009). The agreement on the type annotation task could still be improved, possibly by additional clarification to the definitions. However, it is still to be determined if noise levels are low enough and sufficiently random to be used successfully in supervised learning. This task is easier than the reliability task and requires less expertise of the annotators. 4 Conclusion There is a demonstrated need to provide tools to health care consumers to automatically filter web pages by the reliability, quality, or trustworthiness of the medical information the pages contain. We have shown promising resu</context>
</contexts>
<marker>Klebanov, Beigman, 2009</marker>
<rawString>B. Beigman Klebanov, and E. Beigman. 2009. From annotator agreement to noise models. Comput. Linguist. 35, 4 (Dec. 2009), 495-503.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cohen</author>
</authors>
<title>A coefficient of agreement for nominal scales.</title>
<date>1960</date>
<journal>Educational and Psychological Measurement,</journal>
<volume>20</volume>
<pages>34--46</pages>
<contexts>
<context position="17039" citStr="Cohen, 1960" startWordPosition="2788" endWordPosition="2789">was annotated. During the testing phase, one of the students, L, seemed to annotate less carefully. (Possibly because the timing coincided with graduation and summer vacation.) For example, on the MMED100 corpus L tagged 30% as N (unable to determine the reliability, compared to 12% for E and 10% for M. L was asked to go back and reconsider the web pages tagged as N. We report results with L’s reconsidered tags here for completeness, but further discussion will focus on agreement between M and E. 2.4 Testing and Analysis We report inter-rater agreement using accuracy, Cohen’s Kappa statistic (Cohen, 1960) for chance corrected agreement and F-Measure (Hripcsak and Rothschild, 2005). We consider each annotation axis separately. 2.4.1 Page Reliability We can estimate a baseline distribution of the categories R (reliable), N (unable to determine), and U (unreliable) based on an average of the tags across all training and test sets to be approximately: 68% R; 13% N; 19% U. Table 1 shows the results for the Accuracy (percent agreement) and Kappa statistic on the five reliability classes across all the corpora. It became immediately clear the annotators were not able to make the more fine-grained dis</context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>J. Cohen. 1960. A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20, pages 34-46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alison Cooke</author>
</authors>
<title>A Guide to Finding Quality Information on the Internet: Selection and Evaluation Strategies, Second Edition.</title>
<date>2001</date>
<publisher>Library Association Publishing,</publisher>
<location>London.</location>
<marker>Cooke, 2001</marker>
<rawString>Alison Cooke. 2001. A Guide to Finding Quality Information on the Internet: Selection and Evaluation Strategies, Second Edition. Library Association Publishing, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D R Danielson</author>
</authors>
<title>Web credibility. C. Ghaoui (Ed.), Encyclopedia of Human-Computer Interaction.</title>
<date>2005</date>
<pages>713--721</pages>
<publisher>Idea Group,</publisher>
<location>Hershey, PA:</location>
<contexts>
<context position="22848" citStr="Danielson 2005" startWordPosition="3735" endWordPosition="3736">Endometriosis 0.9 0.851 Fibromyalgia 0.9 0.846 Alzheimer’s 0.8 0.75 Irritable Bowel Syn- 0.8 0.73 drome Obesity 0.8 0.697 Pancreatic Cancer 0.8 0.63 Colloidal Silver 0.8 0.63 Adrenoleukodystrophy 0.8 0.512 Lower Back Pain 0.8 0.512 Late Lyme 0.7 0.483 Table 7. Inter-rater type agreement for M-E by query. 3 Discussion Librarians, scholars, and information scientists have done significant work on the quality (reliability) of print, and more recently, web information (for example, see Cook 2001, Alexander and Tate 1999). It is important to distinguish quality (reliability) from credibility (e.g. Danielson 2005), which is based on the users view of the information. Here we are interested in the quality of the information itself. In a relatively early study, Impicciatore et al. (1997) sampled web documents relating to fever in children and found the quality of the information provided to be very low. In 2002, Eysenbach et al. conducted a review of studies assessing the quality of consumer health information on the web. Of the 79 studies meeting their inclusion criteria (essentially appropriate scope and quantitative analysis), they found that 70% of the studies concluded that reliability of medical in</context>
</contexts>
<marker>Danielson, 2005</marker>
<rawString>D.R. Danielson. 2005. Web credibility. C. Ghaoui (Ed.), Encyclopedia of Human-Computer Interaction. Hershey, PA: Idea Group, 713-721.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gunther Eysenbach</author>
<author>John Powell</author>
<author>Oliver Kuss</author>
<author>Eun-Ryoung Sa</author>
</authors>
<title>Empirical Studies Assessing the Quality of Health Information for Consumers on the World Wide Web: A Systematic Review.</title>
<date>2002</date>
<volume>287</volume>
<issue>20</issue>
<pages>2691--2700</pages>
<location>JAMA,</location>
<contexts>
<context position="1675" citStr="Eysenbach et al., 2002" startWordPosition="268" endWordPosition="271">r tasks where information quality and reliability are vital, from legal and medical research by both professionals and lay people, to fact checking by journalists and research by government policy makers. In particular, there has been a proliferation of web pages in the medical domain for health care consumers. At the first sign of illness or injury more and more people go to the web before consulting medical professionals. The quality and reliability of the information on consumer medical web pages has been of concern for some time to medical professionals and policy makers. (For example see Eysenbach et al., 2002, Impicciatore et al., 1997.) Our goal is to create a system that can automatically measure the reliability of web pages in the medical domain (Martin, 2004). More specifically, given a web page resulting from a user query on a medical topic, we would like to automatically provide an estimate of the extent to which the information on the page can be trusted. In order to make use of supervised natural language processing and machine learning algorithms to create such a system, and to ultimately evaluate the performance of the system, it is necessary to have human annotated data. It is important</context>
</contexts>
<marker>Eysenbach, Powell, Kuss, Sa, 2002</marker>
<rawString>Gunther Eysenbach, John Powell, Oliver Kuss, and Eun-Ryoung Sa. 2002. Empirical Studies Assessing the Quality of Health Information for Consumers on the World Wide Web: A Systematic Review. JAMA, May 22, 2002; 287(20): 2691 - 2700.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Don Fallis</author>
<author>Martin Frick6</author>
</authors>
<date>2002</date>
<journal>Indicators of Accuracy of Consumer Health Information on the Internet. Journal of the American Medical Informatics Association,</journal>
<volume>9</volume>
<pages>73--79</pages>
<marker>Fallis, Frick6, 2002</marker>
<rawString>Don Fallis and Martin Frick6. 2002. Indicators of Accuracy of Consumer Health Information on the Internet. Journal of the American Medical Informatics Association, 9, 1, (2002): 73-79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Hripcsak</author>
<author>Adam S Rothschild</author>
</authors>
<title>Agreement, the F-Measure, and Reliability in Information Retrieval.</title>
<date>2005</date>
<journal>J Am Med Inform Assoc.</journal>
<volume>12</volume>
<issue>3</issue>
<pages>296--298</pages>
<contexts>
<context position="17116" citStr="Hripcsak and Rothschild, 2005" startWordPosition="2796" endWordPosition="2799">s, L, seemed to annotate less carefully. (Possibly because the timing coincided with graduation and summer vacation.) For example, on the MMED100 corpus L tagged 30% as N (unable to determine the reliability, compared to 12% for E and 10% for M. L was asked to go back and reconsider the web pages tagged as N. We report results with L’s reconsidered tags here for completeness, but further discussion will focus on agreement between M and E. 2.4 Testing and Analysis We report inter-rater agreement using accuracy, Cohen’s Kappa statistic (Cohen, 1960) for chance corrected agreement and F-Measure (Hripcsak and Rothschild, 2005). We consider each annotation axis separately. 2.4.1 Page Reliability We can estimate a baseline distribution of the categories R (reliable), N (unable to determine), and U (unreliable) based on an average of the tags across all training and test sets to be approximately: 68% R; 13% N; 19% U. Table 1 shows the results for the Accuracy (percent agreement) and Kappa statistic on the five reliability classes across all the corpora. It became immediately clear the annotators were not able to make the more fine-grained distinctions between “probably” and “possibly” for either the reliable or unreli</context>
<context position="18849" citStr="Hripcsak and Rothschild (2005)" startWordPosition="3066" endWordPosition="3069">70/0.44 0.60/0.25 0.67/0.33 IBS test 0.70/0.43 0.65/0.42 0.75/0.59 MMed100 0.77/0.49 0.66/0.30 0.62/0.22 Table 2. Inter-rater agreement for 3-classs reliability. The results in Table 2 for M-E show improved agreement after training and consistent moderate agreement on the test corpora based on the Kappa statistic. Accuracy (percent agreement) for M-E is 70% for both IBS testing and training and 77% for the MMED100. Further analysis of L’s reliability tags showed a bias toward the “U” tag. For example, in the MMED100 corpus, L tagged 28% as U, compared to 19% and 17% for M and E, respectively. Hripcsak and Rothschild (2005) suggest use of the F-measure (harmonic average of precision – equivalent to positive predictive value - and recall – equivalent to sensitivity - commonly used in Information Retrieval) to calculate inter-rater agreement in the absence of a gold standard. In Table 3 we report the average F-measure between each pair of raters and the F-measure by class. A higher Fmeasure indicates better agreement, so these results show that the “Can’t Tell” class is the most 42 difficult to agree on, followed by the “Unreliable” class. MMED100 3 Classes Reliability F-Measure Class\Raters M-E M-L E-L Reliable 0</context>
</contexts>
<marker>Hripcsak, Rothschild, 2005</marker>
<rawString>George Hripcsak and Adam S. Rothschild. 2005. Agreement, the F-Measure, and Reliability in Information Retrieval. J Am Med Inform Assoc. 2005 May–Jun; 12(3): 296–298.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Piero Impicciatore</author>
<author>Chiara Pandolfini</author>
<author>Nicola Casella</author>
<author>Maurizio Bonati</author>
</authors>
<title>Reliability of Health Information for the Public on the World Wide Web: Systematic Survey of Advice on Managing Fever in Children at Home. BMJ</title>
<date>1997</date>
<pages>314--1875</pages>
<contexts>
<context position="1702" citStr="Impicciatore et al., 1997" startWordPosition="272" endWordPosition="275">n quality and reliability are vital, from legal and medical research by both professionals and lay people, to fact checking by journalists and research by government policy makers. In particular, there has been a proliferation of web pages in the medical domain for health care consumers. At the first sign of illness or injury more and more people go to the web before consulting medical professionals. The quality and reliability of the information on consumer medical web pages has been of concern for some time to medical professionals and policy makers. (For example see Eysenbach et al., 2002, Impicciatore et al., 1997.) Our goal is to create a system that can automatically measure the reliability of web pages in the medical domain (Martin, 2004). More specifically, given a web page resulting from a user query on a medical topic, we would like to automatically provide an estimate of the extent to which the information on the page can be trusted. In order to make use of supervised natural language processing and machine learning algorithms to create such a system, and to ultimately evaluate the performance of the system, it is necessary to have human annotated data. It is important to note the varied uses of</context>
<context position="23023" citStr="Impicciatore et al. (1997)" startWordPosition="3764" endWordPosition="3767">r 0.8 0.63 Adrenoleukodystrophy 0.8 0.512 Lower Back Pain 0.8 0.512 Late Lyme 0.7 0.483 Table 7. Inter-rater type agreement for M-E by query. 3 Discussion Librarians, scholars, and information scientists have done significant work on the quality (reliability) of print, and more recently, web information (for example, see Cook 2001, Alexander and Tate 1999). It is important to distinguish quality (reliability) from credibility (e.g. Danielson 2005), which is based on the users view of the information. Here we are interested in the quality of the information itself. In a relatively early study, Impicciatore et al. (1997) sampled web documents relating to fever in children and found the quality of the information provided to be very low. In 2002, Eysenbach et al. conducted a review of studies assessing the quality of consumer health information on the web. Of the 79 studies meeting their inclusion criteria (essentially appropriate scope and quantitative analysis), they found that 70% of the studies concluded that reliability of medical information on the Web is a problem. To address the question of how to determine the quality of medical information on the web, Fallis and Frick6 (2002) empirically tested sever</context>
</contexts>
<marker>Impicciatore, Pandolfini, Casella, Bonati, 1997</marker>
<rawString>Piero Impicciatore, Chiara Pandolfini, Nicola Casella, and Maurizio Bonati. 1997. Reliability of Health Information for the Public on the World Wide Web: Systematic Survey of Advice on Managing Fever in Children at Home. BMJ 1997; 314:1875 (28 June).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Melanie J Martin</author>
</authors>
<title>Reliability and Verification of Natural Language Text on the World Wide Web. Paper at ACM-SIGIR Doctoral Consortium,</title>
<date>2004</date>
<location>Sheffield, England.</location>
<contexts>
<context position="1832" citStr="Martin, 2004" startWordPosition="297" endWordPosition="298"> research by government policy makers. In particular, there has been a proliferation of web pages in the medical domain for health care consumers. At the first sign of illness or injury more and more people go to the web before consulting medical professionals. The quality and reliability of the information on consumer medical web pages has been of concern for some time to medical professionals and policy makers. (For example see Eysenbach et al., 2002, Impicciatore et al., 1997.) Our goal is to create a system that can automatically measure the reliability of web pages in the medical domain (Martin, 2004). More specifically, given a web page resulting from a user query on a medical topic, we would like to automatically provide an estimate of the extent to which the information on the page can be trusted. In order to make use of supervised natural language processing and machine learning algorithms to create such a system, and to ultimately evaluate the performance of the system, it is necessary to have human annotated data. It is important to note the varied uses of the term “reliability” in the computer and information sciences. In the current context we use it to refer to an intrinsic proper</context>
</contexts>
<marker>Martin, 2004</marker>
<rawString>Melanie J. Martin. 2004. Reliability and Verification of Natural Language Text on the World Wide Web. Paper at ACM-SIGIR Doctoral Consortium, July 25, 2004, Sheffield, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan L Price</author>
<author>William R Hersh</author>
</authors>
<title>Filtering Web Pages for Quality Indicators: An Empirical Approach to Finding High Quality Consumer Health Information.</title>
<date>1999</date>
<publisher>American Medical Informatics Association</publisher>
<contexts>
<context position="23811" citStr="Price and Hersh (1999)" startWordPosition="3893" endWordPosition="3896"> studies assessing the quality of consumer health information on the web. Of the 79 studies meeting their inclusion criteria (essentially appropriate scope and quantitative analysis), they found that 70% of the studies concluded that reliability of medical information on the Web is a problem. To address the question of how to determine the quality of medical information on the web, Fallis and Frick6 (2002) empirically tested several proposed indicators and found that the standard indicators of quality for print media could not be directly translated to consumer medical information on the Web. Price and Hersh (1999) developed a semi-automated system to filter out low quality consumer medical web pages based on approximately 30 criteria. Annotation studies have been discussed and conducted in the computational linguistics community for a variety of annotation tasks, including subjectivity (e.g. Weibe et al. 1999) and opinion (e.g. Somasundaran et al. 2008). Artstein and Poesio (2008) surveyed inter-coder agreement in computational linguistics, including Cohen’s Kappa. To ensure a “gold standard” for training machine learning algorithms to do automatic classification a number of approaches could be pursued</context>
</contexts>
<marker>Price, Hersh, 1999</marker>
<rawString>Susan L. Price and William R. Hersh. 1999. Filtering Web Pages for Quality Indicators: An Empirical Approach to Finding High Quality Consumer Health Information. American Medical Informatics Association 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Swapna Somasundaran</author>
<author>Josef Ruppenhofer</author>
<author>Janyce Wiebe</author>
</authors>
<title>Discourse Level Opinion Relations: An Annotation Study.</title>
<date>2008</date>
<booktitle>Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue Columbus,</booktitle>
<pages>129--137</pages>
<location>Ohio,</location>
<contexts>
<context position="24157" citStr="Somasundaran et al. 2008" startWordPosition="3945" endWordPosition="3948">uality of medical information on the web, Fallis and Frick6 (2002) empirically tested several proposed indicators and found that the standard indicators of quality for print media could not be directly translated to consumer medical information on the Web. Price and Hersh (1999) developed a semi-automated system to filter out low quality consumer medical web pages based on approximately 30 criteria. Annotation studies have been discussed and conducted in the computational linguistics community for a variety of annotation tasks, including subjectivity (e.g. Weibe et al. 1999) and opinion (e.g. Somasundaran et al. 2008). Artstein and Poesio (2008) surveyed inter-coder agreement in computational linguistics, including Cohen’s Kappa. To ensure a “gold standard” for training machine learning algorithms to do automatic classification a number of approaches could be pursued: the production of bias-corrected tags as described by Weibe et al. (1999); a new study with “expert” annotators – having a stronger medical background – and additional training; ask annotators to use existing web tools (e.g. American Accreditation HealthCare Commission) to assess the page quality; systematically assess whether the noise intro</context>
</contexts>
<marker>Somasundaran, Ruppenhofer, Wiebe, 2008</marker>
<rawString>Swapna Somasundaran, Josef Ruppenhofer and Janyce Wiebe. 2008. Discourse Level Opinion Relations: An Annotation Study. Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue Columbus, Ohio, June 19-20, 2008, pp. 129–137.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Janyce M Wiebe</author>
<author>Rebecca F Bruce</author>
<author>Thomas P O&apos;Hara</author>
</authors>
<title>Development and use of a goldstandard data set for subjectivity classifications.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association For Computational Linguistics on Computational Linguistics</booktitle>
<volume>20</volume>
<pages>246--253</pages>
<location>College Park, Maryland,</location>
<marker>Wiebe, Bruce, O&apos;Hara, 1999</marker>
<rawString>Janyce M. Wiebe, Rebecca F. Bruce, and Thomas P. O&apos;Hara. 1999. Development and use of a goldstandard data set for subjectivity classifications. In Proceedings of the 37th Annual Meeting of the Association For Computational Linguistics on Computational Linguistics (College Park, Maryland, June 20 -26, 1999). Annual Meeting of the ACL. Association for Computational Linguistics, Morristown, NJ, 246-253.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>