<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000528">
<title confidence="0.9942305">
Learning to Map Dependency Parses to Abstract Meaning
Representations
</title>
<author confidence="0.994914">
Wei-Te Chen
</author>
<affiliation confidence="0.9983035">
Department of Computer Science
University of Colorado at Boulder
</affiliation>
<email confidence="0.99695">
Weite.Chen@colorado.edu
</email>
<sectionHeader confidence="0.993443" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.995942083333333">
Abstract Meaning Representation (AMR)
is a semantic representation language used
to capture the meaning of English sen-
tences. In this work, we propose an AMR
parser based on dependency parse rewrite
rules. This approach transfers dependency
parses into AMRs by integrating the syn-
tactic dependencies, semantic arguments,
named entity and co-reference informa-
tion. A dependency parse to AMR graph
aligner is also introduced as a preliminary
step for designing the parser.
</bodyText>
<sectionHeader confidence="0.998761" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99750856">
Abstract Meaning Representation (AMR) (Ba-
narescu et al., 2013) is a semantic formalism that
expresses the logical meanings of English sen-
tences in the form of a directed, acyclic graph.
AMR focuses on the semantic concepts (nodes
on the graph), and relations (labeled edges on the
graph) between those concepts. AMR relies heav-
ily on predicate-argument structures defined in the
PropBank (PB) (Palmer et al., 2005). The repre-
sentation encodes rich information, including se-
mantic roles, named entities, and co-reference in-
formation. Fig. 1 shows an example AMR.
In this proposal, we focus on the design of an
automatic AMR parser in a supervised fashion
from dependency parses. In contrast with recent
semantic parsing algorithms, we start the parsing
process from the dependency parses rather than
the sentences. A dependency parse provides both
the semantic dependency information for the sen-
tence, and the structure of the relations between
the head word and their dependencies. These can
provide strong features for semantic parsing. By
using a binary-branching bottom-up shift-reduced
algorithm, the statistical model for the rewrite
rules can be learned discriminatively. Although
</bodyText>
<figureCaption confidence="0.7731">
Figure 1: The AMR annotation of sentence “Pierre
</figureCaption>
<bodyText confidence="0.9805756875">
Vinken, 61 years old, will join the board as a
nonexecutive director Nov. 29.”
the AMR parser is my thesis topic, in this proposal
we will pay more attention to preliminary work -
the AMR -Dependency Parse aligner.
To extract the rewrite rules and the statistical
model, we need the links between AMR con-
cepts and the word nodes within the dependency
parse. An example alignment is shown in Fig.
2. Alignment between an AMR concept and de-
pendency node is needed because 1) it represents
the meaning of the sub-graph of the concept and
its child concepts corresponding to the phrase of
the head word node, and 2) the dependency node
contains sufficient information for the extraction
of rewrite rules. For example, the word node
“Vinken” on the dependency parse side in Fig. 2
links to the lexical concept “Vinken” and, further-
more, links to the “p2/name” and the “p/person”
concepts since “Vinken” is the head of the named
entity (NE) “Pierre Vinken” and the head of the
noun phrase “Pierre Vinken, 61 years old.” The
secondary aim of this proposal is to design an
alignment model between AMR concepts and de-
pendency parses. We use EM to search the hidden
derivations by combining the features of lexical
form, relation label, NE, semantic role, etc. Af-
ter EM processing, both the alignments and all the
feature probabilities can be estimated.
The design of a rewrite-based AMR parser is
described in Sec. 2, and the aligner is in Sec. 3.
Our preliminary experiments and results are pre-
</bodyText>
<page confidence="0.993629">
41
</page>
<note confidence="0.9540255">
Proceedings of the ACL-IJCNLP 2015 Student Research Workshop, pages 41–46,
Beijing, China, July 28, 2015. c�2015 Association for Computational Linguistics
</note>
<figureCaption confidence="0.942831333333333">
Figure 2: The alignment between an AMR sub-
graph and a dependency parse. A red line links
the corresponding concept and dependency node.
</figureCaption>
<bodyText confidence="0.936891">
sented in Sec. 4, followed by future work.
</bodyText>
<sectionHeader confidence="0.921009" genericHeader="method">
2 Rewrite Based AMR Parser
</sectionHeader>
<bodyText confidence="0.9915068">
AMR is a rooted, directed, acyclic graph. For ex-
ample, the concept join-01 in Fig. 1 is the root
meaning of the sentence, which links to the child
concepts Arg0, Arg1, and time. AMR adheres to
the following principles (Banarescu et al., 2013):
</bodyText>
<listItem confidence="0.987451857142857">
• AMRs are rooted acyclic graphs with labels (re-
lations) on edges. These labels indicate the di-
rected relation between two concepts.
• AMRs abstract away from syntactic idiosyncra-
cies of a language, and instead attempt to capture
only the core meaning of a sentence.
• AMRs use the PB framesets as relation la-
bels (Palmer et al., 2005). For example, the rela-
tion labels (i.e., ARG0, ARG1) of “join-01” con-
cept in Fig. 1 correspond to the roles of the PB
frame “join-v.”
• AMRs combine multiple layers of linguistic an-
notation, like coreference, NE, semantic role,
etc., in a single structure.
</listItem>
<bodyText confidence="0.999935666666667">
The above basic characteristics make the parsing
of AMRs a difficult task. First, because AMR
abstracts away from syntactic idiosyncrasies, we
need a model to link the AMR concepts to words
in the original sentence, in order to obtain exter-
nal lexical, syntactic and semantic features. Sec-
ondly, the parser should learn the different feature
transformation probabilities jointly since AMRs
combine several linguistic annotations. Moreover,
</bodyText>
<equation confidence="0.995675">
(x)/NP — :op(x) -r1
(x) nn (y) —:name (name(x, y)) -r2
(x)/CD —:quant(x) -r3
(x)/NNS —:unit(x) -r4
npadvmod (x)(y) — temporal-quanity(x, y) -r5
old/JJ (x) — :age(x) -r6
NE PERSON(x)(y) — person(x, y) -r7
</equation>
<tableCaption confidence="0.961417">
Table 1: Sample Rewrite Rules
</tableCaption>
<bodyText confidence="0.999640090909091">
AMR uses graph variables and reentrancy to ex-
press coreference (e.g., “p” variable in Fig 1 ap-
pears twice – in :ARG0 of join-01 and :ARG0 of
have-org-role-91). The reentrancy prevents the
AMR graph from begin a tree structure. Dur-
ing parsing decoding, a polynomial time algorithm
should be replaced by alternative algorithms, like
beam search, to avoid an exponential running time.
JAMR (Flanigan et al., 2014) is the first sys-
tem for AMR parsing, which identifies the con-
cepts, and then searches for a maximum span-
ning connected subgraph (MSCG) on a fully con-
nected graph to identify the relations between con-
cepts. The search algorithm is similar to the maxi-
mum spanning tree algorithms. To assure the fi-
nal connected graph conforms to linguistic con-
straints, JAMR uses Lagrangian relaxation (Geof-
frion, 2010) to supplement the MSCG algorithm.
JAMR reaches a 58% Smatch score (Cai and
Knight, 2013) on automatic concept and relation
identification data, and 80% on gold concept and
automatic relation identification data.
</bodyText>
<subsectionHeader confidence="0.992987">
2.1 Our Shift-Reduce Rewrite Rule Parser
</subsectionHeader>
<bodyText confidence="0.99966295">
Rewrite rule based parser is a bottom-up converter
from dependency parses to AMRs. The process
starts from the leaf word node on the dependency
parse. By applying rewrite-rules to each word
node, we obtain and assemble the sub-graphs of
our target AMR. Sample rewrite rules are listed in
Table 1. In these rules, the left hand side contains
the dependency information (e.g. word lemma,
POS, relation label, NE tag, etc). The right hand
side is the AMR concept and its template for filling
variables from previous parsing steps. The sample
derivation steps are listed in Table 2. For every
step, it shows the derivation rule applied (in Table
1), and the concept name, c1-c8.
This approach to parsing could be implemented
with a shift-reduce algorithm (Wang et al., 2015).
We define a stack and a list of tokens, which stores
the dependency words in the order of tree traver-
sal. Several actions are defined to operate on the
list(L) and the stack(S):
</bodyText>
<page confidence="0.997864">
42
</page>
<table confidence="0.999817933333333">
Derivation Apply Concept
Rule Name
Pierre/NNP — :op Pierre r1 c1
Vinken/NNP — :op Vinken r1 c2
(c1) nn (c2) r2 c3
—:name (name :op1 Pierre :op2 Vinekn)
61/CD — :quant 61 r3 c4
years/NNS — :unit year r4 c5
npadvmod (c4)(c5) r5 c6
— temporal-quanity :quant 61 :unit year
old/JJ (c6) r6 c7
— :age (temporal-quanity :quant 61 :unit year)
NE PERSON (c3)(c7) r7 c8
— person :name (name :op1 Pierre :op2 Vinekn)
:age (temporal-quanity :quant 61 :unit year)
</table>
<tableCaption confidence="0.944911">
Table 2: The derivation for parsing “Pierre
Vinken, 61 years old” from dep. parse to AMR
</tableCaption>
<listItem confidence="0.992648125">
• Shift Remove the dependency word from L, ap-
ply the rules, and push the new concept to S.
• Reduce Move the two top sub-concepts from S,
apply the rules, and push it back to S.
• Unary Move the top sub-concept from S, apply
the rules, and push it back to S.
• Finish If no more dependency words are in the
list, and one concept is in S, then return.
</listItem>
<bodyText confidence="0.995255">
The final AMR concept would be stored at the top
of the stack. It is guaranteed that all the AMR
expressions can be derived from the dependency
parses by using the shift-reduce algorithm.
</bodyText>
<sectionHeader confidence="0.9899" genericHeader="method">
3 Dependency Parses to AMR Aligner
</sectionHeader>
<bodyText confidence="0.999955321428571">
A preliminary step for our rewrite-based parser is
the alignment between the AMR and the depen-
dency parse. JAMR (Flanigan et al., 2014) pro-
vides a heuristic aligner between an AMR concept
and the word or phrase of a sentence. They use
a set of aligner rules, like NE, fuzzy NE, data en-
tity, etc., with a greedy strategy to match the align-
ments. This aligner achieves a 90% F1 score on
hand aligned AMR-sentence pairs. On the other
hand, Pourdamghani et al. (2014) present a gen-
erative model to align from AMR graphs to sen-
tence strings. They raises concerns about the lack
of sufficient data for learning derivation rules. In-
stead, they propose a string-to-string alignment
model, which transfers the AMR expression to a
linearized string representation. Then they use
several IBM word alignment models (Brown et
al., 1993) on this task. IBM Model-4 with a sym-
metric method reaches the highest F1 score of
83.1%. Separately analyzing the alignments of
roles and non-roles (lexical leaf on AMR), the F1
scores are 49.3% and 89.8%, respectively.
In comparison to previous work, our aligner es-
timates the alignments by learning the transforma-
tion probability of lexical form, relations, named
entities and semantic roles features jointly. Both
the alignment and transformation probabilities are
initialized for the training of parser.
</bodyText>
<subsectionHeader confidence="0.996447">
3.1 Our Aligner Model with EM Algorithm
</subsectionHeader>
<bodyText confidence="0.999912555555556">
Our approach, based on the existing IBM Model
(Brown et al., 1993), is an AMR-to-Dependency
parse aligner, which represents one AMR as a list
of Concepts C = (c1, c2,... , c|C|�, and the cor-
responding dependency parse as a list of depen-
dency word nodes D = (d1, d2,.. . , d|D|�. The
alignment A is a set of mapping functions a, which
link Concept cj to dependency word node di, a :
cj → di. Our model adopts an asymmetric EM
approach, instead of the standard symmetric one.
We can always find the dependency label path be-
tween any pair of dependency word nodes. How-
ever, the number of concept relation label paths is
not deterministic. Thus, we select the alignment
direction of AMR to dependency parse only, and
one-to-one mapping, in our model.
The objective function is to learn the parameter
0 in the AMR-to-Dependency Parse of EM:
</bodyText>
<equation confidence="0.998497">
0 = argmax LB(AMR|DEP)
P(C(k), A|D(k); t, q)
</equation>
<bodyText confidence="0.976035444444444">
where LB is the likelihood that we would like to
maximize, S is the training data set. We will
explain the transformation probability t and the
alignment probability q below.
Expectation-Step
The E-Step estimates the likelihood of the input
AMR and dependency parse by giving the trans-
formation probability t and alignment probability
q. The likelihood can be calculated using:
</bodyText>
<equation confidence="0.99499775">
|C|
P(A|C, D) = H P(cj|a(cj))
j=1
P(cj|di, |C|, |D|) = t(cj|di) ∗ q(di|cj, |C|, |D|)
</equation>
<bodyText confidence="0.97989325">
We would like to calculate all the probabilities
of possible alignments A between cj and di.
The transformation probability t is a combination
(multiple) probability of several different features:
</bodyText>
<listItem confidence="0.992569">
• Plemma(cj|di): the lemma probability is the
probability of the concept name of cj, condi-
tioned on the dependency word of di.
</listItem>
<figure confidence="0.9197025">
LB(AMR|DEP) =
|S|
L
k=1
�
A
</figure>
<page confidence="0.998771">
43
</page>
<listItem confidence="0.881438">
• Prel(Label(cj,cp j)|RelPathdep(a(cj),a(cp j))):
</listItem>
<bodyText confidence="0.862744">
the relation probability is the probability of the
relation label between ci and its parent concept
cpi , given the relation path between the depen-
dency word nodes a(ci) and a(cpi ). e.g., the
relation probability of cj = 61 and a(cj) = 61
in Fig. 2 is P(quant|npadvmod 1 num 1).
</bodyText>
<listItem confidence="0.7875354">
• PNE(Name(cj)|TypeNE(a(cj))): the NE
probability is the probability of the name of cj,
given the NE type (e.g., PERSON, DATE, ORG,
etc.) contained by a(cj)).
• PSR(Label(cj,cp j)|Pred(a(cp j)),Arg(a(cj))):
the semantic role probability is the probability
of relation label between cj and its parent cpj,
conditioned on the predicate word of a(cpj) and
argument type of a(cj) if a(cj) is semantic
argument of predicate a(cpj).
</listItem>
<bodyText confidence="0.953935555555555">
On the other hand, the alignment probability
q(Dist(a(cj),a(cpj))|cj, |C|, |D|) can be inter-
preted as the probability of the distance between
a(cj) and a(cpj) on dependency parse D, condi-
tioned on cj, the lengths of D and C.
Maximization-Step
In the M-Step, the parameter 0r is updated from
the previous round of 0r−1, in order to maximize
the likelihood Lθ(AMR|DEP):
</bodyText>
<equation confidence="0.649219833333333">
t(C|D; AMR, DEP) =
E(AMR,DEP) cnt(C|D; AMR, DEP)
EC E(AMR,DEP) cnt(C |D; AMR, DEP)
q(D|C; AMR, DEP) =
E(AMR,DEP) cnt(D|C; AMR, DEP)
ED E(AMR,DEP) cnt(D |C; AMR, DEP)
</equation>
<bodyText confidence="0.9999445">
where cnt is the normalized count that is collected
from the accumulating probability of all possible
alignment from the E-step. EM iterates the E-step
and M-step until convergence.
</bodyText>
<sectionHeader confidence="0.546992" genericHeader="method">
Initialization
</sectionHeader>
<bodyText confidence="0.998822">
Before iterating, the transformation probability t
and alignment probability q must be initialized.
We use these steps to initialize the parameters:
</bodyText>
<listItem confidence="0.905065416666667">
1. Assign a fixed value, say 0.9, to Plemma(cj|di)
if the concept name of cj is identical or a partial
match to the dependency word node di. Other-
wise, initialize it uniformly;
2. Run the EM algorithm with the initialized
Plemma only (Similar to IBM Model 1, which
is only concerned with translation probability);
3. Initialize all the other parameters, i.e., Prel,
PNE, PSR, and q with the development data;
4. Run the completed EM algorithm with the
Plemma we obtained from Step 2 and other prob-
abilities from Step 3.
</listItem>
<bodyText confidence="0.999788">
The extra EM for the initialization of Plemma is to
estimate a more reasonable Plemma, and to speed
up the convergence of the second round of EM.
</bodyText>
<subsectionHeader confidence="0.723536">
Decoding
</subsectionHeader>
<bodyText confidence="0.996375">
To find the alignment of (C, D), we define the
search for alignments as follows:
</bodyText>
<equation confidence="0.673337666666667">
argmax P(A|C, D)
A
t(cj|a(cj)) * q(a(cj)|cj, |C|, |D|)
</equation>
<bodyText confidence="0.9959305">
This decoding problem finds the alignment A with
the maximum likelihood. A dynamic program-
ming (DP) algorithm is designed to extract the tar-
get alignment without exhaustively searching all
candidate alignments, which will take O(|D||C|).
This DP algorithm starts from the leaf concepts
and then walks through parent concepts. In cj, we
need to produce the following likelihoods:
</bodyText>
<listItem confidence="0.9936324">
1. Accumulated likelihood for aligning to any di
from all the child concepts of cj
2. Likelihood of Plemma and PNE
3. Likelihood of Prel and PSR for parent concept
cp jaligned to any dependency word node dl.
</listItem>
<bodyText confidence="0.999849666666667">
In step (3), we need to find the dl, aligned by cpj,
that maximizes the likelihood. The accumulated
likelihood is then stored in a list with size=|D|. We
can trace back and find the most likely alignments
in the end. The running time of this algorithm
is O(|C||D|2). This algorithm does not include
reentrancy cases. One solution to be explored in
future work is to use a beam-search algorithm in-
stead.
</bodyText>
<sectionHeader confidence="0.878825" genericHeader="method">
4 Preliminary Experiments and Results
</sectionHeader>
<bodyText confidence="0.999971666666667">
Here, we describe a preliminary experiment for
the AMR-Dependency Parse aligner, including the
data description, experimental setup, and results.
</bodyText>
<subsectionHeader confidence="0.990095">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.997832">
The LDC AMR release 1.0 consists of 13,051
AMR-English sentence pairs1. To match an AMR
</bodyText>
<footnote confidence="0.987226">
1LDC AMR release 1.0, Release date: June 16, 2014
https://catalog.ldc.upenn.edu/LDC2014T12
</footnote>
<equation confidence="0.700622">
= argmax
A
|C|
H
j=1
</equation>
<page confidence="0.995888">
44
</page>
<table confidence="0.9988354">
Split Sent. Tokens # of # of # of
NE Pred. Args
Train 1,000 19,923 1,510 4,231 7,739
Dev. 100 2,328 239 235 526
Test 100 1,672 80 199 445
</table>
<tableCaption confidence="0.97901975">
Table 3: The data split of train/dev./test set. “# of
NE”, “# of Pred.” and “# of Args” stand for the
number of named entities, predicate and argument
annotations in the data set, respectively.
</tableCaption>
<table confidence="0.99985575">
P R F1
7� 56.7 50.5 53.4
i lemma 61.1 53.4 57.0
Combination
</table>
<tableCaption confidence="0.999671">
Table 4: Experiment Results
</tableCaption>
<bodyText confidence="0.9995294">
with its corresponding dependency parse, we se-
lect the sentences which appear in the OntoNotes
5.0 release2 as well, then randomly select 1,000 of
them as our training set. The OntoNotes data con-
tains TreeBank, PB, and NE annotations. Statis-
tics about the AMR and OntoNotes corpus and the
train/dev./test splits are given in Table 3. We man-
ually align the AMR concepts and dependency
word nodes in the dev. and test sets. We initial-
ize Prel, PNE, and PSR with the dev. set.
</bodyText>
<sectionHeader confidence="0.799457" genericHeader="evaluation">
4.2 Results
</sectionHeader>
<bodyText confidence="0.999994642857143">
We run our first round of EM (Step 2 in Initializa-
tion of Sec. 3.1) for 100 iterations, then use the
second round (Step 4 in Initialization of Sec. 3.1)
for another 100 iterations. We run our decoding al-
gorithm and evaluation on the test set after the first
and second round of EM. Due to time constraints,
we did not train the q here.
The experimental results are listed in Table 4.
We evaluate the performance on the precision, re-
call, and F1 score. Using just the Plemma (a simi-
lar approach to (Pourdamghani et al., 2014)), we
achieve 53.4% F1 score on the test set. On the
other hand, our aligner reaches 57.0% F1 score
with the full aligner.
</bodyText>
<sectionHeader confidence="0.993532" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999595285714286">
In this research, we briefly introduce AMR. We
describe the design principles and characteristics
of AMR, and show how the AMR parser task
is important, yet difficult. We present the basic
idea for a proposed AMR parser, based on the
shift-reduce algorithm. We also present an AMR-
Dependency Parse aligner, because such an aligner
</bodyText>
<footnote confidence="0.9991425">
2LDC OntoNotes Release 5.0, Release date: October 16,
2013 https://catalog.ldc.upenn.edu/LDC2013T19
</footnote>
<bodyText confidence="0.998753515151515">
will be a necessary first step before parsing. The
alignment and the estimated feature probabilities
are obtained by running the EM algorithm, which
could be use directly for the AMR parser.
In the future, we will be following these steps to
develop the proposed rewrite-based parser:
Implemention of our rewrite-based AMR
parser: We would like to implement the proposed
rewrite-based AMR parser. In comparison to the
parser of Flanigan (2014) , we believe our parser
could perform better on the runtime. We also plan
to experiment with the data generated by an auto-
matic dependency parser.
Expand the experimental data of aligner:
One problem discovered in our preliminary ex-
periments was that of data sparsity, especially for
Plemma. The LDC AMR Release contains 18,779
AMR/English sentences, and 8,996 of them are
contained in the OntoNotes release as well. There-
fore, increasing the training data size from the re-
lease is one solution to improve the performance
of our aligner from the unsatisfactory results. Us-
ing external lexical resources, like WordNet, is an-
other promissing solution to extend to snyonyms.
Evaluation of the aligner with existing
parser: Since our aligner provides the align-
ment between the dependency word node and both
the AMR leaf concept and role concept, we as-
sume that our aligner could improve not only our
rewrite-based parser but other parsers as well. To
verify this, we hope to submit our improved align-
ment results to a state-of-the-art AMR parser, and
evaluate the parsing results.
</bodyText>
<sectionHeader confidence="0.997699" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.93728025">
We gratefully acknowledge the support of the Na-
tional Science Foundation Grants IIS-1116782,
A Bayesian Approach to Dynamic Lexical
Resources for Flexible Language Processing,
0910992 IIS:RI: Richer Representations for Ma-
chine Translation, and NSF IIA-0530118 PIRE
(a subcontract from Johns Hopkins) for the 2014
Frederick Jelinek Memorial Workshop for Mean-
ing Representations in Language and Speech
Processing, and funding under the BOLT and
Machine Reading programs, HR0011-11-C-0145
(BOLT) FA8750-09-C-0179 (M.R.). Any opin-
ions, findings, and conclusions or recommenda-
tions expressed in this material are those of the
authors and do not necessarily reflect the views of
the National Science Foundation.
</bodyText>
<page confidence="0.999106">
45
</page>
<sectionHeader confidence="0.995879" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999870644444444">
Laura Banarescu, Claire Bonial, Shu Cai, Madalina
Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider. 2013. Abstract meaning representation
for sembanking.
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Comput. Linguist., 19(2):263–
311, June.
Shu Cai and Kevin Knight. 2013. Smatch: an evalua-
tion metric for semantic feature structures. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 2: Short
Papers), pages 748–752. Association for Computa-
tional Linguistics.
J. Flanigan, S. Thomson, J. Carbonell, C. Dyer, and
N. A. Smith. 2014. A discriminative graph-based
parser for the abstract meaning representation. In
Proc. of ACL, Baltimore, Maryland, June. Associa-
tion for Computational Linguistics.
ArthurM. Geoffrion. 2010. Lagrangian relaxation for
integer programming. In Michael Jnger, Thomas M.
Liebling, Denis Naddef, George L. Nemhauser,
William R. Pulleyblank, Gerhard Reinelt, Giovanni
Rinaldi, and Laurence A. Wolsey, editors, 50 Years
of Integer Programming 1958-2008, pages 243–281.
Springer Berlin Heidelberg.
Martha Palmer, Dan Guildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71–105, March.
Nima Pourdamghani, Yang Gao, Ulf Hermjakob, and
Kevin Knight. 2014. Aligning english strings with
abstract meaning representation graphs. In Proceed-
ings of the 2014 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
425–429. Association for Computational Linguis-
tics.
Chuan Wang, Xue Nianwen, and Pradhan Sameer.
2015. A transition-based algorithm for amr parsing.
In Proceedings of the 2015 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies.
Association for Computational Linguistics.
</reference>
<page confidence="0.99961">
46
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.899730">
<title confidence="0.9908445">Learning to Map Dependency Parses to Abstract Representations</title>
<author confidence="0.998292">Wei-Te Chen</author>
<affiliation confidence="0.998854">Department of Computer University of Colorado at</affiliation>
<email confidence="0.95324">Weite.Chen@colorado.edu</email>
<abstract confidence="0.996871230769231">Abstract Meaning Representation (AMR) is a semantic representation language used to capture the meaning of English sentences. In this work, we propose an AMR parser based on dependency parse rewrite rules. This approach transfers dependency parses into AMRs by integrating the syntactic dependencies, semantic arguments, named entity and co-reference information. A dependency parse to AMR graph aligner is also introduced as a preliminary step for designing the parser.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Laura Banarescu</author>
<author>Claire Bonial</author>
<author>Shu Cai</author>
<author>Madalina Georgescu</author>
<author>Kira Griffitt</author>
<author>Ulf Hermjakob</author>
<author>Kevin Knight</author>
<author>Philipp Koehn</author>
<author>Martha Palmer</author>
<author>Nathan Schneider</author>
</authors>
<title>Abstract meaning representation for sembanking.</title>
<date>2013</date>
<contexts>
<context position="728" citStr="Banarescu et al., 2013" startWordPosition="99" endWordPosition="103">Science University of Colorado at Boulder Weite.Chen@colorado.edu Abstract Abstract Meaning Representation (AMR) is a semantic representation language used to capture the meaning of English sentences. In this work, we propose an AMR parser based on dependency parse rewrite rules. This approach transfers dependency parses into AMRs by integrating the syntactic dependencies, semantic arguments, named entity and co-reference information. A dependency parse to AMR graph aligner is also introduced as a preliminary step for designing the parser. 1 Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism that expresses the logical meanings of English sentences in the form of a directed, acyclic graph. AMR focuses on the semantic concepts (nodes on the graph), and relations (labeled edges on the graph) between those concepts. AMR relies heavily on predicate-argument structures defined in the PropBank (PB) (Palmer et al., 2005). The representation encodes rich information, including semantic roles, named entities, and co-reference information. Fig. 1 shows an example AMR. In this proposal, we focus on the design of an automatic AMR parser in a supervised fashion from dep</context>
<context position="3994" citStr="Banarescu et al., 2013" startWordPosition="637" endWordPosition="640">d results are pre41 Proceedings of the ACL-IJCNLP 2015 Student Research Workshop, pages 41–46, Beijing, China, July 28, 2015. c�2015 Association for Computational Linguistics Figure 2: The alignment between an AMR subgraph and a dependency parse. A red line links the corresponding concept and dependency node. sented in Sec. 4, followed by future work. 2 Rewrite Based AMR Parser AMR is a rooted, directed, acyclic graph. For example, the concept join-01 in Fig. 1 is the root meaning of the sentence, which links to the child concepts Arg0, Arg1, and time. AMR adheres to the following principles (Banarescu et al., 2013): • AMRs are rooted acyclic graphs with labels (relations) on edges. These labels indicate the directed relation between two concepts. • AMRs abstract away from syntactic idiosyncracies of a language, and instead attempt to capture only the core meaning of a sentence. • AMRs use the PB framesets as relation labels (Palmer et al., 2005). For example, the relation labels (i.e., ARG0, ARG1) of “join-01” concept in Fig. 1 correspond to the roles of the PB frame “join-v.” • AMRs combine multiple layers of linguistic annotation, like coreference, NE, semantic role, etc., in a single structure. The a</context>
</contexts>
<marker>Banarescu, Bonial, Cai, Georgescu, Griffitt, Hermjakob, Knight, Koehn, Palmer, Schneider, 2013</marker>
<rawString>Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. 2013. Abstract meaning representation for sembanking.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Comput. Linguist.,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>311</pages>
<contexts>
<context position="9217" citStr="Brown et al., 1993" startWordPosition="1529" endWordPosition="1532">phrase of a sentence. They use a set of aligner rules, like NE, fuzzy NE, data entity, etc., with a greedy strategy to match the alignments. This aligner achieves a 90% F1 score on hand aligned AMR-sentence pairs. On the other hand, Pourdamghani et al. (2014) present a generative model to align from AMR graphs to sentence strings. They raises concerns about the lack of sufficient data for learning derivation rules. Instead, they propose a string-to-string alignment model, which transfers the AMR expression to a linearized string representation. Then they use several IBM word alignment models (Brown et al., 1993) on this task. IBM Model-4 with a symmetric method reaches the highest F1 score of 83.1%. Separately analyzing the alignments of roles and non-roles (lexical leaf on AMR), the F1 scores are 49.3% and 89.8%, respectively. In comparison to previous work, our aligner estimates the alignments by learning the transformation probability of lexical form, relations, named entities and semantic roles features jointly. Both the alignment and transformation probabilities are initialized for the training of parser. 3.1 Our Aligner Model with EM Algorithm Our approach, based on the existing IBM Model (Brow</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Comput. Linguist., 19(2):263– 311, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shu Cai</author>
<author>Kevin Knight</author>
</authors>
<title>Smatch: an evaluation metric for semantic feature structures.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>748--752</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6163" citStr="Cai and Knight, 2013" startWordPosition="994" endWordPosition="997"> should be replaced by alternative algorithms, like beam search, to avoid an exponential running time. JAMR (Flanigan et al., 2014) is the first system for AMR parsing, which identifies the concepts, and then searches for a maximum spanning connected subgraph (MSCG) on a fully connected graph to identify the relations between concepts. The search algorithm is similar to the maximum spanning tree algorithms. To assure the final connected graph conforms to linguistic constraints, JAMR uses Lagrangian relaxation (Geoffrion, 2010) to supplement the MSCG algorithm. JAMR reaches a 58% Smatch score (Cai and Knight, 2013) on automatic concept and relation identification data, and 80% on gold concept and automatic relation identification data. 2.1 Our Shift-Reduce Rewrite Rule Parser Rewrite rule based parser is a bottom-up converter from dependency parses to AMRs. The process starts from the leaf word node on the dependency parse. By applying rewrite-rules to each word node, we obtain and assemble the sub-graphs of our target AMR. Sample rewrite rules are listed in Table 1. In these rules, the left hand side contains the dependency information (e.g. word lemma, POS, relation label, NE tag, etc). The right hand</context>
</contexts>
<marker>Cai, Knight, 2013</marker>
<rawString>Shu Cai and Kevin Knight. 2013. Smatch: an evaluation metric for semantic feature structures. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 748–752. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Flanigan</author>
<author>S Thomson</author>
<author>J Carbonell</author>
<author>C Dyer</author>
<author>N A Smith</author>
</authors>
<title>A discriminative graph-based parser for the abstract meaning representation.</title>
<date>2014</date>
<booktitle>In Proc. of ACL,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland,</location>
<contexts>
<context position="5673" citStr="Flanigan et al., 2014" startWordPosition="911" endWordPosition="914">) nn (y) —:name (name(x, y)) -r2 (x)/CD —:quant(x) -r3 (x)/NNS —:unit(x) -r4 npadvmod (x)(y) — temporal-quanity(x, y) -r5 old/JJ (x) — :age(x) -r6 NE PERSON(x)(y) — person(x, y) -r7 Table 1: Sample Rewrite Rules AMR uses graph variables and reentrancy to express coreference (e.g., “p” variable in Fig 1 appears twice – in :ARG0 of join-01 and :ARG0 of have-org-role-91). The reentrancy prevents the AMR graph from begin a tree structure. During parsing decoding, a polynomial time algorithm should be replaced by alternative algorithms, like beam search, to avoid an exponential running time. JAMR (Flanigan et al., 2014) is the first system for AMR parsing, which identifies the concepts, and then searches for a maximum spanning connected subgraph (MSCG) on a fully connected graph to identify the relations between concepts. The search algorithm is similar to the maximum spanning tree algorithms. To assure the final connected graph conforms to linguistic constraints, JAMR uses Lagrangian relaxation (Geoffrion, 2010) to supplement the MSCG algorithm. JAMR reaches a 58% Smatch score (Cai and Knight, 2013) on automatic concept and relation identification data, and 80% on gold concept and automatic relation identif</context>
<context position="8529" citStr="Flanigan et al., 2014" startWordPosition="1411" endWordPosition="1414"> S. • Reduce Move the two top sub-concepts from S, apply the rules, and push it back to S. • Unary Move the top sub-concept from S, apply the rules, and push it back to S. • Finish If no more dependency words are in the list, and one concept is in S, then return. The final AMR concept would be stored at the top of the stack. It is guaranteed that all the AMR expressions can be derived from the dependency parses by using the shift-reduce algorithm. 3 Dependency Parses to AMR Aligner A preliminary step for our rewrite-based parser is the alignment between the AMR and the dependency parse. JAMR (Flanigan et al., 2014) provides a heuristic aligner between an AMR concept and the word or phrase of a sentence. They use a set of aligner rules, like NE, fuzzy NE, data entity, etc., with a greedy strategy to match the alignments. This aligner achieves a 90% F1 score on hand aligned AMR-sentence pairs. On the other hand, Pourdamghani et al. (2014) present a generative model to align from AMR graphs to sentence strings. They raises concerns about the lack of sufficient data for learning derivation rules. Instead, they propose a string-to-string alignment model, which transfers the AMR expression to a linearized str</context>
</contexts>
<marker>Flanigan, Thomson, Carbonell, Dyer, Smith, 2014</marker>
<rawString>J. Flanigan, S. Thomson, J. Carbonell, C. Dyer, and N. A. Smith. 2014. A discriminative graph-based parser for the abstract meaning representation. In Proc. of ACL, Baltimore, Maryland, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrion</author>
</authors>
<title>Lagrangian relaxation for integer programming.</title>
<date>2010</date>
<booktitle>50 Years of Integer Programming 1958-2008,</booktitle>
<pages>243--281</pages>
<editor>In Michael Jnger, Thomas M. Liebling, Denis Naddef, George L. Nemhauser, William R. Pulleyblank, Gerhard Reinelt, Giovanni Rinaldi, and Laurence A. Wolsey, editors,</editor>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="6074" citStr="Geoffrion, 2010" startWordPosition="980" endWordPosition="982">ph from begin a tree structure. During parsing decoding, a polynomial time algorithm should be replaced by alternative algorithms, like beam search, to avoid an exponential running time. JAMR (Flanigan et al., 2014) is the first system for AMR parsing, which identifies the concepts, and then searches for a maximum spanning connected subgraph (MSCG) on a fully connected graph to identify the relations between concepts. The search algorithm is similar to the maximum spanning tree algorithms. To assure the final connected graph conforms to linguistic constraints, JAMR uses Lagrangian relaxation (Geoffrion, 2010) to supplement the MSCG algorithm. JAMR reaches a 58% Smatch score (Cai and Knight, 2013) on automatic concept and relation identification data, and 80% on gold concept and automatic relation identification data. 2.1 Our Shift-Reduce Rewrite Rule Parser Rewrite rule based parser is a bottom-up converter from dependency parses to AMRs. The process starts from the leaf word node on the dependency parse. By applying rewrite-rules to each word node, we obtain and assemble the sub-graphs of our target AMR. Sample rewrite rules are listed in Table 1. In these rules, the left hand side contains the d</context>
</contexts>
<marker>Geoffrion, 2010</marker>
<rawString>ArthurM. Geoffrion. 2010. Lagrangian relaxation for integer programming. In Michael Jnger, Thomas M. Liebling, Denis Naddef, George L. Nemhauser, William R. Pulleyblank, Gerhard Reinelt, Giovanni Rinaldi, and Laurence A. Wolsey, editors, 50 Years of Integer Programming 1958-2008, pages 243–281. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Dan Guildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The Proposition Bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="1080" citStr="Palmer et al., 2005" startWordPosition="157" endWordPosition="160">tactic dependencies, semantic arguments, named entity and co-reference information. A dependency parse to AMR graph aligner is also introduced as a preliminary step for designing the parser. 1 Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism that expresses the logical meanings of English sentences in the form of a directed, acyclic graph. AMR focuses on the semantic concepts (nodes on the graph), and relations (labeled edges on the graph) between those concepts. AMR relies heavily on predicate-argument structures defined in the PropBank (PB) (Palmer et al., 2005). The representation encodes rich information, including semantic roles, named entities, and co-reference information. Fig. 1 shows an example AMR. In this proposal, we focus on the design of an automatic AMR parser in a supervised fashion from dependency parses. In contrast with recent semantic parsing algorithms, we start the parsing process from the dependency parses rather than the sentences. A dependency parse provides both the semantic dependency information for the sentence, and the structure of the relations between the head word and their dependencies. These can provide strong feature</context>
<context position="4331" citStr="Palmer et al., 2005" startWordPosition="696" endWordPosition="699"> future work. 2 Rewrite Based AMR Parser AMR is a rooted, directed, acyclic graph. For example, the concept join-01 in Fig. 1 is the root meaning of the sentence, which links to the child concepts Arg0, Arg1, and time. AMR adheres to the following principles (Banarescu et al., 2013): • AMRs are rooted acyclic graphs with labels (relations) on edges. These labels indicate the directed relation between two concepts. • AMRs abstract away from syntactic idiosyncracies of a language, and instead attempt to capture only the core meaning of a sentence. • AMRs use the PB framesets as relation labels (Palmer et al., 2005). For example, the relation labels (i.e., ARG0, ARG1) of “join-01” concept in Fig. 1 correspond to the roles of the PB frame “join-v.” • AMRs combine multiple layers of linguistic annotation, like coreference, NE, semantic role, etc., in a single structure. The above basic characteristics make the parsing of AMRs a difficult task. First, because AMR abstracts away from syntactic idiosyncrasies, we need a model to link the AMR concepts to words in the original sentence, in order to obtain external lexical, syntactic and semantic features. Secondly, the parser should learn the different feature </context>
</contexts>
<marker>Palmer, Guildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Dan Guildea, and Paul Kingsbury. 2005. The Proposition Bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71–105, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nima Pourdamghani</author>
<author>Yang Gao</author>
<author>Ulf Hermjakob</author>
<author>Kevin Knight</author>
</authors>
<title>Aligning english strings with abstract meaning representation graphs.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>425--429</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8857" citStr="Pourdamghani et al. (2014)" startWordPosition="1472" endWordPosition="1475">k. It is guaranteed that all the AMR expressions can be derived from the dependency parses by using the shift-reduce algorithm. 3 Dependency Parses to AMR Aligner A preliminary step for our rewrite-based parser is the alignment between the AMR and the dependency parse. JAMR (Flanigan et al., 2014) provides a heuristic aligner between an AMR concept and the word or phrase of a sentence. They use a set of aligner rules, like NE, fuzzy NE, data entity, etc., with a greedy strategy to match the alignments. This aligner achieves a 90% F1 score on hand aligned AMR-sentence pairs. On the other hand, Pourdamghani et al. (2014) present a generative model to align from AMR graphs to sentence strings. They raises concerns about the lack of sufficient data for learning derivation rules. Instead, they propose a string-to-string alignment model, which transfers the AMR expression to a linearized string representation. Then they use several IBM word alignment models (Brown et al., 1993) on this task. IBM Model-4 with a symmetric method reaches the highest F1 score of 83.1%. Separately analyzing the alignments of roles and non-roles (lexical leaf on AMR), the F1 scores are 49.3% and 89.8%, respectively. In comparison to pr</context>
<context position="16729" citStr="Pourdamghani et al., 2014" startWordPosition="2800" endWordPosition="2803">word nodes in the dev. and test sets. We initialize Prel, PNE, and PSR with the dev. set. 4.2 Results We run our first round of EM (Step 2 in Initialization of Sec. 3.1) for 100 iterations, then use the second round (Step 4 in Initialization of Sec. 3.1) for another 100 iterations. We run our decoding algorithm and evaluation on the test set after the first and second round of EM. Due to time constraints, we did not train the q here. The experimental results are listed in Table 4. We evaluate the performance on the precision, recall, and F1 score. Using just the Plemma (a similar approach to (Pourdamghani et al., 2014)), we achieve 53.4% F1 score on the test set. On the other hand, our aligner reaches 57.0% F1 score with the full aligner. 5 Conclusion and Future Work In this research, we briefly introduce AMR. We describe the design principles and characteristics of AMR, and show how the AMR parser task is important, yet difficult. We present the basic idea for a proposed AMR parser, based on the shift-reduce algorithm. We also present an AMRDependency Parse aligner, because such an aligner 2LDC OntoNotes Release 5.0, Release date: October 16, 2013 https://catalog.ldc.upenn.edu/LDC2013T19 will be a necessar</context>
</contexts>
<marker>Pourdamghani, Gao, Hermjakob, Knight, 2014</marker>
<rawString>Nima Pourdamghani, Yang Gao, Ulf Hermjakob, and Kevin Knight. 2014. Aligning english strings with abstract meaning representation graphs. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 425–429. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chuan Wang</author>
<author>Xue Nianwen</author>
<author>Pradhan Sameer</author>
</authors>
<title>A transition-based algorithm for amr parsing.</title>
<date>2015</date>
<booktitle>In Proceedings of the 2015 Conference of the North American Chapter of the Association</booktitle>
<contexts>
<context position="7098" citStr="Wang et al., 2015" startWordPosition="1146" endWordPosition="1149">y applying rewrite-rules to each word node, we obtain and assemble the sub-graphs of our target AMR. Sample rewrite rules are listed in Table 1. In these rules, the left hand side contains the dependency information (e.g. word lemma, POS, relation label, NE tag, etc). The right hand side is the AMR concept and its template for filling variables from previous parsing steps. The sample derivation steps are listed in Table 2. For every step, it shows the derivation rule applied (in Table 1), and the concept name, c1-c8. This approach to parsing could be implemented with a shift-reduce algorithm (Wang et al., 2015). We define a stack and a list of tokens, which stores the dependency words in the order of tree traversal. Several actions are defined to operate on the list(L) and the stack(S): 42 Derivation Apply Concept Rule Name Pierre/NNP — :op Pierre r1 c1 Vinken/NNP — :op Vinken r1 c2 (c1) nn (c2) r2 c3 —:name (name :op1 Pierre :op2 Vinekn) 61/CD — :quant 61 r3 c4 years/NNS — :unit year r4 c5 npadvmod (c4)(c5) r5 c6 — temporal-quanity :quant 61 :unit year old/JJ (c6) r6 c7 — :age (temporal-quanity :quant 61 :unit year) NE PERSON (c3)(c7) r7 c8 — person :name (name :op1 Pierre :op2 Vinekn) :age (tempor</context>
</contexts>
<marker>Wang, Nianwen, Sameer, 2015</marker>
<rawString>Chuan Wang, Xue Nianwen, and Pradhan Sameer. 2015. A transition-based algorithm for amr parsing. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>