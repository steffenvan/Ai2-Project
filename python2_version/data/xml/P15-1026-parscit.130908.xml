<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000005">
<title confidence="0.9939615">
Question Answering over Freebase with
Multi-Column Convolutional Neural Networks
</title>
<author confidence="0.951971">
Li Dong†* Furu Wei$ Ming Zhou$ Ke Xu††SKLSDE Lab, Beihang University, Beijing, China
</author>
<affiliation confidence="0.586913">
$Microsoft Research, Beijing, China
</affiliation>
<email confidence="0.9814905">
donglixp@gmail.com {fuwei,mingzhou}@microsoft.com
kexu@nlsde.buaa.edu.cn
</email>
<sectionHeader confidence="0.993778" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999911827586207">
Answering natural language questions
over a knowledge base is an important and
challenging task. Most of existing sys-
tems typically rely on hand-crafted fea-
tures and rules to conduct question under-
standing and/or answer ranking. In this pa-
per, we introduce multi-column convolu-
tional neural networks (MCCNNs) to un-
derstand questions from three different as-
pects (namely, answer path, answer con-
text, and answer type) and learn their dis-
tributed representations. Meanwhile, we
jointly learn low-dimensional embeddings
of entities and relations in the knowledge
base. Question-answer pairs are used to
train the model to rank candidate answers.
We also leverage question paraphrases to
train the column networks in a multi-task
learning manner. We use FREEBASE as
the knowledge base and conduct exten-
sive experiments on the WEBQUESTIONS
dataset. Experimental results show that
our method achieves better or comparable
performance compared with baseline sys-
tems. In addition, we develop a method
to compute the salience scores of question
words in different column networks. The
results help us intuitively understand what
MCCNNs learn.
</bodyText>
<sectionHeader confidence="0.999334" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9968028125">
Automatic question answering systems return the
direct and exact answers to natural language ques-
tions. In recent years, the development of large-
scale knowledge bases, such as FREEBASE (Bol-
lacker et al., 2008), provides a rich resource to
answer open-domain questions. However, how
∗Contribution during internship at Microsoft Research.
to understand questions and bridge the gap be-
tween natural languages and structured semantics
of knowledge bases is still very challenging.
Up to now, there are two mainstream methods
for this task. The first one is based on seman-
tic parsing (Berant et al., 2013; Berant and Liang,
2014) and the other relies on information extrac-
tion over the structured knowledge base (Yao and
Van Durme, 2014; Bordes et al., 2014a; Bordes
et al., 2014b). The semantic parsers learn to un-
derstand natural language questions by converting
them into logical forms. Then, the parse results
are used to generate structured queries to search
knowledge bases and obtain the answers. Re-
cent works mainly focus on using question-answer
pairs, instead of annotated logical forms of ques-
tions, as weak training signals (Liang et al., 2011;
Krishnamurthy and Mitchell, 2012) to reduce an-
notation costs. However, some of them still as-
sume a fixed and pre-defined set of lexical trig-
gers which limit their domains and scalability ca-
pability. In addition, they need to manually de-
sign features for semantic parsers. The second
approach uses information extraction techniques
for open question answering. These methods re-
trieve a set of candidate answers from the knowl-
edge base, and the extract features for the question
and these candidates to rank them. However, the
method proposed by Yao and Van Durme (2014)
relies on rules and dependency parse results to ex-
tract hand-crafted features for questions. More-
over, some methods (Bordes et al., 2014a; Bordes
et al., 2014b) use the summation of question word
embeddings to represent questions, which ignores
word order information and cannot process com-
plicated questions.
In this paper, we introduce the multi-column
convolutional neural networks (MCCNNs) to au-
tomatically analyze questions from multiple as-
pects. Specifically, the model shares the same
word embeddings to represent question words.
</bodyText>
<page confidence="0.943491">
260
</page>
<note confidence="0.976571">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 260–269,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.9996464">
MCCNNs use different column networks to ex-
tract answer types, relations, and context informa-
tion from the input questions. The entities and
relations in the knowledge base (namely FREE-
BASE in our experiments) are also represented as
low-dimensional vectors. Then, a score layer is
employed to rank candidate answers according to
the representations of questions and candidate an-
swers. The proposed information extraction based
method utilizes question-answer pairs to automat-
ically learn the model without relying on manually
annotated logical forms and hand-crafted features.
We also do not use any pre-defined lexical triggers
and rules. In addition, the question paraphrases
are also used to train networks and generalize for
the unseen words in a multi-task learning manner.
We have conducted extensive experiments on WE-
BQUESTIONS. Experimental results illustrate that
our method outperforms several baseline systems.
The contributions of this paper are three-fold:
</bodyText>
<listItem confidence="0.9195468">
• We introduce multi-column convolutional
neural networks for question understanding
without relying on hand-crafted features and
rules, and use question paraphrases to train
the column networks and word vectors in a
multi-task learning manner;
• We jointly learn low-dimensional embed-
dings for the entities and relations in FREE-
BASE with question-answer pairs as supervi-
sion signals;
• We conduct extensive experiments on the
WEBQUESTIONS dataset, and provide some
intuitive interpretations for MCCNNs by de-
veloping a method to detect salient question
words in the different column networks.
</listItem>
<sectionHeader confidence="0.999566" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99994073015873">
The state-of-the-art methods for question answer-
ing over a knowledge base can be classified into
two classes, i.e., semantic parsing based and in-
formation retrieval based.
Semantic parsing based approaches aim at
learning semantic parsers which parse natural lan-
guage questions into logical forms and then query
knowledge base to lookup answers. The most im-
portant step is mapping questions into predefined
logical forms, such as combinatory categorial
grammar (Cai and Yates, 2013) and dependency-
based compositional semantics (Liang et al.,
2011). Some semantic parsing based systems
required manually annotated logical forms to
train the parsers (Zettlemoyer and Collins, 2005;
Kwiatkowski et al., 2010). These annotations are
relatively expensive. So recent works (Liang et
al., 2011; Kwiatkowski et al., 2013; Berant et al.,
2013; Berant and Liang, 2014; Bao et al., 2014;
Reddy et al., 2014) mainly aimed at using weak
supervision (question-answer pairs) to effectively
train semantic parsers. These methods achieved
comparable results without using logical forms an-
notated by experts. However, some methods relied
on lexical triggers or manually defined features.
On the other hand, information retrieval based
systems retrieve a set of candidate answers and
then conduct further analysis to obtain answers.
Their main difference is how to select correct an-
swers from the candidate set. Yao and Van Durme
(2014) used rules to extract question features from
dependency parse of questions, and used rela-
tions and properties in the retrieved topic graph
as knowledge base features. Then, the production
of these two kinds of features was fed into a lo-
gistic regression model to classify the question’s
candidate answers into correct/wrong. In contrast,
we do not use rules, dependency parse results, or
hand-crafted features for question understanding.
Some other works (Bordes et al., 2014a; Bordes
et al., 2014b) learned low-dimensional vectors for
question words and knowledge base constitutes,
and used the sum of vectors to represent questions
and candidate answers. However, simple vector
addition ignores word order information and high-
order n-grams. For example, the question repre-
sentations of who killed A and who A killed are
same in the vector addition model. We instead
use multi-column convolutional neural networks
which are more powerful to process complicated
question patterns. Moreover, our multi-column
network architecture distinguishes between infor-
mation of answer type, answer path and answer
context by learning multiple column networks,
while the addition model mixes them together.
Another line of related work is applying deep
learning techniques for the question answering
task. Grefenstette et al. (2014) proposed a deep
architecture to learn a semantic parser from anno-
tated logic forms of questions. Iyyer et al. (2014)
introduced dependency-tree recursive neural net-
works for the quiz bowl game which asked play-
ers to answer an entity for a given paragraph. Yu et
</bodyText>
<page confidence="0.995031">
261
</page>
<bodyText confidence="0.999608153846154">
al. (2014) proposed a bigram model based on con-
volutional neural networks to select answer sen-
tences from text data. The model learned a simi-
larity function between questions and answer sen-
tences. Yih et al. (2014) used convolutional neu-
ral networks to answer single-relation questions
on REVERB (Fader et al., 2011). However, the
system worked on relation-entity triples instead of
more structured knowledge bases. For instance,
the question shown in Figure 1 is answered by us-
ing several triples in FREEBASE. Also, we can
utilize richer information (such as entity types) in
structured knowledge bases.
</bodyText>
<sectionHeader confidence="0.997838" genericHeader="method">
3 Setup
</sectionHeader>
<bodyText confidence="0.999974295454545">
Given a natural language question q = w1 ... wn,
we retrieve related entities and properties from
FREEBASE and use them as the candidate answers
CQ. Our goal is to score these candidates and pre-
dict answers. For instance, the correct output of
the question when did Avatar release in UK is
2009-12-17. It should be noted that there may
be several correct answers for a question. In or-
der to train the model, we use question-answer
pairs without annotated logic forms. We further
describe the datasets used in our work as follows:
WebQuestions This dataset (Berant et al., 2013)
contains 3,778 training instances and 2,032 test
instances. We further split the training instances
into the training set and the development set by
80%/20%. The questions were collected by query-
ing the Google Suggest API. A breadth-first search
beginning with wh- was conducted. Then, answers
were annotated in Amazon Mechanical Turk. All
the answers can be found in FREEBASE.
Freebase It is a large-scale knowledge base that
consists of general facts (Bollacker et al., 2008).
These facts are organized as subject-property-
object triples. For example, the fact Avatar is
directed by James Cameron is represented by
(/m/0bth54, film.film.directed by, /m/03 gd) in
RDF format. The preprocess method presented
in (Bordes et al., 2014a) was used to make FREE-
BASE fit in memory. Specifically, we kept the
triples where one of the entities appeared in the
training/development set of WEBQUESTIONS or
CLUEWEB extractions provided in (Lin et al.,
2012), and removed the entities appearing less
than five times. Then, we obtained 18M triples
that contained 2.9M entities and 7k relation types.
As described in (Bordes et al., 2014a), this prepro-
cess method does not ease the task because WE-
BQUESTIONS only contains about 2k entities.
WikiAnswers Fader et al. (2013) extracted the
similar questions on WIKIANSWERS and used
them as question paraphrases. There are 350,000
paraphrase clusters which contain about two mil-
lion questions. They are used to generalize for un-
seen words and question patterns.
</bodyText>
<sectionHeader confidence="0.997843" genericHeader="method">
4 Methods
</sectionHeader>
<bodyText confidence="0.997935035714286">
The overview of our framework is shown in
Figure 1. For instance, for the question when
did Avatar release in UK, the related nodes of
the entity Avatar are queried from FREEBASE.
These related nodes are regarded as candidate an-
swers (CQ). Then, for every candidate answer a,
the model predicts a score 5 (q, a) to determine
whether it is a correct answer or not.
We use multi-column convolutional neural net-
works (MCCNNs) to learn representations of
questions. The models share the same word em-
beddings, and have multiple columns of convolu-
tional neural networks. The number of columns
is set to three in our QA task. These columns
are used to analyze different aspects of a ques-
tion, i.e., answer path, answer context, and answer
type. The vector representations learned by these
columns are denoted as f1 (q) , f2 (q) , f3 (q). We
also learn embeddings for the candidate answers
appeared in FREEBASE. For every candidate an-
swer a, we compute its vector representations and
denote them as g1 (a) , g2 (a) , g3 (a). These three
vectors correspond to the three aspects used in
question understanding. Using these vector rep-
resentations defined for questions and answers, we
can compute the score for the question-answer pair
(q, a). Specifically, the scoring function 5 (q, a) is
defined as:
</bodyText>
<equation confidence="0.9968496">
5 (q, a) =
+ f3 (q)Tg3 (a)
 |{z }
answer type
(1)
</equation>
<bodyText confidence="0.99985">
where fi (q) and gi (a) have the same dimension.
As shown in Figure 1, the score layer computes
scores and adds them together.
</bodyText>
<subsectionHeader confidence="0.99767">
4.1 Candidate Generation
</subsectionHeader>
<bodyText confidence="0.999703">
The first step is to retrieve candidate answers from
FREEBASE for a question. Questions should con-
tain an identified entity that can be linked to the
</bodyText>
<figure confidence="0.99775193220339">
f1 (q)Tg1 (a) + f2 (q)Tg2 (a)
 |{z }  |{z }
answer path answer context
262
Convolutional Layer
Max-Pooling Layer
Shared Word
Representations
Score Layer
&lt;L&gt; when did Avatar release in UK &lt;R&gt;
Dot Product
+ +
Score
people.person
type.object.type
James Cameron
m.03_gd
type.object.type
Answer
Path
film.film.directed_by
film.producer
Avatar
m.0bth54
Answer
Context
film.film_regional_release
_date.film_release_region
United States
of America
m.09c7w0
film.film.release
_date_s
film.film.
release_date_s
Answer
Type
m.09w09jk
datetime
film.film_regional_release
_date.film_release_region
United Kingdom
m.07ssc
datetime
film.film_regional_release
_date.release_date
value_type
type.object.type
value_type
m.0gdp17z
film.film_regional_release
_date.release_date
type.object.type
2009-12-17
2009-12-18
film.film_region
al_release_date
film.film_region
al_release_date
</figure>
<figureCaption confidence="0.8384975">
Figure 1: Overview for the question-answer pair (when did Avatar release in UK, 2009-12-17). Left:
network architecture for question understanding. Right: embedding candidate answers.
</figureCaption>
<bodyText confidence="0.999625545454546">
knowledge base. We use the Freebase Search
API (Bollacker et al., 2008) to query named en-
tities in a question. If there is not any named en-
tity, noun phrases are queried. We use the top one
entity in the ranked list returned by the API. This
entity resolution method was also used in (Yao and
Van Durme, 2014). Better methods can be devel-
oped, while it is not the focus of this paper. Then,
all the 2-hops nodes of the linked entity are re-
garded as the candidate answers. We denote the
candidate set for the question q as Cq.
</bodyText>
<subsectionHeader confidence="0.920383">
4.2 MCCNNs for Question Understanding
</subsectionHeader>
<bodyText confidence="0.999932555555555">
MCCNNs use multiple convolutional neural net-
works to learn different aspects of questions from
shared input word embeddings. For every single
column, the network structure presented in (Col-
lobert et al., 2011) is used to tackle the variable-
length questions.
We present the model in the left part of Figure 1.
Specifically, for the question q = w1 ... wn, the
lookup layer transforms every word into a vector
wj = Wvu(wj), where Wv ∈ Rdvx|V  |is the
word embedding matrix, u(wj) ∈ {0,1}|V  |is the
one-hot representation of wj, and |V  |is the vocab-
ulary size. The word embeddings are parameters,
and are updated in the training process.
Then, the convolutional layer computes repre-
sentations of the words in sliding windows. For
the i-th column of MCCNNs, the convolutional
layer computes n vectors for question q. The j-
</bodyText>
<equation confidence="0.984651">
th vector is:
x�i) = h W(i) 1wTj_s ... w� ... w+ T
3] + b
</equation>
<bodyText confidence="0.996136545454545">
where (2s + 1) is the window size, W(i) ∈
Rdqx(2s+1)dv is the weight matrix of convolutional
layer, b(i) ∈ Rdqx1 is the bias vector, and h (·) is
the nonlinearity function (such as softsign, tanh,
and sigmoid). Paddings are used for left and right
absent words.
Finally, a max-pooling layer is followed to ob-
tain the fixed-size vector representations of ques-
tions. The max-pooling layer in the i-th column
of MCCNNs computes the representation of the
question q via:
</bodyText>
<equation confidence="0.983660333333333">
fi (q) = max {x(i)
j } (3)
j=1,...,n
</equation>
<bodyText confidence="0.9995155">
where max{·} is an element-wise operator over
vectors.
</bodyText>
<subsectionHeader confidence="0.999139">
4.3 Embedding Candidate Answers
</subsectionHeader>
<bodyText confidence="0.999550555555556">
Vector representations g1 (a) , g2 (a) , g3 (a) are
learned for the candidate answer a. The vectors
are employed to represent different aspects of a.
The embedding methods are described as follows:
Answer Path The answer path is the set of
relations between the answer node and the entity
asked in question. As shown in Figure 1, the
2-hops path between the entity Avatar and the
correct answer is (film.film.release date s,
</bodyText>
<figure confidence="0.62227">
�(i)
(2)
</figure>
<page confidence="0.964457">
263
</page>
<bodyText confidence="0.96328085">
film.film regional release date.release date).
The vector representation g1(a) is computed
1
via g1(a) = kup(a)k1Wpup(a), where 11·111
is 1-norm, up(a) E R|R|×1 is a binary vector
which represents the presence or absence of every
relation in the answer path, Wp E Rdq×|R |is
the parameter matrix, and |R |is the number
of relations. In other words, the embeddings
of relations that appear on the answer path are
averaged.
Answer Context The 1-hop entities and relations
connected to the answer path are regarded as the
answer context. It is used to deal with constraints
in questions. For instance, as shown in Figure 1,
the release date of Avatar in UK is asked, so
it is not enough that only the triples on answer
path are considered. With the help of context in-
formation, the release date in UK has a higher
score than in USA. The context representation is
</bodyText>
<equation confidence="0.7762185">
g2(a) = 1
kuc(a)k1 W,u,(a), where W, E Rdq×|C|
</equation>
<bodyText confidence="0.999350857142857">
is the parameter matrix, u,(a) E R|C|×1 is a bi-
nary vector expressing the presence or absence of
context nodes, and |C |is the number of entities
and relations which appear in answer context.
Answer Type Type information in FREEBASE is
an important clue to score candidate answers. As
illustrated in Figure 1, the type of 2009-12-17
is datetime, and the type of James Cameron is
people.person and film.producer. For the ex-
ample question when did Avatar release in UK,
the candidate answers whose types are datetime
should be assigned with higher scores than others.
The vector representation is defined as g3(a) =
kut(a)k1Wtut(a), where Wt E Rdq×|T |is the
</bodyText>
<equation confidence="0.604957">
1
</equation>
<bodyText confidence="0.999943571428571">
matrix of type embeddings, ut(a) E R|T|×1 is a
binary vector which indicates the presence or ab-
sence of answer types, and |T |is the number of
types. In our implementation, we use the relation
common.topic.notable types to query types. If
a candidate answer is a property value, we instead
use its value type (e.g., float, string, datetime).
</bodyText>
<subsectionHeader confidence="0.99714">
4.4 Model Training
</subsectionHeader>
<bodyText confidence="0.9511809">
For every correct answer a E Aq of the question
q, we randomly sample k wrong answers a0 from
the set of candidate answers Cq, and use them as
negative instances to estimate parameters. To be
more specific, the hinge loss is considered for pairs
(q, a) and (q, a0):
l (q, a, a0) = (m − 5(q, a) + 5(q, a0))+ (4)
where 5(·, ·) is the scoring function defined in
Equation (1), m is the margin parameter employed
to regularize the gap between two scores, and
</bodyText>
<equation confidence="0.84398325">
(z)+ = max{0, z}. The objective function is:
l (q, a, a0) (5)
where |Aq |is the number of correct answers, and
Rq C Cq \ Aq is the set of k wrong answers.
</equation>
<bodyText confidence="0.6438355">
The back-propagation algorithm (Rumelhart et
al., 1986) is used to train the model. It back-
propagates errors from top to the other layers.
Derivatives are calculated and gathered to update
parameters. The AdaGrad algorithm (Duchi et al.,
2011) is then employed to solve this non-convex
optimization problem. Moreover, the max-norm
regularization (Srebro and Shraibman, 2005; Sri-
vastava et al., 2014) is used for the column vectors
of parameter matrices.
</bodyText>
<sectionHeader confidence="0.513581" genericHeader="method">
4.5 Inference
</sectionHeader>
<bodyText confidence="0.999923555555556">
During the test, we retrieve all the candidate an-
swers Cq for the question q. For every candidate
ˆa, we compute its score 5(q, ˆa). Then, the candi-
date answers with the highest scores are regarded
as predicted results.
Because there may be more than one correct
answers for some questions, we need a criterion
to determine the score threshold. Specifically, the
following equation is used to determine outputs:
</bodyText>
<equation confidence="0.966496333333333">
ˆAq = {ˆa  |aˆE Cq and
max{5(q, a0)} − 5(q, ˆa) &lt; m} (6)
a&apos;∈Cq
</equation>
<bodyText confidence="0.999781928571429">
where m is the margin defined in Equation (4).
The candidates whose scores are not far from the
best answer are regarded as predicted results.
Some questions may have a large set of can-
didate answers. So we use a heuristic method to
prune their candidate sets. To be more specific, if
the number of candidates on the same answer path
is greater than 200, we randomly keep 200 candi-
dates for this path. Then, we score and rank all
these generated candidate answers together. If one
of the candidates on the pruned path is regarded as
a predicted answer, we further score the other can-
didates that are pruned on this path and determine
the final results.
</bodyText>
<figure confidence="0.511663333333333">
�min 1 �
q � a&apos;∈Rq
|Aq |a∈Aq
</figure>
<page confidence="0.990373">
264
</page>
<subsectionHeader confidence="0.9942455">
4.6 Question Paraphrases for Multi-Task
Learning
</subsectionHeader>
<bodyText confidence="0.994333857142857">
We use the question paraphrases dataset WIKIAN-
SWERS to generalize for words and question pat-
terns which are unseen in the training set of
question-answer pairs. The question understand-
ing results of paraphrases should be same. Con-
sequently, the representations of two paraphrases
computed by the same column of MCCNNs
should be similar. We use dot similarity to define
the hinge loss lp (q1, q2, q3) as:
(7)
where q1, q2 are questions in the same paraphrase
cluster P, q3 is randomly sampled from another
cluster, and mp is the margin. The objective func-
tion is defined as:
</bodyText>
<equation confidence="0.990975">
lp (q1, q2, q3) (8)
</equation>
<bodyText confidence="0.99979575">
where RP contains kp questions which are ran-
domly sampled from other clusters. The same op-
timization algorithm described in Section 4.4 is
used to update parameters.
</bodyText>
<sectionHeader confidence="0.999423" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.99934705">
In order to evaluate the model, we use the dataset
WEBQUESTIONS (Section 3) to conduct experi-
ments.
Settings The development set is used to select
hyper-parameters in the experiments. The nonlin-
earity function f = tanh is employed. The di-
mension of word vectors is set to 25. They are ini-
tialized by the pre-trained word embeddings pro-
vided in (Turian et al., 2010). The window size
of MCCNNs is 5. The dimension of the pooling
layers and the dimension of answer embeddings
are set to 64. The parameters are initialized by
the techniques described in (Bengio, 2012). The
max value used for max-norm regularization is 3.
The initial learning rate used in AdaGrad is set to
0.01. A mini-batch consists of 10 question-answer
pairs, and every question-answer pair has k nega-
tive samples that are randomly sampled from its
candidate set. The margin values in Equation (4)
and Equation (7) is set to m = 0.5 and mp = 0.1.
</bodyText>
<table confidence="0.998053">
Method F1 P@1
(Berant et al., 2013) 31.4 -
(Berant and Liang, 2014) 39.9 -
(Bao et al., 2014) 37.5 -
(Yao and Van Durme, 2014) 33.0 -
(Bordes et al., 2014a) 39.2 40.4
(Bordes et al., 2014b) 29.7 31.3
MCCNN (our) 40.8 45.1
</table>
<tableCaption confidence="0.951189">
Table 1: Evaluation results on the test split of WE-
BQUESTIONS.
</tableCaption>
<subsectionHeader confidence="0.989724">
5.1 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999898375">
The evaluation metrics macro F1 score (Berant et
al., 2013) and precision @ 1 (Bordes et al., 2014a)
are reported. We use the official evaluation script
provided by Berant et al. (2013) to compute the F1
score. Notably, the F1 score defined in (Yao and
Van Durme, 2014) is slightly different from others
(how to compute scores for the questions without
predicted results). We instead use the original def-
inition in experiments.
As shown in Table 1, our method achieves bet-
ter or comparable results than baseline methods on
WEBQUESTIONS. To be more specific, the first
three rows are semantic parsing based methods,
and the other baselines are information extraction
based methods. These approaches except (Bordes
et al., 2014a; Bordes et al., 2014b) rely on hand-
crafted features and predefined rules. The results
show that automatically question understanding
can be as good as the models using manually de-
signed features. Besides, our multi-column convo-
lutional neural networks based model outperforms
the methods that use the sum of word embeddings
as question representations (Bordes et al., 2014a;
Bordes et al., 2014b).
</bodyText>
<subsectionHeader confidence="0.999822">
5.2 Model Analysis
</subsectionHeader>
<bodyText confidence="0.999997">
We also conduct ablation experiments to compare
the results using different experiment settings. As
shown in Table 2, the abbreviation w/o means re-
moving a particular part from the model. We
find that answer path information is most impor-
tant among these three columns, and answer type
information is more important than answer con-
text information. The reason is that answer path
and answer type are more direct clues for ques-
tions, but answer context is used to handle addi-
tional constraints in questions which are less com-
mon in the dataset. Moreover, we compare to the
</bodyText>
<equation confidence="0.9880115">
lp (q1, q2, q3) =
E3
i=1
(mp − fi (q1)Tfi (q2) + fi (q1)T fi (q3))+
Emin E E
P q1,q2∈P q3∈RP
</equation>
<page confidence="0.978743">
265
</page>
<table confidence="0.99878925">
Setting F1 P@1
all 40.8 45.1
w/o path 32.5 37.1
w/o type 37.7 40.9
w/o context 39.1 41.0
w/o multi-column 38.4 41.8
w/o paraphrase 40.0 43.9
1-hop 29.3 32.2
</table>
<tableCaption confidence="0.8534355">
Table 2: Evaluation results of different set-
tings on the test split of WEBQUESTIONS. w/o
</tableCaption>
<bodyText confidence="0.906873">
path/type/context: without using the specific col-
umn. w/o multi-column: tying parameters of mul-
tiple columns. w/o paraphrase: without using
question paraphrases for training. 1-hop: using 1-
hop paths to generate candidate answers.
model using single-column networks (w/o multi-
column), i.e., tying the parameters of different
columns. The results indicate that using multiple
columns to understand questions from different
aspects improves the performance. Besides, we
find that using question paraphrases in a multi-task
learning manner contributes to the performance.
In addition, we evaluate the results only using 1-
hop paths to generate candidate answers. Com-
pared to using 2-hops paths, we find that the per-
formance drops significantly. This indicates only
using the nodes directly connected to the queried
entity in FREEBASE cannot handle many ques-
tions.
</bodyText>
<subsectionHeader confidence="0.998844">
5.3 Salient Words Detection
</subsectionHeader>
<bodyText confidence="0.999955181818182">
In order to analyze the model, we detect salient
words in questions. The salience score of a ques-
tion word depends on how much the word affects
the computation of question representation. In
other words, if a word plays more important role
in the model, its salience score should be larger.
We compute several salience scores for a same
word to illustrate its importance in different
columns of networks. For the i-th column, the
salience score of word wj in the question q = wn1
is defined as:
</bodyText>
<equation confidence="0.99855125">
� � ��
� �
ei(wj) = �fi (wn wj−1
1 ) − fi 1 w�jwnj+1 2 (9)
</equation>
<bodyText confidence="0.997567125">
where the word wj is replaced with w�, and 11·112
denotes Euclidean norm. In practice, we replace
wj with several stop words (such as is, to, and a),
and then compute their average score.
what type of car does weston drive
what countries speak german as a first language
who is the current leader of cuba today
where is the microsoft located Answer Path
</bodyText>
<subsectionHeader confidence="0.4893495">
Answer Type
Answer Context
</subsectionHeader>
<figureCaption confidence="0.957936833333333">
Figure 2: Salient words detection results for ques-
tions. From left to right, the three bars of every
word correspond to salience scores in answer path
column, answer type column, and answer context
column, respectively. The salience scores are nor-
malized by the max values of different columns.
</figureCaption>
<bodyText confidence="0.999987125">
As shown in Figure 2, we compute salience
scores for several questions, and normalize them
by the max values in different columns. We clearly
see that these words play different roles in a ques-
tion. The overall conclusion is that the wh- words
(such as what, who and where) tend to be impor-
tant for question understanding. Moreover, nouns
dependent of the wh- words and verbs are impor-
tant clues to obtain question representations. For
instance, the figure demonstrates that the nouns
type/country/leader and the verbs speak/located
are salient in the columns of networks. These
observations agree with previous works (Li and
Roth, 2002). Some manually defined rules (Yao
and Van Durme, 2014) used in the question an-
swering task are also based on them.
</bodyText>
<subsectionHeader confidence="0.99092">
5.4 Examples
</subsectionHeader>
<bodyText confidence="0.999954461538461">
Question representations computed by different
columns of MCCNNs are used to query their most
similar neighbors. We use cosine similarity in ex-
periments. This experiment demonstrates whether
the model learns different aspects of questions.
For example, if a column of networks is employed
to analyze answer types, the answer types of near-
est questions should be same as the query.
As shown in Table 3, these three columns of ta-
ble correspond to different columns of networks.
To be more specific, the first column is used to
process answer path. We find that the model
learns different question patterns for the same
</bodyText>
<page confidence="0.994759">
266
</page>
<bodyText confidence="0.956545963636364">
Column 1 (Answer Path) Column 2 (Answer Type) Column 3 (Answer Context)
what to do in hollywood can this weekend
what to do in midland tx this weekend
what to do in cancun with family
what to do at fairfield can
what to see in downtown asheville nc
what to see in toronto top 10
where be george washington originally from
where be george washington carver from
where be george bush from
where be the thame river source
where be the main headquarters of google
in what town do ned kelly and he family grow up
where do charle draw go to college
where do kevin love go to college
where do pauley perrette go to college
where do kevin jame go to college
where do charle draw go to high school
where do draw bree go to college wikianswer
who found collegehumor
who found the roanoke settlement
who own skywest
who start mary kay
who be the owner of kfc
who own wikimedium foundation
who be the leader of north korea today
who be the leader of syrium now
who be the leader of cuba 2012
who be the leader of france 2012
who be the current leader of cuba today
who be the minority leader of the house of representative now
who be judy garland father
who be clint eastwood date
who be emma stone father
who be robin robert father
who miley cyrus engage to
who be chri cooley marry to
what type of money do japanese use
what kind of money do japanese use
what type of money do jamaica use
what type of currency do brazil use
what type of money do you use in cuba
what money do japanese use
what be the two official language of paraguay
what be the local language of israel
what be the four official language of nigerium
what be the official language of jamaica
what be the dominant language of jamaica
what be the official language of brazil now
what be the timezone in vancouver
what be my timezone in californium
what be los angeles california time zone
what be my timezone in oklahoma
what be my timezone in louisiana
what be the time zone in france
</bodyText>
<tableCaption confidence="0.662932">
Table 3: Using question representations obtained by different column networks to query the nearest
</tableCaption>
<bodyText confidence="0.99300125">
neighbors. From left to right, the three columns are used to analyze information about answer path,
answer type, and answer context, respectively. Lemmatization is used to better show question patterns.
path. For instance, the vector representations of
“who found/own/start *” and “who be the owner
of *” obtained by the first column are similar. The
second column is employed to extract answer type
information from questions. The answer types
of example questions in Table 3 are same, while
they may ask different relations. The third col-
umn learns to embed question information into an-
swer context. We find that the similar questions
are clustered together by this column.
</bodyText>
<subsectionHeader confidence="0.637574">
5.5 Error Analysis
</subsectionHeader>
<bodyText confidence="0.999960375">
We investigate the predicted results on the devel-
opment set, and show several error causes as fol-
lows.
Candidate Generation Some entity mentions in
questions are linked incorrectly, hence we can-
not obtain the desired candidate answers. As
described in (Yao and Van Durme, 2014), the
Freebase Search API returned correct entities for
86.4% of questions in top one results. Because
some questions use the abbreviation or a part of
its mention to express an entity. For example, it
is not trivial to link jfk to John F. Kennedy in the
question “where did jfk and his wife live”. A bet-
ter entity retrieval step should be developed for the
open question answering scenario.
Time-Aware Questions We need to compare date
values for some time-aware questions. For in-
stance, to answer the question “who is johnny
cash’s first wife”, we have to know the order of
several marriages by comparing the marriage date.
Its correct response should contain only one en-
tity (vivian liberto). However, our system addi-
tionally outputs june carter cash who is his sec-
ond wife, because both the candidate answers are
connected to johnny cash by the relation peo-
ple.person.spouse s. In order to solve this is-
sue, we need to define some ad-hoc operators used
for comparisons or develop more advanced se-
mantic representations.
Ambiguous Questions Some questions are am-
biguous to obtain their correct representations. For
example, the question what has anna kendrick
been in is used to ask what movies she has played
in. This question does not have explicit clue words
to indicate the meanings, so it is difficult to rank
the candidates. Moreover, the question who is
aidan quinn is employed to ask what his occupa-
tion is. It also lacks sufficient clues for question
understanding, and using who is to ask occupation
is rare in the training data.
</bodyText>
<sectionHeader confidence="0.997011" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999968571428571">
This paper presents a method for question answer-
ing over FREEBASE using multi-column convo-
lutional neural networks (MCCNNs). MCCNNs
share the same word embeddings, and use multi-
ple columns of convolutional neural networks to
learn the representations of different aspects of
questions. Accordingly, we use low-dimensional
embeddings to represent multiple aspects of can-
didate answers, i.e., answer path, answer type,
and answer context. We estimate the parame-
ters from question-answer pairs, and use question
paraphrases to train the columns of MCCNNs in
a multi-task learning manner. Experimental re-
sults on WEBQUESTIONS show that our approach
</bodyText>
<page confidence="0.98609">
267
</page>
<bodyText confidence="0.9998813">
achieves better or comparable performance com-
paring with baselines. There are several interest-
ing directions that are worth exploring in the fu-
ture. For instance, we are integrating more exter-
nal knowledge source, such as CLUEWEB (Lin et
al., 2012), to train MCCNNs in a multi-task learn-
ing manner. Furthermore, as our model is capable
of detecting the most important words in a ques-
tion, it would be interesting to use the results to
mine effective question patterns.
</bodyText>
<sectionHeader confidence="0.997388" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9988675">
This research was supported by NSFC (Grant No.
61421003) and the fund of the State Key Lab of
Software Development Environment (Grant No.
SKLSDE-2015ZX-05).
</bodyText>
<sectionHeader confidence="0.998905" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999818215053764">
Junwei Bao, Nan Duan, Ming Zhou, and Tiejun Zhao.
2014. Knowledge-based question answering as ma-
chine translation. In Proceedings of the 52nd An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 967–
976. Association for Computational Linguistics.
Yoshua Bengio. 2012. Practical recommendations
for gradient-based training of deep architectures. In
Neural Networks: Tricks of the Trade, pages 437–
478.
Jonathan Berant and Percy Liang. 2014. Seman-
tic parsing via paraphrasing. In Proceedings of the
52nd Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
1415–1425. Association for Computational Linguis-
tics.
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on freebase from
question-answer pairs. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1533–1544. Association
for Computational Linguistics.
Kurt D. Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring
human knowledge. In International Conference on
Management of Data, pages 1247–1250.
Antoine Bordes, Sumit Chopra, and Jason Weston.
2014a. Question answering with subgraph embed-
dings. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 615–620. Association for Compu-
tational Linguistics.
Antoine Bordes, Jason Weston, and Nicolas Usunier.
2014b. Open question answering with weakly su-
pervised embedding models. In Machine Learning
and Knowledge Discovery in Databases - European
Conference, ECML PKDD 2014, Nancy, France,
September 15-19, 2014. Proceedings, Part I, pages
165–180.
Qingqing Cai and Alexander Yates. 2013. Large-scale
semantic parsing via schema matching and lexicon
extension. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 423–433. Associa-
tion for Computational Linguistics.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. J. Mach. Learn. Res., 12:2493–2537,
November.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. JMLR, 12:2121–2159,
July.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ’11, pages 1535–1545, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Anthony Fader, Luke Zettlemoyer, and Oren Etzioni.
2013. Paraphrase-driven learning for open question
answering. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 1608–1618. Asso-
ciation for Computational Linguistics.
Edward Grefenstette, Phil Blunsom, Nando de Freitas,
and Moritz Karl Hermann, 2014. Proceedings of the
ACL 2014 Workshop on Semantic Parsing, chapter A
Deep Architecture for Semantic Parsing, pages 22–
27. Association for Computational Linguistics.
Mohit Iyyer, Jordan Boyd-Graber, Leonardo Claudino,
Richard Socher, and Hal Daum´e III. 2014. A neural
network for factoid question answering over para-
graphs. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 633–644. Association for Compu-
tational Linguistics.
Jayant Krishnamurthy and Tom M Mitchell. 2012.
Weakly supervised training of semantic parsers. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
754–765. Association for Computational Linguis-
tics.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2010. Inducing proba-
bilistic ccg grammars from logical form with higher-
order unification. In Proceedings of the 2010 con-
ference on empirical methods in natural language
processing, pages 1223–1233. Association for Com-
putational Linguistics.
</reference>
<page confidence="0.968323">
268
</page>
<reference confidence="0.999804651515152">
Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke
Zettlemoyer. 2013. Scaling semantic parsers with
on-the-fly ontology matching. In Proceedings of the
2013 Conference on Empirical Methods in Natural
Language Processing, pages 1545–1556. Associa-
tion for Computational Linguistics.
Xin Li and Dan Roth. 2002. Learning question classi-
fiers. In COLING, pages 1–7.
P. Liang, M. I. Jordan, and D. Klein. 2011. Learn-
ing dependency-based compositional semantics. In
Association for Computational Linguistics (ACL),
pages 590–599.
Thomas Lin, Mausam, and Oren Etzioni. 2012. En-
tity linking at web scale. In Proceedings of the Joint
Workshop on Automatic Knowledge Base Construc-
tion and Web-scale Knowledge Extraction, AKBC-
WEKEX ’12, pages 84–88, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Siva Reddy, Mirella Lapata, and Mark Steedman.
2014. Large-scale semantic parsing without
question-answer pairs. Transactions of the Associa-
tion of Computational Linguistics – Volume 2, Issue
1, pages 377–392.
D.E. Rumelhart, G.E. Hinton, and R.J. Williams. 1986.
Learning representations by back-propagating er-
rors. Nature, 323(6088):533–536.
Nathan Srebro and Adi Shraibman. 2005. Rank, trace-
norm and max-norm. In Proceedings of the 18th
annual conference on Learning Theory, pages 545–
560. Springer-Verlag.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overfitting. Journal of Machine Learning Re-
search, 15:1929–1958.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In ACL.
Xuchen Yao and Benjamin Van Durme. 2014. Infor-
mation extraction over structured data: Question an-
swering with freebase. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers), pages
956–966. Association for Computational Linguis-
tics.
Xuchen Yao, Jonathan Berant, and Benjamin
Van Durme, 2014. Proceedings of the ACL 2014
Workshop on Semantic Parsing, chapter Freebase
QA: Information Extraction or Semantic Parsing?,
pages 82–86. Association for Computational
Linguistics.
Wen-tau Yih, Xiaodong He, and Christopher Meek.
2014. Semantic parsing for single-relation ques-
tion answering. In Proceedings of the 52nd Annual
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers), pages 643–648.
Association for Computational Linguistics.
Lei Yu, Karl Moritz Hermann, Phil Blunsom, and
Stephen Pulman. 2014. Deep Learning for Answer
Sentence Selection. In NIPS Deep Learning Work-
shop, December.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In In Proceedings of the 21st Conference
on Uncertainty in AI, pages 658–666.
</reference>
<page confidence="0.998571">
269
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.613709">
<title confidence="0.970719">Question Answering over Freebase Multi-Column Convolutional Neural Networks</title>
<affiliation confidence="0.700612">Furu Ming Ke Lab, Beihang University, Beijing,</affiliation>
<address confidence="0.968216">Research, Beijing,</address>
<email confidence="0.991886">kexu@nlsde.buaa.edu.cn</email>
<abstract confidence="0.9990806">Answering natural language questions over a knowledge base is an important and challenging task. Most of existing systems typically rely on hand-crafted features and rules to conduct question understanding and/or answer ranking. In this paper, we introduce multi-column convolutional neural networks (MCCNNs) to understand questions from three different aspects (namely, answer path, answer context, and answer type) and learn their distributed representations. Meanwhile, we jointly learn low-dimensional embeddings of entities and relations in the knowledge base. Question-answer pairs are used to train the model to rank candidate answers. We also leverage question paraphrases to train the column networks in a multi-task manner. We use the knowledge base and conduct extenexperiments on the dataset. Experimental results show that our method achieves better or comparable performance compared with baseline systems. In addition, we develop a method to compute the salience scores of question words in different column networks. The results help us intuitively understand what MCCNNs learn.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Junwei Bao</author>
<author>Nan Duan</author>
<author>Ming Zhou</author>
<author>Tiejun Zhao</author>
</authors>
<title>Knowledge-based question answering as machine translation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>967--976</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6409" citStr="Bao et al., 2014" startWordPosition="953" endWordPosition="956">language questions into logical forms and then query knowledge base to lookup answers. The most important step is mapping questions into predefined logical forms, such as combinatory categorial grammar (Cai and Yates, 2013) and dependencybased compositional semantics (Liang et al., 2011). Some semantic parsing based systems required manually annotated logical forms to train the parsers (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These annotations are relatively expensive. So recent works (Liang et al., 2011; Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Bao et al., 2014; Reddy et al., 2014) mainly aimed at using weak supervision (question-answer pairs) to effectively train semantic parsers. These methods achieved comparable results without using logical forms annotated by experts. However, some methods relied on lexical triggers or manually defined features. On the other hand, information retrieval based systems retrieve a set of candidate answers and then conduct further analysis to obtain answers. Their main difference is how to select correct answers from the candidate set. Yao and Van Durme (2014) used rules to extract question features from dependency p</context>
<context position="22385" citStr="Bao et al., 2014" startWordPosition="3605" endWordPosition="3608"> is 5. The dimension of the pooling layers and the dimension of answer embeddings are set to 64. The parameters are initialized by the techniques described in (Bengio, 2012). The max value used for max-norm regularization is 3. The initial learning rate used in AdaGrad is set to 0.01. A mini-batch consists of 10 question-answer pairs, and every question-answer pair has k negative samples that are randomly sampled from its candidate set. The margin values in Equation (4) and Equation (7) is set to m = 0.5 and mp = 0.1. Method F1 P@1 (Berant et al., 2013) 31.4 - (Berant and Liang, 2014) 39.9 - (Bao et al., 2014) 37.5 - (Yao and Van Durme, 2014) 33.0 - (Bordes et al., 2014a) 39.2 40.4 (Bordes et al., 2014b) 29.7 31.3 MCCNN (our) 40.8 45.1 Table 1: Evaluation results on the test split of WEBQUESTIONS. 5.1 Experimental Results The evaluation metrics macro F1 score (Berant et al., 2013) and precision @ 1 (Bordes et al., 2014a) are reported. We use the official evaluation script provided by Berant et al. (2013) to compute the F1 score. Notably, the F1 score defined in (Yao and Van Durme, 2014) is slightly different from others (how to compute scores for the questions without predicted results). We instead</context>
</contexts>
<marker>Bao, Duan, Zhou, Zhao, 2014</marker>
<rawString>Junwei Bao, Nan Duan, Ming Zhou, and Tiejun Zhao. 2014. Knowledge-based question answering as machine translation. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 967– 976. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
</authors>
<title>Practical recommendations for gradient-based training of deep architectures.</title>
<date>2012</date>
<booktitle>In Neural Networks: Tricks of the Trade,</booktitle>
<pages>437--478</pages>
<contexts>
<context position="21941" citStr="Bengio, 2012" startWordPosition="3525" endWordPosition="3526">4.4 is used to update parameters. 5 Experiments In order to evaluate the model, we use the dataset WEBQUESTIONS (Section 3) to conduct experiments. Settings The development set is used to select hyper-parameters in the experiments. The nonlinearity function f = tanh is employed. The dimension of word vectors is set to 25. They are initialized by the pre-trained word embeddings provided in (Turian et al., 2010). The window size of MCCNNs is 5. The dimension of the pooling layers and the dimension of answer embeddings are set to 64. The parameters are initialized by the techniques described in (Bengio, 2012). The max value used for max-norm regularization is 3. The initial learning rate used in AdaGrad is set to 0.01. A mini-batch consists of 10 question-answer pairs, and every question-answer pair has k negative samples that are randomly sampled from its candidate set. The margin values in Equation (4) and Equation (7) is set to m = 0.5 and mp = 0.1. Method F1 P@1 (Berant et al., 2013) 31.4 - (Berant and Liang, 2014) 39.9 - (Bao et al., 2014) 37.5 - (Yao and Van Durme, 2014) 33.0 - (Bordes et al., 2014a) 39.2 40.4 (Bordes et al., 2014b) 29.7 31.3 MCCNN (our) 40.8 45.1 Table 1: Evaluation results</context>
</contexts>
<marker>Bengio, 2012</marker>
<rawString>Yoshua Bengio. 2012. Practical recommendations for gradient-based training of deep architectures. In Neural Networks: Tricks of the Trade, pages 437– 478.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Berant</author>
<author>Percy Liang</author>
</authors>
<title>Semantic parsing via paraphrasing.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>1415--1425</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2057" citStr="Berant and Liang, 2014" startWordPosition="297" endWordPosition="300">utomatic question answering systems return the direct and exact answers to natural language questions. In recent years, the development of largescale knowledge bases, such as FREEBASE (Bollacker et al., 2008), provides a rich resource to answer open-domain questions. However, how ∗Contribution during internship at Microsoft Research. to understand questions and bridge the gap between natural languages and structured semantics of knowledge bases is still very challenging. Up to now, there are two mainstream methods for this task. The first one is based on semantic parsing (Berant et al., 2013; Berant and Liang, 2014) and the other relies on information extraction over the structured knowledge base (Yao and Van Durme, 2014; Bordes et al., 2014a; Bordes et al., 2014b). The semantic parsers learn to understand natural language questions by converting them into logical forms. Then, the parse results are used to generate structured queries to search knowledge bases and obtain the answers. Recent works mainly focus on using question-answer pairs, instead of annotated logical forms of questions, as weak training signals (Liang et al., 2011; Krishnamurthy and Mitchell, 2012) to reduce annotation costs. However, s</context>
<context position="6391" citStr="Berant and Liang, 2014" startWordPosition="949" endWordPosition="952">ers which parse natural language questions into logical forms and then query knowledge base to lookup answers. The most important step is mapping questions into predefined logical forms, such as combinatory categorial grammar (Cai and Yates, 2013) and dependencybased compositional semantics (Liang et al., 2011). Some semantic parsing based systems required manually annotated logical forms to train the parsers (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These annotations are relatively expensive. So recent works (Liang et al., 2011; Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Bao et al., 2014; Reddy et al., 2014) mainly aimed at using weak supervision (question-answer pairs) to effectively train semantic parsers. These methods achieved comparable results without using logical forms annotated by experts. However, some methods relied on lexical triggers or manually defined features. On the other hand, information retrieval based systems retrieve a set of candidate answers and then conduct further analysis to obtain answers. Their main difference is how to select correct answers from the candidate set. Yao and Van Durme (2014) used rules to extract question features</context>
<context position="22359" citStr="Berant and Liang, 2014" startWordPosition="3599" endWordPosition="3602">2010). The window size of MCCNNs is 5. The dimension of the pooling layers and the dimension of answer embeddings are set to 64. The parameters are initialized by the techniques described in (Bengio, 2012). The max value used for max-norm regularization is 3. The initial learning rate used in AdaGrad is set to 0.01. A mini-batch consists of 10 question-answer pairs, and every question-answer pair has k negative samples that are randomly sampled from its candidate set. The margin values in Equation (4) and Equation (7) is set to m = 0.5 and mp = 0.1. Method F1 P@1 (Berant et al., 2013) 31.4 - (Berant and Liang, 2014) 39.9 - (Bao et al., 2014) 37.5 - (Yao and Van Durme, 2014) 33.0 - (Bordes et al., 2014a) 39.2 40.4 (Bordes et al., 2014b) 29.7 31.3 MCCNN (our) 40.8 45.1 Table 1: Evaluation results on the test split of WEBQUESTIONS. 5.1 Experimental Results The evaluation metrics macro F1 score (Berant et al., 2013) and precision @ 1 (Bordes et al., 2014a) are reported. We use the official evaluation script provided by Berant et al. (2013) to compute the F1 score. Notably, the F1 score defined in (Yao and Van Durme, 2014) is slightly different from others (how to compute scores for the questions without pred</context>
</contexts>
<marker>Berant, Liang, 2014</marker>
<rawString>Jonathan Berant and Percy Liang. 2014. Semantic parsing via paraphrasing. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1415–1425. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Berant</author>
<author>Andrew Chou</author>
<author>Roy Frostig</author>
<author>Percy Liang</author>
</authors>
<title>Semantic parsing on freebase from question-answer pairs.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1533--1544</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2032" citStr="Berant et al., 2013" startWordPosition="293" endWordPosition="296">arn. 1 Introduction Automatic question answering systems return the direct and exact answers to natural language questions. In recent years, the development of largescale knowledge bases, such as FREEBASE (Bollacker et al., 2008), provides a rich resource to answer open-domain questions. However, how ∗Contribution during internship at Microsoft Research. to understand questions and bridge the gap between natural languages and structured semantics of knowledge bases is still very challenging. Up to now, there are two mainstream methods for this task. The first one is based on semantic parsing (Berant et al., 2013; Berant and Liang, 2014) and the other relies on information extraction over the structured knowledge base (Yao and Van Durme, 2014; Bordes et al., 2014a; Bordes et al., 2014b). The semantic parsers learn to understand natural language questions by converting them into logical forms. Then, the parse results are used to generate structured queries to search knowledge bases and obtain the answers. Recent works mainly focus on using question-answer pairs, instead of annotated logical forms of questions, as weak training signals (Liang et al., 2011; Krishnamurthy and Mitchell, 2012) to reduce ann</context>
<context position="6367" citStr="Berant et al., 2013" startWordPosition="945" endWordPosition="948">earning semantic parsers which parse natural language questions into logical forms and then query knowledge base to lookup answers. The most important step is mapping questions into predefined logical forms, such as combinatory categorial grammar (Cai and Yates, 2013) and dependencybased compositional semantics (Liang et al., 2011). Some semantic parsing based systems required manually annotated logical forms to train the parsers (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These annotations are relatively expensive. So recent works (Liang et al., 2011; Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Bao et al., 2014; Reddy et al., 2014) mainly aimed at using weak supervision (question-answer pairs) to effectively train semantic parsers. These methods achieved comparable results without using logical forms annotated by experts. However, some methods relied on lexical triggers or manually defined features. On the other hand, information retrieval based systems retrieve a set of candidate answers and then conduct further analysis to obtain answers. Their main difference is how to select correct answers from the candidate set. Yao and Van Durme (2014) used rules to e</context>
<context position="9728" citStr="Berant et al., 2013" startWordPosition="1476" endWordPosition="1479">es) in structured knowledge bases. 3 Setup Given a natural language question q = w1 ... wn, we retrieve related entities and properties from FREEBASE and use them as the candidate answers CQ. Our goal is to score these candidates and predict answers. For instance, the correct output of the question when did Avatar release in UK is 2009-12-17. It should be noted that there may be several correct answers for a question. In order to train the model, we use question-answer pairs without annotated logic forms. We further describe the datasets used in our work as follows: WebQuestions This dataset (Berant et al., 2013) contains 3,778 training instances and 2,032 test instances. We further split the training instances into the training set and the development set by 80%/20%. The questions were collected by querying the Google Suggest API. A breadth-first search beginning with wh- was conducted. Then, answers were annotated in Amazon Mechanical Turk. All the answers can be found in FREEBASE. Freebase It is a large-scale knowledge base that consists of general facts (Bollacker et al., 2008). These facts are organized as subject-propertyobject triples. For example, the fact Avatar is directed by James Cameron i</context>
<context position="22327" citStr="Berant et al., 2013" startWordPosition="3593" endWordPosition="3596"> provided in (Turian et al., 2010). The window size of MCCNNs is 5. The dimension of the pooling layers and the dimension of answer embeddings are set to 64. The parameters are initialized by the techniques described in (Bengio, 2012). The max value used for max-norm regularization is 3. The initial learning rate used in AdaGrad is set to 0.01. A mini-batch consists of 10 question-answer pairs, and every question-answer pair has k negative samples that are randomly sampled from its candidate set. The margin values in Equation (4) and Equation (7) is set to m = 0.5 and mp = 0.1. Method F1 P@1 (Berant et al., 2013) 31.4 - (Berant and Liang, 2014) 39.9 - (Bao et al., 2014) 37.5 - (Yao and Van Durme, 2014) 33.0 - (Bordes et al., 2014a) 39.2 40.4 (Bordes et al., 2014b) 29.7 31.3 MCCNN (our) 40.8 45.1 Table 1: Evaluation results on the test split of WEBQUESTIONS. 5.1 Experimental Results The evaluation metrics macro F1 score (Berant et al., 2013) and precision @ 1 (Bordes et al., 2014a) are reported. We use the official evaluation script provided by Berant et al. (2013) to compute the F1 score. Notably, the F1 score defined in (Yao and Van Durme, 2014) is slightly different from others (how to compute score</context>
</contexts>
<marker>Berant, Chou, Frostig, Liang, 2013</marker>
<rawString>Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on freebase from question-answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1533–1544. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kurt D Bollacker</author>
<author>Colin Evans</author>
<author>Praveen Paritosh</author>
<author>Tim Sturge</author>
<author>Jamie Taylor</author>
</authors>
<title>Freebase: a collaboratively created graph database for structuring human knowledge.</title>
<date>2008</date>
<booktitle>In International Conference on Management of Data,</booktitle>
<pages>1247--1250</pages>
<contexts>
<context position="1642" citStr="Bollacker et al., 2008" startWordPosition="231" endWordPosition="235">manner. We use FREEBASE as the knowledge base and conduct extensive experiments on the WEBQUESTIONS dataset. Experimental results show that our method achieves better or comparable performance compared with baseline systems. In addition, we develop a method to compute the salience scores of question words in different column networks. The results help us intuitively understand what MCCNNs learn. 1 Introduction Automatic question answering systems return the direct and exact answers to natural language questions. In recent years, the development of largescale knowledge bases, such as FREEBASE (Bollacker et al., 2008), provides a rich resource to answer open-domain questions. However, how ∗Contribution during internship at Microsoft Research. to understand questions and bridge the gap between natural languages and structured semantics of knowledge bases is still very challenging. Up to now, there are two mainstream methods for this task. The first one is based on semantic parsing (Berant et al., 2013; Berant and Liang, 2014) and the other relies on information extraction over the structured knowledge base (Yao and Van Durme, 2014; Bordes et al., 2014a; Bordes et al., 2014b). The semantic parsers learn to u</context>
<context position="10206" citStr="Bollacker et al., 2008" startWordPosition="1551" endWordPosition="1554">airs without annotated logic forms. We further describe the datasets used in our work as follows: WebQuestions This dataset (Berant et al., 2013) contains 3,778 training instances and 2,032 test instances. We further split the training instances into the training set and the development set by 80%/20%. The questions were collected by querying the Google Suggest API. A breadth-first search beginning with wh- was conducted. Then, answers were annotated in Amazon Mechanical Turk. All the answers can be found in FREEBASE. Freebase It is a large-scale knowledge base that consists of general facts (Bollacker et al., 2008). These facts are organized as subject-propertyobject triples. For example, the fact Avatar is directed by James Cameron is represented by (/m/0bth54, film.film.directed by, /m/03 gd) in RDF format. The preprocess method presented in (Bordes et al., 2014a) was used to make FREEBASE fit in memory. Specifically, we kept the triples where one of the entities appeared in the training/development set of WEBQUESTIONS or CLUEWEB extractions provided in (Lin et al., 2012), and removed the entities appearing less than five times. Then, we obtained 18M triples that contained 2.9M entities and 7k relatio</context>
<context position="13991" citStr="Bollacker et al., 2008" startWordPosition="2126" endWordPosition="2129">ate_s Answer Type m.09w09jk datetime film.film_regional_release _date.film_release_region United Kingdom m.07ssc datetime film.film_regional_release _date.release_date value_type type.object.type value_type m.0gdp17z film.film_regional_release _date.release_date type.object.type 2009-12-17 2009-12-18 film.film_region al_release_date film.film_region al_release_date Figure 1: Overview for the question-answer pair (when did Avatar release in UK, 2009-12-17). Left: network architecture for question understanding. Right: embedding candidate answers. knowledge base. We use the Freebase Search API (Bollacker et al., 2008) to query named entities in a question. If there is not any named entity, noun phrases are queried. We use the top one entity in the ranked list returned by the API. This entity resolution method was also used in (Yao and Van Durme, 2014). Better methods can be developed, while it is not the focus of this paper. Then, all the 2-hops nodes of the linked entity are regarded as the candidate answers. We denote the candidate set for the question q as Cq. 4.2 MCCNNs for Question Understanding MCCNNs use multiple convolutional neural networks to learn different aspects of questions from shared input</context>
</contexts>
<marker>Bollacker, Evans, Paritosh, Sturge, Taylor, 2008</marker>
<rawString>Kurt D. Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively created graph database for structuring human knowledge. In International Conference on Management of Data, pages 1247–1250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antoine Bordes</author>
<author>Sumit Chopra</author>
<author>Jason Weston</author>
</authors>
<title>Question answering with subgraph embeddings.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>615--620</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2185" citStr="Bordes et al., 2014" startWordPosition="319" endWordPosition="322">ent of largescale knowledge bases, such as FREEBASE (Bollacker et al., 2008), provides a rich resource to answer open-domain questions. However, how ∗Contribution during internship at Microsoft Research. to understand questions and bridge the gap between natural languages and structured semantics of knowledge bases is still very challenging. Up to now, there are two mainstream methods for this task. The first one is based on semantic parsing (Berant et al., 2013; Berant and Liang, 2014) and the other relies on information extraction over the structured knowledge base (Yao and Van Durme, 2014; Bordes et al., 2014a; Bordes et al., 2014b). The semantic parsers learn to understand natural language questions by converting them into logical forms. Then, the parse results are used to generate structured queries to search knowledge bases and obtain the answers. Recent works mainly focus on using question-answer pairs, instead of annotated logical forms of questions, as weak training signals (Liang et al., 2011; Krishnamurthy and Mitchell, 2012) to reduce annotation costs. However, some of them still assume a fixed and pre-defined set of lexical triggers which limit their domains and scalability capability. I</context>
<context position="7427" citStr="Bordes et al., 2014" startWordPosition="1109" endWordPosition="1112">uct further analysis to obtain answers. Their main difference is how to select correct answers from the candidate set. Yao and Van Durme (2014) used rules to extract question features from dependency parse of questions, and used relations and properties in the retrieved topic graph as knowledge base features. Then, the production of these two kinds of features was fed into a logistic regression model to classify the question’s candidate answers into correct/wrong. In contrast, we do not use rules, dependency parse results, or hand-crafted features for question understanding. Some other works (Bordes et al., 2014a; Bordes et al., 2014b) learned low-dimensional vectors for question words and knowledge base constitutes, and used the sum of vectors to represent questions and candidate answers. However, simple vector addition ignores word order information and highorder n-grams. For example, the question representations of who killed A and who A killed are same in the vector addition model. We instead use multi-column convolutional neural networks which are more powerful to process complicated question patterns. Moreover, our multi-column network architecture distinguishes between information of answer ty</context>
<context position="10460" citStr="Bordes et al., 2014" startWordPosition="1589" endWordPosition="1592">ining set and the development set by 80%/20%. The questions were collected by querying the Google Suggest API. A breadth-first search beginning with wh- was conducted. Then, answers were annotated in Amazon Mechanical Turk. All the answers can be found in FREEBASE. Freebase It is a large-scale knowledge base that consists of general facts (Bollacker et al., 2008). These facts are organized as subject-propertyobject triples. For example, the fact Avatar is directed by James Cameron is represented by (/m/0bth54, film.film.directed by, /m/03 gd) in RDF format. The preprocess method presented in (Bordes et al., 2014a) was used to make FREEBASE fit in memory. Specifically, we kept the triples where one of the entities appeared in the training/development set of WEBQUESTIONS or CLUEWEB extractions provided in (Lin et al., 2012), and removed the entities appearing less than five times. Then, we obtained 18M triples that contained 2.9M entities and 7k relation types. As described in (Bordes et al., 2014a), this preprocess method does not ease the task because WEBQUESTIONS only contains about 2k entities. WikiAnswers Fader et al. (2013) extracted the similar questions on WIKIANSWERS and used them as question </context>
<context position="22446" citStr="Bordes et al., 2014" startWordPosition="3618" endWordPosition="3621">n of answer embeddings are set to 64. The parameters are initialized by the techniques described in (Bengio, 2012). The max value used for max-norm regularization is 3. The initial learning rate used in AdaGrad is set to 0.01. A mini-batch consists of 10 question-answer pairs, and every question-answer pair has k negative samples that are randomly sampled from its candidate set. The margin values in Equation (4) and Equation (7) is set to m = 0.5 and mp = 0.1. Method F1 P@1 (Berant et al., 2013) 31.4 - (Berant and Liang, 2014) 39.9 - (Bao et al., 2014) 37.5 - (Yao and Van Durme, 2014) 33.0 - (Bordes et al., 2014a) 39.2 40.4 (Bordes et al., 2014b) 29.7 31.3 MCCNN (our) 40.8 45.1 Table 1: Evaluation results on the test split of WEBQUESTIONS. 5.1 Experimental Results The evaluation metrics macro F1 score (Berant et al., 2013) and precision @ 1 (Bordes et al., 2014a) are reported. We use the official evaluation script provided by Berant et al. (2013) to compute the F1 score. Notably, the F1 score defined in (Yao and Van Durme, 2014) is slightly different from others (how to compute scores for the questions without predicted results). We instead use the original definition in experiments. As shown in Tabl</context>
<context position="23701" citStr="Bordes et al., 2014" startWordPosition="3820" endWordPosition="3823">omparable results than baseline methods on WEBQUESTIONS. To be more specific, the first three rows are semantic parsing based methods, and the other baselines are information extraction based methods. These approaches except (Bordes et al., 2014a; Bordes et al., 2014b) rely on handcrafted features and predefined rules. The results show that automatically question understanding can be as good as the models using manually designed features. Besides, our multi-column convolutional neural networks based model outperforms the methods that use the sum of word embeddings as question representations (Bordes et al., 2014a; Bordes et al., 2014b). 5.2 Model Analysis We also conduct ablation experiments to compare the results using different experiment settings. As shown in Table 2, the abbreviation w/o means removing a particular part from the model. We find that answer path information is most important among these three columns, and answer type information is more important than answer context information. The reason is that answer path and answer type are more direct clues for questions, but answer context is used to handle additional constraints in questions which are less common in the dataset. Moreover, w</context>
</contexts>
<marker>Bordes, Chopra, Weston, 2014</marker>
<rawString>Antoine Bordes, Sumit Chopra, and Jason Weston. 2014a. Question answering with subgraph embeddings. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 615–620. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antoine Bordes</author>
<author>Jason Weston</author>
<author>Nicolas Usunier</author>
</authors>
<title>Open question answering with weakly supervised embedding models.</title>
<date>2014</date>
<booktitle>In Machine Learning and Knowledge Discovery in Databases - European Conference, ECML PKDD 2014,</booktitle>
<pages>165--180</pages>
<location>Nancy, France,</location>
<contexts>
<context position="2185" citStr="Bordes et al., 2014" startWordPosition="319" endWordPosition="322">ent of largescale knowledge bases, such as FREEBASE (Bollacker et al., 2008), provides a rich resource to answer open-domain questions. However, how ∗Contribution during internship at Microsoft Research. to understand questions and bridge the gap between natural languages and structured semantics of knowledge bases is still very challenging. Up to now, there are two mainstream methods for this task. The first one is based on semantic parsing (Berant et al., 2013; Berant and Liang, 2014) and the other relies on information extraction over the structured knowledge base (Yao and Van Durme, 2014; Bordes et al., 2014a; Bordes et al., 2014b). The semantic parsers learn to understand natural language questions by converting them into logical forms. Then, the parse results are used to generate structured queries to search knowledge bases and obtain the answers. Recent works mainly focus on using question-answer pairs, instead of annotated logical forms of questions, as weak training signals (Liang et al., 2011; Krishnamurthy and Mitchell, 2012) to reduce annotation costs. However, some of them still assume a fixed and pre-defined set of lexical triggers which limit their domains and scalability capability. I</context>
<context position="7427" citStr="Bordes et al., 2014" startWordPosition="1109" endWordPosition="1112">uct further analysis to obtain answers. Their main difference is how to select correct answers from the candidate set. Yao and Van Durme (2014) used rules to extract question features from dependency parse of questions, and used relations and properties in the retrieved topic graph as knowledge base features. Then, the production of these two kinds of features was fed into a logistic regression model to classify the question’s candidate answers into correct/wrong. In contrast, we do not use rules, dependency parse results, or hand-crafted features for question understanding. Some other works (Bordes et al., 2014a; Bordes et al., 2014b) learned low-dimensional vectors for question words and knowledge base constitutes, and used the sum of vectors to represent questions and candidate answers. However, simple vector addition ignores word order information and highorder n-grams. For example, the question representations of who killed A and who A killed are same in the vector addition model. We instead use multi-column convolutional neural networks which are more powerful to process complicated question patterns. Moreover, our multi-column network architecture distinguishes between information of answer ty</context>
<context position="10460" citStr="Bordes et al., 2014" startWordPosition="1589" endWordPosition="1592">ining set and the development set by 80%/20%. The questions were collected by querying the Google Suggest API. A breadth-first search beginning with wh- was conducted. Then, answers were annotated in Amazon Mechanical Turk. All the answers can be found in FREEBASE. Freebase It is a large-scale knowledge base that consists of general facts (Bollacker et al., 2008). These facts are organized as subject-propertyobject triples. For example, the fact Avatar is directed by James Cameron is represented by (/m/0bth54, film.film.directed by, /m/03 gd) in RDF format. The preprocess method presented in (Bordes et al., 2014a) was used to make FREEBASE fit in memory. Specifically, we kept the triples where one of the entities appeared in the training/development set of WEBQUESTIONS or CLUEWEB extractions provided in (Lin et al., 2012), and removed the entities appearing less than five times. Then, we obtained 18M triples that contained 2.9M entities and 7k relation types. As described in (Bordes et al., 2014a), this preprocess method does not ease the task because WEBQUESTIONS only contains about 2k entities. WikiAnswers Fader et al. (2013) extracted the similar questions on WIKIANSWERS and used them as question </context>
<context position="22446" citStr="Bordes et al., 2014" startWordPosition="3618" endWordPosition="3621">n of answer embeddings are set to 64. The parameters are initialized by the techniques described in (Bengio, 2012). The max value used for max-norm regularization is 3. The initial learning rate used in AdaGrad is set to 0.01. A mini-batch consists of 10 question-answer pairs, and every question-answer pair has k negative samples that are randomly sampled from its candidate set. The margin values in Equation (4) and Equation (7) is set to m = 0.5 and mp = 0.1. Method F1 P@1 (Berant et al., 2013) 31.4 - (Berant and Liang, 2014) 39.9 - (Bao et al., 2014) 37.5 - (Yao and Van Durme, 2014) 33.0 - (Bordes et al., 2014a) 39.2 40.4 (Bordes et al., 2014b) 29.7 31.3 MCCNN (our) 40.8 45.1 Table 1: Evaluation results on the test split of WEBQUESTIONS. 5.1 Experimental Results The evaluation metrics macro F1 score (Berant et al., 2013) and precision @ 1 (Bordes et al., 2014a) are reported. We use the official evaluation script provided by Berant et al. (2013) to compute the F1 score. Notably, the F1 score defined in (Yao and Van Durme, 2014) is slightly different from others (how to compute scores for the questions without predicted results). We instead use the original definition in experiments. As shown in Tabl</context>
<context position="23701" citStr="Bordes et al., 2014" startWordPosition="3820" endWordPosition="3823">omparable results than baseline methods on WEBQUESTIONS. To be more specific, the first three rows are semantic parsing based methods, and the other baselines are information extraction based methods. These approaches except (Bordes et al., 2014a; Bordes et al., 2014b) rely on handcrafted features and predefined rules. The results show that automatically question understanding can be as good as the models using manually designed features. Besides, our multi-column convolutional neural networks based model outperforms the methods that use the sum of word embeddings as question representations (Bordes et al., 2014a; Bordes et al., 2014b). 5.2 Model Analysis We also conduct ablation experiments to compare the results using different experiment settings. As shown in Table 2, the abbreviation w/o means removing a particular part from the model. We find that answer path information is most important among these three columns, and answer type information is more important than answer context information. The reason is that answer path and answer type are more direct clues for questions, but answer context is used to handle additional constraints in questions which are less common in the dataset. Moreover, w</context>
</contexts>
<marker>Bordes, Weston, Usunier, 2014</marker>
<rawString>Antoine Bordes, Jason Weston, and Nicolas Usunier. 2014b. Open question answering with weakly supervised embedding models. In Machine Learning and Knowledge Discovery in Databases - European Conference, ECML PKDD 2014, Nancy, France, September 15-19, 2014. Proceedings, Part I, pages 165–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qingqing Cai</author>
<author>Alexander Yates</author>
</authors>
<title>Large-scale semantic parsing via schema matching and lexicon extension.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>423--433</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6016" citStr="Cai and Yates, 2013" startWordPosition="894" endWordPosition="897">itive interpretations for MCCNNs by developing a method to detect salient question words in the different column networks. 2 Related Work The state-of-the-art methods for question answering over a knowledge base can be classified into two classes, i.e., semantic parsing based and information retrieval based. Semantic parsing based approaches aim at learning semantic parsers which parse natural language questions into logical forms and then query knowledge base to lookup answers. The most important step is mapping questions into predefined logical forms, such as combinatory categorial grammar (Cai and Yates, 2013) and dependencybased compositional semantics (Liang et al., 2011). Some semantic parsing based systems required manually annotated logical forms to train the parsers (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These annotations are relatively expensive. So recent works (Liang et al., 2011; Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Bao et al., 2014; Reddy et al., 2014) mainly aimed at using weak supervision (question-answer pairs) to effectively train semantic parsers. These methods achieved comparable results without using logical forms annotated by </context>
</contexts>
<marker>Cai, Yates, 2013</marker>
<rawString>Qingqing Cai and Alexander Yates. 2013. Large-scale semantic parsing via schema matching and lexicon extension. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 423–433. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>12--2493</pages>
<contexts>
<context position="14693" citStr="Collobert et al., 2011" startWordPosition="2251" endWordPosition="2255">phrases are queried. We use the top one entity in the ranked list returned by the API. This entity resolution method was also used in (Yao and Van Durme, 2014). Better methods can be developed, while it is not the focus of this paper. Then, all the 2-hops nodes of the linked entity are regarded as the candidate answers. We denote the candidate set for the question q as Cq. 4.2 MCCNNs for Question Understanding MCCNNs use multiple convolutional neural networks to learn different aspects of questions from shared input word embeddings. For every single column, the network structure presented in (Collobert et al., 2011) is used to tackle the variablelength questions. We present the model in the left part of Figure 1. Specifically, for the question q = w1 ... wn, the lookup layer transforms every word into a vector wj = Wvu(wj), where Wv ∈ Rdvx|V |is the word embedding matrix, u(wj) ∈ {0,1}|V |is the one-hot representation of wj, and |V |is the vocabulary size. The word embeddings are parameters, and are updated in the training process. Then, the convolutional layer computes representations of the words in sliding windows. For the i-th column of MCCNNs, the convolutional layer computes n vectors for question </context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. J. Mach. Learn. Res., 12:2493–2537, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2011</date>
<journal>JMLR,</journal>
<pages>12--2121</pages>
<contexts>
<context position="19168" citStr="Duchi et al., 2011" startWordPosition="3043" endWordPosition="3046">d for pairs (q, a) and (q, a0): l (q, a, a0) = (m − 5(q, a) + 5(q, a0))+ (4) where 5(·, ·) is the scoring function defined in Equation (1), m is the margin parameter employed to regularize the gap between two scores, and (z)+ = max{0, z}. The objective function is: l (q, a, a0) (5) where |Aq |is the number of correct answers, and Rq C Cq \ Aq is the set of k wrong answers. The back-propagation algorithm (Rumelhart et al., 1986) is used to train the model. It backpropagates errors from top to the other layers. Derivatives are calculated and gathered to update parameters. The AdaGrad algorithm (Duchi et al., 2011) is then employed to solve this non-convex optimization problem. Moreover, the max-norm regularization (Srebro and Shraibman, 2005; Srivastava et al., 2014) is used for the column vectors of parameter matrices. 4.5 Inference During the test, we retrieve all the candidate answers Cq for the question q. For every candidate ˆa, we compute its score 5(q, ˆa). Then, the candidate answers with the highest scores are regarded as predicted results. Because there may be more than one correct answers for some questions, we need a criterion to determine the score threshold. Specifically, the following eq</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. JMLR, 12:2121–2159, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Fader</author>
<author>Stephen Soderland</author>
<author>Oren Etzioni</author>
</authors>
<title>Identifying relations for open information extraction.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<pages>1535--1545</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="8854" citStr="Fader et al., 2011" startWordPosition="1330" endWordPosition="1333">ing task. Grefenstette et al. (2014) proposed a deep architecture to learn a semantic parser from annotated logic forms of questions. Iyyer et al. (2014) introduced dependency-tree recursive neural networks for the quiz bowl game which asked players to answer an entity for a given paragraph. Yu et 261 al. (2014) proposed a bigram model based on convolutional neural networks to select answer sentences from text data. The model learned a similarity function between questions and answer sentences. Yih et al. (2014) used convolutional neural networks to answer single-relation questions on REVERB (Fader et al., 2011). However, the system worked on relation-entity triples instead of more structured knowledge bases. For instance, the question shown in Figure 1 is answered by using several triples in FREEBASE. Also, we can utilize richer information (such as entity types) in structured knowledge bases. 3 Setup Given a natural language question q = w1 ... wn, we retrieve related entities and properties from FREEBASE and use them as the candidate answers CQ. Our goal is to score these candidates and predict answers. For instance, the correct output of the question when did Avatar release in UK is 2009-12-17. I</context>
</contexts>
<marker>Fader, Soderland, Etzioni, 2011</marker>
<rawString>Anthony Fader, Stephen Soderland, and Oren Etzioni. 2011. Identifying relations for open information extraction. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 1535–1545, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Fader</author>
<author>Luke Zettlemoyer</author>
<author>Oren Etzioni</author>
</authors>
<title>Paraphrase-driven learning for open question answering.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>1608--1618</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="10986" citStr="Fader et al. (2013)" startWordPosition="1675" endWordPosition="1678">directed by, /m/03 gd) in RDF format. The preprocess method presented in (Bordes et al., 2014a) was used to make FREEBASE fit in memory. Specifically, we kept the triples where one of the entities appeared in the training/development set of WEBQUESTIONS or CLUEWEB extractions provided in (Lin et al., 2012), and removed the entities appearing less than five times. Then, we obtained 18M triples that contained 2.9M entities and 7k relation types. As described in (Bordes et al., 2014a), this preprocess method does not ease the task because WEBQUESTIONS only contains about 2k entities. WikiAnswers Fader et al. (2013) extracted the similar questions on WIKIANSWERS and used them as question paraphrases. There are 350,000 paraphrase clusters which contain about two million questions. They are used to generalize for unseen words and question patterns. 4 Methods The overview of our framework is shown in Figure 1. For instance, for the question when did Avatar release in UK, the related nodes of the entity Avatar are queried from FREEBASE. These related nodes are regarded as candidate answers (CQ). Then, for every candidate answer a, the model predicts a score 5 (q, a) to determine whether it is a correct answe</context>
</contexts>
<marker>Fader, Zettlemoyer, Etzioni, 2013</marker>
<rawString>Anthony Fader, Luke Zettlemoyer, and Oren Etzioni. 2013. Paraphrase-driven learning for open question answering. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1608–1618. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Grefenstette</author>
<author>Phil Blunsom</author>
<author>Nando de Freitas</author>
<author>Moritz Karl Hermann</author>
</authors>
<date>2014</date>
<booktitle>Proceedings of the ACL 2014 Workshop on Semantic Parsing, chapter A Deep Architecture for Semantic Parsing,</booktitle>
<pages>22--27</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Grefenstette, Blunsom, de Freitas, Hermann, 2014</marker>
<rawString>Edward Grefenstette, Phil Blunsom, Nando de Freitas, and Moritz Karl Hermann, 2014. Proceedings of the ACL 2014 Workshop on Semantic Parsing, chapter A Deep Architecture for Semantic Parsing, pages 22– 27. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohit Iyyer</author>
<author>Jordan Boyd-Graber</author>
<author>Leonardo Claudino</author>
<author>Richard Socher</author>
<author>Hal Daum´e</author>
</authors>
<title>A neural network for factoid question answering over paragraphs.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>633--644</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Iyyer, Boyd-Graber, Claudino, Socher, Daum´e, 2014</marker>
<rawString>Mohit Iyyer, Jordan Boyd-Graber, Leonardo Claudino, Richard Socher, and Hal Daum´e III. 2014. A neural network for factoid question answering over paragraphs. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 633–644. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jayant Krishnamurthy</author>
<author>Tom M Mitchell</author>
</authors>
<title>Weakly supervised training of semantic parsers.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>754--765</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2618" citStr="Krishnamurthy and Mitchell, 2012" startWordPosition="386" endWordPosition="389"> based on semantic parsing (Berant et al., 2013; Berant and Liang, 2014) and the other relies on information extraction over the structured knowledge base (Yao and Van Durme, 2014; Bordes et al., 2014a; Bordes et al., 2014b). The semantic parsers learn to understand natural language questions by converting them into logical forms. Then, the parse results are used to generate structured queries to search knowledge bases and obtain the answers. Recent works mainly focus on using question-answer pairs, instead of annotated logical forms of questions, as weak training signals (Liang et al., 2011; Krishnamurthy and Mitchell, 2012) to reduce annotation costs. However, some of them still assume a fixed and pre-defined set of lexical triggers which limit their domains and scalability capability. In addition, they need to manually design features for semantic parsers. The second approach uses information extraction techniques for open question answering. These methods retrieve a set of candidate answers from the knowledge base, and the extract features for the question and these candidates to rank them. However, the method proposed by Yao and Van Durme (2014) relies on rules and dependency parse results to extract hand-cra</context>
</contexts>
<marker>Krishnamurthy, Mitchell, 2012</marker>
<rawString>Jayant Krishnamurthy and Tom M Mitchell. 2012. Weakly supervised training of semantic parsers. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 754–765. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Kwiatkowski</author>
<author>Luke Zettlemoyer</author>
<author>Sharon Goldwater</author>
<author>Mark Steedman</author>
</authors>
<title>Inducing probabilistic ccg grammars from logical form with higherorder unification.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 conference on empirical methods in natural language processing,</booktitle>
<pages>1223--1233</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6239" citStr="Kwiatkowski et al., 2010" startWordPosition="925" endWordPosition="928"> classified into two classes, i.e., semantic parsing based and information retrieval based. Semantic parsing based approaches aim at learning semantic parsers which parse natural language questions into logical forms and then query knowledge base to lookup answers. The most important step is mapping questions into predefined logical forms, such as combinatory categorial grammar (Cai and Yates, 2013) and dependencybased compositional semantics (Liang et al., 2011). Some semantic parsing based systems required manually annotated logical forms to train the parsers (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These annotations are relatively expensive. So recent works (Liang et al., 2011; Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Bao et al., 2014; Reddy et al., 2014) mainly aimed at using weak supervision (question-answer pairs) to effectively train semantic parsers. These methods achieved comparable results without using logical forms annotated by experts. However, some methods relied on lexical triggers or manually defined features. On the other hand, information retrieval based systems retrieve a set of candidate answers and then conduct further analysis to obtain </context>
</contexts>
<marker>Kwiatkowski, Zettlemoyer, Goldwater, Steedman, 2010</marker>
<rawString>Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwater, and Mark Steedman. 2010. Inducing probabilistic ccg grammars from logical form with higherorder unification. In Proceedings of the 2010 conference on empirical methods in natural language processing, pages 1223–1233. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Kwiatkowski</author>
<author>Eunsol Choi</author>
<author>Yoav Artzi</author>
<author>Luke Zettlemoyer</author>
</authors>
<title>Scaling semantic parsers with on-the-fly ontology matching.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1545--1556</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6346" citStr="Kwiatkowski et al., 2013" startWordPosition="941" endWordPosition="944"> based approaches aim at learning semantic parsers which parse natural language questions into logical forms and then query knowledge base to lookup answers. The most important step is mapping questions into predefined logical forms, such as combinatory categorial grammar (Cai and Yates, 2013) and dependencybased compositional semantics (Liang et al., 2011). Some semantic parsing based systems required manually annotated logical forms to train the parsers (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These annotations are relatively expensive. So recent works (Liang et al., 2011; Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Bao et al., 2014; Reddy et al., 2014) mainly aimed at using weak supervision (question-answer pairs) to effectively train semantic parsers. These methods achieved comparable results without using logical forms annotated by experts. However, some methods relied on lexical triggers or manually defined features. On the other hand, information retrieval based systems retrieve a set of candidate answers and then conduct further analysis to obtain answers. Their main difference is how to select correct answers from the candidate set. Yao and Van Durme (</context>
</contexts>
<marker>Kwiatkowski, Choi, Artzi, Zettlemoyer, 2013</marker>
<rawString>Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke Zettlemoyer. 2013. Scaling semantic parsers with on-the-fly ontology matching. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1545–1556. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xin Li</author>
<author>Dan Roth</author>
</authors>
<title>Learning question classifiers.</title>
<date>2002</date>
<booktitle>In COLING,</booktitle>
<pages>1--7</pages>
<contexts>
<context position="27419" citStr="Li and Roth, 2002" startWordPosition="4448" endWordPosition="4451">2, we compute salience scores for several questions, and normalize them by the max values in different columns. We clearly see that these words play different roles in a question. The overall conclusion is that the wh- words (such as what, who and where) tend to be important for question understanding. Moreover, nouns dependent of the wh- words and verbs are important clues to obtain question representations. For instance, the figure demonstrates that the nouns type/country/leader and the verbs speak/located are salient in the columns of networks. These observations agree with previous works (Li and Roth, 2002). Some manually defined rules (Yao and Van Durme, 2014) used in the question answering task are also based on them. 5.4 Examples Question representations computed by different columns of MCCNNs are used to query their most similar neighbors. We use cosine similarity in experiments. This experiment demonstrates whether the model learns different aspects of questions. For example, if a column of networks is employed to analyze answer types, the answer types of nearest questions should be same as the query. As shown in Table 3, these three columns of table correspond to different columns of netwo</context>
</contexts>
<marker>Li, Roth, 2002</marker>
<rawString>Xin Li and Dan Roth. 2002. Learning question classifiers. In COLING, pages 1–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>M I Jordan</author>
<author>D Klein</author>
</authors>
<title>Learning dependency-based compositional semantics.</title>
<date>2011</date>
<booktitle>In Association for Computational Linguistics (ACL),</booktitle>
<pages>590--599</pages>
<contexts>
<context position="2583" citStr="Liang et al., 2011" startWordPosition="382" endWordPosition="385">sk. The first one is based on semantic parsing (Berant et al., 2013; Berant and Liang, 2014) and the other relies on information extraction over the structured knowledge base (Yao and Van Durme, 2014; Bordes et al., 2014a; Bordes et al., 2014b). The semantic parsers learn to understand natural language questions by converting them into logical forms. Then, the parse results are used to generate structured queries to search knowledge bases and obtain the answers. Recent works mainly focus on using question-answer pairs, instead of annotated logical forms of questions, as weak training signals (Liang et al., 2011; Krishnamurthy and Mitchell, 2012) to reduce annotation costs. However, some of them still assume a fixed and pre-defined set of lexical triggers which limit their domains and scalability capability. In addition, they need to manually design features for semantic parsers. The second approach uses information extraction techniques for open question answering. These methods retrieve a set of candidate answers from the knowledge base, and the extract features for the question and these candidates to rank them. However, the method proposed by Yao and Van Durme (2014) relies on rules and dependenc</context>
<context position="6081" citStr="Liang et al., 2011" startWordPosition="903" endWordPosition="906">salient question words in the different column networks. 2 Related Work The state-of-the-art methods for question answering over a knowledge base can be classified into two classes, i.e., semantic parsing based and information retrieval based. Semantic parsing based approaches aim at learning semantic parsers which parse natural language questions into logical forms and then query knowledge base to lookup answers. The most important step is mapping questions into predefined logical forms, such as combinatory categorial grammar (Cai and Yates, 2013) and dependencybased compositional semantics (Liang et al., 2011). Some semantic parsing based systems required manually annotated logical forms to train the parsers (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These annotations are relatively expensive. So recent works (Liang et al., 2011; Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Bao et al., 2014; Reddy et al., 2014) mainly aimed at using weak supervision (question-answer pairs) to effectively train semantic parsers. These methods achieved comparable results without using logical forms annotated by experts. However, some methods relied on lexical triggers or manu</context>
</contexts>
<marker>Liang, Jordan, Klein, 2011</marker>
<rawString>P. Liang, M. I. Jordan, and D. Klein. 2011. Learning dependency-based compositional semantics. In Association for Computational Linguistics (ACL), pages 590–599.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Lin</author>
<author>Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>Entity linking at web scale.</title>
<date>2012</date>
<booktitle>In Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction, AKBCWEKEX ’12,</booktitle>
<pages>84--88</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="10674" citStr="Lin et al., 2012" startWordPosition="1624" endWordPosition="1627">l Turk. All the answers can be found in FREEBASE. Freebase It is a large-scale knowledge base that consists of general facts (Bollacker et al., 2008). These facts are organized as subject-propertyobject triples. For example, the fact Avatar is directed by James Cameron is represented by (/m/0bth54, film.film.directed by, /m/03 gd) in RDF format. The preprocess method presented in (Bordes et al., 2014a) was used to make FREEBASE fit in memory. Specifically, we kept the triples where one of the entities appeared in the training/development set of WEBQUESTIONS or CLUEWEB extractions provided in (Lin et al., 2012), and removed the entities appearing less than five times. Then, we obtained 18M triples that contained 2.9M entities and 7k relation types. As described in (Bordes et al., 2014a), this preprocess method does not ease the task because WEBQUESTIONS only contains about 2k entities. WikiAnswers Fader et al. (2013) extracted the similar questions on WIKIANSWERS and used them as question paraphrases. There are 350,000 paraphrase clusters which contain about two million questions. They are used to generalize for unseen words and question patterns. 4 Methods The overview of our framework is shown in </context>
</contexts>
<marker>Lin, Mausam, Etzioni, 2012</marker>
<rawString>Thomas Lin, Mausam, and Oren Etzioni. 2012. Entity linking at web scale. In Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction, AKBCWEKEX ’12, pages 84–88, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siva Reddy</author>
<author>Mirella Lapata</author>
<author>Mark Steedman</author>
</authors>
<title>Large-scale semantic parsing without question-answer pairs.</title>
<date>2014</date>
<journal>Transactions of the Association of Computational Linguistics –</journal>
<volume>2</volume>
<pages>377--392</pages>
<contexts>
<context position="6430" citStr="Reddy et al., 2014" startWordPosition="957" endWordPosition="960"> into logical forms and then query knowledge base to lookup answers. The most important step is mapping questions into predefined logical forms, such as combinatory categorial grammar (Cai and Yates, 2013) and dependencybased compositional semantics (Liang et al., 2011). Some semantic parsing based systems required manually annotated logical forms to train the parsers (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These annotations are relatively expensive. So recent works (Liang et al., 2011; Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Bao et al., 2014; Reddy et al., 2014) mainly aimed at using weak supervision (question-answer pairs) to effectively train semantic parsers. These methods achieved comparable results without using logical forms annotated by experts. However, some methods relied on lexical triggers or manually defined features. On the other hand, information retrieval based systems retrieve a set of candidate answers and then conduct further analysis to obtain answers. Their main difference is how to select correct answers from the candidate set. Yao and Van Durme (2014) used rules to extract question features from dependency parse of questions, an</context>
</contexts>
<marker>Reddy, Lapata, Steedman, 2014</marker>
<rawString>Siva Reddy, Mirella Lapata, and Mark Steedman. 2014. Large-scale semantic parsing without question-answer pairs. Transactions of the Association of Computational Linguistics – Volume 2, Issue 1, pages 377–392.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D E Rumelhart</author>
<author>G E Hinton</author>
<author>R J Williams</author>
</authors>
<title>Learning representations by back-propagating errors.</title>
<date>1986</date>
<journal>Nature,</journal>
<volume>323</volume>
<issue>6088</issue>
<contexts>
<context position="18980" citStr="Rumelhart et al., 1986" startWordPosition="3012" endWordPosition="3015">stion q, we randomly sample k wrong answers a0 from the set of candidate answers Cq, and use them as negative instances to estimate parameters. To be more specific, the hinge loss is considered for pairs (q, a) and (q, a0): l (q, a, a0) = (m − 5(q, a) + 5(q, a0))+ (4) where 5(·, ·) is the scoring function defined in Equation (1), m is the margin parameter employed to regularize the gap between two scores, and (z)+ = max{0, z}. The objective function is: l (q, a, a0) (5) where |Aq |is the number of correct answers, and Rq C Cq \ Aq is the set of k wrong answers. The back-propagation algorithm (Rumelhart et al., 1986) is used to train the model. It backpropagates errors from top to the other layers. Derivatives are calculated and gathered to update parameters. The AdaGrad algorithm (Duchi et al., 2011) is then employed to solve this non-convex optimization problem. Moreover, the max-norm regularization (Srebro and Shraibman, 2005; Srivastava et al., 2014) is used for the column vectors of parameter matrices. 4.5 Inference During the test, we retrieve all the candidate answers Cq for the question q. For every candidate ˆa, we compute its score 5(q, ˆa). Then, the candidate answers with the highest scores ar</context>
</contexts>
<marker>Rumelhart, Hinton, Williams, 1986</marker>
<rawString>D.E. Rumelhart, G.E. Hinton, and R.J. Williams. 1986. Learning representations by back-propagating errors. Nature, 323(6088):533–536.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathan Srebro</author>
<author>Adi Shraibman</author>
</authors>
<title>Rank, tracenorm and max-norm.</title>
<date>2005</date>
<booktitle>In Proceedings of the 18th annual conference on Learning Theory,</booktitle>
<pages>545--560</pages>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="19298" citStr="Srebro and Shraibman, 2005" startWordPosition="3060" endWordPosition="3063">in Equation (1), m is the margin parameter employed to regularize the gap between two scores, and (z)+ = max{0, z}. The objective function is: l (q, a, a0) (5) where |Aq |is the number of correct answers, and Rq C Cq \ Aq is the set of k wrong answers. The back-propagation algorithm (Rumelhart et al., 1986) is used to train the model. It backpropagates errors from top to the other layers. Derivatives are calculated and gathered to update parameters. The AdaGrad algorithm (Duchi et al., 2011) is then employed to solve this non-convex optimization problem. Moreover, the max-norm regularization (Srebro and Shraibman, 2005; Srivastava et al., 2014) is used for the column vectors of parameter matrices. 4.5 Inference During the test, we retrieve all the candidate answers Cq for the question q. For every candidate ˆa, we compute its score 5(q, ˆa). Then, the candidate answers with the highest scores are regarded as predicted results. Because there may be more than one correct answers for some questions, we need a criterion to determine the score threshold. Specifically, the following equation is used to determine outputs: ˆAq = {ˆa |aˆE Cq and max{5(q, a0)} − 5(q, ˆa) &lt; m} (6) a&apos;∈Cq where m is the margin defined i</context>
</contexts>
<marker>Srebro, Shraibman, 2005</marker>
<rawString>Nathan Srebro and Adi Shraibman. 2005. Rank, tracenorm and max-norm. In Proceedings of the 18th annual conference on Learning Theory, pages 545– 560. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitish Srivastava</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.</title>
<date>2014</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>15--1929</pages>
<marker>Srivastava, Hinton, 2014</marker>
<rawString>Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15:1929–1958.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="21741" citStr="Turian et al., 2010" startWordPosition="3489" endWordPosition="3492">p is the margin. The objective function is defined as: lp (q1, q2, q3) (8) where RP contains kp questions which are randomly sampled from other clusters. The same optimization algorithm described in Section 4.4 is used to update parameters. 5 Experiments In order to evaluate the model, we use the dataset WEBQUESTIONS (Section 3) to conduct experiments. Settings The development set is used to select hyper-parameters in the experiments. The nonlinearity function f = tanh is employed. The dimension of word vectors is set to 25. They are initialized by the pre-trained word embeddings provided in (Turian et al., 2010). The window size of MCCNNs is 5. The dimension of the pooling layers and the dimension of answer embeddings are set to 64. The parameters are initialized by the techniques described in (Bengio, 2012). The max value used for max-norm regularization is 3. The initial learning rate used in AdaGrad is set to 0.01. A mini-batch consists of 10 question-answer pairs, and every question-answer pair has k negative samples that are randomly sampled from its candidate set. The margin values in Equation (4) and Equation (7) is set to m = 0.5 and mp = 0.1. Method F1 P@1 (Berant et al., 2013) 31.4 - (Beran</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuchen Yao</author>
<author>Benjamin Van Durme</author>
</authors>
<title>Information extraction over structured data: Question answering with freebase.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>956--966</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Yao, Van Durme, 2014</marker>
<rawString>Xuchen Yao and Benjamin Van Durme. 2014. Information extraction over structured data: Question answering with freebase. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 956–966. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuchen Yao</author>
<author>Jonathan Berant</author>
<author>Benjamin Van Durme</author>
</authors>
<date>2014</date>
<booktitle>Proceedings of the ACL 2014 Workshop on Semantic Parsing, chapter Freebase QA: Information Extraction or Semantic Parsing?,</booktitle>
<pages>82--86</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Yao, Berant, Van Durme, 2014</marker>
<rawString>Xuchen Yao, Jonathan Berant, and Benjamin Van Durme, 2014. Proceedings of the ACL 2014 Workshop on Semantic Parsing, chapter Freebase QA: Information Extraction or Semantic Parsing?, pages 82–86. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen-tau Yih</author>
<author>Xiaodong He</author>
<author>Christopher Meek</author>
</authors>
<title>Semantic parsing for single-relation question answering.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>643--648</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8752" citStr="Yih et al. (2014)" startWordPosition="1315" endWordPosition="1318"> together. Another line of related work is applying deep learning techniques for the question answering task. Grefenstette et al. (2014) proposed a deep architecture to learn a semantic parser from annotated logic forms of questions. Iyyer et al. (2014) introduced dependency-tree recursive neural networks for the quiz bowl game which asked players to answer an entity for a given paragraph. Yu et 261 al. (2014) proposed a bigram model based on convolutional neural networks to select answer sentences from text data. The model learned a similarity function between questions and answer sentences. Yih et al. (2014) used convolutional neural networks to answer single-relation questions on REVERB (Fader et al., 2011). However, the system worked on relation-entity triples instead of more structured knowledge bases. For instance, the question shown in Figure 1 is answered by using several triples in FREEBASE. Also, we can utilize richer information (such as entity types) in structured knowledge bases. 3 Setup Given a natural language question q = w1 ... wn, we retrieve related entities and properties from FREEBASE and use them as the candidate answers CQ. Our goal is to score these candidates and predict an</context>
</contexts>
<marker>Yih, He, Meek, 2014</marker>
<rawString>Wen-tau Yih, Xiaodong He, and Christopher Meek. 2014. Semantic parsing for single-relation question answering. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 643–648. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lei Yu</author>
<author>Karl Moritz Hermann</author>
<author>Phil Blunsom</author>
<author>Stephen Pulman</author>
</authors>
<title>Deep Learning for Answer Sentence Selection.</title>
<date>2014</date>
<booktitle>In NIPS Deep Learning Workshop,</booktitle>
<marker>Yu, Hermann, Blunsom, Pulman, 2014</marker>
<rawString>Lei Yu, Karl Moritz Hermann, Phil Blunsom, and Stephen Pulman. 2014. Deep Learning for Answer Sentence Selection. In NIPS Deep Learning Workshop, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke S Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In</title>
<date>2005</date>
<booktitle>In Proceedings of the 21st Conference on Uncertainty in AI,</booktitle>
<pages>658--666</pages>
<contexts>
<context position="6212" citStr="Zettlemoyer and Collins, 2005" startWordPosition="921" endWordPosition="924">ng over a knowledge base can be classified into two classes, i.e., semantic parsing based and information retrieval based. Semantic parsing based approaches aim at learning semantic parsers which parse natural language questions into logical forms and then query knowledge base to lookup answers. The most important step is mapping questions into predefined logical forms, such as combinatory categorial grammar (Cai and Yates, 2013) and dependencybased compositional semantics (Liang et al., 2011). Some semantic parsing based systems required manually annotated logical forms to train the parsers (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These annotations are relatively expensive. So recent works (Liang et al., 2011; Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Bao et al., 2014; Reddy et al., 2014) mainly aimed at using weak supervision (question-answer pairs) to effectively train semantic parsers. These methods achieved comparable results without using logical forms annotated by experts. However, some methods relied on lexical triggers or manually defined features. On the other hand, information retrieval based systems retrieve a set of candidate answers and then conduct </context>
</contexts>
<marker>Zettlemoyer, Collins, 2005</marker>
<rawString>Luke S. Zettlemoyer and Michael Collins. 2005. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In In Proceedings of the 21st Conference on Uncertainty in AI, pages 658–666.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>