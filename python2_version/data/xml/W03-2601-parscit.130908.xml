<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000150">
<title confidence="0.641275">
Intermediate Parsing for Anaphora Resolution?
Implementing the Lappin and Leass non-coreference filters
</title>
<author confidence="0.402335">
Judita Preiss and Ted Briscoe*
</author>
<affiliation confidence="0.910197">
Computer Laboratory, University of Cambridge
</affiliation>
<email confidence="0.93861">
Judita.Preiss@cl.cam.ac.uk, Ted.Briscoacl.cam.ac.uk
</email>
<sectionHeader confidence="0.994558" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99810525">
We show that the
Lappin and Leass (1994) anaphora
resolution algorithm, originally
implemented to use McCord&apos;s Slot
Grammar, can be equivalently
expressed in terms of grammatical
relations (GRs) (Carroll et al.,
1998). We use this GR version
of Lappin and Leass&apos; algorithm to
investigate the effect on performance
when the parser is changed. We
find that the performance of the
anaphora resolution algorithm does
not appear to be greatly affected.
The results from the GR version
of the Lappin and Leass anaphora
resolution algorithm are also briefly
compared to the results of the
Kennedy and Boguraev (1996)
anaphora resolution algorithm.
</bodyText>
<sectionHeader confidence="0.998499" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.98712866">
We question the need to use a full parser,
such as McCord&apos;s Slot Grammar, in the
Lappin and Leass (1994) anaphora resolution
algorithm. The goal of our work is to show
that this algorithm can be equivalently im-
plemented in terms of Carroll et al.&apos;s (1998)
grammatical relations (GRs). By a full parser
we understand a system which uses subcatego-
rization information, and it is not necessary to
construct such a full parse for a system which
This work was supported by UK EPSRC project
GR/N36462/93 &apos;Robust Accurate Statistical Parsing
(RASP)&apos;.
outputs GRs. The new implementation en-
ables us to investigate performance variation
in the Lappin and Leass algorithm when the
parser is changed for any other GR producing
parser. This work is motivated by our previ-
ous work (Preiss, 2002), which showed that the
performance of the anaphora resolution algo-
rithm did not change greatly if the full parser
it uses was varied, so long as the grammati-
cal roles could be extracted comparably accu-
rately from the parsers.
As noted by Kennedy and Boguraev (1996)
and still true today, it is a fact that state-of-
the-art full parsers are not yet robust enough
for an application like anaphora resolution.
As we have observed in our own work, if an
anaphora resolution experiment is restricted
to blocks of sentences for which a full parse
can be successfully generated, the anaphora
resolution results turn out &apos;too good&apos;. The
unrealistic bias reflects the grammatical sim-
plicity. On the other hand, it is hard to de-
sign anaphora resolution experiments if the
parser fails on some sentences, since the cor-
rect antecedent might be hidden in a sentence
without a parse. Since parsers which return
labelled headword dependent-word links (in-
termediate parsers) are not obliged to return
a full parse, they generally do not fail com-
pletely on sentences, but instead return frag-
ments of a parse. This robustness makes it
attractive to investigate whether intermediate
parsers can be applied in an anaphora resolu-
tion algorithm.
Kennedy and Boguraev managed to elimi-
nate the need for a full parse, however their al-
gorithm was a substantial modification of the
</bodyText>
<page confidence="0.966251">
1
</page>
<bodyText confidence="0.999890461538462">
Lappin and Leass algorithm. It is not clear
whether this was necessary. It was pointed
out by Barbu and Mitkov (2001) in their com-
parison of anaphora resolution systems, that
performance can only be compared if the sys-
tems are evaluated on the same corpus and
share as many tools as possible. The GR ver-
sion allows us to use a single implementation
of the Lappin and Leass algorithm, and just
to change the set of GRs associated with the
evaluation corpus. This allows us to directly
compare the performance difference and an-
alyze errors. We also present the results of
the Kennedy and Boguraev anaphora resolu-
tion algorithm on our corpus.
In Section 2 we describe the Lappin and Le-
ass anaphora resolution algorithm and our im-
plementation which only uses GRs. A com-
parison of the two intermediate parsers that
we employ can be found in Section 3. The
results of the anaphora resolution algorithm
using the two parsers are presented in Sec-
tion 4. This section also contains the results
for the Kennedy and Boguraev algorithm, and
an analysis of the results. In Section 5 we draw
our conclusions.
</bodyText>
<sectionHeader confidence="0.802079" genericHeader="introduction">
2 Lappin and Leass Algorithm
</sectionHeader>
<subsectionHeader confidence="0.963703">
2.1 Algorithm Description
</subsectionHeader>
<bodyText confidence="0.999623166666667">
For this experiment we choose to re-
implement a non-probabilistic algorithm
due to Lappin and Leass (1994), since this
anaphora resolution algorithm mainly makes
use of grammatical role information, and
therefore only minor modification would be
required for an intermediate parser to be used
instead of the full parser they assume.
For each pronoun, this algorithm uses syn-
tactic criteria to rule out noun phrases that
cannot possibly corefer with it. An antecedent
is then chosen according to a ranking based on
salience weights.
For all types of pronoun, noun phrases are
ruled out if they have incompatible agreement
features. Pronouns are split into two classes:
lexical (reflexives and reciprocals) and non-
lexical anaphors. There are syntactic filters
</bodyText>
<table confidence="0.999137">
Factor Weight
Sentence recency 100
Subject emphasis 80
Existential emphasis 70
Accusative emphasis 50
Indirect object/oblique 40
Head noun emphasis 80
Non-adverbial emphasis 50
</table>
<tableCaption confidence="0.999801">
Table 1: Salience weights
</tableCaption>
<bodyText confidence="0.998183142857143">
for the two types of anaphors.
Candidates which remain after filtering are
ranked according to their salience. A salience
value is computed to be a weighted sum of
the relevant feature weights (summarized in
Table 1). If we consider the sentence John
walks, the salience of John will be:
</bodyText>
<equation confidence="0.754128">
sal(John) = Wsent Wsubj Whead Wnon-adv
= 100 + 80 + 80 + 50
= 310
</equation>
<bodyText confidence="0.9923424">
The weights are scaled by a factor of () 8
where s is the distance (number of sentences)
of the candidate from the pronoun.
The candidate with the highest salience is
proposed as the antecedent.
</bodyText>
<subsectionHeader confidence="0.999604">
2.2 Using Grammatical Relations
</subsectionHeader>
<bodyText confidence="0.999880166666667">
The Lappin and Leass anaphora resolution al-
gorithm exploits two types of information: in-
formation which is directly obtained from GRs
(for example, being a subject or direct object),
and information for which extraction is not so
simple (for example, whether the pronoun is
in the argument or the adjunct domain of a
noun phrase). The original implementation
of Lappin and Leass&apos; algorithm relied mainly
on a clausal (head—argument) representation
of McCord&apos;s Slot Grammar (Lappin and Mc-
Cord, 1990). Kennedy and Boguraev (1996)
modified the Lappin and Leass algorithm to
use flat morpho—syntactic information (a shal-
low parser), this being more flexible for new
domains.
Our implementation of Lappin and Leass
uses (only) GRs conforming to the specifica-
</bodyText>
<page confidence="0.984225">
2
</page>
<figure confidence="0.502609333333333">
Sentence: Mary gave her a book.
Grammatical Relations:
(ncsubj gave Mary _)
(dobj gave her _)
(obj2 gave book)
(detmod _ book a)
</figure>
<figureCaption confidence="0.9370485">
Figure 1: Briscoe and Carroll output for Mary
gave her a book
</figureCaption>
<bodyText confidence="0.998039115384615">
tion of Carroll et al. (1998). An example for
the sentence Mary gave her a book can be seen
in Figure 1. The underscores in the GRs above
indicate the GR&apos;s type. For example, det-
mod has type poss for pronominal determin-
ers, e.g. (detmod poss cat his). Because it
is possible to recover constituency from a set of
GRs (Hays, 1964), (Gaifman, 1965), all infor-
mation necessary for the anaphora resolution
algorithm is present in the GRs.
Information concerning the grammatical
role of each noun phrase in the sentence can
easily be recovered from the GRs. However,
it is not immediately clear that Mary and her
cannot corefer. This is decided by one of a
number of syntactic constraints in the Lappin
and Leass algorithm, which rule out certain in-
trasentential noun phrases from becoming can-
didate antecedents.
As the success of the algorithm is mainly
dependent on the early filtering of candidates,
a substantial part of our work focused on ob-
taining this information from the GRs. For ex-
ample, the argument domain filter which rules
out coreference in the above example can be
encoded by:
</bodyText>
<equation confidence="0.925622">
(arg - X N -)
(arg - X P -)
</equation>
<bodyText confidence="0.999672333333333">
where arg E {ncsubj, dobj,iobj, obj2, xcompl }
In this case, - means that the value of the
type is unimportant.
An adjunct domain filter, ruling out coref-
erence in She sat near her, can be expressed
as:
</bodyText>
<equation confidence="0.966364">
xcomp is only relevant when H is a form of be.
(arg - X N -)
(ncmod Prep X P)
</equation>
<bodyText confidence="0.992309">
where arg E {ncsubj, dobj, iobj , obj2, xcomp2}.
In this way we have encoded all the Lappin
and Leass syntactic constraints. They were
found to depend on the object relations (nc-
subj, dobj, obj2, iobj), the complement rela-
tions (xcomp, ccomp, and clausal), and the
non-clausal modifier ncmod relation. We also
use the GRs to extract the salience factor in-
formation.
</bodyText>
<sectionHeader confidence="0.993957" genericHeader="method">
3 Intermediate Parsing
</sectionHeader>
<subsectionHeader confidence="0.939104">
3.1 Grammatical Relations
</subsectionHeader>
<bodyText confidence="0.9999631">
We use the GRs obtained from the
Briscoe and Carroll (2002) parser and the
Buchholz (2002) GR finder for the parser com-
ponents of our implementation of the Lappin
and Leass anaphora resolution algorithm.
Both these parsers are capable of producing
GRs according to the Carroll et al. (1998)
specification. We will now describe the two
parsers, and present their performance on this
corpus.
</bodyText>
<subsectionHeader confidence="0.999691">
3.2 Briscoe and Carroll (2002)3
</subsectionHeader>
<bodyText confidence="0.997025">
Grammar unification-based (manually cre-
ated) grammar of part of speech and
punctuation labels.
</bodyText>
<table confidence="0.6525742">
Parsing algorithm: LR parser.
Tagger: Acquilex HMM tagger (using the
CLAWS-II labels) (Elworthy, 1994).
Training corpus: Susanne corpus is used
for development (Sampson, 1995).
</table>
<subsectionHeader confidence="0.727389">
3.3 Buchholz (2002)
</subsectionHeader>
<bodyText confidence="0.7101115">
Tagger: Memory-based tagger due to
Daelemans et al. (1996).
</bodyText>
<listItem confidence="0.4339756">
Chunker: Memory-based chunker due to
Veenstra and van den Bosch (2000).
Shallow Parser: Memory-based shallow
parser (Daelemans, 1996), (Buchholz et
al., 1999).
</listItem>
<figure confidence="0.2768095">
&apos;Similarly, xcomp is only relevant when H is a form
of be.
3 Available from http: //www . cogs . susx.ac.uk/
lab/nip/rasp/
</figure>
<page confidence="0.958734">
3
</page>
<table confidence="0.999149454545455">
GR # occ BC BU
Precision Recall Precision Recall
ccomp 81 20.45 11.11 65.00 64.20
detmod 1124 91.15 89.77 92.41 90.93
dobj 409 85.83 78.48 88.42 76.53
iobj 158 31.93 67.09 57.75 51.90
ncmod 2403 69.45 57.72 66.86 51.64
ncsubj 1038 81.99 82.47 85.83 72.93
obj2 19 27.45 73.68 46.15 31.58
xcomp 322 73.99 62.73 78.00 72.67
I-tw - 75.73 70.29 77.45 66.74
</table>
<tableCaption confidence="0.990156">
Table 2: GR Precisions and Recalls
</tableCaption>
<bodyText confidence="0.574803">
Training Corpus: Sections 10-19 of the
Wall Street Journal of Penn Treebank II
(Marcus et al., 1993).
</bodyText>
<subsectionHeader confidence="0.859546">
3.4 Relative Performance
</subsectionHeader>
<bodyText confidence="0.99988615">
As part of the development of their parser,
Carroll et al. manually annotated 500 sen-
tences with their GRs.4 The sentences were se-
lected at random from the Susanne corpus sub-
ject to the constraint that they are within the
coverage of the Briscoe and Carroll parser.5
The performance results of the Briscoe and
Carroll parser (BC) and the Buchholz GR
finder (BU) for the relevant GRs can be found
in Table 2. In this table we also present the
(weighted) mean performance itw.
We split the 500 sentence corpus into ten
50 sentence segments and computed preci-
sion/recall on each segment. The resulting
Fo=1 measures were compared using the one-
tailed t-test. The GRs produced by the Buch-
holz GR finder were found to be significantly
more accurate than those produced by the
Briscoe and Carroll parser (with a confidence
of 99.5%).
</bodyText>
<sectionHeader confidence="0.99985" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999746">
For this experiment, we use a modified version
of an anaphorically resolved 2400 sentence ini-
tial segment of the BNC (Leech, 1992), (Preiss,
</bodyText>
<footnote confidence="0.97811025">
4Available from http: //wee. cogs .susx. ac .uk/
lab/nlp/carroll/greval .html
5The Briscoe and Carroll parser has about a 74%
coverage of the Susanne corpus.
</footnote>
<table confidence="0.9924935">
Text # sents # prons
1 754 135
2 785 118
3 318 154
4 268 135
5 271 137
</table>
<tableCaption confidence="0.999349">
Table 3: Corpus Information
</tableCaption>
<bodyText confidence="0.999848476190476">
2002). The modification consisted of removing
certain tokenizations from the BNC: for exam-
ple, out&lt; blank&gt; of becomes out of. To avoid
any unwanted tokenization, punctuation was
removed from inside of words (e.g. Biny a &apos;a has
become Binyaa). We split the 2400 sentence
corpus into five texts which contain roughly
the same number of pronouns.6 The number
of sentences and pronouns in each of the five
texts is presented in Table 3. Pleonastic pro-
nouns were manually labelled as such and were
thus removed from consideration by the algo-
rithm (as both anaphors and candidates).
In Table 4, we present the results (precision
only as all pronouns are always attempted) ob-
tained from our implementation of the Lap-
pin and Leass algorithm using the Briscoe and
Carroll parser, and using Buchholz&apos; GR finder.
The mean it of the two &apos;algorithms&apos; is identi-
cal and a one-tailed t-test does not find any
significant difference between the two.
</bodyText>
<footnote confidence="0.746346">
6The texts were created to contain as equal a num-
ber of pronouns as possible while not splitting the cor-
pus across any segment boundaries.
</footnote>
<page confidence="0.983939">
4
</page>
<table confidence="0.999178625">
Text # prons Lappin and Leass Kennedy and Boguraev
BC BU BC BU
1 135 60% 63% 67% 69%
2 118 51% 53% 59% 56%
3 154 70% 70% 73% 71%
4 135 67% 65% 64% 59%
5 137 55% 53% 64% 63%
pc 136 61% 61% 65% 64%
</table>
<tableCaption confidence="0.999507">
Table 4: Performance Results
</tableCaption>
<bodyText confidence="0.9999007">
In this table, we also present the results for
our implementation of the Kennedy and Bogu-
raev algorithm. Although the coreference fil-
ters in this case only rely on a shallow parser
and thus are not as accurate, the algorithm
yields better performance than the Lappin and
Leass algorithm. The increase in performance
is probably due to the introduction of extra
salience factors, and warrants futher investi-
gation.
</bodyText>
<subsectionHeader confidence="0.999956">
4.1 Analysis of Results
</subsectionHeader>
<bodyText confidence="0.999938181818182">
There is a total of 679 pronouns to be re-
solved by the Lappin and Leass algorithm, out
of which 218 are misclassified by both ver-
sions of the algorithm. There are only 95 pro-
nouns which are misclassified by just one of
the two versions. In about 40% of the 218
cases, both versions choose the same wrong an-
tecedent which suggests inherent deficiencies
of the anaphora resolution algorithm. By in-
spection, the difference in the remaining 60%
of cases was usually due to:
</bodyText>
<listItem confidence="0.918781571428571">
• (At least) one of the sets of GRs being
incorrect.
• In the case where the GRs were correct,
it was sometimes not possible to choose
the correct antecedent according to the
anaphora resolution algorithm, due to an
inherent deficiency in it.
</listItem>
<bodyText confidence="0.999922818181818">
Interestingly, it is possible for both ver-
sions of the algorithm to choose the same an-
tecedent, but for one version this antecedent is
correct whereas this is not true for the other.
This may occur when the chosen antecedent
is a pronoun. For example, in the segment
Mary... Shei ... She2..., She2 will resolve to
Shei, but She2 may still be scored wrong
if Shei has not been previously correctly re-
solved. This evaluation method may lead to
chaining errors and may cause the precision of
the system to appear lower.
Our results from the Lappin and Leass
anaphora resolution algorithm are lower than
those originally obtained by Lappin and Le-
ass (85%). However, the original evaluation
was carried out on technical manuals and so
it is not clear that the corpora are comparable
in difficulty. It is also possible that the orig-
inal salience weights are more suited for the
domain of manuals and the optimal setting of
weights for our BNC segment would be worth
investigating.
The Lappin and Leass algorithm did not
perform significantly differently with the two
sets of GRs, although Buchholz&apos; GR finder
outperformed Briscoe and Carroll&apos;s parser in
the GR evaluation. However, as neither of the
parsers was trained on the BNC, it is not clear
whether a comparison of the anaphora reso-
lution algorithm using the two parsers with
the parsers&apos; grammatical relation performance
(evaluated on Susanne) is meaningful.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999895">
Primarily, we have implemented a version of
the Lappin and Leass anaphora resolution al-
gorithm which only relies on grammatical re-
lations, without losing information encoded in
a full parse. The extraction of GRs is more ro-
bust, thus our implementation makes the Lap-
</bodyText>
<page confidence="0.969319">
5
</page>
<bodyText confidence="0.9991636875">
pin and Leass algorithm more robust without
a decrease in performance
The performance of the anaphora resolution
algorithm did not appear to decrease when
different intermediate parsers were employed.
The lack of difference in performance supports
the results of our earlier work, where we em-
ployed various full parsers in the anaphora res-
olution algorithm (relying on the full parser
structure), with little effect on accuracy.
Having shown that using different state-
of-the-art parsers has little effect on perfor-
mance, we can thus conclude that future ef-
fort in anaphora resolution should be directed
towards improving the basic resolution algo-
rithm.
</bodyText>
<sectionHeader confidence="0.997986" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999925">
We would like to thank Sabine Buchholz for
the output of her GR finder. Our thanks also
go to Joe Hurd for reading previous drafts of
this paper.
</bodyText>
<sectionHeader confidence="0.99807" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996725125">
C. Barbu and R. Mitkov. 2001. Evaluation tool
for rule-based anaphora resolution methods. In
Proceedings of the 39th ACL/10th EA CL, pages
34-41.
E. J. Briscoe and J. Carroll. 2002. Robust ac-
curate statistical annotation of general text. In
Proceedings of the 3rd International Conference
on Language Resources and Evaluation, pages
1499-1504.
S. Buchholz, J. Veenstra, and W. Daelemans.
1999. Cascaded grammatical relation assign-
ment. In P. Fung and J. Zhou, editors, Proceed-
ings of the Joint SIGDAT Conference on Em-
pirical Methods in Natural Language Processing
and Very Large Corpora (EMNLP/VLC), pages
239-246.
S. Buchholz. 2002. Memory-Based Grammatical
Relation Finding. Ph.D. thesis, University of
Tilburg.
J. Carroll, E. Briscoe, and A. Sanfilippo. 1998.
Parser evaluation: A survey and a new proposal.
In Proceedings of the International Conference
on Language Resources and Evaluation, pages
447-454.
W. Daelemans, J. Zavrel, P. Berck, and S. Gillis.
1996. MBT: A memory-based part of speech
tagger generator. In E. Ejerhed and I. Dagan,
editors, Proceedings of the 4th ACL/SIGDAT
Workshop on Very Large Corpora, pages 14-27.
W. Daelemans. 1996. Abstraction considered
harmful: Lazy learning of language processing.
In H. J. van den Herik and A. Weijiters, editors,
Proceedings of the Sixth Beligian-Dutch Confer-
ence on Machine Learning, pages 3-12.
D. Elworthy. 1994. Does Baum-Welch re-
estimation help taggers? In Proceedings of the
4th Conference on Applied NLP, pages 53-58.
H. Gaifman. 1965. Dependency systems and
phrase-structure systems. Information and
Control, 8(3):304-337.
D. G. Hays. 1964. Dependency theory: A formal-
ism and some observations. Language, 40:511-
525.
C. Kennedy and B. Boguraev. 1996. Anaphora for
everyone: Pronominal anaphora resolution with-
out a parser. In Proceedings of the 16th Interna-
tional Conference on Computational Linguistics
(COLING&apos;96), pages 113-118.
S. Lappin and H. Leass. 1994. An algorithm
for pronominal anaphora resolution. Computa-
tional Linguistics, 20(4):535-561.
S. Lappin and M. McCord. 1990. A syntactic filter
on pronominal anaphora for slot grammar. In
Proceedings of ACL, pages 135-142.
G. Leech. 1992. 100 million words of English: the
British National Corpus. Language Research,
28(1):1-13.
M. Marcus, R. Santorini, and M. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn TreeBank. Computational Lin-
guisitcs, 19(2):313-330.
J. Preiss. 2002. Choosing a parser for anaphora
resolution. In Proceedings of DAARC, pages
175-180.
G. Sampson. 1995. English for the computer. Ox-
ford University Press.
J. Veenstra and A. van den Bosch. 2000. Single-
classifier memory-based phrase chunking. In
Proceedings of the Fourth Conference on Com-
putational Natural Language Learning (CoNLL)
and the Second Learning Language in Logic
Workshop (LLL), pages 157-159.
</reference>
<page confidence="0.998756">
6
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.683213">
<title confidence="0.9984725">Intermediate Parsing for Anaphora Resolution? Implementing the Lappin and Leass non-coreference filters</title>
<author confidence="0.974062">Judita Preiss</author>
<author confidence="0.974062">Ted</author>
<affiliation confidence="0.97597">Computer Laboratory, University of Cambridge</affiliation>
<email confidence="0.827174">Judita.Preiss@cl.cam.ac.uk,Ted.Briscoacl.cam.ac.uk</email>
<abstract confidence="0.984251714285714">We show that the Lappin and Leass (1994) anaphora resolution algorithm, originally implemented to use McCord&apos;s Slot Grammar, can be equivalently expressed in terms of grammatical relations (GRs) (Carroll et al., 1998). We use this GR version of Lappin and Leass&apos; algorithm to investigate the effect on performance when the parser is changed. We find that the performance of the anaphora resolution algorithm does not appear to be greatly affected. The results from the GR version of the Lappin and Leass anaphora resolution algorithm are also briefly compared to the results of the Kennedy and Boguraev (1996) anaphora resolution algorithm.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Barbu</author>
<author>R Mitkov</author>
</authors>
<title>Evaluation tool for rule-based anaphora resolution methods.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th ACL/10th EA CL,</booktitle>
<pages>34--41</pages>
<contexts>
<context position="3153" citStr="Barbu and Mitkov (2001)" startWordPosition="502" endWordPosition="505">e without a parse. Since parsers which return labelled headword dependent-word links (intermediate parsers) are not obliged to return a full parse, they generally do not fail completely on sentences, but instead return fragments of a parse. This robustness makes it attractive to investigate whether intermediate parsers can be applied in an anaphora resolution algorithm. Kennedy and Boguraev managed to eliminate the need for a full parse, however their algorithm was a substantial modification of the 1 Lappin and Leass algorithm. It is not clear whether this was necessary. It was pointed out by Barbu and Mitkov (2001) in their comparison of anaphora resolution systems, that performance can only be compared if the systems are evaluated on the same corpus and share as many tools as possible. The GR version allows us to use a single implementation of the Lappin and Leass algorithm, and just to change the set of GRs associated with the evaluation corpus. This allows us to directly compare the performance difference and analyze errors. We also present the results of the Kennedy and Boguraev anaphora resolution algorithm on our corpus. In Section 2 we describe the Lappin and Leass anaphora resolution algorithm a</context>
</contexts>
<marker>Barbu, Mitkov, 2001</marker>
<rawString>C. Barbu and R. Mitkov. 2001. Evaluation tool for rule-based anaphora resolution methods. In Proceedings of the 39th ACL/10th EA CL, pages 34-41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E J Briscoe</author>
<author>J Carroll</author>
</authors>
<title>Robust accurate statistical annotation of general text.</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd International Conference on Language Resources and Evaluation,</booktitle>
<pages>1499--1504</pages>
<contexts>
<context position="8545" citStr="Briscoe and Carroll (2002)" startWordPosition="1420" endWordPosition="1423"> filter, ruling out coreference in She sat near her, can be expressed as: xcomp is only relevant when H is a form of be. (arg - X N -) (ncmod Prep X P) where arg E {ncsubj, dobj, iobj , obj2, xcomp2}. In this way we have encoded all the Lappin and Leass syntactic constraints. They were found to depend on the object relations (ncsubj, dobj, obj2, iobj), the complement relations (xcomp, ccomp, and clausal), and the non-clausal modifier ncmod relation. We also use the GRs to extract the salience factor information. 3 Intermediate Parsing 3.1 Grammatical Relations We use the GRs obtained from the Briscoe and Carroll (2002) parser and the Buchholz (2002) GR finder for the parser components of our implementation of the Lappin and Leass anaphora resolution algorithm. Both these parsers are capable of producing GRs according to the Carroll et al. (1998) specification. We will now describe the two parsers, and present their performance on this corpus. 3.2 Briscoe and Carroll (2002)3 Grammar unification-based (manually created) grammar of part of speech and punctuation labels. Parsing algorithm: LR parser. Tagger: Acquilex HMM tagger (using the CLAWS-II labels) (Elworthy, 1994). Training corpus: Susanne corpus is use</context>
</contexts>
<marker>Briscoe, Carroll, 2002</marker>
<rawString>E. J. Briscoe and J. Carroll. 2002. Robust accurate statistical annotation of general text. In Proceedings of the 3rd International Conference on Language Resources and Evaluation, pages 1499-1504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Buchholz</author>
<author>J Veenstra</author>
<author>W Daelemans</author>
</authors>
<title>Cascaded grammatical relation assignment.</title>
<date>1999</date>
<booktitle>Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC),</booktitle>
<pages>239--246</pages>
<editor>In P. Fung and J. Zhou, editors,</editor>
<contexts>
<context position="9418" citStr="Buchholz et al., 1999" startWordPosition="1549" endWordPosition="1552">ill now describe the two parsers, and present their performance on this corpus. 3.2 Briscoe and Carroll (2002)3 Grammar unification-based (manually created) grammar of part of speech and punctuation labels. Parsing algorithm: LR parser. Tagger: Acquilex HMM tagger (using the CLAWS-II labels) (Elworthy, 1994). Training corpus: Susanne corpus is used for development (Sampson, 1995). 3.3 Buchholz (2002) Tagger: Memory-based tagger due to Daelemans et al. (1996). Chunker: Memory-based chunker due to Veenstra and van den Bosch (2000). Shallow Parser: Memory-based shallow parser (Daelemans, 1996), (Buchholz et al., 1999). &apos;Similarly, xcomp is only relevant when H is a form of be. 3 Available from http: //www . cogs . susx.ac.uk/ lab/nip/rasp/ 3 GR # occ BC BU Precision Recall Precision Recall ccomp 81 20.45 11.11 65.00 64.20 detmod 1124 91.15 89.77 92.41 90.93 dobj 409 85.83 78.48 88.42 76.53 iobj 158 31.93 67.09 57.75 51.90 ncmod 2403 69.45 57.72 66.86 51.64 ncsubj 1038 81.99 82.47 85.83 72.93 obj2 19 27.45 73.68 46.15 31.58 xcomp 322 73.99 62.73 78.00 72.67 I-tw - 75.73 70.29 77.45 66.74 Table 2: GR Precisions and Recalls Training Corpus: Sections 10-19 of the Wall Street Journal of Penn Treebank II (Marcus</context>
</contexts>
<marker>Buchholz, Veenstra, Daelemans, 1999</marker>
<rawString>S. Buchholz, J. Veenstra, and W. Daelemans. 1999. Cascaded grammatical relation assignment. In P. Fung and J. Zhou, editors, Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC), pages 239-246.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Buchholz</author>
</authors>
<title>Memory-Based Grammatical Relation Finding.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Tilburg.</institution>
<contexts>
<context position="8576" citStr="Buchholz (2002)" startWordPosition="1427" endWordPosition="1428"> near her, can be expressed as: xcomp is only relevant when H is a form of be. (arg - X N -) (ncmod Prep X P) where arg E {ncsubj, dobj, iobj , obj2, xcomp2}. In this way we have encoded all the Lappin and Leass syntactic constraints. They were found to depend on the object relations (ncsubj, dobj, obj2, iobj), the complement relations (xcomp, ccomp, and clausal), and the non-clausal modifier ncmod relation. We also use the GRs to extract the salience factor information. 3 Intermediate Parsing 3.1 Grammatical Relations We use the GRs obtained from the Briscoe and Carroll (2002) parser and the Buchholz (2002) GR finder for the parser components of our implementation of the Lappin and Leass anaphora resolution algorithm. Both these parsers are capable of producing GRs according to the Carroll et al. (1998) specification. We will now describe the two parsers, and present their performance on this corpus. 3.2 Briscoe and Carroll (2002)3 Grammar unification-based (manually created) grammar of part of speech and punctuation labels. Parsing algorithm: LR parser. Tagger: Acquilex HMM tagger (using the CLAWS-II labels) (Elworthy, 1994). Training corpus: Susanne corpus is used for development (Sampson, 199</context>
</contexts>
<marker>Buchholz, 2002</marker>
<rawString>S. Buchholz. 2002. Memory-Based Grammatical Relation Finding. Ph.D. thesis, University of Tilburg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carroll</author>
<author>E Briscoe</author>
<author>A Sanfilippo</author>
</authors>
<title>Parser evaluation: A survey and a new proposal.</title>
<date>1998</date>
<booktitle>In Proceedings of the International Conference on Language Resources and Evaluation,</booktitle>
<pages>447--454</pages>
<contexts>
<context position="6702" citStr="Carroll et al. (1998)" startWordPosition="1091" endWordPosition="1094">tion of Lappin and Leass&apos; algorithm relied mainly on a clausal (head—argument) representation of McCord&apos;s Slot Grammar (Lappin and McCord, 1990). Kennedy and Boguraev (1996) modified the Lappin and Leass algorithm to use flat morpho—syntactic information (a shallow parser), this being more flexible for new domains. Our implementation of Lappin and Leass uses (only) GRs conforming to the specifica2 Sentence: Mary gave her a book. Grammatical Relations: (ncsubj gave Mary _) (dobj gave her _) (obj2 gave book) (detmod _ book a) Figure 1: Briscoe and Carroll output for Mary gave her a book tion of Carroll et al. (1998). An example for the sentence Mary gave her a book can be seen in Figure 1. The underscores in the GRs above indicate the GR&apos;s type. For example, detmod has type poss for pronominal determiners, e.g. (detmod poss cat his). Because it is possible to recover constituency from a set of GRs (Hays, 1964), (Gaifman, 1965), all information necessary for the anaphora resolution algorithm is present in the GRs. Information concerning the grammatical role of each noun phrase in the sentence can easily be recovered from the GRs. However, it is not immediately clear that Mary and her cannot corefer. This </context>
<context position="8776" citStr="Carroll et al. (1998)" startWordPosition="1458" endWordPosition="1461"> Lappin and Leass syntactic constraints. They were found to depend on the object relations (ncsubj, dobj, obj2, iobj), the complement relations (xcomp, ccomp, and clausal), and the non-clausal modifier ncmod relation. We also use the GRs to extract the salience factor information. 3 Intermediate Parsing 3.1 Grammatical Relations We use the GRs obtained from the Briscoe and Carroll (2002) parser and the Buchholz (2002) GR finder for the parser components of our implementation of the Lappin and Leass anaphora resolution algorithm. Both these parsers are capable of producing GRs according to the Carroll et al. (1998) specification. We will now describe the two parsers, and present their performance on this corpus. 3.2 Briscoe and Carroll (2002)3 Grammar unification-based (manually created) grammar of part of speech and punctuation labels. Parsing algorithm: LR parser. Tagger: Acquilex HMM tagger (using the CLAWS-II labels) (Elworthy, 1994). Training corpus: Susanne corpus is used for development (Sampson, 1995). 3.3 Buchholz (2002) Tagger: Memory-based tagger due to Daelemans et al. (1996). Chunker: Memory-based chunker due to Veenstra and van den Bosch (2000). Shallow Parser: Memory-based shallow parser </context>
</contexts>
<marker>Carroll, Briscoe, Sanfilippo, 1998</marker>
<rawString>J. Carroll, E. Briscoe, and A. Sanfilippo. 1998. Parser evaluation: A survey and a new proposal. In Proceedings of the International Conference on Language Resources and Evaluation, pages 447-454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>J Zavrel</author>
<author>P Berck</author>
<author>S Gillis</author>
</authors>
<title>MBT: A memory-based part of speech tagger generator.</title>
<date>1996</date>
<booktitle>Proceedings of the 4th ACL/SIGDAT Workshop on Very Large Corpora,</booktitle>
<pages>14--27</pages>
<editor>In E. Ejerhed and I. Dagan, editors,</editor>
<contexts>
<context position="9258" citStr="Daelemans et al. (1996)" startWordPosition="1527" endWordPosition="1530">of the Lappin and Leass anaphora resolution algorithm. Both these parsers are capable of producing GRs according to the Carroll et al. (1998) specification. We will now describe the two parsers, and present their performance on this corpus. 3.2 Briscoe and Carroll (2002)3 Grammar unification-based (manually created) grammar of part of speech and punctuation labels. Parsing algorithm: LR parser. Tagger: Acquilex HMM tagger (using the CLAWS-II labels) (Elworthy, 1994). Training corpus: Susanne corpus is used for development (Sampson, 1995). 3.3 Buchholz (2002) Tagger: Memory-based tagger due to Daelemans et al. (1996). Chunker: Memory-based chunker due to Veenstra and van den Bosch (2000). Shallow Parser: Memory-based shallow parser (Daelemans, 1996), (Buchholz et al., 1999). &apos;Similarly, xcomp is only relevant when H is a form of be. 3 Available from http: //www . cogs . susx.ac.uk/ lab/nip/rasp/ 3 GR # occ BC BU Precision Recall Precision Recall ccomp 81 20.45 11.11 65.00 64.20 detmod 1124 91.15 89.77 92.41 90.93 dobj 409 85.83 78.48 88.42 76.53 iobj 158 31.93 67.09 57.75 51.90 ncmod 2403 69.45 57.72 66.86 51.64 ncsubj 1038 81.99 82.47 85.83 72.93 obj2 19 27.45 73.68 46.15 31.58 xcomp 322 73.99 62.73 78.0</context>
</contexts>
<marker>Daelemans, Zavrel, Berck, Gillis, 1996</marker>
<rawString>W. Daelemans, J. Zavrel, P. Berck, and S. Gillis. 1996. MBT: A memory-based part of speech tagger generator. In E. Ejerhed and I. Dagan, editors, Proceedings of the 4th ACL/SIGDAT Workshop on Very Large Corpora, pages 14-27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
</authors>
<title>Abstraction considered harmful: Lazy learning of language processing.</title>
<date>1996</date>
<booktitle>Proceedings of the Sixth Beligian-Dutch Conference on Machine Learning,</booktitle>
<pages>3--12</pages>
<editor>In H. J. van den Herik and A. Weijiters, editors,</editor>
<contexts>
<context position="9393" citStr="Daelemans, 1996" startWordPosition="1547" endWordPosition="1548">specification. We will now describe the two parsers, and present their performance on this corpus. 3.2 Briscoe and Carroll (2002)3 Grammar unification-based (manually created) grammar of part of speech and punctuation labels. Parsing algorithm: LR parser. Tagger: Acquilex HMM tagger (using the CLAWS-II labels) (Elworthy, 1994). Training corpus: Susanne corpus is used for development (Sampson, 1995). 3.3 Buchholz (2002) Tagger: Memory-based tagger due to Daelemans et al. (1996). Chunker: Memory-based chunker due to Veenstra and van den Bosch (2000). Shallow Parser: Memory-based shallow parser (Daelemans, 1996), (Buchholz et al., 1999). &apos;Similarly, xcomp is only relevant when H is a form of be. 3 Available from http: //www . cogs . susx.ac.uk/ lab/nip/rasp/ 3 GR # occ BC BU Precision Recall Precision Recall ccomp 81 20.45 11.11 65.00 64.20 detmod 1124 91.15 89.77 92.41 90.93 dobj 409 85.83 78.48 88.42 76.53 iobj 158 31.93 67.09 57.75 51.90 ncmod 2403 69.45 57.72 66.86 51.64 ncsubj 1038 81.99 82.47 85.83 72.93 obj2 19 27.45 73.68 46.15 31.58 xcomp 322 73.99 62.73 78.00 72.67 I-tw - 75.73 70.29 77.45 66.74 Table 2: GR Precisions and Recalls Training Corpus: Sections 10-19 of the Wall Street Journal of</context>
</contexts>
<marker>Daelemans, 1996</marker>
<rawString>W. Daelemans. 1996. Abstraction considered harmful: Lazy learning of language processing. In H. J. van den Herik and A. Weijiters, editors, Proceedings of the Sixth Beligian-Dutch Conference on Machine Learning, pages 3-12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Elworthy</author>
</authors>
<title>Does Baum-Welch reestimation help taggers?</title>
<date>1994</date>
<booktitle>In Proceedings of the 4th Conference on Applied NLP,</booktitle>
<pages>53--58</pages>
<contexts>
<context position="9105" citStr="Elworthy, 1994" startWordPosition="1507" endWordPosition="1508">se the GRs obtained from the Briscoe and Carroll (2002) parser and the Buchholz (2002) GR finder for the parser components of our implementation of the Lappin and Leass anaphora resolution algorithm. Both these parsers are capable of producing GRs according to the Carroll et al. (1998) specification. We will now describe the two parsers, and present their performance on this corpus. 3.2 Briscoe and Carroll (2002)3 Grammar unification-based (manually created) grammar of part of speech and punctuation labels. Parsing algorithm: LR parser. Tagger: Acquilex HMM tagger (using the CLAWS-II labels) (Elworthy, 1994). Training corpus: Susanne corpus is used for development (Sampson, 1995). 3.3 Buchholz (2002) Tagger: Memory-based tagger due to Daelemans et al. (1996). Chunker: Memory-based chunker due to Veenstra and van den Bosch (2000). Shallow Parser: Memory-based shallow parser (Daelemans, 1996), (Buchholz et al., 1999). &apos;Similarly, xcomp is only relevant when H is a form of be. 3 Available from http: //www . cogs . susx.ac.uk/ lab/nip/rasp/ 3 GR # occ BC BU Precision Recall Precision Recall ccomp 81 20.45 11.11 65.00 64.20 detmod 1124 91.15 89.77 92.41 90.93 dobj 409 85.83 78.48 88.42 76.53 iobj 158 </context>
</contexts>
<marker>Elworthy, 1994</marker>
<rawString>D. Elworthy. 1994. Does Baum-Welch reestimation help taggers? In Proceedings of the 4th Conference on Applied NLP, pages 53-58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Gaifman</author>
</authors>
<title>Dependency systems and phrase-structure systems.</title>
<date>1965</date>
<journal>Information and Control,</journal>
<pages>8--3</pages>
<contexts>
<context position="7019" citStr="Gaifman, 1965" startWordPosition="1151" endWordPosition="1152">lementation of Lappin and Leass uses (only) GRs conforming to the specifica2 Sentence: Mary gave her a book. Grammatical Relations: (ncsubj gave Mary _) (dobj gave her _) (obj2 gave book) (detmod _ book a) Figure 1: Briscoe and Carroll output for Mary gave her a book tion of Carroll et al. (1998). An example for the sentence Mary gave her a book can be seen in Figure 1. The underscores in the GRs above indicate the GR&apos;s type. For example, detmod has type poss for pronominal determiners, e.g. (detmod poss cat his). Because it is possible to recover constituency from a set of GRs (Hays, 1964), (Gaifman, 1965), all information necessary for the anaphora resolution algorithm is present in the GRs. Information concerning the grammatical role of each noun phrase in the sentence can easily be recovered from the GRs. However, it is not immediately clear that Mary and her cannot corefer. This is decided by one of a number of syntactic constraints in the Lappin and Leass algorithm, which rule out certain intrasentential noun phrases from becoming candidate antecedents. As the success of the algorithm is mainly dependent on the early filtering of candidates, a substantial part of our work focused on obtain</context>
</contexts>
<marker>Gaifman, 1965</marker>
<rawString>H. Gaifman. 1965. Dependency systems and phrase-structure systems. Information and Control, 8(3):304-337.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D G Hays</author>
</authors>
<title>Dependency theory: A formalism and some observations.</title>
<date>1964</date>
<journal>Language,</journal>
<pages>40--511</pages>
<contexts>
<context position="7002" citStr="Hays, 1964" startWordPosition="1149" endWordPosition="1150">mains. Our implementation of Lappin and Leass uses (only) GRs conforming to the specifica2 Sentence: Mary gave her a book. Grammatical Relations: (ncsubj gave Mary _) (dobj gave her _) (obj2 gave book) (detmod _ book a) Figure 1: Briscoe and Carroll output for Mary gave her a book tion of Carroll et al. (1998). An example for the sentence Mary gave her a book can be seen in Figure 1. The underscores in the GRs above indicate the GR&apos;s type. For example, detmod has type poss for pronominal determiners, e.g. (detmod poss cat his). Because it is possible to recover constituency from a set of GRs (Hays, 1964), (Gaifman, 1965), all information necessary for the anaphora resolution algorithm is present in the GRs. Information concerning the grammatical role of each noun phrase in the sentence can easily be recovered from the GRs. However, it is not immediately clear that Mary and her cannot corefer. This is decided by one of a number of syntactic constraints in the Lappin and Leass algorithm, which rule out certain intrasentential noun phrases from becoming candidate antecedents. As the success of the algorithm is mainly dependent on the early filtering of candidates, a substantial part of our work </context>
</contexts>
<marker>Hays, 1964</marker>
<rawString>D. G. Hays. 1964. Dependency theory: A formalism and some observations. Language, 40:511-525.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Kennedy</author>
<author>B Boguraev</author>
</authors>
<title>Anaphora for everyone: Pronominal anaphora resolution without a parser.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th International Conference on Computational Linguistics (COLING&apos;96),</booktitle>
<pages>113--118</pages>
<contexts>
<context position="850" citStr="Kennedy and Boguraev (1996)" startWordPosition="118" endWordPosition="121">am.ac.uk Abstract We show that the Lappin and Leass (1994) anaphora resolution algorithm, originally implemented to use McCord&apos;s Slot Grammar, can be equivalently expressed in terms of grammatical relations (GRs) (Carroll et al., 1998). We use this GR version of Lappin and Leass&apos; algorithm to investigate the effect on performance when the parser is changed. We find that the performance of the anaphora resolution algorithm does not appear to be greatly affected. The results from the GR version of the Lappin and Leass anaphora resolution algorithm are also briefly compared to the results of the Kennedy and Boguraev (1996) anaphora resolution algorithm. 1 Introduction We question the need to use a full parser, such as McCord&apos;s Slot Grammar, in the Lappin and Leass (1994) anaphora resolution algorithm. The goal of our work is to show that this algorithm can be equivalently implemented in terms of Carroll et al.&apos;s (1998) grammatical relations (GRs). By a full parser we understand a system which uses subcategorization information, and it is not necessary to construct such a full parse for a system which This work was supported by UK EPSRC project GR/N36462/93 &apos;Robust Accurate Statistical Parsing (RASP)&apos;. outputs G</context>
<context position="6254" citStr="Kennedy and Boguraev (1996)" startWordPosition="1014" endWordPosition="1017"> candidate with the highest salience is proposed as the antecedent. 2.2 Using Grammatical Relations The Lappin and Leass anaphora resolution algorithm exploits two types of information: information which is directly obtained from GRs (for example, being a subject or direct object), and information for which extraction is not so simple (for example, whether the pronoun is in the argument or the adjunct domain of a noun phrase). The original implementation of Lappin and Leass&apos; algorithm relied mainly on a clausal (head—argument) representation of McCord&apos;s Slot Grammar (Lappin and McCord, 1990). Kennedy and Boguraev (1996) modified the Lappin and Leass algorithm to use flat morpho—syntactic information (a shallow parser), this being more flexible for new domains. Our implementation of Lappin and Leass uses (only) GRs conforming to the specifica2 Sentence: Mary gave her a book. Grammatical Relations: (ncsubj gave Mary _) (dobj gave her _) (obj2 gave book) (detmod _ book a) Figure 1: Briscoe and Carroll output for Mary gave her a book tion of Carroll et al. (1998). An example for the sentence Mary gave her a book can be seen in Figure 1. The underscores in the GRs above indicate the GR&apos;s type. For example, detmod</context>
</contexts>
<marker>Kennedy, Boguraev, 1996</marker>
<rawString>C. Kennedy and B. Boguraev. 1996. Anaphora for everyone: Pronominal anaphora resolution without a parser. In Proceedings of the 16th International Conference on Computational Linguistics (COLING&apos;96), pages 113-118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Lappin</author>
<author>H Leass</author>
</authors>
<title>An algorithm for pronominal anaphora resolution.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<pages>20--4</pages>
<contexts>
<context position="1001" citStr="Lappin and Leass (1994)" startWordPosition="143" endWordPosition="146">alently expressed in terms of grammatical relations (GRs) (Carroll et al., 1998). We use this GR version of Lappin and Leass&apos; algorithm to investigate the effect on performance when the parser is changed. We find that the performance of the anaphora resolution algorithm does not appear to be greatly affected. The results from the GR version of the Lappin and Leass anaphora resolution algorithm are also briefly compared to the results of the Kennedy and Boguraev (1996) anaphora resolution algorithm. 1 Introduction We question the need to use a full parser, such as McCord&apos;s Slot Grammar, in the Lappin and Leass (1994) anaphora resolution algorithm. The goal of our work is to show that this algorithm can be equivalently implemented in terms of Carroll et al.&apos;s (1998) grammatical relations (GRs). By a full parser we understand a system which uses subcategorization information, and it is not necessary to construct such a full parse for a system which This work was supported by UK EPSRC project GR/N36462/93 &apos;Robust Accurate Statistical Parsing (RASP)&apos;. outputs GRs. The new implementation enables us to investigate performance variation in the Lappin and Leass algorithm when the parser is changed for any other G</context>
<context position="4291" citStr="Lappin and Leass (1994)" startWordPosition="698" endWordPosition="701">corpus. In Section 2 we describe the Lappin and Leass anaphora resolution algorithm and our implementation which only uses GRs. A comparison of the two intermediate parsers that we employ can be found in Section 3. The results of the anaphora resolution algorithm using the two parsers are presented in Section 4. This section also contains the results for the Kennedy and Boguraev algorithm, and an analysis of the results. In Section 5 we draw our conclusions. 2 Lappin and Leass Algorithm 2.1 Algorithm Description For this experiment we choose to reimplement a non-probabilistic algorithm due to Lappin and Leass (1994), since this anaphora resolution algorithm mainly makes use of grammatical role information, and therefore only minor modification would be required for an intermediate parser to be used instead of the full parser they assume. For each pronoun, this algorithm uses syntactic criteria to rule out noun phrases that cannot possibly corefer with it. An antecedent is then chosen according to a ranking based on salience weights. For all types of pronoun, noun phrases are ruled out if they have incompatible agreement features. Pronouns are split into two classes: lexical (reflexives and reciprocals) a</context>
</contexts>
<marker>Lappin, Leass, 1994</marker>
<rawString>S. Lappin and H. Leass. 1994. An algorithm for pronominal anaphora resolution. Computational Linguistics, 20(4):535-561.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Lappin</author>
<author>M McCord</author>
</authors>
<title>A syntactic filter on pronominal anaphora for slot grammar.</title>
<date>1990</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>135--142</pages>
<contexts>
<context position="6225" citStr="Lappin and McCord, 1990" startWordPosition="1009" endWordPosition="1013">date from the pronoun. The candidate with the highest salience is proposed as the antecedent. 2.2 Using Grammatical Relations The Lappin and Leass anaphora resolution algorithm exploits two types of information: information which is directly obtained from GRs (for example, being a subject or direct object), and information for which extraction is not so simple (for example, whether the pronoun is in the argument or the adjunct domain of a noun phrase). The original implementation of Lappin and Leass&apos; algorithm relied mainly on a clausal (head—argument) representation of McCord&apos;s Slot Grammar (Lappin and McCord, 1990). Kennedy and Boguraev (1996) modified the Lappin and Leass algorithm to use flat morpho—syntactic information (a shallow parser), this being more flexible for new domains. Our implementation of Lappin and Leass uses (only) GRs conforming to the specifica2 Sentence: Mary gave her a book. Grammatical Relations: (ncsubj gave Mary _) (dobj gave her _) (obj2 gave book) (detmod _ book a) Figure 1: Briscoe and Carroll output for Mary gave her a book tion of Carroll et al. (1998). An example for the sentence Mary gave her a book can be seen in Figure 1. The underscores in the GRs above indicate the G</context>
</contexts>
<marker>Lappin, McCord, 1990</marker>
<rawString>S. Lappin and M. McCord. 1990. A syntactic filter on pronominal anaphora for slot grammar. In Proceedings of ACL, pages 135-142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Leech</author>
</authors>
<title>100 million words of English:</title>
<date>1992</date>
<journal>the British National Corpus. Language Research,</journal>
<pages>28--1</pages>
<contexts>
<context position="11026" citStr="Leech, 1992" startWordPosition="1830" endWordPosition="1831">r (BU) for the relevant GRs can be found in Table 2. In this table we also present the (weighted) mean performance itw. We split the 500 sentence corpus into ten 50 sentence segments and computed precision/recall on each segment. The resulting Fo=1 measures were compared using the onetailed t-test. The GRs produced by the Buchholz GR finder were found to be significantly more accurate than those produced by the Briscoe and Carroll parser (with a confidence of 99.5%). 4 Results For this experiment, we use a modified version of an anaphorically resolved 2400 sentence initial segment of the BNC (Leech, 1992), (Preiss, 4Available from http: //wee. cogs .susx. ac .uk/ lab/nlp/carroll/greval .html 5The Briscoe and Carroll parser has about a 74% coverage of the Susanne corpus. Text # sents # prons 1 754 135 2 785 118 3 318 154 4 268 135 5 271 137 Table 3: Corpus Information 2002). The modification consisted of removing certain tokenizations from the BNC: for example, out&lt; blank&gt; of becomes out of. To avoid any unwanted tokenization, punctuation was removed from inside of words (e.g. Biny a &apos;a has become Binyaa). We split the 2400 sentence corpus into five texts which contain roughly the same number o</context>
</contexts>
<marker>Leech, 1992</marker>
<rawString>G. Leech. 1992. 100 million words of English: the British National Corpus. Language Research, 28(1):1-13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>R Santorini</author>
<author>M Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn TreeBank. Computational Linguisitcs,</title>
<date>1993</date>
<pages>19--2</pages>
<contexts>
<context position="10032" citStr="Marcus et al., 1993" startWordPosition="1658" endWordPosition="1661"> 1999). &apos;Similarly, xcomp is only relevant when H is a form of be. 3 Available from http: //www . cogs . susx.ac.uk/ lab/nip/rasp/ 3 GR # occ BC BU Precision Recall Precision Recall ccomp 81 20.45 11.11 65.00 64.20 detmod 1124 91.15 89.77 92.41 90.93 dobj 409 85.83 78.48 88.42 76.53 iobj 158 31.93 67.09 57.75 51.90 ncmod 2403 69.45 57.72 66.86 51.64 ncsubj 1038 81.99 82.47 85.83 72.93 obj2 19 27.45 73.68 46.15 31.58 xcomp 322 73.99 62.73 78.00 72.67 I-tw - 75.73 70.29 77.45 66.74 Table 2: GR Precisions and Recalls Training Corpus: Sections 10-19 of the Wall Street Journal of Penn Treebank II (Marcus et al., 1993). 3.4 Relative Performance As part of the development of their parser, Carroll et al. manually annotated 500 sentences with their GRs.4 The sentences were selected at random from the Susanne corpus subject to the constraint that they are within the coverage of the Briscoe and Carroll parser.5 The performance results of the Briscoe and Carroll parser (BC) and the Buchholz GR finder (BU) for the relevant GRs can be found in Table 2. In this table we also present the (weighted) mean performance itw. We split the 500 sentence corpus into ten 50 sentence segments and computed precision/recall on ea</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. Marcus, R. Santorini, and M. Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn TreeBank. Computational Linguisitcs, 19(2):313-330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Preiss</author>
</authors>
<title>Choosing a parser for anaphora resolution.</title>
<date>2002</date>
<booktitle>In Proceedings of DAARC,</booktitle>
<pages>175--180</pages>
<contexts>
<context position="1679" citStr="Preiss, 2002" startWordPosition="256" endWordPosition="257">hat this algorithm can be equivalently implemented in terms of Carroll et al.&apos;s (1998) grammatical relations (GRs). By a full parser we understand a system which uses subcategorization information, and it is not necessary to construct such a full parse for a system which This work was supported by UK EPSRC project GR/N36462/93 &apos;Robust Accurate Statistical Parsing (RASP)&apos;. outputs GRs. The new implementation enables us to investigate performance variation in the Lappin and Leass algorithm when the parser is changed for any other GR producing parser. This work is motivated by our previous work (Preiss, 2002), which showed that the performance of the anaphora resolution algorithm did not change greatly if the full parser it uses was varied, so long as the grammatical roles could be extracted comparably accurately from the parsers. As noted by Kennedy and Boguraev (1996) and still true today, it is a fact that state-ofthe-art full parsers are not yet robust enough for an application like anaphora resolution. As we have observed in our own work, if an anaphora resolution experiment is restricted to blocks of sentences for which a full parse can be successfully generated, the anaphora resolution resu</context>
</contexts>
<marker>Preiss, 2002</marker>
<rawString>J. Preiss. 2002. Choosing a parser for anaphora resolution. In Proceedings of DAARC, pages 175-180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Sampson</author>
</authors>
<title>English for the computer.</title>
<date>1995</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="9178" citStr="Sampson, 1995" startWordPosition="1517" endWordPosition="1518">hholz (2002) GR finder for the parser components of our implementation of the Lappin and Leass anaphora resolution algorithm. Both these parsers are capable of producing GRs according to the Carroll et al. (1998) specification. We will now describe the two parsers, and present their performance on this corpus. 3.2 Briscoe and Carroll (2002)3 Grammar unification-based (manually created) grammar of part of speech and punctuation labels. Parsing algorithm: LR parser. Tagger: Acquilex HMM tagger (using the CLAWS-II labels) (Elworthy, 1994). Training corpus: Susanne corpus is used for development (Sampson, 1995). 3.3 Buchholz (2002) Tagger: Memory-based tagger due to Daelemans et al. (1996). Chunker: Memory-based chunker due to Veenstra and van den Bosch (2000). Shallow Parser: Memory-based shallow parser (Daelemans, 1996), (Buchholz et al., 1999). &apos;Similarly, xcomp is only relevant when H is a form of be. 3 Available from http: //www . cogs . susx.ac.uk/ lab/nip/rasp/ 3 GR # occ BC BU Precision Recall Precision Recall ccomp 81 20.45 11.11 65.00 64.20 detmod 1124 91.15 89.77 92.41 90.93 dobj 409 85.83 78.48 88.42 76.53 iobj 158 31.93 67.09 57.75 51.90 ncmod 2403 69.45 57.72 66.86 51.64 ncsubj 1038 81</context>
</contexts>
<marker>Sampson, 1995</marker>
<rawString>G. Sampson. 1995. English for the computer. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Veenstra</author>
<author>A van den Bosch</author>
</authors>
<title>Singleclassifier memory-based phrase chunking.</title>
<date>2000</date>
<booktitle>In Proceedings of the Fourth Conference on Computational Natural Language Learning (CoNLL) and the Second Learning Language in Logic Workshop (LLL),</booktitle>
<pages>157--159</pages>
<marker>Veenstra, van den Bosch, 2000</marker>
<rawString>J. Veenstra and A. van den Bosch. 2000. Singleclassifier memory-based phrase chunking. In Proceedings of the Fourth Conference on Computational Natural Language Learning (CoNLL) and the Second Learning Language in Logic Workshop (LLL), pages 157-159.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>