<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<note confidence="0.6695374">
Learning a syntagmatic and paradigmatic structure from language
data with a bi-multigram model
Sabine Deligne and Yoshinori Sagisaka
ATR-ITL, deptl, 2-2 Hikaridai
Seika cho, Soraku gun, Kyoto fu 619-0224, Japan.
</note>
<sectionHeader confidence="0.976475" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999688266666667">
In this paper, we present a stochastic language mod-
eling tool which aims at retrieving variable-length
phrases (multigrams), assuming bigram dependen-
cies between them. The phrase retrieval can be in-
termixed with a phrase clustering procedure, so that
the language data are iteratively structured at both
a paradigmatic and a syntagmatic level in a fully in-
tegrated way. Perplexity results on ATR travel ar-
rangement data with a bi-multigram model (assum-
ing bigram correlations between the phrases) come
very close to the trigram scores with a reduced num-
ber of entries in the language model. Also the ability
of the class version of the model to merge semanti-
cally related phrases into a common class is illus-
trated.
</bodyText>
<sectionHeader confidence="0.998429" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.98213415">
There is currently an increasing interest in statisti-
cal language models, which in one way or another
aim at exploiting word-dependencies spanning over
a variable number of words. Though all these mod-
els commonly relax the assumption of fixed-length
dependency of the conventional ngram model, they
cover a wide variety of modeling assumptions and of
parameter estimation frameworks. In this paper, we
focus on a phrase-based approach, as opposed to a
gram-based approach: sentences are structured into
phrases and probabilities are assigned to phrases in-
stead of words. Regardless of whether they are gram
or phrase-based, models can be either determinis-
tic or stochastic. In the phrase-based framework,
non determinism is introduced via an ambiguity on
the parse of the sentence into phrases. In practice,
it means that even if phrase abc is registered as a
phrase, the possibility of parsing the string as, for
instance, [ab] [c] still remains. By contrast, in a de-
terministic approach, all co-occurences of a, b and c
would be systematically interpreted as an occurence
of phrase [abc].
Various criteria have been proposed to derive
phrases in a purely statistical way 1; data likeli-
I i.e. without using grammar rules like in Stochastic Con-
text Free Grammars.
hood, leaving-one-out likelihood (Ries et al., 1996),
mutual information (Suhm and Waibel, 1994), and
entropy (Masataki and Sagisaka, 1996). The use of
the likelihood criterion in a stochastic framework al-
lows EM principled optimization procedures, but it
is prone to overlearning. The other criteria tend
to reduce the risk of overlearning, but their opti-
mization relies on heuristic procedures (e.g. word
grouping via a greedy algorithm (Matsunaga and
Sagayama, 1997)) for which convergence and opti-
mality are not theoretically guaranteed. The work
reported in this paper is based on the multigram
model, which is a stochastic phrase-based model,
the parameters of which are estimated according to
a likelihood criterion using an EM procedure. The
multigram approach was introduced in (Bimbot et
al., 1995), and in (Deligne and Bimbot, 1995) it was
used to derive variable-length phrases under the as-
sumption of independence of the phrases. Various
ways of theoretically releasing this assumption were
given in (Deligne et al., 1996). More recently, ex-
periments with 2-word multigrams embedded in a
deterministic variable ngram scheme were reported
in (Siu, 1998).
In section 2 of this paper, we further formulate
a model with bigram (more generally IT-gram) de-
pendencies between the phrases, by including a
paradigmatic aspect which enables the clustering
of variable-length phrases. It results in a stochas-
tic class-phrase model, which can be interpolated
with the stochastic phrase model, in a similar way
to deterministic approaches. In section 3 and 4,
the phrase and class-phrase models are evaluated in
terms of perplexity values and model size.
</bodyText>
<sectionHeader confidence="0.99871" genericHeader="introduction">
2 Theoretical formulation of the
multigrams
</sectionHeader>
<subsectionHeader confidence="0.999534">
2.1 Variable-length phrase distribution
</subsectionHeader>
<bodyText confidence="0.9999685">
In the multigram framework, the assumption is
made that sentences result from the concatenation
of variable-length phrases, called multigrams. The
likelihood of a sentence is computed by summing
the likelihood values of all possible segmentations of
the sentence into phrases. The likelihood computa-
</bodyText>
<page confidence="0.99497">
300
</page>
<bodyText confidence="0.999840571428572">
tion for any particular segmentation into phrases de-
pends on the model assumed to describe the depen-
dencies between the phrases. We call bi-multigram
model the model where bigram dependencies are
assumed between the phrases. For instance, by lim-
iting to 3 words the maximal length of a phrase, the
bi-multigram likelihood of the string &amp;quot;a b c d&amp;quot; is:
</bodyText>
<equation confidence="0.698911571428572">
1P([61] I #) Pal)] I [a]) P(ici I [b]) Pad] I [c])
AN I #) P([b] I [a]) P([cd] I [b])
p([a] I #) pabc] I [a]) p([d] I [bc])
E P([a] I #) Pabcd] I [a])
p([ab] I #) p([c] I [ab]) p([d] I [c])
p([ab] I #) p([cd] I [ab])
p([abc] I #) p([d] I [abc])
</equation>
<bodyText confidence="0.999650666666667">
To present the general formalism of the model in
this section, we assume 12-gram correlations between
the phrases, and we note n the maximal length of a
phrase (in the above example, Ti=2 and n=3). Let
W denote a string of words, and {S} the set of pos-
sible segmentations on W. The likelihood of W is:
</bodyText>
<equation confidence="0.9527325">
C(W)= E r(W,S) (1)
SE{S}
</equation>
<bodyText confidence="0.842536">
and the likelihood of a segmentation S of W is:
</bodyText>
<equation confidence="0.992574">
,C (W,S) = JJ P(S(r) I S(r—fil-1) • • S(r-1)) (2)
</equation>
<bodyText confidence="0.997373583333333">
with 8(,) denoting the phrase of rank (r) in the seg-
mentation S. The model is thus fully defined by
the set of W-gram probabilities on the set {si}; of all
the phrases which can be formed by combining 1, 2,
...up to n words of the vocabulary. Maximum like-
lihood (ML) estimates of these probabilities can be
obtained by formulating the estimation problem as
a ML estimation from incomplete data (Dempster et
al., 1977), where the unknown data is the underly-
ing segmentation S. Let Q(k, k+ 1) be the following
auxiliary function computed with the likelihoods of
iterations k and k + 1:
</bodyText>
<equation confidence="0.996981666666667">
Q(k,k +1) E ek)(s I W) log ek+1)(W, S)
SE{S)
(3)
</equation>
<bodyText confidence="0.988774666666667">
It has been shown in (Dempster et al., 1977)
that if Q(k,k +1) &gt; Q(k,k), then ek+1)(W) &gt;
ek)(W). Therefore the reestimation equation of
p(si, si, siw_i), at iteration (k + 1), can be
derived by maximizing Q(k, k + 1) over the set of
parameters of iteration (k + 1), under the set of con-
</bodyText>
<equation confidence="0.868034">
straints E,._ p(si, I si, = 1, hence:
P(k-1-1)(siw si, =
EsE(s) c(sii • • •si_,si,, s) x .0)(s 1 14) ,„
EsE{s} c(sii • • • siw_1, s) x ,c(k)(s iv) lq)
</equation>
<bodyText confidence="0.999069285714286">
where c(sii . S) is the number of occurences of
the combination of phrases si, siw in the segmen-
tation S. Reestimation equation (4) can be imple-
mented by means of a forward-backward algorithm,
such as the one described for bi-multigrams (Ti = 2)
in the appendix of this paper. In a decision-oriented
scheme, the reestimation equation reduces to:
</bodyText>
<equation confidence="0.967515">
p(k+1)(siz sil ...siw_i) = c(sii . . . ,
c(si, S*(k))
(5)
</equation>
<bodyText confidence="0.7517705">
the segmentation maximizing
where
</bodyText>
<equation confidence="0.884219">
is retrieved with a Viterbi algo-
1 W),
rithm.
</equation>
<bodyText confidence="0.9933325">
Since each iteration improves the model in the sense
of increasing the likelihood ek)(W), it eventually
converges to a critical point (possibly a local
maximum).
</bodyText>
<subsectionHeader confidence="0.998659">
2.2 Variable-length phrase clustering
</subsectionHeader>
<bodyText confidence="0.999808944444444">
Recently, class-phrase based models have gained
some attention (Ries et al., 1996), but usually
it assumes a previous clustering of the words.
Typically, each word is first assigned a word-class
label &amp;quot;&lt; Ck &gt;&amp;quot;, then variable-length phrases
[Ck1Ck2-.Ck,] of word-class labels are retrieved,
each of which leads to define a phrase-class label
which can be denoted as &amp;quot;&lt; &gt;&amp;quot;. But
in this approach only phrases of the same length
can be assigned the same phrase-class label. For
instance, the phrases &amp;quot;thank you for&amp;quot; and &amp;quot;thank
you very much for&amp;quot; cannot be assigned the same
class label. We propose to address this limitation
by directly clustering phrases instead of words.
For this purpose, we assume bigram correlations
between the phrases = 2), and we modify the
learning procedure of section 2.1, so that each
iteration consists of 2 steps:
</bodyText>
<listItem confidence="0.99460825">
• Step 1 Phrase clustering:
P(k)(si I si)
{P(k)(Cq(3,) I Cq(s,)), P(k)(si I Cq(3,))
• Step 2 Bi-multigram reestimation:
</listItem>
<equation confidence="0.833299">
{ p(k)(Cg(,,) I Cq(„)), P(k)(81 I Qs))) }
fp(k+1)(sj I si)
</equation>
<bodyText confidence="0.999907666666667">
Step 1 takes a phrase distribution as an input,
assigns each phrase si to a class C,7(,1), and out-
puts the corresponding class distribution. In our
experiments, the class assignment is performed by
maximizing the mutual information between adja-
cent phrases, following the line described in (Brown
</bodyText>
<page confidence="0.994031">
301
</page>
<bodyText confidence="0.999954571428571">
et al., 1992), with only the modification that can-
didates to clustering are phrases instead of words.
The clustering process is initialized by assigning each
phrase to its own class. The loss in average mutual
information when merging 2 classes is computed for
every pair of classes, and the 2 classes for which the
loss is minimal are merged. After each merge, the
loss values are updated and the process is repeated
till the required number of classes is obtained.
Step 2 consists in reestimating a phrase distribution
using the bi-multigram reestimation equation (4)
or (5), with the only difference that the likelihood
of a parse, instead of being computed as in Eq. (2),
is now computed with the class estimates, i.e. as:
</bodyText>
<equation confidence="0.786766">
C(W,S) =JJ P(C9(s(o) I C9($(.-1)))1*(7) I Cg(s(,)))
</equation>
<bodyText confidence="0.962839676470588">
This is equivalent to reestimating p(k+1)(s si)
from p(k)(Cq(o,) I Q310 x p(k)(si I Cq(,,)), instead
of P(k)(s.iI si) as was the case in section 2.1.
Overall, step 1 ensures that the class assignment
based on the mutual information criterion is optimal
with respect to the current estimates of the phrase
distribution and step 2 ensures that the phrase dis-
tribution optimizes the likelihood computed accord-
ing to (6) with the current estimates of the class
distribution. The training data are thus iteratively
structured in a fully integrated way, at both a
paradigmatic level (step 1) and a syntagmatic level
(step 2).
2.3 Interpolation of stochastic class-phrase
and phrase models
With a class model, the probabilities of 2 phrases
belonging to the same class are distinguished only
according to their unigram probability. As it is un-
likely that this loss of precision be compensated by
the improved robustness of the estimates of the class
distribution, class based models can be expected to
deteriorate the likelihood of not only train but also
test data, with respect to non-class based models.
However, the performance of non-class models can
be enhanced by interpolating their estimates with
the class estimates. We first recall the way linear
interpolation is performed with conventional word
ngram models, and then we extend it to the case of
our stochastic phrase-based approach. Usually, lin-
ear interpolation weights are computed so as to max-
imize the likelihood of cross evaluation data (Jelinek
and Mercer, 1980). Denoting by A and (1 — A) the
interpolation weights, and by p+ the interpolated es-
timate, it comes for a word bigram model:
</bodyText>
<equation confidence="0.910323333333333">
P+(wi I wi) =
A P(wi I wi) + (1—)) P(C9(wi) I Cq(wo)P(wi I Cetvi))
(7)
</equation>
<bodyText confidence="0.968291">
with A having been iteratively estimated on a cross
evaluation corpus W„o„ as:
</bodyText>
<equation confidence="0.989320333333333">
A(k+1) = 1 A(k) P(w.i wi)
c(w,wi) „ (8)
1,3 P+ki(wi wi)
</equation>
<bodyText confidence="0.9976017">
where T„„, is the number of words inardsnd
c(wiwi) the number of co-occurences of the words
ow
wi and wi in Wcross •
In the case of a stochastic phrase based model -
where the segmentation into phrases is not known a
priori - the above computation of the interpolation
weights still applies, however, it has to be embedded
in dynamic programming to solve the ambiguity on
the segmentation:
</bodyText>
<equation confidence="0.966224333333333">
1csis s.(k)) A(k) P(si si)
;
c(s.(0) 4 (
7-s (k)
si)
(9)
</equation>
<bodyText confidence="0.984436416666667">
where S*(k) the most likely segmentation of Wcross
given the current estimates p(:)(sj si) can be re-
trieved with a Viterbi algorithm, and where c(S*(k))
is the number of sequences in the segmentation
S*(k). A more accurate, but computationally more
involved solution would be to compute A(k+1) as the
A(*) p(s, I s,)
expectation of 7(.-s-j- c(sisi I c)p+(k)(3,13,)
over the set of segmentations {S} on Wcross, us-
ing for this purpose a forward-backward algorithm.
However in the experiments reported in section 4,
we use Eq (9) only.
</bodyText>
<sectionHeader confidence="0.99777" genericHeader="method">
3 Experiments with phrase based
models
</sectionHeader>
<subsectionHeader confidence="0.995719">
3.1 Protocol and database
</subsectionHeader>
<bodyText confidence="0.999550714285714">
Evaluation protocol A motivation to learn bi-
gram dependencies between variable length phrases
is to improve the predictive capability of conven-
tional word bigram models, while keeping the num-
ber of parameters in the model lower than in the
word trigram case. The predictive capability is usu-
ally evaluated with the perplexity measure:
</bodyText>
<equation confidence="0.970794">
PP = cirloggw)
</equation>
<bodyText confidence="0.999251818181818">
where T is the number of words in W. The lower
PP is, the more accurate the prediction of the model
is. In the case of a stochastic model, there are ac-
tually 2 perplexity values PP and PP* computed
respectively from Es C(W,S) and L(W,S*). The
difference PP* — PP is always positive or zero, and
measures the average degree of ambiguity on a parse
S of W, or equivalently the loss in terms of predic-
tion accuracy, when the sentence likelihood is ap-
proximated with the likelihood of the best parse, as
is done in a speech recognizer.
</bodyText>
<equation confidence="0.5024595">
Tc1.03
A (k+1)
</equation>
<page confidence="0.989174">
302
</page>
<bodyText confidence="0.99991459375">
In section 3.2, we first evaluate the loss (PP* -PP)
using the forward-backward estimation procedure,
and then we study the influence of the estimation
procedure itself, i.e. Eq. (4) or (5), in terms of per-
plexity and model size (number of distinct 2-uplets
of phrases in the model). Finally, we compare these
results with the ones obtained with conventional n-
gram models (the model size is thus the number of
distinct n-uplets of words observed), using for this
purpose the CMU-Cambridge toolkit (Clarkson and
Rosenfeld, 1997).
Training protocol Experiments are reported for
phrases having at most n = 1, 2, 3 or 4 words (for
n =1, bi-multigrams correspond to conventional bi-
grams). The bi-multigram probabilities are initial-
ized using the relative frequencies of all the 2-uplets
of phrases observed in the training corpus, and they
are reestimated with 6 iterations. The dictionaries of
phrases are pruned by discarding all phrases occur-
ing less than 20 times at initialization, and less than
10 times after each iteration2, except for the 1-word
phrases which are kept with a number of occurrences
set to 1. Besides, bi-multigram and n-gram prob-
abilities are smoothed with the backoff smoothing
technique (Katz, 1987) using Witten-Bell discount-
ing (Witten and Bell, 1991)3.
Database Experiments are run on ATR travel ar-
rangement data (see Tab. 1). This database con-
sists of semi-spontaneous dialogues between a hotel
clerk and a customer asking for travel/accomodation
informations. All hesitation words and false starts
were mapped to a single marker &amp;quot;*uh*&amp;quot;.
</bodyText>
<table confidence="0.99792">
Train test
Nb sentences 13 650 2 430
Nb tokens 167 000 29 000 (1 % 00V)
Vocabulary 3 525 + 280 00V
</table>
<tableCaption confidence="0.999483">
Table 1: ATR Travel Arrangement Data
</tableCaption>
<subsectionHeader confidence="0.879106">
3.2 Results
</subsectionHeader>
<bodyText confidence="0.98704">
Ambiguity on a parse (Table 2) The difference
(PP* -PP) usually remains within about 1 point of
perplexity, meaning that the average ambiguity on a
parse is low, so that relying on the single best parse
should not decrease the accuracy of the prediction
very much.
Influence of the estimation procedure (Ta-
ble 3) As far as perplexity values are concerned,
</bodyText>
<footnote confidence="0.969017">
2Using different pruning thresholds values did not dra-
matically affect the results on our data, provided that the
threshold at initialization is in the range 20-40, and that the
threshold of the iterations is less than 10.
3The Witten-Bell discounting was chosen, because it
yielded the best perplexity scores with conventional n-grains
on our test data.
</footnote>
<table confidence="0.997627666666667">
n 1 2 3 4
PP 56.0 43.9 44.2 45.0
PP* 56.0 45.1 45.4 46.3
</table>
<tableCaption confidence="0.999424">
Table 2: Ambiguity on a parse.
</tableCaption>
<bodyText confidence="0.999592583333333">
the estimation scheme seems to have very little in-
fluence, with only a slight advantage in using the
forward-backward training. On the other hand, the
size of the model at the end of the training is about
30% less with the forward-backward training: ap-
proximately 40 000 versus 60 000, for a same test
perplexity value. The bi-multigram results tend to
indicate that the pruning heuristic used to discard
phrases does not allow us to fully avoid overtrain-
ing, since perplexities with n =3, 4 (i.e. dependen-
cies possibly spanning over 6 or 8 words) are higher
than with n =2 (dependencies limited to 4 words).
</bodyText>
<table confidence="0.997624125">
Test perp exity values PP*
n 1 2 3 4
F.-B. 56.0 45.1 45.4 46.3
Viterbi 56.0 45.7 45.9 46.2
Model size
n 1 2 3 4
F.-B. 32505 42347 43672 43186
Viterbi 32505 65141 67258 67295
</table>
<tableCaption confidence="0.9545765">
Table 3: Influence of the estimation procedure:
forward-backward (F.-B.) or Viterbi.
</tableCaption>
<bodyText confidence="0.992672">
Comparison with n-grams (Table 4) The low-
est bi-multigram perplexity (43.9) is still higher than
the trigram score, but it is much closer to the tri-
gram value (40.4) than to the bigram one (56.0) 4 .
The number of entries in the bi-multigram model is
much less than in the trigram model (45000 versus
75000), which illustrates the ability of the model to
select most relevant phrases.
</bodyText>
<table confidence="0.911023375">
Test perplexity values PP
n (and n) 1 2 3 4
n-gram 314.2 56.0 40.4 39.8
bimultigrams 56.0 43.9 44.2 45.0
Model size
n (and n) 1 2 3 4
n-gram 3526 32505 75511 112148
bimultigrams 32505 42347 43672 43186
</table>
<tableCaption confidence="0.9965185">
Table 4: Comparison with n-grams: Test perplexity
values and model size.
</tableCaption>
<footnote confidence="0.991476666666667">
4Besides, the trigram score depends on the discounted
scheme: with a linear discounting, the trigram perplexity on
our test data was 48.1.
</footnote>
<page confidence="0.999294">
303
</page>
<sectionHeader confidence="0.917579" genericHeader="method">
4 Experiments with class-phrase
based models
</sectionHeader>
<subsectionHeader confidence="0.999081">
4.1 Protocol and database
</subsectionHeader>
<bodyText confidence="0.999988388888889">
Evaluation protocol In section 4.2, we compare
class versions and interpolated versions of the bi-
gram, trigram and bi-multigram models, in terms
of perplexity values and of model size. For bigrams
(resp. trigrams) of classes, the size of the model is
the number of distinct 2-uplets (resp. 3-uplets) of
word-classes observed, plus the size of the vocab-
ulary. For the class version of the bi-multigrams,
the size of the model is the number of distinct 2-
uplets of phrase-classes, plus the number of distinct
phrases maintained. In section 4.3, we show samples
from classes of up to 5-word phrases, to illustrate
the potential benefit of clustering relatively long and
variable-length phrases for issues related to language
understanding.
Training protocol All non-class models are the
same as in section 3. The class-phrase models are
trained with 5 iterations of the algorithm described
in section 2.2: each iteration consists in clustering
the phrases into 300 phrase-classes (step 1), and in
reestimating the phrase distribution (step 2) with
Eq. (4). The bigrams and trigrams of classes are es-
timated based on 300 word-classes derived with the
same clustering algorithm as the one used to cluster
the phrases. The estimates of all the class ditribu-
tions are smoothed with the backoff technique like
in section 3. Linear interpolation weights between
the class and non-class models are estimated based
on Eq. (8) in the case of the bigram or trigram mod-
els, and on Eq.( 9) in the case of the bi-multigram
model.
Database The training and test data used to train
and evaluate the models are the same as the ones
described in Table 1. We use an additional set of
7350 sentences and 55000 word tokens to estimate
the interpolation weights of the interpolated models.
</bodyText>
<subsectionHeader confidence="0.573755">
4.2 Results
</subsectionHeader>
<bodyText confidence="0.998103826086957">
The perplexity scores obtained with the non-class,
class and interpolated versions of a bi-multigram
model (limiting to 2 words the size of a phrase),
and of the bigram and trigram models are in Ta-
ble 5. Linear interpolation with the class based mod-
els allows us to improve each model&apos;s performance
by about 2 points of perplexity: the Viterbi perplex-
ity score of the interpolated bi-multigrams (43.5) re-
mains intermediate between the bigram (54.7) and
trigram (38.6) scores. However in the trigram case,
the enhancement of the performance is obtained at
the expense of a great increase of the number of
entries in the interpolated model (139256 entries).
In the bi-multigram case, the augmentation of the
model size is much less (63972 entries). As a re-
sult, the interpolated bi-multigram model still has
fewer entries than the word based trigram model
(75511 entries), while its Viterbi perplexity score
comes even closer to the word trigram score (43.5
versus 40.4). Further experiments studying the in-
fluence of the threshold values and of the number
of classes still need to be performed to optimize the
performances for all models.
</bodyText>
<table confidence="0.9998451">
Test perplexity values PP*
non-class class interpolated
bigrams 56.04 66.3 54.7
bimultigrams 45.1 57.4 43.5
trigrams 40.4 49.3 38.6
Model size
non-class class interpolated
bigrams 32505 20471 52976
bimultigrams 42347 21625 63972
trigrams 75511 63745 139256
</table>
<tableCaption confidence="0.816940666666667">
Table 5: Comparison of class-phrase bi-multigrams
and of class-word bigrams and trigrams: Test per-
plexity values and model size.
</tableCaption>
<subsectionHeader confidence="0.998813">
4.3 Examples
</subsectionHeader>
<bodyText confidence="0.99995552">
Clustering variable-length phrases may provide a
natural way of dealing with some of the language dis-
fluencies which characterize spontaneous utterances,
like the insertion of hesitation words for instance. To
illustrate this point, examples of phrases which were
merged into a common cluster during the training
of a model allowing phrases of up to n = 5 words
are listed in Table 6 (the phrases containing the hes-
itation marker &amp;quot;uh*&amp;quot; are in the upper part of the
table). It is often the case that phrases differing
mainly because of a speaker hesitation are merged
together.
Table 6 also illustrates another motivation for phrase
retrieval and clustering, apart from word prediction,
which is to address issues related to topic identifica-
tion, dialogue modeling and language understand-
ing (Kawahara et al., 1997). Indeed, though the
clustered phrases in our experiments were derived
fully blindly, i.e. with no semantic/pragmatic in-
formation, intra-class phrases often display a strong
semantic correlation. To make this approach effec-
tively usable for speech understanding, constraints
derived from semantic or pragmatic knowledge (like
speech act tag of the utterance for instance) could
be placed on the phrase clustering process.
</bodyText>
<sectionHeader confidence="0.996909" genericHeader="method">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999964">
An algorithm to derive variable-length phrases as-
suming bigram dependencies between the phrases
has been proposed for a language modeling task. It
has been shown how a paradigmatic element could
</bodyText>
<page confidence="0.974126">
304
</page>
<equation confidence="0.969341098039216">
yesAhat_will ; *uh*_that_would
{ yesAhat_will_be ; *uh*_yesAhat&apos;s }
{ *uh*_by_the ; and_by_the
{ yes_*uh*_i ; i_see_i
{ okay_i_understand ; *uh*_yes_please }
{ could_you_recommend ; *uh*.isAhere }
{ *uh*_could_you_tell ; and_could_you_tell }
so_that_will ; yesAhat_will ; yes_that_would ;
uh*Ahat_would }
if_possible_i&apos;d_like ; we_would_like ; *uh*_i_want }
that_sounds_good ; *uh*i_understand }
{ *u}*_i_really ; *uh*_i_don&apos;t }
{ *uh*_i&apos;m_staying ; and_i&apos;m_staying }
{ all_right_we ; *uh*_yes_i }
{ good_morning ; good_afternoon ; hello }
{ sorry_to_keep_you_waiting ; hello_front_desk ;
thank_you_very_much ; thank_you_for_calling ;
you&apos;re_very_welcome ; yes_that&apos;s_correct ;
yes_that&apos;s_right }
non_smoking ; western_style ; first_class ;
japanese_style }
familiar_with ; in_charge_of }
could_you_tell_me ; do_you_know }
how_long ; how_much ; whatAime ;
uh*_whatAime ; *uh*_how_much ;
and_how_much ; and_what_time }
{ explain ; tell_us ; tell_rne ; tell_me_about ;
tell_me_what ; tell_me_how ; tell_me_how_much ;
tell_meAhe ; give_me ; give_me_the ;
give_me_your ; pleaseAell_me }
{ are_there ; areAhere_any ; if_there_are ;
if_there_is ; if_you_have ; if_there&apos;s ;
do_you_have ; do_you_have_a ; do_you_have_any ;
we_have_two ; isAhere ; is_there_any ;
isAhere_a ; is_there_anything ; *uh*_is_there ;
uh*_do_you_have }
{ tomorrow_morning ; nine_o&apos;clock ; eight_o&apos;clock
seven_o&apos;clock ; three_p.m. ; augustAenth ;
in_the_morning ; six_p.m. ; six_o&apos;clock
{ we&apos;d_like ; i&apos;d_like ; }
{ that&apos;ll_be_fine ; that&apos;s_fine ; i_understand }
kazuko_suzuki ; mary ; mary_phillips ;
thomas_nelson ; suzuki ; amy_harris ;
john ; john_phillips }
{ fine ; no_problem ; anything_else }
{ return_the_car ; pick_it_up }
{ todaiji ; kofukuji ; brooklyn ; enryakuji ;
hiroshima ; las_vegas ; salt_lake_city ; chicago ;
kinkakuji ; manhattan ; miami ; kyoto—station ;
this_hotel ; our_hotel ; your_hotel ;
the_airport ; the_hotel }
</equation>
<bodyText confidence="0.988897826086957">
Table 6: Example of phrases assigned to a common
cluster, with a model allowing up to 5-word phrases
(clusters are delimited with curly brackets)
be integrated within this framework, allowing to as-
sign common labels to phrases having a different
length. Experiments on a task oriented corpus have
shown that structuring sentences into phrases results
in large reductions in the bigram perplexity value,
while still keeping the number of entries in the lan-
guage model much lower than in a trigram model,
especially when these models are interpolated with
class based models. These results might be further
improved by finding a more efficient pruning strat-
egy, allowing the learning of even longer dependen-
cies without over-training, and by further experi-
menting with the class version of the phrase-based
model.
Additionally, the semantic relevance of the clusters
of phrases motivates the use of this approach in
the areas of dialogue modeling and language under-
standing. In that case, semantic/pragmatic infor-
mations could be used to constrain the clustering of
the phrases.
</bodyText>
<sectionHeader confidence="0.931392333333333" genericHeader="method">
Appendix: Forward-backward
algorithm for the estimation of the
bi-multigram parameters
</sectionHeader>
<bodyText confidence="0.9999494375">
Equation (4) can be implemented at a complexity of
0(n2T), with n the maximal length of a sequence
and T the number of words in the corpus, using a
forward-backward algorithm. Basically, it consists
in re-arranging the order of the summations of the
numerator and denominator of Eq. (4): the likeli-
hood values of all the segmentations where sequence
sj occurs after sequence si, with sequence si end-
ing at the word at rank (t) , are summed up first;
and then the summation is completed by summing
over i. The cumulated likelihood of all the segmen-
tations where 5j follows si, and si ends at (i), can be
directly computed as a product of a forward and of a
backward variable. The forward variable represents
the likelihood of the first t words, where the last
words are constrained to form a sequence:
</bodyText>
<equation confidence="0.795519">
&amp;(t, 12) = .C(IV8T1.) [Wg),+0])
</equation>
<bodyText confidence="0.999898666666667">
The backward variable represents the conditional
likelihood of the last (T — t) words, knowing that
they are preceded by the sequence [tv(j_/1+1)•..w(t));
</bodyText>
<equation confidence="0.984406">
/3(t,/j) = .C(W(7:f)i) I [W(()
</equation>
<bodyText confidence="0.999816333333333">
Assuming that the likelihood of a parse is computed
according to Eq. (2), then the reestimation equation
(4) can be rewritten as shown in Tab. 7.
The variables a and f3 can be calculated according
to the following recursion equations (assuming a
start and an end symbol at rank i = 0 and t = T + 1):
</bodyText>
<page confidence="0.987529">
305
</page>
<equation confidence="0.445624">
P(k+1)(5) Iso ET-1 nit, p(k)(si Isi) )3(1+ lj, 4) o,(t— 4+1) 45)(t +1)
Et a(t, 1i) i3(t, si(t + 1)
</equation>
<bodyText confidence="0.7145575">
and refer respectively to the lengths of the sequences si and si, and where the Kronecker function 4(0
equals 1 if the word sequence starting at rank t is sk, and equals 0 if not.
</bodyText>
<tableCaption confidence="0.861009">
Table 7: Forward-backward reestimation
</tableCaption>
<figure confidence="0.54684125">
for 1 &lt; t &lt; T + 1, and 1 &lt;I &lt;
12
a(t, 1) =E a(t— li,1) pqw((:) [w
1.1
</figure>
<equation confidence="0.966817833333333">
a(0,1) = 1, a(0,2) = = a(0, = O.
for 0 &lt; t &lt; T, and 1 &lt; /i &lt;n:
12
0(1,4)= E P({W((tt::j1I [W(tt—) t34.1))) P(t +1,1)
l=1
/9(T+ 1,1) = 1, + 1,2) = = irAT + 1,n) =0.
</equation>
<bodyText confidence="0.993893727272727">
In the case where the likelihood of a parse is
computed with the class assumption, i.e. ac-
cording to (6), the term p(k)(si Isi) in the
reestimation equation shown in Table 7 should
be replaced by its class equivalent, i.e. by
p(k)(Cg(s,) IC,700) p(k)(si I Cg(s,)). In the recursion
equation of a, the term p([W((:_)/,+1)]
is replaced by the corresponding class bigram prob-
ability multiplied by the class conditional prob-
ability of the sequence [W((:),,+1)]. A similar
change affects the recursion equation of /3, with
</bodyText>
<equation confidence="0.634288">
paw((tt++10)] [147((it)
</equation>
<bodyText confidence="0.95471925">
1,+1)]) being replaced by the cor-
responding class bigram probability multiplied by
the class conditional probability of the sequence
[n+1-
</bodyText>
<sectionHeader confidence="0.998775" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.993635428571429">
F. Bimbot, R. Pieraccini, E. Levin, and B. Atal.
1995. Variable-length sequence modeling: Multi-
grams. IEEE Signal Processing Letters, 2(6),
June.
P.F. Brown, V.J. Della Pietra, P.V. de Souza, J.C.
Lai, and R.L. Mercer. 1992. Class-based n-gram
models of natural language. Computational Lin-
guistics, 18(4):467-479.
P. Clarkson and R. Rosenfeld. 1997. Statistical lan-
guage modeling using the cmu-cambridge toolkit.
Proceedings of EUROSPEECH 97.
S. Deligne and F. Bimbot. 1995. Language modeling
by variable length sequences: theoretical formula-
tion and evaluation of multigrams. Proceedings of
ICASSP 95.
S. Deligne, F. Yvon, and F. Bimbot. 1996. In-
troducing statistical dependencies and structural
constraints in variable-length sequence models. In
Grammatical Inference : Learning Syntax from
Sentences, Lecture Notes in Artificial Intelligence
1147, pages 156-167. Springer.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum-likelihood from incomplete data via the
EM algorithm. Journal of the Royal Statistics So-
ciety, 39(1):1-38.
F. Jelinek and R.L. Mercer. 1980. Interpolated esti-
mation of markov source parameters from sparse
data. Proceedings of the workshop on Pattern
Recognition in Practice, pages 381-397.
S. M. Katz. 1987. Estimation of probabilities from
sparse data for the language model component
of a speech recognizer. IEEE Trans. on Acous-
tic, Speech, and Signal Processing, 35(3):400-401,
March.
T. Kawahara, S. Doshita, and C. H. Lee.
1997. Phrase language models for detection and
verification-based speech understanding. Proceed-
ings of the 1997 IEEE workshop on Automatic
Speech Recognition and Understanding, pages 49-
56, December.
H. Masataki and Y. Sagisaka. 1996. Variable-
order n-gram generation by word-class splitting
and consecutive word grouping. Proceedings of
ICASSP 96.
S. Matsunaga and S. Sagayama. 1997. Variable-
length language modeling integrating global con-
straints. Proceedings of EUROSPEECH 97.
K. Ries, F. D. Buo, and A. Waibel. 1996. Class
phrase models for language modeling. Proceedings
of ICSLP 96.
M. Siu. 1998. Learning local lexical structure in
spontaneous speech language modeling. Ph.D. the-
sis, Boston University.
B. Suhm and A. Waibel. 1994. Towards better lan-
guage models for spontaneous speech. Proceedings
of ICSLP 94.
</reference>
<bodyText confidence="0.8039852">
I.H. Witten and T.C. Bell. 1991. The zero-frequency
problem: estimating the probabilities of novel
events in adaptative text compression. IEEE
Trans. on Information Theory, 37(4):1085-1094,
July.
</bodyText>
<page confidence="0.998709">
306
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.128117">
<title confidence="0.7106135">Learning a syntagmatic and paradigmatic structure from language data with a bi-multigram model</title>
<author confidence="0.413965">Deligne Sagisaka</author>
<email confidence="0.40581">deptl,Hikaridai</email>
<note confidence="0.65257">Seika cho, Soraku gun, Kyoto fu 619-0224, Japan.</note>
<abstract confidence="0.9890155">In this paper, we present a stochastic language modeling tool which aims at retrieving variable-length phrases (multigrams), assuming bigram dependencies between them. The phrase retrieval can be intermixed with a phrase clustering procedure, so that the language data are iteratively structured at both a paradigmatic and a syntagmatic level in a fully integrated way. Perplexity results on ATR travel arrangement data with a bi-multigram model (assuming bigram correlations between the phrases) come very close to the trigram scores with a reduced number of entries in the language model. Also the ability of the class version of the model to merge semantically related phrases into a common class is illustrated.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>F Bimbot</author>
<author>R Pieraccini</author>
<author>E Levin</author>
<author>B Atal</author>
</authors>
<title>Variable-length sequence modeling: Multigrams.</title>
<date>1995</date>
<journal>IEEE Signal Processing Letters,</journal>
<volume>2</volume>
<issue>6</issue>
<contexts>
<context position="3024" citStr="Bimbot et al., 1995" startWordPosition="473" endWordPosition="476">stic framework allows EM principled optimization procedures, but it is prone to overlearning. The other criteria tend to reduce the risk of overlearning, but their optimization relies on heuristic procedures (e.g. word grouping via a greedy algorithm (Matsunaga and Sagayama, 1997)) for which convergence and optimality are not theoretically guaranteed. The work reported in this paper is based on the multigram model, which is a stochastic phrase-based model, the parameters of which are estimated according to a likelihood criterion using an EM procedure. The multigram approach was introduced in (Bimbot et al., 1995), and in (Deligne and Bimbot, 1995) it was used to derive variable-length phrases under the assumption of independence of the phrases. Various ways of theoretically releasing this assumption were given in (Deligne et al., 1996). More recently, experiments with 2-word multigrams embedded in a deterministic variable ngram scheme were reported in (Siu, 1998). In section 2 of this paper, we further formulate a model with bigram (more generally IT-gram) dependencies between the phrases, by including a paradigmatic aspect which enables the clustering of variable-length phrases. It results in a stoch</context>
</contexts>
<marker>Bimbot, Pieraccini, Levin, Atal, 1995</marker>
<rawString>F. Bimbot, R. Pieraccini, E. Levin, and B. Atal. 1995. Variable-length sequence modeling: Multigrams. IEEE Signal Processing Letters, 2(6), June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>V J Della Pietra</author>
<author>P V de Souza</author>
<author>J C Lai</author>
<author>R L Mercer</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<pages>18--4</pages>
<marker>Brown, Pietra, de Souza, Lai, Mercer, 1992</marker>
<rawString>P.F. Brown, V.J. Della Pietra, P.V. de Souza, J.C. Lai, and R.L. Mercer. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18(4):467-479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Clarkson</author>
<author>R Rosenfeld</author>
</authors>
<title>Statistical language modeling using the cmu-cambridge toolkit.</title>
<date>1997</date>
<booktitle>Proceedings of EUROSPEECH 97.</booktitle>
<contexts>
<context position="13461" citStr="Clarkson and Rosenfeld, 1997" startWordPosition="2260" endWordPosition="2263">oximated with the likelihood of the best parse, as is done in a speech recognizer. Tc1.03 A (k+1) 302 In section 3.2, we first evaluate the loss (PP* -PP) using the forward-backward estimation procedure, and then we study the influence of the estimation procedure itself, i.e. Eq. (4) or (5), in terms of perplexity and model size (number of distinct 2-uplets of phrases in the model). Finally, we compare these results with the ones obtained with conventional ngram models (the model size is thus the number of distinct n-uplets of words observed), using for this purpose the CMU-Cambridge toolkit (Clarkson and Rosenfeld, 1997). Training protocol Experiments are reported for phrases having at most n = 1, 2, 3 or 4 words (for n =1, bi-multigrams correspond to conventional bigrams). The bi-multigram probabilities are initialized using the relative frequencies of all the 2-uplets of phrases observed in the training corpus, and they are reestimated with 6 iterations. The dictionaries of phrases are pruned by discarding all phrases occuring less than 20 times at initialization, and less than 10 times after each iteration2, except for the 1-word phrases which are kept with a number of occurrences set to 1. Besides, bi-mul</context>
</contexts>
<marker>Clarkson, Rosenfeld, 1997</marker>
<rawString>P. Clarkson and R. Rosenfeld. 1997. Statistical language modeling using the cmu-cambridge toolkit. Proceedings of EUROSPEECH 97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Deligne</author>
<author>F Bimbot</author>
</authors>
<title>Language modeling by variable length sequences: theoretical formulation and evaluation of multigrams.</title>
<date>1995</date>
<booktitle>Proceedings of ICASSP 95.</booktitle>
<contexts>
<context position="3059" citStr="Deligne and Bimbot, 1995" startWordPosition="479" endWordPosition="482">ipled optimization procedures, but it is prone to overlearning. The other criteria tend to reduce the risk of overlearning, but their optimization relies on heuristic procedures (e.g. word grouping via a greedy algorithm (Matsunaga and Sagayama, 1997)) for which convergence and optimality are not theoretically guaranteed. The work reported in this paper is based on the multigram model, which is a stochastic phrase-based model, the parameters of which are estimated according to a likelihood criterion using an EM procedure. The multigram approach was introduced in (Bimbot et al., 1995), and in (Deligne and Bimbot, 1995) it was used to derive variable-length phrases under the assumption of independence of the phrases. Various ways of theoretically releasing this assumption were given in (Deligne et al., 1996). More recently, experiments with 2-word multigrams embedded in a deterministic variable ngram scheme were reported in (Siu, 1998). In section 2 of this paper, we further formulate a model with bigram (more generally IT-gram) dependencies between the phrases, by including a paradigmatic aspect which enables the clustering of variable-length phrases. It results in a stochastic class-phrase model, which can</context>
</contexts>
<marker>Deligne, Bimbot, 1995</marker>
<rawString>S. Deligne and F. Bimbot. 1995. Language modeling by variable length sequences: theoretical formulation and evaluation of multigrams. Proceedings of ICASSP 95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Deligne</author>
<author>F Yvon</author>
<author>F Bimbot</author>
</authors>
<title>Introducing statistical dependencies and structural constraints in variable-length sequence models.</title>
<date>1996</date>
<booktitle>In Grammatical Inference : Learning Syntax from Sentences, Lecture Notes in Artificial Intelligence 1147,</booktitle>
<pages>156--167</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="3251" citStr="Deligne et al., 1996" startWordPosition="509" endWordPosition="512">g via a greedy algorithm (Matsunaga and Sagayama, 1997)) for which convergence and optimality are not theoretically guaranteed. The work reported in this paper is based on the multigram model, which is a stochastic phrase-based model, the parameters of which are estimated according to a likelihood criterion using an EM procedure. The multigram approach was introduced in (Bimbot et al., 1995), and in (Deligne and Bimbot, 1995) it was used to derive variable-length phrases under the assumption of independence of the phrases. Various ways of theoretically releasing this assumption were given in (Deligne et al., 1996). More recently, experiments with 2-word multigrams embedded in a deterministic variable ngram scheme were reported in (Siu, 1998). In section 2 of this paper, we further formulate a model with bigram (more generally IT-gram) dependencies between the phrases, by including a paradigmatic aspect which enables the clustering of variable-length phrases. It results in a stochastic class-phrase model, which can be interpolated with the stochastic phrase model, in a similar way to deterministic approaches. In section 3 and 4, the phrase and class-phrase models are evaluated in terms of perplexity val</context>
</contexts>
<marker>Deligne, Yvon, Bimbot, 1996</marker>
<rawString>S. Deligne, F. Yvon, and F. Bimbot. 1996. Introducing statistical dependencies and structural constraints in variable-length sequence models. In Grammatical Inference : Learning Syntax from Sentences, Lecture Notes in Artificial Intelligence 1147, pages 156-167. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum-likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistics Society,</journal>
<pages>39--1</pages>
<contexts>
<context position="5695" citStr="Dempster et al., 1977" startWordPosition="935" endWordPosition="938">of words, and {S} the set of possible segmentations on W. The likelihood of W is: C(W)= E r(W,S) (1) SE{S} and the likelihood of a segmentation S of W is: ,C (W,S) = JJ P(S(r) I S(r—fil-1) • • S(r-1)) (2) with 8(,) denoting the phrase of rank (r) in the segmentation S. The model is thus fully defined by the set of W-gram probabilities on the set {si}; of all the phrases which can be formed by combining 1, 2, ...up to n words of the vocabulary. Maximum likelihood (ML) estimates of these probabilities can be obtained by formulating the estimation problem as a ML estimation from incomplete data (Dempster et al., 1977), where the unknown data is the underlying segmentation S. Let Q(k, k+ 1) be the following auxiliary function computed with the likelihoods of iterations k and k + 1: Q(k,k +1) E ek)(s I W) log ek+1)(W, S) SE{S) (3) It has been shown in (Dempster et al., 1977) that if Q(k,k +1) &gt; Q(k,k), then ek+1)(W) &gt; ek)(W). Therefore the reestimation equation of p(si, si, siw_i), at iteration (k + 1), can be derived by maximizing Q(k, k + 1) over the set of parameters of iteration (k + 1), under the set of constraints E,._ p(si, I si, = 1, hence: P(k-1-1)(siw si, = EsE(s) c(sii • • •si_,si,, s) x .0)(s 1 1</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum-likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistics Society, 39(1):1-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
<author>R L Mercer</author>
</authors>
<title>Interpolated estimation of markov source parameters from sparse data.</title>
<date>1980</date>
<booktitle>Proceedings of the workshop on Pattern Recognition in Practice,</booktitle>
<pages>381--397</pages>
<contexts>
<context position="10656" citStr="Jelinek and Mercer, 1980" startWordPosition="1769" endWordPosition="1772">d robustness of the estimates of the class distribution, class based models can be expected to deteriorate the likelihood of not only train but also test data, with respect to non-class based models. However, the performance of non-class models can be enhanced by interpolating their estimates with the class estimates. We first recall the way linear interpolation is performed with conventional word ngram models, and then we extend it to the case of our stochastic phrase-based approach. Usually, linear interpolation weights are computed so as to maximize the likelihood of cross evaluation data (Jelinek and Mercer, 1980). Denoting by A and (1 — A) the interpolation weights, and by p+ the interpolated estimate, it comes for a word bigram model: P+(wi I wi) = A P(wi I wi) + (1—)) P(C9(wi) I Cq(wo)P(wi I Cetvi)) (7) with A having been iteratively estimated on a cross evaluation corpus W„o„ as: A(k+1) = 1 A(k) P(w.i wi) c(w,wi) „ (8) 1,3 P+ki(wi wi) where T„„, is the number of words inardsnd c(wiwi) the number of co-occurences of the words ow wi and wi in Wcross • In the case of a stochastic phrase based model - where the segmentation into phrases is not known a priori - the above computation of the interpolation</context>
</contexts>
<marker>Jelinek, Mercer, 1980</marker>
<rawString>F. Jelinek and R.L. Mercer. 1980. Interpolated estimation of markov source parameters from sparse data. Proceedings of the workshop on Pattern Recognition in Practice, pages 381-397.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Katz</author>
</authors>
<title>Estimation of probabilities from sparse data for the language model component of a speech recognizer.</title>
<date>1987</date>
<journal>IEEE Trans. on Acoustic, Speech, and Signal Processing,</journal>
<pages>35--3</pages>
<contexts>
<context position="14155" citStr="Katz, 1987" startWordPosition="2374" endWordPosition="2375"> or 4 words (for n =1, bi-multigrams correspond to conventional bigrams). The bi-multigram probabilities are initialized using the relative frequencies of all the 2-uplets of phrases observed in the training corpus, and they are reestimated with 6 iterations. The dictionaries of phrases are pruned by discarding all phrases occuring less than 20 times at initialization, and less than 10 times after each iteration2, except for the 1-word phrases which are kept with a number of occurrences set to 1. Besides, bi-multigram and n-gram probabilities are smoothed with the backoff smoothing technique (Katz, 1987) using Witten-Bell discounting (Witten and Bell, 1991)3. Database Experiments are run on ATR travel arrangement data (see Tab. 1). This database consists of semi-spontaneous dialogues between a hotel clerk and a customer asking for travel/accomodation informations. All hesitation words and false starts were mapped to a single marker &amp;quot;*uh*&amp;quot;. Train test Nb sentences 13 650 2 430 Nb tokens 167 000 29 000 (1 % 00V) Vocabulary 3 525 + 280 00V Table 1: ATR Travel Arrangement Data 3.2 Results Ambiguity on a parse (Table 2) The difference (PP* -PP) usually remains within about 1 point of perplexity, m</context>
</contexts>
<marker>Katz, 1987</marker>
<rawString>S. M. Katz. 1987. Estimation of probabilities from sparse data for the language model component of a speech recognizer. IEEE Trans. on Acoustic, Speech, and Signal Processing, 35(3):400-401, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kawahara</author>
<author>S Doshita</author>
<author>C H Lee</author>
</authors>
<title>Phrase language models for detection and verification-based speech understanding.</title>
<date>1997</date>
<booktitle>Proceedings of the 1997 IEEE workshop on Automatic Speech Recognition and Understanding,</booktitle>
<pages>49--56</pages>
<contexts>
<context position="21317" citStr="Kawahara et al., 1997" startWordPosition="3568" endWordPosition="3571"> for instance. To illustrate this point, examples of phrases which were merged into a common cluster during the training of a model allowing phrases of up to n = 5 words are listed in Table 6 (the phrases containing the hesitation marker &amp;quot;uh*&amp;quot; are in the upper part of the table). It is often the case that phrases differing mainly because of a speaker hesitation are merged together. Table 6 also illustrates another motivation for phrase retrieval and clustering, apart from word prediction, which is to address issues related to topic identification, dialogue modeling and language understanding (Kawahara et al., 1997). Indeed, though the clustered phrases in our experiments were derived fully blindly, i.e. with no semantic/pragmatic information, intra-class phrases often display a strong semantic correlation. To make this approach effectively usable for speech understanding, constraints derived from semantic or pragmatic knowledge (like speech act tag of the utterance for instance) could be placed on the phrase clustering process. 5 Conclusion An algorithm to derive variable-length phrases assuming bigram dependencies between the phrases has been proposed for a language modeling task. It has been shown how</context>
</contexts>
<marker>Kawahara, Doshita, Lee, 1997</marker>
<rawString>T. Kawahara, S. Doshita, and C. H. Lee. 1997. Phrase language models for detection and verification-based speech understanding. Proceedings of the 1997 IEEE workshop on Automatic Speech Recognition and Understanding, pages 49-56, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Masataki</author>
<author>Y Sagisaka</author>
</authors>
<title>Variableorder n-gram generation by word-class splitting and consecutive word grouping.</title>
<date>1996</date>
<booktitle>Proceedings of ICASSP 96.</booktitle>
<contexts>
<context position="2355" citStr="Masataki and Sagisaka, 1996" startWordPosition="369" endWordPosition="372">tence into phrases. In practice, it means that even if phrase abc is registered as a phrase, the possibility of parsing the string as, for instance, [ab] [c] still remains. By contrast, in a deterministic approach, all co-occurences of a, b and c would be systematically interpreted as an occurence of phrase [abc]. Various criteria have been proposed to derive phrases in a purely statistical way 1; data likeliI i.e. without using grammar rules like in Stochastic Context Free Grammars. hood, leaving-one-out likelihood (Ries et al., 1996), mutual information (Suhm and Waibel, 1994), and entropy (Masataki and Sagisaka, 1996). The use of the likelihood criterion in a stochastic framework allows EM principled optimization procedures, but it is prone to overlearning. The other criteria tend to reduce the risk of overlearning, but their optimization relies on heuristic procedures (e.g. word grouping via a greedy algorithm (Matsunaga and Sagayama, 1997)) for which convergence and optimality are not theoretically guaranteed. The work reported in this paper is based on the multigram model, which is a stochastic phrase-based model, the parameters of which are estimated according to a likelihood criterion using an EM proc</context>
</contexts>
<marker>Masataki, Sagisaka, 1996</marker>
<rawString>H. Masataki and Y. Sagisaka. 1996. Variableorder n-gram generation by word-class splitting and consecutive word grouping. Proceedings of ICASSP 96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Matsunaga</author>
<author>S Sagayama</author>
</authors>
<title>Variablelength language modeling integrating global constraints.</title>
<date>1997</date>
<booktitle>Proceedings of EUROSPEECH 97.</booktitle>
<contexts>
<context position="2685" citStr="Matsunaga and Sagayama, 1997" startWordPosition="420" endWordPosition="423">ria have been proposed to derive phrases in a purely statistical way 1; data likeliI i.e. without using grammar rules like in Stochastic Context Free Grammars. hood, leaving-one-out likelihood (Ries et al., 1996), mutual information (Suhm and Waibel, 1994), and entropy (Masataki and Sagisaka, 1996). The use of the likelihood criterion in a stochastic framework allows EM principled optimization procedures, but it is prone to overlearning. The other criteria tend to reduce the risk of overlearning, but their optimization relies on heuristic procedures (e.g. word grouping via a greedy algorithm (Matsunaga and Sagayama, 1997)) for which convergence and optimality are not theoretically guaranteed. The work reported in this paper is based on the multigram model, which is a stochastic phrase-based model, the parameters of which are estimated according to a likelihood criterion using an EM procedure. The multigram approach was introduced in (Bimbot et al., 1995), and in (Deligne and Bimbot, 1995) it was used to derive variable-length phrases under the assumption of independence of the phrases. Various ways of theoretically releasing this assumption were given in (Deligne et al., 1996). More recently, experiments with </context>
</contexts>
<marker>Matsunaga, Sagayama, 1997</marker>
<rawString>S. Matsunaga and S. Sagayama. 1997. Variablelength language modeling integrating global constraints. Proceedings of EUROSPEECH 97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Ries</author>
<author>F D Buo</author>
<author>A Waibel</author>
</authors>
<title>Class phrase models for language modeling.</title>
<date>1996</date>
<booktitle>Proceedings of ICSLP 96.</booktitle>
<contexts>
<context position="2268" citStr="Ries et al., 1996" startWordPosition="357" endWordPosition="360">ework, non determinism is introduced via an ambiguity on the parse of the sentence into phrases. In practice, it means that even if phrase abc is registered as a phrase, the possibility of parsing the string as, for instance, [ab] [c] still remains. By contrast, in a deterministic approach, all co-occurences of a, b and c would be systematically interpreted as an occurence of phrase [abc]. Various criteria have been proposed to derive phrases in a purely statistical way 1; data likeliI i.e. without using grammar rules like in Stochastic Context Free Grammars. hood, leaving-one-out likelihood (Ries et al., 1996), mutual information (Suhm and Waibel, 1994), and entropy (Masataki and Sagisaka, 1996). The use of the likelihood criterion in a stochastic framework allows EM principled optimization procedures, but it is prone to overlearning. The other criteria tend to reduce the risk of overlearning, but their optimization relies on heuristic procedures (e.g. word grouping via a greedy algorithm (Matsunaga and Sagayama, 1997)) for which convergence and optimality are not theoretically guaranteed. The work reported in this paper is based on the multigram model, which is a stochastic phrase-based model, the</context>
<context position="7113" citStr="Ries et al., 1996" startWordPosition="1190" endWordPosition="1193">ted by means of a forward-backward algorithm, such as the one described for bi-multigrams (Ti = 2) in the appendix of this paper. In a decision-oriented scheme, the reestimation equation reduces to: p(k+1)(siz sil ...siw_i) = c(sii . . . , c(si, S*(k)) (5) the segmentation maximizing where is retrieved with a Viterbi algo1 W), rithm. Since each iteration improves the model in the sense of increasing the likelihood ek)(W), it eventually converges to a critical point (possibly a local maximum). 2.2 Variable-length phrase clustering Recently, class-phrase based models have gained some attention (Ries et al., 1996), but usually it assumes a previous clustering of the words. Typically, each word is first assigned a word-class label &amp;quot;&lt; Ck &gt;&amp;quot;, then variable-length phrases [Ck1Ck2-.Ck,] of word-class labels are retrieved, each of which leads to define a phrase-class label which can be denoted as &amp;quot;&lt; &gt;&amp;quot;. But in this approach only phrases of the same length can be assigned the same phrase-class label. For instance, the phrases &amp;quot;thank you for&amp;quot; and &amp;quot;thank you very much for&amp;quot; cannot be assigned the same class label. We propose to address this limitation by directly clustering phrases instead of words. For this pur</context>
</contexts>
<marker>Ries, Buo, Waibel, 1996</marker>
<rawString>K. Ries, F. D. Buo, and A. Waibel. 1996. Class phrase models for language modeling. Proceedings of ICSLP 96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Siu</author>
</authors>
<title>Learning local lexical structure in spontaneous speech language modeling.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>Boston University.</institution>
<contexts>
<context position="3381" citStr="Siu, 1998" startWordPosition="530" endWordPosition="531">rted in this paper is based on the multigram model, which is a stochastic phrase-based model, the parameters of which are estimated according to a likelihood criterion using an EM procedure. The multigram approach was introduced in (Bimbot et al., 1995), and in (Deligne and Bimbot, 1995) it was used to derive variable-length phrases under the assumption of independence of the phrases. Various ways of theoretically releasing this assumption were given in (Deligne et al., 1996). More recently, experiments with 2-word multigrams embedded in a deterministic variable ngram scheme were reported in (Siu, 1998). In section 2 of this paper, we further formulate a model with bigram (more generally IT-gram) dependencies between the phrases, by including a paradigmatic aspect which enables the clustering of variable-length phrases. It results in a stochastic class-phrase model, which can be interpolated with the stochastic phrase model, in a similar way to deterministic approaches. In section 3 and 4, the phrase and class-phrase models are evaluated in terms of perplexity values and model size. 2 Theoretical formulation of the multigrams 2.1 Variable-length phrase distribution In the multigram framework</context>
</contexts>
<marker>Siu, 1998</marker>
<rawString>M. Siu. 1998. Learning local lexical structure in spontaneous speech language modeling. Ph.D. thesis, Boston University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Suhm</author>
<author>A Waibel</author>
</authors>
<title>Towards better language models for spontaneous speech.</title>
<date>1994</date>
<booktitle>Proceedings of ICSLP 94.</booktitle>
<contexts>
<context position="2312" citStr="Suhm and Waibel, 1994" startWordPosition="363" endWordPosition="366"> an ambiguity on the parse of the sentence into phrases. In practice, it means that even if phrase abc is registered as a phrase, the possibility of parsing the string as, for instance, [ab] [c] still remains. By contrast, in a deterministic approach, all co-occurences of a, b and c would be systematically interpreted as an occurence of phrase [abc]. Various criteria have been proposed to derive phrases in a purely statistical way 1; data likeliI i.e. without using grammar rules like in Stochastic Context Free Grammars. hood, leaving-one-out likelihood (Ries et al., 1996), mutual information (Suhm and Waibel, 1994), and entropy (Masataki and Sagisaka, 1996). The use of the likelihood criterion in a stochastic framework allows EM principled optimization procedures, but it is prone to overlearning. The other criteria tend to reduce the risk of overlearning, but their optimization relies on heuristic procedures (e.g. word grouping via a greedy algorithm (Matsunaga and Sagayama, 1997)) for which convergence and optimality are not theoretically guaranteed. The work reported in this paper is based on the multigram model, which is a stochastic phrase-based model, the parameters of which are estimated according</context>
</contexts>
<marker>Suhm, Waibel, 1994</marker>
<rawString>B. Suhm and A. Waibel. 1994. Towards better language models for spontaneous speech. Proceedings of ICSLP 94.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>