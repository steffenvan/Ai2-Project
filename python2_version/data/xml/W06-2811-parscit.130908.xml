<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.038628">
<title confidence="0.717248">
Multilingual interactive experiments with Flickr
</title>
<figure confidence="0.8928812">
Julio Gonzalo
Departamento de Lenguajes
y Sistemas Inform´aticos
UNED
Madrid, Spain
u &apos;ulio@lsi.uned.es
Jussi Karlgren
Swedish Institute of
Computer Science
Stockholm
Sweden
jussi@sics.se
Paul Clough
Department of
Information Studies
</figure>
<affiliation confidence="0.805194">
University of Sheffield
</affiliation>
<address confidence="0.520273">
Sheffield, UK
</address>
<email confidence="0.907588">
p.d.clough@sheffield.ac. k�
</email>
<sectionHeader confidence="0.991253" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999857636363636">
This paper presents a proposal for iCLEF
2006, the interactive track of the CLEF
cross-language evaluation campaign. In
the past, iCLEF has addressed applications
such as information retrieval and ques-
tion answering. However, for 2006 the
focus has turned to text-based image re-
trieval from Flickr. We describe Flickr, the
challenges this kind of collection presents
to cross-language researchers, and suggest
initial iCLEF tasks.
</bodyText>
<sectionHeader confidence="0.7777765" genericHeader="method">
1 Information Retrieval Evaluation by
User Experiment
</sectionHeader>
<bodyText confidence="0.997562947368421">
Information retrieval systems, especially text re-
trieval systems, have benefited greatly from a
fairly strict and straight-laced evaluation scheme,
which enables system designers to run tests on
versions of their system using a test collection of
pre-assessed data. These relevance-oriented ex-
periments shed light on comparative system per-
formance and enable both introduction of new al-
gorithms and incremental optimization. However,
batch-oriented system evaluation based on large
amounts of data, abstracted away from situational
constraints, variation in usage, and interactiveness
issues only addresses some of the bottlenecks to
build a successful system.
The CLEF1 Interactive Track (iCLEF2) is de-
voted to the comparative study of user inclusive
cross-language search strategies. Over the past
5 years, iCLEF has studied three cross-language
search tasks: retrieval of documents, answers and
</bodyText>
<footnote confidence="0.9998945">
1http://www.clef-campaign.org/
2http://nlp.uned.es/iCLEF/
</footnote>
<bodyText confidence="0.987895904761905">
annotated images (Gonzalo and Oard, 2002; Gon-
zalo et al., 2005). All tasks involve the user in-
teracting with information systems in a language
different from that of the document collection.
Although iCLEF experiments continue produc-
ing interesting research results, which may have
a substantial impact on the way effective cross-
language search assistants are built, participation
in this track has remained low across the five years
of existence of the track. Interactive studies, how-
ever, remain as a recognized necessity in most
CLEF tracks.
Therefore, to encourage greater participation in
2006 our focus has turned to FLICKR3, a large-
scale, web-based image database with the poten-
tial for offering both challenging and realistic mul-
tilingual search tasks for interactive experiments.
Our aim in selecting a primarily non-textual tar-
get to study textual retrieval is based on some of
the multi-lingual and dynamic characteristics of
FLICKR. We will outline them below.
</bodyText>
<sectionHeader confidence="0.850391" genericHeader="method">
2 The Flickr system
</sectionHeader>
<bodyText confidence="0.998635363636364">
The majority of Web image search is text-based
and the success of such approaches often de-
pends on reliably identifying relevant text associ-
ated with a particular image. FLICKR is an on-
line tool for managing and sharing personal pho-
tographs and currently contains over five million
freely accessible images. These are available via
the web, updated daily by a large number of users
and available to all web users (users can access
FLICKR for free, although limited to the upload of
20MB of photos per month).
</bodyText>
<footnote confidence="0.965007">
3http://www.flickr.com/
</footnote>
<page confidence="0.991467">
70
</page>
<subsectionHeader confidence="0.837944">
2.1 Photographs in the collection
</subsectionHeader>
<bodyText confidence="0.99987175">
It is estimated that the complete FLICKR database
contains 37 million photos with approximately
200,000 images added daily by 1.2 million mem-
bers4. FLICKR provides both private and pub-
lic image storage, and photos which are shared
(around 5 million) can be protected under a Cre-
ative Commons (CC) licensing5 agreement (an al-
ternative to full copyright). Images from a wide
variety of topics can be accessed through FLICKR,
including people, places, landscapes, objects, ani-
mals and events. This makes the collection a rich
resource for image retrieval research.
</bodyText>
<subsectionHeader confidence="0.990977">
2.2 Annotations
</subsectionHeader>
<bodyText confidence="0.999982666666667">
In FLICKR, photos are annotated by authors with
freely chosen keywords in a naturally multilingual
manner: most authors use keywords in their native
language; some combine more than one language.
In addition, photographs have titles, descriptions,
collaborative annotations, and comments in many
languages. Figure 5 provides an example photo
with multilingual annotations; Figure 5 shows
what the query “cats” retrieves from the database,
compared with what the query “chats” retrieves.
Annotations are used by the authors to organize
their images, and by any user to search on. Key-
words assigned to the images can include place
names and subject matter, and photos can also
be submitted to online discussion groups. This
provides additional metadata to the image which
can also be used for retrieval. An explore util-
ity provided by FLICKR makes use of this user-
generated data (plus other information such as
Clickthroughs) to define an ”interestingness” view
of images6.
</bodyText>
<sectionHeader confidence="0.984438" genericHeader="method">
3 Flickr at iCLEF 2006
</sectionHeader>
<bodyText confidence="0.999868111111111">
Many images are accompanied by text, enabling
the use of both text and visual features for image
retrieval and its evaluation (M¨uller et al., 2006,
see e.g.). Images are naturally language indepen-
dent and often successfully retrieved with asso-
ciated texts. This has been explored as part of
ImageCLEF (Clough et al., 2005) for areas such
as information access to medical images and his-
toric photographs. The way in which users search
</bodyText>
<footnote confidence="0.9993644">
4These figures are accurate as of October 2005:
http://www.wired.com/news/ebiz/0,1272,68654,00.html
5http://creativecommons.org/image/flickr,
http://flickr.com/creativecommons/
6http://www.flickr.com/explore/interesting
</footnote>
<bodyText confidence="0.9969636">
for images provides an interesting application for
user-centered design and evaluation. As an iCLEF
task, searching for images from FLICKR presents
a new multilingual challenge which, to date, has
not been explored. Challenges include:
</bodyText>
<listItem confidence="0.998365875">
• Different types of associated text, e.g. key-
words, titles, comments and description
fields.
• Collective classification and annotation us-
ing freely selected keywords (known as folk-
sonomies) resulting in non-uniform and sub-
jective categorization of images.
• Annotations in multiple languages.
</listItem>
<bodyText confidence="0.9998628">
Given the multilingual nature of the FLICKR
annotations, translating the user’s search request
would provide the opportunity of increasing the
number of images found and make more of the
collection accessible to a wider range of users
regardless of their language skills. The aim of
iCLEF using FLICKR will be to determine how
cross-language technologies could enhance ac-
cess, and explore the user interaction resulting
from this.
</bodyText>
<sectionHeader confidence="0.976262" genericHeader="method">
4 Proposed tasks
</sectionHeader>
<bodyText confidence="0.999613">
For iCLEF, participants to this evaluation cam-
paign will be provided with the following:
</bodyText>
<listItem confidence="0.999892142857143">
• A subset of the Flickr collection including an-
notations and photographs7.
• Example (realistic) search tasks. Ideally
these search tasks will reflect real user needs
which could be derived from log files, studies
or similar retrieval tasks.
• A framework in which to run an evaluation.
</listItem>
<sectionHeader confidence="0.9975" genericHeader="conclusions">
5 Summary
</sectionHeader>
<bodyText confidence="0.999144714285714">
Flickr will allow us to create an extremely in-
teresting interactive task based on truly hetero-
geneous annotations (that will in turn hopefully
attract more participants). Using images from
within a Web environment is a realistic and con-
temporary search challenge and allows many im-
portant research questions to be addressed from
</bodyText>
<footnote confidence="0.756853333333333">
7We are currently in negotiations with Yahoo! (owners
of Flickr) and Flickr to provide researchers with legitimate
access to a subset of the collection.
</footnote>
<page confidence="0.997779">
71
</page>
<bodyText confidence="0.999901888888889">
a quickly developing field. User-centered studies
are required within both text and image retrieval,
but are often neglected as they require more effort
and time from participating groups than a system-
centered comparison that can often be run with-
out human intervention. Still, user-centered eval-
uation cannot be replaced and the influence of the
user on the results is in general stronger than the
influence of the system itself.
</bodyText>
<sectionHeader confidence="0.998479" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996481230769231">
Paul Clough, Henning Miller, and Mark Sanderson.
2005. The clef 2004 cross language image retrieval
track. In Carol Peters, Paul Clough, Julio Gon-
zalo, Gareth Jones, Michael Kluck, and Bernardo
Magnini, editors, Multilingual Information Access
for Text, Speech and Images: Results of the Fifth
CLEF Evaluation Campaign, number 3491/2005 in
Lecture Notes in Computer Science, pages 597–613.
Springer, Heidelberg, Germany.
Julio Gonzalo and Doug Oard. 2002. The clef
2002 interactive track. In Advances in Cross-
Language Information Retrieval, number 2785 in
Lecture Notes in Computer Science. Springer-
Verlag, Berlin-Heidelberg-New York.
Julio Gonzalo, Paul Clough, and A Vallin. 2005.
Overview of the clef 2005 interactive track. In
Working notes of the CLEF workshop, Vienna, Aus-
tria, September.
Henning Miller, Paul Clough, William Hersh, Thomas
Deselaers, Thomas Lehmann, and Antoine Geiss-
buhler. 2006. Using heterogeneous annotation and
visual information for the benchmarking of image
retrieval systems. In SPIE conference Photonics
West, Electronic Imaging, special session on bench-
marking image retrieval systems, San Diego, Febru-
ary.
</reference>
<page confidence="0.998329">
72
</page>
<figureCaption confidence="0.9999875">
Figure 1: Example multilingual annotations in Flickr.
Figure 2: Retrieval of “cats” (left) and “chats” (right).
</figureCaption>
<page confidence="0.997199">
73
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.095642">
<title confidence="0.999503">Multilingual interactive experiments with Flickr</title>
<author confidence="0.993179">Julio</author>
<affiliation confidence="0.899268">Departamento de y Sistemas</affiliation>
<address confidence="0.644127">Madrid,</address>
<email confidence="0.838166">u'ulio@lsi.uned.es</email>
<author confidence="0.596117">Jussi</author>
<affiliation confidence="0.830056">Swedish Institute Computer</affiliation>
<email confidence="0.902611">jussi@sics.se</email>
<author confidence="0.983543">Paul</author>
<affiliation confidence="0.996665">Department Information University of</affiliation>
<address confidence="0.938179">Sheffield,</address>
<email confidence="0.94934">p.d.clough@sheffield.ac.k�</email>
<abstract confidence="0.97585980952381">This paper presents a proposal for iCLEF 2006, the interactive track of the CLEF cross-language evaluation campaign. In the past, iCLEF has addressed applications such as information retrieval and question answering. However, for 2006 the focus has turned to text-based image retrieval from Flickr. We describe Flickr, the challenges this kind of collection presents to cross-language researchers, and suggest initial iCLEF tasks. 1 Information Retrieval Evaluation by User Experiment Information retrieval systems, especially text retrieval systems, have benefited greatly from a fairly strict and straight-laced evaluation scheme, which enables system designers to run tests on versions of their system using a test collection of pre-assessed data. These relevance-oriented experiments shed light on comparative system per-</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Paul Clough</author>
<author>Henning Miller</author>
<author>Mark Sanderson</author>
</authors>
<title>The clef 2004 cross language image retrieval track.</title>
<date>2005</date>
<booktitle>Multilingual Information Access for Text, Speech and Images: Results of the Fifth CLEF Evaluation Campaign, number 3491/2005 in Lecture Notes in Computer Science,</booktitle>
<pages>597--613</pages>
<editor>In Carol Peters, Paul Clough, Julio Gonzalo, Gareth Jones, Michael Kluck, and Bernardo Magnini, editors,</editor>
<publisher>Springer,</publisher>
<location>Heidelberg, Germany.</location>
<contexts>
<context position="5258" citStr="Clough et al., 2005" startWordPosition="787" endWordPosition="790">discussion groups. This provides additional metadata to the image which can also be used for retrieval. An explore utility provided by FLICKR makes use of this usergenerated data (plus other information such as Clickthroughs) to define an ”interestingness” view of images6. 3 Flickr at iCLEF 2006 Many images are accompanied by text, enabling the use of both text and visual features for image retrieval and its evaluation (M¨uller et al., 2006, see e.g.). Images are naturally language independent and often successfully retrieved with associated texts. This has been explored as part of ImageCLEF (Clough et al., 2005) for areas such as information access to medical images and historic photographs. The way in which users search 4These figures are accurate as of October 2005: http://www.wired.com/news/ebiz/0,1272,68654,00.html 5http://creativecommons.org/image/flickr, http://flickr.com/creativecommons/ 6http://www.flickr.com/explore/interesting for images provides an interesting application for user-centered design and evaluation. As an iCLEF task, searching for images from FLICKR presents a new multilingual challenge which, to date, has not been explored. Challenges include: • Different types of associated </context>
</contexts>
<marker>Clough, Miller, Sanderson, 2005</marker>
<rawString>Paul Clough, Henning Miller, and Mark Sanderson. 2005. The clef 2004 cross language image retrieval track. In Carol Peters, Paul Clough, Julio Gonzalo, Gareth Jones, Michael Kluck, and Bernardo Magnini, editors, Multilingual Information Access for Text, Speech and Images: Results of the Fifth CLEF Evaluation Campaign, number 3491/2005 in Lecture Notes in Computer Science, pages 597–613. Springer, Heidelberg, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julio Gonzalo</author>
<author>Doug Oard</author>
</authors>
<title>The clef 2002 interactive track.</title>
<date>2002</date>
<booktitle>In Advances in CrossLanguage Information Retrieval, number 2785 in Lecture Notes in Computer Science.</booktitle>
<publisher>SpringerVerlag,</publisher>
<location>Berlin-Heidelberg-New York.</location>
<contexts>
<context position="1839" citStr="Gonzalo and Oard, 2002" startWordPosition="244" endWordPosition="247">on of new algorithms and incremental optimization. However, batch-oriented system evaluation based on large amounts of data, abstracted away from situational constraints, variation in usage, and interactiveness issues only addresses some of the bottlenecks to build a successful system. The CLEF1 Interactive Track (iCLEF2) is devoted to the comparative study of user inclusive cross-language search strategies. Over the past 5 years, iCLEF has studied three cross-language search tasks: retrieval of documents, answers and 1http://www.clef-campaign.org/ 2http://nlp.uned.es/iCLEF/ annotated images (Gonzalo and Oard, 2002; Gonzalo et al., 2005). All tasks involve the user interacting with information systems in a language different from that of the document collection. Although iCLEF experiments continue producing interesting research results, which may have a substantial impact on the way effective crosslanguage search assistants are built, participation in this track has remained low across the five years of existence of the track. Interactive studies, however, remain as a recognized necessity in most CLEF tracks. Therefore, to encourage greater participation in 2006 our focus has turned to FLICKR3, a larges</context>
</contexts>
<marker>Gonzalo, Oard, 2002</marker>
<rawString>Julio Gonzalo and Doug Oard. 2002. The clef 2002 interactive track. In Advances in CrossLanguage Information Retrieval, number 2785 in Lecture Notes in Computer Science. SpringerVerlag, Berlin-Heidelberg-New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julio Gonzalo</author>
<author>Paul Clough</author>
<author>A Vallin</author>
</authors>
<title>Overview of the clef 2005 interactive track.</title>
<date>2005</date>
<booktitle>In Working notes of the CLEF workshop,</booktitle>
<location>Vienna, Austria,</location>
<contexts>
<context position="1862" citStr="Gonzalo et al., 2005" startWordPosition="248" endWordPosition="252"> incremental optimization. However, batch-oriented system evaluation based on large amounts of data, abstracted away from situational constraints, variation in usage, and interactiveness issues only addresses some of the bottlenecks to build a successful system. The CLEF1 Interactive Track (iCLEF2) is devoted to the comparative study of user inclusive cross-language search strategies. Over the past 5 years, iCLEF has studied three cross-language search tasks: retrieval of documents, answers and 1http://www.clef-campaign.org/ 2http://nlp.uned.es/iCLEF/ annotated images (Gonzalo and Oard, 2002; Gonzalo et al., 2005). All tasks involve the user interacting with information systems in a language different from that of the document collection. Although iCLEF experiments continue producing interesting research results, which may have a substantial impact on the way effective crosslanguage search assistants are built, participation in this track has remained low across the five years of existence of the track. Interactive studies, however, remain as a recognized necessity in most CLEF tracks. Therefore, to encourage greater participation in 2006 our focus has turned to FLICKR3, a largescale, web-based image d</context>
</contexts>
<marker>Gonzalo, Clough, Vallin, 2005</marker>
<rawString>Julio Gonzalo, Paul Clough, and A Vallin. 2005. Overview of the clef 2005 interactive track. In Working notes of the CLEF workshop, Vienna, Austria, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Henning Miller</author>
<author>Paul Clough</author>
<author>William Hersh</author>
<author>Thomas Deselaers</author>
<author>Thomas Lehmann</author>
<author>Antoine Geissbuhler</author>
</authors>
<title>Using heterogeneous annotation and visual information for the benchmarking of image retrieval systems.</title>
<date>2006</date>
<booktitle>In SPIE conference Photonics West, Electronic Imaging, special</booktitle>
<location>San Diego,</location>
<marker>Miller, Clough, Hersh, Deselaers, Lehmann, Geissbuhler, 2006</marker>
<rawString>Henning Miller, Paul Clough, William Hersh, Thomas Deselaers, Thomas Lehmann, and Antoine Geissbuhler. 2006. Using heterogeneous annotation and visual information for the benchmarking of image retrieval systems. In SPIE conference Photonics West, Electronic Imaging, special session on benchmarking image retrieval systems, San Diego, February.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>