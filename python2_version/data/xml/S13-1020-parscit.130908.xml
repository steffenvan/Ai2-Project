<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.025850">
<title confidence="0.997668">
UPC-CORE: What Can Machine Translation Evaluation Metrics and
Wikipedia Do for Estimating Semantic Textual Similarity?
</title>
<author confidence="0.998947">
Alberto Barr´on-Cede˜no1,2 Lluis M`arquez1 Maria Fuentes1 Horacio Rodriguez1 Jordi Turmo1
</author>
<affiliation confidence="0.993554">
1 TALP Research Center, Universitat Polit`ecnica de Catalunya
</affiliation>
<address confidence="0.695940666666667">
Jordi Girona Salgado 1–3, 08034, Barcelona, Spain
2 Facultad de Informitica, Universidad Polit´ecnica de Madrid
Boadilla del Monte, 28660 Madrid, Spain
</address>
<email confidence="0.988597">
albarron, lluism, mfuentes, horacio, turmo @lsi.upc.edu
</email>
<sectionHeader confidence="0.995844" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.995698058823529">
In this paper we discuss our participation to
the 2013 Semeval Semantic Textual Similarity
task. Our core features include (i) a set of met-
rics borrowed from automatic machine trans-
lation, originally intended to evaluate auto-
matic against reference translations and (ii) an
instance of explicit semantic analysis, built
upon opening paragraphs of Wikipedia 2010
articles. Our similarity estimator relies on a
support vector regressor with RBF kernel. Our
best approach required 13 machine transla-
tion metrics + explicit semantic analysis and
ranked 65 in the competition. Our post-
competition analysis shows that the features
have a good expression level, but overfitting
and —mainly— normalization issues caused
our correlation values to decrease.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.980710153846154">
Our participation to the 2013 Semantic Textual Sim-
ilarity task (STS) (Agirre et al., 2013)1 was focused
on the CORE problem: GIVEN TWO SENTENCES,
S1 AND S2, QUANTIFIABLY INFORM ON HOW SIMI-
LAR S1 AND S2 ARE. We considered real-valued fea-
tures from four different sources: (i) a set of linguis-
tic measures computed with the Asiya Toolkit for
Automatic MT Evaluation (Gim´enez and M`arquez,
2010b), (ii) an instance of explicit semantic analy-
sis (Gabrilovich and Markovitch, 2007), built on top
of Wikipedia articles, (iii) a dataset predictor, and
(iv) a subset of the features available in Takelab’s
Semantic Text Similarity system (ˇSari´c et al., 2012).
</bodyText>
<footnote confidence="0.927557">
1http://ixa2.si.ehu.es/sts/
</footnote>
<bodyText confidence="0.999782444444444">
Our approaches obtained an overall modest result
compared to other participants (best position: 65 out
of 89). Nevertheless, our post-competition analysis
shows that the low correlation was caused mainly by
a deficient data normalization strategy.
The paper distribution is as follows. Section 2 of-
fers a brief overview of the task. Section 3 describes
our approach. Section 4 discuss our experiments and
obtained results. Section 5 provides conclusions.
</bodyText>
<sectionHeader confidence="0.976303" genericHeader="method">
2 Task Overview
</sectionHeader>
<bodyText confidence="0.999869521739131">
Detecting two similar text fragments is a difficult
task in cases where the similarity occurs at seman-
tic level, independently of the implied lexicon (e.g
in cases of dense paraphrasing). As a result, simi-
larity estimation models must involve features other
than surface aspects. The STS task is proposed as
a challenge focused in short English texts of dif-
ferent nature: from automatic machine translation
alternatives to human descriptions of short videos.
The test partition also included texts extracted from
news headlines and FrameNet–Wordnet pairs.
The range of similarity was defined between 0
(no relation) up to 5 (semantic equivalence). The
gold standard values were averaged from different
human-made annotations. The expected system’s
output was composed of a real similarity value, to-
gether with an optional confidence level (our confi-
dence level was set constant).
Table 1 gives an overview of the development
(2012 training and test) and test datasets. Note
that both collections extracted from SMT data are
highly biased towards the maximum similarity val-
ues (more than 75% of the instances have a similar-
</bodyText>
<page confidence="0.991741">
143
</page>
<note confidence="0.5268915">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 143–147, Atlanta, Georgia, June 13-14, 2013. c�2013 Association for Computational Linguistics
</note>
<tableCaption confidence="0.9668505">
Table 1: Overview of sub-collections in the development and test datasets, including number of instances and distri-
bution of similarity values (in percentage) as well as mean, minimum, and maximum lengths.
</tableCaption>
<table confidence="0.999450461538462">
dataset instances [0, 1) similarity distribution [4, 5] length max
[1, 2) [2, 3) [3, 4) mean min
dev-[train + test]
MSRpar 1,500 1.20 8.13 17.13 48.73 24.80 17.84 5 30
MSRvid 1,500 31.00 14.13 15.47 20.87 18.53 6.66 2 24
SMTEuroparl 1,193 0.67 0.42 1.17 12.32 85.4 21.13 1 72
OnWN 750 2.13 2.67 10.40 25.47 59.33 7.57 1 34
SMTnews 399 1.00 0.75 5.51 13.03 79.70 11.72 2 28
test 750 15.47 22.00 16.27 24.67 21.60 7.21 3 22
headlines
OnWN 561 36.54 9.80 7.49 17.11 29.05 7.17 5 22
FNWN 189 34.39 29.63 28.57 6.88 0.53 19.90 3 71
SMT 750 0.00 0.27 3.47 20.40 75.87 26.40 1 96
</table>
<bodyText confidence="0.99738925">
ity higher than 4) and include the longest instances.
On the other hand, the FNWN instances are shifted
towards low similarity levels (more than 60% have a
similarity lower than 2).
</bodyText>
<sectionHeader confidence="0.997398" genericHeader="method">
3 Approach
</sectionHeader>
<bodyText confidence="0.999984571428571">
Our similarity assessment model relies upon
SVM&amp;quot;ght’s support vector regressor, with RBF ker-
nel (Joachims, 1999).2 Our model estimation pro-
cedure consisted of two steps: parameter defini-
tion and backward elimination-based feature selec-
tion. The considered features belong to four fami-
lies, briefly described in the following subsections.
</bodyText>
<subsectionHeader confidence="0.995776">
3.1 Machine Translation Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.999951125">
We consider a set of linguistic measures originally
intended to evaluate the quality of automatic trans-
lation systems. These measures compute the quality
of a translation by comparing it against one or sev-
eral reference translations, considered as gold stan-
dard. A straightforward application of these mea-
sures to the problem at hand is to consider s1 as the
reference and s2 as the automatic translation, or vice
versa. Some of the metrics are not symmetric so we
compute similarity between s1 and s2 in both direc-
tions and average the resulting scores.
The measures are computed with the Asiya
Toolkit for Automatic MT Evaluation (Gim´enez and
M`arquez, 2010b). The only pre-processing carried
out was tokenization (Asiya performs additional in-
box pre-processing operations, though). We consid-
</bodyText>
<footnote confidence="0.5784675">
2We also tried with linear kernels, but RBF always obtained
better results.
</footnote>
<bodyText confidence="0.999625321428572">
ered a sample from three similarity families, which
was proposed in (Gim´enez and M`arquez, 2010a) as
a varied and robust metric set, showing good corre-
lation with human assessments.3
Lexical Similarity Two metrics of Translation
Error Rate (Snover et al., 2006) (i.e. the esti-
mated human effort to convert s1 into s2): -TER
and -TERpA. Two measures of lexical precision:
BLEU (Papineni et al., 2002) and NIST (Dod-
dington, 2002). One measure of lexical recall:
ROUGEW (Lin and Och, 2004). Finally, four vari-
ants of METEOR (Banerjee and Lavie, 2005) (exact,
stemming, synonyms, and paraphrasing), a lexical
metric accounting for F-Measure.
Syntactic Similarity Three metrics that estimate
the similarity of the sentences over dependency
parse trees (Liu and Gildea, 2005): DP-HWCMi,-4
for grammatical categories chains, DP-HWCMir-4
over grammatical relations, and DP-Or(⋆) over
words ruled by non-terminal nodes. Also, one mea-
sure that estimates the similarity over constituent
parse trees: CP-STM4 (Liu and Gildea, 2005).
Semantic Similarity Three measures that esti-
mate the similarities over semantic roles (i.e. ar-
guments and adjuncts): SR-Or, SR-Mr(⋆), and
SR-Or(⋆). Additionally, two metrics that es-
timate similarities over discourse representations:
DR-Or(⋆) and DR-Orp(⋆).
</bodyText>
<footnote confidence="0.981358666666667">
3Asiya is available at http://asiya.lsi.upc.edu.
Full descriptions of the metrics are available in the Asiya Tech-
nical Manual v2.0, pp. 15–21.
</footnote>
<page confidence="0.990384">
144
</page>
<subsectionHeader confidence="0.995087">
3.2 Explicit Semantic Analysis
</subsectionHeader>
<bodyText confidence="0.9999756">
We built an instance of Explicit Semantic Analy-
sis (ESA) (Gabrilovich and Markovitch, 2007) with
the first paragraph of 100k Wikipedia articles (dump
from 2010).Pre-processing consisted of tokenization
and lemmatization.
</bodyText>
<subsectionHeader confidence="0.991269">
3.3 Dataset Prediction
</subsectionHeader>
<bodyText confidence="0.999952">
Given the similarity shifts in the different datasets
(cf. Table 1), we tried to predict what dataset an in-
stance belonged to on the basis of its vocabulary. We
built binary maxent classifiers for each dataset in the
development set, resulting in five dataset likelihood
features: dMSRpar, dSMTeuroparl, dMSRvid,
dOnWN, and dSMTnews.4 Pre-processing consisted
of tokenization and lemmatization.
</bodyText>
<subsectionHeader confidence="0.630588">
3.4 Baseline
</subsectionHeader>
<bodyText confidence="0.99996425">
We considered the features included in the Takelab
Semantic Text Similarity system (ˇSari´c et al., 2012),
one of the top-systems in last year competition. This
system is used as a black box. The resulting features
are named tklab n, where n = [1, 21].
Our runs departed from three increasing subsets
of features: AE machine translation evaluation met-
rics and explicit semantic analysis, AED the pre-
vious set plus dataset prediction, and AED T the
previous set plus Takelab’s baseline features (cf. Ta-
ble 3). We performed a feature normalization, which
relied on the different feature’s distribution over the
entire dataset. Firstly, features were bounded in the
range µf3*σ2 in order to reduce the potentially neg-
ative impact of outliers. Secondly, we normalized
according to the z-score (Nardo et al., 2008, pp. 28,
84); i.e. x = (x − µ)/σ. As a result, each real-
valued feature distribution in the dataset has µ = 0
and σ = 1. During the model tuning stage we tried
with other numerous normalization options: normal-
izing each dataset independently, together with the
training set, and without normalization at all. Nor-
malizing according to the entire dev-test dataset led
to the best results
</bodyText>
<footnote confidence="0.940869">
4We used the Stanford classifier; http://nlp.
stanford.edu/software/classifier.shtml
</footnote>
<tableCaption confidence="0.926374333333333">
Table 2: Tuning process: parameter definition and feature
selection. Number of features at the beginning and end
of the feature selection step included.
</tableCaption>
<table confidence="0.9995718">
run parameter def. feature sel.
c γ ǫ corr b e corr
AE 3.7 0.06 0.3 0.8257 19 14 0.8299
AED 3.8 0.03 0.2 0.8413 24 19 0.8425
AED T 2.9 0.02 0.3 0.8761 45 33 0.8803
</table>
<sectionHeader confidence="0.994236" genericHeader="evaluation">
4 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999752">
Section 4.1 describes our model tuning strategy.
Sections 4.2 and 4.3 discuss the official and post-
competition results.
</bodyText>
<subsectionHeader confidence="0.996458">
4.1 Model Tuning
</subsectionHeader>
<bodyText confidence="0.99945580952381">
We used only the dev-train partition (2012 training)
for tuning. By means of a 10-fold cross validation
process, we defined the trade-off (c), gamma (γ),
and tube width (ǫ) parameters for the regressor and
performed a backward-elimination feature selection
process (Witten and Frank, 2005, p. 294), indepen-
dently for the three experiments.
The results for the cross-validation process are
summarized in Table 2. The three runs allow for cor-
relations higher than 0.8. On the one hand, the best
regressor parameters obtain better results as more
features are considered, still with very small differ-
ences. On the other hand, the low correlation in-
crease after the feature selection step shows that a
few features are indeed irrelevant.
A summary of the features considered in each ex-
periment (also after feature selection) is displayed in
Table 3. The correlation obtained over the dev-test
partition are corrAE = 0.7269, corrAED = 0.7638,
and corrAEDT = 0.8044 —it would have appeared
in the top-10 ranking of the 2012 competition.
</bodyText>
<subsectionHeader confidence="0.983361">
4.2 Official Results
</subsectionHeader>
<bodyText confidence="0.999975555555556">
We trained three new regressors with the features
considered relevant by the tuning process, but using
the entire development dataset. The test 2013 parti-
tion was normalized again by means of z-score, con-
sidering the means and standard deviations of the en-
tire test dataset. Table 4 displays the official results.
Our best approach —AE—, was positioned in rank
65. The worst results of run AED can be explained
by the difference in the nature of the test respect to
</bodyText>
<page confidence="0.999608">
145
</page>
<tableCaption confidence="0.956558">
Table 3: Features considered at the beginning of each run, represented as empty squares (❑). Filled squares (■)
represent features considered relevant after feature selection.
</tableCaption>
<table confidence="0.995489285714286">
Feature AE AED AED T Feature AE AED AED T Feature AED T
DP-HWCM c-4 ■ ■ ■ METEOR-pa ■ ■ ■ tklab 7 ■
DP-HWCM r-4 ■ ■ ■ METEOR-st ❑ ■ ❑ tklab 8 ■
DP-Or(*) ■ ■ ■ METEOR-sy ■ ■ ❑ tklab 9 ■
CP-STM-4 ❑ ❑ ■ ESA ■ ■ ■ tklab 10 ❑
SR-Or(*) ❑ ❑ ■ dMSRpar ■ ❑ tklab 11 ■
SR-Mr(*) ■ ■ ■ dSMTeuroparl ■ ■ tklab 12 ■
SR-Or ■ ■ ■ dMSRvid ■ ❑ tklab 13 ■
DR-Or(*) ❑ ■ ■ dOnWN ❑ ❑ tklab 14 ■
DR-Orp(*) ■ ■ ■ dSMTnews ❑ ❑ tklab 15 ■
BLEU ■ ■ ❑ tklab 1 ❑ tklab 16 ■
NIST ■ ■ ■ tklab 2 ■ tklab 17 ■
-TER ■ ■ ■ tklab 3 ■ tklab 18 ■
-TERp-A ■ ■ ■ tklab 4 ■ tklab 19 ■
ROUGE-W ■ ■ ❑ tklab 5 ■ tklab 20 ❑
METEOR-ex ❑ ❑ ■ tklab 6 ❑ tklab 21 ■
Table 4: Official results for the three runs (rank included). Table 5: Post-competition experiments results
run headlines OnW1 F1W1 SMT mean run headlines OnW1 F1W1 SMT mean
AE (65) 0.6092 0.5679 -0.1268 0.2090 0.4037 AE (a) 0.6210 0.5905 -0.0987 0.2990 0.4456
AED (83) 0.4136 0.4770 -0.0852 0.1662 0.3050 AE (b) 0.6072 0.4767 -0.0113 0.3236 0.4282
AED T (72) 0.5119 0.6386 -0.0464 0.1235 0.3671 AE (c) 0.6590 0.6973 0.1547 0.3429 0.5208
</table>
<bodyText confidence="0.999056666666667">
the development dataset. AED T obtains worst re-
sults than AE on the headlines and SMT datasets.
The reason behind this behavior can be in the dif-
ference of vocabularies respect to that stored in the
Takelab system (it includes only the vocabulary of
the development partition). This could be the same
reason behind the drop in performance with respect
to the results previously obtained on the dev-test par-
tition (cf. Section 4.1).
</bodyText>
<subsectionHeader confidence="0.999392">
4.3 Post-Competition Results
</subsectionHeader>
<bodyText confidence="0.99998772972973">
Our analysis of the official results showed the main
issue was normalization. Thus, we performed a
manifold of new experiments, using the same con-
figuration as in run AE, but applying other normal-
ization strategies: (a) z-score normalization, but ig-
noring the FNWN dataset (given its shift through
low values); (b) z-score normalization, but consid-
ering independent means and standard deviations for
each test dataset; and (c) without normalizing any of
dataset (including the regressor one).
Table 5 includes the results. (a) makes evident
that the instances in FNWN represent “anomalies”
that harm the normalized values of the rest of sub-
sets. Run (b) shows that normalizing the test sets
independently is not a good option, as the regressor
is trained considering overall normalizations, which
explains the correlation decrease. Run (c) is com-
pletely different: not normalizing any dataset —
both in development and test— reduces the influ-
ence of the datasets to each other and allows for the
best results. Indeed, this configuration would have
advanced practically forty positions at competition
time, locating us in rank 27.
Estimating the adequate similarities over FIWI
seems particularly difficult for our systems. We ob-
serve two main factors. (i) FIWI presents an im-
portant similarity shift respect to the other datasets:
nearly 90% of the instances similarity is lower than
2.5 and (ii) the average lengths of s1 and s2 are very
different: 30 vs 9 words. These characteristics made
it difficult for our MT evaluation metrics to estimate
proper similarity values (be normalized or not).
We performed two more experiments over
FNWN: training regressors with ESA as the only
feature, before and after normalization. The correla-
tion was 0.16017 and 0.3113, respectively. That is,
the normalization mainly affects the MT features.
</bodyText>
<page confidence="0.998383">
146
</page>
<sectionHeader confidence="0.999444" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.9981605">
In this paper we discussed on our participation to the
2013 Semeval Semantic Textual Similarity task. Our
approach relied mainly upon a combination of au-
tomatic machine translation evaluation metrics and
explicit semantic analysis. Building an RBF support
vector regressor with these features allowed us for a
modest result in the competition (our best run was
ranked 65 out of 89).
</bodyText>
<sectionHeader confidence="0.997385" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9996383">
We would like to thank the organizers of this chal-
lenging task for their efforts.
This research work was partially carried out dur-
ing the tenure of an ERCIM “Alain Bensoussan”
Fellowship. The research leading to these results re-
ceived funding from the EU FP7 Programme 2007-
2013 (grants 246016 and 247762). Our research
work is partially supported by the Spanish research
projects OpenMT-2 and SKATER (TIN2009-14675-
C03, TIN2012-38584-C06-01).
</bodyText>
<sectionHeader confidence="0.998128" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999736383561644">
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *SEM 2013 Shared
Task: Semantic Textual Similarity, including a Pilot on
Typed-Similarity. In *SEM 2013: The Second Joint
Conference on Lexical and Computational Semantics.
Association for Computational Linguistics.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Gold-
stein et al. (Goldstein et al., 2005), pages 65–72.
George Doddington. 2002. Automatic Evaluation
of Machine Translation Quality Using N-Gram Co-
occurrence Statistics. In Proceedings of the Second
International Conference on Human Language Tech-
nology Research, pages 138–145, San Francisco, CA.
Morgan Kaufmann Publishers Inc.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting Semantic Relatedness Using Wikipedia-based
Explicit Semantic Analysis. In Proceedings of the
20th International Joint Conference on Artificial Intel-
ligence, pages 1606–1611, San Francisco, CA, USA.
Morgan Kaufmann Publishers Inc.
Jes´us Gim´enez and Lluis M`arquez. 2010a. Asiya:
An Open Toolkit for Automatic Machine Translation
(Meta-)Evaluation. The Prague Bulletin ofMathemat-
ical Linguistics, (94).
Jes´us Gim´enez and Lluis M`arquez. 2010b. Linguistic
Measures for Automatic Machine Translation Evalua-
tion. Machine Translation, 24(3–4):209–240.
Jade Goldstein, Alon Lavie, Chin-Yew Lin, and Clare
Voss, editors. 2005. Proceedings of the ACL Work-
shop on Intrinsic and Extrinsic Evaluation Measures
for Machine Translation and/or Summarization. Asso-
ciation for Computational Linguistics.
Thorsten Joachims, 1999. Advances in Kernel Methods –
Support Vector Learning, chapter Making large-Scale
SVM Learning Practical. MIT Press.
Chin-Yew Lin and Franz Josef Och. 2004. Auto-
matic Evaluation of Machine Translation Quality Us-
ing Longest Common Subsequence and Skip-Bigram
Statistics. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2002), Stroudsburg, PA. Association for Com-
putational Linguistics.
Ding Liu and Daniel Gildea. 2005. Syntactic Features
for Evaluation of Machine Translation. In Goldstein
et al. (Goldstein et al., 2005), pages 25–32.
Michela Nardo, Michaela Saisana, Andrea Saltelli, Ste-
fano Tarantola, Anders Hoffmann, and Enrico Giovan-
nini. 2008. Handbook on Constructing Composite In-
dicators: Methodology and User Guide. OECD Pub-
lishing.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A Method for Automatic
Evaluation of Machine Translation. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics (ACL 2002), pages 311–318,
Philadelphia, PA. Association for Computational Lin-
guistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human An-
notation. In Proceedings of Association for Machine
Translation in the Americas, pages 223–231.
Frane ˇSari´c, Goran Glavaˇs, Mladen Karan, Jan ˇSnajder,
and Bojana Dalbelo Baˇsi´c. 2012. TakeLab: Sys-
tems for Measuring Semantic Text. In First Joint
Conference on Lexical and Computational Semantics
(*SEM), pages 441–448, Montr´eal, Canada. Associa-
tion for Computational Linguistics.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Prac-
tical Machine Learning Tools and Techniques. Mor-
gan Kaufmann, San Francisco, CA, 2 edition.
</reference>
<page confidence="0.99809">
147
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.384061">
<title confidence="0.905589333333333">UPC-CORE: What Can Machine Translation Evaluation Metrics Wikipedia Do for Estimating Semantic Textual Similarity? Research Center, Universitat Polit`ecnica de</title>
<author confidence="0.964582">Jordi Girona Salgado</author>
<affiliation confidence="0.615701">de Informitica, Universidad Polit´ecnica de</affiliation>
<address confidence="0.498961">Boadilla del Monte, 28660 Madrid, Spain</address>
<email confidence="0.997664">albarron,lluism,mfuentes,horacio,turmo@lsi.upc.edu</email>
<abstract confidence="0.998788777777778">In this paper we discuss our participation to the 2013 Semeval Semantic Textual Similarity Our core features include a set of metrics borrowed from automatic machine translation, originally intended to evaluate autoagainst reference translations and an instance of explicit semantic analysis, built upon opening paragraphs of Wikipedia 2010 articles. Our similarity estimator relies on a support vector regressor with RBF kernel. Our best approach required 13 machine translation metrics + explicit semantic analysis and ranked 65 in the competition. Our postcompetition analysis shows that the features have a good expression level, but overfitting and —mainly— normalization issues caused our correlation values to decrease.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor GonzalezAgirre</author>
<author>Weiwei Guo</author>
</authors>
<title>Shared Task: Semantic Textual Similarity, including a Pilot on Typed-Similarity.</title>
<date>2013</date>
<booktitle>In *SEM 2013: The Second Joint Conference on Lexical and Computational Semantics. Association for Computational Linguistics.</booktitle>
<publisher>SEM</publisher>
<contexts>
<context position="1341" citStr="Agirre et al., 2013" startWordPosition="186" endWordPosition="189">ainst reference translations and (ii) an instance of explicit semantic analysis, built upon opening paragraphs of Wikipedia 2010 articles. Our similarity estimator relies on a support vector regressor with RBF kernel. Our best approach required 13 machine translation metrics + explicit semantic analysis and ranked 65 in the competition. Our postcompetition analysis shows that the features have a good expression level, but overfitting and —mainly— normalization issues caused our correlation values to decrease. 1 Introduction Our participation to the 2013 Semantic Textual Similarity task (STS) (Agirre et al., 2013)1 was focused on the CORE problem: GIVEN TWO SENTENCES, S1 AND S2, QUANTIFIABLY INFORM ON HOW SIMILAR S1 AND S2 ARE. We considered real-valued features from four different sources: (i) a set of linguistic measures computed with the Asiya Toolkit for Automatic MT Evaluation (Gim´enez and M`arquez, 2010b), (ii) an instance of explicit semantic analysis (Gabrilovich and Markovitch, 2007), built on top of Wikipedia articles, (iii) a dataset predictor, and (iv) a subset of the features available in Takelab’s Semantic Text Similarity system (ˇSari´c et al., 2012). 1http://ixa2.si.ehu.es/sts/ Our app</context>
</contexts>
<marker>Agirre, Cer, Diab, GonzalezAgirre, Guo, 2013</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, Aitor GonzalezAgirre, and Weiwei Guo. 2013. *SEM 2013 Shared Task: Semantic Textual Similarity, including a Pilot on Typed-Similarity. In *SEM 2013: The Second Joint Conference on Lexical and Computational Semantics. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments.</title>
<date>2005</date>
<booktitle>In Goldstein et</booktitle>
<pages>65--72</pages>
<contexts>
<context position="6540" citStr="Banerjee and Lavie, 2005" startWordPosition="1015" endWordPosition="1018">d2We also tried with linear kernels, but RBF always obtained better results. ered a sample from three similarity families, which was proposed in (Gim´enez and M`arquez, 2010a) as a varied and robust metric set, showing good correlation with human assessments.3 Lexical Similarity Two metrics of Translation Error Rate (Snover et al., 2006) (i.e. the estimated human effort to convert s1 into s2): -TER and -TERpA. Two measures of lexical precision: BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). One measure of lexical recall: ROUGEW (Lin and Och, 2004). Finally, four variants of METEOR (Banerjee and Lavie, 2005) (exact, stemming, synonyms, and paraphrasing), a lexical metric accounting for F-Measure. Syntactic Similarity Three metrics that estimate the similarity of the sentences over dependency parse trees (Liu and Gildea, 2005): DP-HWCMi,-4 for grammatical categories chains, DP-HWCMir-4 over grammatical relations, and DP-Or(⋆) over words ruled by non-terminal nodes. Also, one measure that estimates the similarity over constituent parse trees: CP-STM4 (Liu and Gildea, 2005). Semantic Similarity Three measures that estimate the similarities over semantic roles (i.e. arguments and adjuncts): SR-Or, SR</context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments. In Goldstein et al. (Goldstein et al., 2005), pages 65–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
</authors>
<title>Automatic Evaluation of Machine Translation Quality Using N-Gram Cooccurrence Statistics.</title>
<date>2002</date>
<booktitle>In Proceedings of the Second International Conference on Human Language Technology Research,</booktitle>
<pages>138--145</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA.</location>
<contexts>
<context position="6420" citStr="Doddington, 2002" startWordPosition="996" endWordPosition="998">ssing carried out was tokenization (Asiya performs additional inbox pre-processing operations, though). We consid2We also tried with linear kernels, but RBF always obtained better results. ered a sample from three similarity families, which was proposed in (Gim´enez and M`arquez, 2010a) as a varied and robust metric set, showing good correlation with human assessments.3 Lexical Similarity Two metrics of Translation Error Rate (Snover et al., 2006) (i.e. the estimated human effort to convert s1 into s2): -TER and -TERpA. Two measures of lexical precision: BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). One measure of lexical recall: ROUGEW (Lin and Och, 2004). Finally, four variants of METEOR (Banerjee and Lavie, 2005) (exact, stemming, synonyms, and paraphrasing), a lexical metric accounting for F-Measure. Syntactic Similarity Three metrics that estimate the similarity of the sentences over dependency parse trees (Liu and Gildea, 2005): DP-HWCMi,-4 for grammatical categories chains, DP-HWCMir-4 over grammatical relations, and DP-Or(⋆) over words ruled by non-terminal nodes. Also, one measure that estimates the similarity over constituent parse trees: CP-STM4 (Liu and Gildea, 2005). Semant</context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>George Doddington. 2002. Automatic Evaluation of Machine Translation Quality Using N-Gram Cooccurrence Statistics. In Proceedings of the Second International Conference on Human Language Technology Research, pages 138–145, San Francisco, CA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeniy Gabrilovich</author>
<author>Shaul Markovitch</author>
</authors>
<title>Computing Semantic Relatedness Using Wikipedia-based Explicit Semantic Analysis.</title>
<date>2007</date>
<booktitle>In Proceedings of the 20th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>1606--1611</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="1728" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="249" endWordPosition="252">that the features have a good expression level, but overfitting and —mainly— normalization issues caused our correlation values to decrease. 1 Introduction Our participation to the 2013 Semantic Textual Similarity task (STS) (Agirre et al., 2013)1 was focused on the CORE problem: GIVEN TWO SENTENCES, S1 AND S2, QUANTIFIABLY INFORM ON HOW SIMILAR S1 AND S2 ARE. We considered real-valued features from four different sources: (i) a set of linguistic measures computed with the Asiya Toolkit for Automatic MT Evaluation (Gim´enez and M`arquez, 2010b), (ii) an instance of explicit semantic analysis (Gabrilovich and Markovitch, 2007), built on top of Wikipedia articles, (iii) a dataset predictor, and (iv) a subset of the features available in Takelab’s Semantic Text Similarity system (ˇSari´c et al., 2012). 1http://ixa2.si.ehu.es/sts/ Our approaches obtained an overall modest result compared to other participants (best position: 65 out of 89). Nevertheless, our post-competition analysis shows that the low correlation was caused mainly by a deficient data normalization strategy. The paper distribution is as follows. Section 2 offers a brief overview of the task. Section 3 describes our approach. Section 4 discuss our exper</context>
<context position="7540" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="1153" endWordPosition="1156">o, one measure that estimates the similarity over constituent parse trees: CP-STM4 (Liu and Gildea, 2005). Semantic Similarity Three measures that estimate the similarities over semantic roles (i.e. arguments and adjuncts): SR-Or, SR-Mr(⋆), and SR-Or(⋆). Additionally, two metrics that estimate similarities over discourse representations: DR-Or(⋆) and DR-Orp(⋆). 3Asiya is available at http://asiya.lsi.upc.edu. Full descriptions of the metrics are available in the Asiya Technical Manual v2.0, pp. 15–21. 144 3.2 Explicit Semantic Analysis We built an instance of Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007) with the first paragraph of 100k Wikipedia articles (dump from 2010).Pre-processing consisted of tokenization and lemmatization. 3.3 Dataset Prediction Given the similarity shifts in the different datasets (cf. Table 1), we tried to predict what dataset an instance belonged to on the basis of its vocabulary. We built binary maxent classifiers for each dataset in the development set, resulting in five dataset likelihood features: dMSRpar, dSMTeuroparl, dMSRvid, dOnWN, and dSMTnews.4 Pre-processing consisted of tokenization and lemmatization. 3.4 Baseline We considered the features included in </context>
</contexts>
<marker>Gabrilovich, Markovitch, 2007</marker>
<rawString>Evgeniy Gabrilovich and Shaul Markovitch. 2007. Computing Semantic Relatedness Using Wikipedia-based Explicit Semantic Analysis. In Proceedings of the 20th International Joint Conference on Artificial Intelligence, pages 1606–1611, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Lluis M`arquez</author>
</authors>
<title>Asiya: An Open Toolkit for Automatic Machine Translation (Meta-)Evaluation. The Prague Bulletin ofMathematical Linguistics,</title>
<date>2010</date>
<marker>Gim´enez, M`arquez, 2010</marker>
<rawString>Jes´us Gim´enez and Lluis M`arquez. 2010a. Asiya: An Open Toolkit for Automatic Machine Translation (Meta-)Evaluation. The Prague Bulletin ofMathematical Linguistics, (94).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Lluis M`arquez</author>
</authors>
<title>Linguistic Measures for Automatic Machine Translation Evaluation. Machine Translation,</title>
<date>2010</date>
<pages>24--3</pages>
<marker>Gim´enez, M`arquez, 2010</marker>
<rawString>Jes´us Gim´enez and Lluis M`arquez. 2010b. Linguistic Measures for Automatic Machine Translation Evaluation. Machine Translation, 24(3–4):209–240.</rawString>
</citation>
<citation valid="true">
<date>2005</date>
<booktitle>Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization. Association for Computational Linguistics.</booktitle>
<editor>Jade Goldstein, Alon Lavie, Chin-Yew Lin, and Clare Voss, editors.</editor>
<marker>2005</marker>
<rawString>Jade Goldstein, Alon Lavie, Chin-Yew Lin, and Clare Voss, editors. 2005. Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<date>1999</date>
<booktitle>Advances in Kernel Methods – Support Vector Learning, chapter Making large-Scale SVM Learning Practical.</booktitle>
<publisher>MIT Press.</publisher>
<contexts>
<context position="4852" citStr="Joachims, 1999" startWordPosition="752" endWordPosition="753">4 21.13 1 72 OnWN 750 2.13 2.67 10.40 25.47 59.33 7.57 1 34 SMTnews 399 1.00 0.75 5.51 13.03 79.70 11.72 2 28 test 750 15.47 22.00 16.27 24.67 21.60 7.21 3 22 headlines OnWN 561 36.54 9.80 7.49 17.11 29.05 7.17 5 22 FNWN 189 34.39 29.63 28.57 6.88 0.53 19.90 3 71 SMT 750 0.00 0.27 3.47 20.40 75.87 26.40 1 96 ity higher than 4) and include the longest instances. On the other hand, the FNWN instances are shifted towards low similarity levels (more than 60% have a similarity lower than 2). 3 Approach Our similarity assessment model relies upon SVM&amp;quot;ght’s support vector regressor, with RBF kernel (Joachims, 1999).2 Our model estimation procedure consisted of two steps: parameter definition and backward elimination-based feature selection. The considered features belong to four families, briefly described in the following subsections. 3.1 Machine Translation Evaluation Metrics We consider a set of linguistic measures originally intended to evaluate the quality of automatic translation systems. These measures compute the quality of a translation by comparing it against one or several reference translations, considered as gold standard. A straightforward application of these measures to the problem at ha</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims, 1999. Advances in Kernel Methods – Support Vector Learning, chapter Making large-Scale SVM Learning Practical. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Franz Josef Och</author>
</authors>
<title>Automatic Evaluation of Machine Translation Quality Using Longest Common Subsequence and Skip-Bigram Statistics.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL 2002),</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA.</location>
<contexts>
<context position="6479" citStr="Lin and Och, 2004" startWordPosition="1005" endWordPosition="1008">nal inbox pre-processing operations, though). We consid2We also tried with linear kernels, but RBF always obtained better results. ered a sample from three similarity families, which was proposed in (Gim´enez and M`arquez, 2010a) as a varied and robust metric set, showing good correlation with human assessments.3 Lexical Similarity Two metrics of Translation Error Rate (Snover et al., 2006) (i.e. the estimated human effort to convert s1 into s2): -TER and -TERpA. Two measures of lexical precision: BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). One measure of lexical recall: ROUGEW (Lin and Och, 2004). Finally, four variants of METEOR (Banerjee and Lavie, 2005) (exact, stemming, synonyms, and paraphrasing), a lexical metric accounting for F-Measure. Syntactic Similarity Three metrics that estimate the similarity of the sentences over dependency parse trees (Liu and Gildea, 2005): DP-HWCMi,-4 for grammatical categories chains, DP-HWCMir-4 over grammatical relations, and DP-Or(⋆) over words ruled by non-terminal nodes. Also, one measure that estimates the similarity over constituent parse trees: CP-STM4 (Liu and Gildea, 2005). Semantic Similarity Three measures that estimate the similarities</context>
</contexts>
<marker>Lin, Och, 2004</marker>
<rawString>Chin-Yew Lin and Franz Josef Och. 2004. Automatic Evaluation of Machine Translation Quality Using Longest Common Subsequence and Skip-Bigram Statistics. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL 2002), Stroudsburg, PA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ding Liu</author>
<author>Daniel Gildea</author>
</authors>
<title>Syntactic Features for Evaluation of Machine Translation.</title>
<date>2005</date>
<journal>In Goldstein</journal>
<pages>25--32</pages>
<contexts>
<context position="6762" citStr="Liu and Gildea, 2005" startWordPosition="1045" endWordPosition="1048">rrelation with human assessments.3 Lexical Similarity Two metrics of Translation Error Rate (Snover et al., 2006) (i.e. the estimated human effort to convert s1 into s2): -TER and -TERpA. Two measures of lexical precision: BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). One measure of lexical recall: ROUGEW (Lin and Och, 2004). Finally, four variants of METEOR (Banerjee and Lavie, 2005) (exact, stemming, synonyms, and paraphrasing), a lexical metric accounting for F-Measure. Syntactic Similarity Three metrics that estimate the similarity of the sentences over dependency parse trees (Liu and Gildea, 2005): DP-HWCMi,-4 for grammatical categories chains, DP-HWCMir-4 over grammatical relations, and DP-Or(⋆) over words ruled by non-terminal nodes. Also, one measure that estimates the similarity over constituent parse trees: CP-STM4 (Liu and Gildea, 2005). Semantic Similarity Three measures that estimate the similarities over semantic roles (i.e. arguments and adjuncts): SR-Or, SR-Mr(⋆), and SR-Or(⋆). Additionally, two metrics that estimate similarities over discourse representations: DR-Or(⋆) and DR-Orp(⋆). 3Asiya is available at http://asiya.lsi.upc.edu. Full descriptions of the metrics are avail</context>
</contexts>
<marker>Liu, Gildea, 2005</marker>
<rawString>Ding Liu and Daniel Gildea. 2005. Syntactic Features for Evaluation of Machine Translation. In Goldstein et al. (Goldstein et al., 2005), pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michela Nardo</author>
<author>Michaela Saisana</author>
<author>Andrea Saltelli</author>
<author>Stefano Tarantola</author>
<author>Anders Hoffmann</author>
<author>Enrico Giovannini</author>
</authors>
<title>Handbook on Constructing Composite Indicators: Methodology and User Guide.</title>
<date>2008</date>
<publisher>OECD Publishing.</publisher>
<contexts>
<context position="8909" citStr="Nardo et al., 2008" startWordPosition="1363" endWordPosition="1366">. The resulting features are named tklab n, where n = [1, 21]. Our runs departed from three increasing subsets of features: AE machine translation evaluation metrics and explicit semantic analysis, AED the previous set plus dataset prediction, and AED T the previous set plus Takelab’s baseline features (cf. Table 3). We performed a feature normalization, which relied on the different feature’s distribution over the entire dataset. Firstly, features were bounded in the range µf3*σ2 in order to reduce the potentially negative impact of outliers. Secondly, we normalized according to the z-score (Nardo et al., 2008, pp. 28, 84); i.e. x = (x − µ)/σ. As a result, each realvalued feature distribution in the dataset has µ = 0 and σ = 1. During the model tuning stage we tried with other numerous normalization options: normalizing each dataset independently, together with the training set, and without normalization at all. Normalizing according to the entire dev-test dataset led to the best results 4We used the Stanford classifier; http://nlp. stanford.edu/software/classifier.shtml Table 2: Tuning process: parameter definition and feature selection. Number of features at the beginning and end of the feature s</context>
</contexts>
<marker>Nardo, Saisana, Saltelli, Tarantola, Hoffmann, Giovannini, 2008</marker>
<rawString>Michela Nardo, Michaela Saisana, Andrea Saltelli, Stefano Tarantola, Anders Hoffmann, and Enrico Giovannini. 2008. Handbook on Constructing Composite Indicators: Methodology and User Guide. OECD Publishing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: A Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Philadelphia, PA.</location>
<contexts>
<context position="6392" citStr="Papineni et al., 2002" startWordPosition="990" endWordPosition="993">rquez, 2010b). The only pre-processing carried out was tokenization (Asiya performs additional inbox pre-processing operations, though). We consid2We also tried with linear kernels, but RBF always obtained better results. ered a sample from three similarity families, which was proposed in (Gim´enez and M`arquez, 2010a) as a varied and robust metric set, showing good correlation with human assessments.3 Lexical Similarity Two metrics of Translation Error Rate (Snover et al., 2006) (i.e. the estimated human effort to convert s1 into s2): -TER and -TERpA. Two measures of lexical precision: BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). One measure of lexical recall: ROUGEW (Lin and Och, 2004). Finally, four variants of METEOR (Banerjee and Lavie, 2005) (exact, stemming, synonyms, and paraphrasing), a lexical metric accounting for F-Measure. Syntactic Similarity Three metrics that estimate the similarity of the sentences over dependency parse trees (Liu and Gildea, 2005): DP-HWCMi,-4 for grammatical categories chains, DP-HWCMir-4 over grammatical relations, and DP-Or(⋆) over words ruled by non-terminal nodes. Also, one measure that estimates the similarity over constituent parse trees: CP-STM4 (L</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: A Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL 2002), pages 311–318, Philadelphia, PA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A Study of Translation Edit Rate with Targeted Human Annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of Association for Machine Translation in the Americas,</booktitle>
<pages>223--231</pages>
<contexts>
<context position="6254" citStr="Snover et al., 2006" startWordPosition="966" endWordPosition="969">ections and average the resulting scores. The measures are computed with the Asiya Toolkit for Automatic MT Evaluation (Gim´enez and M`arquez, 2010b). The only pre-processing carried out was tokenization (Asiya performs additional inbox pre-processing operations, though). We consid2We also tried with linear kernels, but RBF always obtained better results. ered a sample from three similarity families, which was proposed in (Gim´enez and M`arquez, 2010a) as a varied and robust metric set, showing good correlation with human assessments.3 Lexical Similarity Two metrics of Translation Error Rate (Snover et al., 2006) (i.e. the estimated human effort to convert s1 into s2): -TER and -TERpA. Two measures of lexical precision: BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). One measure of lexical recall: ROUGEW (Lin and Och, 2004). Finally, four variants of METEOR (Banerjee and Lavie, 2005) (exact, stemming, synonyms, and paraphrasing), a lexical metric accounting for F-Measure. Syntactic Similarity Three metrics that estimate the similarity of the sentences over dependency parse trees (Liu and Gildea, 2005): DP-HWCMi,-4 for grammatical categories chains, DP-HWCMir-4 over grammatical relations, and</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A Study of Translation Edit Rate with Targeted Human Annotation. In Proceedings of Association for Machine Translation in the Americas, pages 223–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frane ˇSari´c</author>
<author>Goran Glavaˇs</author>
<author>Mladen Karan</author>
<author>Jan ˇSnajder</author>
<author>Bojana Dalbelo Baˇsi´c</author>
</authors>
<title>TakeLab: Systems for Measuring Semantic Text.</title>
<date>2012</date>
<booktitle>In First Joint Conference on Lexical and Computational Semantics (*SEM),</booktitle>
<pages>441--448</pages>
<publisher>Association for</publisher>
<institution>Computational Linguistics.</institution>
<location>Montr´eal, Canada.</location>
<marker>ˇSari´c, Glavaˇs, Karan, ˇSnajder, Baˇsi´c, 2012</marker>
<rawString>Frane ˇSari´c, Goran Glavaˇs, Mladen Karan, Jan ˇSnajder, and Bojana Dalbelo Baˇsi´c. 2012. TakeLab: Systems for Measuring Semantic Text. In First Joint Conference on Lexical and Computational Semantics (*SEM), pages 441–448, Montr´eal, Canada. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Eibe Frank</author>
</authors>
<date>2005</date>
<booktitle>Data Mining: Practical Machine Learning Tools and Techniques.</booktitle>
<volume>2</volume>
<pages>edition.</pages>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco, CA,</location>
<contexts>
<context position="10148" citStr="Witten and Frank, 2005" startWordPosition="1567" endWordPosition="1570">luded. run parameter def. feature sel. c γ ǫ corr b e corr AE 3.7 0.06 0.3 0.8257 19 14 0.8299 AED 3.8 0.03 0.2 0.8413 24 19 0.8425 AED T 2.9 0.02 0.3 0.8761 45 33 0.8803 4 Experiments and Results Section 4.1 describes our model tuning strategy. Sections 4.2 and 4.3 discuss the official and postcompetition results. 4.1 Model Tuning We used only the dev-train partition (2012 training) for tuning. By means of a 10-fold cross validation process, we defined the trade-off (c), gamma (γ), and tube width (ǫ) parameters for the regressor and performed a backward-elimination feature selection process (Witten and Frank, 2005, p. 294), independently for the three experiments. The results for the cross-validation process are summarized in Table 2. The three runs allow for correlations higher than 0.8. On the one hand, the best regressor parameters obtain better results as more features are considered, still with very small differences. On the other hand, the low correlation increase after the feature selection step shows that a few features are indeed irrelevant. A summary of the features considered in each experiment (also after feature selection) is displayed in Table 3. The correlation obtained over the dev-test</context>
</contexts>
<marker>Witten, Frank, 2005</marker>
<rawString>Ian H. Witten and Eibe Frank. 2005. Data Mining: Practical Machine Learning Tools and Techniques. Morgan Kaufmann, San Francisco, CA, 2 edition.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>