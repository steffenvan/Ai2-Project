<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.031138">
<title confidence="0.994696">
SAIL: Sentiment Analysis using Semantic Similarity and Contrast
Features
</title>
<author confidence="0.9579225">
Nikolaos Malandrakis, Michael Falcone, Colin Vaz, Jesse Bisogni,
Alexandros Potamianos, Shrikanth Narayanan
</author>
<affiliation confidence="0.830453">
Signal Analysis and Interpretation Laboratory (SAIL), USC, Los Angeles, CA 90089, USA
</affiliation>
<email confidence="0.9902505">
{malandra,mfalcone,cvaz,jbisogni}@usc.edu,
potam@telecom.tuc.gr, shri@sipi.usc.edu
</email>
<sectionHeader confidence="0.997365" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999861">
This paper describes our submission to Se-
mEval2014 Task 9: Sentiment Analysis in
Twitter. Our model is primarily a lexi-
con based one, augmented by some pre-
processing, including detection of Multi-
Word Expressions, negation propagation
and hashtag expansion and by the use of
pairwise semantic similarity at the tweet
level. Feature extraction is repeated for
sub-strings and contrasting sub-string fea-
tures are used to better capture complex
phenomena like sarcasm. The resulting
supervised system, using a Naive Bayes
model, achieved high performance in clas-
sifying entire tweets, ranking 7th on the
main set and 2nd when applied to sarcastic
tweets.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.946936452380952">
The analysis of the emotional content of text is
relevant to numerous natural language process-
ing (NLP), web and multi-modal dialogue appli-
cations. In recent years the increased popularity
of social media and increased availability of rele-
vant data has led to a focus of scientific efforts on
the emotion expressed through social media, with
Twitter being the most common subject.
Sentiment analysis in Twitter is usually per-
formed by combining techniques used for related
tasks, like word-level (Esuli and Sebastiani, 2006;
Strapparava and Valitutti, 2004) and sentence-
level (Turney and Littman, 2002; Turney and
Littman, 2003) emotion extraction. Twitter how-
ever does present specific challenges: the breadth
of possible content is virtually unlimited, the writ-
ing style is informal, the use of orthography and
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
grammar can be “unconventional” and there are
unique artifacts like hashtags. Computation sys-
tems, like those submitted to SemEval 2013 task
2 (Nakov et al., 2013) mostly use bag-of-words
models with specific features added to model emo-
tion indicators like hashtags and emoticons (Davi-
dov et al., 2010).
This paper describes our submissions to Se-
mEval 2014 task 9 (Rosenthal et al., 2014), which
deals with sentiment analysis in twitter. The sys-
tem is an expansion of our submission to the same
task in 2013 (Malandrakis et al., 2013a), which
used only token rating statistics as features. We
expanded the system by using multiple lexica and
more statistics, added steps to the pre-processing
stage (including negation and multi-word expres-
sion handling), incorporated pairwise tweet-level
semantic similarities as features and finally per-
formed feature extraction on substrings and used
the partial features as indicators of irony, sarcasm
or humor.
</bodyText>
<sectionHeader confidence="0.945412" genericHeader="method">
2 Model Description
</sectionHeader>
<subsectionHeader confidence="0.9777">
2.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.999249">
POS-tagging / Tokenization was performed
using the ARK NLP tweeter tagger (Owoputi et
al., 2013), a Twitter-specific tagger.
Negations were detected using the list from
Christopher Potts’ tutorial. All tokens up to the
next punctuation were marked as negated.
Hashtag expansion into word strings was per-
formed using a combination of a word insertion
Finite State Machine and a language model. A
normalized perplexity threshold was used to
detect if the output was a “proper” English string
and expansion was not performed if it was not.
Multi-word Expressions (MWEs) were detected
using the MIT jMWE library (Kulkarni and
Finlayson, 2011). MWEs are non-compositional
expressions (Sag et al., 2002), which should be
</bodyText>
<page confidence="0.961537">
512
</page>
<note confidence="0.7313125">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 512–516,
Dublin, Ireland, August 23-24, 2014.
</note>
<bodyText confidence="0.9573145">
handled as a single token instead of attempting to
reconstruct their meaning from their parts.
</bodyText>
<subsectionHeader confidence="0.99779">
2.2 Lexicon-based features
</subsectionHeader>
<bodyText confidence="0.999896666666667">
The core of the system was formed by the lexicon-
based features. We used a total of four lexica and
some derivatives.
</bodyText>
<subsectionHeader confidence="0.863978">
2.2.1 Third party lexica
</subsectionHeader>
<bodyText confidence="0.96819347368421">
We used three third party affective lexica.
SentiWordNet (Esuli and Sebastiani, 2006) pro-
vides continuous positive, negative and neutral rat-
ings for each sense of every word in WordNet.
We created two versions of SentiWordNet: one
where ratings are averaged over all senses of a
word (e.g., one ratings for “good”) and one where
ratings are averaged over lexeme-pos pairs (e.g.,
one rating for the adjective “good” and one for the
noun “good”).
NRC Hashtag (Mohammad et al., 2013) Senti-
ment Lexicon provides continuous polarity ratings
for tokens, generated from a collection of tweets
that had a positive or a negative word hashtag.
Sentiment140 (Mohammad et al., 2013) Lexi-
con provides continuous polarity ratings for to-
kens, generated from the sentiment140 corpus of
1.6 million tweets, with emoticons used as posi-
tive and negative labels.
</bodyText>
<subsubsectionHeader confidence="0.646159">
2.2.2 Emotiword: expansion and adaptation
</subsubsectionHeader>
<bodyText confidence="0.999920181818182">
To create our own lexicon we used an automated
algorithm of affective lexicon expansion based on
the one presented in (Malandrakis et al., 2011;
Malandrakis et al., 2013b), which in turn is an ex-
pansion of (Turney and Littman, 2002).
We assume that the continuous (in [−1, 1]) va-
lence, arousal and dominance ratings of any term
tj can be represented as a linear combination of
its semantic similarities dij to a set of seed words
wi and the known affective ratings of these words
v(wi), as follows:
</bodyText>
<equation confidence="0.982843">
N
ˆv(tj) = a0 + ai v(wi) dij, (1)
i=1
</equation>
<bodyText confidence="0.999991972972973">
where ai is the weight corresponding to seed word
wi (that is estimated as described next). For the
purposes of this work, dij is the cosine similarity
between context vectors computed over a corpus
of 116 million web snippets (up to 1000 for each
word in the Aspell spellchecker) collected using
the Yahoo! search engine.
Given the starting, manually annotated, lexi-
con Affective Norms for English Words (Bradley
and Lang, 1999) we selected 600 out of the 1034
words contained in it to serve as seed words and
all 1034 words to act as the training set and used
Least Squares Estimation to estimate the weights
ai. Seed word selection was performed by a sim-
ple heuristic: we want seed words to have extreme
affective ratings (high absolute value) and the set
to be close to balanced (sum of seed ratings equal
to zero). The equation learned was used to gener-
ate ratings for any new terms.
The lexicon created by this method is task-
independent, since both the starting lexicon and
the raw text corpus are task-independent. To cre-
ate task-specific lexica we used corpus filtering on
the 116 million sentences to select ones that match
our domain, using either a normalized perplex-
ity threshold (using a maximum likelihood trigram
model created from the training set tweets) or a
combination of pragmatic constraints (keywords
with high mutual information with the task) and
perplexity threshold (Malandrakis et al., 2014).
Then we re-calculated semantic similarities on the
filtered corpora. In total we created three lexica: a
task-independent (base) version and two adapted
versions (filtered by perplexity alone and filtered
by combining pragmatics and perplexity), all con-
taining valence, arousal and dominance token rat-
ings.
</bodyText>
<subsubsectionHeader confidence="0.76302">
2.2.3 Statistics extraction
</subsubsectionHeader>
<bodyText confidence="0.99990175">
The lexica provide up to 17 ratings for each to-
ken. To extract tweet-level features we used sim-
ple statistics and selection criteria. First, all token
unigrams and bigrams contained in a tweet were
collected. Some of these n-grams were selected
based on a criterion: POS tags, whether a token is
(part of) a MWE, is negated or was expanded from
a hashtag. The criteria were applied separately
to token unigrams and token bigrams (POS tags
only applied to unigrams). Then ratings statistics
were extracted from the selected n-grams: length
(cardinality), min, max, max amplitude, sum, av-
erage, range (max minus min), standard deviation
and variance. We also created normalized versions
by dividing by the same statistics calculated over
all tokens, e.g., the maximum of adjectives over
the maximum of all unigrams. The results of this
process are features like “maximum of Emotiword
valence over unigram adjectives” and “average of
SentiWordNet objectivity among MWE bigrams”.
</bodyText>
<page confidence="0.991927">
513
</page>
<subsectionHeader confidence="0.99163">
2.3 Tweet-level similarity ratings
</subsectionHeader>
<bodyText confidence="0.999983866666667">
Our lexicon was formed under the assumption
that semantic similarity implies affective similar-
ity, which should apply to larger lexical units like
entire tweets. To estimate semantic similarity
scores between tweets we used the publicly avail-
able TakeLab semantic similarity toolkit (ˇSari´c et
al., 2012) which is based on a submission to Se-
mEval 2012 task 6 (Agirre et al., 2012). We used
the data of SemEval 2012 task 6 to train three
semantic similarity models corresponding to the
three datasets of that task, plus an overall model.
Using these models we created four similarity rat-
ings between each tweet of interest and each tweet
in the training set. These similarity ratings were
used as features of the final model.
</bodyText>
<subsectionHeader confidence="0.959418">
2.4 Character features
</subsectionHeader>
<bodyText confidence="0.99972725">
Capitalization features are frequencies and rela-
tive frequencies at the word and letter level, ex-
tracted from all words that either start with a capi-
tal letter, have a capital letter in them (but the first
letter is non-capital) or are in all capital letters.
Punctuation features are frequencies, relative fre-
quencies and punctuation unigrams.
Character repetition features are frequencies,
relative frequencies and longest string statistics of
words containing a repetition of the same letter.
Emoticon features are frequencies, relative fre-
quencies, and emoticon unigrams.
</bodyText>
<subsectionHeader confidence="0.98447">
2.5 Contrast features
</subsectionHeader>
<bodyText confidence="0.999898357142857">
Cognitive Dissonance is an important phe-
nomenon associated with complex linguistic cases
like sarcasm, irony and humor (Reyes et al., 2012).
To estimate it we used a simple approach, inspired
by one-liner joke detection: we assumed that the
final few tokens of each tweet (the “suffix”) con-
trast the rest of the tweet (the “prefix”) and created
split versions of the tweet where the last N tokens
are the suffix and all other tokens are the prefix,
for N = 2 and N = 3. We repeated the fea-
ture extraction process for all features mentioned
above (except for the semantic similarity features)
for the prefix and suffix, nearly tripling the total
number of features.
</bodyText>
<subsectionHeader confidence="0.99663">
2.6 Feature selection and Training
</subsectionHeader>
<bodyText confidence="0.959413333333333">
The extraction process lead to tens of thousands
of candidate features, so we performed forward
stepwise feature selection using a correlation crite-
</bodyText>
<tableCaption confidence="0.9731685">
Table 1: Performance and rank achieved by our
submission for all datasets of subtasks A and B.
</tableCaption>
<table confidence="0.998761818181818">
task dataset avg. F1 rank
LJ2014 70.62 16
SMS2013 74.46 16
A TW2013 78.47 14
TW2014 76.89 13
TW2014SC 65.56 15
LJ2014 69.34 15
SMS2013 56.98 24
B TW2013 66.80 10
TW2014 67.77 7
TW2014SC 57.26 2
</table>
<bodyText confidence="0.998761444444444">
rion (Hall, 1999) and used the resulting set of 222
features to train a model. The model chosen is a
Naive Bayes tree, a tree with Naive Bayes clas-
sifiers on each leaf. The motivation comes from
considering this a two stage problem: subjectivity
detection and polarity classification, making a hi-
erarchical model a natural choice. The feature se-
lection and model training/classification was con-
ducted using Weka (Witten and Frank, 2000).
</bodyText>
<tableCaption confidence="0.992307">
Table 2: Selected features for subtask B.
</tableCaption>
<table confidence="0.999179176470588">
Features number
Lexicon-derived 178
By lexicon
Ewrd / S140 / SWNet / NRC 71 / 53 / 33 / 21
By POS tag
all (ignore tag) 103
adj / verb / proper noun 25 / 11 / 11
other tags 28
By function
avg / min / sum / max 45 / 40 / 38 / 26
other functions 29
Semantic similarity 29
Punctuation 7
Emoticon 5
Other features 3
Contrast 72
prefix / suffix 54 / 18
</table>
<sectionHeader confidence="0.999898" genericHeader="evaluation">
3 Results
</sectionHeader>
<bodyText confidence="0.99997175">
We took part in subtasks A and B of SemEval
2014 task 9, submitting constrained runs trained
with the data the task organizers provided. Sub-
task B was the priority and the subtask A model
was created as an afterthought: it only uses the
lexicon-based and morphology features for the tar-
get string and the entire tweet as features of an NB
Tree.
The overall performance of our submission
on all datasets (LiveJournal, SMS, Twitter 2013,
Twitter 2014 and Twitter 2014 Sarcasm) can be
seen in Table 1. The subtask A system performed
</bodyText>
<page confidence="0.998603">
514
</page>
<tableCaption confidence="0.9952255">
Table 3: Performance on all data sets of subtask B after removing 1 set of features. Performance differ-
ence with the complete system listed if greater than 1%.
</tableCaption>
<table confidence="0.999783944444444">
Features removed LJ2014 SMS2013 TW2013 TW2014 TW2014SC
avg. F1 diff avg. F1 diff avg. F1 diff avg. F1 diff avg. F1 diff
None (Submitted) 69.3 57.0 66.8 67.8 57.3
Lexicon-derived 43.6 -25.8 38.2 -18.8 49.5 -17.4 51.5 -16.3 43.5 -13.8
Emotiword 67.5 -1.9 56.4 63.5 -3.3 66.1 -1.7 54.8 -2.5
Base 68.4 56.3 65.0 -1.9 66.4 -1.4 59.6 2.3
Adapted 69.3 57.4 66.7 67.5 50.8 -6.5
Sentiment140 68.1 -1.3 54.5 -2.5 64.4 -2.4 64.2 -3.6 45.4 -11.9
NRC Tag 70.6 1.3 58.5 1.6 66.3 66.0 -1.7 55.3 -2.0
SentiWordNet 68.7 56.0 66.2 68.1 52.7 -4.6
per Lexeme 69.3 56.7 66.1 68.0 52.7 -4.5
per Lexeme-POS 68.8 57.1 66.7 67.4 55.0 -2.2
Semantic Similarity 69.0 58.2 1.2 64.9 -2.0 65.5 -2.2 52.2 -5.0
Punctuation 69.7 57.4 66.6 67.1 53.9 -3.4
Emoticon 69.3 57.0 66.8 67.8 57.3
Contrast 69.2 57.5 66.7 67.0 51.9 -5.4
Prefix 69.5 57.2 66.8 67.2 47.4 -9.9
Suffix 68.6 57.2 66.5 67.9 56.3
</table>
<bodyText confidence="0.999977943396227">
badly, ranking near the bottom (among 20 submis-
sions) on all datasets, a result perhaps expected
given the limited attention we gave to the model.
The subtask B system did very well on the three
Twitter datasets, ranking near the top (among 42
teams) on all three sets and placing second on the
sarcastic tweets set, but did notably worse on the
two non-Twitter sets.
A compact list of the features selected by the
subtask B system can be seen in Table 2. The ma-
jority of features (178 of 222) are lexicon-based,
29 are semantic similarities to known tweets and
the rest are mainly punctuation and emoticon fea-
tures. The lexicon-based features mostly come
from Emotiword, though that is probably because
Emotiword contains a rating for every unigram
and bigram in the tweets, unlike the other lexica.
The most important part-of-speech tags are adjec-
tives and verbs, as expected, with proper nouns
being also highly important, presumably as indi-
cators of attribution. Still, most features are cal-
culated over all tokens (including stop words). Fi-
nally it is worth noting the 72 contrast features se-
lected.
We also conducted a set of experiments using
partial feature sets: each time we use all features
minus one set, then apply feature selection and
classification. The results are presented in Ta-
ble 3. As expected, the lexicon-based features are
the most important ones by a wide margin though
the relative usefulness of the lexica changes de-
pending on the dataset: the twitter-specific NRC
lexicon actually hurts performance on non-tweets,
while the task-independent Emotiword hurts per-
formance on the sarcastic tweets set. Overall
though using all is the optimal choice. Among the
other features only semantic similarity provides a
relatively consistent improvement.
A lot of features provide very little benefit on
most sets, but virtually everything is important for
the sarcasm set. Lexica, particularly the twitter
specific ones like Sentiment 140 and the adapted
version of Emotiword make a big difference, per-
haps indicating some domain-specific aspects of
sarcasm expression (though such assumptions are
shaky at best due to the small size of the test
set). The contrast features perform their intended
function well, providing a large performance boost
when dealing with sarcastic tweets and perhaps
explaining our high ranking on that dataset.
Overall the subtask B system performed very
well and the semantic similarity features and con-
trast features provide potential for further growth.
</bodyText>
<sectionHeader confidence="0.999635" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999940909090909">
We presented a system of twitter sentiment anal-
ysis combining lexicon-based features with se-
mantic similarity and contrast features. The sys-
tem proved very successful, achieving high ranks
among all competing systems in the tasks of senti-
ment analysis of generic and sarcastic tweets.
Future work will focus on the semantic similar-
ity and contrast features by attempting more accu-
rately estimate semantic similarity and using some
more systematic way of identifying the “contrast-
ing” text areas.
</bodyText>
<page confidence="0.997334">
515
</page>
<sectionHeader confidence="0.996119" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999816411764706">
Eneko Agirre, Mona Diab, Daniel Cer, and Aitor
Gonzalez-Agirre. 2012. SemEval-2012 Task 6: A
pilot on semantic textual similarity. In proc. Se-
mEval, pages 385–393.
Margaret Bradley and Peter Lang. 1999. Affective
Norms for English Words (ANEW): Stimuli, in-
struction manual and affective ratings. technical re-
port C-1. The Center for Research in Psychophysi-
ology, University of Florida.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using Twitter hashtags
and smileys. In Proc. COLING, pages 241–249.
Andrea Esuli and Fabrizio Sebastiani. 2006. SENTI-
WORDNET: A publicly available lexical resource
for opinion mining. In Proc. LREC, pages 417–422.
Mark A. Hall. 1999. Correlation-based feature selec-
tion for machine learning. Ph.D. thesis, The Univer-
sity of Waikato.
Nidhi Kulkarni and Mark Alan Finlayson. 2011.
jMWE: A java toolkit for detecting multi-word ex-
pressions. In proc. Workshop on Multiword Expres-
sions, pages 122–124.
Nikolaos Malandrakis, Alexandros Potamianos, Elias
Iosif, and Shrikanth Narayanan. 2011. Kernel mod-
els for affective lexicon creation. In Proc. Inter-
speech, pages 2977–2980.
Nikolaos Malandrakis, Abe Kazemzadeh, Alexandros
Potamianos, and Shrikanth Narayanan. 2013a.
SAIL: A hybrid approach to sentiment analysis. In
proc. SemEval, pages 438–442.
Nikolaos Malandrakis, Alexandros Potamianos, Elias
Iosif, and Shrikanth Narayanan. 2013b. Distri-
butional semantic models for affective text analy-
sis. Audio, Speech, and Language Processing, IEEE
Transactions on, 21(11):2379–2392.
Nikolaos Malandrakis, Alexandros Potamianos,
Kean J. Hsu, Kalina N. Babeva, Michelle C. Feng,
Gerald C. Davison, and Shrikanth Narayanan. 2014.
Affective language model adaptation via corpus
selection. In proc. ICASSP, pages 4871–4874.
Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. NRC-Canada: Building the state-of-
the-art in sentiment analysis of tweets. In proc. Se-
mEval, pages 321–327.
Preslav Nakov, Zornitsa Kozareva, Alan Ritter, Sara
Rosenthal, Veselin Stoyanov, and Theresa Wilson.
2013. SemEval-2013 Task 2: Sentiment analysis in
Twitter. In Proc. SemEval, pages 312–320.
Olutobi Owoputi, Brendan O’Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A.
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
proc. NAACL, pages 380–390.
Antonio Reyes, Paolo Rosso, and Davide Buscaldi.
2012. From humor recognition to irony detection:
The figurative language of social media. Data &amp;
Knowledge Engineering, 74(0):1 – 12.
Sara Rosenthal, Preslav Nakov, Alan Ritter, and
Veselin Stoyanov. 2014. SemEval-2014 Task 9:
Sentiment analysis in Twitter. In Proc. SemEval.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann A.
Copestake, and Dan Flickinger. 2002. Multiword
expressions: A pain in the neck for NLP. In Compu-
tational Linguistics and Intelligent Text Processing,
volume 2276 of Lecture Notes in Computer Science,
pages 189–206.
Carlo Strapparava and Alessandro Valitutti. 2004.
WordNet-Affect: an affective extension of WordNet.
In Proc. LREC, volume 4, pages 1083–1086.
Peter D. Turney and Michael L. Littman. 2002. Un-
supervised learning of semantic orientation from a
hundred-billion-word corpus. technical report ERC-
1094 (NRC 44929). National Research Council of
Canada.
Peter D. Turney and Michael L. Littman. 2003. Mea-
suring praise and criticism: Inference of semantic
orientation from association. ACM Transactions on
Information Systems, 21:315–346.
Frane ˇSari´c, Goran Glavaˇs, Mladen Karan, Jan ˇSnajder,
and Bojana Dalbelo Baˇsi´c. 2012. Takelab: Systems
for measuring semantic text similarity. In proc. Se-
mEval, pages 441–448.
Ian H. Witten and Eibe Frank. 2000. Data Mining:
Practical Machine Learning Tools and Techniques.
Morgan Kaufmann.
</reference>
<page confidence="0.998333">
516
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.805515">
<title confidence="0.9988865">SAIL: Sentiment Analysis using Semantic Similarity and Contrast Features</title>
<author confidence="0.967659">Nikolaos Malandrakis</author>
<author confidence="0.967659">Michael Falcone</author>
<author confidence="0.967659">Colin Vaz</author>
<author confidence="0.967659">Jesse Alexandros Potamianos</author>
<author confidence="0.967659">Shrikanth Narayanan</author>
<address confidence="0.935219">Signal Analysis and Interpretation Laboratory (SAIL), USC, Los Angeles, CA 90089, USA</address>
<email confidence="0.998646">potam@telecom.tuc.gr,shri@sipi.usc.edu</email>
<abstract confidence="0.995097666666667">This paper describes our submission to SemEval2014 Task 9: Sentiment Analysis in Twitter. Our model is primarily a lexicon based one, augmented by some preprocessing, including detection of Multi- Word Expressions, negation propagation and hashtag expansion and by the use of pairwise semantic similarity at the tweet level. Feature extraction is repeated for sub-strings and contrasting sub-string features are used to better capture complex phenomena like sarcasm. The resulting supervised system, using a Naive Bayes model, achieved high performance in classifying entire tweets, ranking 7th on the main set and 2nd when applied to sarcastic tweets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Mona Diab</author>
<author>Daniel Cer</author>
<author>Aitor Gonzalez-Agirre</author>
</authors>
<title>SemEval-2012 Task 6: A pilot on semantic textual similarity.</title>
<date>2012</date>
<booktitle>In proc. SemEval,</booktitle>
<pages>385--393</pages>
<contexts>
<context position="8733" citStr="Agirre et al., 2012" startWordPosition="1360" endWordPosition="1363">ves over the maximum of all unigrams. The results of this process are features like “maximum of Emotiword valence over unigram adjectives” and “average of SentiWordNet objectivity among MWE bigrams”. 513 2.3 Tweet-level similarity ratings Our lexicon was formed under the assumption that semantic similarity implies affective similarity, which should apply to larger lexical units like entire tweets. To estimate semantic similarity scores between tweets we used the publicly available TakeLab semantic similarity toolkit (ˇSari´c et al., 2012) which is based on a submission to SemEval 2012 task 6 (Agirre et al., 2012). We used the data of SemEval 2012 task 6 to train three semantic similarity models corresponding to the three datasets of that task, plus an overall model. Using these models we created four similarity ratings between each tweet of interest and each tweet in the training set. These similarity ratings were used as features of the final model. 2.4 Character features Capitalization features are frequencies and relative frequencies at the word and letter level, extracted from all words that either start with a capital letter, have a capital letter in them (but the first letter is non-capital) or </context>
</contexts>
<marker>Agirre, Diab, Cer, Gonzalez-Agirre, 2012</marker>
<rawString>Eneko Agirre, Mona Diab, Daniel Cer, and Aitor Gonzalez-Agirre. 2012. SemEval-2012 Task 6: A pilot on semantic textual similarity. In proc. SemEval, pages 385–393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Margaret Bradley</author>
<author>Peter Lang</author>
</authors>
<title>Affective Norms for English Words (ANEW): Stimuli, instruction manual and affective ratings. technical report C-1. The Center for Research in Psychophysiology,</title>
<date>1999</date>
<institution>University of Florida.</institution>
<contexts>
<context position="6010" citStr="Bradley and Lang, 1999" startWordPosition="924" endWordPosition="927">sented as a linear combination of its semantic similarities dij to a set of seed words wi and the known affective ratings of these words v(wi), as follows: N ˆv(tj) = a0 + ai v(wi) dij, (1) i=1 where ai is the weight corresponding to seed word wi (that is estimated as described next). For the purposes of this work, dij is the cosine similarity between context vectors computed over a corpus of 116 million web snippets (up to 1000 for each word in the Aspell spellchecker) collected using the Yahoo! search engine. Given the starting, manually annotated, lexicon Affective Norms for English Words (Bradley and Lang, 1999) we selected 600 out of the 1034 words contained in it to serve as seed words and all 1034 words to act as the training set and used Least Squares Estimation to estimate the weights ai. Seed word selection was performed by a simple heuristic: we want seed words to have extreme affective ratings (high absolute value) and the set to be close to balanced (sum of seed ratings equal to zero). The equation learned was used to generate ratings for any new terms. The lexicon created by this method is taskindependent, since both the starting lexicon and the raw text corpus are task-independent. To crea</context>
</contexts>
<marker>Bradley, Lang, 1999</marker>
<rawString>Margaret Bradley and Peter Lang. 1999. Affective Norms for English Words (ANEW): Stimuli, instruction manual and affective ratings. technical report C-1. The Center for Research in Psychophysiology, University of Florida.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Oren Tsur</author>
<author>Ari Rappoport</author>
</authors>
<title>Enhanced sentiment learning using Twitter hashtags and smileys.</title>
<date>2010</date>
<booktitle>In Proc. COLING,</booktitle>
<pages>241--249</pages>
<contexts>
<context position="2352" citStr="Davidov et al., 2010" startWordPosition="335" endWordPosition="339">adth of possible content is virtually unlimited, the writing style is informal, the use of orthography and This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ grammar can be “unconventional” and there are unique artifacts like hashtags. Computation systems, like those submitted to SemEval 2013 task 2 (Nakov et al., 2013) mostly use bag-of-words models with specific features added to model emotion indicators like hashtags and emoticons (Davidov et al., 2010). This paper describes our submissions to SemEval 2014 task 9 (Rosenthal et al., 2014), which deals with sentiment analysis in twitter. The system is an expansion of our submission to the same task in 2013 (Malandrakis et al., 2013a), which used only token rating statistics as features. We expanded the system by using multiple lexica and more statistics, added steps to the pre-processing stage (including negation and multi-word expression handling), incorporated pairwise tweet-level semantic similarities as features and finally performed feature extraction on substrings and used the partial fe</context>
</contexts>
<marker>Davidov, Tsur, Rappoport, 2010</marker>
<rawString>Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010. Enhanced sentiment learning using Twitter hashtags and smileys. In Proc. COLING, pages 241–249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>SENTIWORDNET: A publicly available lexical resource for opinion mining.</title>
<date>2006</date>
<booktitle>In Proc. LREC,</booktitle>
<pages>417--422</pages>
<contexts>
<context position="1548" citStr="Esuli and Sebastiani, 2006" startWordPosition="218" endWordPosition="221">ng entire tweets, ranking 7th on the main set and 2nd when applied to sarcastic tweets. 1 Introduction The analysis of the emotional content of text is relevant to numerous natural language processing (NLP), web and multi-modal dialogue applications. In recent years the increased popularity of social media and increased availability of relevant data has led to a focus of scientific efforts on the emotion expressed through social media, with Twitter being the most common subject. Sentiment analysis in Twitter is usually performed by combining techniques used for related tasks, like word-level (Esuli and Sebastiani, 2006; Strapparava and Valitutti, 2004) and sentencelevel (Turney and Littman, 2002; Turney and Littman, 2003) emotion extraction. Twitter however does present specific challenges: the breadth of possible content is virtually unlimited, the writing style is informal, the use of orthography and This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ grammar can be “unconventional” and there are unique artifacts like hashtags. Computation systems,</context>
<context position="4244" citStr="Esuli and Sebastiani, 2006" startWordPosition="627" endWordPosition="630">ed using the MIT jMWE library (Kulkarni and Finlayson, 2011). MWEs are non-compositional expressions (Sag et al., 2002), which should be 512 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 512–516, Dublin, Ireland, August 23-24, 2014. handled as a single token instead of attempting to reconstruct their meaning from their parts. 2.2 Lexicon-based features The core of the system was formed by the lexiconbased features. We used a total of four lexica and some derivatives. 2.2.1 Third party lexica We used three third party affective lexica. SentiWordNet (Esuli and Sebastiani, 2006) provides continuous positive, negative and neutral ratings for each sense of every word in WordNet. We created two versions of SentiWordNet: one where ratings are averaged over all senses of a word (e.g., one ratings for “good”) and one where ratings are averaged over lexeme-pos pairs (e.g., one rating for the adjective “good” and one for the noun “good”). NRC Hashtag (Mohammad et al., 2013) Sentiment Lexicon provides continuous polarity ratings for tokens, generated from a collection of tweets that had a positive or a negative word hashtag. Sentiment140 (Mohammad et al., 2013) Lexicon provid</context>
</contexts>
<marker>Esuli, Sebastiani, 2006</marker>
<rawString>Andrea Esuli and Fabrizio Sebastiani. 2006. SENTIWORDNET: A publicly available lexical resource for opinion mining. In Proc. LREC, pages 417–422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark A Hall</author>
</authors>
<title>Correlation-based feature selection for machine learning.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>The University of Waikato.</institution>
<contexts>
<context position="10853" citStr="Hall, 1999" startWordPosition="1711" endWordPosition="1712">ve (except for the semantic similarity features) for the prefix and suffix, nearly tripling the total number of features. 2.6 Feature selection and Training The extraction process lead to tens of thousands of candidate features, so we performed forward stepwise feature selection using a correlation criteTable 1: Performance and rank achieved by our submission for all datasets of subtasks A and B. task dataset avg. F1 rank LJ2014 70.62 16 SMS2013 74.46 16 A TW2013 78.47 14 TW2014 76.89 13 TW2014SC 65.56 15 LJ2014 69.34 15 SMS2013 56.98 24 B TW2013 66.80 10 TW2014 67.77 7 TW2014SC 57.26 2 rion (Hall, 1999) and used the resulting set of 222 features to train a model. The model chosen is a Naive Bayes tree, a tree with Naive Bayes classifiers on each leaf. The motivation comes from considering this a two stage problem: subjectivity detection and polarity classification, making a hierarchical model a natural choice. The feature selection and model training/classification was conducted using Weka (Witten and Frank, 2000). Table 2: Selected features for subtask B. Features number Lexicon-derived 178 By lexicon Ewrd / S140 / SWNet / NRC 71 / 53 / 33 / 21 By POS tag all (ignore tag) 103 adj / verb / p</context>
</contexts>
<marker>Hall, 1999</marker>
<rawString>Mark A. Hall. 1999. Correlation-based feature selection for machine learning. Ph.D. thesis, The University of Waikato.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nidhi Kulkarni</author>
<author>Mark Alan Finlayson</author>
</authors>
<title>jMWE: A java toolkit for detecting multi-word expressions.</title>
<date>2011</date>
<booktitle>In proc. Workshop on Multiword Expressions,</booktitle>
<pages>122--124</pages>
<contexts>
<context position="3677" citStr="Kulkarni and Finlayson, 2011" startWordPosition="541" endWordPosition="544">ng / Tokenization was performed using the ARK NLP tweeter tagger (Owoputi et al., 2013), a Twitter-specific tagger. Negations were detected using the list from Christopher Potts’ tutorial. All tokens up to the next punctuation were marked as negated. Hashtag expansion into word strings was performed using a combination of a word insertion Finite State Machine and a language model. A normalized perplexity threshold was used to detect if the output was a “proper” English string and expansion was not performed if it was not. Multi-word Expressions (MWEs) were detected using the MIT jMWE library (Kulkarni and Finlayson, 2011). MWEs are non-compositional expressions (Sag et al., 2002), which should be 512 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 512–516, Dublin, Ireland, August 23-24, 2014. handled as a single token instead of attempting to reconstruct their meaning from their parts. 2.2 Lexicon-based features The core of the system was formed by the lexiconbased features. We used a total of four lexica and some derivatives. 2.2.1 Third party lexica We used three third party affective lexica. SentiWordNet (Esuli and Sebastiani, 2006) provides continuous positive, ne</context>
</contexts>
<marker>Kulkarni, Finlayson, 2011</marker>
<rawString>Nidhi Kulkarni and Mark Alan Finlayson. 2011. jMWE: A java toolkit for detecting multi-word expressions. In proc. Workshop on Multiword Expressions, pages 122–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikolaos Malandrakis</author>
<author>Alexandros Potamianos</author>
<author>Elias Iosif</author>
<author>Shrikanth Narayanan</author>
</authors>
<title>Kernel models for affective lexicon creation.</title>
<date>2011</date>
<booktitle>In Proc. Interspeech,</booktitle>
<pages>2977--2980</pages>
<contexts>
<context position="5187" citStr="Malandrakis et al., 2011" startWordPosition="779" endWordPosition="782">ive “good” and one for the noun “good”). NRC Hashtag (Mohammad et al., 2013) Sentiment Lexicon provides continuous polarity ratings for tokens, generated from a collection of tweets that had a positive or a negative word hashtag. Sentiment140 (Mohammad et al., 2013) Lexicon provides continuous polarity ratings for tokens, generated from the sentiment140 corpus of 1.6 million tweets, with emoticons used as positive and negative labels. 2.2.2 Emotiword: expansion and adaptation To create our own lexicon we used an automated algorithm of affective lexicon expansion based on the one presented in (Malandrakis et al., 2011; Malandrakis et al., 2013b), which in turn is an expansion of (Turney and Littman, 2002). We assume that the continuous (in [−1, 1]) valence, arousal and dominance ratings of any term tj can be represented as a linear combination of its semantic similarities dij to a set of seed words wi and the known affective ratings of these words v(wi), as follows: N ˆv(tj) = a0 + ai v(wi) dij, (1) i=1 where ai is the weight corresponding to seed word wi (that is estimated as described next). For the purposes of this work, dij is the cosine similarity between context vectors computed over a corpus of 116 </context>
</contexts>
<marker>Malandrakis, Potamianos, Iosif, Narayanan, 2011</marker>
<rawString>Nikolaos Malandrakis, Alexandros Potamianos, Elias Iosif, and Shrikanth Narayanan. 2011. Kernel models for affective lexicon creation. In Proc. Interspeech, pages 2977–2980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikolaos Malandrakis</author>
<author>Abe Kazemzadeh</author>
<author>Alexandros Potamianos</author>
<author>Shrikanth Narayanan</author>
</authors>
<title>SAIL: A hybrid approach to sentiment analysis.</title>
<date>2013</date>
<booktitle>In proc. SemEval,</booktitle>
<pages>438--442</pages>
<contexts>
<context position="2583" citStr="Malandrakis et al., 2013" startWordPosition="377" endWordPosition="380"> are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ grammar can be “unconventional” and there are unique artifacts like hashtags. Computation systems, like those submitted to SemEval 2013 task 2 (Nakov et al., 2013) mostly use bag-of-words models with specific features added to model emotion indicators like hashtags and emoticons (Davidov et al., 2010). This paper describes our submissions to SemEval 2014 task 9 (Rosenthal et al., 2014), which deals with sentiment analysis in twitter. The system is an expansion of our submission to the same task in 2013 (Malandrakis et al., 2013a), which used only token rating statistics as features. We expanded the system by using multiple lexica and more statistics, added steps to the pre-processing stage (including negation and multi-word expression handling), incorporated pairwise tweet-level semantic similarities as features and finally performed feature extraction on substrings and used the partial features as indicators of irony, sarcasm or humor. 2 Model Description 2.1 Preprocessing POS-tagging / Tokenization was performed using the ARK NLP tweeter tagger (Owoputi et al., 2013), a Twitter-specific tagger. Negations were dete</context>
<context position="5213" citStr="Malandrakis et al., 2013" startWordPosition="783" endWordPosition="786"> noun “good”). NRC Hashtag (Mohammad et al., 2013) Sentiment Lexicon provides continuous polarity ratings for tokens, generated from a collection of tweets that had a positive or a negative word hashtag. Sentiment140 (Mohammad et al., 2013) Lexicon provides continuous polarity ratings for tokens, generated from the sentiment140 corpus of 1.6 million tweets, with emoticons used as positive and negative labels. 2.2.2 Emotiword: expansion and adaptation To create our own lexicon we used an automated algorithm of affective lexicon expansion based on the one presented in (Malandrakis et al., 2011; Malandrakis et al., 2013b), which in turn is an expansion of (Turney and Littman, 2002). We assume that the continuous (in [−1, 1]) valence, arousal and dominance ratings of any term tj can be represented as a linear combination of its semantic similarities dij to a set of seed words wi and the known affective ratings of these words v(wi), as follows: N ˆv(tj) = a0 + ai v(wi) dij, (1) i=1 where ai is the weight corresponding to seed word wi (that is estimated as described next). For the purposes of this work, dij is the cosine similarity between context vectors computed over a corpus of 116 million web snippets (up t</context>
</contexts>
<marker>Malandrakis, Kazemzadeh, Potamianos, Narayanan, 2013</marker>
<rawString>Nikolaos Malandrakis, Abe Kazemzadeh, Alexandros Potamianos, and Shrikanth Narayanan. 2013a. SAIL: A hybrid approach to sentiment analysis. In proc. SemEval, pages 438–442.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikolaos Malandrakis</author>
<author>Alexandros Potamianos</author>
<author>Elias Iosif</author>
<author>Shrikanth Narayanan</author>
</authors>
<title>Distributional semantic models for affective text analysis. Audio, Speech, and Language Processing,</title>
<date>2013</date>
<journal>IEEE Transactions on,</journal>
<volume>21</volume>
<issue>11</issue>
<contexts>
<context position="2583" citStr="Malandrakis et al., 2013" startWordPosition="377" endWordPosition="380"> are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ grammar can be “unconventional” and there are unique artifacts like hashtags. Computation systems, like those submitted to SemEval 2013 task 2 (Nakov et al., 2013) mostly use bag-of-words models with specific features added to model emotion indicators like hashtags and emoticons (Davidov et al., 2010). This paper describes our submissions to SemEval 2014 task 9 (Rosenthal et al., 2014), which deals with sentiment analysis in twitter. The system is an expansion of our submission to the same task in 2013 (Malandrakis et al., 2013a), which used only token rating statistics as features. We expanded the system by using multiple lexica and more statistics, added steps to the pre-processing stage (including negation and multi-word expression handling), incorporated pairwise tweet-level semantic similarities as features and finally performed feature extraction on substrings and used the partial features as indicators of irony, sarcasm or humor. 2 Model Description 2.1 Preprocessing POS-tagging / Tokenization was performed using the ARK NLP tweeter tagger (Owoputi et al., 2013), a Twitter-specific tagger. Negations were dete</context>
<context position="5213" citStr="Malandrakis et al., 2013" startWordPosition="783" endWordPosition="786"> noun “good”). NRC Hashtag (Mohammad et al., 2013) Sentiment Lexicon provides continuous polarity ratings for tokens, generated from a collection of tweets that had a positive or a negative word hashtag. Sentiment140 (Mohammad et al., 2013) Lexicon provides continuous polarity ratings for tokens, generated from the sentiment140 corpus of 1.6 million tweets, with emoticons used as positive and negative labels. 2.2.2 Emotiword: expansion and adaptation To create our own lexicon we used an automated algorithm of affective lexicon expansion based on the one presented in (Malandrakis et al., 2011; Malandrakis et al., 2013b), which in turn is an expansion of (Turney and Littman, 2002). We assume that the continuous (in [−1, 1]) valence, arousal and dominance ratings of any term tj can be represented as a linear combination of its semantic similarities dij to a set of seed words wi and the known affective ratings of these words v(wi), as follows: N ˆv(tj) = a0 + ai v(wi) dij, (1) i=1 where ai is the weight corresponding to seed word wi (that is estimated as described next). For the purposes of this work, dij is the cosine similarity between context vectors computed over a corpus of 116 million web snippets (up t</context>
</contexts>
<marker>Malandrakis, Potamianos, Iosif, Narayanan, 2013</marker>
<rawString>Nikolaos Malandrakis, Alexandros Potamianos, Elias Iosif, and Shrikanth Narayanan. 2013b. Distributional semantic models for affective text analysis. Audio, Speech, and Language Processing, IEEE Transactions on, 21(11):2379–2392.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikolaos Malandrakis</author>
<author>Alexandros Potamianos</author>
<author>Kean J Hsu</author>
<author>Kalina N Babeva</author>
<author>Michelle C Feng</author>
<author>Gerald C Davison</author>
<author>Shrikanth Narayanan</author>
</authors>
<title>Affective language model adaptation via corpus selection.</title>
<date>2014</date>
<booktitle>In proc. ICASSP,</booktitle>
<pages>4871--4874</pages>
<contexts>
<context position="7000" citStr="Malandrakis et al., 2014" startWordPosition="1092" endWordPosition="1095">d ratings equal to zero). The equation learned was used to generate ratings for any new terms. The lexicon created by this method is taskindependent, since both the starting lexicon and the raw text corpus are task-independent. To create task-specific lexica we used corpus filtering on the 116 million sentences to select ones that match our domain, using either a normalized perplexity threshold (using a maximum likelihood trigram model created from the training set tweets) or a combination of pragmatic constraints (keywords with high mutual information with the task) and perplexity threshold (Malandrakis et al., 2014). Then we re-calculated semantic similarities on the filtered corpora. In total we created three lexica: a task-independent (base) version and two adapted versions (filtered by perplexity alone and filtered by combining pragmatics and perplexity), all containing valence, arousal and dominance token ratings. 2.2.3 Statistics extraction The lexica provide up to 17 ratings for each token. To extract tweet-level features we used simple statistics and selection criteria. First, all token unigrams and bigrams contained in a tweet were collected. Some of these n-grams were selected based on a criteri</context>
</contexts>
<marker>Malandrakis, Potamianos, Hsu, Babeva, Feng, Davison, Narayanan, 2014</marker>
<rawString>Nikolaos Malandrakis, Alexandros Potamianos, Kean J. Hsu, Kalina N. Babeva, Michelle C. Feng, Gerald C. Davison, and Shrikanth Narayanan. 2014. Affective language model adaptation via corpus selection. In proc. ICASSP, pages 4871–4874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammad</author>
<author>Svetlana Kiritchenko</author>
<author>Xiaodan Zhu</author>
</authors>
<title>NRC-Canada: Building the state-ofthe-art in sentiment analysis of tweets.</title>
<date>2013</date>
<booktitle>In proc. SemEval,</booktitle>
<pages>321--327</pages>
<contexts>
<context position="4639" citStr="Mohammad et al., 2013" startWordPosition="694" endWordPosition="697">The core of the system was formed by the lexiconbased features. We used a total of four lexica and some derivatives. 2.2.1 Third party lexica We used three third party affective lexica. SentiWordNet (Esuli and Sebastiani, 2006) provides continuous positive, negative and neutral ratings for each sense of every word in WordNet. We created two versions of SentiWordNet: one where ratings are averaged over all senses of a word (e.g., one ratings for “good”) and one where ratings are averaged over lexeme-pos pairs (e.g., one rating for the adjective “good” and one for the noun “good”). NRC Hashtag (Mohammad et al., 2013) Sentiment Lexicon provides continuous polarity ratings for tokens, generated from a collection of tweets that had a positive or a negative word hashtag. Sentiment140 (Mohammad et al., 2013) Lexicon provides continuous polarity ratings for tokens, generated from the sentiment140 corpus of 1.6 million tweets, with emoticons used as positive and negative labels. 2.2.2 Emotiword: expansion and adaptation To create our own lexicon we used an automated algorithm of affective lexicon expansion based on the one presented in (Malandrakis et al., 2011; Malandrakis et al., 2013b), which in turn is an ex</context>
</contexts>
<marker>Mohammad, Kiritchenko, Zhu, 2013</marker>
<rawString>Saif Mohammad, Svetlana Kiritchenko, and Xiaodan Zhu. 2013. NRC-Canada: Building the state-ofthe-art in sentiment analysis of tweets. In proc. SemEval, pages 321–327.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Zornitsa Kozareva</author>
<author>Alan Ritter</author>
<author>Sara Rosenthal</author>
<author>Veselin Stoyanov</author>
<author>Theresa Wilson</author>
</authors>
<date>2013</date>
<booktitle>SemEval-2013 Task 2: Sentiment analysis in Twitter. In Proc. SemEval,</booktitle>
<pages>312--320</pages>
<contexts>
<context position="2213" citStr="Nakov et al., 2013" startWordPosition="314" endWordPosition="317">elevel (Turney and Littman, 2002; Turney and Littman, 2003) emotion extraction. Twitter however does present specific challenges: the breadth of possible content is virtually unlimited, the writing style is informal, the use of orthography and This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ grammar can be “unconventional” and there are unique artifacts like hashtags. Computation systems, like those submitted to SemEval 2013 task 2 (Nakov et al., 2013) mostly use bag-of-words models with specific features added to model emotion indicators like hashtags and emoticons (Davidov et al., 2010). This paper describes our submissions to SemEval 2014 task 9 (Rosenthal et al., 2014), which deals with sentiment analysis in twitter. The system is an expansion of our submission to the same task in 2013 (Malandrakis et al., 2013a), which used only token rating statistics as features. We expanded the system by using multiple lexica and more statistics, added steps to the pre-processing stage (including negation and multi-word expression handling), incorpo</context>
</contexts>
<marker>Nakov, Kozareva, Ritter, Rosenthal, Stoyanov, Wilson, 2013</marker>
<rawString>Preslav Nakov, Zornitsa Kozareva, Alan Ritter, Sara Rosenthal, Veselin Stoyanov, and Theresa Wilson. 2013. SemEval-2013 Task 2: Sentiment analysis in Twitter. In Proc. SemEval, pages 312–320.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olutobi Owoputi</author>
<author>Brendan O’Connor</author>
<author>Chris Dyer</author>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Noah A Smith</author>
</authors>
<title>Improved part-of-speech tagging for online conversational text with word clusters.</title>
<date>2013</date>
<booktitle>In proc. NAACL,</booktitle>
<pages>380--390</pages>
<marker>Owoputi, O’Connor, Dyer, Gimpel, Schneider, Smith, 2013</marker>
<rawString>Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A. Smith. 2013. Improved part-of-speech tagging for online conversational text with word clusters. In proc. NAACL, pages 380–390.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antonio Reyes</author>
<author>Paolo Rosso</author>
<author>Davide Buscaldi</author>
</authors>
<title>From humor recognition to irony detection: The figurative language of social media.</title>
<date>2012</date>
<journal>Data &amp; Knowledge Engineering,</journal>
<volume>74</volume>
<issue>0</issue>
<contexts>
<context position="9838" citStr="Reyes et al., 2012" startWordPosition="1531" endWordPosition="1534">s that either start with a capital letter, have a capital letter in them (but the first letter is non-capital) or are in all capital letters. Punctuation features are frequencies, relative frequencies and punctuation unigrams. Character repetition features are frequencies, relative frequencies and longest string statistics of words containing a repetition of the same letter. Emoticon features are frequencies, relative frequencies, and emoticon unigrams. 2.5 Contrast features Cognitive Dissonance is an important phenomenon associated with complex linguistic cases like sarcasm, irony and humor (Reyes et al., 2012). To estimate it we used a simple approach, inspired by one-liner joke detection: we assumed that the final few tokens of each tweet (the “suffix”) contrast the rest of the tweet (the “prefix”) and created split versions of the tweet where the last N tokens are the suffix and all other tokens are the prefix, for N = 2 and N = 3. We repeated the feature extraction process for all features mentioned above (except for the semantic similarity features) for the prefix and suffix, nearly tripling the total number of features. 2.6 Feature selection and Training The extraction process lead to tens of </context>
</contexts>
<marker>Reyes, Rosso, Buscaldi, 2012</marker>
<rawString>Antonio Reyes, Paolo Rosso, and Davide Buscaldi. 2012. From humor recognition to irony detection: The figurative language of social media. Data &amp; Knowledge Engineering, 74(0):1 – 12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Rosenthal</author>
<author>Preslav Nakov</author>
<author>Alan Ritter</author>
<author>Veselin Stoyanov</author>
</authors>
<date>2014</date>
<booktitle>SemEval-2014 Task 9: Sentiment analysis in Twitter. In Proc. SemEval.</booktitle>
<contexts>
<context position="2438" citStr="Rosenthal et al., 2014" startWordPosition="351" endWordPosition="354">use of orthography and This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ grammar can be “unconventional” and there are unique artifacts like hashtags. Computation systems, like those submitted to SemEval 2013 task 2 (Nakov et al., 2013) mostly use bag-of-words models with specific features added to model emotion indicators like hashtags and emoticons (Davidov et al., 2010). This paper describes our submissions to SemEval 2014 task 9 (Rosenthal et al., 2014), which deals with sentiment analysis in twitter. The system is an expansion of our submission to the same task in 2013 (Malandrakis et al., 2013a), which used only token rating statistics as features. We expanded the system by using multiple lexica and more statistics, added steps to the pre-processing stage (including negation and multi-word expression handling), incorporated pairwise tweet-level semantic similarities as features and finally performed feature extraction on substrings and used the partial features as indicators of irony, sarcasm or humor. 2 Model Description 2.1 Preprocessing</context>
</contexts>
<marker>Rosenthal, Nakov, Ritter, Stoyanov, 2014</marker>
<rawString>Sara Rosenthal, Preslav Nakov, Alan Ritter, and Veselin Stoyanov. 2014. SemEval-2014 Task 9: Sentiment analysis in Twitter. In Proc. SemEval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan A Sag</author>
<author>Timothy Baldwin</author>
<author>Francis Bond</author>
<author>Ann A Copestake</author>
<author>Dan Flickinger</author>
</authors>
<title>Multiword expressions: A pain in the neck for NLP.</title>
<date>2002</date>
<booktitle>In Computational Linguistics and Intelligent Text Processing,</booktitle>
<volume>2276</volume>
<pages>189--206</pages>
<contexts>
<context position="3736" citStr="Sag et al., 2002" startWordPosition="549" endWordPosition="552">ti et al., 2013), a Twitter-specific tagger. Negations were detected using the list from Christopher Potts’ tutorial. All tokens up to the next punctuation were marked as negated. Hashtag expansion into word strings was performed using a combination of a word insertion Finite State Machine and a language model. A normalized perplexity threshold was used to detect if the output was a “proper” English string and expansion was not performed if it was not. Multi-word Expressions (MWEs) were detected using the MIT jMWE library (Kulkarni and Finlayson, 2011). MWEs are non-compositional expressions (Sag et al., 2002), which should be 512 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 512–516, Dublin, Ireland, August 23-24, 2014. handled as a single token instead of attempting to reconstruct their meaning from their parts. 2.2 Lexicon-based features The core of the system was formed by the lexiconbased features. We used a total of four lexica and some derivatives. 2.2.1 Third party lexica We used three third party affective lexica. SentiWordNet (Esuli and Sebastiani, 2006) provides continuous positive, negative and neutral ratings for each sense of every word in </context>
</contexts>
<marker>Sag, Baldwin, Bond, Copestake, Flickinger, 2002</marker>
<rawString>Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann A. Copestake, and Dan Flickinger. 2002. Multiword expressions: A pain in the neck for NLP. In Computational Linguistics and Intelligent Text Processing, volume 2276 of Lecture Notes in Computer Science, pages 189–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlo Strapparava</author>
<author>Alessandro Valitutti</author>
</authors>
<title>WordNet-Affect: an affective extension of WordNet.</title>
<date>2004</date>
<booktitle>In Proc. LREC,</booktitle>
<volume>4</volume>
<pages>1083--1086</pages>
<contexts>
<context position="1582" citStr="Strapparava and Valitutti, 2004" startWordPosition="222" endWordPosition="225">h on the main set and 2nd when applied to sarcastic tweets. 1 Introduction The analysis of the emotional content of text is relevant to numerous natural language processing (NLP), web and multi-modal dialogue applications. In recent years the increased popularity of social media and increased availability of relevant data has led to a focus of scientific efforts on the emotion expressed through social media, with Twitter being the most common subject. Sentiment analysis in Twitter is usually performed by combining techniques used for related tasks, like word-level (Esuli and Sebastiani, 2006; Strapparava and Valitutti, 2004) and sentencelevel (Turney and Littman, 2002; Turney and Littman, 2003) emotion extraction. Twitter however does present specific challenges: the breadth of possible content is virtually unlimited, the writing style is informal, the use of orthography and This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ grammar can be “unconventional” and there are unique artifacts like hashtags. Computation systems, like those submitted to SemEval 2</context>
</contexts>
<marker>Strapparava, Valitutti, 2004</marker>
<rawString>Carlo Strapparava and Alessandro Valitutti. 2004. WordNet-Affect: an affective extension of WordNet. In Proc. LREC, volume 4, pages 1083–1086.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Michael L Littman</author>
</authors>
<title>Unsupervised learning of semantic orientation from a hundred-billion-word corpus. technical report</title>
<date>2002</date>
<booktitle>ERC1094 (NRC 44929). National Research Council of Canada.</booktitle>
<contexts>
<context position="1626" citStr="Turney and Littman, 2002" startWordPosition="229" endWordPosition="232">tweets. 1 Introduction The analysis of the emotional content of text is relevant to numerous natural language processing (NLP), web and multi-modal dialogue applications. In recent years the increased popularity of social media and increased availability of relevant data has led to a focus of scientific efforts on the emotion expressed through social media, with Twitter being the most common subject. Sentiment analysis in Twitter is usually performed by combining techniques used for related tasks, like word-level (Esuli and Sebastiani, 2006; Strapparava and Valitutti, 2004) and sentencelevel (Turney and Littman, 2002; Turney and Littman, 2003) emotion extraction. Twitter however does present specific challenges: the breadth of possible content is virtually unlimited, the writing style is informal, the use of orthography and This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ grammar can be “unconventional” and there are unique artifacts like hashtags. Computation systems, like those submitted to SemEval 2013 task 2 (Nakov et al., 2013) mostly use b</context>
<context position="5276" citStr="Turney and Littman, 2002" startWordPosition="795" endWordPosition="798">exicon provides continuous polarity ratings for tokens, generated from a collection of tweets that had a positive or a negative word hashtag. Sentiment140 (Mohammad et al., 2013) Lexicon provides continuous polarity ratings for tokens, generated from the sentiment140 corpus of 1.6 million tweets, with emoticons used as positive and negative labels. 2.2.2 Emotiword: expansion and adaptation To create our own lexicon we used an automated algorithm of affective lexicon expansion based on the one presented in (Malandrakis et al., 2011; Malandrakis et al., 2013b), which in turn is an expansion of (Turney and Littman, 2002). We assume that the continuous (in [−1, 1]) valence, arousal and dominance ratings of any term tj can be represented as a linear combination of its semantic similarities dij to a set of seed words wi and the known affective ratings of these words v(wi), as follows: N ˆv(tj) = a0 + ai v(wi) dij, (1) i=1 where ai is the weight corresponding to seed word wi (that is estimated as described next). For the purposes of this work, dij is the cosine similarity between context vectors computed over a corpus of 116 million web snippets (up to 1000 for each word in the Aspell spellchecker) collected usin</context>
</contexts>
<marker>Turney, Littman, 2002</marker>
<rawString>Peter D. Turney and Michael L. Littman. 2002. Unsupervised learning of semantic orientation from a hundred-billion-word corpus. technical report ERC1094 (NRC 44929). National Research Council of Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Michael L Littman</author>
</authors>
<title>Measuring praise and criticism: Inference of semantic orientation from association.</title>
<date>2003</date>
<journal>ACM Transactions on Information Systems,</journal>
<pages>21--315</pages>
<contexts>
<context position="1653" citStr="Turney and Littman, 2003" startWordPosition="233" endWordPosition="236"> analysis of the emotional content of text is relevant to numerous natural language processing (NLP), web and multi-modal dialogue applications. In recent years the increased popularity of social media and increased availability of relevant data has led to a focus of scientific efforts on the emotion expressed through social media, with Twitter being the most common subject. Sentiment analysis in Twitter is usually performed by combining techniques used for related tasks, like word-level (Esuli and Sebastiani, 2006; Strapparava and Valitutti, 2004) and sentencelevel (Turney and Littman, 2002; Turney and Littman, 2003) emotion extraction. Twitter however does present specific challenges: the breadth of possible content is virtually unlimited, the writing style is informal, the use of orthography and This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ grammar can be “unconventional” and there are unique artifacts like hashtags. Computation systems, like those submitted to SemEval 2013 task 2 (Nakov et al., 2013) mostly use bag-of-words models with spe</context>
</contexts>
<marker>Turney, Littman, 2003</marker>
<rawString>Peter D. Turney and Michael L. Littman. 2003. Measuring praise and criticism: Inference of semantic orientation from association. ACM Transactions on Information Systems, 21:315–346.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frane ˇSari´c</author>
<author>Goran Glavaˇs</author>
<author>Mladen Karan</author>
</authors>
<title>Snajder, and Bojana Dalbelo Baˇsi´c.</title>
<date></date>
<booktitle>In proc. SemEval,</booktitle>
<pages>441--448</pages>
<marker>ˇSari´c, Glavaˇs, Karan, </marker>
<rawString>Frane ˇSari´c, Goran Glavaˇs, Mladen Karan, Jan ˇSnajder, and Bojana Dalbelo Baˇsi´c. 2012. Takelab: Systems for measuring semantic text similarity. In proc. SemEval, pages 441–448.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Eibe Frank</author>
</authors>
<title>Data Mining: Practical Machine Learning Tools and Techniques.</title>
<date>2000</date>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="11272" citStr="Witten and Frank, 2000" startWordPosition="1778" endWordPosition="1781">taset avg. F1 rank LJ2014 70.62 16 SMS2013 74.46 16 A TW2013 78.47 14 TW2014 76.89 13 TW2014SC 65.56 15 LJ2014 69.34 15 SMS2013 56.98 24 B TW2013 66.80 10 TW2014 67.77 7 TW2014SC 57.26 2 rion (Hall, 1999) and used the resulting set of 222 features to train a model. The model chosen is a Naive Bayes tree, a tree with Naive Bayes classifiers on each leaf. The motivation comes from considering this a two stage problem: subjectivity detection and polarity classification, making a hierarchical model a natural choice. The feature selection and model training/classification was conducted using Weka (Witten and Frank, 2000). Table 2: Selected features for subtask B. Features number Lexicon-derived 178 By lexicon Ewrd / S140 / SWNet / NRC 71 / 53 / 33 / 21 By POS tag all (ignore tag) 103 adj / verb / proper noun 25 / 11 / 11 other tags 28 By function avg / min / sum / max 45 / 40 / 38 / 26 other functions 29 Semantic similarity 29 Punctuation 7 Emoticon 5 Other features 3 Contrast 72 prefix / suffix 54 / 18 3 Results We took part in subtasks A and B of SemEval 2014 task 9, submitting constrained runs trained with the data the task organizers provided. Subtask B was the priority and the subtask A model was created</context>
</contexts>
<marker>Witten, Frank, 2000</marker>
<rawString>Ian H. Witten and Eibe Frank. 2000. Data Mining: Practical Machine Learning Tools and Techniques. Morgan Kaufmann.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>