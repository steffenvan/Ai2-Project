<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000240">
<title confidence="0.972715">
Implicit Entity Recognition in Clinical Documents
</title>
<author confidence="0.747602">
Sujan Perera*, Pablo Mendes†, Amit Sheth*, Krishnaprasad Thirunarayan*,
Adarsh Alex*, Christopher Heid¶, Greg Mott¶
</author>
<affiliation confidence="0.365169666666667">
*Kno.e.sis Center, Wright State University, Dayton, OH, USA
†IBM Research, San Jose, CA, USA
¶Boonshoft School of Medicine, Wright State University, Dayton, OH, USA
</affiliation>
<email confidence="0.874316">
sujan@knoesis.org, pnmendes@us.ibm.com, amit@knoesis.org, tkprasad@knoesis.org
adarsh@knoesis.org, caheid2@gmail.com, mott11@wright.edu
</email>
<sectionHeader confidence="0.99735" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999966033333334">
With the increasing automation of health care
information processing, it has become crucial
to extract meaningful information from textual
notes in electronic medical records. One of
the key challenges is to extract and normalize
entity mentions. State-of-the-art approaches
have focused on the recognition of entities that
are explicitly mentioned in a sentence. How-
ever, clinical documents often contain phrases
that indicate the entities but do not contain
their names. We term those implicit entity
mentions and introduce the problem of im-
plicit entity recognition (IER) in clinical doc-
uments. We propose a solution to IER that
leverages entity definitions from a knowledge
base to create entity models, projects sen-
tences to the entity models and identifies im-
plicit entity mentions by evaluating semantic
similarity between sentences and entity mod-
els. The evaluation with 857 sentences se-
lected for 8 different entities shows that our al-
gorithm outperforms the most closely related
unsupervised solution. The similarity value
calculated by our algorithm proved to be an
effective feature in a supervised learning set-
ting, helping it to improve over the baselines,
and achieving F1 scores of .81 and .73 for dif-
ferent classes of implicit mentions. Our gold
standard annotations are made available to en-
courage further research in the area of IER.
</bodyText>
<sectionHeader confidence="0.999629" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998903975609756">
Consider the following sentence, extracted from
a clinical document: “Patient has shortness of
breath with reaccumulation of fluid in extremities.”
It states that the patient has ‘shortness of breath’ and
‘edema’. The former is explicitly mentioned, while
the latter is implied by the semantics of the phrase
‘reaccumulation of fluid in extremities’. We term
such occurrences implicit entity mentions.
While implicit entity mentions are common in
many domains, resolving them is particularly valu-
able in the clinical domain. Clinical documents
are rich in information content that plays a cen-
tral role in understanding patients’ health status and
improving the quality of the delivered services. It
is a common practice to employ computer assisted
coding (CAC) solutions to assist expert “coders”
in determining the unique identifier (e.g., ICD9 or
ICD10) for each medical condition or combina-
tion of conditions. These identifiers are impor-
tant to unambiguously represent the medical con-
ditions, to prepare the post-discharge plan, and to
perform secondary data analysis tasks. A human
coder reading the sentence ‘Patient has shortness
of breath with reaccumulation of fluid in extremi-
ties’ would generate the corresponding codes for en-
tities ‘shortness of breath’ and ‘edema’. However,
the solutions developed to perform entity recogni-
tion in clinical documents (Aronson, 2006) (Fried-
man et al., 1994) (Savova et al., 2010) (Friedman
et al., 2004) (Fu and Ananiadou, 2014) (Pradhan et
al., 2015) do not recognize the presence of entity
‘edema’ in this sentence.
Implicit entity mentions are a common occur-
rence in clinical documents as they are often typed
during a patient visit in a way that is natural in spo-
ken language and meant for consumption by the
professionals with similar backgrounds. An anal-
ysis with 300 documents in our corpus showed
that 35% of the ‘edema’ mentions and 40% of the
‘shortness of breath’ mentions are implicit.
Recognizing implicit mentions is particularly
</bodyText>
<page confidence="0.968848">
228
</page>
<note confidence="0.9533095">
Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 228–238,
Denver, Colorado, June 4–5, 2015.
</note>
<bodyText confidence="0.999672043478261">
challenging since, besides the fact that they lack the
entity name, they can be embedded with negations.
For example, the semantics of the sentence ‘The pa-
tients’ respiration become unlabored’ implies that
the patient does not have ‘shortness of breath’. Iden-
tification of the negated mentions of entities in clin-
ical documents is crucial as they provide valuable
insights into the patients’ health status.
We propose an unsupervised solution to the IER
problem that leverages knowledge embedded in en-
tity definitions obtained for each entity from the
Unified Medical Language System (UMLS) (Bo-
denreider, 2004). UMLS provides a standard vocab-
ulary for the clinical domain. Our solution: a) Cre-
ates an entity model from these definitions, b) Iden-
tifies the sentences in input text that may contain
implicit entity mentions, c) Projects these sentences
onto our entity model, and d) Classifies the sen-
tences to distinguish between those containing im-
plicit entity mentions or negated implicit mentions,
by calculating the semantic similarity between the
entity model and the projected sentences.
The contributions of this work are as follows:
</bodyText>
<listItem confidence="0.994961">
1. We introduce the problem of implicit entity
recognition (IER) in clinical documents.
2. We propose an unsupervised solution to IER
that outperforms the most relevant unsuper-
vised baseline and improves the results of a su-
pervised baseline.
3. We create a gold standard corpus annotated for
IER in the clinical domain and make it avail-
able to encourage research in this area.
</listItem>
<sectionHeader confidence="0.999783" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999900233333333">
To the best of our knowledge, this is the first work
to address the problem of Implicit Entity Recogni-
tion (IER) in clinical documents. However, there is a
large body of research that is relevant to the problem,
including Named Entity Recognition (NER), Entity
Linking (EL), Coreference Resolution, Paraphrase
Recognition, and Textual Entailment Recognition.
Much like IER, both NER and EL have the ob-
jective of binding a natural language expression
to a semantic identifier. However, related work
in NER and EL expect the proper name (explicit
mention) of entities and assume the presence of
noun phrases (Collins and Singer, 1999) (Bunescu
and Pasca, 2006). The solutions developed for
NER leverage regularities on morphological and
syntactical features that are unlikely to hold in the
case of IER. The most successful NER approaches
use word-level features (such as capitalization, pre-
fixes/suffixes, and punctuation), list lookup fea-
tures (such as gazetteers, lexicons, or dictionaries),
as well as corpus-level features (such as multiple
occurrences, syntax, and frequency) (Nadeau and
Sekine, 2007) that are not exhibited by the phrases
with implicit entity mentions.
Many approaches couple NER with a follow up
EL step (Hachey et al., 2013) in order to assign an
unique entity identifiers to mentions. Therefore, the
inadequacy of NER techniques will limit the capa-
bility of recognizing implicit entity mentions by a
solution developed for EL. Moreover, state-of-the-
art EL approaches include a ‘candidate mapping’
step that uses entity names to narrow down the space
of possible entity identifiers, which is also a limiting
factor in the IER case. Finally, neither NER nor EL
deal with the negated mentions of entities.
Coreference resolution (CR) focuses on grouping
multiple mentions of the same entity with different
surface forms. The solutions to CR focus on map-
ping explicit mentions of entity names to other pro-
nouns and noun phrases referring to the same en-
tity (Ng, 2010) (Durrett and Klein, 2013). In IER
implicit mentions occur without co-referring corre-
sponding entity. Hence, they must be resolved with-
out dependencies on co-referents.
In contrast to NER, EL, and CR problems and
their solutions, IER addresses instances where nei-
ther explicit mention of an entity nor noun phrases
or any of the above mentioned features are guaran-
teed to appear in the text but still have a reference to
a known entity. Hence, IER solutions require treat-
ment for implied meaning of the phrases beyond its
syntactic features.
Since our solution to IER establishes a relation-
ship between entity definitions and the input text,
the tasks of paraphrase recognition (Barzilay and El-
hadad, 2003) (Dolan et al., 2004) and textual entail-
ment recognition (Giampiccolo et al., 2007) are re-
lated to our solution. However, these tasks are fun-
damentally different in two aspects: 1) Both para-
phrase recognition and textual entailment recogni-
</bodyText>
<page confidence="0.99735">
229
</page>
<bodyText confidence="0.999975090909091">
tion are defined at the sentence level, whereas text
phrases considered for IER can exist as a sentence
fragment or span across multiple sentences, and
2) The objective of IER is to find whether a given
text phrase has a mention of an entity—as opposed
to determining whether two sentences are similar or
entail one another. However, our solution benefits
from the lessons learned from both tasks.
The question answering solutions cope with the
questions that describe the characteristics of a con-
cept and expect that concept as the answer. This
particular type of questions resembles implicit en-
tity mentions. However, they assume that the ques-
tions are referring to some concept and the problem
is to uncover which one, whereas the implicit entity
mention problem requires us to first check whether
a particular sentence/phrase has a mention of an en-
tity at all. Furthermore, question answering systems
benefit from the presence of pronouns, nouns, and
noun phrases in the questions and the candidate an-
swers to derive helpful syntactic and semantic fea-
tures (Lally et al., 2012)(Wang, 2006), while phrases
with implicit entity mentions may not contain such
features.
The existing work on clinical document annota-
tion focused on explicit entity mentions with con-
tiguous phrases (Aronson, 2006) (Savova et al.,
2010) (Friedman et al., 2004) (Fu and Ananiadou,
2014). Going one step beyond, the SemEval 2014
task 7 recognized the need for identifying discon-
tiguous mentions of explicit entities (Pradhan et al.,
2014). However, the recognition of implicit entities
has yet to address by this community.
</bodyText>
<sectionHeader confidence="0.934981" genericHeader="method">
3 Implicit Entity Recognition (IER) in
</sectionHeader>
<subsectionHeader confidence="0.699431">
Clinical Documents
</subsectionHeader>
<bodyText confidence="0.999485636363637">
We define the Implicit Entity Recognition (IER) task
in clinical documents as: given input text that does
not have explicit mentions of target entities, find
which target entities are implied (including implied
negations) in the input text.
Negation detection is traditionally separated from
the entity recognition task because negation indi-
cating terms can be recognized separately from the
phrases that contain explicit mention of an entity. In
contrast, implicit mention can involve an antonym
that fuses the entity indication with negated sense
</bodyText>
<figureCaption confidence="0.999752">
Figure 1: Components of the Proposed Solution
</figureCaption>
<bodyText confidence="0.999966542857143">
(e.g., ‘patient denies shortness of breath’ vs ‘patient
is breathing comfortably’). Hence, negation detec-
tion is considered as a sub-task of IER.
Typical entity recognition task considers the de-
tection of the boundaries of the phrases with enti-
ties (i.e., segmentation) as a sub-task. We consider
boundary detection of the implicit entity mentions
as an optional step due to two reasons: 1) It is con-
sidered an optional step in biomedical entity recog-
nition task (Tsai et al., 2006), and 2) The phrases
with implicit entity mentions can be noncontiguous
and span multiple sentences. Further, in some cases,
even domain experts disagree on the precise phrase
boundaries.
We define the IER as a classification task. Given
an input text, classify it to one of the three cate-
gories: TPe if the text has a mention of entity e, or
Tnege if the text has a negated mention of entity e, or
TNe if the entity e is not mentioned at all. As men-
tioned, the phrases with implicit entity mentions can
span to multiple sentences. However, this work will
focus only on implicit mentions exist within a sen-
tence. Our unsupervised solution to this classifica-
tion task: 1) Creates an entity model from the entity
definitions, 2) Selects candidate sentences that may
contain implicit entity mentions, 3) Projects the can-
didate sentences into entity model space, and 4) Cal-
culates the semantic similarity between projected
sentences and the entity model. Figure 1 shows the
components of our solution which are discussed be-
low in detail.
In order to facilitate these sub-tasks, our algo-
rithm introduces the concept of an entity represen-
tative term for each entity and propose an automatic
way to select these terms from entity definitions.
</bodyText>
<figure confidence="0.971596928571429">
Knowledge Base
on Entities
Entity
Representative
Term Selection
Candidate
Sentence Selection
Entity Model
Creation
Similarity
Calculation
Candidate
Sentence Pruning
Annotations
</figure>
<page confidence="0.920602">
230
</page>
<subsectionHeader confidence="0.991636">
3.1 Entity Representative Term Selection
</subsectionHeader>
<bodyText confidence="0.999916538461539">
Entity representative term (ERT) selection finds a
term with high representative power to an entity and
plays an important role in defining it.
The representative power of a term t for entity e
is defined based on two properties: its dominance
among the definitions of entity e, and its ability to
discriminate the mentions of entity e from other en-
tities. This is formalized in eq. (1). Consider the
entity ‘appendicitis’ as an example. It is defined as
‘acute inflammation of appendix’. Intuitively, both
terms inflammation and appendix are candidates to
explain the entity appendicitis. However, the term
appendix has more potential to discriminate the im-
plicit mentions of appendicitis than the term inflam-
mation, because the term inflammation is used to de-
scribe many entities. Also, none of the definitions
define appendicitis without using the term appendix;
therefore, appendix is the dominant term, and conse-
quently it has the most representative power for the
entity ‘appendicitis’.
We used a score inspired by the TF-IDF measure
to capture this intuition. The IDF (inverse document
frequency) value measures the specificity of a term
in the definitions. The TF (term frequency) captures
the dominance of a term. Hence the representative
power of a term t for entity e (rt) is defined as,
</bodyText>
<equation confidence="0.9692">
|E |(1)
|Et|
</equation>
<bodyText confidence="0.989606588235294">
Qe is the set of definitions of entity e, E is the
set of all entities. freq(t, Qe) is the frequency of
term t in set Qe, |E |is the size of the set E (3962 in
our corpus), and the denominator |Et |calculates the
number of entities defined using term t. We expand
the ERT found for an entity with this technique by
adding its synonyms obtained from WordNet.
We can define entity representative terms based
on the definition of representative power.
Definition 3.1 (Entity Representative Term). Let
Le = {t1, t2, ..., tn} be the set of terms in a defi-
nitions of an entity e. Let RL, = {rtl, rte, ..., rtn}
be the representative power calculated for each term
ti in Le for e. We select term tm as the entity rep-
resentative term of the entity e if its representative
power is maximum, i.e., rt. ≥ rt, for all i where
1 ≤ i ≤ n.
</bodyText>
<subsectionHeader confidence="0.981118">
3.2 Entity Model Creation
</subsectionHeader>
<bodyText confidence="0.999982535714286">
Our algorithm creates an entity indicator from a
definition of the entity. An entity indicator con-
sists of terms that describe the entity. Consider
the definition ‘A disorder characterized by an un-
comfortable sensation of dif�culty breathing’ for
‘shortness of breath’, for which the selected ERT is
‘breathing’. The terms uncomfortable, sensation,
dif�culty, and breathing collectively describe the en-
tity. Adding other terms in this definition to the en-
tity indicator negatively affects the similarity calcu-
lation with the candidate sentences since they are
less likely to appear in a candidate sentence. We
exploited the neighborhood of the ERT in the defini-
tion to create the entity indicator and automatically
selected the nouns, verbs, adjectives, and adverbs in
the definition within a given window size to the left
and to the right of the ERT. We used a window size
of four in our experiments.
An entity can have multiple definitions each ex-
plaining it using diverse vocabulary. On average,
an entity in our corpus had 3 definitions. We cre-
ate an entity indicator from each definition of the
entity, hence an entity has multiple indicators. We
call the collection of indicators of an entity as its en-
tity model. In other words, an entity model consists
of multiple entity indicators that capture diverse and
orthogonal ways an entity can be expressed in the
text.
</bodyText>
<subsectionHeader confidence="0.999601">
3.3 Candidate Sentence Selection
</subsectionHeader>
<bodyText confidence="0.999989142857143">
The sentences with ERT in an input text are identi-
fied as candidate sentences containing implicit men-
tion of the corresponding entity. A sentence may
contain multiple ERTs and consequently become a
candidate sentence for multiple entities. This step
reduces the complexity of the classification task as
now a sentence has only a few target entities.
</bodyText>
<subsectionHeader confidence="0.980031">
3.4 Candidate Sentence Pruning
</subsectionHeader>
<bodyText confidence="0.999960428571429">
In order to evaluate the similarity between any given
candidate sentence and the entity model, we per-
form a projection of candidate sentences onto the
same semantic space. We perform this by pruning
the terms in candidate sentences that does not par-
ticipate in forming the segment with implicit entity
mentions. Candidate sentences are pruned by fol-
</bodyText>
<equation confidence="0.810585">
rt = freq(t, Qe) ∗ log
</equation>
<page confidence="0.908841">
231
</page>
<bodyText confidence="0.969562481481481">
lowing the same steps followed to create the entity models and pruned sentences before calculating the
indicators from the entity definitions. similarity. We found the adjective for an adverb us-
3.5 Semantic Similarity Calculation ing relationship ‘pertainym’ and noun for an adjec-
As the last step, our solution calculates the similarity tive or a verb using the relationship ‘derivationally
between the entity model and the pruned candidate related form’ in WordNet.
sentence. The sentences with implicit entity men- Handling negations: Negations are of two types:
tions often use adjectives and adverbs to describe the 1) Negations mentioned with explicit terms such as
entity and they may indicate the absence of the en- no, not, and deny, and 2) Negations indicated with
tities using antonyms or explicit negations. These antonyms (e.g., 2nd example in above list). We used
two characteristics pose challenges to the applica- the NegEx algorithm (Chapman et al., 2001) to ad-
bility of existing text similarity algorithms such as dress the first type of negations. To address the sec-
MCS (Mihalcea et al., 2006) and matrixJcn (Fer- ond type of negations, we exploited the antonym re-
nando and Stevenson, 2008) which are proven to lationships in the WordNet.
perform well among the unsupervised algorithms in The similarity between an entity model and the
paraphrase identification task (ACLWiki, 2014). pruned candidate sentence is calculated by comput-
The existing text similarity algorithms largely ing the similarities of their terms. The term sim-
benefit from the WordNet similarity measures. Most ilarity is computed by forming an ensemble using
of these measures use the semantics of the hierar- the standard WordNet similarity measures namely,
chical arrangement of the terms in WordNet. Unfor- WUP (Wu and Palmer, 1994), LCH (Leacock and
tunately, adjectives and adverbs are not arranged in Chodorow, 1998), Resnik (Resnik, 1995), LIN (Lin,
a hierarchy, and terms with different part of speech 1998), JCN (Jiang and Conrath, 1997), as well as a
(POS) tags cannot be mapped to the same hierarchy. predict vector-based measure Word2vec (Mikolov et
Hence, they are limited in calculating the similar- al., 2013) and a morphology-based similarity metric
ity between terms of these categories. This limi- Levenshtein1 as:
tation negatively affects the performance of IER as sim(t1, t2) = maxm∈M(simm(t1, t2)) (2)
the entity models and pruned sentences often contain
terms from these categories. Consider the following
</bodyText>
<listItem confidence="0.892023">
examples:
1. Her breathing is still uncomfortableadjective.
2. She is breathing comfortablyadverb in room air.
3. His tip of the appendix was inflamedverb.
</listItem>
<bodyText confidence="0.991574788461539">
The first two examples use an adjective and an ad-
verb to mention the entity ‘shortness of breath’ im-
plicitly. The third example uses a verb to men-
tion the entity ‘appendicitis’ implicitly instead of the
where t1 and t2 are input terms and M is the
set of above mentioned similarity measures. This
ensemble-based similarity measure exploits orthog-
onal ways of comparing terms: semantic, statisti-
cal, and syntactic. An ensemble-based approach
is preferable over picking one of them exclusively
since they are complementary in nature, that is, each
outperforms the other two in certain scenarios.
The similarity values calculated by WordNet sim-
ilarity measures in simm(t1, t2) are normalized to
range between 0 and 1.
The similarity of a pruned candidate sentence to
the entity model is calculated by calculating its sim-
ilarity to each entity indicator in the entity model,
and picking the maximum value as the final simi-
larity value for the candidate sentence. The similar-
ity between entity indicator e and pruned sentence
s, sim(e, s), is calculated by summing the similari-
ties calculated for each term te in the entity indica-
tor weighted by its representative power as defined
noun inflammation that is used by its definition.
We have developed a text similarity measure over-
coming these challenges and weigh the contributions
of the words in the entity model to the similarity
value based on their representative power.
Handling adjectives, adverbs and words with
different POS tags: To get the best out of all Word-
Net similarity measures, we exploited the relation-
ships between different forms of the terms in Word-
Net to find the noun form of the terms in the entity
1http://en.wikipedia.org/wiki/Levenshtein distance
232
in eq. (1). If te is an antonym for any term in s
(ts), it contributes negatively to the overall similar-
ity value, else it contributes in linear portion of the
maximum similarity value between te and some ts
(eqs. (4) and (5)). The overall similarity value is
normalized based on the total representative power
of all the terms tes (eq. (1)) and ranges between -1
and +1.
Note that this formulation weighs the contribution
of each term according to its importance in defining
the entity. The higher similarity with a term that has
higher representative power leads to higher overall
similarity value, while the lower similarity with such
terms leads to a lower total similarity value. The spe-
cial treatment for antonyms takes care of the negated
mentions of an entity.
</bodyText>
<equation confidence="0.9440236">
sim(e, s) = EteEet(Ee r
,t� ∗ rte (3)
�
−1 α(te, s) == 0
f(te, s) =
maxtsEs sim(te, ts) otherwise
α(te, s) = 11
77�77 10 if te is an antonym of ts
tsEs Sl
1 otherwise
</equation>
<bodyText confidence="0.965905666666667">
Finally, the sentences are classified based on a
configurable threshold values selected between -1
and +1.
</bodyText>
<sectionHeader confidence="0.999407" genericHeader="method">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.9999634">
We reannotated a sample of the corpus created for
SemEval-2014 task 7 (Pradhan et al., 2014) to in-
clude implicit mention annotations and measured
the performance of our proposed method in classify-
ing entities annotated with TP and Tneg mentions2.
</bodyText>
<subsectionHeader confidence="0.993852">
4.1 Gold Standard Dataset
</subsectionHeader>
<bodyText confidence="0.999602">
The SemEval-2014 task 7 corpus consists of 24,147
de-identified clinical notes. We used this corpus to
create a gold standard for IER with the help of three
domain experts. The gold standard consists of 857
</bodyText>
<footnote confidence="0.919242">
2We do not explicitly report performance on TN because our
focus is to find sentences that contain entity mentions rather
than those devoid of mentions.
</footnote>
<table confidence="0.999571">
Entity TP Tneg TN
Shortness of breath 93 94 29
Edema 115 35 81
Syncope 96 92 24
Cholecystitis 78 36 4
Gastrointestinal gas 18 14 5
Colitis 12 11 0
Cellulitis 8 2 0
Fasciitis 7 3 0
</table>
<tableCaption confidence="0.999735">
Table 1: Candidate Sentence Statistics
</tableCaption>
<bodyText confidence="0.999012162162162">
sentences selected for eight entities. The creation of
the gold standard is described below in detail.
We have annotated the corpus for explicit men-
tions of the entities using cTAKES (Savova et
al., 2010) and ranked the entities based on their
frequency. The domain experts on our team
then selected a subset of these entities that they
judged to be frequently mentioned implicitly in
clinical documents. For example, the frequent
entity ‘shortness of breath’ was selected but not
‘chest pain’ since the former is mentioned implic-
itly often but not the latter. We used four frequently
implicitly mentioned entities as the primary focus of
our evaluation. We refer to these as primary enti-
ties from here on (the first four entities in Table 1).
To test the generalizability of our method, as well
as to evaluate its robustness when lacking training
data, we selected another four entities (the last four
entities in Table 1). We then selected a random sam-
ple of candidate sentences for each of these entities
based on their ERTs and winnowed it down further
by manually selecting a subset that exhibits syntactic
diversity. Ultimately, our corpus consisted of 120-
200 sentences for each primary entity and additional
80 sentences selected from the other four entities.
Each candidate sentence was annotated as TPe
(contains a mention of entity e), Tnege (contains a
negated mention of entity e), or TNe (does not con-
tain a mention of entity e). Each sentence was anno-
tated by two domain experts, and we used the third
one to break the ties. The Cohens’ kappa value for
the annotation agreement was 0.58. While the anno-
tators have good agreement on annotating sentences
in category TP, they agreed less on the categories
Tneg and TN. The latter categories are indeed dif-
ficult to distinguish. For example, annotators often
argue whether ‘patient breathing at a rate of 15-20’
</bodyText>
<page confidence="0.996377">
233
</page>
<bodyText confidence="0.99992775">
means the negation of entity ‘shortness of breath’
(because that is a normal breathing pattern) or just
lacks a mention of the entity. The final anno-
tation label for a sentence is decided based on
majority voting. Table 1 shows the statistics of
the annotated candidate sentences. The prepared
data set is available at http://knoesis.org/
researchers/sujan/data-sets.html
</bodyText>
<subsectionHeader confidence="0.993176">
4.2 Implicit Entity Recognition Performance
</subsectionHeader>
<bodyText confidence="0.99577980952381">
Since IER is a novel task, there are no baseline algo-
rithms that can be directly applied such that it would
yield a fair comparison with our algorithm. How-
ever, we deem some of the related algorithms to have
good potential applicability for this task. Therefore,
we included two strong algorithms from the closest
related work as baseline solutions to the problem.
The first baseline is the well-known text similar-
ity algorithm MCS (Mihalcea et al., 2006). MCS is
one of the best performing unsupervised algorithms
in paraphrase recognition task (ACLWiki, 2014). It
uses an ensemble of statistical and semantic similar-
ity measures, which is a preferable feature for the
IER as opposed to one measure used by the ma-
trixJcn (Fernando and Stevenson, 2008). Both MCS
and our algorithm classify the candidate sentences
based on threshold values selected experimentally.
To include also a supervised baseline, we trained
an SVM (Cortes and Vapnik, 1995) one of the
state-of-the-art learning algorithms, shown to per-
form remarkably well in a number of classifica-
tion tasks. We trained separate SVMs for each pri-
mary entity, considering unigrams, bigrams, and tri-
grams as the features. It has been shown that SVM
trained on ngrams performed well on text classifica-
tion tasks (Pang et al., 2002) (Zhang and Lee, 2003).
The SVMs trained with bigrams consistently pro-
duced the best results for the 4-fold cross validation.
Therefore, our testing phase used the SVMs trained
with the bigrams.
Preparation of training and testing datasets:
We created training and testing datasets by splitting
the dataset annotated for each primary entity as 70%
(training) and 30% (testing). The training datasets
were used to train the SVM models for each pri-
mary entity and to select the threshold values for
both MCS and our algorithm.
The classification performance of each algorithm
is studied in the TP and Tneg categories using preci-
sion, recall, and F-measure.
The precision (PP) and recall (PR) for category
TP at threshold t are defined as:
</bodyText>
<equation confidence="0.855158875">
PP__ STP with sim≥t
all sentences with sim≥t
STP with sim≥t
PRt = STP
Similarly, NP and NR for Tneg are defined as:
STneg with sim&lt;t
NPt = all sentences with sim&lt;t
NRt = ST neg with sim&lt;t STneg
</equation>
<bodyText confidence="0.999961473684211">
where STP and STneg denote the sentences anno-
tated with TP and Tneg respectively by domain ex-
perts and sim is the calculated similarity value for
the pruned sentence.
Selecting threshold value: The threshold values
for both MCS and our algorithm are selected based
on their classification performance in the training
dataset. The MCS algorithm produced the best F1
score for the TP category with a threshold value of
0.5, and for the Tneg category with a threshold value
of 0.9, while our algorithm produced the best F1 for
the TP category with 0.4 and for the Tneg category
with 0.3. We examined threshold values that pro-
duce best F1 scores by the two algorithms by starting
with 10% of the training data and gradually increas-
ing the size of the training data. The threshold val-
ues with best F1 scores were stabilized after adding
30% of the training data. Hence, we could select the
threshold values with just 50% of the training data.
</bodyText>
<subsectionHeader confidence="0.999586">
4.3 Classification Performance
</subsectionHeader>
<bodyText confidence="0.9991675">
The first experiment evaluates the classification per-
formance of our algorithm, MCS, and SVM.
</bodyText>
<table confidence="0.9993655">
Method PP PR PF1 NP NR NF1
Our 0.66 0.87 0.75 0.73 0.73 0.73
MCS 0.50 0.93 0.65 0.31 0.76 0.44
SVM 0.73 0.82 0.77 0.66 0.67 0.67
</table>
<tableCaption confidence="0.9675135">
Table 2: precision, recall, and F1 values for each algorithm (PF1 and
NF1 indicate F1 scores for the TP and Tneg categories respectively).
SVM outperforms our algorithm in the TP category, while our algorithm
outperforms SVM on the Tneg category.
</tableCaption>
<bodyText confidence="0.9997424">
Our algorithm outperforms the other unsuper-
vised solution MCS, but the SVM was able to lever-
age supervision to outperform our algorithm in the
TP category in terms of F-measure (PF1 on Ta-
ble 2). For example, the sentence ‘he was placed on
</bodyText>
<page confidence="0.99445">
234
</page>
<bodyText confidence="0.99894335">
mechanical ventilation shortly after presentation’ is
annotated as TP in the gold standard for the entity
‘shortness of breath’ since ‘mechanical ventilation’
indicates the presence of ‘shortness of breath’. This
annotation requires domain knowledge that was not
present in the entity definitions that we used to build
entity models. However, with enough examples, the
SVM was able to learn the importance of the bigram
‘mechanical ventilation’ and classify it as TP.
For the Tneg category, however, our algorithm
outperforms the SVM (NF1 on Table 2). This is
due to the explicit treatment for the negated men-
tions by our algorithm to capture different variations
of the negated mentions.
The MCS algorithm underperformed in both cat-
egories. We observed that this was mostly due to
its limitations described in Section 3.5. The over-
all classification accuracy—the accuracy of classify-
ing both TP and Tneg instances—of our algorithm,
MCS, and SVM are 0.7, 0.4, and 0.7 respectively.
</bodyText>
<table confidence="0.99905075">
Method PP PR PF1 NP NR NF1
SVM 0.73 0.82 0.77 0.66 0.67 0.67
SVM+MCS 0.73 0.82 0.77 0.66 0.66 0.66
SVM+Our 0.77 0.85 0.81 0.72 0.75 0.73
</table>
<tableCaption confidence="0.8460885">
Table 3: Comparison of SVM results incorporating similarity values
calculated by our algorithm and MCS as a feature. Our algorithm com-
plements the SVM in both categories whereas MCS does not contribute
to improve the classification.
</tableCaption>
<bodyText confidence="0.999897290322581">
The second experiment evaluates the impact of in-
cluding the similarity scores calculated by MCS and
our algorithm for each candidate sentence as a fea-
ture to the best performing SVM model. Table 3
shows that the inclusion of MCS scores as a fea-
ture did not help to improve the SVM results. In
fact, it negatively affected the results for the Tneg
category. Since the MCS showed low precision for
the Tneg category in the previous experiment (Ta-
ble 2), it is potentially introducing too much noise
that the SVM is not able to linearly separate. How-
ever, the similarity value calculated by our algorithm
improves the SVM classifiers. It increased the pre-
cision and recall values for both the TP and Tneg
categories. This shows that the similarity value cal-
culated by our algorithm can be used as an effective
feature for a learning algorithm that is designed to
solve the IER problem. The overall classification
accuracy of SVM, SVM+MCS, and SVM+Our con-
figurations are 0.7, 0.7, and 0.74 respectively.
We were interested in exploring how much la-
beled data would be needed for supervised solution
to outperform our unsupervised score alone. We an-
alyzed the behavior of the three configurations of the
SVM with our unsupervised approach with different
training set sizes. Figure 2 shows the F1 values ob-
tained by gradually increasing the size of the train-
ing dataset3, while testing on the same test set. The
F1 value of our approach remains constant after 50%
training data since it has already decided the thresh-
old values. Figure 2 shows that the SVM trained
with bigrams needs 76% of the training dataset to
achieve the F1 value achieved by our unsupervised
approach in the TP category, and it does not achieve
the F1 achieved by our algorithm in Tneg category
(note the crossing points of the line marked with ‘X’
and line marked with circles).
Figure 2 also shows that the similarity score cal-
culated by our algorithm complements the SVM at
each data point. After adding our similarity score to
the SVM as a feature, it achieved the F1 achieved by
our unsupervised algorithm with 50% of the training
data in the TP category and with 90% of the training
data in the Tneg category (note the crossing points
of the line marked with ‘X’ and line marked with
‘+’). Also, SVM+Our configuration achieved the
best F1 value for SVM with just 70% of the train-
ing data in the TP category and with just 50% of the
training data in the Tneg category. This shows that
our similarity score can be used as an effective fea-
ture to reduce manual labeling effort and to improve
the supervised learning algorithms to solve the IER
problem.
Finally, to evaluate the generalization ability of
our algorithm and to demonstrate its usage in situ-
ations with a lack of training data, we applied it to
a set of 80 sentences selected for four new entities
(the last four entities in Table 1). Our algorithm pro-
duced the following results for these entities when
we classify their sentences with the threshold values
selected using the training dataset created for the pri-
mary entities.
</bodyText>
<construct confidence="0.340898">
PP = 0.72, PR = 0.77, PF1 = 0.74
NP = 0.78, NR = 0.83, NF1 = 0.80
</construct>
<footnote confidence="0.9799655">
3We draw these graphs considering training dataset size
&gt;50% for clarity.
</footnote>
<page confidence="0.995145">
235
</page>
<figure confidence="0.988505222222222">
F1
0.55 0.65 0.75 0.85
●
●
●
●
●
●
F1
0.55 0.65 0.75 0.85
●
●
● ●
● ●
Our ● SVM SVM+MCS SVM+Our
50 60 70 80 90 100 50 60 70 80 90 100
Training Size(%) Training Size(%)
TP Category: [t=3.8, df=10, p&lt;0.01] Tneg Category: [t=5.05, df=10, p&lt;0.01]
</figure>
<figureCaption confidence="0.89249">
Figure 2: The variation of the F1 value in the TP (left) and Tneg (right) categories with varying sizes of training datasets. These graphs show that
the SVM trained with bigrams needs 76% of the training data to achieve the F1 score of our unsupervised method in the TP category while it does
not achieve the F1 score of our algorithm in the Tneg category. This also shows that the similarity value calculated by our algorithm complements
the SVM trained with bigrams at each data point and helps it to beat or perform on par with our algorithm. The paired T-test values calculated for
SVM and SVM+Our configurations show that this is not a random behavior (t- T-test value, df- degree of freedom, p- probability value).
</figureCaption>
<bodyText confidence="0.999984111111111">
Although negation detection with NegEx is not a
contribution of our work, our algorithm enables its
application to IER. This is not possible for MCS.
NegEx requires two inputs: 1) The sentence, and
2) The term being considered for possible negation.
MCS does not detect the key term in the sentence,
hence it is not possible to apply NegEx with MCS.
However, our algorithm starts with identifying the
ERT which is considered for possible negation.
</bodyText>
<sectionHeader confidence="0.998749" genericHeader="evaluation">
5 Limitations
</sectionHeader>
<bodyText confidence="0.999282375">
The candidate sentence selection based on the ERT
can be seen as a limitation of our approach since it
does not select sentences with implicit entity men-
tions that do not use the selected ERT. However, we
do not expect this limitation to have a major impact.
We asked our domain experts to come up with sen-
tences that contain implicit mentions of the entity
‘shortness of breath’ without using its ERT ‘breath-
ing’ or its synonyms (‘respiration’ and ‘ventila-
tion’). According to them, the sentences ‘the patient
had low oxygen saturation’, ‘the patient was gasp-
ing for air’, and ‘patient was air hunger’ are such
sentences (the emphasis indicates the phrases that
imply ‘shortness of breath’). However, we found
only 113 occurrences of these phrases as opposed
to 8990 occurrences of its ERTs in our corpus.
</bodyText>
<sectionHeader confidence="0.998095" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999994352941176">
We defined the problem of implicit entity recogni-
tion in clinical documents and proposed an unsuper-
vised solution that recognizes the implicit mentions
of entities using a model built from their definitions
in a knowledge base. We showed that our algorithm
outperforms the most relevant unsupervised method
and it can be used as an effective feature for a su-
pervised learning solution based on an SVM. The
ability to capture the diverse ways in which an en-
tity can be implicitly mentioned by exploiting their
definitions with special treatment for two types of
negations are the main strengths of our method.
In the future, we will explore the ability to detect
the boundary of the phrases with implicit mentions,
capture the sentences with implicit mentions with-
out selected ERT, and investigate more intensive ex-
ploitation of domain knowledge for IER.
</bodyText>
<sectionHeader confidence="0.99826" genericHeader="acknowledgments">
7 Acknowledgement
</sectionHeader>
<bodyText confidence="0.99570925">
We acknowledge the medical students Logan
Markins, Kara Joseph, and Robert Beaulieu of
Wright State University for their assistance in cre-
ating the gold-standard corpus.
</bodyText>
<page confidence="0.99702">
236
</page>
<sectionHeader confidence="0.996295" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.938997605769231">
ACLWiki. 2014. Paraphrase identification (state of the
art). http://aclweb.org/aclwiki/index.
php?title=Paraphrase_Identification_
(State_of_the_art). [Online; accessed
19-Dec-2014].
Alan R Aronson. 2006. Metamap: Mapping text to
the umls metathesaurus. Bethesda, MD: NLM, NIH,
DHHS.
Regina Barzilay and Noemie Elhadad. 2003. Sen-
tence alignment for monolingual comparable corpora.
In Proceedings of the 2003 conference on Empirical
methods in natural language processing, pages 25–32.
Association for Computational Linguistics.
Olivier Bodenreider. 2004. The unified medical lan-
guage system (umls): integrating biomedical terminol-
ogy. Nucleic acids research, 32(suppl 1):D267–D270.
Razvan C Bunescu and Marius Pasca. 2006. Using ency-
clopedic knowledge for named entity disambiguation.
In EACL, volume 6, pages 9–16.
Wendy W Chapman, Will Bridewell, Paul Hanbury, Gre-
gory F Cooper, and Bruce G Buchanan. 2001. A sim-
ple algorithm for identifying negated findings and dis-
eases in discharge summaries. Journal of biomedical
informatics, 34(5):301–310.
Michael Collins and Yoram Singer. 1999. Unsupervised
models for named entity classification. In Proceedings
of the joint SIGDAT conference on empirical methods
in natural language processing and very large cor-
pora, pages 100–110. Citeseer.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Machine learning, 20(3):273–297.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: Exploiting massively parallel news sources. In
Proceedings of the 20th international conference on
Computational Linguistics, page 350. Association for
Computational Linguistics.
Greg Durrett and Dan Klein. 2013. Easy victories and
uphill battles in coreference resolution. In EMNLP,
pages 1971–1982.
Samuel Fernando and Mark Stevenson. 2008. A seman-
tic similarity approach to paraphrase detection. In Pro-
ceedings of the 11th Annual Research Colloquium of
the UK Special Interest Group for Computational Lin-
guistics, pages 45–52. Citeseer.
Carol Friedman, Philip O Alderson, John HM Austin,
James J Cimino, and Stephen B Johnson. 1994. A
general natural-language text processor for clinical ra-
diology. Journal of the American Medical Informatics
Association, 1(2):161–174.
Carol Friedman, Lyudmila Shagina, Yves Lussier, and
George Hripcsak. 2004. Automated encoding of clin-
ical documents based on natural language processing.
Journal of the American Medical Informatics Associa-
tion, 11(5):392–402.
Xiao Fu and Sophia Ananiadou. 2014. Improving the
extraction of clinical concepts from clinical records.
Proceedings of BioTxtM14.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and
Bill Dolan. 2007. The third pascal recognizing tex-
tual entailment challenge. In Proceedings of the ACL-
PASCAL workshop on textual entailment and para-
phrasing, pages 1–9. Association for Computational
Linguistics.
Ben Hachey, Will Radford, Joel Nothman, Matthew Hon-
nibal, and James R. Curran. 2013. Evaluating entity
linking with wikipedia. Artif. Intell., 194:130–150,
January.
Jay J Jiang and David W Conrath. 1997. Semantic simi-
larity based on corpus statistics and lexical taxonomy.
arXiv preprint cmp-lg/9709008.
Adam Lally, John M Prager, Michael C McCord,
BK Boguraev, Siddharth Patwardhan, James Fan, Paul
Fodor, and Jennifer Chu-Carroll. 2012. Question
analysis: How watson reads a clue. IBM Journal of
Research and Development, 56(3.4):2–1.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and wordnet similarity for word
sense identification. WordNet: An electronic lexical
database, 49(2):265–283.
Dekang Lin. 1998. An information-theoretic definition
of similarity. In ICML, volume 98, pages 296–304.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and knowledge-based measures
of text semantic similarity. In AAAI, volume 6, pages
775–780.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word representa-
tions in vector space. arXiv preprint arXiv:1301.3781.
David Nadeau and Satoshi Sekine. 2007. A survey of
named entity recognition and classification. Lingvisti-
cae Investigationes, 30(1):3–26.
Vincent Ng. 2010. Supervised noun phrase coreference
research: The first fifteen years. In Proceedings of
the 48th Annual Meeting of the Association for Com-
putational Linguistics, ACL ’10, pages 1396–1411,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using ma-
chine learning techniques. In Proceedings of the ACL-
02 conference on Empirical methods in natural lan-
guage processing-Volume 10, pages 79–86. Associa-
tion for Computational Linguistics.
</reference>
<page confidence="0.96124">
237
</page>
<reference confidence="0.999770880952381">
Sameer Pradhan, No´emie Elhadad, Wendy Chapman,
Suresh Manandhar, and Guergana Savova. 2014.
Semeval-2014 task 7: Analysis of clinical text. Se-
mEval2014, 199(99):54.
Sameer Pradhan, No´emie Elhadad, Brett R South, David
Martinez, Lee Christensen, Amy Vogel, Hanna Suomi-
nen, Wendy W Chapman, and Guergana Savova.
2015. Evaluating the state of the art in disorder
recognition and normalization of the clinical narrative.
Journal of the American Medical Informatics Associa-
tion, 22(1):143–154.
Philip Resnik. 1995. Using information content to evalu-
ate semantic similarity in a taxonomy. In Proceedings
of the 14th International Joint Conference on Artifi-
cial Intelligence - Volume 1, IJCAI’95, pages 448–453,
San Francisco, CA, USA. Morgan Kaufmann Publish-
ers Inc.
Guergana K Savova, James J Masanz, Philip V Ogren,
Jiaping Zheng, Sunghwan Sohn, Karin C Kipper-
Schuler, and Christopher G Chute. 2010. Mayo
clinical text analysis and knowledge extraction system
(ctakes): architecture, component evaluation and ap-
plications. Journal of the American Medical Informat-
ics Association, 17(5):507–513.
Richard Tzong-Han Tsai, Shih-Hung Wu, Wen-Chi
Chou, Yu-Chun Lin, Ding He, Jieh Hsiang, Ting-Yi
Sung, and Wen-Lian Hsu. 2006. Various criteria in
the evaluation of biomedical named entity recognition.
BMC bioinformatics, 7(1):92.
Mengqiu Wang. 2006. A survey of answer extraction
techniques in factoid question answering. Computa-
tional Linguistics, 1(1).
Zhibiao Wu and Martha Palmer. 1994. Verbs semantics
and lexical selection. In Proceedings of the 32Nd An-
nual Meeting on Association for Computational Lin-
guistics, ACL ’94, pages 133–138, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Dell Zhang and Wee Sun Lee. 2003. Question classifica-
tion using support vector machines. In Proceedings of
the 26th annual international ACM SIGIR conference
on Research and development in informaion retrieval,
pages 26–32. ACM.
</reference>
<page confidence="0.997061">
238
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.402431">
<title confidence="0.999802">Implicit Entity Recognition in Clinical Documents</title>
<author confidence="0.989102">Pablo Amit Krishnaprasad</author>
<affiliation confidence="0.871007333333333">Center, Wright State University, Dayton, OH, Research, San Jose, CA, School of Medicine, Wright State University, Dayton, OH,</affiliation>
<email confidence="0.8103255">sujan@knoesis.org,pnmendes@us.ibm.com,amit@knoesis.org,adarsh@knoesis.org,caheid2@gmail.com,mott11@wright.edu</email>
<abstract confidence="0.996236774193548">With the increasing automation of health care information processing, it has become crucial to extract meaningful information from textual notes in electronic medical records. One of the key challenges is to extract and normalize entity mentions. State-of-the-art approaches have focused on the recognition of entities that are explicitly mentioned in a sentence. However, clinical documents often contain phrases that indicate the entities but do not contain names. We term those entity introduce the problem of implicit entity recognition (IER) in clinical documents. We propose a solution to IER that leverages entity definitions from a knowledge base to create entity models, projects sentences to the entity models and identifies implicit entity mentions by evaluating semantic similarity between sentences and entity models. The evaluation with 857 sentences selected for 8 different entities shows that our algorithm outperforms the most closely related unsupervised solution. The similarity value calculated by our algorithm proved to be an effective feature in a supervised learning setting, helping it to improve over the baselines, and achieving F1 scores of .81 and .73 for different classes of implicit mentions. Our gold standard annotations are made available to encourage further research in the area of IER.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>ACLWiki</author>
</authors>
<title>Paraphrase identification (state of the art).</title>
<date>2014</date>
<note>http://aclweb.org/aclwiki/index. php?title=Paraphrase_Identification_ (State_of_the_art). [Online; accessed 19-Dec-2014].</note>
<contexts>
<context position="18372" citStr="ACLWiki, 2014" startWordPosition="2926" endWordPosition="2927"> antonyms or explicit negations. These antonyms (e.g., 2nd example in above list). We used two characteristics pose challenges to the applica- the NegEx algorithm (Chapman et al., 2001) to adbility of existing text similarity algorithms such as dress the first type of negations. To address the secMCS (Mihalcea et al., 2006) and matrixJcn (Fer- ond type of negations, we exploited the antonym renando and Stevenson, 2008) which are proven to lationships in the WordNet. perform well among the unsupervised algorithms in The similarity between an entity model and the paraphrase identification task (ACLWiki, 2014). pruned candidate sentence is calculated by computThe existing text similarity algorithms largely ing the similarities of their terms. The term simbenefit from the WordNet similarity measures. Most ilarity is computed by forming an ensemble using of these measures use the semantics of the hierar- the standard WordNet similarity measures namely, chical arrangement of the terms in WordNet. Unfor- WUP (Wu and Palmer, 1994), LCH (Leacock and tunately, adjectives and adverbs are not arranged in Chodorow, 1998), Resnik (Resnik, 1995), LIN (Lin, a hierarchy, and terms with different part of speech 1</context>
<context position="26134" citStr="ACLWiki, 2014" startWordPosition="4206" endWordPosition="4207">ets.html 4.2 Implicit Entity Recognition Performance Since IER is a novel task, there are no baseline algorithms that can be directly applied such that it would yield a fair comparison with our algorithm. However, we deem some of the related algorithms to have good potential applicability for this task. Therefore, we included two strong algorithms from the closest related work as baseline solutions to the problem. The first baseline is the well-known text similarity algorithm MCS (Mihalcea et al., 2006). MCS is one of the best performing unsupervised algorithms in paraphrase recognition task (ACLWiki, 2014). It uses an ensemble of statistical and semantic similarity measures, which is a preferable feature for the IER as opposed to one measure used by the matrixJcn (Fernando and Stevenson, 2008). Both MCS and our algorithm classify the candidate sentences based on threshold values selected experimentally. To include also a supervised baseline, we trained an SVM (Cortes and Vapnik, 1995) one of the state-of-the-art learning algorithms, shown to perform remarkably well in a number of classification tasks. We trained separate SVMs for each primary entity, considering unigrams, bigrams, and trigrams </context>
</contexts>
<marker>ACLWiki, 2014</marker>
<rawString>ACLWiki. 2014. Paraphrase identification (state of the art). http://aclweb.org/aclwiki/index. php?title=Paraphrase_Identification_ (State_of_the_art). [Online; accessed 19-Dec-2014].</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan R Aronson</author>
</authors>
<title>Metamap: Mapping text to the umls metathesaurus.</title>
<date>2006</date>
<location>Bethesda, MD: NLM, NIH, DHHS.</location>
<contexts>
<context position="3222" citStr="Aronson, 2006" startWordPosition="470" endWordPosition="471">utions to assist expert “coders” in determining the unique identifier (e.g., ICD9 or ICD10) for each medical condition or combination of conditions. These identifiers are important to unambiguously represent the medical conditions, to prepare the post-discharge plan, and to perform secondary data analysis tasks. A human coder reading the sentence ‘Patient has shortness of breath with reaccumulation of fluid in extremities’ would generate the corresponding codes for entities ‘shortness of breath’ and ‘edema’. However, the solutions developed to perform entity recognition in clinical documents (Aronson, 2006) (Friedman et al., 1994) (Savova et al., 2010) (Friedman et al., 2004) (Fu and Ananiadou, 2014) (Pradhan et al., 2015) do not recognize the presence of entity ‘edema’ in this sentence. Implicit entity mentions are a common occurrence in clinical documents as they are often typed during a patient visit in a way that is natural in spoken language and meant for consumption by the professionals with similar backgrounds. An analysis with 300 documents in our corpus showed that 35% of the ‘edema’ mentions and 40% of the ‘shortness of breath’ mentions are implicit. Recognizing implicit mentions is pa</context>
<context position="9746" citStr="Aronson, 2006" startWordPosition="1515" endWordPosition="1516">and the problem is to uncover which one, whereas the implicit entity mention problem requires us to first check whether a particular sentence/phrase has a mention of an entity at all. Furthermore, question answering systems benefit from the presence of pronouns, nouns, and noun phrases in the questions and the candidate answers to derive helpful syntactic and semantic features (Lally et al., 2012)(Wang, 2006), while phrases with implicit entity mentions may not contain such features. The existing work on clinical document annotation focused on explicit entity mentions with contiguous phrases (Aronson, 2006) (Savova et al., 2010) (Friedman et al., 2004) (Fu and Ananiadou, 2014). Going one step beyond, the SemEval 2014 task 7 recognized the need for identifying discontiguous mentions of explicit entities (Pradhan et al., 2014). However, the recognition of implicit entities has yet to address by this community. 3 Implicit Entity Recognition (IER) in Clinical Documents We define the Implicit Entity Recognition (IER) task in clinical documents as: given input text that does not have explicit mentions of target entities, find which target entities are implied (including implied negations) in the input</context>
</contexts>
<marker>Aronson, 2006</marker>
<rawString>Alan R Aronson. 2006. Metamap: Mapping text to the umls metathesaurus. Bethesda, MD: NLM, NIH, DHHS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Noemie Elhadad</author>
</authors>
<title>Sentence alignment for monolingual comparable corpora.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 conference on Empirical methods in natural language processing,</booktitle>
<pages>25--32</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8208" citStr="Barzilay and Elhadad, 2003" startWordPosition="1264" endWordPosition="1268"> corresponding entity. Hence, they must be resolved without dependencies on co-referents. In contrast to NER, EL, and CR problems and their solutions, IER addresses instances where neither explicit mention of an entity nor noun phrases or any of the above mentioned features are guaranteed to appear in the text but still have a reference to a known entity. Hence, IER solutions require treatment for implied meaning of the phrases beyond its syntactic features. Since our solution to IER establishes a relationship between entity definitions and the input text, the tasks of paraphrase recognition (Barzilay and Elhadad, 2003) (Dolan et al., 2004) and textual entailment recognition (Giampiccolo et al., 2007) are related to our solution. However, these tasks are fundamentally different in two aspects: 1) Both paraphrase recognition and textual entailment recogni229 tion are defined at the sentence level, whereas text phrases considered for IER can exist as a sentence fragment or span across multiple sentences, and 2) The objective of IER is to find whether a given text phrase has a mention of an entity—as opposed to determining whether two sentences are similar or entail one another. However, our solution benefits f</context>
</contexts>
<marker>Barzilay, Elhadad, 2003</marker>
<rawString>Regina Barzilay and Noemie Elhadad. 2003. Sentence alignment for monolingual comparable corpora. In Proceedings of the 2003 conference on Empirical methods in natural language processing, pages 25–32. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivier Bodenreider</author>
</authors>
<title>The unified medical language system (umls): integrating biomedical terminology. Nucleic acids research, 32(suppl 1):D267–D270.</title>
<date>2004</date>
<contexts>
<context position="4591" citStr="Bodenreider, 2004" startWordPosition="688" endWordPosition="690">, 2015. challenging since, besides the fact that they lack the entity name, they can be embedded with negations. For example, the semantics of the sentence ‘The patients’ respiration become unlabored’ implies that the patient does not have ‘shortness of breath’. Identification of the negated mentions of entities in clinical documents is crucial as they provide valuable insights into the patients’ health status. We propose an unsupervised solution to the IER problem that leverages knowledge embedded in entity definitions obtained for each entity from the Unified Medical Language System (UMLS) (Bodenreider, 2004). UMLS provides a standard vocabulary for the clinical domain. Our solution: a) Creates an entity model from these definitions, b) Identifies the sentences in input text that may contain implicit entity mentions, c) Projects these sentences onto our entity model, and d) Classifies the sentences to distinguish between those containing implicit entity mentions or negated implicit mentions, by calculating the semantic similarity between the entity model and the projected sentences. The contributions of this work are as follows: 1. We introduce the problem of implicit entity recognition (IER) in c</context>
</contexts>
<marker>Bodenreider, 2004</marker>
<rawString>Olivier Bodenreider. 2004. The unified medical language system (umls): integrating biomedical terminology. Nucleic acids research, 32(suppl 1):D267–D270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan C Bunescu</author>
<author>Marius Pasca</author>
</authors>
<title>Using encyclopedic knowledge for named entity disambiguation.</title>
<date>2006</date>
<booktitle>In EACL,</booktitle>
<volume>6</volume>
<pages>9--16</pages>
<contexts>
<context position="6172" citStr="Bunescu and Pasca, 2006" startWordPosition="941" endWordPosition="944"> is the first work to address the problem of Implicit Entity Recognition (IER) in clinical documents. However, there is a large body of research that is relevant to the problem, including Named Entity Recognition (NER), Entity Linking (EL), Coreference Resolution, Paraphrase Recognition, and Textual Entailment Recognition. Much like IER, both NER and EL have the objective of binding a natural language expression to a semantic identifier. However, related work in NER and EL expect the proper name (explicit mention) of entities and assume the presence of noun phrases (Collins and Singer, 1999) (Bunescu and Pasca, 2006). The solutions developed for NER leverage regularities on morphological and syntactical features that are unlikely to hold in the case of IER. The most successful NER approaches use word-level features (such as capitalization, prefixes/suffixes, and punctuation), list lookup features (such as gazetteers, lexicons, or dictionaries), as well as corpus-level features (such as multiple occurrences, syntax, and frequency) (Nadeau and Sekine, 2007) that are not exhibited by the phrases with implicit entity mentions. Many approaches couple NER with a follow up EL step (Hachey et al., 2013) in order </context>
</contexts>
<marker>Bunescu, Pasca, 2006</marker>
<rawString>Razvan C Bunescu and Marius Pasca. 2006. Using encyclopedic knowledge for named entity disambiguation. In EACL, volume 6, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wendy W Chapman</author>
<author>Will Bridewell</author>
<author>Paul Hanbury</author>
<author>Gregory F Cooper</author>
<author>Bruce G Buchanan</author>
</authors>
<title>A simple algorithm for identifying negated findings and diseases in discharge summaries.</title>
<date>2001</date>
<journal>Journal of biomedical informatics,</journal>
<volume>34</volume>
<issue>5</issue>
<contexts>
<context position="17943" citStr="Chapman et al., 2001" startWordPosition="2855" endWordPosition="2858">b using the relationship ‘derivationally between the entity model and the pruned candidate related form’ in WordNet. sentence. The sentences with implicit entity men- Handling negations: Negations are of two types: tions often use adjectives and adverbs to describe the 1) Negations mentioned with explicit terms such as entity and they may indicate the absence of the en- no, not, and deny, and 2) Negations indicated with tities using antonyms or explicit negations. These antonyms (e.g., 2nd example in above list). We used two characteristics pose challenges to the applica- the NegEx algorithm (Chapman et al., 2001) to adbility of existing text similarity algorithms such as dress the first type of negations. To address the secMCS (Mihalcea et al., 2006) and matrixJcn (Fer- ond type of negations, we exploited the antonym renando and Stevenson, 2008) which are proven to lationships in the WordNet. perform well among the unsupervised algorithms in The similarity between an entity model and the paraphrase identification task (ACLWiki, 2014). pruned candidate sentence is calculated by computThe existing text similarity algorithms largely ing the similarities of their terms. The term simbenefit from the WordNe</context>
</contexts>
<marker>Chapman, Bridewell, Hanbury, Cooper, Buchanan, 2001</marker>
<rawString>Wendy W Chapman, Will Bridewell, Paul Hanbury, Gregory F Cooper, and Bruce G Buchanan. 2001. A simple algorithm for identifying negated findings and diseases in discharge summaries. Journal of biomedical informatics, 34(5):301–310.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Yoram Singer</author>
</authors>
<title>Unsupervised models for named entity classification.</title>
<date>1999</date>
<booktitle>In Proceedings of the joint SIGDAT conference on empirical methods in natural language</booktitle>
<pages>100--110</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="6146" citStr="Collins and Singer, 1999" startWordPosition="937" endWordPosition="940">best of our knowledge, this is the first work to address the problem of Implicit Entity Recognition (IER) in clinical documents. However, there is a large body of research that is relevant to the problem, including Named Entity Recognition (NER), Entity Linking (EL), Coreference Resolution, Paraphrase Recognition, and Textual Entailment Recognition. Much like IER, both NER and EL have the objective of binding a natural language expression to a semantic identifier. However, related work in NER and EL expect the proper name (explicit mention) of entities and assume the presence of noun phrases (Collins and Singer, 1999) (Bunescu and Pasca, 2006). The solutions developed for NER leverage regularities on morphological and syntactical features that are unlikely to hold in the case of IER. The most successful NER approaches use word-level features (such as capitalization, prefixes/suffixes, and punctuation), list lookup features (such as gazetteers, lexicons, or dictionaries), as well as corpus-level features (such as multiple occurrences, syntax, and frequency) (Nadeau and Sekine, 2007) that are not exhibited by the phrases with implicit entity mentions. Many approaches couple NER with a follow up EL step (Hach</context>
</contexts>
<marker>Collins, Singer, 1999</marker>
<rawString>Michael Collins and Yoram Singer. 1999. Unsupervised models for named entity classification. In Proceedings of the joint SIGDAT conference on empirical methods in natural language processing and very large corpora, pages 100–110. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Corinna Cortes</author>
<author>Vladimir Vapnik</author>
</authors>
<title>Supportvector networks.</title>
<date>1995</date>
<booktitle>Machine learning,</booktitle>
<pages>20--3</pages>
<contexts>
<context position="26520" citStr="Cortes and Vapnik, 1995" startWordPosition="4266" endWordPosition="4269">ork as baseline solutions to the problem. The first baseline is the well-known text similarity algorithm MCS (Mihalcea et al., 2006). MCS is one of the best performing unsupervised algorithms in paraphrase recognition task (ACLWiki, 2014). It uses an ensemble of statistical and semantic similarity measures, which is a preferable feature for the IER as opposed to one measure used by the matrixJcn (Fernando and Stevenson, 2008). Both MCS and our algorithm classify the candidate sentences based on threshold values selected experimentally. To include also a supervised baseline, we trained an SVM (Cortes and Vapnik, 1995) one of the state-of-the-art learning algorithms, shown to perform remarkably well in a number of classification tasks. We trained separate SVMs for each primary entity, considering unigrams, bigrams, and trigrams as the features. It has been shown that SVM trained on ngrams performed well on text classification tasks (Pang et al., 2002) (Zhang and Lee, 2003). The SVMs trained with bigrams consistently produced the best results for the 4-fold cross validation. Therefore, our testing phase used the SVMs trained with the bigrams. Preparation of training and testing datasets: We created training </context>
</contexts>
<marker>Cortes, Vapnik, 1995</marker>
<rawString>Corinna Cortes and Vladimir Vapnik. 1995. Supportvector networks. Machine learning, 20(3):273–297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill Dolan</author>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
</authors>
<title>Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th international conference on Computational Linguistics,</booktitle>
<pages>350</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8229" citStr="Dolan et al., 2004" startWordPosition="1269" endWordPosition="1272"> they must be resolved without dependencies on co-referents. In contrast to NER, EL, and CR problems and their solutions, IER addresses instances where neither explicit mention of an entity nor noun phrases or any of the above mentioned features are guaranteed to appear in the text but still have a reference to a known entity. Hence, IER solutions require treatment for implied meaning of the phrases beyond its syntactic features. Since our solution to IER establishes a relationship between entity definitions and the input text, the tasks of paraphrase recognition (Barzilay and Elhadad, 2003) (Dolan et al., 2004) and textual entailment recognition (Giampiccolo et al., 2007) are related to our solution. However, these tasks are fundamentally different in two aspects: 1) Both paraphrase recognition and textual entailment recogni229 tion are defined at the sentence level, whereas text phrases considered for IER can exist as a sentence fragment or span across multiple sentences, and 2) The objective of IER is to find whether a given text phrase has a mention of an entity—as opposed to determining whether two sentences are similar or entail one another. However, our solution benefits from the lessons learn</context>
</contexts>
<marker>Dolan, Quirk, Brockett, 2004</marker>
<rawString>Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources. In Proceedings of the 20th international conference on Computational Linguistics, page 350. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Durrett</author>
<author>Dan Klein</author>
</authors>
<title>Easy victories and uphill battles in coreference resolution.</title>
<date>2013</date>
<booktitle>In EMNLP,</booktitle>
<pages>1971--1982</pages>
<contexts>
<context position="7528" citStr="Durrett and Klein, 2013" startWordPosition="1154" endWordPosition="1157">zing implicit entity mentions by a solution developed for EL. Moreover, state-of-theart EL approaches include a ‘candidate mapping’ step that uses entity names to narrow down the space of possible entity identifiers, which is also a limiting factor in the IER case. Finally, neither NER nor EL deal with the negated mentions of entities. Coreference resolution (CR) focuses on grouping multiple mentions of the same entity with different surface forms. The solutions to CR focus on mapping explicit mentions of entity names to other pronouns and noun phrases referring to the same entity (Ng, 2010) (Durrett and Klein, 2013). In IER implicit mentions occur without co-referring corresponding entity. Hence, they must be resolved without dependencies on co-referents. In contrast to NER, EL, and CR problems and their solutions, IER addresses instances where neither explicit mention of an entity nor noun phrases or any of the above mentioned features are guaranteed to appear in the text but still have a reference to a known entity. Hence, IER solutions require treatment for implied meaning of the phrases beyond its syntactic features. Since our solution to IER establishes a relationship between entity definitions and </context>
</contexts>
<marker>Durrett, Klein, 2013</marker>
<rawString>Greg Durrett and Dan Klein. 2013. Easy victories and uphill battles in coreference resolution. In EMNLP, pages 1971–1982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel Fernando</author>
<author>Mark Stevenson</author>
</authors>
<title>A semantic similarity approach to paraphrase detection.</title>
<date>2008</date>
<booktitle>In Proceedings of the 11th Annual Research Colloquium of the UK Special Interest Group for Computational Linguistics,</booktitle>
<pages>45--52</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="26325" citStr="Fernando and Stevenson, 2008" startWordPosition="4237" endWordPosition="4240">mparison with our algorithm. However, we deem some of the related algorithms to have good potential applicability for this task. Therefore, we included two strong algorithms from the closest related work as baseline solutions to the problem. The first baseline is the well-known text similarity algorithm MCS (Mihalcea et al., 2006). MCS is one of the best performing unsupervised algorithms in paraphrase recognition task (ACLWiki, 2014). It uses an ensemble of statistical and semantic similarity measures, which is a preferable feature for the IER as opposed to one measure used by the matrixJcn (Fernando and Stevenson, 2008). Both MCS and our algorithm classify the candidate sentences based on threshold values selected experimentally. To include also a supervised baseline, we trained an SVM (Cortes and Vapnik, 1995) one of the state-of-the-art learning algorithms, shown to perform remarkably well in a number of classification tasks. We trained separate SVMs for each primary entity, considering unigrams, bigrams, and trigrams as the features. It has been shown that SVM trained on ngrams performed well on text classification tasks (Pang et al., 2002) (Zhang and Lee, 2003). The SVMs trained with bigrams consistently</context>
</contexts>
<marker>Fernando, Stevenson, 2008</marker>
<rawString>Samuel Fernando and Mark Stevenson. 2008. A semantic similarity approach to paraphrase detection. In Proceedings of the 11th Annual Research Colloquium of the UK Special Interest Group for Computational Linguistics, pages 45–52. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carol Friedman</author>
<author>Philip O Alderson</author>
<author>John HM Austin</author>
<author>James J Cimino</author>
<author>Stephen B Johnson</author>
</authors>
<title>A general natural-language text processor for clinical radiology.</title>
<date>1994</date>
<journal>Journal of the American Medical Informatics Association,</journal>
<volume>1</volume>
<issue>2</issue>
<contexts>
<context position="3246" citStr="Friedman et al., 1994" startWordPosition="472" endWordPosition="476"> expert “coders” in determining the unique identifier (e.g., ICD9 or ICD10) for each medical condition or combination of conditions. These identifiers are important to unambiguously represent the medical conditions, to prepare the post-discharge plan, and to perform secondary data analysis tasks. A human coder reading the sentence ‘Patient has shortness of breath with reaccumulation of fluid in extremities’ would generate the corresponding codes for entities ‘shortness of breath’ and ‘edema’. However, the solutions developed to perform entity recognition in clinical documents (Aronson, 2006) (Friedman et al., 1994) (Savova et al., 2010) (Friedman et al., 2004) (Fu and Ananiadou, 2014) (Pradhan et al., 2015) do not recognize the presence of entity ‘edema’ in this sentence. Implicit entity mentions are a common occurrence in clinical documents as they are often typed during a patient visit in a way that is natural in spoken language and meant for consumption by the professionals with similar backgrounds. An analysis with 300 documents in our corpus showed that 35% of the ‘edema’ mentions and 40% of the ‘shortness of breath’ mentions are implicit. Recognizing implicit mentions is particularly 228 Proceedin</context>
</contexts>
<marker>Friedman, Alderson, Austin, Cimino, Johnson, 1994</marker>
<rawString>Carol Friedman, Philip O Alderson, John HM Austin, James J Cimino, and Stephen B Johnson. 1994. A general natural-language text processor for clinical radiology. Journal of the American Medical Informatics Association, 1(2):161–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carol Friedman</author>
<author>Lyudmila Shagina</author>
<author>Yves Lussier</author>
<author>George Hripcsak</author>
</authors>
<title>Automated encoding of clinical documents based on natural language processing.</title>
<date>2004</date>
<journal>Journal of the American Medical Informatics Association,</journal>
<volume>11</volume>
<issue>5</issue>
<contexts>
<context position="3292" citStr="Friedman et al., 2004" startWordPosition="481" endWordPosition="484">ntifier (e.g., ICD9 or ICD10) for each medical condition or combination of conditions. These identifiers are important to unambiguously represent the medical conditions, to prepare the post-discharge plan, and to perform secondary data analysis tasks. A human coder reading the sentence ‘Patient has shortness of breath with reaccumulation of fluid in extremities’ would generate the corresponding codes for entities ‘shortness of breath’ and ‘edema’. However, the solutions developed to perform entity recognition in clinical documents (Aronson, 2006) (Friedman et al., 1994) (Savova et al., 2010) (Friedman et al., 2004) (Fu and Ananiadou, 2014) (Pradhan et al., 2015) do not recognize the presence of entity ‘edema’ in this sentence. Implicit entity mentions are a common occurrence in clinical documents as they are often typed during a patient visit in a way that is natural in spoken language and meant for consumption by the professionals with similar backgrounds. An analysis with 300 documents in our corpus showed that 35% of the ‘edema’ mentions and 40% of the ‘shortness of breath’ mentions are implicit. Recognizing implicit mentions is particularly 228 Proceedings of the Fourth Joint Conference on Lexical a</context>
<context position="9792" citStr="Friedman et al., 2004" startWordPosition="1521" endWordPosition="1524">e, whereas the implicit entity mention problem requires us to first check whether a particular sentence/phrase has a mention of an entity at all. Furthermore, question answering systems benefit from the presence of pronouns, nouns, and noun phrases in the questions and the candidate answers to derive helpful syntactic and semantic features (Lally et al., 2012)(Wang, 2006), while phrases with implicit entity mentions may not contain such features. The existing work on clinical document annotation focused on explicit entity mentions with contiguous phrases (Aronson, 2006) (Savova et al., 2010) (Friedman et al., 2004) (Fu and Ananiadou, 2014). Going one step beyond, the SemEval 2014 task 7 recognized the need for identifying discontiguous mentions of explicit entities (Pradhan et al., 2014). However, the recognition of implicit entities has yet to address by this community. 3 Implicit Entity Recognition (IER) in Clinical Documents We define the Implicit Entity Recognition (IER) task in clinical documents as: given input text that does not have explicit mentions of target entities, find which target entities are implied (including implied negations) in the input text. Negation detection is traditionally sep</context>
</contexts>
<marker>Friedman, Shagina, Lussier, Hripcsak, 2004</marker>
<rawString>Carol Friedman, Lyudmila Shagina, Yves Lussier, and George Hripcsak. 2004. Automated encoding of clinical documents based on natural language processing. Journal of the American Medical Informatics Association, 11(5):392–402.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiao Fu</author>
<author>Sophia Ananiadou</author>
</authors>
<title>Improving the extraction of clinical concepts from clinical records.</title>
<date>2014</date>
<booktitle>Proceedings of BioTxtM14.</booktitle>
<contexts>
<context position="3317" citStr="Fu and Ananiadou, 2014" startWordPosition="485" endWordPosition="488">CD10) for each medical condition or combination of conditions. These identifiers are important to unambiguously represent the medical conditions, to prepare the post-discharge plan, and to perform secondary data analysis tasks. A human coder reading the sentence ‘Patient has shortness of breath with reaccumulation of fluid in extremities’ would generate the corresponding codes for entities ‘shortness of breath’ and ‘edema’. However, the solutions developed to perform entity recognition in clinical documents (Aronson, 2006) (Friedman et al., 1994) (Savova et al., 2010) (Friedman et al., 2004) (Fu and Ananiadou, 2014) (Pradhan et al., 2015) do not recognize the presence of entity ‘edema’ in this sentence. Implicit entity mentions are a common occurrence in clinical documents as they are often typed during a patient visit in a way that is natural in spoken language and meant for consumption by the professionals with similar backgrounds. An analysis with 300 documents in our corpus showed that 35% of the ‘edema’ mentions and 40% of the ‘shortness of breath’ mentions are implicit. Recognizing implicit mentions is particularly 228 Proceedings of the Fourth Joint Conference on Lexical and Computational Semantic</context>
<context position="9817" citStr="Fu and Ananiadou, 2014" startWordPosition="1525" endWordPosition="1528">entity mention problem requires us to first check whether a particular sentence/phrase has a mention of an entity at all. Furthermore, question answering systems benefit from the presence of pronouns, nouns, and noun phrases in the questions and the candidate answers to derive helpful syntactic and semantic features (Lally et al., 2012)(Wang, 2006), while phrases with implicit entity mentions may not contain such features. The existing work on clinical document annotation focused on explicit entity mentions with contiguous phrases (Aronson, 2006) (Savova et al., 2010) (Friedman et al., 2004) (Fu and Ananiadou, 2014). Going one step beyond, the SemEval 2014 task 7 recognized the need for identifying discontiguous mentions of explicit entities (Pradhan et al., 2014). However, the recognition of implicit entities has yet to address by this community. 3 Implicit Entity Recognition (IER) in Clinical Documents We define the Implicit Entity Recognition (IER) task in clinical documents as: given input text that does not have explicit mentions of target entities, find which target entities are implied (including implied negations) in the input text. Negation detection is traditionally separated from the entity re</context>
</contexts>
<marker>Fu, Ananiadou, 2014</marker>
<rawString>Xiao Fu and Sophia Ananiadou. 2014. Improving the extraction of clinical concepts from clinical records. Proceedings of BioTxtM14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danilo Giampiccolo</author>
<author>Bernardo Magnini</author>
<author>Ido Dagan</author>
<author>Bill Dolan</author>
</authors>
<title>The third pascal recognizing textual entailment challenge.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACLPASCAL workshop on textual entailment and paraphrasing,</booktitle>
<pages>1--9</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8291" citStr="Giampiccolo et al., 2007" startWordPosition="1278" endWordPosition="1281">nts. In contrast to NER, EL, and CR problems and their solutions, IER addresses instances where neither explicit mention of an entity nor noun phrases or any of the above mentioned features are guaranteed to appear in the text but still have a reference to a known entity. Hence, IER solutions require treatment for implied meaning of the phrases beyond its syntactic features. Since our solution to IER establishes a relationship between entity definitions and the input text, the tasks of paraphrase recognition (Barzilay and Elhadad, 2003) (Dolan et al., 2004) and textual entailment recognition (Giampiccolo et al., 2007) are related to our solution. However, these tasks are fundamentally different in two aspects: 1) Both paraphrase recognition and textual entailment recogni229 tion are defined at the sentence level, whereas text phrases considered for IER can exist as a sentence fragment or span across multiple sentences, and 2) The objective of IER is to find whether a given text phrase has a mention of an entity—as opposed to determining whether two sentences are similar or entail one another. However, our solution benefits from the lessons learned from both tasks. The question answering solutions cope with</context>
</contexts>
<marker>Giampiccolo, Magnini, Dagan, Dolan, 2007</marker>
<rawString>Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. 2007. The third pascal recognizing textual entailment challenge. In Proceedings of the ACLPASCAL workshop on textual entailment and paraphrasing, pages 1–9. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Hachey</author>
<author>Will Radford</author>
<author>Joel Nothman</author>
<author>Matthew Honnibal</author>
<author>James R Curran</author>
</authors>
<title>Evaluating entity linking with wikipedia.</title>
<date>2013</date>
<journal>Artif. Intell.,</journal>
<volume>194</volume>
<contexts>
<context position="6762" citStr="Hachey et al., 2013" startWordPosition="1029" endWordPosition="1032">999) (Bunescu and Pasca, 2006). The solutions developed for NER leverage regularities on morphological and syntactical features that are unlikely to hold in the case of IER. The most successful NER approaches use word-level features (such as capitalization, prefixes/suffixes, and punctuation), list lookup features (such as gazetteers, lexicons, or dictionaries), as well as corpus-level features (such as multiple occurrences, syntax, and frequency) (Nadeau and Sekine, 2007) that are not exhibited by the phrases with implicit entity mentions. Many approaches couple NER with a follow up EL step (Hachey et al., 2013) in order to assign an unique entity identifiers to mentions. Therefore, the inadequacy of NER techniques will limit the capability of recognizing implicit entity mentions by a solution developed for EL. Moreover, state-of-theart EL approaches include a ‘candidate mapping’ step that uses entity names to narrow down the space of possible entity identifiers, which is also a limiting factor in the IER case. Finally, neither NER nor EL deal with the negated mentions of entities. Coreference resolution (CR) focuses on grouping multiple mentions of the same entity with different surface forms. The s</context>
</contexts>
<marker>Hachey, Radford, Nothman, Honnibal, Curran, 2013</marker>
<rawString>Ben Hachey, Will Radford, Joel Nothman, Matthew Honnibal, and James R. Curran. 2013. Evaluating entity linking with wikipedia. Artif. Intell., 194:130–150, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay J Jiang</author>
<author>David W Conrath</author>
</authors>
<title>Semantic similarity based on corpus statistics and lexical taxonomy. arXiv preprint cmp-lg/9709008.</title>
<date>1997</date>
<contexts>
<context position="19007" citStr="Jiang and Conrath, 1997" startWordPosition="3022" endWordPosition="3025">ndidate sentence is calculated by computThe existing text similarity algorithms largely ing the similarities of their terms. The term simbenefit from the WordNet similarity measures. Most ilarity is computed by forming an ensemble using of these measures use the semantics of the hierar- the standard WordNet similarity measures namely, chical arrangement of the terms in WordNet. Unfor- WUP (Wu and Palmer, 1994), LCH (Leacock and tunately, adjectives and adverbs are not arranged in Chodorow, 1998), Resnik (Resnik, 1995), LIN (Lin, a hierarchy, and terms with different part of speech 1998), JCN (Jiang and Conrath, 1997), as well as a (POS) tags cannot be mapped to the same hierarchy. predict vector-based measure Word2vec (Mikolov et Hence, they are limited in calculating the similar- al., 2013) and a morphology-based similarity metric ity between terms of these categories. This limi- Levenshtein1 as: tation negatively affects the performance of IER as sim(t1, t2) = maxm∈M(simm(t1, t2)) (2) the entity models and pruned sentences often contain terms from these categories. Consider the following examples: 1. Her breathing is still uncomfortableadjective. 2. She is breathing comfortablyadverb in room air. 3. His</context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>Jay J Jiang and David W Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. arXiv preprint cmp-lg/9709008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Lally</author>
<author>John M Prager</author>
<author>Michael C McCord</author>
<author>BK Boguraev</author>
<author>Siddharth Patwardhan</author>
<author>James Fan</author>
<author>Paul Fodor</author>
<author>Jennifer Chu-Carroll</author>
</authors>
<title>Question analysis: How watson reads a clue.</title>
<date>2012</date>
<journal>IBM Journal of Research and Development,</journal>
<pages>56--3</pages>
<contexts>
<context position="9532" citStr="Lally et al., 2012" startWordPosition="1482" endWordPosition="1485">scribe the characteristics of a concept and expect that concept as the answer. This particular type of questions resembles implicit entity mentions. However, they assume that the questions are referring to some concept and the problem is to uncover which one, whereas the implicit entity mention problem requires us to first check whether a particular sentence/phrase has a mention of an entity at all. Furthermore, question answering systems benefit from the presence of pronouns, nouns, and noun phrases in the questions and the candidate answers to derive helpful syntactic and semantic features (Lally et al., 2012)(Wang, 2006), while phrases with implicit entity mentions may not contain such features. The existing work on clinical document annotation focused on explicit entity mentions with contiguous phrases (Aronson, 2006) (Savova et al., 2010) (Friedman et al., 2004) (Fu and Ananiadou, 2014). Going one step beyond, the SemEval 2014 task 7 recognized the need for identifying discontiguous mentions of explicit entities (Pradhan et al., 2014). However, the recognition of implicit entities has yet to address by this community. 3 Implicit Entity Recognition (IER) in Clinical Documents We define the Implic</context>
</contexts>
<marker>Lally, Prager, McCord, Boguraev, Patwardhan, Fan, Fodor, Chu-Carroll, 2012</marker>
<rawString>Adam Lally, John M Prager, Michael C McCord, BK Boguraev, Siddharth Patwardhan, James Fan, Paul Fodor, and Jennifer Chu-Carroll. 2012. Question analysis: How watson reads a clue. IBM Journal of Research and Development, 56(3.4):2–1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Leacock</author>
<author>Martin Chodorow</author>
</authors>
<title>Combining local context and wordnet similarity for word sense identification. WordNet: An electronic lexical database,</title>
<date>1998</date>
<pages>49--2</pages>
<marker>Leacock, Chodorow, 1998</marker>
<rawString>Claudia Leacock and Martin Chodorow. 1998. Combining local context and wordnet similarity for word sense identification. WordNet: An electronic lexical database, 49(2):265–283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>An information-theoretic definition of similarity.</title>
<date>1998</date>
<booktitle>In ICML,</booktitle>
<volume>98</volume>
<pages>296--304</pages>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. An information-theoretic definition of similarity. In ICML, volume 98, pages 296–304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Courtney Corley</author>
<author>Carlo Strapparava</author>
</authors>
<title>Corpus-based and knowledge-based measures of text semantic similarity.</title>
<date>2006</date>
<booktitle>In AAAI,</booktitle>
<volume>6</volume>
<pages>775--780</pages>
<contexts>
<context position="18083" citStr="Mihalcea et al., 2006" startWordPosition="2880" endWordPosition="2883">s with implicit entity men- Handling negations: Negations are of two types: tions often use adjectives and adverbs to describe the 1) Negations mentioned with explicit terms such as entity and they may indicate the absence of the en- no, not, and deny, and 2) Negations indicated with tities using antonyms or explicit negations. These antonyms (e.g., 2nd example in above list). We used two characteristics pose challenges to the applica- the NegEx algorithm (Chapman et al., 2001) to adbility of existing text similarity algorithms such as dress the first type of negations. To address the secMCS (Mihalcea et al., 2006) and matrixJcn (Fer- ond type of negations, we exploited the antonym renando and Stevenson, 2008) which are proven to lationships in the WordNet. perform well among the unsupervised algorithms in The similarity between an entity model and the paraphrase identification task (ACLWiki, 2014). pruned candidate sentence is calculated by computThe existing text similarity algorithms largely ing the similarities of their terms. The term simbenefit from the WordNet similarity measures. Most ilarity is computed by forming an ensemble using of these measures use the semantics of the hierar- the standard</context>
<context position="26028" citStr="Mihalcea et al., 2006" startWordPosition="4189" endWordPosition="4192"> annotated candidate sentences. The prepared data set is available at http://knoesis.org/ researchers/sujan/data-sets.html 4.2 Implicit Entity Recognition Performance Since IER is a novel task, there are no baseline algorithms that can be directly applied such that it would yield a fair comparison with our algorithm. However, we deem some of the related algorithms to have good potential applicability for this task. Therefore, we included two strong algorithms from the closest related work as baseline solutions to the problem. The first baseline is the well-known text similarity algorithm MCS (Mihalcea et al., 2006). MCS is one of the best performing unsupervised algorithms in paraphrase recognition task (ACLWiki, 2014). It uses an ensemble of statistical and semantic similarity measures, which is a preferable feature for the IER as opposed to one measure used by the matrixJcn (Fernando and Stevenson, 2008). Both MCS and our algorithm classify the candidate sentences based on threshold values selected experimentally. To include also a supervised baseline, we trained an SVM (Cortes and Vapnik, 1995) one of the state-of-the-art learning algorithms, shown to perform remarkably well in a number of classifica</context>
</contexts>
<marker>Mihalcea, Corley, Strapparava, 2006</marker>
<rawString>Rada Mihalcea, Courtney Corley, and Carlo Strapparava. 2006. Corpus-based and knowledge-based measures of text semantic similarity. In AAAI, volume 6, pages 775–780.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</title>
<date>2013</date>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Nadeau</author>
<author>Satoshi Sekine</author>
</authors>
<title>A survey of named entity recognition and classification.</title>
<date>2007</date>
<journal>Lingvisticae Investigationes,</journal>
<volume>30</volume>
<issue>1</issue>
<contexts>
<context position="6619" citStr="Nadeau and Sekine, 2007" startWordPosition="1004" endWordPosition="1007">er, related work in NER and EL expect the proper name (explicit mention) of entities and assume the presence of noun phrases (Collins and Singer, 1999) (Bunescu and Pasca, 2006). The solutions developed for NER leverage regularities on morphological and syntactical features that are unlikely to hold in the case of IER. The most successful NER approaches use word-level features (such as capitalization, prefixes/suffixes, and punctuation), list lookup features (such as gazetteers, lexicons, or dictionaries), as well as corpus-level features (such as multiple occurrences, syntax, and frequency) (Nadeau and Sekine, 2007) that are not exhibited by the phrases with implicit entity mentions. Many approaches couple NER with a follow up EL step (Hachey et al., 2013) in order to assign an unique entity identifiers to mentions. Therefore, the inadequacy of NER techniques will limit the capability of recognizing implicit entity mentions by a solution developed for EL. Moreover, state-of-theart EL approaches include a ‘candidate mapping’ step that uses entity names to narrow down the space of possible entity identifiers, which is also a limiting factor in the IER case. Finally, neither NER nor EL deal with the negated</context>
</contexts>
<marker>Nadeau, Sekine, 2007</marker>
<rawString>David Nadeau and Satoshi Sekine. 2007. A survey of named entity recognition and classification. Lingvisticae Investigationes, 30(1):3–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
</authors>
<title>Supervised noun phrase coreference research: The first fifteen years.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10,</booktitle>
<pages>1396--1411</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="7502" citStr="Ng, 2010" startWordPosition="1152" endWordPosition="1153"> of recognizing implicit entity mentions by a solution developed for EL. Moreover, state-of-theart EL approaches include a ‘candidate mapping’ step that uses entity names to narrow down the space of possible entity identifiers, which is also a limiting factor in the IER case. Finally, neither NER nor EL deal with the negated mentions of entities. Coreference resolution (CR) focuses on grouping multiple mentions of the same entity with different surface forms. The solutions to CR focus on mapping explicit mentions of entity names to other pronouns and noun phrases referring to the same entity (Ng, 2010) (Durrett and Klein, 2013). In IER implicit mentions occur without co-referring corresponding entity. Hence, they must be resolved without dependencies on co-referents. In contrast to NER, EL, and CR problems and their solutions, IER addresses instances where neither explicit mention of an entity nor noun phrases or any of the above mentioned features are guaranteed to appear in the text but still have a reference to a known entity. Hence, IER solutions require treatment for implied meaning of the phrases beyond its syntactic features. Since our solution to IER establishes a relationship betwe</context>
</contexts>
<marker>Ng, 2010</marker>
<rawString>Vincent Ng. 2010. Supervised noun phrase coreference research: The first fifteen years. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 1396–1411, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up?: sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL02 conference on Empirical methods in natural language processing-Volume 10,</booktitle>
<pages>79--86</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="26859" citStr="Pang et al., 2002" startWordPosition="4323" endWordPosition="4326">e IER as opposed to one measure used by the matrixJcn (Fernando and Stevenson, 2008). Both MCS and our algorithm classify the candidate sentences based on threshold values selected experimentally. To include also a supervised baseline, we trained an SVM (Cortes and Vapnik, 1995) one of the state-of-the-art learning algorithms, shown to perform remarkably well in a number of classification tasks. We trained separate SVMs for each primary entity, considering unigrams, bigrams, and trigrams as the features. It has been shown that SVM trained on ngrams performed well on text classification tasks (Pang et al., 2002) (Zhang and Lee, 2003). The SVMs trained with bigrams consistently produced the best results for the 4-fold cross validation. Therefore, our testing phase used the SVMs trained with the bigrams. Preparation of training and testing datasets: We created training and testing datasets by splitting the dataset annotated for each primary entity as 70% (training) and 30% (testing). The training datasets were used to train the SVM models for each primary entity and to select the threshold values for both MCS and our algorithm. The classification performance of each algorithm is studied in the TP and T</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up?: sentiment classification using machine learning techniques. In Proceedings of the ACL02 conference on Empirical methods in natural language processing-Volume 10, pages 79–86. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
</authors>
<title>No´emie Elhadad, Wendy Chapman, Suresh Manandhar, and Guergana Savova.</title>
<date>2014</date>
<booktitle>Semeval-2014 task 7: Analysis of clinical text. SemEval2014,</booktitle>
<marker>Pradhan, 2014</marker>
<rawString>Sameer Pradhan, No´emie Elhadad, Wendy Chapman, Suresh Manandhar, and Guergana Savova. 2014. Semeval-2014 task 7: Analysis of clinical text. SemEval2014, 199(99):54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>No´emie Elhadad</author>
<author>Brett R South</author>
<author>David Martinez</author>
<author>Lee Christensen</author>
<author>Amy Vogel</author>
<author>Hanna Suominen</author>
<author>Wendy W Chapman</author>
<author>Guergana Savova</author>
</authors>
<title>Evaluating the state of the art in disorder recognition and normalization of the clinical narrative.</title>
<date>2015</date>
<journal>Journal of the American Medical Informatics Association,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="3340" citStr="Pradhan et al., 2015" startWordPosition="489" endWordPosition="492">ndition or combination of conditions. These identifiers are important to unambiguously represent the medical conditions, to prepare the post-discharge plan, and to perform secondary data analysis tasks. A human coder reading the sentence ‘Patient has shortness of breath with reaccumulation of fluid in extremities’ would generate the corresponding codes for entities ‘shortness of breath’ and ‘edema’. However, the solutions developed to perform entity recognition in clinical documents (Aronson, 2006) (Friedman et al., 1994) (Savova et al., 2010) (Friedman et al., 2004) (Fu and Ananiadou, 2014) (Pradhan et al., 2015) do not recognize the presence of entity ‘edema’ in this sentence. Implicit entity mentions are a common occurrence in clinical documents as they are often typed during a patient visit in a way that is natural in spoken language and meant for consumption by the professionals with similar backgrounds. An analysis with 300 documents in our corpus showed that 35% of the ‘edema’ mentions and 40% of the ‘shortness of breath’ mentions are implicit. Recognizing implicit mentions is particularly 228 Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 22</context>
</contexts>
<marker>Pradhan, Elhadad, South, Martinez, Christensen, Vogel, Suominen, Chapman, Savova, 2015</marker>
<rawString>Sameer Pradhan, No´emie Elhadad, Brett R South, David Martinez, Lee Christensen, Amy Vogel, Hanna Suominen, Wendy W Chapman, and Guergana Savova. 2015. Evaluating the state of the art in disorder recognition and normalization of the clinical narrative. Journal of the American Medical Informatics Association, 22(1):143–154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Using information content to evaluate semantic similarity in a taxonomy.</title>
<date>1995</date>
<booktitle>In Proceedings of the 14th International Joint Conference on Artificial Intelligence - Volume 1, IJCAI’95,</booktitle>
<pages>448--453</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="18906" citStr="Resnik, 1995" startWordPosition="3007" endWordPosition="3008"> between an entity model and the paraphrase identification task (ACLWiki, 2014). pruned candidate sentence is calculated by computThe existing text similarity algorithms largely ing the similarities of their terms. The term simbenefit from the WordNet similarity measures. Most ilarity is computed by forming an ensemble using of these measures use the semantics of the hierar- the standard WordNet similarity measures namely, chical arrangement of the terms in WordNet. Unfor- WUP (Wu and Palmer, 1994), LCH (Leacock and tunately, adjectives and adverbs are not arranged in Chodorow, 1998), Resnik (Resnik, 1995), LIN (Lin, a hierarchy, and terms with different part of speech 1998), JCN (Jiang and Conrath, 1997), as well as a (POS) tags cannot be mapped to the same hierarchy. predict vector-based measure Word2vec (Mikolov et Hence, they are limited in calculating the similar- al., 2013) and a morphology-based similarity metric ity between terms of these categories. This limi- Levenshtein1 as: tation negatively affects the performance of IER as sim(t1, t2) = maxm∈M(simm(t1, t2)) (2) the entity models and pruned sentences often contain terms from these categories. Consider the following examples: 1. Her</context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>Philip Resnik. 1995. Using information content to evaluate semantic similarity in a taxonomy. In Proceedings of the 14th International Joint Conference on Artificial Intelligence - Volume 1, IJCAI’95, pages 448–453, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guergana K Savova</author>
<author>James J Masanz</author>
<author>Philip V Ogren</author>
<author>Jiaping Zheng</author>
<author>Sunghwan Sohn</author>
<author>Karin C KipperSchuler</author>
<author>Christopher G Chute</author>
</authors>
<title>Mayo clinical text analysis and knowledge extraction system (ctakes): architecture, component evaluation and applications.</title>
<date>2010</date>
<journal>Journal of the American Medical Informatics Association,</journal>
<volume>17</volume>
<issue>5</issue>
<contexts>
<context position="3268" citStr="Savova et al., 2010" startWordPosition="477" endWordPosition="480">rmining the unique identifier (e.g., ICD9 or ICD10) for each medical condition or combination of conditions. These identifiers are important to unambiguously represent the medical conditions, to prepare the post-discharge plan, and to perform secondary data analysis tasks. A human coder reading the sentence ‘Patient has shortness of breath with reaccumulation of fluid in extremities’ would generate the corresponding codes for entities ‘shortness of breath’ and ‘edema’. However, the solutions developed to perform entity recognition in clinical documents (Aronson, 2006) (Friedman et al., 1994) (Savova et al., 2010) (Friedman et al., 2004) (Fu and Ananiadou, 2014) (Pradhan et al., 2015) do not recognize the presence of entity ‘edema’ in this sentence. Implicit entity mentions are a common occurrence in clinical documents as they are often typed during a patient visit in a way that is natural in spoken language and meant for consumption by the professionals with similar backgrounds. An analysis with 300 documents in our corpus showed that 35% of the ‘edema’ mentions and 40% of the ‘shortness of breath’ mentions are implicit. Recognizing implicit mentions is particularly 228 Proceedings of the Fourth Joint</context>
<context position="9768" citStr="Savova et al., 2010" startWordPosition="1517" endWordPosition="1520">is to uncover which one, whereas the implicit entity mention problem requires us to first check whether a particular sentence/phrase has a mention of an entity at all. Furthermore, question answering systems benefit from the presence of pronouns, nouns, and noun phrases in the questions and the candidate answers to derive helpful syntactic and semantic features (Lally et al., 2012)(Wang, 2006), while phrases with implicit entity mentions may not contain such features. The existing work on clinical document annotation focused on explicit entity mentions with contiguous phrases (Aronson, 2006) (Savova et al., 2010) (Friedman et al., 2004) (Fu and Ananiadou, 2014). Going one step beyond, the SemEval 2014 task 7 recognized the need for identifying discontiguous mentions of explicit entities (Pradhan et al., 2014). However, the recognition of implicit entities has yet to address by this community. 3 Implicit Entity Recognition (IER) in Clinical Documents We define the Implicit Entity Recognition (IER) task in clinical documents as: given input text that does not have explicit mentions of target entities, find which target entities are implied (including implied negations) in the input text. Negation detect</context>
<context position="23482" citStr="Savova et al., 2010" startWordPosition="3770" endWordPosition="3773"> experts. The gold standard consists of 857 2We do not explicitly report performance on TN because our focus is to find sentences that contain entity mentions rather than those devoid of mentions. Entity TP Tneg TN Shortness of breath 93 94 29 Edema 115 35 81 Syncope 96 92 24 Cholecystitis 78 36 4 Gastrointestinal gas 18 14 5 Colitis 12 11 0 Cellulitis 8 2 0 Fasciitis 7 3 0 Table 1: Candidate Sentence Statistics sentences selected for eight entities. The creation of the gold standard is described below in detail. We have annotated the corpus for explicit mentions of the entities using cTAKES (Savova et al., 2010) and ranked the entities based on their frequency. The domain experts on our team then selected a subset of these entities that they judged to be frequently mentioned implicitly in clinical documents. For example, the frequent entity ‘shortness of breath’ was selected but not ‘chest pain’ since the former is mentioned implicitly often but not the latter. We used four frequently implicitly mentioned entities as the primary focus of our evaluation. We refer to these as primary entities from here on (the first four entities in Table 1). To test the generalizability of our method, as well as to ev</context>
</contexts>
<marker>Savova, Masanz, Ogren, Zheng, Sohn, KipperSchuler, Chute, 2010</marker>
<rawString>Guergana K Savova, James J Masanz, Philip V Ogren, Jiaping Zheng, Sunghwan Sohn, Karin C KipperSchuler, and Christopher G Chute. 2010. Mayo clinical text analysis and knowledge extraction system (ctakes): architecture, component evaluation and applications. Journal of the American Medical Informatics Association, 17(5):507–513.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Tzong-Han Tsai</author>
<author>Shih-Hung Wu</author>
<author>Wen-Chi Chou</author>
<author>Yu-Chun Lin</author>
<author>Ding He</author>
<author>Jieh Hsiang</author>
<author>Ting-Yi Sung</author>
<author>Wen-Lian Hsu</author>
</authors>
<title>Various criteria in the evaluation of biomedical named entity recognition.</title>
<date>2006</date>
<journal>BMC bioinformatics,</journal>
<volume>7</volume>
<issue>1</issue>
<contexts>
<context position="11189" citStr="Tsai et al., 2006" startWordPosition="1738" endWordPosition="1741">plicit mention can involve an antonym that fuses the entity indication with negated sense Figure 1: Components of the Proposed Solution (e.g., ‘patient denies shortness of breath’ vs ‘patient is breathing comfortably’). Hence, negation detection is considered as a sub-task of IER. Typical entity recognition task considers the detection of the boundaries of the phrases with entities (i.e., segmentation) as a sub-task. We consider boundary detection of the implicit entity mentions as an optional step due to two reasons: 1) It is considered an optional step in biomedical entity recognition task (Tsai et al., 2006), and 2) The phrases with implicit entity mentions can be noncontiguous and span multiple sentences. Further, in some cases, even domain experts disagree on the precise phrase boundaries. We define the IER as a classification task. Given an input text, classify it to one of the three categories: TPe if the text has a mention of entity e, or Tnege if the text has a negated mention of entity e, or TNe if the entity e is not mentioned at all. As mentioned, the phrases with implicit entity mentions can span to multiple sentences. However, this work will focus only on implicit mentions exist within</context>
</contexts>
<marker>Tsai, Wu, Chou, Lin, He, Hsiang, Sung, Hsu, 2006</marker>
<rawString>Richard Tzong-Han Tsai, Shih-Hung Wu, Wen-Chi Chou, Yu-Chun Lin, Ding He, Jieh Hsiang, Ting-Yi Sung, and Wen-Lian Hsu. 2006. Various criteria in the evaluation of biomedical named entity recognition. BMC bioinformatics, 7(1):92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mengqiu Wang</author>
</authors>
<title>A survey of answer extraction techniques in factoid question answering.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="9544" citStr="Wang, 2006" startWordPosition="1485" endWordPosition="1486">istics of a concept and expect that concept as the answer. This particular type of questions resembles implicit entity mentions. However, they assume that the questions are referring to some concept and the problem is to uncover which one, whereas the implicit entity mention problem requires us to first check whether a particular sentence/phrase has a mention of an entity at all. Furthermore, question answering systems benefit from the presence of pronouns, nouns, and noun phrases in the questions and the candidate answers to derive helpful syntactic and semantic features (Lally et al., 2012)(Wang, 2006), while phrases with implicit entity mentions may not contain such features. The existing work on clinical document annotation focused on explicit entity mentions with contiguous phrases (Aronson, 2006) (Savova et al., 2010) (Friedman et al., 2004) (Fu and Ananiadou, 2014). Going one step beyond, the SemEval 2014 task 7 recognized the need for identifying discontiguous mentions of explicit entities (Pradhan et al., 2014). However, the recognition of implicit entities has yet to address by this community. 3 Implicit Entity Recognition (IER) in Clinical Documents We define the Implicit Entity Re</context>
</contexts>
<marker>Wang, 2006</marker>
<rawString>Mengqiu Wang. 2006. A survey of answer extraction techniques in factoid question answering. Computational Linguistics, 1(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhibiao Wu</author>
<author>Martha Palmer</author>
</authors>
<title>Verbs semantics and lexical selection.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32Nd Annual Meeting on Association for Computational Linguistics, ACL ’94,</booktitle>
<pages>133--138</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="18796" citStr="Wu and Palmer, 1994" startWordPosition="2989" endWordPosition="2992">008) which are proven to lationships in the WordNet. perform well among the unsupervised algorithms in The similarity between an entity model and the paraphrase identification task (ACLWiki, 2014). pruned candidate sentence is calculated by computThe existing text similarity algorithms largely ing the similarities of their terms. The term simbenefit from the WordNet similarity measures. Most ilarity is computed by forming an ensemble using of these measures use the semantics of the hierar- the standard WordNet similarity measures namely, chical arrangement of the terms in WordNet. Unfor- WUP (Wu and Palmer, 1994), LCH (Leacock and tunately, adjectives and adverbs are not arranged in Chodorow, 1998), Resnik (Resnik, 1995), LIN (Lin, a hierarchy, and terms with different part of speech 1998), JCN (Jiang and Conrath, 1997), as well as a (POS) tags cannot be mapped to the same hierarchy. predict vector-based measure Word2vec (Mikolov et Hence, they are limited in calculating the similar- al., 2013) and a morphology-based similarity metric ity between terms of these categories. This limi- Levenshtein1 as: tation negatively affects the performance of IER as sim(t1, t2) = maxm∈M(simm(t1, t2)) (2) the entity </context>
</contexts>
<marker>Wu, Palmer, 1994</marker>
<rawString>Zhibiao Wu and Martha Palmer. 1994. Verbs semantics and lexical selection. In Proceedings of the 32Nd Annual Meeting on Association for Computational Linguistics, ACL ’94, pages 133–138, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dell Zhang</author>
<author>Wee Sun Lee</author>
</authors>
<title>Question classification using support vector machines.</title>
<date>2003</date>
<booktitle>In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval,</booktitle>
<pages>26--32</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="26881" citStr="Zhang and Lee, 2003" startWordPosition="4327" endWordPosition="4330">one measure used by the matrixJcn (Fernando and Stevenson, 2008). Both MCS and our algorithm classify the candidate sentences based on threshold values selected experimentally. To include also a supervised baseline, we trained an SVM (Cortes and Vapnik, 1995) one of the state-of-the-art learning algorithms, shown to perform remarkably well in a number of classification tasks. We trained separate SVMs for each primary entity, considering unigrams, bigrams, and trigrams as the features. It has been shown that SVM trained on ngrams performed well on text classification tasks (Pang et al., 2002) (Zhang and Lee, 2003). The SVMs trained with bigrams consistently produced the best results for the 4-fold cross validation. Therefore, our testing phase used the SVMs trained with the bigrams. Preparation of training and testing datasets: We created training and testing datasets by splitting the dataset annotated for each primary entity as 70% (training) and 30% (testing). The training datasets were used to train the SVM models for each primary entity and to select the threshold values for both MCS and our algorithm. The classification performance of each algorithm is studied in the TP and Tneg categories using p</context>
</contexts>
<marker>Zhang, Lee, 2003</marker>
<rawString>Dell Zhang and Wee Sun Lee. 2003. Question classification using support vector machines. In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 26–32. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>