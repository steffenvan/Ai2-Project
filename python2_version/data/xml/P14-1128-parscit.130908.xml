<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.964819">
Toward Better Chinese Word Segmentation for SMT via Bilingual
Constraints
</title>
<author confidence="0.651968">
Xiaodong Zeng† Lidia S. Chao† Derek F. Wong† Isabel Trancoso‡ Liang Tian††NLP2CT Lab / Department of Computer and Information Science, University of Macau
</author>
<affiliation confidence="0.40661">
‡INESC-ID / Instituto Superior T´enico, Lisboa, Portugal
</affiliation>
<email confidence="0.7476275">
nlp2ct.samuel@gmail.com, {lidiasc, derekfw}@umac.mo,
isabel.trancoso@inesc-id.pt,tianliang0123@gmail.com
</email>
<sectionHeader confidence="0.979553" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999941904761905">
This study investigates on building a
better Chinese word segmentation mod-
el for statistical machine translation. It
aims at leveraging word boundary infor-
mation, automatically learned by bilin-
gual character-based alignments, to induce
a preferable segmentation model. We
propose dealing with the induced word
boundaries as soft constraints to bias the
continuous learning of a supervised CRF-
s model, trained by the treebank data (la-
beled), on the bilingual data (unlabeled).
The induced word boundary information
is encoded as a graph propagation con-
straint. The constrained model induction
is accomplished by using posterior reg-
ularization algorithm. The experiments
on a Chinese-to-English machine transla-
tion task reveal that the proposed model
can bring positive segmentation effects to
translation quality.
</bodyText>
<sectionHeader confidence="0.99512" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.989414551724138">
Word segmentation is regarded as a critical pro-
cedure for high-level Chinese language process-
ing tasks, since Chinese scripts are written in con-
tinuous characters without explicit word bound-
aries (e.g., space in English). The empirical works
show that word segmentation can be beneficial to
Chinese-to-English statistical machine translation
(SMT) (Xu et al., 2005; Chang et al., 2008; Zhao
et al., 2013). In fact most current SMT models
assume that parallel bilingual sentences should be
segmented into sequences of tokens that are meant
to be “words” (Ma and Way, 2009). The practice
in state-of-the-art MT systems is that Chinese sen-
tences are tokenized by a monolingual supervised
word segmentation model trained on the hand-
annotated treebank data, e.g., Chinese treebank
(CTB) (Xue et al., 2005). These models are con-
ducive to MT to some extent, since they common-
ly have relatively good aggregate performance and
segmentation consistency (Chang et al., 2008).
But one outstanding problem is that these mod-
els may leave out some crucial segmentation fea-
tures for SMT, since the output words conform to
the treebank segmentation standard designed for
monolingually linguistic intuition, rather than spe-
cific to the SMT task.
In recent years, a number of works (Xu et al.,
2005; Chang et al., 2008; Ma and Way, 2009;
Xi et al., 2012) attempted to build segmentation
models for SMT based on bilingual unsegment-
ed data, instead of monolingual segmented data.
They proposed to learn gainful bilingual knowl-
edge as golden-standard segmentation supervi-
sions for training a bilingual unsupervised mod-
el. Frequently, the bilingual knowledge refers to
the mappings of an individual English word to one
or more consecutive Chinese characters, generat-
ed via statistical character-based alignment. They
leverage such mappings to either constitute a Chi-
nese word dictionary for maximum-matching seg-
mentation (Xu et al., 2004), or form labeled data
for training a sequence labeling model (Paul et al.,
2011). The prior works showed that these models
help to find some segmentations tailored for SMT,
since the bilingual word occurrence feature can be
captured by the character-based alignment (Och
and Ney, 2003). However, these models tend to
miss out other linguistic segmentation patterns as
monolingual supervised models, and suffer from
the negative effects of erroneously alignments to
word segmentation.
This paper proposes an alternative Chinese
Word Segmentation (CWS) model adapted to the
SMT task, which seeks not only to maintain the
advantages of a monolingual supervised model,
having hand-annotated linguistic knowledge, but
also to assimilate the relevant bilingual segmenta-
1360
</bodyText>
<note confidence="0.963201">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1360–1369,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999982303030303">
tion nature. We propose leveraging the bilingual
knowledge to form learning constraints that guide
a supervised segmentation model toward a better
solution for SMT. Besides the bilingual motivat-
ed models, character-based alignment is also em-
ployed to achieve the mappings of the successive
Chinese characters and the target language word-
s. Instead of directly merging the characters in-
to concrete segmentations, this work attempts to
extract word boundary distributions for character-
level trigrams (types) from the “chars-to-word”
mappings. Furthermore, these word boundaries
are encoded into a graph propagation (GP) expres-
sion, in order to widen the influence of the induced
bilingual knowledge among Chinese texts. The G-
P expression constrains similar types having ap-
proximated word boundary distributions. Crucial-
ly, the GP expression with the bilingual knowledge
is then used as side information to regularize a
CRFs (conditional random fields) model’s learn-
ing over treebank and bitext data, based on the
posterior regularization (PR) framework (Ganchev
et al., 2010). This constrained learning amounts to
a jointly coupling of GP and CRFs, i.e., integrating
GP into the estimation of a parametric structural
model.
This paper is structured as follows: Section 2
points out the main differences with the related
works of this study. Section 3 presents the de-
tails of the proposed segmentation model. Section
4 reports the experimental results of the proposed
model for a Chinese-to-English MT task. The con-
clusion is drawn in Section 5.
</bodyText>
<sectionHeader confidence="0.999298" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999818029850747">
In the literature, many approaches have been pro-
posed to learn CWS models for SMT. They can
be put into two categories, monolingual-motivated
and bilingual-motivated. The former primarily op-
timizes monolingual supervised models according
to some predefined segmentation properties that
are manually summarized from empirical MT e-
valuations. Chang et al. (2008) enhanced a CRF-
s segmentation model in MT tasks by tuning the
word granularity and improving the segmentation
consistence. Zhang et al. (2008) produced a bet-
ter segmentation model for SMT by concatenat-
ing various corpora regardless of their differen-
t specifications. Distinct from their behaviors,
this work uses automatically learned constraints
instead of manually defined ones. Most impor-
tantly, the constraints have a better learning guid-
ance since they originate from the bilingual texts.
On the other hand, the bilingual-motivated CWS
models typically rely on character-based align-
ments to generate segmentation supervisions. Xu
et al. (2004) proposed to employ “chars-to-word”
alignments to generate a word dictionary for max-
imum matching segmentation in SMT task. The
works in (Ma and Way, 2009; Zhao et al., 2013)
extended the dictionary extraction strategy. Ma
and Way (2009) adopted co-occurrence frequency
metric to iteratively optimize “candidate words”
extract from the alignments. Zhao et al. (2013) at-
tempted to find an optimal subset of the dictionary
learned by the character-based alignment to maxi-
mize the MT performance. Paul et al. (2011) used
the words learned from “chars-to-word” align-
ments to train a maximum entropy segmentation
model. Rather than playing the “hard” uses of
the bilingual segmentation knowledge, i.e., direct-
ly merging “char-to-word” alignments to words
as supervisions, this study extracts word bound-
ary information of characters from the alignments
as soft constraints to regularize a CRFs model’s
learning.
The graph propagation (GP) technique provides
a natural way to represent data in a variety of tar-
get domains (Belkin et al., 2006). In this tech-
nique, the constructed graph has vertices consist-
ing of labeled and unlabeled examples. Pairs of
vertices are connected by weighted edges encod-
ing the degree to which they are expected to have
the same label (Zhu et al., 2003). Many recent
works, such as by Subramanya et al. (2010), Das
and Petrov (2011), Zeng et al. (2013; 2014) and
Zhu et al. (2014), proposed GP for inferring the la-
bel information of unlabeled data, and then lever-
age these GP outcomes to learn a semi-supervised
scalable model (e.g., CRFs). These approaches are
referred to as pipelined learning with GP. This s-
tudy also works with a similarity graph, encoding
the learned bilingual knowledge. But, unlike the
prior pipelined approaches, this study performs a
joint learning behavior in which GP is used as a
learning constraint to interact with the CRFs mod-
el estimation.
One of our main objectives is to bias CRF-
s model’s learning on unlabeled data, under a
non-linear GP constraint encoding the bilingual
knowledge. This is accomplished by the poste-
rior regularization (PR) framework (Ganchev et
</bodyText>
<page confidence="0.515608">
1361
</page>
<bodyText confidence="0.9993112">
al., 2010). PR performs regularization on poste-
riors, so that the learned model itself remains sim-
ple and tractable, while during learning it is driven
to obey the constraints through setting appropriate
parameters. The closest prior study is constrained
learning, or learning with prior knowledge. Chang
et al. (2008) described constraint driven learning
(CODL) that augments model learning on unla-
beled data by adding a cost for violating expec-
tations of constraint features designed by domain
knowledge. Mann and McCallum (2008) and M-
cCallum et al. (2007) proposed to employ gener-
alized expectation criteria (GE) to specify prefer-
ences about model expectations in the form of lin-
ear constraints on some feature expectations.
</bodyText>
<sectionHeader confidence="0.997496" genericHeader="method">
3 Methodology
</sectionHeader>
<bodyText confidence="0.99994812">
This work aims at building a CWS model adapted
to the SMT task. The model induction is shown in
Algorithm 1. The input data requires two type-
s of training resources, segmented Chinese sen-
tences from treebank Dcl and parallel unsegment-
ed sentences of Chinese and foreign language Dcu
and Duf. The first step is to conduct character-
based alignment over bitexts Dcu and Dfu, where
every Chinese character is an alignment target.
Here, we are interested on n-to-1 alignment pat-
terns, i.e., one target word is aligned to one or
more source Chinese characters. The second step
aims to collect word boundary distributions for al-
l types, i.e., character-level trigrams, according to
the n-to-1 mappings (Section 3.1). The third step
is to encode the induced word boundary informa-
tion into a k-nearest-neighbors (k-NN) similarity
graph constructed over the entire set of types from
Dcl and Dcu (Section 3.2). The final step trains a
discriminative sequential labeling model, condi-
tional random fields, on Dcl and Dcu under bilin-
gual constraints in a graph propagation expression
(Section 3.3). This constrained learning is carried
out based on posterior regularization (PR) frame-
work (Ganchev et al., 2010).
</bodyText>
<subsectionHeader confidence="0.998686">
3.1 Word Boundaries Learned from
Character-based Alignments
</subsectionHeader>
<bodyText confidence="0.955443222222222">
The gainful supervisions toward a better segmen-
tation solution for SMT are naturally extracted
from MT training resources, i.e., bilingual parallel
data. This study employs an approximated method
introduced in (Xu et al., 2004; Ma and Way, 2009;
Chung and Gildea, 2009) to learn bilingual seg-
Algorithm 1 CWS model induction with bilingual
constraints
Require:
</bodyText>
<listItem confidence="0.653683777777778">
Segmented Chinese sentences from treebank
Dcl ; Parallel sentences of Chinese and foreign
language Dcu and Df u
Ensure:
0: the CRFs model parameters
1: Dc↔f char align bitext (Duc, Dfu)
2: r learn word bound (Dc↔f)
3: G encode graph constraint (Dcl , Dcu, r)
4: 0 pr crf graph (Dcl , Dcu, G)
</listItem>
<bodyText confidence="0.998218717948718">
mentation knowledge. This relies on statistical
character-based alignment: first, every Chinese
character in the bitexts is divided by a white s-
pace so that individual characters are regarded as
special “words” or alignment targets, and second,
they are connected with English words by using
a statistical word aligner, e.g., GIZA++ (Och and
Ney, 2003). Note that the aligner is restricted to
use an n-to-1 alignment pattern. The primary idea
is that consecutive Chinese characters are grouped
to a candidate word, if they are aligned to the same
foreign word. It is worth mentioning that prior
works presented a straightforward usage for can-
didate words, treating them as golden segmenta-
tions, either dictionary units or labeled resources.
But this study treats the induced candidate word-
s in a different way. We propose to extract the
word boundary distributions1 for character-level
trigrams (type)2, as shown in Figure 1, instead of
the very specific words. There are two main rea-
sons to do so. First, it is a more general expression
which can reduce the impact amplification of er-
roneous character alignments. Second, boundary
distributions can play more flexible roles as con-
straints over labelings to bias the model learning.
The type-level word boundary extraction is for-
mally described as follows. Given the ith sen-
tence pair hxci, xfi , Ac→f
i ) of the aligned bilin-
gual corpus Dc↔f, the Chinese sentence xci con-
sisting of m characters fxci,1, xci,2, ..., xci,m}, and
the foreign language sentence xfi , consisting of
1The distribution is on four word boundary labels indi-
cating the character positions in a word, i.e., B (begin), M
(middle), E (end) and S (single character).
2A word boundary distribution corresponds to the center
character of a type. In fact, it aims at reducing label ambi-
guities to collect boundary information of character trigrams,
rather than individual characters (Altun et al., 2006).
</bodyText>
<equation confidence="0.429502">
1362
n words {xfi,1, xfi,2, ..., xfi,n}, Ac→f
i represents a
</equation>
<bodyText confidence="0.965782857142857">
set of alignment pairs aj = (Cj, xfi,j) that de-
fines connections between a few Chinese char-
acters Cj = {xci,j1, xci,j2, ..., xci,jk} and a sin-
gle foreign word xfi,j. For an alignment aj =
(Cj, xfi,j), only the sequence of characters Cj =
{xci,j1, xci,j2, ..., xci,jk} ∀d E [1, k −1], jd+1 − jd =
1 constitutes a valid candidate word. For the w-
hole bilingual corpus, we assign each character
in the candidate words with a word boundary tag
T E {B, M, E, S}, and then count across the en-
tire corpus to collect the tag distributions ri =
{ri,t; t E T} for each type xci,j−1xci,jxci,j+1.
Figure 1: An example of similarity graph over
character-level trigrams (types).
</bodyText>
<subsectionHeader confidence="0.9982365">
3.2 Constraints Encoded by Graph
Propagation Expression
</subsectionHeader>
<bodyText confidence="0.999979454545455">
The previous step contributes to generate bilingual
segmentation supervisions, i.e., type-level word
boundary distributions. An intuitive manner is to
directly leverage the induced boundary distribu-
tions as label constraints to regularize segmenta-
tion model learning, based on a constrained learn-
ing algorithm. This study, however, makes further
efforts to elevate the positive effects of the bilin-
gual knowledge via the graph propagation tech-
nique. We adopt a similarity graph to encode
the learned type-level word boundary distribution-
s. The GP expression will be defined as a PR con-
straint in Section 3.3 that reflects the interactions
between the graph and the CRFs model. In other
words, GP is integrated with estimation of para-
metric structural model. This is greatly different
from the prior pipelined approaches (Subramanya
et al., 2010; Das and Petrov, 2011; Zeng et al.,
2013), where GP is run first and its propagated
outcomes are then used to bias the structural mod-
el. This work seeks to capture the GP benefits dur-
ing the modeling of sequential correlations.
In what follows, the graph setting and propa-
gation expression are introduced. As in conven-
tional GP examples (Das and Smith, 2012), a sim-
ilarity graph G = (V, E) is constructed over N
types extracted from Chinese training data, includ-
ing treebank Dcl and bitexts Dcu. Each vertex Vi
has a |T|-dimensional estimated measure vi =
{vi,t; t E T} representing a probability distribu-
tion on word boundary tags. The induced type-
level word boundary distributions ri = {ri,t; t E
T} are empirical measures for the corresponding
M graph vertices. The edges E E Vi x Vj connect
all the vertices. Scores between pairs of graph ver-
tices (types), wij, refer to the similarities of their
syntactic environment, which are computed fol-
lowing the method in (Subramanya et al., 2010;
Das and Petrov, 2011; Zeng et al., 2013). The
similarities are measured based on co-occurrence
statistics over a set of predefined features (intro-
duced in Section 4.1). Specifically, the point-wise
mutual information (PMI) values, between ver-
tices and each feature instantiation that they have
in common, are summed to sparse vectors, and
their cosine distances are computed as the sim-
ilarities. The nature of this similarity graph en-
forces that the connected types with high weight-
s appearing in different texts should have similar
word boundary distributions.
The quality (smoothness) of the similarity graph
can be estimated by using a standard propagation
function, as shown in Equation 1. The square-loss
criterion (Zhu et al., 2003; Bengio et al., 2006) is
used to formulate this function:
</bodyText>
<equation confidence="0.990695857142857">
T M
P(v) = (vi,t − ri,t)2
t=1 i=1
+µ N N wij(vi,t − vj,t)2 + ρ N �
j=1 i=1 i=1 2
(vi,t)
(1)
</equation>
<bodyText confidence="0.999946222222222">
The first term in this equation refers to seed match-
es that compute the distances between the estimat-
ed measure vi and the empirical probabilities ri.
The second term refers to edge smoothness that
measures how vertices vi are smoothed with re-
spect to the graph. Two types connected by an
edge with high weight should be assigned similar
word boundary distributions. The third term, a `2
norm, evaluates the distribution sparsity (Das and
</bodyText>
<figure confidence="0.996982925925926">
0.9
Bei Ping Shi
北平市
Bei Jing Di
北京地
QuanYun Hui
全运会
0.2
Beijing Olympus
北 京 奥 运 会
B E B M E
Word boundaries
Beijing Olympus
Bei Jing Shi
北京市
0.3
Bei Jing Ren
北京人
0.8
0.6
bo
Ao Yun Hui
奥运会
0.2
北 京 奥 运 会
Character-based alignment
1363
</figure>
<bodyText confidence="0.999512666666667">
Smith, 2012) per vertex. Typically, the GP process
amounts to an optimization process with respect
to parameter v such that Equation 1 is minimized.
This propagation function can be used to reflect
the graph smoothness, where the higher the score,
the lower the smoothness.
</bodyText>
<subsectionHeader confidence="0.973474">
3.3 PR Learning with GP Constraint
</subsectionHeader>
<bodyText confidence="0.999879961538462">
Our learning problem belongs to semi-supervised
learning (SSL), as the training is done on treebank
labeled data (XL, YL) = {(x1,y1),...,(xl,yl)},
and bilingual unlabeled data (XU) = {x1, ..., xu}
where xi = {x1, ..., xm} is an input word se-
quence and yi = {y1, ..., ym}, y ∈ T is its corre-
sponding label sequence. Supervised linear-chain
CRFs can be modeled in a standard conditional
log-likelihood objective with a Gaussian prior:
Y for xi, and penalizes the CRFs marginal log-
likelihood by a KL-divergence term4, represent-
ing the distance between the estimated posteriors
p and the desired posteriors q, as well as a penal-
ty term, formed by the GP function. The hy-
perparameter λ is used to control the impacts of
the penalty term. Note that the penalty is fired
if the graph score computed based on the expect-
ed taggings given by the current CRFs model is
increased vis-a-vis the previous training iteration.
This nature requires that the penalty term P(v)
should be formed as a function of posteriors q over
CRFs model predictions5, i.e., P(q). To state this,
a mapping M : ({1, ..., u}, {1, ..., m}) → V from
words in the corpus to vertices in the graph is de-
fined. We can thus decompose vi,t into a function
of q as follows:
</bodyText>
<equation confidence="0.97701568">
u
X
a=1
1(yb = t, yb−1 = c)q(y|xa)
T
X
c=1
X
y∈Y
vi,t =
(2)
L(θ) = pθ(yi|xi) − kθk2
2σ
The conditional probabilities pθ are expressed as a
log-linear form:
Xm
b=1;
M(a,b)=Vi
Xu
a=1
Xm 1(M(a, b) = Vi)
b=1
θTf(yk−1
i , yki , xi))
Zθ(xi)
</equation>
<bodyText confidence="0.996221">
Where Zθ(xi) is a partition function that normal-
izes the exponential form to be a probability dis-
tribution, and f(yk−1
i , yki , xi) are arbitrary feature
functions.
In our setting, the CRFs model is required
to learn from unlabeled data. This work em-
ploys the posterior regularization (PR) frame-
work3 (Ganchev et al., 2010) to bias the CRFs
model’s learning on unlabeled data, under a con-
straint encoded by the graph propagation expres-
sion. It is expected that similar types in the graph
should have approximated expected taggings un-
der the CRFs model. We follow the approach in-
troduced by (He et al., 2013) to set up a penalty-
based PR objective with GP: the CRFs likelihood
is modified by adding a regularization term, as
shown in Equation 4, representing the constraints:
</bodyText>
<equation confidence="0.991823">
RU(θ, q) = KL(q||pθ) + λP(v) (4)
</equation>
<bodyText confidence="0.956113083333333">
Rather than regularize CRFs model’s posteriors
pθ(Y|xi) directly, our model uses an auxiliary
distribution q(Y|xi) over the possible labelings
3The readers are refered to the original paper of Ganchev
et al. (2010).
(5)
The final learning objective combines the CRF-
s likelihood with the PR regularization term:
J (θ, q) = L(θ) + RU(θ, q). This joint objec-
tive, over θ and q, can be optimized by an expecta-
tion maximization (EM) style algorithm as report-
ed in (Ganchev et al., 2010). We start from ini-
tial parameters θ0, estimated by supervised CRFs
model training on treebank data. The E-step is to
minimize RU(θ, q) over the posteriors q that are
constrained to the probability simplex. Since the
penalty term P(v) is a non-linear form, the opti-
mization method in (Ganchev et al., 2010) via pro-
jected gradient descent on the dual is inefficient6.
This study follows the optimization method (He et
al., 2013) that uses exponentiated gradient descent
(EGD) algorithm. It allows that the variable up-
date expression, as shown in Equation 6, takes a
multiplicative rather than an additive form.
</bodyText>
<equation confidence="0.946723">
q(w+1)(y|xi) = q(w)(y|xi) exp(−η
</equation>
<bodyText confidence="0.999917">
where the parameter η controls the optimization
rate in the E-step. With the contributions from
</bodyText>
<note confidence="0.671045">
4The form of KL term: KL(q||p) = Pq∈Y q(y) log q(y)
</note>
<bodyText confidence="0.536141">
p(y).
5The original PR setting also requires that the penalty ter-
m should be a linear (Ganchev et al., 2010) or non-linear (He
et al., 2013) function on q.
6According to (He et al., 2013), the dual of quadratic pro-
gram implies an expensive matrix inverse.
</bodyText>
<equation confidence="0.949222333333333">
exp(
pθ(yi|xi) =
�m
k=1
(3)
∂q(w)(y|xi))
(6)
∂R
1364
</equation>
<bodyText confidence="0.999917428571429">
the E-step that further encourage q and p to agree,
the M-step aims to optimize the objective J (θ, q)
with respect to θ. The M-step is similar to the stan-
dard CRFs parameter estimation, where the gradi-
ent ascent approach still works. This EM-style ap-
proach monotonically increases J (θ, q) and thus
is guaranteed to converge to a local optimum.
</bodyText>
<equation confidence="0.998669833333333">
E-step: q(t+1) = arg minRU(θ(t), q(t))
q
M-step: θ(t+1) = arg maxL(θ)
s
q(t+1)(y|xi) log ps(y|xi)
(7)
</equation>
<sectionHeader confidence="0.998156" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.990917">
4.1 Data and Setup
</subsectionHeader>
<bodyText confidence="0.999435575757576">
The experiments in this study evaluated the per-
formances of various CWS models in a Chinese-
to-English translation task. The influence of
the word segmentation on the final translation
is our main investigation. We adopted three
state-of-the-art metrics, BLEU (Papineni et al.,
2002), NIST (Doddington et al., 2000) and ME-
TEOR (Banerjee and Lavie, 2005), to evaluate the
translation quality.
The monolingual segmented data, trainTB, is
extracted from the Penn Chinese Treebank (CTB-
7) (Xue et al., 2005), containing 51,447 sentences.
The bilingual training data, trainMT, is formed
by a large in-house Chinese-English parallel cor-
pus (Tian et al., 2014). There are in total 2,244,319
Chinese-English sentence pairs crawled from on-
line resources, concentrated in 5 different domains
including laws, novels, spoken, news and miscel-
laneous7. This in-house bilingual corpus is the
MT training data as well. The target-side lan-
guage model is built on over 35 million mono-
lingual English sentences, trainLM, crawled from
online resources. The NIST evaluation campaign
data, MT-03 and MT-05, are selected to comprise
the MT development data, devMT, and testing da-
ta, testMT, respectively.
For the settings of our model, we adopted the
standard feature templates introduced by Zhao et
al. (2006) for CRFs. The character-based align-
ment for achieving the “chars-to-word” mappings
is accomplished by GIZA++ aligner (Och and
Ney, 2003). For the GP, a 10-NNs similarity graph
7The in-house corpus has been manually validated, in a
long process that exceeded 500 hours.
was constructed8. Following (Subramanya et al.,
2010; Zeng et al., 2013), the features used to
compute similarities between vertices were (Sup-
pose given a type “ w2w3w4” surrounding contexts
“w1w2w3w4w5”): unigram (w3), bigram (w1w2,
w4w5, w2w4), trigram (w2w3w4, w2w4w5,
w1w2w4), trigram+context (w1w2w3w4w5) and
character classes in number, punctuation, alpha-
betic letter and other (t(w2)t(w3)t(w4)). There
are four hyperparameters in our model to be tuned
by using the development data (devMT) among
the following settings: for the graph propagation,
µ E 10.2, 0.5, 0.8} and ρ E 10.1, 0.3,0.5,0.81;
for the PR learning, λ E 10 &lt; λi &lt; 11 and σ E
10 &lt; σi &lt; 11 where the step is 0.1. The best per-
formed joint settings, µ = 0.5, ρ = 0.5, λ = 0.9
and σ = 0.8, were used to measure the final per-
formance.
The MT experiment was conducted based on
a standard log-linear phrase-based SMT model.
The GIZA++ aligner was also adopted to obtain
word alignments (Och and Ney, 2003) over the
segmented bitexts. The heuristic strategy of grow-
diag-final-and (Koehn et al., 2007) was used to
combine the bidirectional alignments for extract-
ing phrase translations and reordering tables. A
5-gram language model with Kneser-Ney smooth-
ing was trained with SRILM (Stolcke, 2002) on
monolingual English data. Moses (Koehn et al.,
2007) was used as decoder. The Minimum Error
Rate Training (MERT) (Och, 2003) was used to
tune the feature parameters on development data.
</bodyText>
<subsectionHeader confidence="0.989108">
4.2 Various Segmentation Models
</subsectionHeader>
<bodyText confidence="0.828530166666667">
To provide a thorough analysis, the MT experi-
ments in this study evaluated three baseline seg-
mentation models and two off-the-shelf models,
in addition to four variant models that also employ
the bilingual constraints. We start from three base-
line models:
</bodyText>
<listItem confidence="0.992518125">
• Character Segmenter (CS): this model sim-
ply divides Chinese sentences into sequences
of characters.
• Supervised Monolingual Segmenter (SM-
S): this model is trained by CRFs on treebank
training data (trainTB). The same feature
templates (Zhao et al., 2006) are used. The
standard four-tags (B, M, E and S) were used
</listItem>
<bodyText confidence="0.903205">
8We evaluated graphs with top k (from 3 to 20) nearest
neighbors on development data, and found that the perfor-
mance converged beyond 10-NNs.
</bodyText>
<equation confidence="0.723057833333333">
+δ
�u
i=1
�
y∈Y
1365
</equation>
<bodyText confidence="0.922387">
as the labels. The stochastic gradient descent
is adopted to optimize the parameters.
• Unsupervised Bilingual Segmenter (UBS):
this model is trained on the bitexts (trainMT)
following the approach introduced in (Ma
and Way, 2009). The optimal set of the mod-
el parameter values was found on devMT to
be k = 3, tAC = 0.0 and tCOOC = 15.
The comparison candidates also involve two pop-
ular off-the-shelf segmentation models:
</bodyText>
<listItem confidence="0.961052382352941">
• Stanford Segmenter: this model, trained by
Chang et al. (2008), treats CWS as a binary
word boundary decision task. It covers sev-
eral features specific to the MT task, e.g., ex-
ternal lexicons and proper noun features.
• ICTCLAS Segmenter: this model, trained
by Zhang et al. (2003), is a hierarchical
HMM segmenter that incorporates parts-of-
speech (POS) information into the probabili-
ty models and generates multiple HMM mod-
els for solving segmentation ambiguities.
This work also evaluated four variant models9
that perform alternative ways to incorporate the
bilingual constraints based on two state-of-the-art
graph-based SSL approaches.
• Self-training Segmenters (STS): two vari-
ant models were defined by the approach re-
ported in (Subramanya et al., 2010) that us-
es the supervised CRFs model’s decodings,
incorporating empirical and constraint infor-
mation, for unlabeled examples as additional
labeled data to retrain a CRFs model. One
variant (STS-NO-GP) skips the GP step, di-
rectly decoding with type-level word bound-
ary probabilities induced from bitexts, while
the other (STS-GP-PL) runs the GP at first
and then decodes with GP outcomes. The
optimal hyperparameter values were found to
be: STS-NO-GP (α = 0.8) and q = 0.6) and
STS-GP-PL (µ = 0.5, ρ = 0.3, α = 0.8 and
q = 0.6).
• Virtual Evidences Segmenters (VES): T-
wo variant models based on the approach
in (Zeng et al., 2013) were defined. The type-
</listItem>
<bodyText confidence="0.986832363636364">
level word boundary distributions, induced
9Note that there are two variant models working with GP.
To be fair, the same similarity graph settings introduced in
this paper were used.
by the character-based alignment (VES-NO-
GP), and the graph propagation (VES-GP-
PL), are regarded as virtual evidences to bias
CRFs model’s learning on the unlabeled da-
ta. The optimal hyperparameter values were
found to be: VES-NO-GP (α = 0.7) and
VES-GP-PL (µ = 0.5, ρ = 0.3 and α = 0.7).
</bodyText>
<subsectionHeader confidence="0.999761">
4.3 Main Results
</subsectionHeader>
<bodyText confidence="0.999991186046512">
Table 1 summarizes the final MT performance on
the MT-05 test data, evaluated with ten different
CWS models. In what follows, we summarized
four major observations from the results. First-
ly, as expected, having word segmentation does
help Chinese-to-English MT. All other nine CWS
models outperforms the CS baseline which does
not try to identify Chinese words at all. Second-
ly, the other two baselines, SMS and UBS, are on
a par with each other, showing less than 0.36 av-
erage performance differences on the three eval-
uation metrics. This outcome validated that the
models, trained by either the treebank or the bilin-
gual data, performed reasonably well. But they
only capture partial segmentation features so that
less gains for SMT are achieved when compar-
ing to other sophisticated models. Thirdly, we no-
tice that the two off-the-shelf models, Stanford and
ICTCLAS, just brought minor improvements over
the SMS baseline, although they are trained us-
ing richer supervisions. This behaviour illustrates
that the conventional optimizations to the mono-
lingual supervised model, e.g., accumulating more
supervised data or predefined segmentation prop-
erties, are insufficient to help model for achiev-
ing better segmentations for SMT. Finally, high-
lighting the five models working with the bilingual
constraints, most of them can achieve significant
gains over the other ones without using the bilin-
gual constraints. This strongly demonstrates that
bilingually-learned segmentation knowledge does
helps CWS for SMT. The models working with G-
P, STS-GP-PL, VES-GP-PL and ours outperform
all others. We attribute this to the role of GP in
assisting the spread of bilingual knowledge on the
Chinese side. Importantly, it can be observed that
our model outperforms STS-GP, VES-GP, which
greatly supports that joint learning of CRFs and
GP can alleviate the error transfer by the pipelined
models. This is one of the most crucial findings
in this study. Overall, the boldface numbers in the
last row illustrate that our model obtains average
improvements of 1.89, 1.76 and 1.61 on BLEU,
</bodyText>
<table confidence="0.938335692307692">
1366
NIST and METEOR over others.
Models BLEU NIST METEOR
CS 29.38 59.85 54.07
SMS 30.05 61.33 55.95
UBS 30.15 61.56 55.39
Stanford 30.40 61.94 56.01
ICTCLAS 30.29 61.26 55.72
STS-NO-GP 31.47 62.35 56.12
STS-GP-PL 31.94 63.20 57.09
VES-NO-GP 31.98 62.63 56.59
VES-GP-PL 32.04 63.49 57.34
Our Model 32.75 63.72 57.64
</table>
<tableCaption confidence="0.9837985">
Table 1: Translation performances (%) on MT-05
testing data by using ten different CWS models.
</tableCaption>
<subsectionHeader confidence="0.998889">
4.4 Analysis &amp; Discussion
</subsectionHeader>
<bodyText confidence="0.999991869047619">
This section aims to further analyze the three pri-
mary observations concluded in Section 4.3: i)
word segmentation is useful to SMT; ii) the tree-
bank and the bilingual segmentation knowledge
are helpful, performing segmentation of differen-
t nature; and iii) the bilingual constraints lead to
learn segmentations better tailored for SMT.
The first observation derives from the compar-
isons between the CS baseline and other model-
s. Our results, showing the significant CWS ben-
efits to SMT, are consistent with the works re-
ported in the literature (Xu et al., 2004; Chang
et al., 2008). In our experiment, two additional
evidences found in the translation model are pro-
vided to further support that NO tokenization of
Chinese (i.e., the CS model’s output) could har-
m the MT system. First, the SMT phrase extrac-
tion, i.e., building “phrases” on top of the char-
acter sequences, cannot fully capture all meaning-
ful segmentations produced by the CS model. The
character based model leads to missing some use-
ful longer phrases, and to generate many meaning-
less or redundant translations in the phrase table.
Moreover, it is affected by translation ambiguities,
caused by the cases where a Chinese character has
very different meanings in different contextual en-
vironments.
The second observation shifts the emphasis to
SMS and UBS, based on the treebank and the
bilingual segmentation, respectively. Our result-
s show that both segmentation patterns can bring
positive effects to MT. Through analyzing both
models’ segmentations for trainMT and testMT,
we attempted to get a closer inspection on the seg-
mentation preferences and their influence on MT.
Our first finding is that the segmentation consen-
suses between SMS and UBS are positive to MT.
There have about 35% identical segmentations
produced by the two models. If these identical
segmentations are removed, and the experiments
are rerun, the translation scores decrease (on av-
erage) by 0.50, 0.85 and 0.70 on BLEU, NIST
and METEOR, respectively. Our second finding
is that SMS exhibits better segmentation consis-
tency than UBS. One representative example is the
segmentations for “X零零 (lonely)”. All the out-
puts of SMS were “X零零”, while UBS generat-
ed three ambiguous segmentations, “X(alone) 零
零(double zero)”, “X 零(lonely) 零(zero)” and
“X(alone) 零(zero) 零(zero)”. The segmentation
consistency of SMS rests on the high-quality tree-
bank data and the robust CRFs tagging mod-
el. On the other hand, the advantage of UB-
S is to capture the segmentations matching the
aligned target words. For example, UBS grouped
“国(country) 际(border) f 7(between)” to a word
“国 际 f 7(international)”, rather than two word-
s “国际(international) f 7(between)” (as given by
SMS), since these three characters are aligned to
a single English word “international”. The above
analysis shows that SMS and UBS have their own
merits and combining the knowledge derived from
both segmentations is highly encouraged.
The third observation concerns the great im-
pact of the bilingual constraints to the segmenta-
tion models in the MT task. The use of the bilin-
gual constraints is the prime objective of this s-
tudy. Our first contribution for this purpose is
on using the word boundary distributions to cap-
ture the bilingual segmentation supervisions. This
representation contributes to reduce the negative
impacts of erroneous “chars-to-word” alignments.
The ambiguous types (having relatively uniform
boundary distribution), caused by alignment er-
rors, cannot directly bias the model tagging pref-
erences. Furthermore, the word boundary distri-
butions are convenient to make up the learning
constraints over the labelings among various con-
strained learning approaches. They have success-
fully played in three types of constraints for our
experiments: PR penalty (Our model), decoding
constraints in self-training (STS) and virtual evi-
dences (VES). The second contribution is the use
of GP, illustrated by STS-GP-PL, VES-GP-PL and
</bodyText>
<page confidence="0.470082">
1367
</page>
<bodyText confidence="0.999991294117647">
Our model. The major effect is to multiply the im-
pacts of the bilingual knowledge through the sim-
ilarity graph. The graph vertices (types)10, with-
out any supervisions, can learn the word bound-
ary information from their similar types (neigh-
borhoods) having the empirical boundary prob-
abilities. The segmentations given by the three
GP models show about 70% positive segmenta-
tion changes, affected by the unlabeled graph ver-
tices, with respect to the ones given by the NO-
GP models, STS-NO-GP and VES-NO-GP. In our
opinion, the learning mechanism of our approach,
joint coupling of GP and CRFs, rather than the
pipelined one as the other two models, contributes
to maximizing the graph smoothness effects to the
CRFs estimation so that the error propagation of
the pipelined approaches is alleviated.
</bodyText>
<sectionHeader confidence="0.9977" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999979333333333">
This paper proposed a novel CWS model for the
SMT task. This model aims to maintain the lin-
guistic segmentation supervisions from treebank
data and simultaneously integrate useful bilingual
segmentations induced from the bitexts. This ob-
jective is accomplished by three main steps: 1)
learn word boundaries from character-based align-
ments; 2) encode the learned word boundaries into
a GP constraint; and 3) training a CRFs model, un-
der the GP constraint, by using the PR framework.
The empirical results indicate that the proposed
model can yield better segmentations for SMT.
</bodyText>
<sectionHeader confidence="0.995491" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999866066666667">
The authors are grateful to the Science and
Technology Development Fund of Macau and
the Research Committee of the University of
Macau (Grant No. MYRG076 (Y1-L2)-FST13-
WF and MYRG070 (Y1-L2)-FST12-CS) for the
funding support for our research. The work of
Isabel Trancoso was supported by national funds
through FCT-Fundac¸˜ao para a Ciˆecia e a Tecnolo-
gia, under project PEst-OE/EEI/LA0021/2013.
The authors also wish to thank the anonymous re-
viewers for many helpful comments.
10This experiment yielded a similarity graph that consists
of 11,909,620 types from trainTB and trainMT, where there
have 8,593,220 (72.15%) types without any empirical bound-
ary distributions.
</bodyText>
<sectionHeader confidence="0.989896" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999182603773586">
Yasemin Altun, David McAllester, and Mikhail Belkin.
2006. Maximum margin semi-supervised learning
for structured variables. Advances in Neural Infor-
mation Processing Systems, 18:33.
Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for mt evaluation with improved
correlation with human judgments. In Proceedings
of the ACL Workshop on Intrinsic and Extrinsic E-
valuation Measures for Machine Translation and/or
Summarization, pages 65–72. Association for Com-
putational Linguistics.
Yoshua Bengio, Olivier Delalleau, and Nicolas
Le Roux. 2006. Label propagation and quadrat-
ic criterion. Semi-Supervised Learning, pages 193–
216.
Pi-Chuan Chang, Michel Galley, and Christopher D
Manning. 2008. Optimizing Chinese word segmen-
tation for machine translation performance. In Pro-
ceedings of WMT, pages 224–232. Association for
Computational Linguistics.
Tagyoung Chung and Daniel Gildea. 2009. Unsuper-
vised tokenization for machine translation. In Pro-
ceedings of EMNLP, pages 718–726. Association
for Computational Linguistics.
Dipanjan Das and Slav Petrov. 2011. Unsupervised
part-of-speech tagging with bilingual graph-based
projections. In Proceedings ofACL, pages 600–609.
Association for Computational Linguistics.
Dipanjan Das and Noah A Smith. 2012. Graph-based
lexicon expansion with sparsity-inducing penalties.
In Proceedings of NAACL, pages 677–687. Associa-
tion for Computational Linguistics.
George R. Doddington, Mark A. Przybocki, Alvin F.
Martin, and Douglas A. Reynolds. 2000. The nist
speaker recognition evaluation–overview, methodol-
ogy, systems, results, perspective. Speech Commu-
nication, 31(2):225–254.
Kuzman Ganchev, J˜oao Grac¸a, Jennifer Gillenwater,
and Ben Taskar. 2010. Posterior regularization for
structured latent variable models. The Journal of
Machine Learning Research, 11:2001–2049.
Luheng He, Jennifer Gillenwater, and Ben Taskar.
2013. Graph-based posterior regularization for
semi-supervised structured prediction. In Proceed-
ings of CoNLL, page 38. Association for Computa-
tional Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertol-
di, Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of ACL on Interactive Poster and Demon-
stration Sessions, pages 177–180. Association for
Computational Linguistics.
1368
Yanjun Ma and Andy Way. 2009. Bilingually motivat-
ed domain-adapted word segmentation for statistical
machine translation. In Proceedings of EACL, pages
549–557. Association for Computational Linguistic-
s.
Gideon S. Mann and Andrew McCallum. 2008.
Generalized expectation criteria for semi-supervised
learning of conditional random fields. In Proceed-
ings of ACL, pages 870–878. Association for Com-
putational Linguistics.
Andrew McCallum, Gideon Mann, and Gregory
Druck. 2007. Generalized expectation criteri-
a. Computer Science Technical Note, University of
Massachusetts, Amherst, MA.
Franz Josef Och and Hermann Ney. 2003. A systemat-
ic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19–51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of A-
CL, pages 160–167. Association for Computational
Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic e-
valuation of machine translation. In Proceedings of
ACL, pages 311–318. Association for Computation-
al Linguistics.
Michael Paul, Finch Andrew, and Sumita Eiichiro.
2011. Integration of multiple bilingually-trained
segmentation schemes into statistical machine trans-
lation. IEICE Transactions on Information and Sys-
tems, 94(3):690–697.
Andreas Stolcke. 2002. SRILM-an extensible lan-
guage modeling toolkit. In Proceedings of Inter-
speech.
Amarnag Subramanya, Slav Petrov, and Fernando
Pereira. 2010. Efficient graph-based semi-
supervised learning of structured tagging models. In
Proceedings of EMNLP, pages 167–176. Associa-
tion for Computational Linguistics.
Liang Tian, Derek F. Wong, Lidia S. Chao, Paulo
Quaresma, Francisco Oliveira, Shuo Li, Yiming
Wang, and Yi Lu. 2014. UM-Corpus: A large
English-Chinese parallel corpus for statistical ma-
chine translation. In Proceedings of LREC. Euro-
pean Language Resources Association.
Ning Xi, Guangchao Tang, Xinyu Dai, Shujian Huang,
and Jiajun Chen. 2012. Enhancing statistical ma-
chine translation with character alignment. In Pro-
ceedings of ACL, pages 285–290. Association for
Computational Linguistics.
Jia Xu, Richard Zens, and Hermann Ney. 2004. Do
we need Chinese word segmentation for statistical
machine translation? In Proceedings of the Third
SIGHAN Workshop on Chinese Language Learning,
pages 122–128. Association for Computational Lin-
guistics.
Jia Xu, Evgeny Matusov, Richard Zens, and Hermann
Ney. 2005. Integrated Chinese word segmentation
in statistical machine translation. In Proceedings of
IWSLT, pages 216–223. Association for Computa-
tional Linguistics.
Naiwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese TreeBank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2):207–238.
Xiaodong Zeng, Derek F. Wong, Lidia S. Chao, and Is-
abel Trancoso. 2013. Graph-based semi-supervised
model for joint Chinese word segmentation and part-
of-speech tagging. In Proceedings of ACL, pages
770–779. Association for Computational Linguistic-
s.
Xiaodong Zeng, Derek F. Wong, Lidia S. Chao, Is-
abel Trancoso, Liangye He, and Qiuping Huang.
2014. Lexicon expansion for latent variable gram-
mars. Pattern Recognition Letters, 42:47–55.
Hua-Ping Zhang, Hong-Kui Yu, De-Yi Xiong, and Qun
Liu. 2003. HHMM-based Chinese lexical analyzer
ICTCLAS. In Proceedings of the Second SIGHAN
Workshop on Chinese Language Processing, pages
184–187. Association for Computational Linguistic-
s.
Ruiqiang Zhang, Keiji Yasuda, and Eiichiro Sumita.
2008. Improved statistical machine translation by
multiple Chinese word segmentation. In Proceed-
ings of WMT, pages 216–223. Association for Com-
putational Linguistics.
Hai Zhao, Chang-Ning Huang, and Mu Li. 2006. An
improved Chinese word segmentation system with
conditional random field. In Proceedings of the Fifth
SIGHAN Workshop on Chinese Language Process-
ing. Association for Computational Linguistics.
Hai Zhao, Masao Utiyama, Eiichiro Sumita, and Bao-
Liang Lu. 2013. An empirical study on word seg-
mentation for Chinese machine translation. In Com-
putational Linguistics and Intelligent Text Process-
ing, pages 248–263. Springer.
Xiaojin Zhu, Zoubin Ghahramani, and John Laffer-
ty. 2003. Semi-supervised learning using gaussian
fields and harmonic functions. In Proceedings of
ICML, volume 3, pages 912–919.
Ling Zhu, Derek F. Wong, and Lidia S. Chao. 2014.
Unsupervised chunking based on graph propagation
from bilingual corpus. The Scientific World Journal,
2014(401943):10.
</reference>
<page confidence="0.856768">
1369
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.883255">
<title confidence="0.9931">Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints</title>
<author confidence="0.919239">S F Lab Department of Computer</author>
<author confidence="0.919239">Information Science</author>
<author confidence="0.919239">University of</author>
<affiliation confidence="0.977223">Instituto Superior T´enico, Lisboa,</affiliation>
<email confidence="0.998397">isabel.trancoso@inesc-id.pt,tianliang0123@gmail.com</email>
<abstract confidence="0.998975590909091">This study investigates on building a better Chinese word segmentation model for statistical machine translation. It aims at leveraging word boundary information, automatically learned by bilingual character-based alignments, to induce a preferable segmentation model. We propose dealing with the induced word boundaries as soft constraints to bias the continuous learning of a supervised CRFs model, trained by the treebank data (labeled), on the bilingual data (unlabeled). The induced word boundary information is encoded as a graph propagation constraint. The constrained model induction is accomplished by using posterior regularization algorithm. The experiments on a Chinese-to-English machine translation task reveal that the proposed model can bring positive segmentation effects to translation quality.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yasemin Altun</author>
<author>David McAllester</author>
<author>Mikhail Belkin</author>
</authors>
<title>Maximum margin semi-supervised learning for structured variables.</title>
<date>2006</date>
<booktitle>Advances in Neural Information Processing Systems,</booktitle>
<pages>18--33</pages>
<contexts>
<context position="13376" citStr="Altun et al., 2006" startWordPosition="2069" endWordPosition="2072">lows. Given the ith sentence pair hxci, xfi , Ac→f i ) of the aligned bilingual corpus Dc↔f, the Chinese sentence xci consisting of m characters fxci,1, xci,2, ..., xci,m}, and the foreign language sentence xfi , consisting of 1The distribution is on four word boundary labels indicating the character positions in a word, i.e., B (begin), M (middle), E (end) and S (single character). 2A word boundary distribution corresponds to the center character of a type. In fact, it aims at reducing label ambiguities to collect boundary information of character trigrams, rather than individual characters (Altun et al., 2006). 1362 n words {xfi,1, xfi,2, ..., xfi,n}, Ac→f i represents a set of alignment pairs aj = (Cj, xfi,j) that defines connections between a few Chinese characters Cj = {xci,j1, xci,j2, ..., xci,jk} and a single foreign word xfi,j. For an alignment aj = (Cj, xfi,j), only the sequence of characters Cj = {xci,j1, xci,j2, ..., xci,jk} ∀d E [1, k −1], jd+1 − jd = 1 constitutes a valid candidate word. For the whole bilingual corpus, we assign each character in the candidate words with a word boundary tag T E {B, M, E, S}, and then count across the entire corpus to collect the tag distributions ri = {r</context>
</contexts>
<marker>Altun, McAllester, Belkin, 2006</marker>
<rawString>Yasemin Altun, David McAllester, and Mikhail Belkin. 2006. Maximum margin semi-supervised learning for structured variables. Advances in Neural Information Processing Systems, 18:33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>Meteor: An automatic metric for mt evaluation with improved correlation with human judgments.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,</booktitle>
<pages>65--72</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="22554" citStr="Banerjee and Lavie, 2005" startWordPosition="3660" endWordPosition="3663">cent approach still works. This EM-style approach monotonically increases J (θ, q) and thus is guaranteed to converge to a local optimum. E-step: q(t+1) = arg minRU(θ(t), q(t)) q M-step: θ(t+1) = arg maxL(θ) s q(t+1)(y|xi) log ps(y|xi) (7) 4 Experiments 4.1 Data and Setup The experiments in this study evaluated the performances of various CWS models in a Chineseto-English translation task. The influence of the word segmentation on the final translation is our main investigation. We adopted three state-of-the-art metrics, BLEU (Papineni et al., 2002), NIST (Doddington et al., 2000) and METEOR (Banerjee and Lavie, 2005), to evaluate the translation quality. The monolingual segmented data, trainTB, is extracted from the Penn Chinese Treebank (CTB7) (Xue et al., 2005), containing 51,447 sentences. The bilingual training data, trainMT, is formed by a large in-house Chinese-English parallel corpus (Tian et al., 2014). There are in total 2,244,319 Chinese-English sentence pairs crawled from online resources, concentrated in 5 different domains including laws, novels, spoken, news and miscellaneous7. This in-house bilingual corpus is the MT training data as well. The target-side language model is built on over 35 </context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 65–72. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Olivier Delalleau</author>
<author>Nicolas Le Roux</author>
</authors>
<title>Label propagation and quadratic criterion. Semi-Supervised Learning,</title>
<date>2006</date>
<pages>193--216</pages>
<marker>Bengio, Delalleau, Le Roux, 2006</marker>
<rawString>Yoshua Bengio, Olivier Delalleau, and Nicolas Le Roux. 2006. Label propagation and quadratic criterion. Semi-Supervised Learning, pages 193– 216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pi-Chuan Chang</author>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>Optimizing Chinese word segmentation for machine translation performance.</title>
<date>2008</date>
<booktitle>In Proceedings of WMT,</booktitle>
<pages>224--232</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1612" citStr="Chang et al., 2008" startWordPosition="219" endWordPosition="222"> accomplished by using posterior regularization algorithm. The experiments on a Chinese-to-English machine translation task reveal that the proposed model can bring positive segmentation effects to translation quality. 1 Introduction Word segmentation is regarded as a critical procedure for high-level Chinese language processing tasks, since Chinese scripts are written in continuous characters without explicit word boundaries (e.g., space in English). The empirical works show that word segmentation can be beneficial to Chinese-to-English statistical machine translation (SMT) (Xu et al., 2005; Chang et al., 2008; Zhao et al., 2013). In fact most current SMT models assume that parallel bilingual sentences should be segmented into sequences of tokens that are meant to be “words” (Ma and Way, 2009). The practice in state-of-the-art MT systems is that Chinese sentences are tokenized by a monolingual supervised word segmentation model trained on the handannotated treebank data, e.g., Chinese treebank (CTB) (Xue et al., 2005). These models are conducive to MT to some extent, since they commonly have relatively good aggregate performance and segmentation consistency (Chang et al., 2008). But one outstanding</context>
<context position="6028" citStr="Chang et al. (2008)" startWordPosition="891" endWordPosition="894">s with the related works of this study. Section 3 presents the details of the proposed segmentation model. Section 4 reports the experimental results of the proposed model for a Chinese-to-English MT task. The conclusion is drawn in Section 5. 2 Related Work In the literature, many approaches have been proposed to learn CWS models for SMT. They can be put into two categories, monolingual-motivated and bilingual-motivated. The former primarily optimizes monolingual supervised models according to some predefined segmentation properties that are manually summarized from empirical MT evaluations. Chang et al. (2008) enhanced a CRFs segmentation model in MT tasks by tuning the word granularity and improving the segmentation consistence. Zhang et al. (2008) produced a better segmentation model for SMT by concatenating various corpora regardless of their different specifications. Distinct from their behaviors, this work uses automatically learned constraints instead of manually defined ones. Most importantly, the constraints have a better learning guidance since they originate from the bilingual texts. On the other hand, the bilingual-motivated CWS models typically rely on character-based alignments to gene</context>
<context position="9121" citStr="Chang et al. (2008)" startWordPosition="1380" endWordPosition="1383"> as a learning constraint to interact with the CRFs model estimation. One of our main objectives is to bias CRFs model’s learning on unlabeled data, under a non-linear GP constraint encoding the bilingual knowledge. This is accomplished by the posterior regularization (PR) framework (Ganchev et 1361 al., 2010). PR performs regularization on posteriors, so that the learned model itself remains simple and tractable, while during learning it is driven to obey the constraints through setting appropriate parameters. The closest prior study is constrained learning, or learning with prior knowledge. Chang et al. (2008) described constraint driven learning (CODL) that augments model learning on unlabeled data by adding a cost for violating expectations of constraint features designed by domain knowledge. Mann and McCallum (2008) and McCallum et al. (2007) proposed to employ generalized expectation criteria (GE) to specify preferences about model expectations in the form of linear constraints on some feature expectations. 3 Methodology This work aims at building a CWS model adapted to the SMT task. The model induction is shown in Algorithm 1. The input data requires two types of training resources, segmented </context>
<context position="26461" citStr="Chang et al. (2008)" startWordPosition="4294" endWordPosition="4297">k (from 3 to 20) nearest neighbors on development data, and found that the performance converged beyond 10-NNs. +δ �u i=1 � y∈Y 1365 as the labels. The stochastic gradient descent is adopted to optimize the parameters. • Unsupervised Bilingual Segmenter (UBS): this model is trained on the bitexts (trainMT) following the approach introduced in (Ma and Way, 2009). The optimal set of the model parameter values was found on devMT to be k = 3, tAC = 0.0 and tCOOC = 15. The comparison candidates also involve two popular off-the-shelf segmentation models: • Stanford Segmenter: this model, trained by Chang et al. (2008), treats CWS as a binary word boundary decision task. It covers several features specific to the MT task, e.g., external lexicons and proper noun features. • ICTCLAS Segmenter: this model, trained by Zhang et al. (2003), is a hierarchical HMM segmenter that incorporates parts-ofspeech (POS) information into the probability models and generates multiple HMM models for solving segmentation ambiguities. This work also evaluated four variant models9 that perform alternative ways to incorporate the bilingual constraints based on two state-of-the-art graph-based SSL approaches. • Self-training Segme</context>
<context position="31396" citStr="Chang et al., 2008" startWordPosition="5091" endWordPosition="5094">WS models. 4.4 Analysis &amp; Discussion This section aims to further analyze the three primary observations concluded in Section 4.3: i) word segmentation is useful to SMT; ii) the treebank and the bilingual segmentation knowledge are helpful, performing segmentation of different nature; and iii) the bilingual constraints lead to learn segmentations better tailored for SMT. The first observation derives from the comparisons between the CS baseline and other models. Our results, showing the significant CWS benefits to SMT, are consistent with the works reported in the literature (Xu et al., 2004; Chang et al., 2008). In our experiment, two additional evidences found in the translation model are provided to further support that NO tokenization of Chinese (i.e., the CS model’s output) could harm the MT system. First, the SMT phrase extraction, i.e., building “phrases” on top of the character sequences, cannot fully capture all meaningful segmentations produced by the CS model. The character based model leads to missing some useful longer phrases, and to generate many meaningless or redundant translations in the phrase table. Moreover, it is affected by translation ambiguities, caused by the cases where a C</context>
</contexts>
<marker>Chang, Galley, Manning, 2008</marker>
<rawString>Pi-Chuan Chang, Michel Galley, and Christopher D Manning. 2008. Optimizing Chinese word segmentation for machine translation performance. In Proceedings of WMT, pages 224–232. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tagyoung Chung</author>
<author>Daniel Gildea</author>
</authors>
<title>Unsupervised tokenization for machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>718--726</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="11073" citStr="Chung and Gildea, 2009" startWordPosition="1689" endWordPosition="1692">inal step trains a discriminative sequential labeling model, conditional random fields, on Dcl and Dcu under bilingual constraints in a graph propagation expression (Section 3.3). This constrained learning is carried out based on posterior regularization (PR) framework (Ganchev et al., 2010). 3.1 Word Boundaries Learned from Character-based Alignments The gainful supervisions toward a better segmentation solution for SMT are naturally extracted from MT training resources, i.e., bilingual parallel data. This study employs an approximated method introduced in (Xu et al., 2004; Ma and Way, 2009; Chung and Gildea, 2009) to learn bilingual segAlgorithm 1 CWS model induction with bilingual constraints Require: Segmented Chinese sentences from treebank Dcl ; Parallel sentences of Chinese and foreign language Dcu and Df u Ensure: 0: the CRFs model parameters 1: Dc↔f char align bitext (Duc, Dfu) 2: r learn word bound (Dc↔f) 3: G encode graph constraint (Dcl , Dcu, r) 4: 0 pr crf graph (Dcl , Dcu, G) mentation knowledge. This relies on statistical character-based alignment: first, every Chinese character in the bitexts is divided by a white space so that individual characters are regarded as special “words” or ali</context>
</contexts>
<marker>Chung, Gildea, 2009</marker>
<rawString>Tagyoung Chung and Daniel Gildea. 2009. Unsupervised tokenization for machine translation. In Proceedings of EMNLP, pages 718–726. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Slav Petrov</author>
</authors>
<title>Unsupervised part-of-speech tagging with bilingual graph-based projections.</title>
<date>2011</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>600--609</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8031" citStr="Das and Petrov (2011)" startWordPosition="1204" endWordPosition="1207">nments to words as supervisions, this study extracts word boundary information of characters from the alignments as soft constraints to regularize a CRFs model’s learning. The graph propagation (GP) technique provides a natural way to represent data in a variety of target domains (Belkin et al., 2006). In this technique, the constructed graph has vertices consisting of labeled and unlabeled examples. Pairs of vertices are connected by weighted edges encoding the degree to which they are expected to have the same label (Zhu et al., 2003). Many recent works, such as by Subramanya et al. (2010), Das and Petrov (2011), Zeng et al. (2013; 2014) and Zhu et al. (2014), proposed GP for inferring the label information of unlabeled data, and then leverage these GP outcomes to learn a semi-supervised scalable model (e.g., CRFs). These approaches are referred to as pipelined learning with GP. This study also works with a similarity graph, encoding the learned bilingual knowledge. But, unlike the prior pipelined approaches, this study performs a joint learning behavior in which GP is used as a learning constraint to interact with the CRFs model estimation. One of our main objectives is to bias CRFs model’s learning</context>
<context position="15025" citStr="Das and Petrov, 2011" startWordPosition="2343" endWordPosition="2346">ation model learning, based on a constrained learning algorithm. This study, however, makes further efforts to elevate the positive effects of the bilingual knowledge via the graph propagation technique. We adopt a similarity graph to encode the learned type-level word boundary distributions. The GP expression will be defined as a PR constraint in Section 3.3 that reflects the interactions between the graph and the CRFs model. In other words, GP is integrated with estimation of parametric structural model. This is greatly different from the prior pipelined approaches (Subramanya et al., 2010; Das and Petrov, 2011; Zeng et al., 2013), where GP is run first and its propagated outcomes are then used to bias the structural model. This work seeks to capture the GP benefits during the modeling of sequential correlations. In what follows, the graph setting and propagation expression are introduced. As in conventional GP examples (Das and Smith, 2012), a similarity graph G = (V, E) is constructed over N types extracted from Chinese training data, including treebank Dcl and bitexts Dcu. Each vertex Vi has a |T|-dimensional estimated measure vi = {vi,t; t E T} representing a probability distribution on word bou</context>
</contexts>
<marker>Das, Petrov, 2011</marker>
<rawString>Dipanjan Das and Slav Petrov. 2011. Unsupervised part-of-speech tagging with bilingual graph-based projections. In Proceedings ofACL, pages 600–609. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Noah A Smith</author>
</authors>
<title>Graph-based lexicon expansion with sparsity-inducing penalties.</title>
<date>2012</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>677--687</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="15362" citStr="Das and Smith, 2012" startWordPosition="2402" endWordPosition="2405">onstraint in Section 3.3 that reflects the interactions between the graph and the CRFs model. In other words, GP is integrated with estimation of parametric structural model. This is greatly different from the prior pipelined approaches (Subramanya et al., 2010; Das and Petrov, 2011; Zeng et al., 2013), where GP is run first and its propagated outcomes are then used to bias the structural model. This work seeks to capture the GP benefits during the modeling of sequential correlations. In what follows, the graph setting and propagation expression are introduced. As in conventional GP examples (Das and Smith, 2012), a similarity graph G = (V, E) is constructed over N types extracted from Chinese training data, including treebank Dcl and bitexts Dcu. Each vertex Vi has a |T|-dimensional estimated measure vi = {vi,t; t E T} representing a probability distribution on word boundary tags. The induced typelevel word boundary distributions ri = {ri,t; t E T} are empirical measures for the corresponding M graph vertices. The edges E E Vi x Vj connect all the vertices. Scores between pairs of graph vertices (types), wij, refer to the similarities of their syntactic environment, which are computed following the m</context>
</contexts>
<marker>Das, Smith, 2012</marker>
<rawString>Dipanjan Das and Noah A Smith. 2012. Graph-based lexicon expansion with sparsity-inducing penalties. In Proceedings of NAACL, pages 677–687. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George R Doddington</author>
<author>Mark A Przybocki</author>
<author>Alvin F Martin</author>
<author>Douglas A Reynolds</author>
</authors>
<title>The nist speaker recognition evaluation–overview, methodology, systems, results, perspective.</title>
<date>2000</date>
<journal>Speech Communication,</journal>
<volume>31</volume>
<issue>2</issue>
<contexts>
<context position="22516" citStr="Doddington et al., 2000" startWordPosition="3653" endWordPosition="3656">ter estimation, where the gradient ascent approach still works. This EM-style approach monotonically increases J (θ, q) and thus is guaranteed to converge to a local optimum. E-step: q(t+1) = arg minRU(θ(t), q(t)) q M-step: θ(t+1) = arg maxL(θ) s q(t+1)(y|xi) log ps(y|xi) (7) 4 Experiments 4.1 Data and Setup The experiments in this study evaluated the performances of various CWS models in a Chineseto-English translation task. The influence of the word segmentation on the final translation is our main investigation. We adopted three state-of-the-art metrics, BLEU (Papineni et al., 2002), NIST (Doddington et al., 2000) and METEOR (Banerjee and Lavie, 2005), to evaluate the translation quality. The monolingual segmented data, trainTB, is extracted from the Penn Chinese Treebank (CTB7) (Xue et al., 2005), containing 51,447 sentences. The bilingual training data, trainMT, is formed by a large in-house Chinese-English parallel corpus (Tian et al., 2014). There are in total 2,244,319 Chinese-English sentence pairs crawled from online resources, concentrated in 5 different domains including laws, novels, spoken, news and miscellaneous7. This in-house bilingual corpus is the MT training data as well. The target-si</context>
</contexts>
<marker>Doddington, Przybocki, Martin, Reynolds, 2000</marker>
<rawString>George R. Doddington, Mark A. Przybocki, Alvin F. Martin, and Douglas A. Reynolds. 2000. The nist speaker recognition evaluation–overview, methodology, systems, results, perspective. Speech Communication, 31(2):225–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kuzman Ganchev</author>
<author>J˜oao Grac¸a</author>
<author>Jennifer Gillenwater</author>
<author>Ben Taskar</author>
</authors>
<title>Posterior regularization for structured latent variable models.</title>
<date>2010</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>11--2001</pages>
<marker>Ganchev, Grac¸a, Gillenwater, Taskar, 2010</marker>
<rawString>Kuzman Ganchev, J˜oao Grac¸a, Jennifer Gillenwater, and Ben Taskar. 2010. Posterior regularization for structured latent variable models. The Journal of Machine Learning Research, 11:2001–2049.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luheng He</author>
<author>Jennifer Gillenwater</author>
<author>Ben Taskar</author>
</authors>
<title>Graph-based posterior regularization for semi-supervised structured prediction.</title>
<date>2013</date>
<booktitle>In Proceedings of CoNLL,</booktitle>
<pages>page</pages>
<contexts>
<context position="19940" citStr="He et al., 2013" startWordPosition="3217" endWordPosition="3220"> Zθ(xi) Where Zθ(xi) is a partition function that normalizes the exponential form to be a probability distribution, and f(yk−1 i , yki , xi) are arbitrary feature functions. In our setting, the CRFs model is required to learn from unlabeled data. This work employs the posterior regularization (PR) framework3 (Ganchev et al., 2010) to bias the CRFs model’s learning on unlabeled data, under a constraint encoded by the graph propagation expression. It is expected that similar types in the graph should have approximated expected taggings under the CRFs model. We follow the approach introduced by (He et al., 2013) to set up a penaltybased PR objective with GP: the CRFs likelihood is modified by adding a regularization term, as shown in Equation 4, representing the constraints: RU(θ, q) = KL(q||pθ) + λP(v) (4) Rather than regularize CRFs model’s posteriors pθ(Y|xi) directly, our model uses an auxiliary distribution q(Y|xi) over the possible labelings 3The readers are refered to the original paper of Ganchev et al. (2010). (5) The final learning objective combines the CRFs likelihood with the PR regularization term: J (θ, q) = L(θ) + RU(θ, q). This joint objective, over θ and q, can be optimized by an ex</context>
<context position="21552" citStr="He et al., 2013" startWordPosition="3490" endWordPosition="3493">ected gradient descent on the dual is inefficient6. This study follows the optimization method (He et al., 2013) that uses exponentiated gradient descent (EGD) algorithm. It allows that the variable update expression, as shown in Equation 6, takes a multiplicative rather than an additive form. q(w+1)(y|xi) = q(w)(y|xi) exp(−η where the parameter η controls the optimization rate in the E-step. With the contributions from 4The form of KL term: KL(q||p) = Pq∈Y q(y) log q(y) p(y). 5The original PR setting also requires that the penalty term should be a linear (Ganchev et al., 2010) or non-linear (He et al., 2013) function on q. 6According to (He et al., 2013), the dual of quadratic program implies an expensive matrix inverse. exp( pθ(yi|xi) = �m k=1 (3) ∂q(w)(y|xi)) (6) ∂R 1364 the E-step that further encourage q and p to agree, the M-step aims to optimize the objective J (θ, q) with respect to θ. The M-step is similar to the standard CRFs parameter estimation, where the gradient ascent approach still works. This EM-style approach monotonically increases J (θ, q) and thus is guaranteed to converge to a local optimum. E-step: q(t+1) = arg minRU(θ(t), q(t)) q M-step: θ(t+1) = arg maxL(θ) s q(t+1)(y|xi) </context>
</contexts>
<marker>He, Gillenwater, Taskar, 2013</marker>
<rawString>Luheng He, Jennifer Gillenwater, and Ben Taskar. 2013. Graph-based posterior regularization for semi-supervised structured prediction. In Proceedings of CoNLL, page 38. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL on Interactive Poster and Demonstration Sessions,</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="24821" citStr="Koehn et al., 2007" startWordPosition="4025" endWordPosition="4028">l to be tuned by using the development data (devMT) among the following settings: for the graph propagation, µ E 10.2, 0.5, 0.8} and ρ E 10.1, 0.3,0.5,0.81; for the PR learning, λ E 10 &lt; λi &lt; 11 and σ E 10 &lt; σi &lt; 11 where the step is 0.1. The best performed joint settings, µ = 0.5, ρ = 0.5, λ = 0.9 and σ = 0.8, were used to measure the final performance. The MT experiment was conducted based on a standard log-linear phrase-based SMT model. The GIZA++ aligner was also adopted to obtain word alignments (Och and Ney, 2003) over the segmented bitexts. The heuristic strategy of growdiag-final-and (Koehn et al., 2007) was used to combine the bidirectional alignments for extracting phrase translations and reordering tables. A 5-gram language model with Kneser-Ney smoothing was trained with SRILM (Stolcke, 2002) on monolingual English data. Moses (Koehn et al., 2007) was used as decoder. The Minimum Error Rate Training (MERT) (Och, 2003) was used to tune the feature parameters on development data. 4.2 Various Segmentation Models To provide a thorough analysis, the MT experiments in this study evaluated three baseline segmentation models and two off-the-shelf models, in addition to four variant models that al</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, et al. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of ACL on Interactive Poster and Demonstration Sessions, pages 177–180. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yanjun Ma</author>
<author>Andy Way</author>
</authors>
<title>Bilingually motivated domain-adapted word segmentation for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>549--557</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1799" citStr="Ma and Way, 2009" startWordPosition="251" endWordPosition="254">ffects to translation quality. 1 Introduction Word segmentation is regarded as a critical procedure for high-level Chinese language processing tasks, since Chinese scripts are written in continuous characters without explicit word boundaries (e.g., space in English). The empirical works show that word segmentation can be beneficial to Chinese-to-English statistical machine translation (SMT) (Xu et al., 2005; Chang et al., 2008; Zhao et al., 2013). In fact most current SMT models assume that parallel bilingual sentences should be segmented into sequences of tokens that are meant to be “words” (Ma and Way, 2009). The practice in state-of-the-art MT systems is that Chinese sentences are tokenized by a monolingual supervised word segmentation model trained on the handannotated treebank data, e.g., Chinese treebank (CTB) (Xue et al., 2005). These models are conducive to MT to some extent, since they commonly have relatively good aggregate performance and segmentation consistency (Chang et al., 2008). But one outstanding problem is that these models may leave out some crucial segmentation features for SMT, since the output words conform to the treebank segmentation standard designed for monolingually lin</context>
<context position="6830" citStr="Ma and Way, 2009" startWordPosition="1013" endWordPosition="1016">T by concatenating various corpora regardless of their different specifications. Distinct from their behaviors, this work uses automatically learned constraints instead of manually defined ones. Most importantly, the constraints have a better learning guidance since they originate from the bilingual texts. On the other hand, the bilingual-motivated CWS models typically rely on character-based alignments to generate segmentation supervisions. Xu et al. (2004) proposed to employ “chars-to-word” alignments to generate a word dictionary for maximum matching segmentation in SMT task. The works in (Ma and Way, 2009; Zhao et al., 2013) extended the dictionary extraction strategy. Ma and Way (2009) adopted co-occurrence frequency metric to iteratively optimize “candidate words” extract from the alignments. Zhao et al. (2013) attempted to find an optimal subset of the dictionary learned by the character-based alignment to maximize the MT performance. Paul et al. (2011) used the words learned from “chars-to-word” alignments to train a maximum entropy segmentation model. Rather than playing the “hard” uses of the bilingual segmentation knowledge, i.e., directly merging “char-to-word” alignments to words as s</context>
<context position="11048" citStr="Ma and Way, 2009" startWordPosition="1685" endWordPosition="1688">ection 3.2). The final step trains a discriminative sequential labeling model, conditional random fields, on Dcl and Dcu under bilingual constraints in a graph propagation expression (Section 3.3). This constrained learning is carried out based on posterior regularization (PR) framework (Ganchev et al., 2010). 3.1 Word Boundaries Learned from Character-based Alignments The gainful supervisions toward a better segmentation solution for SMT are naturally extracted from MT training resources, i.e., bilingual parallel data. This study employs an approximated method introduced in (Xu et al., 2004; Ma and Way, 2009; Chung and Gildea, 2009) to learn bilingual segAlgorithm 1 CWS model induction with bilingual constraints Require: Segmented Chinese sentences from treebank Dcl ; Parallel sentences of Chinese and foreign language Dcu and Df u Ensure: 0: the CRFs model parameters 1: Dc↔f char align bitext (Duc, Dfu) 2: r learn word bound (Dc↔f) 3: G encode graph constraint (Dcl , Dcu, r) 4: 0 pr crf graph (Dcl , Dcu, G) mentation knowledge. This relies on statistical character-based alignment: first, every Chinese character in the bitexts is divided by a white space so that individual characters are regarded </context>
<context position="26205" citStr="Ma and Way, 2009" startWordPosition="4247" endWordPosition="4250">racters. • Supervised Monolingual Segmenter (SMS): this model is trained by CRFs on treebank training data (trainTB). The same feature templates (Zhao et al., 2006) are used. The standard four-tags (B, M, E and S) were used 8We evaluated graphs with top k (from 3 to 20) nearest neighbors on development data, and found that the performance converged beyond 10-NNs. +δ �u i=1 � y∈Y 1365 as the labels. The stochastic gradient descent is adopted to optimize the parameters. • Unsupervised Bilingual Segmenter (UBS): this model is trained on the bitexts (trainMT) following the approach introduced in (Ma and Way, 2009). The optimal set of the model parameter values was found on devMT to be k = 3, tAC = 0.0 and tCOOC = 15. The comparison candidates also involve two popular off-the-shelf segmentation models: • Stanford Segmenter: this model, trained by Chang et al. (2008), treats CWS as a binary word boundary decision task. It covers several features specific to the MT task, e.g., external lexicons and proper noun features. • ICTCLAS Segmenter: this model, trained by Zhang et al. (2003), is a hierarchical HMM segmenter that incorporates parts-ofspeech (POS) information into the probability models and generate</context>
</contexts>
<marker>Ma, Way, 2009</marker>
<rawString>Yanjun Ma and Andy Way. 2009. Bilingually motivated domain-adapted word segmentation for statistical machine translation. In Proceedings of EACL, pages 549–557. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gideon S Mann</author>
<author>Andrew McCallum</author>
</authors>
<title>Generalized expectation criteria for semi-supervised learning of conditional random fields.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>870--878</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9334" citStr="Mann and McCallum (2008)" startWordPosition="1412" endWordPosition="1415">wledge. This is accomplished by the posterior regularization (PR) framework (Ganchev et 1361 al., 2010). PR performs regularization on posteriors, so that the learned model itself remains simple and tractable, while during learning it is driven to obey the constraints through setting appropriate parameters. The closest prior study is constrained learning, or learning with prior knowledge. Chang et al. (2008) described constraint driven learning (CODL) that augments model learning on unlabeled data by adding a cost for violating expectations of constraint features designed by domain knowledge. Mann and McCallum (2008) and McCallum et al. (2007) proposed to employ generalized expectation criteria (GE) to specify preferences about model expectations in the form of linear constraints on some feature expectations. 3 Methodology This work aims at building a CWS model adapted to the SMT task. The model induction is shown in Algorithm 1. The input data requires two types of training resources, segmented Chinese sentences from treebank Dcl and parallel unsegmented sentences of Chinese and foreign language Dcu and Duf. The first step is to conduct characterbased alignment over bitexts Dcu and Dfu, where every Chine</context>
</contexts>
<marker>Mann, McCallum, 2008</marker>
<rawString>Gideon S. Mann and Andrew McCallum. 2008. Generalized expectation criteria for semi-supervised learning of conditional random fields. In Proceedings of ACL, pages 870–878. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Gideon Mann</author>
<author>Gregory Druck</author>
</authors>
<title>Generalized expectation criteria.</title>
<date>2007</date>
<institution>Computer Science Technical Note, University of Massachusetts,</institution>
<location>Amherst, MA.</location>
<contexts>
<context position="9361" citStr="McCallum et al. (2007)" startWordPosition="1417" endWordPosition="1421">by the posterior regularization (PR) framework (Ganchev et 1361 al., 2010). PR performs regularization on posteriors, so that the learned model itself remains simple and tractable, while during learning it is driven to obey the constraints through setting appropriate parameters. The closest prior study is constrained learning, or learning with prior knowledge. Chang et al. (2008) described constraint driven learning (CODL) that augments model learning on unlabeled data by adding a cost for violating expectations of constraint features designed by domain knowledge. Mann and McCallum (2008) and McCallum et al. (2007) proposed to employ generalized expectation criteria (GE) to specify preferences about model expectations in the form of linear constraints on some feature expectations. 3 Methodology This work aims at building a CWS model adapted to the SMT task. The model induction is shown in Algorithm 1. The input data requires two types of training resources, segmented Chinese sentences from treebank Dcl and parallel unsegmented sentences of Chinese and foreign language Dcu and Duf. The first step is to conduct characterbased alignment over bitexts Dcu and Dfu, where every Chinese character is an alignmen</context>
</contexts>
<marker>McCallum, Mann, Druck, 2007</marker>
<rawString>Andrew McCallum, Gideon Mann, and Gregory Druck. 2007. Generalized expectation criteria. Computer Science Technical Note, University of Massachusetts, Amherst, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="3429" citStr="Och and Ney, 2003" startWordPosition="507" endWordPosition="510">upervised model. Frequently, the bilingual knowledge refers to the mappings of an individual English word to one or more consecutive Chinese characters, generated via statistical character-based alignment. They leverage such mappings to either constitute a Chinese word dictionary for maximum-matching segmentation (Xu et al., 2004), or form labeled data for training a sequence labeling model (Paul et al., 2011). The prior works showed that these models help to find some segmentations tailored for SMT, since the bilingual word occurrence feature can be captured by the character-based alignment (Och and Ney, 2003). However, these models tend to miss out other linguistic segmentation patterns as monolingual supervised models, and suffer from the negative effects of erroneously alignments to word segmentation. This paper proposes an alternative Chinese Word Segmentation (CWS) model adapted to the SMT task, which seeks not only to maintain the advantages of a monolingual supervised model, having hand-annotated linguistic knowledge, but also to assimilate the relevant bilingual segmenta1360 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1360–1369, Baltimore, </context>
<context position="11808" citStr="Och and Ney, 2003" startWordPosition="1811" endWordPosition="1814">s from treebank Dcl ; Parallel sentences of Chinese and foreign language Dcu and Df u Ensure: 0: the CRFs model parameters 1: Dc↔f char align bitext (Duc, Dfu) 2: r learn word bound (Dc↔f) 3: G encode graph constraint (Dcl , Dcu, r) 4: 0 pr crf graph (Dcl , Dcu, G) mentation knowledge. This relies on statistical character-based alignment: first, every Chinese character in the bitexts is divided by a white space so that individual characters are regarded as special “words” or alignment targets, and second, they are connected with English words by using a statistical word aligner, e.g., GIZA++ (Och and Ney, 2003). Note that the aligner is restricted to use an n-to-1 alignment pattern. The primary idea is that consecutive Chinese characters are grouped to a candidate word, if they are aligned to the same foreign word. It is worth mentioning that prior works presented a straightforward usage for candidate words, treating them as golden segmentations, either dictionary units or labeled resources. But this study treats the induced candidate words in a different way. We propose to extract the word boundary distributions1 for character-level trigrams (type)2, as shown in Figure 1, instead of the very specif</context>
<context position="23624" citStr="Och and Ney, 2003" startWordPosition="3823" endWordPosition="3826">spoken, news and miscellaneous7. This in-house bilingual corpus is the MT training data as well. The target-side language model is built on over 35 million monolingual English sentences, trainLM, crawled from online resources. The NIST evaluation campaign data, MT-03 and MT-05, are selected to comprise the MT development data, devMT, and testing data, testMT, respectively. For the settings of our model, we adopted the standard feature templates introduced by Zhao et al. (2006) for CRFs. The character-based alignment for achieving the “chars-to-word” mappings is accomplished by GIZA++ aligner (Och and Ney, 2003). For the GP, a 10-NNs similarity graph 7The in-house corpus has been manually validated, in a long process that exceeded 500 hours. was constructed8. Following (Subramanya et al., 2010; Zeng et al., 2013), the features used to compute similarities between vertices were (Suppose given a type “ w2w3w4” surrounding contexts “w1w2w3w4w5”): unigram (w3), bigram (w1w2, w4w5, w2w4), trigram (w2w3w4, w2w4w5, w1w2w4), trigram+context (w1w2w3w4w5) and character classes in number, punctuation, alphabetic letter and other (t(w2)t(w3)t(w4)). There are four hyperparameters in our model to be tuned by using</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>160--167</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="25145" citStr="Och, 2003" startWordPosition="4077" endWordPosition="4078"> the final performance. The MT experiment was conducted based on a standard log-linear phrase-based SMT model. The GIZA++ aligner was also adopted to obtain word alignments (Och and Ney, 2003) over the segmented bitexts. The heuristic strategy of growdiag-final-and (Koehn et al., 2007) was used to combine the bidirectional alignments for extracting phrase translations and reordering tables. A 5-gram language model with Kneser-Ney smoothing was trained with SRILM (Stolcke, 2002) on monolingual English data. Moses (Koehn et al., 2007) was used as decoder. The Minimum Error Rate Training (MERT) (Och, 2003) was used to tune the feature parameters on development data. 4.2 Various Segmentation Models To provide a thorough analysis, the MT experiments in this study evaluated three baseline segmentation models and two off-the-shelf models, in addition to four variant models that also employ the bilingual constraints. We start from three baseline models: • Character Segmenter (CS): this model simply divides Chinese sentences into sequences of characters. • Supervised Monolingual Segmenter (SMS): this model is trained by CRFs on treebank training data (trainTB). The same feature templates (Zhao et al.</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of ACL, pages 160–167. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="22484" citStr="Papineni et al., 2002" startWordPosition="3648" endWordPosition="3651">ar to the standard CRFs parameter estimation, where the gradient ascent approach still works. This EM-style approach monotonically increases J (θ, q) and thus is guaranteed to converge to a local optimum. E-step: q(t+1) = arg minRU(θ(t), q(t)) q M-step: θ(t+1) = arg maxL(θ) s q(t+1)(y|xi) log ps(y|xi) (7) 4 Experiments 4.1 Data and Setup The experiments in this study evaluated the performances of various CWS models in a Chineseto-English translation task. The influence of the word segmentation on the final translation is our main investigation. We adopted three state-of-the-art metrics, BLEU (Papineni et al., 2002), NIST (Doddington et al., 2000) and METEOR (Banerjee and Lavie, 2005), to evaluate the translation quality. The monolingual segmented data, trainTB, is extracted from the Penn Chinese Treebank (CTB7) (Xue et al., 2005), containing 51,447 sentences. The bilingual training data, trainMT, is formed by a large in-house Chinese-English parallel corpus (Tian et al., 2014). There are in total 2,244,319 Chinese-English sentence pairs crawled from online resources, concentrated in 5 different domains including laws, novels, spoken, news and miscellaneous7. This in-house bilingual corpus is the MT trai</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of ACL, pages 311–318. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Paul</author>
<author>Finch Andrew</author>
<author>Sumita Eiichiro</author>
</authors>
<title>Integration of multiple bilingually-trained segmentation schemes into statistical machine translation.</title>
<date>2011</date>
<journal>IEICE Transactions on Information and Systems,</journal>
<volume>94</volume>
<issue>3</issue>
<contexts>
<context position="3224" citStr="Paul et al., 2011" startWordPosition="475" endWordPosition="478"> SMT based on bilingual unsegmented data, instead of monolingual segmented data. They proposed to learn gainful bilingual knowledge as golden-standard segmentation supervisions for training a bilingual unsupervised model. Frequently, the bilingual knowledge refers to the mappings of an individual English word to one or more consecutive Chinese characters, generated via statistical character-based alignment. They leverage such mappings to either constitute a Chinese word dictionary for maximum-matching segmentation (Xu et al., 2004), or form labeled data for training a sequence labeling model (Paul et al., 2011). The prior works showed that these models help to find some segmentations tailored for SMT, since the bilingual word occurrence feature can be captured by the character-based alignment (Och and Ney, 2003). However, these models tend to miss out other linguistic segmentation patterns as monolingual supervised models, and suffer from the negative effects of erroneously alignments to word segmentation. This paper proposes an alternative Chinese Word Segmentation (CWS) model adapted to the SMT task, which seeks not only to maintain the advantages of a monolingual supervised model, having hand-ann</context>
<context position="7188" citStr="Paul et al. (2011)" startWordPosition="1068" endWordPosition="1071">odels typically rely on character-based alignments to generate segmentation supervisions. Xu et al. (2004) proposed to employ “chars-to-word” alignments to generate a word dictionary for maximum matching segmentation in SMT task. The works in (Ma and Way, 2009; Zhao et al., 2013) extended the dictionary extraction strategy. Ma and Way (2009) adopted co-occurrence frequency metric to iteratively optimize “candidate words” extract from the alignments. Zhao et al. (2013) attempted to find an optimal subset of the dictionary learned by the character-based alignment to maximize the MT performance. Paul et al. (2011) used the words learned from “chars-to-word” alignments to train a maximum entropy segmentation model. Rather than playing the “hard” uses of the bilingual segmentation knowledge, i.e., directly merging “char-to-word” alignments to words as supervisions, this study extracts word boundary information of characters from the alignments as soft constraints to regularize a CRFs model’s learning. The graph propagation (GP) technique provides a natural way to represent data in a variety of target domains (Belkin et al., 2006). In this technique, the constructed graph has vertices consisting of labele</context>
</contexts>
<marker>Paul, Andrew, Eiichiro, 2011</marker>
<rawString>Michael Paul, Finch Andrew, and Sumita Eiichiro. 2011. Integration of multiple bilingually-trained segmentation schemes into statistical machine translation. IEICE Transactions on Information and Systems, 94(3):690–697.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM-an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of Interspeech.</booktitle>
<contexts>
<context position="25017" citStr="Stolcke, 2002" startWordPosition="4056" endWordPosition="4057">E 10 &lt; σi &lt; 11 where the step is 0.1. The best performed joint settings, µ = 0.5, ρ = 0.5, λ = 0.9 and σ = 0.8, were used to measure the final performance. The MT experiment was conducted based on a standard log-linear phrase-based SMT model. The GIZA++ aligner was also adopted to obtain word alignments (Och and Ney, 2003) over the segmented bitexts. The heuristic strategy of growdiag-final-and (Koehn et al., 2007) was used to combine the bidirectional alignments for extracting phrase translations and reordering tables. A 5-gram language model with Kneser-Ney smoothing was trained with SRILM (Stolcke, 2002) on monolingual English data. Moses (Koehn et al., 2007) was used as decoder. The Minimum Error Rate Training (MERT) (Och, 2003) was used to tune the feature parameters on development data. 4.2 Various Segmentation Models To provide a thorough analysis, the MT experiments in this study evaluated three baseline segmentation models and two off-the-shelf models, in addition to four variant models that also employ the bilingual constraints. We start from three baseline models: • Character Segmenter (CS): this model simply divides Chinese sentences into sequences of characters. • Supervised Monolin</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM-an extensible language modeling toolkit. In Proceedings of Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amarnag Subramanya</author>
<author>Slav Petrov</author>
<author>Fernando Pereira</author>
</authors>
<title>Efficient graph-based semisupervised learning of structured tagging models.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>167--176</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8008" citStr="Subramanya et al. (2010)" startWordPosition="1200" endWordPosition="1203">erging “char-to-word” alignments to words as supervisions, this study extracts word boundary information of characters from the alignments as soft constraints to regularize a CRFs model’s learning. The graph propagation (GP) technique provides a natural way to represent data in a variety of target domains (Belkin et al., 2006). In this technique, the constructed graph has vertices consisting of labeled and unlabeled examples. Pairs of vertices are connected by weighted edges encoding the degree to which they are expected to have the same label (Zhu et al., 2003). Many recent works, such as by Subramanya et al. (2010), Das and Petrov (2011), Zeng et al. (2013; 2014) and Zhu et al. (2014), proposed GP for inferring the label information of unlabeled data, and then leverage these GP outcomes to learn a semi-supervised scalable model (e.g., CRFs). These approaches are referred to as pipelined learning with GP. This study also works with a similarity graph, encoding the learned bilingual knowledge. But, unlike the prior pipelined approaches, this study performs a joint learning behavior in which GP is used as a learning constraint to interact with the CRFs model estimation. One of our main objectives is to bia</context>
<context position="15003" citStr="Subramanya et al., 2010" startWordPosition="2339" endWordPosition="2342">nts to regularize segmentation model learning, based on a constrained learning algorithm. This study, however, makes further efforts to elevate the positive effects of the bilingual knowledge via the graph propagation technique. We adopt a similarity graph to encode the learned type-level word boundary distributions. The GP expression will be defined as a PR constraint in Section 3.3 that reflects the interactions between the graph and the CRFs model. In other words, GP is integrated with estimation of parametric structural model. This is greatly different from the prior pipelined approaches (Subramanya et al., 2010; Das and Petrov, 2011; Zeng et al., 2013), where GP is run first and its propagated outcomes are then used to bias the structural model. This work seeks to capture the GP benefits during the modeling of sequential correlations. In what follows, the graph setting and propagation expression are introduced. As in conventional GP examples (Das and Smith, 2012), a similarity graph G = (V, E) is constructed over N types extracted from Chinese training data, including treebank Dcl and bitexts Dcu. Each vertex Vi has a |T|-dimensional estimated measure vi = {vi,t; t E T} representing a probability di</context>
<context position="23809" citStr="Subramanya et al., 2010" startWordPosition="3852" endWordPosition="3855">ences, trainLM, crawled from online resources. The NIST evaluation campaign data, MT-03 and MT-05, are selected to comprise the MT development data, devMT, and testing data, testMT, respectively. For the settings of our model, we adopted the standard feature templates introduced by Zhao et al. (2006) for CRFs. The character-based alignment for achieving the “chars-to-word” mappings is accomplished by GIZA++ aligner (Och and Ney, 2003). For the GP, a 10-NNs similarity graph 7The in-house corpus has been manually validated, in a long process that exceeded 500 hours. was constructed8. Following (Subramanya et al., 2010; Zeng et al., 2013), the features used to compute similarities between vertices were (Suppose given a type “ w2w3w4” surrounding contexts “w1w2w3w4w5”): unigram (w3), bigram (w1w2, w4w5, w2w4), trigram (w2w3w4, w2w4w5, w1w2w4), trigram+context (w1w2w3w4w5) and character classes in number, punctuation, alphabetic letter and other (t(w2)t(w3)t(w4)). There are four hyperparameters in our model to be tuned by using the development data (devMT) among the following settings: for the graph propagation, µ E 10.2, 0.5, 0.8} and ρ E 10.1, 0.3,0.5,0.81; for the PR learning, λ E 10 &lt; λi &lt; 11 and σ E 10 &lt;</context>
<context position="27159" citStr="Subramanya et al., 2010" startWordPosition="4401" endWordPosition="4404">atures specific to the MT task, e.g., external lexicons and proper noun features. • ICTCLAS Segmenter: this model, trained by Zhang et al. (2003), is a hierarchical HMM segmenter that incorporates parts-ofspeech (POS) information into the probability models and generates multiple HMM models for solving segmentation ambiguities. This work also evaluated four variant models9 that perform alternative ways to incorporate the bilingual constraints based on two state-of-the-art graph-based SSL approaches. • Self-training Segmenters (STS): two variant models were defined by the approach reported in (Subramanya et al., 2010) that uses the supervised CRFs model’s decodings, incorporating empirical and constraint information, for unlabeled examples as additional labeled data to retrain a CRFs model. One variant (STS-NO-GP) skips the GP step, directly decoding with type-level word boundary probabilities induced from bitexts, while the other (STS-GP-PL) runs the GP at first and then decodes with GP outcomes. The optimal hyperparameter values were found to be: STS-NO-GP (α = 0.8) and q = 0.6) and STS-GP-PL (µ = 0.5, ρ = 0.3, α = 0.8 and q = 0.6). • Virtual Evidences Segmenters (VES): Two variant models based on the ap</context>
</contexts>
<marker>Subramanya, Petrov, Pereira, 2010</marker>
<rawString>Amarnag Subramanya, Slav Petrov, and Fernando Pereira. 2010. Efficient graph-based semisupervised learning of structured tagging models. In Proceedings of EMNLP, pages 167–176. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Tian</author>
<author>Derek F Wong</author>
<author>Lidia S Chao</author>
<author>Paulo Quaresma</author>
<author>Francisco Oliveira</author>
<author>Shuo Li</author>
<author>Yiming Wang</author>
<author>Yi Lu</author>
</authors>
<title>UM-Corpus: A large English-Chinese parallel corpus for statistical machine translation.</title>
<date>2014</date>
<booktitle>In Proceedings of LREC. European Language Resources Association.</booktitle>
<contexts>
<context position="22853" citStr="Tian et al., 2014" startWordPosition="3705" endWordPosition="3708">luated the performances of various CWS models in a Chineseto-English translation task. The influence of the word segmentation on the final translation is our main investigation. We adopted three state-of-the-art metrics, BLEU (Papineni et al., 2002), NIST (Doddington et al., 2000) and METEOR (Banerjee and Lavie, 2005), to evaluate the translation quality. The monolingual segmented data, trainTB, is extracted from the Penn Chinese Treebank (CTB7) (Xue et al., 2005), containing 51,447 sentences. The bilingual training data, trainMT, is formed by a large in-house Chinese-English parallel corpus (Tian et al., 2014). There are in total 2,244,319 Chinese-English sentence pairs crawled from online resources, concentrated in 5 different domains including laws, novels, spoken, news and miscellaneous7. This in-house bilingual corpus is the MT training data as well. The target-side language model is built on over 35 million monolingual English sentences, trainLM, crawled from online resources. The NIST evaluation campaign data, MT-03 and MT-05, are selected to comprise the MT development data, devMT, and testing data, testMT, respectively. For the settings of our model, we adopted the standard feature template</context>
</contexts>
<marker>Tian, Wong, Chao, Quaresma, Oliveira, Li, Wang, Lu, 2014</marker>
<rawString>Liang Tian, Derek F. Wong, Lidia S. Chao, Paulo Quaresma, Francisco Oliveira, Shuo Li, Yiming Wang, and Yi Lu. 2014. UM-Corpus: A large English-Chinese parallel corpus for statistical machine translation. In Proceedings of LREC. European Language Resources Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ning Xi</author>
<author>Guangchao Tang</author>
<author>Xinyu Dai</author>
<author>Shujian Huang</author>
<author>Jiajun Chen</author>
</authors>
<title>Enhancing statistical machine translation with character alignment.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>285--290</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2563" citStr="Xi et al., 2012" startWordPosition="377" endWordPosition="380"> the handannotated treebank data, e.g., Chinese treebank (CTB) (Xue et al., 2005). These models are conducive to MT to some extent, since they commonly have relatively good aggregate performance and segmentation consistency (Chang et al., 2008). But one outstanding problem is that these models may leave out some crucial segmentation features for SMT, since the output words conform to the treebank segmentation standard designed for monolingually linguistic intuition, rather than specific to the SMT task. In recent years, a number of works (Xu et al., 2005; Chang et al., 2008; Ma and Way, 2009; Xi et al., 2012) attempted to build segmentation models for SMT based on bilingual unsegmented data, instead of monolingual segmented data. They proposed to learn gainful bilingual knowledge as golden-standard segmentation supervisions for training a bilingual unsupervised model. Frequently, the bilingual knowledge refers to the mappings of an individual English word to one or more consecutive Chinese characters, generated via statistical character-based alignment. They leverage such mappings to either constitute a Chinese word dictionary for maximum-matching segmentation (Xu et al., 2004), or form labeled da</context>
</contexts>
<marker>Xi, Tang, Dai, Huang, Chen, 2012</marker>
<rawString>Ning Xi, Guangchao Tang, Xinyu Dai, Shujian Huang, and Jiajun Chen. 2012. Enhancing statistical machine translation with character alignment. In Proceedings of ACL, pages 285–290. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jia Xu</author>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>Do we need Chinese word segmentation for statistical machine translation?</title>
<date>2004</date>
<booktitle>In Proceedings of the Third SIGHAN Workshop on Chinese Language Learning,</booktitle>
<pages>122--128</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3143" citStr="Xu et al., 2004" startWordPosition="461" endWordPosition="464">; Ma and Way, 2009; Xi et al., 2012) attempted to build segmentation models for SMT based on bilingual unsegmented data, instead of monolingual segmented data. They proposed to learn gainful bilingual knowledge as golden-standard segmentation supervisions for training a bilingual unsupervised model. Frequently, the bilingual knowledge refers to the mappings of an individual English word to one or more consecutive Chinese characters, generated via statistical character-based alignment. They leverage such mappings to either constitute a Chinese word dictionary for maximum-matching segmentation (Xu et al., 2004), or form labeled data for training a sequence labeling model (Paul et al., 2011). The prior works showed that these models help to find some segmentations tailored for SMT, since the bilingual word occurrence feature can be captured by the character-based alignment (Och and Ney, 2003). However, these models tend to miss out other linguistic segmentation patterns as monolingual supervised models, and suffer from the negative effects of erroneously alignments to word segmentation. This paper proposes an alternative Chinese Word Segmentation (CWS) model adapted to the SMT task, which seeks not o</context>
<context position="6676" citStr="Xu et al. (2004)" startWordPosition="988" endWordPosition="991">del in MT tasks by tuning the word granularity and improving the segmentation consistence. Zhang et al. (2008) produced a better segmentation model for SMT by concatenating various corpora regardless of their different specifications. Distinct from their behaviors, this work uses automatically learned constraints instead of manually defined ones. Most importantly, the constraints have a better learning guidance since they originate from the bilingual texts. On the other hand, the bilingual-motivated CWS models typically rely on character-based alignments to generate segmentation supervisions. Xu et al. (2004) proposed to employ “chars-to-word” alignments to generate a word dictionary for maximum matching segmentation in SMT task. The works in (Ma and Way, 2009; Zhao et al., 2013) extended the dictionary extraction strategy. Ma and Way (2009) adopted co-occurrence frequency metric to iteratively optimize “candidate words” extract from the alignments. Zhao et al. (2013) attempted to find an optimal subset of the dictionary learned by the character-based alignment to maximize the MT performance. Paul et al. (2011) used the words learned from “chars-to-word” alignments to train a maximum entropy segme</context>
<context position="11030" citStr="Xu et al., 2004" startWordPosition="1681" endWordPosition="1684">om Dcl and Dcu (Section 3.2). The final step trains a discriminative sequential labeling model, conditional random fields, on Dcl and Dcu under bilingual constraints in a graph propagation expression (Section 3.3). This constrained learning is carried out based on posterior regularization (PR) framework (Ganchev et al., 2010). 3.1 Word Boundaries Learned from Character-based Alignments The gainful supervisions toward a better segmentation solution for SMT are naturally extracted from MT training resources, i.e., bilingual parallel data. This study employs an approximated method introduced in (Xu et al., 2004; Ma and Way, 2009; Chung and Gildea, 2009) to learn bilingual segAlgorithm 1 CWS model induction with bilingual constraints Require: Segmented Chinese sentences from treebank Dcl ; Parallel sentences of Chinese and foreign language Dcu and Df u Ensure: 0: the CRFs model parameters 1: Dc↔f char align bitext (Duc, Dfu) 2: r learn word bound (Dc↔f) 3: G encode graph constraint (Dcl , Dcu, r) 4: 0 pr crf graph (Dcl , Dcu, G) mentation knowledge. This relies on statistical character-based alignment: first, every Chinese character in the bitexts is divided by a white space so that individual charac</context>
<context position="31375" citStr="Xu et al., 2004" startWordPosition="5087" endWordPosition="5090">g ten different CWS models. 4.4 Analysis &amp; Discussion This section aims to further analyze the three primary observations concluded in Section 4.3: i) word segmentation is useful to SMT; ii) the treebank and the bilingual segmentation knowledge are helpful, performing segmentation of different nature; and iii) the bilingual constraints lead to learn segmentations better tailored for SMT. The first observation derives from the comparisons between the CS baseline and other models. Our results, showing the significant CWS benefits to SMT, are consistent with the works reported in the literature (Xu et al., 2004; Chang et al., 2008). In our experiment, two additional evidences found in the translation model are provided to further support that NO tokenization of Chinese (i.e., the CS model’s output) could harm the MT system. First, the SMT phrase extraction, i.e., building “phrases” on top of the character sequences, cannot fully capture all meaningful segmentations produced by the CS model. The character based model leads to missing some useful longer phrases, and to generate many meaningless or redundant translations in the phrase table. Moreover, it is affected by translation ambiguities, caused b</context>
</contexts>
<marker>Xu, Zens, Ney, 2004</marker>
<rawString>Jia Xu, Richard Zens, and Hermann Ney. 2004. Do we need Chinese word segmentation for statistical machine translation? In Proceedings of the Third SIGHAN Workshop on Chinese Language Learning, pages 122–128. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jia Xu</author>
<author>Evgeny Matusov</author>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>Integrated Chinese word segmentation in statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of IWSLT,</booktitle>
<pages>216--223</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1592" citStr="Xu et al., 2005" startWordPosition="215" endWordPosition="218">odel induction is accomplished by using posterior regularization algorithm. The experiments on a Chinese-to-English machine translation task reveal that the proposed model can bring positive segmentation effects to translation quality. 1 Introduction Word segmentation is regarded as a critical procedure for high-level Chinese language processing tasks, since Chinese scripts are written in continuous characters without explicit word boundaries (e.g., space in English). The empirical works show that word segmentation can be beneficial to Chinese-to-English statistical machine translation (SMT) (Xu et al., 2005; Chang et al., 2008; Zhao et al., 2013). In fact most current SMT models assume that parallel bilingual sentences should be segmented into sequences of tokens that are meant to be “words” (Ma and Way, 2009). The practice in state-of-the-art MT systems is that Chinese sentences are tokenized by a monolingual supervised word segmentation model trained on the handannotated treebank data, e.g., Chinese treebank (CTB) (Xue et al., 2005). These models are conducive to MT to some extent, since they commonly have relatively good aggregate performance and segmentation consistency (Chang et al., 2008).</context>
</contexts>
<marker>Xu, Matusov, Zens, Ney, 2005</marker>
<rawString>Jia Xu, Evgeny Matusov, Richard Zens, and Hermann Ney. 2005. Integrated Chinese word segmentation in statistical machine translation. In Proceedings of IWSLT, pages 216–223. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naiwen Xue</author>
<author>Fei Xia</author>
<author>Fu-Dong Chiou</author>
<author>Martha Palmer</author>
</authors>
<title>The Penn Chinese TreeBank: Phrase structure annotation of a large corpus.</title>
<date>2005</date>
<journal>Natural Language Engineering,</journal>
<volume>11</volume>
<issue>2</issue>
<contexts>
<context position="2028" citStr="Xue et al., 2005" startWordPosition="286" endWordPosition="289">d boundaries (e.g., space in English). The empirical works show that word segmentation can be beneficial to Chinese-to-English statistical machine translation (SMT) (Xu et al., 2005; Chang et al., 2008; Zhao et al., 2013). In fact most current SMT models assume that parallel bilingual sentences should be segmented into sequences of tokens that are meant to be “words” (Ma and Way, 2009). The practice in state-of-the-art MT systems is that Chinese sentences are tokenized by a monolingual supervised word segmentation model trained on the handannotated treebank data, e.g., Chinese treebank (CTB) (Xue et al., 2005). These models are conducive to MT to some extent, since they commonly have relatively good aggregate performance and segmentation consistency (Chang et al., 2008). But one outstanding problem is that these models may leave out some crucial segmentation features for SMT, since the output words conform to the treebank segmentation standard designed for monolingually linguistic intuition, rather than specific to the SMT task. In recent years, a number of works (Xu et al., 2005; Chang et al., 2008; Ma and Way, 2009; Xi et al., 2012) attempted to build segmentation models for SMT based on bilingua</context>
<context position="22703" citStr="Xue et al., 2005" startWordPosition="3683" endWordPosition="3686">g minRU(θ(t), q(t)) q M-step: θ(t+1) = arg maxL(θ) s q(t+1)(y|xi) log ps(y|xi) (7) 4 Experiments 4.1 Data and Setup The experiments in this study evaluated the performances of various CWS models in a Chineseto-English translation task. The influence of the word segmentation on the final translation is our main investigation. We adopted three state-of-the-art metrics, BLEU (Papineni et al., 2002), NIST (Doddington et al., 2000) and METEOR (Banerjee and Lavie, 2005), to evaluate the translation quality. The monolingual segmented data, trainTB, is extracted from the Penn Chinese Treebank (CTB7) (Xue et al., 2005), containing 51,447 sentences. The bilingual training data, trainMT, is formed by a large in-house Chinese-English parallel corpus (Tian et al., 2014). There are in total 2,244,319 Chinese-English sentence pairs crawled from online resources, concentrated in 5 different domains including laws, novels, spoken, news and miscellaneous7. This in-house bilingual corpus is the MT training data as well. The target-side language model is built on over 35 million monolingual English sentences, trainLM, crawled from online resources. The NIST evaluation campaign data, MT-03 and MT-05, are selected to co</context>
</contexts>
<marker>Xue, Xia, Chiou, Palmer, 2005</marker>
<rawString>Naiwen Xue, Fei Xia, Fu-Dong Chiou, and Martha Palmer. 2005. The Penn Chinese TreeBank: Phrase structure annotation of a large corpus. Natural Language Engineering, 11(2):207–238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodong Zeng</author>
<author>Derek F Wong</author>
<author>Lidia S Chao</author>
<author>Isabel Trancoso</author>
</authors>
<title>Graph-based semi-supervised model for joint Chinese word segmentation and partof-speech tagging.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>770--779</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8050" citStr="Zeng et al. (2013" startWordPosition="1208" endWordPosition="1211">rvisions, this study extracts word boundary information of characters from the alignments as soft constraints to regularize a CRFs model’s learning. The graph propagation (GP) technique provides a natural way to represent data in a variety of target domains (Belkin et al., 2006). In this technique, the constructed graph has vertices consisting of labeled and unlabeled examples. Pairs of vertices are connected by weighted edges encoding the degree to which they are expected to have the same label (Zhu et al., 2003). Many recent works, such as by Subramanya et al. (2010), Das and Petrov (2011), Zeng et al. (2013; 2014) and Zhu et al. (2014), proposed GP for inferring the label information of unlabeled data, and then leverage these GP outcomes to learn a semi-supervised scalable model (e.g., CRFs). These approaches are referred to as pipelined learning with GP. This study also works with a similarity graph, encoding the learned bilingual knowledge. But, unlike the prior pipelined approaches, this study performs a joint learning behavior in which GP is used as a learning constraint to interact with the CRFs model estimation. One of our main objectives is to bias CRFs model’s learning on unlabeled data,</context>
<context position="15045" citStr="Zeng et al., 2013" startWordPosition="2347" endWordPosition="2350">based on a constrained learning algorithm. This study, however, makes further efforts to elevate the positive effects of the bilingual knowledge via the graph propagation technique. We adopt a similarity graph to encode the learned type-level word boundary distributions. The GP expression will be defined as a PR constraint in Section 3.3 that reflects the interactions between the graph and the CRFs model. In other words, GP is integrated with estimation of parametric structural model. This is greatly different from the prior pipelined approaches (Subramanya et al., 2010; Das and Petrov, 2011; Zeng et al., 2013), where GP is run first and its propagated outcomes are then used to bias the structural model. This work seeks to capture the GP benefits during the modeling of sequential correlations. In what follows, the graph setting and propagation expression are introduced. As in conventional GP examples (Das and Smith, 2012), a similarity graph G = (V, E) is constructed over N types extracted from Chinese training data, including treebank Dcl and bitexts Dcu. Each vertex Vi has a |T|-dimensional estimated measure vi = {vi,t; t E T} representing a probability distribution on word boundary tags. The indu</context>
<context position="23829" citStr="Zeng et al., 2013" startWordPosition="3856" endWordPosition="3859">rom online resources. The NIST evaluation campaign data, MT-03 and MT-05, are selected to comprise the MT development data, devMT, and testing data, testMT, respectively. For the settings of our model, we adopted the standard feature templates introduced by Zhao et al. (2006) for CRFs. The character-based alignment for achieving the “chars-to-word” mappings is accomplished by GIZA++ aligner (Och and Ney, 2003). For the GP, a 10-NNs similarity graph 7The in-house corpus has been manually validated, in a long process that exceeded 500 hours. was constructed8. Following (Subramanya et al., 2010; Zeng et al., 2013), the features used to compute similarities between vertices were (Suppose given a type “ w2w3w4” surrounding contexts “w1w2w3w4w5”): unigram (w3), bigram (w1w2, w4w5, w2w4), trigram (w2w3w4, w2w4w5, w1w2w4), trigram+context (w1w2w3w4w5) and character classes in number, punctuation, alphabetic letter and other (t(w2)t(w3)t(w4)). There are four hyperparameters in our model to be tuned by using the development data (devMT) among the following settings: for the graph propagation, µ E 10.2, 0.5, 0.8} and ρ E 10.1, 0.3,0.5,0.81; for the PR learning, λ E 10 &lt; λi &lt; 11 and σ E 10 &lt; σi &lt; 11 where the s</context>
<context position="27788" citStr="Zeng et al., 2013" startWordPosition="4510" endWordPosition="4513">the supervised CRFs model’s decodings, incorporating empirical and constraint information, for unlabeled examples as additional labeled data to retrain a CRFs model. One variant (STS-NO-GP) skips the GP step, directly decoding with type-level word boundary probabilities induced from bitexts, while the other (STS-GP-PL) runs the GP at first and then decodes with GP outcomes. The optimal hyperparameter values were found to be: STS-NO-GP (α = 0.8) and q = 0.6) and STS-GP-PL (µ = 0.5, ρ = 0.3, α = 0.8 and q = 0.6). • Virtual Evidences Segmenters (VES): Two variant models based on the approach in (Zeng et al., 2013) were defined. The typelevel word boundary distributions, induced 9Note that there are two variant models working with GP. To be fair, the same similarity graph settings introduced in this paper were used. by the character-based alignment (VES-NOGP), and the graph propagation (VES-GPPL), are regarded as virtual evidences to bias CRFs model’s learning on the unlabeled data. The optimal hyperparameter values were found to be: VES-NO-GP (α = 0.7) and VES-GP-PL (µ = 0.5, ρ = 0.3 and α = 0.7). 4.3 Main Results Table 1 summarizes the final MT performance on the MT-05 test data, evaluated with ten di</context>
</contexts>
<marker>Zeng, Wong, Chao, Trancoso, 2013</marker>
<rawString>Xiaodong Zeng, Derek F. Wong, Lidia S. Chao, and Isabel Trancoso. 2013. Graph-based semi-supervised model for joint Chinese word segmentation and partof-speech tagging. In Proceedings of ACL, pages 770–779. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodong Zeng</author>
<author>Derek F Wong</author>
<author>Lidia S Chao</author>
<author>Isabel Trancoso</author>
<author>Liangye He</author>
<author>Qiuping Huang</author>
</authors>
<title>Lexicon expansion for latent variable grammars. Pattern Recognition Letters,</title>
<date>2014</date>
<pages>42--47</pages>
<marker>Zeng, Wong, Chao, Trancoso, He, Huang, 2014</marker>
<rawString>Xiaodong Zeng, Derek F. Wong, Lidia S. Chao, Isabel Trancoso, Liangye He, and Qiuping Huang. 2014. Lexicon expansion for latent variable grammars. Pattern Recognition Letters, 42:47–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hua-Ping Zhang</author>
<author>Hong-Kui Yu</author>
<author>De-Yi Xiong</author>
<author>Qun Liu</author>
</authors>
<title>HHMM-based Chinese lexical analyzer ICTCLAS.</title>
<date>2003</date>
<booktitle>In Proceedings of the Second SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>184--187</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="26680" citStr="Zhang et al. (2003)" startWordPosition="4332" endWordPosition="4335">• Unsupervised Bilingual Segmenter (UBS): this model is trained on the bitexts (trainMT) following the approach introduced in (Ma and Way, 2009). The optimal set of the model parameter values was found on devMT to be k = 3, tAC = 0.0 and tCOOC = 15. The comparison candidates also involve two popular off-the-shelf segmentation models: • Stanford Segmenter: this model, trained by Chang et al. (2008), treats CWS as a binary word boundary decision task. It covers several features specific to the MT task, e.g., external lexicons and proper noun features. • ICTCLAS Segmenter: this model, trained by Zhang et al. (2003), is a hierarchical HMM segmenter that incorporates parts-ofspeech (POS) information into the probability models and generates multiple HMM models for solving segmentation ambiguities. This work also evaluated four variant models9 that perform alternative ways to incorporate the bilingual constraints based on two state-of-the-art graph-based SSL approaches. • Self-training Segmenters (STS): two variant models were defined by the approach reported in (Subramanya et al., 2010) that uses the supervised CRFs model’s decodings, incorporating empirical and constraint information, for unlabeled examp</context>
</contexts>
<marker>Zhang, Yu, Xiong, Liu, 2003</marker>
<rawString>Hua-Ping Zhang, Hong-Kui Yu, De-Yi Xiong, and Qun Liu. 2003. HHMM-based Chinese lexical analyzer ICTCLAS. In Proceedings of the Second SIGHAN Workshop on Chinese Language Processing, pages 184–187. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruiqiang Zhang</author>
<author>Keiji Yasuda</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Improved statistical machine translation by multiple Chinese word segmentation.</title>
<date>2008</date>
<booktitle>In Proceedings of WMT,</booktitle>
<pages>216--223</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6170" citStr="Zhang et al. (2008)" startWordPosition="914" endWordPosition="917">results of the proposed model for a Chinese-to-English MT task. The conclusion is drawn in Section 5. 2 Related Work In the literature, many approaches have been proposed to learn CWS models for SMT. They can be put into two categories, monolingual-motivated and bilingual-motivated. The former primarily optimizes monolingual supervised models according to some predefined segmentation properties that are manually summarized from empirical MT evaluations. Chang et al. (2008) enhanced a CRFs segmentation model in MT tasks by tuning the word granularity and improving the segmentation consistence. Zhang et al. (2008) produced a better segmentation model for SMT by concatenating various corpora regardless of their different specifications. Distinct from their behaviors, this work uses automatically learned constraints instead of manually defined ones. Most importantly, the constraints have a better learning guidance since they originate from the bilingual texts. On the other hand, the bilingual-motivated CWS models typically rely on character-based alignments to generate segmentation supervisions. Xu et al. (2004) proposed to employ “chars-to-word” alignments to generate a word dictionary for maximum match</context>
</contexts>
<marker>Zhang, Yasuda, Sumita, 2008</marker>
<rawString>Ruiqiang Zhang, Keiji Yasuda, and Eiichiro Sumita. 2008. Improved statistical machine translation by multiple Chinese word segmentation. In Proceedings of WMT, pages 216–223. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chang-Ning Huang</author>
<author>Mu Li</author>
</authors>
<title>An improved Chinese word segmentation system with conditional random field.</title>
<date>2006</date>
<booktitle>In Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="23487" citStr="Zhao et al. (2006)" startWordPosition="3803" endWordPosition="3806">otal 2,244,319 Chinese-English sentence pairs crawled from online resources, concentrated in 5 different domains including laws, novels, spoken, news and miscellaneous7. This in-house bilingual corpus is the MT training data as well. The target-side language model is built on over 35 million monolingual English sentences, trainLM, crawled from online resources. The NIST evaluation campaign data, MT-03 and MT-05, are selected to comprise the MT development data, devMT, and testing data, testMT, respectively. For the settings of our model, we adopted the standard feature templates introduced by Zhao et al. (2006) for CRFs. The character-based alignment for achieving the “chars-to-word” mappings is accomplished by GIZA++ aligner (Och and Ney, 2003). For the GP, a 10-NNs similarity graph 7The in-house corpus has been manually validated, in a long process that exceeded 500 hours. was constructed8. Following (Subramanya et al., 2010; Zeng et al., 2013), the features used to compute similarities between vertices were (Suppose given a type “ w2w3w4” surrounding contexts “w1w2w3w4w5”): unigram (w3), bigram (w1w2, w4w5, w2w4), trigram (w2w3w4, w2w4w5, w1w2w4), trigram+context (w1w2w3w4w5) and character classe</context>
<context position="25752" citStr="Zhao et al., 2006" startWordPosition="4170" endWordPosition="4173">(Och, 2003) was used to tune the feature parameters on development data. 4.2 Various Segmentation Models To provide a thorough analysis, the MT experiments in this study evaluated three baseline segmentation models and two off-the-shelf models, in addition to four variant models that also employ the bilingual constraints. We start from three baseline models: • Character Segmenter (CS): this model simply divides Chinese sentences into sequences of characters. • Supervised Monolingual Segmenter (SMS): this model is trained by CRFs on treebank training data (trainTB). The same feature templates (Zhao et al., 2006) are used. The standard four-tags (B, M, E and S) were used 8We evaluated graphs with top k (from 3 to 20) nearest neighbors on development data, and found that the performance converged beyond 10-NNs. +δ �u i=1 � y∈Y 1365 as the labels. The stochastic gradient descent is adopted to optimize the parameters. • Unsupervised Bilingual Segmenter (UBS): this model is trained on the bitexts (trainMT) following the approach introduced in (Ma and Way, 2009). The optimal set of the model parameter values was found on devMT to be k = 3, tAC = 0.0 and tCOOC = 15. The comparison candidates also involve tw</context>
</contexts>
<marker>Zhao, Huang, Li, 2006</marker>
<rawString>Hai Zhao, Chang-Ning Huang, and Mu Li. 2006. An improved Chinese word segmentation system with conditional random field. In Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Masao Utiyama</author>
<author>Eiichiro Sumita</author>
<author>BaoLiang Lu</author>
</authors>
<title>An empirical study on word segmentation for Chinese machine translation.</title>
<date>2013</date>
<booktitle>In Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>248--263</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="1632" citStr="Zhao et al., 2013" startWordPosition="223" endWordPosition="226">ng posterior regularization algorithm. The experiments on a Chinese-to-English machine translation task reveal that the proposed model can bring positive segmentation effects to translation quality. 1 Introduction Word segmentation is regarded as a critical procedure for high-level Chinese language processing tasks, since Chinese scripts are written in continuous characters without explicit word boundaries (e.g., space in English). The empirical works show that word segmentation can be beneficial to Chinese-to-English statistical machine translation (SMT) (Xu et al., 2005; Chang et al., 2008; Zhao et al., 2013). In fact most current SMT models assume that parallel bilingual sentences should be segmented into sequences of tokens that are meant to be “words” (Ma and Way, 2009). The practice in state-of-the-art MT systems is that Chinese sentences are tokenized by a monolingual supervised word segmentation model trained on the handannotated treebank data, e.g., Chinese treebank (CTB) (Xue et al., 2005). These models are conducive to MT to some extent, since they commonly have relatively good aggregate performance and segmentation consistency (Chang et al., 2008). But one outstanding problem is that the</context>
<context position="6850" citStr="Zhao et al., 2013" startWordPosition="1017" endWordPosition="1020"> various corpora regardless of their different specifications. Distinct from their behaviors, this work uses automatically learned constraints instead of manually defined ones. Most importantly, the constraints have a better learning guidance since they originate from the bilingual texts. On the other hand, the bilingual-motivated CWS models typically rely on character-based alignments to generate segmentation supervisions. Xu et al. (2004) proposed to employ “chars-to-word” alignments to generate a word dictionary for maximum matching segmentation in SMT task. The works in (Ma and Way, 2009; Zhao et al., 2013) extended the dictionary extraction strategy. Ma and Way (2009) adopted co-occurrence frequency metric to iteratively optimize “candidate words” extract from the alignments. Zhao et al. (2013) attempted to find an optimal subset of the dictionary learned by the character-based alignment to maximize the MT performance. Paul et al. (2011) used the words learned from “chars-to-word” alignments to train a maximum entropy segmentation model. Rather than playing the “hard” uses of the bilingual segmentation knowledge, i.e., directly merging “char-to-word” alignments to words as supervisions, this st</context>
</contexts>
<marker>Zhao, Utiyama, Sumita, Lu, 2013</marker>
<rawString>Hai Zhao, Masao Utiyama, Eiichiro Sumita, and BaoLiang Lu. 2013. An empirical study on word segmentation for Chinese machine translation. In Computational Linguistics and Intelligent Text Processing, pages 248–263. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojin Zhu</author>
<author>Zoubin Ghahramani</author>
<author>John Lafferty</author>
</authors>
<title>Semi-supervised learning using gaussian fields and harmonic functions.</title>
<date>2003</date>
<booktitle>In Proceedings of ICML,</booktitle>
<volume>3</volume>
<pages>912--919</pages>
<contexts>
<context position="7952" citStr="Zhu et al., 2003" startWordPosition="1190" endWordPosition="1193">ilingual segmentation knowledge, i.e., directly merging “char-to-word” alignments to words as supervisions, this study extracts word boundary information of characters from the alignments as soft constraints to regularize a CRFs model’s learning. The graph propagation (GP) technique provides a natural way to represent data in a variety of target domains (Belkin et al., 2006). In this technique, the constructed graph has vertices consisting of labeled and unlabeled examples. Pairs of vertices are connected by weighted edges encoding the degree to which they are expected to have the same label (Zhu et al., 2003). Many recent works, such as by Subramanya et al. (2010), Das and Petrov (2011), Zeng et al. (2013; 2014) and Zhu et al. (2014), proposed GP for inferring the label information of unlabeled data, and then leverage these GP outcomes to learn a semi-supervised scalable model (e.g., CRFs). These approaches are referred to as pipelined learning with GP. This study also works with a similarity graph, encoding the learned bilingual knowledge. But, unlike the prior pipelined approaches, this study performs a joint learning behavior in which GP is used as a learning constraint to interact with the CRF</context>
<context position="16734" citStr="Zhu et al., 2003" startWordPosition="2627" endWordPosition="2630">redefined features (introduced in Section 4.1). Specifically, the point-wise mutual information (PMI) values, between vertices and each feature instantiation that they have in common, are summed to sparse vectors, and their cosine distances are computed as the similarities. The nature of this similarity graph enforces that the connected types with high weights appearing in different texts should have similar word boundary distributions. The quality (smoothness) of the similarity graph can be estimated by using a standard propagation function, as shown in Equation 1. The square-loss criterion (Zhu et al., 2003; Bengio et al., 2006) is used to formulate this function: T M P(v) = (vi,t − ri,t)2 t=1 i=1 +µ N N wij(vi,t − vj,t)2 + ρ N � j=1 i=1 i=1 2 (vi,t) (1) The first term in this equation refers to seed matches that compute the distances between the estimated measure vi and the empirical probabilities ri. The second term refers to edge smoothness that measures how vertices vi are smoothed with respect to the graph. Two types connected by an edge with high weight should be assigned similar word boundary distributions. The third term, a `2 norm, evaluates the distribution sparsity (Das and 0.9 Bei Pi</context>
</contexts>
<marker>Zhu, Ghahramani, Lafferty, 2003</marker>
<rawString>Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty. 2003. Semi-supervised learning using gaussian fields and harmonic functions. In Proceedings of ICML, volume 3, pages 912–919.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ling Zhu</author>
<author>Derek F Wong</author>
<author>Lidia S Chao</author>
</authors>
<title>Unsupervised chunking based on graph propagation from bilingual corpus. The Scientific World Journal,</title>
<date>2014</date>
<contexts>
<context position="8079" citStr="Zhu et al. (2014)" startWordPosition="1214" endWordPosition="1217"> word boundary information of characters from the alignments as soft constraints to regularize a CRFs model’s learning. The graph propagation (GP) technique provides a natural way to represent data in a variety of target domains (Belkin et al., 2006). In this technique, the constructed graph has vertices consisting of labeled and unlabeled examples. Pairs of vertices are connected by weighted edges encoding the degree to which they are expected to have the same label (Zhu et al., 2003). Many recent works, such as by Subramanya et al. (2010), Das and Petrov (2011), Zeng et al. (2013; 2014) and Zhu et al. (2014), proposed GP for inferring the label information of unlabeled data, and then leverage these GP outcomes to learn a semi-supervised scalable model (e.g., CRFs). These approaches are referred to as pipelined learning with GP. This study also works with a similarity graph, encoding the learned bilingual knowledge. But, unlike the prior pipelined approaches, this study performs a joint learning behavior in which GP is used as a learning constraint to interact with the CRFs model estimation. One of our main objectives is to bias CRFs model’s learning on unlabeled data, under a non-linear GP constr</context>
</contexts>
<marker>Zhu, Wong, Chao, 2014</marker>
<rawString>Ling Zhu, Derek F. Wong, and Lidia S. Chao. 2014. Unsupervised chunking based on graph propagation from bilingual corpus. The Scientific World Journal, 2014(401943):10.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>