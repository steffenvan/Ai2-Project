<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000710">
<title confidence="0.9984935">
JU_CSE_NLP: Multi-grade Classification of Semantic Similarity
Between Text Pairs
</title>
<author confidence="0.998427">
Snehasis Neogi1, Partha Pakray2, Sivaji Bandyopadhyay1 Alexander Gelbukh
</author>
<affiliation confidence="0.957363">
1Computer Science &amp; Engineering Department Center for Computing Research
Jadavpur University, Kolkata, India National Polytechnic Institute
2Computer Science &amp; Engineering Department Mexico City, Mexico
Jadavpur University, Kolkata, India gelbukh@gelbukh.com
</affiliation>
<address confidence="0.518586">
Intern at Xerox Research Centre Europe
Grenoble, France
</address>
<email confidence="0.9823225">
{snehasis1981,parthapakray}@gmail.com
sbandyopadhyay@cse.jdvu.ac.in
</email>
<sectionHeader confidence="0.995427" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999972347826087">
This article presents the experiments car-
ried out at Jadavpur University as part of
the participation in Semantic Textual Si-
milarity (STS) of Task 6 @ Semantic
Evaluation Exercises (SemEval-2012).
Task-6 of SemEval- 2012 focused on se-
mantic relations of text pair. Task-6 pro-
vides five different text pair files to
compare different semantic relations and
judge these relations through a similarity
and confidence score. Similarity score is
one kind of multi way classification in the
form of grade between 0 to 5. We have
submitted one run for the STS task. Our
system has two basic modules - one deals
with lexical relations and another deals
with dependency based syntactic relations
of the text pair. Similarity score given to a
pair is the average of the scores of the
above-mentioned modules. The scores
from each module are identified using rule
based techniques. The Pearson Correlation
of our system in the task is 0.3880.
</bodyText>
<sectionHeader confidence="0.99933" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9993938">
Task-61 [1] of SemEval-2012 deals with seman-
tic similarity of text pairs. The task is to find the
similarity between the sentences in the text pair
(s1 and s2) and return a similarity score and an
optional confidence score. There are five datasets
</bodyText>
<footnote confidence="0.748481">
1 http://www.cs.york.ac.uk/semeval-2012/task6/
</footnote>
<bodyText confidence="0.992116">
in the test data and with tab separated text pairs.
The datasets are as follows:
</bodyText>
<listItem confidence="0.967094272727273">
➢ MSR-Paraphrase, Microsoft Research Pa-
raphrase Corpus (750 pairs of sentences.)
➢ MSR-Video, Microsoft Research Video De-
scription Corpus (750 pairs of sentences.)
➢ SMTeuroparl: WMT2008 development data-
set (Europarl section) (459 pairs of sen-
tences.)
➢ SMTnews: news conversation sentence pairs
from WMT.(399 pairs of sentences.)
➢ OnWN: pairs of sentences where the first
comes from Ontonotes and the second from a
</listItem>
<bodyText confidence="0.983351952380952">
WordNet definition. (750 pairs of sentences.)
Similarity score ranges from 0 to 5 and confi-
dence score from 0 to 100. An s1-s2 pair gets a
similarity score of 5 if they are completely
equivalent. Similarity score 4 is allocated for
mostly equivalent s1-s2 pair. Similarly, score 3 is
allocated for roughly equivalent pair. Score 2, 1
and 0 are allocated for non-equivalent details
sharing, non-equivalent topic sharing and totally
different pairs respectively. Major challenge of
this task is to find the similarity score based simi-
larity for the text pair. Generally text entailment
tasks refer whether sentence pairs are entailed or
not: binary classification (YES, NO) [2] or multi-
classification (Forward, Backward, bidirectional
or no entailment) [3][4]. But multi grade classifi-
cation of semantic similarity assigns a score to
the sentence pair. Our system considers lexical
and dependency based syntactic measures for
semantic similarity. Similarity scores are the ba-
sic average of these module scores. A subsequent
</bodyText>
<page confidence="0.967158">
571
</page>
<note confidence="0.52659">
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 571–574,
Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.99952725">
section describes the system architecture. Section
2 describes JU_NLP_CSE system for STS task.
Section 3 describes evaluation and experimental
results. Conclusions are drawn in Section 4.
</bodyText>
<sectionHeader confidence="0.901771" genericHeader="method">
2 System Architecture
</sectionHeader>
<bodyText confidence="0.918491285714286">
The system of Semantic textual similarity task
has two main modules: one is lexical module and
another one is dependency parsing based syntac-
tic module. Both these module have some pre-
processing tasks such as stop word removal, co-
reference resolution and dependency parsing etc.
Figure 1 displays the architecture of the system.
</bodyText>
<figureCaption confidence="0.99944">
Figure 1: System Architecture
</figureCaption>
<subsectionHeader confidence="0.977515">
2.1 Pre-processing Module
</subsectionHeader>
<bodyText confidence="0.999908">
The system separates the s1-s2 sentence pairs
contained in the different STS task datasets.
These separated pairs are then passed through the
following sub modules:
</bodyText>
<listItem confidence="0.922097636363636">
i. Stop word Removal: Stop words are removed
from s1 - s2 sentence pairs.
ii. Co-reference: Co-reference resolutions are
carried out on the datasets before passing through
the TE module. The objective is to increase the
score of the entailment percentage. A word or
phrase in the sentence is used to refer to an entity
introduced earlier or later in the discourse and
both having same things then they have the same
referent or co reference. When the reader must
look back to the previous context, reference is
called &amp;quot;Anaphoric Reference&amp;quot;. When the reader
must look forward, it is termed &amp;quot;Cataphoric Ref-
erence&amp;quot;. To address this problem we used a tool
called JavaRAP2 (A java based implementation
of Anaphora Procedure (RAP) - an algorithm by
Lappin and Leass (1994)).
iii. Dependency Parsing: Separated s1 – s2 sen-
tences are parsed using Stanford dependency
parser3 to produce the dependency relations in
the texts. These dependency relations are used
for WordNet based syntactic matching.
</listItem>
<subsectionHeader confidence="0.99934">
2.2 Lexical Matching Module
</subsectionHeader>
<bodyText confidence="0.99870825">
In this module the TE system calculates different
matching scores such as N – Gram match, Text
Similarity, Chunk match, Named Entity match
and POS match.
</bodyText>
<listItem confidence="0.625112555555556">
i. I-Gram Match module: The N-Gram match
basically measures the percentage match of the
unigram, bigram and trigram of hypothesis
present in the corresponding text. These scores
are simply combined to get an overall N – Gram
matching score for a particular pair.
ii. Chunk Match module: In this sub module
our system evaluates the key NP-chunks of both
text (s1) and hypothesis (s2) using NP Chunker
v1.13 (The University of Sheffield). The hypo-
thesis NP chunks are matched in the text NP
chunks. System calculates an overall value for
the chunk matching, i.e., number of text NP
chunks that match the hypothesis NP chunks. If
the chunks are not similar in their surface form
then our system goes for wordnet synonyms
matching for the words and if they match in
wordnet synsets information, it will be encoun-
tered as a similar chunk. WordNet [5] is one of
most important resource for lexical analysis. The
WordNet 2.0 has been used for WordNet based
chunk matching. The API for WordNet Search-
ing (JAWS)4 is an API that provides Java appli-
cations with the ability to retrieve data from the
WordNet synsets.
iii. Text Similarity Module: System takes into
consideration several text similarities calculated
</listItem>
<footnote confidence="0.998967333333333">
2 http://aye.comp.nus.edu.sg/~qiu/NLPTools/JavaRAP.html
3 http://www.dcs.shef.ac.uk/~mark/phd/software/
4 http://lyle.smu.edu/~tspell/jaws/index.html
</footnote>
<page confidence="0.991717">
572
</page>
<bodyText confidence="0.99723375">
over the s1-s2 pair. These text similarity values
are summed up to produce a total score for a par-
ticular s1-s2 pair. Major Text similarity measures
that our system considers are:
</bodyText>
<table confidence="0.895552333333333">
➢ Cosine Similarity
➢ Lavenstine Distance
➢ Euclidean Distance
➢ MongeElkan Distance
➢ IeedlemanWunch Distance
➢ SmithWaterman Distance
➢ Block Distance
➢ Jaro Similarity
➢ MatchingCoefficient Distance
➢ Dice Similarity
➢ OverlapCoefficient
➢ QGrams Distance
</table>
<listItem confidence="0.9821242">
iv. Named Entity Matching: It is based on the
detection and matching of Named Entities in the
s1-s2 pair. Stanford Named Entity Recognizer5 is
used to tag the named entities in both s1 and s2.
System simply maps the number of hypothesis
(s2) NEs present in the text (s1). A score is allo-
cated for the matching.
IE_match = (Iumber of common IEs in Text
and Hypothesis) / (Iumber of IE in Hypothesis).
v. Part –of – Speech (POS) Matching: This
</listItem>
<bodyText confidence="0.909191875">
module basically deals with matching the com-
mon POS tags between s1 and s2 sentences.
Stanford POS tagger6 is used to tag the part of
speech in both s1 and s2. System matches the
verb and noun POS words in the hypothesis that
match in the text. A score is allocated based on
the number of POS matching.
POS_match = (Iumber of common verb and
noun POS in Text and Hypothesis) / (Total num-
ber of verb and noun POS in hypothesis).
System calculates the sum of the entire sub mod-
ule (modules described in section 2.2) scores and
forms a single percentage score for the lexical
matching. This score is then compared with some
predetermined threshold value to assign a final
lexical score for each pair. If percentage value is
</bodyText>
<footnote confidence="0.9916185">
5 http://nlp.stanford.edu/software/CRF-NER.shtml
6 http://nlp.stanford.edu/software/tagger.shtml
</footnote>
<bodyText confidence="0.999822333333333">
above 0.80 then lexical score 5 is allocated. If the
value is between 0.60 to 0.80 then lexical score 4
is allocated. Similarly, lexical score 3 is allocated
for percentage score of 0.40 to 0.60 and so on.
One lexical score is finally generated for each
text pair.
</bodyText>
<subsectionHeader confidence="0.976346">
2.3. Syntactic Matching Module:
</subsectionHeader>
<bodyText confidence="0.999871">
TE system considers the preprocessed dependen-
cy parsed text pairs (s1 – s2) and goes for word
net based matching technique. After parsing the
sentences, they have some attributes like subject,
object, verb, auxiliaries and prepositions tagged
by the dependency parser tag set. System uses
these attributes for the matching procedure and
depending on the nature of matching a score is
allocated to the s1-s2 pair. Matching procedure is
basically done through comparison of the follow-
ing features that are present in both the text and
the hypothesis.
</bodyText>
<listItem confidence="0.999979285714286">
• Subject – Subject comparison.
• Verb – Verb Comparison.
• Subject – Verbs Comparison.
• Object – Object Comparison.
• Cross Subject – Object Comparison.
• Object – Verbs Comparison.
• Prepositional phrase comparison.
</listItem>
<bodyText confidence="0.990432363636364">
Each of these comparisons produces one match-
ing score for the s1-s2 pair that are finally com-
bined with previously generated lexical score to
generate the final similarity score by taking sim-
ple average of lexical and syntactic matching
scores. The basic heuristics are as follows:
(i) If the feature of the text (s1) directly matches
the same feature of the hypothesis (s2), matching
score 5 is allocated for the text pair.
(ii) If the feature of either text (s1) or hypothesis
(s2) matches with the wordnet synsets of the cor-
responding text (s1) or hypothesis (s2), matching
score 4 is allocated.
(iii) If wordnet synsets of the feature of the text
(s1) match with one of the synsets of the feature
of the hypothesis (s2), matching score 3 is given
to the pair.
(iv) If wordnet synsets of the feature of either
text (s1) or hypothesis (s2) match with the syn-
sets of the corresponding text (s1) or hypothesis
(s2) then matching score 2 is allocated for the
pair.
</bodyText>
<page confidence="0.995705">
573
</page>
<bodyText confidence="0.964222352941177">
(v) Similarly if in both the cases match occurs in
the second level of wordnet synsets, matching
score 1is allocated.
(vi) Matching score 0 is allocated for the pair
having no match in their features.
After execution of the module, system generates
some scores. Lexical module generates one lexi-
cal score and wordnet based syntactic matching
module generates seven matching scores. At the
final stage of the system all these scores are
combined and the mean is evaluated on this
combined score. This mean gives the similarity
score for a particular s1-s2 pair of different data-
sets of STS task. Optional confidence score is
also allocated which is basically the similarity
score multiplied by 10, i.e., if the similarity score
is 5.22, the confidence score will be 52.2.
</bodyText>
<sectionHeader confidence="0.991106" genericHeader="method">
3. Experiments on Dataset and Result
</sectionHeader>
<bodyText confidence="0.994852666666667">
We have submitted one run in SemEval-2012
Task 6. The results for Run on STS Test set are
shown in Table 1.
</bodyText>
<table confidence="0.9899887">
task6-JU_CSE_ILP- Correlations
Semantic_Syntactic_Approach
ALL 0.3880
ALLnrm 0.6706
Mean 0.4111
MSRpar 0.3427
MSRvid 0.3549
SMT-eur 0.4271
On-WN 0.5298
SMT-news 0.4034
</table>
<tableCaption confidence="0.99992">
Table 1: Results of Test Set
</tableCaption>
<bodyText confidence="0.9993585">
ALL: Pearson correlation with the gold standard
for the five datasets and the corresponding rank
82.
ALLnrm: Pearson correlation after the system
outputs for each dataset are fitted to the gold
standard using least squares and the correspond-
ing rank 86.
Mean: Weighted mean across the 5 datasets,
where the weight depends on the number of pairs
in the dataset and the corresponding rank 76.
The subsequent rows show the pearson correla-
tion scores for each of the individual datasets.
</bodyText>
<sectionHeader confidence="0.994922" genericHeader="conclusions">
4. Conclusion
</sectionHeader>
<bodyText confidence="0.999977933333333">
Our JU_CSE_NLP system for the STS task
mainly focus on lexical and syntactic approaches.
There are some limitations in the lexical match-
ing module that shows a correlation that is not
higher in the range. In case of simple sentences
lexical matching is helpful for entailment but for
complex and compound sentences the lexical
matching module loses its accuracy. Semantic
graph matching or conceptual graph implementa-
tion can improve the system. That is not consi-
dered in our present work. Machine learning
tools can be used to learn the system based on the
features. It can also improve the correlation. In
future work our system will include semantic
graph matching and a machine-learning module.
</bodyText>
<sectionHeader confidence="0.998954" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.972334">
The work was done under support of the DST
India-CONACYT Mexico project “Answer Vali-
dation through Textual Entailment” funded by
DST, Government of India.
</bodyText>
<sectionHeader confidence="0.996801" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999981181818182">
[1] Eneko Agirre, Daniel Cer, Mona Diab and Aitor
Gonzalez. SemEval-2012 Task 6: A Pilot on Se-
mantic Textual Similarity. In Proceedings of the
6th International Workshop on Semantic Evalua-
tion (SemEval 2012), in conjunction with the First
Joint Conference on Lexical and Computational
Semantics (*SEM 2012). (2012)
[2] Dagan, I., Glickman, O., Magnini, B.: The
PASCAL Recognising Textual Entailment Chal-
lenge. Proceedings of the First PASCAL Recogniz-
ing Textual Entailment Workshop. (2005).
[3] H. Shima, H. Kanayama, C.-W. Lee, C.-J. Lin,T.
Mitamura, S. S. Y. Miyao, and K. Takeda. Over-
view of ntcir-9 rite: Recognizing inference in text.
In NTCIR-9 Proceedings,2011.
[4] Pakray, P., Neogi, S., Bandyopadhyay, S., Gel-
bukh, A.: A Textual Entailment System using Web
based Machine Translation System. NTCIR-9, Na-
tional Center of Sciences, Tokyo, Japan. December
6-9, 2011. (2011).
[5] Fellbaum, C.: WordNet: An Electronic Lexical
Database. MIT Press (1998).
</reference>
<page confidence="0.998288">
574
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.272019">
<title confidence="0.997999">JU_CSE_NLP: Multi-grade Classification of Semantic Between Text Pairs</title>
<author confidence="0.999236">Partha Sivaji Alexander Gelbukh</author>
<affiliation confidence="0.985644666666667">Science &amp; Engineering Department Center for Computing Research Jadavpur University, Kolkata, India National Polytechnic Institute Science &amp; Engineering Department Mexico City, Mexico</affiliation>
<address confidence="0.76345">University, Kolkata, India Intern at Xerox Research Centre Europe Grenoble, France</address>
<email confidence="0.9497605">snehasis1981@gmail.comsbandyopadhyay@cse.jdvu.ac.in</email>
<email confidence="0.9497605">parthapakray@gmail.comsbandyopadhyay@cse.jdvu.ac.in</email>
<abstract confidence="0.995103826086957">This article presents the experiments carried out at Jadavpur University as part of the participation in Semantic Textual Similarity (STS) of Task 6 @ Semantic Evaluation Exercises (SemEval-2012). Task-6 of SemEval- 2012 focused on semantic relations of text pair. Task-6 provides five different text pair files to compare different semantic relations and judge these relations through a similarity and confidence score. Similarity score is one kind of multi way classification in the form of grade between 0 to 5. We have submitted one run for the STS task. Our system has two basic modules one deals with lexical relations and another deals with dependency based syntactic relations of the text pair. Similarity score given to a pair is the average of the scores of the above-mentioned modules. The scores from each module are identified using rule based techniques. The Pearson Correlation</abstract>
<note confidence="0.693556">of our system in the task is 0.3880.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor Gonzalez</author>
</authors>
<title>SemEval-2012 Task 6: A Pilot on Semantic Textual Similarity.</title>
<booktitle>In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), in conjunction with the First Joint Conference on Lexical and Computational Semantics (*SEM</booktitle>
<contexts>
<context position="1504" citStr="[1]" startWordPosition="214" endWordPosition="214"> judge these relations through a similarity and confidence score. Similarity score is one kind of multi way classification in the form of grade between 0 to 5. We have submitted one run for the STS task. Our system has two basic modules - one deals with lexical relations and another deals with dependency based syntactic relations of the text pair. Similarity score given to a pair is the average of the scores of the above-mentioned modules. The scores from each module are identified using rule based techniques. The Pearson Correlation of our system in the task is 0.3880. 1 Introduction Task-61 [1] of SemEval-2012 deals with semantic similarity of text pairs. The task is to find the similarity between the sentences in the text pair (s1 and s2) and return a similarity score and an optional confidence score. There are five datasets 1 http://www.cs.york.ac.uk/semeval-2012/task6/ in the test data and with tab separated text pairs. The datasets are as follows: ➢ MSR-Paraphrase, Microsoft Research Paraphrase Corpus (750 pairs of sentences.) ➢ MSR-Video, Microsoft Research Video Description Corpus (750 pairs of sentences.) ➢ SMTeuroparl: WMT2008 development dataset (Europarl section) (459 pair</context>
</contexts>
<marker>[1]</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab and Aitor Gonzalez. SemEval-2012 Task 6: A Pilot on Semantic Textual Similarity. In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), in conjunction with the First Joint Conference on Lexical and Computational Semantics (*SEM 2012). (2012)</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>O Glickman</author>
<author>B Magnini</author>
</authors>
<title>The PASCAL Recognising Textual Entailment Challenge.</title>
<date>2005</date>
<booktitle>Proceedings of the First PASCAL Recognizing Textual Entailment Workshop.</booktitle>
<contexts>
<context position="2961" citStr="[2]" startWordPosition="443" endWordPosition="443">om 0 to 5 and confidence score from 0 to 100. An s1-s2 pair gets a similarity score of 5 if they are completely equivalent. Similarity score 4 is allocated for mostly equivalent s1-s2 pair. Similarly, score 3 is allocated for roughly equivalent pair. Score 2, 1 and 0 are allocated for non-equivalent details sharing, non-equivalent topic sharing and totally different pairs respectively. Major challenge of this task is to find the similarity score based similarity for the text pair. Generally text entailment tasks refer whether sentence pairs are entailed or not: binary classification (YES, NO) [2] or multiclassification (Forward, Backward, bidirectional or no entailment) [3][4]. But multi grade classification of semantic similarity assigns a score to the sentence pair. Our system considers lexical and dependency based syntactic measures for semantic similarity. Similarity scores are the basic average of these module scores. A subsequent 571 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 571–574, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics section describes the system architecture. Section 2 describes JU_NLP_CSE system</context>
</contexts>
<marker>[2]</marker>
<rawString>Dagan, I., Glickman, O., Magnini, B.: The PASCAL Recognising Textual Entailment Challenge. Proceedings of the First PASCAL Recognizing Textual Entailment Workshop. (2005).</rawString>
</citation>
<citation valid="false">
<authors>
<author>H Shima</author>
<author>H Kanayama</author>
<author>C-W Lee</author>
<author>C-J Lin</author>
<author>T Mitamura</author>
<author>S S Y Miyao</author>
<author>K Takeda</author>
</authors>
<title>Overview of ntcir-9 rite: Recognizing inference in text.</title>
<booktitle>In NTCIR-9 Proceedings,2011.</booktitle>
<contexts>
<context position="3040" citStr="[3]" startWordPosition="453" endWordPosition="453">core of 5 if they are completely equivalent. Similarity score 4 is allocated for mostly equivalent s1-s2 pair. Similarly, score 3 is allocated for roughly equivalent pair. Score 2, 1 and 0 are allocated for non-equivalent details sharing, non-equivalent topic sharing and totally different pairs respectively. Major challenge of this task is to find the similarity score based similarity for the text pair. Generally text entailment tasks refer whether sentence pairs are entailed or not: binary classification (YES, NO) [2] or multiclassification (Forward, Backward, bidirectional or no entailment) [3][4]. But multi grade classification of semantic similarity assigns a score to the sentence pair. Our system considers lexical and dependency based syntactic measures for semantic similarity. Similarity scores are the basic average of these module scores. A subsequent 571 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 571–574, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics section describes the system architecture. Section 2 describes JU_NLP_CSE system for STS task. Section 3 describes evaluation and experimental results. Conclus</context>
</contexts>
<marker>[3]</marker>
<rawString>H. Shima, H. Kanayama, C.-W. Lee, C.-J. Lin,T. Mitamura, S. S. Y. Miyao, and K. Takeda. Overview of ntcir-9 rite: Recognizing inference in text. In NTCIR-9 Proceedings,2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pakray</author>
<author>S Neogi</author>
<author>S Bandyopadhyay</author>
<author>Gelbukh</author>
</authors>
<title>A.: A Textual Entailment System using Web based</title>
<date>2011</date>
<booktitle>Machine Translation System. NTCIR-9, National Center of Sciences,</booktitle>
<location>Tokyo, Japan.</location>
<contexts>
<context position="3043" citStr="[4]" startWordPosition="453" endWordPosition="453">e of 5 if they are completely equivalent. Similarity score 4 is allocated for mostly equivalent s1-s2 pair. Similarly, score 3 is allocated for roughly equivalent pair. Score 2, 1 and 0 are allocated for non-equivalent details sharing, non-equivalent topic sharing and totally different pairs respectively. Major challenge of this task is to find the similarity score based similarity for the text pair. Generally text entailment tasks refer whether sentence pairs are entailed or not: binary classification (YES, NO) [2] or multiclassification (Forward, Backward, bidirectional or no entailment) [3][4]. But multi grade classification of semantic similarity assigns a score to the sentence pair. Our system considers lexical and dependency based syntactic measures for semantic similarity. Similarity scores are the basic average of these module scores. A subsequent 571 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 571–574, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics section describes the system architecture. Section 2 describes JU_NLP_CSE system for STS task. Section 3 describes evaluation and experimental results. Conclusion</context>
</contexts>
<marker>[4]</marker>
<rawString>Pakray, P., Neogi, S., Bandyopadhyay, S., Gelbukh, A.: A Textual Entailment System using Web based Machine Translation System. NTCIR-9, National Center of Sciences, Tokyo, Japan. December 6-9, 2011. (2011).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press</publisher>
<contexts>
<context position="6259" citStr="[5]" startWordPosition="966" endWordPosition="966">r a particular pair. ii. Chunk Match module: In this sub module our system evaluates the key NP-chunks of both text (s1) and hypothesis (s2) using NP Chunker v1.13 (The University of Sheffield). The hypothesis NP chunks are matched in the text NP chunks. System calculates an overall value for the chunk matching, i.e., number of text NP chunks that match the hypothesis NP chunks. If the chunks are not similar in their surface form then our system goes for wordnet synonyms matching for the words and if they match in wordnet synsets information, it will be encountered as a similar chunk. WordNet [5] is one of most important resource for lexical analysis. The WordNet 2.0 has been used for WordNet based chunk matching. The API for WordNet Searching (JAWS)4 is an API that provides Java applications with the ability to retrieve data from the WordNet synsets. iii. Text Similarity Module: System takes into consideration several text similarities calculated 2 http://aye.comp.nus.edu.sg/~qiu/NLPTools/JavaRAP.html 3 http://www.dcs.shef.ac.uk/~mark/phd/software/ 4 http://lyle.smu.edu/~tspell/jaws/index.html 572 over the s1-s2 pair. These text similarity values are summed up to produce a total scor</context>
</contexts>
<marker>[5]</marker>
<rawString>Fellbaum, C.: WordNet: An Electronic Lexical Database. MIT Press (1998).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>