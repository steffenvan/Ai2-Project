<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004847">
<note confidence="0.444414">
Proceedings of the Conference on Empirical Methods in Natural
Language Processing (EMNLP), Philadelphia, July 2002, pp. 41-48.
Association for Computational Linguistics.
</note>
<title confidence="0.9994865">
An Empirical Evaluation of Knowledge Sources and
Learning Algorithms for Word Sense Disambiguation
</title>
<author confidence="0.998609">
Yoong Keok Lee and Hwee Tou Ng
</author>
<affiliation confidence="0.94352525">
Department of Computer Science
School of Computing
National University of Singapore
3 Science Drive 2, Singapore 117543
</affiliation>
<email confidence="0.982988">
leeyoong, nght @comp.nus.edu.sg
</email>
<sectionHeader confidence="0.995517" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999933095238095">
In this paper, we evaluate a vari-
ety of knowledge sources and super-
vised learning algorithms for word sense
disambiguation on SENSEVAL-2 and
SENSEVAL-1 data. Our knowledge
sources include the part-of-speech of
neighboring words, single words in the
surrounding context, local collocations,
and syntactic relations. The learning al-
gorithms evaluated include Support Vec-
tor Machines (SVM), Naive Bayes, Ad-
aBoost, and decision tree algorithms. We
present empirical results showing the rela-
tive contribution of the component knowl-
edge sources and the different learning
algorithms. In particular, using all of
these knowledge sources and SVM (i.e.,
a single learning algorithm) achieves ac-
curacy higher than the best official scores
on both SENSEVAL-2 and SENSEVAL-1
test data.
</bodyText>
<sectionHeader confidence="0.999334" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999950911764706">
Natural language is inherently ambiguous. A word
can have multiple meanings (or senses). Given an
occurrence of a word in a natural language text,
the task of word sense disambiguation (WSD) is to
determine the correct sense of in that context.
WSD is a fundamental problem of natural language
processing. For example, effective WSD is crucial
for high quality machine translation.
One could envisage building a WSD system us-
ing handcrafted rules or knowledge obtained from
linguists. Such an approach would be highly labor-
intensive, with questionable scalability. Another ap-
proach involves the use of dictionary or thesaurus to
perform WSD.
In this paper, we focus on a corpus-based, super-
vised learning approach. In this approach, to disam-
biguate a word , we first collect training texts in
which instances of occur. Each occurrence of
is manually tagged with the correct sense. We then
train a WSD classifier based on these sample texts,
such that the trained classifier is able to assign the
sense of in a new context.
Two WSD evaluation exercises, SENSEVAL-1
(Kilgarriff and Palmer, 2000) and SENSEVAL-2
(Edmonds and Cotton, 2001), were conducted in
1998 and 2001, respectively. The lexical sample
task in these two SENSEVALs focuses on evalu-
ating WSD systems in disambiguating a subset of
nouns, verbs, and adjectives, for which manually
sense-tagged training data have been collected.
In this paper, we conduct a systematic evaluation
of the various knowledge sources and supervised
learning algorithms on the English lexical sample
data sets of both SENSEVALs.
</bodyText>
<sectionHeader confidence="0.999853" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999950319148936">
There is a large body of prior research on WSD. Due
to space constraints, we will only highlight prior re-
search efforts that have investigated (1) contribution
of various knowledge sources, or (2) relative perfor-
mance of different learning algorithms.
Early research efforts on comparing different
learning algorithms (Mooney, 1996; Pedersen and
Bruce, 1997) tend to base their comparison on only
one word or at most a dozen words. Ng (1997) com-
pared two learning algorithms, k-nearest neighbor
and Naive Bayes, on the DSO corpus (191 words).
Escudero et al. (2000) evaluated k-nearest neighbor,
Naive Bayes, Winnow-based, and LazyBoosting al-
gorithms on the DSO corpus. The recent work of
Pedersen (2001a) and Zavrel et al. (2000) evaluated
a variety of learning algorithms on the SENSEVAL-
1 data set. However, all of these research efforts con-
centrate only on evaluating different learning algo-
rithms, without systematically considering their in-
teraction with knowledge sources.
Ng and Lee (1996) reported the relative contribu-
tion of different knowledge sources, but on only one
word “interest”. Stevenson and Wilks (2001) inves-
tigated the interaction of knowledge sources, such as
part-of-speech, dictionary definition, subject codes,
etc. on WSD. However, they do not evaluate their
method on a common benchmark data set, and there
is no exploration on the interaction of knowledge
sources with different learning algorithms.
Participating systems at SENSEVAL-1 and
SENSEVAL-2 tend to report accuracy using a par-
ticular set of knowledge sources and some partic-
ular learning algorithm, without investigating the
effect of varying knowledge sources and learning
algorithms. In SENSEVAL-2, the various Duluth
systems (Pedersen, 2001b) attempted to investigate
whether features or learning algorithms are more im-
portant. However, relative contribution of knowl-
edge sources was not reported and only two main
types of algorithms (Naive Bayes and decision tree)
were tested.
In contrast, in this paper, we systematically vary
both knowledge sources and learning algorithms,
and investigate the interaction between them. We
also base our evaluation on both SENSEVAL-2 and
SENSEVAL-1 official test data sets, and compare
with the official scores of participating systems.
</bodyText>
<sectionHeader confidence="0.994324" genericHeader="method">
3 Knowledge Sources
</sectionHeader>
<bodyText confidence="0.9998865">
To disambiguate a word occurrence , we consider
four knowledge sources listed below. Each training
(or test) context of generates one training (or test)
feature vector.
</bodyText>
<subsectionHeader confidence="0.9839995">
3.1 Part-of-Speech (POS) of Neighboring
Words
</subsectionHeader>
<bodyText confidence="0.999059866666667">
We use 7 features to encode this knowledge source:
, where ( ) is the
POS of theth token to the left (right) of , and
is the POS of . A token can be a word or a punc-
tuation symbol, and each of these neighboring to-
kens must be in the same sentence as . We use a
sentence segmentation program (Reynar and Ratna-
parkhi, 1997) and a POS tagger (Ratnaparkhi, 1996)
to segment the tokens surrounding into sentences
and assign POS tags to these tokens.
For example, to disambiguate the word
bars in the POS-tagged sentence “Reid/NNP
saw/VBD me/PRP looking/VBG at/IN the/DT
iron/NN bars/NNS ./.”, the POS feature vector is
where denotes
</bodyText>
<subsectionHeader confidence="0.999951">
3.2 Single Words in the Surrounding Context
</subsectionHeader>
<bodyText confidence="0.99914934375">
For this knowledge source, we consider all single
words (unigrams) in the surrounding context of ,
and these words can be in a different sentence from
. For each training or test example, the SENSE-
VAL data sets provide up to a few sentences as the
surrounding context. In the results reported in this
paper, we consider all words in the provided context.
Specifically, all tokens in the surrounding context
of are converted to lower case and replaced by
their morphological root forms. Tokens present in
a list of stop words or tokens that do not contain
at least an alphabet character (such as numbers and
punctuation symbols) are removed. All remaining
tokens from all training contexts provided for are
gathered. Each remaining token contributes one
feature. In a training (or test) example, the feature
corresponding to is set to 1 iff the context of in
that training (or test) example contains.
We attempted a simple feature selection method
to investigate if a learning algorithm performs better
with or without feature selection. The feature selec-
tion method employed has one parameter: . A
feature is selected if occurs in some sense of
or more times in the training data. This param-
eter is also used by (Ng and Lee, 1996). We have
tried and (i.e., no feature selection)
in the results reported in this paper.
the POS tag of a null token.
For example, if is the word bars and the set
of selected unigrams is chocolate, iron, beer , the
feature vector for the sentence “Reid saw me looking
at the iron bars .” is 0, 1, 0 .
</bodyText>
<subsectionHeader confidence="0.999088">
3.3 Local Collocations
</subsectionHeader>
<bodyText confidence="0.995389571428571">
A local collocation refers to the ordered se-
quence of tokens in the local, narrow context of .
Offsets and denote the starting and ending posi-
tion (relative to ) of the sequence, where a neg-
ative (positive) offset refers to a token to its left
(right). For example, let be the word bars in
the sentence “Reid saw me looking at the iron bars
</bodyText>
<tableCaption confidence="0.9452505">
Table 1: Examples of syntactic relations (assuming
no feature selection)
</tableCaption>
<figure confidence="0.93106225">
3(a) green (adj)
3(b) The modern tram is a green machine.
3(c) machine, NN
1(a) attention (noun)
1(b) He turned his attention to the workbench.
1(c) turned, VBD, active, left
2(a) turned (verb)
2(b) He turned his attention to the workbench.
2(c) he, attention, PRP, NN, VBD, active
.” Then
is the iron and is iron . ,
3.4 Syntactic Relations
</figure>
<bodyText confidence="0.997533507936509">
where denotes a null token. Like POS, a colloca-
tion does not cross sentence boundary. To represent
this knowledge source of local collocations, we ex-
tracted 11 features corresponding to the following
collocations: , , , , ,
, , , ,,and
. This
set of 11 features is the union of the collocation fea-
tures used in Ng and Lee (1996) and Ng (1997).
To extract the feature values of the collocation
feature , we first collect all possible collocation
strings (converted into lower case) corresponding to
in all training contexts of . Unlike the case for
surrounding words, we do not remove stop words,
numbers, or punctuation symbols. Each collocation
string is a possible feature value. Feature value se-
lection using , analogous to that used to select
surrounding words, can be optionally applied. If a
training (or test) context of has collocation , and
is a selected feature value, then the feature of
has value . Otherwise, it has the value , denot-
ing the null string.
Note that each collocation is represented by
one feature that can have many possible feature val-
ues (the local collocation strings), whereas each dis-
tinct surrounding word is represented by one feature
that takes binary values (indicating presence or ab-
sence of that word). For example, if is the word
bars and suppose the set of selected collocations for
is a chocolate, the wine, the iron , then
the feature value for collocation in the sen-
tence “Reid saw me looking at the iron bars .” is
the iron.
We first parse the sentence containing with a sta-
tistical parser (Charniak, 2000). The constituent tree
structure generated by Charniak’s parser is then con-
verted into a dependency tree in which every word
points to a parent headword. For example, in the
sentence “Reid saw me looking at the iron bars .”,
the word Reid points to the parent headword saw.
Similarly, the word me also points to the parent
headword saw.
We use different types of syntactic relations, de-
pending on the POS of . If is a noun, we use
four features: its parent headword , the POS of ,
the voice of (active, passive, or if is nota verb),
and the relative position of from (whether is
to the left or right of ). If is a verb, we use six
features: the nearest word to the left of such that
is the parent headword of, the nearest word to
the right of such that is the parent headword of
, the POS of , the POS of , the POS of , and
the voice of . If is an adjective, we use two fea-
tures: its parent headword and the POS of . We
also investigated the effect of feature selection on
syntactic-relation features that are words (i.e., POS,
voice, and relative position are excluded).
Some examples are shown in Table 1. Each POS
noun, verb, or adjective is illustrated by one exam-
ple. For each example, (a) shows and its POS; (b)
shows the sentence where occurs; and (c) shows
the feature vector corresponding to syntactic rela-
tions.
</bodyText>
<sectionHeader confidence="0.985322" genericHeader="method">
4 Learning Algorithms
</sectionHeader>
<bodyText confidence="0.999704125">
We evaluated four supervised learning algorithms:
Support Vector Machines (SVM), AdaBoost with
decision stumps (AdB), Naive Bayes (NB), and de-
cision trees (DT). All the experimental results re-
ported in this paper are obtained using the imple-
mentation of these algorithms in WEKA (Witten and
Frank, 2000). All learning parameters use the de-
fault values in WEKA unless otherwise stated.
</bodyText>
<subsectionHeader confidence="0.995714">
4.1 Support Vector Machines
</subsectionHeader>
<bodyText confidence="0.999976857142857">
The SVM (Vapnik, 1995) performs optimization to
find a hyperplane with the largest margin that sep-
arates training examples into two classes. A test
example is classified depending on the side of the
hyperplane it lies in. Input features can be mapped
into high dimensional space before performing the
optimization and classification. A kernel function
(linear by default) can be used to reduce the compu-
tational cost of training and testing in high dimen-
sional space. If the training examples are nonsep-
arable, a regularization parameter ( by de-
fault) can be used to control the trade-off between
achieving a large margin and a low training error.
In WEKA’s implementation of SVM, each nominal
feature with possible values is converted into
binary (0 or 1) features. If a nominal feature takes
the th feature value, then the th binary feature is
set to 1 and all the other binary features are set to 0.
We tried higher order polynomial kernels, but they
gave poorer results. Our reported results in this pa-
per used the linear kernel.
</bodyText>
<subsectionHeader confidence="0.97561">
4.2 AdaBoost
</subsectionHeader>
<bodyText confidence="0.999395153846154">
AdaBoost (Freund and Schapire, 1996) is a method
of training an ensemble of weak learners such that
the performance of the whole ensemble is higher
than its constituents. The basic idea of boosting is
to give more weights to misclassified training ex-
amples, forcing the new classifier to concentrate on
these hard-to-classify examples. A test example is
classified by a weighted vote of all trained classi-
fiers. We use the decision stump (decision tree with
only the root node) as the weak learner in AdaBoost.
WEKA implements AdaBoost.M1. We used 100 it-
erations in AdaBoost as it gives higher accuracy than
the default number of iterations in WEKA (10).
</bodyText>
<subsectionHeader confidence="0.991591">
4.3 Naive Bayes
</subsectionHeader>
<bodyText confidence="0.99564">
The Naive Bayes classifier (Duda and Hart, 1973)
assumes the features are independent given the class.
During classification, it chooses the class with the
highest posterior probability. The default setting
uses Laplace (“add one”) smoothing.
</bodyText>
<subsectionHeader confidence="0.99042">
4.4 Decision Trees
</subsectionHeader>
<bodyText confidence="0.999993625">
The decision tree algorithm (Quinlan, 1993) parti-
tions the training examples using the feature with the
highest information gain. It repeats this process re-
cursively for each partition until all examples in each
partition belong to one class. A test example is clas-
sified by traversing the learned decision tree. WEKA
implements Quinlan’s C4.5 decision tree algorithm,
with pruning by default.
</bodyText>
<sectionHeader confidence="0.991941" genericHeader="method">
5 Evaluation Data Sets
</sectionHeader>
<bodyText confidence="0.999836454545455">
In the SENSEVAL-2 English lexical sample task,
participating systems are required to disambiguate
73 words that have their POS predetermined. There
are 8,611 training instances and 4,328 test instances
tagged with WORDNET senses. Our evaluation is
based on all the official training and test data of
SENSEVAL-2.
For SENSEVAL-1, we used the 36 trainable
words for our evaluation. There are 13,845 train-
ing instances1 for these trainable words, and 7,446
test instances. For SENSEVAL-1, 4 trainable words
belong to the indeterminate category, i.e., the POS is
not provided. For these words, we first used a POS
tagger (Ratnaparkhi, 1996) to determine the correct
POS.
For a word that may occur in phrasal word
form (eg, the verb “turn” and the phrasal form
“turn down”), we train a separate classifier for each
phrasal word form. During testing, if appears in
a phrasal word form, the classifier for that phrasal
word form is used. Otherwise, the classifier for is
used.
</bodyText>
<sectionHeader confidence="0.995553" genericHeader="method">
6 Empirical Results
</sectionHeader>
<bodyText confidence="0.9996835">
We ran the different learning algorithms using var-
ious knowledge sources. Table 2 (Table 3) shows
</bodyText>
<footnote confidence="0.964663">
1We included 718 training instances from the HECTOR dic-
tionary used in SENSEVAL-1, together with 13,127 training in-
stances from the training corpus supplied.
</footnote>
<table confidence="0.999772">
Algorithm POS Surrounding Words Collocations Syntactic Relations Combined
(i)
(ii) (iii) (iv) (v) (vi) (vii) (viii) = (ix) =
=3 =0 =3 =0 =3 =0 i+ii+iv+vi i+iii+v+vii
SVM
- 1-per-class 54.7 51.6 57.7 52.8 60.5 49.1 54.5 61.5 65.4
AdB
- normal 53.0 51.9 52.5 52.5 53.2 52.4 51.2 54.6 53.6
- 1-per-class 55.9 53.9 55.4 55.7 59.3 53.5 52.4 62.4 62.8
NB
- normal 58.0 55.8 52.5 54.5 39.5 54.1 54.0 61.6 53.4
- 1-per-class 57.6 56.2 51.5 55.8 37.9 54.0 54.2 62.7 52.7
DT
- normal 55.3 50.9 49.1 57.2 52.4 54.2 53.7 56.8 52.6
- 1-per-class 54.9 49.7 48.1 54.3 51.3 52.7 51.5 52.2 50.0
</table>
<tableCaption confidence="0.990847">
Table 2: Contribution of knowledge sources on SENSEVAL-2 data set (micro-averaged recall on all words)
</tableCaption>
<table confidence="0.9995364">
Algorithm POS Surrounding Words Collocations Syntactic Relations Combined
(i)
(ii) (iii) (iv) (v) (vi) (vii) (viii) = (ix) =
=3 =0 =3 =0 =3 =0 i+ii+iv+vi i+iii+v+vii
SVM
- 1-per-class 70.3 65.5 70.3 69.5 74.0 65.1 69.8 76.3 79.2
AdB
- normal 67.2 63.5 64.4 64.2 65.2 65.7 65.6 68.2 68.4
- 1-per-class 71.6 67.0 68.9 69.7 71.2 69.4 68.3 77.7 78.0
NB
- normal 71.5 66.6 63.5 69.1 53.9 69.4 69.6 75.7 67.2
- 1-per-class 71.6 67.3 64.1 70.3 53.0 69.8 70.4 76.3 68.2
DT
- normal 69.2 66.2 65.0 70.2 67.9 68.9 68.6 73.4 70.2
- 1-per-class 68.7 66.6 65.4 67.0 64.4 67.6 64.8 71.4 67.8
</table>
<tableCaption confidence="0.998292">
Table 3: Contribution of knowledge sources on SENSEVAL-1 data set (micro-averaged recall on all words)
</tableCaption>
<table confidence="0.996956461538462">
POS SVM AdB NB DT S1 S2 S3
noun 68.8 69.2 66.4 60.0 68.2 69.5 66.8
verb 61.1 56.1 56.6 51.8 56.6 56.3 57.6
adj 68.0 64.3 68.4 63.8 73.2 68.8 66.8
all 65.4 62.8 62.7 57.2 64.2 63.8 62.9
(a) SENSEVAL-2 data set
POS SVM AdB NB DT s1 s2 s3
noun 85.2 84.9 82.3 81.3 84.9 80.6 80.8
verb 77.0 74.4 73.3 69.5 70.5 70.9 68.7
adj 75.8 74.6 74.5 70.9 76.1 74.3 73.5
indet 76.9 76.8 74.3 70.2 77.6 76.9 76.6
all 79.2 78.0 76.3 73.4 77.1 75.5 74.6
(b) SENSEVAL-1 data set
</table>
<tableCaption confidence="0.984569">
Table 4: Best micro-averaged recall accuracies for
</tableCaption>
<bodyText confidence="0.93380025">
each algorithm evaluated and official scores of the
top 3 participating systems of SENSEVAL-2 and
SENSEVAL-1
the accuracy figures for the different combinations
of knowledge sources and learning algorithms for
the SENSEVAL-2 (SENSEVAL-1) data set. The
nine columns correspond to: (i) using only POS
of neighboring words (ii) using only single words
in the surrounding context with feature selection
( ) (iii) same as (ii) but without feature se-
lection ( ) (iv) using only local collocations
with feature selection ( ) (v) same as (iv) but
without feature selection ( ) (vi) using only
syntactic relations with feature selection on words
( ) (vii) same as (vi) but without feature se-
lection ( ) (viii) combining all four knowl-
edge sources with feature selection (ix) combining
all four knowledge sources without feature selec-
tion.
SVM is only capable of handling binary class
problems. The usual practice to deal with multi-
class problems is to build one binary classifier per
output class (denoted “1-per-class”). The original
AdaBoost, Naive Bayes, and decision tree algo-
</bodyText>
<table confidence="0.972653866666667">
POS SVM AdB NB DT
S1 S2 S3 S1 S2 S3 S1 S2 S3 S1 S2 S3
noun
verb
adj
all
(a) SENSEVAL-2 data set (using micro-averaged recall)
POS SVM AdB NB DT
s1 s2 s3 s1 s2 s3 s1 s2 s3 s1 s2 s3
noun
verb
adj
indet
all
(b) SENSEVAL-1 data set (using macro-averaged recall)
</table>
<tableCaption confidence="0.99948">
Table 5: Paired t-test on SENSEVAL-2 and SENSEVAL-1 data sets: “ ”, (“ ” and “ ”), and (“ ” and
</tableCaption>
<bodyText confidence="0.966975696428572">
algorithm is significantly better.
“ ”) correspond to the p-value
, , and respectively. “ ” or “ ” means our
rithms can already handle multi-class problems, and
we denote runs using the original AdB, NB, and DT
algorithms as “normal” in Table 2 and Table 3.
Accuracy for each word task can be measured
by recall (r) or precision (p), defined by:
no. of test instances correctly labeled
no. of test instances in word task
no. of test instances correctly labeled
no. of test instances output in word task
Recall is very close (but not always identical) to pre-
cision for the top SENSEVAL participating systems.
In this paper, our reported results are based on the
official fine-grained scoring method.
To compute an average recall figure over a set of
words, we can either adopt micro-averaging (mi) or
macro-averaging (ma), defined by:
total no. of test instances correctly labeled
mi
total no. of test instances in all word tasks
That is, micro-averaging treats each test instance
equally, so that a word task with many test instances
will dominate the micro-averaged recall. On the
other hand, macro-averaging treats each word task
equally.
As shown in Table 2 and Table 3, the best micro-
averaged recall for SENSEVAL-2 (SENSEVAL-1)
is 65.4% (79.2%), obtained by combining all knowl-
edge sources (without feature selection) and using
SVM as the learning algorithm.
In Table 4, we tabulate the best micro-averaged
recall for each learning algorithm, broken down ac-
cording to nouns, verbs, adjectives, indeterminates
(for SENSEVAL-1), and all words. We also tabu-
late analogous figures for the top three participating
systems for both SENSEVALs. The top three sys-
tems for SENSEVAL-2 are: JHU (S1) (Yarowsky
et al., 2001), SMUls (S2) (Mihalcea and Moldovan,
2001), and KUNLP (S3) (Seo et al., 2001). The
top three systems for SENSEVAL-1 are: hopkins
(s1) (Yarowsky, 2000), ets-pu (s2) (Chodorow et
al., 2000), and tilburg (s3) (Veenstra et al., 2000).
As shown in Table 4, SVM with all four knowledge
sources achieves accuracy higher than the best offi-
cial scores of both SENSEVALs.
We also conducted paired t test to see if one
system is significantly better than another. The
t statistic of the difference between each pair of
recall figures (between each test instance pair for
micro-averaging and between each word task pair
for macro-averaging) is computed, giving rise to a
p value. A large p value indicates that the two sys-
tems are not significantly different from each other.
The comparison between our learning algorithms
</bodyText>
<figure confidence="0.7504724">
word tasks
recall for word task
word tasks
ma
p r
</figure>
<bodyText confidence="0.998524970588235">
and the top three participating systems is given in
Table 5. Note that we can only compare macro-
averaged recall for SENSEVAL-1 systems, since
the sense of each individual test instance output by
the SENSEVAL-1 participating systems is not avail-
able. The comparison indicates that our SVM sys-
tem is better than the best official SENSEVAL-2 and
SENSEVAL-1 systems at the level of significance
0.05.
Note that we are able to obtain state-of-the-art re-
sults using a single learning algorithm (SVM), with-
out resorting to combining multiple learning algo-
rithms. Several top SENSEVAL-2 participating sys-
tems have attempted the combination of classifiers
using different learning algorithms.
In SENSEVAL-2, JHU used a combination of
various learning algorithms (decision lists, cosine-
based vector models, and Bayesian models) with
various knowledge sources such as surrounding
words, local collocations, syntactic relations, and
morphological information. SMUls used a k-nearest
neighbor algorithm with features such as keywords,
collocations, POS, and name entities. KUNLP used
Classification Information Model, an entropy-based
learning algorithm, with local, topical, and bigram
contexts and their POS.
In SENSEVAL-1, hopkins used hierarchical de-
cision lists with features similar to those used by
JHU in SENSEVAL-2. ets-pu used a Naive Bayes
classifier with topical and local words and their POS.
tilburg used a k-nearest neighbor algorithm with fea-
tures similar to those used by (Ng and Lee, 1996).
tilburg also used dictionary examples as additional
training data.
</bodyText>
<sectionHeader confidence="0.999057" genericHeader="conclusions">
7 Discussions
</sectionHeader>
<bodyText confidence="0.993856857142857">
Based on our experimental results, there appears to
be no single, universally best knowledge source. In-
stead, knowledge sources and learning algorithms
interact and influence each other. For example, lo-
cal collocations contribute the most for SVM, while
parts-of-speech (POS) contribute the most for NB.
NB even outperforms SVM if only POS is used. In
addition, different learning algorithms benefit dif-
ferently from feature selection. SVM performs best
without feature selection, whereas NB performs best
with some feature selection ( ). We will in-
vestigate the effect of more elaborate feature selec-
tion schemes on the performance of different learn-
ing algorithms for WSD in future work.
Also, using the combination of four knowledge
sources gives better performance than using any
single individual knowledge source for most al-
gorithms. On the SENSEVAL-2 test set, SVM
achieves 65.4% (all 4 knowledge sources), 64.8%
(remove syntactic relations), 61.8% (further remove
POS), and 60.5% (only collocations) as knowledge
sources are removed one at a time.
Before concluding, we note that the SENSEVAL-
2 participating system UMD-SST (Cabezas et al.,
2001) also used SVM, with surrounding words and
local collocations as features. However, they re-
ported recall of only 56.8%. In contrast, our im-
plementation of SVM using the two knowledge
sources of surrounding words and local collocations
achieves recall of 61.8%. Following the description
in (Cabezas et al., 2001), our own re-implementation
of UMD-SST gives a recall of 58.6%, close to their
reported figure of 56.8%. The performance drop
from 61.8% may be due to the different collocations
used in the two systems.
</bodyText>
<sectionHeader confidence="0.998581" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997818342592592">
Clara Cabezas, Philip Resnik, and Jessica Stevens. 2001.
Supervised sense tagging using support vector ma-
chines. In Proceedings of the Second International
Workshop on Evaluating Word Sense Disambiguation
Systems (SENSEVAL-2), pages 59–62.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 1st Meeting of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 132–139.
Martin Chodorow, Claudia Leacock, and George A.
Miller. 2000. A topical/local classifier for word sense
identification. Computers and the Humanities, 34(1–
2):115–120.
Richard O. Duda and Peter E. Hart. 1973. Pattern Clas-
sification and Scene Analysis. Wiley, New York.
Philip Edmonds and Scott Cotton. 2001. SENSEVAL-2:
Overview. In Proceedings of the Second International
Workshop on Evaluating Word Sense Disambiguation
Systems (SENSEVAL-2), pages 1–5.
Gerard Escudero, Llu´is M`arquez, and German Rigau.
2000. An empirical study of the domain dependence
of supervised word sense disambiguation systems. In
Proceedings of the Joint SIGDAT Conference on Em-
pirical Methods in Natural Language Processing and
Very Large Corpora, pages 172–180.
Yoav Freund and Robert E. Schapire. 1996. Experi-
ments with a new boosting algorithm. In Proceedings
of the Thirteenth International Conference on Machine
Learning, pages 148–156.
Adam Kilgarriff and Martha Palmer. 2000. Introduction
to the special issue on SENSEVAL. Computers and
the Humanities, 34(1–2):1–13.
Rada F. Mihalcea and Dan I. Moldovan. 2001. Pattern
learning and active feature selection for word sense
disambiguation. In Proceedings of the Second Inter-
national Workshop on Evaluating Word Sense Disam-
biguation Systems (SENSEVAL-2), pages 127–130.
Raymond J. Mooney. 1996. Comparative experiments
on disambiguating word senses: An illustration of the
role of bias in machine learning. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing, pages 82–91.
Hwee Tou Ng and Hian Beng Lee. 1996. Integrat-
ing multiple knowledge sources to disambiguate word
sense: An exemplar-based approach. In Proceedings
of the 34th Annual Meeting of the Association for
Computational Linguistics, pages 40–47.
Hwee Tou Ng. 1997. Exemplar-based word sense dis-
ambiguation: Some recent improvements. In Proceed-
ings of the Second Conference on Empirical Methods
in Natural Language Processing, pages 208–213.
Ted Pedersen and Rebecca Bruce. 1997. A new super-
vised learning algorithm for word sense disambigua-
tion. In Proceedings of the 14th National Conference
on Artificial Intelligence, pages 604–609.
Ted Pedersen. 2001a. A decision tree of bigrams is an
accurate predictor of word sense. In Proceedings of
the 2nd Meeting of the North American Chapter of the
Association for Computational Linguistics, pages 79–
86.
Ted Pedersen. 2001b. Machine learning with lexical fea-
tures: The Duluth approach to Senseval-2. In Proceed-
ings of the Second International Workshop on Evaluat-
ing Word Sense Disambiguation Systems (SENSEVAL-
2), pages 139–142.
J. Ross Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann, San Francisco.
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 133–142.
Jeffrey C. Reynar and Adwait Ratnaparkhi. 1997. A
maximum entropy approach to identifying sentence
boundaries. In Proceedings of the Fifth Conference on
Applied Natural Language Processing, pages 16–19.
Hee-Cheol Seo, Sang-Zoo Lee, Hae-Chang Rim, and
Ho Lee. 2001. KUNLP system using classification
information model at SENSEVAL-2. In Proceedings
of the Second International Workshop on Evaluating
Word Sense Disambiguation Systems (SENSEVAL-2),
pages 147–150.
Mark Stevenson and Yorick Wilks. 2001. The interaction
of knowledge sources in word sense disambiguation.
Computational Linguistics, 27(3):321–349.
Vladimir N. Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer-Verlag, New York.
Jorn Veenstra, Antal van den Bosch, Sabine Buchholz,
Walter Daelemans, and Jakub Zavrel. 2000. Memory-
based word sense disambiguation. Computers and the
Humanities, 34(1–2):171–177.
Ian H. Witten and Eibe Frank. 2000. Data Mining: Prac-
tical Machine Learning Tools and Techniques with
Java Implementations. Morgan Kaufmann, San Fran-
cisco.
David Yarowsky, Silviu Cucerzan, Radu Florian, Charles
Schafer, and Richard Wicentowski. 2001. The
Johns Hopkins SENSEVAL2 system descriptions. In
Proceedings of the Second International Workshop
on Evaluating Word Sense Disambiguation Systems
(SENSEVAL-2), pages 163–166.
David Yarowsky. 2000. Hierarchical decision lists for
word sense disambiguation. Computers and the Hu-
manities, 34(1–2):179–186.
Jakub Zavrel, Sven Degroeve, Anne Kool, Walter Daele-
mans, and Kristiina Jokinen. 2000. Diverse classifiers
for NLP disambiguation tasks: Comparison, optimiza-
tion, combination, and evolution. In TWLT 18. Learn-
ing to Behave, pages 201–221.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.204874">
<note confidence="0.877206333333333">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Philadelphia, July 2002, pp. 41-48. Association for Computational Linguistics.</note>
<title confidence="0.8996785">An Empirical Evaluation of Knowledge Sources and Learning Algorithms for Word Sense Disambiguation</title>
<author confidence="0.999597">Keok Lee Tou</author>
<affiliation confidence="0.88251025">Department of Computer School of National University of 3 Science Drive 2, Singapore</affiliation>
<email confidence="0.944456">leeyoong,nght@comp.nus.edu.sg</email>
<abstract confidence="0.996017590909091">In this paper, we evaluate a variety of knowledge sources and supervised learning algorithms for word sense disambiguation on SENSEVAL-2 and SENSEVAL-1 data. Our knowledge sources include the part-of-speech of neighboring words, single words in the surrounding context, local collocations, and syntactic relations. The learning algorithms evaluated include Support Vector Machines (SVM), Naive Bayes, AdaBoost, and decision tree algorithms. We present empirical results showing the relative contribution of the component knowledge sources and the different learning algorithms. In particular, using all of these knowledge sources and SVM (i.e., a single learning algorithm) achieves accuracy higher than the best official scores on both SENSEVAL-2 and SENSEVAL-1 test data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Clara Cabezas</author>
<author>Philip Resnik</author>
<author>Jessica Stevens</author>
</authors>
<title>Supervised sense tagging using support vector machines.</title>
<date>2001</date>
<booktitle>In Proceedings of the Second International Workshop on Evaluating Word Sense Disambiguation Systems (SENSEVAL-2),</booktitle>
<pages>59--62</pages>
<marker>Cabezas, Resnik, Stevens, 2001</marker>
<rawString>Clara Cabezas, Philip Resnik, and Jessica Stevens. 2001. Supervised sense tagging using support vector machines. In Proceedings of the Second International Workshop on Evaluating Word Sense Disambiguation Systems (SENSEVAL-2), pages 59–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st Meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>132--139</pages>
<contexts>
<context position="9867" citStr="Charniak, 2000" startWordPosition="1610" endWordPosition="1611">lue , denoting the null string. Note that each collocation is represented by one feature that can have many possible feature values (the local collocation strings), whereas each distinct surrounding word is represented by one feature that takes binary values (indicating presence or absence of that word). For example, if is the word bars and suppose the set of selected collocations for is a chocolate, the wine, the iron , then the feature value for collocation in the sentence “Reid saw me looking at the iron bars .” is the iron. We first parse the sentence containing with a statistical parser (Charniak, 2000). The constituent tree structure generated by Charniak’s parser is then converted into a dependency tree in which every word points to a parent headword. For example, in the sentence “Reid saw me looking at the iron bars .”, the word Reid points to the parent headword saw. Similarly, the word me also points to the parent headword saw. We use different types of syntactic relations, depending on the POS of . If is a noun, we use four features: its parent headword , the POS of , the voice of (active, passive, or if is nota verb), and the relative position of from (whether is to the left or right </context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of the 1st Meeting of the North American Chapter of the Association for Computational Linguistics, pages 132–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Chodorow</author>
<author>Claudia Leacock</author>
<author>George A Miller</author>
</authors>
<title>A topical/local classifier for word sense identification. Computers and the Humanities,</title>
<date>2000</date>
<volume>34</volume>
<issue>1</issue>
<pages>2--115</pages>
<contexts>
<context position="20465" citStr="Chodorow et al., 2000" startWordPosition="3434" endWordPosition="3437">ledge sources (without feature selection) and using SVM as the learning algorithm. In Table 4, we tabulate the best micro-averaged recall for each learning algorithm, broken down according to nouns, verbs, adjectives, indeterminates (for SENSEVAL-1), and all words. We also tabulate analogous figures for the top three participating systems for both SENSEVALs. The top three systems for SENSEVAL-2 are: JHU (S1) (Yarowsky et al., 2001), SMUls (S2) (Mihalcea and Moldovan, 2001), and KUNLP (S3) (Seo et al., 2001). The top three systems for SENSEVAL-1 are: hopkins (s1) (Yarowsky, 2000), ets-pu (s2) (Chodorow et al., 2000), and tilburg (s3) (Veenstra et al., 2000). As shown in Table 4, SVM with all four knowledge sources achieves accuracy higher than the best official scores of both SENSEVALs. We also conducted paired t test to see if one system is significantly better than another. The t statistic of the difference between each pair of recall figures (between each test instance pair for micro-averaging and between each word task pair for macro-averaging) is computed, giving rise to a p value. A large p value indicates that the two systems are not significantly different from each other. The comparison between </context>
</contexts>
<marker>Chodorow, Leacock, Miller, 2000</marker>
<rawString>Martin Chodorow, Claudia Leacock, and George A. Miller. 2000. A topical/local classifier for word sense identification. Computers and the Humanities, 34(1– 2):115–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard O Duda</author>
<author>Peter E Hart</author>
</authors>
<title>Pattern Classification and Scene Analysis.</title>
<date>1973</date>
<publisher>Wiley,</publisher>
<location>New York.</location>
<contexts>
<context position="13390" citStr="Duda and Hart, 1973" startWordPosition="2224" endWordPosition="2227"> the performance of the whole ensemble is higher than its constituents. The basic idea of boosting is to give more weights to misclassified training examples, forcing the new classifier to concentrate on these hard-to-classify examples. A test example is classified by a weighted vote of all trained classifiers. We use the decision stump (decision tree with only the root node) as the weak learner in AdaBoost. WEKA implements AdaBoost.M1. We used 100 iterations in AdaBoost as it gives higher accuracy than the default number of iterations in WEKA (10). 4.3 Naive Bayes The Naive Bayes classifier (Duda and Hart, 1973) assumes the features are independent given the class. During classification, it chooses the class with the highest posterior probability. The default setting uses Laplace (“add one”) smoothing. 4.4 Decision Trees The decision tree algorithm (Quinlan, 1993) partitions the training examples using the feature with the highest information gain. It repeats this process recursively for each partition until all examples in each partition belong to one class. A test example is classified by traversing the learned decision tree. WEKA implements Quinlan’s C4.5 decision tree algorithm, with pruning by d</context>
</contexts>
<marker>Duda, Hart, 1973</marker>
<rawString>Richard O. Duda and Peter E. Hart. 1973. Pattern Classification and Scene Analysis. Wiley, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Edmonds</author>
<author>Scott Cotton</author>
</authors>
<title>SENSEVAL-2: Overview.</title>
<date>2001</date>
<booktitle>In Proceedings of the Second International Workshop on Evaluating Word Sense Disambiguation Systems (SENSEVAL-2),</booktitle>
<pages>1--5</pages>
<contexts>
<context position="2385" citStr="Edmonds and Cotton, 2001" startWordPosition="359" endWordPosition="362"> highly laborintensive, with questionable scalability. Another approach involves the use of dictionary or thesaurus to perform WSD. In this paper, we focus on a corpus-based, supervised learning approach. In this approach, to disambiguate a word , we first collect training texts in which instances of occur. Each occurrence of is manually tagged with the correct sense. We then train a WSD classifier based on these sample texts, such that the trained classifier is able to assign the sense of in a new context. Two WSD evaluation exercises, SENSEVAL-1 (Kilgarriff and Palmer, 2000) and SENSEVAL-2 (Edmonds and Cotton, 2001), were conducted in 1998 and 2001, respectively. The lexical sample task in these two SENSEVALs focuses on evaluating WSD systems in disambiguating a subset of nouns, verbs, and adjectives, for which manually sense-tagged training data have been collected. In this paper, we conduct a systematic evaluation of the various knowledge sources and supervised learning algorithms on the English lexical sample data sets of both SENSEVALs. 2 Related Work There is a large body of prior research on WSD. Due to space constraints, we will only highlight prior research efforts that have investigated (1) cont</context>
</contexts>
<marker>Edmonds, Cotton, 2001</marker>
<rawString>Philip Edmonds and Scott Cotton. 2001. SENSEVAL-2: Overview. In Proceedings of the Second International Workshop on Evaluating Word Sense Disambiguation Systems (SENSEVAL-2), pages 1–5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Escudero</author>
<author>Llu´is M`arquez</author>
<author>German Rigau</author>
</authors>
<title>An empirical study of the domain dependence of supervised word sense disambiguation systems.</title>
<date>2000</date>
<booktitle>In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,</booktitle>
<pages>172--180</pages>
<marker>Escudero, M`arquez, Rigau, 2000</marker>
<rawString>Gerard Escudero, Llu´is M`arquez, and German Rigau. 2000. An empirical study of the domain dependence of supervised word sense disambiguation systems. In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, pages 172–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Robert E Schapire</author>
</authors>
<title>Experiments with a new boosting algorithm.</title>
<date>1996</date>
<booktitle>In Proceedings of the Thirteenth International Conference on Machine Learning,</booktitle>
<pages>148--156</pages>
<contexts>
<context position="12707" citStr="Freund and Schapire, 1996" startWordPosition="2109" endWordPosition="2112">nal space. If the training examples are nonseparable, a regularization parameter ( by default) can be used to control the trade-off between achieving a large margin and a low training error. In WEKA’s implementation of SVM, each nominal feature with possible values is converted into binary (0 or 1) features. If a nominal feature takes the th feature value, then the th binary feature is set to 1 and all the other binary features are set to 0. We tried higher order polynomial kernels, but they gave poorer results. Our reported results in this paper used the linear kernel. 4.2 AdaBoost AdaBoost (Freund and Schapire, 1996) is a method of training an ensemble of weak learners such that the performance of the whole ensemble is higher than its constituents. The basic idea of boosting is to give more weights to misclassified training examples, forcing the new classifier to concentrate on these hard-to-classify examples. A test example is classified by a weighted vote of all trained classifiers. We use the decision stump (decision tree with only the root node) as the weak learner in AdaBoost. WEKA implements AdaBoost.M1. We used 100 iterations in AdaBoost as it gives higher accuracy than the default number of iterat</context>
</contexts>
<marker>Freund, Schapire, 1996</marker>
<rawString>Yoav Freund and Robert E. Schapire. 1996. Experiments with a new boosting algorithm. In Proceedings of the Thirteenth International Conference on Machine Learning, pages 148–156.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
<author>Martha Palmer</author>
</authors>
<title>Introduction to the special issue on</title>
<date>2000</date>
<booktitle>SENSEVAL. Computers and the Humanities,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="2343" citStr="Kilgarriff and Palmer, 2000" startWordPosition="353" endWordPosition="356">ned from linguists. Such an approach would be highly laborintensive, with questionable scalability. Another approach involves the use of dictionary or thesaurus to perform WSD. In this paper, we focus on a corpus-based, supervised learning approach. In this approach, to disambiguate a word , we first collect training texts in which instances of occur. Each occurrence of is manually tagged with the correct sense. We then train a WSD classifier based on these sample texts, such that the trained classifier is able to assign the sense of in a new context. Two WSD evaluation exercises, SENSEVAL-1 (Kilgarriff and Palmer, 2000) and SENSEVAL-2 (Edmonds and Cotton, 2001), were conducted in 1998 and 2001, respectively. The lexical sample task in these two SENSEVALs focuses on evaluating WSD systems in disambiguating a subset of nouns, verbs, and adjectives, for which manually sense-tagged training data have been collected. In this paper, we conduct a systematic evaluation of the various knowledge sources and supervised learning algorithms on the English lexical sample data sets of both SENSEVALs. 2 Related Work There is a large body of prior research on WSD. Due to space constraints, we will only highlight prior resear</context>
</contexts>
<marker>Kilgarriff, Palmer, 2000</marker>
<rawString>Adam Kilgarriff and Martha Palmer. 2000. Introduction to the special issue on SENSEVAL. Computers and the Humanities, 34(1–2):1–13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada F Mihalcea</author>
<author>Dan I Moldovan</author>
</authors>
<title>Pattern learning and active feature selection for word sense disambiguation.</title>
<date>2001</date>
<booktitle>In Proceedings of the Second International Workshop on Evaluating Word Sense Disambiguation Systems (SENSEVAL-2),</booktitle>
<pages>127--130</pages>
<contexts>
<context position="20320" citStr="Mihalcea and Moldovan, 2001" startWordPosition="3410" endWordPosition="3413">sk equally. As shown in Table 2 and Table 3, the best microaveraged recall for SENSEVAL-2 (SENSEVAL-1) is 65.4% (79.2%), obtained by combining all knowledge sources (without feature selection) and using SVM as the learning algorithm. In Table 4, we tabulate the best micro-averaged recall for each learning algorithm, broken down according to nouns, verbs, adjectives, indeterminates (for SENSEVAL-1), and all words. We also tabulate analogous figures for the top three participating systems for both SENSEVALs. The top three systems for SENSEVAL-2 are: JHU (S1) (Yarowsky et al., 2001), SMUls (S2) (Mihalcea and Moldovan, 2001), and KUNLP (S3) (Seo et al., 2001). The top three systems for SENSEVAL-1 are: hopkins (s1) (Yarowsky, 2000), ets-pu (s2) (Chodorow et al., 2000), and tilburg (s3) (Veenstra et al., 2000). As shown in Table 4, SVM with all four knowledge sources achieves accuracy higher than the best official scores of both SENSEVALs. We also conducted paired t test to see if one system is significantly better than another. The t statistic of the difference between each pair of recall figures (between each test instance pair for micro-averaging and between each word task pair for macro-averaging) is computed, </context>
</contexts>
<marker>Mihalcea, Moldovan, 2001</marker>
<rawString>Rada F. Mihalcea and Dan I. Moldovan. 2001. Pattern learning and active feature selection for word sense disambiguation. In Proceedings of the Second International Workshop on Evaluating Word Sense Disambiguation Systems (SENSEVAL-2), pages 127–130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raymond J Mooney</author>
</authors>
<title>Comparative experiments on disambiguating word senses: An illustration of the role of bias in machine learning.</title>
<date>1996</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>82--91</pages>
<contexts>
<context position="3165" citStr="Mooney, 1996" startWordPosition="480" endWordPosition="481">erbs, and adjectives, for which manually sense-tagged training data have been collected. In this paper, we conduct a systematic evaluation of the various knowledge sources and supervised learning algorithms on the English lexical sample data sets of both SENSEVALs. 2 Related Work There is a large body of prior research on WSD. Due to space constraints, we will only highlight prior research efforts that have investigated (1) contribution of various knowledge sources, or (2) relative performance of different learning algorithms. Early research efforts on comparing different learning algorithms (Mooney, 1996; Pedersen and Bruce, 1997) tend to base their comparison on only one word or at most a dozen words. Ng (1997) compared two learning algorithms, k-nearest neighbor and Naive Bayes, on the DSO corpus (191 words). Escudero et al. (2000) evaluated k-nearest neighbor, Naive Bayes, Winnow-based, and LazyBoosting algorithms on the DSO corpus. The recent work of Pedersen (2001a) and Zavrel et al. (2000) evaluated a variety of learning algorithms on the SENSEVAL1 data set. However, all of these research efforts concentrate only on evaluating different learning algorithms, without systematically consid</context>
</contexts>
<marker>Mooney, 1996</marker>
<rawString>Raymond J. Mooney. 1996. Comparative experiments on disambiguating word senses: An illustration of the role of bias in machine learning. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 82–91.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Hian Beng Lee</author>
</authors>
<title>Integrating multiple knowledge sources to disambiguate word sense: An exemplar-based approach.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>40--47</pages>
<contexts>
<context position="3830" citStr="Ng and Lee (1996)" startWordPosition="584" endWordPosition="587">omparison on only one word or at most a dozen words. Ng (1997) compared two learning algorithms, k-nearest neighbor and Naive Bayes, on the DSO corpus (191 words). Escudero et al. (2000) evaluated k-nearest neighbor, Naive Bayes, Winnow-based, and LazyBoosting algorithms on the DSO corpus. The recent work of Pedersen (2001a) and Zavrel et al. (2000) evaluated a variety of learning algorithms on the SENSEVAL1 data set. However, all of these research efforts concentrate only on evaluating different learning algorithms, without systematically considering their interaction with knowledge sources. Ng and Lee (1996) reported the relative contribution of different knowledge sources, but on only one word “interest”. Stevenson and Wilks (2001) investigated the interaction of knowledge sources, such as part-of-speech, dictionary definition, subject codes, etc. on WSD. However, they do not evaluate their method on a common benchmark data set, and there is no exploration on the interaction of knowledge sources with different learning algorithms. Participating systems at SENSEVAL-1 and SENSEVAL-2 tend to report accuracy using a particular set of knowledge sources and some particular learning algorithm, without </context>
<context position="7238" citStr="Ng and Lee, 1996" startWordPosition="1137" endWordPosition="1140">ation symbols) are removed. All remaining tokens from all training contexts provided for are gathered. Each remaining token contributes one feature. In a training (or test) example, the feature corresponding to is set to 1 iff the context of in that training (or test) example contains. We attempted a simple feature selection method to investigate if a learning algorithm performs better with or without feature selection. The feature selection method employed has one parameter: . A feature is selected if occurs in some sense of or more times in the training data. This parameter is also used by (Ng and Lee, 1996). We have tried and (i.e., no feature selection) in the results reported in this paper. the POS tag of a null token. For example, if is the word bars and the set of selected unigrams is chocolate, iron, beer , the feature vector for the sentence “Reid saw me looking at the iron bars .” is 0, 1, 0 . 3.3 Local Collocations A local collocation refers to the ordered sequence of tokens in the local, narrow context of . Offsets and denote the starting and ending position (relative to ) of the sequence, where a negative (positive) offset refers to a token to its left (right). For example, let be the </context>
<context position="8645" citStr="Ng and Lee (1996)" startWordPosition="1398" endWordPosition="1401">ine. 3(c) machine, NN 1(a) attention (noun) 1(b) He turned his attention to the workbench. 1(c) turned, VBD, active, left 2(a) turned (verb) 2(b) He turned his attention to the workbench. 2(c) he, attention, PRP, NN, VBD, active .” Then is the iron and is iron . , 3.4 Syntactic Relations where denotes a null token. Like POS, a collocation does not cross sentence boundary. To represent this knowledge source of local collocations, we extracted 11 features corresponding to the following collocations: , , , , , , , , ,,and . This set of 11 features is the union of the collocation features used in Ng and Lee (1996) and Ng (1997). To extract the feature values of the collocation feature , we first collect all possible collocation strings (converted into lower case) corresponding to in all training contexts of . Unlike the case for surrounding words, we do not remove stop words, numbers, or punctuation symbols. Each collocation string is a possible feature value. Feature value selection using , analogous to that used to select surrounding words, can be optionally applied. If a training (or test) context of has collocation , and is a selected feature value, then the feature of has value . Otherwise, it has</context>
<context position="22633" citStr="Ng and Lee, 1996" startWordPosition="3773" endWordPosition="3776">ocal collocations, syntactic relations, and morphological information. SMUls used a k-nearest neighbor algorithm with features such as keywords, collocations, POS, and name entities. KUNLP used Classification Information Model, an entropy-based learning algorithm, with local, topical, and bigram contexts and their POS. In SENSEVAL-1, hopkins used hierarchical decision lists with features similar to those used by JHU in SENSEVAL-2. ets-pu used a Naive Bayes classifier with topical and local words and their POS. tilburg used a k-nearest neighbor algorithm with features similar to those used by (Ng and Lee, 1996). tilburg also used dictionary examples as additional training data. 7 Discussions Based on our experimental results, there appears to be no single, universally best knowledge source. Instead, knowledge sources and learning algorithms interact and influence each other. For example, local collocations contribute the most for SVM, while parts-of-speech (POS) contribute the most for NB. NB even outperforms SVM if only POS is used. In addition, different learning algorithms benefit differently from feature selection. SVM performs best without feature selection, whereas NB performs best with some f</context>
</contexts>
<marker>Ng, Lee, 1996</marker>
<rawString>Hwee Tou Ng and Hian Beng Lee. 1996. Integrating multiple knowledge sources to disambiguate word sense: An exemplar-based approach. In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics, pages 40–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
</authors>
<title>Exemplar-based word sense disambiguation: Some recent improvements.</title>
<date>1997</date>
<booktitle>In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>208--213</pages>
<contexts>
<context position="3275" citStr="Ng (1997)" startWordPosition="501" endWordPosition="502">t a systematic evaluation of the various knowledge sources and supervised learning algorithms on the English lexical sample data sets of both SENSEVALs. 2 Related Work There is a large body of prior research on WSD. Due to space constraints, we will only highlight prior research efforts that have investigated (1) contribution of various knowledge sources, or (2) relative performance of different learning algorithms. Early research efforts on comparing different learning algorithms (Mooney, 1996; Pedersen and Bruce, 1997) tend to base their comparison on only one word or at most a dozen words. Ng (1997) compared two learning algorithms, k-nearest neighbor and Naive Bayes, on the DSO corpus (191 words). Escudero et al. (2000) evaluated k-nearest neighbor, Naive Bayes, Winnow-based, and LazyBoosting algorithms on the DSO corpus. The recent work of Pedersen (2001a) and Zavrel et al. (2000) evaluated a variety of learning algorithms on the SENSEVAL1 data set. However, all of these research efforts concentrate only on evaluating different learning algorithms, without systematically considering their interaction with knowledge sources. Ng and Lee (1996) reported the relative contribution of differ</context>
<context position="8659" citStr="Ng (1997)" startWordPosition="1403" endWordPosition="1404">1(a) attention (noun) 1(b) He turned his attention to the workbench. 1(c) turned, VBD, active, left 2(a) turned (verb) 2(b) He turned his attention to the workbench. 2(c) he, attention, PRP, NN, VBD, active .” Then is the iron and is iron . , 3.4 Syntactic Relations where denotes a null token. Like POS, a collocation does not cross sentence boundary. To represent this knowledge source of local collocations, we extracted 11 features corresponding to the following collocations: , , , , , , , , ,,and . This set of 11 features is the union of the collocation features used in Ng and Lee (1996) and Ng (1997). To extract the feature values of the collocation feature , we first collect all possible collocation strings (converted into lower case) corresponding to in all training contexts of . Unlike the case for surrounding words, we do not remove stop words, numbers, or punctuation symbols. Each collocation string is a possible feature value. Feature value selection using , analogous to that used to select surrounding words, can be optionally applied. If a training (or test) context of has collocation , and is a selected feature value, then the feature of has value . Otherwise, it has the value , d</context>
</contexts>
<marker>Ng, 1997</marker>
<rawString>Hwee Tou Ng. 1997. Exemplar-based word sense disambiguation: Some recent improvements. In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing, pages 208–213.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Rebecca Bruce</author>
</authors>
<title>A new supervised learning algorithm for word sense disambiguation.</title>
<date>1997</date>
<booktitle>In Proceedings of the 14th National Conference on Artificial Intelligence,</booktitle>
<pages>604--609</pages>
<contexts>
<context position="3192" citStr="Pedersen and Bruce, 1997" startWordPosition="482" endWordPosition="485">ctives, for which manually sense-tagged training data have been collected. In this paper, we conduct a systematic evaluation of the various knowledge sources and supervised learning algorithms on the English lexical sample data sets of both SENSEVALs. 2 Related Work There is a large body of prior research on WSD. Due to space constraints, we will only highlight prior research efforts that have investigated (1) contribution of various knowledge sources, or (2) relative performance of different learning algorithms. Early research efforts on comparing different learning algorithms (Mooney, 1996; Pedersen and Bruce, 1997) tend to base their comparison on only one word or at most a dozen words. Ng (1997) compared two learning algorithms, k-nearest neighbor and Naive Bayes, on the DSO corpus (191 words). Escudero et al. (2000) evaluated k-nearest neighbor, Naive Bayes, Winnow-based, and LazyBoosting algorithms on the DSO corpus. The recent work of Pedersen (2001a) and Zavrel et al. (2000) evaluated a variety of learning algorithms on the SENSEVAL1 data set. However, all of these research efforts concentrate only on evaluating different learning algorithms, without systematically considering their interaction wit</context>
</contexts>
<marker>Pedersen, Bruce, 1997</marker>
<rawString>Ted Pedersen and Rebecca Bruce. 1997. A new supervised learning algorithm for word sense disambiguation. In Proceedings of the 14th National Conference on Artificial Intelligence, pages 604–609.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
</authors>
<title>A decision tree of bigrams is an accurate predictor of word sense.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2nd Meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>79--86</pages>
<contexts>
<context position="3537" citStr="Pedersen (2001" startWordPosition="541" endWordPosition="542">ghlight prior research efforts that have investigated (1) contribution of various knowledge sources, or (2) relative performance of different learning algorithms. Early research efforts on comparing different learning algorithms (Mooney, 1996; Pedersen and Bruce, 1997) tend to base their comparison on only one word or at most a dozen words. Ng (1997) compared two learning algorithms, k-nearest neighbor and Naive Bayes, on the DSO corpus (191 words). Escudero et al. (2000) evaluated k-nearest neighbor, Naive Bayes, Winnow-based, and LazyBoosting algorithms on the DSO corpus. The recent work of Pedersen (2001a) and Zavrel et al. (2000) evaluated a variety of learning algorithms on the SENSEVAL1 data set. However, all of these research efforts concentrate only on evaluating different learning algorithms, without systematically considering their interaction with knowledge sources. Ng and Lee (1996) reported the relative contribution of different knowledge sources, but on only one word “interest”. Stevenson and Wilks (2001) investigated the interaction of knowledge sources, such as part-of-speech, dictionary definition, subject codes, etc. on WSD. However, they do not evaluate their method on a commo</context>
</contexts>
<marker>Pedersen, 2001</marker>
<rawString>Ted Pedersen. 2001a. A decision tree of bigrams is an accurate predictor of word sense. In Proceedings of the 2nd Meeting of the North American Chapter of the Association for Computational Linguistics, pages 79– 86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
</authors>
<title>Machine learning with lexical features: The Duluth approach to Senseval-2.</title>
<date>2001</date>
<booktitle>In Proceedings of the Second International Workshop on Evaluating Word Sense Disambiguation Systems (SENSEVAL2),</booktitle>
<pages>139--142</pages>
<contexts>
<context position="3537" citStr="Pedersen (2001" startWordPosition="541" endWordPosition="542">ghlight prior research efforts that have investigated (1) contribution of various knowledge sources, or (2) relative performance of different learning algorithms. Early research efforts on comparing different learning algorithms (Mooney, 1996; Pedersen and Bruce, 1997) tend to base their comparison on only one word or at most a dozen words. Ng (1997) compared two learning algorithms, k-nearest neighbor and Naive Bayes, on the DSO corpus (191 words). Escudero et al. (2000) evaluated k-nearest neighbor, Naive Bayes, Winnow-based, and LazyBoosting algorithms on the DSO corpus. The recent work of Pedersen (2001a) and Zavrel et al. (2000) evaluated a variety of learning algorithms on the SENSEVAL1 data set. However, all of these research efforts concentrate only on evaluating different learning algorithms, without systematically considering their interaction with knowledge sources. Ng and Lee (1996) reported the relative contribution of different knowledge sources, but on only one word “interest”. Stevenson and Wilks (2001) investigated the interaction of knowledge sources, such as part-of-speech, dictionary definition, subject codes, etc. on WSD. However, they do not evaluate their method on a commo</context>
</contexts>
<marker>Pedersen, 2001</marker>
<rawString>Ted Pedersen. 2001b. Machine learning with lexical features: The Duluth approach to Senseval-2. In Proceedings of the Second International Workshop on Evaluating Word Sense Disambiguation Systems (SENSEVAL2), pages 139–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ross Quinlan</author>
</authors>
<date>1993</date>
<booktitle>C4.5: Programs for Machine Learning.</booktitle>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco.</location>
<contexts>
<context position="13647" citStr="Quinlan, 1993" startWordPosition="2262" endWordPosition="2263">ed by a weighted vote of all trained classifiers. We use the decision stump (decision tree with only the root node) as the weak learner in AdaBoost. WEKA implements AdaBoost.M1. We used 100 iterations in AdaBoost as it gives higher accuracy than the default number of iterations in WEKA (10). 4.3 Naive Bayes The Naive Bayes classifier (Duda and Hart, 1973) assumes the features are independent given the class. During classification, it chooses the class with the highest posterior probability. The default setting uses Laplace (“add one”) smoothing. 4.4 Decision Trees The decision tree algorithm (Quinlan, 1993) partitions the training examples using the feature with the highest information gain. It repeats this process recursively for each partition until all examples in each partition belong to one class. A test example is classified by traversing the learned decision tree. WEKA implements Quinlan’s C4.5 decision tree algorithm, with pruning by default. 5 Evaluation Data Sets In the SENSEVAL-2 English lexical sample task, participating systems are required to disambiguate 73 words that have their POS predetermined. There are 8,611 training instances and 4,328 test instances tagged with WORDNET sens</context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>J. Ross Quinlan. 1993. C4.5: Programs for Machine Learning. Morgan Kaufmann, San Francisco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A maximum entropy model for part-of-speech tagging.</title>
<date>1996</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>133--142</pages>
<contexts>
<context position="5694" citStr="Ratnaparkhi, 1996" startWordPosition="881" endWordPosition="882">f participating systems. 3 Knowledge Sources To disambiguate a word occurrence , we consider four knowledge sources listed below. Each training (or test) context of generates one training (or test) feature vector. 3.1 Part-of-Speech (POS) of Neighboring Words We use 7 features to encode this knowledge source: , where ( ) is the POS of theth token to the left (right) of , and is the POS of . A token can be a word or a punctuation symbol, and each of these neighboring tokens must be in the same sentence as . We use a sentence segmentation program (Reynar and Ratnaparkhi, 1997) and a POS tagger (Ratnaparkhi, 1996) to segment the tokens surrounding into sentences and assign POS tags to these tokens. For example, to disambiguate the word bars in the POS-tagged sentence “Reid/NNP saw/VBD me/PRP looking/VBG at/IN the/DT iron/NN bars/NNS ./.”, the POS feature vector is where denotes 3.2 Single Words in the Surrounding Context For this knowledge source, we consider all single words (unigrams) in the surrounding context of , and these words can be in a different sentence from . For each training or test example, the SENSEVAL data sets provide up to a few sentences as the surrounding context. In the results re</context>
<context position="14656" citStr="Ratnaparkhi, 1996" startWordPosition="2420" endWordPosition="2421"> English lexical sample task, participating systems are required to disambiguate 73 words that have their POS predetermined. There are 8,611 training instances and 4,328 test instances tagged with WORDNET senses. Our evaluation is based on all the official training and test data of SENSEVAL-2. For SENSEVAL-1, we used the 36 trainable words for our evaluation. There are 13,845 training instances1 for these trainable words, and 7,446 test instances. For SENSEVAL-1, 4 trainable words belong to the indeterminate category, i.e., the POS is not provided. For these words, we first used a POS tagger (Ratnaparkhi, 1996) to determine the correct POS. For a word that may occur in phrasal word form (eg, the verb “turn” and the phrasal form “turn down”), we train a separate classifier for each phrasal word form. During testing, if appears in a phrasal word form, the classifier for that phrasal word form is used. Otherwise, the classifier for is used. 6 Empirical Results We ran the different learning algorithms using various knowledge sources. Table 2 (Table 3) shows 1We included 718 training instances from the HECTOR dictionary used in SENSEVAL-1, together with 13,127 training instances from the training corpus </context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A maximum entropy model for part-of-speech tagging. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 133–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey C Reynar</author>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A maximum entropy approach to identifying sentence boundaries.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fifth Conference on Applied Natural Language Processing,</booktitle>
<pages>16--19</pages>
<contexts>
<context position="5657" citStr="Reynar and Ratnaparkhi, 1997" startWordPosition="872" endWordPosition="876">ata sets, and compare with the official scores of participating systems. 3 Knowledge Sources To disambiguate a word occurrence , we consider four knowledge sources listed below. Each training (or test) context of generates one training (or test) feature vector. 3.1 Part-of-Speech (POS) of Neighboring Words We use 7 features to encode this knowledge source: , where ( ) is the POS of theth token to the left (right) of , and is the POS of . A token can be a word or a punctuation symbol, and each of these neighboring tokens must be in the same sentence as . We use a sentence segmentation program (Reynar and Ratnaparkhi, 1997) and a POS tagger (Ratnaparkhi, 1996) to segment the tokens surrounding into sentences and assign POS tags to these tokens. For example, to disambiguate the word bars in the POS-tagged sentence “Reid/NNP saw/VBD me/PRP looking/VBG at/IN the/DT iron/NN bars/NNS ./.”, the POS feature vector is where denotes 3.2 Single Words in the Surrounding Context For this knowledge source, we consider all single words (unigrams) in the surrounding context of , and these words can be in a different sentence from . For each training or test example, the SENSEVAL data sets provide up to a few sentences as the s</context>
</contexts>
<marker>Reynar, Ratnaparkhi, 1997</marker>
<rawString>Jeffrey C. Reynar and Adwait Ratnaparkhi. 1997. A maximum entropy approach to identifying sentence boundaries. In Proceedings of the Fifth Conference on Applied Natural Language Processing, pages 16–19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hee-Cheol Seo</author>
<author>Sang-Zoo Lee</author>
<author>Hae-Chang Rim</author>
<author>Ho Lee</author>
</authors>
<title>KUNLP system using classification information model at SENSEVAL-2.</title>
<date>2001</date>
<booktitle>In Proceedings of the Second International Workshop on Evaluating Word Sense Disambiguation Systems (SENSEVAL-2),</booktitle>
<pages>147--150</pages>
<contexts>
<context position="20355" citStr="Seo et al., 2001" startWordPosition="3417" endWordPosition="3420">he best microaveraged recall for SENSEVAL-2 (SENSEVAL-1) is 65.4% (79.2%), obtained by combining all knowledge sources (without feature selection) and using SVM as the learning algorithm. In Table 4, we tabulate the best micro-averaged recall for each learning algorithm, broken down according to nouns, verbs, adjectives, indeterminates (for SENSEVAL-1), and all words. We also tabulate analogous figures for the top three participating systems for both SENSEVALs. The top three systems for SENSEVAL-2 are: JHU (S1) (Yarowsky et al., 2001), SMUls (S2) (Mihalcea and Moldovan, 2001), and KUNLP (S3) (Seo et al., 2001). The top three systems for SENSEVAL-1 are: hopkins (s1) (Yarowsky, 2000), ets-pu (s2) (Chodorow et al., 2000), and tilburg (s3) (Veenstra et al., 2000). As shown in Table 4, SVM with all four knowledge sources achieves accuracy higher than the best official scores of both SENSEVALs. We also conducted paired t test to see if one system is significantly better than another. The t statistic of the difference between each pair of recall figures (between each test instance pair for micro-averaging and between each word task pair for macro-averaging) is computed, giving rise to a p value. A large p</context>
</contexts>
<marker>Seo, Lee, Rim, Lee, 2001</marker>
<rawString>Hee-Cheol Seo, Sang-Zoo Lee, Hae-Chang Rim, and Ho Lee. 2001. KUNLP system using classification information model at SENSEVAL-2. In Proceedings of the Second International Workshop on Evaluating Word Sense Disambiguation Systems (SENSEVAL-2), pages 147–150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Stevenson</author>
<author>Yorick Wilks</author>
</authors>
<title>The interaction of knowledge sources in word sense disambiguation.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>3</issue>
<contexts>
<context position="3957" citStr="Stevenson and Wilks (2001)" startWordPosition="603" endWordPosition="606">d Naive Bayes, on the DSO corpus (191 words). Escudero et al. (2000) evaluated k-nearest neighbor, Naive Bayes, Winnow-based, and LazyBoosting algorithms on the DSO corpus. The recent work of Pedersen (2001a) and Zavrel et al. (2000) evaluated a variety of learning algorithms on the SENSEVAL1 data set. However, all of these research efforts concentrate only on evaluating different learning algorithms, without systematically considering their interaction with knowledge sources. Ng and Lee (1996) reported the relative contribution of different knowledge sources, but on only one word “interest”. Stevenson and Wilks (2001) investigated the interaction of knowledge sources, such as part-of-speech, dictionary definition, subject codes, etc. on WSD. However, they do not evaluate their method on a common benchmark data set, and there is no exploration on the interaction of knowledge sources with different learning algorithms. Participating systems at SENSEVAL-1 and SENSEVAL-2 tend to report accuracy using a particular set of knowledge sources and some particular learning algorithm, without investigating the effect of varying knowledge sources and learning algorithms. In SENSEVAL-2, the various Duluth systems (Peder</context>
</contexts>
<marker>Stevenson, Wilks, 2001</marker>
<rawString>Mark Stevenson and Yorick Wilks. 2001. The interaction of knowledge sources in word sense disambiguation. Computational Linguistics, 27(3):321–349.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir N Vapnik</author>
</authors>
<title>The Nature of Statistical Learning Theory.</title>
<date>1995</date>
<publisher>Springer-Verlag,</publisher>
<location>New York.</location>
<contexts>
<context position="11646" citStr="Vapnik, 1995" startWordPosition="1932" endWordPosition="1933">ample. For each example, (a) shows and its POS; (b) shows the sentence where occurs; and (c) shows the feature vector corresponding to syntactic relations. 4 Learning Algorithms We evaluated four supervised learning algorithms: Support Vector Machines (SVM), AdaBoost with decision stumps (AdB), Naive Bayes (NB), and decision trees (DT). All the experimental results reported in this paper are obtained using the implementation of these algorithms in WEKA (Witten and Frank, 2000). All learning parameters use the default values in WEKA unless otherwise stated. 4.1 Support Vector Machines The SVM (Vapnik, 1995) performs optimization to find a hyperplane with the largest margin that separates training examples into two classes. A test example is classified depending on the side of the hyperplane it lies in. Input features can be mapped into high dimensional space before performing the optimization and classification. A kernel function (linear by default) can be used to reduce the computational cost of training and testing in high dimensional space. If the training examples are nonseparable, a regularization parameter ( by default) can be used to control the trade-off between achieving a large margin </context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>Vladimir N. Vapnik. 1995. The Nature of Statistical Learning Theory. Springer-Verlag, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorn Veenstra</author>
<author>Antal van den Bosch</author>
<author>Sabine Buchholz</author>
<author>Walter Daelemans</author>
<author>Jakub Zavrel</author>
</authors>
<title>Memorybased word sense disambiguation. Computers and the Humanities,</title>
<date>2000</date>
<pages>34--1</pages>
<marker>Veenstra, van den Bosch, Buchholz, Daelemans, Zavrel, 2000</marker>
<rawString>Jorn Veenstra, Antal van den Bosch, Sabine Buchholz, Walter Daelemans, and Jakub Zavrel. 2000. Memorybased word sense disambiguation. Computers and the Humanities, 34(1–2):171–177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Eibe Frank</author>
</authors>
<date>2000</date>
<booktitle>Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations.</booktitle>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco.</location>
<contexts>
<context position="11514" citStr="Witten and Frank, 2000" startWordPosition="1909" endWordPosition="1912">OS, voice, and relative position are excluded). Some examples are shown in Table 1. Each POS noun, verb, or adjective is illustrated by one example. For each example, (a) shows and its POS; (b) shows the sentence where occurs; and (c) shows the feature vector corresponding to syntactic relations. 4 Learning Algorithms We evaluated four supervised learning algorithms: Support Vector Machines (SVM), AdaBoost with decision stumps (AdB), Naive Bayes (NB), and decision trees (DT). All the experimental results reported in this paper are obtained using the implementation of these algorithms in WEKA (Witten and Frank, 2000). All learning parameters use the default values in WEKA unless otherwise stated. 4.1 Support Vector Machines The SVM (Vapnik, 1995) performs optimization to find a hyperplane with the largest margin that separates training examples into two classes. A test example is classified depending on the side of the hyperplane it lies in. Input features can be mapped into high dimensional space before performing the optimization and classification. A kernel function (linear by default) can be used to reduce the computational cost of training and testing in high dimensional space. If the training exampl</context>
</contexts>
<marker>Witten, Frank, 2000</marker>
<rawString>Ian H. Witten and Eibe Frank. 2000. Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations. Morgan Kaufmann, San Francisco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
<author>Silviu Cucerzan</author>
<author>Radu Florian</author>
<author>Charles Schafer</author>
<author>Richard Wicentowski</author>
</authors>
<title>The Johns Hopkins SENSEVAL2 system descriptions.</title>
<date>2001</date>
<booktitle>In Proceedings of the Second International Workshop on Evaluating Word Sense Disambiguation Systems (SENSEVAL-2),</booktitle>
<pages>163--166</pages>
<contexts>
<context position="20278" citStr="Yarowsky et al., 2001" startWordPosition="3404" endWordPosition="3407"> macro-averaging treats each word task equally. As shown in Table 2 and Table 3, the best microaveraged recall for SENSEVAL-2 (SENSEVAL-1) is 65.4% (79.2%), obtained by combining all knowledge sources (without feature selection) and using SVM as the learning algorithm. In Table 4, we tabulate the best micro-averaged recall for each learning algorithm, broken down according to nouns, verbs, adjectives, indeterminates (for SENSEVAL-1), and all words. We also tabulate analogous figures for the top three participating systems for both SENSEVALs. The top three systems for SENSEVAL-2 are: JHU (S1) (Yarowsky et al., 2001), SMUls (S2) (Mihalcea and Moldovan, 2001), and KUNLP (S3) (Seo et al., 2001). The top three systems for SENSEVAL-1 are: hopkins (s1) (Yarowsky, 2000), ets-pu (s2) (Chodorow et al., 2000), and tilburg (s3) (Veenstra et al., 2000). As shown in Table 4, SVM with all four knowledge sources achieves accuracy higher than the best official scores of both SENSEVALs. We also conducted paired t test to see if one system is significantly better than another. The t statistic of the difference between each pair of recall figures (between each test instance pair for micro-averaging and between each word ta</context>
</contexts>
<marker>Yarowsky, Cucerzan, Florian, Schafer, Wicentowski, 2001</marker>
<rawString>David Yarowsky, Silviu Cucerzan, Radu Florian, Charles Schafer, and Richard Wicentowski. 2001. The Johns Hopkins SENSEVAL2 system descriptions. In Proceedings of the Second International Workshop on Evaluating Word Sense Disambiguation Systems (SENSEVAL-2), pages 163–166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Hierarchical decision lists for word sense disambiguation. Computers and the Humanities,</title>
<date>2000</date>
<pages>34--1</pages>
<contexts>
<context position="20428" citStr="Yarowsky, 2000" startWordPosition="3430" endWordPosition="3431">obtained by combining all knowledge sources (without feature selection) and using SVM as the learning algorithm. In Table 4, we tabulate the best micro-averaged recall for each learning algorithm, broken down according to nouns, verbs, adjectives, indeterminates (for SENSEVAL-1), and all words. We also tabulate analogous figures for the top three participating systems for both SENSEVALs. The top three systems for SENSEVAL-2 are: JHU (S1) (Yarowsky et al., 2001), SMUls (S2) (Mihalcea and Moldovan, 2001), and KUNLP (S3) (Seo et al., 2001). The top three systems for SENSEVAL-1 are: hopkins (s1) (Yarowsky, 2000), ets-pu (s2) (Chodorow et al., 2000), and tilburg (s3) (Veenstra et al., 2000). As shown in Table 4, SVM with all four knowledge sources achieves accuracy higher than the best official scores of both SENSEVALs. We also conducted paired t test to see if one system is significantly better than another. The t statistic of the difference between each pair of recall figures (between each test instance pair for micro-averaging and between each word task pair for macro-averaging) is computed, giving rise to a p value. A large p value indicates that the two systems are not significantly different fro</context>
</contexts>
<marker>Yarowsky, 2000</marker>
<rawString>David Yarowsky. 2000. Hierarchical decision lists for word sense disambiguation. Computers and the Humanities, 34(1–2):179–186.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jakub Zavrel</author>
<author>Sven Degroeve</author>
<author>Anne Kool</author>
<author>Walter Daelemans</author>
<author>Kristiina Jokinen</author>
</authors>
<title>Diverse classifiers for NLP disambiguation tasks: Comparison, optimization, combination, and evolution.</title>
<date>2000</date>
<booktitle>In TWLT 18. Learning to Behave,</booktitle>
<pages>201--221</pages>
<contexts>
<context position="3564" citStr="Zavrel et al. (2000)" startWordPosition="544" endWordPosition="547">h efforts that have investigated (1) contribution of various knowledge sources, or (2) relative performance of different learning algorithms. Early research efforts on comparing different learning algorithms (Mooney, 1996; Pedersen and Bruce, 1997) tend to base their comparison on only one word or at most a dozen words. Ng (1997) compared two learning algorithms, k-nearest neighbor and Naive Bayes, on the DSO corpus (191 words). Escudero et al. (2000) evaluated k-nearest neighbor, Naive Bayes, Winnow-based, and LazyBoosting algorithms on the DSO corpus. The recent work of Pedersen (2001a) and Zavrel et al. (2000) evaluated a variety of learning algorithms on the SENSEVAL1 data set. However, all of these research efforts concentrate only on evaluating different learning algorithms, without systematically considering their interaction with knowledge sources. Ng and Lee (1996) reported the relative contribution of different knowledge sources, but on only one word “interest”. Stevenson and Wilks (2001) investigated the interaction of knowledge sources, such as part-of-speech, dictionary definition, subject codes, etc. on WSD. However, they do not evaluate their method on a common benchmark data set, and t</context>
</contexts>
<marker>Zavrel, Degroeve, Kool, Daelemans, Jokinen, 2000</marker>
<rawString>Jakub Zavrel, Sven Degroeve, Anne Kool, Walter Daelemans, and Kristiina Jokinen. 2000. Diverse classifiers for NLP disambiguation tasks: Comparison, optimization, combination, and evolution. In TWLT 18. Learning to Behave, pages 201–221.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>