<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000015">
<title confidence="0.9997705">
A Voting Mechanism for Named Entity Translation in English–
Chinese Question Answering
</title>
<author confidence="0.998711">
Ling-Xiang Tang1, Shlomo Geva1, Andrew Trotman2, Yue Xu1
</author>
<affiliation confidence="0.9989795">
1Faculty of Science and Technology
Queensland University of Technology
</affiliation>
<email confidence="0.984509">
{l4.tang,s.geva,yue.xu}@qut.edu.au
</email>
<affiliation confidence="0.9975645">
2Department of Computer Science
University of Otago
</affiliation>
<email confidence="0.997303">
andrew@cs.otago.ac.nz
</email>
<sectionHeader confidence="0.995614" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9993314375">
In this paper, we describe a voting
mechanism for accurate named entity
(NE) translation in English–Chinese
question answering (QA). This mecha-
nism involves translations from three
different sources: machine translation,
online encyclopaedia, and web docu-
ments. The translation with the highest
number of votes is selected. We evalu-
ated this approach using test collection,
topics and assessment results from the
NTCIR-8 evaluation forum. This
mechanism achieved 95% accuracy in
NEs translation and 0.3756 MAP in
English–Chinese cross-lingual infor-
mation retrieval of QA.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999947173913044">
Nowadays, it is easy for people to access
multi-lingual information on the Internet. Key
term searching on an information retrieval (IR)
system is common for information lookup.
However, when people try to look for answers
in a different language, it is more natural and
comfortable for them to provide the IR system
with questions in their own natural languages
(e.g. looking for a Chinese answer with an
English question: “what is Taiji”?). Cross-
lingual question answering (CLQA) tries to
satisfy such needs by directly finding the cor-
rect answer for the question in a different lan-
guage.
In order to return a cross-lingual answer, a
CLQA system needs to understand the ques-
tion, choose proper query terms, and then ex-
tract correct answers. Cross-lingual informa-
tion retrieval (CLIR) plays a very important
role in this process because the relevancy of
retrieved documents (or passages) affects the
accuracy of the answers.
A simple approach to achieving CLIR is to
translate the query into the language of the tar-
get documents and then to use a monolingual
IR system to locate the relevant ones. How-
ever, it is essential but difficult to translate the
question correctly. Currently, machine transla-
tion (MT) can achieve very high accuracy
when translating general text. However, the
complex phrases and possible ambiguities pre-
sent in a question challenge general purpose
MT approaches. Out-of-vocabulary (OOV)
terms are particularly problematic. So the key
for successful CLQA is being able to correctly
translate all terms in the question, especially
the OOV phrases.
In this paper, we discuss an approach for
accurate question translation that targets the
OOV phrases and uses a translation voting
mechanism. This mechanism involves transla-
tions from three different sources: machine
translation, online encyclopaedia, and web
documents. The translation with the highest
number of votes is selected. To demonstrate
this mechanism, we use Google Translate
</bodyText>
<page confidence="0.997705">
43
</page>
<note confidence="0.686279">
Proceedings of the 4th International Workshop on Cross Lingual Information Access at COLING 2010, pages 43–51,
Beijing, August 2010
</note>
<bodyText confidence="0.985000458333333">
(GT)1 as the MT source, Wikipedia as the en-
cyclopaedia source, and Google web search
engine to retrieve Wikipedia links and relevant
Web document snippets.
English questions on the Chinese corpus for
CLQA are used to illustrate of this approach.
Finally, the approach is examined and evalu-
ated in terms of translation accuracy and re-
sulting CLIR performance using the test col-
lection, topics and assessment results from
NTCIR-82.
English Question Templates (QTs)
who [is  |was  |were  |will], what is the definition of,
what is the [relationship  |interrelationship  |inter-
relationship] [of  |between], what links are there,
what link is there, what [is  |was  |are  |were  |does |
happened], when [is  |was  |were  |will  |did  |do],
where [will  |is  |are  |were], how [is  |was  |were |
did], why [does  |is  |was  |do  |did  |were  |can |
had], which [is  |was  |year], please list, describe
[relationship  |interrelationship  |inter-relationship]
[of  |between], could you [please  |EMPTY] give
short description[s] to, who, where, what, which,
how, describe, explain
</bodyText>
<construct confidence="0.498140615384615">
Chinese QT Counterparts
之间有什么关系, 的定义是什么, 的关系是什么, 发生了什
么事, 是什么关系,是什么时候, 的关系如何, 之间有什么,
请简短简述, 请简单简述, 什么时候, 什么关系,的关系是,
有何关系, 关系如何, 有何相关, 有何渊源, 为什么会, 为
什么要, 为什么能, 是哪一年, 什么时候, 位于哪里, 什么
样的, 你能不能, 相互之间,代表什么, 简短简述, 简单简
述, 简短描述, 简单描述, 为什么,是什么, 什么是, 的关
系, 在哪里, 怎么样,有哪些, 什么事, 是哪个, 是哪家,
有什么, 请列出, 请列举,请描述,哪一年, 请简述,能不能,
的定义,何时,谁是, 是谁,如何, 哪个, 列举, 请问, 何谓,
何以, 为何, 描述, 有何, 简述, 哪些, 什么, 之间, 有
关,定义, 解释
</construct>
<tableCaption confidence="0.997501">
Table 1. Question templates
</tableCaption>
<sectionHeader confidence="0.771992" genericHeader="introduction">
2 CLIR Issue and Related Work
</sectionHeader>
<bodyText confidence="0.9998041">
In CLIR, retrieving documents with a cross-
lingual query with out-of-vocabulary phrases
has always been difficult. To resolve this prob-
lem, an external resource such as Web or
Wikipedia is often used to discover the possi-
ble translation for the OOV term. Wikipedia
and other Web documents are thought of as
treasure troves for OOV problem solving be-
cause they potentially cover the most recent
OOV terms.
</bodyText>
<footnote confidence="0.9930795">
1 http://translate.google.com.
2 http://research.nii.ac.jp/ntcir/ntcir-ws8/ws-en.html.
</footnote>
<bodyText confidence="0.999760944444444">
The Web-based translation method was
shown to be an effective way to solve the OOV
phrase problem (Chen et al., 2000; Lu et al.,
2007; Zhang &amp; Vines, 2004; Zhang et al.,
2005). The idea behind this method is that a
term/phrase and its corresponding translation
normally co-exist in the same document be-
cause authors often provide the new terms’
translation for easy reading.
In Wikipedia the language links provided
for each entry cover most popular written lan-
guages, therefore, it was used to solve a low
coverage issue on named entities in Eu-
roWordNet (Ferrández et al., 2007); a number
of research groups (Chan et al., 2007; Shi et
al., 2008; Su et al., 2007; Tatsunori Mori,
2007) employed Wikipedia to tackle OOV
problems in the NTCIR evaluation forum.
</bodyText>
<sectionHeader confidence="0.997947" genericHeader="method">
3 CLQA Question Analysis
</sectionHeader>
<bodyText confidence="0.99525032">
Questions for CLQA can be very complex. For
example, “What is the relationship between the
movie &amp;quot;Riding Alone for Thousands of Miles&amp;quot;
and ZHANG Yimou?”. In this example, it is
important to recognise two named entities
(&amp;quot;Riding Alone for Thousands of Miles&amp;quot; and
“ZHANG Yimou”) and to translate them pre-
cisely.
In order to recognise the NEs in the ques-
tion, first, English question template phrases in
Table 1 are removed from question; next, we
use the Stanford NLP POS tagger (The Stan-
ford Natural Language Processing Group,
2010) to identify the named entities; then
translate them accordingly. Chinese question
template phrases are also pruned from the
translated question at the end to reduce the
noise words in the final query.
There are three scenarios in which a term or
phrase is considered a named entity. First, it is
consecutively labelled NNP or NNPS (Univer-
sity of Pennsylvania, 2010). Second, term(s)
are grouped by quotation marks. For example,
to extract a named entity from the example
question above, three steps are needed:
</bodyText>
<listItem confidence="0.992764">
1. Remove the question template phrase
“What is the relationship between” from
the question.
2. Process the remaining using the POS tag-
ger, giving “the_DT movie_NN ``_`` Rid-
ing_NNP Alone_NNP for_IN Thou-
</listItem>
<page confidence="0.961813">
44
</page>
<equation confidence="0.55608">
sands_NNS of_IN Miles_NNP
and_CC ZHANG_NNP Yimou_NNP
?_.”
</equation>
<bodyText confidence="0.944235923076923">
3. “Riding Alone for Thousands of Miles” is
between two tags (̏) and so is an entity,
and the phrase “ZHANG Yimou”, as indi-
cated by two consecutive NNP tags is also
a named entity.
Third, if a named entity recognised in the two
scenarios above is followed in the question by
a phrase enclosed in bracket pairs, this phrase
will be used as a tip term providing additional
information about this named entity. For in-
stance, in the question “Who is David Ho (Da-i
Ho)?”, “Da-i Ho” is the tip term of the named
entity “David Ho”.
</bodyText>
<sectionHeader confidence="0.9777465" genericHeader="method">
4 A Voting Mechanism for Named
Entity Translation (VMNET)
</sectionHeader>
<bodyText confidence="0.900018">
Observations have been made:
</bodyText>
<listItem confidence="0.866288333333333">
• Wikipedia has over 100,000 Chinese en-
tries describing various up-to-date events,
people, organizations, locations, and facts.
Most importantly, there are links between
English articles and their Chinese counter-
parts.
• When people post information on the
Internet, they often provide a translation
(where necessary) in the same document.
These pages contain bilingual phrase pairs.
For example, if an English term/phrase is
used in a Chinese article, it is often fol-
lowed by its Chinese translation enclosed
in parentheses.
• A web search engine such as Google can
identify Wikipedia entries, and return
popular bi-lingual web document snippets
that are closely related to the query.
• Statistical machine translation relying on
parallel corpus such as Google Translate
can achieve very high translation accuracy.
</listItem>
<bodyText confidence="0.998646">
Given these observations, there could be up
to three different sources from which we can
obtain translations for a named entity; the task
is to find the best one.
</bodyText>
<subsectionHeader confidence="0.968831">
4.1 VMNET Algorithm
</subsectionHeader>
<bodyText confidence="0.9999356">
A Google search on the extracted named entity
is performed to return related Wikipedia links
and bilingual web document snippets. Then
from the results of Web search and MT, three
different translations could be acquired.
</bodyText>
<subsectionHeader confidence="0.853594">
Wikipedia Translation
</subsectionHeader>
<bodyText confidence="0.99998">
The Chinese equivalent Wikipedia pages
could be found by following the language links
in English pages. The title of the discovered
Chinese Wikipedia page is then used as the
Wikipedia translation.
</bodyText>
<subsectionHeader confidence="0.993803">
Bilingual Clue Text Translation
</subsectionHeader>
<bodyText confidence="0.999830454545454">
The Chinese text contained in the snippets
returned by the search engine is processed for
bilingual clue text translation. The phrase in a
different language enclosed in parentheses
which come directly after the named entity is
used as a candidate translation. For example,
from a web document snippet, “YouTube -
Sean Chen (陳信安) dunks on Yao Ming...”, “
陳信安” can be extracted and used as a candi-
date translation of “Sean Chen”, who is a bas-
ket ball player from Taiwan.
</bodyText>
<subsectionHeader confidence="0.98448">
Machine Translation
</subsectionHeader>
<bodyText confidence="0.99990695">
In the meantime, translations for the named
entity and its tip term (if there is one) are also
retrieved using Google Translate.
Regarding the translation using Wikipedia,
the number of results could be more than one
because of ambiguity. So for a given named
entity, we could have at least one, but possibly
more than three candidate translations.
With all possible candidate translations, the
best one then can be selected. Translations
from all three sources are equally weighted.
Each translation contributes one vote, and the
votes for identical translation are cumulated.
The best translation is the one with the highest
number of votes. In the case of a tie, the first
choice of the best translation is the Wikipedia
translation if only one Wiki-entry is found;
otherwise, the priority for choosing the best is
bilingual clue text translation, then machine
translation.
</bodyText>
<subsectionHeader confidence="0.991497">
4.2 Query Generation with VMNET
</subsectionHeader>
<bodyText confidence="0.999984571428571">
Because terms can have multiple meanings,
ambiguity often occurs if only a single term is
given in machine translation. A state-of-the-art
MT toolkit/service could perform better if
more contextual information is provided. So a
better translation is possible if the whole sen-
tence is given (e.g. the question). For this rea-
</bodyText>
<page confidence="0.995913">
45
</page>
<bodyText confidence="0.999874">
son, the machine translation of the question is
the whole query and not with the templates
removed.
However, issues arise: 1) how do we know
if all the named entities in question are trans-
lated correctly? 2) if there is an error in named
entity translation, how can it be fixed? Particu-
larly for case 2, the translation for the whole
question is considered acceptable, except for
the named entity translation part. We intend to
keep most of the translation and replace the
bad named entity translation with the good
one. But finding the incorrect named entity
translation is difficult because the translation
for a named entity can be different in different
contexts. The missing boundaries in Chinese
sentences make the problem harder. To solve
this, when a translation error is detected, the
question is reformatted by replacing all the
named entities with some nonsense strings
containing special characters as place holders.
These place holders remain unchanged during
the translation process. The good NE transla-
tions then can be put back for the nearly trans-
lated question.
Given an English question Q, the detailed
steps for the Chinese query generation are as
following:
</bodyText>
<listItem confidence="0.970769519230769">
1. Retrieve machine translation Tmt for the
whole question from Google Translate.
2. Remove question template phrase from
question.
3. Process the remaining using the POS tag-
ger.
4. Extract the named entities from the tagged
words using the method discussed in Sec-
tion 3.
5. Replace each named entity in question Q
with a special string Si,(i =0,1,2,..) which
makes nonsense in translation and is
formed by a few non-alphabet characters.
In our experiments, Si is created by joining
a double quote character with a ^ character
and the named entity id (a number, starting
from 0, then increasing by 1 in order of
occurrence of the named entity) followed
by another double quote character. The fi-
nal Si, becomes “Aid”. The resulting ques-
tion is used as Qs.
6. Retrieve machine translation Tqs for Qs
from Google Translate. Since Si consists
of special characters, it remains unchanged
in Tqs.
7. Start the VMNET loop for each named
entity.
8. With an option set to return both English
and Chinese results, Google the named en-
tity and its tip term (if there is one).
9. If there are any English Wikipedia links in
the top 10 search results, then retrieve
them all. Else, jump to step 12.
10. Retrieve all the corresponding Chinese
Wikipedia articles by following the lan-
guages links in the English pages. If none,
then jump to step 12.
11. Save the title NETwiki(i) of each Chinese
Wikipedia article Wiki(i).
12. Process the search results again to locate a
bilingual clue text translation candidate -
NETct, as discussed in Section 4.1.
13. Retrieve machine translation NETmt, and
NETtip for this named entity and its tip term
(if there is one).
14. Gather all candidate translations: NET-
wiki(*), NETct, NETtip, and NETmt for vot-
ing. The translation with the highest num-
ber of votes is considered the best
(NETbest). If there is a tie, NETbest is then
assigned the translation with the highest
priority. The priority order of candidate
</listItem>
<bodyText confidence="0.5897688">
translation is NETwiki(0) (if
sizeof(NETwiki(*))=1) &gt; NETct &gt; NETmt. It
means when a tie occurs and if there are
more than one Wikipedia translation, all
the Wikipedia translations are skipped.
</bodyText>
<listItem confidence="0.730192294117647">
15. If Tmt does not contain NETbest, it is then
considered a faulty translation.
16. Replace Si in Tqs with NETbest.
17. If NETbest is different from any NETwiki(i)
but can be found in the content of a
Wikipedia article (Wiki(i)), then the corre-
sponding NETwiki(i) is used as an addi-
tional query term, and appended to the fi-
nal Chinese query.
18. Continue the VMNET loop and jump back
to step 8 until no more named entities re-
main in the question.
19. If Tmt was considered a faulty translation,
use Tqs as the final translation of Q. Other-
wise, just use Tmt. The Chinese question
template phrases are pruned from the
translation for the final query generation.
</listItem>
<page confidence="0.969363">
46
</page>
<listItem confidence="0.960565076923077">
A short question translation example is
given below:
• For the question “What is the relationship
between the movie &amp;quot;Riding Alone for
Thousands of Miles&amp;quot; and ZHANG Yi-
mou?”, retrieving its Chinese translation
from a MT service, we get the following:
之间有什么电影“利民为千里单独的关系”
和张艺谋.
• The translation for the movie name &amp;quot;Rid-
ing Alone for Thousands of Miles&amp;quot; of
“ZHANG Yimou” is however incorrect.
• Since the question is also reformatted into
“What is the relationship between the
movie &amp;quot;^0&amp;quot; and “^1”?”, machine transla-
tion returns a second translation: 什么是电
影之间的关系“^ 0”和“^ 1”?
• VMNET obtains the correct translations:
千里走单骑and 张艺谋, for two named en-
tities &amp;quot;Riding Alone for Thousands of
Miles&amp;quot; and “ZHANG Yimou” respectively.
• Replace the place holders with the correct
translations in the second translation and
give the final Chinese translation: 什么是
电影之间的关系“千里走单骑”和“张艺
谋”?
</listItem>
<sectionHeader confidence="0.9973" genericHeader="method">
5 Information Retrieval
</sectionHeader>
<subsectionHeader confidence="0.996509">
5.1 Chinese Document Processing
</subsectionHeader>
<bodyText confidence="0.99997125">
Approaches to Chinese text indexing vary:
Unigrams, bigrams and whole words are all
commonly used as tokens. The performance of
various IR systems using different segmenta-
tion algorithms or techniques varies as well
(Chen et al., 1997; Robert &amp; Kwok, 2002). It
was seen in prior experiments that using an
indexing technique requiring no dictionary can
have similar performance to word-based index-
ing (Chen, et al., 1997). Using bigrams that
exhibit high mutual information and unigrams
as index terms can achieve good results. Moti-
vated by indexing efficiency and without the
need for Chinese text segmentation, we use
both bigrams and unigrams as indexing units
for our Chinese IR experiments.
</bodyText>
<subsectionHeader confidence="0.988228">
5.2 Weighting Model
</subsectionHeader>
<bodyText confidence="0.999878">
A slightly modified BM25 ranking function
was used for document ordering.
When calculating the inverse document fre-
quency, we use:
</bodyText>
<equation confidence="0.999328">
IDF(qi) = log&amp;quot; (1)
</equation>
<bodyText confidence="0.958893142857143">
where N is the number of documents in the
corpus, and n is the document frequency of
query term ql. The retrieval status value of a
document d with respect to query q(qi, ..., qm)
is given as:
�
(2)
where tf (ql, d) is the term frequency of term
q in document d; en(d) is the length of
document d in words and avgdl is the mean
document length. The number of bigrams is
included in the document length. The values of
the tuneable parameters ki and b used in our
experiments are 0.7 and 0.3 respectively.
</bodyText>
<sectionHeader confidence="0.999454" genericHeader="method">
6 CLIR Experiment
</sectionHeader>
<subsectionHeader confidence="0.999983">
6.1 Test Collection and Topics
</subsectionHeader>
<bodyText confidence="0.996010857142857">
Table 2 gives the statistics of the test collection
and the topics used in our experiments. The
collection contains 308,845 documents in sim-
plified Chinese from Xinhua News. There are
in total 100 topics consisting of both English
and Chinese questions. This is a NTCIR-8 col-
lection for ACLIA task.
</bodyText>
<table confidence="0.80914">
Corpus #docs #topics
Xinhua Chinese (simplified) 308,845 100
</table>
<tableCaption confidence="0.996946">
Table 2. Statistics of test corpus and topics
</tableCaption>
<subsectionHeader confidence="0.993901">
6.2 Evaluation Measures
</subsectionHeader>
<bodyText confidence="0.99984825">
The evaluation of VMNET performance cov-
ers two main aspects: translation accuracy and
CLIR performance.
As we focus on named entity translation, the
translation accuracy is measured using the pre-
cision of translated named entities at the topic
level. So the translation precision -P is defined
as:
</bodyText>
<equation confidence="0.914211333333333">
-
P= (3)
&amp;quot;
</equation>
<bodyText confidence="0.999794">
where c is the number of topics in which all
the named entities are correctly translated; N is
the number of topics evaluated.
</bodyText>
<page confidence="0.9985">
47
</page>
<bodyText confidence="0.999928736842105">
The effectiveness of different translation
methods can be further measured by the result-
ing CLIR performance. In NTCIR-8, CLIR
performance is measured using the mean aver-
age precision. The MAP values are obtained
by running the ir4qa_eval2 toolkit with the
assessment results 3 on experimental run
s(NTCIR Project, 2010). MAP is computed
using only 73 topics due to an insufficient
number of relevant document found for the
other 27 topics (Sakai et al., 2010). This is the
case for all NTCIR-8 ACLIA submissions and
not our decision.
It also must be noted that there are five top-
ics that have misspelled terms in their English
questions. The misspelled terms in those 5 top-
ics are given in Table 3. It is interesting to see
how different translations cope with misspelled
terms and how this affects the CLIR result.
</bodyText>
<table confidence="0.997573">
Topic ID Misspelling Correction
ACLIA2-CS-0024 Qingling Qinling
ACLIA2-CS-0035 Initials D Initial D
ACLIA2-CS-0066 Kasianov Kasyanov
ACLIA2-CS-0074 Northern northern
Territories territories
ACLIA2-CS-0075 Kashimir Kashmir
</table>
<tableCaption confidence="0.999921">
Table 3. The misspelled terms in topics
</tableCaption>
<subsectionHeader confidence="0.887165">
6.3 CLIR Experiment runs
</subsectionHeader>
<bodyText confidence="0.999877125">
A few experimental runs were created for
VMNET and CLIR system performance
evaluation. Their details are listed in Table 7.
Those with name *CS-CS* are the Chinese
monolingual IR runs; and those with the name
*EN-CS* are the English-to-Chinese CLIR
runs. Mono-lingual IR runs are used for
benchmarking our CLIR system performance.
</bodyText>
<sectionHeader confidence="0.997936" genericHeader="evaluation">
7 Results and Discussion
</sectionHeader>
<subsectionHeader confidence="0.996855">
7.1 Translation Evaluation
</subsectionHeader>
<bodyText confidence="0.99997525">
The translations in our experiments using
Google Translate reflect only the results re-
trieved at the time of the experiments because
Google Translate is believed to be improved
over time.
The result of the final translation evaluation
on the 100 topics is given in Table 4. Google
Translate had difficulties in 13 topics. If all
</bodyText>
<footnote confidence="0.486765">
3 http://research.nii.ac.jp/ntcir/ntcir-ws8/ws-en.html.
</footnote>
<bodyText confidence="0.998277166666667">
thirteen named entities in those topics where
Google Translate failed are considered OOV
terms, the portion of topics with OOV phrases
is relatively small. Regardless, there is an 8%
improvement achieved by VMNET reaching
95% precision.
</bodyText>
<table confidence="0.998515333333333">
Method c N P
Google Translate 87 100 87%
VMNET 95 100 95%
</table>
<tableCaption confidence="0.999776">
Table 4. Translation Evaluation Results
</tableCaption>
<bodyText confidence="0.995015375">
There are in total 14 topics in which Google
Translate or VMNET failed to correctly trans-
late all named entities. These topics are listed
in Table 8. Interestingly, for topic (ACLIA2-
CS-0066) with the misspelled term
“Kasianov”, VMNET still managed to find a
correct translation ( .� · � � � � �
p·-f�AgiMJc). This has to be attributed to
the search engine’s capability in handling mis-
spellings. On the other hand, Google Translate
was correct in its translation of “Northern Ter-
ritories” of Japan, but VMNET incorrectly
chose “Northern Territory” (of Australia). For
the rest of the misspelled phrases (Qingling,
Initials D, Kashimir), neither Google Translate
nor VMNET could pick the correct translation.
</bodyText>
<subsectionHeader confidence="0.98895">
7.2 IR Evaluation
</subsectionHeader>
<bodyText confidence="0.999926347826087">
The MAP values of all experimental runs cor-
responding to each query processing technique
and Chinese indexing strategy are given in Ta-
ble 5. The results of mono-lingual runs give
benchmarking scores for CLIR runs.
As expected, the highest MAP 0.4681 is
achieved by the monolingual run VMNET-CS-
CS-01-T, in which the questions were manu-
ally segmented and all the noise words were
removed.
It is encouraging to see that the automatic
run VMNET-CS-CS-02-T with only question
template phrase removal has a slightly lower
MAP 0.4419 than that (0.4488) of the best per-
formance CS-CS run in the NTCIR-8 evalua-
tion forum (Sakai, et al., 2010).
If unigrams were used as the only indexing
units, the MAP of VMNET-CS-CS-04-T
dropped from 0.4681 to 0.3406. On the other
hand, all runs using bigrams as indexing units
either exclusively or jointly performed very
well. The MAP of run VMNET-CS-CS-05-T
using bigrams only is 0.4653, which is slightly
</bodyText>
<page confidence="0.99813">
48
</page>
<bodyText confidence="0.99981803030303">
lower than that of the top performer run
VMNET-CS-CS-01-T, which used two forms
of indexing units. However, retrieval perform-
ance could be maximised by using both uni-
grams and bigrams as indexing units.
The highest MAP (0.3756) of a CLIR run is
achieved by run VMNET-EN-CS-03-T, which
used VMNET for translation. Comparing it to
our manual run VMNET-CS-CS-01-T, there is
around 9% performance degradation as a result
of the influence of noise words in the ques-
tions, and the possible information loss or
added noise due to English-to-Chinese transla-
tion, even though the named entities translation
precision is relatively high.
The best EN-CS CLIR run (MAP 0.4209)
in all submissions to the NTCIR-8 ACLIA task
used the same indexing technique (bigrams
and unigrams) and ranking function (BM25) as
run VMNET-EN-CS-03-T but with “query
expansion based on RSV” (Sakai, et al., 2010).
The MAP difference 4.5% between the forum
best run and our CLIR best run could suggest
that using query expansion is an effective way
to improve the CLIR system performance.
Runs VMNET-EN-CS-01-T and VMNET-
EN-CS-04-T, that both used Google Translate
provide direct comparisons with runs
VMNET-EN-CS-02-T and VMNET-EN-CS-
03-T, respectively, which employed VMNET
for translation. All runs using VMNET per-
formed better than the runs using Google
Translate.
</bodyText>
<table confidence="0.998951666666667">
Run Name MAP
NTCIR-8 CS-CS BEST 0.4488
VMNET-CS-CS-01-T 0.4681
VMNET-CS-CS-02-T 0.4419
VMNET-CS-CS-03-T 0.4189
VMNET-CS-CS-04-T 0.3406
VMNET-CS-CS-05-T 0.4653
NTCIR-8 EN-CS BEST 0.4209
VMNET-EN-CS-01-T 0.3161
VMNET-EN-CS-02-T 0.3408
VMNET-EN-CS-03-T 0.3756
VMNET-EN-CS-04-T 0.3449
</table>
<tableCaption confidence="0.999596">
Table 5. Results of all experimental runs
</tableCaption>
<bodyText confidence="0.9991595">
The different performances between CLIR
runs using Google Translate and VMENT is
the joint result of the translation improvement
and other translation differences. As shown in
Table 8, VMNET found the correct transla-
tions for 8 more topics than Google Translate.
It should be noted that there are two topics
(ACLIA2-CS-0008 and ACLIA2-CS-0088)
not included in the final CLIR evaluation (Sa-
kai, et al., 2010). Also, there is one phrase,
“Kenneth Yen (K. T. Yen) (i�1L�)”, which
VMNET couldn’t find the correct translation
for, but it detected a highly associated term
“Yulon - W�AIF”, an automaker company in
Taiwan; Kenneth Yen is the CEO of Yulon.
Although Yulon is not a correct translation, it is
still a good query term because it is then possi-
ble to find the correct answer for the question:
“Who is Kenneth Yen?”. However, this topic
was not included in the NTCIR-8 IR4QA
evaluation.
Moreover, it is possible to have multiple
explanations for a term. In order to discover as
many question-related documents as possible,
alternative translations found by VMNET are
also used as additional query terms. They are
shown in Table 6. For example, 丁A is the
Chinese term for DINK in Mainland China,
but ffilv is used in Taiwan. Furthermore,
because VMNET gives the Wikipedia transla-
tion the highest priority if only one entry is
found, a person’s full name is used in person
name translation rather than the short com-
monly used name. For example, Cheney (for-
mer vice president of U.S.) is translated into A—
A·VITE rather than just VITE.
</bodyText>
<table confidence="0.9989582">
NE VMNET Wiki Title
Princess Nori k a± mf g
DINK 丁A ffilv
BSE At_A t_*N4krA
Three Gorges Dam _ )�A _=W-Lim
</table>
<tableCaption confidence="0.984429">
Table 6. Alternative translations
</tableCaption>
<bodyText confidence="0.999868111111111">
The biggest difference, 3.07%, between
runs that used different translation is from runs
VMNET-EN-CS-03-T and VMNET-EN-CS-
04-T, which both pruned the question template
phrase for simple query processing. Although
the performance improvement is not obvious,
the correct translations and the additional
query terms found by VMNET are still very
valuable.
</bodyText>
<sectionHeader confidence="0.999248" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.99922">
General machine translation can already
achieve very good translation results, but with
our proposed approach we can further improve
the translation accuracy. With a proper adjust-
</bodyText>
<page confidence="0.998097">
49
</page>
<bodyText confidence="0.9999611">
ment of this approach, it could be used in a
situation where there is a need for higher pre-
cision of complex phrase translation.
The results from our CLIR experiments in-
dicate that VMNET is also capable of provid-
ing high quality query terms. A CLIR system
can achieve good results for answer finding by
using the VMNET for translation, simple in-
dexing technique (bigrams and unigrams), and
plain question template phrase pruning.
</bodyText>
<table confidence="0.996964307692308">
Run Name Indexing Query Processing
Units
VMNET-CS-CS-01-T U + B Manually segment the question and remove all the noise words
VMNET-CS-CS-02-T U + B Prune the question template phrase
VMNET-CS-CS-03-T U + B Use the whole question without doing any extra processing work
VMNET-CS-CS-04-T U As VMNET-CS-CS-01-T
VMNET-CS-CS-05-T B As VMNET-CS-CS-01-T
VMNET-EN-CS-01-T U + B Use Google Translate on the whole question and use the entire translation
as query
VMNET-EN-CS-02-T U + B Use VMNET translation result without doing any further processing
VMNET-EN-CS-03-T U + B As above, but prune the Chinese question template from translation
VMNET-EN-CS-04-T U + B Use Google Translate on the whole question and prune the Chinese ques-
tion template phrase from the translation
</table>
<tableCaption confidence="0.999433">
Table 7. The experimental runs. For indexing units, U means unigrams; B means bigrams.
</tableCaption>
<figureCaption confidence="0.687247129032258">
Topic ID Question with OOV Phrases Correct GT VMNET
ACLIA2-CS-0002 What is the relationship between the movie 千里走单 利民为千里单独 千里走单骑
&amp;quot;Riding Alone for Thousands of Miles&amp;quot; 骑
and ZHANG Yimou?
ACLIA2-CS-0008 Who is LI Yuchun? 李宇春 李玉春 李宇春
ACLIA2-CS-0024 Why does Qingling build &amp;quot;panda corridor 秦岭 宋庆龄 宋庆龄
zone&amp;quot;
ACLIA2-CS-0035 Please list the events related to the movie 头文字 D 缩写 D 的事件 缩写 D 的事
&amp;quot;Initials D&amp;quot;. 件
ACLIA2-CS-0036 Please list the movies in which Zhao Wei 赵薇 照委 赵薇
participated.
ACLIA2-CS-0038 What is the relationship between Xia Yu 袁泉 袁区广 袁泉
and Yuan Quan.
ACLIA2-CS-0048 Who is Sean Chen(Chen Shin-An)? 陈信安 肖恩陈(陈新的) 陳信安
ACLIA2-CS-0049 Who is Lung Yingtai? 龙应台 龙瀛台 龙应台
ACLIA2-CS-0057 What is the disputes between China and 东海 东中国海域 东海
Japan for the undersea natural gas field in
the East China Sea?
ACLIA2-CS-0066 What is the relationship between two Rus- 卡西亚诺 Kasianov 米哈伊
sian politicians, Kasianov and Putin? 夫 尔·米哈伊
洛维奇·卡
西亚诺夫
ACLIA2-CS-0074 Where are Japan&apos;s Northern Territories 北方领土 北方领土 北领地
located?
ACLIA2-CS-0075 Which countries have borders in the Ka- 克什米尔 Kashimir Kashimir
shimir region?
ACLIA2-CS-0088 What is the relationship between the 断臂山 残破的背山 断臂山
Golden Globe Awards and Broken-back
Mountain?
ACLIA2-CS-0089 What is the relationship between Kenneth 严凯泰 肯尼思日元(观塘 裕隆汽车
Yen(K. T. Yen) and China? 日元)
</figureCaption>
<tableCaption confidence="0.9125675">
Table 8. The differences between Google Translate and VMNET translation of OOV
phrases in which GT or VMNET was wrong.
</tableCaption>
<page confidence="0.996579">
50
</page>
<sectionHeader confidence="0.995849" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999768353846154">
Chan, Y.-C., Chen, K.-H., &amp; Lu, W.-H. (2007).
Extracting and Ranking Question-Focused
Terms Using the Titles of Wikipedia Articles.
Paper presented at the NTCIR-6.
Chen, A., He, J., Xu, L., Gey, F. C., &amp; Meggs, J.
(1997, 1997). Chinese text retrieval without
using a dictionary. Paper presented at the SIGIR
&apos;97: Proceedings of the 20th annual international
ACM SIGIR conference on Research and
development in information retrieval.
Chen, A., Jiang, H., &amp; Gey, F. (2000). Combining
multiple sources for short query translation in
Chinese-English cross-language information
retrieval. 17-23.
Ferrández, S., Toral, A., Ferrández, Ó., Ferrández,
A., &amp; Muñoz, R. (2007). Applying Wikipedia’s
Multilingual Knowledge to Cross–Lingual
Question Answering Natural Language
Processing and Information Systems (pp. 352-
363).
Lu, C., Xu, Y., &amp; Geva, S. (2007). Translation
disambiguation in web-based translation
extraction for English–Chinese CLIR. 819-823.
NTCIR Project. (2010). Tools. from
http://research.nii.ac.jp/ntcir/tools/tools-en.html
Robert, W. P. L., &amp; Kwok, K. L. (2002). A
comparison of Chinese document indexing
strategies and retrieval models. ACM
Transactions on Asian Language Information
Processing (TALIP), 1(3), 225-268.
Sakai, T., Shima, H., Kando, N., Song, R., Lin, C.-
J., Mitamura, T., et al. (2010). Overview of
NTCIR-8 ACLIA IR4QA. Paper presented at the
Proceedings of NTCIR-8, to appear.
Shi, L., Nie, J.-Y., &amp; Cao, G. (2008). RALI
Experiments in IR4QA at NTCIR-7. Paper
presented at the NTCIR-7.
Su, C.-Y., Lin, T.-C., &amp; Wu, S.-H. (2007). Using
Wikipedia to Translate OOV Terms on MLIR.
Paper presented at the NTCIR-6.
Tatsunori Mori, K. T. (2007). A method of Cross-
Lingual Question-Answering Based on Machine
Translation and Noun Phrase Translation using
Web documents. Paper presented at the NTCIR-
6.
The Stanford Natural Language Processing Group.
(2010). Stanford Log-linear Part-Of-Speech
Tagger. from
http://nlp.stanford.edu/software/tagger.shtml
University of Pennsylvania. (2010). POS tags. from
http://bioie.ldc.upenn.edu/wiki/index.php/POS_t
ags
Zhang, Y., &amp; Vines, P. (2004). Using the web for
automated translation extraction in cross-
language information retrieval. Paper presented
at the Proceedings of the 27th annual
international ACM SIGIR conference on
Research and development in information
retrieval.
Zhang, Y., Vines, P., &amp; Zobel, J. (2005). Chinese
OOV translation and post-translation query
expansion in Chinese–English cross-lingual
information retrieval. ACM Transactions on
Asian Language Information Processing
(TALIP), 4(2), 57-77.
</reference>
<page confidence="0.999118">
51
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.430452">
<title confidence="0.982363">Voting Mechanism for Named Entity Translation in</title>
<author confidence="0.693273">Chinese Question Answering</author>
<affiliation confidence="0.995778">of Science and Queensland University of Technology</affiliation>
<email confidence="0.722437">l4.tang@qut.edu.au</email>
<email confidence="0.722437">s.geva@qut.edu.au</email>
<email confidence="0.722437">yue.xu@qut.edu.au</email>
<affiliation confidence="0.998304">of Computer University of Otago</affiliation>
<email confidence="0.939184">andrew@cs.otago.ac.nz</email>
<abstract confidence="0.995383352941177">In this paper, we describe a voting mechanism for accurate named entity translation in question answering (QA). This mechanism involves translations from three different sources: machine translation, online encyclopaedia, and web documents. The translation with the highest number of votes is selected. We evaluated this approach using test collection, topics and assessment results from the NTCIR-8 evaluation forum. This mechanism achieved 95% accuracy in NEs translation and 0.3756 MAP in cross-lingual information retrieval of QA.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Y-C Chan</author>
<author>K-H Chen</author>
<author>W-H Lu</author>
</authors>
<title>Extracting and Ranking Question-Focused Terms Using the Titles of Wikipedia Articles. Paper presented at the NTCIR-6.</title>
<date>2007</date>
<contexts>
<context position="5617" citStr="Chan et al., 2007" startWordPosition="864" endWordPosition="867">sed translation method was shown to be an effective way to solve the OOV phrase problem (Chen et al., 2000; Lu et al., 2007; Zhang &amp; Vines, 2004; Zhang et al., 2005). The idea behind this method is that a term/phrase and its corresponding translation normally co-exist in the same document because authors often provide the new terms’ translation for easy reading. In Wikipedia the language links provided for each entry cover most popular written languages, therefore, it was used to solve a low coverage issue on named entities in EuroWordNet (Ferrández et al., 2007); a number of research groups (Chan et al., 2007; Shi et al., 2008; Su et al., 2007; Tatsunori Mori, 2007) employed Wikipedia to tackle OOV problems in the NTCIR evaluation forum. 3 CLQA Question Analysis Questions for CLQA can be very complex. For example, “What is the relationship between the movie &amp;quot;Riding Alone for Thousands of Miles&amp;quot; and ZHANG Yimou?”. In this example, it is important to recognise two named entities (&amp;quot;Riding Alone for Thousands of Miles&amp;quot; and “ZHANG Yimou”) and to translate them precisely. In order to recognise the NEs in the question, first, English question template phrases in Table 1 are removed from question; next, w</context>
</contexts>
<marker>Chan, Chen, Lu, 2007</marker>
<rawString>Chan, Y.-C., Chen, K.-H., &amp; Lu, W.-H. (2007). Extracting and Ranking Question-Focused Terms Using the Titles of Wikipedia Articles. Paper presented at the NTCIR-6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Chen</author>
<author>J He</author>
<author>L Xu</author>
<author>F C Gey</author>
<author>J Meggs</author>
</authors>
<title>Chinese text retrieval without using a dictionary.</title>
<date>1997</date>
<booktitle>Paper presented at the SIGIR &apos;97: Proceedings of the 20th annual international ACM SIGIR conference on Research and development in information retrieval.</booktitle>
<contexts>
<context position="16023" citStr="Chen et al., 1997" startWordPosition="2577" endWordPosition="2580">n: 什么是电 影之间的关系“^ 0”和“^ 1”? • VMNET obtains the correct translations: 千里走单骑and 张艺谋, for two named entities &amp;quot;Riding Alone for Thousands of Miles&amp;quot; and “ZHANG Yimou” respectively. • Replace the place holders with the correct translations in the second translation and give the final Chinese translation: 什么是 电影之间的关系“千里走单骑”和“张艺 谋”? 5 Information Retrieval 5.1 Chinese Document Processing Approaches to Chinese text indexing vary: Unigrams, bigrams and whole words are all commonly used as tokens. The performance of various IR systems using different segmentation algorithms or techniques varies as well (Chen et al., 1997; Robert &amp; Kwok, 2002). It was seen in prior experiments that using an indexing technique requiring no dictionary can have similar performance to word-based indexing (Chen, et al., 1997). Using bigrams that exhibit high mutual information and unigrams as index terms can achieve good results. Motivated by indexing efficiency and without the need for Chinese text segmentation, we use both bigrams and unigrams as indexing units for our Chinese IR experiments. 5.2 Weighting Model A slightly modified BM25 ranking function was used for document ordering. When calculating the inverse document frequen</context>
</contexts>
<marker>Chen, He, Xu, Gey, Meggs, 1997</marker>
<rawString>Chen, A., He, J., Xu, L., Gey, F. C., &amp; Meggs, J. (1997, 1997). Chinese text retrieval without using a dictionary. Paper presented at the SIGIR &apos;97: Proceedings of the 20th annual international ACM SIGIR conference on Research and development in information retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Chen</author>
<author>H Jiang</author>
<author>F Gey</author>
</authors>
<title>Combining multiple sources for short query translation in Chinese-English cross-language information retrieval.</title>
<date>2000</date>
<pages>17--23</pages>
<contexts>
<context position="5106" citStr="Chen et al., 2000" startWordPosition="777" endWordPosition="780">ated Work In CLIR, retrieving documents with a crosslingual query with out-of-vocabulary phrases has always been difficult. To resolve this problem, an external resource such as Web or Wikipedia is often used to discover the possible translation for the OOV term. Wikipedia and other Web documents are thought of as treasure troves for OOV problem solving because they potentially cover the most recent OOV terms. 1 http://translate.google.com. 2 http://research.nii.ac.jp/ntcir/ntcir-ws8/ws-en.html. The Web-based translation method was shown to be an effective way to solve the OOV phrase problem (Chen et al., 2000; Lu et al., 2007; Zhang &amp; Vines, 2004; Zhang et al., 2005). The idea behind this method is that a term/phrase and its corresponding translation normally co-exist in the same document because authors often provide the new terms’ translation for easy reading. In Wikipedia the language links provided for each entry cover most popular written languages, therefore, it was used to solve a low coverage issue on named entities in EuroWordNet (Ferrández et al., 2007); a number of research groups (Chan et al., 2007; Shi et al., 2008; Su et al., 2007; Tatsunori Mori, 2007) employed Wikipedia to tackle O</context>
</contexts>
<marker>Chen, Jiang, Gey, 2000</marker>
<rawString>Chen, A., Jiang, H., &amp; Gey, F. (2000). Combining multiple sources for short query translation in Chinese-English cross-language information retrieval. 17-23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Ferrández</author>
<author>A Toral</author>
<author>Ó Ferrández</author>
<author>A Ferrández</author>
<author>R Muñoz</author>
</authors>
<title>Applying Wikipedia’s Multilingual Knowledge to Cross–Lingual Question Answering Natural Language Processing and Information Systems</title>
<date>2007</date>
<pages>352--363</pages>
<contexts>
<context position="5569" citStr="Ferrández et al., 2007" startWordPosition="855" endWordPosition="858">earch.nii.ac.jp/ntcir/ntcir-ws8/ws-en.html. The Web-based translation method was shown to be an effective way to solve the OOV phrase problem (Chen et al., 2000; Lu et al., 2007; Zhang &amp; Vines, 2004; Zhang et al., 2005). The idea behind this method is that a term/phrase and its corresponding translation normally co-exist in the same document because authors often provide the new terms’ translation for easy reading. In Wikipedia the language links provided for each entry cover most popular written languages, therefore, it was used to solve a low coverage issue on named entities in EuroWordNet (Ferrández et al., 2007); a number of research groups (Chan et al., 2007; Shi et al., 2008; Su et al., 2007; Tatsunori Mori, 2007) employed Wikipedia to tackle OOV problems in the NTCIR evaluation forum. 3 CLQA Question Analysis Questions for CLQA can be very complex. For example, “What is the relationship between the movie &amp;quot;Riding Alone for Thousands of Miles&amp;quot; and ZHANG Yimou?”. In this example, it is important to recognise two named entities (&amp;quot;Riding Alone for Thousands of Miles&amp;quot; and “ZHANG Yimou”) and to translate them precisely. In order to recognise the NEs in the question, first, English question template phras</context>
</contexts>
<marker>Ferrández, Toral, Ferrández, Ferrández, Muñoz, 2007</marker>
<rawString>Ferrández, S., Toral, A., Ferrández, Ó., Ferrández, A., &amp; Muñoz, R. (2007). Applying Wikipedia’s Multilingual Knowledge to Cross–Lingual Question Answering Natural Language Processing and Information Systems (pp. 352-363).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Lu</author>
<author>Y Xu</author>
<author>S Geva</author>
</authors>
<title>Translation disambiguation in web-based translation extraction for English–Chinese CLIR.</title>
<date>2007</date>
<pages>819--823</pages>
<contexts>
<context position="5123" citStr="Lu et al., 2007" startWordPosition="781" endWordPosition="784">retrieving documents with a crosslingual query with out-of-vocabulary phrases has always been difficult. To resolve this problem, an external resource such as Web or Wikipedia is often used to discover the possible translation for the OOV term. Wikipedia and other Web documents are thought of as treasure troves for OOV problem solving because they potentially cover the most recent OOV terms. 1 http://translate.google.com. 2 http://research.nii.ac.jp/ntcir/ntcir-ws8/ws-en.html. The Web-based translation method was shown to be an effective way to solve the OOV phrase problem (Chen et al., 2000; Lu et al., 2007; Zhang &amp; Vines, 2004; Zhang et al., 2005). The idea behind this method is that a term/phrase and its corresponding translation normally co-exist in the same document because authors often provide the new terms’ translation for easy reading. In Wikipedia the language links provided for each entry cover most popular written languages, therefore, it was used to solve a low coverage issue on named entities in EuroWordNet (Ferrández et al., 2007); a number of research groups (Chan et al., 2007; Shi et al., 2008; Su et al., 2007; Tatsunori Mori, 2007) employed Wikipedia to tackle OOV problems in th</context>
</contexts>
<marker>Lu, Xu, Geva, 2007</marker>
<rawString>Lu, C., Xu, Y., &amp; Geva, S. (2007). Translation disambiguation in web-based translation extraction for English–Chinese CLIR. 819-823.</rawString>
</citation>
<citation valid="true">
<authors>
<author>NTCIR Project</author>
</authors>
<date>2010</date>
<note>Tools. from http://research.nii.ac.jp/ntcir/tools/tools-en.html</note>
<contexts>
<context position="18396" citStr="Project, 2010" startWordPosition="2978" endWordPosition="2979">the translation accuracy is measured using the precision of translated named entities at the topic level. So the translation precision -P is defined as: - P= (3) &amp;quot; where c is the number of topics in which all the named entities are correctly translated; N is the number of topics evaluated. 47 The effectiveness of different translation methods can be further measured by the resulting CLIR performance. In NTCIR-8, CLIR performance is measured using the mean average precision. The MAP values are obtained by running the ir4qa_eval2 toolkit with the assessment results 3 on experimental run s(NTCIR Project, 2010). MAP is computed using only 73 topics due to an insufficient number of relevant document found for the other 27 topics (Sakai et al., 2010). This is the case for all NTCIR-8 ACLIA submissions and not our decision. It also must be noted that there are five topics that have misspelled terms in their English questions. The misspelled terms in those 5 topics are given in Table 3. It is interesting to see how different translations cope with misspelled terms and how this affects the CLIR result. Topic ID Misspelling Correction ACLIA2-CS-0024 Qingling Qinling ACLIA2-CS-0035 Initials D Initial D ACL</context>
</contexts>
<marker>Project, 2010</marker>
<rawString>NTCIR Project. (2010). Tools. from http://research.nii.ac.jp/ntcir/tools/tools-en.html</rawString>
</citation>
<citation valid="true">
<authors>
<author>W P L Robert</author>
<author>K L Kwok</author>
</authors>
<title>A comparison of Chinese document indexing strategies and retrieval models.</title>
<date>2002</date>
<journal>ACM Transactions on Asian Language Information Processing (TALIP),</journal>
<volume>1</volume>
<issue>3</issue>
<pages>225--268</pages>
<contexts>
<context position="16045" citStr="Robert &amp; Kwok, 2002" startWordPosition="2581" endWordPosition="2584">和“^ 1”? • VMNET obtains the correct translations: 千里走单骑and 张艺谋, for two named entities &amp;quot;Riding Alone for Thousands of Miles&amp;quot; and “ZHANG Yimou” respectively. • Replace the place holders with the correct translations in the second translation and give the final Chinese translation: 什么是 电影之间的关系“千里走单骑”和“张艺 谋”? 5 Information Retrieval 5.1 Chinese Document Processing Approaches to Chinese text indexing vary: Unigrams, bigrams and whole words are all commonly used as tokens. The performance of various IR systems using different segmentation algorithms or techniques varies as well (Chen et al., 1997; Robert &amp; Kwok, 2002). It was seen in prior experiments that using an indexing technique requiring no dictionary can have similar performance to word-based indexing (Chen, et al., 1997). Using bigrams that exhibit high mutual information and unigrams as index terms can achieve good results. Motivated by indexing efficiency and without the need for Chinese text segmentation, we use both bigrams and unigrams as indexing units for our Chinese IR experiments. 5.2 Weighting Model A slightly modified BM25 ranking function was used for document ordering. When calculating the inverse document frequency, we use: IDF(qi) = </context>
</contexts>
<marker>Robert, Kwok, 2002</marker>
<rawString>Robert, W. P. L., &amp; Kwok, K. L. (2002). A comparison of Chinese document indexing strategies and retrieval models. ACM Transactions on Asian Language Information Processing (TALIP), 1(3), 225-268.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Sakai</author>
<author>H Shima</author>
<author>N Kando</author>
<author>R Song</author>
<author>C-J Lin</author>
<author>T Mitamura</author>
</authors>
<date>2010</date>
<booktitle>Overview of NTCIR-8 ACLIA IR4QA. Paper presented at the Proceedings of NTCIR-8,</booktitle>
<note>to appear.</note>
<contexts>
<context position="18536" citStr="Sakai et al., 2010" startWordPosition="3001" endWordPosition="3004">-P is defined as: - P= (3) &amp;quot; where c is the number of topics in which all the named entities are correctly translated; N is the number of topics evaluated. 47 The effectiveness of different translation methods can be further measured by the resulting CLIR performance. In NTCIR-8, CLIR performance is measured using the mean average precision. The MAP values are obtained by running the ir4qa_eval2 toolkit with the assessment results 3 on experimental run s(NTCIR Project, 2010). MAP is computed using only 73 topics due to an insufficient number of relevant document found for the other 27 topics (Sakai et al., 2010). This is the case for all NTCIR-8 ACLIA submissions and not our decision. It also must be noted that there are five topics that have misspelled terms in their English questions. The misspelled terms in those 5 topics are given in Table 3. It is interesting to see how different translations cope with misspelled terms and how this affects the CLIR result. Topic ID Misspelling Correction ACLIA2-CS-0024 Qingling Qinling ACLIA2-CS-0035 Initials D Initial D ACLIA2-CS-0066 Kasianov Kasyanov ACLIA2-CS-0074 Northern northern Territories territories ACLIA2-CS-0075 Kashimir Kashmir Table 3. The misspell</context>
<context position="21643" citStr="Sakai, et al., 2010" startWordPosition="3492" endWordPosition="3495">s of all experimental runs corresponding to each query processing technique and Chinese indexing strategy are given in Table 5. The results of mono-lingual runs give benchmarking scores for CLIR runs. As expected, the highest MAP 0.4681 is achieved by the monolingual run VMNET-CSCS-01-T, in which the questions were manually segmented and all the noise words were removed. It is encouraging to see that the automatic run VMNET-CS-CS-02-T with only question template phrase removal has a slightly lower MAP 0.4419 than that (0.4488) of the best performance CS-CS run in the NTCIR-8 evaluation forum (Sakai, et al., 2010). If unigrams were used as the only indexing units, the MAP of VMNET-CS-CS-04-T dropped from 0.4681 to 0.3406. On the other hand, all runs using bigrams as indexing units either exclusively or jointly performed very well. The MAP of run VMNET-CS-CS-05-T using bigrams only is 0.4653, which is slightly 48 lower than that of the top performer run VMNET-CS-CS-01-T, which used two forms of indexing units. However, retrieval performance could be maximised by using both unigrams and bigrams as indexing units. The highest MAP (0.3756) of a CLIR run is achieved by run VMNET-EN-CS-03-T, which used VMNET</context>
<context position="24011" citStr="Sakai, et al., 2010" startWordPosition="3851" endWordPosition="3855"> VMNET-CS-CS-04-T 0.3406 VMNET-CS-CS-05-T 0.4653 NTCIR-8 EN-CS BEST 0.4209 VMNET-EN-CS-01-T 0.3161 VMNET-EN-CS-02-T 0.3408 VMNET-EN-CS-03-T 0.3756 VMNET-EN-CS-04-T 0.3449 Table 5. Results of all experimental runs The different performances between CLIR runs using Google Translate and VMENT is the joint result of the translation improvement and other translation differences. As shown in Table 8, VMNET found the correct translations for 8 more topics than Google Translate. It should be noted that there are two topics (ACLIA2-CS-0008 and ACLIA2-CS-0088) not included in the final CLIR evaluation (Sakai, et al., 2010). Also, there is one phrase, “Kenneth Yen (K. T. Yen) (i�1L�)”, which VMNET couldn’t find the correct translation for, but it detected a highly associated term “Yulon - W�AIF”, an automaker company in Taiwan; Kenneth Yen is the CEO of Yulon. Although Yulon is not a correct translation, it is still a good query term because it is then possible to find the correct answer for the question: “Who is Kenneth Yen?”. However, this topic was not included in the NTCIR-8 IR4QA evaluation. Moreover, it is possible to have multiple explanations for a term. In order to discover as many question-related docu</context>
</contexts>
<marker>Sakai, Shima, Kando, Song, Lin, Mitamura, 2010</marker>
<rawString>Sakai, T., Shima, H., Kando, N., Song, R., Lin, C.-J., Mitamura, T., et al. (2010). Overview of NTCIR-8 ACLIA IR4QA. Paper presented at the Proceedings of NTCIR-8, to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Shi</author>
<author>J-Y Nie</author>
<author>G Cao</author>
</authors>
<date>2008</date>
<note>RALI Experiments in IR4QA at NTCIR-7. Paper presented at the NTCIR-7.</note>
<contexts>
<context position="5635" citStr="Shi et al., 2008" startWordPosition="868" endWordPosition="871">hod was shown to be an effective way to solve the OOV phrase problem (Chen et al., 2000; Lu et al., 2007; Zhang &amp; Vines, 2004; Zhang et al., 2005). The idea behind this method is that a term/phrase and its corresponding translation normally co-exist in the same document because authors often provide the new terms’ translation for easy reading. In Wikipedia the language links provided for each entry cover most popular written languages, therefore, it was used to solve a low coverage issue on named entities in EuroWordNet (Ferrández et al., 2007); a number of research groups (Chan et al., 2007; Shi et al., 2008; Su et al., 2007; Tatsunori Mori, 2007) employed Wikipedia to tackle OOV problems in the NTCIR evaluation forum. 3 CLQA Question Analysis Questions for CLQA can be very complex. For example, “What is the relationship between the movie &amp;quot;Riding Alone for Thousands of Miles&amp;quot; and ZHANG Yimou?”. In this example, it is important to recognise two named entities (&amp;quot;Riding Alone for Thousands of Miles&amp;quot; and “ZHANG Yimou”) and to translate them precisely. In order to recognise the NEs in the question, first, English question template phrases in Table 1 are removed from question; next, we use the Stanford</context>
</contexts>
<marker>Shi, Nie, Cao, 2008</marker>
<rawString>Shi, L., Nie, J.-Y., &amp; Cao, G. (2008). RALI Experiments in IR4QA at NTCIR-7. Paper presented at the NTCIR-7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-Y Su</author>
<author>T-C Lin</author>
<author>S-H Wu</author>
</authors>
<date>2007</date>
<booktitle>Using Wikipedia to Translate OOV Terms on MLIR. Paper presented at the NTCIR-6.</booktitle>
<contexts>
<context position="5652" citStr="Su et al., 2007" startWordPosition="872" endWordPosition="875">e an effective way to solve the OOV phrase problem (Chen et al., 2000; Lu et al., 2007; Zhang &amp; Vines, 2004; Zhang et al., 2005). The idea behind this method is that a term/phrase and its corresponding translation normally co-exist in the same document because authors often provide the new terms’ translation for easy reading. In Wikipedia the language links provided for each entry cover most popular written languages, therefore, it was used to solve a low coverage issue on named entities in EuroWordNet (Ferrández et al., 2007); a number of research groups (Chan et al., 2007; Shi et al., 2008; Su et al., 2007; Tatsunori Mori, 2007) employed Wikipedia to tackle OOV problems in the NTCIR evaluation forum. 3 CLQA Question Analysis Questions for CLQA can be very complex. For example, “What is the relationship between the movie &amp;quot;Riding Alone for Thousands of Miles&amp;quot; and ZHANG Yimou?”. In this example, it is important to recognise two named entities (&amp;quot;Riding Alone for Thousands of Miles&amp;quot; and “ZHANG Yimou”) and to translate them precisely. In order to recognise the NEs in the question, first, English question template phrases in Table 1 are removed from question; next, we use the Stanford NLP POS tagger (</context>
</contexts>
<marker>Su, Lin, Wu, 2007</marker>
<rawString>Su, C.-Y., Lin, T.-C., &amp; Wu, S.-H. (2007). Using Wikipedia to Translate OOV Terms on MLIR. Paper presented at the NTCIR-6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tatsunori Mori</author>
<author>K T</author>
</authors>
<title>A method of CrossLingual Question-Answering Based on Machine Translation and Noun Phrase Translation using Web documents. Paper presented at the NTCIR6.</title>
<date>2007</date>
<marker>Mori, T, 2007</marker>
<rawString>Tatsunori Mori, K. T. (2007). A method of CrossLingual Question-Answering Based on Machine Translation and Noun Phrase Translation using Web documents. Paper presented at the NTCIR6.</rawString>
</citation>
<citation valid="true">
<title>The Stanford Natural Language Processing Group.</title>
<date>2010</date>
<note>from http://nlp.stanford.edu/software/tagger.shtml</note>
<marker>2010</marker>
<rawString>The Stanford Natural Language Processing Group. (2010). Stanford Log-linear Part-Of-Speech Tagger. from http://nlp.stanford.edu/software/tagger.shtml</rawString>
</citation>
<citation valid="false">
<date>2010</date>
<institution>University of Pennsylvania.</institution>
<note>POS tags. from http://bioie.ldc.upenn.edu/wiki/index.php/POS_t ags</note>
<marker>2010</marker>
<rawString>University of Pennsylvania. (2010). POS tags. from http://bioie.ldc.upenn.edu/wiki/index.php/POS_t ags</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhang</author>
<author>P Vines</author>
</authors>
<title>Using the web for automated translation extraction in crosslanguage information retrieval.</title>
<date>2004</date>
<booktitle>Paper presented at the Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval.</booktitle>
<contexts>
<context position="5144" citStr="Zhang &amp; Vines, 2004" startWordPosition="785" endWordPosition="788">nts with a crosslingual query with out-of-vocabulary phrases has always been difficult. To resolve this problem, an external resource such as Web or Wikipedia is often used to discover the possible translation for the OOV term. Wikipedia and other Web documents are thought of as treasure troves for OOV problem solving because they potentially cover the most recent OOV terms. 1 http://translate.google.com. 2 http://research.nii.ac.jp/ntcir/ntcir-ws8/ws-en.html. The Web-based translation method was shown to be an effective way to solve the OOV phrase problem (Chen et al., 2000; Lu et al., 2007; Zhang &amp; Vines, 2004; Zhang et al., 2005). The idea behind this method is that a term/phrase and its corresponding translation normally co-exist in the same document because authors often provide the new terms’ translation for easy reading. In Wikipedia the language links provided for each entry cover most popular written languages, therefore, it was used to solve a low coverage issue on named entities in EuroWordNet (Ferrández et al., 2007); a number of research groups (Chan et al., 2007; Shi et al., 2008; Su et al., 2007; Tatsunori Mori, 2007) employed Wikipedia to tackle OOV problems in the NTCIR evaluation fo</context>
</contexts>
<marker>Zhang, Vines, 2004</marker>
<rawString>Zhang, Y., &amp; Vines, P. (2004). Using the web for automated translation extraction in crosslanguage information retrieval. Paper presented at the Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhang</author>
<author>P Vines</author>
<author>J Zobel</author>
</authors>
<title>Chinese OOV translation and post-translation query expansion in Chinese–English cross-lingual information retrieval.</title>
<date>2005</date>
<journal>ACM Transactions on Asian Language Information Processing (TALIP),</journal>
<volume>4</volume>
<issue>2</issue>
<pages>57--77</pages>
<contexts>
<context position="5165" citStr="Zhang et al., 2005" startWordPosition="789" endWordPosition="792">al query with out-of-vocabulary phrases has always been difficult. To resolve this problem, an external resource such as Web or Wikipedia is often used to discover the possible translation for the OOV term. Wikipedia and other Web documents are thought of as treasure troves for OOV problem solving because they potentially cover the most recent OOV terms. 1 http://translate.google.com. 2 http://research.nii.ac.jp/ntcir/ntcir-ws8/ws-en.html. The Web-based translation method was shown to be an effective way to solve the OOV phrase problem (Chen et al., 2000; Lu et al., 2007; Zhang &amp; Vines, 2004; Zhang et al., 2005). The idea behind this method is that a term/phrase and its corresponding translation normally co-exist in the same document because authors often provide the new terms’ translation for easy reading. In Wikipedia the language links provided for each entry cover most popular written languages, therefore, it was used to solve a low coverage issue on named entities in EuroWordNet (Ferrández et al., 2007); a number of research groups (Chan et al., 2007; Shi et al., 2008; Su et al., 2007; Tatsunori Mori, 2007) employed Wikipedia to tackle OOV problems in the NTCIR evaluation forum. 3 CLQA Question </context>
</contexts>
<marker>Zhang, Vines, Zobel, 2005</marker>
<rawString>Zhang, Y., Vines, P., &amp; Zobel, J. (2005). Chinese OOV translation and post-translation query expansion in Chinese–English cross-lingual information retrieval. ACM Transactions on Asian Language Information Processing (TALIP), 4(2), 57-77.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>