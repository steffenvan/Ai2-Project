<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.9978065">
Automatic Classification of Semantic Relations
between Facts and Opinions
</title>
<author confidence="0.988076">
Koji Murakami† Eric Nichols$ Junta Mizuno†$ Yotaro Watanabe$
</author>
<affiliation confidence="0.7289615">
Hayato Goto† Megumi Ohki† Suguru Matsuyoshi† Kentaro Inui$ Yuji Matsumoto††Nara Institute of Science and Technology
$Tohoku University
</affiliation>
<email confidence="0.99243">
{kmurakami,matuyosi,hayato-g,megumi-o,matsu}@is.naist.jp
{eric-n,junta-m,inui}@ecei.tohoku.ac.jp
</email>
<sectionHeader confidence="0.997346" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999933846153846">
Classifying and identifying semantic re-
lations between facts and opinions on
the Web is of utmost importance for or-
ganizing information on the Web, how-
ever, this requires consideration of a
broader set of semantic relations than are
typically handled in Recognizing Tex-
tual Entailment (RTE), Cross-document
Structure Theory (CST), and similar
tasks. In this paper, we describe the con-
struction and evaluation of a system that
identifies and classifies semantic rela-
tions in Internet data. Our system targets
a set of semantic relations that have been
inspired by CST but that have been gen-
eralized and broadened to facilitate ap-
plication to mixed fact and opinion data
from the Internet. Our system identi-
fies these semantic relations in Japanese
Web texts using a combination of lexical,
syntactic, and semantic information and
evaluate our system against gold stan-
dard data that was manually constructed
for this task. We will release all gold
standard data used in training and eval-
uation of our system this summer.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99996578">
The task of organizing the information on the In-
ternet to help users find facts and opinions on
their topics of interest is increasingly important
as more people turn to the Web as a source of
important information. The vast amounts of re-
search conducted in NLP on automatic summa-
rization, opinion mining, and question answer-
ing are illustrative of the great interest in mak-
ing relevant information easier to find. Provid-
ing Internet users with thorough information re-
quires recognizing semantic relations between
both facts and opinions, however the assump-
tions made by current approaches are often in-
compatible with this goal. For example, the
existing semantic relations considered in Rec-
ognizing Textual Entailment (RTE) (Dagan et
al., 2005) are often too narrow in scope to be
directly applicable to text on the Internet, and
theories like Cross-document Structure Theory
(CST) (Radev, 2000) are only applicable to facts
or second-hand reporting of opinions rather than
relations between both.
As part of the STATEMENT MAP project we
proposed the development of a system to sup-
port information credibility analysis on the Web
(Murakami et al., 2009b) by automatically sum-
marizing facts and opinions on topics of inter-
est to users and showing them the evidence and
conflicts for each viewpoint. To facilitate the de-
tection of semantic relations in Internet data, we
defined a sentence-like unit of information called
the statement that encompasses both facts and
opinions, started compiling a corpus of state-
ments annotated with semantic relations (Mu-
rakami et al., 2009a), and begin constructing a
system to automatically identify semantic rela-
tions between statements.
In this paper, we describe the construction and
evaluation of a prototype semantic relation iden-
tification system. We build on the semantic rela-
tions proposed in RTE and CST and in our pre-
vious work, refining them into a streamlined set
of semantic relations that apply across facts and
opinions, but that are simple enough to make
automatic recognition of semantic relations be-
tween statements in Internet text possible.Our
semantic relations are [AGREEMENT], [CON-
FLICT], [CONFINEMENT], and [EVIDENCE].
[AGREEMENT] and [CONFLICT] are expansions
of the [EQUIVALENCE] and [CONTRADICTION]
</bodyText>
<page confidence="0.992428">
21
</page>
<note confidence="0.737688">
Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 21–30,
Beijing, August 2010
</note>
<bodyText confidence="0.999128363636364">
relations used in RTE. [CONFINEMENT] and
[EVIDENCE] are new relations between facts
and opinions that are essential for understanding
how statements on a topic are inter-related.
Our task differs from opinion mining and sen-
timent analysis which largely focus on identify-
ing the polarity of an opinion for defined param-
eters rather than identify how facts and opinions
relate to each other, and it differs from web doc-
ument summarization tasks which focus on ex-
tracting information from web page structure and
contextual information from hyperlinks rather
than analyzing the semantics of the language on
the webpage itself.
We present a system that automatically iden-
tifies semantic relations between statements in
Japanese Internet texts. Our system uses struc-
tural alignment to identify statement pairs that
are likely to be related, then classifies seman-
tic relations using a combination of lexical, syn-
tactic, and semantic information. We evaluate
cross-statement semantic relation classification
on sentence pairs that were taken from Japanese
Internet texts on several topics and manually an-
notated with a semantic relation where one is
present. In our evaluation, we look closely at the
impact that each of the resources has on semantic
relation classification quality.
The rest of this paper is organized as follows.
In Section 2, we discuss related work in summa-
rization, semantic relation classification, opinion
mining, and sentiment analysis, showing how
existing classification schemes are insufficient
for our task. In Section 3, we introduce a set of
cross-sentential semantic relations for use in the
opinion classification needed to support informa-
tion credibility analysis on the Web. In Section
4, we present our cross-sentential semantic re-
lation recognition system, and discuss the algo-
rithms and resources that are employed. In Sec-
tion 5, we evaluate our system in a semantic rela-
tion classification task. In Section 6, we discuss
our findings and conduct error analysis. Finally,
we conclude the paper in Section 7.
</bodyText>
<sectionHeader confidence="0.999978" genericHeader="introduction">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.999612">
2.1 Recognizing Textual Entailment
</subsectionHeader>
<bodyText confidence="0.999936864864865">
Identifying logical relations between texts is the
focus of Recognizing Textual Entailment, the
task of deciding whether the meaning of one
text is entailed from another text. A major
task in the RTE Challenge (Recognizing Tex-
tual Entailment Challenge) is classifying the se-
mantic relation between a Text (T) and a Hy-
pothesis (H) into [ENTAILMENT], [CONTRA-
DICTION], or [UNKNOWN]. Over the last sev-
eral years, several corpora annotated with thou-
sands of (T,H) pairs have been constructed for
this task. In these corpora, each pair was tagged
indicating its related task (e.g. Information Ex-
traction, Question Answering, Information Re-
trieval or Summarization).
The RTE Challenge has successfully em-
ployed a variety of techniques in order to rec-
ognize instances of textual entailment, including
methods based on: measuring the degree of lex-
ical overlap between bag of words (Glickman
et al., 2005; Jijkoun and de Rijke, 2005), the
alignment of graphs created from syntactic or se-
mantic dependencies (Marsi and Krahmer, 2005;
MacCartney et al., 2006), statistical classifiers
which leverage a wide range of features (Hickl
et al., 2005), or reference rule generation (Szpek-
tor et al., 2007). These approaches have shown
great promise in RTE for entailment pairs in the
corpus, but more robust models of recognizing
logical relations are still desirable.
The definition of contradiction in RTE is that
T contradicts H if it is very unlikely that both T
and H can be true at the same time. However, in
real documents on the Web, there are many pairs
of examples which are contradictory in part, or
where one statement confines the applicability of
another, as shown in the examples in Table 1.
</bodyText>
<subsectionHeader confidence="0.999855">
2.2 Cross-document Structure Theory
</subsectionHeader>
<bodyText confidence="0.999353157894737">
Cross-document Structure Theory (CST), devel-
oped by Radev (2000), is another task of rec-
ognizing semantic relations between sentences.
CST is an expanded rhetorical structure analy-
sis based on Rhetorical Structure Theory (RST:
(William and Thompson, 1988)), and attempts
to describe the semantic relations that exist
between two or more sentences from differ-
ent source documents that are related to the
same topic, as well as those that come from
a single source document. A corpus of cross-
document sentences annotated with CST rela-
tions has also been constructed (The CSTBank
Corpus: (Radev et al., 2003)). CSTBank is
organized into clusters of topically-related ar-
ticles. There are 18 kinds of semantic rela-
tions in this corpus, not limited to [EQUIVA-
LENCE] or [CONTRADICTION], but also includ-
ing [JUDGEMENT], [ELABORATION], and [RE-
</bodyText>
<page confidence="0.983765">
22
</page>
<equation confidence="0.90610696">
Query Matching sentences Output
キシリトールは虫歯予防に効果
がある
Xylitol is effective at preventing
cavities.
キシリトールの含まれている量が多いほどむし歯予防の効果は高いようです 同意
The cavity-prevention effects are greater the more Xylitol is included. [AGREEMENT].
キシリトールがお口の健康維持や虫歯予防にも効果を発揮します 同意
Xylitol shows effectiveness at maintaining good oral hygiene and preventing cavities. [AGREEMENT]
キシリトールの虫歯抑制効果についてはいろいろな意見がありますが実際は効 対立
果があるわけではありません
There are many opinions about the cavity-prevention effectiveness of Xylitol, but it [CONFLICT]
is not really effective.
弱アルカリ性のアルカリイオン還元水があなたと家族の健康を支えます 同意
還元水は健康に良い Reduced water, which has weak alkaline ions, supports the health of you and your [AGREEMENT]
family.
Reduced water is good for the
health.
還元水は活性酸素を除去すると言われ健康を維持してくれる働きをもたらす 同意
Reduced water is said to remove active oxygen from the body, making it effective at [AGREEMENT]
promoting good health.
美味しくても酸化させる水は健康には役立ちません 対立
Even if oxidized water tastes good, it does not help one’s health. [CONFLICT]
イソフラボンは健康維持に効果 大豆イソフラボンをサプリメントで過剰摂取すると健康維持には負の影響を与 限定
がある える結果となります
</equation>
<bodyText confidence="0.517315">
Isoflavone is effective at Taking too much soy isoflavone as a supplement will have a negative effect on one’s [CONFINEMENT]
maintaining good health. health
</bodyText>
<tableCaption confidence="0.998481">
Table 1: Example semantic relation classification.
</tableCaption>
<bodyText confidence="0.985228285714286">
FINEMENT]. Etoh et al. (Etoh and Okumura,
2005) constructed a Japanese Cross-document
Relation Corpus, and they redefined 14 kinds of
semantic relations in their corpus.
CST was designed for objective expressions
because its target data is newspaper articles re-
lated to the same topic. Facts, which can be ex-
tracted from newspaper articles, have been used
in conventional NLP research, such as Informa-
tion Extraction or Factoid Question Answering.
However, there are a lot of opinions on the Web,
and it is important to survey opinions in addition
to facts to give Internet users a comprehensive
view of the discussions on topics of interest.
</bodyText>
<subsectionHeader confidence="0.993297">
2.3 Cross-document Summarization Based
on CST Relations between Sentences
</subsectionHeader>
<bodyText confidence="0.999935823529412">
Zhang and Radev (2004) attempted to classify
CST relations between sentence pairs extracted
from topically related documents. However, they
used a vector space model and tried multi-class
classification. The results were not satisfactory.
This observation may indicate that the recog-
nition methods for each relation should be de-
veloped separately. Miyabe et al. (2008) at-
tempted to recognize relations that were defined
in a Japanese cross-document relation corpus
(Etoh and Okumura, 2005). However, their tar-
get relations were limited to [EQUIVALENCE]
and [TRANSITION]; other relations were not tar-
geted. Recognizing [EVIDENCE] is indispens-
able for organizing information on the Internet.
We need to develop satisfactory methods of [EV-
IDENCE] recognition.
</bodyText>
<subsectionHeader confidence="0.998669">
2.4 Opinion Mining and Sentiment Analysis
</subsectionHeader>
<bodyText confidence="0.999984387096774">
Subjective statements, such as opinions, have
recently been the focus of much NLP re-
search including review analysis, opinion ex-
traction, opinion question answering, and senti-
ment analysis. In the corpus constructed in the
Multi-Perspective Question Answering (MPQA)
Project (Wiebe et al., 2005), individual expres-
sions are tagged that correspond to explicit men-
tions of private states, speech event, and expres-
sive subjective elements.
The goal of opinion mining to extract expres-
sions with polarity from texts, not to recognize
semantic relations between sentences. Sentiment
analysis also focus classifying subjective expres-
sions in texts into positive/negative classes. In
comparison, although we deal with sentiment in-
formation in text, our objective is to recognize
semantic relations between sentences. If a user’s
query requires positive/negative information, we
will also need to extract sentences including sen-
timent expression like in opinion mining, how-
ever, our semantic relation, [CONFINEMENT], is
more precise because it identifies the condition
or scope of the polarity. Queries do not neces-
sarily include sentiment information; we also ac-
cept queries that are intended to be a statement
of fact. For example, for the query “Xylitol is
effective at preventing cavities.” in Table 1, we
extract a variety of sentences from the Web and
recognize semantic relations between the query
and many kinds of sentences.
</bodyText>
<page confidence="0.996128">
23
</page>
<sectionHeader confidence="0.962713" genericHeader="method">
3 Semantic Relations between
Statements
</sectionHeader>
<bodyText confidence="0.999966571428571">
In this section, we define the semantic relations
that we will classify in Japanese Internet texts as
well as their corresponding relations in RTE and
CST. Our goal is to define semantic relations that
are applicable over both fact and opinions, mak-
ing them more appropriate for handling Internet
texts. See Table 1 for real examples.
</bodyText>
<subsectionHeader confidence="0.955535">
3.1 [AGREEMENT]
</subsectionHeader>
<bodyText confidence="0.99986375">
A bi-directional relation where statements have
equivalent semantic content on a shared topic.
Here we use topic in a narrow sense to mean that
the semantic contents of both statements are rel-
evant to each other.
The following is an example of [AGREE-
MENT] on the topic of bio-ethanol environmental
impact.
</bodyText>
<listItem confidence="0.779261">
(1) a. Bio-ethanol is good for the environment.
b. Bio-ethanol is a high-quality fuel, and it
has the power to deal with the environ-
ment problems that we are facing.
</listItem>
<bodyText confidence="0.954614857142857">
Once relevance has been established,
[AGREEMENT] can range from strict logi-
cal entailment or identical polarity of opinions.
Here is an example of two statements that
share a broad topic, but that are not classified as
[AGREEMENT] because preventing cavities and
tooth calcification are not intuitively relevant.
</bodyText>
<listItem confidence="0.97018">
(2) a. Xylitol is effective at preventing cavities.
b. Xylitol advances tooth calcification.
</listItem>
<subsectionHeader confidence="0.788049">
3.2 [CONFLICT]
</subsectionHeader>
<bodyText confidence="0.9994312">
A bi-directional relation where statements have
negative or contradicting semantic content on a
shared topic. This can range from strict logical
contradiction to opposite polarity of opinions.
The next pair is a [CONFLICT] example.
</bodyText>
<listItem confidence="0.91524825">
(3) a. Bio-ethanol is good for our earth.
b. There is a fact that bio-ethanol further the
destruction of the environment.
3.3 [EVIDENCE]
</listItem>
<bodyText confidence="0.814976083333333">
A uni-directional relation where one statement
provides justification or supporting evidence for
the other. Both statements can be either facts or
opinions. The following is a typical example:
(4) a. I believe that applying the technology of
cloning must be controlled by law.
b. There is a need to regulate cloning, be-
cause it can be open to abuse.
The statement containing the evidence con-
sists of two parts: one part has a [AGREEMENT]
or [CONFLICT] with the other statement, the
other part provides support or justification for it.
</bodyText>
<subsectionHeader confidence="0.748991">
3.4 [CONFINEMENT]
</subsectionHeader>
<bodyText confidence="0.999693166666667">
A uni-directional relation where one statement
provides more specific information about the
other or quantifies the situations in which it ap-
plies. The pair below is an example, in which
one statement gives a condition under which the
other can be true.
</bodyText>
<listItem confidence="0.952327">
(5) a. Steroids have side-effects.
</listItem>
<bodyText confidence="0.901585333333333">
b. There is almost no need to worry about
side-effects when steroids are used for lo-
cal treatment.
</bodyText>
<sectionHeader confidence="0.971932" genericHeader="method">
4 Recognizing Semantic Relations
</sectionHeader>
<bodyText confidence="0.999978296296297">
In order to organize the information on the
Internet, we need to identify the [AGREE-
MENT], [CONFLICT], [CONFINEMENT], and
[EVIDENCE] semantic relations. Because iden-
tification of [AGREEMENT] and [CONFLICT] is
a problem of measuring semantic similarity be-
tween two statements, it can be cast as a sen-
tence alignment problem and solved using an
RTE framework. The two sentences do not need
to be from the same source.
However, the identification of [CONFINE-
MENT] and [EVIDENCE] relations depend on
contextual information in the sentence. For ex-
ample, conditional statements or specific dis-
course markers like “because” act as important
cues for their identification. Thus, to identify
these two relations across documents, we must
first identify [AGREEMENT] or [CONFLICT] be-
tween sentences in different documents and then
determine if there is a [CONFINEMENT] or [EV-
IDENCE] relation in one of the documents.
Furthermore, the surrounding text often con-
tains contextual information that is important for
identifying these two relations. Proper handling
of surrounding context requires discourse analy-
sis and is an area of future work, but our basic
detection strategy is as follows:
</bodyText>
<listItem confidence="0.99597125">
1. Identify a [AGREEMENT] or [CONFLICT] re-
lation between the Query and Text
2. Search the Text sentence for cues that iden-
tify [CONFINEMENT] or [EVIDENCE]
</listItem>
<page confidence="0.978907">
24
</page>
<figure confidence="0.999818078947369">
B
C
897:;
&amp; 85# )&amp; 9 3-$)3/$+3
J
3&apos; #)*$
()BC- : B
#$)23# )..2%5)0&apos;+&amp;
&lt;=&gt;?@A/
#)&amp; :$$+ &amp;#&apos;;+
&amp;quot;#$%&amp;&apos;
!
&amp;quot;#$ %&amp;&apos;()*&apos;+$
1234
0
3$&apos;.&apos;-&apos;&amp;%&amp;
&apos;&amp;
6 7
67
5
-%&apos;8&amp;
*)
!&amp;quot; #$%&amp;&apos;
&amp;quot;#$ %&amp;&apos;()*&apos;+$
J
)
J
()*
,&apos;- .-&apos;/&apos;0+1 #$)23#
B
+,-
$4$50*$
C
5
C
./
%&amp;
?
</figure>
<figureCaption confidence="0.999917">
Figure 1: An example of structural alignment
</figureCaption>
<bodyText confidence="0.994947923076923">
sumoto, 2002) and the predicate-argument struc-
ture analyzer ChaPAS (Watanabe et al., 2010).
CaboCha splits the Japanese text into phrase-like
chunks and represents syntactic dependencies
between the chunks as edges in a graph. Cha-
PAS identifies predicate-argument structures in
the dependency graph produced by CaboCha.
We also conduct extended modality analysis
using the resources provided by Matsuyoshi et
al. (2010), focusing on tense, modality, and po-
larity, because such information provides impor-
tant clues for the recognition of semantic rela-
tions between statements.
</bodyText>
<subsectionHeader confidence="0.959369">
4.3 Structural Alignment
</subsectionHeader>
<bodyText confidence="0.992291">
In this section, we describe our approach to
structural alignment. The structural alignment
process is shown in Figure 1. It consists of the
following two phases:
</bodyText>
<listItem confidence="0.3492025">
1. lexical alignment
2. structural alignment
</listItem>
<bodyText confidence="0.982932833333333">
We developed a heuristic-based algorithm to
align chunk based on lexical similarity infor-
mation. We incorporate the following informa-
tion into an alignment confidence score that has
a range of 0.0-1.0 and align chunk whose scores
cross an empirically-determined threshold.
</bodyText>
<listItem confidence="0.8804885">
• surface level similarity: identical content
words or cosine similarity of chunk contents
• semantic similarity of predicate-argument
structures
</listItem>
<bodyText confidence="0.97119025">
predicates we check for matches in predi-
cate entailment databases (Hashimoto et
al., 2009; Matsuyoshi et al., 2008) consid-
ering the default case frames reported by
ChaPAS
arguments we check for synonym or hy-
pernym matches in the Japanese WordNet
(2008) or the Japanese hypernym collec-
tion of Sumida et al. (2008)
3. Infer the applicability of the [CONFINE-
MENT] or [EVIDENCE] relations in the Text
to the Query
</bodyText>
<subsectionHeader confidence="0.99428">
4.1 System Overview
</subsectionHeader>
<bodyText confidence="0.9994285">
We have finished constructing a prototype sys-
tem that detects semantic relation between state-
ments. It has a three-stage architecture similar to
the RTE system of MacCartney et al. (2006):
</bodyText>
<listItem confidence="0.9996114">
1. Linguistic analysis
2. Structural alignment
3. Feature extraction for detecting [EVIDENCE]
and [CONFINEMENT]
4. Semantic relation classification
</listItem>
<bodyText confidence="0.999971448275862">
However, we differ in the following respects.
First, our relation classification is broader than
RTE’s simple distinction between [ENTAIL-
MENT], [CONTRADICTION], and [UNKNOWN];
in place of [ENTAILMENT] and [CONTRA-
DICTION, we use broader [AGREEMENT] and
[CONFLICT] relations. We also consider cover
gradations of applicability of statements with the
[CONFINEMENT] relation.
Second, we conduct structural alignment with
the goal of aligning semantic structures. We do
this by directly incorporating dependency align-
ments and predicate-argument structure informa-
tion for both the user query and the Web text
into the alignment scoring process. This allows
us to effectively capture many long-distance
alignments that cannot be represented as lexical
alignments. This contrasts with MacCartney et
al. (2006), who uses dependency structures for
the Hypothesis to reduce the lexical alignment
search space but do not produce structural align-
ments and do not use the dependencies in detect-
ing entailment.
Finally, we apply several rich semantic re-
sources in alignment and classification: extended
modality information that helps align and clas-
sify structures that are semantically similar but
divergent in tense or polarity; and lexical simi-
larity through ontologies like WordNet.
</bodyText>
<subsectionHeader confidence="0.994068">
4.2 Linguistic Analysis
</subsectionHeader>
<bodyText confidence="0.994730857142857">
In order to identify semantic relations between
the user query (Q) and the sentence extracted
from Web text (T), we first conduct syntactic and
semantic linguistic analysis to provide a basis for
alignment and relation classification.
For syntactic analysis, we use the Japanese
dependency parser CaboCha (Kudo and Mat-
</bodyText>
<page confidence="0.998272">
25
</page>
<figureCaption confidence="0.9797195">
Figure 2: Determining the compatibility of se-
mantic structures
</figureCaption>
<bodyText confidence="0.997976923076923">
We compare the predicate-argument structure
of the query to that of the text and determine
if the argument structures are compatible. This
process is illustrated in Figure 2 where the T(ext)
“Agricultural chemicals are used in the field.” is
aligned with the H(ypothesis) “Over the field,
agricultural chemicals are sprayed.” Although
the verbs used and sprayed are not directly se-
mantically related, they are aligned because they
share the same argument structures. This lets up
align predicates for which we lack semantic re-
sources. We use the following information to de-
termine predicate-argument alignment:
</bodyText>
<listItem confidence="0.990938333333333">
• the number of aligned children
• the number of aligned case frame arguments
• the number of possible alignments in a win-
dow of n chunk
• predicates indicating existence or quantity.
E.g. many, few, to exist, etc.
• polarity of both parent and child chunks us-
ing the resources in (Higashiyama et al.,
2008; Kobayashi et al., 2005)
</listItem>
<bodyText confidence="0.999845">
We treat structural alignment as a machine
learning problem and train a Support Vector Ma-
chine (SVM) model to decide if lexically aligned
chunks are semantically aligned.
We train on gold-standard labeled alignment
of 370 sentence pairs. This data set is described
in more detail in Section 5.1. As features for our
SVM model, we use the following information:
</bodyText>
<listItem confidence="0.998797615384615">
• the distance in edges in the dependency graph
between parent and child for both sentences
• the distance in chunks between parent and
child in both sentences
• binary features indicating whether each
chunk is a predicate or argument according
to ChaPAS
• the parts-of-speech of first and last word in
each chunk
• when the chunk ends with a case marker, the
case of the chunk, otherwise none
• the lexical alignment score of each chunk
pair
</listItem>
<subsectionHeader confidence="0.8574325">
4.4 Feature Extraction for Detecting
Evidence and Confinement
</subsectionHeader>
<bodyText confidence="0.999965833333333">
Once the structural alignment system has iden-
tified potential [AGREEMENT] or [CONFLICT]
relations, we need to extract contextual cues in
the Text as features for detecting [CONFINE-
MENT] and [EVIDENCE] relations. Conditional
statements, degree adverbs, and partial negation,
which play a role in limiting the scope or degree
of a query’s contents in the statement, can be im-
portant cues for detecting the these two semantic
relations. We currently use a set of heuristics to
extract a set of expressions to use as features for
classifying these relations using SVM models.
</bodyText>
<subsectionHeader confidence="0.925612">
4.5 Relation Classification
</subsectionHeader>
<bodyText confidence="0.99699664516129">
Once the structural alignment is successfully
identified, the task of semantic relation classi-
fication is straightforward. We also solve this
problem with machine learning by training an
SVM classifier. As features, we draw on a com-
bination of lexical, syntactic, and semantic infor-
mation including the syntactic alignments from
the previous section. The feature set is:
alignments We define two binary function,
ALIGNword(qi, tm) for the lexical align-
ment and ALIGNstruct((qi, qj), (tm, tk))
for the structural alignment to be true if and
only if the node qi, qj E Q has been seman-
tically and structurally aligned to the node
tm, tk E T. Q and T are the (Q)uery and the
(T)ext, respectively. We also use a separate
feature for a score representing the likelihood
of the alignment.
modality We have a feature that encodes all of
the possible polarities of a predicate node
from modality analysis, which indicates the
utterance type, and can be assertive, voli-
tional, wish, imperative, permissive, or in-
terrogative. Modalities that do not repre-
sent opinions (i.e. imperative, permissive and
interrogative) often indicate [OTHER] rela-
tions.
antonym We define a binary function
ANTONY M(qi, tm) that indicates if
the pair is identified as an antonym. This
information helps identify [CONFLICT].
</bodyText>
<figure confidence="0.910388">
a z A ;P__ Vft7@
(field) (in) (agricultural chemicals) (ACC)
�
a (Z A ;P__
(field) (on)(agricultural chemicals) (ACC) (spray)
(use)
ak_tt7@
T :
H :
</figure>
<page confidence="0.986512">
26
</page>
<table confidence="0.9999668">
Relation Measure 3-class Cascaded 3-class ∆
[AGREEMENT] precision 0.79 (128 / 162) 0.80 (126 / 157) +0.01
[AGREEMENT] recall 0.86 (128 / 149) 0.85 (126 / 149) -0.01
[AGREEMENT] f-score 0.82 0.82 -
[CONFLICT] precision 0 (0 / 5) 0.36 (5 / 14) +0.36
[CONFLICT] recall 0 (0 / 12) 0.42 (5 / 12) +0.42
[CONFLICT] f-score 0 0.38 +0.38
[CONFINEMENT] precision 0.4 (4 / 10) 0.8 (4 / 5) +0.4
[CONFINEMENT] recall 0.17 (4 / 23) 0.17 (4 / 23) -
[CONFINEMENT] f-score 0.24 0.29 +0.05
</table>
<tableCaption confidence="0.998057">
Table 2: Semantic relation classification results comparing 3-class and cascaded 3-class approaches
</tableCaption>
<bodyText confidence="0.999691857142857">
negation To identify negations, we primar-
ily rely on a predicate’s Actuality value,
which represents epistemic modality and
existential negation. If a predicate pair
ALIGNword(qi, tm) has mismatching actu-
ality labels, the pair is likely a [OTHER].
contextual cues This set of features is used to
mark the presence of any contextual cues
that identify of [CONFINEMENT] or [EVI-
DENCE] relations in a chunk . For example,
“0)&apos;2 (because)” or “k0) (due to)” are typ-
ical contextual cues for [EVIDENCE], and “
L2� (when)” or “4S1;6 (if)” are typical for
[CONFINEMENT].
</bodyText>
<sectionHeader confidence="0.999559" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.99938">
5.1 Data Preparation
</subsectionHeader>
<bodyText confidence="0.9997808">
In order to evaluate our semantic relation clas-
sification system on realistic Web data, we con-
structed a corpus of sentence pairs gathered from
a vast collection of webpages (2009a). Our basic
approach is as follows:
</bodyText>
<listItem confidence="0.979836153846154">
1. Retrieve documents related to a set number
of topics using the Tsubaki1 search engine
2. Extract real sentences that include major sub-
topic words which are detected based on
TF/IDF in the document set
3. Reduce noise in data by using heuristics to
eliminate advertisements and comment spam
4. Reduce the search space for identifying sen-
tence pairs and prepare pairs, which look fea-
sible to annotate
5. Annotate corresponding sentences with
[AGREEMENT], [CONFLICT], [CONFINE-
MENT], or [OTHER]
</listItem>
<footnote confidence="0.948499">
1http://tsubaki.ixnlp.nii.ac.jp/
</footnote>
<bodyText confidence="0.999766466666667">
Although our target semantic relations in-
clude [EVIDENCE], they difficult annotate con-
sistently, so we do not annotate them at this
time. Expanding our corpus and semantic re-
lation classifier to handle [EVIDENCE] remains
and area of future work.
The data that composes our corpus comes
from a diverse number of sources. A hand sur-
vey of a random sample of the types of domains
of 100 document URLs is given below. Half of
the URL domains were not readily identifiable,
but the known URL domains included govern-
mental, corporate, and personal webpages. We
believe this distribution is representative of in-
formation sources on the Internet.
</bodyText>
<figure confidence="0.7069375">
type count
academic 2
blogs 23
corporate 10
governmental 4
news 5
press releases 4
q&amp;a site 1
reference 1
other 50
</figure>
<bodyText confidence="0.9999676">
We have made a partial release of our corpus
of sentence pairs manually annotated with the
correct semantic relations2. We will fully release
all the data annotated semantic relations and with
gold standard alignments at a future date.
</bodyText>
<subsectionHeader confidence="0.999644">
5.2 Experiment Settings
</subsectionHeader>
<bodyText confidence="0.9999822">
In this section, we present results of empiri-
cal evaluation of our proposed semantic rela-
tion classification system on the dataset we con-
structed in the previous section. For this experi-
ment, we use SVMs as described in Section 4.5
</bodyText>
<footnote confidence="0.99147">
2http://stmap.naist.jp/corpus/ja/
index.html (in Japanese)
</footnote>
<page confidence="0.998823">
27
</page>
<bodyText confidence="0.99996005882353">
to classify semantic relations into one of the four
classes: [AGREEMENT], [CONFLICT], [CON-
FINEMENT], or [OTHER] in the case of no re-
lation. As data we use 370 sentence pairs that
have been manually annotated both with the cor-
rect semantic relation and with gold standard
alignments. Annotations are checked by two na-
tive speakers of Japanese, and any sentence pair
where annotation agreement is not reached is
discarded. Because we have limited data that is
annotated with correct alignments and semantic
relations, we perform five-fold cross validation,
training both the structural aligner and semantic
relation classifier on 296 sentence pairs and eval-
uating on the held out 74 sentence pairs. The
figures presented in the next section are for the
combined results on all 370 sentence pairs.
</bodyText>
<subsectionHeader confidence="0.850333">
5.3 Results
</subsectionHeader>
<bodyText confidence="0.99982715">
We compare two different approaches to classi-
fication using SVMs:
3-class semantic relations are directly classified
into one of [AGREEMENT], [CONFLICT],
and [CONFINEMENT] with all features de-
scribed in 4.5
cascaded 3-class semantic relations are first
classified into one of [AGREEMENT], [CON-
FLICT] without contextual cue features. Then
an additional judgement with all features de-
termines if [AGREEMENT] and [CONFLICT]
should be reclassified as [CONFINEMENT]
Initial results using the 3-class classifica-
tion model produced high f-scores for [AGREE-
MENT] but unfavorable results for [CONFLICT]
and [CONFINEMENT]. We significantly im-
proved classification of [CONFLICT] and [CON-
FINEMENT] by adopting the cascaded 3-class
model. We present these results in Table 2 and
successfully recognized examples in Table 1.
</bodyText>
<sectionHeader confidence="0.998819" genericHeader="discussions">
6 Discussion and Error Analysis
</sectionHeader>
<bodyText confidence="0.999745909090909">
We constructed a prototype semantic relation
classification system by combining the compo-
nents described in the previous section. While
the system developed is not domain-specific and
capable of accepting queries on any topic, we
evaluate its semantic relation classification on
several queries that are representative of our
training data.
Figure 3 shows a snapshot of the semantic re-
lation classification system and the various se-
mantic relations it recognized for the query.
</bodyText>
<table confidence="0.9283309">
Structural Upper-bound
Baseline
Alignment
0.44 0.52 0.74
Precision
(56/126) (96/186) (135/183)
0.30 0.52 0.73
Recall
(56/184) (96/184) (135/184)
F1-score 0.36 0.52 0.74
</table>
<tableCaption confidence="0.876212333333333">
Table 3: Comparison of lexical, structural, and
upper-bound alignments on semantic relation
classification
</tableCaption>
<bodyText confidence="0.980155625">
In the example (6), recognized as [CONFINE-
MENT] in Figure 3, our system correctly identi-
fied negation and analyzed the description “Xyl-
itol alone can not completely” as playing a role
of requirement.
(6) a. キシリトールは虫歯予防に効果がある
(Xylitol is effective at preventing cavi-
ties.)
</bodyText>
<equation confidence="0.489258">
b. キシリトールだけでは完全な予防は出
来ません
</equation>
<bodyText confidence="0.9792306">
(Xylitol alone can not completely prevent
cavities.)
Our system correctly identifies [AGREE-
MENT] relations in other examples about re-
duced water from Table 1 by structurally align-
ing phrases like “promoting good health” and
“supports the health” to “good for the health.”
These examples show how resources like
(Matsuyoshi et al., 2010) and WordNet (Bond et
al., 2008) have contributed to the relation clas-
sification improvement of structural alignment
over them baseline in Table 3. Focusing on sim-
ilarity of syntactic and semantic structures gives
our alignment method greater flexibility.
However, there are still various examples
which the system cannot recognized correctly.
In examples on cavity prevention, the phrase
“effective at preventing cavities” could not be
aligned with “can prevent cavities” or “good for
cavity prevention,” nor can “cavity prevention”
and “cavity-causing bacteria control.”
The above examples illustrate the importance
of the role played by the alignment phase in the
whole system’s performance.
Table 3 compares the semantic relation classi-
fication performance of using lexical alignment
only (as the baseline), lexical alignment and
structural alignment, and, to calculate the maxi-
mum possible precision, classification using cor-
rect alignment data (the upper-bound). We can
</bodyText>
<page confidence="0.997747">
28
</page>
<figureCaption confidence="0.890114">
Figure 3: Alignment and classification example for the query “Xylitol is effective at preventing
cavities.”
</figureCaption>
<bodyText confidence="0.999976894736842">
see that structural alignment makes it possible to
align more words than lexical alignment alone,
leading to an improvement in semantic relation
classification. However, there is still a large gap
between the performance of structural alignment
and the maximum possible precision. Error anal-
ysis shows that a big cause of incorrect classifi-
cation is incorrect lexical alignment. Improving
lexical alignment is a serious problem that must
be addressed. This entails expanding our cur-
rent lexical resources and finding more effective
methods of apply them in alignment.
The most serious problem we currently face is
the feature engineering necessary to find the op-
timal way of applying structural alignments or
other semantic information to semantic relation
classification. We need to conduct a quantita-
tive evaluation of our current classification mod-
els and find ways to improve them.
</bodyText>
<sectionHeader confidence="0.998919" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.99991719047619">
Classifying and identifying semantic relations
between facts and opinions on the Web is of ut-
most importance to organizing information on
the Web, however, this requires consideration of
a broader set of semantic relations than are typi-
cally handled in RTE, CST, and similar tasks. In
this paper, we introduced a set of cross-sentential
semantic relations specifically designed for this
task that apply over both facts and opinions. We
presented a system that identifies these semantic
relations in Japanese Web texts using a combina-
tion of lexical, syntactic, and semantic informa-
tion and evaluated our system against data that
was manually constructed for this task. Prelimi-
nary evaluation showed that we are able to detect
[AGREEMENT] with high levels of confidence.
Our method also shows promise in [CONFLICT]
and [CONFINEMENT] detection. We also dis-
cussed some of the technical issues that need to
be solved in order to identify [CONFLICT] and
[CONFINEMENT].
</bodyText>
<sectionHeader confidence="0.998628" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999402">
This work is supported by the National Institute
of Information and Communications Technology
Japan.
</bodyText>
<sectionHeader confidence="0.998937" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.990798">
Bond, Francis, Hitoshi Isahara, Kyoko Kanzaki, and
Kiyotaka Uchimoto. 2008. Boot-strapping a
wordnet using multiple existing wordnets. In Proc.
of the 6th International Language Resources and
Evaluation (LREC’08).
Dagan, Ido, Oren Glickman, and Bernardo Magnini.
2005. The pascal recognising textual entailment
challenge. In Proc. of the PASCAL Challenges
Workshop on Recognising Textual Entailment.
Etoh, Junji and Manabu Okumura. 2005. Cross-
document relationship between sentences corpus.
</reference>
<page confidence="0.975246">
29
</page>
<reference confidence="0.999628151785714">
In Proc. of the 14th Annual Meeting of the Associa-
tion for Natural Language Processing, pages 482–
485. (in Japanese).
Glickman, Oren, Ido Dagan, and Moshe Koppel.
2005. Web based textual entailment. In Proc. of
the First PASCAL Recognizing Textual Entailment
Workshop.
Hashimoto, Chikara, Kentaro Torisawa, Kow
Kuroda, Masaki Murata, and Jun’ichi Kazama.
2009. Large-scale verb entailment acquisition
from the web. In Conference on Empiri-
cal Methods in Natural Language Processing
(EMNLP2009), pages 1172–1181.
Hickl, Andrew, John Williams, Jeremy Bensley, Kirk
Roberts, Bryan Rink, and Ying Shi. 2005. Recog-
nizing textual entailment with lcc’s groundhog sys-
tem. In Proc. of the Second PASCAL Challenges
Workshop.
Higashiyama, Masahiko, Kentaro Inui, and Yuji Mat-
sumoto. 2008. Acquiring noun polarity knowl-
edge using selectional preferences. In Proc. of the
14th Annual Meeting of the Association for Natu-
ral Language Processing.
Jijkoun, Valentin and Maarten de Rijke. 2005. Rec-
ognizing textual entailment using lexical similar-
ity. In Proc. of the First PASCAL Challenges Work-
shop.
Kobayashi, Nozomi, Kentaro Inui, Yuji Matsumoto,
Kenji Tateishi, and Toshikazu Fukushima. 2005.
Collecting evaluative expressions for opinion ex-
traction. Journal of natural language processing,
12(3):203–222.
Kudo, Taku and Yuji Matsumoto. 2002. Japanese
dependency analysis using cascaded chunking. In
Proc of CoNLL 2002, pages 63–69.
MacCartney, Bill, Trond Grenager, Marie-Catherine
de Marneffe, Daniel Cer, and Christopher D.
Manning. 2006. Learning to recognize fea-
tures of valid textual entailments. In Proc. of
HLT/NAACL 2006.
Marsi, Erwin and Emiel Krahmer. 2005. Classifi-
cation of semantic relations by humans and ma-
chines. In Proc. of ACL-05 Workshop on Empiri-
cal Modeling of Semantic Equivalence and Entail-
ment, pages 1–6.
Matsuyoshi, Suguru, Koji Murakami, Yuji Mat-
sumoto, and Kentaro Inui. 2008. A database of re-
lations between predicate argument structures for
recognizing textual entailment and contradiction.
In Proc. of the Second International Symposium
on Universal Communication, pages 366–373, De-
cember.
Matsuyoshi, Suguru, Megumi Eguchi, Chitose Sao,
Koji Murakami, Kentaro Inui, and Yuji Mat-
sumoto. 2010. Annotating event mentions in text
with modality, focus, and source information. In
Proc. of the 7th International Language Resources
and Evaluation (LREC’10), pages 1456–1463.
Miyabe, Yasunari, Hiroya Takamura, and Manabu
Okumura. 2008. Identifying cross-document re-
lations between sentences. In Proc. of the 3rd In-
ternational Joint Conference on Natural Language
Processing (IJCNLP-08), pages 141–148.
Murakami, Koji, Shouko Masuda, Suguru Mat-
suyoshi, Eric Nichols, Kentaro Inui, and Yuji Mat-
sumoto. 2009a. Annotating semantic relations
combining facts and opinions. In Proceedings of
the Third Linguistic Annotation Workshop, pages
150–153, Suntec, Singapore, August. Association
for Computational Linguistics.
Murakami, Koji, Eric Nichols, Suguru Matsuyoshi,
Asuka Sumida, Shouko Masuda, Kentaro Inui, and
Yuji Matsumoto. 2009b. Statement map: Assist-
ing information credibility analysis by visualizing
arguments. In Proc. of the 3rd ACM Workshop
on Information Credibility on the Web (WICOW
2009), pages 43–50.
Radev, Dragomir, Jahna Otterbacher,
and Zhu Zhang. 2003. CSTBank:
Cross-document Structure Theory Bank.
http://tangra.si.umich.edu/clair/CSTBank.
Radev, Dragomir R. 2000. Common theory of infor-
mation fusion from multiple text sources step one:
Cross-document structure. In Proc. of the 1st SIG-
dial workshop on Discourse and dialogue, pages
74–83.
Sumida, Asuka, Naoki Yoshinaga, and Kentaro Tori-
sawa. 2008. Boosting precision and recall of hy-
ponymy relation acquisition from hierarchical lay-
outs in wikipedia. In Proc. of the 6th International
Language Resources and Evaluation (LREC’08).
Szpektor, Idan, Eyal Shnarch, and Ido Dagan. 2007.
Instance-based evaluation of entailment rule acqui-
sition. In Proc. of the 45th Annual Meeting of the
Association of Computational Linguistics, pages
456–463.
Watanabe, Yotaro, Masayuki Asahara, and Yuji Mat-
sumoto. 2010. A structured model for joint learn-
ing of argument roles and predicate senses. In Pro-
ceedings of the 48th Annual Meeting of the Associ-
ation of Computational Linguistics (to appear).
Wiebe, Janyce, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and
emotions in language. Language Resources and
Evaluation, 39(2-3):165–210.
William, Mann and Sandra Thompson. 1988.
Rhetorical structure theory: towards a functional
theory of text organization. Text, 8(3):243–281.
Zhang, Zhu and Dragomir Radev. 2004. Combin-
ing labeled and unlabeled data for learning cross-
document structural relationships. In Proc. of the
Proceedings of IJC-NLP.
</reference>
<page confidence="0.998803">
30
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.742631">
<title confidence="0.8979685">Automatic Classification of Semantic between Facts and Opinions</title>
<affiliation confidence="0.946508">Institute of Science and</affiliation>
<abstract confidence="0.999097148148149">Classifying and identifying semantic relations between facts and opinions on the Web is of utmost importance for organizing information on the Web, however, this requires consideration of a broader set of semantic relations than are typically handled in Recognizing Textual Entailment (RTE), Cross-document Structure Theory (CST), and similar tasks. In this paper, we describe the construction and evaluation of a system that identifies and classifies semantic relations in Internet data. Our system targets a set of semantic relations that have been inspired by CST but that have been generalized and broadened to facilitate application to mixed fact and opinion data from the Internet. Our system identifies these semantic relations in Japanese Web texts using a combination of lexical, syntactic, and semantic information and evaluate our system against gold standard data that was manually constructed for this task. We will release all gold standard data used in training and evaluation of our system this summer.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Francis Bond</author>
<author>Hitoshi Isahara</author>
<author>Kyoko Kanzaki</author>
<author>Kiyotaka Uchimoto</author>
</authors>
<title>Boot-strapping a wordnet using multiple existing wordnets.</title>
<date>2008</date>
<booktitle>In Proc. of the 6th International Language Resources and Evaluation (LREC’08).</booktitle>
<contexts>
<context position="30714" citStr="Bond et al., 2008" startWordPosition="4762" endWordPosition="4765">in Figure 3, our system correctly identified negation and analyzed the description “Xylitol alone can not completely” as playing a role of requirement. (6) a. キシリトールは虫歯予防に効果がある (Xylitol is effective at preventing cavities.) b. キシリトールだけでは完全な予防は出 来ません (Xylitol alone can not completely prevent cavities.) Our system correctly identifies [AGREEMENT] relations in other examples about reduced water from Table 1 by structurally aligning phrases like “promoting good health” and “supports the health” to “good for the health.” These examples show how resources like (Matsuyoshi et al., 2010) and WordNet (Bond et al., 2008) have contributed to the relation classification improvement of structural alignment over them baseline in Table 3. Focusing on similarity of syntactic and semantic structures gives our alignment method greater flexibility. However, there are still various examples which the system cannot recognized correctly. In examples on cavity prevention, the phrase “effective at preventing cavities” could not be aligned with “can prevent cavities” or “good for cavity prevention,” nor can “cavity prevention” and “cavity-causing bacteria control.” The above examples illustrate the importance of the role pl</context>
</contexts>
<marker>Bond, Isahara, Kanzaki, Uchimoto, 2008</marker>
<rawString>Bond, Francis, Hitoshi Isahara, Kyoko Kanzaki, and Kiyotaka Uchimoto. 2008. Boot-strapping a wordnet using multiple existing wordnets. In Proc. of the 6th International Language Resources and Evaluation (LREC’08).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The pascal recognising textual entailment challenge.</title>
<date>2005</date>
<booktitle>In Proc. of the PASCAL Challenges Workshop on Recognising Textual Entailment.</booktitle>
<contexts>
<context position="2155" citStr="Dagan et al., 2005" startWordPosition="320" endWordPosition="323">creasingly important as more people turn to the Web as a source of important information. The vast amounts of research conducted in NLP on automatic summarization, opinion mining, and question answering are illustrative of the great interest in making relevant information easier to find. Providing Internet users with thorough information requires recognizing semantic relations between both facts and opinions, however the assumptions made by current approaches are often incompatible with this goal. For example, the existing semantic relations considered in Recognizing Textual Entailment (RTE) (Dagan et al., 2005) are often too narrow in scope to be directly applicable to text on the Internet, and theories like Cross-document Structure Theory (CST) (Radev, 2000) are only applicable to facts or second-hand reporting of opinions rather than relations between both. As part of the STATEMENT MAP project we proposed the development of a system to support information credibility analysis on the Web (Murakami et al., 2009b) by automatically summarizing facts and opinions on topics of interest to users and showing them the evidence and conflicts for each viewpoint. To facilitate the detection of semantic relati</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2005</marker>
<rawString>Dagan, Ido, Oren Glickman, and Bernardo Magnini. 2005. The pascal recognising textual entailment challenge. In Proc. of the PASCAL Challenges Workshop on Recognising Textual Entailment.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Junji Etoh</author>
<author>Manabu Okumura</author>
</authors>
<title>Crossdocument relationship between sentences corpus.</title>
<date>2005</date>
<contexts>
<context position="9753" citStr="Etoh and Okumura, 2005" startWordPosition="1474" endWordPosition="1477">Reduced water is good for the health. 還元水は活性酸素を除去すると言われ健康を維持してくれる働きをもたらす 同意 Reduced water is said to remove active oxygen from the body, making it effective at [AGREEMENT] promoting good health. 美味しくても酸化させる水は健康には役立ちません 対立 Even if oxidized water tastes good, it does not help one’s health. [CONFLICT] イソフラボンは健康維持に効果 大豆イソフラボンをサプリメントで過剰摂取すると健康維持には負の影響を与 限定 がある える結果となります Isoflavone is effective at Taking too much soy isoflavone as a supplement will have a negative effect on one’s [CONFINEMENT] maintaining good health. health Table 1: Example semantic relation classification. FINEMENT]. Etoh et al. (Etoh and Okumura, 2005) constructed a Japanese Cross-document Relation Corpus, and they redefined 14 kinds of semantic relations in their corpus. CST was designed for objective expressions because its target data is newspaper articles related to the same topic. Facts, which can be extracted from newspaper articles, have been used in conventional NLP research, such as Information Extraction or Factoid Question Answering. However, there are a lot of opinions on the Web, and it is important to survey opinions in addition to facts to give Internet users a comprehensive view of the discussions on topics of interest. 2.3 </context>
</contexts>
<marker>Etoh, Okumura, 2005</marker>
<rawString>Etoh, Junji and Manabu Okumura. 2005. Crossdocument relationship between sentences corpus.</rawString>
</citation>
<citation valid="false">
<booktitle>In Proc. of the 14th Annual Meeting of the Association for Natural Language Processing,</booktitle>
<pages>482--485</pages>
<note>(in Japanese).</note>
<marker></marker>
<rawString>In Proc. of the 14th Annual Meeting of the Association for Natural Language Processing, pages 482– 485. (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Glickman</author>
<author>Ido Dagan</author>
<author>Moshe Koppel</author>
</authors>
<title>Web based textual entailment.</title>
<date>2005</date>
<booktitle>In Proc. of the First PASCAL Recognizing Textual Entailment Workshop.</booktitle>
<contexts>
<context position="6791" citStr="Glickman et al., 2005" startWordPosition="1040" endWordPosition="1043">emantic relation between a Text (T) and a Hypothesis (H) into [ENTAILMENT], [CONTRADICTION], or [UNKNOWN]. Over the last several years, several corpora annotated with thousands of (T,H) pairs have been constructed for this task. In these corpora, each pair was tagged indicating its related task (e.g. Information Extraction, Question Answering, Information Retrieval or Summarization). The RTE Challenge has successfully employed a variety of techniques in order to recognize instances of textual entailment, including methods based on: measuring the degree of lexical overlap between bag of words (Glickman et al., 2005; Jijkoun and de Rijke, 2005), the alignment of graphs created from syntactic or semantic dependencies (Marsi and Krahmer, 2005; MacCartney et al., 2006), statistical classifiers which leverage a wide range of features (Hickl et al., 2005), or reference rule generation (Szpektor et al., 2007). These approaches have shown great promise in RTE for entailment pairs in the corpus, but more robust models of recognizing logical relations are still desirable. The definition of contradiction in RTE is that T contradicts H if it is very unlikely that both T and H can be true at the same time. However, </context>
</contexts>
<marker>Glickman, Dagan, Koppel, 2005</marker>
<rawString>Glickman, Oren, Ido Dagan, and Moshe Koppel. 2005. Web based textual entailment. In Proc. of the First PASCAL Recognizing Textual Entailment Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chikara Hashimoto</author>
<author>Kentaro Torisawa</author>
<author>Kow Kuroda</author>
<author>Masaki Murata</author>
<author>Jun’ichi Kazama</author>
</authors>
<title>Large-scale verb entailment acquisition from the web.</title>
<date>2009</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing (EMNLP2009),</booktitle>
<pages>1172--1181</pages>
<contexts>
<context position="18213" citStr="Hashimoto et al., 2009" startWordPosition="2795" endWordPosition="2798">ocess is shown in Figure 1. It consists of the following two phases: 1. lexical alignment 2. structural alignment We developed a heuristic-based algorithm to align chunk based on lexical similarity information. We incorporate the following information into an alignment confidence score that has a range of 0.0-1.0 and align chunk whose scores cross an empirically-determined threshold. • surface level similarity: identical content words or cosine similarity of chunk contents • semantic similarity of predicate-argument structures predicates we check for matches in predicate entailment databases (Hashimoto et al., 2009; Matsuyoshi et al., 2008) considering the default case frames reported by ChaPAS arguments we check for synonym or hypernym matches in the Japanese WordNet (2008) or the Japanese hypernym collection of Sumida et al. (2008) 3. Infer the applicability of the [CONFINEMENT] or [EVIDENCE] relations in the Text to the Query 4.1 System Overview We have finished constructing a prototype system that detects semantic relation between statements. It has a three-stage architecture similar to the RTE system of MacCartney et al. (2006): 1. Linguistic analysis 2. Structural alignment 3. Feature extraction f</context>
</contexts>
<marker>Hashimoto, Torisawa, Kuroda, Murata, Kazama, 2009</marker>
<rawString>Hashimoto, Chikara, Kentaro Torisawa, Kow Kuroda, Masaki Murata, and Jun’ichi Kazama. 2009. Large-scale verb entailment acquisition from the web. In Conference on Empirical Methods in Natural Language Processing (EMNLP2009), pages 1172–1181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Hickl</author>
<author>John Williams</author>
<author>Jeremy Bensley</author>
<author>Kirk Roberts</author>
<author>Bryan Rink</author>
<author>Ying Shi</author>
</authors>
<title>Recognizing textual entailment with lcc’s groundhog system.</title>
<date>2005</date>
<booktitle>In Proc. of the Second PASCAL Challenges Workshop.</booktitle>
<contexts>
<context position="7030" citStr="Hickl et al., 2005" startWordPosition="1077" endWordPosition="1080">pora, each pair was tagged indicating its related task (e.g. Information Extraction, Question Answering, Information Retrieval or Summarization). The RTE Challenge has successfully employed a variety of techniques in order to recognize instances of textual entailment, including methods based on: measuring the degree of lexical overlap between bag of words (Glickman et al., 2005; Jijkoun and de Rijke, 2005), the alignment of graphs created from syntactic or semantic dependencies (Marsi and Krahmer, 2005; MacCartney et al., 2006), statistical classifiers which leverage a wide range of features (Hickl et al., 2005), or reference rule generation (Szpektor et al., 2007). These approaches have shown great promise in RTE for entailment pairs in the corpus, but more robust models of recognizing logical relations are still desirable. The definition of contradiction in RTE is that T contradicts H if it is very unlikely that both T and H can be true at the same time. However, in real documents on the Web, there are many pairs of examples which are contradictory in part, or where one statement confines the applicability of another, as shown in the examples in Table 1. 2.2 Cross-document Structure Theory Cross-do</context>
</contexts>
<marker>Hickl, Williams, Bensley, Roberts, Rink, Shi, 2005</marker>
<rawString>Hickl, Andrew, John Williams, Jeremy Bensley, Kirk Roberts, Bryan Rink, and Ying Shi. 2005. Recognizing textual entailment with lcc’s groundhog system. In Proc. of the Second PASCAL Challenges Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masahiko Higashiyama</author>
<author>Kentaro Inui</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Acquiring noun polarity knowledge using selectional preferences.</title>
<date>2008</date>
<booktitle>In Proc. of the 14th Annual Meeting of the Association for Natural Language Processing.</booktitle>
<contexts>
<context position="21489" citStr="Higashiyama et al., 2008" startWordPosition="3297" endWordPosition="3300">cultural chemicals are sprayed.” Although the verbs used and sprayed are not directly semantically related, they are aligned because they share the same argument structures. This lets up align predicates for which we lack semantic resources. We use the following information to determine predicate-argument alignment: • the number of aligned children • the number of aligned case frame arguments • the number of possible alignments in a window of n chunk • predicates indicating existence or quantity. E.g. many, few, to exist, etc. • polarity of both parent and child chunks using the resources in (Higashiyama et al., 2008; Kobayashi et al., 2005) We treat structural alignment as a machine learning problem and train a Support Vector Machine (SVM) model to decide if lexically aligned chunks are semantically aligned. We train on gold-standard labeled alignment of 370 sentence pairs. This data set is described in more detail in Section 5.1. As features for our SVM model, we use the following information: • the distance in edges in the dependency graph between parent and child for both sentences • the distance in chunks between parent and child in both sentences • binary features indicating whether each chunk is a </context>
</contexts>
<marker>Higashiyama, Inui, Matsumoto, 2008</marker>
<rawString>Higashiyama, Masahiko, Kentaro Inui, and Yuji Matsumoto. 2008. Acquiring noun polarity knowledge using selectional preferences. In Proc. of the 14th Annual Meeting of the Association for Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valentin Jijkoun</author>
<author>Maarten de Rijke</author>
</authors>
<title>Recognizing textual entailment using lexical similarity.</title>
<date>2005</date>
<booktitle>In Proc. of the First PASCAL Challenges Workshop.</booktitle>
<marker>Jijkoun, de Rijke, 2005</marker>
<rawString>Jijkoun, Valentin and Maarten de Rijke. 2005. Recognizing textual entailment using lexical similarity. In Proc. of the First PASCAL Challenges Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nozomi Kobayashi</author>
<author>Kentaro Inui</author>
<author>Yuji Matsumoto</author>
<author>Kenji Tateishi</author>
<author>Toshikazu Fukushima</author>
</authors>
<title>Collecting evaluative expressions for opinion extraction. Journal of natural language processing,</title>
<date>2005</date>
<pages>12--3</pages>
<contexts>
<context position="21514" citStr="Kobayashi et al., 2005" startWordPosition="3301" endWordPosition="3304">ayed.” Although the verbs used and sprayed are not directly semantically related, they are aligned because they share the same argument structures. This lets up align predicates for which we lack semantic resources. We use the following information to determine predicate-argument alignment: • the number of aligned children • the number of aligned case frame arguments • the number of possible alignments in a window of n chunk • predicates indicating existence or quantity. E.g. many, few, to exist, etc. • polarity of both parent and child chunks using the resources in (Higashiyama et al., 2008; Kobayashi et al., 2005) We treat structural alignment as a machine learning problem and train a Support Vector Machine (SVM) model to decide if lexically aligned chunks are semantically aligned. We train on gold-standard labeled alignment of 370 sentence pairs. This data set is described in more detail in Section 5.1. As features for our SVM model, we use the following information: • the distance in edges in the dependency graph between parent and child for both sentences • the distance in chunks between parent and child in both sentences • binary features indicating whether each chunk is a predicate or argument acc</context>
</contexts>
<marker>Kobayashi, Inui, Matsumoto, Tateishi, Fukushima, 2005</marker>
<rawString>Kobayashi, Nozomi, Kentaro Inui, Yuji Matsumoto, Kenji Tateishi, and Toshikazu Fukushima. 2005. Collecting evaluative expressions for opinion extraction. Journal of natural language processing, 12(3):203–222.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Japanese dependency analysis using cascaded chunking.</title>
<date>2002</date>
<booktitle>In Proc of CoNLL</booktitle>
<pages>63--69</pages>
<marker>Kudo, Matsumoto, 2002</marker>
<rawString>Kudo, Taku and Yuji Matsumoto. 2002. Japanese dependency analysis using cascaded chunking. In Proc of CoNLL 2002, pages 63–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill MacCartney</author>
<author>Trond Grenager</author>
<author>Marie-Catherine de Marneffe</author>
<author>Daniel Cer</author>
<author>Christopher D Manning</author>
</authors>
<title>Learning to recognize features of valid textual entailments.</title>
<date>2006</date>
<booktitle>In Proc. of HLT/NAACL</booktitle>
<marker>MacCartney, Grenager, de Marneffe, Cer, Manning, 2006</marker>
<rawString>MacCartney, Bill, Trond Grenager, Marie-Catherine de Marneffe, Daniel Cer, and Christopher D. Manning. 2006. Learning to recognize features of valid textual entailments. In Proc. of HLT/NAACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erwin Marsi</author>
<author>Emiel Krahmer</author>
</authors>
<title>Classification of semantic relations by humans and machines.</title>
<date>2005</date>
<booktitle>In Proc. of ACL-05 Workshop on Empirical Modeling of Semantic Equivalence and Entailment,</booktitle>
<pages>1--6</pages>
<contexts>
<context position="6918" citStr="Marsi and Krahmer, 2005" startWordPosition="1060" endWordPosition="1063">ral years, several corpora annotated with thousands of (T,H) pairs have been constructed for this task. In these corpora, each pair was tagged indicating its related task (e.g. Information Extraction, Question Answering, Information Retrieval or Summarization). The RTE Challenge has successfully employed a variety of techniques in order to recognize instances of textual entailment, including methods based on: measuring the degree of lexical overlap between bag of words (Glickman et al., 2005; Jijkoun and de Rijke, 2005), the alignment of graphs created from syntactic or semantic dependencies (Marsi and Krahmer, 2005; MacCartney et al., 2006), statistical classifiers which leverage a wide range of features (Hickl et al., 2005), or reference rule generation (Szpektor et al., 2007). These approaches have shown great promise in RTE for entailment pairs in the corpus, but more robust models of recognizing logical relations are still desirable. The definition of contradiction in RTE is that T contradicts H if it is very unlikely that both T and H can be true at the same time. However, in real documents on the Web, there are many pairs of examples which are contradictory in part, or where one statement confines</context>
</contexts>
<marker>Marsi, Krahmer, 2005</marker>
<rawString>Marsi, Erwin and Emiel Krahmer. 2005. Classification of semantic relations by humans and machines. In Proc. of ACL-05 Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pages 1–6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Suguru Matsuyoshi</author>
<author>Koji Murakami</author>
<author>Yuji Matsumoto</author>
<author>Kentaro Inui</author>
</authors>
<title>A database of relations between predicate argument structures for recognizing textual entailment and contradiction.</title>
<date>2008</date>
<booktitle>In Proc. of the Second International Symposium on Universal Communication,</booktitle>
<pages>366--373</pages>
<contexts>
<context position="18239" citStr="Matsuyoshi et al., 2008" startWordPosition="2799" endWordPosition="2802"> 1. It consists of the following two phases: 1. lexical alignment 2. structural alignment We developed a heuristic-based algorithm to align chunk based on lexical similarity information. We incorporate the following information into an alignment confidence score that has a range of 0.0-1.0 and align chunk whose scores cross an empirically-determined threshold. • surface level similarity: identical content words or cosine similarity of chunk contents • semantic similarity of predicate-argument structures predicates we check for matches in predicate entailment databases (Hashimoto et al., 2009; Matsuyoshi et al., 2008) considering the default case frames reported by ChaPAS arguments we check for synonym or hypernym matches in the Japanese WordNet (2008) or the Japanese hypernym collection of Sumida et al. (2008) 3. Infer the applicability of the [CONFINEMENT] or [EVIDENCE] relations in the Text to the Query 4.1 System Overview We have finished constructing a prototype system that detects semantic relation between statements. It has a three-stage architecture similar to the RTE system of MacCartney et al. (2006): 1. Linguistic analysis 2. Structural alignment 3. Feature extraction for detecting [EVIDENCE] an</context>
</contexts>
<marker>Matsuyoshi, Murakami, Matsumoto, Inui, 2008</marker>
<rawString>Matsuyoshi, Suguru, Koji Murakami, Yuji Matsumoto, and Kentaro Inui. 2008. A database of relations between predicate argument structures for recognizing textual entailment and contradiction. In Proc. of the Second International Symposium on Universal Communication, pages 366–373, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Suguru Matsuyoshi</author>
<author>Megumi Eguchi</author>
<author>Chitose Sao</author>
<author>Koji Murakami</author>
<author>Kentaro Inui</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Annotating event mentions in text with modality, focus, and source information.</title>
<date>2010</date>
<booktitle>In Proc. of the 7th International Language Resources and Evaluation (LREC’10),</booktitle>
<pages>1456--1463</pages>
<contexts>
<context position="17315" citStr="Matsuyoshi et al. (2010)" startWordPosition="2664" endWordPosition="2667">/ #)&amp; :$$+ &amp;#&apos;;+ &amp;quot;#$%&amp;&apos; ! &amp;quot;#$ %&amp;&apos;()*&apos;+$ 1234 0 3$&apos;.&apos;-&apos;&amp;%&amp; &apos;&amp; 6 7 67 5 -%&apos;8&amp; *) !&amp;quot; #$%&amp;&apos; &amp;quot;#$ %&amp;&apos;()*&apos;+$ J ) J ()* ,&apos;- .-&apos;/&apos;0+1 #$)23# B +,- $4$50*$ C 5 C ./ %&amp; ? Figure 1: An example of structural alignment sumoto, 2002) and the predicate-argument structure analyzer ChaPAS (Watanabe et al., 2010). CaboCha splits the Japanese text into phrase-like chunks and represents syntactic dependencies between the chunks as edges in a graph. ChaPAS identifies predicate-argument structures in the dependency graph produced by CaboCha. We also conduct extended modality analysis using the resources provided by Matsuyoshi et al. (2010), focusing on tense, modality, and polarity, because such information provides important clues for the recognition of semantic relations between statements. 4.3 Structural Alignment In this section, we describe our approach to structural alignment. The structural alignment process is shown in Figure 1. It consists of the following two phases: 1. lexical alignment 2. structural alignment We developed a heuristic-based algorithm to align chunk based on lexical similarity information. We incorporate the following information into an alignment confidence score that has a range of 0.0-1.0 and align</context>
<context position="30682" citStr="Matsuyoshi et al., 2010" startWordPosition="4756" endWordPosition="4759">mple (6), recognized as [CONFINEMENT] in Figure 3, our system correctly identified negation and analyzed the description “Xylitol alone can not completely” as playing a role of requirement. (6) a. キシリトールは虫歯予防に効果がある (Xylitol is effective at preventing cavities.) b. キシリトールだけでは完全な予防は出 来ません (Xylitol alone can not completely prevent cavities.) Our system correctly identifies [AGREEMENT] relations in other examples about reduced water from Table 1 by structurally aligning phrases like “promoting good health” and “supports the health” to “good for the health.” These examples show how resources like (Matsuyoshi et al., 2010) and WordNet (Bond et al., 2008) have contributed to the relation classification improvement of structural alignment over them baseline in Table 3. Focusing on similarity of syntactic and semantic structures gives our alignment method greater flexibility. However, there are still various examples which the system cannot recognized correctly. In examples on cavity prevention, the phrase “effective at preventing cavities” could not be aligned with “can prevent cavities” or “good for cavity prevention,” nor can “cavity prevention” and “cavity-causing bacteria control.” The above examples illustra</context>
</contexts>
<marker>Matsuyoshi, Eguchi, Sao, Murakami, Inui, Matsumoto, 2010</marker>
<rawString>Matsuyoshi, Suguru, Megumi Eguchi, Chitose Sao, Koji Murakami, Kentaro Inui, and Yuji Matsumoto. 2010. Annotating event mentions in text with modality, focus, and source information. In Proc. of the 7th International Language Resources and Evaluation (LREC’10), pages 1456–1463.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yasunari Miyabe</author>
<author>Hiroya Takamura</author>
<author>Manabu Okumura</author>
</authors>
<title>Identifying cross-document relations between sentences.</title>
<date>2008</date>
<booktitle>In Proc. of the 3rd International Joint Conference on Natural Language Processing (IJCNLP-08),</booktitle>
<pages>141--148</pages>
<contexts>
<context position="10791" citStr="Miyabe et al. (2008)" startWordPosition="1633" endWordPosition="1636">e a lot of opinions on the Web, and it is important to survey opinions in addition to facts to give Internet users a comprehensive view of the discussions on topics of interest. 2.3 Cross-document Summarization Based on CST Relations between Sentences Zhang and Radev (2004) attempted to classify CST relations between sentence pairs extracted from topically related documents. However, they used a vector space model and tried multi-class classification. The results were not satisfactory. This observation may indicate that the recognition methods for each relation should be developed separately. Miyabe et al. (2008) attempted to recognize relations that were defined in a Japanese cross-document relation corpus (Etoh and Okumura, 2005). However, their target relations were limited to [EQUIVALENCE] and [TRANSITION]; other relations were not targeted. Recognizing [EVIDENCE] is indispensable for organizing information on the Internet. We need to develop satisfactory methods of [EVIDENCE] recognition. 2.4 Opinion Mining and Sentiment Analysis Subjective statements, such as opinions, have recently been the focus of much NLP research including review analysis, opinion extraction, opinion question answering, and</context>
</contexts>
<marker>Miyabe, Takamura, Okumura, 2008</marker>
<rawString>Miyabe, Yasunari, Hiroya Takamura, and Manabu Okumura. 2008. Identifying cross-document relations between sentences. In Proc. of the 3rd International Joint Conference on Natural Language Processing (IJCNLP-08), pages 141–148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koji Murakami</author>
<author>Shouko Masuda</author>
<author>Suguru Matsuyoshi</author>
<author>Eric Nichols</author>
<author>Kentaro Inui</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Annotating semantic relations combining facts and opinions.</title>
<date>2009</date>
<booktitle>In Proceedings of the Third Linguistic Annotation Workshop,</booktitle>
<pages>150--153</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="2563" citStr="Murakami et al., 2009" startWordPosition="386" endWordPosition="389">nions, however the assumptions made by current approaches are often incompatible with this goal. For example, the existing semantic relations considered in Recognizing Textual Entailment (RTE) (Dagan et al., 2005) are often too narrow in scope to be directly applicable to text on the Internet, and theories like Cross-document Structure Theory (CST) (Radev, 2000) are only applicable to facts or second-hand reporting of opinions rather than relations between both. As part of the STATEMENT MAP project we proposed the development of a system to support information credibility analysis on the Web (Murakami et al., 2009b) by automatically summarizing facts and opinions on topics of interest to users and showing them the evidence and conflicts for each viewpoint. To facilitate the detection of semantic relations in Internet data, we defined a sentence-like unit of information called the statement that encompasses both facts and opinions, started compiling a corpus of statements annotated with semantic relations (Murakami et al., 2009a), and begin constructing a system to automatically identify semantic relations between statements. In this paper, we describe the construction and evaluation of a prototype sema</context>
</contexts>
<marker>Murakami, Masuda, Matsuyoshi, Nichols, Inui, Matsumoto, 2009</marker>
<rawString>Murakami, Koji, Shouko Masuda, Suguru Matsuyoshi, Eric Nichols, Kentaro Inui, and Yuji Matsumoto. 2009a. Annotating semantic relations combining facts and opinions. In Proceedings of the Third Linguistic Annotation Workshop, pages 150–153, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koji Murakami</author>
</authors>
<title>Eric Nichols, Suguru Matsuyoshi, Asuka Sumida, Shouko Masuda, Kentaro Inui, and Yuji Matsumoto. 2009b. Statement map: Assisting information credibility analysis by visualizing arguments.</title>
<date>2009</date>
<booktitle>In Proc. of the 3rd ACM Workshop on Information Credibility on the Web (WICOW</booktitle>
<pages>43--50</pages>
<marker>Murakami, 2009</marker>
<rawString>Murakami, Koji, Eric Nichols, Suguru Matsuyoshi, Asuka Sumida, Shouko Masuda, Kentaro Inui, and Yuji Matsumoto. 2009b. Statement map: Assisting information credibility analysis by visualizing arguments. In Proc. of the 3rd ACM Workshop on Information Credibility on the Web (WICOW 2009), pages 43–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir Radev</author>
<author>Jahna Otterbacher</author>
<author>Zhu Zhang</author>
</authors>
<title>CSTBank: Cross-document Structure Theory Bank.</title>
<date>2003</date>
<note>http://tangra.si.umich.edu/clair/CSTBank.</note>
<contexts>
<context position="8227" citStr="Radev et al., 2003" startWordPosition="1274" endWordPosition="1277">ucture Theory Cross-document Structure Theory (CST), developed by Radev (2000), is another task of recognizing semantic relations between sentences. CST is an expanded rhetorical structure analysis based on Rhetorical Structure Theory (RST: (William and Thompson, 1988)), and attempts to describe the semantic relations that exist between two or more sentences from different source documents that are related to the same topic, as well as those that come from a single source document. A corpus of crossdocument sentences annotated with CST relations has also been constructed (The CSTBank Corpus: (Radev et al., 2003)). CSTBank is organized into clusters of topically-related articles. There are 18 kinds of semantic relations in this corpus, not limited to [EQUIVALENCE] or [CONTRADICTION], but also including [JUDGEMENT], [ELABORATION], and [RE22 Query Matching sentences Output キシリトールは虫歯予防に効果 がある Xylitol is effective at preventing cavities. キシリトールの含まれている量が多いほどむし歯予防の効果は高いようです 同意 The cavity-prevention effects are greater the more Xylitol is included. [AGREEMENT]. キシリトールがお口の健康維持や虫歯予防にも効果を発揮します 同意 Xylitol shows effectiveness at maintaining good oral hygiene and preventing cavities. [AGREEMENT] キシリトールの虫歯抑制効果については</context>
</contexts>
<marker>Radev, Otterbacher, Zhang, 2003</marker>
<rawString>Radev, Dragomir, Jahna Otterbacher, and Zhu Zhang. 2003. CSTBank: Cross-document Structure Theory Bank. http://tangra.si.umich.edu/clair/CSTBank.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir R Radev</author>
</authors>
<title>Common theory of information fusion from multiple text sources step one: Cross-document structure.</title>
<date>2000</date>
<booktitle>In Proc. of the 1st SIGdial workshop on Discourse and dialogue,</booktitle>
<pages>74--83</pages>
<contexts>
<context position="2306" citStr="Radev, 2000" startWordPosition="346" endWordPosition="347">tion, opinion mining, and question answering are illustrative of the great interest in making relevant information easier to find. Providing Internet users with thorough information requires recognizing semantic relations between both facts and opinions, however the assumptions made by current approaches are often incompatible with this goal. For example, the existing semantic relations considered in Recognizing Textual Entailment (RTE) (Dagan et al., 2005) are often too narrow in scope to be directly applicable to text on the Internet, and theories like Cross-document Structure Theory (CST) (Radev, 2000) are only applicable to facts or second-hand reporting of opinions rather than relations between both. As part of the STATEMENT MAP project we proposed the development of a system to support information credibility analysis on the Web (Murakami et al., 2009b) by automatically summarizing facts and opinions on topics of interest to users and showing them the evidence and conflicts for each viewpoint. To facilitate the detection of semantic relations in Internet data, we defined a sentence-like unit of information called the statement that encompasses both facts and opinions, started compiling a</context>
<context position="7686" citStr="Radev (2000)" startWordPosition="1189" endWordPosition="1190">al., 2007). These approaches have shown great promise in RTE for entailment pairs in the corpus, but more robust models of recognizing logical relations are still desirable. The definition of contradiction in RTE is that T contradicts H if it is very unlikely that both T and H can be true at the same time. However, in real documents on the Web, there are many pairs of examples which are contradictory in part, or where one statement confines the applicability of another, as shown in the examples in Table 1. 2.2 Cross-document Structure Theory Cross-document Structure Theory (CST), developed by Radev (2000), is another task of recognizing semantic relations between sentences. CST is an expanded rhetorical structure analysis based on Rhetorical Structure Theory (RST: (William and Thompson, 1988)), and attempts to describe the semantic relations that exist between two or more sentences from different source documents that are related to the same topic, as well as those that come from a single source document. A corpus of crossdocument sentences annotated with CST relations has also been constructed (The CSTBank Corpus: (Radev et al., 2003)). CSTBank is organized into clusters of topically-related </context>
</contexts>
<marker>Radev, 2000</marker>
<rawString>Radev, Dragomir R. 2000. Common theory of information fusion from multiple text sources step one: Cross-document structure. In Proc. of the 1st SIGdial workshop on Discourse and dialogue, pages 74–83.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Asuka Sumida</author>
<author>Naoki Yoshinaga</author>
<author>Kentaro Torisawa</author>
</authors>
<title>Boosting precision and recall of hyponymy relation acquisition from hierarchical layouts in wikipedia.</title>
<date>2008</date>
<booktitle>In Proc. of the 6th International Language Resources and Evaluation (LREC’08).</booktitle>
<contexts>
<context position="18436" citStr="Sumida et al. (2008)" startWordPosition="2833" endWordPosition="2836"> the following information into an alignment confidence score that has a range of 0.0-1.0 and align chunk whose scores cross an empirically-determined threshold. • surface level similarity: identical content words or cosine similarity of chunk contents • semantic similarity of predicate-argument structures predicates we check for matches in predicate entailment databases (Hashimoto et al., 2009; Matsuyoshi et al., 2008) considering the default case frames reported by ChaPAS arguments we check for synonym or hypernym matches in the Japanese WordNet (2008) or the Japanese hypernym collection of Sumida et al. (2008) 3. Infer the applicability of the [CONFINEMENT] or [EVIDENCE] relations in the Text to the Query 4.1 System Overview We have finished constructing a prototype system that detects semantic relation between statements. It has a three-stage architecture similar to the RTE system of MacCartney et al. (2006): 1. Linguistic analysis 2. Structural alignment 3. Feature extraction for detecting [EVIDENCE] and [CONFINEMENT] 4. Semantic relation classification However, we differ in the following respects. First, our relation classification is broader than RTE’s simple distinction between [ENTAILMENT], [</context>
</contexts>
<marker>Sumida, Yoshinaga, Torisawa, 2008</marker>
<rawString>Sumida, Asuka, Naoki Yoshinaga, and Kentaro Torisawa. 2008. Boosting precision and recall of hyponymy relation acquisition from hierarchical layouts in wikipedia. In Proc. of the 6th International Language Resources and Evaluation (LREC’08).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Idan Szpektor</author>
<author>Eyal Shnarch</author>
<author>Ido Dagan</author>
</authors>
<title>Instance-based evaluation of entailment rule acquisition.</title>
<date>2007</date>
<booktitle>In Proc. of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>456--463</pages>
<contexts>
<context position="7084" citStr="Szpektor et al., 2007" startWordPosition="1085" endWordPosition="1089">ask (e.g. Information Extraction, Question Answering, Information Retrieval or Summarization). The RTE Challenge has successfully employed a variety of techniques in order to recognize instances of textual entailment, including methods based on: measuring the degree of lexical overlap between bag of words (Glickman et al., 2005; Jijkoun and de Rijke, 2005), the alignment of graphs created from syntactic or semantic dependencies (Marsi and Krahmer, 2005; MacCartney et al., 2006), statistical classifiers which leverage a wide range of features (Hickl et al., 2005), or reference rule generation (Szpektor et al., 2007). These approaches have shown great promise in RTE for entailment pairs in the corpus, but more robust models of recognizing logical relations are still desirable. The definition of contradiction in RTE is that T contradicts H if it is very unlikely that both T and H can be true at the same time. However, in real documents on the Web, there are many pairs of examples which are contradictory in part, or where one statement confines the applicability of another, as shown in the examples in Table 1. 2.2 Cross-document Structure Theory Cross-document Structure Theory (CST), developed by Radev (200</context>
</contexts>
<marker>Szpektor, Shnarch, Dagan, 2007</marker>
<rawString>Szpektor, Idan, Eyal Shnarch, and Ido Dagan. 2007. Instance-based evaluation of entailment rule acquisition. In Proc. of the 45th Annual Meeting of the Association of Computational Linguistics, pages 456–463.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yotaro Watanabe</author>
<author>Masayuki Asahara</author>
<author>Yuji Matsumoto</author>
</authors>
<title>A structured model for joint learning of argument roles and predicate senses.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association of Computational Linguistics</booktitle>
<note>(to appear).</note>
<contexts>
<context position="16986" citStr="Watanabe et al., 2010" startWordPosition="2617" endWordPosition="2620">urse analysis and is an area of future work, but our basic detection strategy is as follows: 1. Identify a [AGREEMENT] or [CONFLICT] relation between the Query and Text 2. Search the Text sentence for cues that identify [CONFINEMENT] or [EVIDENCE] 24 B C 897:; &amp; 85# )&amp; 9 3-$)3/$+3 J 3&apos; #)*$ ()BC- : B #$)23# )..2%5)0&apos;+&amp; &lt;=&gt;?@A/ #)&amp; :$$+ &amp;#&apos;;+ &amp;quot;#$%&amp;&apos; ! &amp;quot;#$ %&amp;&apos;()*&apos;+$ 1234 0 3$&apos;.&apos;-&apos;&amp;%&amp; &apos;&amp; 6 7 67 5 -%&apos;8&amp; *) !&amp;quot; #$%&amp;&apos; &amp;quot;#$ %&amp;&apos;()*&apos;+$ J ) J ()* ,&apos;- .-&apos;/&apos;0+1 #$)23# B +,- $4$50*$ C 5 C ./ %&amp; ? Figure 1: An example of structural alignment sumoto, 2002) and the predicate-argument structure analyzer ChaPAS (Watanabe et al., 2010). CaboCha splits the Japanese text into phrase-like chunks and represents syntactic dependencies between the chunks as edges in a graph. ChaPAS identifies predicate-argument structures in the dependency graph produced by CaboCha. We also conduct extended modality analysis using the resources provided by Matsuyoshi et al. (2010), focusing on tense, modality, and polarity, because such information provides important clues for the recognition of semantic relations between statements. 4.3 Structural Alignment In this section, we describe our approach to structural alignment. The structural alignme</context>
</contexts>
<marker>Watanabe, Asahara, Matsumoto, 2010</marker>
<rawString>Watanabe, Yotaro, Masayuki Asahara, and Yuji Matsumoto. 2010. A structured model for joint learning of argument roles and predicate senses. In Proceedings of the 48th Annual Meeting of the Association of Computational Linguistics (to appear).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Claire Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language. Language Resources and Evaluation,</title>
<date>2005</date>
<pages>39--2</pages>
<contexts>
<context position="11517" citStr="Wiebe et al., 2005" startWordPosition="1738" endWordPosition="1741">Okumura, 2005). However, their target relations were limited to [EQUIVALENCE] and [TRANSITION]; other relations were not targeted. Recognizing [EVIDENCE] is indispensable for organizing information on the Internet. We need to develop satisfactory methods of [EVIDENCE] recognition. 2.4 Opinion Mining and Sentiment Analysis Subjective statements, such as opinions, have recently been the focus of much NLP research including review analysis, opinion extraction, opinion question answering, and sentiment analysis. In the corpus constructed in the Multi-Perspective Question Answering (MPQA) Project (Wiebe et al., 2005), individual expressions are tagged that correspond to explicit mentions of private states, speech event, and expressive subjective elements. The goal of opinion mining to extract expressions with polarity from texts, not to recognize semantic relations between sentences. Sentiment analysis also focus classifying subjective expressions in texts into positive/negative classes. In comparison, although we deal with sentiment information in text, our objective is to recognize semantic relations between sentences. If a user’s query requires positive/negative information, we will also need to extrac</context>
</contexts>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>Wiebe, Janyce, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emotions in language. Language Resources and Evaluation, 39(2-3):165–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mann William</author>
<author>Sandra Thompson</author>
</authors>
<title>Rhetorical structure theory: towards a functional theory of text organization.</title>
<date>1988</date>
<tech>Text, 8(3):243–281.</tech>
<contexts>
<context position="7877" citStr="William and Thompson, 1988" startWordPosition="1215" endWordPosition="1218">e definition of contradiction in RTE is that T contradicts H if it is very unlikely that both T and H can be true at the same time. However, in real documents on the Web, there are many pairs of examples which are contradictory in part, or where one statement confines the applicability of another, as shown in the examples in Table 1. 2.2 Cross-document Structure Theory Cross-document Structure Theory (CST), developed by Radev (2000), is another task of recognizing semantic relations between sentences. CST is an expanded rhetorical structure analysis based on Rhetorical Structure Theory (RST: (William and Thompson, 1988)), and attempts to describe the semantic relations that exist between two or more sentences from different source documents that are related to the same topic, as well as those that come from a single source document. A corpus of crossdocument sentences annotated with CST relations has also been constructed (The CSTBank Corpus: (Radev et al., 2003)). CSTBank is organized into clusters of topically-related articles. There are 18 kinds of semantic relations in this corpus, not limited to [EQUIVALENCE] or [CONTRADICTION], but also including [JUDGEMENT], [ELABORATION], and [RE22 Query Matching sen</context>
</contexts>
<marker>William, Thompson, 1988</marker>
<rawString>William, Mann and Sandra Thompson. 1988. Rhetorical structure theory: towards a functional theory of text organization. Text, 8(3):243–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhu Zhang</author>
<author>Dragomir Radev</author>
</authors>
<title>Combining labeled and unlabeled data for learning crossdocument structural relationships.</title>
<date>2004</date>
<booktitle>In Proc. of the Proceedings of IJC-NLP.</booktitle>
<contexts>
<context position="10445" citStr="Zhang and Radev (2004)" startWordPosition="1583" endWordPosition="1586">ined 14 kinds of semantic relations in their corpus. CST was designed for objective expressions because its target data is newspaper articles related to the same topic. Facts, which can be extracted from newspaper articles, have been used in conventional NLP research, such as Information Extraction or Factoid Question Answering. However, there are a lot of opinions on the Web, and it is important to survey opinions in addition to facts to give Internet users a comprehensive view of the discussions on topics of interest. 2.3 Cross-document Summarization Based on CST Relations between Sentences Zhang and Radev (2004) attempted to classify CST relations between sentence pairs extracted from topically related documents. However, they used a vector space model and tried multi-class classification. The results were not satisfactory. This observation may indicate that the recognition methods for each relation should be developed separately. Miyabe et al. (2008) attempted to recognize relations that were defined in a Japanese cross-document relation corpus (Etoh and Okumura, 2005). However, their target relations were limited to [EQUIVALENCE] and [TRANSITION]; other relations were not targeted. Recognizing [EVI</context>
</contexts>
<marker>Zhang, Radev, 2004</marker>
<rawString>Zhang, Zhu and Dragomir Radev. 2004. Combining labeled and unlabeled data for learning crossdocument structural relationships. In Proc. of the Proceedings of IJC-NLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>