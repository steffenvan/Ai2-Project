<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003346">
<title confidence="0.8203835">
Interpretation of Compound Nominalisations using Corpus and Web
Statistics
</title>
<author confidence="0.916914">
Jeremy Nicholson and Timothy Baldwin
</author>
<affiliation confidence="0.9808766">
Department of Computer Science and Software Engineering
University of Melbourne, VIC 3010, Australia
and
NICTA Victoria Research Laboratories
University of Melbourne, VIC 3010, Australia
</affiliation>
<email confidence="0.997517">
{jeremymn,tim}@csse.unimelb.edu.au
</email>
<sectionHeader confidence="0.994765" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999854866666667">
We present two novel paraphrase tests for
automatically predicting the inherent se-
mantic relation of a given compound nom-
inalisation as one of subject, direct object,
or prepositional object. We compare these
to the usual verb–argument paraphrase test
using corpus statistics, and frequencies ob-
tained by scraping the Google search en-
gine interface. We also implemented a
more robust statistical measure than max-
imum likelihood estimation — the con-
fidence interval. A significant reduction
in data sparseness was achieved, but this
alone is insufficient to provide a substan-
tial performance improvement.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999876333333333">
Compound nouns are a class of multiword expres-
sion (MWE) that have been of interest in recent
computational linguistic work, as any task with a
lexical semantic dimension (like machine transla-
tion or information extraction) must take into ac-
count their semantic markedness. A compound
noun is a sequence of two or more nouns compris-
ing an N, for example, polystyrene garden-gnome.
The productivity of compound nouns makes their
treatment equally desirable and difficult. They ap-
pear frequently: more than 1% of the words in the
British National Corpus (BNC: Burnard (2000))
participate in noun compounds (Tanaka and Bald-
win, 2003). However, unestablished compounds
are common: almost 70% of compounds identi-
fied in the BNC co-occur with a frequency of only
one (Lapata and Lascarides, 2003).
Analysis of the entire space of compound nouns
has been hampered to some degree as the space de-
fies some regular set of predicates to define the im-
plicit semantics between a modifier and its head.
This semantic underspecification led early analy-
sis to be primarily of a semantic nature, but more
recent work has advanced into using syntax to pre-
dict the semantics, in the spirit of the study by
Levin (1993) on diathesis alternations.
In this work, we examine compound nominal-
isations, a subset of compound nouns where the
head has a morphologically–related verb. For
example, product replacement has an underlying
verbal head replace, whereas garden-gnome has
no such form. While compound nouns in gen-
eral have a set of semantic relationships between
the head and modifier that is potentially non-finite,
compound nominalisations are better defined, in
that the modifier fills a syntactic argument rela-
tion with respect to the head. For example, prod-
uct might fill the direct object slot of the verb
to replace for the compound above. Compound
nominalisations comprise a substantial minority of
compound nouns, with figures of about 35% being
observed (Grover et al., 2005; Nicholson, 2005).
We propose two novel paraphrases for a corpus
statistical approach to predicting the relationship
for a set of compound nominalisations, and inves-
tigate how using the World Wide Web as a cor-
pus alleviates the common phenomenon of data
sparseness, and how the volume of data impacts
on the classification results. We also examine
a more robust statistical approach to interpreta-
tion of the statistics than maximum likelihood es-
timates, called the confidence interval.
The rest of the paper is structured as follows: in
Section 2, we present a brief background for our
work, with a listing of our resources in Section 3.
We detail our proposed method in Section 4, the
corresponding results in Section 5, with a discus-
</bodyText>
<page confidence="0.980184">
54
</page>
<note confidence="0.7008955">
Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties, pages 54–61,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.979442">
sion in Section 6 and a brief conclusion in Sec-
tion 7.
</bodyText>
<sectionHeader confidence="0.997331" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.990936">
2.1 Compound Noun Interpretation
</subsectionHeader>
<bodyText confidence="0.999953151515152">
Compound nouns were seminally and thoroughly
analysed by Levi (1978), who hand–constructs a
nine–way set of semantic relations that she identi-
fies as broadly defining the observed relationships
between the compound head and modifier. War-
ren (1978) also inspects the syntax of compound
nouns, to create a somewhat different set of twelve
conceptual categories.
Early attempts to automatically classify com-
pound nouns have taken a semantic approach:
Finin (1980) and Isabelle (1984) use “role nomi-
nals” derived from the head of the compound to
fill a slot with the modifier. Vanderwende (1994)
uses a rule–based technique that scores a com-
pound on possible semantic interpretations, while
Jones (1995) implements a graph–based unifica-
tion procedure over semantic feature structures for
the head. Finally, Rosario and Hearst (2001) make
use of a domain–specific lexical resource to clas-
sify according to neural networks and decision
trees.
Syntactic classification, using paraphrasing,
was first used by Leonard (1984), who uses a pri-
oritised rule–based approach across a number of
possible readings. Lauer (1995) employs a cor-
pus statistical model over a similar paraphrase
set based on prepositions. Lapata (2002) and
Grover et al. (2005) again use a corpus statis-
tical paraphrase–based approach, but with verb–
argument relations for compound nominalisations
— attempting to define the relation as one of sub-
ject, direct object, or a number of prepositional ob-
jects in the latter.
</bodyText>
<subsectionHeader confidence="0.99564">
2.2 Web–as–Corpus Approaches
</subsectionHeader>
<bodyText confidence="0.999665952380952">
Using the World Wide Web for corpus statistics
is a relatively recent phenomenon; we present a
few notable examples. Grefenstette (1998) anal-
yses the plausibility of candidate translations in
a machine translation task through Web statistics,
and avoids some data sparseness within that con-
text. Zhu and Rosenfeld (2001) train a language
model from a large corpus, and use the Web to
estimate low–density trigram frequencies. Keller
and Lapata (2003) show that Web counts can
obviate data sparseness for syntactic predicate–
argument bigrams. They also observe that the
noisiness of the Web, while unexplored in detail,
does not greatly reduce the reliability of their re-
sults. Nakov and Hearst (2005) demonstrate that
Web counts can aid in identifying the bracketing in
higher–arity noun compounds. Finally, Lapata and
Keller (2005) evaluate the performance of Web
counts on a wide range of natural language pro-
cessing tasks, including compound noun bracket-
ing and compound noun interpretation.
</bodyText>
<subsectionHeader confidence="0.998603">
2.3 Confidence Intervals
</subsectionHeader>
<bodyText confidence="0.9992798">
Maximum likelihood statistics are not robust when
many sparse vectors are under consideration, i.e.
naively “choosing the largest number” may not be
accurate in contexts when the relative value across
samplings may be relevant, for example, in ma-
chine learning. As such, we apply a statistical
test with confidence intervals (Kenney and Keep-
ing, 1962), where we compare sample z-scores in
a pairwise manner, instead of frequencies globally.
The confidence interval P, for z-score n, is:
</bodyText>
<equation confidence="0.9730735">
P = 2 2 e−t2dt (1)
,/7rfn
</equation>
<bodyText confidence="0.998986714285714">
t is chosen to normalise the curve, and P is strictly
increasing on n, so we are only required to find the
largest z-score.
Calculating the z-score exactly can be quite
costly, so we instead use the binomial approxi-
mation to the normal distribution with equal prior
probabilities and find that a given z-score Z is:
</bodyText>
<equation confidence="0.997890666666667">
Z = f (2)
− µ
Q
</equation>
<bodyText confidence="0.9996935">
where f is the frequency count, µ is the mean in
a pairwise test, and Q is the standard deviation of
the test. A more complete derivation appears in
Nicholson (2005).
</bodyText>
<sectionHeader confidence="0.999324" genericHeader="method">
3 Resources
</sectionHeader>
<bodyText confidence="0.999748125">
We make use of a number of lexical resources
in our implementation and evaluation. For cor-
pus statistics, we use the written component of
the BNC, a balanced 90M token corpus. To find
verb–argument frequencies, we parse this using
RASP (Briscoe and Carroll, 2002), a tag sequence
grammar–based statistical parser. We contrast
the corpus statistics with ones collected from the
</bodyText>
<page confidence="0.991715">
55
</page>
<bodyText confidence="0.999669178571429">
Web, using an implementation of a freely avail-
able Google “scraper” from CPAN.1
For a given compound nominalisation, we wish
to determine all possible verbal forms of the head.
We do so using the combination of the morpho-
logical component of CELEX (Burnage, 1990), a
lexical database, NOMLEX (Macleod et al., 1998),
a nominalisation database, and CATVAR (Habash
and Dorr, 2003), an automatically–constructed
database of clusters of inflected words based on
the Porter stemmer (Porter, 1997).
Once the verbal forms have been identified, we
construct canonical forms of the present partici-
ple (+ing) and the past participle (+ed), using the
morph lemmatiser (Minnen et al., 2001). We con-
struct canonical forms of the plural head and plural
modifier (+s) in the same manner.
For evaluation, we have the two–way classified
data set used by Lapata (2002), and a three–way
classified data set constructed from open text.
Lapata automatically extracts candidates from
the British National Corpus, and hand–curates
a set of 796 compound nominalisations which
were interpreted as either a subjective relation
SUBJ (e.g. wood appearance “wood appears”),
or a (direct) objective relation OBJ (e.g. stress
avoidance “[SO] avoids stress”. We automatically
validated this data set for consistency, removing:
</bodyText>
<listItem confidence="0.979090571428571">
1. items that did not occur in the same chunk,
according to a chunker based on fnTBL 1.0
(Ngai and Florian, 2001),
2. items whose head did not have a verbal form
according to our lexical resources, and
3. items which consisted in part of proper
nouns,
</listItem>
<bodyText confidence="0.848577733333334">
to end up with 695 consistent compounds. We
used the method of Nicholson and Baldwin (2005)
to derive a small data set of 129 compound
nominalisations, also from the BNC, which we
instructed three unskilled annotators to identify
each as one of subjective (SUB), direct object
(DOB), or prepositional object (POB, e.g. side
show “[SO] show [ST] on the side”). The an-
notators identified nine prepositional relations:
{about,against,for,from,in,into,on,to,with}.
&apos;www.cpan.org: We limit our usage to examining the
“Estimated Results Returned”, so that our usage is identi-
cal to running the queries manually from the website. The
Google API (www.google.com/apis) gives a method
for examining the actual text of the returned documents.
</bodyText>
<sectionHeader confidence="0.975183" genericHeader="method">
4 Proposed Method
</sectionHeader>
<subsectionHeader confidence="0.995845">
4.1 Paraphrase Tests
</subsectionHeader>
<bodyText confidence="0.999968242424242">
To derive preferences for the SUB, DOB, and var-
ious POB interpretations for a given compound
nominalisation, the most obvious approach is to
examine a parsed corpus for instances of the verbal
form of the head and the modifier occurring in the
corresponding verb–argument relation. There are
other constructions that can be informative, how-
ever.
We examine two novel paraphrase tests: one
prepositional and one participial. The preposi-
tional test is based in part on the work by Leonard
(1984) and Lauer (1995): for a given compound,
we search for instances of the head and modifier
nouns separated by a preposition. For example,
for the compound nominalisation leg operation,
we might search for operation on the leg, corre-
sponding to the POB relation on. Special cases are
by, corresponding to a subjective reading akin to a
passive construction (e.g. investor hesitancy, hesi-
tancy by the investor - “the investor hesitates”),
and of, corresponding to a direct object reading
(e.g. language speaker, speaker of the language
� “[SO] speaks the language”).
The participial test is based on the paraphras-
ing equivalence of using the present participle of
the verbal head as an adjective before the modifier,
for the SUB relation (e.g. the hesitating investor �
“the investor hesitates”), compared to the past par-
ticiple for the DOB relation (the spoken language
� “[SO] speaks the language”). The correspond-
ing prepositional object construction is unusual in
English, but still possible: compare ?the operated-
on leg and the lived-in village.
</bodyText>
<subsectionHeader confidence="0.994033">
4.2 The Algorithm
</subsectionHeader>
<bodyText confidence="0.999976642857143">
Given a compound nominalisation, we perform a
number of steps to arrive at an interpretation. First,
we derive a set of verbal forms for the head from
the combination of CELEX, NOMLEX, and CAT-
VAR. We find the participial forms of each of the
verbal heads, and plurals for the nominal head and
modifier, using the morph lemmatiser.
Next, we examine the BNC for instances of the
modifier and one of the verbal head forms oc-
curring in a verb–argument relation, with the aid
of the RASP parse. Using these frequencies, we
calculate the pairwise z-scores between SUB and
DOB, and between SUB and POB: the score given
to the SUB interpretation is the greater of the two.
</bodyText>
<page confidence="0.984966">
56
</page>
<bodyText confidence="0.999780647058824">
We further examine the RASP parsed data for in-
stances of the prepositional and participial tests for
the compound, and calculate the z-scores for these
as well.
We then collect our Google counts. Because the
Web data is unparsed, we cannot look for syntactic
structures explictly. Instead, we query a number of
collocations which we expect to be representative
of the desired structure.
For the prepositional test, the head can be sin-
gular or plural, the modifier can be singular or plu-
ral, and there may or may not be an article be-
tween the preposition and the modifier. For exam-
ple, for the compound nominalisation product re-
placement and preposition of we search for all of
the following: (and similarly for the other prepo-
sitions)
</bodyText>
<construct confidence="0.57693625">
replacement ofproduct
replacement of the product
replacement ofproducts
replacement of the products
replacements ofproduct
replacements of the product
replacements ofproducts
replacements of the products
</construct>
<bodyText confidence="0.9996002">
For the participial test, the modifier can be sin-
gular or plural, and if we are examining a prepo-
sitional relation, the head can be either a present
or past participle. For product replacement, we
search for, as well as other prepositions:
</bodyText>
<construct confidence="0.9855505">
the replacing product
the replacing products
the replaced product
the replaced products
the replacing–about product
the replacing–about products
the replaced–about product
the replaced–about products
</construct>
<bodyText confidence="0.999755411764706">
We comment briefly on these tests in Section 6.
We choose to use the as our canonical article be-
cause it is a reliable marker of the left boundary of
an NP and number-neutral; using a/an represents
a needless complication.
We then calculate the z-scores using the method
described in Section 2, where the individual fre-
quency counts are the maximum of the results ob-
tained across the query set.
Once the z-scores have been obtained, we
choose a classification based on the greatest-
valued observed test. We contrast the confidence
interval–based approach with the maximum like-
lihood method of choosing the largest of the raw
frequencies. We also experiment with a machine
learning package, to examine the mutual predic-
tiveness of the separate tests.
</bodyText>
<sectionHeader confidence="0.995267" genericHeader="method">
5 Observed Results
</sectionHeader>
<bodyText confidence="0.9998738">
First, we found majority-class baselines for each
of the data sets. The two–way data set had
258 SUBJ–classified items, and 437 OBJ–classified
items, so choosing OBJ each time gives a baseline
of 62.9%. The three–way set had 22 SUB items,
63 of DOB, and 44 of POB, giving a baseline of
48.8%.
Contrasting this with human performance on
the data set, Lapata recorded a raw inter-annotator
agreement of 89.7% on her test set, which cor-
responds to a Kappa value n = 0.78. On the
three–way data set, three annotators had a agree-
ment of 98.4% for identification and classification
of observed compound nominalisations in open
text, and n = 0.83. For the three-way data set,
the annotators were asked to both identify and
classify compound nominalisations in free text,
and agreement is thus calculated over all words
in the test. The high agreement figure is due to
the fact that most words could be trivially disre-
garded (e.g. were not nouns). Kappa corrects this
for chance agreement, so we conclude that this
task was still better-defined than the one posed
by Lapata. One possible reason for this was the
number of poorly–behaved compounds that we re-
moved due to chunk inconsistencies, lack of a ver-
bal form, or proper nouns: it would be difficult for
the annotators to agree over compounds where an
obvious well–defined interpretation was not avail-
able.
</bodyText>
<subsectionHeader confidence="0.987457">
5.1 Comparison Classification
</subsectionHeader>
<bodyText confidence="0.998767833333333">
Results for classification over the Lapata two–way
data set are given in Table 1, and results over
the open data three–way set are given in Table 2.
For these, we selected the greatest raw frequency
count for a given test as the intended relation
(Raw), or the greatest confidence interval accord-
ing to the z-score (Z-Score). If a relation could not
be selected due to ties (e.g., the scores were all 0),
we selected the majority baseline. To deal with the
nature of the two–way data set with respect to our
three–way selection, we mapped compounds that
we would prefer to be POB to OBJ, as there are
</bodyText>
<page confidence="0.998272">
57
</page>
<table confidence="0.9996434">
Paraphrase Default Corpus Counts Web Counts
Raw Z-Score Raw Z-Score
Verb–Argument 62.9 67.9 68.3 – –
Prepositional 62.9 62.1 62.4 62.6 63.0
Participial 62.9 63.0 63.2 61.4 58.8
</table>
<tableCaption confidence="0.790935">
Table 1: Classification Results over the two–way data set, in %. Comparison of raw frequency counts
vs. confidence–based z-scores, for BNC data and Google scrapings shown.
</tableCaption>
<table confidence="0.9998824">
Paraphrase Default Corpus Counts Web Counts
Raw Z-Score Raw Z-Score
Verb–Argument 48.8 54.3 55.0 – –
Prepositional 48.8 48.4 50.0 59.7 58.9
Participial 48.8 43.2 45.4 43.4 38.0
</table>
<tableCaption confidence="0.994896">
Table 2: Classification results over the three-way data set, in %. Comparison of raw frequency counts
</tableCaption>
<bodyText confidence="0.973539111111111">
vs. confidence-based z-scores, for BNC data and Google scrapings shown.
compounds in the set (e.g. adult provision) that
have a prepositional object reading (“provide for
adults”) but have been classified as a direct object
OBJ.
The verb–argument counts obtained from the
parsed BNC are significantly better than the base-
line for the Lapata data set (x2 = 4.12,p &lt; 0.05),
but not significantly better for the open data set
(x2 = 0.99,p &lt; 1). Similar results were reported
by Lapata (2002) over her data set using backed–
off smoothing, the most closely related method.
Neither the prepositional nor participial para-
phrases were significantly better than the baseline
for either the two–way (x2 = 0.00,p &lt; 1), or
the three–way data set (x2 = 3.52,p &lt; 0.10), al-
though the prepositional test did slightly improve
on the verb–argument results.
</bodyText>
<subsectionHeader confidence="0.997057">
5.2 Machine Learning Classification
</subsectionHeader>
<bodyText confidence="0.9994412">
Although the results were not impressive, we still
believed that there was complementing informa-
tion within the data, which could be extracted with
the aid of a machine learner. For this, we made
use of TiMBL (Daelemans et al., 2003), a nearest-
neighbour classifier which stores the entire train-
ing set and extrapolates further samples, as a prin-
cipled method for combination of the data. We use
TiMBL’s in-built cross-validation method: 90% of
the data set is used as training data to test the other
10%, for each stratified tenth of the set. The results
it achieves are assumed to be able to generalise to
new samples if they are compared to the current
training data set.
The results observed using TiMBL are shown
</bodyText>
<subsectionHeader confidence="0.599191">
Corpus Counts Web Counts
</subsectionHeader>
<bodyText confidence="0.94857888">
Two–way Set
Three–way Set
Table 3: TiMBL results for the combination of
paraphrase tests over the two–way and three–way
data sets for corpus and Web frequencies
in Table 3. This was from the combination
of all of the available paraphrase tests: verb–
argument, prepositional, and participial for the
corpus counts, and just prepositional and particip-
ial for the Web counts. The results for the two–
way data set derived from Lapata’s data set were a
good improvement over the simple classification
results, significantly so for the Web frequencies
(x2 = 20.3,p &lt; 0.01). However, we also no-
tice a corresponding decrease in the results for the
three–way open data set, which make these im-
provements immaterial.
Examining the other possible combinations for
the tests did indeed lead to varying results, but not
in a consistent manner. For example, the best com-
bination for the open data set was using the par-
ticipial raw scores and z-scores (58.1%), which
performed particularly badly in simple compar-
isons, and comparatively poorly (70.2%) for the
two–way set.
</bodyText>
<sectionHeader confidence="0.999151" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.9974005">
Although the observed results failed to match, or
even approach, various benchmarks set by La-
pata (2002) (87.3% accuracy) and Grover et al.
(2005) (77%) for the subject–object and subject–
</bodyText>
<figure confidence="0.899342">
72.4
51.1
74.2
50.4
</figure>
<page confidence="0.99803">
58
</page>
<bodyText confidence="0.994579170000001">
direct object–prepositional objects classification
tasks respectively, the presented approach is not
without merit. Indeed, these results relied on
machine learning approaches incorporating many
features independent of corpus counts: namely,
context, suffix information, and semantic similar-
ity resources. Our results were an examination
of the possible contribution of lexical information
available from high–volume unparsed text.
One important concept used in the above bench-
marks was that of statistical smoothing, both
class–based and distance–based. The reason for
this is the inherent data sparseness within the
corpus statistics for these paraphrase tests. La-
pata (2002) observes that almost half (47%) of
the verb–noun pairs constructed are not attested
within the BNC. Grover et al. (2005) also note the
sparseness of observed relations. Using the im-
mense data source of the Web allows one to cir-
cumvent this problem: only one compound (an-
archist prohibition) has no instances of the para-
phrases from the scraping,2 from more than 900
compounds between the two data sets. This ex-
tra information, we surmise, would be beneficial
for the smoothing procedures, as the comparative
accuracy between the two methods is similar.
On the other hand, we also observe that sim-
ply alleviating the data sparseness is insufficient
to provide a reliable interpretation. These results
reinforce the contribution made by the statistical
and semantic resources used in arriving at these
benchmarks.
The approach suggested by Keller and Lapata
(2003) for obtaining bigram information from the
Web could provide an approach for estimating the
syntactic verb–argument counts for a given com-
pound (dashes in Tables 1 and 2). In spite of
the inherent unreliability of approximating long–
range dependencies with n-gram information, re-
sults look promising. An examination of the effec-
tiveness of this approach is left as further research.
Similarly, various methods of combining corpus
counts with the Web counts, including smooth-
ing, backing–off, and machine learning, could also
lead to interesting performance impacts.
Another item of interest is the comparative dif-
ficulty of the task presented by the three–way data
set extracted from open data, and the two–way
data set hand–curated by Lapata. The baseline
2Interestingly, Google only lists 3 occurrences of this
compound anyway, so token relevance is low — further in-
spection shows that those 3 are not well-formed in any case.
of this set is much lower, even compared that
of the similar task (albeit domain–specific) from
Grover et al. (2005) of 58.6%. We posit that the
hand–filtering of the data set in these works con-
tributes to a biased sample. For example, remov-
ing prepositional objects for a two–way classifica-
tion, which make up about a third of the open data
set, renders the task somewhat artificial.
Comparison of the results between the maxi-
mum likelihood estimates used in earlier work,
and the more statistically robust confidence inter-
vals were inconclusive as to performance improve-
ment, and were most effective as a feature expan-
sion algorithm. The only obvious result is an aes-
thetic one, in using “robust statistics”.
Finally, the paraphrase tests which we propose
are not without drawbacks. In the prepositional
test, a paraphrase with of does not strictly con-
tribute to a direct object reading: consider school
aim “school aims”, for which instances of aim by
the school are overwhelmed by aim of the school.
We experimented with permutations of the avail-
able queries (e.g. requiring the head and modifier
to be of different number, to reflect the pluralis-
ability of the head in such compounds, e.g. aims
of the school), without observing substantially dif-
ferent results.
Another observation is the inherent bias of the
prepositional test to the prepositional object re-
lation. Apparent prepositional relations can oc-
cur in spite of the available verb frames: con-
sider cash limitation, where the most populous in-
stance is limitation on cash, despite the impossi-
bility of *to limit on cash (for to place a limit on
cash). Another example, is bank agreement: find-
ing instances of agreement with bank does not lead
to the pragmatically absurd [so] agrees with the
bank.
Correspondingly, the participial relation has the
opposite bias: constructions of the form the lived-
in flat “[SO] lived in the flat” are usually lexi-
calised in English. As such, only 17% of com-
pounds in the two–way data set and 34% of the
three-way data set display non-zero values in the
prepositional object relation for the participial test.
We hoped that the inherent biases of the two tests
might balance each other, but there is little evi-
dence of that from the results.
</bodyText>
<page confidence="0.998527">
59
</page>
<sectionHeader confidence="0.999107" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999946923076923">
We presented two novel paraphrase tests for au-
tomatically predicting the inherent semantic rela-
tion of a given compound nominalisation as one of
subject, direct object, or prepositional object. We
compared these to the usual verb–argument para-
phrase test, using corpus statistics, and frequen-
cies obtained by scraping the Google search en-
gine. We also implemented a more robust statisti-
cal measure than the insipid maximum likelihood
estimates — the confidence interval. A significant
reduction in data sparseness was achieved, but this
alone is insufficient to provide a substantial per-
formance improvement.
</bodyText>
<sectionHeader confidence="0.996278" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999836">
We would like to thank the members of the Univer-
sity of Melbourne LT group and the three anony-
mous reviewers for their valuable input on this re-
search, as well as Mirella Lapata for allowing use
of the data.
</bodyText>
<sectionHeader confidence="0.998002" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.989385556962026">
Ted Briscoe and John Carroll. 2002. Robust accurate
statistical annotation of general text. In Proceedings
of the 3rd International Conference on Language
Resources and Evaluation, pages 1499–1504, Las
Palmas, Canary Islands.
Gavin Burnage. 1990. CELEX: A guide for users.
Technical report, University of Nijmegen.
Lou Burnard. 2000. User Reference Guide for the
British National Corpus. Technical report, Oxford
University Computing Services.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and
Antal van den Bosch. 2003. TiMBL: Tilburg Mem-
ory Based Learner, version 5.0, Reference Guide.
ILK Technical Report 03-10.
Tim Finin. 1980. The semantic interpretation of nom-
inal compounds. In Proceedings of the First Na-
tional Conference on Artificial Intelligence, pages
310–315, Stanford, USA. AAAI Press.
Gregory Grefenstette. 1998. The World Wide Web
as a resource for example-based machine translation
tasks. In Proceedings of the ASLIB Conference on
Translating and the Computer, London, UK.
Claire Grover, Mirella Lapata, and Alex Lascarides.
2005. A comparison of parsing technologies for the
biomedical domain. Journal of Natural Language
Engineering, 11(01):27–65.
Nizar Habash and Bonnie Dorr. 2003. A categorial
variation database for English. In Proceedings of
the 2003 Human Language Technology Conference
of the North American Chapter of the ACL, pages
17–23, Edmonton, Canada.
Pierre Isabelle. 1984. Another look at nominal com-
pounds. In Proceeedings of the 10th International
Conference on Computational Linguistics and 22nd
Annual Meeting of the ACL, pages 509–516, Stan-
ford, USA.
Bernard Jones. 1995. Predicating nominal com-
pounds. In Proceedings of the 17th International
Conference of the Cognitive Science Society, pages
130–5, Pittsburgh, USA.
Frank Keller and Mirella Lapata. 2003. Using the web
to obtain frequencies for unseen bigrams. Computa-
tional Linguistics, 29(3):459–484.
John F. Kenney and E. S. Keeping, 1962. Mathematics
of Statistics, Pt. 1, chapter 11.4, pages 167–9. Van
Nostrand, Princeton, USA, 3rd edition.
Mirella Lapata and Frank Keller. 2005. Web-based
models for natural language processing. ACM
Transactions on Speech and Language Processing,
2(1).
Mirella Lapata and Alex Lascarides. 2003. Detect-
ing novel compounds: The role of distributional
evidence. In Proceedings of the 10th Conference
of the European Chapter of the Association for
Computional Linguistics, pages 235–242, Budapest,
Hungary.
Maria Lapata. 2002. The disambiguation of nomi-
nalizations. Computational Linguistics, 28(3):357–
388.
Mark Lauer. 1995. Designing Statistical Language
Learners: Experiments on Noun Compounds. Ph.D.
thesis, Macquarie University, Sydney, Australia.
Rosemary Leonard. 1984. The Interpretation of En-
glish Noun Sequences on the Computer. Elsevier
Science, Amsterdam, the Netherlands.
Judith Levi. 1978. The Syntax and Semantics of Com-
plex Nominals. Academic Press, New York, USA.
Beth Levin. 1993. English Verb Classes and Alter-
nations. The University of Chicago Press, Chicago,
USA.
Catherine Macleod, Ralph Grishman, Adam Meyers,
Leslie Barrett, and Ruth Reeves. 1998. NOMLEX:
A lexicon of nominalizations. In Proceedings of the
8th International Congress of the European Associ-
ation for Lexicography, pages 187–193, Liege, Bel-
gium.
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of English. Nat-
ural Language Engineering, 7(3):207–23.
</reference>
<page confidence="0.966955">
60
</page>
<reference confidence="0.999592636363636">
Preslov Nakov and Marti Hearst. 2005. Search en-
gine statistics beyond the n-gram: Application to
noun compound bracketing. In Proceedings of the
Ninth Conference on Computational Natural Lan-
guage Learning, pages 17–24, Ann Arbor, USA.
Grace Ngai and Radu Florian. 2001. Transformation-
based learning in the fast lane. In Proceedings of the
2nd Annual Meeting of the North American Chap-
ter of the Association for Computational Linguistics,
pages 40–7, Pittsburgh, USA.
Jeremy Nicholson and Timothy Baldwin. 2005. Sta-
tistical interpretation of compound nominalisations.
In Proceeding of the Australasian Langugae Tech-
nology Workshop 2005, Sydney, Australia.
Jeremy Nicholson. 2005. Statistical interpretation of
compound nouns. Honours Thesis, University of
Melbourne, Melbourne, Australia.
Martin Porter. 1997. An algorithm for suffix strip-
ping. In Karen Sparck Jones and Peter Willett,
editors, Readings in information retrieval. Morgan
Kaufmann, San Francisco, USA.
Barbara Rosario and Marti Hearst. 2001. Classify-
ing the semantic relations in noun compounds via a
domain-specific lexical hierarchy. In Proceedings of
the 6th Conference on Empirical Methods in Natural
Language Processing, Pittsburgh, USA.
Takaaki Tanaka and Timothy Baldwin. 2003. Noun-
noun compound machine translation: A feasibility
study on shallow processing. In Proceedings of
the ACL 2003 Workshop on Multiword Expressions:
Analysis, Acquisition and Treatment, pages 17–24,
Sapporo, Japan.
Lucy Vanderwende. 1994. Algorithm for automatic
interpretation of noun sequences. In Proceedings of
the 15th International Conference on Computational
Linguistics, pages 782–788, Kyoto, Japan.
Beatrice Warren. 1978. Semantic Patterns of Noun-
Noun Compounds. Acta Universitatis Gothoburgen-
sis, G¨oteborg, Sweden.
Xiaojin Zhu and Ronald Rosenfeld. 2001. Improv-
ing trigram language modeling with the World Wide
Web. In Proceedings of the International Confer-
ence on Acoustics, Speech, and Signal Processing,
pages 533–6, Salt Lake City, USA.
</reference>
<page confidence="0.999271">
61
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.401114">
<title confidence="0.990468">Interpretation of Compound Nominalisations using Corpus and Web Statistics</title>
<author confidence="0.999892">Jeremy Nicholson</author>
<author confidence="0.999892">Timothy Baldwin</author>
<affiliation confidence="0.90029675">Department of Computer Science and Software University of Melbourne, VIC 3010, Australia and NICTA Victoria Research</affiliation>
<address confidence="0.580378">University of Melbourne, VIC 3010, Australia</address>
<email confidence="0.997617">jeremymn@csse.unimelb.edu.au</email>
<email confidence="0.997617">tim@csse.unimelb.edu.au</email>
<abstract confidence="0.997656125">We present two novel paraphrase tests for automatically predicting the inherent semantic relation of a given compound nominalisation as one of subject, direct object, or prepositional object. We compare these to the usual verb–argument paraphrase test using corpus statistics, and frequencies obtained by scraping the Google search engine interface. We also implemented a more robust statistical measure than maximum likelihood estimation — the confidence interval. A significant reduction in data sparseness was achieved, but this alone is insufficient to provide a substantial performance improvement.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
</authors>
<title>Robust accurate statistical annotation of general text.</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd International Conference on Language Resources and Evaluation,</booktitle>
<pages>1499--1504</pages>
<location>Las Palmas, Canary Islands.</location>
<contexts>
<context position="7770" citStr="Briscoe and Carroll, 2002" startWordPosition="1221" endWordPosition="1224">exactly can be quite costly, so we instead use the binomial approximation to the normal distribution with equal prior probabilities and find that a given z-score Z is: Z = f (2) − µ Q where f is the frequency count, µ is the mean in a pairwise test, and Q is the standard deviation of the test. A more complete derivation appears in Nicholson (2005). 3 Resources We make use of a number of lexical resources in our implementation and evaluation. For corpus statistics, we use the written component of the BNC, a balanced 90M token corpus. To find verb–argument frequencies, we parse this using RASP (Briscoe and Carroll, 2002), a tag sequence grammar–based statistical parser. We contrast the corpus statistics with ones collected from the 55 Web, using an implementation of a freely available Google “scraper” from CPAN.1 For a given compound nominalisation, we wish to determine all possible verbal forms of the head. We do so using the combination of the morphological component of CELEX (Burnage, 1990), a lexical database, NOMLEX (Macleod et al., 1998), a nominalisation database, and CATVAR (Habash and Dorr, 2003), an automatically–constructed database of clusters of inflected words based on the Porter stemmer (Porter</context>
</contexts>
<marker>Briscoe, Carroll, 2002</marker>
<rawString>Ted Briscoe and John Carroll. 2002. Robust accurate statistical annotation of general text. In Proceedings of the 3rd International Conference on Language Resources and Evaluation, pages 1499–1504, Las Palmas, Canary Islands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gavin Burnage</author>
</authors>
<title>CELEX: A guide for users.</title>
<date>1990</date>
<tech>Technical report,</tech>
<institution>University of Nijmegen.</institution>
<contexts>
<context position="8150" citStr="Burnage, 1990" startWordPosition="1284" endWordPosition="1285">xical resources in our implementation and evaluation. For corpus statistics, we use the written component of the BNC, a balanced 90M token corpus. To find verb–argument frequencies, we parse this using RASP (Briscoe and Carroll, 2002), a tag sequence grammar–based statistical parser. We contrast the corpus statistics with ones collected from the 55 Web, using an implementation of a freely available Google “scraper” from CPAN.1 For a given compound nominalisation, we wish to determine all possible verbal forms of the head. We do so using the combination of the morphological component of CELEX (Burnage, 1990), a lexical database, NOMLEX (Macleod et al., 1998), a nominalisation database, and CATVAR (Habash and Dorr, 2003), an automatically–constructed database of clusters of inflected words based on the Porter stemmer (Porter, 1997). Once the verbal forms have been identified, we construct canonical forms of the present participle (+ing) and the past participle (+ed), using the morph lemmatiser (Minnen et al., 2001). We construct canonical forms of the plural head and plural modifier (+s) in the same manner. For evaluation, we have the two–way classified data set used by Lapata (2002), and a three–</context>
</contexts>
<marker>Burnage, 1990</marker>
<rawString>Gavin Burnage. 1990. CELEX: A guide for users. Technical report, University of Nijmegen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lou Burnard</author>
</authors>
<title>User Reference Guide for the British National Corpus.</title>
<date>2000</date>
<tech>Technical report,</tech>
<institution>Oxford University Computing Services.</institution>
<contexts>
<context position="1534" citStr="Burnard (2000)" startWordPosition="223" endWordPosition="224">l performance improvement. 1 Introduction Compound nouns are a class of multiword expression (MWE) that have been of interest in recent computational linguistic work, as any task with a lexical semantic dimension (like machine translation or information extraction) must take into account their semantic markedness. A compound noun is a sequence of two or more nouns comprising an N, for example, polystyrene garden-gnome. The productivity of compound nouns makes their treatment equally desirable and difficult. They appear frequently: more than 1% of the words in the British National Corpus (BNC: Burnard (2000)) participate in noun compounds (Tanaka and Baldwin, 2003). However, unestablished compounds are common: almost 70% of compounds identified in the BNC co-occur with a frequency of only one (Lapata and Lascarides, 2003). Analysis of the entire space of compound nouns has been hampered to some degree as the space defies some regular set of predicates to define the implicit semantics between a modifier and its head. This semantic underspecification led early analysis to be primarily of a semantic nature, but more recent work has advanced into using syntax to predict the semantics, in the spirit o</context>
</contexts>
<marker>Burnard, 2000</marker>
<rawString>Lou Burnard. 2000. User Reference Guide for the British National Corpus. Technical report, Oxford University Computing Services.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
<author>Jakub Zavrel</author>
<author>Ko van der Sloot</author>
<author>Antal van den Bosch</author>
</authors>
<title>TiMBL: Tilburg Memory Based Learner, version 5.0, Reference Guide.</title>
<date>2003</date>
<tech>ILK Technical Report 03-10.</tech>
<marker>Daelemans, Zavrel, van der Sloot, van den Bosch, 2003</marker>
<rawString>Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and Antal van den Bosch. 2003. TiMBL: Tilburg Memory Based Learner, version 5.0, Reference Guide. ILK Technical Report 03-10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Finin</author>
</authors>
<title>The semantic interpretation of nominal compounds.</title>
<date>1980</date>
<booktitle>In Proceedings of the First National Conference on Artificial Intelligence,</booktitle>
<pages>310--315</pages>
<publisher>AAAI Press.</publisher>
<location>Stanford, USA.</location>
<contexts>
<context position="4412" citStr="Finin (1980)" startWordPosition="681" endWordPosition="682">. c�2006 Association for Computational Linguistics sion in Section 6 and a brief conclusion in Section 7. 2 Background 2.1 Compound Noun Interpretation Compound nouns were seminally and thoroughly analysed by Levi (1978), who hand–constructs a nine–way set of semantic relations that she identifies as broadly defining the observed relationships between the compound head and modifier. Warren (1978) also inspects the syntax of compound nouns, to create a somewhat different set of twelve conceptual categories. Early attempts to automatically classify compound nouns have taken a semantic approach: Finin (1980) and Isabelle (1984) use “role nominals” derived from the head of the compound to fill a slot with the modifier. Vanderwende (1994) uses a rule–based technique that scores a compound on possible semantic interpretations, while Jones (1995) implements a graph–based unification procedure over semantic feature structures for the head. Finally, Rosario and Hearst (2001) make use of a domain–specific lexical resource to classify according to neural networks and decision trees. Syntactic classification, using paraphrasing, was first used by Leonard (1984), who uses a prioritised rule–based approach </context>
</contexts>
<marker>Finin, 1980</marker>
<rawString>Tim Finin. 1980. The semantic interpretation of nominal compounds. In Proceedings of the First National Conference on Artificial Intelligence, pages 310–315, Stanford, USA. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<title>The World Wide Web as a resource for example-based machine translation tasks.</title>
<date>1998</date>
<booktitle>In Proceedings of the ASLIB Conference on Translating and the Computer,</booktitle>
<location>London, UK.</location>
<contexts>
<context position="5598" citStr="Grefenstette (1998)" startWordPosition="864" endWordPosition="865"> prioritised rule–based approach across a number of possible readings. Lauer (1995) employs a corpus statistical model over a similar paraphrase set based on prepositions. Lapata (2002) and Grover et al. (2005) again use a corpus statistical paraphrase–based approach, but with verb– argument relations for compound nominalisations — attempting to define the relation as one of subject, direct object, or a number of prepositional objects in the latter. 2.2 Web–as–Corpus Approaches Using the World Wide Web for corpus statistics is a relatively recent phenomenon; we present a few notable examples. Grefenstette (1998) analyses the plausibility of candidate translations in a machine translation task through Web statistics, and avoids some data sparseness within that context. Zhu and Rosenfeld (2001) train a language model from a large corpus, and use the Web to estimate low–density trigram frequencies. Keller and Lapata (2003) show that Web counts can obviate data sparseness for syntactic predicate– argument bigrams. They also observe that the noisiness of the Web, while unexplored in detail, does not greatly reduce the reliability of their results. Nakov and Hearst (2005) demonstrate that Web counts can ai</context>
</contexts>
<marker>Grefenstette, 1998</marker>
<rawString>Gregory Grefenstette. 1998. The World Wide Web as a resource for example-based machine translation tasks. In Proceedings of the ASLIB Conference on Translating and the Computer, London, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claire Grover</author>
<author>Mirella Lapata</author>
<author>Alex Lascarides</author>
</authors>
<title>A comparison of parsing technologies for the biomedical domain.</title>
<date>2005</date>
<journal>Journal of Natural Language Engineering,</journal>
<volume>11</volume>
<issue>01</issue>
<contexts>
<context position="2927" citStr="Grover et al., 2005" startWordPosition="447" endWordPosition="450">ted verb. For example, product replacement has an underlying verbal head replace, whereas garden-gnome has no such form. While compound nouns in general have a set of semantic relationships between the head and modifier that is potentially non-finite, compound nominalisations are better defined, in that the modifier fills a syntactic argument relation with respect to the head. For example, product might fill the direct object slot of the verb to replace for the compound above. Compound nominalisations comprise a substantial minority of compound nouns, with figures of about 35% being observed (Grover et al., 2005; Nicholson, 2005). We propose two novel paraphrases for a corpus statistical approach to predicting the relationship for a set of compound nominalisations, and investigate how using the World Wide Web as a corpus alleviates the common phenomenon of data sparseness, and how the volume of data impacts on the classification results. We also examine a more robust statistical approach to interpretation of the statistics than maximum likelihood estimates, called the confidence interval. The rest of the paper is structured as follows: in Section 2, we present a brief background for our work, with a </context>
<context position="5189" citStr="Grover et al. (2005)" startWordPosition="799" endWordPosition="802">ue that scores a compound on possible semantic interpretations, while Jones (1995) implements a graph–based unification procedure over semantic feature structures for the head. Finally, Rosario and Hearst (2001) make use of a domain–specific lexical resource to classify according to neural networks and decision trees. Syntactic classification, using paraphrasing, was first used by Leonard (1984), who uses a prioritised rule–based approach across a number of possible readings. Lauer (1995) employs a corpus statistical model over a similar paraphrase set based on prepositions. Lapata (2002) and Grover et al. (2005) again use a corpus statistical paraphrase–based approach, but with verb– argument relations for compound nominalisations — attempting to define the relation as one of subject, direct object, or a number of prepositional objects in the latter. 2.2 Web–as–Corpus Approaches Using the World Wide Web for corpus statistics is a relatively recent phenomenon; we present a few notable examples. Grefenstette (1998) analyses the plausibility of candidate translations in a machine translation task through Web statistics, and avoids some data sparseness within that context. Zhu and Rosenfeld (2001) train </context>
<context position="20031" citStr="Grover et al. (2005)" startWordPosition="3222" endWordPosition="3225">responding decrease in the results for the three–way open data set, which make these improvements immaterial. Examining the other possible combinations for the tests did indeed lead to varying results, but not in a consistent manner. For example, the best combination for the open data set was using the participial raw scores and z-scores (58.1%), which performed particularly badly in simple comparisons, and comparatively poorly (70.2%) for the two–way set. 6 Discussion Although the observed results failed to match, or even approach, various benchmarks set by Lapata (2002) (87.3% accuracy) and Grover et al. (2005) (77%) for the subject–object and subject– 72.4 51.1 74.2 50.4 58 direct object–prepositional objects classification tasks respectively, the presented approach is not without merit. Indeed, these results relied on machine learning approaches incorporating many features independent of corpus counts: namely, context, suffix information, and semantic similarity resources. Our results were an examination of the possible contribution of lexical information available from high–volume unparsed text. One important concept used in the above benchmarks was that of statistical smoothing, both class–based</context>
<context position="22683" citStr="Grover et al. (2005)" startWordPosition="3630" endWordPosition="3633">rpus counts with the Web counts, including smoothing, backing–off, and machine learning, could also lead to interesting performance impacts. Another item of interest is the comparative difficulty of the task presented by the three–way data set extracted from open data, and the two–way data set hand–curated by Lapata. The baseline 2Interestingly, Google only lists 3 occurrences of this compound anyway, so token relevance is low — further inspection shows that those 3 are not well-formed in any case. of this set is much lower, even compared that of the similar task (albeit domain–specific) from Grover et al. (2005) of 58.6%. We posit that the hand–filtering of the data set in these works contributes to a biased sample. For example, removing prepositional objects for a two–way classification, which make up about a third of the open data set, renders the task somewhat artificial. Comparison of the results between the maximum likelihood estimates used in earlier work, and the more statistically robust confidence intervals were inconclusive as to performance improvement, and were most effective as a feature expansion algorithm. The only obvious result is an aesthetic one, in using “robust statistics”. Final</context>
</contexts>
<marker>Grover, Lapata, Lascarides, 2005</marker>
<rawString>Claire Grover, Mirella Lapata, and Alex Lascarides. 2005. A comparison of parsing technologies for the biomedical domain. Journal of Natural Language Engineering, 11(01):27–65.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Bonnie Dorr</author>
</authors>
<title>A categorial variation database for English.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the ACL,</booktitle>
<pages>17--23</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="8264" citStr="Habash and Dorr, 2003" startWordPosition="1299" endWordPosition="1302">of the BNC, a balanced 90M token corpus. To find verb–argument frequencies, we parse this using RASP (Briscoe and Carroll, 2002), a tag sequence grammar–based statistical parser. We contrast the corpus statistics with ones collected from the 55 Web, using an implementation of a freely available Google “scraper” from CPAN.1 For a given compound nominalisation, we wish to determine all possible verbal forms of the head. We do so using the combination of the morphological component of CELEX (Burnage, 1990), a lexical database, NOMLEX (Macleod et al., 1998), a nominalisation database, and CATVAR (Habash and Dorr, 2003), an automatically–constructed database of clusters of inflected words based on the Porter stemmer (Porter, 1997). Once the verbal forms have been identified, we construct canonical forms of the present participle (+ing) and the past participle (+ed), using the morph lemmatiser (Minnen et al., 2001). We construct canonical forms of the plural head and plural modifier (+s) in the same manner. For evaluation, we have the two–way classified data set used by Lapata (2002), and a three–way classified data set constructed from open text. Lapata automatically extracts candidates from the British Nati</context>
</contexts>
<marker>Habash, Dorr, 2003</marker>
<rawString>Nizar Habash and Bonnie Dorr. 2003. A categorial variation database for English. In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the ACL, pages 17–23, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pierre Isabelle</author>
</authors>
<title>Another look at nominal compounds.</title>
<date>1984</date>
<booktitle>In Proceeedings of the 10th International Conference on Computational Linguistics and 22nd Annual Meeting of the ACL,</booktitle>
<pages>509--516</pages>
<location>Stanford, USA.</location>
<contexts>
<context position="4432" citStr="Isabelle (1984)" startWordPosition="684" endWordPosition="685">ion for Computational Linguistics sion in Section 6 and a brief conclusion in Section 7. 2 Background 2.1 Compound Noun Interpretation Compound nouns were seminally and thoroughly analysed by Levi (1978), who hand–constructs a nine–way set of semantic relations that she identifies as broadly defining the observed relationships between the compound head and modifier. Warren (1978) also inspects the syntax of compound nouns, to create a somewhat different set of twelve conceptual categories. Early attempts to automatically classify compound nouns have taken a semantic approach: Finin (1980) and Isabelle (1984) use “role nominals” derived from the head of the compound to fill a slot with the modifier. Vanderwende (1994) uses a rule–based technique that scores a compound on possible semantic interpretations, while Jones (1995) implements a graph–based unification procedure over semantic feature structures for the head. Finally, Rosario and Hearst (2001) make use of a domain–specific lexical resource to classify according to neural networks and decision trees. Syntactic classification, using paraphrasing, was first used by Leonard (1984), who uses a prioritised rule–based approach across a number of p</context>
</contexts>
<marker>Isabelle, 1984</marker>
<rawString>Pierre Isabelle. 1984. Another look at nominal compounds. In Proceeedings of the 10th International Conference on Computational Linguistics and 22nd Annual Meeting of the ACL, pages 509–516, Stanford, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Jones</author>
</authors>
<title>Predicating nominal compounds.</title>
<date>1995</date>
<booktitle>In Proceedings of the 17th International Conference of the Cognitive Science Society,</booktitle>
<pages>130--5</pages>
<location>Pittsburgh, USA.</location>
<contexts>
<context position="4651" citStr="Jones (1995)" startWordPosition="720" endWordPosition="721">ts a nine–way set of semantic relations that she identifies as broadly defining the observed relationships between the compound head and modifier. Warren (1978) also inspects the syntax of compound nouns, to create a somewhat different set of twelve conceptual categories. Early attempts to automatically classify compound nouns have taken a semantic approach: Finin (1980) and Isabelle (1984) use “role nominals” derived from the head of the compound to fill a slot with the modifier. Vanderwende (1994) uses a rule–based technique that scores a compound on possible semantic interpretations, while Jones (1995) implements a graph–based unification procedure over semantic feature structures for the head. Finally, Rosario and Hearst (2001) make use of a domain–specific lexical resource to classify according to neural networks and decision trees. Syntactic classification, using paraphrasing, was first used by Leonard (1984), who uses a prioritised rule–based approach across a number of possible readings. Lauer (1995) employs a corpus statistical model over a similar paraphrase set based on prepositions. Lapata (2002) and Grover et al. (2005) again use a corpus statistical paraphrase–based approach, but</context>
</contexts>
<marker>Jones, 1995</marker>
<rawString>Bernard Jones. 1995. Predicating nominal compounds. In Proceedings of the 17th International Conference of the Cognitive Science Society, pages 130–5, Pittsburgh, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Keller</author>
<author>Mirella Lapata</author>
</authors>
<title>Using the web to obtain frequencies for unseen bigrams.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>3</issue>
<contexts>
<context position="5912" citStr="Keller and Lapata (2003)" startWordPosition="911" endWordPosition="914">mpound nominalisations — attempting to define the relation as one of subject, direct object, or a number of prepositional objects in the latter. 2.2 Web–as–Corpus Approaches Using the World Wide Web for corpus statistics is a relatively recent phenomenon; we present a few notable examples. Grefenstette (1998) analyses the plausibility of candidate translations in a machine translation task through Web statistics, and avoids some data sparseness within that context. Zhu and Rosenfeld (2001) train a language model from a large corpus, and use the Web to estimate low–density trigram frequencies. Keller and Lapata (2003) show that Web counts can obviate data sparseness for syntactic predicate– argument bigrams. They also observe that the noisiness of the Web, while unexplored in detail, does not greatly reduce the reliability of their results. Nakov and Hearst (2005) demonstrate that Web counts can aid in identifying the bracketing in higher–arity noun compounds. Finally, Lapata and Keller (2005) evaluate the performance of Web counts on a wide range of natural language processing tasks, including compound noun bracketing and compound noun interpretation. 2.3 Confidence Intervals Maximum likelihood statistics</context>
<context position="21637" citStr="Keller and Lapata (2003)" startWordPosition="3464" endWordPosition="3467">s problem: only one compound (anarchist prohibition) has no instances of the paraphrases from the scraping,2 from more than 900 compounds between the two data sets. This extra information, we surmise, would be beneficial for the smoothing procedures, as the comparative accuracy between the two methods is similar. On the other hand, we also observe that simply alleviating the data sparseness is insufficient to provide a reliable interpretation. These results reinforce the contribution made by the statistical and semantic resources used in arriving at these benchmarks. The approach suggested by Keller and Lapata (2003) for obtaining bigram information from the Web could provide an approach for estimating the syntactic verb–argument counts for a given compound (dashes in Tables 1 and 2). In spite of the inherent unreliability of approximating long– range dependencies with n-gram information, results look promising. An examination of the effectiveness of this approach is left as further research. Similarly, various methods of combining corpus counts with the Web counts, including smoothing, backing–off, and machine learning, could also lead to interesting performance impacts. Another item of interest is the c</context>
</contexts>
<marker>Keller, Lapata, 2003</marker>
<rawString>Frank Keller and Mirella Lapata. 2003. Using the web to obtain frequencies for unseen bigrams. Computational Linguistics, 29(3):459–484.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John F Kenney</author>
<author>E S Keeping</author>
</authors>
<date>1962</date>
<journal>Mathematics of Statistics, Pt.</journal>
<volume>1</volume>
<pages>167--9</pages>
<location>Van Nostrand, Princeton, USA,</location>
<note>3rd edition.</note>
<contexts>
<context position="6834" citStr="Kenney and Keeping, 1962" startWordPosition="1052" endWordPosition="1056">fying the bracketing in higher–arity noun compounds. Finally, Lapata and Keller (2005) evaluate the performance of Web counts on a wide range of natural language processing tasks, including compound noun bracketing and compound noun interpretation. 2.3 Confidence Intervals Maximum likelihood statistics are not robust when many sparse vectors are under consideration, i.e. naively “choosing the largest number” may not be accurate in contexts when the relative value across samplings may be relevant, for example, in machine learning. As such, we apply a statistical test with confidence intervals (Kenney and Keeping, 1962), where we compare sample z-scores in a pairwise manner, instead of frequencies globally. The confidence interval P, for z-score n, is: P = 2 2 e−t2dt (1) ,/7rfn t is chosen to normalise the curve, and P is strictly increasing on n, so we are only required to find the largest z-score. Calculating the z-score exactly can be quite costly, so we instead use the binomial approximation to the normal distribution with equal prior probabilities and find that a given z-score Z is: Z = f (2) − µ Q where f is the frequency count, µ is the mean in a pairwise test, and Q is the standard deviation of the t</context>
</contexts>
<marker>Kenney, Keeping, 1962</marker>
<rawString>John F. Kenney and E. S. Keeping, 1962. Mathematics of Statistics, Pt. 1, chapter 11.4, pages 167–9. Van Nostrand, Princeton, USA, 3rd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
<author>Frank Keller</author>
</authors>
<title>Web-based models for natural language processing.</title>
<date>2005</date>
<journal>ACM Transactions on Speech and Language Processing,</journal>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="6295" citStr="Lapata and Keller (2005)" startWordPosition="970" endWordPosition="973">tion task through Web statistics, and avoids some data sparseness within that context. Zhu and Rosenfeld (2001) train a language model from a large corpus, and use the Web to estimate low–density trigram frequencies. Keller and Lapata (2003) show that Web counts can obviate data sparseness for syntactic predicate– argument bigrams. They also observe that the noisiness of the Web, while unexplored in detail, does not greatly reduce the reliability of their results. Nakov and Hearst (2005) demonstrate that Web counts can aid in identifying the bracketing in higher–arity noun compounds. Finally, Lapata and Keller (2005) evaluate the performance of Web counts on a wide range of natural language processing tasks, including compound noun bracketing and compound noun interpretation. 2.3 Confidence Intervals Maximum likelihood statistics are not robust when many sparse vectors are under consideration, i.e. naively “choosing the largest number” may not be accurate in contexts when the relative value across samplings may be relevant, for example, in machine learning. As such, we apply a statistical test with confidence intervals (Kenney and Keeping, 1962), where we compare sample z-scores in a pairwise manner, inst</context>
</contexts>
<marker>Lapata, Keller, 2005</marker>
<rawString>Mirella Lapata and Frank Keller. 2005. Web-based models for natural language processing. ACM Transactions on Speech and Language Processing, 2(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
<author>Alex Lascarides</author>
</authors>
<title>Detecting novel compounds: The role of distributional evidence.</title>
<date>2003</date>
<booktitle>In Proceedings of the 10th Conference of the European Chapter of the Association for Computional Linguistics,</booktitle>
<pages>235--242</pages>
<location>Budapest, Hungary.</location>
<contexts>
<context position="1752" citStr="Lapata and Lascarides, 2003" startWordPosition="255" endWordPosition="258">imension (like machine translation or information extraction) must take into account their semantic markedness. A compound noun is a sequence of two or more nouns comprising an N, for example, polystyrene garden-gnome. The productivity of compound nouns makes their treatment equally desirable and difficult. They appear frequently: more than 1% of the words in the British National Corpus (BNC: Burnard (2000)) participate in noun compounds (Tanaka and Baldwin, 2003). However, unestablished compounds are common: almost 70% of compounds identified in the BNC co-occur with a frequency of only one (Lapata and Lascarides, 2003). Analysis of the entire space of compound nouns has been hampered to some degree as the space defies some regular set of predicates to define the implicit semantics between a modifier and its head. This semantic underspecification led early analysis to be primarily of a semantic nature, but more recent work has advanced into using syntax to predict the semantics, in the spirit of the study by Levin (1993) on diathesis alternations. In this work, we examine compound nominalisations, a subset of compound nouns where the head has a morphologically–related verb. For example, product replacement h</context>
</contexts>
<marker>Lapata, Lascarides, 2003</marker>
<rawString>Mirella Lapata and Alex Lascarides. 2003. Detecting novel compounds: The role of distributional evidence. In Proceedings of the 10th Conference of the European Chapter of the Association for Computional Linguistics, pages 235–242, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Lapata</author>
</authors>
<title>The disambiguation of nominalizations.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<pages>388</pages>
<contexts>
<context position="5164" citStr="Lapata (2002)" startWordPosition="796" endWordPosition="797">rule–based technique that scores a compound on possible semantic interpretations, while Jones (1995) implements a graph–based unification procedure over semantic feature structures for the head. Finally, Rosario and Hearst (2001) make use of a domain–specific lexical resource to classify according to neural networks and decision trees. Syntactic classification, using paraphrasing, was first used by Leonard (1984), who uses a prioritised rule–based approach across a number of possible readings. Lauer (1995) employs a corpus statistical model over a similar paraphrase set based on prepositions. Lapata (2002) and Grover et al. (2005) again use a corpus statistical paraphrase–based approach, but with verb– argument relations for compound nominalisations — attempting to define the relation as one of subject, direct object, or a number of prepositional objects in the latter. 2.2 Web–as–Corpus Approaches Using the World Wide Web for corpus statistics is a relatively recent phenomenon; we present a few notable examples. Grefenstette (1998) analyses the plausibility of candidate translations in a machine translation task through Web statistics, and avoids some data sparseness within that context. Zhu an</context>
<context position="8736" citStr="Lapata (2002)" startWordPosition="1376" endWordPosition="1377">nt of CELEX (Burnage, 1990), a lexical database, NOMLEX (Macleod et al., 1998), a nominalisation database, and CATVAR (Habash and Dorr, 2003), an automatically–constructed database of clusters of inflected words based on the Porter stemmer (Porter, 1997). Once the verbal forms have been identified, we construct canonical forms of the present participle (+ing) and the past participle (+ed), using the morph lemmatiser (Minnen et al., 2001). We construct canonical forms of the plural head and plural modifier (+s) in the same manner. For evaluation, we have the two–way classified data set used by Lapata (2002), and a three–way classified data set constructed from open text. Lapata automatically extracts candidates from the British National Corpus, and hand–curates a set of 796 compound nominalisations which were interpreted as either a subjective relation SUBJ (e.g. wood appearance “wood appears”), or a (direct) objective relation OBJ (e.g. stress avoidance “[SO] avoids stress”. We automatically validated this data set for consistency, removing: 1. items that did not occur in the same chunk, according to a chunker based on fnTBL 1.0 (Ngai and Florian, 2001), 2. items whose head did not have a verba</context>
<context position="17682" citStr="Lapata (2002)" startWordPosition="2836" endWordPosition="2837">.8 43.2 45.4 43.4 38.0 Table 2: Classification results over the three-way data set, in %. Comparison of raw frequency counts vs. confidence-based z-scores, for BNC data and Google scrapings shown. compounds in the set (e.g. adult provision) that have a prepositional object reading (“provide for adults”) but have been classified as a direct object OBJ. The verb–argument counts obtained from the parsed BNC are significantly better than the baseline for the Lapata data set (x2 = 4.12,p &lt; 0.05), but not significantly better for the open data set (x2 = 0.99,p &lt; 1). Similar results were reported by Lapata (2002) over her data set using backed– off smoothing, the most closely related method. Neither the prepositional nor participial paraphrases were significantly better than the baseline for either the two–way (x2 = 0.00,p &lt; 1), or the three–way data set (x2 = 3.52,p &lt; 0.10), although the prepositional test did slightly improve on the verb–argument results. 5.2 Machine Learning Classification Although the results were not impressive, we still believed that there was complementing information within the data, which could be extracted with the aid of a machine learner. For this, we made use of TiMBL (Da</context>
<context position="19989" citStr="Lapata (2002)" startWordPosition="3216" endWordPosition="3218">.01). However, we also notice a corresponding decrease in the results for the three–way open data set, which make these improvements immaterial. Examining the other possible combinations for the tests did indeed lead to varying results, but not in a consistent manner. For example, the best combination for the open data set was using the participial raw scores and z-scores (58.1%), which performed particularly badly in simple comparisons, and comparatively poorly (70.2%) for the two–way set. 6 Discussion Although the observed results failed to match, or even approach, various benchmarks set by Lapata (2002) (87.3% accuracy) and Grover et al. (2005) (77%) for the subject–object and subject– 72.4 51.1 74.2 50.4 58 direct object–prepositional objects classification tasks respectively, the presented approach is not without merit. Indeed, these results relied on machine learning approaches incorporating many features independent of corpus counts: namely, context, suffix information, and semantic similarity resources. Our results were an examination of the possible contribution of lexical information available from high–volume unparsed text. One important concept used in the above benchmarks was that </context>
</contexts>
<marker>Lapata, 2002</marker>
<rawString>Maria Lapata. 2002. The disambiguation of nominalizations. Computational Linguistics, 28(3):357– 388.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Lauer</author>
</authors>
<title>Designing Statistical Language Learners: Experiments on Noun Compounds.</title>
<date>1995</date>
<tech>Ph.D. thesis,</tech>
<institution>Macquarie University,</institution>
<location>Sydney, Australia.</location>
<contexts>
<context position="5062" citStr="Lauer (1995)" startWordPosition="780" endWordPosition="781">s” derived from the head of the compound to fill a slot with the modifier. Vanderwende (1994) uses a rule–based technique that scores a compound on possible semantic interpretations, while Jones (1995) implements a graph–based unification procedure over semantic feature structures for the head. Finally, Rosario and Hearst (2001) make use of a domain–specific lexical resource to classify according to neural networks and decision trees. Syntactic classification, using paraphrasing, was first used by Leonard (1984), who uses a prioritised rule–based approach across a number of possible readings. Lauer (1995) employs a corpus statistical model over a similar paraphrase set based on prepositions. Lapata (2002) and Grover et al. (2005) again use a corpus statistical paraphrase–based approach, but with verb– argument relations for compound nominalisations — attempting to define the relation as one of subject, direct object, or a number of prepositional objects in the latter. 2.2 Web–as–Corpus Approaches Using the World Wide Web for corpus statistics is a relatively recent phenomenon; we present a few notable examples. Grefenstette (1998) analyses the plausibility of candidate translations in a machin</context>
<context position="10714" citStr="Lauer (1995)" startWordPosition="1684" endWordPosition="1685">ethod for examining the actual text of the returned documents. 4 Proposed Method 4.1 Paraphrase Tests To derive preferences for the SUB, DOB, and various POB interpretations for a given compound nominalisation, the most obvious approach is to examine a parsed corpus for instances of the verbal form of the head and the modifier occurring in the corresponding verb–argument relation. There are other constructions that can be informative, however. We examine two novel paraphrase tests: one prepositional and one participial. The prepositional test is based in part on the work by Leonard (1984) and Lauer (1995): for a given compound, we search for instances of the head and modifier nouns separated by a preposition. For example, for the compound nominalisation leg operation, we might search for operation on the leg, corresponding to the POB relation on. Special cases are by, corresponding to a subjective reading akin to a passive construction (e.g. investor hesitancy, hesitancy by the investor - “the investor hesitates”), and of, corresponding to a direct object reading (e.g. language speaker, speaker of the language � “[SO] speaks the language”). The participial test is based on the paraphrasing equ</context>
</contexts>
<marker>Lauer, 1995</marker>
<rawString>Mark Lauer. 1995. Designing Statistical Language Learners: Experiments on Noun Compounds. Ph.D. thesis, Macquarie University, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rosemary Leonard</author>
</authors>
<date>1984</date>
<booktitle>The Interpretation of English Noun Sequences on the Computer. Elsevier Science,</booktitle>
<location>Amsterdam, the Netherlands.</location>
<contexts>
<context position="4967" citStr="Leonard (1984)" startWordPosition="765" endWordPosition="766">compound nouns have taken a semantic approach: Finin (1980) and Isabelle (1984) use “role nominals” derived from the head of the compound to fill a slot with the modifier. Vanderwende (1994) uses a rule–based technique that scores a compound on possible semantic interpretations, while Jones (1995) implements a graph–based unification procedure over semantic feature structures for the head. Finally, Rosario and Hearst (2001) make use of a domain–specific lexical resource to classify according to neural networks and decision trees. Syntactic classification, using paraphrasing, was first used by Leonard (1984), who uses a prioritised rule–based approach across a number of possible readings. Lauer (1995) employs a corpus statistical model over a similar paraphrase set based on prepositions. Lapata (2002) and Grover et al. (2005) again use a corpus statistical paraphrase–based approach, but with verb– argument relations for compound nominalisations — attempting to define the relation as one of subject, direct object, or a number of prepositional objects in the latter. 2.2 Web–as–Corpus Approaches Using the World Wide Web for corpus statistics is a relatively recent phenomenon; we present a few notabl</context>
<context position="10697" citStr="Leonard (1984)" startWordPosition="1681" endWordPosition="1682">com/apis) gives a method for examining the actual text of the returned documents. 4 Proposed Method 4.1 Paraphrase Tests To derive preferences for the SUB, DOB, and various POB interpretations for a given compound nominalisation, the most obvious approach is to examine a parsed corpus for instances of the verbal form of the head and the modifier occurring in the corresponding verb–argument relation. There are other constructions that can be informative, however. We examine two novel paraphrase tests: one prepositional and one participial. The prepositional test is based in part on the work by Leonard (1984) and Lauer (1995): for a given compound, we search for instances of the head and modifier nouns separated by a preposition. For example, for the compound nominalisation leg operation, we might search for operation on the leg, corresponding to the POB relation on. Special cases are by, corresponding to a subjective reading akin to a passive construction (e.g. investor hesitancy, hesitancy by the investor - “the investor hesitates”), and of, corresponding to a direct object reading (e.g. language speaker, speaker of the language � “[SO] speaks the language”). The participial test is based on the</context>
</contexts>
<marker>Leonard, 1984</marker>
<rawString>Rosemary Leonard. 1984. The Interpretation of English Noun Sequences on the Computer. Elsevier Science, Amsterdam, the Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judith Levi</author>
</authors>
<title>The Syntax and Semantics of Complex Nominals.</title>
<date>1978</date>
<publisher>Academic Press,</publisher>
<location>New York, USA.</location>
<contexts>
<context position="4020" citStr="Levi (1978)" startWordPosition="622" endWordPosition="623">val. The rest of the paper is structured as follows: in Section 2, we present a brief background for our work, with a listing of our resources in Section 3. We detail our proposed method in Section 4, the corresponding results in Section 5, with a discus54 Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties, pages 54–61, Sydney, July 2006. c�2006 Association for Computational Linguistics sion in Section 6 and a brief conclusion in Section 7. 2 Background 2.1 Compound Noun Interpretation Compound nouns were seminally and thoroughly analysed by Levi (1978), who hand–constructs a nine–way set of semantic relations that she identifies as broadly defining the observed relationships between the compound head and modifier. Warren (1978) also inspects the syntax of compound nouns, to create a somewhat different set of twelve conceptual categories. Early attempts to automatically classify compound nouns have taken a semantic approach: Finin (1980) and Isabelle (1984) use “role nominals” derived from the head of the compound to fill a slot with the modifier. Vanderwende (1994) uses a rule–based technique that scores a compound on possible semantic inte</context>
</contexts>
<marker>Levi, 1978</marker>
<rawString>Judith Levi. 1978. The Syntax and Semantics of Complex Nominals. Academic Press, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Levin</author>
</authors>
<title>English Verb Classes and Alternations.</title>
<date>1993</date>
<publisher>The University of Chicago Press,</publisher>
<location>Chicago, USA.</location>
<contexts>
<context position="2161" citStr="Levin (1993)" startWordPosition="330" endWordPosition="331">n noun compounds (Tanaka and Baldwin, 2003). However, unestablished compounds are common: almost 70% of compounds identified in the BNC co-occur with a frequency of only one (Lapata and Lascarides, 2003). Analysis of the entire space of compound nouns has been hampered to some degree as the space defies some regular set of predicates to define the implicit semantics between a modifier and its head. This semantic underspecification led early analysis to be primarily of a semantic nature, but more recent work has advanced into using syntax to predict the semantics, in the spirit of the study by Levin (1993) on diathesis alternations. In this work, we examine compound nominalisations, a subset of compound nouns where the head has a morphologically–related verb. For example, product replacement has an underlying verbal head replace, whereas garden-gnome has no such form. While compound nouns in general have a set of semantic relationships between the head and modifier that is potentially non-finite, compound nominalisations are better defined, in that the modifier fills a syntactic argument relation with respect to the head. For example, product might fill the direct object slot of the verb to rep</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>Beth Levin. 1993. English Verb Classes and Alternations. The University of Chicago Press, Chicago, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Catherine Macleod</author>
<author>Ralph Grishman</author>
<author>Adam Meyers</author>
<author>Leslie Barrett</author>
<author>Ruth Reeves</author>
</authors>
<title>NOMLEX: A lexicon of nominalizations.</title>
<date>1998</date>
<booktitle>In Proceedings of the 8th International Congress of the European Association for Lexicography,</booktitle>
<pages>187--193</pages>
<location>Liege, Belgium.</location>
<contexts>
<context position="8201" citStr="Macleod et al., 1998" startWordPosition="1290" endWordPosition="1293">aluation. For corpus statistics, we use the written component of the BNC, a balanced 90M token corpus. To find verb–argument frequencies, we parse this using RASP (Briscoe and Carroll, 2002), a tag sequence grammar–based statistical parser. We contrast the corpus statistics with ones collected from the 55 Web, using an implementation of a freely available Google “scraper” from CPAN.1 For a given compound nominalisation, we wish to determine all possible verbal forms of the head. We do so using the combination of the morphological component of CELEX (Burnage, 1990), a lexical database, NOMLEX (Macleod et al., 1998), a nominalisation database, and CATVAR (Habash and Dorr, 2003), an automatically–constructed database of clusters of inflected words based on the Porter stemmer (Porter, 1997). Once the verbal forms have been identified, we construct canonical forms of the present participle (+ing) and the past participle (+ed), using the morph lemmatiser (Minnen et al., 2001). We construct canonical forms of the plural head and plural modifier (+s) in the same manner. For evaluation, we have the two–way classified data set used by Lapata (2002), and a three–way classified data set constructed from open text.</context>
</contexts>
<marker>Macleod, Grishman, Meyers, Barrett, Reeves, 1998</marker>
<rawString>Catherine Macleod, Ralph Grishman, Adam Meyers, Leslie Barrett, and Ruth Reeves. 1998. NOMLEX: A lexicon of nominalizations. In Proceedings of the 8th International Congress of the European Association for Lexicography, pages 187–193, Liege, Belgium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guido Minnen</author>
<author>John Carroll</author>
<author>Darren Pearce</author>
</authors>
<title>Applied morphological processing of English.</title>
<date>2001</date>
<journal>Natural Language Engineering,</journal>
<volume>7</volume>
<issue>3</issue>
<contexts>
<context position="8564" citStr="Minnen et al., 2001" startWordPosition="1344" endWordPosition="1347"> “scraper” from CPAN.1 For a given compound nominalisation, we wish to determine all possible verbal forms of the head. We do so using the combination of the morphological component of CELEX (Burnage, 1990), a lexical database, NOMLEX (Macleod et al., 1998), a nominalisation database, and CATVAR (Habash and Dorr, 2003), an automatically–constructed database of clusters of inflected words based on the Porter stemmer (Porter, 1997). Once the verbal forms have been identified, we construct canonical forms of the present participle (+ing) and the past participle (+ed), using the morph lemmatiser (Minnen et al., 2001). We construct canonical forms of the plural head and plural modifier (+s) in the same manner. For evaluation, we have the two–way classified data set used by Lapata (2002), and a three–way classified data set constructed from open text. Lapata automatically extracts candidates from the British National Corpus, and hand–curates a set of 796 compound nominalisations which were interpreted as either a subjective relation SUBJ (e.g. wood appearance “wood appears”), or a (direct) objective relation OBJ (e.g. stress avoidance “[SO] avoids stress”. We automatically validated this data set for consis</context>
</contexts>
<marker>Minnen, Carroll, Pearce, 2001</marker>
<rawString>Guido Minnen, John Carroll, and Darren Pearce. 2001. Applied morphological processing of English. Natural Language Engineering, 7(3):207–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslov Nakov</author>
<author>Marti Hearst</author>
</authors>
<title>Search engine statistics beyond the n-gram: Application to noun compound bracketing.</title>
<date>2005</date>
<booktitle>In Proceedings of the Ninth Conference on Computational Natural Language Learning,</booktitle>
<pages>17--24</pages>
<location>Ann Arbor, USA.</location>
<contexts>
<context position="6163" citStr="Nakov and Hearst (2005)" startWordPosition="951" endWordPosition="954">on; we present a few notable examples. Grefenstette (1998) analyses the plausibility of candidate translations in a machine translation task through Web statistics, and avoids some data sparseness within that context. Zhu and Rosenfeld (2001) train a language model from a large corpus, and use the Web to estimate low–density trigram frequencies. Keller and Lapata (2003) show that Web counts can obviate data sparseness for syntactic predicate– argument bigrams. They also observe that the noisiness of the Web, while unexplored in detail, does not greatly reduce the reliability of their results. Nakov and Hearst (2005) demonstrate that Web counts can aid in identifying the bracketing in higher–arity noun compounds. Finally, Lapata and Keller (2005) evaluate the performance of Web counts on a wide range of natural language processing tasks, including compound noun bracketing and compound noun interpretation. 2.3 Confidence Intervals Maximum likelihood statistics are not robust when many sparse vectors are under consideration, i.e. naively “choosing the largest number” may not be accurate in contexts when the relative value across samplings may be relevant, for example, in machine learning. As such, we apply </context>
</contexts>
<marker>Nakov, Hearst, 2005</marker>
<rawString>Preslov Nakov and Marti Hearst. 2005. Search engine statistics beyond the n-gram: Application to noun compound bracketing. In Proceedings of the Ninth Conference on Computational Natural Language Learning, pages 17–24, Ann Arbor, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grace Ngai</author>
<author>Radu Florian</author>
</authors>
<title>Transformationbased learning in the fast lane.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2nd Annual Meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>40--7</pages>
<location>Pittsburgh, USA.</location>
<contexts>
<context position="9294" citStr="Ngai and Florian, 2001" startWordPosition="1458" endWordPosition="1461">on, we have the two–way classified data set used by Lapata (2002), and a three–way classified data set constructed from open text. Lapata automatically extracts candidates from the British National Corpus, and hand–curates a set of 796 compound nominalisations which were interpreted as either a subjective relation SUBJ (e.g. wood appearance “wood appears”), or a (direct) objective relation OBJ (e.g. stress avoidance “[SO] avoids stress”. We automatically validated this data set for consistency, removing: 1. items that did not occur in the same chunk, according to a chunker based on fnTBL 1.0 (Ngai and Florian, 2001), 2. items whose head did not have a verbal form according to our lexical resources, and 3. items which consisted in part of proper nouns, to end up with 695 consistent compounds. We used the method of Nicholson and Baldwin (2005) to derive a small data set of 129 compound nominalisations, also from the BNC, which we instructed three unskilled annotators to identify each as one of subjective (SUB), direct object (DOB), or prepositional object (POB, e.g. side show “[SO] show [ST] on the side”). The annotators identified nine prepositional relations: {about,against,for,from,in,into,on,to,with}. </context>
</contexts>
<marker>Ngai, Florian, 2001</marker>
<rawString>Grace Ngai and Radu Florian. 2001. Transformationbased learning in the fast lane. In Proceedings of the 2nd Annual Meeting of the North American Chapter of the Association for Computational Linguistics, pages 40–7, Pittsburgh, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeremy Nicholson</author>
<author>Timothy Baldwin</author>
</authors>
<title>Statistical interpretation of compound nominalisations.</title>
<date>2005</date>
<booktitle>In Proceeding of the Australasian Langugae Technology Workshop</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="9524" citStr="Nicholson and Baldwin (2005)" startWordPosition="1499" endWordPosition="1502">set of 796 compound nominalisations which were interpreted as either a subjective relation SUBJ (e.g. wood appearance “wood appears”), or a (direct) objective relation OBJ (e.g. stress avoidance “[SO] avoids stress”. We automatically validated this data set for consistency, removing: 1. items that did not occur in the same chunk, according to a chunker based on fnTBL 1.0 (Ngai and Florian, 2001), 2. items whose head did not have a verbal form according to our lexical resources, and 3. items which consisted in part of proper nouns, to end up with 695 consistent compounds. We used the method of Nicholson and Baldwin (2005) to derive a small data set of 129 compound nominalisations, also from the BNC, which we instructed three unskilled annotators to identify each as one of subjective (SUB), direct object (DOB), or prepositional object (POB, e.g. side show “[SO] show [ST] on the side”). The annotators identified nine prepositional relations: {about,against,for,from,in,into,on,to,with}. &apos;www.cpan.org: We limit our usage to examining the “Estimated Results Returned”, so that our usage is identical to running the queries manually from the website. The Google API (www.google.com/apis) gives a method for examining th</context>
</contexts>
<marker>Nicholson, Baldwin, 2005</marker>
<rawString>Jeremy Nicholson and Timothy Baldwin. 2005. Statistical interpretation of compound nominalisations. In Proceeding of the Australasian Langugae Technology Workshop 2005, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeremy Nicholson</author>
</authors>
<title>Statistical interpretation of compound nouns. Honours Thesis,</title>
<date>2005</date>
<institution>University of Melbourne,</institution>
<location>Melbourne, Australia.</location>
<contexts>
<context position="2945" citStr="Nicholson, 2005" startWordPosition="451" endWordPosition="452">, product replacement has an underlying verbal head replace, whereas garden-gnome has no such form. While compound nouns in general have a set of semantic relationships between the head and modifier that is potentially non-finite, compound nominalisations are better defined, in that the modifier fills a syntactic argument relation with respect to the head. For example, product might fill the direct object slot of the verb to replace for the compound above. Compound nominalisations comprise a substantial minority of compound nouns, with figures of about 35% being observed (Grover et al., 2005; Nicholson, 2005). We propose two novel paraphrases for a corpus statistical approach to predicting the relationship for a set of compound nominalisations, and investigate how using the World Wide Web as a corpus alleviates the common phenomenon of data sparseness, and how the volume of data impacts on the classification results. We also examine a more robust statistical approach to interpretation of the statistics than maximum likelihood estimates, called the confidence interval. The rest of the paper is structured as follows: in Section 2, we present a brief background for our work, with a listing of our res</context>
<context position="7493" citStr="Nicholson (2005)" startWordPosition="1177" endWordPosition="1178">rwise manner, instead of frequencies globally. The confidence interval P, for z-score n, is: P = 2 2 e−t2dt (1) ,/7rfn t is chosen to normalise the curve, and P is strictly increasing on n, so we are only required to find the largest z-score. Calculating the z-score exactly can be quite costly, so we instead use the binomial approximation to the normal distribution with equal prior probabilities and find that a given z-score Z is: Z = f (2) − µ Q where f is the frequency count, µ is the mean in a pairwise test, and Q is the standard deviation of the test. A more complete derivation appears in Nicholson (2005). 3 Resources We make use of a number of lexical resources in our implementation and evaluation. For corpus statistics, we use the written component of the BNC, a balanced 90M token corpus. To find verb–argument frequencies, we parse this using RASP (Briscoe and Carroll, 2002), a tag sequence grammar–based statistical parser. We contrast the corpus statistics with ones collected from the 55 Web, using an implementation of a freely available Google “scraper” from CPAN.1 For a given compound nominalisation, we wish to determine all possible verbal forms of the head. We do so using the combinatio</context>
</contexts>
<marker>Nicholson, 2005</marker>
<rawString>Jeremy Nicholson. 2005. Statistical interpretation of compound nouns. Honours Thesis, University of Melbourne, Melbourne, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Porter</author>
</authors>
<title>An algorithm for suffix stripping.</title>
<date>1997</date>
<booktitle>Readings in information retrieval.</booktitle>
<editor>In Karen Sparck Jones and Peter Willett, editors,</editor>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco, USA.</location>
<contexts>
<context position="8377" citStr="Porter, 1997" startWordPosition="1316" endWordPosition="1317"> 2002), a tag sequence grammar–based statistical parser. We contrast the corpus statistics with ones collected from the 55 Web, using an implementation of a freely available Google “scraper” from CPAN.1 For a given compound nominalisation, we wish to determine all possible verbal forms of the head. We do so using the combination of the morphological component of CELEX (Burnage, 1990), a lexical database, NOMLEX (Macleod et al., 1998), a nominalisation database, and CATVAR (Habash and Dorr, 2003), an automatically–constructed database of clusters of inflected words based on the Porter stemmer (Porter, 1997). Once the verbal forms have been identified, we construct canonical forms of the present participle (+ing) and the past participle (+ed), using the morph lemmatiser (Minnen et al., 2001). We construct canonical forms of the plural head and plural modifier (+s) in the same manner. For evaluation, we have the two–way classified data set used by Lapata (2002), and a three–way classified data set constructed from open text. Lapata automatically extracts candidates from the British National Corpus, and hand–curates a set of 796 compound nominalisations which were interpreted as either a subjective</context>
</contexts>
<marker>Porter, 1997</marker>
<rawString>Martin Porter. 1997. An algorithm for suffix stripping. In Karen Sparck Jones and Peter Willett, editors, Readings in information retrieval. Morgan Kaufmann, San Francisco, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Rosario</author>
<author>Marti Hearst</author>
</authors>
<title>Classifying the semantic relations in noun compounds via a domain-specific lexical hierarchy.</title>
<date>2001</date>
<booktitle>In Proceedings of the 6th Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Pittsburgh, USA.</location>
<contexts>
<context position="4780" citStr="Rosario and Hearst (2001)" startWordPosition="736" endWordPosition="739">he compound head and modifier. Warren (1978) also inspects the syntax of compound nouns, to create a somewhat different set of twelve conceptual categories. Early attempts to automatically classify compound nouns have taken a semantic approach: Finin (1980) and Isabelle (1984) use “role nominals” derived from the head of the compound to fill a slot with the modifier. Vanderwende (1994) uses a rule–based technique that scores a compound on possible semantic interpretations, while Jones (1995) implements a graph–based unification procedure over semantic feature structures for the head. Finally, Rosario and Hearst (2001) make use of a domain–specific lexical resource to classify according to neural networks and decision trees. Syntactic classification, using paraphrasing, was first used by Leonard (1984), who uses a prioritised rule–based approach across a number of possible readings. Lauer (1995) employs a corpus statistical model over a similar paraphrase set based on prepositions. Lapata (2002) and Grover et al. (2005) again use a corpus statistical paraphrase–based approach, but with verb– argument relations for compound nominalisations — attempting to define the relation as one of subject, direct object,</context>
</contexts>
<marker>Rosario, Hearst, 2001</marker>
<rawString>Barbara Rosario and Marti Hearst. 2001. Classifying the semantic relations in noun compounds via a domain-specific lexical hierarchy. In Proceedings of the 6th Conference on Empirical Methods in Natural Language Processing, Pittsburgh, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takaaki Tanaka</author>
<author>Timothy Baldwin</author>
</authors>
<title>Nounnoun compound machine translation: A feasibility study on shallow processing.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL 2003 Workshop on Multiword Expressions: Analysis, Acquisition and Treatment,</booktitle>
<pages>17--24</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="1592" citStr="Tanaka and Baldwin, 2003" startWordPosition="229" endWordPosition="233">und nouns are a class of multiword expression (MWE) that have been of interest in recent computational linguistic work, as any task with a lexical semantic dimension (like machine translation or information extraction) must take into account their semantic markedness. A compound noun is a sequence of two or more nouns comprising an N, for example, polystyrene garden-gnome. The productivity of compound nouns makes their treatment equally desirable and difficult. They appear frequently: more than 1% of the words in the British National Corpus (BNC: Burnard (2000)) participate in noun compounds (Tanaka and Baldwin, 2003). However, unestablished compounds are common: almost 70% of compounds identified in the BNC co-occur with a frequency of only one (Lapata and Lascarides, 2003). Analysis of the entire space of compound nouns has been hampered to some degree as the space defies some regular set of predicates to define the implicit semantics between a modifier and its head. This semantic underspecification led early analysis to be primarily of a semantic nature, but more recent work has advanced into using syntax to predict the semantics, in the spirit of the study by Levin (1993) on diathesis alternations. In </context>
</contexts>
<marker>Tanaka, Baldwin, 2003</marker>
<rawString>Takaaki Tanaka and Timothy Baldwin. 2003. Nounnoun compound machine translation: A feasibility study on shallow processing. In Proceedings of the ACL 2003 Workshop on Multiword Expressions: Analysis, Acquisition and Treatment, pages 17–24, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucy Vanderwende</author>
</authors>
<title>Algorithm for automatic interpretation of noun sequences.</title>
<date>1994</date>
<booktitle>In Proceedings of the 15th International Conference on Computational Linguistics,</booktitle>
<pages>782--788</pages>
<location>Kyoto, Japan.</location>
<contexts>
<context position="4543" citStr="Vanderwende (1994)" startWordPosition="704" endWordPosition="705">mpound Noun Interpretation Compound nouns were seminally and thoroughly analysed by Levi (1978), who hand–constructs a nine–way set of semantic relations that she identifies as broadly defining the observed relationships between the compound head and modifier. Warren (1978) also inspects the syntax of compound nouns, to create a somewhat different set of twelve conceptual categories. Early attempts to automatically classify compound nouns have taken a semantic approach: Finin (1980) and Isabelle (1984) use “role nominals” derived from the head of the compound to fill a slot with the modifier. Vanderwende (1994) uses a rule–based technique that scores a compound on possible semantic interpretations, while Jones (1995) implements a graph–based unification procedure over semantic feature structures for the head. Finally, Rosario and Hearst (2001) make use of a domain–specific lexical resource to classify according to neural networks and decision trees. Syntactic classification, using paraphrasing, was first used by Leonard (1984), who uses a prioritised rule–based approach across a number of possible readings. Lauer (1995) employs a corpus statistical model over a similar paraphrase set based on prepos</context>
</contexts>
<marker>Vanderwende, 1994</marker>
<rawString>Lucy Vanderwende. 1994. Algorithm for automatic interpretation of noun sequences. In Proceedings of the 15th International Conference on Computational Linguistics, pages 782–788, Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beatrice Warren</author>
</authors>
<title>Semantic Patterns of NounNoun Compounds. Acta Universitatis Gothoburgensis,</title>
<date>1978</date>
<location>G¨oteborg,</location>
<contexts>
<context position="4199" citStr="Warren (1978)" startWordPosition="648" endWordPosition="650">ed method in Section 4, the corresponding results in Section 5, with a discus54 Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties, pages 54–61, Sydney, July 2006. c�2006 Association for Computational Linguistics sion in Section 6 and a brief conclusion in Section 7. 2 Background 2.1 Compound Noun Interpretation Compound nouns were seminally and thoroughly analysed by Levi (1978), who hand–constructs a nine–way set of semantic relations that she identifies as broadly defining the observed relationships between the compound head and modifier. Warren (1978) also inspects the syntax of compound nouns, to create a somewhat different set of twelve conceptual categories. Early attempts to automatically classify compound nouns have taken a semantic approach: Finin (1980) and Isabelle (1984) use “role nominals” derived from the head of the compound to fill a slot with the modifier. Vanderwende (1994) uses a rule–based technique that scores a compound on possible semantic interpretations, while Jones (1995) implements a graph–based unification procedure over semantic feature structures for the head. Finally, Rosario and Hearst (2001) make use of a doma</context>
</contexts>
<marker>Warren, 1978</marker>
<rawString>Beatrice Warren. 1978. Semantic Patterns of NounNoun Compounds. Acta Universitatis Gothoburgensis, G¨oteborg, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojin Zhu</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>Improving trigram language modeling with the World Wide Web.</title>
<date>2001</date>
<booktitle>In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing,</booktitle>
<pages>533--6</pages>
<location>Salt Lake City, USA.</location>
<contexts>
<context position="5782" citStr="Zhu and Rosenfeld (2001)" startWordPosition="890" endWordPosition="893">(2002) and Grover et al. (2005) again use a corpus statistical paraphrase–based approach, but with verb– argument relations for compound nominalisations — attempting to define the relation as one of subject, direct object, or a number of prepositional objects in the latter. 2.2 Web–as–Corpus Approaches Using the World Wide Web for corpus statistics is a relatively recent phenomenon; we present a few notable examples. Grefenstette (1998) analyses the plausibility of candidate translations in a machine translation task through Web statistics, and avoids some data sparseness within that context. Zhu and Rosenfeld (2001) train a language model from a large corpus, and use the Web to estimate low–density trigram frequencies. Keller and Lapata (2003) show that Web counts can obviate data sparseness for syntactic predicate– argument bigrams. They also observe that the noisiness of the Web, while unexplored in detail, does not greatly reduce the reliability of their results. Nakov and Hearst (2005) demonstrate that Web counts can aid in identifying the bracketing in higher–arity noun compounds. Finally, Lapata and Keller (2005) evaluate the performance of Web counts on a wide range of natural language processing </context>
</contexts>
<marker>Zhu, Rosenfeld, 2001</marker>
<rawString>Xiaojin Zhu and Ronald Rosenfeld. 2001. Improving trigram language modeling with the World Wide Web. In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing, pages 533–6, Salt Lake City, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>