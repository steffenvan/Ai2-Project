<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000159">
<title confidence="0.898681">
SemEval-2010 Task 7: Argument Selection and Coercion
</title>
<author confidence="0.949486">
James Pustejovsky
</author>
<affiliation confidence="0.873137666666667">
Computer Science Department
Brandeis University
Waltham, Massachusetts, USA
</affiliation>
<email confidence="0.99736">
jamesp@cs.brandeis.edu
</email>
<author confidence="0.987893">
Anna Rumshisky
</author>
<affiliation confidence="0.936984">
Computer Science Department
Brandeis University
Waltham, Massachusetts, USA
</affiliation>
<email confidence="0.998613">
arum@cs.brandeis.edu
</email>
<sectionHeader confidence="0.995648" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999816583333333">
In this paper, we describe the Argument Se-
lection and Coercion task, currently in devel-
opment for the SemEval-2 evaluation exercise
scheduled for 2010. This task involves char-
acterizing the type of compositional operation
that exists between a predicate and the argu-
ments it selects. Specifically, the goal is to
identify whether the type that a verb selects is
satisfied directly by the argument, or whether
the argument must change type to satisfy the
verb typing. We discuss the problem in detail
and describe the data preparation for the task.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999953157894737">
In recent years, a number of annotation schemes that
encode semantic information have been developed
and used to produce data sets for training machine
learning algorithms. Semantic markup schemes that
have focused on annotating entity types and, more
generally, word senses, have been extended to in-
clude semantic relationships between sentence ele-
ments, such as the semantic role (or label) assigned
to the argument by the predicate (Palmer et al., 2005;
Ruppenhofer et al., 2006; Kipper, 2005; Burchardt
et al., 2006; Ohara, 2008; Subirats, 2004).
In this task, we take this one step further, in that
this task attempts to capture the “compositional his-
tory” of the argument selection relative to the pred-
icate. In particular, this task attempts to identify the
operations of type adjustment induced by a predicate
over its arguments when they do not match its selec-
tional properties. The task is defined as follows: for
each argument of a predicate, identify whether the
</bodyText>
<page confidence="0.983218">
88
</page>
<bodyText confidence="0.999598">
entity in that argument position satisfies the type ex-
pected by the predicate. If not, then one needs to
identify how the entity in that position satisfies the
typing expected by the predicate; that is, to identify
the source and target types in a type-shifting (or co-
ercion) operation.
Consider the example below, where the verb re-
port normally selects for a human in subject po-
sition as in (1). Notice, however, that through a
metonymic interpretation, this constraint can be vi-
olated as demonstrated in (1).
</bodyText>
<listItem confidence="0.971379">
(1) a. John reported in late from Washington.
b. Washington reported in late.
</listItem>
<bodyText confidence="0.971028952380952">
Neither the surface annotation of entity extents and
types, nor assigning semantic roles associated with
the predicate would reflect in this case a crucial
point: namely, that in order for the typing require-
ments of the predicate to be satisfied, what has been
referred to a type coercion or a metonymy (Hobbs et
al., 1993; Pustejovsky, 1991; Nunberg, 1979; Egg,
2005) has taken place.
The SemEval Metonymy task (Markert and Nis-
sim, 2007) was a good attempt to annotate such
metonymic relations over a larger data set. This task
involved two types with their metonymic variants:
(2) i. Categories for Locations: literal, place-for-people,
place-for-event, place-for-product;
ii. Categories for Organizations: literal, organization-
for-members, organization-for-event, organization-for-
product, organization-for-facility.
One of the limitations of this approach, how-
ever, is that, while appropriate for these special-
ized metonymy relations, the annotation specifica-
tion and resulting corpus are not an informative
</bodyText>
<note confidence="0.6745355">
Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 88–93,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.99781875">
guide for extending the annotation of argument se-
lection more broadly.
In fact, the metonymy example in (1) is an in-
stance of a much more pervasive phenomenon of
type shifting and coercion in argument selection.
For example, in (3) below, the sense annotation for
the verb enjoy should arguably assign similar values
to both (3a) and (3b).
</bodyText>
<listItem confidence="0.993597">
(3) a. Mary enjoyed drinking her beer.
b. Mary enjoyed her beer.
</listItem>
<bodyText confidence="0.999902833333333">
The consequence of this, however, is that, under cur-
rent sense and role annotation strategies, the map-
ping to a syntactic realization for a given sense is
made more complex, and is in fact, perplexing for a
clustering or learning algorithm operating over sub-
categorization types for the verb.
</bodyText>
<sectionHeader confidence="0.860488" genericHeader="method">
2 Methodology of Annotation
</sectionHeader>
<bodyText confidence="0.97206795">
Before introducing the specifics of the argument se-
lection and coercion task, let us review briefly our
assumptions regarding the role of annotation within
the development and deployment of computational
linguistic systems.
We assume that the features we use for encoding
a specific linguistic phenomenon are rich enough to
capture the desired behavior. These linguistic de-
scriptions are typically distilled from extensive the-
oretical modeling of the phenomenon. The descrip-
tions in turn form the basis for the annotation values
of the specification language, which are themselves
the features used in a development cycle for training
and testing an identification or labeling algorithm
over text. Finally, based on an analysis and evalu-
ation of the performance of a system, the model of
the phenomenon may be revised, for retraining and
testing.
We call this particular cycle of development the
MATTER methodology:
</bodyText>
<listItem confidence="0.996748125">
(4) a. Model: Structural descriptions provide
theoretically-informed attributes derived from
empirical observations over the data;
b. Annotate: Annotation scheme assumes a feature
set that encodes specific structural descriptions and
properties of the input data;
c. Train: Algorithm is trained over a corpus annotated
with the target feature set;
</listItem>
<figureCaption confidence="0.881348">
Figure 1: The MATTER Methodology
</figureCaption>
<listItem confidence="0.9986824">
d. Test: Algorithm is tested against held-out data;
e. Evaluate: Standardized evaluation of results;
f. Revise: Revisit the model, annotation specification,
or algorithm, in order to make the annotation more
robust and reliable.
</listItem>
<bodyText confidence="0.867276">
Some of the current and completed annotation ef-
forts that have undergone such a development cycle
include:
</bodyText>
<listItem confidence="0.9999476">
• PropBank (Palmer et al., 2005)
• NomBank (Meyers et al., 2004)
• TimeBank (Pustejovsky et al., 2005)
• Opinion Corpus (Wiebe et al., 2005)
• Penn Discourse TreeBank (Miltsakaki et al., 2004)
</listItem>
<sectionHeader confidence="0.968958" genericHeader="method">
3 Task Description
</sectionHeader>
<bodyText confidence="0.998928571428571">
This task involves identifying the selectional mech-
anism used by the predicate over a particular argu-
ment.1 For the purposes of this task, the possible re-
lations between the predicate and a given argument
are restricted to selection and coercion. In selection,
the argument NP satisfies the typing requirements of
the predicate, as in (5).
</bodyText>
<listItem confidence="0.7475855">
(5) a. The spokesman denied the statement (PROPOSITION).
b. The child threw the stone (PHYSICAL OBJECT).
</listItem>
<bodyText confidence="0.9489436">
c. The audience didn’t believe the rumor (PROPOSI-
TION).
Coercion encompasses all cases when a type-
shifting operation must be performed on the com-
plement NP in order to satisfy selectional require-
ments of the predicate, as in (6). Note that coercion
operations may apply to any argument position in a
sentence, including the subject, as seen in (6b). Co-
ercion can also be seen as an object of a proposition
as in (6c).
</bodyText>
<listItem confidence="0.625807333333333">
(6) a. The president denied the attack (EVENT → PROPOSI-
TION).
b. The White House (LOCATION → HUMAN) denied this
statement.
c. The Boston office called with an update (EVENT →
INFO).
</listItem>
<footnote confidence="0.9462165">
1This task is part of a larger effort to annotate text with com-
positional operations (Pustejovsky et al., 2009).
</footnote>
<page confidence="0.999664">
89
</page>
<bodyText confidence="0.999053">
The definition of coercion will be extended to in-
clude instances of type-shifting due to what we term
the qua-relation.
</bodyText>
<listItem confidence="0.9452988">
(7) a. You can crush the pill (PHYSICAL OBJECT) between
two spoons. (Selection)
b. It is always possible to crush imagination (ABSTRACT
ENTITY qua PHYSICAL OBJECT) under the weight of
numbers. (Coercion/qua-relation)
</listItem>
<bodyText confidence="0.9997282">
In order to determine whether type-shifting has
taken place, the classification task must then in-
volve the following (1) identifying the verb sense
and the associated syntactic frame, (2) identifying
selectional requirements imposed by that verb sense
on the target argument, and (3) identifying semantic
type of the target argument. Sense inventories for
the verbs and the type templates associated with dif-
ferent syntactic frames will be provided to the par-
ticipants.
</bodyText>
<subsectionHeader confidence="0.998205">
3.1 Semantic Types
</subsectionHeader>
<bodyText confidence="0.999470666666667">
In the present task, we use a subset of semantic types
from the Brandeis Shallow Ontology (BSO), which
is a shallow hierarchy of types developed as a part
of the CPA effort (Hanks, 2009; Pustejovsky et al.,
2004; Rumshisky et al., 2006). The BSO types were
selected for their prevalence in manually identified
selection context patterns developed for several hun-
dreds English verbs. That is, they capture common
semantic distinctions associated with the selectional
properties of many verbs.
The following list of types is currently being used
for annotation:
</bodyText>
<listItem confidence="0.858217">
(8) HUMAN, ANIMATE, PHYSICAL OBJECT, ARTIFACT,
ORGANIZATION, EVENT, PROPOSITION, INFORMA-
TION, SENSATION, LOCATION, TIME PERIOD, AB-
STRACT ENTITY, ATTITUDE, EMOTION, PROPERTY,
PRIVILEGE, OBLIGATION, RULE
</listItem>
<bodyText confidence="0.9998198">
The subset of types chosen for annotation is pur-
posefully shallow, and is not structured in a hierar-
chy. For example, we include both HUMAN and AN-
IMATE in the type system along with PHYSICAL OB-
JECT. While HUMAN is a subtype of both ANIMATE
and PHYSICAL OBJECT, the system should simply
choose the most relevant type (i.e. HUMAN) and not
be concerned with type inheritance. The present set
of types may be revised if necessary as the annota-
tion proceeds.
</bodyText>
<figureCaption confidence="0.989845">
Figure 2: Corpus Development Architecture
</figureCaption>
<sectionHeader confidence="0.990862" genericHeader="method">
4 Resources and Corpus Development
</sectionHeader>
<bodyText confidence="0.999753846153846">
Preparing the data for this task will be done in two
phases: the data set construction phase and the an-
notation phase. The first phase consists of (1) select-
ing the target verbs to be annotated and compiling a
sense inventory for each target, and (2) data extrac-
tion and preprocessing. The prepared data is then
loaded into the annotation interface. During the an-
notation phase, the annotation judgments are entered
into the database, and the adjudicator resolves dis-
agreements. The resulting database representation is
used by the exporting module to generate the corre-
sponding XML markup or stand-off annotation. The
corpus development architecture is shown in Fig. 2.
</bodyText>
<subsectionHeader confidence="0.995151">
4.1 Data Set Construction Phase
</subsectionHeader>
<bodyText confidence="0.9996115">
In the set of target verbs selected for the task, pref-
erence will be given to the verbs that are strongly
coercive in at least one of their senses, i.e. tend to
impose semantic typing on one of their arguments.
The verbs will be selected by examining the data
from several sources, using the Sketch Engine (Kil-
garriff et al., 2004) as described in (Rumshisky and
Batiukova, 2008).
An inventory of senses will be compiled for each
verb. Whenever possible, the senses will be mapped
to OntoNotes (Pradhan et al., 2007) and to the CPA
patterns (Hanks, 2009). For each sense, a set of type
</bodyText>
<page confidence="0.995566">
90
</page>
<bodyText confidence="0.997993">
templates will be compiled, associating each sense
with one or more syntactic patterns which will in-
clude type specification for all arguments. For ex-
ample, one of the senses of the verb deny is refuse
to grant. This sense is associated with the following
type templates:
</bodyText>
<footnote confidence="0.2777745">
(9) HUMAN deny ENTITY to HUMAN
HUMAN deny HUMAN ENTITY
</footnote>
<bodyText confidence="0.9990013125">
The set of type templates for each verb will be built
using a modification of the CPA technique (Hanks
and Pustejovsky, 2005; Pustejovsky et al., 2004)).
A set of sentences will be randomly extracted for
each target verb from the BNC (BNC, 2000) and
the American National Corpus (Ide and Suderman,
2004). This choice of corpora should ensure a more
balanced representation of language than is available
in commonly annotated WSJ and other newswire
text. Each extracted sentence will be automatically
parsed, and the sentences organized according to the
grammatical relation involving the target verb. Sen-
tences will be excluded from the set if the target ar-
gument is expressed as anaphor, or is not present in
the sentence. Semantic head for the target grammat-
ical relation will be identified in each case.
</bodyText>
<subsectionHeader confidence="0.996623">
4.2 Annotation Phase
</subsectionHeader>
<bodyText confidence="0.9998452">
Word sense disambiguation will need to be per-
formed as a preliminary stage for the annotation of
compositional operations. The annotation task is
thus divided into two subtasks, presented succes-
sively to the annotator:
</bodyText>
<listItem confidence="0.9548775">
(1) Word sense disambiguation of the target predi-
cate
(2) Identification of the compositional relationship
between target predicate and its arguments
</listItem>
<bodyText confidence="0.989751565217391">
In the first subtask, the annotator is presented with
a set of sentences containing the target verb and the
chosen grammatical relation. The annotator is asked
to select the most fitting sense of the target verb, or
to throw out the example (pick the “N/A” option) if
no sense can be chosen either due to insufficient con-
text, because the appropriate sense does not appear
in the inventory, or simply no disambiguation can be
made in good faith. The interface is shown in Fig.
3. After this step is complete, the appropriate sense
is saved into the database, along with the associated
type template.
In the second subtask, the annotator is presented
with a list of sentences in which the target verb
is used in the same sense. The data is annotated
one grammatical relation at a time. The annotator
is asked to determine whether the argument in the
specified grammatical relation to the target belongs
to the type associated with that sense in the corre-
sponding template. The illustration of this can be
seen in Fig. 4. We will perform double annotation
and subsequent adjudication at each of the above an-
notation stages.
</bodyText>
<sectionHeader confidence="0.986131" genericHeader="method">
5 Data Format
</sectionHeader>
<bodyText confidence="0.990083571428572">
The test and training data will be provided in XML
format. The relation between the predicate (viewed
as function) and its argument will be represented by
a composition link (CompLink) as shown below.
In case of coercion, there is a mismatch between the
source and the target types, and both types need to
be identified:
</bodyText>
<table confidence="0.785403863636364">
The State Department repeatedly denied the attack.
The State Department repeatedly
&lt;SELECTOR sid=&amp;quot;s1&amp;quot;&gt;denied&lt;/SELECTOR&gt;
the
&lt;NOUN nid=&amp;quot;n1&amp;quot;&gt;attack&lt;/NOUN&gt; .
&lt;CompLink cid=&amp;quot;cid1&amp;quot; sID=&amp;quot;s1&amp;quot;
relatedToNoun=&amp;quot;n1&amp;quot; gramRel=&amp;quot;dobj&amp;quot;
compType=&amp;quot;COERCION&amp;quot;
sourceType=&amp;quot;EVENT&amp;quot;
targetType=&amp;quot;PROPOSITION&amp;quot;/&gt;
When the compositional operation is selection, the
source and the target types must match:
The State Department repeatedly denied this statement.
The State Department repeatedly
&lt;SELECTOR sid=&amp;quot;s1&amp;quot;&gt;denied&lt;/SELECTOR&gt;
this
&lt;NOUN nid=&amp;quot;n1&amp;quot;&gt;statement&lt;/NOUN&gt; .
&lt;CompLink cid=&amp;quot;cid1&amp;quot; sID=&amp;quot;s1&amp;quot;
relatedToNoun=&amp;quot;n1&amp;quot; gramRel=&amp;quot;dobj&amp;quot;
compType=&amp;quot;selection&amp;quot;
sourceType=&amp;quot;PROPOSITION&amp;quot;
targetType=&amp;quot;PROPOSITION&amp;quot;/&gt;
</table>
<sectionHeader confidence="0.989102" genericHeader="evaluation">
6 Evaluation Methodology
</sectionHeader>
<bodyText confidence="0.981421666666667">
Precision and recall will be used as evaluation met-
rics. A scoring program will be supplied for partic-
ipants. Two subtasks will be evaluated separately:
</bodyText>
<page confidence="0.998122">
91
</page>
<figureCaption confidence="0.999621">
Figure 3: Predicate Sense Disambiguation for deny.
</figureCaption>
<bodyText confidence="0.9826151875">
(1) identifying the compositional operation (i.e. se-
lection vs. coercion) and (2) identifying the source
and target argument type, for each relevant argu-
ment. Both subtasks require sense disambiguation
which will not be evaluated separately.
Since type-shifting is by its nature a relatively
rare event, the distribution between different types
of compositional operations in the data set will be
necessarily skewed. One of the standard sampling
methods for handling class imbalance is downsiz-
ing (Japkowicz, 2000; Monard and Batista, 2002),
where the number of instances of the major class in
the training set is artificially reduced. Another possi-
ble alternative is to assign higher error costs to mis-
classification of minor class instances (Chawla et al.,
2004; Domingos, 1999).
</bodyText>
<sectionHeader confidence="0.990792" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999951214285714">
In this paper, we have described the Argument Se-
lection and Coercion task for SemEval-2, to be held
in 2010. This task involves the identifying the rela-
tion between a predicate and its argument as one that
encodes the compositional history of the selection
process. This allows us to distinguish surface forms
that directly satisfy the selectional (type) require-
ments of a predicate from those that are coerced in
context. We described some details of a specifica-
tion language for selection and the annotation task
using this specification to identify argument selec-
tion behavior. Finally, we discussed data preparation
for the task and evaluation techniques for analyzing
the results.
</bodyText>
<sectionHeader confidence="0.996532" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997714321428571">
BNC. 2000. The British National Corpus.
The BNC Consortium, University of Oxford,
http://www.natcorp.ox.ac.uk/.
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pado, and Manfred Pinkal. 2006.
The salsa corpus: a german corpus resource for lexical
semantics. In Proceedings ofLREC, Genoa, Italy.
N. Chawla, N. Japkowicz, and A. Kotcz. 2004. Editorial:
special issue on learning from imbalanced data sets.
ACMSIGKDD Explorations Newsletter, 6(1):1–6.
P. Domingos. 1999. Metacost: A general method for
making classifiers cost-sensitive. In Proceedings of
the fifth ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 155–
164. ACM New York, NY, USA.
Marcus Egg. 2005. Flexible semantics for reinterpreta-
tion phenomena. CSLI, Stanford.
P. Hanks and J. Pustejovsky. 2005. A pattern dictionary
for natural language processing. Revue Franc¸aise de
Linguistique Appliqu´ee.
P. Hanks. 2009. Corpus pattern analysis. CPA
Project Page. Retrieved April 11, 2009, from
http://nlp.fi.muni.cz/projekty/cpa/.
J. R. Hobbs, M. Stickel, and P. Martin. 1993. Interpreta-
tion as abduction. Artificial Intelligence, 63:69–142.
N. Ide and K. Suderman. 2004. The American National
Corpus first release. In Proceedings of LREC 2004,
pages 1681–1684.
</reference>
<page confidence="0.990439">
92
</page>
<figureCaption confidence="0.987667">
Figure 4: Identifying Compositional Relationship for deny.
</figureCaption>
<reference confidence="0.992700112676056">
N. Japkowicz. 2000. Learning from imbalanced data
sets: a comparison of various strategies. In AAAI
workshop on learning from imbalanced data sets,
pages 00–05.
A. Kilgarriff, P. Rychly, P. Smrz, and D. Tugwell. 2004.
The Sketch Engine. Proceedings of Euralex, Lorient,
France, pages 105–116.
Karin Kipper. 2005. VerbNet: A broad-coverage, com-
prehensive verb lexicon. Phd dissertation, University
of Pennsylvania, PA.
K. Markert and M. Nissim. 2007. Metonymy resolution
at SemEval I: Guidelines for participants. In Proceed-
ings of the ACL 2007 Conference.
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-
ska, B. Young, and R. Grishman. 2004. The Nom-
Bank project: An interim report. In HLT-NAACL 2004
Workshop: Frontiers in Corpus Annotation, pages 24–
31.
E. Miltsakaki, R. Prasad, A. Joshi, and B. Webber. 2004.
The Penn Discourse Treebank. In Proceedings of the
4th International Conference on Language Resources
and Evaluation.
M.C. Monard and G.E. Batista. 2002. Learning with
skewed class distributions. Advances in logic, artifi-
cial intelligence and robotics (LAPTEC’02).
Geoffrey Nunberg. 1979. The non-uniqueness of seman-
tic solutions: Polysemy. Linguistics and Philosophy,
3:143–184.
Kyoko Hirose Ohara. 2008. Lexicon, grammar, and mul-
tilinguality in the japanese framenet. In Proceedings
ofLREC, Marrakech, Marocco.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71–106.
S. Pradhan, E. Loper, D. Dligach, and M. Palmer. 2007.
Semeval-2007 task-17: English lexical sample, srl and
all words. In Proceedings of the Fourth International
Workshop on Semantic Evaluations (SemEval-2007),
pages 87–92, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
J. Pustejovsky, P. Hanks, and A. Rumshisky. 2004. Au-
tomated Induction of Sense in Context. In COLING
2004, Geneva, Switzerland, pages 924–931.
J. Pustejovsky, R. Knippen, J. Littman, and R. Sauri.
2005. Temporal and event information in natural
language text. Language Resources and Evaluation,
39(2):123–164.
J. Pustejovsky, A. Rumshisky, J. Moszkowicz, and
O. Batiukova. 2009. GLML: Annotating argument
selection and coercion. IWCS-8: Eighth International
Conference on Computational Semantics.
J. Pustejovsky. 1991. The generative lexicon. Computa-
tional Linguistics, 17(4).
A. Rumshisky and O. Batiukova. 2008. Polysemy in
verbs: systematic relations between senses and their
effect on annotation. In COLING Workshop on Hu-
man Judgement in Computational Linguistics (HJCL-
2008), Manchester, England.
A. Rumshisky, P. Hanks, C. Havasi, and J. Pustejovsky.
2006. Constructing a corpus-based ontology using
model bias. In The 19th International FLAIRS Confer-
ence, FLAIRS 2006, Melbourne Beach, Florida, USA.
J. Ruppenhofer, M. Ellsworth, M. Petruck, C. Johnson,
and J. Scheffczyk. 2006. FrameNet II: Extended The-
ory and Practice.
Carlos Subirats. 2004. FrameNet Espa˜nol. Una red
sem´antica de marcos conceptuales. In VIInternational
Congress ofHispanic Linguistics, Leipzig.
J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating ex-
pressions of opinions and emotions in language. Lan-
guage Resources and Evaluation, 39(2):165–210.
</reference>
<page confidence="0.999172">
93
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.422075">
<title confidence="0.995861">SemEval-2010 Task 7: Argument Selection and Coercion</title>
<author confidence="0.99977">James Pustejovsky</author>
<affiliation confidence="0.999895">Computer Science Department Brandeis University</affiliation>
<address confidence="0.998382">Waltham, Massachusetts, USA</address>
<email confidence="0.999546">jamesp@cs.brandeis.edu</email>
<author confidence="0.787365">Anna</author>
<affiliation confidence="0.817121">Computer Science Brandeis Waltham, Massachusetts,</affiliation>
<email confidence="0.999844">arum@cs.brandeis.edu</email>
<abstract confidence="0.999449307692308">this paper, we describe the Seand Coercion currently in development for the SemEval-2 evaluation exercise scheduled for 2010. This task involves characterizing the type of compositional operation that exists between a predicate and the arguments it selects. Specifically, the goal is to identify whether the type that a verb selects is satisfied directly by the argument, or whether the argument must change type to satisfy the verb typing. We discuss the problem in detail and describe the data preparation for the task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>BNC</author>
</authors>
<title>The British National Corpus. The BNC Consortium,</title>
<date>2000</date>
<location>University of Oxford, http://www.natcorp.ox.ac.uk/.</location>
<contexts>
<context position="11345" citStr="BNC, 2000" startWordPosition="1799" endWordPosition="1800">09). For each sense, a set of type 90 templates will be compiled, associating each sense with one or more syntactic patterns which will include type specification for all arguments. For example, one of the senses of the verb deny is refuse to grant. This sense is associated with the following type templates: (9) HUMAN deny ENTITY to HUMAN HUMAN deny HUMAN ENTITY The set of type templates for each verb will be built using a modification of the CPA technique (Hanks and Pustejovsky, 2005; Pustejovsky et al., 2004)). A set of sentences will be randomly extracted for each target verb from the BNC (BNC, 2000) and the American National Corpus (Ide and Suderman, 2004). This choice of corpora should ensure a more balanced representation of language than is available in commonly annotated WSJ and other newswire text. Each extracted sentence will be automatically parsed, and the sentences organized according to the grammatical relation involving the target verb. Sentences will be excluded from the set if the target argument is expressed as anaphor, or is not present in the sentence. Semantic head for the target grammatical relation will be identified in each case. 4.2 Annotation Phase Word sense disamb</context>
</contexts>
<marker>BNC, 2000</marker>
<rawString>BNC. 2000. The British National Corpus. The BNC Consortium, University of Oxford, http://www.natcorp.ox.ac.uk/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aljoscha Burchardt</author>
<author>Katrin Erk</author>
<author>Anette Frank</author>
<author>Andrea Kowalski</author>
<author>Sebastian Pado</author>
<author>Manfred Pinkal</author>
</authors>
<title>The salsa corpus: a german corpus resource for lexical semantics.</title>
<date>2006</date>
<booktitle>In Proceedings ofLREC,</booktitle>
<location>Genoa, Italy.</location>
<contexts>
<context position="1373" citStr="Burchardt et al., 2006" startWordPosition="199" endWordPosition="202">yping. We discuss the problem in detail and describe the data preparation for the task. 1 Introduction In recent years, a number of annotation schemes that encode semantic information have been developed and used to produce data sets for training machine learning algorithms. Semantic markup schemes that have focused on annotating entity types and, more generally, word senses, have been extended to include semantic relationships between sentence elements, such as the semantic role (or label) assigned to the argument by the predicate (Palmer et al., 2005; Ruppenhofer et al., 2006; Kipper, 2005; Burchardt et al., 2006; Ohara, 2008; Subirats, 2004). In this task, we take this one step further, in that this task attempts to capture the “compositional history” of the argument selection relative to the predicate. In particular, this task attempts to identify the operations of type adjustment induced by a predicate over its arguments when they do not match its selectional properties. The task is defined as follows: for each argument of a predicate, identify whether the 88 entity in that argument position satisfies the type expected by the predicate. If not, then one needs to identify how the entity in that posi</context>
</contexts>
<marker>Burchardt, Erk, Frank, Kowalski, Pado, Pinkal, 2006</marker>
<rawString>Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea Kowalski, Sebastian Pado, and Manfred Pinkal. 2006. The salsa corpus: a german corpus resource for lexical semantics. In Proceedings ofLREC, Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chawla</author>
<author>N Japkowicz</author>
<author>A Kotcz</author>
</authors>
<title>Editorial: special issue on learning from imbalanced data sets.</title>
<date>2004</date>
<journal>ACMSIGKDD Explorations Newsletter,</journal>
<volume>6</volume>
<issue>1</issue>
<contexts>
<context position="15425" citStr="Chawla et al., 2004" startWordPosition="2414" endWordPosition="2417">vant argument. Both subtasks require sense disambiguation which will not be evaluated separately. Since type-shifting is by its nature a relatively rare event, the distribution between different types of compositional operations in the data set will be necessarily skewed. One of the standard sampling methods for handling class imbalance is downsizing (Japkowicz, 2000; Monard and Batista, 2002), where the number of instances of the major class in the training set is artificially reduced. Another possible alternative is to assign higher error costs to misclassification of minor class instances (Chawla et al., 2004; Domingos, 1999). 7 Conclusion In this paper, we have described the Argument Selection and Coercion task for SemEval-2, to be held in 2010. This task involves the identifying the relation between a predicate and its argument as one that encodes the compositional history of the selection process. This allows us to distinguish surface forms that directly satisfy the selectional (type) requirements of a predicate from those that are coerced in context. We described some details of a specification language for selection and the annotation task using this specification to identify argument selecti</context>
</contexts>
<marker>Chawla, Japkowicz, Kotcz, 2004</marker>
<rawString>N. Chawla, N. Japkowicz, and A. Kotcz. 2004. Editorial: special issue on learning from imbalanced data sets. ACMSIGKDD Explorations Newsletter, 6(1):1–6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Domingos</author>
</authors>
<title>Metacost: A general method for making classifiers cost-sensitive.</title>
<date>1999</date>
<booktitle>In Proceedings of the fifth ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>155--164</pages>
<publisher>ACM</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="15442" citStr="Domingos, 1999" startWordPosition="2418" endWordPosition="2419">ubtasks require sense disambiguation which will not be evaluated separately. Since type-shifting is by its nature a relatively rare event, the distribution between different types of compositional operations in the data set will be necessarily skewed. One of the standard sampling methods for handling class imbalance is downsizing (Japkowicz, 2000; Monard and Batista, 2002), where the number of instances of the major class in the training set is artificially reduced. Another possible alternative is to assign higher error costs to misclassification of minor class instances (Chawla et al., 2004; Domingos, 1999). 7 Conclusion In this paper, we have described the Argument Selection and Coercion task for SemEval-2, to be held in 2010. This task involves the identifying the relation between a predicate and its argument as one that encodes the compositional history of the selection process. This allows us to distinguish surface forms that directly satisfy the selectional (type) requirements of a predicate from those that are coerced in context. We described some details of a specification language for selection and the annotation task using this specification to identify argument selection behavior. Fina</context>
</contexts>
<marker>Domingos, 1999</marker>
<rawString>P. Domingos. 1999. Metacost: A general method for making classifiers cost-sensitive. In Proceedings of the fifth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 155– 164. ACM New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcus Egg</author>
</authors>
<title>Flexible semantics for reinterpretation phenomena.</title>
<date>2005</date>
<publisher>CSLI, Stanford.</publisher>
<contexts>
<context position="2789" citStr="Egg, 2005" startWordPosition="439" endWordPosition="440">elects for a human in subject position as in (1). Notice, however, that through a metonymic interpretation, this constraint can be violated as demonstrated in (1). (1) a. John reported in late from Washington. b. Washington reported in late. Neither the surface annotation of entity extents and types, nor assigning semantic roles associated with the predicate would reflect in this case a crucial point: namely, that in order for the typing requirements of the predicate to be satisfied, what has been referred to a type coercion or a metonymy (Hobbs et al., 1993; Pustejovsky, 1991; Nunberg, 1979; Egg, 2005) has taken place. The SemEval Metonymy task (Markert and Nissim, 2007) was a good attempt to annotate such metonymic relations over a larger data set. This task involved two types with their metonymic variants: (2) i. Categories for Locations: literal, place-for-people, place-for-event, place-for-product; ii. Categories for Organizations: literal, organizationfor-members, organization-for-event, organization-forproduct, organization-for-facility. One of the limitations of this approach, however, is that, while appropriate for these specialized metonymy relations, the annotation specification a</context>
</contexts>
<marker>Egg, 2005</marker>
<rawString>Marcus Egg. 2005. Flexible semantics for reinterpretation phenomena. CSLI, Stanford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Hanks</author>
<author>J Pustejovsky</author>
</authors>
<title>A pattern dictionary for natural language processing. Revue Franc¸aise de Linguistique Appliqu´ee.</title>
<date>2005</date>
<contexts>
<context position="11224" citStr="Hanks and Pustejovsky, 2005" startWordPosition="1776" endWordPosition="1779">ompiled for each verb. Whenever possible, the senses will be mapped to OntoNotes (Pradhan et al., 2007) and to the CPA patterns (Hanks, 2009). For each sense, a set of type 90 templates will be compiled, associating each sense with one or more syntactic patterns which will include type specification for all arguments. For example, one of the senses of the verb deny is refuse to grant. This sense is associated with the following type templates: (9) HUMAN deny ENTITY to HUMAN HUMAN deny HUMAN ENTITY The set of type templates for each verb will be built using a modification of the CPA technique (Hanks and Pustejovsky, 2005; Pustejovsky et al., 2004)). A set of sentences will be randomly extracted for each target verb from the BNC (BNC, 2000) and the American National Corpus (Ide and Suderman, 2004). This choice of corpora should ensure a more balanced representation of language than is available in commonly annotated WSJ and other newswire text. Each extracted sentence will be automatically parsed, and the sentences organized according to the grammatical relation involving the target verb. Sentences will be excluded from the set if the target argument is expressed as anaphor, or is not present in the sentence. </context>
</contexts>
<marker>Hanks, Pustejovsky, 2005</marker>
<rawString>P. Hanks and J. Pustejovsky. 2005. A pattern dictionary for natural language processing. Revue Franc¸aise de Linguistique Appliqu´ee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Hanks</author>
</authors>
<title>Corpus pattern analysis.</title>
<date>2009</date>
<journal>CPA</journal>
<institution>Project Page. Retrieved</institution>
<note>from http://nlp.fi.muni.cz/projekty/cpa/.</note>
<contexts>
<context position="8373" citStr="Hanks, 2009" startWordPosition="1310" endWordPosition="1311">e classification task must then involve the following (1) identifying the verb sense and the associated syntactic frame, (2) identifying selectional requirements imposed by that verb sense on the target argument, and (3) identifying semantic type of the target argument. Sense inventories for the verbs and the type templates associated with different syntactic frames will be provided to the participants. 3.1 Semantic Types In the present task, we use a subset of semantic types from the Brandeis Shallow Ontology (BSO), which is a shallow hierarchy of types developed as a part of the CPA effort (Hanks, 2009; Pustejovsky et al., 2004; Rumshisky et al., 2006). The BSO types were selected for their prevalence in manually identified selection context patterns developed for several hundreds English verbs. That is, they capture common semantic distinctions associated with the selectional properties of many verbs. The following list of types is currently being used for annotation: (8) HUMAN, ANIMATE, PHYSICAL OBJECT, ARTIFACT, ORGANIZATION, EVENT, PROPOSITION, INFORMATION, SENSATION, LOCATION, TIME PERIOD, ABSTRACT ENTITY, ATTITUDE, EMOTION, PROPERTY, PRIVILEGE, OBLIGATION, RULE The subset of types cho</context>
<context position="10738" citStr="Hanks, 2009" startWordPosition="1692" endWordPosition="1693">cture is shown in Fig. 2. 4.1 Data Set Construction Phase In the set of target verbs selected for the task, preference will be given to the verbs that are strongly coercive in at least one of their senses, i.e. tend to impose semantic typing on one of their arguments. The verbs will be selected by examining the data from several sources, using the Sketch Engine (Kilgarriff et al., 2004) as described in (Rumshisky and Batiukova, 2008). An inventory of senses will be compiled for each verb. Whenever possible, the senses will be mapped to OntoNotes (Pradhan et al., 2007) and to the CPA patterns (Hanks, 2009). For each sense, a set of type 90 templates will be compiled, associating each sense with one or more syntactic patterns which will include type specification for all arguments. For example, one of the senses of the verb deny is refuse to grant. This sense is associated with the following type templates: (9) HUMAN deny ENTITY to HUMAN HUMAN deny HUMAN ENTITY The set of type templates for each verb will be built using a modification of the CPA technique (Hanks and Pustejovsky, 2005; Pustejovsky et al., 2004)). A set of sentences will be randomly extracted for each target verb from the BNC (BNC</context>
</contexts>
<marker>Hanks, 2009</marker>
<rawString>P. Hanks. 2009. Corpus pattern analysis. CPA Project Page. Retrieved April 11, 2009, from http://nlp.fi.muni.cz/projekty/cpa/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Hobbs</author>
<author>M Stickel</author>
<author>P Martin</author>
</authors>
<title>Interpretation as abduction.</title>
<date>1993</date>
<journal>Artificial Intelligence,</journal>
<pages>63--69</pages>
<contexts>
<context position="2743" citStr="Hobbs et al., 1993" startWordPosition="431" endWordPosition="434">er the example below, where the verb report normally selects for a human in subject position as in (1). Notice, however, that through a metonymic interpretation, this constraint can be violated as demonstrated in (1). (1) a. John reported in late from Washington. b. Washington reported in late. Neither the surface annotation of entity extents and types, nor assigning semantic roles associated with the predicate would reflect in this case a crucial point: namely, that in order for the typing requirements of the predicate to be satisfied, what has been referred to a type coercion or a metonymy (Hobbs et al., 1993; Pustejovsky, 1991; Nunberg, 1979; Egg, 2005) has taken place. The SemEval Metonymy task (Markert and Nissim, 2007) was a good attempt to annotate such metonymic relations over a larger data set. This task involved two types with their metonymic variants: (2) i. Categories for Locations: literal, place-for-people, place-for-event, place-for-product; ii. Categories for Organizations: literal, organizationfor-members, organization-for-event, organization-forproduct, organization-for-facility. One of the limitations of this approach, however, is that, while appropriate for these specialized meto</context>
</contexts>
<marker>Hobbs, Stickel, Martin, 1993</marker>
<rawString>J. R. Hobbs, M. Stickel, and P. Martin. 1993. Interpretation as abduction. Artificial Intelligence, 63:69–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ide</author>
<author>K Suderman</author>
</authors>
<title>The American National Corpus first release.</title>
<date>2004</date>
<booktitle>In Proceedings of LREC</booktitle>
<pages>1681--1684</pages>
<contexts>
<context position="11403" citStr="Ide and Suderman, 2004" startWordPosition="1806" endWordPosition="1809">es will be compiled, associating each sense with one or more syntactic patterns which will include type specification for all arguments. For example, one of the senses of the verb deny is refuse to grant. This sense is associated with the following type templates: (9) HUMAN deny ENTITY to HUMAN HUMAN deny HUMAN ENTITY The set of type templates for each verb will be built using a modification of the CPA technique (Hanks and Pustejovsky, 2005; Pustejovsky et al., 2004)). A set of sentences will be randomly extracted for each target verb from the BNC (BNC, 2000) and the American National Corpus (Ide and Suderman, 2004). This choice of corpora should ensure a more balanced representation of language than is available in commonly annotated WSJ and other newswire text. Each extracted sentence will be automatically parsed, and the sentences organized according to the grammatical relation involving the target verb. Sentences will be excluded from the set if the target argument is expressed as anaphor, or is not present in the sentence. Semantic head for the target grammatical relation will be identified in each case. 4.2 Annotation Phase Word sense disambiguation will need to be performed as a preliminary stage </context>
</contexts>
<marker>Ide, Suderman, 2004</marker>
<rawString>N. Ide and K. Suderman. 2004. The American National Corpus first release. In Proceedings of LREC 2004, pages 1681–1684.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Japkowicz</author>
</authors>
<title>Learning from imbalanced data sets: a comparison of various strategies.</title>
<date>2000</date>
<booktitle>In AAAI workshop on learning from imbalanced data sets,</booktitle>
<pages>00--05</pages>
<contexts>
<context position="15175" citStr="Japkowicz, 2000" startWordPosition="2375" endWordPosition="2376">nts. Two subtasks will be evaluated separately: 91 Figure 3: Predicate Sense Disambiguation for deny. (1) identifying the compositional operation (i.e. selection vs. coercion) and (2) identifying the source and target argument type, for each relevant argument. Both subtasks require sense disambiguation which will not be evaluated separately. Since type-shifting is by its nature a relatively rare event, the distribution between different types of compositional operations in the data set will be necessarily skewed. One of the standard sampling methods for handling class imbalance is downsizing (Japkowicz, 2000; Monard and Batista, 2002), where the number of instances of the major class in the training set is artificially reduced. Another possible alternative is to assign higher error costs to misclassification of minor class instances (Chawla et al., 2004; Domingos, 1999). 7 Conclusion In this paper, we have described the Argument Selection and Coercion task for SemEval-2, to be held in 2010. This task involves the identifying the relation between a predicate and its argument as one that encodes the compositional history of the selection process. This allows us to distinguish surface forms that dir</context>
</contexts>
<marker>Japkowicz, 2000</marker>
<rawString>N. Japkowicz. 2000. Learning from imbalanced data sets: a comparison of various strategies. In AAAI workshop on learning from imbalanced data sets, pages 00–05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kilgarriff</author>
<author>P Rychly</author>
<author>P Smrz</author>
<author>D Tugwell</author>
</authors>
<title>The Sketch Engine.</title>
<date>2004</date>
<booktitle>Proceedings of Euralex,</booktitle>
<pages>105--116</pages>
<location>Lorient, France,</location>
<contexts>
<context position="10515" citStr="Kilgarriff et al., 2004" startWordPosition="1652" endWordPosition="1656">tered into the database, and the adjudicator resolves disagreements. The resulting database representation is used by the exporting module to generate the corresponding XML markup or stand-off annotation. The corpus development architecture is shown in Fig. 2. 4.1 Data Set Construction Phase In the set of target verbs selected for the task, preference will be given to the verbs that are strongly coercive in at least one of their senses, i.e. tend to impose semantic typing on one of their arguments. The verbs will be selected by examining the data from several sources, using the Sketch Engine (Kilgarriff et al., 2004) as described in (Rumshisky and Batiukova, 2008). An inventory of senses will be compiled for each verb. Whenever possible, the senses will be mapped to OntoNotes (Pradhan et al., 2007) and to the CPA patterns (Hanks, 2009). For each sense, a set of type 90 templates will be compiled, associating each sense with one or more syntactic patterns which will include type specification for all arguments. For example, one of the senses of the verb deny is refuse to grant. This sense is associated with the following type templates: (9) HUMAN deny ENTITY to HUMAN HUMAN deny HUMAN ENTITY The set of type</context>
</contexts>
<marker>Kilgarriff, Rychly, Smrz, Tugwell, 2004</marker>
<rawString>A. Kilgarriff, P. Rychly, P. Smrz, and D. Tugwell. 2004. The Sketch Engine. Proceedings of Euralex, Lorient, France, pages 105–116.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karin Kipper</author>
</authors>
<title>VerbNet: A broad-coverage, comprehensive verb lexicon.</title>
<date>2005</date>
<institution>University of Pennsylvania, PA.</institution>
<note>Phd dissertation,</note>
<contexts>
<context position="1349" citStr="Kipper, 2005" startWordPosition="197" endWordPosition="198">sfy the verb typing. We discuss the problem in detail and describe the data preparation for the task. 1 Introduction In recent years, a number of annotation schemes that encode semantic information have been developed and used to produce data sets for training machine learning algorithms. Semantic markup schemes that have focused on annotating entity types and, more generally, word senses, have been extended to include semantic relationships between sentence elements, such as the semantic role (or label) assigned to the argument by the predicate (Palmer et al., 2005; Ruppenhofer et al., 2006; Kipper, 2005; Burchardt et al., 2006; Ohara, 2008; Subirats, 2004). In this task, we take this one step further, in that this task attempts to capture the “compositional history” of the argument selection relative to the predicate. In particular, this task attempts to identify the operations of type adjustment induced by a predicate over its arguments when they do not match its selectional properties. The task is defined as follows: for each argument of a predicate, identify whether the 88 entity in that argument position satisfies the type expected by the predicate. If not, then one needs to identify how</context>
</contexts>
<marker>Kipper, 2005</marker>
<rawString>Karin Kipper. 2005. VerbNet: A broad-coverage, comprehensive verb lexicon. Phd dissertation, University of Pennsylvania, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Markert</author>
<author>M Nissim</author>
</authors>
<title>Metonymy resolution at SemEval I: Guidelines for participants.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL 2007 Conference.</booktitle>
<contexts>
<context position="2859" citStr="Markert and Nissim, 2007" startWordPosition="448" endWordPosition="452">e, however, that through a metonymic interpretation, this constraint can be violated as demonstrated in (1). (1) a. John reported in late from Washington. b. Washington reported in late. Neither the surface annotation of entity extents and types, nor assigning semantic roles associated with the predicate would reflect in this case a crucial point: namely, that in order for the typing requirements of the predicate to be satisfied, what has been referred to a type coercion or a metonymy (Hobbs et al., 1993; Pustejovsky, 1991; Nunberg, 1979; Egg, 2005) has taken place. The SemEval Metonymy task (Markert and Nissim, 2007) was a good attempt to annotate such metonymic relations over a larger data set. This task involved two types with their metonymic variants: (2) i. Categories for Locations: literal, place-for-people, place-for-event, place-for-product; ii. Categories for Organizations: literal, organizationfor-members, organization-for-event, organization-forproduct, organization-for-facility. One of the limitations of this approach, however, is that, while appropriate for these specialized metonymy relations, the annotation specification and resulting corpus are not an informative Proceedings of the NAACL HL</context>
</contexts>
<marker>Markert, Nissim, 2007</marker>
<rawString>K. Markert and M. Nissim. 2007. Metonymy resolution at SemEval I: Guidelines for participants. In Proceedings of the ACL 2007 Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Meyers</author>
<author>R Reeves</author>
<author>C Macleod</author>
<author>R Szekely</author>
<author>V Zielinska</author>
<author>B Young</author>
<author>R Grishman</author>
</authors>
<title>The NomBank project: An interim report.</title>
<date>2004</date>
<booktitle>In HLT-NAACL 2004 Workshop: Frontiers in Corpus Annotation,</booktitle>
<pages>24--31</pages>
<contexts>
<context position="6053" citStr="Meyers et al., 2004" startWordPosition="926" endWordPosition="929"> assumes a feature set that encodes specific structural descriptions and properties of the input data; c. Train: Algorithm is trained over a corpus annotated with the target feature set; Figure 1: The MATTER Methodology d. Test: Algorithm is tested against held-out data; e. Evaluate: Standardized evaluation of results; f. Revise: Revisit the model, annotation specification, or algorithm, in order to make the annotation more robust and reliable. Some of the current and completed annotation efforts that have undergone such a development cycle include: • PropBank (Palmer et al., 2005) • NomBank (Meyers et al., 2004) • TimeBank (Pustejovsky et al., 2005) • Opinion Corpus (Wiebe et al., 2005) • Penn Discourse TreeBank (Miltsakaki et al., 2004) 3 Task Description This task involves identifying the selectional mechanism used by the predicate over a particular argument.1 For the purposes of this task, the possible relations between the predicate and a given argument are restricted to selection and coercion. In selection, the argument NP satisfies the typing requirements of the predicate, as in (5). (5) a. The spokesman denied the statement (PROPOSITION). b. The child threw the stone (PHYSICAL OBJECT). c. The </context>
</contexts>
<marker>Meyers, Reeves, Macleod, Szekely, Zielinska, Young, Grishman, 2004</marker>
<rawString>A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielinska, B. Young, and R. Grishman. 2004. The NomBank project: An interim report. In HLT-NAACL 2004 Workshop: Frontiers in Corpus Annotation, pages 24– 31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Miltsakaki</author>
<author>R Prasad</author>
<author>A Joshi</author>
<author>B Webber</author>
</authors>
<title>The Penn Discourse Treebank.</title>
<date>2004</date>
<booktitle>In Proceedings of the 4th International Conference on Language Resources and Evaluation.</booktitle>
<contexts>
<context position="6181" citStr="Miltsakaki et al., 2004" startWordPosition="947" endWordPosition="950">s trained over a corpus annotated with the target feature set; Figure 1: The MATTER Methodology d. Test: Algorithm is tested against held-out data; e. Evaluate: Standardized evaluation of results; f. Revise: Revisit the model, annotation specification, or algorithm, in order to make the annotation more robust and reliable. Some of the current and completed annotation efforts that have undergone such a development cycle include: • PropBank (Palmer et al., 2005) • NomBank (Meyers et al., 2004) • TimeBank (Pustejovsky et al., 2005) • Opinion Corpus (Wiebe et al., 2005) • Penn Discourse TreeBank (Miltsakaki et al., 2004) 3 Task Description This task involves identifying the selectional mechanism used by the predicate over a particular argument.1 For the purposes of this task, the possible relations between the predicate and a given argument are restricted to selection and coercion. In selection, the argument NP satisfies the typing requirements of the predicate, as in (5). (5) a. The spokesman denied the statement (PROPOSITION). b. The child threw the stone (PHYSICAL OBJECT). c. The audience didn’t believe the rumor (PROPOSITION). Coercion encompasses all cases when a typeshifting operation must be performed </context>
</contexts>
<marker>Miltsakaki, Prasad, Joshi, Webber, 2004</marker>
<rawString>E. Miltsakaki, R. Prasad, A. Joshi, and B. Webber. 2004. The Penn Discourse Treebank. In Proceedings of the 4th International Conference on Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M C Monard</author>
<author>G E Batista</author>
</authors>
<title>Learning with skewed class distributions. Advances in logic, artificial intelligence and robotics (LAPTEC’02).</title>
<date>2002</date>
<contexts>
<context position="15202" citStr="Monard and Batista, 2002" startWordPosition="2377" endWordPosition="2380"> will be evaluated separately: 91 Figure 3: Predicate Sense Disambiguation for deny. (1) identifying the compositional operation (i.e. selection vs. coercion) and (2) identifying the source and target argument type, for each relevant argument. Both subtasks require sense disambiguation which will not be evaluated separately. Since type-shifting is by its nature a relatively rare event, the distribution between different types of compositional operations in the data set will be necessarily skewed. One of the standard sampling methods for handling class imbalance is downsizing (Japkowicz, 2000; Monard and Batista, 2002), where the number of instances of the major class in the training set is artificially reduced. Another possible alternative is to assign higher error costs to misclassification of minor class instances (Chawla et al., 2004; Domingos, 1999). 7 Conclusion In this paper, we have described the Argument Selection and Coercion task for SemEval-2, to be held in 2010. This task involves the identifying the relation between a predicate and its argument as one that encodes the compositional history of the selection process. This allows us to distinguish surface forms that directly satisfy the selection</context>
</contexts>
<marker>Monard, Batista, 2002</marker>
<rawString>M.C. Monard and G.E. Batista. 2002. Learning with skewed class distributions. Advances in logic, artificial intelligence and robotics (LAPTEC’02).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Nunberg</author>
</authors>
<title>The non-uniqueness of semantic solutions: Polysemy. Linguistics and Philosophy,</title>
<date>1979</date>
<pages>3--143</pages>
<contexts>
<context position="2777" citStr="Nunberg, 1979" startWordPosition="437" endWordPosition="438">port normally selects for a human in subject position as in (1). Notice, however, that through a metonymic interpretation, this constraint can be violated as demonstrated in (1). (1) a. John reported in late from Washington. b. Washington reported in late. Neither the surface annotation of entity extents and types, nor assigning semantic roles associated with the predicate would reflect in this case a crucial point: namely, that in order for the typing requirements of the predicate to be satisfied, what has been referred to a type coercion or a metonymy (Hobbs et al., 1993; Pustejovsky, 1991; Nunberg, 1979; Egg, 2005) has taken place. The SemEval Metonymy task (Markert and Nissim, 2007) was a good attempt to annotate such metonymic relations over a larger data set. This task involved two types with their metonymic variants: (2) i. Categories for Locations: literal, place-for-people, place-for-event, place-for-product; ii. Categories for Organizations: literal, organizationfor-members, organization-for-event, organization-forproduct, organization-for-facility. One of the limitations of this approach, however, is that, while appropriate for these specialized metonymy relations, the annotation spe</context>
</contexts>
<marker>Nunberg, 1979</marker>
<rawString>Geoffrey Nunberg. 1979. The non-uniqueness of semantic solutions: Polysemy. Linguistics and Philosophy, 3:143–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kyoko Hirose Ohara</author>
</authors>
<title>Lexicon, grammar, and multilinguality in the japanese framenet.</title>
<date>2008</date>
<booktitle>In Proceedings ofLREC,</booktitle>
<location>Marrakech, Marocco.</location>
<contexts>
<context position="1386" citStr="Ohara, 2008" startWordPosition="203" endWordPosition="204">oblem in detail and describe the data preparation for the task. 1 Introduction In recent years, a number of annotation schemes that encode semantic information have been developed and used to produce data sets for training machine learning algorithms. Semantic markup schemes that have focused on annotating entity types and, more generally, word senses, have been extended to include semantic relationships between sentence elements, such as the semantic role (or label) assigned to the argument by the predicate (Palmer et al., 2005; Ruppenhofer et al., 2006; Kipper, 2005; Burchardt et al., 2006; Ohara, 2008; Subirats, 2004). In this task, we take this one step further, in that this task attempts to capture the “compositional history” of the argument selection relative to the predicate. In particular, this task attempts to identify the operations of type adjustment induced by a predicate over its arguments when they do not match its selectional properties. The task is defined as follows: for each argument of a predicate, identify whether the 88 entity in that argument position satisfies the type expected by the predicate. If not, then one needs to identify how the entity in that position satisfie</context>
</contexts>
<marker>Ohara, 2008</marker>
<rawString>Kyoko Hirose Ohara. 2008. Lexicon, grammar, and multilinguality in the japanese framenet. In Proceedings ofLREC, Marrakech, Marocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Palmer</author>
<author>D Gildea</author>
<author>P Kingsbury</author>
</authors>
<title>The proposition bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="1309" citStr="Palmer et al., 2005" startWordPosition="189" endWordPosition="192">r whether the argument must change type to satisfy the verb typing. We discuss the problem in detail and describe the data preparation for the task. 1 Introduction In recent years, a number of annotation schemes that encode semantic information have been developed and used to produce data sets for training machine learning algorithms. Semantic markup schemes that have focused on annotating entity types and, more generally, word senses, have been extended to include semantic relationships between sentence elements, such as the semantic role (or label) assigned to the argument by the predicate (Palmer et al., 2005; Ruppenhofer et al., 2006; Kipper, 2005; Burchardt et al., 2006; Ohara, 2008; Subirats, 2004). In this task, we take this one step further, in that this task attempts to capture the “compositional history” of the argument selection relative to the predicate. In particular, this task attempts to identify the operations of type adjustment induced by a predicate over its arguments when they do not match its selectional properties. The task is defined as follows: for each argument of a predicate, identify whether the 88 entity in that argument position satisfies the type expected by the predicate</context>
<context position="6021" citStr="Palmer et al., 2005" startWordPosition="920" endWordPosition="923">; b. Annotate: Annotation scheme assumes a feature set that encodes specific structural descriptions and properties of the input data; c. Train: Algorithm is trained over a corpus annotated with the target feature set; Figure 1: The MATTER Methodology d. Test: Algorithm is tested against held-out data; e. Evaluate: Standardized evaluation of results; f. Revise: Revisit the model, annotation specification, or algorithm, in order to make the annotation more robust and reliable. Some of the current and completed annotation efforts that have undergone such a development cycle include: • PropBank (Palmer et al., 2005) • NomBank (Meyers et al., 2004) • TimeBank (Pustejovsky et al., 2005) • Opinion Corpus (Wiebe et al., 2005) • Penn Discourse TreeBank (Miltsakaki et al., 2004) 3 Task Description This task involves identifying the selectional mechanism used by the predicate over a particular argument.1 For the purposes of this task, the possible relations between the predicate and a given argument are restricted to selection and coercion. In selection, the argument NP satisfies the typing requirements of the predicate, as in (5). (5) a. The spokesman denied the statement (PROPOSITION). b. The child threw the </context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>M. Palmer, D. Gildea, and P. Kingsbury. 2005. The proposition bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pradhan</author>
<author>E Loper</author>
<author>D Dligach</author>
<author>M Palmer</author>
</authors>
<title>Semeval-2007 task-17: English lexical sample, srl and all words.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>87--92</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="10700" citStr="Pradhan et al., 2007" startWordPosition="1683" endWordPosition="1686">-off annotation. The corpus development architecture is shown in Fig. 2. 4.1 Data Set Construction Phase In the set of target verbs selected for the task, preference will be given to the verbs that are strongly coercive in at least one of their senses, i.e. tend to impose semantic typing on one of their arguments. The verbs will be selected by examining the data from several sources, using the Sketch Engine (Kilgarriff et al., 2004) as described in (Rumshisky and Batiukova, 2008). An inventory of senses will be compiled for each verb. Whenever possible, the senses will be mapped to OntoNotes (Pradhan et al., 2007) and to the CPA patterns (Hanks, 2009). For each sense, a set of type 90 templates will be compiled, associating each sense with one or more syntactic patterns which will include type specification for all arguments. For example, one of the senses of the verb deny is refuse to grant. This sense is associated with the following type templates: (9) HUMAN deny ENTITY to HUMAN HUMAN deny HUMAN ENTITY The set of type templates for each verb will be built using a modification of the CPA technique (Hanks and Pustejovsky, 2005; Pustejovsky et al., 2004)). A set of sentences will be randomly extracted </context>
</contexts>
<marker>Pradhan, Loper, Dligach, Palmer, 2007</marker>
<rawString>S. Pradhan, E. Loper, D. Dligach, and M. Palmer. 2007. Semeval-2007 task-17: English lexical sample, srl and all words. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), pages 87–92, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pustejovsky</author>
<author>P Hanks</author>
<author>A Rumshisky</author>
</authors>
<title>Automated Induction of Sense in Context.</title>
<date>2004</date>
<booktitle>In COLING 2004,</booktitle>
<pages>924--931</pages>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="8399" citStr="Pustejovsky et al., 2004" startWordPosition="1312" endWordPosition="1315">ion task must then involve the following (1) identifying the verb sense and the associated syntactic frame, (2) identifying selectional requirements imposed by that verb sense on the target argument, and (3) identifying semantic type of the target argument. Sense inventories for the verbs and the type templates associated with different syntactic frames will be provided to the participants. 3.1 Semantic Types In the present task, we use a subset of semantic types from the Brandeis Shallow Ontology (BSO), which is a shallow hierarchy of types developed as a part of the CPA effort (Hanks, 2009; Pustejovsky et al., 2004; Rumshisky et al., 2006). The BSO types were selected for their prevalence in manually identified selection context patterns developed for several hundreds English verbs. That is, they capture common semantic distinctions associated with the selectional properties of many verbs. The following list of types is currently being used for annotation: (8) HUMAN, ANIMATE, PHYSICAL OBJECT, ARTIFACT, ORGANIZATION, EVENT, PROPOSITION, INFORMATION, SENSATION, LOCATION, TIME PERIOD, ABSTRACT ENTITY, ATTITUDE, EMOTION, PROPERTY, PRIVILEGE, OBLIGATION, RULE The subset of types chosen for annotation is purp</context>
<context position="11251" citStr="Pustejovsky et al., 2004" startWordPosition="1780" endWordPosition="1783">er possible, the senses will be mapped to OntoNotes (Pradhan et al., 2007) and to the CPA patterns (Hanks, 2009). For each sense, a set of type 90 templates will be compiled, associating each sense with one or more syntactic patterns which will include type specification for all arguments. For example, one of the senses of the verb deny is refuse to grant. This sense is associated with the following type templates: (9) HUMAN deny ENTITY to HUMAN HUMAN deny HUMAN ENTITY The set of type templates for each verb will be built using a modification of the CPA technique (Hanks and Pustejovsky, 2005; Pustejovsky et al., 2004)). A set of sentences will be randomly extracted for each target verb from the BNC (BNC, 2000) and the American National Corpus (Ide and Suderman, 2004). This choice of corpora should ensure a more balanced representation of language than is available in commonly annotated WSJ and other newswire text. Each extracted sentence will be automatically parsed, and the sentences organized according to the grammatical relation involving the target verb. Sentences will be excluded from the set if the target argument is expressed as anaphor, or is not present in the sentence. Semantic head for the targe</context>
</contexts>
<marker>Pustejovsky, Hanks, Rumshisky, 2004</marker>
<rawString>J. Pustejovsky, P. Hanks, and A. Rumshisky. 2004. Automated Induction of Sense in Context. In COLING 2004, Geneva, Switzerland, pages 924–931.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pustejovsky</author>
<author>R Knippen</author>
<author>J Littman</author>
<author>R Sauri</author>
</authors>
<title>Temporal and event information in natural language text.</title>
<date>2005</date>
<journal>Language Resources and Evaluation,</journal>
<volume>39</volume>
<issue>2</issue>
<contexts>
<context position="6091" citStr="Pustejovsky et al., 2005" startWordPosition="932" endWordPosition="935">es specific structural descriptions and properties of the input data; c. Train: Algorithm is trained over a corpus annotated with the target feature set; Figure 1: The MATTER Methodology d. Test: Algorithm is tested against held-out data; e. Evaluate: Standardized evaluation of results; f. Revise: Revisit the model, annotation specification, or algorithm, in order to make the annotation more robust and reliable. Some of the current and completed annotation efforts that have undergone such a development cycle include: • PropBank (Palmer et al., 2005) • NomBank (Meyers et al., 2004) • TimeBank (Pustejovsky et al., 2005) • Opinion Corpus (Wiebe et al., 2005) • Penn Discourse TreeBank (Miltsakaki et al., 2004) 3 Task Description This task involves identifying the selectional mechanism used by the predicate over a particular argument.1 For the purposes of this task, the possible relations between the predicate and a given argument are restricted to selection and coercion. In selection, the argument NP satisfies the typing requirements of the predicate, as in (5). (5) a. The spokesman denied the statement (PROPOSITION). b. The child threw the stone (PHYSICAL OBJECT). c. The audience didn’t believe the rumor (PRO</context>
</contexts>
<marker>Pustejovsky, Knippen, Littman, Sauri, 2005</marker>
<rawString>J. Pustejovsky, R. Knippen, J. Littman, and R. Sauri. 2005. Temporal and event information in natural language text. Language Resources and Evaluation, 39(2):123–164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pustejovsky</author>
<author>A Rumshisky</author>
<author>J Moszkowicz</author>
<author>O Batiukova</author>
</authors>
<title>GLML: Annotating argument selection and coercion.</title>
<date>2009</date>
<booktitle>IWCS-8: Eighth International Conference on Computational Semantics.</booktitle>
<contexts>
<context position="7357" citStr="Pustejovsky et al., 2009" startWordPosition="1146" endWordPosition="1149"> when a typeshifting operation must be performed on the complement NP in order to satisfy selectional requirements of the predicate, as in (6). Note that coercion operations may apply to any argument position in a sentence, including the subject, as seen in (6b). Coercion can also be seen as an object of a proposition as in (6c). (6) a. The president denied the attack (EVENT → PROPOSITION). b. The White House (LOCATION → HUMAN) denied this statement. c. The Boston office called with an update (EVENT → INFO). 1This task is part of a larger effort to annotate text with compositional operations (Pustejovsky et al., 2009). 89 The definition of coercion will be extended to include instances of type-shifting due to what we term the qua-relation. (7) a. You can crush the pill (PHYSICAL OBJECT) between two spoons. (Selection) b. It is always possible to crush imagination (ABSTRACT ENTITY qua PHYSICAL OBJECT) under the weight of numbers. (Coercion/qua-relation) In order to determine whether type-shifting has taken place, the classification task must then involve the following (1) identifying the verb sense and the associated syntactic frame, (2) identifying selectional requirements imposed by that verb sense on the</context>
</contexts>
<marker>Pustejovsky, Rumshisky, Moszkowicz, Batiukova, 2009</marker>
<rawString>J. Pustejovsky, A. Rumshisky, J. Moszkowicz, and O. Batiukova. 2009. GLML: Annotating argument selection and coercion. IWCS-8: Eighth International Conference on Computational Semantics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pustejovsky</author>
</authors>
<title>The generative lexicon.</title>
<date>1991</date>
<journal>Computational Linguistics,</journal>
<volume>17</volume>
<issue>4</issue>
<contexts>
<context position="2762" citStr="Pustejovsky, 1991" startWordPosition="435" endWordPosition="436">, where the verb report normally selects for a human in subject position as in (1). Notice, however, that through a metonymic interpretation, this constraint can be violated as demonstrated in (1). (1) a. John reported in late from Washington. b. Washington reported in late. Neither the surface annotation of entity extents and types, nor assigning semantic roles associated with the predicate would reflect in this case a crucial point: namely, that in order for the typing requirements of the predicate to be satisfied, what has been referred to a type coercion or a metonymy (Hobbs et al., 1993; Pustejovsky, 1991; Nunberg, 1979; Egg, 2005) has taken place. The SemEval Metonymy task (Markert and Nissim, 2007) was a good attempt to annotate such metonymic relations over a larger data set. This task involved two types with their metonymic variants: (2) i. Categories for Locations: literal, place-for-people, place-for-event, place-for-product; ii. Categories for Organizations: literal, organizationfor-members, organization-for-event, organization-forproduct, organization-for-facility. One of the limitations of this approach, however, is that, while appropriate for these specialized metonymy relations, the</context>
</contexts>
<marker>Pustejovsky, 1991</marker>
<rawString>J. Pustejovsky. 1991. The generative lexicon. Computational Linguistics, 17(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rumshisky</author>
<author>O Batiukova</author>
</authors>
<title>Polysemy in verbs: systematic relations between senses and their effect on annotation.</title>
<date>2008</date>
<booktitle>In COLING Workshop on Human Judgement in Computational Linguistics (HJCL2008),</booktitle>
<location>Manchester, England.</location>
<contexts>
<context position="10563" citStr="Rumshisky and Batiukova, 2008" startWordPosition="1660" endWordPosition="1663">or resolves disagreements. The resulting database representation is used by the exporting module to generate the corresponding XML markup or stand-off annotation. The corpus development architecture is shown in Fig. 2. 4.1 Data Set Construction Phase In the set of target verbs selected for the task, preference will be given to the verbs that are strongly coercive in at least one of their senses, i.e. tend to impose semantic typing on one of their arguments. The verbs will be selected by examining the data from several sources, using the Sketch Engine (Kilgarriff et al., 2004) as described in (Rumshisky and Batiukova, 2008). An inventory of senses will be compiled for each verb. Whenever possible, the senses will be mapped to OntoNotes (Pradhan et al., 2007) and to the CPA patterns (Hanks, 2009). For each sense, a set of type 90 templates will be compiled, associating each sense with one or more syntactic patterns which will include type specification for all arguments. For example, one of the senses of the verb deny is refuse to grant. This sense is associated with the following type templates: (9) HUMAN deny ENTITY to HUMAN HUMAN deny HUMAN ENTITY The set of type templates for each verb will be built using a m</context>
</contexts>
<marker>Rumshisky, Batiukova, 2008</marker>
<rawString>A. Rumshisky and O. Batiukova. 2008. Polysemy in verbs: systematic relations between senses and their effect on annotation. In COLING Workshop on Human Judgement in Computational Linguistics (HJCL2008), Manchester, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rumshisky</author>
<author>P Hanks</author>
<author>C Havasi</author>
<author>J Pustejovsky</author>
</authors>
<title>Constructing a corpus-based ontology using model bias.</title>
<date>2006</date>
<booktitle>In The 19th International FLAIRS Conference, FLAIRS 2006,</booktitle>
<location>Melbourne Beach, Florida, USA.</location>
<contexts>
<context position="8424" citStr="Rumshisky et al., 2006" startWordPosition="1316" endWordPosition="1319"> the following (1) identifying the verb sense and the associated syntactic frame, (2) identifying selectional requirements imposed by that verb sense on the target argument, and (3) identifying semantic type of the target argument. Sense inventories for the verbs and the type templates associated with different syntactic frames will be provided to the participants. 3.1 Semantic Types In the present task, we use a subset of semantic types from the Brandeis Shallow Ontology (BSO), which is a shallow hierarchy of types developed as a part of the CPA effort (Hanks, 2009; Pustejovsky et al., 2004; Rumshisky et al., 2006). The BSO types were selected for their prevalence in manually identified selection context patterns developed for several hundreds English verbs. That is, they capture common semantic distinctions associated with the selectional properties of many verbs. The following list of types is currently being used for annotation: (8) HUMAN, ANIMATE, PHYSICAL OBJECT, ARTIFACT, ORGANIZATION, EVENT, PROPOSITION, INFORMATION, SENSATION, LOCATION, TIME PERIOD, ABSTRACT ENTITY, ATTITUDE, EMOTION, PROPERTY, PRIVILEGE, OBLIGATION, RULE The subset of types chosen for annotation is purposefully shallow, and is </context>
</contexts>
<marker>Rumshisky, Hanks, Havasi, Pustejovsky, 2006</marker>
<rawString>A. Rumshisky, P. Hanks, C. Havasi, and J. Pustejovsky. 2006. Constructing a corpus-based ontology using model bias. In The 19th International FLAIRS Conference, FLAIRS 2006, Melbourne Beach, Florida, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ruppenhofer</author>
<author>M Ellsworth</author>
<author>M Petruck</author>
<author>C Johnson</author>
<author>J Scheffczyk</author>
</authors>
<title>FrameNet II: Extended Theory and Practice.</title>
<date>2006</date>
<contexts>
<context position="1335" citStr="Ruppenhofer et al., 2006" startWordPosition="193" endWordPosition="196">t must change type to satisfy the verb typing. We discuss the problem in detail and describe the data preparation for the task. 1 Introduction In recent years, a number of annotation schemes that encode semantic information have been developed and used to produce data sets for training machine learning algorithms. Semantic markup schemes that have focused on annotating entity types and, more generally, word senses, have been extended to include semantic relationships between sentence elements, such as the semantic role (or label) assigned to the argument by the predicate (Palmer et al., 2005; Ruppenhofer et al., 2006; Kipper, 2005; Burchardt et al., 2006; Ohara, 2008; Subirats, 2004). In this task, we take this one step further, in that this task attempts to capture the “compositional history” of the argument selection relative to the predicate. In particular, this task attempts to identify the operations of type adjustment induced by a predicate over its arguments when they do not match its selectional properties. The task is defined as follows: for each argument of a predicate, identify whether the 88 entity in that argument position satisfies the type expected by the predicate. If not, then one needs t</context>
</contexts>
<marker>Ruppenhofer, Ellsworth, Petruck, Johnson, Scheffczyk, 2006</marker>
<rawString>J. Ruppenhofer, M. Ellsworth, M. Petruck, C. Johnson, and J. Scheffczyk. 2006. FrameNet II: Extended Theory and Practice.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos Subirats</author>
</authors>
<title>FrameNet Espa˜nol. Una red sem´antica de marcos conceptuales.</title>
<date>2004</date>
<booktitle>In VIInternational Congress ofHispanic Linguistics,</booktitle>
<location>Leipzig.</location>
<contexts>
<context position="1403" citStr="Subirats, 2004" startWordPosition="205" endWordPosition="206">il and describe the data preparation for the task. 1 Introduction In recent years, a number of annotation schemes that encode semantic information have been developed and used to produce data sets for training machine learning algorithms. Semantic markup schemes that have focused on annotating entity types and, more generally, word senses, have been extended to include semantic relationships between sentence elements, such as the semantic role (or label) assigned to the argument by the predicate (Palmer et al., 2005; Ruppenhofer et al., 2006; Kipper, 2005; Burchardt et al., 2006; Ohara, 2008; Subirats, 2004). In this task, we take this one step further, in that this task attempts to capture the “compositional history” of the argument selection relative to the predicate. In particular, this task attempts to identify the operations of type adjustment induced by a predicate over its arguments when they do not match its selectional properties. The task is defined as follows: for each argument of a predicate, identify whether the 88 entity in that argument position satisfies the type expected by the predicate. If not, then one needs to identify how the entity in that position satisfies the typing expe</context>
</contexts>
<marker>Subirats, 2004</marker>
<rawString>Carlos Subirats. 2004. FrameNet Espa˜nol. Una red sem´antica de marcos conceptuales. In VIInternational Congress ofHispanic Linguistics, Leipzig.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wiebe</author>
<author>T Wilson</author>
<author>C Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language.</title>
<date>2005</date>
<journal>Language Resources and Evaluation,</journal>
<volume>39</volume>
<issue>2</issue>
<contexts>
<context position="6129" citStr="Wiebe et al., 2005" startWordPosition="939" endWordPosition="942">erties of the input data; c. Train: Algorithm is trained over a corpus annotated with the target feature set; Figure 1: The MATTER Methodology d. Test: Algorithm is tested against held-out data; e. Evaluate: Standardized evaluation of results; f. Revise: Revisit the model, annotation specification, or algorithm, in order to make the annotation more robust and reliable. Some of the current and completed annotation efforts that have undergone such a development cycle include: • PropBank (Palmer et al., 2005) • NomBank (Meyers et al., 2004) • TimeBank (Pustejovsky et al., 2005) • Opinion Corpus (Wiebe et al., 2005) • Penn Discourse TreeBank (Miltsakaki et al., 2004) 3 Task Description This task involves identifying the selectional mechanism used by the predicate over a particular argument.1 For the purposes of this task, the possible relations between the predicate and a given argument are restricted to selection and coercion. In selection, the argument NP satisfies the typing requirements of the predicate, as in (5). (5) a. The spokesman denied the statement (PROPOSITION). b. The child threw the stone (PHYSICAL OBJECT). c. The audience didn’t believe the rumor (PROPOSITION). Coercion encompasses all ca</context>
</contexts>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating expressions of opinions and emotions in language. Language Resources and Evaluation, 39(2):165–210.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>