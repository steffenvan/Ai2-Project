<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.998431">
Integrating Multiple Dependency Corpora
for Inducing Wide-coverage Japanese CCG Resources
</title>
<author confidence="0.525442">
Sumire Uematsu† Takuya Matsuzaki‡
</author>
<email confidence="0.75293">
uematsu@cks.u-tokyo.ac.jp takuya-matsuzaki@nii.ac.jp
</email>
<author confidence="0.937233">
Hiroki Hanaoka† Yusuke Miyao‡ Hideki Mima†
</author>
<email confidence="0.953608">
hanaoka@nii.ac.jp yusuke@nii.ac.jp mima@t-adm.t.u-tokyo.ac.jp
</email>
<affiliation confidence="0.881884">
†The University of Tokyo ‡National Institute of Infomatics
</affiliation>
<address confidence="0.323923">
Hongo 7-3-1, Bunkyo, Tokyo, Japan Hitotsubashi 2-1-2, Chiyoda, Tokyo, Japan
</address>
<sectionHeader confidence="0.973568" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999987214285714">
This paper describes a method of in-
ducing wide-coverage CCG resources for
Japanese. While deep parsers with corpus-
induced grammars have been emerging
for some languages, those for Japanese
have not been widely studied, mainly be-
cause most Japanese syntactic resources
are dependency-based. Our method first
integrates multiple dependency-based cor-
pora into phrase structure trees and then
converts the trees into CCG derivations.
The method is empirically evaluated in
terms of the coverage of the obtained lexi-
con and the accuracy of parsing.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999691413793104">
Syntactic parsing for Japanese has been domi-
nated by a dependency-based pipeline in which
chunk-based dependency parsing is applied and
then semantic role labeling is performed on the de-
pendencies (Sasano and Kurohashi, 2011; Kawa-
hara and Kurohashi, 2011; Kudo and Matsumoto,
2002; Iida and Poesio, 2011; Hayashibe et al.,
2011). This dominance is mainly because chunk-
based dependency analysis looks most appropriate
for Japanese syntax due to its morphosyntactic ty-
pology, which includes agglutination and scram-
bling (Bekki, 2010). However, it is also true that
this type of analysis has prevented us from deeper
syntactic analysis such as deep parsing (Clark and
Curran, 2007) and logical inference (Bos et al.,
2004; Bos, 2007), both of which have been sur-
passing shallow parsing-based approaches in lan-
guages like English.
In this paper, we present our work on induc-
ing wide-coverage Japanese resources based on
combinatory categorial grammar (CCG) (Steed-
man, 2001). Our work is basically an extension of
a seminal work on CCGbank (Hockenmaier and
Steedman, 2007), in which the phrase structure
trees of the Penn Treebank (PTB) (Marcus et al.,
1993) are converted into CCG derivations and a
wide-coverage CCG lexicon is then extracted from
these derivations. As CCGbank has enabled a va-
riety of outstanding works on wide-coverage deep
parsing for English, our resources are expected to
significantly contribute to Japanese deep parsing.
The application of the CCGbank method to
Japanese is not trivial, as resources like PTB are
not available in Japanese. The widely used re-
sources for parsing research are the Kyoto corpus
(Kawahara et al., 2002) and the NAIST text corpus
(Iida et al., 2007), both of which are based on the
dependency structures of chunks. Moreover, the
relation between chunk-based dependency struc-
tures and CCG derivations is not obvious.
In this work, we propose a method to integrate
multiple dependency-based corpora into phrase
structure trees augmented with predicate argument
relations. We can then convert the phrase structure
trees into CCG derivations. In the following, we
describe the details of the integration method as
well as Japanese-specific issues in the conversion
into CCG derivations. The method is empirically
evaluated in terms of the quality of the corpus con-
version, the coverage of the obtained lexicon, and
the accuracy of parsing with the obtained gram-
mar. Additionally, we discuss problems that re-
main in Japanese resources from the viewpoint of
developing CCG derivations.
There are three primary contributions of this pa-
per: 1) we show the first comprehensive results for
Japanese CCG parsing, 2) we present a methodol-
ogy for integrating multiple dependency-based re-
</bodyText>
<page confidence="0.957362">
1042
</page>
<note confidence="0.9167345">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1042–1051,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<figure confidence="0.9710154">
I
give
them money
NP :them&apos; NP :money&apos;
NP: I&apos; S\NP/NP/NP :
axayaz.give&apos;yxz
S\NP/NP :ayaz.give&apos;y them&apos;z
S\NP :az.give&apos;money&apos;them&apos;z
&lt;
S :give&apos;money&apos;them&apos;I&apos;
&gt;
&gt;
Sentence S Verb S\$ (e.g. S\NPga)
Noun phrase NP Postparticle NPga,�J LZlt�\NP
Auxiliary verb S\S
</figure>
<tableCaption confidence="0.94406">
Table 1: Typical categories for Japanese syntax.
</tableCaption>
<figureCaption confidence="0.871805">
Figure 2: Combinatory rule
implementation).
</figureCaption>
<figure confidence="0.9758634">
X/Y : f Y: a X : fa (&gt;)
Y : a X\Y : a X : f a (&lt;)
X/Y : f Y/Z : g X/Z : ax. f (gx) (&gt; B)
Y\Z : g X\Y : f X\Z : ax. fs(gx) (&lt; B)
C
</figure>
<figureCaption confidence="0.944665">
Figure 1: A CCG derivation. NP
</figureCaption>
<figure confidence="0.971645060606061">
nc none &lt;
\
S form
&lt; stem stem
base base
s (used in the current neg imperfect or negative
cont continuative
participation
vo s causative
\NP\N
Cat .FeatureValue Interpretation
�
case ON
ga nominal
PAST
oS accusative
Sbase\S
B
ni dativeSb
&lt;B
to comitative, complementizer, etc.
NPPi
sources to induce CCG derivations, and 3) we in-
NPga
vestigate the possibility of further improving CCG
analysis by additional resources.
�� � ���
2 Background
NPg Nga
2.1 Combinatory Categorial Grammar
NP
CCG is a syntactic theory widely accepted in the
Scont
</figure>
<figureCaption confidence="0.82705225">
NLP field. A grammar based on CCG theory con-
sists ofocategories, whicharepresent syntactic cat-
egories of words and phrases, and combinatory \NP
rules, which are rules to combine the categories.
</figureCaption>
<figure confidence="0.490589">
NPg Scont\NPni
Categories are either ground categories like S and
NPga
NP or complex categories in the form of X/Y or
Sbase
</figure>
<bodyText confidence="0.888944391304348">
X\Y , where X and Y are the categories. Cate-
gory X/Y intuitively means that it becomes cat-
egory X when it is combined with another cat-
egory Y to its right, and X\Y means it takes a
category Y to its left. Categories are combined
by applying combinatory rules (Fig. 2) to form
categories for larger phrases. Figure 1 shows a
CCG analysis of a simple English sentence, which
is called a derivation. The verb give is assigned
category S\NP/NP/NP, which indicates that it
takes two NPs to its right, one NP to its left, and fi-
nally becomes S. Starting from lexical categories
assigned to words, we can obtain categories for
phrases by applying the rules recursively.
An important property of CCG is a clear inter-
face between syntax and semantics. As shown in
Fig. 1, each category is associated with a lambda
term of semantic representations, and each com-
binatory rule is associated with rules for semantic
composition. Since these rules are universal, we
can obtain different semantic representations by
switching the semantic representations of lexical
categories. This means that we can plug in a vari-
</bodyText>
<tableCaption confidence="0.70989">
Table 2: Features for Japanese syntax (those used
</tableCaption>
<figure confidence="0.6166831">
Scnt\NPga\NPwo
&lt;
in the examples in this paper).
ety of semantic theories with CCG-based syntactic
join cause
parsing (Bos et al., 2004).
Svo s\NPga\NPn Sco\NPga\N
St\
2.2 CCG-based syntactic theory for Japanese
ga &lt;
</figure>
<bodyText confidence="0.785768">
Bekki (2010) proposed a comprehensive theory
for Japanese syntax based on CCG. While the the-
ory is basedPon Steedman (2001), it provides con-
crete explanations for a variety of constructions of
</bodyText>
<subsectionHeader confidence="0.459177">
\S S\S
</subsectionHeader>
<bodyText confidence="0.998488576923077">
Japanese, such as agglutination, scrambling, long-
distance dependencies, etc. (Fig. 3).
The ground categories in his theory are S, NP,
and CONJ (for conjunctions). Table 1 presents
typical lexical categories. While most of them
are obvious from the theory of CCG, categories
for auxiliary verbs require an explanation. In
Japanese, auxiliary verbs are extensively used to
express various semantic information, such as
tense and modality. They agglutinate to the main
verb in a sequential order. This is explained in
Bekki’s theory by the category S\S combined with
a main verb via the function composition rule
(&lt;B). Syntactic features are assigned to categories
NP and S (Table 2). The feature case represents a
syntactic case of a noun phrase. The feature form
denotes an inflection form, and is necessary for
constraining the grammaticality of agglutination.
Our implementation of the grammar basically
follows Bekki (2010)’s theory. However, as a first
step in implementing a wide-coverage Japanese
parser, we focused on the frequent syntactic con-
structions that are necessary for computing pred-
icate argument relations, including agglutination,
inflection, scrambling, case alternation, etc. Other
details of the theory are largely simplified (Fig. 3),
</bodyText>
<page confidence="0.543654">
1043
</page>
<equation confidence="0.97041316">
し
do-CONT
Scont\Sstem
&lt;B
大使
ambassador
NPnc
が
NOM
NPga\NPnc
&lt;
NPga
交渉
negotiation
NPnc
に
DAT
NPni\NPnc
&lt;
NPni
参加
participation
Sstem\NPga\NPni
た
PAST-BASE
</equation>
<figure confidence="0.901653625">
Sbase\Scont
&lt;B
Scont\NPga\NPni
Sbase\NPga\NPni
&lt;
Sbase\NPga
&lt;
Sbase
</figure>
<figureCaption confidence="0.999205">
Figure 3: A simplified CCG analysis of the sentence “The ambassador participated in the negotiation.”.
</figureCaption>
<equation confidence="0.998022333333333">
府 が 大使 を
t NOM b titi DAT particpation
S -+ NP/NP NPwo
(RelExt)
S\NP1 -+ NP1/1
NPga (RelIn)
S -+ S1/S1 (Con)
St
S\$1\NP1 -+ (S1\$1\NP1)/(S1\$1\NP1) (ConCoord)
</equation>
<figure confidence="0.902712272727273">
Dep.
Figure4: Type changing rules. The upper two are
govrnment NOM ambassadorACC ngotiaDAT
Kyoto
for relative clausesand the others for continuous
N NPga\NP NPwo N
&lt;
clauses.
NAIST Corpus
apcf bi
Chun�overnment NOM
C
Corpus
Causer
ARG-ga
ARG-ni
J:iK �k
ambassador ACC
Z! t_-
negotiation DAT
Oba -IV t7&apos; t
participation do cause PAST
</figure>
<bodyText confidence="0.9457705">
coordination and semantic representation in par-
ticular. The current implementation recognizes
</bodyText>
<equation confidence="0.847006454545454">
交渉 に 参加 さ
ga tiiti o
coordinated verbs in continuous clauses (e.g., “N
NP NP\NP S\NPi Svo sSstm
6 :fアl;kW��p -:)t/he played the piano and
NPn Svo s\NPni
sang”), but the treatment of other types of coor-
NPga Scont
Ng
dination is largely simplified. For semantic repre-
S
</equation>
<bodyText confidence="0.999973421052632">
sentation, we define predicate argument structures
(PASs) rather than the theory’s formal representa-
tion based on dynamic logic. Sophisticating our
semantic representation is left for future work.
For parsing efficiency, we modified the treat-
ment of some constructions so that empty el-
ements are excluded from the implementation.
First, we define type changing rules to produce
relative and continuous clauses (shown in Fig. 4).
The rules produce almost the same results as the
theory’s treatment, but without using empty ele-
ments (pro, etc.). We also used lexical rules to
treat pro-drop and scrambling. For the sentence in
Fig. 3, the deletion of the nominal phrase (Mit
7)1), the dative phrase (3Z9-I:), or both results in
valid sentences, and shuffling the two phrases does
so as well. Lexical entries with the scrambled or
dropped arguments are produced by lexical rules
in our implementation.
</bodyText>
<subsectionHeader confidence="0.997298">
2.3 Linguistic resources for Japanese parsing
</subsectionHeader>
<bodyText confidence="0.874077285714286">
As described in Sec. 1, dependency-based analysis
has been accepted for Japanese syntax. Research
on Japanese parsing also relies on dependency-
based corpora. Among them, we used the follow-
ing resources in this work.
Kyoto corpus A news text corpus annotated
with morphological information, chunk bound-
</bodyText>
<figureCaption confidence="0.997868">
Figure 5: The Kyoto and NAIST annotations for
</figureCaption>
<figure confidence="0.342546636363636">
ont\Pga\Pw
&lt;
t\NP
“The government had the ambassador participate
&lt;
in the negotiation.”. Accusatives are labeled as
ARG-ga in causative (see Sec. 3.2).
CAUSE PAST
aries, and dependency relations among chunks
&lt;B
i St B
</figure>
<figureCaption confidence="0.465779">
(Fig. 5). The dependencies are classified into four
</figureCaption>
<bodyText confidence="0.898811">
Sbse\Pni
types: Para (coordination), A (apposition), I (ar-
gument cluster), and Dep (default). Most of the
dependencies are annotated as Dep.
NAIST text corpus A corpus annotated with
anaphora and coreference relations. The same set
as the Kyoto corpus is annotated.&apos; The corpus
</bodyText>
<equation confidence="0.594778">
1
</equation>
<bodyText confidence="0.9931224">
only focuses on three cases: “ga” (subject), “o”
(direct object), and “ni” (indirect object) (Fig. 5).
Japanese particle corpus (JP) (Hanaoka et al.,
2010) A corpus annotated with distinct gram-
matical functions of the Japanese particle (postpo-
sition) “to”. In Japanese, “to” has many functions,
including a complementizer (similar to “that”), a
subordinate conjunction (similar to “then”), a co-
ordination conjunction (similar to “and”), and a
case marker (similar to “with”).
</bodyText>
<sectionHeader confidence="0.774742" genericHeader="introduction">
2.4 Related work
</sectionHeader>
<bodyText confidence="0.999825285714286">
Research on Japanese deep parsing is fairly lim-
ited. Formal theories of Japanese syntax were
presented by Gunji (1987) based on Head-driven
Phrase Structure Grammar (HPSG) (Sag et al.,
2003) and by Komagata (1999) based on CCG, al-
though their implementations in real-world pars-
ing have not been very successful. JACY (Siegel
</bodyText>
<footnote confidence="0.606945">
&apos;In fact, the NAIST text corpus includes additional texts,
but in this work we only use the news text section.
</footnote>
<page confidence="0.986868">
1044
</page>
<bodyText confidence="0.999985852941176">
and Bender, 2002) is a large-scale Japanese gram-
mar based on HPSG, but its semantics is tightly
embedded in the grammar and it is not as easy
to systematically switch them as it is in CCG.
Yoshida (2005) proposed methods for extracting
a wide-coverage lexicon based on HPSG from a
phrase structure treebank of Japanese. We largely
extended their work by exploiting the standard
chunk-based Japanese corpora and demonstrated
the first results for Japanese deep parsing with
grammar induced from large corpora.
Corpus-based acquisition of wide-coverage
CCG resources has enjoyed great success for En-
glish (Hockenmaier and Steedman, 2007). In
that method, PTB was converted into CCG-based
derivations from which a wide-coverage CCG lex-
icon was extracted. CCGbank has been used for
the development of wide-coverage CCG parsers
(Clark and Curran, 2007). The same methodology
has been applied to German (Hockenmaier, 2006),
Italian (Bos et al., 2009), and Turkish (C¸akıcı,
2005). Their treebanks are annotated with depen-
dencies of words, the conversion of which into
phrase structures is not a big concern. A notable
contribution of the present work is a method for in-
ducing CCG grammars from chunk-based depen-
dency structures, which is not obvious, as we dis-
cuss later in this paper.
CCG parsing provides not only predicate argu-
ment relations but also CCG derivations, which
can be used for various semantic processing tasks
(Bos et al., 2004; Bos, 2007). Our work consti-
tutes a starting point for such deep linguistic pro-
cessing for languages like Japanese.
</bodyText>
<sectionHeader confidence="0.941805" genericHeader="method">
3 Corpus integration and conversion
</sectionHeader>
<bodyText confidence="0.973718272727273">
For wide-coverage CCG parsing, we need a)
a wide-coverage CCG lexicon, b) combinatory
rules, c) training data for parse disambiguation,
and d) a parser (e.g., a CKY parser). Since d) is
grammar- and language-independent, all we have
to develop for a new language is a)–c).
As we have adopted the method of CCGbank,
which relies on a source treebank to be converted
into CCG derivations, a critical issue to address is
the absence of a Japanese counterpart to PTB. We
only have chunk-based dependency corpora, and
their relationship to CCG analysis is not clear.
Our solution is to first integrate multiple
dependency-based resources and convert them
into a phrase structure treebank that is independent
Figure 6: Internal structures of a nominal chunk
(left) and a verbal chunk (right).
of CCG analysis (Step 1). Next, we translate the
treebank into CCG derivations (Step 2). The idea
of Step 2 is similar to what has been done with
the English CCGbank, but obviously we have to
address language-specific issues.
</bodyText>
<subsectionHeader confidence="0.995364">
3.1 Dependencies to phrase structure trees
</subsectionHeader>
<bodyText confidence="0.999969181818182">
We first integrate and convert available Japanese
corpora―namely, the Kyoto corpus, the NAIST
text corpus, and the JP corpus ―into a phrase
structure treebank, which is similar in spirit to
PTB. Our approach is to convert the depen-
dency structures of the Kyoto corpus into phrase
structures and then augment them with syntac-
tic/semantic roles from the other two corpora.
The conversion involves two steps: 1) recogniz-
ing the chunk-internal structures, and (2) convert-
ing inter-chunk dependencies into phrase struc-
tures. For 1), we don’t have any explicit infor-
mation in the Kyoto corpus although, in princi-
ple, each chunk has internal structures (Vadas and
Curran, 2007; Yamada et al., 2010). The lack of
a chunk-internal structure makes the dependency-
to-constituency conversion more complex than a
similar procedure by Bos et al. (2009) that con-
verts an Italian dependency treebank into con-
stituency trees since their dependency trees are an-
notated down to the level of each word. For the
current implementation, we abandon the idea of
identifying exact structures and instead basically
rely on the following generic rules (Fig. 6):
Nominal chunks Compound nouns are first
formed as a right-branching phrase and
post-positions are then attached to it.
Verbal chunks Verbal chunks are analyzed as
left-branching structures.
The rules amount to assume that all but the last
word in a compound noun modify the head noun
(i.e., the last word) and that a verbal chunk is typ-
ically in a form V Al ... A, where V is a verb
</bodyText>
<figure confidence="0.999501620689655">
PP VP
NP
PostP
VP Aux
“to Russian president Yeltsin” “(one) was not forgiven”
NP
Aux
VP
{C
DAT
t
PAST
tables
not
Verb
WIt
forgive
ProperNoun
2&apos;J&amp;quot;lTi
Yeltsin
VerbSuffix
h
PASSIVE
Noun
NIM
president
ProperNoun
❑iP
Russia
</figure>
<page confidence="0.754576">
1045
</page>
<subsectionHeader confidence="0.650176">
Para Dep
</subsectionHeader>
<bodyText confidence="0.9997770625">
(or other predicative word) and Ais are auxiliaries
(see Fig. 6). We chose the left-branching structure
as default for verbal chunks because the semantic
scopes of the auxiliaries are generally in that or-
der (i.e., A1 has the narrowest scope). For both
cases, phrase symbols are percolated upward from
the right-most daughters of the branches (except
for a few cases like punctuation) because in almost
all cases the syntactic head of a Japanese phrase is
the right-most element.
In practice, we have found several patterns of
exceptions for the above rules. We implemented
exceptional patterns as a small CFG and deter-
mined the chunk-internal structures by determin-
istic parsing with the generic rules and the CFG.
For example, two of the rules we came up with are
</bodyText>
<listItem confidence="0.5899785">
rule A: Number -+ PrefixOfNumber Number
rule B: ClassifierPhrase -+ Number Classifier
</listItem>
<bodyText confidence="0.973374">
in the precedence: rule A &gt; B &gt; generic rules.
Using the above, we bracket a compound noun
</bodyText>
<equation confidence="0.600830714285714">
約 千 人 死亡
approximately thousand people death
PrefixOfNumber Number Classifier CommonNoun
“death of approximately one thousand people”
as in
(((約 千)人) 死亡)
(((approximately thousand) people) death)
</equation>
<bodyText confidence="0.999590444444445">
We can improve chunk-internal structures to some
extent by refining the CFG rules. A complete solu-
tion like the manual annotation by Vadas and Cur-
ran (2007) is left for future work.
The conversion of inter-chunk dependencies
into phrase structures may sound trivial, but it is
not necessarily easy when combined with chunk-
internal structures. The problem is to which node
in the internal structure of the head the dependent
</bodyText>
<table confidence="0.98865025">
dep modifier-type precedence
Para から/PostPrm まで/PostPrm, */(Verb|Aux), ...
Dep */PostPrm */(Verb|Aux), */Noun,...
Dep */PostPadnom */Noun, */(Verb|Aux), ...
</table>
<tableCaption confidence="0.998699">
Table 3: Rules to determine adjoin position.
</tableCaption>
<figure confidence="0.561127">
NP
</figure>
<figureCaption confidence="0.987996">
Figure 8: Overlay of pred-arg structure annotation
(“The white cat who said “Go!” to the dog.”).
</figureCaption>
<bodyText confidence="0.98313264516129">
tree is adjoined (Fig. 7). In the case shown in the
figure, three chunks are in the dependency relation
indicated by arrows on the top. The dotted arrows
show the nodes to which the subtrees are adjoined.
Without any human-created resources, we can-
not always determine the adjoin positions cor-
rectly. Therefore, as a compromise, we wound up
implementing approximate heuristic rules to deter-
mine the adjoin positions. Table 3 shows examples
of such rules. A rule specifies a precedence of the
possible adjoin nodes as an ordered list of patterns
on the lexical head of the subtree under an ad-
join position. The precedence is defined for each
combination of the type of the dependent phrase,
which is determined by its lexical head, and the
dependency type in the Kyoto corpus.
To select the adjoin position for the left-most
subtree in Fig. 7, for instance, we look up the
rule table using the dependency type, “Para”, and
the lexical head of the modifier subtree, “ 7)1,h
/PostP,m”, as the key, and find the precedence “ I
-C/PostP,m, */(Verb|Aux), ...”. We thus select the
PP-node on the middle subtree indicated by the
dotted arrow because its lexical head (the right-
most word), “ I-C/PostP,m”, matches the first
pattern in the precedence list. In general, we seek
for an adjoin node for each pattern p in the prece-
dence list, until we find a first match.
The semantic annotation given in the NAIST
corpus and the JP corpus is overlaid on the phrase
structure trees with slight modifications (Fig. 8).
</bodyText>
<figure confidence="0.998796305555556">
PP PP
PP
Noun
PostPcm
PostPcm
PP
PostPad
om
Noun
H§H
birth
A`5
from
Mr.
process
It
ACC
PostPcm 0)
adnominal
Noun
PP
� Ve
death to
“A process from birth to death”
Noun
H§H
birth
PostPcm
A`5
from
Noun
�
death
PostPcm
Ve
to
</figure>
<figureCaption confidence="0.891472">
Figure 7: From inter-chunk dependencies to a tree.
</figureCaption>
<figure confidence="0.996011872727273">
PP
PostPadnom
Mr.
process
PP
PP
0)
adnominal
PP
Noun
It
ACC
NP
PostPcm
PP
ARG-to
PP
ARG-ga
VP
NP
VP
ARG-ni
VP
Noun
犬
dog
NAIST
PostP
に
DAT
Verb
行け
go!
Verb
Aux
Adj
Noun
PostP
と
CMP
言っ
say
た
PAST
白い
white
ARG-ga
猫
cat
ARG-ga
ARG-ni ARG-ga
JP
ARG-CLS
1046
S
</figure>
<figureCaption confidence="0.9824795">
Figure 9: A phrase structure into a CCG deriva-
tion.
</figureCaption>
<bodyText confidence="0.999895857142857">
In the figure, the annotation given in the two cor-
pora is shown inside the dotted box at the bottom.
We converted the predicate-argument annotations
given as labeled word-to-word dependencies into
the relations between the predicate words and their
argument phrases. The results are thus similar to
the annotation style of PropBank (Palmer et al.,
2005). In the NAIST corpus, each pred-arg re-
lation is labeled with the argument-type (ga/o/ni)
and a flag indicating that the relation is medi-
ated by either a syntactic dependency or a zero
anaphora. For a relation of a predicate wp and its
argument wa in the NAIST corpus, the boundary
of the argument phrase is determined as follows:
</bodyText>
<listItem confidence="0.969951142857143">
1. If wa precedes wp and the relation is medi-
ated by a syntactic dep., select the maximum
PP that is formed by attaching one or more
postpositions to the NP headed by wa.
2. If wp precedes wa or the relation is mediated
by a zero anaphora, select the maximum NP
headed by wa that does not include wp.
</listItem>
<bodyText confidence="0.9998768">
In the figure, “;k/dog 1:/DAT” is marked as the ni-
argument of the predicate “A--/say” (Case 1), and
“nL `/white M/cat” is marked as its ga-argument
(Case 2). Case 1 is for the most basic construction,
where an argument PP precedes its predicate. Case
</bodyText>
<figure confidence="0.417029">
“(to) meet a friend”
“(to) meet at ten”
</figure>
<figureCaption confidence="0.5473785">
Figure 10: An argument post particle phrase (PP)
(upper) and an adjunct PP (lower).
</figureCaption>
<bodyText confidence="0.999264181818182">
2 covers the relative clause construction, where a
relative clause precedes the head NP, the modifi-
cation of a noun by an adjective, and the relations
mediated by zero anaphora.
The JP corpus provides only the function label
to each particle “to” in the text. We determined
the argument phrases marked by the “to” particles
labeled as (nominal or clausal) argument-markers
in a similar way to Case 1 above and identified the
predicate words as the lexical heads of the phrases
to which the PPto phrases attach.
</bodyText>
<subsectionHeader confidence="0.997551">
3.2 Phrase structures to CCG derivations
</subsectionHeader>
<bodyText confidence="0.989106">
This step consists of three procedures (Fig. 9):
</bodyText>
<listItem confidence="0.999513666666667">
1. Add constraints on categories and features
to tree nodes as far as possible and assign a
combinatory rule to each branching.
2. Apply combinatory rules to all branching and
obtain CCG derivations.
3. Add feature constraints to terminal nodes.
</listItem>
<subsectionHeader confidence="0.955287">
3.2.1 Local constraint on derivations
</subsectionHeader>
<bodyText confidence="0.999156285714286">
According to the phrase structures, the first proce-
dure in Step 2 imposes restrictions on the resulting
CCG derivations. To describe the restrictions, we
focus on some of the notable constructions and il-
lustrate the restrictions for each of them.
Phrases headed by case marker particles A
phrase of this type must be either an argument
(Fig. 10, upper) or a modifier (Fig. 10, lower) of a
predicative. Distinction between the two is made
based on the pred-arg annotation of the predica-
tive. If a phrase is found to be an argument, 1) cat-
egory NP is assigned to the corresponding node,
2) the case feature of the category is given accord-
ing to the particle (in the case of Fig. 10 (upper),
</bodyText>
<figure confidence="0.9942205">
NPni
Sbase\NPni
PP
VP
Noun
交渉
negotiation
PostPcm
に
DAT
VP
VerbSuffix
Aux
た
PAST
VP
Noun
参加
participation
Step 2-1
S
&lt;
Verb
さ
do
せ
CAUSE
Step 2-2, 2-3
Sbase
NPnc
交渉
negotiation
に
DAT
Svo_s\NPni
参加
participation
Svo_s\Svo_s
さ
do
Sbase\Sc
た
PAST
せ
CAUSE
Scont\NPni
NPni\NPnc
Svo_s\NPni
Scont\Svo_s
&lt; or &lt;B
&lt;
T2
NPni
NP
交渉
negotiation
Tr
に
DAT T4
Tg
参加
participation
&lt; or &lt;B
S\S
さ
do
T3
S\S
せ
CAUSE
&lt; or &lt;B
S\S
た
PAST
S
NPni S\NPni
友達 に
friend-DAT
会う
meet-BASE
友達 に
friend-DAT
会う
meet-BASE
VP
NPni &lt;
PP VP
VP
T/T &gt;
PP X VP
S
S\S S
10時 に
10 o’clock-TIME
会う
meet-BASE
10時 に
10 o’clock-TIME
会う
meet-BASE
1047
“did not speak”
</figure>
<figureCaption confidence="0.916467">
Figure 11: An auxiliary verb and its conversion.
</figureCaption>
<figure confidence="0.630941">
“(to) have her inquire”
</figure>
<figureCaption confidence="0.999072">
Figure 12: A causative construction.
</figureCaption>
<bodyText confidence="0.999860766666667">
ni for dative), and 3) the combinatory rule that
combines the particle phrase and the predicative
phrase is assigned backward function application
rule (&lt;). Otherwise, a category T/T is assigned to
the corresponding modifier node and the rule will
be forward function application (&gt;).
Auxiliary verbs As described in Sec. 2.2, an
auxiliary verb is always given the category S\S
and is combined with a verbal phrase via &lt; or &lt;B
(Fig. 11). Furthermore, we assign the form feature
value of the returning category S according to the
inflection form of the auxiliary. In the case shown
in the figure, Sbase\S is assigned for “た/PAST-
BASE” and Scont\S for “なかっ/not-CONT”. As
a result of this restriction, we can obtain condi-
tions for every auxiliary agglutination because the
two form values in S\S are both restricted after
applying combinatory rules (Sec. 3.2.2).
Case alternations In addition to the argu-
ment/adjunct distinction illustrated above, a pro-
cess is needed for argument phrases of predicates
involving case alternation. Such predicates are
either causative (see Fig. 12) or passive verbs
and can be detected by voice annotation from the
NAIST corpus. For an argument of that type of
verb, its deep case (ga for Fig. 12) must be used
to construct the semantic representation, namely
the PAS. As well as assigning the shallow case
value (ni in Fig. 12) to the argument’s category
NP, as usual, we assign a restriction to the PAS
</bodyText>
<figureCaption confidence="0.786965333333333">
“a store where (I) bought the book”
Figure 13: A relative clause with/without argu-
ment extraction (upper/lower, respectively).
</figureCaption>
<bodyText confidence="0.999682838709678">
of the verb so that the semantic argument corre-
sponding to the deep case is co-indexed with the
argument NP. These restrictions are then utilized
for PAS construction in Sec. 3.2.3.
Relative clauses A relative clause can be de-
tected as a subtree that has a VP as its left child
and an NP as its right child, as shown in Fig. 13.
The conversion of the subtree consists of 1) in-
serting a node on the top of the left VP (see the
right-hand side of Fig. 13), and 2) assigning the
appropriate unary rule to make the new node. The
difference between candidate rules RelExt and Re-
lIn (see Fig. 4) is whether the right-hand NP is
an obligatory argument of the VP or not, which
can be determined by the pred-arg annotation on
the predicate in the VP. In the upper example in
Fig. 13, RelIn is assigned because the right NP
“book” is annotated as an accusative argument of
the predicate “buy”. In contrast, RelExt is as-
signed in the lower side in the figure because the
right NP “store” is not annotated as an argument.
Continuous clauses A continuous clause can be
detected as a subtree with a VP of continuous form
as its left child and a VP as its right child. Its
conversion is similar to that of a relative clause,
and only differs in that the candidate rules are Con
and ConCoord. ConCoord generates a continu-
ous clause that shares arguments with the main
clause while Con produces one without shared ar-
guments. Rule assignment is done by comparing
the pred-arg annotations of the two phrases.
</bodyText>
<figure confidence="0.999468327272727">
Sbase\NPga
VP
Aux
&lt; or &lt;B Scont\S Scont\NPga
Sbase\Scont
Verb
��
Speak-NEG
Aux
tafi��
not-CONT
t
PAST-BASE
Sneg\NPga
��
Speak-NEG
Scont\Sneg
tafi��
not-CONT
t
PAST-BASE
VP
&lt; or &lt;B
Sbase\S
S
NPni[1]
VP
&lt;
S\NPni[1]
PP
ARG-ga
VP
NPni[1]
Verb
調べ
inquire-NEG
ga: [1]
VerbSuffix
させる
cause-BASE
S\NPni[1]
ga [1]
調べ
inquire-NEG
S\S
させる
cause-BASE
彼女 に
her-DAT
彼女 に
her-DAT
NP[1]/NP[1]
S\NP[1]
Verb
買っ
buy-CONT
NP
Aux
た
PAST-ATTR
S\NPo
買っ
buy-CONT
NP
S\S
た
PAST-ATTR
Noun
本
book
VP
S\NPo[1]
“a book which (I) bought”
NP[1]/NP[1]
NP
本
book
NP/NP
NP
NP
NP/NP
NP
VP
Verb
買っ
buy-CONT
X
Aux
た
PAST-ATTR
S
S\NPo
S\NPo
買っ
buy-CONT
S
Noun
店
store
店
store
PP VP
NPo
本 を
book-ACC
S\S
た
PAST-ATTR
本 を
book-ACC
</figure>
<page confidence="0.943101">
1048
</page>
<table confidence="0.99795625">
Training Develop. Test
#Sentences 24,283 4,833 9,284
#Chunks 234,685 47,571 89,874
#Words 664,898 136,585 255,624
</table>
<tableCaption confidence="0.999359">
Table 4: Statistics of input linguistic resources.
</tableCaption>
<subsectionHeader confidence="0.520355">
3.2.2 Inverse application of rules
</subsectionHeader>
<bodyText confidence="0.9999626">
The second procedure in Step 2 begins with as-
signing a category 5 to the root node. A combi-
natory rule assigned to each branching is then “in-
versely” applied so that the constraint assigned to
the parent transfers to the children.
</bodyText>
<subsectionHeader confidence="0.70795">
3.2.3 Constraints on terminal nodes
</subsectionHeader>
<bodyText confidence="0.9999505">
The final process consists of a) imposing restric-
tions on the terminal category in order to instan-
tiate all the feature values, and b) constructing a
PAS for each verbal terminal. An example of pro-
cess a) includes setting the form features in the
verb category, such as S\NP,,,i, according to the
inflection form of the verb. As for b), arguments
in a PAS are given according to the category and
the partial restriction. For instance, if a category
S\NP,,,i is obtained for “MA/inquire” (Fig. 12),
the PAS for “inquire” is unary because the cate-
gory has one argument category (NP,,,i), and the
category is co-indexed with the semantic argument
ga in the PAS due to the partial restriction depicted
in Sec. 3.2.1. As a result, a lexical entry is ob-
tained as MA �- S\NP,,,i[1]: inquire([1]).
</bodyText>
<subsectionHeader confidence="0.998523">
3.3 Lexical entries
</subsectionHeader>
<bodyText confidence="0.999986333333333">
Finally, lexical rules are applied to each of the ob-
tained lexical entries in order to reduce them to
the canonical form. Since words in the corpus (es-
pecially verbs) often involve pro-drop and scram-
bling, there are a lot of obtained entries that have
slightly varied categories yet share a PAS. We as-
sume that an obtained entry is a variation of the
canonical one and register the canonical entries in
the lexicon. We treat only subject deletion for pro-
drop because there is not sufficient information to
judge the deletion of other arguments. Scrambling
is simply treated as permutation of arguments.
</bodyText>
<sectionHeader confidence="0.999526" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.9465405">
We used the following for the implementation of
our resources: Kyoto corpus ver. 4.02, NAIST text
</bodyText>
<footnote confidence="0.9830345">
2http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?
Kyoto\%20University\%20Text\%20Corpus
</footnote>
<table confidence="0.9759622">
Training Develop. Test
St.1 St.2 St.1 St.2 St.1 St.2
Sent. 24,283 24,116 4,833 4,803 9,284 9,245
Converted 24,116 22,820 4,803 4,559 9,245 8,769
Con. rate 99.3 94.6 99.4 94.9 99.6 94.9
</table>
<tableCaption confidence="0.72325">
Table 5: Statistics of corpus conversion.
Sentential Coverage
</tableCaption>
<table confidence="0.993735">
Covered Uncovered Cov. (%)
Devel. 3,920 639 85.99
Test 7,610 1,159 86.78
Lexical Coverage
Word Known Unknown
combi. cat. word
Devel. 127,144 126,383 682 79 0
Test 238,083 236,651 1,242 145 0
</table>
<tableCaption confidence="0.998182">
Table 6: Sentential and lexical coverage.
</tableCaption>
<bodyText confidence="0.99957">
corpus ver. 1.53, and JP corpus ver. 1.04. The
integrated corpus is divided into training, devel-
opment, and final test sets following the standard
data split in previous works on Japanese depen-
dency parsing (Kudo and Matsumoto, 2002). The
details of these resources are shown in Table 4.
</bodyText>
<subsectionHeader confidence="0.999546">
4.1 Corpus conversion and lexicon extraction
</subsectionHeader>
<bodyText confidence="0.999993380952381">
Table 5 shows the number of successful conver-
sions performed by our method. In total, we ob-
tained 22,820 CCG derivations from 24,283 sen-
tences (in the training set), resulting in the to-
tal conversion rate of 93.98%. The table shows
we lost more sentences in Step 2 than in Step 1.
This is natural because Step 2 imposed more re-
strictions on resulting structures and therefore de-
tected more discrepancies including compounding
errors. Our conversion rate is about 5.5 points
lower than the English counterpart (Hockenmaier
and Steedman, 2007). Manual investigation of the
sampled derivations would be beneficial for the
conversion improvement.
For the lexicon extraction from the CCGbank,
we obtained 699 types of lexical categories from
616,305 word tokens. After lexical reduction, the
number of categories decreased to 454, which in
turn may produce 5,342 categories by lexical ex-
pansion. The average number of categories for a
word type was 11.68 as a result.
</bodyText>
<subsectionHeader confidence="0.998723">
4.2 Evaluation of coverage
</subsectionHeader>
<bodyText confidence="0.9531735">
Following the evaluation criteria in (Hockenmaier
and Steedman, 2007), we measured the coverage
</bodyText>
<footnote confidence="0.994637666666667">
3http://cl.naist.jp/nldata/corpus/
4https://alaginrc.nict.go.jp/resources/tocorpus/
tocorpusabstract.html
</footnote>
<page confidence="0.996429">
1049
</page>
<bodyText confidence="0.999982789473684">
of the grammar on unseen texts. First, we obtained
CCG derivations for evaluation sets by applying
our conversion method and then used these deriva-
tions as gold standard. Lexical coverage indicates
the number of words to which the grammar assigns
a gold standard category. Sentential coverage indi-
cates the number of sentences in which all words
are assigned gold standard categories 5.
Table 6 shows the evaluation results. Lexical
coverage was 99.40% with rare word treatment,
which is in the same level as the case of the En-
glish CCG parser C&amp;C (Clark and Curran, 2007).
We also measured coverage in a “weak” sense,
which means the number of sentences that are
given at least one analysis (not necessarily cor-
rect) by the obtained grammar. This number was
99.12 % and 99.06 % for the development and the
test set, respectively, which is sufficiently high for
wide-coverage parsing of real-world texts.
</bodyText>
<subsectionHeader confidence="0.998701">
4.3 Evaluation of parsing accuracy
</subsectionHeader>
<bodyText confidence="0.999979074074074">
Finally, we evaluated the parsing accuracy. We
employed the parser and the supertagger of
(Miyao and Tsujii, 2008), specifically, its gen-
eralized modules for lexicalized grammars. We
trained log-linear models in the same way as
(Clark and Curran, 2007) using the training set as
training data. Feature sets were simply borrowed
from an English parser; no tuning was performed.
Following conventions in research on Japanese de-
pendency parsing, gold morphological analysis re-
sults were input to a parser. Following C&amp;C, the
evaluation measure was precision and recall over
dependencies, where a dependency is defined as a
4-tuple: a head of a functor, a functor category, an
argument slot, and a head of an argument.
Table 7 shows the parsing accuracy on the de-
velopment and the test sets. The supertagging ac-
curacy is presented in the upper table. While our
coverage was almost the same as C&amp;C, the perfor-
mance of our supertagger and parser was lower.
To improve the performance, tuning disambigua-
tion models for Japanese is a possible approach.
Comparing the parser’s performance with previ-
ous works on Japanese dependency parsing is dif-
ficult as our figures are not directly comparable
to theirs. Sassano and Kurohashi (2009) reported
the accuracy of their parser as 88.48 and 95.09
</bodyText>
<footnote confidence="0.9985365">
5Since a gold derivation can logically be obtained if gold
categories are assigned to all words in a sentence, sentential
coverage means that the obtained lexicon has the ability to
produce exactly correct derivations for those sentences.
</footnote>
<table confidence="0.9183709">
Supertagging accuracy
Lex. Cov. Cat. Acc.
Devel. 99.40 90.86
Test 99.40 90.69
C&amp;C 99.63 94.32
Overall performance
LP LR LF UP UR UF
Devel. 82.55 82.73 82.64 90.02 90.22 90.12
Test 82.40 82.59 82.50 89.95 90.15 90.05
C&amp;C 88.34 86.96 87.64 93.74 92.28 93.00
</table>
<tableCaption confidence="0.994486">
Table 7: Parsing accuracy. LP, LR and LF refer to
</tableCaption>
<bodyText confidence="0.976516363636364">
labeled precision, recall, and F-score respectively.
UP, UR, and UF are for unlabeled.
in unlabeled chunk-based and word-based F1 re-
spectively. While our score of 90.05 in unlabeled
category dependency seems to be lower than their
word-based score, this is reasonable because our
category dependency includes more difficult prob-
lems, such as whether a subject PP is shared by
coordinated verbs. Thus, our parser is expected to
be capable of real-world Japanese text analysis as
well as dependency parsers.
</bodyText>
<sectionHeader confidence="0.999113" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999978208333333">
In this paper, we proposed a method to induce
wide-coverage Japanese resources based on CCG
that will lead to deeper syntactic analysis for
Japanese and presented empirical evaluation in
terms of the quality of the obtained lexicon and
the parsing accuracy. Although our work is basi-
cally in line with CCGbank, the application of the
method to Japanese is not trivial due to the fact that
the relationship between chunk-based dependency
structures and CCG derivations is not obvious.
Our method integrates multiple dependency-
based resources to convert them into an integrated
phrase structure treebank. The obtained treebank
is then transformed into CCG derivations. The
empirical evaluation in Sec. 4 shows that our cor-
pus conversion successfully converts 94 % of the
corpus sentences and the coverage of the lexicon
is 99.4 %, which is sufficiently high for analyz-
ing real-world texts. A comparison of the parsing
accuracy with previous works on Japanese depen-
dency parsing and English CCG parsing indicates
that our parser can analyze real-world Japanese
texts fairly well and that there is room for improve-
ment in disambiguation models.
</bodyText>
<page confidence="0.991185">
1050
</page>
<sectionHeader confidence="0.995868" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999783378640777">
Daisuke Bekki. 2010. Formal Theory of Japanese Syn-
tax. Kuroshio Shuppan. (In Japanese).
Johan Bos, Stephen Clark, Mark Steedman, James R.
Curran, and Julia Hockenmaier. 2004. Wide-
coverage semantic representations from a CCG
parser. In Proceedings of COLING 2004, pages
1240–1246.
Johan Bos, Cristina Bosco, and Alessandro Mazzei.
2009. Converting a dependency treebank to a cate-
gorial grammar treebank for Italian. In Proceedings
of the Eighth International Workshop on Treebanks
and Linguistic Theories (TLT8), pages 27–38.
Johan Bos. 2007. Recognising textual entailment and
computational semantics. In Proceedings of Seventh
International Workshop on Computational Seman-
tics IWCS-7, page 1.
Ruken C¸akıcı. 2005. Automatic induction of a CCG
grammar for Turkish. In Proceedings of ACL Stu-
dent Research Workshop, pages 73–78.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4).
Takao Gunji. 1987. Japanese Phrase Structure Gram-
mar: A Unification-based Approach. D. Reidel.
Hiroki Hanaoka, Hideki Mima, and Jun’ichi Tsujii.
2010. A Japanese particle corpus built by example-
based annotation. In Proceedings of LREC 2010.
Yuta Hayashibe, Mamoru Komachi, and Yuji Mat-
sumoto. 2011. Japanese predicate argument struc-
ture analysis exploiting argument position and type.
In Proceedings of IJCNLP 2011, pages 201–209.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Com-
putational Linguistics, 33(3):355–396.
Julia Hockenmaier. 2006. Creating a CCGbank and
a wide-coverage CCG lexicon for German. In Pro-
ceedings of the Joint Conference of COLING/ACL
2006.
Ryu Iida and Massimo Poesio. 2011. A cross-lingual
ILP solution to zero anaphora resolution. In Pro-
ceedings of ACL-HLT 2011, pages 804–813.
Ryu Iida, Mamoru Komachi, Kentaro Inui, and Yuji
Matsumoto. 2007. Annotating a Japanese text
corpus with predicate-argument and coreference re-
lations. In Proceedings of Linguistic Annotation
Workshop, pages 132–139.
Daisuke Kawahara and Sadao Kurohashi. 2011. Gen-
erative modeling of coordination by factoring paral-
lelism and selectional preferences. In Proceedings
of IJCNLP 2011.
Daisuke Kawahara, Sadao Kurohashi, and Koiti
Hasida. 2002. Construction of a Japanese
relevance-tagged corpus. In Proceedings of the 8th
Annual Meeting of the Association for Natural Lan-
guage Processing, pages 495–498. (In Japanese).
Nobo Komagata. 1999. Information Structure in Texts:
A Computational Analysis of Contextual Appropri-
ateness in English and Japanese. Ph.D. thesis, Uni-
versity of Pennsylvania.
Taku Kudo and Yuji Matsumoto. 2002. Japanese
dependency analyisis using cascaded chunking. In
Proceedings of CoNLL 2002.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313–330.
Yusuke Miyao and Jun’ichi Tsujii. 2008. Feature for-
est models for probabilistic HPSG parsing. Compu-
tational Linguistics, 34(1):35–80.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71–106.
Ivan A. Sag, Thomas Wasow, and Emily M. Bender.
2003. Syntactic Theory: A Formal Introduction, 2nd
Edition. CSLI Publications.
Ryohei Sasano and Sadao Kurohashi. 2011. A dis-
criminative approach to Japanese zero anaphora res-
olution with large-scale lexicalized case frames. In
Proceedings of IJCNLP 2011.
Manabu Sassano and Sadao Kurohashi. 2009. A uni-
fied single scan algorithm for Japanese base phrase
chunking and dependency parsing. In Proceedings
of ACL-IJCNLP 2009.
Melanie Siegel and Emily M. Bender. 2002. Efficient
deep processing of Japanese. In Proceedings of the
3rd Workshop on Asian Language Resources and In-
ternational Standardization.
Mark Steedman. 2001. The Syntactic Process. MIT
Press.
David Vadas and James Curran. 2007. Adding noun
phrase structure to the Penn Treebank. In Proceed-
ings of ACL 2007, pages 240–247.
Emiko Yamada, Eiji Aramaki, Takeshi Imai, and
Kazuhiko Ohe. 2010. Internal structure of a disease
name and its application for ICD coding. Studies
in health technology and informatics, 160(2):1010–
1014.
Kazuhiro Yoshida. 2005. Corpus-oriented develop-
ment of Japanese HPSG parsers. In Proceedings of
the ACL Student Research Workshop.
</reference>
<page confidence="0.993132">
1051
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.644253">
<title confidence="0.979068">Integrating Multiple Dependency for Inducing Wide-coverage Japanese CCG Resources</title>
<abstract confidence="0.8385155">uematsu@cks.u-tokyo.ac.jp takuya-matsuzaki@nii.ac.jp hanaoka@nii.ac.jp yusuke@nii.ac.jp mima@t-adm.t.u-tokyo.ac.jp</abstract>
<affiliation confidence="0.947099">University of Tokyo Institute of Infomatics</affiliation>
<address confidence="0.869291">Hongo 7-3-1, Bunkyo, Tokyo, Japan Hitotsubashi 2-1-2, Chiyoda, Tokyo, Japan</address>
<abstract confidence="0.996659666666667">This paper describes a method of inducing wide-coverage CCG resources for Japanese. While deep parsers with corpusinduced grammars have been emerging for some languages, those for Japanese have not been widely studied, mainly because most Japanese syntactic resources are dependency-based. Our method first integrates multiple dependency-based corpora into phrase structure trees and then converts the trees into CCG derivations. The method is empirically evaluated in terms of the coverage of the obtained lexicon and the accuracy of parsing.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Daisuke Bekki</author>
</authors>
<title>Formal Theory of Japanese Syntax. Kuroshio Shuppan.</title>
<date>2010</date>
<note>(In Japanese).</note>
<contexts>
<context position="1516" citStr="Bekki, 2010" startWordPosition="207" endWordPosition="208">the coverage of the obtained lexicon and the accuracy of parsing. 1 Introduction Syntactic parsing for Japanese has been dominated by a dependency-based pipeline in which chunk-based dependency parsing is applied and then semantic role labeling is performed on the dependencies (Sasano and Kurohashi, 2011; Kawahara and Kurohashi, 2011; Kudo and Matsumoto, 2002; Iida and Poesio, 2011; Hayashibe et al., 2011). This dominance is mainly because chunkbased dependency analysis looks most appropriate for Japanese syntax due to its morphosyntactic typology, which includes agglutination and scrambling (Bekki, 2010). However, it is also true that this type of analysis has prevented us from deeper syntactic analysis such as deep parsing (Clark and Curran, 2007) and logical inference (Bos et al., 2004; Bos, 2007), both of which have been surpassing shallow parsing-based approaches in languages like English. In this paper, we present our work on inducing wide-coverage Japanese resources based on combinatory categorial grammar (CCG) (Steedman, 2001). Our work is basically an extension of a seminal work on CCGbank (Hockenmaier and Steedman, 2007), in which the phrase structure trees of the Penn Treebank (PTB)</context>
<context position="6650" citStr="Bekki (2010)" startWordPosition="1058" endWordPosition="1059"> category is associated with a lambda term of semantic representations, and each combinatory rule is associated with rules for semantic composition. Since these rules are universal, we can obtain different semantic representations by switching the semantic representations of lexical categories. This means that we can plug in a variTable 2: Features for Japanese syntax (those used Scnt\NPga\NPwo &lt; in the examples in this paper). ety of semantic theories with CCG-based syntactic join cause parsing (Bos et al., 2004). Svo s\NPga\NPn Sco\NPga\N St\ 2.2 CCG-based syntactic theory for Japanese ga &lt; Bekki (2010) proposed a comprehensive theory for Japanese syntax based on CCG. While the theory is basedPon Steedman (2001), it provides concrete explanations for a variety of constructions of \S S\S Japanese, such as agglutination, scrambling, longdistance dependencies, etc. (Fig. 3). The ground categories in his theory are S, NP, and CONJ (for conjunctions). Table 1 presents typical lexical categories. While most of them are obvious from the theory of CCG, categories for auxiliary verbs require an explanation. In Japanese, auxiliary verbs are extensively used to express various semantic information, suc</context>
</contexts>
<marker>Bekki, 2010</marker>
<rawString>Daisuke Bekki. 2010. Formal Theory of Japanese Syntax. Kuroshio Shuppan. (In Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bos</author>
<author>Stephen Clark</author>
<author>Mark Steedman</author>
<author>James R Curran</author>
<author>Julia Hockenmaier</author>
</authors>
<title>Widecoverage semantic representations from a CCG parser.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING</booktitle>
<pages>1240--1246</pages>
<contexts>
<context position="1703" citStr="Bos et al., 2004" startWordPosition="237" endWordPosition="240">dependency parsing is applied and then semantic role labeling is performed on the dependencies (Sasano and Kurohashi, 2011; Kawahara and Kurohashi, 2011; Kudo and Matsumoto, 2002; Iida and Poesio, 2011; Hayashibe et al., 2011). This dominance is mainly because chunkbased dependency analysis looks most appropriate for Japanese syntax due to its morphosyntactic typology, which includes agglutination and scrambling (Bekki, 2010). However, it is also true that this type of analysis has prevented us from deeper syntactic analysis such as deep parsing (Clark and Curran, 2007) and logical inference (Bos et al., 2004; Bos, 2007), both of which have been surpassing shallow parsing-based approaches in languages like English. In this paper, we present our work on inducing wide-coverage Japanese resources based on combinatory categorial grammar (CCG) (Steedman, 2001). Our work is basically an extension of a seminal work on CCGbank (Hockenmaier and Steedman, 2007), in which the phrase structure trees of the Penn Treebank (PTB) (Marcus et al., 1993) are converted into CCG derivations and a wide-coverage CCG lexicon is then extracted from these derivations. As CCGbank has enabled a variety of outstanding works o</context>
<context position="6557" citStr="Bos et al., 2004" startWordPosition="1042" endWordPosition="1045">ortant property of CCG is a clear interface between syntax and semantics. As shown in Fig. 1, each category is associated with a lambda term of semantic representations, and each combinatory rule is associated with rules for semantic composition. Since these rules are universal, we can obtain different semantic representations by switching the semantic representations of lexical categories. This means that we can plug in a variTable 2: Features for Japanese syntax (those used Scnt\NPga\NPwo &lt; in the examples in this paper). ety of semantic theories with CCG-based syntactic join cause parsing (Bos et al., 2004). Svo s\NPga\NPn Sco\NPga\N St\ 2.2 CCG-based syntactic theory for Japanese ga &lt; Bekki (2010) proposed a comprehensive theory for Japanese syntax based on CCG. While the theory is basedPon Steedman (2001), it provides concrete explanations for a variety of constructions of \S S\S Japanese, such as agglutination, scrambling, longdistance dependencies, etc. (Fig. 3). The ground categories in his theory are S, NP, and CONJ (for conjunctions). Table 1 presents typical lexical categories. While most of them are obvious from the theory of CCG, categories for auxiliary verbs require an explanation. I</context>
<context position="13493" citStr="Bos et al., 2004" startWordPosition="2134" endWordPosition="2137"> (Clark and Curran, 2007). The same methodology has been applied to German (Hockenmaier, 2006), Italian (Bos et al., 2009), and Turkish (C¸akıcı, 2005). Their treebanks are annotated with dependencies of words, the conversion of which into phrase structures is not a big concern. A notable contribution of the present work is a method for inducing CCG grammars from chunk-based dependency structures, which is not obvious, as we discuss later in this paper. CCG parsing provides not only predicate argument relations but also CCG derivations, which can be used for various semantic processing tasks (Bos et al., 2004; Bos, 2007). Our work constitutes a starting point for such deep linguistic processing for languages like Japanese. 3 Corpus integration and conversion For wide-coverage CCG parsing, we need a) a wide-coverage CCG lexicon, b) combinatory rules, c) training data for parse disambiguation, and d) a parser (e.g., a CKY parser). Since d) is grammar- and language-independent, all we have to develop for a new language is a)–c). As we have adopted the method of CCGbank, which relies on a source treebank to be converted into CCG derivations, a critical issue to address is the absence of a Japanese cou</context>
</contexts>
<marker>Bos, Clark, Steedman, Curran, Hockenmaier, 2004</marker>
<rawString>Johan Bos, Stephen Clark, Mark Steedman, James R. Curran, and Julia Hockenmaier. 2004. Widecoverage semantic representations from a CCG parser. In Proceedings of COLING 2004, pages 1240–1246.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bos</author>
<author>Cristina Bosco</author>
<author>Alessandro Mazzei</author>
</authors>
<title>Converting a dependency treebank to a categorial grammar treebank for Italian.</title>
<date>2009</date>
<booktitle>In Proceedings of the Eighth International Workshop on Treebanks and Linguistic Theories (TLT8),</booktitle>
<pages>27--38</pages>
<contexts>
<context position="12999" citStr="Bos et al., 2009" startWordPosition="2051" endWordPosition="2054">ely extended their work by exploiting the standard chunk-based Japanese corpora and demonstrated the first results for Japanese deep parsing with grammar induced from large corpora. Corpus-based acquisition of wide-coverage CCG resources has enjoyed great success for English (Hockenmaier and Steedman, 2007). In that method, PTB was converted into CCG-based derivations from which a wide-coverage CCG lexicon was extracted. CCGbank has been used for the development of wide-coverage CCG parsers (Clark and Curran, 2007). The same methodology has been applied to German (Hockenmaier, 2006), Italian (Bos et al., 2009), and Turkish (C¸akıcı, 2005). Their treebanks are annotated with dependencies of words, the conversion of which into phrase structures is not a big concern. A notable contribution of the present work is a method for inducing CCG grammars from chunk-based dependency structures, which is not obvious, as we discuss later in this paper. CCG parsing provides not only predicate argument relations but also CCG derivations, which can be used for various semantic processing tasks (Bos et al., 2004; Bos, 2007). Our work constitutes a starting point for such deep linguistic processing for languages like</context>
<context position="15539" citStr="Bos et al. (2009)" startWordPosition="2464" endWordPosition="2467">t the dependency structures of the Kyoto corpus into phrase structures and then augment them with syntactic/semantic roles from the other two corpora. The conversion involves two steps: 1) recognizing the chunk-internal structures, and (2) converting inter-chunk dependencies into phrase structures. For 1), we don’t have any explicit information in the Kyoto corpus although, in principle, each chunk has internal structures (Vadas and Curran, 2007; Yamada et al., 2010). The lack of a chunk-internal structure makes the dependencyto-constituency conversion more complex than a similar procedure by Bos et al. (2009) that converts an Italian dependency treebank into constituency trees since their dependency trees are annotated down to the level of each word. For the current implementation, we abandon the idea of identifying exact structures and instead basically rely on the following generic rules (Fig. 6): Nominal chunks Compound nouns are first formed as a right-branching phrase and post-positions are then attached to it. Verbal chunks Verbal chunks are analyzed as left-branching structures. The rules amount to assume that all but the last word in a compound noun modify the head noun (i.e., the last wor</context>
</contexts>
<marker>Bos, Bosco, Mazzei, 2009</marker>
<rawString>Johan Bos, Cristina Bosco, and Alessandro Mazzei. 2009. Converting a dependency treebank to a categorial grammar treebank for Italian. In Proceedings of the Eighth International Workshop on Treebanks and Linguistic Theories (TLT8), pages 27–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bos</author>
</authors>
<title>Recognising textual entailment and computational semantics.</title>
<date>2007</date>
<booktitle>In Proceedings of Seventh International Workshop on Computational Semantics IWCS-7,</booktitle>
<pages>1</pages>
<contexts>
<context position="1715" citStr="Bos, 2007" startWordPosition="241" endWordPosition="242"> is applied and then semantic role labeling is performed on the dependencies (Sasano and Kurohashi, 2011; Kawahara and Kurohashi, 2011; Kudo and Matsumoto, 2002; Iida and Poesio, 2011; Hayashibe et al., 2011). This dominance is mainly because chunkbased dependency analysis looks most appropriate for Japanese syntax due to its morphosyntactic typology, which includes agglutination and scrambling (Bekki, 2010). However, it is also true that this type of analysis has prevented us from deeper syntactic analysis such as deep parsing (Clark and Curran, 2007) and logical inference (Bos et al., 2004; Bos, 2007), both of which have been surpassing shallow parsing-based approaches in languages like English. In this paper, we present our work on inducing wide-coverage Japanese resources based on combinatory categorial grammar (CCG) (Steedman, 2001). Our work is basically an extension of a seminal work on CCGbank (Hockenmaier and Steedman, 2007), in which the phrase structure trees of the Penn Treebank (PTB) (Marcus et al., 1993) are converted into CCG derivations and a wide-coverage CCG lexicon is then extracted from these derivations. As CCGbank has enabled a variety of outstanding works on wide-cover</context>
<context position="13505" citStr="Bos, 2007" startWordPosition="2138" endWordPosition="2139">, 2007). The same methodology has been applied to German (Hockenmaier, 2006), Italian (Bos et al., 2009), and Turkish (C¸akıcı, 2005). Their treebanks are annotated with dependencies of words, the conversion of which into phrase structures is not a big concern. A notable contribution of the present work is a method for inducing CCG grammars from chunk-based dependency structures, which is not obvious, as we discuss later in this paper. CCG parsing provides not only predicate argument relations but also CCG derivations, which can be used for various semantic processing tasks (Bos et al., 2004; Bos, 2007). Our work constitutes a starting point for such deep linguistic processing for languages like Japanese. 3 Corpus integration and conversion For wide-coverage CCG parsing, we need a) a wide-coverage CCG lexicon, b) combinatory rules, c) training data for parse disambiguation, and d) a parser (e.g., a CKY parser). Since d) is grammar- and language-independent, all we have to develop for a new language is a)–c). As we have adopted the method of CCGbank, which relies on a source treebank to be converted into CCG derivations, a critical issue to address is the absence of a Japanese counterpart to </context>
</contexts>
<marker>Bos, 2007</marker>
<rawString>Johan Bos. 2007. Recognising textual entailment and computational semantics. In Proceedings of Seventh International Workshop on Computational Semantics IWCS-7, page 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruken C¸akıcı</author>
</authors>
<title>Automatic induction of a CCG grammar for Turkish.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL Student Research Workshop,</booktitle>
<pages>73--78</pages>
<marker>C¸akıcı, 2005</marker>
<rawString>Ruken C¸akıcı. 2005. Automatic induction of a CCG grammar for Turkish. In Proceedings of ACL Student Research Workshop, pages 73–78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Widecoverage efficient statistical parsing with CCG and log-linear models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>4</issue>
<contexts>
<context position="1663" citStr="Clark and Curran, 2007" startWordPosition="230" endWordPosition="233">dependency-based pipeline in which chunk-based dependency parsing is applied and then semantic role labeling is performed on the dependencies (Sasano and Kurohashi, 2011; Kawahara and Kurohashi, 2011; Kudo and Matsumoto, 2002; Iida and Poesio, 2011; Hayashibe et al., 2011). This dominance is mainly because chunkbased dependency analysis looks most appropriate for Japanese syntax due to its morphosyntactic typology, which includes agglutination and scrambling (Bekki, 2010). However, it is also true that this type of analysis has prevented us from deeper syntactic analysis such as deep parsing (Clark and Curran, 2007) and logical inference (Bos et al., 2004; Bos, 2007), both of which have been surpassing shallow parsing-based approaches in languages like English. In this paper, we present our work on inducing wide-coverage Japanese resources based on combinatory categorial grammar (CCG) (Steedman, 2001). Our work is basically an extension of a seminal work on CCGbank (Hockenmaier and Steedman, 2007), in which the phrase structure trees of the Penn Treebank (PTB) (Marcus et al., 1993) are converted into CCG derivations and a wide-coverage CCG lexicon is then extracted from these derivations. As CCGbank has </context>
<context position="12902" citStr="Clark and Curran, 2007" startWordPosition="2036" endWordPosition="2039"> extracting a wide-coverage lexicon based on HPSG from a phrase structure treebank of Japanese. We largely extended their work by exploiting the standard chunk-based Japanese corpora and demonstrated the first results for Japanese deep parsing with grammar induced from large corpora. Corpus-based acquisition of wide-coverage CCG resources has enjoyed great success for English (Hockenmaier and Steedman, 2007). In that method, PTB was converted into CCG-based derivations from which a wide-coverage CCG lexicon was extracted. CCGbank has been used for the development of wide-coverage CCG parsers (Clark and Curran, 2007). The same methodology has been applied to German (Hockenmaier, 2006), Italian (Bos et al., 2009), and Turkish (C¸akıcı, 2005). Their treebanks are annotated with dependencies of words, the conversion of which into phrase structures is not a big concern. A notable contribution of the present work is a method for inducing CCG grammars from chunk-based dependency structures, which is not obvious, as we discuss later in this paper. CCG parsing provides not only predicate argument relations but also CCG derivations, which can be used for various semantic processing tasks (Bos et al., 2004; Bos, 20</context>
<context position="32467" citStr="Clark and Curran, 2007" startWordPosition="5349" endWordPosition="5352">.jp/resources/tocorpus/ tocorpusabstract.html 1049 of the grammar on unseen texts. First, we obtained CCG derivations for evaluation sets by applying our conversion method and then used these derivations as gold standard. Lexical coverage indicates the number of words to which the grammar assigns a gold standard category. Sentential coverage indicates the number of sentences in which all words are assigned gold standard categories 5. Table 6 shows the evaluation results. Lexical coverage was 99.40% with rare word treatment, which is in the same level as the case of the English CCG parser C&amp;C (Clark and Curran, 2007). We also measured coverage in a “weak” sense, which means the number of sentences that are given at least one analysis (not necessarily correct) by the obtained grammar. This number was 99.12 % and 99.06 % for the development and the test set, respectively, which is sufficiently high for wide-coverage parsing of real-world texts. 4.3 Evaluation of parsing accuracy Finally, we evaluated the parsing accuracy. We employed the parser and the supertagger of (Miyao and Tsujii, 2008), specifically, its generalized modules for lexicalized grammars. We trained log-linear models in the same way as (Cla</context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>Stephen Clark and James R. Curran. 2007. Widecoverage efficient statistical parsing with CCG and log-linear models. Computational Linguistics, 33(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takao Gunji</author>
</authors>
<title>Japanese Phrase Structure Grammar: A Unification-based Approach.</title>
<date>1987</date>
<tech>D. Reidel.</tech>
<contexts>
<context position="11732" citStr="Gunji (1987)" startWordPosition="1852" endWordPosition="1853"> only focuses on three cases: “ga” (subject), “o” (direct object), and “ni” (indirect object) (Fig. 5). Japanese particle corpus (JP) (Hanaoka et al., 2010) A corpus annotated with distinct grammatical functions of the Japanese particle (postposition) “to”. In Japanese, “to” has many functions, including a complementizer (similar to “that”), a subordinate conjunction (similar to “then”), a coordination conjunction (similar to “and”), and a case marker (similar to “with”). 2.4 Related work Research on Japanese deep parsing is fairly limited. Formal theories of Japanese syntax were presented by Gunji (1987) based on Head-driven Phrase Structure Grammar (HPSG) (Sag et al., 2003) and by Komagata (1999) based on CCG, although their implementations in real-world parsing have not been very successful. JACY (Siegel &apos;In fact, the NAIST text corpus includes additional texts, but in this work we only use the news text section. 1044 and Bender, 2002) is a large-scale Japanese grammar based on HPSG, but its semantics is tightly embedded in the grammar and it is not as easy to systematically switch them as it is in CCG. Yoshida (2005) proposed methods for extracting a wide-coverage lexicon based on HPSG fro</context>
</contexts>
<marker>Gunji, 1987</marker>
<rawString>Takao Gunji. 1987. Japanese Phrase Structure Grammar: A Unification-based Approach. D. Reidel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroki Hanaoka</author>
<author>Hideki Mima</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>A Japanese particle corpus built by examplebased annotation.</title>
<date>2010</date>
<booktitle>In Proceedings of LREC</booktitle>
<contexts>
<context position="11276" citStr="Hanaoka et al., 2010" startWordPosition="1781" endWordPosition="1784">n.”. Accusatives are labeled as ARG-ga in causative (see Sec. 3.2). CAUSE PAST aries, and dependency relations among chunks &lt;B i St B (Fig. 5). The dependencies are classified into four Sbse\Pni types: Para (coordination), A (apposition), I (argument cluster), and Dep (default). Most of the dependencies are annotated as Dep. NAIST text corpus A corpus annotated with anaphora and coreference relations. The same set as the Kyoto corpus is annotated.&apos; The corpus 1 only focuses on three cases: “ga” (subject), “o” (direct object), and “ni” (indirect object) (Fig. 5). Japanese particle corpus (JP) (Hanaoka et al., 2010) A corpus annotated with distinct grammatical functions of the Japanese particle (postposition) “to”. In Japanese, “to” has many functions, including a complementizer (similar to “that”), a subordinate conjunction (similar to “then”), a coordination conjunction (similar to “and”), and a case marker (similar to “with”). 2.4 Related work Research on Japanese deep parsing is fairly limited. Formal theories of Japanese syntax were presented by Gunji (1987) based on Head-driven Phrase Structure Grammar (HPSG) (Sag et al., 2003) and by Komagata (1999) based on CCG, although their implementations in </context>
</contexts>
<marker>Hanaoka, Mima, Tsujii, 2010</marker>
<rawString>Hiroki Hanaoka, Hideki Mima, and Jun’ichi Tsujii. 2010. A Japanese particle corpus built by examplebased annotation. In Proceedings of LREC 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuta Hayashibe</author>
<author>Mamoru Komachi</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Japanese predicate argument structure analysis exploiting argument position and type.</title>
<date>2011</date>
<booktitle>In Proceedings of IJCNLP 2011,</booktitle>
<pages>201--209</pages>
<contexts>
<context position="1313" citStr="Hayashibe et al., 2011" startWordPosition="176" endWordPosition="179">ces are dependency-based. Our method first integrates multiple dependency-based corpora into phrase structure trees and then converts the trees into CCG derivations. The method is empirically evaluated in terms of the coverage of the obtained lexicon and the accuracy of parsing. 1 Introduction Syntactic parsing for Japanese has been dominated by a dependency-based pipeline in which chunk-based dependency parsing is applied and then semantic role labeling is performed on the dependencies (Sasano and Kurohashi, 2011; Kawahara and Kurohashi, 2011; Kudo and Matsumoto, 2002; Iida and Poesio, 2011; Hayashibe et al., 2011). This dominance is mainly because chunkbased dependency analysis looks most appropriate for Japanese syntax due to its morphosyntactic typology, which includes agglutination and scrambling (Bekki, 2010). However, it is also true that this type of analysis has prevented us from deeper syntactic analysis such as deep parsing (Clark and Curran, 2007) and logical inference (Bos et al., 2004; Bos, 2007), both of which have been surpassing shallow parsing-based approaches in languages like English. In this paper, we present our work on inducing wide-coverage Japanese resources based on combinatory </context>
</contexts>
<marker>Hayashibe, Komachi, Matsumoto, 2011</marker>
<rawString>Yuta Hayashibe, Mamoru Komachi, and Yuji Matsumoto. 2011. Japanese predicate argument structure analysis exploiting argument position and type. In Proceedings of IJCNLP 2011, pages 201–209.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>CCGbank: A corpus of CCG derivations and dependency structures extracted from the Penn Treebank.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>3</issue>
<contexts>
<context position="2052" citStr="Hockenmaier and Steedman, 2007" startWordPosition="292" endWordPosition="295"> due to its morphosyntactic typology, which includes agglutination and scrambling (Bekki, 2010). However, it is also true that this type of analysis has prevented us from deeper syntactic analysis such as deep parsing (Clark and Curran, 2007) and logical inference (Bos et al., 2004; Bos, 2007), both of which have been surpassing shallow parsing-based approaches in languages like English. In this paper, we present our work on inducing wide-coverage Japanese resources based on combinatory categorial grammar (CCG) (Steedman, 2001). Our work is basically an extension of a seminal work on CCGbank (Hockenmaier and Steedman, 2007), in which the phrase structure trees of the Penn Treebank (PTB) (Marcus et al., 1993) are converted into CCG derivations and a wide-coverage CCG lexicon is then extracted from these derivations. As CCGbank has enabled a variety of outstanding works on wide-coverage deep parsing for English, our resources are expected to significantly contribute to Japanese deep parsing. The application of the CCGbank method to Japanese is not trivial, as resources like PTB are not available in Japanese. The widely used resources for parsing research are the Kyoto corpus (Kawahara et al., 2002) and the NAIST t</context>
<context position="12690" citStr="Hockenmaier and Steedman, 2007" startWordPosition="2003" endWordPosition="2006">Bender, 2002) is a large-scale Japanese grammar based on HPSG, but its semantics is tightly embedded in the grammar and it is not as easy to systematically switch them as it is in CCG. Yoshida (2005) proposed methods for extracting a wide-coverage lexicon based on HPSG from a phrase structure treebank of Japanese. We largely extended their work by exploiting the standard chunk-based Japanese corpora and demonstrated the first results for Japanese deep parsing with grammar induced from large corpora. Corpus-based acquisition of wide-coverage CCG resources has enjoyed great success for English (Hockenmaier and Steedman, 2007). In that method, PTB was converted into CCG-based derivations from which a wide-coverage CCG lexicon was extracted. CCGbank has been used for the development of wide-coverage CCG parsers (Clark and Curran, 2007). The same methodology has been applied to German (Hockenmaier, 2006), Italian (Bos et al., 2009), and Turkish (C¸akıcı, 2005). Their treebanks are annotated with dependencies of words, the conversion of which into phrase structures is not a big concern. A notable contribution of the present work is a method for inducing CCG grammars from chunk-based dependency structures, which is not</context>
<context position="31239" citStr="Hockenmaier and Steedman, 2007" startWordPosition="5166" endWordPosition="5169">ils of these resources are shown in Table 4. 4.1 Corpus conversion and lexicon extraction Table 5 shows the number of successful conversions performed by our method. In total, we obtained 22,820 CCG derivations from 24,283 sentences (in the training set), resulting in the total conversion rate of 93.98%. The table shows we lost more sentences in Step 2 than in Step 1. This is natural because Step 2 imposed more restrictions on resulting structures and therefore detected more discrepancies including compounding errors. Our conversion rate is about 5.5 points lower than the English counterpart (Hockenmaier and Steedman, 2007). Manual investigation of the sampled derivations would be beneficial for the conversion improvement. For the lexicon extraction from the CCGbank, we obtained 699 types of lexical categories from 616,305 word tokens. After lexical reduction, the number of categories decreased to 454, which in turn may produce 5,342 categories by lexical expansion. The average number of categories for a word type was 11.68 as a result. 4.2 Evaluation of coverage Following the evaluation criteria in (Hockenmaier and Steedman, 2007), we measured the coverage 3http://cl.naist.jp/nldata/corpus/ 4https://alaginrc.ni</context>
</contexts>
<marker>Hockenmaier, Steedman, 2007</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2007. CCGbank: A corpus of CCG derivations and dependency structures extracted from the Penn Treebank. Computational Linguistics, 33(3):355–396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
</authors>
<title>Creating a CCGbank and a wide-coverage CCG lexicon for German.</title>
<date>2006</date>
<booktitle>In Proceedings of the Joint Conference of COLING/ACL</booktitle>
<contexts>
<context position="12971" citStr="Hockenmaier, 2006" startWordPosition="2048" endWordPosition="2049">treebank of Japanese. We largely extended their work by exploiting the standard chunk-based Japanese corpora and demonstrated the first results for Japanese deep parsing with grammar induced from large corpora. Corpus-based acquisition of wide-coverage CCG resources has enjoyed great success for English (Hockenmaier and Steedman, 2007). In that method, PTB was converted into CCG-based derivations from which a wide-coverage CCG lexicon was extracted. CCGbank has been used for the development of wide-coverage CCG parsers (Clark and Curran, 2007). The same methodology has been applied to German (Hockenmaier, 2006), Italian (Bos et al., 2009), and Turkish (C¸akıcı, 2005). Their treebanks are annotated with dependencies of words, the conversion of which into phrase structures is not a big concern. A notable contribution of the present work is a method for inducing CCG grammars from chunk-based dependency structures, which is not obvious, as we discuss later in this paper. CCG parsing provides not only predicate argument relations but also CCG derivations, which can be used for various semantic processing tasks (Bos et al., 2004; Bos, 2007). Our work constitutes a starting point for such deep linguistic p</context>
</contexts>
<marker>Hockenmaier, 2006</marker>
<rawString>Julia Hockenmaier. 2006. Creating a CCGbank and a wide-coverage CCG lexicon for German. In Proceedings of the Joint Conference of COLING/ACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryu Iida</author>
<author>Massimo Poesio</author>
</authors>
<title>A cross-lingual ILP solution to zero anaphora resolution.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL-HLT 2011,</booktitle>
<pages>804--813</pages>
<contexts>
<context position="1288" citStr="Iida and Poesio, 2011" startWordPosition="172" endWordPosition="175">panese syntactic resources are dependency-based. Our method first integrates multiple dependency-based corpora into phrase structure trees and then converts the trees into CCG derivations. The method is empirically evaluated in terms of the coverage of the obtained lexicon and the accuracy of parsing. 1 Introduction Syntactic parsing for Japanese has been dominated by a dependency-based pipeline in which chunk-based dependency parsing is applied and then semantic role labeling is performed on the dependencies (Sasano and Kurohashi, 2011; Kawahara and Kurohashi, 2011; Kudo and Matsumoto, 2002; Iida and Poesio, 2011; Hayashibe et al., 2011). This dominance is mainly because chunkbased dependency analysis looks most appropriate for Japanese syntax due to its morphosyntactic typology, which includes agglutination and scrambling (Bekki, 2010). However, it is also true that this type of analysis has prevented us from deeper syntactic analysis such as deep parsing (Clark and Curran, 2007) and logical inference (Bos et al., 2004; Bos, 2007), both of which have been surpassing shallow parsing-based approaches in languages like English. In this paper, we present our work on inducing wide-coverage Japanese resour</context>
</contexts>
<marker>Iida, Poesio, 2011</marker>
<rawString>Ryu Iida and Massimo Poesio. 2011. A cross-lingual ILP solution to zero anaphora resolution. In Proceedings of ACL-HLT 2011, pages 804–813.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryu Iida</author>
<author>Mamoru Komachi</author>
<author>Kentaro Inui</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Annotating a Japanese text corpus with predicate-argument and coreference relations.</title>
<date>2007</date>
<booktitle>In Proceedings of Linguistic Annotation Workshop,</booktitle>
<pages>132--139</pages>
<contexts>
<context position="2682" citStr="Iida et al., 2007" startWordPosition="395" endWordPosition="398">he phrase structure trees of the Penn Treebank (PTB) (Marcus et al., 1993) are converted into CCG derivations and a wide-coverage CCG lexicon is then extracted from these derivations. As CCGbank has enabled a variety of outstanding works on wide-coverage deep parsing for English, our resources are expected to significantly contribute to Japanese deep parsing. The application of the CCGbank method to Japanese is not trivial, as resources like PTB are not available in Japanese. The widely used resources for parsing research are the Kyoto corpus (Kawahara et al., 2002) and the NAIST text corpus (Iida et al., 2007), both of which are based on the dependency structures of chunks. Moreover, the relation between chunk-based dependency structures and CCG derivations is not obvious. In this work, we propose a method to integrate multiple dependency-based corpora into phrase structure trees augmented with predicate argument relations. We can then convert the phrase structure trees into CCG derivations. In the following, we describe the details of the integration method as well as Japanese-specific issues in the conversion into CCG derivations. The method is empirically evaluated in terms of the quality of the</context>
</contexts>
<marker>Iida, Komachi, Inui, Matsumoto, 2007</marker>
<rawString>Ryu Iida, Mamoru Komachi, Kentaro Inui, and Yuji Matsumoto. 2007. Annotating a Japanese text corpus with predicate-argument and coreference relations. In Proceedings of Linguistic Annotation Workshop, pages 132–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Generative modeling of coordination by factoring parallelism and selectional preferences.</title>
<date>2011</date>
<booktitle>In Proceedings of IJCNLP</booktitle>
<contexts>
<context position="1239" citStr="Kawahara and Kurohashi, 2011" startWordPosition="163" endWordPosition="167">ese have not been widely studied, mainly because most Japanese syntactic resources are dependency-based. Our method first integrates multiple dependency-based corpora into phrase structure trees and then converts the trees into CCG derivations. The method is empirically evaluated in terms of the coverage of the obtained lexicon and the accuracy of parsing. 1 Introduction Syntactic parsing for Japanese has been dominated by a dependency-based pipeline in which chunk-based dependency parsing is applied and then semantic role labeling is performed on the dependencies (Sasano and Kurohashi, 2011; Kawahara and Kurohashi, 2011; Kudo and Matsumoto, 2002; Iida and Poesio, 2011; Hayashibe et al., 2011). This dominance is mainly because chunkbased dependency analysis looks most appropriate for Japanese syntax due to its morphosyntactic typology, which includes agglutination and scrambling (Bekki, 2010). However, it is also true that this type of analysis has prevented us from deeper syntactic analysis such as deep parsing (Clark and Curran, 2007) and logical inference (Bos et al., 2004; Bos, 2007), both of which have been surpassing shallow parsing-based approaches in languages like English. In this paper, we present o</context>
</contexts>
<marker>Kawahara, Kurohashi, 2011</marker>
<rawString>Daisuke Kawahara and Sadao Kurohashi. 2011. Generative modeling of coordination by factoring parallelism and selectional preferences. In Proceedings of IJCNLP 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
<author>Koiti Hasida</author>
</authors>
<title>Construction of a Japanese relevance-tagged corpus.</title>
<date>2002</date>
<booktitle>In Proceedings of the 8th Annual Meeting of the Association for Natural Language Processing,</booktitle>
<pages>495--498</pages>
<note>(In Japanese).</note>
<contexts>
<context position="2636" citStr="Kawahara et al., 2002" startWordPosition="386" endWordPosition="389">Gbank (Hockenmaier and Steedman, 2007), in which the phrase structure trees of the Penn Treebank (PTB) (Marcus et al., 1993) are converted into CCG derivations and a wide-coverage CCG lexicon is then extracted from these derivations. As CCGbank has enabled a variety of outstanding works on wide-coverage deep parsing for English, our resources are expected to significantly contribute to Japanese deep parsing. The application of the CCGbank method to Japanese is not trivial, as resources like PTB are not available in Japanese. The widely used resources for parsing research are the Kyoto corpus (Kawahara et al., 2002) and the NAIST text corpus (Iida et al., 2007), both of which are based on the dependency structures of chunks. Moreover, the relation between chunk-based dependency structures and CCG derivations is not obvious. In this work, we propose a method to integrate multiple dependency-based corpora into phrase structure trees augmented with predicate argument relations. We can then convert the phrase structure trees into CCG derivations. In the following, we describe the details of the integration method as well as Japanese-specific issues in the conversion into CCG derivations. The method is empiri</context>
</contexts>
<marker>Kawahara, Kurohashi, Hasida, 2002</marker>
<rawString>Daisuke Kawahara, Sadao Kurohashi, and Koiti Hasida. 2002. Construction of a Japanese relevance-tagged corpus. In Proceedings of the 8th Annual Meeting of the Association for Natural Language Processing, pages 495–498. (In Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nobo Komagata</author>
</authors>
<date>1999</date>
<booktitle>Information Structure in Texts: A Computational Analysis of Contextual Appropriateness in English and Japanese. Ph.D. thesis,</booktitle>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="11827" citStr="Komagata (1999)" startWordPosition="1867" endWordPosition="1868">) (Fig. 5). Japanese particle corpus (JP) (Hanaoka et al., 2010) A corpus annotated with distinct grammatical functions of the Japanese particle (postposition) “to”. In Japanese, “to” has many functions, including a complementizer (similar to “that”), a subordinate conjunction (similar to “then”), a coordination conjunction (similar to “and”), and a case marker (similar to “with”). 2.4 Related work Research on Japanese deep parsing is fairly limited. Formal theories of Japanese syntax were presented by Gunji (1987) based on Head-driven Phrase Structure Grammar (HPSG) (Sag et al., 2003) and by Komagata (1999) based on CCG, although their implementations in real-world parsing have not been very successful. JACY (Siegel &apos;In fact, the NAIST text corpus includes additional texts, but in this work we only use the news text section. 1044 and Bender, 2002) is a large-scale Japanese grammar based on HPSG, but its semantics is tightly embedded in the grammar and it is not as easy to systematically switch them as it is in CCG. Yoshida (2005) proposed methods for extracting a wide-coverage lexicon based on HPSG from a phrase structure treebank of Japanese. We largely extended their work by exploiting the sta</context>
</contexts>
<marker>Komagata, 1999</marker>
<rawString>Nobo Komagata. 1999. Information Structure in Texts: A Computational Analysis of Contextual Appropriateness in English and Japanese. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Japanese dependency analyisis using cascaded chunking.</title>
<date>2002</date>
<booktitle>In Proceedings of CoNLL</booktitle>
<contexts>
<context position="1265" citStr="Kudo and Matsumoto, 2002" startWordPosition="168" endWordPosition="171">ed, mainly because most Japanese syntactic resources are dependency-based. Our method first integrates multiple dependency-based corpora into phrase structure trees and then converts the trees into CCG derivations. The method is empirically evaluated in terms of the coverage of the obtained lexicon and the accuracy of parsing. 1 Introduction Syntactic parsing for Japanese has been dominated by a dependency-based pipeline in which chunk-based dependency parsing is applied and then semantic role labeling is performed on the dependencies (Sasano and Kurohashi, 2011; Kawahara and Kurohashi, 2011; Kudo and Matsumoto, 2002; Iida and Poesio, 2011; Hayashibe et al., 2011). This dominance is mainly because chunkbased dependency analysis looks most appropriate for Japanese syntax due to its morphosyntactic typology, which includes agglutination and scrambling (Bekki, 2010). However, it is also true that this type of analysis has prevented us from deeper syntactic analysis such as deep parsing (Clark and Curran, 2007) and logical inference (Bos et al., 2004; Bos, 2007), both of which have been surpassing shallow parsing-based approaches in languages like English. In this paper, we present our work on inducing wide-c</context>
<context position="30598" citStr="Kudo and Matsumoto, 2002" startWordPosition="5060" endWordPosition="5063">84 9,245 Converted 24,116 22,820 4,803 4,559 9,245 8,769 Con. rate 99.3 94.6 99.4 94.9 99.6 94.9 Table 5: Statistics of corpus conversion. Sentential Coverage Covered Uncovered Cov. (%) Devel. 3,920 639 85.99 Test 7,610 1,159 86.78 Lexical Coverage Word Known Unknown combi. cat. word Devel. 127,144 126,383 682 79 0 Test 238,083 236,651 1,242 145 0 Table 6: Sentential and lexical coverage. corpus ver. 1.53, and JP corpus ver. 1.04. The integrated corpus is divided into training, development, and final test sets following the standard data split in previous works on Japanese dependency parsing (Kudo and Matsumoto, 2002). The details of these resources are shown in Table 4. 4.1 Corpus conversion and lexicon extraction Table 5 shows the number of successful conversions performed by our method. In total, we obtained 22,820 CCG derivations from 24,283 sentences (in the training set), resulting in the total conversion rate of 93.98%. The table shows we lost more sentences in Step 2 than in Step 1. This is natural because Step 2 imposed more restrictions on resulting structures and therefore detected more discrepancies including compounding errors. Our conversion rate is about 5.5 points lower than the English cou</context>
</contexts>
<marker>Kudo, Matsumoto, 2002</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2002. Japanese dependency analyisis using cascaded chunking. In Proceedings of CoNLL 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="2138" citStr="Marcus et al., 1993" startWordPosition="307" endWordPosition="310">However, it is also true that this type of analysis has prevented us from deeper syntactic analysis such as deep parsing (Clark and Curran, 2007) and logical inference (Bos et al., 2004; Bos, 2007), both of which have been surpassing shallow parsing-based approaches in languages like English. In this paper, we present our work on inducing wide-coverage Japanese resources based on combinatory categorial grammar (CCG) (Steedman, 2001). Our work is basically an extension of a seminal work on CCGbank (Hockenmaier and Steedman, 2007), in which the phrase structure trees of the Penn Treebank (PTB) (Marcus et al., 1993) are converted into CCG derivations and a wide-coverage CCG lexicon is then extracted from these derivations. As CCGbank has enabled a variety of outstanding works on wide-coverage deep parsing for English, our resources are expected to significantly contribute to Japanese deep parsing. The application of the CCGbank method to Japanese is not trivial, as resources like PTB are not available in Japanese. The widely used resources for parsing research are the Kyoto corpus (Kawahara et al., 2002) and the NAIST text corpus (Iida et al., 2007), both of which are based on the dependency structures o</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Feature forest models for probabilistic HPSG parsing.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="32949" citStr="Miyao and Tsujii, 2008" startWordPosition="5427" endWordPosition="5430">l coverage was 99.40% with rare word treatment, which is in the same level as the case of the English CCG parser C&amp;C (Clark and Curran, 2007). We also measured coverage in a “weak” sense, which means the number of sentences that are given at least one analysis (not necessarily correct) by the obtained grammar. This number was 99.12 % and 99.06 % for the development and the test set, respectively, which is sufficiently high for wide-coverage parsing of real-world texts. 4.3 Evaluation of parsing accuracy Finally, we evaluated the parsing accuracy. We employed the parser and the supertagger of (Miyao and Tsujii, 2008), specifically, its generalized modules for lexicalized grammars. We trained log-linear models in the same way as (Clark and Curran, 2007) using the training set as training data. Feature sets were simply borrowed from an English parser; no tuning was performed. Following conventions in research on Japanese dependency parsing, gold morphological analysis results were input to a parser. Following C&amp;C, the evaluation measure was precision and recall over dependencies, where a dependency is defined as a 4-tuple: a head of a functor, a functor category, an argument slot, and a head of an argument.</context>
</contexts>
<marker>Miyao, Tsujii, 2008</marker>
<rawString>Yusuke Miyao and Jun’ichi Tsujii. 2008. Feature forest models for probabilistic HPSG parsing. Computational Linguistics, 34(1):35–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The Proposition Bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="20754" citStr="Palmer et al., 2005" startWordPosition="3352" endWordPosition="3355">nal PP Noun It ACC NP PostPcm PP ARG-to PP ARG-ga VP NP VP ARG-ni VP Noun 犬 dog NAIST PostP に DAT Verb 行け go! Verb Aux Adj Noun PostP と CMP 言っ say た PAST 白い white ARG-ga 猫 cat ARG-ga ARG-ni ARG-ga JP ARG-CLS 1046 S Figure 9: A phrase structure into a CCG derivation. In the figure, the annotation given in the two corpora is shown inside the dotted box at the bottom. We converted the predicate-argument annotations given as labeled word-to-word dependencies into the relations between the predicate words and their argument phrases. The results are thus similar to the annotation style of PropBank (Palmer et al., 2005). In the NAIST corpus, each pred-arg relation is labeled with the argument-type (ga/o/ni) and a flag indicating that the relation is mediated by either a syntactic dependency or a zero anaphora. For a relation of a predicate wp and its argument wa in the NAIST corpus, the boundary of the argument phrase is determined as follows: 1. If wa precedes wp and the relation is mediated by a syntactic dep., select the maximum PP that is formed by attaching one or more postpositions to the NP headed by wa. 2. If wp precedes wa or the relation is mediated by a zero anaphora, select the maximum NP headed </context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The Proposition Bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan A Sag</author>
<author>Thomas Wasow</author>
<author>Emily M Bender</author>
</authors>
<title>Syntactic Theory: A Formal Introduction, 2nd Edition.</title>
<date>2003</date>
<publisher>CSLI Publications.</publisher>
<contexts>
<context position="11804" citStr="Sag et al., 2003" startWordPosition="1861" endWordPosition="1864">and “ni” (indirect object) (Fig. 5). Japanese particle corpus (JP) (Hanaoka et al., 2010) A corpus annotated with distinct grammatical functions of the Japanese particle (postposition) “to”. In Japanese, “to” has many functions, including a complementizer (similar to “that”), a subordinate conjunction (similar to “then”), a coordination conjunction (similar to “and”), and a case marker (similar to “with”). 2.4 Related work Research on Japanese deep parsing is fairly limited. Formal theories of Japanese syntax were presented by Gunji (1987) based on Head-driven Phrase Structure Grammar (HPSG) (Sag et al., 2003) and by Komagata (1999) based on CCG, although their implementations in real-world parsing have not been very successful. JACY (Siegel &apos;In fact, the NAIST text corpus includes additional texts, but in this work we only use the news text section. 1044 and Bender, 2002) is a large-scale Japanese grammar based on HPSG, but its semantics is tightly embedded in the grammar and it is not as easy to systematically switch them as it is in CCG. Yoshida (2005) proposed methods for extracting a wide-coverage lexicon based on HPSG from a phrase structure treebank of Japanese. We largely extended their wor</context>
</contexts>
<marker>Sag, Wasow, Bender, 2003</marker>
<rawString>Ivan A. Sag, Thomas Wasow, and Emily M. Bender. 2003. Syntactic Theory: A Formal Introduction, 2nd Edition. CSLI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryohei Sasano</author>
<author>Sadao Kurohashi</author>
</authors>
<title>A discriminative approach to Japanese zero anaphora resolution with large-scale lexicalized case frames.</title>
<date>2011</date>
<booktitle>In Proceedings of IJCNLP</booktitle>
<contexts>
<context position="1209" citStr="Sasano and Kurohashi, 2011" startWordPosition="159" endWordPosition="162">e languages, those for Japanese have not been widely studied, mainly because most Japanese syntactic resources are dependency-based. Our method first integrates multiple dependency-based corpora into phrase structure trees and then converts the trees into CCG derivations. The method is empirically evaluated in terms of the coverage of the obtained lexicon and the accuracy of parsing. 1 Introduction Syntactic parsing for Japanese has been dominated by a dependency-based pipeline in which chunk-based dependency parsing is applied and then semantic role labeling is performed on the dependencies (Sasano and Kurohashi, 2011; Kawahara and Kurohashi, 2011; Kudo and Matsumoto, 2002; Iida and Poesio, 2011; Hayashibe et al., 2011). This dominance is mainly because chunkbased dependency analysis looks most appropriate for Japanese syntax due to its morphosyntactic typology, which includes agglutination and scrambling (Bekki, 2010). However, it is also true that this type of analysis has prevented us from deeper syntactic analysis such as deep parsing (Clark and Curran, 2007) and logical inference (Bos et al., 2004; Bos, 2007), both of which have been surpassing shallow parsing-based approaches in languages like Englis</context>
</contexts>
<marker>Sasano, Kurohashi, 2011</marker>
<rawString>Ryohei Sasano and Sadao Kurohashi. 2011. A discriminative approach to Japanese zero anaphora resolution with large-scale lexicalized case frames. In Proceedings of IJCNLP 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manabu Sassano</author>
<author>Sadao Kurohashi</author>
</authors>
<title>A unified single scan algorithm for Japanese base phrase chunking and dependency parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-IJCNLP</booktitle>
<contexts>
<context position="34061" citStr="Sassano and Kurohashi (2009)" startWordPosition="5608" endWordPosition="5611">dency is defined as a 4-tuple: a head of a functor, a functor category, an argument slot, and a head of an argument. Table 7 shows the parsing accuracy on the development and the test sets. The supertagging accuracy is presented in the upper table. While our coverage was almost the same as C&amp;C, the performance of our supertagger and parser was lower. To improve the performance, tuning disambiguation models for Japanese is a possible approach. Comparing the parser’s performance with previous works on Japanese dependency parsing is difficult as our figures are not directly comparable to theirs. Sassano and Kurohashi (2009) reported the accuracy of their parser as 88.48 and 95.09 5Since a gold derivation can logically be obtained if gold categories are assigned to all words in a sentence, sentential coverage means that the obtained lexicon has the ability to produce exactly correct derivations for those sentences. Supertagging accuracy Lex. Cov. Cat. Acc. Devel. 99.40 90.86 Test 99.40 90.69 C&amp;C 99.63 94.32 Overall performance LP LR LF UP UR UF Devel. 82.55 82.73 82.64 90.02 90.22 90.12 Test 82.40 82.59 82.50 89.95 90.15 90.05 C&amp;C 88.34 86.96 87.64 93.74 92.28 93.00 Table 7: Parsing accuracy. LP, LR and LF refer </context>
</contexts>
<marker>Sassano, Kurohashi, 2009</marker>
<rawString>Manabu Sassano and Sadao Kurohashi. 2009. A unified single scan algorithm for Japanese base phrase chunking and dependency parsing. In Proceedings of ACL-IJCNLP 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Melanie Siegel</author>
<author>Emily M Bender</author>
</authors>
<title>Efficient deep processing of Japanese.</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd Workshop on Asian Language Resources and International Standardization.</booktitle>
<marker>Siegel, Bender, 2002</marker>
<rawString>Melanie Siegel and Emily M. Bender. 2002. Efficient deep processing of Japanese. In Proceedings of the 3rd Workshop on Asian Language Resources and International Standardization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2001</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="1954" citStr="Steedman, 2001" startWordPosition="277" endWordPosition="279"> because chunkbased dependency analysis looks most appropriate for Japanese syntax due to its morphosyntactic typology, which includes agglutination and scrambling (Bekki, 2010). However, it is also true that this type of analysis has prevented us from deeper syntactic analysis such as deep parsing (Clark and Curran, 2007) and logical inference (Bos et al., 2004; Bos, 2007), both of which have been surpassing shallow parsing-based approaches in languages like English. In this paper, we present our work on inducing wide-coverage Japanese resources based on combinatory categorial grammar (CCG) (Steedman, 2001). Our work is basically an extension of a seminal work on CCGbank (Hockenmaier and Steedman, 2007), in which the phrase structure trees of the Penn Treebank (PTB) (Marcus et al., 1993) are converted into CCG derivations and a wide-coverage CCG lexicon is then extracted from these derivations. As CCGbank has enabled a variety of outstanding works on wide-coverage deep parsing for English, our resources are expected to significantly contribute to Japanese deep parsing. The application of the CCGbank method to Japanese is not trivial, as resources like PTB are not available in Japanese. The widel</context>
<context position="6761" citStr="Steedman (2001)" startWordPosition="1076" endWordPosition="1077">ted with rules for semantic composition. Since these rules are universal, we can obtain different semantic representations by switching the semantic representations of lexical categories. This means that we can plug in a variTable 2: Features for Japanese syntax (those used Scnt\NPga\NPwo &lt; in the examples in this paper). ety of semantic theories with CCG-based syntactic join cause parsing (Bos et al., 2004). Svo s\NPga\NPn Sco\NPga\N St\ 2.2 CCG-based syntactic theory for Japanese ga &lt; Bekki (2010) proposed a comprehensive theory for Japanese syntax based on CCG. While the theory is basedPon Steedman (2001), it provides concrete explanations for a variety of constructions of \S S\S Japanese, such as agglutination, scrambling, longdistance dependencies, etc. (Fig. 3). The ground categories in his theory are S, NP, and CONJ (for conjunctions). Table 1 presents typical lexical categories. While most of them are obvious from the theory of CCG, categories for auxiliary verbs require an explanation. In Japanese, auxiliary verbs are extensively used to express various semantic information, such as tense and modality. They agglutinate to the main verb in a sequential order. This is explained in Bekki’s </context>
</contexts>
<marker>Steedman, 2001</marker>
<rawString>Mark Steedman. 2001. The Syntactic Process. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vadas</author>
<author>James Curran</author>
</authors>
<title>Adding noun phrase structure to the Penn Treebank.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>240--247</pages>
<contexts>
<context position="15371" citStr="Vadas and Curran, 2007" startWordPosition="2438" endWordPosition="2441">se corpora―namely, the Kyoto corpus, the NAIST text corpus, and the JP corpus ―into a phrase structure treebank, which is similar in spirit to PTB. Our approach is to convert the dependency structures of the Kyoto corpus into phrase structures and then augment them with syntactic/semantic roles from the other two corpora. The conversion involves two steps: 1) recognizing the chunk-internal structures, and (2) converting inter-chunk dependencies into phrase structures. For 1), we don’t have any explicit information in the Kyoto corpus although, in principle, each chunk has internal structures (Vadas and Curran, 2007; Yamada et al., 2010). The lack of a chunk-internal structure makes the dependencyto-constituency conversion more complex than a similar procedure by Bos et al. (2009) that converts an Italian dependency treebank into constituency trees since their dependency trees are annotated down to the level of each word. For the current implementation, we abandon the idea of identifying exact structures and instead basically rely on the following generic rules (Fig. 6): Nominal chunks Compound nouns are first formed as a right-branching phrase and post-positions are then attached to it. Verbal chunks Ve</context>
<context position="17747" citStr="Vadas and Curran (2007)" startWordPosition="2833" endWordPosition="2837">parsing with the generic rules and the CFG. For example, two of the rules we came up with are rule A: Number -+ PrefixOfNumber Number rule B: ClassifierPhrase -+ Number Classifier in the precedence: rule A &gt; B &gt; generic rules. Using the above, we bracket a compound noun 約 千 人 死亡 approximately thousand people death PrefixOfNumber Number Classifier CommonNoun “death of approximately one thousand people” as in (((約 千)人) 死亡) (((approximately thousand) people) death) We can improve chunk-internal structures to some extent by refining the CFG rules. A complete solution like the manual annotation by Vadas and Curran (2007) is left for future work. The conversion of inter-chunk dependencies into phrase structures may sound trivial, but it is not necessarily easy when combined with chunkinternal structures. The problem is to which node in the internal structure of the head the dependent dep modifier-type precedence Para から/PostPrm まで/PostPrm, */(Verb|Aux), ... Dep */PostPrm */(Verb|Aux), */Noun,... Dep */PostPadnom */Noun, */(Verb|Aux), ... Table 3: Rules to determine adjoin position. NP Figure 8: Overlay of pred-arg structure annotation (“The white cat who said “Go!” to the dog.”). tree is adjoined (Fig. 7). In </context>
</contexts>
<marker>Vadas, Curran, 2007</marker>
<rawString>David Vadas and James Curran. 2007. Adding noun phrase structure to the Penn Treebank. In Proceedings of ACL 2007, pages 240–247.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emiko Yamada</author>
<author>Eiji Aramaki</author>
<author>Takeshi Imai</author>
<author>Kazuhiko Ohe</author>
</authors>
<title>Internal structure of a disease name and its application for ICD coding. Studies in health technology and informatics,</title>
<date>2010</date>
<volume>160</volume>
<issue>2</issue>
<pages>1014</pages>
<contexts>
<context position="15393" citStr="Yamada et al., 2010" startWordPosition="2442" endWordPosition="2445">yoto corpus, the NAIST text corpus, and the JP corpus ―into a phrase structure treebank, which is similar in spirit to PTB. Our approach is to convert the dependency structures of the Kyoto corpus into phrase structures and then augment them with syntactic/semantic roles from the other two corpora. The conversion involves two steps: 1) recognizing the chunk-internal structures, and (2) converting inter-chunk dependencies into phrase structures. For 1), we don’t have any explicit information in the Kyoto corpus although, in principle, each chunk has internal structures (Vadas and Curran, 2007; Yamada et al., 2010). The lack of a chunk-internal structure makes the dependencyto-constituency conversion more complex than a similar procedure by Bos et al. (2009) that converts an Italian dependency treebank into constituency trees since their dependency trees are annotated down to the level of each word. For the current implementation, we abandon the idea of identifying exact structures and instead basically rely on the following generic rules (Fig. 6): Nominal chunks Compound nouns are first formed as a right-branching phrase and post-positions are then attached to it. Verbal chunks Verbal chunks are analyz</context>
</contexts>
<marker>Yamada, Aramaki, Imai, Ohe, 2010</marker>
<rawString>Emiko Yamada, Eiji Aramaki, Takeshi Imai, and Kazuhiko Ohe. 2010. Internal structure of a disease name and its application for ICD coding. Studies in health technology and informatics, 160(2):1010– 1014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazuhiro Yoshida</author>
</authors>
<title>Corpus-oriented development of Japanese HPSG parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Student Research Workshop.</booktitle>
<contexts>
<context position="12258" citStr="Yoshida (2005)" startWordPosition="1944" endWordPosition="1945">ing is fairly limited. Formal theories of Japanese syntax were presented by Gunji (1987) based on Head-driven Phrase Structure Grammar (HPSG) (Sag et al., 2003) and by Komagata (1999) based on CCG, although their implementations in real-world parsing have not been very successful. JACY (Siegel &apos;In fact, the NAIST text corpus includes additional texts, but in this work we only use the news text section. 1044 and Bender, 2002) is a large-scale Japanese grammar based on HPSG, but its semantics is tightly embedded in the grammar and it is not as easy to systematically switch them as it is in CCG. Yoshida (2005) proposed methods for extracting a wide-coverage lexicon based on HPSG from a phrase structure treebank of Japanese. We largely extended their work by exploiting the standard chunk-based Japanese corpora and demonstrated the first results for Japanese deep parsing with grammar induced from large corpora. Corpus-based acquisition of wide-coverage CCG resources has enjoyed great success for English (Hockenmaier and Steedman, 2007). In that method, PTB was converted into CCG-based derivations from which a wide-coverage CCG lexicon was extracted. CCGbank has been used for the development of wide-c</context>
</contexts>
<marker>Yoshida, 2005</marker>
<rawString>Kazuhiro Yoshida. 2005. Corpus-oriented development of Japanese HPSG parsers. In Proceedings of the ACL Student Research Workshop.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>