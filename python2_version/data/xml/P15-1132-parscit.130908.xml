<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.9974735">
Learning Tag Embeddings and Tag-specific
Composition Functions in Recursive Neural Network
</title>
<author confidence="0.775346">
Qiao Qian, Bo Tian, Minlie Huang, Yang Liu*, Xuan Zhu*, Xiaoyan Zhu
</author>
<affiliation confidence="0.520685666666667">
State Key Lab. of Intelligent Technology and Systems, National Lab. for Information Science
and Technology, Dept. of Computer Science and Technology, Tsinghua University, Beijing 100084, PR China
*Samsung R&amp;D Institute Beijing, China
</affiliation>
<email confidence="0.833680666666667">
qianqiaodecember29@126.com , smxtianbo@gmail.com
aihuang@tsinghua.edu.cn, yang.liu@samsung.com
xuan.zhu@samsung.com, zxy-dcs@tsinghua.edu.cn
</email>
<sectionHeader confidence="0.993818" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999833344827586">
Recursive neural network is one of the
most successful deep learning models
for natural language processing due to
the compositional nature of text. The
model recursively composes the vector
of a parent phrase from those of child
words or phrases, with a key compo-
nent named composition function. Al-
though a variety of composition func-
tions have been proposed, the syntactic
information has not been fully encoded
in the composition process. We pro-
pose two models, Tag Guided RNN (TG-
RNN for short) which chooses a compo-
sition function according to the part-of-
speech tag of a phrase, and Tag Embedded
RNN/RNTN (TE-RNN/RNTN for short)
which learns tag embeddings and then
combines tag and word embeddings to-
gether. In the fine-grained sentiment
classification, experiment results show
the proposed models obtain remarkable
improvement: TG-RNN/TE-RNN obtain
remarkable improvement over baselines,
TE-RNTN obtains the second best result
among all the top performing models, and
all the proposed models have much less
parameters/complexity than their counter-
parts.
</bodyText>
<sectionHeader confidence="0.999337" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.969383">
Among a variety of deep learning models for nat-
ural language processing, Recursive Neural Net-
work (RNN) may be one of the most popular mod-
els. Thanks to the compositional nature of natu-
ral text, recursive neural network utilizes the re-
cursive structure of the input such as a phrase or
sentence, and has shown to be very effective for
many natural language processing tasks including
semantic relationship classification (Socher et al.,
2012), syntactic parsing (Socher et al., 2013a),
sentiment analysis (Socher et al., 2013b), and ma-
chine translation (Li et al., 2013).
The key component of RNN and its variants is
the composition function: how to compose the
vector representation for a longer text from the
vector of its child words or phrases. For instance,
as shown in Figure 2, the vector of ‘is very inter-
esting’ can be composed from the vector of the left
node ‘is’ and that of the right node ‘very interest-
ing’. It’s worth to mention again, the composition
process is conducted with the syntactic structure
of the text, making RNN more interpretable than
other deep learning models.
very interesting
</bodyText>
<figureCaption confidence="0.937286">
Figure 1: The example process of vector composi-
</figureCaption>
<bodyText confidence="0.976462416666667">
tion in RNN. The vector of node ‘very interesting’
is composed from the vectors of node ‘very’ and
node ‘interesting’. Similarly, the node ‘is very in-
teresting’ is composed from the phrase node ‘very
interesting’ and the word node ‘is’ .
There are various attempts to design the com-
position function in RNN (or related models). In
RNN (Socher et al., 2011), a global matrix is used
to linearly combine the elements of vectors. In
RNTN (Socher et al., 2013b), a global tensor is
used to compute the tensor products of dimen-
sions to favor the association between different el-
</bodyText>
<figure confidence="0.990180545454545">
softmax
is
...
softmax
g
is very interesting
softmax
g
softmax
very interesting
softmax
</figure>
<page confidence="0.934792">
1365
</page>
<note confidence="0.973599333333333">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1365–1374,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.999941138888889">
ements of the vectors. Sometimes it is challeng-
ing to find a single function to model the compo-
sition process. As an alternative, multiple com-
position functions can be used. For instance, in
MV-RNN (Socher et al., 2012), different matrices
is designed for different words though the model
is suffered from too much parameters. In AdaMC
RNN/RNTN (Dong et al., 2014), a fixed number
of composition functions is linearly combined and
the weight for each function is adaptively learned.
In spite of the success of RNN and its variants,
the syntactic knowledge of the text is not yet fully
employed in these models. Two ideas are moti-
vated by the example shown in Figure 2: First,
the composition function for the noun phrase ‘the
movie/NP’ should be different from that for the
adjective phrase ‘very interesting/ADJP’ since the
two phrases are quite syntactically different. More
specifically to sentiment analysis, a noun phrase is
much less likely to express sentiment than an ad-
jective phrase. There are two notable works men-
tioned here: (Socher et al., 2013a) presented to
combine the parsing and composition processes,
but the purpose is for parsing; (Hermann and
Blunsom, 2013) designed composition functions
according to the combinatory rules and categories
in CCG grammar, however, only marginal im-
provement against Naive Bayes was reported. Our
proposed model, tag guided RNN (TG-RNN), is
designed to use the syntactic tag of the parent
phrase to guide the composition process from the
child nodes. As an example, we design a function
for composing noun phrase (NP) and another one
for adjective phrase (ADJP). This simple strategy
obtains remarkable improvements against strong
baselines.
</bodyText>
<figureCaption confidence="0.706846">
Figure 2: The parse tree for sentence ‘The movie
is very interesting’ built by Stanford Parser.
</figureCaption>
<bodyText confidence="0.990549190476191">
Second, when composing the adjective phrase
‘very interesting/ADJP’ from the left node
‘very/RB’ and the right node ‘interesting/JJ’, the
right node is obviously more important than the
left one. Furthermore, the right node ‘interest-
ing/JJ’ apparently contributes more to sentiment
expression. To address this issue, we propose
Tag embedded RNN/RNTN (TE-RNN/RNTN), to
learn an embedding vector for each word/phrase
tag, and concatenate the tag vector with the
word/phrase vector as input to the composition
function. For instance, we have tag vectors for
DT,NN,RB,JJ,ADJP,NP, etc. and the tag vectors
are then used in composing the parent’s vector.
The proposed TE-RNTN obtain the second best re-
sult among all the top performing models but with
much less parameters and complexity. To the best
of our knowledge, this is the first time that tag em-
bedding is proposed.
To summarize, the contributions of our work are
as follows:
</bodyText>
<listItem confidence="0.918739">
• We propose tag-guided composition func-
tions in recursive neural network, TG-RNN.
Tag-guided RNN allocates a composition
function for a phrase according to the part-
of-speech tag of the phrase.
• We propose to learn embedding vectors for
part-of-speech tags of words/phrases, and
integrate the tag embeddings in RNN and
RNTN respectively. The two models, TE-
RNN and TE-RNTN, can leverage the syn-
tactic information of child nodes when gen-
erating the vector of parent nodes.
• The proposed models are efficient and effec-
tive. The scale of the parameters is well con-
trolled. Experimental results on the Stanford
Sentiment Treebank corpus show the effec-
tiveness of the models. TE-RNTN obtains
the second best result among all publicly re-
ported approaches, but with much less pa-
rameters and complexity.
</listItem>
<bodyText confidence="0.994908">
The rest of the paper is structured as follows: in
Section 2, we survey related work. In Section 3,
we introduce the traditional recursive neural net-
work as background. We present our ideas in Sec-
tion 4. The experiments are introduced in Section
5. We summarize the work in Section 6.
</bodyText>
<sectionHeader confidence="0.999737" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.984198333333333">
Different kinds of representations are used in
sentiment analysis. Traditionally, the bag-of-
words representations are used for sentiment anal-
ysis (Pang and Lee, 2008). To exploit the rela-
tionship between words, word co-occurrence (Tur-
ney et al., 2010) and syntactic contexts (Pad´o
</bodyText>
<equation confidence="0.997215444444445">
the / DT
the movie / NP
movie/NN
the movie is very interesting/ S
is / VBZ
is very interesting / VP
very interesting / ADJP
interesting / JJ
very / RB
</equation>
<page confidence="0.942334">
1366
</page>
<bodyText confidence="0.999585425925926">
and Lapata, 2007) are considered. In order to
distinguish antonyms with similar contexts, neu-
ral word vectors (Bengio et al., 2003) are pro-
posed and can be learnt in an unsupervised man-
ner. Word2vec (Mikolov et al., 2013a) introduces
a simpler network structure making computation
more efficiently and makes billions of samples
feasible for training.
Semantic composition deals with representing
a longer text from its shorter components, which
is extensively studied recently. In many previ-
ous works, a phrase vector is usually obtained by
average (Landauer and Dumais, 1997), addition,
element-wise multiplication (Mitchell and Lap-
ata, 2008) or tensor product (Smolensky, 1990) of
word vectors. In addition to using vector repre-
sentations, matrices can also be used to represent
phrases and the composition process can be done
through matrix multiplication (Rudolph and Gies-
brecht, 2010; Yessenalina and Cardie, 2011).
Recursive neural models utilize the recursive
structure (usually a parse tree) of a phrase or sen-
tence for semantic composition. In Recursive
Neural Network (Socher et al., 2011), the tree
with the least reconstruction error is built and the
vectors for interior nodes is composed by a global
matrix. Matrix-Vector Recursive Neural Network
(MV-RNN) (Socher et al., 2012) assigns matri-
ces for every words so that it could capture the
relationship between two children. In Recursive
Neural Tensor Networks (RNTN) (Socher et al.,
2013b), the composition process is performed on
a parse tree in which every node is annotated
with fine-grained sentiment labels, and a global
tensor is used for composition. Adaptive Multi-
Compositionality (Dong et al., 2014) uses multiple
weighted composition matrices instead of sharing
a single matrix.
The employment of syntactic information in
RNN is still in its infant. In (Socher et al., 2013a),
the part-of-speech tag of child nodes is considered
in combining the processes of both composition
and parsing. The main purpose is for better pars-
ing by employing RNN, but it is not designed for
sentiment analysis. In (Hermann and Blunsom,
2013), the authors designed composition functions
according to the combinatory rules and categories
in CCG grammar. However, only marginal im-
provement against Naive Bayes was reported. Un-
like (Hermann and Blunsom, 2013), our TG-RNN
obtains remarkable improvements against strong
baselines, and we are the first to propose tag em-
bedded RNTN which obtains the second best re-
sult among all reported approaches.
</bodyText>
<sectionHeader confidence="0.953118" genericHeader="method">
3 Background: Recursive Neural Models
</sectionHeader>
<bodyText confidence="0.995797846153846">
In recursive neural models, the vector of a longer
text (e.g., sentence) is composed from those of its
shorter components (e.g., words or phrases). To
compose a sentence vector through word/phrase
vectors, a binary parse tree has to be built with
a parser. The leaf nodes represent words and in-
terior nodes represent phrases. Vectors of interior
nodes are computed recursively by composition of
child nodes’ vectors. Specially, the root vector is
regarded as the sentence representation. The com-
position process is shown in Figure 1.
More formally, vector vi E Rd for node i is
calculated via:
</bodyText>
<equation confidence="0.876579">
vi = f(g(vli, vri )) (1)
</equation>
<bodyText confidence="0.9999382">
where vli and vri are child vectors, g is a compo-
sition function, and f is a nonlinearity function,
usually tanh. Different recursive neural models
mainly differ in composition function. For exam-
ple, the composition function for RNN is as below:
</bodyText>
<equation confidence="0.9716502">
[ ]
vl
g(vl i
i, vr i ) = W + b (2)
vr i
</equation>
<bodyText confidence="0.989493666666667">
where W E Rdx2d is a composition matrix and b
is a bias vector. And the composition function for
RNTN is as follows:
</bodyText>
<equation confidence="0.997194285714286">
[ ] [ ] [ ]
vl vl vl
g(vl i
i, vr T [1:d] i i
i ) = + W + b (3)
vr vr vr
i i i
</equation>
<bodyText confidence="0.999872333333333">
where W and b are defined in the previous model
and T[1:d] E R2dx2dxd is the tensor that defines
multiple bilinear forms.
The vectors are used as feature inputs to a soft-
max classifier. The posterior probability over class
labels on a node vector vi is given by
</bodyText>
<equation confidence="0.998959">
yi = softmax(Wsvi + bs). (4)
</equation>
<bodyText confidence="0.999533">
The parameters in these models include the
word table L, a composition matrix W in RNN,
and W and T[1:d] in RNTN, and the classification
matrix Ws for the softmax classifier.
</bodyText>
<page confidence="0.940142">
1367
</page>
<figure confidence="0.276354">
4 Incorporating Syntactic Knowledge The process is depicted in Figure 3. We term this
into Recursive Neural Model model Tag guided RNN, TG-RNN for short.
</figure>
<bodyText confidence="0.999988208333333">
The central idea of the paper is inspired by the fact
that words/phrases of different part-of-speech tags
play different roles in semantic composition. As
discussed in the introduction, a noun phrase (e.g.,
a movie/NP) may be composed different from a
verb phrase (e.g., love movie/VP). Furthermore,
when composing the phrase a movie/NP, the two
child words, a/DT and movie/NN, may play dif-
ferent roles in the composition process. Unfor-
tunately, the previous RNN models neglect such
syntactic information, though the models do em-
ploy the parsing structure of a sentence.
We have two approaches to improve the compo-
sition process by leveraging tags on parent nodes
and child nodes. One approach is to use different
composition matrices for parent nodes with differ-
ent tags so that the composition process could be
guided by phrase type, for example, the matrix for
‘NP’ is different from that for ‘VP’ . The other ap-
proach is to introduce ‘tag embedding’ for words
and phrases, for example, to learn tag vectors for
‘NP, VP, ADJP’, etc., and then integrate the tag
vectors with the word/phrase vectors during the
composition process.
</bodyText>
<subsectionHeader confidence="0.998785">
4.1 Tag Guided RNN (TG-RNN)
</subsectionHeader>
<bodyText confidence="0.9999876">
We propose Tag Guided RNN (TG-RNN) to re-
spect the tag of a parent phrase during the com-
position process. The model chooses a composi-
tion function according to the part-of-speech tag
of a phrase. For example, ‘the movie’ has tag NP,
‘very interesting’ has tag ADJP, the two phrases
have different composition matrices.
More formally, we design composition func-
tions g with a factor of the phrase tag of a parent
node. The composition function becomes
</bodyText>
<equation confidence="0.961904333333333">
[ vl
g(ti, vli, vz) = gti (vli, vz) = Wti r + bt i (5)
vi
</equation>
<bodyText confidence="0.999989888888889">
where ti is the phrase tag for node i, Wti and bti
are the parameters of function gti, as defined in
Equation 2. In other words, phrase nodes with
various tags have their own composition functions
such as gNP, gu P, and so on. There are to-
tally k composition function in this model where
k is the number of phrase tags. When composing
child vectors, a function is chosen from the func-
tion pool according to the tag of the parent node.
</bodyText>
<figure confidence="0.543557">
very / RB interesting / JJ
</figure>
<figureCaption confidence="0.860842666666667">
Figure 3: The vector of phrase ‘very interesting’
is composed with highlighted gADJP and ‘is very
interesting’ with gu P.
</figureCaption>
<bodyText confidence="0.9997846">
But some tags have few occurrences in the cor-
pus. It is hard and meaningless to train compo-
sition functions for those infrequent tags. So we
simply choose top k frequent tags and train k com-
position functions. A common composition func-
tion is shared across phrases with all infrequent
tags. The value of k depends on the size of the
training set and the occurrences of each tag. Spe-
cially, when k = 0, the model is the same as the
traditional RNN.
</bodyText>
<subsectionHeader confidence="0.538579">
4.2 Tag Embedded RNN and RNTN
</subsectionHeader>
<bodyText confidence="0.989955904761905">
(TE-RNN/RNTN)
In this section, we propose tag embedded RNN
(TE-RNN) and tag embedded RNTN (TE-RNTN)
to respect the part-of-speech tags of child nodes
during composition. As mentioned above, tags of
parent nodes have impact on composition. How-
ever, some phrases with the same tag should be
composed in different ways. For example, ‘is in-
teresting’ and ‘like swimming’ have the same tag
VP. But it is not reasonable to compose the two
phrases using the previous model because the part-
of-speech tags of their children are quite different.
If we use different composition functions for chil-
dren with different tags like TG-RNN, the number
of tag pairs will amount to as many as kxk, which
makes the models infeasible due to too many pa-
rameters.
In order to capture the compositional effects of
the tags of child nodes, an embedding et E R4 is
created for every tag t, where de is the dimension
of tag vector. The tag vector and phrase vector are
</bodyText>
<figure confidence="0.974655111111111">
...
softmax
is / VBZ
gNP
softmax
gADJP
...
is very interesting / VP
softmax
gNP
gVP
gADJP
...
softmax
very interesting / ADJP
gVP
softmax
1368
</figure>
<figureCaption confidence="0.9465295">
concatenated during composition as illustrated in
Figure 4.
</figureCaption>
<bodyText confidence="0.891343705882353">
Formally, the phrase vector is composed by the
function
g(vl i, etl i, vr i , etr i ) = W
where tli and tri are tags of the left and the right
nodes respectively, etli and etri are tag vectors, and
W E Rdx(2de+2d) is the composition matrix. We
term this model Tag embedded RNN, TE-RNN for
short.
very / RB interesting / JJ
Figure 4: RNN with tag embedding. There is a tag
embedding table, storing vectors for RB, JJ, and
ADJP, etc. Then we compose the phrase vector
’very interesting’ from the vectors for ’very’ and
’interesting’, and the tag vectors for RB and JJ.
Similarly, this idea can be applied to Recursive
Neural Tensor Network (Socher et al., 2013b). In
RNTN, the tag vector and the phrase vector can
be interweaved together through a tensor. More
specifically, the phrase vectors and tag vectors are
multiplied by the composed tensor. The composi-
tion function changes to the following:
g(vl i, etl i, vr i , etri )
⎡ vli ⎤ T[1:dl ⎡ vli ⎤ + W ⎡ vli ⎤ + b (7)
⎢ ⎢ ⎢ ⎢ ⎣ etl ⎦⎥⎥⎥⎥ ⎢ ⎢ ⎢ ⎢ ⎣ etl ⎦⎥⎥⎥⎥ ⎢ ⎢ ⎢ ⎢ ⎣ etl ⎦⎥⎥⎥⎥
i i i
vri vri vri
etr etr etr
i i i
where the variables are similar to those defined in
equation 3 and equation 7. We term this model
Tag embedded RNTN, TE-RNTN for short.
The phrase vectors and tag vectors are used as
input to a softmax classifier, giving the posterior
probability over labels via
</bodyText>
<equation confidence="0.93792175">
[ ]
vi
yi = softmax(Ws + bs) (8)
eti
</equation>
<subsectionHeader confidence="0.997098">
4.3 Model Training
</subsectionHeader>
<bodyText confidence="0.9998708">
Let yi be the target distribution for node i, yi be
the predicted sentiment distribution. Our goal is to
minimize the cross-entropy error between yi and
yi for all nodes. The loss function is defined as
follows:
</bodyText>
<equation confidence="0.959755">
E(θ) = − ∑ ∑ yji log yij + λl1θ112 (9)
i j
</equation>
<bodyText confidence="0.999775285714286">
where j is the label index, λ is a L2-regularization
term, and θ is the parameter set.
Similar to RNN, the parameters for our mod-
els include word vector table L, the composition
matrix W, and the sentiment classification matrix
Ws. Besides, our models have some additional pa-
rameters, as discussed below:
TG-RNN: There are k composition matrices for
top k frequent tags. They are defined as Wt E
Rkxdx2d. The original composition matrix W is
for all infrequent tags. As a result, the parameter
set of TG-RNN is θ = (L, W, Wt, Ws).
TE-RNN: The parameters include the tag em-
bedding table E, which contains all the em-
beddings for part-of-speech tags for words and
phrases. And the size of matrix W E Rdx(2d+2de)
and the softmax classifier Ws E RNx(de+d). The
parameter set of TE-RNN is θ = (L, E, W, Ws).
TE-RNTN: This model has one more tensor
T E R(2d+2de)x(2d+2de)xd than TE-RNN. The pa-
rameter set of TE-RNTN is θ = (L, E, W, T, Ws)
</bodyText>
<sectionHeader confidence="0.9997" genericHeader="method">
5 Experiment
</sectionHeader>
<subsectionHeader confidence="0.990384">
5.1 Dataset and Experiment Setting
</subsectionHeader>
<bodyText confidence="0.999983181818182">
We evaluate our models on Stanford Sentiment
Treebank which contains fully labeled parse trees.
It is built upon 10,662 reviews and each sentence
has sentiment labels on each node in the parse
tree. The sentiment label set is 10,1,2,3,41, where
the numbers mean very negative, negative, neu-
tral, positive, and very positive, respectively. We
use standard split (train: 8,544 dev: 1,101, test:
2,210) on the corpus in our experiments. In addi-
tion, we add the part-of-speech tag for each leaf
node and phrase-type tag for each interior node
</bodyText>
<figure confidence="0.99930075">
g
softmax
softmax
...
softmax
is very interesting / VP
is / VBZ
very interesting / ADJP
g
softmax
softmax
⎡ vli ⎤ + b (6)
⎢ ⎢ ⎢ ⎢ ⎣ etl ⎦⎥⎥⎥⎥
i
vri
etri
</figure>
<page confidence="0.993571">
1369
</page>
<bodyText confidence="0.999754296296296">
using the latest version of Stanford Parser. Be-
cause the newer parser generated trees different
from those provided in the datasets, 74/11/11 re-
views in train/dev/test datasets are ignored. Af-
ter removing the broken reviews, our dataset con-
tains 10566 reviews (train: 8,470, dev: 1,090, test:
2,199).
The word vectors were pre-trained on an unla-
beled corpus (about 100,000 movie reviews) by
word2vec (Mikolov et al., 2013b) as initial val-
ues and the other vectors is initialized by sampling
from a uniform distribution U(−E, E) where E is
0.01 in our experiments. The dimension of word
vectors is 25 for RNN models and 20 for RNTN
models. Tanh is chosen as the nonlinearity func-
tion. And after computing the output of node i
with vi = f(g(vz, vz )), we set vi = vi
||vi ||so that
the resulting vector has a limited norm. Back-
propagation algorithm (Rumelhart et al., 1986)
is used to compute gradients and we use mini-
batch SGD with momentum as the optimization
method, implemented with Theano (Bastien et al.,
2012). We trained all our models using stochas-
tic gradient descent with a batch size of 30 exam-
ples, momentum of 0.9, L2-regularization weight
of 0.0001 and a constant learning rate of 0.005.
</bodyText>
<subsectionHeader confidence="0.998637">
5.2 System Comparison
</subsectionHeader>
<bodyText confidence="0.997593833333333">
We compare our models with several methods
which are evaluated on the Sentiment Treebank
corpus. The baseline results are reported in (Dong
et al., 2014) and (Kim, 2014).
We make comparison to the following base-
lines:
</bodyText>
<listItem confidence="0.999206571428571">
• SVM. A SVM model with bag-of-words rep-
resentation (Pang and Lee, 2008).
• MNB/bi-MNB. Multinomial Naive Bayes
and its bigram variant, adopted from (Wang
and Manning, 2012).
• RNN. The first Recursive Neural Network
model proposed by (Socher et al., 2011).
• MV-RNN. Matrix Vector Recursive Neural
Network (Socher et al., 2012) represents
each word and phrase with a vector and a ma-
trix. As reported, this model suffers from too
many parameters.
• RNTN. Recursive Neural Tenser Net-
work (Socher et al., 2013b) employs a tensor
</listItem>
<table confidence="0.9997495">
Method Fine-grained Pos./Neg.
SVM 40.7 79.4
MNB 41.0 81.8
bi-MNB 41.9 83.1
RNN 43.2 82.4
MV-RNN 44.4 82.9
RNTN 45.7 85.4
AdaMC-RNN 45.8 87.1
AdaMC-RNTN 46.7 88.5
DRNN 49.8 87.7
TG-RNN (ours) 47.0 86.3
TE-RNN (ours) 48.0 86.8
TE-RNTN (ours) 48.9 87.7
CNN 48.0 88.1
DCNN 48.5 86.8
Para-Vec 48.7 87.8
</table>
<tableCaption confidence="0.999562">
Table 1: Classification accuray. Fine-grained
</tableCaption>
<bodyText confidence="0.971235714285714">
stands for 5-class prediction and Pos./Neg. means
binary prediction which ignores all neutral in-
stances. All the accuracy is at the sentence level
(root).
for composition function which could model
the meaning of longer phrases and capture
negation rules.
</bodyText>
<listItem confidence="0.982490157894737">
• AdaMC. Adaptive Multi-Compositionality
for RNN and RNTN (Dong et al., 2014)
trains more than one composition functions
and adaptively learns the weight for each
function.
• DCNN/CNN. Dynamic Convolutional Neu-
ral Network (Kalchbrenner et al., 2014) and a
simple Convolutional Neural Network (Kim,
2014), though these models are of different
genres to RNN, we include them here for fair
comparison since they are among top per-
forming approaches on this task.
• Para-Vec. A word2vec variant (Le and
Mikolov, 2014) that encodes paragraph in-
formation into word embedding learning. A
simple but very competitive model.
• DRNN. Deep Recursive Neural Network (Ir-
soy and Cardie, 2014) stacks multiple recur-
sive layers.
</listItem>
<bodyText confidence="0.858650666666667">
The comparative results are shown in Ta-
ble 1. As illustrated, TG-RNN outperforms
RNN, RNTN, MV-RNN, AdMC-RNN/RNTN.
</bodyText>
<page confidence="0.980168">
1370
</page>
<bodyText confidence="0.999965142857143">
Compared with RNN, the fine-grained accuracy
and binary accuracy of TG-RNN is improved by
3.8% and 3.9% respectively. When compared with
AdaMC-RNN, the accuracy of our method rises by
1.2% on the fine-grained prediction. The results
show that the syntactic knowledge does facilitate
phrase vector composition in this task.
As for TE-RNN/RNTN, the fine-grained accu-
racy of TE-RNN is boosted by 4.8% compared
with RNN and the accuracy of TE-RNTN by 3.2%
compared with RNTN. TE-RNTN also beat the
AdaMC-RNTN by 2.2% on the fine-grained clas-
sification task. TE-RNN is comparable to CNN
and DCNN, another line of models for this task.
TE-RNTN is better than CNN, DCNN, and Para-
Vec, which are the top performing approaches on
this task. TE-RNTN is worse than DRNN, but
the complexity of DRNN is much higher than TE-
RNTN, which will be discussed in the next sec-
tion. Furthermore, TE-RNN is also better than
TG-RNN. This implies that learning the tag em-
beddings for child nodes is more effective than
simply using the tags of parent phrases in com-
position.
Note that the fine-grained accuracy is more
convincible and reliable to compare different ap-
proaches due to the two facts: First, for the bi-
nary classification task, some approaches train an-
other binary classifier for positive/negative clas-
sification while other approaches, like ours, di-
rectly use the fine-grained classifier for this pur-
pose. Second, how the neutral instances are pro-
cessed is quite tricky and the details are not re-
ported in the literature. In our work, we sim-
ply remove neural instances from the test data be-
fore the evaluation. Let the 5-dimension vector
y be the probabilities for each sentiment label in
a test instance. The prediction will be positive if
arg maxi,i̸�2 yi is greater than 2, otherwise nega-
tive, where i E 10, 1, 2, 3, 41 means very negative,
negative, neutral, positive, very positive, respec-
tively.
</bodyText>
<subsectionHeader confidence="0.999459">
5.3 Complexity Analysis
</subsectionHeader>
<bodyText confidence="0.998711212121212">
To gain deeper understanding of the models pre-
sented in Table 1, we discuss here about the pa-
rameter scale of the RNN/RNTN models since
the prediction power of neural network models is
highly correlated with the number of parameters.
The analysis is presented in Table 2 (the opti-
mal values are adopted from the cited papers). The
parameters for the word table have the same size
n x d across all recursive neural models, where n
is the number of words and d is the dimension of
word vector. Therefore, we ignore this part but fo-
cus on the parameters of composition functions,
termed model size. Our models, TG-RNN/TE-
RNN, have much less parameters than RNTN and
AdMC-RNN/RNTN, but have much better perfor-
mance. Although TE-RNTN is worse than DRNN,
however, the parameters of DRNN are almost 9
times of ours. This indicates that DRNN is much
more complex, which requires much more data
and time to train. As a matter of a fact, our TE-
RNTN only takes 20 epochs for training which is
10 times less than DRNN.
Table 2: The model size. d is the dimension
of word/phrase vectors (the optimal value is 30
for RNN &amp; RNTN, 25 for AdaMC-RNN, 15 for
AdaMC-RNTN, 300 for DRNN). For AdaMC, c
is the number of composition functions (15 is the
optimal setting). For DRNN, l and h is the number
of layers and the width for each layer (the optimal
values l = 4, h = 174). For our methods, k is the
number of unshared composition matrices and de
the dimension of tag embedding, for the optimal
setting refer to Section 5.4.
</bodyText>
<subsectionHeader confidence="0.997982">
5.4 Parameter Analysis
</subsectionHeader>
<bodyText confidence="0.999884454545455">
We have two key parameters to tune in our pro-
posed models. For TG-RNN, the number of com-
position functions k is an important parameter,
which corresponds to the number of distinct POS
tags of phrases.
Let’s start from the corpus analysis. As shown
in Table 3, the corpus contains 215,154 phrases
but the distribution of phrase tags is extremely im-
balanced. For example, the phrase tag ‘NP’ ap-
pears 60,239 times while ‘NAC’ appears only 10
times. Hence, it is impossible to learn a composi-
</bodyText>
<figure confidence="0.992326526315789">
Method
model size # of parameters
2d2 1.8K
4d3 108K
2d2 x c 18.7K
4d3 x c 202K
dxhxl
+2h2 x l 451K
RNN
RNTN
AdaMC-RNN
AdaMC-RNTN
DRNN
TG-RNN (ours)
TE-RNN (ours)
TE-RNTN (ours)
2d2 x (k + 1) 8.8K
2(d + de) x d 1.7K
4(d + de)2 x d 54K
</figure>
<page confidence="0.923881">
1371
</page>
<table confidence="0.999207">
Phrase tag Frequency Phrase tag Frequency
NP 60,239 ADVP 1,140
S 33,138 PRN 976
VP 26,956 FARG 792
PP 14,979 UCP 362
ADJP 7,912 SSINV 266
SBAR 5,308 others 1,102
</table>
<tableCaption confidence="0.956602">
Table 3: The distribution of phrase-type tags in the
training data. The top 6 frequency tags cover more
than 95% phrases.
</tableCaption>
<bodyText confidence="0.9958925">
tion function for the infrequent phrase tags.
Each of the top k frequent phrase tags corre-
sponds to a unique composition function, while all
the other phrase tags share a same function. We
compare different k for TG-RNN. The accuracy is
shown in Figure 5. Our model obtains the best per-
formance when k is 6, which is accordant with the
statistics in Table 3.
</bodyText>
<figureCaption confidence="0.951457">
Figure 5: The accuracy for TG-RNN with differ-
ent k.
</figureCaption>
<bodyText confidence="0.9999168">
For TE-RNN/RNTN, the key parameter to tune
is the dimension of tag vectors. In the corpus, we
have 70 types of tags for leaf nodes (words) and in-
terior nodes (phrases). Infrequent tags whose fre-
quency is less than 1,000 are ignored. There are
30 tags left and we learn an embedding for each
of these frequent tags. We varies the dimension of
the embedding de from 0 to 30.
Figure 6 shows the accuracy for TE-RNN and
TE-RNTN with different dimensions of de. Our
model obtains the best performance when de is
8 for TE-RNN and 6 for TE-RNTN. The re-
sults show that too small dimensions may not be
sufficient to encode the syntactic information of
tags and too large dimensions damage the perfor-
</bodyText>
<figure confidence="0.975006">
0 5 10 15 20 25 30
de
</figure>
<figureCaption confidence="0.971578">
Figure 6: The accuracy for TE-RNN and TE-
RNTN with different dimensions of de.
</figureCaption>
<subsectionHeader confidence="0.961229">
5.5 Tag Vectors Analysis
</subsectionHeader>
<bodyText confidence="0.999275">
In order to prove tag vectors obtained from tag
embedded models are meaningful, we inspect the
similarity between vectors of tags. For each tag
vector, we find the nearest neighbors based on Eu-
clidean distance, summarized in Table 4.
</bodyText>
<table confidence="0.997981833333333">
Tag Most Similar Tags
JJ (Adjective) ADJP
(Adjective Phrase)
VP (Verb Phrase) VBD (past tense)
VBN (past participle)
. (Dot) : (Colon)
</table>
<tableCaption confidence="0.957598">
Table 4: Top 1 or 2 nearest neighboring tags with
definition in brackets.
</tableCaption>
<bodyText confidence="0.995909230769231">
Adjectives and verbs are of significant impor-
tance in sentiment analysis. Although ‘JJ’ and
‘ADJP’ are word and phrase tag respectively, they
have similar tag vectors, because of playing the
same role of Adjective in sentences. ‘VP’, ‘VBD’
and ‘VBN’ with similar representations all repre-
sent verbs. What is more interesting is that the
nearest neighbor of dot is colon, probably because
both of them are punctuation marks. Note that tag
classification is none of our training objectives and
surprisingly the vectors of similiar tags are clus-
tered together, which can provides additional in-
formation during sentence composition.
</bodyText>
<sectionHeader confidence="0.99948" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.983597">
In this paper, we present two ways to leverage syn-
tactic knowledge in Recursive Neural Networks.
</bodyText>
<figure confidence="0.996722392857143">
mance.
accuracy
0.49
0.48
0.47
0.46
0.45
0.44
0.43
0.42
TE-RNTN
TE-RNN
AdaMC-RNTN
AdaMC-RNN
RNTN
RNN
0.48
TG-RNN
AdaMC-RNN
RNN
0.46
0.45
0.44
0.43
0.420 2 4 6 8 10 12
k
accuracy
0.47
</figure>
<page confidence="0.991245">
1372
</page>
<bodyText confidence="0.99989975">
The first way is to use different composition func-
tions for phrases with different tags so that the
composition processing is guided by phrase types
(TG-RNN). The second way is to learn tag em-
beddings and combine tag and word embeddings
during composition (TE-RNN/RNTN). The pro-
posed models are not only effective (w.r.t com-
peting performance) but also efficient (w.r.t well-
controlled parameter scale). Experiment results
show that our models are among the top perform-
ing approaches up to date, but with much less pa-
rameters and complexity.
</bodyText>
<sectionHeader confidence="0.998295" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.987047555555556">
This work was partly supported by the Na-
tional Basic Research Program (973 Program) un-
der grant No.2012CB316301/2013CB329403, the
National Science Foundation of China under grant
No.61272227/61332007, and the Beijing Higher
Education Young Elite Teacher Project. The work
was also supported by Tsinghua University Bei-
jing Samsung Telecom R&amp;D Center Joint Labora-
tory for Intelligent Media Computing.
</bodyText>
<sectionHeader confidence="0.997976" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999912181818182">
Fr´ed´eric Bastien, Pascal Lamblin, Razvan Pascanu,
James Bergstra, Ian J. Goodfellow, Arnaud Berg-
eron, Nicolas Bouchard, and Yoshua Bengio. 2012.
Theano: new features and speed improvements.
Deep Learning and Unsupervised Feature Learning
NIPS 2012 Workshop.
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. The Journal of Machine Learning Re-
search, 3:1137–1155.
Li Dong, Furu Wei, Ming Zhou, and Ke Xu. 2014.
Adaptive multi-compositionality for recursive neu-
ral models with applications to sentiment analysis.
In AAAI. AAAI.
Karl Moritz Hermann and Phil Blunsom. 2013. The
role of syntax in vector space models of composi-
tional semantics. In ACL, pages 894–904. Associa-
tion for Computer Linguistics.
Ozan Irsoy and Claire Cardie. 2014. Deep recursive
neural networks for compositionality in language.
In NIPS, pages 2096–2104.
Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. In ACL, pages 655–665. As-
sociation for Computer Linguistics.
Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In EMNLP, pages 1746–
1751. Association for Computational Linguistics.
Thomas K Landauer and Susan T Dumais. 1997. A
solution to plato’s problem: The latent semantic
analysis theory of acquisition, induction, and rep-
resentation of knowledge. Psychological Review,
104(2):211.
Quoc V Le and Tomas Mikolov. 2014. Distributed rep-
resentations of sentences and documents. In ICML,
volume 32, pages 1188–1196.
Peng Li, Yang Liu, and Maosong Sun. 2013. Re-
cursive autoencoders for ITG-based translation. In
EMNLP, pages 567–577. Association for Computer
Linguistics.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. CoRR.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In NIPS, pages 3111–3119.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In ACL, pages
236–244.
Sebastian Pad´o and Mirella Lapata. 2007.
Dependency-based construction of semantic space
models. Computational Linguistics, 33(2):161–199.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in In-
formation Retrieval, 2(1-2):1–135.
Sebastian Rudolph and Eugenie Giesbrecht. 2010.
Compositional matrix-space models of language.
In ACL, pages 907–916. Association for Computer
Linguistics.
David E Rumelhart, Geoffrey E Hinton, and Ronald J
Williams. 1986. Learning representations by back-
propagating errors. Nature, 323:533–536.
Paul Smolensky. 1990. Tensor product variable bind-
ing and the representation of symbolic structures
in connectionist systems. Artificial intelligence,
46(1):159–216.
Richard Socher, Jeffrey Pennington, Eric H Huang,
Andrew Y Ng, and Christopher D Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In EMNLP, pages 151–
161. Association for Computational Linguistics.
Richard Socher, Brody Huval, Christopher D Man-
ning, and Andrew Y Ng. 2012. Semantic composi-
tionality through recursive matrix-vector spaces. In
EMNLP, pages 1201–1211. Association for Compu-
tational Linguistics.
</reference>
<page confidence="0.678194">
1373
</page>
<reference confidence="0.998622772727273">
Richard Socher, John Bauer, Christopher D Manning,
and Andrew Y Ng. 2013a. Parsing with compo-
sitional vector grammars. In ACL, pages 455–465.
Association for Computer Linguistics.
Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013b. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In EMNLP, pages 1631–1642. Associa-
tion for Computational Linguistics.
Peter D Turney, Patrick Pantel, et al. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37(1):141–188.
Sida I Wang and Christopher D Manning. 2012. Base-
lines and bigrams: Simple, good sentiment and topic
classification. In ACL, pages 90–94. Association for
Computational Linguistics.
Ainur Yessenalina and Claire Cardie. 2011. Compo-
sitional matrix-space models for sentiment analysis.
In EMNLP, pages 172–182. Association for Com-
puter Linguistics.
</reference>
<page confidence="0.994937">
1374
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.189997">
<title confidence="0.9993225">Learning Tag Embeddings and Composition Functions in Recursive Neural Network</title>
<author confidence="0.92472">Qiao Qian</author>
<author confidence="0.92472">Bo Tian</author>
<author confidence="0.92472">Minlie Huang</author>
<author confidence="0.92472">Yang Liu</author>
<author confidence="0.92472">Xuan Zhu</author>
<author confidence="0.92472">Xiaoyan</author>
<affiliation confidence="0.36701">State Key Lab. of Intelligent Technology and Systems, National Lab. for Information and Technology, Dept. of Computer Science and Technology, Tsinghua University, Beijing 100084, PR *Samsung R&amp;D Institute Beijing,</affiliation>
<abstract confidence="0.993083266666667">Recursive neural network is one of the most successful deep learning models for natural language processing due to the compositional nature of text. The model recursively composes the vector of a parent phrase from those of child words or phrases, with a key component named composition function. Although a variety of composition functions have been proposed, the syntactic information has not been fully encoded in the composition process. We protwo models, RNN (TG- RNN for short) which chooses a composition function according to the part-oftag of a phrase, and RNN/RNTN (TE-RNN/RNTN for short) which learns tag embeddings and then combines tag and word embeddings together. In the fine-grained sentiment classification, experiment results show the proposed models obtain remarkable improvement: TG-RNN/TE-RNN obtain remarkable improvement over baselines, TE-RNTN obtains the second best result among all the top performing models, and all the proposed models have much less parameters/complexity than their counterparts.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Fr´ed´eric Bastien</author>
<author>Pascal Lamblin</author>
<author>Razvan Pascanu</author>
<author>James Bergstra</author>
<author>Ian J Goodfellow</author>
<author>Arnaud Bergeron</author>
<author>Nicolas Bouchard</author>
<author>Yoshua Bengio</author>
</authors>
<title>Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS</title>
<date>2012</date>
<note>Workshop.</note>
<contexts>
<context position="20432" citStr="Bastien et al., 2012" startWordPosition="3463" endWordPosition="3466">rd2vec (Mikolov et al., 2013b) as initial values and the other vectors is initialized by sampling from a uniform distribution U(−E, E) where E is 0.01 in our experiments. The dimension of word vectors is 25 for RNN models and 20 for RNTN models. Tanh is chosen as the nonlinearity function. And after computing the output of node i with vi = f(g(vz, vz )), we set vi = vi ||vi ||so that the resulting vector has a limited norm. Backpropagation algorithm (Rumelhart et al., 1986) is used to compute gradients and we use minibatch SGD with momentum as the optimization method, implemented with Theano (Bastien et al., 2012). We trained all our models using stochastic gradient descent with a batch size of 30 examples, momentum of 0.9, L2-regularization weight of 0.0001 and a constant learning rate of 0.005. 5.2 System Comparison We compare our models with several methods which are evaluated on the Sentiment Treebank corpus. The baseline results are reported in (Dong et al., 2014) and (Kim, 2014). We make comparison to the following baselines: • SVM. A SVM model with bag-of-words representation (Pang and Lee, 2008). • MNB/bi-MNB. Multinomial Naive Bayes and its bigram variant, adopted from (Wang and Manning, 2012)</context>
</contexts>
<marker>Bastien, Lamblin, Pascanu, Bergstra, Goodfellow, Bergeron, Bouchard, Bengio, 2012</marker>
<rawString>Fr´ed´eric Bastien, Pascal Lamblin, Razvan Pascanu, James Bergstra, Ian J. Goodfellow, Arnaud Bergeron, Nicolas Bouchard, and Yoshua Bengio. 2012. Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Janvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="8089" citStr="Bengio et al., 2003" startWordPosition="1279" endWordPosition="1282">We summarize the work in Section 6. 2 Related Work Different kinds of representations are used in sentiment analysis. Traditionally, the bag-ofwords representations are used for sentiment analysis (Pang and Lee, 2008). To exploit the relationship between words, word co-occurrence (Turney et al., 2010) and syntactic contexts (Pad´o the / DT the movie / NP movie/NN the movie is very interesting/ S is / VBZ is very interesting / VP very interesting / ADJP interesting / JJ very / RB 1366 and Lapata, 2007) are considered. In order to distinguish antonyms with similar contexts, neural word vectors (Bengio et al., 2003) are proposed and can be learnt in an unsupervised manner. Word2vec (Mikolov et al., 2013a) introduces a simpler network structure making computation more efficiently and makes billions of samples feasible for training. Semantic composition deals with representing a longer text from its shorter components, which is extensively studied recently. In many previous works, a phrase vector is usually obtained by average (Landauer and Dumais, 1997), addition, element-wise multiplication (Mitchell and Lapata, 2008) or tensor product (Smolensky, 1990) of word vectors. In addition to using vector repres</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. The Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Li Dong</author>
<author>Furu Wei</author>
<author>Ming Zhou</author>
<author>Ke Xu</author>
</authors>
<title>Adaptive multi-compositionality for recursive neural models with applications to sentiment analysis.</title>
<date>2014</date>
<booktitle>In AAAI.</booktitle>
<publisher>AAAI.</publisher>
<contexts>
<context position="4068" citStr="Dong et al., 2014" startWordPosition="628" endWordPosition="631">gs of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1365–1374, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics ements of the vectors. Sometimes it is challenging to find a single function to model the composition process. As an alternative, multiple composition functions can be used. For instance, in MV-RNN (Socher et al., 2012), different matrices is designed for different words though the model is suffered from too much parameters. In AdaMC RNN/RNTN (Dong et al., 2014), a fixed number of composition functions is linearly combined and the weight for each function is adaptively learned. In spite of the success of RNN and its variants, the syntactic knowledge of the text is not yet fully employed in these models. Two ideas are motivated by the example shown in Figure 2: First, the composition function for the noun phrase ‘the movie/NP’ should be different from that for the adjective phrase ‘very interesting/ADJP’ since the two phrases are quite syntactically different. More specifically to sentiment analysis, a noun phrase is much less likely to express sentim</context>
<context position="9631" citStr="Dong et al., 2014" startWordPosition="1515" endWordPosition="1518">cursive Neural Network (Socher et al., 2011), the tree with the least reconstruction error is built and the vectors for interior nodes is composed by a global matrix. Matrix-Vector Recursive Neural Network (MV-RNN) (Socher et al., 2012) assigns matrices for every words so that it could capture the relationship between two children. In Recursive Neural Tensor Networks (RNTN) (Socher et al., 2013b), the composition process is performed on a parse tree in which every node is annotated with fine-grained sentiment labels, and a global tensor is used for composition. Adaptive MultiCompositionality (Dong et al., 2014) uses multiple weighted composition matrices instead of sharing a single matrix. The employment of syntactic information in RNN is still in its infant. In (Socher et al., 2013a), the part-of-speech tag of child nodes is considered in combining the processes of both composition and parsing. The main purpose is for better parsing by employing RNN, but it is not designed for sentiment analysis. In (Hermann and Blunsom, 2013), the authors designed composition functions according to the combinatory rules and categories in CCG grammar. However, only marginal improvement against Naive Bayes was repor</context>
<context position="20794" citStr="Dong et al., 2014" startWordPosition="3523" endWordPosition="3526">i = vi ||vi ||so that the resulting vector has a limited norm. Backpropagation algorithm (Rumelhart et al., 1986) is used to compute gradients and we use minibatch SGD with momentum as the optimization method, implemented with Theano (Bastien et al., 2012). We trained all our models using stochastic gradient descent with a batch size of 30 examples, momentum of 0.9, L2-regularization weight of 0.0001 and a constant learning rate of 0.005. 5.2 System Comparison We compare our models with several methods which are evaluated on the Sentiment Treebank corpus. The baseline results are reported in (Dong et al., 2014) and (Kim, 2014). We make comparison to the following baselines: • SVM. A SVM model with bag-of-words representation (Pang and Lee, 2008). • MNB/bi-MNB. Multinomial Naive Bayes and its bigram variant, adopted from (Wang and Manning, 2012). • RNN. The first Recursive Neural Network model proposed by (Socher et al., 2011). • MV-RNN. Matrix Vector Recursive Neural Network (Socher et al., 2012) represents each word and phrase with a vector and a matrix. As reported, this model suffers from too many parameters. • RNTN. Recursive Neural Tenser Network (Socher et al., 2013b) employs a tensor Method F</context>
<context position="22063" citStr="Dong et al., 2014" startWordPosition="3729" endWordPosition="3732">i-MNB 41.9 83.1 RNN 43.2 82.4 MV-RNN 44.4 82.9 RNTN 45.7 85.4 AdaMC-RNN 45.8 87.1 AdaMC-RNTN 46.7 88.5 DRNN 49.8 87.7 TG-RNN (ours) 47.0 86.3 TE-RNN (ours) 48.0 86.8 TE-RNTN (ours) 48.9 87.7 CNN 48.0 88.1 DCNN 48.5 86.8 Para-Vec 48.7 87.8 Table 1: Classification accuray. Fine-grained stands for 5-class prediction and Pos./Neg. means binary prediction which ignores all neutral instances. All the accuracy is at the sentence level (root). for composition function which could model the meaning of longer phrases and capture negation rules. • AdaMC. Adaptive Multi-Compositionality for RNN and RNTN (Dong et al., 2014) trains more than one composition functions and adaptively learns the weight for each function. • DCNN/CNN. Dynamic Convolutional Neural Network (Kalchbrenner et al., 2014) and a simple Convolutional Neural Network (Kim, 2014), though these models are of different genres to RNN, we include them here for fair comparison since they are among top performing approaches on this task. • Para-Vec. A word2vec variant (Le and Mikolov, 2014) that encodes paragraph information into word embedding learning. A simple but very competitive model. • DRNN. Deep Recursive Neural Network (Irsoy and Cardie, 2014)</context>
</contexts>
<marker>Dong, Wei, Zhou, Xu, 2014</marker>
<rawString>Li Dong, Furu Wei, Ming Zhou, and Ke Xu. 2014. Adaptive multi-compositionality for recursive neural models with applications to sentiment analysis. In AAAI. AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl Moritz Hermann</author>
<author>Phil Blunsom</author>
</authors>
<title>The role of syntax in vector space models of compositional semantics.</title>
<date>2013</date>
<booktitle>In ACL,</booktitle>
<pages>894--904</pages>
<institution>Association for Computer Linguistics.</institution>
<contexts>
<context position="4884" citStr="Hermann and Blunsom, 2013" startWordPosition="761" endWordPosition="764">e of the text is not yet fully employed in these models. Two ideas are motivated by the example shown in Figure 2: First, the composition function for the noun phrase ‘the movie/NP’ should be different from that for the adjective phrase ‘very interesting/ADJP’ since the two phrases are quite syntactically different. More specifically to sentiment analysis, a noun phrase is much less likely to express sentiment than an adjective phrase. There are two notable works mentioned here: (Socher et al., 2013a) presented to combine the parsing and composition processes, but the purpose is for parsing; (Hermann and Blunsom, 2013) designed composition functions according to the combinatory rules and categories in CCG grammar, however, only marginal improvement against Naive Bayes was reported. Our proposed model, tag guided RNN (TG-RNN), is designed to use the syntactic tag of the parent phrase to guide the composition process from the child nodes. As an example, we design a function for composing noun phrase (NP) and another one for adjective phrase (ADJP). This simple strategy obtains remarkable improvements against strong baselines. Figure 2: The parse tree for sentence ‘The movie is very interesting’ built by Stanf</context>
<context position="10056" citStr="Hermann and Blunsom, 2013" startWordPosition="1584" endWordPosition="1587">process is performed on a parse tree in which every node is annotated with fine-grained sentiment labels, and a global tensor is used for composition. Adaptive MultiCompositionality (Dong et al., 2014) uses multiple weighted composition matrices instead of sharing a single matrix. The employment of syntactic information in RNN is still in its infant. In (Socher et al., 2013a), the part-of-speech tag of child nodes is considered in combining the processes of both composition and parsing. The main purpose is for better parsing by employing RNN, but it is not designed for sentiment analysis. In (Hermann and Blunsom, 2013), the authors designed composition functions according to the combinatory rules and categories in CCG grammar. However, only marginal improvement against Naive Bayes was reported. Unlike (Hermann and Blunsom, 2013), our TG-RNN obtains remarkable improvements against strong baselines, and we are the first to propose tag embedded RNTN which obtains the second best result among all reported approaches. 3 Background: Recursive Neural Models In recursive neural models, the vector of a longer text (e.g., sentence) is composed from those of its shorter components (e.g., words or phrases). To compose </context>
</contexts>
<marker>Hermann, Blunsom, 2013</marker>
<rawString>Karl Moritz Hermann and Phil Blunsom. 2013. The role of syntax in vector space models of compositional semantics. In ACL, pages 894–904. Association for Computer Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ozan Irsoy</author>
<author>Claire Cardie</author>
</authors>
<title>Deep recursive neural networks for compositionality in language. In</title>
<date>2014</date>
<booktitle>NIPS,</booktitle>
<pages>2096--2104</pages>
<contexts>
<context position="22663" citStr="Irsoy and Cardie, 2014" startWordPosition="3823" endWordPosition="3827">NTN (Dong et al., 2014) trains more than one composition functions and adaptively learns the weight for each function. • DCNN/CNN. Dynamic Convolutional Neural Network (Kalchbrenner et al., 2014) and a simple Convolutional Neural Network (Kim, 2014), though these models are of different genres to RNN, we include them here for fair comparison since they are among top performing approaches on this task. • Para-Vec. A word2vec variant (Le and Mikolov, 2014) that encodes paragraph information into word embedding learning. A simple but very competitive model. • DRNN. Deep Recursive Neural Network (Irsoy and Cardie, 2014) stacks multiple recursive layers. The comparative results are shown in Table 1. As illustrated, TG-RNN outperforms RNN, RNTN, MV-RNN, AdMC-RNN/RNTN. 1370 Compared with RNN, the fine-grained accuracy and binary accuracy of TG-RNN is improved by 3.8% and 3.9% respectively. When compared with AdaMC-RNN, the accuracy of our method rises by 1.2% on the fine-grained prediction. The results show that the syntactic knowledge does facilitate phrase vector composition in this task. As for TE-RNN/RNTN, the fine-grained accuracy of TE-RNN is boosted by 4.8% compared with RNN and the accuracy of TE-RNTN b</context>
</contexts>
<marker>Irsoy, Cardie, 2014</marker>
<rawString>Ozan Irsoy and Claire Cardie. 2014. Deep recursive neural networks for compositionality in language. In NIPS, pages 2096–2104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Edward Grefenstette</author>
<author>Phil Blunsom</author>
</authors>
<title>A convolutional neural network for modelling sentences. In</title>
<date>2014</date>
<booktitle>ACL,</booktitle>
<pages>655--665</pages>
<institution>Association for Computer Linguistics.</institution>
<contexts>
<context position="22235" citStr="Kalchbrenner et al., 2014" startWordPosition="3754" endWordPosition="3757">8 TE-RNTN (ours) 48.9 87.7 CNN 48.0 88.1 DCNN 48.5 86.8 Para-Vec 48.7 87.8 Table 1: Classification accuray. Fine-grained stands for 5-class prediction and Pos./Neg. means binary prediction which ignores all neutral instances. All the accuracy is at the sentence level (root). for composition function which could model the meaning of longer phrases and capture negation rules. • AdaMC. Adaptive Multi-Compositionality for RNN and RNTN (Dong et al., 2014) trains more than one composition functions and adaptively learns the weight for each function. • DCNN/CNN. Dynamic Convolutional Neural Network (Kalchbrenner et al., 2014) and a simple Convolutional Neural Network (Kim, 2014), though these models are of different genres to RNN, we include them here for fair comparison since they are among top performing approaches on this task. • Para-Vec. A word2vec variant (Le and Mikolov, 2014) that encodes paragraph information into word embedding learning. A simple but very competitive model. • DRNN. Deep Recursive Neural Network (Irsoy and Cardie, 2014) stacks multiple recursive layers. The comparative results are shown in Table 1. As illustrated, TG-RNN outperforms RNN, RNTN, MV-RNN, AdMC-RNN/RNTN. 1370 Compared with RNN</context>
</contexts>
<marker>Kalchbrenner, Grefenstette, Blunsom, 2014</marker>
<rawString>Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. 2014. A convolutional neural network for modelling sentences. In ACL, pages 655–665. Association for Computer Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoon Kim</author>
</authors>
<title>Convolutional neural networks for sentence classification. In</title>
<date>2014</date>
<booktitle>EMNLP,</booktitle>
<pages>1746--1751</pages>
<institution>Association for Computational Linguistics.</institution>
<contexts>
<context position="20810" citStr="Kim, 2014" startWordPosition="3528" endWordPosition="3529">e resulting vector has a limited norm. Backpropagation algorithm (Rumelhart et al., 1986) is used to compute gradients and we use minibatch SGD with momentum as the optimization method, implemented with Theano (Bastien et al., 2012). We trained all our models using stochastic gradient descent with a batch size of 30 examples, momentum of 0.9, L2-regularization weight of 0.0001 and a constant learning rate of 0.005. 5.2 System Comparison We compare our models with several methods which are evaluated on the Sentiment Treebank corpus. The baseline results are reported in (Dong et al., 2014) and (Kim, 2014). We make comparison to the following baselines: • SVM. A SVM model with bag-of-words representation (Pang and Lee, 2008). • MNB/bi-MNB. Multinomial Naive Bayes and its bigram variant, adopted from (Wang and Manning, 2012). • RNN. The first Recursive Neural Network model proposed by (Socher et al., 2011). • MV-RNN. Matrix Vector Recursive Neural Network (Socher et al., 2012) represents each word and phrase with a vector and a matrix. As reported, this model suffers from too many parameters. • RNTN. Recursive Neural Tenser Network (Socher et al., 2013b) employs a tensor Method Fine-grained Pos.</context>
<context position="22289" citStr="Kim, 2014" startWordPosition="3764" endWordPosition="3765">87.8 Table 1: Classification accuray. Fine-grained stands for 5-class prediction and Pos./Neg. means binary prediction which ignores all neutral instances. All the accuracy is at the sentence level (root). for composition function which could model the meaning of longer phrases and capture negation rules. • AdaMC. Adaptive Multi-Compositionality for RNN and RNTN (Dong et al., 2014) trains more than one composition functions and adaptively learns the weight for each function. • DCNN/CNN. Dynamic Convolutional Neural Network (Kalchbrenner et al., 2014) and a simple Convolutional Neural Network (Kim, 2014), though these models are of different genres to RNN, we include them here for fair comparison since they are among top performing approaches on this task. • Para-Vec. A word2vec variant (Le and Mikolov, 2014) that encodes paragraph information into word embedding learning. A simple but very competitive model. • DRNN. Deep Recursive Neural Network (Irsoy and Cardie, 2014) stacks multiple recursive layers. The comparative results are shown in Table 1. As illustrated, TG-RNN outperforms RNN, RNTN, MV-RNN, AdMC-RNN/RNTN. 1370 Compared with RNN, the fine-grained accuracy and binary accuracy of TG-</context>
</contexts>
<marker>Kim, 2014</marker>
<rawString>Yoon Kim. 2014. Convolutional neural networks for sentence classification. In EMNLP, pages 1746– 1751. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Susan T Dumais</author>
</authors>
<title>A solution to plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<issue>2</issue>
<contexts>
<context position="8534" citStr="Landauer and Dumais, 1997" startWordPosition="1347" endWordPosition="1350">nteresting / ADJP interesting / JJ very / RB 1366 and Lapata, 2007) are considered. In order to distinguish antonyms with similar contexts, neural word vectors (Bengio et al., 2003) are proposed and can be learnt in an unsupervised manner. Word2vec (Mikolov et al., 2013a) introduces a simpler network structure making computation more efficiently and makes billions of samples feasible for training. Semantic composition deals with representing a longer text from its shorter components, which is extensively studied recently. In many previous works, a phrase vector is usually obtained by average (Landauer and Dumais, 1997), addition, element-wise multiplication (Mitchell and Lapata, 2008) or tensor product (Smolensky, 1990) of word vectors. In addition to using vector representations, matrices can also be used to represent phrases and the composition process can be done through matrix multiplication (Rudolph and Giesbrecht, 2010; Yessenalina and Cardie, 2011). Recursive neural models utilize the recursive structure (usually a parse tree) of a phrase or sentence for semantic composition. In Recursive Neural Network (Socher et al., 2011), the tree with the least reconstruction error is built and the vectors for i</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Thomas K Landauer and Susan T Dumais. 1997. A solution to plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological Review, 104(2):211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quoc V Le and Tomas Mikolov</author>
</authors>
<title>Distributed representations of sentences and documents.</title>
<date>2014</date>
<booktitle>In ICML,</booktitle>
<volume>32</volume>
<pages>1188--1196</pages>
<contexts>
<context position="22498" citStr="Mikolov, 2014" startWordPosition="3800" endWordPosition="3801"> for composition function which could model the meaning of longer phrases and capture negation rules. • AdaMC. Adaptive Multi-Compositionality for RNN and RNTN (Dong et al., 2014) trains more than one composition functions and adaptively learns the weight for each function. • DCNN/CNN. Dynamic Convolutional Neural Network (Kalchbrenner et al., 2014) and a simple Convolutional Neural Network (Kim, 2014), though these models are of different genres to RNN, we include them here for fair comparison since they are among top performing approaches on this task. • Para-Vec. A word2vec variant (Le and Mikolov, 2014) that encodes paragraph information into word embedding learning. A simple but very competitive model. • DRNN. Deep Recursive Neural Network (Irsoy and Cardie, 2014) stacks multiple recursive layers. The comparative results are shown in Table 1. As illustrated, TG-RNN outperforms RNN, RNTN, MV-RNN, AdMC-RNN/RNTN. 1370 Compared with RNN, the fine-grained accuracy and binary accuracy of TG-RNN is improved by 3.8% and 3.9% respectively. When compared with AdaMC-RNN, the accuracy of our method rises by 1.2% on the fine-grained prediction. The results show that the syntactic knowledge does facilita</context>
</contexts>
<marker>Mikolov, 2014</marker>
<rawString>Quoc V Le and Tomas Mikolov. 2014. Distributed representations of sentences and documents. In ICML, volume 32, pages 1188–1196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peng Li</author>
<author>Yang Liu</author>
<author>Maosong Sun</author>
</authors>
<title>Recursive autoencoders for ITG-based translation. In</title>
<date>2013</date>
<booktitle>EMNLP,</booktitle>
<pages>567--577</pages>
<institution>Association for Computer Linguistics.</institution>
<contexts>
<context position="2188" citStr="Li et al., 2013" startWordPosition="317" endWordPosition="320">exity than their counterparts. 1 Introduction Among a variety of deep learning models for natural language processing, Recursive Neural Network (RNN) may be one of the most popular models. Thanks to the compositional nature of natural text, recursive neural network utilizes the recursive structure of the input such as a phrase or sentence, and has shown to be very effective for many natural language processing tasks including semantic relationship classification (Socher et al., 2012), syntactic parsing (Socher et al., 2013a), sentiment analysis (Socher et al., 2013b), and machine translation (Li et al., 2013). The key component of RNN and its variants is the composition function: how to compose the vector representation for a longer text from the vector of its child words or phrases. For instance, as shown in Figure 2, the vector of ‘is very interesting’ can be composed from the vector of the left node ‘is’ and that of the right node ‘very interesting’. It’s worth to mention again, the composition process is conducted with the syntactic structure of the text, making RNN more interpretable than other deep learning models. very interesting Figure 1: The example process of vector composition in RNN. </context>
</contexts>
<marker>Li, Liu, Sun, 2013</marker>
<rawString>Peng Li, Yang Liu, and Maosong Sun. 2013. Recursive autoencoders for ITG-based translation. In EMNLP, pages 567–577. Association for Computer Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<publisher>CoRR.</publisher>
<contexts>
<context position="8178" citStr="Mikolov et al., 2013" startWordPosition="1296" endWordPosition="1299"> used in sentiment analysis. Traditionally, the bag-ofwords representations are used for sentiment analysis (Pang and Lee, 2008). To exploit the relationship between words, word co-occurrence (Turney et al., 2010) and syntactic contexts (Pad´o the / DT the movie / NP movie/NN the movie is very interesting/ S is / VBZ is very interesting / VP very interesting / ADJP interesting / JJ very / RB 1366 and Lapata, 2007) are considered. In order to distinguish antonyms with similar contexts, neural word vectors (Bengio et al., 2003) are proposed and can be learnt in an unsupervised manner. Word2vec (Mikolov et al., 2013a) introduces a simpler network structure making computation more efficiently and makes billions of samples feasible for training. Semantic composition deals with representing a longer text from its shorter components, which is extensively studied recently. In many previous works, a phrase vector is usually obtained by average (Landauer and Dumais, 1997), addition, element-wise multiplication (Mitchell and Lapata, 2008) or tensor product (Smolensky, 1990) of word vectors. In addition to using vector representations, matrices can also be used to represent phrases and the composition process can</context>
<context position="19839" citStr="Mikolov et al., 2013" startWordPosition="3355" endWordPosition="3358">d phrase-type tag for each interior node g softmax softmax ... softmax is very interesting / VP is / VBZ very interesting / ADJP g softmax softmax ⎡ vli ⎤ + b (6) ⎢ ⎢ ⎢ ⎢ ⎣ etl ⎦⎥⎥⎥⎥ i vri etri 1369 using the latest version of Stanford Parser. Because the newer parser generated trees different from those provided in the datasets, 74/11/11 reviews in train/dev/test datasets are ignored. After removing the broken reviews, our dataset contains 10566 reviews (train: 8,470, dev: 1,090, test: 2,199). The word vectors were pre-trained on an unlabeled corpus (about 100,000 movie reviews) by word2vec (Mikolov et al., 2013b) as initial values and the other vectors is initialized by sampling from a uniform distribution U(−E, E) where E is 0.01 in our experiments. The dimension of word vectors is 25 for RNN models and 20 for RNTN models. Tanh is chosen as the nonlinearity function. And after computing the output of node i with vi = f(g(vz, vz )), we set vi = vi ||vi ||so that the resulting vector has a limited norm. Backpropagation algorithm (Rumelhart et al., 1986) is used to compute gradients and we use minibatch SGD with momentum as the optimization method, implemented with Theano (Bastien et al., 2012). We tr</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. CoRR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In NIPS,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="8178" citStr="Mikolov et al., 2013" startWordPosition="1296" endWordPosition="1299"> used in sentiment analysis. Traditionally, the bag-ofwords representations are used for sentiment analysis (Pang and Lee, 2008). To exploit the relationship between words, word co-occurrence (Turney et al., 2010) and syntactic contexts (Pad´o the / DT the movie / NP movie/NN the movie is very interesting/ S is / VBZ is very interesting / VP very interesting / ADJP interesting / JJ very / RB 1366 and Lapata, 2007) are considered. In order to distinguish antonyms with similar contexts, neural word vectors (Bengio et al., 2003) are proposed and can be learnt in an unsupervised manner. Word2vec (Mikolov et al., 2013a) introduces a simpler network structure making computation more efficiently and makes billions of samples feasible for training. Semantic composition deals with representing a longer text from its shorter components, which is extensively studied recently. In many previous works, a phrase vector is usually obtained by average (Landauer and Dumais, 1997), addition, element-wise multiplication (Mitchell and Lapata, 2008) or tensor product (Smolensky, 1990) of word vectors. In addition to using vector representations, matrices can also be used to represent phrases and the composition process can</context>
<context position="19839" citStr="Mikolov et al., 2013" startWordPosition="3355" endWordPosition="3358">d phrase-type tag for each interior node g softmax softmax ... softmax is very interesting / VP is / VBZ very interesting / ADJP g softmax softmax ⎡ vli ⎤ + b (6) ⎢ ⎢ ⎢ ⎢ ⎣ etl ⎦⎥⎥⎥⎥ i vri etri 1369 using the latest version of Stanford Parser. Because the newer parser generated trees different from those provided in the datasets, 74/11/11 reviews in train/dev/test datasets are ignored. After removing the broken reviews, our dataset contains 10566 reviews (train: 8,470, dev: 1,090, test: 2,199). The word vectors were pre-trained on an unlabeled corpus (about 100,000 movie reviews) by word2vec (Mikolov et al., 2013b) as initial values and the other vectors is initialized by sampling from a uniform distribution U(−E, E) where E is 0.01 in our experiments. The dimension of word vectors is 25 for RNN models and 20 for RNTN models. Tanh is chosen as the nonlinearity function. And after computing the output of node i with vi = f(g(vz, vz )), we set vi = vi ||vi ||so that the resulting vector has a limited norm. Backpropagation algorithm (Rumelhart et al., 1986) is used to compute gradients and we use minibatch SGD with momentum as the optimization method, implemented with Theano (Bastien et al., 2012). We tr</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013b. Distributed representations of words and phrases and their compositionality. In NIPS, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Vector-based models of semantic composition.</title>
<date>2008</date>
<booktitle>In ACL,</booktitle>
<pages>236--244</pages>
<contexts>
<context position="8601" citStr="Mitchell and Lapata, 2008" startWordPosition="1354" endWordPosition="1358"> are considered. In order to distinguish antonyms with similar contexts, neural word vectors (Bengio et al., 2003) are proposed and can be learnt in an unsupervised manner. Word2vec (Mikolov et al., 2013a) introduces a simpler network structure making computation more efficiently and makes billions of samples feasible for training. Semantic composition deals with representing a longer text from its shorter components, which is extensively studied recently. In many previous works, a phrase vector is usually obtained by average (Landauer and Dumais, 1997), addition, element-wise multiplication (Mitchell and Lapata, 2008) or tensor product (Smolensky, 1990) of word vectors. In addition to using vector representations, matrices can also be used to represent phrases and the composition process can be done through matrix multiplication (Rudolph and Giesbrecht, 2010; Yessenalina and Cardie, 2011). Recursive neural models utilize the recursive structure (usually a parse tree) of a phrase or sentence for semantic composition. In Recursive Neural Network (Socher et al., 2011), the tree with the least reconstruction error is built and the vectors for interior nodes is composed by a global matrix. Matrix-Vector Recursi</context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. In ACL, pages 236–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
<author>Mirella Lapata</author>
</authors>
<title>Dependency-based construction of semantic space models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<marker>Pad´o, Lapata, 2007</marker>
<rawString>Sebastian Pad´o and Mirella Lapata. 2007. Dependency-based construction of semantic space models. Computational Linguistics, 33(2):161–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis.</title>
<date>2008</date>
<booktitle>Foundations and Trends in Information Retrieval,</booktitle>
<pages>2--1</pages>
<contexts>
<context position="7686" citStr="Pang and Lee, 2008" startWordPosition="1207" endWordPosition="1210">ffectiveness of the models. TE-RNTN obtains the second best result among all publicly reported approaches, but with much less parameters and complexity. The rest of the paper is structured as follows: in Section 2, we survey related work. In Section 3, we introduce the traditional recursive neural network as background. We present our ideas in Section 4. The experiments are introduced in Section 5. We summarize the work in Section 6. 2 Related Work Different kinds of representations are used in sentiment analysis. Traditionally, the bag-ofwords representations are used for sentiment analysis (Pang and Lee, 2008). To exploit the relationship between words, word co-occurrence (Turney et al., 2010) and syntactic contexts (Pad´o the / DT the movie / NP movie/NN the movie is very interesting/ S is / VBZ is very interesting / VP very interesting / ADJP interesting / JJ very / RB 1366 and Lapata, 2007) are considered. In order to distinguish antonyms with similar contexts, neural word vectors (Bengio et al., 2003) are proposed and can be learnt in an unsupervised manner. Word2vec (Mikolov et al., 2013a) introduces a simpler network structure making computation more efficiently and makes billions of samples </context>
<context position="20931" citStr="Pang and Lee, 2008" startWordPosition="3547" endWordPosition="3550">dients and we use minibatch SGD with momentum as the optimization method, implemented with Theano (Bastien et al., 2012). We trained all our models using stochastic gradient descent with a batch size of 30 examples, momentum of 0.9, L2-regularization weight of 0.0001 and a constant learning rate of 0.005. 5.2 System Comparison We compare our models with several methods which are evaluated on the Sentiment Treebank corpus. The baseline results are reported in (Dong et al., 2014) and (Kim, 2014). We make comparison to the following baselines: • SVM. A SVM model with bag-of-words representation (Pang and Lee, 2008). • MNB/bi-MNB. Multinomial Naive Bayes and its bigram variant, adopted from (Wang and Manning, 2012). • RNN. The first Recursive Neural Network model proposed by (Socher et al., 2011). • MV-RNN. Matrix Vector Recursive Neural Network (Socher et al., 2012) represents each word and phrase with a vector and a matrix. As reported, this model suffers from too many parameters. • RNTN. Recursive Neural Tenser Network (Socher et al., 2013b) employs a tensor Method Fine-grained Pos./Neg. SVM 40.7 79.4 MNB 41.0 81.8 bi-MNB 41.9 83.1 RNN 43.2 82.4 MV-RNN 44.4 82.9 RNTN 45.7 85.4 AdaMC-RNN 45.8 87.1 AdaM</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval, 2(1-2):1–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Rudolph</author>
<author>Eugenie Giesbrecht</author>
</authors>
<title>Compositional matrix-space models of language. In</title>
<date>2010</date>
<booktitle>ACL,</booktitle>
<pages>907--916</pages>
<institution>Association for Computer Linguistics.</institution>
<contexts>
<context position="8846" citStr="Rudolph and Giesbrecht, 2010" startWordPosition="1392" endWordPosition="1396">e making computation more efficiently and makes billions of samples feasible for training. Semantic composition deals with representing a longer text from its shorter components, which is extensively studied recently. In many previous works, a phrase vector is usually obtained by average (Landauer and Dumais, 1997), addition, element-wise multiplication (Mitchell and Lapata, 2008) or tensor product (Smolensky, 1990) of word vectors. In addition to using vector representations, matrices can also be used to represent phrases and the composition process can be done through matrix multiplication (Rudolph and Giesbrecht, 2010; Yessenalina and Cardie, 2011). Recursive neural models utilize the recursive structure (usually a parse tree) of a phrase or sentence for semantic composition. In Recursive Neural Network (Socher et al., 2011), the tree with the least reconstruction error is built and the vectors for interior nodes is composed by a global matrix. Matrix-Vector Recursive Neural Network (MV-RNN) (Socher et al., 2012) assigns matrices for every words so that it could capture the relationship between two children. In Recursive Neural Tensor Networks (RNTN) (Socher et al., 2013b), the composition process is perfo</context>
</contexts>
<marker>Rudolph, Giesbrecht, 2010</marker>
<rawString>Sebastian Rudolph and Eugenie Giesbrecht. 2010. Compositional matrix-space models of language. In ACL, pages 907–916. Association for Computer Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David E Rumelhart</author>
<author>Geoffrey E Hinton</author>
<author>Ronald J Williams</author>
</authors>
<title>Learning representations by backpropagating errors.</title>
<date>1986</date>
<journal>Nature,</journal>
<pages>323--533</pages>
<contexts>
<context position="20289" citStr="Rumelhart et al., 1986" startWordPosition="3439" endWordPosition="3442">566 reviews (train: 8,470, dev: 1,090, test: 2,199). The word vectors were pre-trained on an unlabeled corpus (about 100,000 movie reviews) by word2vec (Mikolov et al., 2013b) as initial values and the other vectors is initialized by sampling from a uniform distribution U(−E, E) where E is 0.01 in our experiments. The dimension of word vectors is 25 for RNN models and 20 for RNTN models. Tanh is chosen as the nonlinearity function. And after computing the output of node i with vi = f(g(vz, vz )), we set vi = vi ||vi ||so that the resulting vector has a limited norm. Backpropagation algorithm (Rumelhart et al., 1986) is used to compute gradients and we use minibatch SGD with momentum as the optimization method, implemented with Theano (Bastien et al., 2012). We trained all our models using stochastic gradient descent with a batch size of 30 examples, momentum of 0.9, L2-regularization weight of 0.0001 and a constant learning rate of 0.005. 5.2 System Comparison We compare our models with several methods which are evaluated on the Sentiment Treebank corpus. The baseline results are reported in (Dong et al., 2014) and (Kim, 2014). We make comparison to the following baselines: • SVM. A SVM model with bag-of</context>
</contexts>
<marker>Rumelhart, Hinton, Williams, 1986</marker>
<rawString>David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. 1986. Learning representations by backpropagating errors. Nature, 323:533–536.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Smolensky</author>
</authors>
<title>Tensor product variable binding and the representation of symbolic structures in connectionist systems.</title>
<date>1990</date>
<journal>Artificial intelligence,</journal>
<volume>46</volume>
<issue>1</issue>
<contexts>
<context position="8637" citStr="Smolensky, 1990" startWordPosition="1362" endWordPosition="1363">yms with similar contexts, neural word vectors (Bengio et al., 2003) are proposed and can be learnt in an unsupervised manner. Word2vec (Mikolov et al., 2013a) introduces a simpler network structure making computation more efficiently and makes billions of samples feasible for training. Semantic composition deals with representing a longer text from its shorter components, which is extensively studied recently. In many previous works, a phrase vector is usually obtained by average (Landauer and Dumais, 1997), addition, element-wise multiplication (Mitchell and Lapata, 2008) or tensor product (Smolensky, 1990) of word vectors. In addition to using vector representations, matrices can also be used to represent phrases and the composition process can be done through matrix multiplication (Rudolph and Giesbrecht, 2010; Yessenalina and Cardie, 2011). Recursive neural models utilize the recursive structure (usually a parse tree) of a phrase or sentence for semantic composition. In Recursive Neural Network (Socher et al., 2011), the tree with the least reconstruction error is built and the vectors for interior nodes is composed by a global matrix. Matrix-Vector Recursive Neural Network (MV-RNN) (Socher e</context>
</contexts>
<marker>Smolensky, 1990</marker>
<rawString>Paul Smolensky. 1990. Tensor product variable binding and the representation of symbolic structures in connectionist systems. Artificial intelligence, 46(1):159–216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Jeffrey Pennington</author>
<author>Eric H Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Semi-supervised recursive autoencoders for predicting sentiment distributions.</title>
<date>2011</date>
<booktitle>In EMNLP,</booktitle>
<pages>151--161</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3131" citStr="Socher et al., 2011" startWordPosition="480" endWordPosition="483">ght node ‘very interesting’. It’s worth to mention again, the composition process is conducted with the syntactic structure of the text, making RNN more interpretable than other deep learning models. very interesting Figure 1: The example process of vector composition in RNN. The vector of node ‘very interesting’ is composed from the vectors of node ‘very’ and node ‘interesting’. Similarly, the node ‘is very interesting’ is composed from the phrase node ‘very interesting’ and the word node ‘is’ . There are various attempts to design the composition function in RNN (or related models). In RNN (Socher et al., 2011), a global matrix is used to linearly combine the elements of vectors. In RNTN (Socher et al., 2013b), a global tensor is used to compute the tensor products of dimensions to favor the association between different elsoftmax is ... softmax g is very interesting softmax g softmax very interesting softmax 1365 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1365–1374, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics ements of the vectors. Some</context>
<context position="9057" citStr="Socher et al., 2011" startWordPosition="1425" endWordPosition="1428">n many previous works, a phrase vector is usually obtained by average (Landauer and Dumais, 1997), addition, element-wise multiplication (Mitchell and Lapata, 2008) or tensor product (Smolensky, 1990) of word vectors. In addition to using vector representations, matrices can also be used to represent phrases and the composition process can be done through matrix multiplication (Rudolph and Giesbrecht, 2010; Yessenalina and Cardie, 2011). Recursive neural models utilize the recursive structure (usually a parse tree) of a phrase or sentence for semantic composition. In Recursive Neural Network (Socher et al., 2011), the tree with the least reconstruction error is built and the vectors for interior nodes is composed by a global matrix. Matrix-Vector Recursive Neural Network (MV-RNN) (Socher et al., 2012) assigns matrices for every words so that it could capture the relationship between two children. In Recursive Neural Tensor Networks (RNTN) (Socher et al., 2013b), the composition process is performed on a parse tree in which every node is annotated with fine-grained sentiment labels, and a global tensor is used for composition. Adaptive MultiCompositionality (Dong et al., 2014) uses multiple weighted co</context>
<context position="21115" citStr="Socher et al., 2011" startWordPosition="3576" endWordPosition="3579">ith a batch size of 30 examples, momentum of 0.9, L2-regularization weight of 0.0001 and a constant learning rate of 0.005. 5.2 System Comparison We compare our models with several methods which are evaluated on the Sentiment Treebank corpus. The baseline results are reported in (Dong et al., 2014) and (Kim, 2014). We make comparison to the following baselines: • SVM. A SVM model with bag-of-words representation (Pang and Lee, 2008). • MNB/bi-MNB. Multinomial Naive Bayes and its bigram variant, adopted from (Wang and Manning, 2012). • RNN. The first Recursive Neural Network model proposed by (Socher et al., 2011). • MV-RNN. Matrix Vector Recursive Neural Network (Socher et al., 2012) represents each word and phrase with a vector and a matrix. As reported, this model suffers from too many parameters. • RNTN. Recursive Neural Tenser Network (Socher et al., 2013b) employs a tensor Method Fine-grained Pos./Neg. SVM 40.7 79.4 MNB 41.0 81.8 bi-MNB 41.9 83.1 RNN 43.2 82.4 MV-RNN 44.4 82.9 RNTN 45.7 85.4 AdaMC-RNN 45.8 87.1 AdaMC-RNTN 46.7 88.5 DRNN 49.8 87.7 TG-RNN (ours) 47.0 86.3 TE-RNN (ours) 48.0 86.8 TE-RNTN (ours) 48.9 87.7 CNN 48.0 88.1 DCNN 48.5 86.8 Para-Vec 48.7 87.8 Table 1: Classification accuray</context>
</contexts>
<marker>Socher, Pennington, Huang, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Jeffrey Pennington, Eric H Huang, Andrew Y Ng, and Christopher D Manning. 2011. Semi-supervised recursive autoencoders for predicting sentiment distributions. In EMNLP, pages 151– 161. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic compositionality through recursive matrix-vector spaces.</title>
<date>2012</date>
<booktitle>In EMNLP,</booktitle>
<pages>1201--1211</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2060" citStr="Socher et al., 2012" startWordPosition="297" endWordPosition="300">RNTN obtains the second best result among all the top performing models, and all the proposed models have much less parameters/complexity than their counterparts. 1 Introduction Among a variety of deep learning models for natural language processing, Recursive Neural Network (RNN) may be one of the most popular models. Thanks to the compositional nature of natural text, recursive neural network utilizes the recursive structure of the input such as a phrase or sentence, and has shown to be very effective for many natural language processing tasks including semantic relationship classification (Socher et al., 2012), syntactic parsing (Socher et al., 2013a), sentiment analysis (Socher et al., 2013b), and machine translation (Li et al., 2013). The key component of RNN and its variants is the composition function: how to compose the vector representation for a longer text from the vector of its child words or phrases. For instance, as shown in Figure 2, the vector of ‘is very interesting’ can be composed from the vector of the left node ‘is’ and that of the right node ‘very interesting’. It’s worth to mention again, the composition process is conducted with the syntactic structure of the text, making RNN m</context>
<context position="3923" citStr="Socher et al., 2012" startWordPosition="605" endWordPosition="608">to favor the association between different elsoftmax is ... softmax g is very interesting softmax g softmax very interesting softmax 1365 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1365–1374, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics ements of the vectors. Sometimes it is challenging to find a single function to model the composition process. As an alternative, multiple composition functions can be used. For instance, in MV-RNN (Socher et al., 2012), different matrices is designed for different words though the model is suffered from too much parameters. In AdaMC RNN/RNTN (Dong et al., 2014), a fixed number of composition functions is linearly combined and the weight for each function is adaptively learned. In spite of the success of RNN and its variants, the syntactic knowledge of the text is not yet fully employed in these models. Two ideas are motivated by the example shown in Figure 2: First, the composition function for the noun phrase ‘the movie/NP’ should be different from that for the adjective phrase ‘very interesting/ADJP’ sinc</context>
<context position="9249" citStr="Socher et al., 2012" startWordPosition="1455" endWordPosition="1458">y, 1990) of word vectors. In addition to using vector representations, matrices can also be used to represent phrases and the composition process can be done through matrix multiplication (Rudolph and Giesbrecht, 2010; Yessenalina and Cardie, 2011). Recursive neural models utilize the recursive structure (usually a parse tree) of a phrase or sentence for semantic composition. In Recursive Neural Network (Socher et al., 2011), the tree with the least reconstruction error is built and the vectors for interior nodes is composed by a global matrix. Matrix-Vector Recursive Neural Network (MV-RNN) (Socher et al., 2012) assigns matrices for every words so that it could capture the relationship between two children. In Recursive Neural Tensor Networks (RNTN) (Socher et al., 2013b), the composition process is performed on a parse tree in which every node is annotated with fine-grained sentiment labels, and a global tensor is used for composition. Adaptive MultiCompositionality (Dong et al., 2014) uses multiple weighted composition matrices instead of sharing a single matrix. The employment of syntactic information in RNN is still in its infant. In (Socher et al., 2013a), the part-of-speech tag of child nodes i</context>
<context position="21187" citStr="Socher et al., 2012" startWordPosition="3587" endWordPosition="3590">ht of 0.0001 and a constant learning rate of 0.005. 5.2 System Comparison We compare our models with several methods which are evaluated on the Sentiment Treebank corpus. The baseline results are reported in (Dong et al., 2014) and (Kim, 2014). We make comparison to the following baselines: • SVM. A SVM model with bag-of-words representation (Pang and Lee, 2008). • MNB/bi-MNB. Multinomial Naive Bayes and its bigram variant, adopted from (Wang and Manning, 2012). • RNN. The first Recursive Neural Network model proposed by (Socher et al., 2011). • MV-RNN. Matrix Vector Recursive Neural Network (Socher et al., 2012) represents each word and phrase with a vector and a matrix. As reported, this model suffers from too many parameters. • RNTN. Recursive Neural Tenser Network (Socher et al., 2013b) employs a tensor Method Fine-grained Pos./Neg. SVM 40.7 79.4 MNB 41.0 81.8 bi-MNB 41.9 83.1 RNN 43.2 82.4 MV-RNN 44.4 82.9 RNTN 45.7 85.4 AdaMC-RNN 45.8 87.1 AdaMC-RNTN 46.7 88.5 DRNN 49.8 87.7 TG-RNN (ours) 47.0 86.3 TE-RNN (ours) 48.0 86.8 TE-RNTN (ours) 48.9 87.7 CNN 48.0 88.1 DCNN 48.5 86.8 Para-Vec 48.7 87.8 Table 1: Classification accuray. Fine-grained stands for 5-class prediction and Pos./Neg. means binary </context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Christopher D Manning, and Andrew Y Ng. 2012. Semantic compositionality through recursive matrix-vector spaces. In EMNLP, pages 1201–1211. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>John Bauer</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Parsing with compositional vector grammars.</title>
<date>2013</date>
<booktitle>In ACL,</booktitle>
<pages>455--465</pages>
<institution>Association for Computer Linguistics.</institution>
<contexts>
<context position="2100" citStr="Socher et al., 2013" startWordPosition="303" endWordPosition="306"> all the top performing models, and all the proposed models have much less parameters/complexity than their counterparts. 1 Introduction Among a variety of deep learning models for natural language processing, Recursive Neural Network (RNN) may be one of the most popular models. Thanks to the compositional nature of natural text, recursive neural network utilizes the recursive structure of the input such as a phrase or sentence, and has shown to be very effective for many natural language processing tasks including semantic relationship classification (Socher et al., 2012), syntactic parsing (Socher et al., 2013a), sentiment analysis (Socher et al., 2013b), and machine translation (Li et al., 2013). The key component of RNN and its variants is the composition function: how to compose the vector representation for a longer text from the vector of its child words or phrases. For instance, as shown in Figure 2, the vector of ‘is very interesting’ can be composed from the vector of the left node ‘is’ and that of the right node ‘very interesting’. It’s worth to mention again, the composition process is conducted with the syntactic structure of the text, making RNN more interpretable than other deep learni</context>
<context position="4762" citStr="Socher et al., 2013" startWordPosition="743" endWordPosition="746">ht for each function is adaptively learned. In spite of the success of RNN and its variants, the syntactic knowledge of the text is not yet fully employed in these models. Two ideas are motivated by the example shown in Figure 2: First, the composition function for the noun phrase ‘the movie/NP’ should be different from that for the adjective phrase ‘very interesting/ADJP’ since the two phrases are quite syntactically different. More specifically to sentiment analysis, a noun phrase is much less likely to express sentiment than an adjective phrase. There are two notable works mentioned here: (Socher et al., 2013a) presented to combine the parsing and composition processes, but the purpose is for parsing; (Hermann and Blunsom, 2013) designed composition functions according to the combinatory rules and categories in CCG grammar, however, only marginal improvement against Naive Bayes was reported. Our proposed model, tag guided RNN (TG-RNN), is designed to use the syntactic tag of the parent phrase to guide the composition process from the child nodes. As an example, we design a function for composing noun phrase (NP) and another one for adjective phrase (ADJP). This simple strategy obtains remarkable i</context>
<context position="9410" citStr="Socher et al., 2013" startWordPosition="1481" endWordPosition="1484">ugh matrix multiplication (Rudolph and Giesbrecht, 2010; Yessenalina and Cardie, 2011). Recursive neural models utilize the recursive structure (usually a parse tree) of a phrase or sentence for semantic composition. In Recursive Neural Network (Socher et al., 2011), the tree with the least reconstruction error is built and the vectors for interior nodes is composed by a global matrix. Matrix-Vector Recursive Neural Network (MV-RNN) (Socher et al., 2012) assigns matrices for every words so that it could capture the relationship between two children. In Recursive Neural Tensor Networks (RNTN) (Socher et al., 2013b), the composition process is performed on a parse tree in which every node is annotated with fine-grained sentiment labels, and a global tensor is used for composition. Adaptive MultiCompositionality (Dong et al., 2014) uses multiple weighted composition matrices instead of sharing a single matrix. The employment of syntactic information in RNN is still in its infant. In (Socher et al., 2013a), the part-of-speech tag of child nodes is considered in combining the processes of both composition and parsing. The main purpose is for better parsing by employing RNN, but it is not designed for sent</context>
<context position="16758" citStr="Socher et al., 2013" startWordPosition="2782" endWordPosition="2785"> by the function g(vl i, etl i, vr i , etr i ) = W where tli and tri are tags of the left and the right nodes respectively, etli and etri are tag vectors, and W E Rdx(2de+2d) is the composition matrix. We term this model Tag embedded RNN, TE-RNN for short. very / RB interesting / JJ Figure 4: RNN with tag embedding. There is a tag embedding table, storing vectors for RB, JJ, and ADJP, etc. Then we compose the phrase vector ’very interesting’ from the vectors for ’very’ and ’interesting’, and the tag vectors for RB and JJ. Similarly, this idea can be applied to Recursive Neural Tensor Network (Socher et al., 2013b). In RNTN, the tag vector and the phrase vector can be interweaved together through a tensor. More specifically, the phrase vectors and tag vectors are multiplied by the composed tensor. The composition function changes to the following: g(vl i, etl i, vr i , etri ) ⎡ vli ⎤ T[1:dl ⎡ vli ⎤ + W ⎡ vli ⎤ + b (7) ⎢ ⎢ ⎢ ⎢ ⎣ etl ⎦⎥⎥⎥⎥ ⎢ ⎢ ⎢ ⎢ ⎣ etl ⎦⎥⎥⎥⎥ ⎢ ⎢ ⎢ ⎢ ⎣ etl ⎦⎥⎥⎥⎥ i i i vri vri vri etr etr etr i i i where the variables are similar to those defined in equation 3 and equation 7. We term this model Tag embedded RNTN, TE-RNTN for short. The phrase vectors and tag vectors are used as input to </context>
<context position="21366" citStr="Socher et al., 2013" startWordPosition="3619" endWordPosition="3622">ine results are reported in (Dong et al., 2014) and (Kim, 2014). We make comparison to the following baselines: • SVM. A SVM model with bag-of-words representation (Pang and Lee, 2008). • MNB/bi-MNB. Multinomial Naive Bayes and its bigram variant, adopted from (Wang and Manning, 2012). • RNN. The first Recursive Neural Network model proposed by (Socher et al., 2011). • MV-RNN. Matrix Vector Recursive Neural Network (Socher et al., 2012) represents each word and phrase with a vector and a matrix. As reported, this model suffers from too many parameters. • RNTN. Recursive Neural Tenser Network (Socher et al., 2013b) employs a tensor Method Fine-grained Pos./Neg. SVM 40.7 79.4 MNB 41.0 81.8 bi-MNB 41.9 83.1 RNN 43.2 82.4 MV-RNN 44.4 82.9 RNTN 45.7 85.4 AdaMC-RNN 45.8 87.1 AdaMC-RNTN 46.7 88.5 DRNN 49.8 87.7 TG-RNN (ours) 47.0 86.3 TE-RNN (ours) 48.0 86.8 TE-RNTN (ours) 48.9 87.7 CNN 48.0 88.1 DCNN 48.5 86.8 Para-Vec 48.7 87.8 Table 1: Classification accuray. Fine-grained stands for 5-class prediction and Pos./Neg. means binary prediction which ignores all neutral instances. All the accuracy is at the sentence level (root). for composition function which could model the meaning of longer phrases and capt</context>
</contexts>
<marker>Socher, Bauer, Manning, Ng, 2013</marker>
<rawString>Richard Socher, John Bauer, Christopher D Manning, and Andrew Y Ng. 2013a. Parsing with compositional vector grammars. In ACL, pages 455–465. Association for Computer Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Y Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank. In</title>
<date>2013</date>
<booktitle>EMNLP,</booktitle>
<pages>1631--1642</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2100" citStr="Socher et al., 2013" startWordPosition="303" endWordPosition="306"> all the top performing models, and all the proposed models have much less parameters/complexity than their counterparts. 1 Introduction Among a variety of deep learning models for natural language processing, Recursive Neural Network (RNN) may be one of the most popular models. Thanks to the compositional nature of natural text, recursive neural network utilizes the recursive structure of the input such as a phrase or sentence, and has shown to be very effective for many natural language processing tasks including semantic relationship classification (Socher et al., 2012), syntactic parsing (Socher et al., 2013a), sentiment analysis (Socher et al., 2013b), and machine translation (Li et al., 2013). The key component of RNN and its variants is the composition function: how to compose the vector representation for a longer text from the vector of its child words or phrases. For instance, as shown in Figure 2, the vector of ‘is very interesting’ can be composed from the vector of the left node ‘is’ and that of the right node ‘very interesting’. It’s worth to mention again, the composition process is conducted with the syntactic structure of the text, making RNN more interpretable than other deep learni</context>
<context position="4762" citStr="Socher et al., 2013" startWordPosition="743" endWordPosition="746">ht for each function is adaptively learned. In spite of the success of RNN and its variants, the syntactic knowledge of the text is not yet fully employed in these models. Two ideas are motivated by the example shown in Figure 2: First, the composition function for the noun phrase ‘the movie/NP’ should be different from that for the adjective phrase ‘very interesting/ADJP’ since the two phrases are quite syntactically different. More specifically to sentiment analysis, a noun phrase is much less likely to express sentiment than an adjective phrase. There are two notable works mentioned here: (Socher et al., 2013a) presented to combine the parsing and composition processes, but the purpose is for parsing; (Hermann and Blunsom, 2013) designed composition functions according to the combinatory rules and categories in CCG grammar, however, only marginal improvement against Naive Bayes was reported. Our proposed model, tag guided RNN (TG-RNN), is designed to use the syntactic tag of the parent phrase to guide the composition process from the child nodes. As an example, we design a function for composing noun phrase (NP) and another one for adjective phrase (ADJP). This simple strategy obtains remarkable i</context>
<context position="9410" citStr="Socher et al., 2013" startWordPosition="1481" endWordPosition="1484">ugh matrix multiplication (Rudolph and Giesbrecht, 2010; Yessenalina and Cardie, 2011). Recursive neural models utilize the recursive structure (usually a parse tree) of a phrase or sentence for semantic composition. In Recursive Neural Network (Socher et al., 2011), the tree with the least reconstruction error is built and the vectors for interior nodes is composed by a global matrix. Matrix-Vector Recursive Neural Network (MV-RNN) (Socher et al., 2012) assigns matrices for every words so that it could capture the relationship between two children. In Recursive Neural Tensor Networks (RNTN) (Socher et al., 2013b), the composition process is performed on a parse tree in which every node is annotated with fine-grained sentiment labels, and a global tensor is used for composition. Adaptive MultiCompositionality (Dong et al., 2014) uses multiple weighted composition matrices instead of sharing a single matrix. The employment of syntactic information in RNN is still in its infant. In (Socher et al., 2013a), the part-of-speech tag of child nodes is considered in combining the processes of both composition and parsing. The main purpose is for better parsing by employing RNN, but it is not designed for sent</context>
<context position="16758" citStr="Socher et al., 2013" startWordPosition="2782" endWordPosition="2785"> by the function g(vl i, etl i, vr i , etr i ) = W where tli and tri are tags of the left and the right nodes respectively, etli and etri are tag vectors, and W E Rdx(2de+2d) is the composition matrix. We term this model Tag embedded RNN, TE-RNN for short. very / RB interesting / JJ Figure 4: RNN with tag embedding. There is a tag embedding table, storing vectors for RB, JJ, and ADJP, etc. Then we compose the phrase vector ’very interesting’ from the vectors for ’very’ and ’interesting’, and the tag vectors for RB and JJ. Similarly, this idea can be applied to Recursive Neural Tensor Network (Socher et al., 2013b). In RNTN, the tag vector and the phrase vector can be interweaved together through a tensor. More specifically, the phrase vectors and tag vectors are multiplied by the composed tensor. The composition function changes to the following: g(vl i, etl i, vr i , etri ) ⎡ vli ⎤ T[1:dl ⎡ vli ⎤ + W ⎡ vli ⎤ + b (7) ⎢ ⎢ ⎢ ⎢ ⎣ etl ⎦⎥⎥⎥⎥ ⎢ ⎢ ⎢ ⎢ ⎣ etl ⎦⎥⎥⎥⎥ ⎢ ⎢ ⎢ ⎢ ⎣ etl ⎦⎥⎥⎥⎥ i i i vri vri vri etr etr etr i i i where the variables are similar to those defined in equation 3 and equation 7. We term this model Tag embedded RNTN, TE-RNTN for short. The phrase vectors and tag vectors are used as input to </context>
<context position="21366" citStr="Socher et al., 2013" startWordPosition="3619" endWordPosition="3622">ine results are reported in (Dong et al., 2014) and (Kim, 2014). We make comparison to the following baselines: • SVM. A SVM model with bag-of-words representation (Pang and Lee, 2008). • MNB/bi-MNB. Multinomial Naive Bayes and its bigram variant, adopted from (Wang and Manning, 2012). • RNN. The first Recursive Neural Network model proposed by (Socher et al., 2011). • MV-RNN. Matrix Vector Recursive Neural Network (Socher et al., 2012) represents each word and phrase with a vector and a matrix. As reported, this model suffers from too many parameters. • RNTN. Recursive Neural Tenser Network (Socher et al., 2013b) employs a tensor Method Fine-grained Pos./Neg. SVM 40.7 79.4 MNB 41.0 81.8 bi-MNB 41.9 83.1 RNN 43.2 82.4 MV-RNN 44.4 82.9 RNTN 45.7 85.4 AdaMC-RNN 45.8 87.1 AdaMC-RNTN 46.7 88.5 DRNN 49.8 87.7 TG-RNN (ours) 47.0 86.3 TE-RNN (ours) 48.0 86.8 TE-RNTN (ours) 48.9 87.7 CNN 48.0 88.1 DCNN 48.5 86.8 Para-Vec 48.7 87.8 Table 1: Classification accuray. Fine-grained stands for 5-class prediction and Pos./Neg. means binary prediction which ignores all neutral instances. All the accuracy is at the sentence level (root). for composition function which could model the meaning of longer phrases and capt</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013b. Recursive deep models for semantic compositionality over a sentiment treebank. In EMNLP, pages 1631–1642. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>37</volume>
<issue>1</issue>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D Turney, Patrick Pantel, et al. 2010. From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research, 37(1):141–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sida I Wang</author>
<author>Christopher D Manning</author>
</authors>
<title>Baselines and bigrams: Simple, good sentiment and topic classification.</title>
<date>2012</date>
<booktitle>In ACL,</booktitle>
<pages>90--94</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="21032" citStr="Wang and Manning, 2012" startWordPosition="3562" endWordPosition="3565"> (Bastien et al., 2012). We trained all our models using stochastic gradient descent with a batch size of 30 examples, momentum of 0.9, L2-regularization weight of 0.0001 and a constant learning rate of 0.005. 5.2 System Comparison We compare our models with several methods which are evaluated on the Sentiment Treebank corpus. The baseline results are reported in (Dong et al., 2014) and (Kim, 2014). We make comparison to the following baselines: • SVM. A SVM model with bag-of-words representation (Pang and Lee, 2008). • MNB/bi-MNB. Multinomial Naive Bayes and its bigram variant, adopted from (Wang and Manning, 2012). • RNN. The first Recursive Neural Network model proposed by (Socher et al., 2011). • MV-RNN. Matrix Vector Recursive Neural Network (Socher et al., 2012) represents each word and phrase with a vector and a matrix. As reported, this model suffers from too many parameters. • RNTN. Recursive Neural Tenser Network (Socher et al., 2013b) employs a tensor Method Fine-grained Pos./Neg. SVM 40.7 79.4 MNB 41.0 81.8 bi-MNB 41.9 83.1 RNN 43.2 82.4 MV-RNN 44.4 82.9 RNTN 45.7 85.4 AdaMC-RNN 45.8 87.1 AdaMC-RNTN 46.7 88.5 DRNN 49.8 87.7 TG-RNN (ours) 47.0 86.3 TE-RNN (ours) 48.0 86.8 TE-RNTN (ours) 48.9 8</context>
</contexts>
<marker>Wang, Manning, 2012</marker>
<rawString>Sida I Wang and Christopher D Manning. 2012. Baselines and bigrams: Simple, good sentiment and topic classification. In ACL, pages 90–94. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ainur Yessenalina</author>
<author>Claire Cardie</author>
</authors>
<title>Compositional matrix-space models for sentiment analysis.</title>
<date>2011</date>
<booktitle>In EMNLP,</booktitle>
<pages>172--182</pages>
<institution>Association for Computer Linguistics.</institution>
<contexts>
<context position="8877" citStr="Yessenalina and Cardie, 2011" startWordPosition="1397" endWordPosition="1400">ciently and makes billions of samples feasible for training. Semantic composition deals with representing a longer text from its shorter components, which is extensively studied recently. In many previous works, a phrase vector is usually obtained by average (Landauer and Dumais, 1997), addition, element-wise multiplication (Mitchell and Lapata, 2008) or tensor product (Smolensky, 1990) of word vectors. In addition to using vector representations, matrices can also be used to represent phrases and the composition process can be done through matrix multiplication (Rudolph and Giesbrecht, 2010; Yessenalina and Cardie, 2011). Recursive neural models utilize the recursive structure (usually a parse tree) of a phrase or sentence for semantic composition. In Recursive Neural Network (Socher et al., 2011), the tree with the least reconstruction error is built and the vectors for interior nodes is composed by a global matrix. Matrix-Vector Recursive Neural Network (MV-RNN) (Socher et al., 2012) assigns matrices for every words so that it could capture the relationship between two children. In Recursive Neural Tensor Networks (RNTN) (Socher et al., 2013b), the composition process is performed on a parse tree in which e</context>
</contexts>
<marker>Yessenalina, Cardie, 2011</marker>
<rawString>Ainur Yessenalina and Claire Cardie. 2011. Compositional matrix-space models for sentiment analysis. In EMNLP, pages 172–182. Association for Computer Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>