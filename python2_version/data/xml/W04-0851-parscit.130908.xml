<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000199">
<note confidence="0.539083666666667">
SENSEVAL-3: Third International Workshop on the Evaluation of Systems
for the Semantic Analysis of Text, Barcelona, Spain, July 2004
Association for Computational Linguistics
</note>
<title confidence="0.9980445">
Regularized Least-Squares Classification for Word Sense
Disambiguation
</title>
<author confidence="0.997632">
Marius Popescu
</author>
<affiliation confidence="0.999766">
Department of Computer Science, University of Bucharest
</affiliation>
<address confidence="0.872260333333333">
Str. Academiei 14
70109 Bucharest,
Romania,
</address>
<email confidence="0.996145">
mpopescu@phobos.cs.unibuc.ro
</email>
<sectionHeader confidence="0.995602" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9982799">
The paper describes RLSC-LIN and RLSC-
COMB systems which participated in the
Senseval-3 English lexical sample task. These
systems are based on Regularized Least-Squares
Classification (RLSC) learning method. We
describe the reasons of choosing this method,
how we applied it to word sense disambigua-
tion, what results we obtained on Senseval-
1, Senseval-2 and Senseval-3 data and discuss
some possible improvements.
</bodyText>
<sectionHeader confidence="0.998791" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999941147540983">
Word sense disambiguation can be viewed as
a classification problem and one way to ob-
tain a classifier is by machine learning methods.
Unfortunately, there is no single one universal
good learning procedure. The No Free Lunch
Theorem assures us that we can not design a
good learning algorithm without any assump-
tions about the structure of the problem. So,
we start by trying to find out what are the par-
ticular characteristics of the learning problem
posed by the word sense disambiguation.
In our opinion, one of the most important
particularities of the word sense disambiguation
learning problem, seems to be the dimensional-
ity problem, more specifically the fact that the
number of features is much greater than the
number of training examples. This is clearly
true about data in Senseval-1, Senseval-2 and
Senseval-3. One can argue that this happens
because of the small number of training exam-
ples in these data sets, but we think that this
is an intrinsic propriety of learning task in the
case of word sense disambiguation.
In word sense disambiguation one important
knowledge source is the words that co-occur (in
local or broad context) with the word that had
to be disambiguated, and every different word
that appears in the training examples will be-
come a feature. Increasing the number of train-
ing examples will increase also the number of
different words that appear in the training ex-
amples, and so will increase the number of fea-
tures. Obviously, the rate of growth will not be
the same, but we consider that for any reason-
able number of training examples (reasonable as
the possibility of obtaining these training exam-
ples and as the capacity of processing, learning
from these examples) the dimension of the fea-
ture space will be greater.
Actually, the high dimensionality of the fea-
ture space with respect to the number of exam-
ples is a general scenario of learning in the case
of Natural Language Processing tasks and word
sense disambiguation is one of these examples.
In such situations, when the dimension of
the feature space is greater than the number
of training examples, the potential for over-
fitting is huge and some form of regulariza-
tion is needed. This is the reason why we
chose to use Regularized Least-Squares Classifi-
cation (RLSC) (Rifkin, 2002; Poggio and Smale,
2003), a method of learning based on kernels
and Tikhonov regularization.
In the next section we explain what source
of information we used and how this informa-
tion is transformed into features. In section 3
we briefly describe the RLSC learning algorithm
and in section 4, how we applied this algorithm
for word sense disambiguation and what results
we have obtained. Finally, in section 5, we dis-
cuss some possible improvements.
</bodyText>
<sectionHeader confidence="0.8450845" genericHeader="method">
2 Knowledge Sources and Feature
Space
</sectionHeader>
<bodyText confidence="0.993237333333333">
We follow the common practice (Yarowsky,
1993; Florian and Yarowsky, 2002; Lee and Ng,
2002) to represent the training instances as fea-
ture vectors. This features are derived from var-
ious knowledge sources. We used the following
knowledge sources:
</bodyText>
<listItem confidence="0.875579">
• Local information:
</listItem>
<bodyText confidence="0.905665714285714">
– the word form of words that appear
near the target word in a window of
size 3
– the part-of-speech (POS) tags that ap-
pear near the target word in a window
of size 3
– the lexical form of the target word
</bodyText>
<listItem confidence="0.796333">
– the POS tag of the target word
• Broad context information:
</listItem>
<bodyText confidence="0.9972731">
– the lemmas of all words that appear
in the provided context of target word
(stop words are removed)
In the case of broad context we use the
bag-of-words representation with two weighting
schema. Binary weighting for RLSC-LIN and
term frequency weighting1 for RLSC-COMB.
For stemming we used Porter stem-
mer (Porter, 1980) and for tagging we used
Brill tagger (Brill, 1995).
</bodyText>
<sectionHeader confidence="0.999267" genericHeader="method">
3 RLSC
</sectionHeader>
<bodyText confidence="0.999351222222222">
RLSC (Rifkin, 2002; Poggio and Smale, 2003)
is a learning method that obtains solutions for
binary classification problems via Tikhonov reg-
ularization in a Reproducing Kernel Hilbert
Space using the square loss.
Let 5 = (x1,y1), ... , (xn, yn) be a training
sample with xi E Rd and yi E 1−1, 11 for all i.
The hypothesis space H of RLSC is the set of
functions f : Rd —* R of the form:
</bodyText>
<equation confidence="0.967043333333333">
n
f(x) = cik(x, xi)
i=1
</equation>
<bodyText confidence="0.880411705882353">
with ci E R for all i and k : Rd x Rd —* R
a kernel function (a symmetric positive definite
function) that measures the similarity between
two instances.
RLSC tries to find a function from this hy-
pothesis space that simultaneously has small
empirical error and small norm in Reproduc-
ing Kernel Hilbert Space generated by kernel k.
The resulting minimization problem is:
(yi − f(xi))2 + AIlfI2K
In spite of the complex mathematical tools
used, the resulted learning algorithm is a very
&apos;We didn’t use any kind of smoothing. The weight of
a term is simply the number of time the term appears in
the context divided by the length of the context.
simple one (for details of how this algorithm is
derived see Rifkin, 2002):
</bodyText>
<listItem confidence="0.97626">
• From the training set 5 =
</listItem>
<equation confidence="0.808924666666667">
(x1, y1), ... , (xn, yn) construct the
kernel matrix K
K = (kij)1&lt;i,j&lt;n kij = k(xi,xj)
</equation>
<listItem confidence="0.995659666666667">
• Compute the vector of coefficients c =
(c1, ... , cn)′ by solving the system of lin-
ear equations:
</listItem>
<equation confidence="0.9989565">
(K + nAI)c = y
c = (K + nAI)−1y
</equation>
<bodyText confidence="0.992201">
where y = (y1, ... , yn)′ and I is the iden-
tity matrix of dimension n
</bodyText>
<listItem confidence="0.980994">
• Form the classifier:
</listItem>
<equation confidence="0.982162">
n
f(x) = cik(x, xi)
i=1
</equation>
<bodyText confidence="0.99994675">
The sign(f(x)) will be interpreted as the pre-
dicted label (−1 or +1) to be assigned to in-
stance x, and the magnitude |f(x) |as the con-
fidence in this prediction.
</bodyText>
<sectionHeader confidence="0.9979555" genericHeader="method">
4 Applying RLSC to Word Sense
Disambiguation
</sectionHeader>
<bodyText confidence="0.99985725">
To apply the RLSC learning method we must
take care about some details.
First, RLSC produces a binary classifier and
word sense disambiguation is a multi-class clas-
sification problem. There are a lot of ap-
proaches for combining binary classifiers to
solve multi-class problems. We used one-vs-all
scheme. We trained a different binary classi-
fier for each sense. For a word with m senses
we train m different binary classifiers, each one
being trained to distinguish the examples in a
single class from the examples in all remaining
classes. When a new example had to be classi-
fied, the m classifiers are run, and the classifier
with the highest confidence, which outputs the
largest (most positive) value, is chosen. If more
than one such classifiers exists, than from the
senses output by these classifiers we chose the
one that appears most frequently in the train-
ing set. One advantage of one-vs-all combining
scheme is the fact that it exploits the confidence
(real value) of classifiers produced by RLSC. For
more arguments in favor of one-vs-all see (Rifkin
and Klautau, 2004).
</bodyText>
<equation confidence="0.346228666666667">
1 n
min
fEH n i=1
</equation>
<bodyText confidence="0.91595475">
Second, RLSC needs a kernel function.
Preliminary experiments with Senseval-1 and
Senseval-2 data show us that the best perfor-
mance is obtained by linear kernel. This obser-
vation agrees with the Lee and Ng results (Lee
and Ng, 2002), that in the case of SVM also
have obtained the best performance with linear
kernel. One-vs-all combining scheme requires
comparison of confidences output by different
classifiers, and for an unbiased comparison the
real values produced by classifiers correspond-
ing to different senses of the target word must
be on the same scale. To achieve this goal we
need a normalized version of linear kernel.
Our first system RLSC-LIN used the follow-
ing kernel:
</bodyText>
<equation confidence="0.946146">
&lt; x, y &gt;
k(x, y) =
kxkkyk
</equation>
<bodyText confidence="0.9993004375">
where x and y are two instances (feature vec-
tors), &lt; ·, · &gt; is the dot product on Rd and k · k
is the L2 norm on Rd.
In the case of RLSC-LIN we used a binary
weighting scheme for coding broad context. In
the RLSC-COMB we tried to obtain more in-
formation from broad context and we used a
term frequency weighting scheme. Now, the fea-
ture vectors will have apart form 0 two kind of
values: 1 for features that encode local informa-
tion and much small values (of order of 10−2)
for features encoding broad context. A simple
linear kernel will not work in this case because
its value will be dominated by the similarity of
local contexts. To solve this problem we split
the kernel in two parts:
</bodyText>
<equation confidence="0.9636995">
1 1
k(x,y) = kl(x,y) + kb(x, y)
</equation>
<page confidence="0.358055">
22
</page>
<bodyText confidence="0.973268833333333">
where kl is a linear normalized kernel that uses
only the components of the feature vectors that
encode local information (and have 0/1 values)
and kb is a normalized kernel that uses only the
components of the feature vectors that encode
broad context.
The last detail concerning application of
RLSC is the value of regularization parameter
A. Experimenting on Senseval-1 and Senseval-
2 data sets we establish that small values of A
achieve best performance. In all reported re-
sults we used A = 10−9.
</bodyText>
<note confidence="0.830978">
The results2 of RLSC-LIN and RLSC-
</note>
<footnote confidence="0.776481">
2The coarse-grained score on Senseval-3 for both
RLSC-LIN and RLSC-COMB was 0.784
</footnote>
<table confidence="0.884886666666667">
COMB on Senseval-1, Senseval-2 and Senseval-
3 data are summarized in Table 1.
RLSC-LIN RLSC-COMB
Senseval-1 0.772 0.775
Senseval-2 0.652 0.656
Senseval-3 0.718 0.722
</table>
<tableCaption confidence="0.921562166666667">
Table 1: Fine-grained score for RLSC-LIN and
RLSC-COMB on Senseval data sets
Because RLSC has many points in common
with the well-known Support Vector Machine
(SVM), we list in Table 2 for comparison the
results obtained by SVM with the same kernels.
</tableCaption>
<table confidence="0.99941425">
SVM-LIN SVM-COMB
Senseval-1 0.771 0.773
Senseval-2 0.644 0.642
Senseval-3 0.714 0.708
</table>
<tableCaption confidence="0.8866625">
Table 2: Fine-grained score for SVM-LIN and
SVM-COMB on Senseval data sets
</tableCaption>
<bodyText confidence="0.99721175">
The results are competitive with the state of
the art results reported until now. For exam-
ple the best two results reported until now on
Senseval-2 data are 0.654 (Lee and Ng, 2002)
obtained with SVM and 0.665 (Florian and
Yarowsky, 2002) obtained by classifiers combi-
nation.
The results are especially good if we take into
account the fact that our systems do not use
syntactic information3 while the others do. Lee
and Ng (Lee and Ng, 2002) report a fine-grained
score for SVM of only 0.648 if they do not use
syntactic knowledge source.
These results encourage us to participate
with RLSC-LIN and RLSC-COMB to the
Senseval-3 competition.
</bodyText>
<sectionHeader confidence="0.99639" genericHeader="conclusions">
5 Possible Improvements
</sectionHeader>
<bodyText confidence="0.98995925">
First evident improvement is to incorporate
syntactic information as knowledge source into
our systems.
It is quite possible to substantially improve
the results of RLSC-COMB using a combina-
tion of more adequate kernels (each kernel in the
combination being adequate to the source of in-
formation represented by the part of the feature
3It takes too long to adapt to our systems a parser
(to prepare the data for parsing, parse it with a free
statistical parser and extract useful features from the
parser output)
vector that the kernel uses). For example, we
can use a combination of a linear kernel for local
information a string kernel (Lodhi et al., 2002)
for broad context and a tree kernel (Collins and
Duffy, 2002) for syntactic relations.
Also, instead of using an equal weight for each
kernel in the combination we can use weights4
that reflect the importance for disambiguation
of knowledge source that the kernel uses, or we
can establish the weight of each kernel experi-
mentally by kernel-target alignment (Cristianini
et al., 2002).
</bodyText>
<sectionHeader confidence="0.9989" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999142551020408">
Eric Brill. 1995. Transformation-based error-
driven learning and natural language process-
ing: A case study in part of speech tagging.
Computational Linguistics, 21(4):543–565.
M. Collins and N. Duffy. 2002. Convolution
kernels for natural language. In T. G. Di-
etterich, S. Becker, and Z. Ghahramani, ed-
itors, Advances in Neural Information Pro-
cessing Systems 14, pages 625–632, Cam-
bridge, MA. MIT Press.
N. Cristianini, J. Shawe-Taylor, A. Elisseeff,
and J. Kandola. 2002. On kernel-target
alignment. In T. G. Dietterich, S. Becker, and
Z. Ghahramani, editors, Advances in Neu-
ral Information Processing Systems 14, pages
367–373, Cambridge, MA. MIT Press.
Radu Florian and David Yarowsky. 2002. Mod-
eling consensus: Classifier combination for
word sense disambiguation. In Proceedings of
EMNLP’02, pages 25–32, Philadelphia, PA,
USA.
Yoong Lee and Hwee Ng. 2002. An empirical
evaluation of knowledge sources and learn-
ing algorithms for word sense disambiguation.
In Proceedings of EMNLP’02, pages 41–48,
Philadelphia, PA, USA.
Huma Lodhi, Craig Saunders, John Shawe-
Taylor, Nello Cristianini, and Chris Watkins.
2002. Text classification using string ker-
nels. Journal of Machine Learning Research,
2(February):419–444.
Tomaso Poggio and Steve Smale. 2003. The
mathematics of learning: Dealing with data.
Notices of the American Mathematical Soci-
ety (AMS), 50(5):537–544.
Martin Porter. 1980. An algorithm for suffix
stripping. Program, 14(3):130–137.
Ryan Rifkin and Aldebaro Klautau. 2004. In
4The weights must sum to one
defense of one-vs-all classification. Journal of
Machine Learning Research, 5(January):101–
141.
Ryan Rifkin. 2002. Everything Old Is New
Again: A Fresh Look at Historical Approaches
to Machine Learning. Ph.D. thesis, Mas-
sachusetts Institute of Technology.
David Yarowsky. 1993. One sense per colloca-
tion. In ARPA Human Language Technology
Workshop, pages 266–271, Princeton, USA.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.194785">
<note confidence="0.712021">SENSEVAL-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, Barcelona, Spain, July 2004</note>
<title confidence="0.913473">Association for Computational Linguistics Regularized Least-Squares Classification for Word Sense Disambiguation</title>
<author confidence="0.993771">Marius</author>
<affiliation confidence="0.7305135">Department of Computer Science, University of Str. Academiei</affiliation>
<address confidence="0.843851">70109</address>
<email confidence="0.967454">mpopescu@phobos.cs.unibuc.ro</email>
<abstract confidence="0.999201727272727">paper describes RLSCwhich participated in the Senseval-3 English lexical sample task. These systems are based on Regularized Least-Squares Classification (RLSC) learning method. We describe the reasons of choosing this method, how we applied it to word sense disambiguation, what results we obtained on Senseval- 1, Senseval-2 and Senseval-3 data and discuss some possible improvements.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Transformation-based errordriven learning and natural language processing: A case study in part of speech tagging.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>4</issue>
<contexts>
<context position="4509" citStr="Brill, 1995" startWordPosition="734" endWordPosition="735">t appear near the target word in a window of size 3 – the part-of-speech (POS) tags that appear near the target word in a window of size 3 – the lexical form of the target word – the POS tag of the target word • Broad context information: – the lemmas of all words that appear in the provided context of target word (stop words are removed) In the case of broad context we use the bag-of-words representation with two weighting schema. Binary weighting for RLSC-LIN and term frequency weighting1 for RLSC-COMB. For stemming we used Porter stemmer (Porter, 1980) and for tagging we used Brill tagger (Brill, 1995). 3 RLSC RLSC (Rifkin, 2002; Poggio and Smale, 2003) is a learning method that obtains solutions for binary classification problems via Tikhonov regularization in a Reproducing Kernel Hilbert Space using the square loss. Let 5 = (x1,y1), ... , (xn, yn) be a training sample with xi E Rd and yi E 1−1, 11 for all i. The hypothesis space H of RLSC is the set of functions f : Rd —* R of the form: n f(x) = cik(x, xi) i=1 with ci E R for all i and k : Rd x Rd —* R a kernel function (a symmetric positive definite function) that measures the similarity between two instances. RLSC tries to find a functi</context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>Eric Brill. 1995. Transformation-based errordriven learning and natural language processing: A case study in part of speech tagging. Computational Linguistics, 21(4):543–565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>N Duffy</author>
</authors>
<title>Convolution kernels for natural language. In</title>
<date>2002</date>
<booktitle>Advances in Neural Information Processing Systems 14,</booktitle>
<pages>625--632</pages>
<editor>T. G. Dietterich, S. Becker, and Z. Ghahramani, editors,</editor>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA.</location>
<marker>Collins, Duffy, 2002</marker>
<rawString>M. Collins and N. Duffy. 2002. Convolution kernels for natural language. In T. G. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information Processing Systems 14, pages 625–632, Cambridge, MA. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Cristianini</author>
<author>J Shawe-Taylor</author>
<author>A Elisseeff</author>
<author>J Kandola</author>
</authors>
<title>On kernel-target alignment. In</title>
<date>2002</date>
<booktitle>Advances in Neural Information Processing Systems 14,</booktitle>
<pages>367--373</pages>
<editor>T. G. Dietterich, S. Becker, and Z. Ghahramani, editors,</editor>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA.</location>
<marker>Cristianini, Shawe-Taylor, Elisseeff, Kandola, 2002</marker>
<rawString>N. Cristianini, J. Shawe-Taylor, A. Elisseeff, and J. Kandola. 2002. On kernel-target alignment. In T. G. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information Processing Systems 14, pages 367–373, Cambridge, MA. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Florian</author>
<author>David Yarowsky</author>
</authors>
<title>Modeling consensus: Classifier combination for word sense disambiguation.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP’02,</booktitle>
<pages>25--32</pages>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="3673" citStr="Florian and Yarowsky, 2002" startWordPosition="583" endWordPosition="586">se Regularized Least-Squares Classification (RLSC) (Rifkin, 2002; Poggio and Smale, 2003), a method of learning based on kernels and Tikhonov regularization. In the next section we explain what source of information we used and how this information is transformed into features. In section 3 we briefly describe the RLSC learning algorithm and in section 4, how we applied this algorithm for word sense disambiguation and what results we have obtained. Finally, in section 5, we discuss some possible improvements. 2 Knowledge Sources and Feature Space We follow the common practice (Yarowsky, 1993; Florian and Yarowsky, 2002; Lee and Ng, 2002) to represent the training instances as feature vectors. This features are derived from various knowledge sources. We used the following knowledge sources: • Local information: – the word form of words that appear near the target word in a window of size 3 – the part-of-speech (POS) tags that appear near the target word in a window of size 3 – the lexical form of the target word – the POS tag of the target word • Broad context information: – the lemmas of all words that appear in the provided context of target word (stop words are removed) In the case of broad context we use</context>
<context position="10188" citStr="Florian and Yarowsky, 2002" startWordPosition="1746" endWordPosition="1749">core for RLSC-LIN and RLSC-COMB on Senseval data sets Because RLSC has many points in common with the well-known Support Vector Machine (SVM), we list in Table 2 for comparison the results obtained by SVM with the same kernels. SVM-LIN SVM-COMB Senseval-1 0.771 0.773 Senseval-2 0.644 0.642 Senseval-3 0.714 0.708 Table 2: Fine-grained score for SVM-LIN and SVM-COMB on Senseval data sets The results are competitive with the state of the art results reported until now. For example the best two results reported until now on Senseval-2 data are 0.654 (Lee and Ng, 2002) obtained with SVM and 0.665 (Florian and Yarowsky, 2002) obtained by classifiers combination. The results are especially good if we take into account the fact that our systems do not use syntactic information3 while the others do. Lee and Ng (Lee and Ng, 2002) report a fine-grained score for SVM of only 0.648 if they do not use syntactic knowledge source. These results encourage us to participate with RLSC-LIN and RLSC-COMB to the Senseval-3 competition. 5 Possible Improvements First evident improvement is to incorporate syntactic information as knowledge source into our systems. It is quite possible to substantially improve the results of RLSC-COM</context>
</contexts>
<marker>Florian, Yarowsky, 2002</marker>
<rawString>Radu Florian and David Yarowsky. 2002. Modeling consensus: Classifier combination for word sense disambiguation. In Proceedings of EMNLP’02, pages 25–32, Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoong Lee</author>
<author>Hwee Ng</author>
</authors>
<title>An empirical evaluation of knowledge sources and learning algorithms for word sense disambiguation.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP’02,</booktitle>
<pages>41--48</pages>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="3692" citStr="Lee and Ng, 2002" startWordPosition="587" endWordPosition="590"> Classification (RLSC) (Rifkin, 2002; Poggio and Smale, 2003), a method of learning based on kernels and Tikhonov regularization. In the next section we explain what source of information we used and how this information is transformed into features. In section 3 we briefly describe the RLSC learning algorithm and in section 4, how we applied this algorithm for word sense disambiguation and what results we have obtained. Finally, in section 5, we discuss some possible improvements. 2 Knowledge Sources and Feature Space We follow the common practice (Yarowsky, 1993; Florian and Yarowsky, 2002; Lee and Ng, 2002) to represent the training instances as feature vectors. This features are derived from various knowledge sources. We used the following knowledge sources: • Local information: – the word form of words that appear near the target word in a window of size 3 – the part-of-speech (POS) tags that appear near the target word in a window of size 3 – the lexical form of the target word – the POS tag of the target word • Broad context information: – the lemmas of all words that appear in the provided context of target word (stop words are removed) In the case of broad context we use the bag-of-words r</context>
<context position="7552" citStr="Lee and Ng, 2002" startWordPosition="1288" endWordPosition="1291">If more than one such classifiers exists, than from the senses output by these classifiers we chose the one that appears most frequently in the training set. One advantage of one-vs-all combining scheme is the fact that it exploits the confidence (real value) of classifiers produced by RLSC. For more arguments in favor of one-vs-all see (Rifkin and Klautau, 2004). 1 n min fEH n i=1 Second, RLSC needs a kernel function. Preliminary experiments with Senseval-1 and Senseval-2 data show us that the best performance is obtained by linear kernel. This observation agrees with the Lee and Ng results (Lee and Ng, 2002), that in the case of SVM also have obtained the best performance with linear kernel. One-vs-all combining scheme requires comparison of confidences output by different classifiers, and for an unbiased comparison the real values produced by classifiers corresponding to different senses of the target word must be on the same scale. To achieve this goal we need a normalized version of linear kernel. Our first system RLSC-LIN used the following kernel: &lt; x, y &gt; k(x, y) = kxkkyk where x and y are two instances (feature vectors), &lt; ·, · &gt; is the dot product on Rd and k · k is the L2 norm on Rd. In </context>
<context position="10131" citStr="Lee and Ng, 2002" startWordPosition="1737" endWordPosition="1740"> Senseval-3 0.718 0.722 Table 1: Fine-grained score for RLSC-LIN and RLSC-COMB on Senseval data sets Because RLSC has many points in common with the well-known Support Vector Machine (SVM), we list in Table 2 for comparison the results obtained by SVM with the same kernels. SVM-LIN SVM-COMB Senseval-1 0.771 0.773 Senseval-2 0.644 0.642 Senseval-3 0.714 0.708 Table 2: Fine-grained score for SVM-LIN and SVM-COMB on Senseval data sets The results are competitive with the state of the art results reported until now. For example the best two results reported until now on Senseval-2 data are 0.654 (Lee and Ng, 2002) obtained with SVM and 0.665 (Florian and Yarowsky, 2002) obtained by classifiers combination. The results are especially good if we take into account the fact that our systems do not use syntactic information3 while the others do. Lee and Ng (Lee and Ng, 2002) report a fine-grained score for SVM of only 0.648 if they do not use syntactic knowledge source. These results encourage us to participate with RLSC-LIN and RLSC-COMB to the Senseval-3 competition. 5 Possible Improvements First evident improvement is to incorporate syntactic information as knowledge source into our systems. It is quite </context>
</contexts>
<marker>Lee, Ng, 2002</marker>
<rawString>Yoong Lee and Hwee Ng. 2002. An empirical evaluation of knowledge sources and learning algorithms for word sense disambiguation. In Proceedings of EMNLP’02, pages 41–48, Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huma Lodhi</author>
<author>Craig Saunders</author>
<author>John ShaweTaylor</author>
<author>Nello Cristianini</author>
<author>Chris Watkins</author>
</authors>
<title>Text classification using string kernels.</title>
<date>2002</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>2--419</pages>
<marker>Lodhi, Saunders, ShaweTaylor, Cristianini, Watkins, 2002</marker>
<rawString>Huma Lodhi, Craig Saunders, John ShaweTaylor, Nello Cristianini, and Chris Watkins. 2002. Text classification using string kernels. Journal of Machine Learning Research, 2(February):419–444.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomaso Poggio</author>
<author>Steve Smale</author>
</authors>
<title>The mathematics of learning: Dealing with data.</title>
<date>2003</date>
<journal>Notices of the American Mathematical Society (AMS),</journal>
<volume>50</volume>
<issue>5</issue>
<contexts>
<context position="3136" citStr="Poggio and Smale, 2003" startWordPosition="496" endWordPosition="499">g from these examples) the dimension of the feature space will be greater. Actually, the high dimensionality of the feature space with respect to the number of examples is a general scenario of learning in the case of Natural Language Processing tasks and word sense disambiguation is one of these examples. In such situations, when the dimension of the feature space is greater than the number of training examples, the potential for overfitting is huge and some form of regularization is needed. This is the reason why we chose to use Regularized Least-Squares Classification (RLSC) (Rifkin, 2002; Poggio and Smale, 2003), a method of learning based on kernels and Tikhonov regularization. In the next section we explain what source of information we used and how this information is transformed into features. In section 3 we briefly describe the RLSC learning algorithm and in section 4, how we applied this algorithm for word sense disambiguation and what results we have obtained. Finally, in section 5, we discuss some possible improvements. 2 Knowledge Sources and Feature Space We follow the common practice (Yarowsky, 1993; Florian and Yarowsky, 2002; Lee and Ng, 2002) to represent the training instances as feat</context>
<context position="4561" citStr="Poggio and Smale, 2003" startWordPosition="741" endWordPosition="744"> of size 3 – the part-of-speech (POS) tags that appear near the target word in a window of size 3 – the lexical form of the target word – the POS tag of the target word • Broad context information: – the lemmas of all words that appear in the provided context of target word (stop words are removed) In the case of broad context we use the bag-of-words representation with two weighting schema. Binary weighting for RLSC-LIN and term frequency weighting1 for RLSC-COMB. For stemming we used Porter stemmer (Porter, 1980) and for tagging we used Brill tagger (Brill, 1995). 3 RLSC RLSC (Rifkin, 2002; Poggio and Smale, 2003) is a learning method that obtains solutions for binary classification problems via Tikhonov regularization in a Reproducing Kernel Hilbert Space using the square loss. Let 5 = (x1,y1), ... , (xn, yn) be a training sample with xi E Rd and yi E 1−1, 11 for all i. The hypothesis space H of RLSC is the set of functions f : Rd —* R of the form: n f(x) = cik(x, xi) i=1 with ci E R for all i and k : Rd x Rd —* R a kernel function (a symmetric positive definite function) that measures the similarity between two instances. RLSC tries to find a function from this hypothesis space that simultaneously ha</context>
</contexts>
<marker>Poggio, Smale, 2003</marker>
<rawString>Tomaso Poggio and Steve Smale. 2003. The mathematics of learning: Dealing with data. Notices of the American Mathematical Society (AMS), 50(5):537–544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Porter</author>
</authors>
<title>An algorithm for suffix stripping.</title>
<date>1980</date>
<journal>Program,</journal>
<volume>14</volume>
<issue>3</issue>
<contexts>
<context position="4458" citStr="Porter, 1980" startWordPosition="725" endWordPosition="726">s: • Local information: – the word form of words that appear near the target word in a window of size 3 – the part-of-speech (POS) tags that appear near the target word in a window of size 3 – the lexical form of the target word – the POS tag of the target word • Broad context information: – the lemmas of all words that appear in the provided context of target word (stop words are removed) In the case of broad context we use the bag-of-words representation with two weighting schema. Binary weighting for RLSC-LIN and term frequency weighting1 for RLSC-COMB. For stemming we used Porter stemmer (Porter, 1980) and for tagging we used Brill tagger (Brill, 1995). 3 RLSC RLSC (Rifkin, 2002; Poggio and Smale, 2003) is a learning method that obtains solutions for binary classification problems via Tikhonov regularization in a Reproducing Kernel Hilbert Space using the square loss. Let 5 = (x1,y1), ... , (xn, yn) be a training sample with xi E Rd and yi E 1−1, 11 for all i. The hypothesis space H of RLSC is the set of functions f : Rd —* R of the form: n f(x) = cik(x, xi) i=1 with ci E R for all i and k : Rd x Rd —* R a kernel function (a symmetric positive definite function) that measures the similarity</context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>Martin Porter. 1980. An algorithm for suffix stripping. Program, 14(3):130–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan Rifkin</author>
<author>Aldebaro Klautau</author>
</authors>
<title>In 4The weights must sum to one defense of one-vs-all classification.</title>
<date>2004</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>5</volume>
<pages>141</pages>
<contexts>
<context position="7300" citStr="Rifkin and Klautau, 2004" startWordPosition="1243" endWordPosition="1246">tinguish the examples in a single class from the examples in all remaining classes. When a new example had to be classified, the m classifiers are run, and the classifier with the highest confidence, which outputs the largest (most positive) value, is chosen. If more than one such classifiers exists, than from the senses output by these classifiers we chose the one that appears most frequently in the training set. One advantage of one-vs-all combining scheme is the fact that it exploits the confidence (real value) of classifiers produced by RLSC. For more arguments in favor of one-vs-all see (Rifkin and Klautau, 2004). 1 n min fEH n i=1 Second, RLSC needs a kernel function. Preliminary experiments with Senseval-1 and Senseval-2 data show us that the best performance is obtained by linear kernel. This observation agrees with the Lee and Ng results (Lee and Ng, 2002), that in the case of SVM also have obtained the best performance with linear kernel. One-vs-all combining scheme requires comparison of confidences output by different classifiers, and for an unbiased comparison the real values produced by classifiers corresponding to different senses of the target word must be on the same scale. To achieve this</context>
</contexts>
<marker>Rifkin, Klautau, 2004</marker>
<rawString>Ryan Rifkin and Aldebaro Klautau. 2004. In 4The weights must sum to one defense of one-vs-all classification. Journal of Machine Learning Research, 5(January):101– 141.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan Rifkin</author>
</authors>
<title>Everything Old Is New Again: A Fresh Look at Historical Approaches to Machine Learning.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>Massachusetts Institute of Technology.</institution>
<contexts>
<context position="3111" citStr="Rifkin, 2002" startWordPosition="494" endWordPosition="495">ssing, learning from these examples) the dimension of the feature space will be greater. Actually, the high dimensionality of the feature space with respect to the number of examples is a general scenario of learning in the case of Natural Language Processing tasks and word sense disambiguation is one of these examples. In such situations, when the dimension of the feature space is greater than the number of training examples, the potential for overfitting is huge and some form of regularization is needed. This is the reason why we chose to use Regularized Least-Squares Classification (RLSC) (Rifkin, 2002; Poggio and Smale, 2003), a method of learning based on kernels and Tikhonov regularization. In the next section we explain what source of information we used and how this information is transformed into features. In section 3 we briefly describe the RLSC learning algorithm and in section 4, how we applied this algorithm for word sense disambiguation and what results we have obtained. Finally, in section 5, we discuss some possible improvements. 2 Knowledge Sources and Feature Space We follow the common practice (Yarowsky, 1993; Florian and Yarowsky, 2002; Lee and Ng, 2002) to represent the t</context>
<context position="4536" citStr="Rifkin, 2002" startWordPosition="739" endWordPosition="740">rd in a window of size 3 – the part-of-speech (POS) tags that appear near the target word in a window of size 3 – the lexical form of the target word – the POS tag of the target word • Broad context information: – the lemmas of all words that appear in the provided context of target word (stop words are removed) In the case of broad context we use the bag-of-words representation with two weighting schema. Binary weighting for RLSC-LIN and term frequency weighting1 for RLSC-COMB. For stemming we used Porter stemmer (Porter, 1980) and for tagging we used Brill tagger (Brill, 1995). 3 RLSC RLSC (Rifkin, 2002; Poggio and Smale, 2003) is a learning method that obtains solutions for binary classification problems via Tikhonov regularization in a Reproducing Kernel Hilbert Space using the square loss. Let 5 = (x1,y1), ... , (xn, yn) be a training sample with xi E Rd and yi E 1−1, 11 for all i. The hypothesis space H of RLSC is the set of functions f : Rd —* R of the form: n f(x) = cik(x, xi) i=1 with ci E R for all i and k : Rd x Rd —* R a kernel function (a symmetric positive definite function) that measures the similarity between two instances. RLSC tries to find a function from this hypothesis spa</context>
</contexts>
<marker>Rifkin, 2002</marker>
<rawString>Ryan Rifkin. 2002. Everything Old Is New Again: A Fresh Look at Historical Approaches to Machine Learning. Ph.D. thesis, Massachusetts Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>One sense per collocation.</title>
<date>1993</date>
<booktitle>In ARPA Human Language Technology Workshop,</booktitle>
<pages>266--271</pages>
<location>Princeton, USA.</location>
<contexts>
<context position="3645" citStr="Yarowsky, 1993" startWordPosition="581" endWordPosition="582">hy we chose to use Regularized Least-Squares Classification (RLSC) (Rifkin, 2002; Poggio and Smale, 2003), a method of learning based on kernels and Tikhonov regularization. In the next section we explain what source of information we used and how this information is transformed into features. In section 3 we briefly describe the RLSC learning algorithm and in section 4, how we applied this algorithm for word sense disambiguation and what results we have obtained. Finally, in section 5, we discuss some possible improvements. 2 Knowledge Sources and Feature Space We follow the common practice (Yarowsky, 1993; Florian and Yarowsky, 2002; Lee and Ng, 2002) to represent the training instances as feature vectors. This features are derived from various knowledge sources. We used the following knowledge sources: • Local information: – the word form of words that appear near the target word in a window of size 3 – the part-of-speech (POS) tags that appear near the target word in a window of size 3 – the lexical form of the target word – the POS tag of the target word • Broad context information: – the lemmas of all words that appear in the provided context of target word (stop words are removed) In the </context>
</contexts>
<marker>Yarowsky, 1993</marker>
<rawString>David Yarowsky. 1993. One sense per collocation. In ARPA Human Language Technology Workshop, pages 266–271, Princeton, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>