<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.993607">
Efficient Transformation-Based Parsing
</title>
<author confidence="0.85124">
Giorgio Satta Eric Brill
</author>
<note confidence="0.764878">
Dipartimento di Elettronica ed Informatica Department of Computer Science
Universita di Padova Johns Hopkins University
via Gradenigo, 6/A Baltimore, MD 21218-2694
</note>
<address confidence="0.899138">
1-35131 Padova, Italy brillOcs.jhu.edu
</address>
<email confidence="0.906237">
sattaOdei.unipd.it
</email>
<sectionHeader confidence="0.995783" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997889">
In transformation-based parsing, a finite
sequence of tree rewriting rules are checked
for application to an input structure. Since
in practice only a small percentage of rules
are applied to any particular structure, the
naive parsing algorithm is rather ineffi-
cient. We exploit this sparseness in rule
applications to derive an algorithm two to
three orders of magnitude faster than the
standard parsing algorithm.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99989695945946">
The idea of using transformational rules in natu-
ral language analysis dates back at least to Chom-
sky, who attempted to define a set of transfor-
mations that would apply to a word sequence to
map it from deep structure to surface structure
(see (Chomsky, 1965)). Transformations have also
been used in much of generative phonology to cap-
ture contextual variants in pronunciation, start-
ing with (Chomsky and Halle, 1968). More re-
cently, transformations have been applied to a di-
verse set of problems, including part of speech
tagging, pronunciation network creation, preposi-
tional phrase attachment disambiguation, and pars-
ing, under the paradigm of transformation-based
error-driven learning (see (Brill, 1993; Brill, 1995)
and (Brill and Resnik, 1994)). In this paradigm,
rules can be learned automatically from a training
corpus, instead of being written by hand.
Transformation-based systems are typically deter-
ministic. Each rule in an ordered list of rules is ap-
plied once wherever it can apply, then is discarded,
and the next rule is processed until the last rule in
the list has been processed. Since for each rule the
application algorithm must check for a matching at
all possible sites to see whether the rule can apply,
these systems run in 0(irpn) time, where 7r is the
number of rules, p is the cost of a single rule match-
ing, and n is the size of the input structure. While
this results in fast processing, it is possible to create
much faster systems. In (Roche and Schabes, 1995),
a method is described for converting a list of trans-
formations that operates on strings into a determin-
istic finite state transducer, resulting in an optimal
tagger in the sense that tagging requires only one
state transition per word, giving a linear time tag-
ger whose run-time is independent of the number
and size of rules.
In this paper we consider transformation-based
parsing, introduced in (Brill, 1993), and we im-
prove upon the °(irion) time upper bound. In
transformation-based parsing, an ordered sequence
of tree-rewriting rules (tree transformations) are ap-
plied to an initial parse structure for an input sen-
tence, to derive the final parse structure. We observe
that in most transformation-based parsers, only a
small percentage of rules are actually applied, for
any particular input sentence. For example, in an
application of the transformation-based parser de-
scribed in (Brill, 1993), 7r = 300 rules were learned,
to be applied at each node of the initial parse struc-
ture, but the average number of rules that are suc-
cessfully applied at each node is only about one. So
a lot of time is spent testing whether the conditions
are met for applying a transformation and finding
out that they are not met. This paper presents an
original algorithm for transformation-based parsing
working in 0(ptlog(t)) time, where t is the total
number of rules applied for an input sentence. Since
in practical cases t is smaller than n and we can
neglect the log(n) factor, we have achieved a time
improvement of a factor of 7r. We emphasize that
7r can be several hundreds large in actual systems
where transformations are lexicalized.
Our result is achieved by preprocessing the trans-
formation list, deriving a finite state, deterministic
tree automaton. The algorithm then exploits the au-
tomaton in a way that obviates the need for checking
the conditions of a rule when that rule will not apply,
thereby greatly improving parsing run-time over the
straightforward parsing algorithm. In a sense, our
algorithm spends time only with rules that can be
applied, as if it knew in advance which rules cannot
be applied during the parsing process.
The remainder of this paper is organized as fol-
</bodyText>
<page confidence="0.995929">
255
</page>
<bodyText confidence="0.987350833333333">
lows. In Section 2 we introduce some preliminaries,
and in Section 3 we provide a representation of trans-
formations that uses finite state, deterministic tree
automata. Our algorithm is then specified in Sec-
tion 4. Finally, in Section 5 we discuss related work
in the existing literature.
</bodyText>
<sectionHeader confidence="0.988838" genericHeader="introduction">
2 Preliminaries
</sectionHeader>
<bodyText confidence="0.997846">
We review in the following subsections some termi-
nology that is used throughout this paper.
</bodyText>
<subsectionHeader confidence="0.843475">
2.1 Trees
</subsectionHeader>
<bodyText confidence="0.786669133333333">
We consider ordered trees whose nodes are assigned
labels over some finite alphabet E; this set is denoted
as ET. Let T E ET. A node of T is called leftmost
if it does not have any left sibling (a root node is
a leftmost node). The height of T is the length
of a longest path from the root to one of its leaves
(a tree composed of a single node has height zero).
We define IT as the number of nodes in T. A tree
T E ET is denoted as A if it consists of a single leaf
node labeled by A, and as A(Ti, T2, ,T), d &gt; 1,
if T has root labeled by A with d (ordered) children
denoted by Sometimes in the examples
we draw trees in the usual way, indicating each node
with its label.
What follows is standard terminology from the
tree pattern matching literature, with the simplifi-
cation that we do not use variable terms. See (Hoff-
mann and O&apos;Donnell, 1982) for general definitions.
Let 71 be a node of T. We say that a tree S matches
T at n if there exists a one-to-one mapping from the
nodes of S to the nodes of T, such that the follow-
ing conditions are all satisfied: (i) if n&apos; maps to n&amp;quot;,
then n&apos; and n&amp;quot; have the same label; (ii) the root of
S maps to n; and (iii) if n&apos; maps to n&amp;quot; and n&apos; is not
a leaf in S, then n&apos; and n&amp;quot; have the same degree and
the i-th child of n&apos; maps to the i-th child of n&amp;quot;. We
say that T and S are equivalent if they match each
other at the respective root nodes. In what follows
trees that are equivalent are not treated as the same
object. We say that a tree T&apos; is a subtree of T at
</bodyText>
<listItem confidence="0.636687181818182">
n if there exists a tree S that matches T at n, and
T&apos; consists of the nodes of T that are matched by
some node of S and the arcs of T between two such
nodes. We also say that T&apos; is matched by S at n. In
addition, T&apos; is a prefix of T if n is the root of T; T&apos;
is the suffix of T at n if T&apos; contains all nodes of T
dominated by n.
Example I Let T = B(D,C(B(D, B), C)) and let
n be the second child of T&apos;s root. S = C(B,C)
matches Tat n. S&apos; = B(D,C(B), C)) is a prefix of S
and S&amp;quot; = C(B(D, B), C) is the suffix of T at n.
</listItem>
<bodyText confidence="0.8869316">
We now introduce a tree replacement operator
that will be used throughout the paper. Let S
be a subtree of T and let S&apos; be a tree having the
same number of leaves as S. Let n1, n2, , ni and
,• n&apos;2, 1 &gt; 1, be all the leaves from left to
</bodyText>
<figure confidence="0.718975">
D E
</figure>
<figureCaption confidence="0.9404715">
Figure 1: From left to right, top to bottom: tree T
with subtree S indicated using underlined labels at
its nodes; tree 8&apos; having the same number of leaves
as S; tree T[S/S1 obtained by &amp;quot;replacing&amp;quot; S with S&apos;.
</figureCaption>
<bodyText confidence="0.910839461538462">
right of S and 5&apos;, respectively. We write T[S Si]
to denote the tree obtained by embedding S&apos; within
T in place of S, through the following steps: (i) if
the root of S is the i-th child of a node ni in T,
the root of S&apos; becomes the i-th child of nf ; and (ii)
the (ordered) children of ni in T, if any, become the
children of n&apos;i, 1 &lt;i &lt; 1. The root of T[S151 is the
root of T if node nf above exists, and is the root of
S&apos; otherwise.
Example 2 Figure 1 depicts trees T, S&apos; and T&apos; in
this order. A subtree S of T is also indicated using
underlined labels at nodes of T. Note that S and
S&apos; have the same number of leaves. Then we have
</bodyText>
<equation confidence="0.494897">
T&apos; = 71,5 I Sq. 0
</equation>
<subsectionHeader confidence="0.996273">
2.2 Tree automata
</subsectionHeader>
<bodyText confidence="0.9826675">
Deterministic (bottom-up) tree automata were first
introduced in (Thatcher, 1967) (called FRT there).
The definition we propose here is a generalization
of the canonical one to trees of any degree. Note
that the transition function below is computed on
a number of states that is independent of the de-
gree of the input tree. Deterministic tree automata
will be used later to implement the bottom-up tree
pattern matching algorithm of (Hoffmann and 0&apos;-
Donnell, 1982).
</bodyText>
<construct confidence="0.9931418">
Definition 1 A deterministic tree automaton (DTA)
is a 5-tuple M = (Q, E, 6, go, F), where Q is a finite
set of states, E is a finite alphabet, go E Q is the
initial state, F c Q is the set of final slates and 6 is
a transition function mapping Q2 x E into Q.
</construct>
<bodyText confidence="0.871107">
Informally, a DTA M walks through a tree T by vis-
iting its nodes in post-order, one node at a time.
Every time a node is read, the current state of
the device is computed on the basis of the states
</bodyText>
<page confidence="0.996615">
256
</page>
<bodyText confidence="0.998743555555556">
reached upon reading the immediate left sibling and
the rightmost child of the current node, if any. In
this way the decision of the DTA is affected not only
by the portion of the tree below the currently read
node, but also by each subtree rooted in a left sib-
ling of the current node. This is formally stated in
what follows. Let T E ET and let n be one of its
nodes, labeled by a. The state reached by M upon
reading n is recursively specified as:
</bodyText>
<equation confidence="0.957106">
6(T, n) = (5(X, X&apos; , a), (1)
</equation>
<bodyText confidence="0.9899494">
where X = qo if n is a leftmost node, X = S(T, n&apos;) if
n&apos; is the immediate left sibling of n; and X&apos; = qo if
n is a leaf node, X&apos; = n&amp;quot;) if n&amp;quot; is the rightmost
child of n. The tree language recognized by M is the
set
</bodyText>
<equation confidence="0.9904305">
L(M) = IT I (T, n) E F, T E ET,
n the root of T}. (2)
</equation>
<bodyText confidence="0.9403885">
Example 3 Consider the infinite set L =
{B(A, C), B(A, B(A, C)), B(A, B(A, B(A, C))), .}
consisting of all right-branching trees with internal
nodes labeled by B and with strings AnC, n &gt; 1
as their yields. Let M = (Q, {A, B, C}, (5, q0,
{qBc}) be a DTA specified as follows: Q = {q0,
qA7 qBC q-1}; 4040, A) = qA, (5(qA, qo,C) =
°(qA, q.Bc , B) = qBc and q_i is the value of all
other entries of 8. It is not difficult to see that
L(M) = L. 0
Observe that when we restrict to monadic trees, that
is trees whose nodes have degree not greater than
one, the above definitions correspond to the well
known formalisms of deterministic finite state au-
tomata, the associated extended transition function,
and the regular languages.
</bodyText>
<subsectionHeader confidence="0.981149">
2.3 Transformation-based parsing
</subsectionHeader>
<bodyText confidence="0.998820294117647">
Transformation-based parsing was first introduced
in (Brill, 1993). Informally, a transformation-based
parser assigns to an input sentence an initial parse
structure, in some uniform way. Then the parser
iteratively checks an ordered sequence of tree trans-
formations for application to the initial parse tree,
in order to derive the final parse structure. This
results in a deterministic, linear time parser. In
order to present our algorithm, we abstract away
from the assignment of the initial parse to the input,
and introduce below the notion of transformation-
based tree rewriting system. The formulation we
give here is inspired by (Kaplan and Kay, 1994)
and (Roche and Schabes, 1995). The relationship
between transformation-based tree rewriting sys-
tems and standard term-rewriting systems will be
discussed in the final section.
</bodyText>
<construct confidence="0.912349">
Definition 2 A transformation-based tree rewriting
system (TTS) is a pair G = (E, R), where E is a
</construct>
<bodyText confidence="0.994186923076923">
finite alphabet and R = (ri, r2, , r,), ir&gt; 1, is
a finite sequence of tree rewriting rules having the
form Q Q&apos;, with Q, Q&apos; E ET and such that Q and
Q&apos; have the same number of leaves.
If r = (Q —+ Q&apos;), we write lhs(r) for Q and rhs(r)
for Q&apos;. We also write lhs(R) for {lhs(r) r E R}.
(Recall that we regard lhs(ri) and lhs(ri), i j, as
different objects, even if these trees are equivalent.)
We define Ir I = I lhs(r) + rhs(r) I.
The notion of transformation associated with a
TTS G = (E, R) is now introduced. Let C, C&apos; E ET.
For any node n of C and any rule r = (Q Q&apos;) of
G, we write
</bodyText>
<equation confidence="0.976034">
C C&apos; (3)
</equation>
<bodyText confidence="0.87727">
if Q does not match C at n and C = C&apos;; or if Q
matches C at n and C&apos; = C[SIQ&apos;e], where S is the
subtree of T matched by Q at n and Q&apos;c is a fresh
copy of Q&apos;. Let (ni , n2, .,n), t &gt; 1, be the post-
ordered sequence of all nodes of C. We write
</bodyText>
<equation confidence="0.753589">
C (4)
</equation>
<bodyText confidence="0.28955725">
if Ci, 1 &lt; i &lt; t, Co = C and Ct =
C&apos;. Finally, we define the translation induced by
G on ET as the map M(G) = {(C, C&amp;quot;) I C E
ET, Ci_17- Ci for 1 &lt; i &lt; Co = C, =
</bodyText>
<sectionHeader confidence="0.882948" genericHeader="method">
3 Rule representation
</sectionHeader>
<bodyText confidence="0.997029526315789">
We develop here a representation of rule sequences
that makes use of DTA and that is at the basis of
the main result of this paper. Our technique im-
proves the preprocessing phase of a bottom-up tree
pattern matching algorithm presented in (Hoffmann
and O&apos;Donnell, 1982), as it will be discussed in the
final section.
Let G= (E, R) be a TTS, R= (ri , r2, , r,). In
what follows we construct a DTA that &amp;quot;detects&amp;quot; each
subtree of an input tree that is equivalent to some
tree in lhs(R). We need to introduce some additional
notation. Let N be the set of all nodes from the trees
in lhs(R). Call N,. the set of all root nodes (in N),
N„, the set of all leftmost nodes, NI the set of all leaf
nodes, and Na the set of all nodes labeled by a E E.
For each q E 2N, let right(q) = In n E N, E
q, n has immediate left sibling n&apos;l and let up(q) =-
In I n E N, E q, n has rightmost child n&apos;}.
Also, let qo be a fresh symbol.
</bodyText>
<construct confidence="0.5150431">
Definition 3 G is associated with a DTA AG =
(21V U {q0},E, SG, go, F), where F = {q I q E
2N, (q n 0} and SG is specified as follows:
(i) (q0, go, a) = Na n N„,„ n NI;
(ii) OG(qo,qi, a) = Nan Nmn(N,U up(g&apos;)), for q&apos;
go,
(iii) 6(q, go, a) = Ne,, fl N1 n (NrU right(q)), for q
qo;
(iv) (5G(q,q&apos; , a) = Na n up(e) n (Nr U right(q)), for
q qo
</construct>
<page confidence="0.986837">
257
</page>
<bodyText confidence="0.999670777777778">
Observe that each state of AG simultaneously car-
ries over the recognition of several suffixes of trees
in lhs(R). These processes are started whenever AG
reads a leftmost node n with the same label as a
leftmost leaf node in some tree in lhs(R) (items (i)
and (ii) in Definition 3). Note also that we do
not require any matching of the left siblings when
we match the root of a tree in lhs(R) (items (iii)
and (iv)).
</bodyText>
<figure confidence="0.999379571428572">
A
A
CCA
A
A B
A B
A B
</figure>
<figureCaption confidence="0.9945585">
Figure 2: From top to bottom: rules r1, r2 and r3
of G.
</figureCaption>
<bodyText confidence="0.99254">
Example 4 Let G = (E, R), where E = {A, B,
C, D} and R = (ri, r2, r3). Rules ri are depicted
in Figure 2. We write nii to denote the j-th node
in a post-order enumeration of the nodes of lhs(r),
1 &lt; i &lt; 3 and 1 &lt; j &lt; 5. (Therefore n35 denotes the
root node of lhs(r3) and n22 denotes the first child
of the second child of the root node of lhs(r2).) If we
consider only the useful states, that is those states
that can be reached on an actual input, the DTA
AG = (Q, E, 6, go, F), is specified as follows: Q =
{qi I 0 &lt; i &lt; 11}, where q1 = Inii,n12, n22) n321)
q2 = {n21,77,31}, q3 = {n13, n23}7 gel = 0331 q5
Ini41, q6 = {n24}, (17 = {n34, q = 0151, go =
Inssl, qio = {n25}, qii = 0; F = {q8, go, gio}. The
transition function 6, restricted to the useful states,
is specified in Figure 3. Note that among the 215+ 1
possible states, only 12 are useful. 0
</bodyText>
<construct confidence="0.640784">
6(0,0, A) = qi 6(90,0,C) = 92
8(qi , B) = q3 O(qi , , C) = Q4
5(gi , g3, B) = Q5 6(0, q3, B) = qfi
8(92, q4, B) = 97 40, q5, B) = qio
6(q0 ,q6, B) = q9 .6(go, q7, B) =
</construct>
<figureCaption confidence="0.9706925">
Figure 3: Transition function of G. For all (q,q&apos; , a) E
Q2 x E not indicated above, 6(g , g&apos; , a) =
</figureCaption>
<bodyText confidence="0.995495">
Although the number of states of AG is exponen-
tial in I NI, in practical cases most of these states
are never reached by the automaton on an actual
input, and can therefore be ignored. This happens
whenever there are few pairs of suffix trees of trees
in lhs(R) that share a common prefix tree but no
tree in the pair matches the other at the root node.
This is discussed at length in (Hoffmann and O&apos;Don-
nell, 1982), where an upper bound on the number of
useful states is provided.
The following lemma provides a characterization
of AG that will be used later.
</bodyText>
<construct confidence="0.794686">
Lemma 1 Lei n be a node of T E ET and let n&apos; be
the root node of r E R. Tree lhs(r) matches T at n
if and only if n&apos; E 6G(T,n).
</construct>
<bodyText confidence="0.97534225">
Proof (outline). The statement can be shown by
proving the following claim. Let m be a node in T
and m&apos; be a node in lhs(r). Call mi, , mk = m,
k &gt; 1, the ordered sequence of the left siblings of m,
with m included, and call mc. , , m&apos;k, = rn&apos;, k&apos; &gt; 1,
the ordered sequence of the left siblings of m&apos;, with
m&apos; included. If m&apos; N,., then the two following
conditions are equivalent:
</bodyText>
<listItem confidence="0.971350333333333">
• m&apos; E m);
• k = k&apos; and, for 1 &lt;i &lt; k, the suffix of lhs(r) at
rn matches T at mi.
</listItem>
<bodyText confidence="0.999415857142857">
The claim can be shown by induction on the posi-
tion of m&apos; in a post-order enumeration of the nodes
of lhs(r). The lemma then follows from the spec-
ification of set F and the treatment of set N,. in
items (iii) and (iv) in Definition 3. 0
We also need a function mapping F x {1..(r+ 1)}
into {1..7r} U {I}, specified as (mini!) =1):
</bodyText>
<equation confidence="0.555161">
next(q,i) = mint.? i &lt;j &lt; 7r, lhs(r) has
root node in ql. (5)
</equation>
<bodyText confidence="0.8112442">
Assume that q E F is reached by AG upon reading a
node n (in some tree). In the next section next(9, i)
is used to select the index of the rule that should be
next applied at node n, after the first i — 1 rules of
R have been considered.
</bodyText>
<sectionHeader confidence="0.982479" genericHeader="method">
4 The algorithm
</sectionHeader>
<bodyText confidence="0.99999125">
We present a translation algorithm for TTS that
can immediately be converted into a transformation-
based parsing algorithm. We use all definitions in-
troduced in the previous sections. To simplify the
presentation, we first make the assumption that the
order in which we apply several instances of the same
rule to a given tree does not affect the outcome.
Later we will deal with the general case.
</bodyText>
<subsectionHeader confidence="0.593386">
4.1 Order-free case
</subsectionHeader>
<bodyText confidence="0.99898525">
We start with an important property that is used
by the algorithm below and that can be easily shown
(see also (Hoffmann and O&apos;Donnell, 1982)). Let G =
(E, R) be a TTS and let hG be the maximum height
</bodyText>
<page confidence="0.986164">
258
</page>
<bodyText confidence="0.772688625">
of a tree in lhs(R). Given trees T and S, S a subtree
of T, we write local(T, S) to denote the set of all
nodes of S and the first hG proper ancestors of the
root of S in T (when these nodes are defined).
Lemma 2 Assume that Ihs(r), r E R, matches a
tree T at some node n. Let T T&apos; and let S be the
copy of rhs(r) used in the rewriting. For every node
n&apos; not included in local(T&apos;,S), we have &apos;6G(T,n&apos;) =
</bodyText>
<equation confidence="0.837473">
G (T&apos; , n&apos;). 0
</equation>
<bodyText confidence="0.997225063829787">
We precede the specification of the method with
an informal presentation. The following three data
structures are used. An associative list state asso-
ciates each node n of the rewritten input tree with
the state reached by AG upon reading n. If n is
no longer a node of the rewritten input tree, state
associates n with the emptyset. A set rule(i) is as-
sociated with each rule ri, containing some of the
nodes of the rewritten input tree at which lhs(ri)
matches. A heap data structure H is also used to
order the indices of the non-empty sets rule(i) ac-
cording to the priority of the associated rules in the
rule sequence. All the above data structures are up-
dated by a procedure called update.
To compute the translation M(G) we first visit
the input tree with AG and initialize our data struc-
tures in the following way. For each node n, state is
assigned a state of AG as specified above. If rule ri
must be applied first at n, n is added to rule(i) and
H is updated. We then enter a main loop and re-
trieve elements from the heap. When i is retrieved,
rule ri is considered for application at each node
n in rule(i). It is important to observe that, since
some rewriting of the input tree might have occurred
in between the time n has been inserted in rule(i)
and the time i is retrieved from H, it could be that
the current rule ri can no longer be applied at n.
Information in state is used to detect these cases.
Crucial to the efficiency of our algorithm, each time
a rule is applied only a small portion of the current
tree needs to be reread by AG, in order to update
our data structures, as specified by Lemma 2 above.
Finally, the main loop is exited when the heap is
empty.
Algorithm 1 Let G = (E, 10 be a TTS, R =
r2, , rir). and let T E ET be an input tree.
Let AG = (2-1v U {go}, E, 6G, go, F) be the DTA as-
sociated with G and SG the reached state function.
Let also i be an integer valued variable, state be an
associative array, rule(i) be an initially empty set,
for 1 &lt; i &lt; r, and let H be a heap data structure.
(n rule(i) adds n to rule(i); i —÷ H inserts i in H;
i H assigns to i the least element in H, if H is not
empty.) The algorithm is specified in Figure 4. 0
Example 4 (continued) We describe a run of Al-
gorithm 1 working with the sample TTS G = (E, R)
previously specified (see Figure 2).
</bodyText>
<equation confidence="0.90980492">
proc update(oldset, newset, j)
for each node n E oldset
state(n) 4-4 0
for each node n E newset do
state(n) 4-- CSG(C, n)
if state(n) E F and next(state(n), j) 01 then do
if rule(next(state(n), j)) = 0
then next(state(n), j) —+ H
n —+ rule(next(state(n), j))
od
od
main
C T; i 4- 1
update(0, nodes of C, i)
while H not empty do
i +— H
for each node n E rule(i) s.t. the root of lhs(r.)
is in state(n) do
S 4- the subtree of C matched by lhs(r.) at
S&apos; ■— copy of rhs(rt)
C C[57451
update(nodes of S, local(C, S&apos;),i 1)
od
od
return C.
</equation>
<figureCaption confidence="0.9787805">
Figure 4: Translation algorithm computing M(G)
for a TTS G.
</figureCaption>
<bodyText confidence="0.998351862068965">
Let Ci E ET, 1 &lt; i &lt; 3, be as depicted in Figure 5.
We write mii to denote the j-th node in a post-
order enumeration of the nodes of C, 1 &lt;i &lt; 3 and
1 &lt;j &lt;7. Assume that C1 is the input tree.
After the first call to procedure update, we have
state(m17) = gio = {n25} and state(mis) = Q8 =
{n15}; no other final state is associated with a node
of C1. We also have that rule(1) =
{mis}, rule(2) =
{m17}, rule(3) = 0 and H contains indices 1 and 2.
Index 1 is then retrieved from H and the only
node in rule(1), i.e., m16, is considered. Since the
root of lhs(ri), i.e., node n15, belongs to 0, rn16
passes the test in the head of the for-statement in
the main program. Then r1 is applied to C1, yielding
C2. Observe that mil = m21 and m17 = 1n27; all
the remaining nodes of C2 are fresh nodes.
The next call to update, associated with the appli-
cation of r1, updates the associative list state in such
a way that state(m27) = g9 = {n35}, and no other
final state is associated with a node of C2. Also, we
now have rule(1) = frnisl, rule(2) = {m27} (recall
that 1fl17 = m27), rule(3) = {m27}, and H contains
indices 2 and 3.
Index 2 is next retrieved from H and node M27
is considered. However, at this point the root of
lhs(r2), i.e., node n25, does no longer belong to
state(m27), indicating that r2 is no longer applicable
to that node. The body of the for-statement in the
</bodyText>
<page confidence="0.993467">
259
</page>
<figure confidence="0.999889">
A A
A3 A D
A
A D
</figure>
<figureCaption confidence="0.9782595">
Figure 5: From left to right, top to bottom: trees Ci
C2 and C3. In the sample TTS G we have (C1, Cs) E
</figureCaption>
<bodyText confidence="0.98323948">
M(G), since Ci C2 C2 C3.
main program is not executed this time.
Finally, index 3 is retrieved from H and node m27
is again considered, this time for the application of
rule r3. Since the root of lhs(r3), i.e., node n35, be-
longs to state(m27), r3 is applied to C2 at node M27)
yielding C3. Data structures are again updated by
a call to procedure update with the second param-
eter equal to 4. Then state q8 is associated with
node rn37, the root node of C3. Despite of the fact
that q8 E F, we now have next(q8, 4) = 1. There-
fore rule r1 is not considered for application to C3.
Since H is now empty, the computation terminates
returning C3. 0
The results in Lemma 1 and Lemma 2 can be used
to show that, in the main program, a node n passes
the test in the head of the for-statement if and only
if lhs(ri) matches C at n. The correctness of Algo-
rithm 1 then follows from the definition of the heap
data structure.
We now turn to computational complexity issues.
Let p = maxi&lt;i‹, I ri I. For T E ET, let also t(T)
be the total number of rules that are successfully
applied on a run of Algorithm 1 on input T, counting
repetitions.
</bodyText>
<construct confidence="0.6882455">
Theorem 1 The running time of Algorithm 1 on
input tree T is 0(IT I pt(T)log(t(T))).
</construct>
<bodyText confidence="0.999681641025641">
Proof. We can implement our data structures in
such a way that each of the primitive access oper-
ations that are executed by the algorithm takes a
constant amount of time.
Consider each instance of the membership of a
node n in a set rule(i) and represent it as a pair
(n, i). We call active each pair (n, i) such that lhs(ri)
matches C at n at the time i is retrieved from H. As
already mentioned, these pairs pass the test in the
head of the for-loop in the main program. The num-
ber of active pairs is therefore t(T). All remaining
pairs are called dead. Note that an active pair (n, i)
can turn at most I lhs(ri) d-hR active pairs into dead
ones, through a call to the procedure update. Hence
the total number of dead pairs must be 0(pt(T)).
We conclude that the number of pairs totally in-
stantiated by the algorithm is 0(pt(T)).
It is easy to see that the number of pairs totally
instantiated by the algorithm is also a bound on the
number of indices inserted in or retrieved from the
heap. Then the time spent by the algorithm with
the heap is 0(pt(T)log(t(T))) (see for instance (Cor-
men, Leiserson, and Rivest, 1990)). The first call
to the procedure update in the main program takes
time proportional to IT. All remaining operations
of the algorithm will now be charged to some active
pair.
For each active pair, the body of the for-loop in the
main program and the body of the update procedure
are executed, taking an amount of time 0(p). For
each dead pair, only the test in the head of the for-
loop is executed, taking a constant amount of time.
This time is charged to the active node that turned
the pair under consideration into a dead one. In this
way each active node is charged an extra amount of
time 0(p).
Every operation executed by the algorithm has
been considered in the above analysis. We can then
conclude that the running time of Algorithm 1 is
</bodyText>
<equation confidence="0.594042">
0 (IT I + pt(T) log(t (T))) 0
</equation>
<bodyText confidence="0.9990353125">
Let us compare the above result with the
time performance of the standard algorithm for
transformation-based parsing. The standard algo-
rithm checks each rule in R for application to an
initial parse tree T, trying to match the left-hand
side of the current rule at each node of T. Using
the notation of Theorem 1, the running time is then
0(irpl T I). In practical applications, t(T) and I T
are very close (of the order of the length of the in-
put string). Therefore we have achieved a time im-
provement of a factor of 71/ log(t(T)). We empha-
size that ir might be several hundreds large if the
learned transformations are lexicalized. Therefore
we have improved the asymptotic time complexity
of transformation-based parsing of a factor between
two to three orders of magnitude.
</bodyText>
<subsectionHeader confidence="0.997657">
4.2 Order-dependent parsing
</subsectionHeader>
<bodyText confidence="0.999853833333333">
We consider here the general case for the TTS trans-
lation problem, in which the order of application of
several instances of rule r to a tree can affect the final
result of the rewriting. In this case rule r is called
critical. According to the definition of translation
induced by a TTS, a critical rule should always be
applied in post-order w.r.t. the nodes of the tree
to be rewritten. The solution we propose here for
critical rules is based on a preprocessing of the rule
sequence of the system.
We informally describe the technique presented
below. Assume that a critical rule r is to be applied
</bodyText>
<page confidence="0.984799">
260
</page>
<bodyText confidence="0.999610666666667">
at several matching nodes of a tree C. We partition
the matching nodes into two sets. The first set con-
tains all the nodes n at which the matching of lhs(r)
overlaps with a second matching at a node n&apos; dom-
inated by n. All the remaining matching nodes are
inserted in the second set. Then rule r is applied to
the nodes of the second set. After that, the nodes
in the first set are in turn partitioned according to
the above criterion, and the process is iterated until
all the matching nodes have been considered for ap-
plication of r. This is more precisely stated in what
follows.
</bodyText>
<figure confidence="0.981461">
B C
</figure>
<figureCaption confidence="0.8946395">
Figure 6: From left to right: trees Q and Q9. Node
p of Q is indicated by underlying its label.
</figureCaption>
<bodyText confidence="0.999367196078431">
We start with some additional notation. Let r
(Q --+ Q&apos;) be a tree-rewriting rule. Also, let p be a
node of Q and let S be the suffix of Q at p. We say
that p is periodic if (i) p is not the root of Q; and
(ii) S matches Q at the root node. It is easy to see
that the fact that lhs(r) has some periodic node is
a necessary condition for r to be critical. Let the
root of S be the i-th child of a node nf in Q, and
let Q, be a copy of Q. We write Q to denote the
tree obtained starting from Q by excising S and by
letting the root of Q, be the new i-th child of nf.
Finally, call ni the root of Qp and n2 the root of Q.
Example 5 Figure 6 depicts trees Q and Qp. The
periodic node p of Q under consideration is indicated
by underlying its label. 0
Let us assume that rule r is critical and that p is
the only periodic node in Q. We add Qp to set lhs(R)
and construct AG accordingly. Algorithm 1 should
then be modified as follows. We call p-chain any
sequence of one or more subtrees of C, all matched
by Q, that partially overlap in C. Let n be a node
of C and let q = state(n). Assume that n2 E q and
call S the subtree of C at n matched by Q (S exists
by Lemma 1). We distinguish two possible cases.
Case 1: If n1 E q, then we know that Q also matches
some portion of C that overlaps with S (at the node
matched by the periodic node p of Q). In this case
S belongs to a p-chain consisting of at least two sub-
trees and S is not the bottom-most subtree in the
p-chain.
Case 2: If ni q, then we know that S is the
bottom-most subtree in a p-chain.
Let i be the index of rule r under consideration.
We use an additional set chain(i). Each node n
of C such that n2 E state(n) is then inserted in
chain(i) if state(n) satisfies Case 1 above, and is
inserted in rule(i) otherwise. Note that chain(i) is
non-empty only in case rule(i) is such. Whenever i is
retrieved from H, we process each node n in rule(i),
as usual. But when we update our data structures
with the procedure update, we also look for match-
ings of lhs(r) at nodes of C in chain(i). The overall
effect of this is that each p-chain is considered in a
bottom-up fashion in the application of r. This is
compatible with the post-order application require-
ment.
The above technique can be applied for each peri-
odic node in a critical rule, and for each critical rule
of G. This only affects the size of AG, not the time
requirements of Algorithm 1. In fact, the proposed
preprocessing can at worst double hc•
</bodyText>
<sectionHeader confidence="0.999222" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999955692307692">
In this section we relate our work with the existing
literature and further discuss our result.
There are several alternative ways in which one
could see transformation-based rewriting systems.
TTS&apos;s are closely related to a class of graph rewriting
systems called neighbourhood-controlled embedding
graph grammars (NCE grammars; see (Janssens and
Rozenberg, 1982)). In fact our definition of the
relation and of the underlying V] operator has been
inspired by similar definitions in the NCE formal-
ism. Apart from the restriction to tree rewriting, the
main difference between NCE grammars and TTS&apos;s
is that in the latter formalism the productions are
totally ordered, therefore there is no recursion.
Ordered trees can also be seen as ground terms. If
we extend the alphabet E with variable symbols, we
can redefine the 1.-1 relation through variable sub-
stitution. In this way a TTS becomes a particular
kind of term-rewriting system. The idea of imposing
a total order on the rules of a term-rewriting system
can be found in the literature, but in these cases all
rules are reconsidered for application at each step
in the rewriting, using their priority (see for in-
stance the priority term-rewriting systems (Baeten,
Bergstra, and Klop, 1987)). Therefore these systems
allow recursion. There are cases in which a critical
rule in a TTS does not give rise to order-dependency
in rewriting. Methods for deciding the confluency
property for a term-rewriting system with critical
pairs (see (Dershowitz and Jouannaud, 1990) for def-
initions and an overview) can also be used to detect
the above cases for TTS.
As already pointed out, the translation problem
investigated here is closely related with the stan-
dard tree pattern matching problem. Our automata
AG (Definition 3) can be seen as an abstraction of
the bottom-up tree pattern matching algorithm pre-
sented in (Hoffmann and O&apos;Donnell, 1982). While
that result uses a representation of the pattern set
</bodyText>
<figure confidence="0.837995">
B C
</figure>
<page confidence="0.992467">
261
</page>
<bodyText confidence="0.999990388888889">
(our set lhs(R)) requiring an amount of space which
is exponential in the degree of the pattern trees, as
an improvement, our transition function does not de-
pend on this parameter. However, in the worst case
the space requirements of both algorithm are expo-
nential in the number of nodes in lhs(R) (see the
analysis in (Hoffmann and O&apos;Donnell, 1982)). As
already discussed in Section 3, the worst case condi-
tion is hardly met in natural language applications.
Polynomial space requirements can be guaranteed
if one switches to top-down tree pattern matching
algorithms. One such a method is reported in (Hoff-
mann and O&apos;Donnell, 1982), but in this case the
running-time of Algorithm 1 cannot be maintained.
Faster top-down matching algorithms have been re-
ported in (Kosaraju, 1989) and (Dubiner, Gaul, and
Magen, 1994), but these methods seems impractical,
due to very large hidden constants.
A tree-based extension of the very fast algorithm
described in (Roche and Schabes, 1995) is in prin-
ciple possible for transformation-based parsing, but
is likely to result in huge space requirements and
seems impractical. The algorithm presented here
might then be a good compromise between fast pars-
ing and reasonable space requirements.
When restricted to monadic trees, our automa-
ton AG comes down to the finite state device used
in the well-known string pattern matching algorithm
of Aho and Corasick (see (Aho and Corasick, 1975)),
requiring linear space only. If space requirements are
of primary importance or when the rule set is very
large, our method can then be considered for string-
based transformation rewriting as an alternative to
the already mentioned method in (Roche and Sch-
abes, 1995), which is faster but has more onerous
space requirements.
</bodyText>
<sectionHeader confidence="0.986131" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999961416666667">
The present research was done while the first author
was visiting the Center for Language and Speech
Processing, Johns Hopkins University, Baltimore,
MD. The second author is also a member of the Cen-
ter for Language and Speech Processing. This work
was funded in part by NSF grant IRI-9502312. The
authors are indebted with Alberto Apostolico, Rao
Kosaraju, Fernando Pereira and Murat Saraclar for
technical discussions on topics related to this paper.
The authors whish to thank an anonymous referee
for having pointed out important connections be-
tween TTS and term-rewriting systems.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.977971898305085">
Aho, A. V. and M. Corasick. 1975. Efficient
string matching: An aid to bibliographic search.
Communications of the Association for Comput-
ing Machinery, 18(6):333-340.
Baeten, J., J. Bergstra, and J. Klop. 1987. Prior-
ity rewrite systems. In Proc. Second International
Conference on Rewriting Techniques and Applica-
tions, LNCS 256, pages 83-94, Berlin, Germany.
Springer-Verlag.
Brill, E. 1993. Automatic grammar induction and
parsing free text: A transformation-based ap-
proach. In Proceedings of the 31st Meeting of the
Association of Computational Linguistics, Colum-
bus, Oh.
Brill, E. 1995. Transformation-based error-driven
learning and natural language processing: A case
study in part of speech tagging. Computational
Linguistics.
Brill, E. and P. Resnik. 1994. A transformation-
based approach to prepositional phrase attach-
ment disambiguation. In Proceedings of the
Fifteenth International Conference on Computa-
tional Linguistics (COLING-1994), Kyoto, Japan.
Chomsky, N. 1965. Aspects of the Theory of Syntax.
The MIT Press, Cambridge, MA.
Chomsky, N. and M. Halle. 1968. The Sound Pat-
tern of English. Harper and Row.
Cormen, T. H., C. E. Leiserson, and R. L. Rivest.
1990. Introduction to Algorithms. The MIT Press,
Cambridge, MA.
Dershowitz, N. and J. Jouannaud. 1990. Rewrite
systems. In J. Van Leeuwen, editor, Handbook
of Theoretical Computer Science, volume B. Else-
vier and The MIT Press, Amsterdam, The Nether-
lands and Cambridge, MA, chapter 6, pages 243-
320.
Dubiner, M., Z. Gafil, and E. Magen. 1994. Faster
tree pattern matching. Journal of the Association
for Computing Machinery, 41(2):205-213.
Hoffmann, C. M. and M. J. O&apos;Donnell. 1982. Pat-
tern matching in trees. Journal of the Association
for Computing Machinery, 29(1):68-95.
Janssens, D. and G. Rozenberg. 1982. Graph gram-
mars with neighbourhood-controlled embedding.
Theoretical Computer Science, 21:55-74.
Kaplan, R. M. and M. Kay. 1994. Regular models
of phonological rule sistems. Computational Lin-
guistics, 20(3):331-378.
Kosaraju, S. R. 1989. Efficient tree-pattern match-
ing. In Proceedings of the 30 Conference on Foun-
dations of Computer Science (FOCS), pages 178-
183.
Roche, E. and Y. Schabes. 1995. Deterministic part
of speech tagging with finite state transducers.
Computational Linguistics.
Thatcher, J. W. 1967. Characterizing derivation
trees of context-free grammars through a general-
ization of finite automata theory. Journal of Com-
puter and System Science, 1:317-322.
</reference>
<page confidence="0.996988">
262
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.647341">
<title confidence="0.999981">Efficient Transformation-Based Parsing</title>
<author confidence="0.99955">Giorgio Satta Eric Brill</author>
<affiliation confidence="0.9999565">Dipartimento di Elettronica ed Informatica Department of Computer Science Universita di Padova Johns Hopkins University</affiliation>
<address confidence="0.8292555">Gradenigo, 6/A Baltimore, Padova, Italy brillOcs.jhu.edu</address>
<email confidence="0.974792">sattaOdei.unipd.it</email>
<abstract confidence="0.998841636363636">In transformation-based parsing, a finite sequence of tree rewriting rules are checked for application to an input structure. Since in practice only a small percentage of rules are applied to any particular structure, the naive parsing algorithm is rather inefficient. We exploit this sparseness in rule applications to derive an algorithm two to three orders of magnitude faster than the standard parsing algorithm.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A V Aho</author>
<author>M Corasick</author>
</authors>
<title>Efficient string matching: An aid to bibliographic search.</title>
<date>1975</date>
<journal>Communications of the Association for Computing Machinery,</journal>
<pages>18--6</pages>
<contexts>
<context position="33574" citStr="Aho and Corasick, 1975" startWordPosition="6475" endWordPosition="6478">d Magen, 1994), but these methods seems impractical, due to very large hidden constants. A tree-based extension of the very fast algorithm described in (Roche and Schabes, 1995) is in principle possible for transformation-based parsing, but is likely to result in huge space requirements and seems impractical. The algorithm presented here might then be a good compromise between fast parsing and reasonable space requirements. When restricted to monadic trees, our automaton AG comes down to the finite state device used in the well-known string pattern matching algorithm of Aho and Corasick (see (Aho and Corasick, 1975)), requiring linear space only. If space requirements are of primary importance or when the rule set is very large, our method can then be considered for stringbased transformation rewriting as an alternative to the already mentioned method in (Roche and Schabes, 1995), which is faster but has more onerous space requirements. Acknowledgements The present research was done while the first author was visiting the Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD. The second author is also a member of the Center for Language and Speech Processing. This work was fu</context>
</contexts>
<marker>Aho, Corasick, 1975</marker>
<rawString>Aho, A. V. and M. Corasick. 1975. Efficient string matching: An aid to bibliographic search. Communications of the Association for Computing Machinery, 18(6):333-340.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Baeten</author>
<author>J Bergstra</author>
<author>J Klop</author>
</authors>
<title>Priority rewrite systems.</title>
<date>1987</date>
<booktitle>In Proc. Second International Conference on Rewriting Techniques and Applications, LNCS 256,</booktitle>
<pages>83--94</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Germany.</location>
<contexts>
<context position="31428" citStr="Baeten, Bergstra, and Klop, 1987" startWordPosition="6127" endWordPosition="6131">he latter formalism the productions are totally ordered, therefore there is no recursion. Ordered trees can also be seen as ground terms. If we extend the alphabet E with variable symbols, we can redefine the 1.-1 relation through variable substitution. In this way a TTS becomes a particular kind of term-rewriting system. The idea of imposing a total order on the rules of a term-rewriting system can be found in the literature, but in these cases all rules are reconsidered for application at each step in the rewriting, using their priority (see for instance the priority term-rewriting systems (Baeten, Bergstra, and Klop, 1987)). Therefore these systems allow recursion. There are cases in which a critical rule in a TTS does not give rise to order-dependency in rewriting. Methods for deciding the confluency property for a term-rewriting system with critical pairs (see (Dershowitz and Jouannaud, 1990) for definitions and an overview) can also be used to detect the above cases for TTS. As already pointed out, the translation problem investigated here is closely related with the standard tree pattern matching problem. Our automata AG (Definition 3) can be seen as an abstraction of the bottom-up tree pattern matching al</context>
</contexts>
<marker>Baeten, Bergstra, Klop, 1987</marker>
<rawString>Baeten, J., J. Bergstra, and J. Klop. 1987. Priority rewrite systems. In Proc. Second International Conference on Rewriting Techniques and Applications, LNCS 256, pages 83-94, Berlin, Germany. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>Automatic grammar induction and parsing free text: A transformation-based approach.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st Meeting of the Association of Computational Linguistics,</booktitle>
<location>Columbus, Oh.</location>
<contexts>
<context position="1434" citStr="Brill, 1993" startWordPosition="208" endWordPosition="209">msky, who attempted to define a set of transformations that would apply to a word sequence to map it from deep structure to surface structure (see (Chomsky, 1965)). Transformations have also been used in much of generative phonology to capture contextual variants in pronunciation, starting with (Chomsky and Halle, 1968). More recently, transformations have been applied to a diverse set of problems, including part of speech tagging, pronunciation network creation, prepositional phrase attachment disambiguation, and parsing, under the paradigm of transformation-based error-driven learning (see (Brill, 1993; Brill, 1995) and (Brill and Resnik, 1994)). In this paradigm, rules can be learned automatically from a training corpus, instead of being written by hand. Transformation-based systems are typically deterministic. Each rule in an ordered list of rules is applied once wherever it can apply, then is discarded, and the next rule is processed until the last rule in the list has been processed. Since for each rule the application algorithm must check for a matching at all possible sites to see whether the rule can apply, these systems run in 0(irpn) time, where 7r is the number of rules, p is the </context>
<context position="3124" citStr="Brill, 1993" startWordPosition="490" endWordPosition="491">dent of the number and size of rules. In this paper we consider transformation-based parsing, introduced in (Brill, 1993), and we improve upon the °(irion) time upper bound. In transformation-based parsing, an ordered sequence of tree-rewriting rules (tree transformations) are applied to an initial parse structure for an input sentence, to derive the final parse structure. We observe that in most transformation-based parsers, only a small percentage of rules are actually applied, for any particular input sentence. For example, in an application of the transformation-based parser described in (Brill, 1993), 7r = 300 rules were learned, to be applied at each node of the initial parse structure, but the average number of rules that are successfully applied at each node is only about one. So a lot of time is spent testing whether the conditions are met for applying a transformation and finding out that they are not met. This paper presents an original algorithm for transformation-based parsing working in 0(ptlog(t)) time, where t is the total number of rules applied for an input sentence. Since in practical cases t is smaller than n and we can neglect the log(n) factor, we have achieved a time imp</context>
<context position="10474" citStr="Brill, 1993" startWordPosition="1957" endWordPosition="1958"> = (Q, {A, B, C}, (5, q0, {qBc}) be a DTA specified as follows: Q = {q0, qA7 qBC q-1}; 4040, A) = qA, (5(qA, qo,C) = °(qA, q.Bc , B) = qBc and q_i is the value of all other entries of 8. It is not difficult to see that L(M) = L. 0 Observe that when we restrict to monadic trees, that is trees whose nodes have degree not greater than one, the above definitions correspond to the well known formalisms of deterministic finite state automata, the associated extended transition function, and the regular languages. 2.3 Transformation-based parsing Transformation-based parsing was first introduced in (Brill, 1993). Informally, a transformation-based parser assigns to an input sentence an initial parse structure, in some uniform way. Then the parser iteratively checks an ordered sequence of tree transformations for application to the initial parse tree, in order to derive the final parse structure. This results in a deterministic, linear time parser. In order to present our algorithm, we abstract away from the assignment of the initial parse to the input, and introduce below the notion of transformationbased tree rewriting system. The formulation we give here is inspired by (Kaplan and Kay, 1994) and (R</context>
</contexts>
<marker>Brill, 1993</marker>
<rawString>Brill, E. 1993. Automatic grammar induction and parsing free text: A transformation-based approach. In Proceedings of the 31st Meeting of the Association of Computational Linguistics, Columbus, Oh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>Transformation-based error-driven learning and natural language processing: A case study in part of speech tagging.</title>
<date>1995</date>
<journal>Computational Linguistics.</journal>
<contexts>
<context position="1448" citStr="Brill, 1995" startWordPosition="210" endWordPosition="211">empted to define a set of transformations that would apply to a word sequence to map it from deep structure to surface structure (see (Chomsky, 1965)). Transformations have also been used in much of generative phonology to capture contextual variants in pronunciation, starting with (Chomsky and Halle, 1968). More recently, transformations have been applied to a diverse set of problems, including part of speech tagging, pronunciation network creation, prepositional phrase attachment disambiguation, and parsing, under the paradigm of transformation-based error-driven learning (see (Brill, 1993; Brill, 1995) and (Brill and Resnik, 1994)). In this paradigm, rules can be learned automatically from a training corpus, instead of being written by hand. Transformation-based systems are typically deterministic. Each rule in an ordered list of rules is applied once wherever it can apply, then is discarded, and the next rule is processed until the last rule in the list has been processed. Since for each rule the application algorithm must check for a matching at all possible sites to see whether the rule can apply, these systems run in 0(irpn) time, where 7r is the number of rules, p is the cost of a sing</context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>Brill, E. 1995. Transformation-based error-driven learning and natural language processing: A case study in part of speech tagging. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
<author>P Resnik</author>
</authors>
<title>A transformationbased approach to prepositional phrase attachment disambiguation.</title>
<date>1994</date>
<booktitle>In Proceedings of the Fifteenth International Conference on Computational Linguistics (COLING-1994),</booktitle>
<location>Kyoto, Japan.</location>
<contexts>
<context position="1477" citStr="Brill and Resnik, 1994" startWordPosition="213" endWordPosition="216"> set of transformations that would apply to a word sequence to map it from deep structure to surface structure (see (Chomsky, 1965)). Transformations have also been used in much of generative phonology to capture contextual variants in pronunciation, starting with (Chomsky and Halle, 1968). More recently, transformations have been applied to a diverse set of problems, including part of speech tagging, pronunciation network creation, prepositional phrase attachment disambiguation, and parsing, under the paradigm of transformation-based error-driven learning (see (Brill, 1993; Brill, 1995) and (Brill and Resnik, 1994)). In this paradigm, rules can be learned automatically from a training corpus, instead of being written by hand. Transformation-based systems are typically deterministic. Each rule in an ordered list of rules is applied once wherever it can apply, then is discarded, and the next rule is processed until the last rule in the list has been processed. Since for each rule the application algorithm must check for a matching at all possible sites to see whether the rule can apply, these systems run in 0(irpn) time, where 7r is the number of rules, p is the cost of a single rule matching, and n is th</context>
</contexts>
<marker>Brill, Resnik, 1994</marker>
<rawString>Brill, E. and P. Resnik. 1994. A transformationbased approach to prepositional phrase attachment disambiguation. In Proceedings of the Fifteenth International Conference on Computational Linguistics (COLING-1994), Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chomsky</author>
</authors>
<title>Aspects of the Theory of Syntax.</title>
<date>1965</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="985" citStr="Chomsky, 1965" startWordPosition="144" endWordPosition="145">re checked for application to an input structure. Since in practice only a small percentage of rules are applied to any particular structure, the naive parsing algorithm is rather inefficient. We exploit this sparseness in rule applications to derive an algorithm two to three orders of magnitude faster than the standard parsing algorithm. 1 Introduction The idea of using transformational rules in natural language analysis dates back at least to Chomsky, who attempted to define a set of transformations that would apply to a word sequence to map it from deep structure to surface structure (see (Chomsky, 1965)). Transformations have also been used in much of generative phonology to capture contextual variants in pronunciation, starting with (Chomsky and Halle, 1968). More recently, transformations have been applied to a diverse set of problems, including part of speech tagging, pronunciation network creation, prepositional phrase attachment disambiguation, and parsing, under the paradigm of transformation-based error-driven learning (see (Brill, 1993; Brill, 1995) and (Brill and Resnik, 1994)). In this paradigm, rules can be learned automatically from a training corpus, instead of being written by </context>
</contexts>
<marker>Chomsky, 1965</marker>
<rawString>Chomsky, N. 1965. Aspects of the Theory of Syntax. The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chomsky</author>
<author>M Halle</author>
</authors>
<title>The Sound Pattern of English. Harper and Row.</title>
<date>1968</date>
<contexts>
<context position="1144" citStr="Chomsky and Halle, 1968" startWordPosition="166" endWordPosition="169">ve parsing algorithm is rather inefficient. We exploit this sparseness in rule applications to derive an algorithm two to three orders of magnitude faster than the standard parsing algorithm. 1 Introduction The idea of using transformational rules in natural language analysis dates back at least to Chomsky, who attempted to define a set of transformations that would apply to a word sequence to map it from deep structure to surface structure (see (Chomsky, 1965)). Transformations have also been used in much of generative phonology to capture contextual variants in pronunciation, starting with (Chomsky and Halle, 1968). More recently, transformations have been applied to a diverse set of problems, including part of speech tagging, pronunciation network creation, prepositional phrase attachment disambiguation, and parsing, under the paradigm of transformation-based error-driven learning (see (Brill, 1993; Brill, 1995) and (Brill and Resnik, 1994)). In this paradigm, rules can be learned automatically from a training corpus, instead of being written by hand. Transformation-based systems are typically deterministic. Each rule in an ordered list of rules is applied once wherever it can apply, then is discarded,</context>
</contexts>
<marker>Chomsky, Halle, 1968</marker>
<rawString>Chomsky, N. and M. Halle. 1968. The Sound Pattern of English. Harper and Row.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T H Cormen</author>
<author>C E Leiserson</author>
<author>R L Rivest</author>
</authors>
<title>Introduction to Algorithms.</title>
<date>1990</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="24832" citStr="Cormen, Leiserson, and Rivest, 1990" startWordPosition="4894" endWordPosition="4899">f active pairs is therefore t(T). All remaining pairs are called dead. Note that an active pair (n, i) can turn at most I lhs(ri) d-hR active pairs into dead ones, through a call to the procedure update. Hence the total number of dead pairs must be 0(pt(T)). We conclude that the number of pairs totally instantiated by the algorithm is 0(pt(T)). It is easy to see that the number of pairs totally instantiated by the algorithm is also a bound on the number of indices inserted in or retrieved from the heap. Then the time spent by the algorithm with the heap is 0(pt(T)log(t(T))) (see for instance (Cormen, Leiserson, and Rivest, 1990)). The first call to the procedure update in the main program takes time proportional to IT. All remaining operations of the algorithm will now be charged to some active pair. For each active pair, the body of the for-loop in the main program and the body of the update procedure are executed, taking an amount of time 0(p). For each dead pair, only the test in the head of the forloop is executed, taking a constant amount of time. This time is charged to the active node that turned the pair under consideration into a dead one. In this way each active node is charged an extra amount of time 0(p)</context>
</contexts>
<marker>Cormen, Leiserson, Rivest, 1990</marker>
<rawString>Cormen, T. H., C. E. Leiserson, and R. L. Rivest. 1990. Introduction to Algorithms. The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Dershowitz</author>
<author>J Jouannaud</author>
</authors>
<title>Rewrite systems.</title>
<date>1990</date>
<booktitle>Handbook of Theoretical Computer Science, volume B. Elsevier and The</booktitle>
<pages>243--320</pages>
<editor>In J. Van Leeuwen, editor,</editor>
<publisher>MIT Press,</publisher>
<location>Amsterdam,</location>
<contexts>
<context position="31706" citStr="Dershowitz and Jouannaud, 1990" startWordPosition="6170" endWordPosition="6173"> a particular kind of term-rewriting system. The idea of imposing a total order on the rules of a term-rewriting system can be found in the literature, but in these cases all rules are reconsidered for application at each step in the rewriting, using their priority (see for instance the priority term-rewriting systems (Baeten, Bergstra, and Klop, 1987)). Therefore these systems allow recursion. There are cases in which a critical rule in a TTS does not give rise to order-dependency in rewriting. Methods for deciding the confluency property for a term-rewriting system with critical pairs (see (Dershowitz and Jouannaud, 1990) for definitions and an overview) can also be used to detect the above cases for TTS. As already pointed out, the translation problem investigated here is closely related with the standard tree pattern matching problem. Our automata AG (Definition 3) can be seen as an abstraction of the bottom-up tree pattern matching algorithm presented in (Hoffmann and O&apos;Donnell, 1982). While that result uses a representation of the pattern set B C 261 (our set lhs(R)) requiring an amount of space which is exponential in the degree of the pattern trees, as an improvement, our transition function does not dep</context>
</contexts>
<marker>Dershowitz, Jouannaud, 1990</marker>
<rawString>Dershowitz, N. and J. Jouannaud. 1990. Rewrite systems. In J. Van Leeuwen, editor, Handbook of Theoretical Computer Science, volume B. Elsevier and The MIT Press, Amsterdam, The Netherlands and Cambridge, MA, chapter 6, pages 243-320.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Dubiner</author>
<author>Z Gafil</author>
<author>E Magen</author>
</authors>
<title>Faster tree pattern matching.</title>
<date>1994</date>
<journal>Journal of the Association for Computing Machinery,</journal>
<pages>41--2</pages>
<marker>Dubiner, Gafil, Magen, 1994</marker>
<rawString>Dubiner, M., Z. Gafil, and E. Magen. 1994. Faster tree pattern matching. Journal of the Association for Computing Machinery, 41(2):205-213.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C M Hoffmann</author>
<author>M J O&apos;Donnell</author>
</authors>
<title>Pattern matching in trees.</title>
<date>1982</date>
<journal>Journal of the Association for Computing Machinery,</journal>
<pages>29--1</pages>
<contexts>
<context position="5673" citStr="Hoffmann and O&apos;Donnell, 1982" startWordPosition="947" endWordPosition="951">st node). The height of T is the length of a longest path from the root to one of its leaves (a tree composed of a single node has height zero). We define IT as the number of nodes in T. A tree T E ET is denoted as A if it consists of a single leaf node labeled by A, and as A(Ti, T2, ,T), d &gt; 1, if T has root labeled by A with d (ordered) children denoted by Sometimes in the examples we draw trees in the usual way, indicating each node with its label. What follows is standard terminology from the tree pattern matching literature, with the simplification that we do not use variable terms. See (Hoffmann and O&apos;Donnell, 1982) for general definitions. Let 71 be a node of T. We say that a tree S matches T at n if there exists a one-to-one mapping from the nodes of S to the nodes of T, such that the following conditions are all satisfied: (i) if n&apos; maps to n&amp;quot;, then n&apos; and n&amp;quot; have the same label; (ii) the root of S maps to n; and (iii) if n&apos; maps to n&amp;quot; and n&apos; is not a leaf in S, then n&apos; and n&amp;quot; have the same degree and the i-th child of n&apos; maps to the i-th child of n&amp;quot;. We say that T and S are equivalent if they match each other at the respective root nodes. In what follows trees that are equivalent are not treated as t</context>
<context position="12638" citStr="Hoffmann and O&apos;Donnell, 1982" startWordPosition="2385" endWordPosition="2388"> C[SIQ&apos;e], where S is the subtree of T matched by Q at n and Q&apos;c is a fresh copy of Q&apos;. Let (ni , n2, .,n), t &gt; 1, be the postordered sequence of all nodes of C. We write C (4) if Ci, 1 &lt; i &lt; t, Co = C and Ct = C&apos;. Finally, we define the translation induced by G on ET as the map M(G) = {(C, C&amp;quot;) I C E ET, Ci_17- Ci for 1 &lt; i &lt; Co = C, = 3 Rule representation We develop here a representation of rule sequences that makes use of DTA and that is at the basis of the main result of this paper. Our technique improves the preprocessing phase of a bottom-up tree pattern matching algorithm presented in (Hoffmann and O&apos;Donnell, 1982), as it will be discussed in the final section. Let G= (E, R) be a TTS, R= (ri , r2, , r,). In what follows we construct a DTA that &amp;quot;detects&amp;quot; each subtree of an input tree that is equivalent to some tree in lhs(R). We need to introduce some additional notation. Let N be the set of all nodes from the trees in lhs(R). Call N,. the set of all root nodes (in N), N„, the set of all leftmost nodes, NI the set of all leaf nodes, and Na the set of all nodes labeled by a E E. For each q E 2N, let right(q) = In n E N, E q, n has immediate left sibling n&apos;l and let up(q) =- In I n E N, E q, n has rightmos</context>
<context position="15662" citStr="Hoffmann and O&apos;Donnell, 1982" startWordPosition="3051" endWordPosition="3055">) = Q4 5(gi , g3, B) = Q5 6(0, q3, B) = qfi 8(92, q4, B) = 97 40, q5, B) = qio 6(q0 ,q6, B) = q9 .6(go, q7, B) = Figure 3: Transition function of G. For all (q,q&apos; , a) E Q2 x E not indicated above, 6(g , g&apos; , a) = Although the number of states of AG is exponential in I NI, in practical cases most of these states are never reached by the automaton on an actual input, and can therefore be ignored. This happens whenever there are few pairs of suffix trees of trees in lhs(R) that share a common prefix tree but no tree in the pair matches the other at the root node. This is discussed at length in (Hoffmann and O&apos;Donnell, 1982), where an upper bound on the number of useful states is provided. The following lemma provides a characterization of AG that will be used later. Lemma 1 Lei n be a node of T E ET and let n&apos; be the root node of r E R. Tree lhs(r) matches T at n if and only if n&apos; E 6G(T,n). Proof (outline). The statement can be shown by proving the following claim. Let m be a node in T and m&apos; be a node in lhs(r). Call mi, , mk = m, k &gt; 1, the ordered sequence of the left siblings of m, with m included, and call mc. , , m&apos;k, = rn&apos;, k&apos; &gt; 1, the ordered sequence of the left siblings of m&apos;, with m&apos; included. If m&apos; </context>
<context position="17605" citStr="Hoffmann and O&apos;Donnell, 1982" startWordPosition="3446" endWordPosition="3449">after the first i — 1 rules of R have been considered. 4 The algorithm We present a translation algorithm for TTS that can immediately be converted into a transformationbased parsing algorithm. We use all definitions introduced in the previous sections. To simplify the presentation, we first make the assumption that the order in which we apply several instances of the same rule to a given tree does not affect the outcome. Later we will deal with the general case. 4.1 Order-free case We start with an important property that is used by the algorithm below and that can be easily shown (see also (Hoffmann and O&apos;Donnell, 1982)). Let G = (E, R) be a TTS and let hG be the maximum height 258 of a tree in lhs(R). Given trees T and S, S a subtree of T, we write local(T, S) to denote the set of all nodes of S and the first hG proper ancestors of the root of S in T (when these nodes are defined). Lemma 2 Assume that Ihs(r), r E R, matches a tree T at some node n. Let T T&apos; and let S be the copy of rhs(r) used in the rewriting. For every node n&apos; not included in local(T&apos;,S), we have &apos;6G(T,n&apos;) = G (T&apos; , n&apos;). 0 We precede the specification of the method with an informal presentation. The following three data structures are use</context>
<context position="32079" citStr="Hoffmann and O&apos;Donnell, 1982" startWordPosition="6232" endWordPosition="6235">stems allow recursion. There are cases in which a critical rule in a TTS does not give rise to order-dependency in rewriting. Methods for deciding the confluency property for a term-rewriting system with critical pairs (see (Dershowitz and Jouannaud, 1990) for definitions and an overview) can also be used to detect the above cases for TTS. As already pointed out, the translation problem investigated here is closely related with the standard tree pattern matching problem. Our automata AG (Definition 3) can be seen as an abstraction of the bottom-up tree pattern matching algorithm presented in (Hoffmann and O&apos;Donnell, 1982). While that result uses a representation of the pattern set B C 261 (our set lhs(R)) requiring an amount of space which is exponential in the degree of the pattern trees, as an improvement, our transition function does not depend on this parameter. However, in the worst case the space requirements of both algorithm are exponential in the number of nodes in lhs(R) (see the analysis in (Hoffmann and O&apos;Donnell, 1982)). As already discussed in Section 3, the worst case condition is hardly met in natural language applications. Polynomial space requirements can be guaranteed if one switches to top-</context>
</contexts>
<marker>Hoffmann, O&apos;Donnell, 1982</marker>
<rawString>Hoffmann, C. M. and M. J. O&apos;Donnell. 1982. Pattern matching in trees. Journal of the Association for Computing Machinery, 29(1):68-95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Janssens</author>
<author>G Rozenberg</author>
</authors>
<title>Graph grammars with neighbourhood-controlled embedding.</title>
<date>1982</date>
<journal>Theoretical Computer Science,</journal>
<pages>21--55</pages>
<contexts>
<context position="30548" citStr="Janssens and Rozenberg, 1982" startWordPosition="5981" endWordPosition="5984">ent. The above technique can be applied for each periodic node in a critical rule, and for each critical rule of G. This only affects the size of AG, not the time requirements of Algorithm 1. In fact, the proposed preprocessing can at worst double hc• 5 Discussion In this section we relate our work with the existing literature and further discuss our result. There are several alternative ways in which one could see transformation-based rewriting systems. TTS&apos;s are closely related to a class of graph rewriting systems called neighbourhood-controlled embedding graph grammars (NCE grammars; see (Janssens and Rozenberg, 1982)). In fact our definition of the relation and of the underlying V] operator has been inspired by similar definitions in the NCE formalism. Apart from the restriction to tree rewriting, the main difference between NCE grammars and TTS&apos;s is that in the latter formalism the productions are totally ordered, therefore there is no recursion. Ordered trees can also be seen as ground terms. If we extend the alphabet E with variable symbols, we can redefine the 1.-1 relation through variable substitution. In this way a TTS becomes a particular kind of term-rewriting system. The idea of imposing a total</context>
</contexts>
<marker>Janssens, Rozenberg, 1982</marker>
<rawString>Janssens, D. and G. Rozenberg. 1982. Graph grammars with neighbourhood-controlled embedding. Theoretical Computer Science, 21:55-74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M Kaplan</author>
<author>M Kay</author>
</authors>
<title>Regular models of phonological rule sistems.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<pages>20--3</pages>
<contexts>
<context position="11067" citStr="Kaplan and Kay, 1994" startWordPosition="2049" endWordPosition="2052"> introduced in (Brill, 1993). Informally, a transformation-based parser assigns to an input sentence an initial parse structure, in some uniform way. Then the parser iteratively checks an ordered sequence of tree transformations for application to the initial parse tree, in order to derive the final parse structure. This results in a deterministic, linear time parser. In order to present our algorithm, we abstract away from the assignment of the initial parse to the input, and introduce below the notion of transformationbased tree rewriting system. The formulation we give here is inspired by (Kaplan and Kay, 1994) and (Roche and Schabes, 1995). The relationship between transformation-based tree rewriting systems and standard term-rewriting systems will be discussed in the final section. Definition 2 A transformation-based tree rewriting system (TTS) is a pair G = (E, R), where E is a finite alphabet and R = (ri, r2, , r,), ir&gt; 1, is a finite sequence of tree rewriting rules having the form Q Q&apos;, with Q, Q&apos; E ET and such that Q and Q&apos; have the same number of leaves. If r = (Q —+ Q&apos;), we write lhs(r) for Q and rhs(r) for Q&apos;. We also write lhs(R) for {lhs(r) r E R}. (Recall that we regard lhs(ri) and lhs(</context>
</contexts>
<marker>Kaplan, Kay, 1994</marker>
<rawString>Kaplan, R. M. and M. Kay. 1994. Regular models of phonological rule sistems. Computational Linguistics, 20(3):331-378.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S R Kosaraju</author>
</authors>
<title>Efficient tree-pattern matching.</title>
<date>1989</date>
<booktitle>In Proceedings of the 30 Conference on Foundations of Computer Science (FOCS),</booktitle>
<pages>178--183</pages>
<contexts>
<context position="32928" citStr="Kosaraju, 1989" startWordPosition="6375" endWordPosition="6376">this parameter. However, in the worst case the space requirements of both algorithm are exponential in the number of nodes in lhs(R) (see the analysis in (Hoffmann and O&apos;Donnell, 1982)). As already discussed in Section 3, the worst case condition is hardly met in natural language applications. Polynomial space requirements can be guaranteed if one switches to top-down tree pattern matching algorithms. One such a method is reported in (Hoffmann and O&apos;Donnell, 1982), but in this case the running-time of Algorithm 1 cannot be maintained. Faster top-down matching algorithms have been reported in (Kosaraju, 1989) and (Dubiner, Gaul, and Magen, 1994), but these methods seems impractical, due to very large hidden constants. A tree-based extension of the very fast algorithm described in (Roche and Schabes, 1995) is in principle possible for transformation-based parsing, but is likely to result in huge space requirements and seems impractical. The algorithm presented here might then be a good compromise between fast parsing and reasonable space requirements. When restricted to monadic trees, our automaton AG comes down to the finite state device used in the well-known string pattern matching algorithm of </context>
</contexts>
<marker>Kosaraju, 1989</marker>
<rawString>Kosaraju, S. R. 1989. Efficient tree-pattern matching. In Proceedings of the 30 Conference on Foundations of Computer Science (FOCS), pages 178-183.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Roche</author>
<author>Y Schabes</author>
</authors>
<title>Deterministic part of speech tagging with finite state transducers. Computational Linguistics.</title>
<date>1995</date>
<contexts>
<context position="2221" citStr="Roche and Schabes, 1995" startWordPosition="346" endWordPosition="349">ormation-based systems are typically deterministic. Each rule in an ordered list of rules is applied once wherever it can apply, then is discarded, and the next rule is processed until the last rule in the list has been processed. Since for each rule the application algorithm must check for a matching at all possible sites to see whether the rule can apply, these systems run in 0(irpn) time, where 7r is the number of rules, p is the cost of a single rule matching, and n is the size of the input structure. While this results in fast processing, it is possible to create much faster systems. In (Roche and Schabes, 1995), a method is described for converting a list of transformations that operates on strings into a deterministic finite state transducer, resulting in an optimal tagger in the sense that tagging requires only one state transition per word, giving a linear time tagger whose run-time is independent of the number and size of rules. In this paper we consider transformation-based parsing, introduced in (Brill, 1993), and we improve upon the °(irion) time upper bound. In transformation-based parsing, an ordered sequence of tree-rewriting rules (tree transformations) are applied to an initial parse str</context>
<context position="11097" citStr="Roche and Schabes, 1995" startWordPosition="2054" endWordPosition="2057">). Informally, a transformation-based parser assigns to an input sentence an initial parse structure, in some uniform way. Then the parser iteratively checks an ordered sequence of tree transformations for application to the initial parse tree, in order to derive the final parse structure. This results in a deterministic, linear time parser. In order to present our algorithm, we abstract away from the assignment of the initial parse to the input, and introduce below the notion of transformationbased tree rewriting system. The formulation we give here is inspired by (Kaplan and Kay, 1994) and (Roche and Schabes, 1995). The relationship between transformation-based tree rewriting systems and standard term-rewriting systems will be discussed in the final section. Definition 2 A transformation-based tree rewriting system (TTS) is a pair G = (E, R), where E is a finite alphabet and R = (ri, r2, , r,), ir&gt; 1, is a finite sequence of tree rewriting rules having the form Q Q&apos;, with Q, Q&apos; E ET and such that Q and Q&apos; have the same number of leaves. If r = (Q —+ Q&apos;), we write lhs(r) for Q and rhs(r) for Q&apos;. We also write lhs(R) for {lhs(r) r E R}. (Recall that we regard lhs(ri) and lhs(ri), i j, as different objects</context>
<context position="33128" citStr="Roche and Schabes, 1995" startWordPosition="6404" endWordPosition="6407">lready discussed in Section 3, the worst case condition is hardly met in natural language applications. Polynomial space requirements can be guaranteed if one switches to top-down tree pattern matching algorithms. One such a method is reported in (Hoffmann and O&apos;Donnell, 1982), but in this case the running-time of Algorithm 1 cannot be maintained. Faster top-down matching algorithms have been reported in (Kosaraju, 1989) and (Dubiner, Gaul, and Magen, 1994), but these methods seems impractical, due to very large hidden constants. A tree-based extension of the very fast algorithm described in (Roche and Schabes, 1995) is in principle possible for transformation-based parsing, but is likely to result in huge space requirements and seems impractical. The algorithm presented here might then be a good compromise between fast parsing and reasonable space requirements. When restricted to monadic trees, our automaton AG comes down to the finite state device used in the well-known string pattern matching algorithm of Aho and Corasick (see (Aho and Corasick, 1975)), requiring linear space only. If space requirements are of primary importance or when the rule set is very large, our method can then be considered for </context>
</contexts>
<marker>Roche, Schabes, 1995</marker>
<rawString>Roche, E. and Y. Schabes. 1995. Deterministic part of speech tagging with finite state transducers. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J W Thatcher</author>
</authors>
<title>Characterizing derivation trees of context-free grammars through a generalization of finite automata theory.</title>
<date>1967</date>
<journal>Journal of Computer and System Science,</journal>
<pages>1--317</pages>
<contexts>
<context position="8052" citStr="Thatcher, 1967" startWordPosition="1475" endWordPosition="1476">ing steps: (i) if the root of S is the i-th child of a node ni in T, the root of S&apos; becomes the i-th child of nf ; and (ii) the (ordered) children of ni in T, if any, become the children of n&apos;i, 1 &lt;i &lt; 1. The root of T[S151 is the root of T if node nf above exists, and is the root of S&apos; otherwise. Example 2 Figure 1 depicts trees T, S&apos; and T&apos; in this order. A subtree S of T is also indicated using underlined labels at nodes of T. Note that S and S&apos; have the same number of leaves. Then we have T&apos; = 71,5 I Sq. 0 2.2 Tree automata Deterministic (bottom-up) tree automata were first introduced in (Thatcher, 1967) (called FRT there). The definition we propose here is a generalization of the canonical one to trees of any degree. Note that the transition function below is computed on a number of states that is independent of the degree of the input tree. Deterministic tree automata will be used later to implement the bottom-up tree pattern matching algorithm of (Hoffmann and 0&apos;- Donnell, 1982). Definition 1 A deterministic tree automaton (DTA) is a 5-tuple M = (Q, E, 6, go, F), where Q is a finite set of states, E is a finite alphabet, go E Q is the initial state, F c Q is the set of final slates and 6 i</context>
</contexts>
<marker>Thatcher, 1967</marker>
<rawString>Thatcher, J. W. 1967. Characterizing derivation trees of context-free grammars through a generalization of finite automata theory. Journal of Computer and System Science, 1:317-322.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>