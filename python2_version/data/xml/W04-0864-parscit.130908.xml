<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.013264">
<note confidence="0.7224385">
SENSEVAL-3: Third International Workshop on the Evaluation of Systems
for the Semantic Analysis of Text, Barcelona, Spain, July 2004
</note>
<subsectionHeader confidence="0.4952345">
Association for Computational Linguistics
An Example
</subsectionHeader>
<bodyText confidence="0.999787125">
The features and the associated computations
in the system are best explained using an ex-
ample. Consider the following sentence from
the all-words task, and the word companion as
the target to be disambiguated.
Haney peered doubtfully at his
drinking companion through bleary,
tear-filled eyes.
</bodyText>
<subsectionHeader confidence="0.883194">
Part of Speech Tagging
</subsectionHeader>
<bodyText confidence="0.998838142857143">
The system uses the parts of speech given in
the associated Treebank file distributed with
the test document and only considers senses in
the proper part of speech. In our example, the
word companion has three noun senses and one
verb sense in WordNet 1.7.1, and only the noun
senses will be considered.
</bodyText>
<subsectionHeader confidence="0.678556">
The Prior Probability
</subsectionHeader>
<bodyText confidence="0.999929428571429">
The Naive Bayes model requires a prior prob-
ability Pr(s1w) for each sense. WordNet 1.7.1
includes sense frequency information in the in-
dex.sense file. The frequencies for the three
noun senses of companion are given 25, 9, and 0.
To avoid zero probabilities we add one to these
counts and use 26/37, 10/37, 1/37 as our priors.
</bodyText>
<subsectionHeader confidence="0.901433">
The Bag of Words Model
</subsectionHeader>
<bodyText confidence="0.999712210526316">
The bag-of-words model uses the words within
a certain radius of the target word as the fea-
tures of the test instance. In the final system,
all non-skip words within 128 words of the tar-
get were considered as features. The skip words
were taken from the file stoplist.pl distributed
with WordNet 1.6. All words are lowercased but
not stemmed The list of features for our exam-
ple contains 39 words after the elimination of
skips:
man, haney, peered, doubtfully,
drinking, companion, bleary, tear,
filled, eyes, ready, answer, ich, ... ,
burning, noticed, wondered, tied
The model uses WordNet glosses and pointers
as training data. For each synset, all non-skip
words within its gloss, morphological variants
of the lemmas in the synset, and the lemmas
of all the neighbor synsets that are linked with
a single pointer are considered as the training
data. The training data for the correct sense of
companion thus consists of:
a person who is frequently in the
company of another; &amp;quot;drinking com-
panions&amp;quot;; &amp;quot;comrades in arms&amp;quot; as-
sociates familiars fellows companions
comrade companion fellow associate
comrades familiar friend friends dates
date escorts escort playfellows play-
mates playmate playfellow
There are 27 non-skip words in the training
data. These include only 2 of the 39 features
from the test instance: &amp;quot;drinking&amp;quot;, and &amp;quot;com-
panion&amp;quot;. The others have zero count, thus we
need to use smoothing. For the bag-of-words
model the parameters used for smoothing are
a = 2 and N = 65536. The Naive Bayes prod-
uct for this sense is:
</bodyText>
<equation confidence="0.998104666666667">
Pr(81w)Pr(f118)Pr(his) • • • Pr(f3918) =
26 (1 + 2)(1 + 2)(0 + 2) ... (0 + 2)
37 (27 ± 65536)39
</equation>
<bodyText confidence="0.99991375">
When we normalize this product by dividing
it with the sum of the Naive Bayes products of
all three senses we get 77.88% as the probability
of this sense.
</bodyText>
<subsectionHeader confidence="0.934447">
The Local Collocation Model
</subsectionHeader>
<bodyText confidence="0.999935677419355">
The features representing local collocations
around the target word for our example are:
HEAD=companion
HEADAhrough
HEAD_through_bleary
drinking_HEAD
drinking_HEADAhrough
drinking_HEAD_through_bleary
The first entry gives the exact form of the
head word. The remaining entries include vary-
ing amounts of left and right context up to the
first non-skip word. WordNet 1.6 stoplist.pl is
used to determine skip words.
The training data for the local colloca-
tion model consists of the following publicly
available sense tagged data: SemCor 1.7.1
(222199), DSO Corpus of Sense-Tagged English
(160225), WordNet 1.7.1 glosses (44902), Open
Mind Word Expert (29382), Senseval2 lexsam-
ple (13290), Senseval3 lexsample (8391). The
numbers indicate the number of instances in
each source.
In our example, the synset associated with
the correct sense was observed 18 times in the
training data. &amp;quot;HEAD=companion&amp;quot; was ob-
served in 6 of these and the collocation &amp;quot;drink-
ing_HEAD&amp;quot; was observed once. The other fea-
tures have zero counts. For the local colloca-
tion model the smoothing parameters used are
a = 0.15 and N = 5000. The Naive Bayes prod-
uct for this sense is
</bodyText>
<equation confidence="0.999527333333333">
Pr(81w)Pr(f118)Pr(f218) • • • Pr(f618) =
26 (6 + 0.15)(1 + 0.15)(0 + 0.15) ...
37 (18 + 5000)6
</equation>
<bodyText confidence="0.944713125">
When we normalize this product by dividing
it with the sum of the Naive Bayes products of
all three senses we get 99.04% for the probabil-
ity of the correct sense.
Tie Breaking
If a model assigns equal probability to more
than one sense, the one with the smallest Word-
Net sense number is preferred.
</bodyText>
<sectionHeader confidence="0.742148" genericHeader="abstract">
Merging Results
</sectionHeader>
<bodyText confidence="0.999984571428571">
In our example, both models predicted the cor-
rect sense, so this would be the answer given
by the program. However had they predicted
two different senses, the answer returned by the
local-collocation model would be used because
its confidence 99.04% is higher than the confi-
dence of the bag-of-words model 77.88%.
</bodyText>
<sectionHeader confidence="0.996378" genericHeader="keywords">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.591364">
Naive Bayes Smoothing
</subsectionHeader>
<bodyText confidence="0.999246835820896">
Naive Bayes has long been a popular WSD tool
(Mooney, 1996; Pedersen, 2000) and according
to (Yarowsky and Florian, 2002) it is one of the
best performing algorithms However usually
little detail is given on smoothing methods used
with Naive Bayes.
To compare various smoothing methods, a set
of experiments were run with a Naive Bayes
algorithm on the SemCor corpus with six fold
cross validation. The algorithm used the words
in the same sentence as bag-of-words features
and tried to disambiguate all tagged words.
The add-one smoothing as described in
(Mitchell, 1997) smooths all frequencies r/n
with (r + 1)/(n + V) where V is the vocabu-
lary size taken here to be 100000. Naive Bayes
with add-one smoothing did 0.5% better than
the baseline of picking the first WordNet sense.
For low sample sizes Good-Turing is known
as a better estimator of frequency (Gale and
Church, 1994). However, in the context of Naive
Bayes, Good-Turing estimation results in a sig-
nificant 4.5% performance loss compared to the
baseline.
Lacking a satisfactory theory to explain these
results I decided to use the additive smoothing
with both the numerator and the denominator
as adjustable parameters to be optimized. Thus
a frequency r/n was smoothed as (r+a)I(n+N)
where both a and N are adjustable parameters.
The parameters used by the final system were
selected using a simple search of the parameter
space.
Context size for training and testing
Another important set of parameters for the bag
of words model is the size of the context window
for training and testing. See (Pedersen, 2000)
for previous work on window sizes.
A number of experiments were run using the
SemCor data for training and the Senseval-2 all
words task data for testing. Training and test-
ing window radii from 1 to 64 were tried, also
optimizing the N and a parameters for each
case. The best results were achieved using small
test window sizes. Setting both the training and
testing radius to 8 results in 2.66% improvement
above the baseline whereas using window sizes
of 64 achieves no improvement above the base-
line.
Unfortunately all the experiments with Sem-
Cor underperformed the model that just uses
WordNet glosses for training. In fact, when
both WordNet glosses and the SemCor data was
used together the performance was worse than
using the glosses alone. Various attempts to
use semi-supervised methods to enhance Sem-
Cor with similar contexts from untagged data
failed.
The fact that small test window sizes consis-
tently outperformed larger ones with SemCor
indicates the importance of local collocations.
The best performing system in Senseval-2 (Mi-
halcea, 2002) uses almost exclusively local fea-
tures. As a result, I have decided to use only
WordNet glosses and pointers for the bag of
words model and implement an independent lo-
cal collocation model.
</bodyText>
<subsectionHeader confidence="0.62741">
Merging models
</subsectionHeader>
<bodyText confidence="0.999986956521739">
The final system consists of two independent
models, one based on bag of words, another on
local collocations. The model with the higher
confidence provides the final answer for each in-
stance. It may be useful to find out how differ-
ent their answers are and whether the combi-
nation of models leads to a significant improve-
ment.
The performance of the baseline for the
Senseval-3 all words task, picking the first sense,
had 60.90% performance (precision and recall
are the same because all instances were an-
swered). The final system achieved a perfor-
mance of 3.23% above the baseline. The bag of
words model was 0.59% above the baseline and
the local collocation model was 2.65% above the
baseline. The union of the correct answers from
both models give us 10.29% above the baseline.
This is an upper limit that could be achieved
if we were able to correctly pick the best model
for every instance. As things stand however, the
bag of words model does not seem to add much
to the performance.
</bodyText>
<sectionHeader confidence="0.998528" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.99998675862069">
The percentage improvement above the baseline
is very small. The main reason seems to be
data sparseness. You may have noticed that in
the companion example even though we used a
nontrivial probabilistic model, the final decision
was largely due to a single word in the context:
drinking. It would be interesting to see if this is
a typical situation.
In the bag of words model 50% of the in-
stances in the Senseval-3 all words task did not
contain any context words recognized by the
model other than the target word itself. 23%
had a single context word recognized other than
the head word as in the companion example.
This is in spite of the fact that the context is
defined with a relatively wide 256 word window.
In the local collocation model 40% of the in-
stances match nothing but the HEAD=xxx fea-
ture. 36% of the instances match one or two
other patterns which usually involve only func-
tion words.
These figures show that only in less than a
quarter of the instances a decision is made based
on a neighboring content word. On all other in-
stances, we fall back on the first WordNet sense.
The key to significantly better accuracy lies in
finding a way to learn more from each example,
possibly utilizing untagged data or semantic re-
sources.
</bodyText>
<sectionHeader confidence="0.9979" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.964914227272727">
W. Gale and K. Church. 1994. What&apos;s wrong
with adding one? In N. Oostdijk and
P. de Haan, editors, Corpus based research
into language. Rodopi.
Rada F. Mihalcea. 2002. Word sense disam-
biguation with pattern learning and auto-
matic feature selection. Natural Language
Engineering, 8(4):343-358.
Tom M. Mitchell. 1997. Machine Learning.
McGraw-Hill.
R. Mooney. 1996. Comparative experiments on
disambiguating word senses: An illustration
of the role of bias in machine learning. In
EMNLP-96.
Ted Pedersen. 2000. A simple approach to
building ensembles of naive bayesian clas-
sifiers for word sense disambiguation. In
NAA CL-OO.
David Yarowsky and Radu Florian. 2002. Eval-
uating sense disambiguation across diverse
parameter spaces. Natural Language Engi-
neering, 8(4):293-310.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.025029">
<note confidence="0.716502">SENSEVAL-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, Barcelona, Spain, July 2004</note>
<title confidence="0.684613">Association for Computational Linguistics An Example</title>
<abstract confidence="0.990596193181818">The features and the associated computations in the system are best explained using an example. Consider the following sentence from all-words task, and the word the target to be disambiguated. Haney peered doubtfully at his bleary, tear-filled eyes. Part of Speech Tagging The system uses the parts of speech given in the associated Treebank file distributed with the test document and only considers senses in the proper part of speech. In our example, the three noun senses and one verb sense in WordNet 1.7.1, and only the noun senses will be considered. The Prior Probability The Naive Bayes model requires a prior probability Pr(s1w) for each sense. WordNet 1.7.1 includes sense frequency information in the index.sense file. The frequencies for the three senses of given 25, 9, and 0. To avoid zero probabilities we add one to these counts and use 26/37, 10/37, 1/37 as our priors. The Bag of Words Model The bag-of-words model uses the words within a certain radius of the target word as the features of the test instance. In the final system, all non-skip words within 128 words of the target were considered as features. The skip words were taken from the file stoplist.pl distributed with WordNet 1.6. All words are lowercased but not stemmed The list of features for our example contains 39 words after the elimination of skips: man, haney, peered, doubtfully, drinking, companion, bleary, tear, filled, eyes, ready, answer, ich, ... , burning, noticed, wondered, tied The model uses WordNet glosses and pointers as training data. For each synset, all non-skip words within its gloss, morphological variants of the lemmas in the synset, and the lemmas of all the neighbor synsets that are linked with a single pointer are considered as the training data. The training data for the correct sense of companion thus consists of: a person who is frequently in the company of another; &amp;quot;drinking companions&amp;quot;; &amp;quot;comrades in arms&amp;quot; associates familiars fellows companions comrade companion fellow associate comrades familiar friend friends dates date escorts escort playfellows playmates playmate playfellow There are 27 non-skip words in the training data. These include only 2 of the 39 features from the test instance: &amp;quot;drinking&amp;quot;, and &amp;quot;companion&amp;quot;. The others have zero count, thus we need to use smoothing. For the bag-of-words model the parameters used for smoothing are = 2 and = The Naive Bayes product for this sense is: • • • = + 2)(1 + 2)(0 2) ... (0 +2) (27 ± When we normalize this product by dividing it with the sum of the Naive Bayes products of all three senses we get 77.88% as the probability of this sense. The Local Collocation Model The features representing local collocations around the target word for our example are: HEAD=companion HEADAhrough HEAD_through_bleary drinking_HEAD drinking_HEADAhrough drinking_HEAD_through_bleary The first entry gives the exact form of the head word. The remaining entries include varying amounts of left and right context up to the first non-skip word. WordNet 1.6 stoplist.pl is used to determine skip words. The training data for the local collocation model consists of the following publicly available sense tagged data: SemCor 1.7.1 (222199), DSO Corpus of Sense-Tagged English (160225), WordNet 1.7.1 glosses (44902), Open Mind Word Expert (29382), Senseval2 lexsample (13290), Senseval3 lexsample (8391). The numbers indicate the number of instances in each source. In our example, the synset associated with the correct sense was observed 18 times in the training data. &amp;quot;HEAD=companion&amp;quot; was obin 6 of these and the collocation &amp;quot;drinking_HEAD&amp;quot; was observed once. The other features have zero counts. For the local collocation model the smoothing parameters used are = 0.15 and = The Naive Bayes product for this sense is • • = (6 + 0.15)(1+ 0.15)(0 + ... (18 + When we normalize this product by dividing it with the sum of the Naive Bayes products of all three senses we get 99.04% for the probability of the correct sense. Tie Breaking If a model assigns equal probability to more than one sense, the one with the smallest Word- Net sense number is preferred. Merging Results In our example, both models predicted the correct sense, so this would be the answer given by the program. However had they predicted two different senses, the answer returned by the local-collocation model would be used because its confidence 99.04% is higher than the confidence of the bag-of-words model 77.88%. 3 Experiments Naive Bayes Smoothing Naive Bayes has long been a popular WSD tool (Mooney, 1996; Pedersen, 2000) and according to (Yarowsky and Florian, 2002) it is one of the best performing algorithms However usually little detail is given on smoothing methods used with Naive Bayes. To compare various smoothing methods, a set of experiments were run with a Naive Bayes algorithm on the SemCor corpus with six fold cross validation. The algorithm used the words in the same sentence as bag-of-words features and tried to disambiguate all tagged words. The add-one smoothing as described in (Mitchell, 1997) smooths all frequencies r/n with (r + 1)/(n + V) where V is the vocabulary size taken here to be 100000. Naive Bayes with add-one smoothing did 0.5% better than the baseline of picking the first WordNet sense. For low sample sizes Good-Turing is known as a better estimator of frequency (Gale and Church, 1994). However, in the context of Naive Bayes, Good-Turing estimation results in a sig- 4.5% performance to the baseline. Lacking a satisfactory theory to explain these results I decided to use the additive smoothing with both the numerator and the denominator as adjustable parameters to be optimized. Thus frequency r/n was smoothed as both a and adjustable parameters. The parameters used by the final system were selected using a simple search of the parameter space. Context size for training and testing Another important set of parameters for the bag of words model is the size of the context window for training and testing. See (Pedersen, 2000) for previous work on window sizes. A number of experiments were run using the SemCor data for training and the Senseval-2 all words task data for testing. Training and testing window radii from 1 to 64 were tried, also the a parameters for each case. The best results were achieved using small test window sizes. Setting both the training and testing radius to 8 results in 2.66% improvement above the baseline whereas using window sizes of 64 achieves no improvement above the baseline. Unfortunately all the experiments with Sem- Cor underperformed the model that just uses WordNet glosses for training. In fact, when both WordNet glosses and the SemCor data was used together the performance was worse than using the glosses alone. Various attempts to use semi-supervised methods to enhance Sem- Cor with similar contexts from untagged data failed. The fact that small test window sizes consistently outperformed larger ones with SemCor indicates the importance of local collocations. The best performing system in Senseval-2 (Mihalcea, 2002) uses almost exclusively local features. As a result, I have decided to use only WordNet glosses and pointers for the bag of words model and implement an independent local collocation model. Merging models The final system consists of two independent models, one based on bag of words, another on local collocations. The model with the higher confidence provides the final answer for each instance. It may be useful to find out how different their answers are and whether the combination of models leads to a significant improvement. The performance of the baseline for the Senseval-3 all words task, picking the first sense, had 60.90% performance (precision and recall are the same because all instances were answered). The final system achieved a performance of 3.23% above the baseline. The bag of words model was 0.59% above the baseline and the local collocation model was 2.65% above the baseline. The union of the correct answers from both models give us 10.29% above the baseline. This is an upper limit that could be achieved if we were able to correctly pick the best model for every instance. As things stand however, the bag of words model does not seem to add much to the performance. 4 Conclusion The percentage improvement above the baseline is very small. The main reason seems to be data sparseness. You may have noticed that in even though we used a nontrivial probabilistic model, the final decision was largely due to a single word in the context: would be interesting to see if this is a typical situation. In the bag of words model 50% of the instances in the Senseval-3 all words task did not contain any context words recognized by the model other than the target word itself. 23% had a single context word recognized other than head word as in the This is in spite of the fact that the context is defined with a relatively wide 256 word window. In the local collocation model 40% of the instances match nothing but the HEAD=xxx feature. 36% of the instances match one or two other patterns which usually involve only function words. These figures show that only in less than a quarter of the instances a decision is made based on a neighboring content word. On all other instances, we fall back on the first WordNet sense. The key to significantly better accuracy lies in finding a way to learn more from each example, possibly utilizing untagged data or semantic resources. References W. Gale and K. Church. 1994. What&apos;s wrong with adding one? In N. Oostdijk de Haan, editors, based research language. Rada F. Mihalcea. 2002. Word sense disamwith pattern learning and autofeature selection. Language M. Mitchell. 1997. Learning. McGraw-Hill. R. Mooney. 1996. Comparative experiments on disambiguating word senses: An illustration of the role of bias in machine learning. In EMNLP-96. Ted Pedersen. 2000. A simple approach to building ensembles of naive bayesian classifiers for word sense disambiguation. In NAA CL-OO. David Yarowsky and Radu Florian. 2002. Evaluating sense disambiguation across diverse</abstract>
<intro confidence="0.508875">spaces. Language Engi-</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>W Gale</author>
<author>K Church</author>
</authors>
<title>What&apos;s wrong with adding one?</title>
<date>1994</date>
<editor>In N. Oostdijk and P. de Haan, editors,</editor>
<publisher>Rodopi.</publisher>
<contexts>
<context position="5780" citStr="Gale and Church, 1994" startWordPosition="958" endWordPosition="961">arious smoothing methods, a set of experiments were run with a Naive Bayes algorithm on the SemCor corpus with six fold cross validation. The algorithm used the words in the same sentence as bag-of-words features and tried to disambiguate all tagged words. The add-one smoothing as described in (Mitchell, 1997) smooths all frequencies r/n with (r + 1)/(n + V) where V is the vocabulary size taken here to be 100000. Naive Bayes with add-one smoothing did 0.5% better than the baseline of picking the first WordNet sense. For low sample sizes Good-Turing is known as a better estimator of frequency (Gale and Church, 1994). However, in the context of Naive Bayes, Good-Turing estimation results in a significant 4.5% performance loss compared to the baseline. Lacking a satisfactory theory to explain these results I decided to use the additive smoothing with both the numerator and the denominator as adjustable parameters to be optimized. Thus a frequency r/n was smoothed as (r+a)I(n+N) where both a and N are adjustable parameters. The parameters used by the final system were selected using a simple search of the parameter space. Context size for training and testing Another important set of parameters for the bag </context>
</contexts>
<marker>Gale, Church, 1994</marker>
<rawString>W. Gale and K. Church. 1994. What&apos;s wrong with adding one? In N. Oostdijk and P. de Haan, editors, Corpus based research into language. Rodopi.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada F Mihalcea</author>
</authors>
<title>Word sense disambiguation with pattern learning and automatic feature selection.</title>
<date>2002</date>
<journal>Natural Language Engineering,</journal>
<pages>8--4</pages>
<contexts>
<context position="7534" citStr="Mihalcea, 2002" startWordPosition="1245" endWordPosition="1247">indow sizes of 64 achieves no improvement above the baseline. Unfortunately all the experiments with SemCor underperformed the model that just uses WordNet glosses for training. In fact, when both WordNet glosses and the SemCor data was used together the performance was worse than using the glosses alone. Various attempts to use semi-supervised methods to enhance SemCor with similar contexts from untagged data failed. The fact that small test window sizes consistently outperformed larger ones with SemCor indicates the importance of local collocations. The best performing system in Senseval-2 (Mihalcea, 2002) uses almost exclusively local features. As a result, I have decided to use only WordNet glosses and pointers for the bag of words model and implement an independent local collocation model. Merging models The final system consists of two independent models, one based on bag of words, another on local collocations. The model with the higher confidence provides the final answer for each instance. It may be useful to find out how different their answers are and whether the combination of models leads to a significant improvement. The performance of the baseline for the Senseval-3 all words task,</context>
</contexts>
<marker>Mihalcea, 2002</marker>
<rawString>Rada F. Mihalcea. 2002. Word sense disambiguation with pattern learning and automatic feature selection. Natural Language Engineering, 8(4):343-358.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom M Mitchell</author>
</authors>
<date>1997</date>
<journal>Machine Learning. McGraw-Hill.</journal>
<contexts>
<context position="5469" citStr="Mitchell, 1997" startWordPosition="904" endWordPosition="905">7.88%. 3 Experiments Naive Bayes Smoothing Naive Bayes has long been a popular WSD tool (Mooney, 1996; Pedersen, 2000) and according to (Yarowsky and Florian, 2002) it is one of the best performing algorithms However usually little detail is given on smoothing methods used with Naive Bayes. To compare various smoothing methods, a set of experiments were run with a Naive Bayes algorithm on the SemCor corpus with six fold cross validation. The algorithm used the words in the same sentence as bag-of-words features and tried to disambiguate all tagged words. The add-one smoothing as described in (Mitchell, 1997) smooths all frequencies r/n with (r + 1)/(n + V) where V is the vocabulary size taken here to be 100000. Naive Bayes with add-one smoothing did 0.5% better than the baseline of picking the first WordNet sense. For low sample sizes Good-Turing is known as a better estimator of frequency (Gale and Church, 1994). However, in the context of Naive Bayes, Good-Turing estimation results in a significant 4.5% performance loss compared to the baseline. Lacking a satisfactory theory to explain these results I decided to use the additive smoothing with both the numerator and the denominator as adjustabl</context>
</contexts>
<marker>Mitchell, 1997</marker>
<rawString>Tom M. Mitchell. 1997. Machine Learning. McGraw-Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mooney</author>
</authors>
<title>Comparative experiments on disambiguating word senses: An illustration of the role of bias in machine learning.</title>
<date>1996</date>
<booktitle>In EMNLP-96.</booktitle>
<contexts>
<context position="4955" citStr="Mooney, 1996" startWordPosition="822" endWordPosition="823">et 99.04% for the probability of the correct sense. Tie Breaking If a model assigns equal probability to more than one sense, the one with the smallest WordNet sense number is preferred. Merging Results In our example, both models predicted the correct sense, so this would be the answer given by the program. However had they predicted two different senses, the answer returned by the local-collocation model would be used because its confidence 99.04% is higher than the confidence of the bag-of-words model 77.88%. 3 Experiments Naive Bayes Smoothing Naive Bayes has long been a popular WSD tool (Mooney, 1996; Pedersen, 2000) and according to (Yarowsky and Florian, 2002) it is one of the best performing algorithms However usually little detail is given on smoothing methods used with Naive Bayes. To compare various smoothing methods, a set of experiments were run with a Naive Bayes algorithm on the SemCor corpus with six fold cross validation. The algorithm used the words in the same sentence as bag-of-words features and tried to disambiguate all tagged words. The add-one smoothing as described in (Mitchell, 1997) smooths all frequencies r/n with (r + 1)/(n + V) where V is the vocabulary size taken</context>
</contexts>
<marker>Mooney, 1996</marker>
<rawString>R. Mooney. 1996. Comparative experiments on disambiguating word senses: An illustration of the role of bias in machine learning. In EMNLP-96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
</authors>
<title>A simple approach to building ensembles of naive bayesian classifiers for word sense disambiguation.</title>
<date>2000</date>
<booktitle>In NAA CL-OO.</booktitle>
<contexts>
<context position="4972" citStr="Pedersen, 2000" startWordPosition="824" endWordPosition="825">the probability of the correct sense. Tie Breaking If a model assigns equal probability to more than one sense, the one with the smallest WordNet sense number is preferred. Merging Results In our example, both models predicted the correct sense, so this would be the answer given by the program. However had they predicted two different senses, the answer returned by the local-collocation model would be used because its confidence 99.04% is higher than the confidence of the bag-of-words model 77.88%. 3 Experiments Naive Bayes Smoothing Naive Bayes has long been a popular WSD tool (Mooney, 1996; Pedersen, 2000) and according to (Yarowsky and Florian, 2002) it is one of the best performing algorithms However usually little detail is given on smoothing methods used with Naive Bayes. To compare various smoothing methods, a set of experiments were run with a Naive Bayes algorithm on the SemCor corpus with six fold cross validation. The algorithm used the words in the same sentence as bag-of-words features and tried to disambiguate all tagged words. The add-one smoothing as described in (Mitchell, 1997) smooths all frequencies r/n with (r + 1)/(n + V) where V is the vocabulary size taken here to be 10000</context>
<context position="6475" citStr="Pedersen, 2000" startWordPosition="1073" endWordPosition="1074">gnificant 4.5% performance loss compared to the baseline. Lacking a satisfactory theory to explain these results I decided to use the additive smoothing with both the numerator and the denominator as adjustable parameters to be optimized. Thus a frequency r/n was smoothed as (r+a)I(n+N) where both a and N are adjustable parameters. The parameters used by the final system were selected using a simple search of the parameter space. Context size for training and testing Another important set of parameters for the bag of words model is the size of the context window for training and testing. See (Pedersen, 2000) for previous work on window sizes. A number of experiments were run using the SemCor data for training and the Senseval-2 all words task data for testing. Training and testing window radii from 1 to 64 were tried, also optimizing the N and a parameters for each case. The best results were achieved using small test window sizes. Setting both the training and testing radius to 8 results in 2.66% improvement above the baseline whereas using window sizes of 64 achieves no improvement above the baseline. Unfortunately all the experiments with SemCor underperformed the model that just uses WordNet </context>
</contexts>
<marker>Pedersen, 2000</marker>
<rawString>Ted Pedersen. 2000. A simple approach to building ensembles of naive bayesian classifiers for word sense disambiguation. In NAA CL-OO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
<author>Radu Florian</author>
</authors>
<title>Evaluating sense disambiguation across diverse parameter spaces.</title>
<date>2002</date>
<journal>Natural Language Engineering,</journal>
<pages>8--4</pages>
<contexts>
<context position="5018" citStr="Yarowsky and Florian, 2002" startWordPosition="829" endWordPosition="832">se. Tie Breaking If a model assigns equal probability to more than one sense, the one with the smallest WordNet sense number is preferred. Merging Results In our example, both models predicted the correct sense, so this would be the answer given by the program. However had they predicted two different senses, the answer returned by the local-collocation model would be used because its confidence 99.04% is higher than the confidence of the bag-of-words model 77.88%. 3 Experiments Naive Bayes Smoothing Naive Bayes has long been a popular WSD tool (Mooney, 1996; Pedersen, 2000) and according to (Yarowsky and Florian, 2002) it is one of the best performing algorithms However usually little detail is given on smoothing methods used with Naive Bayes. To compare various smoothing methods, a set of experiments were run with a Naive Bayes algorithm on the SemCor corpus with six fold cross validation. The algorithm used the words in the same sentence as bag-of-words features and tried to disambiguate all tagged words. The add-one smoothing as described in (Mitchell, 1997) smooths all frequencies r/n with (r + 1)/(n + V) where V is the vocabulary size taken here to be 100000. Naive Bayes with add-one smoothing did 0.5%</context>
</contexts>
<marker>Yarowsky, Florian, 2002</marker>
<rawString>David Yarowsky and Radu Florian. 2002. Evaluating sense disambiguation across diverse parameter spaces. Natural Language Engineering, 8(4):293-310.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>