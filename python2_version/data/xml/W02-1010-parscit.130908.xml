<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000066">
<note confidence="0.568555">
Proceedings of the Conference on Empirical Methods in Natural
Language Processing (EMNLP), Philadelphia, July 2002, pp. 71-78.
Association for Computational Linguistics.
</note>
<bodyText confidence="0.999601727272727">
representations (of shallow parses) and are used
within learning algorithms only via computing
a similarity (or kernel&apos;) function between them.
Such a use of examples allows our learning sys-
tem to implicitly explore a much larger feature
space than one computationally feasible for pro-
cessing with feature-based learning algorithms.
We conduct an experimental evaluation of our
approach in section 6. We compare our ap-
proach with the feature-based linear methods
(Roth, 1999), with promising results.
</bodyText>
<sectionHeader confidence="0.9945395" genericHeader="related work">
2 Related Work on Information
Extraction
</sectionHeader>
<bodyText confidence="0.987826153846154">
The problem of relation extraction from nat-
ural language texts was previously addressed
by Message Understanding Conferences (MUC).
A number of systems were developed that re-
lied on parsing and manual pattern development
for identifying the relations of interest (see, for
example, (Aone et al., 1998)). An adaptive
system (Miller et al., 1998), presented under
the aegis of MUC, used lexicalized probabilis-
tic context-free grammars augmented with se-
mantic information to produce a semantic parse
of text for detecting organization-location
relations. Among other popular probabilistic
formalisms for information extraction are Hid-
den Markov Models (HMM) (Bikel et al., 1999),
Maximum Entropy Markov Models (MEMM)
(McCallum et al., 2000) and Conditional Ran-
dom Fields (CRF) (Lafferty et al., 2001).
Online learning algorithms for learning lin-
ear models (e.g., Perceptron, Winnow) are be-
coming increasingly popular for NLP problems
(Roth, 1999). The algorithms exhibit a num-
ber of attractive features such as incremental
learning and scalability to a very large num-
ber of examples. Their recent applications to
shallow parsing (Munoz et al., 1999) and infor-
mation extraction (Roth and Yih, 2001) exhibit
state-of-the-art performance. The linear mod-
els are, however, feature-based which imposes
constraints on their exploiting long-range depen-
dencies in text. In section 6, we compare the
lik kernel function is a similarity function satisfy-
ing certain properties, see (Cristianini and Shawe-Taylor,
2000) for details.
methods with our approach for the relation ex-
traction problem.
We next introduce a class of kernel machine
learning methods and apply them to relation ex-
traction.
</bodyText>
<sectionHeader confidence="0.97239" genericHeader="method">
3 Kernel-based Machine Learning
</sectionHeader>
<bodyText confidence="0.99990728358209">
Most learning algorithms rely on feature-based
representation of objects. That is, an object is
transformed into a collection features f,,... , fN,
thereby producing a N-dimensional vector.
In many cases, data cannot be easily ex-
pressed via features. For example, in most NLP
problems, feature based representations produce
inherently local representations of objects, for
it is computationally infeasible to generate fea-
tures involving long-range dependencies.
Kernel methods (Vapnik, 1998; Cristianini
and Shawe-Taylor, 2000) are an attractive alter-
native to feature-based methods. Kernel meth-
ods retain the original representation of objects
and use the objects in algorithms only via com-
puting a kernel (or similarity) function between
a pair of objects.
In many cases, it may be possible to compute
a similarity function in terms of certain features
without enumerating all the features. An ex-
cellent example is that of subsequence kernels
(Lodhi et al., 2002). In this case, the objects
are strings of characters, and the similarity (ker-
nel) function computes the number of common
subsequences of characters in two strings. De-
spite the exponential number of features (sub-
sequences), it is possible to compute the sub-
sequence kernel in polytime. We therefore are
able to take advantage of long-range features in
strings without enumerating the features explic-
itly. In section 5, we will extend the subsequence
kernel to operate on shallow parses for relation
extraction.
Another pertinent example is that of parse
tree kernels(Collins and Duffy, 2001), where ob-
jects represent trees and the kernel function
computes the number of common subtrees in
two trees. The tree kernel used within the
Voted Perceptron learning algorithm (Freund
and Schapire, 1999) was shown to deliver excel-
lent performance in improving Penn Treebank
parsing. There are a number of learning al-
gorithms that can operate only using kernels of
examples. The models produced by the learn-
ing algorithms are also expressed using only ex-
amples&apos; kernels. The algorithms that process
examples only via computing their kernels are
sometimes called dual learning algorithms.
The Support Vector Machine(SVM) (Cortes
and Vapnik, 1995) is a learning algorithm that
not only allows for a dual formulation, but also
provides a rigorous rationale for resisting over-
fitting (Vapnik, 1998). After discovery of the
kernel methods, several existing learning algo-
rithms were shown to have dual analogues. For
instance, the Perceptron learning algorithm can
be easily represented in the dual form (Cris-
tianini and Shawe-Taylor, 2000). A variance-
reducing improvement of Perceptron, Voted Per-
ceptron (Freund and Schapire, 1999), is a ro-
bust and efficient learning algorithm that is very
easy to implement. It has been shown to ex-
hibit performance comparable to that of SVM.
In section 6, we experimentally evaluate SVM
and Voted Perceptron for relation extraction.
We next show how to formalize relation ex-
traction as a learning problem.
</bodyText>
<sectionHeader confidence="0.977758" genericHeader="method">
4 Problem Formalization
</sectionHeader>
<bodyText confidence="0.999806666666667">
Let us consider the sentence, &amp;quot;John Smith is the
chief scientist of the Hardcom Corp.&amp;quot;. The shal-
low parsing system produces the representation
of the sentence shown in Figure 1.
We convert the shallow parse tree into one or
more examples for the person-affiliation re-
lation. This type of relation holds between a
person and an organization. There are three
nodes in the shallow parse tree in Figure 1 refer-
ring to people, namely, the &amp;quot;John Smith&amp;quot; node
with the type &amp;quot;Person&amp;quot;, and the &amp;quot;PNP&amp;quot; nodes2.
There is one &amp;quot;Organization&amp;quot; node in the tree
that refers to an organization. We create an
example for the person-affiliation relation
by taking a person node and an organization
</bodyText>
<footnote confidence="0.923486333333333">
2Note that after the tree is produced, we do not know
if the &amp;quot;Person&amp;quot; and the &amp;quot;PNP&amp;quot; nodes refer to the same
person.
</footnote>
<figureCaption confidence="0.99083">
Figure 1: The shallow parse representation of the the
sentence &amp;quot;John Smith is the chief scientist of the Hard-
com Corp.&amp;quot; .The types &amp;quot;PNP&amp;quot;, &amp;quot;Det&amp;quot;, &amp;quot;Adj&amp;quot;, and &amp;quot;Prep&amp;quot;
denote &amp;quot;Personal Noun Phrase&amp;quot;, &amp;quot;Determiner&amp;quot;, &amp;quot;Adjec-
tive&amp;quot;, and &amp;quot;Preposition&amp;quot;, respectively.
</figureCaption>
<bodyText confidence="0.999922074074074">
node in the shallow parse tree and assigning at-
tributes to the nodes specifying the role that
a node plays in the person-affiliation rela-
tion. The person and organization under consid-
eration will receive the member and affiliation
roles, respectively. The rest of the nodes will re-
ceive none roles reflecting that they do not par-
ticipate in the relation. We then attach a label
to the example by asking the question whether
the node with the role of member and the node
with the role of affiliation are indeed (semanti-
cally) affiliated, according to the sentence. For
the above sentence, we will then generate three
positive examples, shown in Figure 2.
Note that in generating the examples between
the &amp;quot;PNP&amp;quot; and the &amp;quot;Organization&amp;quot; we elimi-
nated the nodes that did not belong to the least
common subtree of &amp;quot;Organization&amp;quot; and &amp;quot;PNP&amp;quot;,
thereby removing irrelevant subtrees.
To summarize, a relation example is shallow
parse, in which nodes are augmented with the
role attribute, and each node of the shallow
parse belongs to the least common subtree com-
prising the relation entities under consideration.
We now formalize the notion of relation ex-
ample. We first define the notion of the example
node.
</bodyText>
<construct confidence="0.5245085">
Definition 1 A node p is a set of attributes
{a1,a2,...}. The attributes are named.
</construct>
<bodyText confidence="0.997156">
We use p.a to denote the value of attribute
with the name a in the node p, e.g., p.Type =
Person and p.Role = member.
</bodyText>
<figure confidence="0.999558810810811">
Type = Sentence
Type = Person
Text =John Smith
Type = Verb
Head= be
Type = PNP
Head= scientist
Type = PNP Type = Prep
Hend scientis Text = of
Type =Entity
Text =Itardcom Corp.
Type =De Type = Adj Type =Noun
Text = the Text = chief He3d = scientis
Example #1 Type=Sentence Label=+1
Role=none
Type=Person Type=Verb Type = PNP
Text=John Smith _ Head=be Head = scientist
Rale=memher Role=none Role=none
I I
Type = PNP Type=Prep Type=Entity Corp.
Head = scientist _ Text=of _ Text=Hardcom
Rule=rione Rolen one Role = affiliation
#2
Example Type = PNP Label=+1
Head = scientist
Role=nienther
I I
Type = PNP Type=Prep Type=Entity
Head = scientist Text=of Text=Hardcom Corp.
Role=none Role=none Role = affiliation
Example #3 Type = PNP Label=+1
Head = scientist
Role=none
I I
Type = PNP Type=Prep Type=Entity
Head = scientist Text=of Text=Hardcom Corp.
Role=ineinher Role=none Role = affiliation
</figure>
<figureCaption confidence="0.9783245">
Figure 2: The three person-affiliation examples
generated from the shallow parse in Figure 1. The &amp;quot;La-
bel=+1&amp;quot; means that the examples do express the rela-
tion.
</figureCaption>
<bodyText confidence="0.819111">
Definition 2 An (unlabeled) relation example
is defined inductively as follows:
</bodyText>
<listItem confidence="0.998083">
• Let p be a node, then the pair P = (p, H)
is a relation example, where by H we denote
an empty sequence.
• Let p be a node, and [131,P2,... ,131] be a
sequence of relation examples. Then, the
pair P = (p, [Pi , P2, . ,P1]) is a relation
example.
</listItem>
<bodyText confidence="0.999521538461538">
We say that p is the parent of Pi, P2, • • •
and P2&apos;s are the children of p. We denote by
P.p the first element of the example pair, by P.c
the second element of the example pair, and use
the shorthand P.a to refer to P.p.a, and P[i] to
denote P2.
A labeled relation example is an unlabeled
relation example augmented with a label 1 E
{ —1, +1}. An example is positive, if 1 = +1,
and negative, otherwise.
We now define kernels on relation examples
that represent similarity of two shallow parse
trees.
</bodyText>
<sectionHeader confidence="0.997717" genericHeader="method">
5 Kernels for Relation Extraction
</sectionHeader>
<bodyText confidence="0.99988444">
Kernels on parse trees were previously defined
by (Collins and Duffy, 2001). The kernels enu-
merated (implicitly) all subtrees of two parse
trees, and used the number of common subtrees,
weighted appropriately, as the measure of simi-
larity between two parse trees. Since we are op-
erating with shallow parse trees, and the focus
of our problem is relation extraction rather than
parsing, we use a different definition of kernels.
We first define a matching function t(.,.) E
{0, 1} and a similarity function k(.,.) on nodes.
The matching function defined on nodes de-
termines whether the nodes are matchable or
not. For example, the nodes may be matchable
only if their types and roles match. That is,
if two nodes have the same roles, and compati-
ble types3, then their node matching function is
equal to 1; otherwise, it is equal to 0. The sim-
ilarity function on nodes is computed in terms
of the nodes&apos; attributes.
Then, for two relation examples P1, P2 we de-
fine the similarity function K P2) in terms of
similarity function of the parent nodes and the
similarity function K of the children. Formally
(o.w. means &amp;quot;otherwise&amp;quot;),
</bodyText>
<equation confidence="0.7100945">
K(Pi,P2)= {0, if t(Pi.p,P2,p)=0 (1)
k(Pi.p,P2.p)+Ke(Pi.c,P2.c), ow.
</equation>
<bodyText confidence="0.99905975">
Different definitions of the similarity function
K on children give rise to different K&apos;s. We now
give a general definition of Ke in terms of simi-
larities of children subsequences. We first intro-
duce some helpful notation (similar to (Lodhi et
al., 2002)).
We denote by i a sequence i1 &lt; i2 &lt; &lt; in
of indices, and we say that i E 1., if i is one
of the sequence indices. We also use d(i) for
— 1, and 1(i) for length of the sequence i.
For a relation example P, we denote by P[i] the
sequence of children [P[ii],
</bodyText>
<footnote confidence="0.870309">
3Some distinct types are compatible, for example,
&amp;quot;PNP&amp;quot; may be compatible with &amp;quot;Person&amp;quot;.
</footnote>
<bodyText confidence="0.6893315">
For a similarity function K,
we use K (Pi [i], P2W) to denote
K (Pi P2[ j5]). Then, we de-
fine the similarity function Ke as follows
</bodyText>
<equation confidence="0.986772">
Ke(Pi.c,P2.0= E Aci(i)+cid) K (Pi [i],P2 [j])T(i,j) (2)
iJ
/(i)=/(i)
</equation>
<bodyText confidence="0.97647688">
where
T(i,i)=1-13=1, ,l(i)r(PlEisl.v,P2D.91.19)
The formula (2) enumerates all subsequences
of relation example children with matching par-
ents, accumulates the similarity for each subse-
quence by adding the corresponding child exam-
ples&apos; similarities, and decreases the similarity by
the factor of AO) AO), 0 &lt; A &lt; 1, reflecting how
spread out the subsequences within children se-
quences. Finally, the similarity of two children
sequences is the sum all matching subsequences
similarities.
The following theorem states that the for-
mulas (1) and (2) define a kernel, under mild
assumptions (the proof is omitted for lack of
space).
Theorem 1 Let k(.,.) and t(.,.) be kernels over
nodes. Then, K as defined by (1) and (2) is a
kernel over relation examples.
We first consider a special case of Ke, where
the subsequences i and j are assumed to be con-
tiguous and give a very efficient algorithm for
computing K. In section 5.2, we address a more
general case, when the subsequences are allowed
to be sparse (non-contiguous).
</bodyText>
<subsectionHeader confidence="0.982892">
5.1 Contiguous Subtree Kernels
</subsectionHeader>
<bodyText confidence="0.999618571428571">
For contiguous subtree kernels, the similarity
function Ke enumerates only children contigu-
ous subsequences, that is, for a subsequence i in
(2), i5+1 = is ± 1 and d(i) = 1(i). Since then
d(i) = d(j) as well, we slightly abuse notation in
this section by making A stand for A2 in formula
(2). Hence, (2) becomes
</bodyText>
<equation confidence="0.9306295">
Ke(Pi.c,P2.0= E Ai(i) K (Pi [i],P2 [j])T(i,j) (3)
/(i)=/(i)
</equation>
<bodyText confidence="0.9998925">
The core of the kernel computation resides
in the formula (3). The formula enumerates
all contiguous subsequences of two children se-
quences. We now give a fast algorithm for com-
puting Ke between P1 and P2, which, given ker-
nel values for children, runs in time 0(nin),
where ni and n is the number of children of P1
and P2, respectively.
Let C(i, j) be the Ke computed for suffixes
of children sequences of P1 and P2, where every
subsequence starts with indices i and j, respec-
tively. That is,
</bodyText>
<equation confidence="0.994476333333333">
c(2,3)= E Ai(i) K (Pi [i],P2[j]T(i,j)
ij,11=1,31=3
/(i)=/(i)
</equation>
<bodyText confidence="0.997973">
Let L(i, j) be the length of the longest sequence
matching states in the children of P1 and P2
starting with indices i and j, respectively:
</bodyText>
<equation confidence="0.806468666666667">
L(i,j)=max{/ t(Pi [i±s].p,P2[j-Ps].p)=1}
Then, the following recurrences hold:
L(i,j)= 0, if t(Pifthp,P2Lii,p)=o
L(i-k1,j+1)+1, otherwise
{0, if t(Pi[i].P,P2[j],p)=0
C(i,j)- A - ALI (,j)+1 K (Pi [i],P2 [i])+AC(i+1,j+1)
otherwise
The boundary conditions are:
L(rn+1,n+1)=0 and c(rn-k1,n+1)=13
</equation>
<bodyText confidence="0.998442857142857">
The recurrence (5) follows from the observa-
tion that, if Pi [i] and P2[j] match, then ev-
ery matching pair (c1, c2) of sequences that
participated in computation of C(i ± 1,j
1) will be extended to the matching pair
( [Pi [i], c1], [P2 [j], c2]). Now we can easily com-
pute K ,(131.c, P2.c) from C (i, j).
</bodyText>
<equation confidence="0.879458">
xe(P1.c,P2.0=E„ c(i,d) (6)
</equation>
<bodyText confidence="0.999937285714286">
The time and space complexity of Ke compu-
tation is 0(nin), given kernel values for children.
Hence, for two relation examples the complex-
ity of computing K(Pi, P2) is the sum of com-
puting Ke for the matching internal nodes (as-
suming that complexities of k(.,.) and 4.,.) are
constant).
</bodyText>
<subsectionHeader confidence="0.997421">
5.2 Sparse Subtree Kernels
</subsectionHeader>
<bodyText confidence="0.988639857142857">
For sparse subtree kernels, we use the gen-
eral definition of similarity between children se-
quences as expressed by (2).
As in the previous section, we give an efficient
algorithm for computing K between P1 and P2.
The algorithm runs in time 0(m,n3), given ker-
nel values for children, where 77, and n (7n, &gt; n)
is the number of children of P1 and P2 respec-
tively.
Derivation of an efficient programming algo-
rithm for sparse subtree computation will be
presented in a full version of the paper, for lack
of space. Below we list the recurrences for com-
puting K.
</bodyText>
<equation confidence="0.943587444444444">
K 7-
K q=1 , ... ,min(m ,n) K , ,q (n,n)
Cq(i,j,a) AK,,q(i,j-1)+E . [t(P1[s].p,P2M.P)•
Cq(i,j) A2 eq- 1 (.9 -1,j -1,K(Pi [S],P2 MA
C:4(i,j) aCq (i,j)+Er =1 , ,q eq,,, (i,j)
Cq,r(i,j) AC q (i ,j -1)+ C 1,4(i ,i)
t(Pi[i],P2DDA2 C q -1 (i -1, j -1)+ AC:4 (i , -1)
ACq,r(i,j-1)±q,r(i,j)
t(P1[i],P2M) A2 eq-i, (i -1,j —1), if q$T
AC:4 , (i,i -1)±t(Pi [i],P2M) •
A2 K(Pi N,P2 [Beg- 1 (i -1,i -1), 0.1V.
The boundary conditions are
o, if q&gt;min(2,j)
0, if q&gt;min(2,j)
1,
0, if q&gt;min(2,j)
0, if q&gt;min(2,j) or q&lt;r
o, if q&gt;min(2,j) or q&lt;r
</equation>
<bodyText confidence="0.985125666666667">
As can be seen from the recurrences, the time
complexity of the algorithm is 0(777n3) (assum-
ing 77, &gt; n). The space complexity is 0(777772).
</bodyText>
<sectionHeader confidence="0.993124" genericHeader="method">
6 Experiments
</sectionHeader>
<bodyText confidence="0.998933863636364">
In this section, we apply kernel meth-
ods to extracting two types of relations
from text: person-affiliation and
organization-location.
A person and an organization are part of
the person-affiliation relation, if the person
is a member of or employed by organization.
A company founder, for example, is defined not
to be affiliated with the company (unless, it is
stated that (s)he also happens to be a company
employee).
An organization and a location are part
of the organization-location relation, if
the organization&apos;s headquarters is at the
location. Hence, if a single division of a com-
pany is located in a particular city, the company
is not necessarily located in the city.
The nuances in the above relation definitions
make the extraction problem more difficult, but
they also allow to make fine-grained distinctions
between relationships that connect entities in
text.
</bodyText>
<subsectionHeader confidence="0.992502">
6.1 Experimental Methodology
</subsectionHeader>
<bodyText confidence="0.998661393939394">
The (text) corpus for our experiments comprises
200 news articles from different news agencies
and publications (Associated Press, Wall Street
Journal, Washington Post, Los Angeles Times,
Philadelphia Inquirer).
We used the existing shallow parsing system
to generate the shallow parses for the news ar-
ticles. We generated relation examples from the
shallow parses for both relations, as described
in section 4. We retained only the examples,
for which the shallow parsing system did not
make major mistakes (90% of the generated ex-
amples). We then labeled the retained exam-
ples whether they expressed the relation of in-
terest, whereby we obtained 3524 (1262 posi-
tive) examples for the person-affiliation re-
lation and 1915 (506 positive) examples for the
org-location relation.
For each relation, we randomly split the set of
examples into a training set (60% of the exam-
ples) and a testing set (40% of the examples).
We obtained the models by running learning al-
gorithms (with kernels, where appropriate) on
the training set, testing the models on the test
set, and computing performance measures. In
order to get stable performance estimates, we
averaged performance results over 10 random
train/test splits.
We report the standard performance esti-
mates (precision, recall, and F-measure) for each
experiment.
We now describe the experimental setup of
the algorithms used in evaluation.
</bodyText>
<subsectionHeader confidence="0.958205">
6.2 Kernel Methods Configuration
</subsectionHeader>
<bodyText confidence="0.9995554375">
We evaluated two kernel learning algorithms:
Support Vector Machine (SVM) (Cortes and
Vapnik, 1995) and Voted Perceptron (Freund
and Schapire, 1999). For SVM, we used the
SVMLzght (Joachims, 1998) implementation of
the algorithm, with custom kernels incorporated
therein. We implemented the Voted Perceptron
algorithm as described in (Freund and Schapire,
1999).
We implemented both contiguous and sparse
subtree kernels and incorporated them in the
kernel learning algorithms. For both kernels, A
was set to 0.5. The only domain specific in-
formation in the two kernels was encapsulated
by the following matching t(.,.) and similarity
k(.,.) functions on nodes.4
</bodyText>
<figure confidence="0.839073428571429">
1, if Class(Pi.Type)=Class(P2.TYPe)
and PRI P Role
0,
l•- 0 e=_ 2.-0 e
0, otherwise
1, if p1.Text=P2.Text
0, otherwise
</figure>
<bodyText confidence="0.999465666666667">
We also normalized the computed kernels be-
fore their use within the algorithms. The nor-
malization corresponds to the standard unit
norm normalization of examples in the feature
space corresponding to the kernel space (Cris-
tianini and Shawe-Taylor, 2000):
</bodyText>
<equation confidence="0.505957">
K(P1,P2)- ic(pl ,p2)
oc(pl ,P1)R-(p2,P2)
</equation>
<sectionHeader confidence="0.463625" genericHeader="method">
6.3 Linear Methods Configuration
</sectionHeader>
<bodyText confidence="0.99837525">
We evaluated two feature-based algorithms for
learning linear discriminant functions: Naive-
Bayes (Duda and Hart, 1973) and Winnow (Lit-
tlestone, 1987).
</bodyText>
<footnote confidence="0.907179">
4The function Class combines some types into a
single equivalence class: Class(PNP) = Person,
Class(ONP) = Organization, Class(LNP) =
</footnote>
<bodyText confidence="0.949205153846154">
Location, and Class(Type)= Type for other types.
We implemented the two algorithms in the
spirit of the SNOW system (Roth, 1999). The
algorithms learn models that, given an exam-
ple, produce a score for each label (+1 and -1),
the predict the label corresponding to the larger
score.
Since the algorithms are feature-based, we de-
signed features for the relation extraction prob-
lem. The features are conjunctions of conditions
involving &amp;quot;Text&amp;quot;, &amp;quot;Type&amp;quot;, &amp;quot;Role&amp;quot; attributes for
neighboring example nodes. We do not list fea-
tures herein for lack of space.
</bodyText>
<subsectionHeader confidence="0.908696">
6.4 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999984521739131">
The performance results for relation extraction
are shown in Table 1.
The results indicate that kernel methods do
exhibit excellent performance and fare better
than feature-based algorithms in relation extrac-
tion. The results also highlights importance of
kernels: algorithms with the sparse subtree ker-
nels are always significantly better than their
contiguous counterparts.
The results show that performance of
the Voted Perceptron is much less accu-
rate, compared with other algorithms, for
the organization-location relation than for
the person-affiliation relation. This phe-
nomenon can be probably attributed to the fact
that the organization-location examples are
more noisy, with more boundary cases present.
For such a training set, regularization performed
by SVM is crucial; it is more noise-tolerant than
the Perceptron voting mechanism. Performance
of Naive Bayes for organization-location re-
lation is notable, since it performs as good as or
better than more elaborate algorithms.
</bodyText>
<sectionHeader confidence="0.980083" genericHeader="conclusions">
7 Conclusions and Further Work
</sectionHeader>
<bodyText confidence="0.999915625">
We presented an approach for using kernel-based
machine learning methods for extracting rela-
tions from natural language sources. We de-
fined kernels over shallow parse representations
of text and designed efficient dynamic program-
ming algorithms for computing the kernels. We
applied SVM and Voted Perceptron learning al-
gorithms with the kernels incorporated therein
</bodyText>
<equation confidence="0.997217">
t(Pi.p,P2.p) =
k(Pi.p,P2.P) =
</equation>
<table confidence="0.99739375">
person-affiliation org-location
Recall Precision F-measure Recall Precision F-measure
Naive Bayes 75.59 91.88 82.93 71.94 90.40 80.04
Winnow 80.87 88.42 84.46 75.14 85.02 79.71
Voted Perceptron (contig.) 79.58 89.74 84.34 64.43 92.85 76.02
SVM (contig.) 79.78 89.9 84.52 71.43 92.03 80.39
Voted Perceptron (sparse) 81.62 90.05 85.61 71 91.9 80.05
SVM (sparse) 82.73 91.32 86.8 76.33 91.78 83.3
</table>
<tableCaption confidence="0.99995">
Table 1: Relation extraction performance (in percentage points)
</tableCaption>
<bodyText confidence="0.9998413125">
to the tasks of relation extraction. We also com-
pared performance of the kernel-based methods
with that of the feature methods, and concluded
that kernels lead to superior performance.
In the future, we intend to apply the kernel
methodology to other sub-problems of informa-
tion extraction. For example, the shallow pars-
ing and entity extraction mechanism may also
be learned, and, perhaps, combined in a seam-
less fashion with the relation extraction formal-
ism presented herein. Furthermore, the real-
world use of extraction results requires discourse
resolution that collapses entities, noun phrases,
and pronouns into a set of equivalence classes.
We plan to apply kernel methods for discourse
processing as well.
</bodyText>
<sectionHeader confidence="0.995154" genericHeader="acknowledgments">
8 Acknowledgements
</sectionHeader>
<bodyText confidence="0.998829333333333">
Our work was supported through the DARPA
Evidence Extraction and Link Discovery Pro-
gram.
</bodyText>
<sectionHeader confidence="0.998645" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99984185483871">
C. Aone and M. Ramos-Santacruz. 2000. REES: A large-
scale relation and event extraction system. In Proceed-
ings of ANLP-2000.
C. Aone, L. Halverson, T. Hampton, and M. Ramos-
Santacruz. 1998. SRA: Description of the 1E2 system
used for MUC-7. In Proceedings of MUC-7.
D. Bikel, R. Schwartz, and R. Weischedel. 1999. An algo-
rithm that learns what&apos;s in a name. Machine Learning,
34(1-3):211-231.
M. Collins and N. Duffy. 2001. Convolution kernels for
natural language. In Proceedings of NIPS-2001.
C. Cortes and V. Vapnik. 1995. Support-vector net-
works. Machine Learning, 20(3):273-297.
N. Cristianini and J. Shawe-Taylor. 2000. An Introduc-
tion to Support Vector Machines (and Other Kernel-
Based Learning Methods). CUP
R. 0. Duda and P. E. Hart. 1973. Pattern Classification
and Scene Analysis. John Wiley, New York.
Y. Freund and R. Schapire. 1999. Large margin clas-
sification using the perceptron algorithm. Machine
Learning, 37(3):277-296.
T. Furey, N. Cristianini, N. Duffy, D. Bednarski,
M. Schummer, and D. Haussler. 2000. Support vector
machine classification and validation of cancer tissue
samples using microarray expression. Bioinformatics,
16.
D. Haussler. 1999. Convolution kernels on discrete struc-
tures. UC Santa Cruz Technical Report UCS-99-10.
T. Joachims. 1998. Text categorization with support
vector machines: learning with many relevant features.
Proceedings of ECML-98.
T. Joachims. 2002. Learning Text Classifiers with Sup-
port Vector Machines. Kluwer Academic Publishers,
Dordrecht, NL.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings of
ICML -2001.
N. Littlestone. 1987. Learning quickly when irrelevant
attributes abound: A new linear-threshold algorithm.
Machine Learning, 2:285.
H. Lodhi, C. Saunders, J. Shawe-Taylor, N. Cristianini,
and C. Watkins. 2002. Text classification using string
kernels. Journal of Machine Learning Research.
A. McCallum, D. Freitag, and F. Pereira. 2000. Maxi-
mum entropy Markov models for information extrac-
tion and segmentation. In Proceedings of International
Conference on Machine Learning, 2000.
S. Miller, M. Crystal, H. Fox, L. Ramshaw, R. Schwartz,
R. Stone, and R. Weischedel. 1998. Algorithms that
learn to extract information - BBN: Description of the
SIFT system. In Proceedings of MUC-7.
. Munoz, V. Punyakanok, D. Roth, and D. Zimak.
1999. A learning approach to shallow parsing. TR-
2087, University of Illinois at Urbana-Champaign.
D. Roth and W. Yih. 2001. Relational learning via
propositional algorithms: An information extraction
case study. In Proceedings of IJCAI-01.
D. Roth. 1999. Learning in natural language. In Pro-
ceedings of IJCAI-99.
V. Vapnik. 1998. Statistical Learning Theory. John Wi-
ley.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.939061333333333">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Philadelphia, July 2002, pp. 71-78. Association for Computational Linguistics.</note>
<abstract confidence="0.994205301136363">representations (of shallow parses) and are used within learning algorithms only via computing a similarity (or kernel&apos;) function between them. Such a use of examples allows our learning systo a much larger feature space than one computationally feasible for processing with feature-based learning algorithms. We conduct an experimental evaluation of our approach in section 6. We compare our approach with the feature-based linear methods (Roth, 1999), with promising results. 2 Related Work on Information Extraction The problem of relation extraction from natural language texts was previously addressed by Message Understanding Conferences (MUC). A number of systems were developed that relied on parsing and manual pattern development for identifying the relations of interest (see, for example, (Aone et al., 1998)). An adaptive system (Miller et al., 1998), presented under the aegis of MUC, used lexicalized probabilistic context-free grammars augmented with semantic information to produce a semantic parse text for detecting relations. Among other popular probabilistic formalisms for information extraction are Hidden Markov Models (HMM) (Bikel et al., 1999), Maximum Entropy Markov Models (MEMM) (McCallum et al., 2000) and Conditional Random Fields (CRF) (Lafferty et al., 2001). Online learning algorithms for learning linear models (e.g., Perceptron, Winnow) are beincreasingly popular for (Roth, 1999). The algorithms exhibit a number of attractive features such as incremental learning and scalability to a very large number of examples. Their recent applications to shallow parsing (Munoz et al., 1999) and information extraction (Roth and Yih, 2001) exhibit state-of-the-art performance. The linear models are, however, feature-based which imposes constraints on their exploiting long-range dependencies in text. In section 6, we compare the kernel function is a similarity function satisfying certain properties, see (Cristianini and Shawe-Taylor, 2000) for details. methods with our approach for the relation extraction problem. We next introduce a class of kernel machine learning methods and apply them to relation extraction. Machine Learning Most learning algorithms rely on feature-based representation of objects. That is, an object is into a collection features f,,... , thereby producing a N-dimensional vector. In many cases, data cannot be easily exvia features. For example, in most problems, feature based representations produce inherently local representations of objects, for it is computationally infeasible to generate features involving long-range dependencies. Kernel methods (Vapnik, 1998; Cristianini and Shawe-Taylor, 2000) are an attractive alternative to feature-based methods. Kernel methods retain the original representation of objects and use the objects in algorithms only via computing a kernel (or similarity) function between a pair of objects. In many cases, it may be possible to compute a similarity function in terms of certain features without enumerating all the features. An excellent example is that of subsequence kernels (Lodhi et al., 2002). In this case, the objects are strings of characters, and the similarity (kernel) function computes the number of common subsequences of characters in two strings. Despite the exponential number of features (subsequences), it is possible to compute the subsequence kernel in polytime. We therefore are able to take advantage of long-range features in strings without enumerating the features explicitly. In section 5, we will extend the subsequence kernel to operate on shallow parses for relation extraction. Another pertinent example is that of parse tree kernels(Collins and Duffy, 2001), where objects represent trees and the kernel function computes the number of common subtrees in two trees. The tree kernel used within the Voted Perceptron learning algorithm (Freund Schapire, 1999) was shown to deliver excellent performance in improving Penn Treebank There are a number of learning gorithms that can operate only using kernels of examples. The models produced by the learning algorithms are also expressed using only examples&apos; kernels. The algorithms that process examples only via computing their kernels are called algorithms. The Support Vector Machine(SVM) (Cortes and Vapnik, 1995) is a learning algorithm that not only allows for a dual formulation, but also provides a rigorous rationale for resisting overfitting (Vapnik, 1998). After discovery of the kernel methods, several existing learning algorithms were shown to have dual analogues. For instance, the Perceptron learning algorithm can be easily represented in the dual form (Cristianini and Shawe-Taylor, 2000). A variancereducing improvement of Perceptron, Voted Perceptron (Freund and Schapire, 1999), is a robust and efficient learning algorithm that is very easy to implement. It has been shown to exhibit performance comparable to that of SVM. In section 6, we experimentally evaluate SVM and Voted Perceptron for relation extraction. We next show how to formalize relation extraction as a learning problem. 4 Problem Formalization Let us consider the sentence, &amp;quot;John Smith is the chief scientist of the Hardcom Corp.&amp;quot;. The shallow parsing system produces the representation of the sentence shown in Figure 1. We convert the shallow parse tree into one or examples for the relation. This type of relation holds between a an are three nodes in the shallow parse tree in Figure 1 referring to people, namely, the &amp;quot;John Smith&amp;quot; node the type &amp;quot;Person&amp;quot;, and the There is one &amp;quot;Organization&amp;quot; node in the tree that refers to an organization. We create an for the taking a and an that after the tree is produced, we do not know if the &amp;quot;Person&amp;quot; and the &amp;quot;PNP&amp;quot; nodes refer to the same person. 1: shallow parse representation of the the sentence &amp;quot;John Smith is the chief scientist of the Hardcom Corp.&amp;quot; .The types &amp;quot;PNP&amp;quot;, &amp;quot;Det&amp;quot;, &amp;quot;Adj&amp;quot;, and &amp;quot;Prep&amp;quot; denote &amp;quot;Personal Noun Phrase&amp;quot;, &amp;quot;Determiner&amp;quot;, &amp;quot;Adjective&amp;quot;, and &amp;quot;Preposition&amp;quot;, respectively. node in the shallow parse tree and assigning attributes to the nodes specifying the role that node plays in the relation. The person and organization under considwill receive the roles, respectively. The rest of the nodes will rereflecting that they do not participate in the relation. We then attach a label to the example by asking the question whether node with the role of the node the role of indeed (semantically) affiliated, according to the sentence. For the above sentence, we will then generate three positive examples, shown in Figure 2. Note that in generating the examples between the &amp;quot;Organization&amp;quot; we eliminated the nodes that did not belong to the least subtree of &amp;quot;Organization&amp;quot; and thereby removing irrelevant subtrees. To summarize, a relation example is shallow parse, in which nodes are augmented with the role attribute, and each node of the shallow parse belongs to the least common subtree comprising the relation entities under consideration. We now formalize the notion of relation example. We first define the notion of the example node. 1 A p is a set of attributes The attributes are named. use denote the value of attribute the name a in the node = = member.</abstract>
<title confidence="0.709123684210526">Type = Sentence Type = Person Text =John Smith Type = Verb Head= be Type = PNP Head= scientist Type = PNP Hend scientis Type = Prep Text = of Type Text =Itardcom Corp. Type =De = Type = Adj chief Type =Noun He3d = scientis Example #1 Type=Sentence Label=+1 Role=none Text=John Type=Verb _ Head=be Role=none Type = Rale=memher Head = Role=none I I Type = Type=Prep Type=Entity _ Text=Hardcom Role = affiliation Corp. = _</title>
<author confidence="0.56355">Rulerione Rolen one</author>
<title confidence="0.95625725">Example Type = Label=+1 Head = scientist Role=nienther I I Type = Role=none Text=Hardcom Head = Role = affiliation Role=none Example #3 Type = PNP Head = scientist Label=+1 Role=none I I Type = Role=none Text=Hardcom = Role = affiliation</title>
<abstract confidence="0.986494257053292">Role=ineinher 2: three generated from the shallow parse in Figure 1. The &amp;quot;Label=+1&amp;quot; means that the examples do express the relation. 2 (unlabeled) relation example is defined inductively as follows: • Let p be a node, then the pair P = (p, H) is a relation example, where by H we denote an empty sequence. Let p be a node, and be a sequence of relation examples. Then, the P = (p, , . is a relation example. say that the parent of Pi, P2, • • • the children of denote by first element of the example pair, by the second element of the example pair, and use shorthand refer to A labeled relation example is an unlabeled example augmented with a label —1, An example is positive, if = +1, and negative, otherwise. We now define kernels on relation examples that represent similarity of two shallow parse trees. 5 Kernels for Relation Extraction Kernels on parse trees were previously defined by (Collins and Duffy, 2001). The kernels enumerated (implicitly) all subtrees of two parse trees, and used the number of common subtrees, weighted appropriately, as the measure of similarity between two parse trees. Since we are operating with shallow parse trees, and the focus of our problem is relation extraction rather than parsing, we use a different definition of kernels. first define a matching function and a similarity function nodes. The matching function defined on nodes determines whether the nodes are matchable or not. For example, the nodes may be matchable only if their types and roles match. That is, if two nodes have the same roles, and compatithen their node matching function is equal to 1; otherwise, it is equal to 0. The similarity function on nodes is computed in terms of the nodes&apos; attributes. for two relation examples we dethe similarity function terms similarity function of the parent nodes and the function the children. Formally (o.w. means &amp;quot;otherwise&amp;quot;), if t(Pi.p,P2,p)=0 (1) Different definitions of the similarity function children give rise to different now a general definition of in terms of similarities of children subsequences. We first introduce some helpful notation (similar to (Lodhi et al., 2002)). denote by sequence &lt; i2 &lt; &lt; indices, and we say that i 1., i is one of the sequence indices. We also use d(i) for 1, and 1(i) for length of the sequence a relation example denote by of children distinct types are compatible, for example, &amp;quot;PNP&amp;quot; may be compatible with &amp;quot;Person&amp;quot;. a similarity function use [i], denote we the similarity function as follows [i],P2 [j])T(i,j) (2) iJ /(i)=/(i) where The formula (2) enumerates all subsequences of relation example children with matching parents, accumulates the similarity for each subsequence by adding the corresponding child examples&apos; similarities, and decreases the similarity by the factor of AO) AO), 0 &lt; A &lt; 1, reflecting how spread out the subsequences within children sequences. Finally, the similarity of two children sequences is the sum all matching subsequences similarities. The following theorem states that the formulas (1) and (2) define a kernel, under mild assumptions (the proof is omitted for lack of space). 1 k(.,.) and t(.,.) be kernels over nodes. Then, K as defined by (1) and (2) is a kernel over relation examples. first consider a special case of subsequences j are assumed to be congive a very efficient algorithm for section 5.2, we address a more general case, when the subsequences are allowed to be sparse (non-contiguous). 5.1 Contiguous Subtree Kernels For contiguous subtree kernels, the similarity enumerates only children contigusubsequences, that is, for a subsequence = ± 1 and d(i) = then d(i) = d(j) as well, we slightly abuse notation in section by making A stand for in formula (2). Hence, (2) becomes [i],P2 [j])T(i,j) (3) /(i)=/(i) The core of the kernel computation resides in the formula (3). The formula enumerates all contiguous subsequences of two children sequences. We now give a fast algorithm for combetween and given kervalues for children, runs in time the number of children of j) the computed for suffixes children sequences of and every starts with indices i and respectively. That is, [i],P2[j]T(i,j) ij,11=1,31=3 /(i)=/(i) j) the length of the longest sequence states in the children of and with indices i and L(i,j)=max{/ [i±s].p,P2[j-Ps].p)=1} Then, the following recurrences hold: L(i,j)= if t(Pifthp,P2Lii,p)=o {0, if t(Pi[i].P,P2[j],p)=0 A-ALI (,j)+1K (Pi [i],P2 otherwise The boundary conditions are: c(rn-k1,n+1)=13 The recurrence (5) follows from the observathat, if [i] and then evmatching pair of sequences that in computation of 1) will be extended to the matching pair [Pi [i], c1], [P2 Now we can easily com- P2.c) C (i, (6) time and space complexity of compuis kernel values for children. Hence, for two relation examples the complexof computing is the sum of comfor the matching internal nodes (asthat complexities of constant). 5.2 Sparse Subtree Kernels For sparse subtree kernels, we use the general definition of similarity between children sequences as expressed by (2). As in the previous section, we give an efficient for computing P1 and algorithm runs in time kervalues for children, where n) the number of children of and respectively. Derivation of an efficient programming algorithm for sparse subtree computation will be presented in a full version of the paper, for lack of space. Below we list the recurrences for com- K , ... ,n) , K . eq- (.9 -1,K(Pi [S],P2 =1, ,q ,j -1)+ C ,i) C q-1 -1, j -1)+ (i , eq-i, -1,j if , -1)±t(Pi [i],P2M) • K(Pi -1,i -1), The boundary conditions are if 1, if As can be seen from the recurrences, the time of the algorithm is (assumn). space complexity is 6 Experiments In this section, we apply kernel methods to extracting two types of relations text: organization-location. an part of if the a member of or employed by A company founder, for example, is defined not to be affiliated with the company (unless, it is stated that (s)he also happens to be a company employee). a part the if is at the if a single division of a company is located in a particular city, the company is not necessarily located in the city. The nuances in the above relation definitions make the extraction problem more difficult, but they also allow to make fine-grained distinctions between relationships that connect entities in text. 6.1 Experimental Methodology The (text) corpus for our experiments comprises 200 news articles from different news agencies and publications (Associated Press, Wall Street Journal, Washington Post, Los Angeles Times, Philadelphia Inquirer). We used the existing shallow parsing system to generate the shallow parses for the news articles. We generated relation examples from the shallow parses for both relations, as described in section 4. We retained only the examples, for which the shallow parsing system did not make major mistakes (90% of the generated examples). We then labeled the retained examples whether they expressed the relation of interest, whereby we obtained 3524 (1262 posiexamples for the relation and 1915 (506 positive) examples for the For each relation, we randomly split the set of examples into a training set (60% of the examples) and a testing set (40% of the examples). We obtained the models by running learning algorithms (with kernels, where appropriate) on the training set, testing the models on the test set, and computing performance measures. In order to get stable performance estimates, we averaged performance results over 10 random train/test splits. We report the standard performance estimates (precision, recall, and F-measure) for each experiment. We now describe the experimental setup of the algorithms used in evaluation. 6.2 Kernel Methods Configuration We evaluated two kernel learning algorithms: Support Vector Machine (SVM) (Cortes and Vapnik, 1995) and Voted Perceptron (Freund and Schapire, 1999). For SVM, we used the (Joachims, 1998) implementation of the algorithm, with custom kernels incorporated therein. We implemented the Voted Perceptron algorithm as described in (Freund and Schapire, 1999). We implemented both contiguous and sparse subtree kernels and incorporated them in the kernel learning algorithms. For both kernels, A was set to 0.5. The only domain specific information in the two kernels was encapsulated the following matching similarity on 1, if P 0, 0 0, 1, if 0, We also normalized the computed kernels before their use within the algorithms. The normalization corresponds to the standard unit norm normalization of examples in the feature space corresponding to the kernel space (Cristianini and Shawe-Taylor, 2000): K(P1,P2)- 6.3 Linear Methods Configuration We evaluated two feature-based algorithms for learning linear discriminant functions: Naive- Bayes (Duda and Hart, 1973) and Winnow (Littlestone, 1987). function some types into a equivalence class: = Person, Class(ONP) = Organization, Class(LNP) = Type other types. We implemented the two algorithms in the spirit of the SNOW system (Roth, 1999). The algorithms learn models that, given an example, produce a score for each label (+1 and -1), the predict the label corresponding to the larger score. Since the algorithms are feature-based, we designed features for the relation extraction problem. The features are conjunctions of conditions involving &amp;quot;Text&amp;quot;, &amp;quot;Type&amp;quot;, &amp;quot;Role&amp;quot; attributes for neighboring example nodes. We do not list features herein for lack of space. 6.4 Experimental Results The performance results for relation extraction are shown in Table 1. The results indicate that kernel methods do exhibit excellent performance and fare better than feature-based algorithms in relation extraction. The results also highlights importance of kernels: algorithms with the sparse subtree kernels are always significantly better than their contiguous counterparts. The results show that performance of the Voted Perceptron is much less accurate, compared with other algorithms, for than for This phenomenon can be probably attributed to the fact the are more noisy, with more boundary cases present. For such a training set, regularization performed by SVM is crucial; it is more noise-tolerant than the Perceptron voting mechanism. Performance Naive Bayes for relation is notable, since it performs as good as or better than more elaborate algorithms. 7 Conclusions and Further Work We presented an approach for using kernel-based machine learning methods for extracting relations from natural language sources. We defined kernels over shallow parse representations of text and designed efficient dynamic programming algorithms for computing the kernels. We applied SVM and Voted Perceptron learning algorithms with the kernels incorporated therein person-affiliation org-location</abstract>
<note confidence="0.895028857142857">Recall Precision F-measure Recall Precision F-measure Naive Bayes 75.59 91.88 82.93 71.94 90.40 80.04 Winnow 80.87 88.42 84.46 75.14 85.02 79.71 Voted Perceptron (contig.) 79.58 89.74 84.34 64.43 92.85 76.02 SVM (contig.) 79.78 89.9 84.52 71.43 92.03 80.39 Voted Perceptron (sparse) 81.62 90.05 85.61 71 91.9 80.05 SVM (sparse) 82.73 91.32 86.8 76.33 91.78 83.3</note>
<abstract confidence="0.979453470588235">1: extraction performance (in percentage points) to the tasks of relation extraction. We also compared performance of the kernel-based methods with that of the feature methods, and concluded that kernels lead to superior performance. In the future, we intend to apply the kernel methodology to other sub-problems of information extraction. For example, the shallow parsing and entity extraction mechanism may also be learned, and, perhaps, combined in a seamless fashion with the relation extraction formalism presented herein. Furthermore, the realworld use of extraction results requires discourse resolution that collapses entities, noun phrases, and pronouns into a set of equivalence classes. We plan to apply kernel methods for discourse processing as well.</abstract>
<note confidence="0.854864666666667">8 Acknowledgements Our work was supported through the DARPA Evidence Extraction and Link Discovery Program. References C. Aone and M. Ramos-Santacruz. 2000. REES: A largerelation and event extraction system. In Proceedings of ANLP-2000. C. Aone, L. Halverson, T. Hampton, and M. Ramos- Santacruz. 1998. SRA: Description of the 1E2 system for MUC-7. In of MUC-7. D. Bikel, R. Schwartz, and R. Weischedel. 1999. An algothat learns what&apos;s in a name. Learning, 34(1-3):211-231. M. Collins and N. Duffy. 2001. Convolution kernels for language. In of NIPS-2001. C. Cortes and V. Vapnik. 1995. Support-vector net- Learning, Cristianini and J. Shawe-Taylor. 2000. Introduction to Support Vector Machines (and Other Kernel- Learning Methods). 0. Duda and P. E. Hart. 1973. Classification Scene Analysis. Wiley, New York. Y. Freund and R. Schapire. 1999. Large margin clas-</note>
<abstract confidence="0.7947867">using the perceptron algorithm. T. Furey, N. Cristianini, N. Duffy, D. Bednarski, M. Schummer, and D. Haussler. 2000. Support vector machine classification and validation of cancer tissue using microarray expression. 16. D. Haussler. 1999. Convolution kernels on discrete structures. UC Santa Cruz Technical Report UCS-99-10. T. Joachims. 1998. Text categorization with support vector machines: learning with many relevant features.</abstract>
<note confidence="0.9726805">Proceedings of ECML-98. Joachims. 2002. Text Classifiers with Sup-</note>
<affiliation confidence="0.711553">Vector Machines. Academic Publishers,</affiliation>
<address confidence="0.784344">Dordrecht, NL.</address>
<author confidence="0.325198">Con-</author>
<abstract confidence="0.880986583333333">ditional random fields: Probabilistic models for segand labeling sequence data. In of ICML -2001. N. Littlestone. 1987. Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. Learning, H. Lodhi, C. Saunders, J. Shawe-Taylor, N. Cristianini, and C. Watkins. 2002. Text classification using string of Machine Learning Research. A. McCallum, D. Freitag, and F. Pereira. 2000. Maximum entropy Markov models for information extracand segmentation. In of International</abstract>
<note confidence="0.617602466666667">Conference on Machine Learning, 2000. S. Miller, M. Crystal, H. Fox, L. Ramshaw, R. Schwartz, R. Stone, and R. Weischedel. 1998. Algorithms that learn to extract information - BBN: Description of the system. In of MUC-7. . Munoz, V. Punyakanok, D. Roth, and D. Zimak. 1999. A learning approach to shallow parsing. TR- 2087, University of Illinois at Urbana-Champaign. D. Roth and W. Yih. 2001. Relational learning via propositional algorithms: An information extraction study. In of IJCAI-01. Roth. 1999. Learning in natural language. In Proceedings of IJCAI-99. Vapnik. 1998. Learning Theory. Wiley.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Aone</author>
<author>M Ramos-Santacruz</author>
</authors>
<title>REES: A largescale relation and event extraction system.</title>
<date>2000</date>
<booktitle>In Proceedings of ANLP-2000.</booktitle>
<marker>Aone, Ramos-Santacruz, 2000</marker>
<rawString>C. Aone and M. Ramos-Santacruz. 2000. REES: A largescale relation and event extraction system. In Proceedings of ANLP-2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Aone</author>
<author>L Halverson</author>
<author>T Hampton</author>
<author>M RamosSantacruz</author>
</authors>
<title>SRA: Description of the 1E2 system used for MUC-7.</title>
<date>1998</date>
<booktitle>In Proceedings of MUC-7.</booktitle>
<contexts>
<context position="1013" citStr="Aone et al., 1998" startWordPosition="144" endWordPosition="147">rger feature space than one computationally feasible for processing with feature-based learning algorithms. We conduct an experimental evaluation of our approach in section 6. We compare our approach with the feature-based linear methods (Roth, 1999), with promising results. 2 Related Work on Information Extraction The problem of relation extraction from natural language texts was previously addressed by Message Understanding Conferences (MUC). A number of systems were developed that relied on parsing and manual pattern development for identifying the relations of interest (see, for example, (Aone et al., 1998)). An adaptive system (Miller et al., 1998), presented under the aegis of MUC, used lexicalized probabilistic context-free grammars augmented with semantic information to produce a semantic parse of text for detecting organization-location relations. Among other popular probabilistic formalisms for information extraction are Hidden Markov Models (HMM) (Bikel et al., 1999), Maximum Entropy Markov Models (MEMM) (McCallum et al., 2000) and Conditional Random Fields (CRF) (Lafferty et al., 2001). Online learning algorithms for learning linear models (e.g., Perceptron, Winnow) are becoming increasi</context>
</contexts>
<marker>Aone, Halverson, Hampton, RamosSantacruz, 1998</marker>
<rawString>C. Aone, L. Halverson, T. Hampton, and M. RamosSantacruz. 1998. SRA: Description of the 1E2 system used for MUC-7. In Proceedings of MUC-7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bikel</author>
<author>R Schwartz</author>
<author>R Weischedel</author>
</authors>
<title>An algorithm that learns what&apos;s in a name.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="1387" citStr="Bikel et al., 1999" startWordPosition="197" endWordPosition="200"> texts was previously addressed by Message Understanding Conferences (MUC). A number of systems were developed that relied on parsing and manual pattern development for identifying the relations of interest (see, for example, (Aone et al., 1998)). An adaptive system (Miller et al., 1998), presented under the aegis of MUC, used lexicalized probabilistic context-free grammars augmented with semantic information to produce a semantic parse of text for detecting organization-location relations. Among other popular probabilistic formalisms for information extraction are Hidden Markov Models (HMM) (Bikel et al., 1999), Maximum Entropy Markov Models (MEMM) (McCallum et al., 2000) and Conditional Random Fields (CRF) (Lafferty et al., 2001). Online learning algorithms for learning linear models (e.g., Perceptron, Winnow) are becoming increasingly popular for NLP problems (Roth, 1999). The algorithms exhibit a number of attractive features such as incremental learning and scalability to a very large number of examples. Their recent applications to shallow parsing (Munoz et al., 1999) and information extraction (Roth and Yih, 2001) exhibit state-of-the-art performance. The linear models are, however, feature-ba</context>
</contexts>
<marker>Bikel, Schwartz, Weischedel, 1999</marker>
<rawString>D. Bikel, R. Schwartz, and R. Weischedel. 1999. An algorithm that learns what&apos;s in a name. Machine Learning, 34(1-3):211-231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>N Duffy</author>
</authors>
<title>Convolution kernels for natural language.</title>
<date>2001</date>
<booktitle>In Proceedings of NIPS-2001.</booktitle>
<contexts>
<context position="3980" citStr="Collins and Duffy, 2001" startWordPosition="592" endWordPosition="595">ence kernels (Lodhi et al., 2002). In this case, the objects are strings of characters, and the similarity (kernel) function computes the number of common subsequences of characters in two strings. Despite the exponential number of features (subsequences), it is possible to compute the subsequence kernel in polytime. We therefore are able to take advantage of long-range features in strings without enumerating the features explicitly. In section 5, we will extend the subsequence kernel to operate on shallow parses for relation extraction. Another pertinent example is that of parse tree kernels(Collins and Duffy, 2001), where objects represent trees and the kernel function computes the number of common subtrees in two trees. The tree kernel used within the Voted Perceptron learning algorithm (Freund and Schapire, 1999) was shown to deliver excellent performance in improving Penn Treebank parsing. There are a number of learning algorithms that can operate only using kernels of examples. The models produced by the learning algorithms are also expressed using only examples&apos; kernels. The algorithms that process examples only via computing their kernels are sometimes called dual learning algorithms. The Support </context>
<context position="9873" citStr="Collins and Duffy, 2001" startWordPosition="1593" endWordPosition="1596">on example. We say that p is the parent of Pi, P2, • • • and P2&apos;s are the children of p. We denote by P.p the first element of the example pair, by P.c the second element of the example pair, and use the shorthand P.a to refer to P.p.a, and P[i] to denote P2. A labeled relation example is an unlabeled relation example augmented with a label 1 E { —1, +1}. An example is positive, if 1 = +1, and negative, otherwise. We now define kernels on relation examples that represent similarity of two shallow parse trees. 5 Kernels for Relation Extraction Kernels on parse trees were previously defined by (Collins and Duffy, 2001). The kernels enumerated (implicitly) all subtrees of two parse trees, and used the number of common subtrees, weighted appropriately, as the measure of similarity between two parse trees. Since we are operating with shallow parse trees, and the focus of our problem is relation extraction rather than parsing, we use a different definition of kernels. We first define a matching function t(.,.) E {0, 1} and a similarity function k(.,.) on nodes. The matching function defined on nodes determines whether the nodes are matchable or not. For example, the nodes may be matchable only if their types an</context>
</contexts>
<marker>Collins, Duffy, 2001</marker>
<rawString>M. Collins and N. Duffy. 2001. Convolution kernels for natural language. In Proceedings of NIPS-2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cortes</author>
<author>V Vapnik</author>
</authors>
<title>Support-vector networks.</title>
<date>1995</date>
<booktitle>Machine Learning,</booktitle>
<pages>20--3</pages>
<contexts>
<context position="4625" citStr="Cortes and Vapnik, 1995" startWordPosition="692" endWordPosition="695">sent trees and the kernel function computes the number of common subtrees in two trees. The tree kernel used within the Voted Perceptron learning algorithm (Freund and Schapire, 1999) was shown to deliver excellent performance in improving Penn Treebank parsing. There are a number of learning algorithms that can operate only using kernels of examples. The models produced by the learning algorithms are also expressed using only examples&apos; kernels. The algorithms that process examples only via computing their kernels are sometimes called dual learning algorithms. The Support Vector Machine(SVM) (Cortes and Vapnik, 1995) is a learning algorithm that not only allows for a dual formulation, but also provides a rigorous rationale for resisting overfitting (Vapnik, 1998). After discovery of the kernel methods, several existing learning algorithms were shown to have dual analogues. For instance, the Perceptron learning algorithm can be easily represented in the dual form (Cristianini and Shawe-Taylor, 2000). A variancereducing improvement of Perceptron, Voted Perceptron (Freund and Schapire, 1999), is a robust and efficient learning algorithm that is very easy to implement. It has been shown to exhibit performance</context>
<context position="18517" citStr="Cortes and Vapnik, 1995" startWordPosition="3052" endWordPosition="3055">0% of the examples). We obtained the models by running learning algorithms (with kernels, where appropriate) on the training set, testing the models on the test set, and computing performance measures. In order to get stable performance estimates, we averaged performance results over 10 random train/test splits. We report the standard performance estimates (precision, recall, and F-measure) for each experiment. We now describe the experimental setup of the algorithms used in evaluation. 6.2 Kernel Methods Configuration We evaluated two kernel learning algorithms: Support Vector Machine (SVM) (Cortes and Vapnik, 1995) and Voted Perceptron (Freund and Schapire, 1999). For SVM, we used the SVMLzght (Joachims, 1998) implementation of the algorithm, with custom kernels incorporated therein. We implemented the Voted Perceptron algorithm as described in (Freund and Schapire, 1999). We implemented both contiguous and sparse subtree kernels and incorporated them in the kernel learning algorithms. For both kernels, A was set to 0.5. The only domain specific information in the two kernels was encapsulated by the following matching t(.,.) and similarity k(.,.) functions on nodes.4 1, if Class(Pi.Type)=Class(P2.TYPe) </context>
</contexts>
<marker>Cortes, Vapnik, 1995</marker>
<rawString>C. Cortes and V. Vapnik. 1995. Support-vector networks. Machine Learning, 20(3):273-297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Cristianini</author>
<author>J Shawe-Taylor</author>
</authors>
<title>An Introduction to Support Vector Machines (and Other KernelBased Learning Methods).</title>
<date>2000</date>
<booktitle>CUP R. 0. Duda</booktitle>
<publisher>John Wiley,</publisher>
<location>New York.</location>
<contexts>
<context position="2215" citStr="Cristianini and Shawe-Taylor, 2000" startWordPosition="321" endWordPosition="324">on, Winnow) are becoming increasingly popular for NLP problems (Roth, 1999). The algorithms exhibit a number of attractive features such as incremental learning and scalability to a very large number of examples. Their recent applications to shallow parsing (Munoz et al., 1999) and information extraction (Roth and Yih, 2001) exhibit state-of-the-art performance. The linear models are, however, feature-based which imposes constraints on their exploiting long-range dependencies in text. In section 6, we compare the lik kernel function is a similarity function satisfying certain properties, see (Cristianini and Shawe-Taylor, 2000) for details. methods with our approach for the relation extraction problem. We next introduce a class of kernel machine learning methods and apply them to relation extraction. 3 Kernel-based Machine Learning Most learning algorithms rely on feature-based representation of objects. That is, an object is transformed into a collection features f,,... , fN, thereby producing a N-dimensional vector. In many cases, data cannot be easily expressed via features. For example, in most NLP problems, feature based representations produce inherently local representations of objects, for it is computationa</context>
<context position="5014" citStr="Cristianini and Shawe-Taylor, 2000" startWordPosition="751" endWordPosition="755">he learning algorithms are also expressed using only examples&apos; kernels. The algorithms that process examples only via computing their kernels are sometimes called dual learning algorithms. The Support Vector Machine(SVM) (Cortes and Vapnik, 1995) is a learning algorithm that not only allows for a dual formulation, but also provides a rigorous rationale for resisting overfitting (Vapnik, 1998). After discovery of the kernel methods, several existing learning algorithms were shown to have dual analogues. For instance, the Perceptron learning algorithm can be easily represented in the dual form (Cristianini and Shawe-Taylor, 2000). A variancereducing improvement of Perceptron, Voted Perceptron (Freund and Schapire, 1999), is a robust and efficient learning algorithm that is very easy to implement. It has been shown to exhibit performance comparable to that of SVM. In section 6, we experimentally evaluate SVM and Voted Perceptron for relation extraction. We next show how to formalize relation extraction as a learning problem. 4 Problem Formalization Let us consider the sentence, &amp;quot;John Smith is the chief scientist of the Hardcom Corp.&amp;quot;. The shallow parsing system produces the representation of the sentence shown in Figur</context>
<context position="19453" citStr="Cristianini and Shawe-Taylor, 2000" startWordPosition="3194" endWordPosition="3198">arse subtree kernels and incorporated them in the kernel learning algorithms. For both kernels, A was set to 0.5. The only domain specific information in the two kernels was encapsulated by the following matching t(.,.) and similarity k(.,.) functions on nodes.4 1, if Class(Pi.Type)=Class(P2.TYPe) and PRI P Role 0, l•- 0 e=_ 2.-0 e 0, otherwise 1, if p1.Text=P2.Text 0, otherwise We also normalized the computed kernels before their use within the algorithms. The normalization corresponds to the standard unit norm normalization of examples in the feature space corresponding to the kernel space (Cristianini and Shawe-Taylor, 2000): K(P1,P2)- ic(pl ,p2) oc(pl ,P1)R-(p2,P2) 6.3 Linear Methods Configuration We evaluated two feature-based algorithms for learning linear discriminant functions: NaiveBayes (Duda and Hart, 1973) and Winnow (Littlestone, 1987). 4The function Class combines some types into a single equivalence class: Class(PNP) = Person, Class(ONP) = Organization, Class(LNP) = Location, and Class(Type)= Type for other types. We implemented the two algorithms in the spirit of the SNOW system (Roth, 1999). The algorithms learn models that, given an example, produce a score for each label (+1 and -1), the predict t</context>
</contexts>
<marker>Cristianini, Shawe-Taylor, 2000</marker>
<rawString>N. Cristianini and J. Shawe-Taylor. 2000. An Introduction to Support Vector Machines (and Other KernelBased Learning Methods). CUP R. 0. Duda and P. E. Hart. 1973. Pattern Classification and Scene Analysis. John Wiley, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>R Schapire</author>
</authors>
<title>Large margin classification using the perceptron algorithm.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>37--3</pages>
<contexts>
<context position="4184" citStr="Freund and Schapire, 1999" startWordPosition="624" endWordPosition="627">ite the exponential number of features (subsequences), it is possible to compute the subsequence kernel in polytime. We therefore are able to take advantage of long-range features in strings without enumerating the features explicitly. In section 5, we will extend the subsequence kernel to operate on shallow parses for relation extraction. Another pertinent example is that of parse tree kernels(Collins and Duffy, 2001), where objects represent trees and the kernel function computes the number of common subtrees in two trees. The tree kernel used within the Voted Perceptron learning algorithm (Freund and Schapire, 1999) was shown to deliver excellent performance in improving Penn Treebank parsing. There are a number of learning algorithms that can operate only using kernels of examples. The models produced by the learning algorithms are also expressed using only examples&apos; kernels. The algorithms that process examples only via computing their kernels are sometimes called dual learning algorithms. The Support Vector Machine(SVM) (Cortes and Vapnik, 1995) is a learning algorithm that not only allows for a dual formulation, but also provides a rigorous rationale for resisting overfitting (Vapnik, 1998). After di</context>
<context position="18566" citStr="Freund and Schapire, 1999" startWordPosition="3059" endWordPosition="3062">running learning algorithms (with kernels, where appropriate) on the training set, testing the models on the test set, and computing performance measures. In order to get stable performance estimates, we averaged performance results over 10 random train/test splits. We report the standard performance estimates (precision, recall, and F-measure) for each experiment. We now describe the experimental setup of the algorithms used in evaluation. 6.2 Kernel Methods Configuration We evaluated two kernel learning algorithms: Support Vector Machine (SVM) (Cortes and Vapnik, 1995) and Voted Perceptron (Freund and Schapire, 1999). For SVM, we used the SVMLzght (Joachims, 1998) implementation of the algorithm, with custom kernels incorporated therein. We implemented the Voted Perceptron algorithm as described in (Freund and Schapire, 1999). We implemented both contiguous and sparse subtree kernels and incorporated them in the kernel learning algorithms. For both kernels, A was set to 0.5. The only domain specific information in the two kernels was encapsulated by the following matching t(.,.) and similarity k(.,.) functions on nodes.4 1, if Class(Pi.Type)=Class(P2.TYPe) and PRI P Role 0, l•- 0 e=_ 2.-0 e 0, otherwise 1</context>
</contexts>
<marker>Freund, Schapire, 1999</marker>
<rawString>Y. Freund and R. Schapire. 1999. Large margin classification using the perceptron algorithm. Machine Learning, 37(3):277-296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Furey</author>
<author>N Cristianini</author>
<author>N Duffy</author>
<author>D Bednarski</author>
<author>M Schummer</author>
<author>D Haussler</author>
</authors>
<title>Support vector machine classification and validation of cancer tissue samples using microarray expression.</title>
<date>2000</date>
<journal>Bioinformatics,</journal>
<volume>16</volume>
<marker>Furey, Cristianini, Duffy, Bednarski, Schummer, Haussler, 2000</marker>
<rawString>T. Furey, N. Cristianini, N. Duffy, D. Bednarski, M. Schummer, and D. Haussler. 2000. Support vector machine classification and validation of cancer tissue samples using microarray expression. Bioinformatics, 16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Haussler</author>
</authors>
<title>Convolution kernels on discrete structures. UC Santa Cruz</title>
<date>1999</date>
<tech>Technical Report UCS-99-10.</tech>
<marker>Haussler, 1999</marker>
<rawString>D. Haussler. 1999. Convolution kernels on discrete structures. UC Santa Cruz Technical Report UCS-99-10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Text categorization with support vector machines: learning with many relevant features.</title>
<date>1998</date>
<booktitle>Proceedings of ECML-98.</booktitle>
<contexts>
<context position="18614" citStr="Joachims, 1998" startWordPosition="3069" endWordPosition="3070">e) on the training set, testing the models on the test set, and computing performance measures. In order to get stable performance estimates, we averaged performance results over 10 random train/test splits. We report the standard performance estimates (precision, recall, and F-measure) for each experiment. We now describe the experimental setup of the algorithms used in evaluation. 6.2 Kernel Methods Configuration We evaluated two kernel learning algorithms: Support Vector Machine (SVM) (Cortes and Vapnik, 1995) and Voted Perceptron (Freund and Schapire, 1999). For SVM, we used the SVMLzght (Joachims, 1998) implementation of the algorithm, with custom kernels incorporated therein. We implemented the Voted Perceptron algorithm as described in (Freund and Schapire, 1999). We implemented both contiguous and sparse subtree kernels and incorporated them in the kernel learning algorithms. For both kernels, A was set to 0.5. The only domain specific information in the two kernels was encapsulated by the following matching t(.,.) and similarity k(.,.) functions on nodes.4 1, if Class(Pi.Type)=Class(P2.TYPe) and PRI P Role 0, l•- 0 e=_ 2.-0 e 0, otherwise 1, if p1.Text=P2.Text 0, otherwise We also normal</context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>T. Joachims. 1998. Text categorization with support vector machines: learning with many relevant features. Proceedings of ECML-98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Learning Text Classifiers with Support Vector Machines.</title>
<date>2002</date>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Dordrecht, NL.</location>
<marker>Joachims, 2002</marker>
<rawString>T. Joachims. 2002. Learning Text Classifiers with Support Vector Machines. Kluwer Academic Publishers, Dordrecht, NL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of ICML</booktitle>
<contexts>
<context position="1509" citStr="Lafferty et al., 2001" startWordPosition="216" endWordPosition="219">ied on parsing and manual pattern development for identifying the relations of interest (see, for example, (Aone et al., 1998)). An adaptive system (Miller et al., 1998), presented under the aegis of MUC, used lexicalized probabilistic context-free grammars augmented with semantic information to produce a semantic parse of text for detecting organization-location relations. Among other popular probabilistic formalisms for information extraction are Hidden Markov Models (HMM) (Bikel et al., 1999), Maximum Entropy Markov Models (MEMM) (McCallum et al., 2000) and Conditional Random Fields (CRF) (Lafferty et al., 2001). Online learning algorithms for learning linear models (e.g., Perceptron, Winnow) are becoming increasingly popular for NLP problems (Roth, 1999). The algorithms exhibit a number of attractive features such as incremental learning and scalability to a very large number of examples. Their recent applications to shallow parsing (Munoz et al., 1999) and information extraction (Roth and Yih, 2001) exhibit state-of-the-art performance. The linear models are, however, feature-based which imposes constraints on their exploiting long-range dependencies in text. In section 6, we compare the lik kernel</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of ICML -2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Littlestone</author>
</authors>
<title>Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm.</title>
<date>1987</date>
<booktitle>Machine Learning,</booktitle>
<pages>2--285</pages>
<contexts>
<context position="19678" citStr="Littlestone, 1987" startWordPosition="3226" endWordPosition="3228">functions on nodes.4 1, if Class(Pi.Type)=Class(P2.TYPe) and PRI P Role 0, l•- 0 e=_ 2.-0 e 0, otherwise 1, if p1.Text=P2.Text 0, otherwise We also normalized the computed kernels before their use within the algorithms. The normalization corresponds to the standard unit norm normalization of examples in the feature space corresponding to the kernel space (Cristianini and Shawe-Taylor, 2000): K(P1,P2)- ic(pl ,p2) oc(pl ,P1)R-(p2,P2) 6.3 Linear Methods Configuration We evaluated two feature-based algorithms for learning linear discriminant functions: NaiveBayes (Duda and Hart, 1973) and Winnow (Littlestone, 1987). 4The function Class combines some types into a single equivalence class: Class(PNP) = Person, Class(ONP) = Organization, Class(LNP) = Location, and Class(Type)= Type for other types. We implemented the two algorithms in the spirit of the SNOW system (Roth, 1999). The algorithms learn models that, given an example, produce a score for each label (+1 and -1), the predict the label corresponding to the larger score. Since the algorithms are feature-based, we designed features for the relation extraction problem. The features are conjunctions of conditions involving &amp;quot;Text&amp;quot;, &amp;quot;Type&amp;quot;, &amp;quot;Role&amp;quot; attrib</context>
</contexts>
<marker>Littlestone, 1987</marker>
<rawString>N. Littlestone. 1987. Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. Machine Learning, 2:285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Lodhi</author>
<author>C Saunders</author>
<author>J Shawe-Taylor</author>
<author>N Cristianini</author>
<author>C Watkins</author>
</authors>
<title>Text classification using string kernels.</title>
<date>2002</date>
<journal>Journal of Machine Learning Research.</journal>
<contexts>
<context position="3389" citStr="Lodhi et al., 2002" startWordPosition="499" endWordPosition="502">sentations of objects, for it is computationally infeasible to generate features involving long-range dependencies. Kernel methods (Vapnik, 1998; Cristianini and Shawe-Taylor, 2000) are an attractive alternative to feature-based methods. Kernel methods retain the original representation of objects and use the objects in algorithms only via computing a kernel (or similarity) function between a pair of objects. In many cases, it may be possible to compute a similarity function in terms of certain features without enumerating all the features. An excellent example is that of subsequence kernels (Lodhi et al., 2002). In this case, the objects are strings of characters, and the similarity (kernel) function computes the number of common subsequences of characters in two strings. Despite the exponential number of features (subsequences), it is possible to compute the subsequence kernel in polytime. We therefore are able to take advantage of long-range features in strings without enumerating the features explicitly. In section 5, we will extend the subsequence kernel to operate on shallow parses for relation extraction. Another pertinent example is that of parse tree kernels(Collins and Duffy, 2001), where o</context>
<context position="11247" citStr="Lodhi et al., 2002" startWordPosition="1825" endWordPosition="1828"> 0. The similarity function on nodes is computed in terms of the nodes&apos; attributes. Then, for two relation examples P1, P2 we define the similarity function K P2) in terms of similarity function of the parent nodes and the similarity function K of the children. Formally (o.w. means &amp;quot;otherwise&amp;quot;), K(Pi,P2)= {0, if t(Pi.p,P2,p)=0 (1) k(Pi.p,P2.p)+Ke(Pi.c,P2.c), ow. Different definitions of the similarity function K on children give rise to different K&apos;s. We now give a general definition of Ke in terms of similarities of children subsequences. We first introduce some helpful notation (similar to (Lodhi et al., 2002)). We denote by i a sequence i1 &lt; i2 &lt; &lt; in of indices, and we say that i E 1., if i is one of the sequence indices. We also use d(i) for — 1, and 1(i) for length of the sequence i. For a relation example P, we denote by P[i] the sequence of children [P[ii], 3Some distinct types are compatible, for example, &amp;quot;PNP&amp;quot; may be compatible with &amp;quot;Person&amp;quot;. For a similarity function K, we use K (Pi [i], P2W) to denote K (Pi P2[ j5]). Then, we define the similarity function Ke as follows Ke(Pi.c,P2.0= E Aci(i)+cid) K (Pi [i],P2 [j])T(i,j) (2) iJ /(i)=/(i) where T(i,i)=1-13=1, ,l(i)r(PlEisl.v,P2D.91.19) The</context>
</contexts>
<marker>Lodhi, Saunders, Shawe-Taylor, Cristianini, Watkins, 2002</marker>
<rawString>H. Lodhi, C. Saunders, J. Shawe-Taylor, N. Cristianini, and C. Watkins. 2002. Text classification using string kernels. Journal of Machine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>D Freitag</author>
<author>F Pereira</author>
</authors>
<title>Maximum entropy Markov models for information extraction and segmentation.</title>
<date>2000</date>
<booktitle>In Proceedings of International Conference on Machine Learning,</booktitle>
<contexts>
<context position="1449" citStr="McCallum et al., 2000" startWordPosition="206" endWordPosition="209">nferences (MUC). A number of systems were developed that relied on parsing and manual pattern development for identifying the relations of interest (see, for example, (Aone et al., 1998)). An adaptive system (Miller et al., 1998), presented under the aegis of MUC, used lexicalized probabilistic context-free grammars augmented with semantic information to produce a semantic parse of text for detecting organization-location relations. Among other popular probabilistic formalisms for information extraction are Hidden Markov Models (HMM) (Bikel et al., 1999), Maximum Entropy Markov Models (MEMM) (McCallum et al., 2000) and Conditional Random Fields (CRF) (Lafferty et al., 2001). Online learning algorithms for learning linear models (e.g., Perceptron, Winnow) are becoming increasingly popular for NLP problems (Roth, 1999). The algorithms exhibit a number of attractive features such as incremental learning and scalability to a very large number of examples. Their recent applications to shallow parsing (Munoz et al., 1999) and information extraction (Roth and Yih, 2001) exhibit state-of-the-art performance. The linear models are, however, feature-based which imposes constraints on their exploiting long-range d</context>
</contexts>
<marker>McCallum, Freitag, Pereira, 2000</marker>
<rawString>A. McCallum, D. Freitag, and F. Pereira. 2000. Maximum entropy Markov models for information extraction and segmentation. In Proceedings of International Conference on Machine Learning, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Miller</author>
<author>M Crystal</author>
<author>H Fox</author>
<author>L Ramshaw</author>
<author>R Schwartz</author>
<author>R Stone</author>
<author>R Weischedel</author>
</authors>
<title>Algorithms that learn to extract information - BBN: Description of the SIFT system.</title>
<date>1998</date>
<booktitle>In Proceedings of MUC-7.</booktitle>
<contexts>
<context position="1056" citStr="Miller et al., 1998" startWordPosition="151" endWordPosition="154">ly feasible for processing with feature-based learning algorithms. We conduct an experimental evaluation of our approach in section 6. We compare our approach with the feature-based linear methods (Roth, 1999), with promising results. 2 Related Work on Information Extraction The problem of relation extraction from natural language texts was previously addressed by Message Understanding Conferences (MUC). A number of systems were developed that relied on parsing and manual pattern development for identifying the relations of interest (see, for example, (Aone et al., 1998)). An adaptive system (Miller et al., 1998), presented under the aegis of MUC, used lexicalized probabilistic context-free grammars augmented with semantic information to produce a semantic parse of text for detecting organization-location relations. Among other popular probabilistic formalisms for information extraction are Hidden Markov Models (HMM) (Bikel et al., 1999), Maximum Entropy Markov Models (MEMM) (McCallum et al., 2000) and Conditional Random Fields (CRF) (Lafferty et al., 2001). Online learning algorithms for learning linear models (e.g., Perceptron, Winnow) are becoming increasingly popular for NLP problems (Roth, 1999).</context>
</contexts>
<marker>Miller, Crystal, Fox, Ramshaw, Schwartz, Stone, Weischedel, 1998</marker>
<rawString>S. Miller, M. Crystal, H. Fox, L. Ramshaw, R. Schwartz, R. Stone, and R. Weischedel. 1998. Algorithms that learn to extract information - BBN: Description of the SIFT system. In Proceedings of MUC-7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Punyakanok Munoz</author>
<author>D Roth</author>
<author>D Zimak</author>
</authors>
<title>A learning approach to shallow parsing.</title>
<date>1999</date>
<tech>TR2087,</tech>
<institution>University of Illinois at Urbana-Champaign.</institution>
<contexts>
<context position="1858" citStr="Munoz et al., 1999" startWordPosition="270" endWordPosition="273">tion-location relations. Among other popular probabilistic formalisms for information extraction are Hidden Markov Models (HMM) (Bikel et al., 1999), Maximum Entropy Markov Models (MEMM) (McCallum et al., 2000) and Conditional Random Fields (CRF) (Lafferty et al., 2001). Online learning algorithms for learning linear models (e.g., Perceptron, Winnow) are becoming increasingly popular for NLP problems (Roth, 1999). The algorithms exhibit a number of attractive features such as incremental learning and scalability to a very large number of examples. Their recent applications to shallow parsing (Munoz et al., 1999) and information extraction (Roth and Yih, 2001) exhibit state-of-the-art performance. The linear models are, however, feature-based which imposes constraints on their exploiting long-range dependencies in text. In section 6, we compare the lik kernel function is a similarity function satisfying certain properties, see (Cristianini and Shawe-Taylor, 2000) for details. methods with our approach for the relation extraction problem. We next introduce a class of kernel machine learning methods and apply them to relation extraction. 3 Kernel-based Machine Learning Most learning algorithms rely on f</context>
</contexts>
<marker>Munoz, Roth, Zimak, 1999</marker>
<rawString>. Munoz, V. Punyakanok, D. Roth, and D. Zimak. 1999. A learning approach to shallow parsing. TR2087, University of Illinois at Urbana-Champaign.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>Relational learning via propositional algorithms: An information extraction case study.</title>
<date>2001</date>
<booktitle>In Proceedings of IJCAI-01.</booktitle>
<contexts>
<context position="1906" citStr="Roth and Yih, 2001" startWordPosition="278" endWordPosition="281">babilistic formalisms for information extraction are Hidden Markov Models (HMM) (Bikel et al., 1999), Maximum Entropy Markov Models (MEMM) (McCallum et al., 2000) and Conditional Random Fields (CRF) (Lafferty et al., 2001). Online learning algorithms for learning linear models (e.g., Perceptron, Winnow) are becoming increasingly popular for NLP problems (Roth, 1999). The algorithms exhibit a number of attractive features such as incremental learning and scalability to a very large number of examples. Their recent applications to shallow parsing (Munoz et al., 1999) and information extraction (Roth and Yih, 2001) exhibit state-of-the-art performance. The linear models are, however, feature-based which imposes constraints on their exploiting long-range dependencies in text. In section 6, we compare the lik kernel function is a similarity function satisfying certain properties, see (Cristianini and Shawe-Taylor, 2000) for details. methods with our approach for the relation extraction problem. We next introduce a class of kernel machine learning methods and apply them to relation extraction. 3 Kernel-based Machine Learning Most learning algorithms rely on feature-based representation of objects. That is,</context>
</contexts>
<marker>Roth, Yih, 2001</marker>
<rawString>D. Roth and W. Yih. 2001. Relational learning via propositional algorithms: An information extraction case study. In Proceedings of IJCAI-01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roth</author>
</authors>
<title>Learning in natural language.</title>
<date>1999</date>
<booktitle>In Proceedings of IJCAI-99.</booktitle>
<contexts>
<context position="645" citStr="Roth, 1999" startWordPosition="91" endWordPosition="92">mpirical Methods in Natural Language Processing (EMNLP), Philadelphia, July 2002, pp. 71-78. Association for Computational Linguistics. representations (of shallow parses) and are used within learning algorithms only via computing a similarity (or kernel&apos;) function between them. Such a use of examples allows our learning system to implicitly explore a much larger feature space than one computationally feasible for processing with feature-based learning algorithms. We conduct an experimental evaluation of our approach in section 6. We compare our approach with the feature-based linear methods (Roth, 1999), with promising results. 2 Related Work on Information Extraction The problem of relation extraction from natural language texts was previously addressed by Message Understanding Conferences (MUC). A number of systems were developed that relied on parsing and manual pattern development for identifying the relations of interest (see, for example, (Aone et al., 1998)). An adaptive system (Miller et al., 1998), presented under the aegis of MUC, used lexicalized probabilistic context-free grammars augmented with semantic information to produce a semantic parse of text for detecting organization-l</context>
<context position="19942" citStr="Roth, 1999" startWordPosition="3267" endWordPosition="3268">norm normalization of examples in the feature space corresponding to the kernel space (Cristianini and Shawe-Taylor, 2000): K(P1,P2)- ic(pl ,p2) oc(pl ,P1)R-(p2,P2) 6.3 Linear Methods Configuration We evaluated two feature-based algorithms for learning linear discriminant functions: NaiveBayes (Duda and Hart, 1973) and Winnow (Littlestone, 1987). 4The function Class combines some types into a single equivalence class: Class(PNP) = Person, Class(ONP) = Organization, Class(LNP) = Location, and Class(Type)= Type for other types. We implemented the two algorithms in the spirit of the SNOW system (Roth, 1999). The algorithms learn models that, given an example, produce a score for each label (+1 and -1), the predict the label corresponding to the larger score. Since the algorithms are feature-based, we designed features for the relation extraction problem. The features are conjunctions of conditions involving &amp;quot;Text&amp;quot;, &amp;quot;Type&amp;quot;, &amp;quot;Role&amp;quot; attributes for neighboring example nodes. We do not list features herein for lack of space. 6.4 Experimental Results The performance results for relation extraction are shown in Table 1. The results indicate that kernel methods do exhibit excellent performance and fare </context>
</contexts>
<marker>Roth, 1999</marker>
<rawString>D. Roth. 1999. Learning in natural language. In Proceedings of IJCAI-99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Vapnik</author>
</authors>
<title>Statistical Learning Theory.</title>
<date>1998</date>
<publisher>John Wiley.</publisher>
<contexts>
<context position="2914" citStr="Vapnik, 1998" startWordPosition="425" endWordPosition="426">troduce a class of kernel machine learning methods and apply them to relation extraction. 3 Kernel-based Machine Learning Most learning algorithms rely on feature-based representation of objects. That is, an object is transformed into a collection features f,,... , fN, thereby producing a N-dimensional vector. In many cases, data cannot be easily expressed via features. For example, in most NLP problems, feature based representations produce inherently local representations of objects, for it is computationally infeasible to generate features involving long-range dependencies. Kernel methods (Vapnik, 1998; Cristianini and Shawe-Taylor, 2000) are an attractive alternative to feature-based methods. Kernel methods retain the original representation of objects and use the objects in algorithms only via computing a kernel (or similarity) function between a pair of objects. In many cases, it may be possible to compute a similarity function in terms of certain features without enumerating all the features. An excellent example is that of subsequence kernels (Lodhi et al., 2002). In this case, the objects are strings of characters, and the similarity (kernel) function computes the number of common sub</context>
<context position="4774" citStr="Vapnik, 1998" startWordPosition="718" endWordPosition="719">und and Schapire, 1999) was shown to deliver excellent performance in improving Penn Treebank parsing. There are a number of learning algorithms that can operate only using kernels of examples. The models produced by the learning algorithms are also expressed using only examples&apos; kernels. The algorithms that process examples only via computing their kernels are sometimes called dual learning algorithms. The Support Vector Machine(SVM) (Cortes and Vapnik, 1995) is a learning algorithm that not only allows for a dual formulation, but also provides a rigorous rationale for resisting overfitting (Vapnik, 1998). After discovery of the kernel methods, several existing learning algorithms were shown to have dual analogues. For instance, the Perceptron learning algorithm can be easily represented in the dual form (Cristianini and Shawe-Taylor, 2000). A variancereducing improvement of Perceptron, Voted Perceptron (Freund and Schapire, 1999), is a robust and efficient learning algorithm that is very easy to implement. It has been shown to exhibit performance comparable to that of SVM. In section 6, we experimentally evaluate SVM and Voted Perceptron for relation extraction. We next show how to formalize </context>
</contexts>
<marker>Vapnik, 1998</marker>
<rawString>V. Vapnik. 1998. Statistical Learning Theory. John Wiley.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>