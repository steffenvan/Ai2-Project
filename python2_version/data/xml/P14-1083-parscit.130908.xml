<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001929">
<title confidence="0.71672">
Response-based Learning for Grounded Machine Translation
</title>
<author confidence="0.953468">
Stefan Riezler and Patrick Simianer and Carolin Haas
</author>
<affiliation confidence="0.762843">
Department of Computational Linguistics
Heidelberg University, 69120 Heidelberg, Germany
</affiliation>
<email confidence="0.973612">
{riezler,simianer,haas1}@cl.uni-heidelberg.de
</email>
<sectionHeader confidence="0.994247" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999972611111111">
We propose a novel learning approach for
statistical machine translation (SMT) that
allows to extract supervision signals for
structured learning from an extrinsic re-
sponse to a translation input. We show
how to generate responses by grounding
SMT in the task of executing a seman-
tic parse of a translated query against
a database. Experiments on the GEO-
QUERY database show an improvement of
about 6 points in F1-score for response-
based learning over learning from refer-
ences only on returning the correct an-
swer from a semantic parse of a translated
query. In general, our approach alleviates
the dependency on human reference trans-
lations and solves the reachability problem
in structured learning for SMT.
</bodyText>
<sectionHeader confidence="0.998784" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999186558823529">
In this paper, we propose a novel approach
for learning and evaluation in statistical ma-
chine translation (SMT) that borrows ideas from
response-based learning for grounded semantic
parsing. In this framework, the meaning of a sen-
tence is defined in the context of an extrinsic task.
Successful communication of meaning is mea-
sured by a successful interaction in this task, and
feedback from this interaction is used for learning.
We suggest that in a similar way the preser-
vation of meaning in machine translation should
be defined in the context of an interaction in an
extrinsic task. For example, in the context of a
game, a description of a game rule is translated
successfully if correct game moves can be per-
formed based only on the translation. In the con-
text of a question-answering scenario, a question
is translated successfully if the correct answer is
returned based only on the translation of the query.
We propose a framework of response-based
learning that allows to extract supervision signals
for structured learning from the response of an
extrinsic task to a translation input. Here, learn-
ing proceeds by “trying out” translation hypothe-
ses, receiving a response from interacting in the
task, and converting this response into a supervi-
sion signal for updating model parameters. In case
of positive feedback, the predicted translation can
be treated as reference translation for a structured
learning update. In case of negative feedback, a
structural update can be performed against transla-
tions that have been approved previously by pos-
itive task feedback. This framework has several
advantages:
</bodyText>
<listItem confidence="0.985679095238095">
• The supervision signal in response-based
learning has a different quality than super-
vision by human-generated reference transla-
tions. While a human reference translation
is generated independently of the SMT task,
conversion of predicted translations into ref-
erences is always done with respect to a spe-
cific task. In this sense we speak of ground-
ing meaning transfer in an extrinsic task.
• Response-based learning can repeatedly try
out system predictions by interacting in the
extrinsic task. Instead of and in addition
to learning from human reference transla-
tions, response-based learning allows to con-
vert multiple system translations into refer-
ences. This alleviates the supervision prob-
lem in cases where parallel data are scarce.
• Task-specific response acts upon system
translations. This avoids the problem of un-
reachability of independently generated ref-
erence translations by the SMT system.
</listItem>
<bodyText confidence="0.9969585">
The proposed approach of response-based
learning opens the doors for various extrinsic tasks
</bodyText>
<page confidence="0.974922">
881
</page>
<note confidence="0.8308565">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 881–891,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.9999105">
in which SMT systems can be trained and evalu-
ated. In this paper, we present a proof-of-concept
experiment that uses feedback from a simulated
world environment. Building on prior work in
grounded semantic parsing, we generate transla-
tions of queries, and receive feedback by execut-
ing semantic parses of translated queries against
the database. Successful response is defined as re-
ceiving the same answer from the semantic parses
for the translation and the original query. Our ex-
perimental results show an improvement of about
6 points in F1-score for response-based learning
over standard structured learning from reference
translations. We show in an error analysis that
this improvement can be attributed to using struc-
tural and lexical variants of reference translations
as positive examples in response-based learning.
Furthermore, translations produced by response-
based learning are found to be grammatical. This
is due to the possibility to boost similarity to hu-
man reference translations by the additional use of
a cost function in our approach.
</bodyText>
<sectionHeader confidence="0.99979" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999981666666667">
The key idea of grounded language learning
is to study natural language in the context of
a non-linguistic environment, in which meaning
is grounded in perception and/or action. This
presents an analogy to human learning, where a
learner tests her understanding in an actionable
setting. Such a setting can be a simulated world
environment in which the linguistic representa-
tion can be directly executed by a computer sys-
tem. For example, in semantic parsing, the learn-
ing goal is to produce and successfully execute
a meaning representation. Executable system ac-
tions include access to databases such as the GEO-
QUERY database on U.S. geography (Wong and
Mooney (2006), inter alia), the ATIS travel plan-
ning database (Zettlemoyer and Collins (2009),
inter alia), robotic control in simulated naviga-
tion tasks (Chen and Mooney (2011), inter alia),
databases of simulated card games (Goldwasser
and Roth (2013), inter alia), or the user-generated
contents of FREEBASE (Cai and Yates (2013), in-
ter alia). Since there are many possible correct
parses, matching against a single gold standard
falls short of grounding in a non-linguistic envi-
ronment. Rather, the semantic context for inter-
pretation, as well as the success criterion in evalua-
tion is defined by successful execution of an action
in the extrinsic environment, e.g., by receiving the
correct answer from the database or by successful
navigation to the destination. Recent attempts to
learn semantic parsing from question-answer pairs
without recurring to annotated logical forms have
been presented by Kwiatowski et al. (2013), Be-
rant et al. (2013), or Goldwasser and Roth (2013).
The algorithms presented in these works are vari-
ants of structured prediction that take executability
of semantic parses into account. Our work builds
upon these ideas, however, to our knowledge the
presented work is the first to embed translations
into grounded scenarios in order to use feedback
from interactions in these scenarios for structured
learning in SMT.
A recent important research direction in SMT
has focused on employing automated translation
as an aid to human translators. Computer as-
sisted translation (CAT) subsumes several modes
of interaction, ranging from binary feedback on
the quality of the system prediction (Saluja et
al., 2012), to human post-editing operations on a
system prediction resulting in a reference transla-
tion (Cesa-Bianchi et al., 2008), to human accep-
tance or overriding of sentence completion pre-
dictions (Langlais et al., 2000; Barrachina et al.,
2008; Koehn and Haddow, 2009). In all inter-
action scenarios, it is important that the system
learns dynamically from its errors in order to of-
fer the user the experience of a system that adapts
to the provided feedback. Since retraining the
SMT model after each interaction is too costly,
online adaptation after each interaction has be-
come the learning protocol of choice for CAT. On-
line learning has been applied in generative SMT,
e.g., using incremental versions of the EM algo-
rithm (Ortiz-Martinez et al., 2010; Hardt and Elm-
ing, 2010), or in discriminative SMT, e.g., using
perceptron-type algorithms (Cesa-Bianchi et al.,
2008; Martinez-G´omez et al., 2012; W¨aschle et
al., 2013; Denkowski et al., 2014). In a simi-
lar way to deploying human feedback, extrinsic
loss functions have been used to provide learn-
ing signals for SMT. For example, Nikoulina et
al. (2012) propose a setup where an SMT system
feeds into cross-language information retrieval,
and receives feedback from the performance of
translated queries with respect to cross-language
retrieval performance. This feedback is used to
train a reranker on an n-best list of translations or-
der with respect to retrieval performance. In con-
</bodyText>
<page confidence="0.99806">
882
</page>
<figureCaption confidence="0.999873">
Figure 1: Response-based learning cycle for grounding SMT in virtual trivia gameplay.
</figureCaption>
<bodyText confidence="0.999982576923077">
trast to our work, all mentioned approaches to in-
teractive or adaptive learning in SMT rely on hu-
man post-edits or human reference translations.
Our work differs from these approaches in that
exactly this dependency is alleviated by learning
from responses in an extrinsic task.
Interactive scenarios have been used for eval-
uation purposes of translation systems for nearly
50 years, especially using human reading compre-
hension testing (Pfafflin, 1965; Fuji, 1999; Jones
et al., 2005), and more recently, using face-to-
face conversation mediated via machine transla-
tion (Sakamoto et al., 2013). However, despite of-
fering direct and reliable prediction of translation
quality, the cost and lack of reusability has con-
fined task-based evaluations involving humans to
testing scenarios, but prevented a use for interac-
tive training of SMT systems as in our work.
Lastly, our work is related to cross-lingual nat-
ural language processing such as cross-lingual
question answering or cross-lingual information
retrieval as conducted at recent evaluation cam-
paigns of the CLEF initiative.1 While these ap-
proaches focus on improvements of the respective
natural language processing task, our goal is to im-
prove SMT by gathering feedback from the task.
</bodyText>
<footnote confidence="0.956106">
1http://www.clef-initiative.eu
</footnote>
<sectionHeader confidence="0.797166" genericHeader="method">
3 Grounding SMT in Semantic Parsing
</sectionHeader>
<bodyText confidence="0.999876">
In this paper, we present a proof-of-concept of our
ideas of embedding SMT into simulated world en-
vironments as used in semantic parsing. We use
the well-known GEOQUERY database on U.S. ge-
ography for this purpose. Embedding SMT in a
semantic parsing scenario means to define transla-
tion quality by the ability of a semantic parser to
construct a meaning representation from the trans-
lated query, which returns the correct answer when
executed against the database. If viewed as simu-
lated gameplay, a valid game move in this scenario
returns the correct answer to a translated query.
The diagram in Figure 1 gives a sketch of
response-based learning from semantic parsing in
the geographical domain. Given a manual Ger-
man translation of the English query as source sen-
tence, the SMT system produces an English target
translation. This sentence is fed into a semantic
parser that produces an executable parse represen-
tation ph. Feedback is generated by executing the
parse against the database of geographical facts.
Positive feedback means that the correct answer is
received, i.e., exec(pg) ?= exec(ph) indicates that
the same answer is received from the gold standard
parse pg and the parse for the hypothesis transla-
tion ph; negative feedback results in case a differ-
ent or no answer is received.
The key advantage of response-based learning
</bodyText>
<page confidence="0.993062">
883
</page>
<bodyText confidence="0.993385611111111">
is the possibility to receive positive feedback even
from predictions that differ from gold standard
reference translations, but yet receive the cor-
rect answer when parsed and matched against the
database. Such structural and lexical variation
broadens the learning capabilities in contrast to
learning from fixed labeled data. For example,
assume the following English query in the geo-
graphical domain, and assume positive feedback
from executing the corresponding semantic parse
against the geographical database:
Name prominent elevations in the
USA
The manual translation of the English original
reads
Nenne prominente Erhebungen in
den USA
An automatic translation2 of the German string
produces the result
Give prominent surveys in the US
This translation will trigger negative task-based
feedback: A comparison with the original allows
the error to be traced back to the ambiguity of
the German word Erhebung. Choosing a gen-
eral domain translation instead of a translation ap-
propriate for the geographical domain hinders the
construction of a semantic parse that returns the
correct answer from the database. An alternative
translation might look as follows:
Give prominent heights in the US
Despite a large difference to the original En-
glish string, key terms such as elevations and
heights, or USA and US, can be mapped into the
same predicate in the semantic parse, thus allow-
ing to receive positive feedback from parse execu-
tion against the geographical database.
</bodyText>
<sectionHeader confidence="0.986025" genericHeader="method">
4 Response-based Online Learning
</sectionHeader>
<bodyText confidence="0.99961475">
Recent approaches to machine learning for SMT
formalize the task of discriminating good from
bad translations as a structured prediction prob-
lem. Assume a joint feature representation φ(x, y)
of input sentences x and output translations y E
Y (x), and a linear scoring function s(x, y; w) for
predicting a translation yˆ (where (,) denotes the
standard vector dot product) s.t.
</bodyText>
<equation confidence="0.9899725">
yˆ = arg max s(x, y; w) = arg max (w, φ(x, y)) .
Y∈Y (x) Y∈Y (x)
</equation>
<footnote confidence="0.547519">
2http://translate.google.com
</footnote>
<bodyText confidence="0.9999352">
The structured perceptron algorithm (Collins,
2002) learns an optimal weight vector w by updat-
ing w on input x(Z) by the following rule, in case
the predicted translation yˆ is different from and
scored higher than the reference translation y(Z):
</bodyText>
<equation confidence="0.983847">
w = w + φ(x(Z), y(Z)) − φ(x(Z), ˆy).
</equation>
<bodyText confidence="0.999985295454546">
This stochastic structural update aims to demote
weights of features corresponding to incorrect de-
cisions, and to promote weights of features for cor-
rect decisions.
An application of structured prediction to SMT
involves more than a straightforward replacement
of labeled output structures by reference transla-
tions. Firstly, update rules that require to com-
pute a feature representation for the reference
translation are suboptimal in SMT, because of-
ten human-generated reference translations can-
not be generated by the SMT system. Such “un-
reachable” gold-standard translations need to be
replaced by “surrogate” gold-standard translations
that are close to the human-generated translations
and still lie within the reach of the SMT sys-
tem. Computation of distance to the reference
translation usually involves cost functions based
on sentence-level BLEU (Nakov et al. (2012), in-
ter alia) and incorporates the current model score,
leading to various ramp loss objectives described
in Gimpel and Smith (2012).
An alternative approach to alleviate the depen-
dency on labeled training data is response-based
learning. Clarke et al. (2010) or Goldwasser and
Roth (2013) describe a response-driven learning
framework for the area of semantic parsing: Here
a meaning representation is “tried out” by itera-
tively generating system outputs, receiving feed-
back from world interaction, and updating the
model parameters. Applied to SMT, this means
that we predict translations and use positive re-
sponse from acting in the world to create “surro-
gate” gold-standard translations. This decreases
the dependency on a few (mostly only one) refer-
ence translations and guides the learner to promote
translations that perform well with respect to the
extrinsic task.
In the following, we will present a framework
that combines standard structured learning from
given reference translations with response-based
learning from task-approved references. We need
to ensure that gold-standard translations lead to
positive task-based feedback, that means they can
</bodyText>
<page confidence="0.990472">
884
</page>
<bodyText confidence="0.99499175">
be parsed and executed successfully against the
database. In addition, we can use translation-
specific cost functions based on sentence-level
BLEU in order to boost similarity of translations
to human reference translations.
We denote feedback by a binary execution func-
tion e(y) ∈ {1, 0} that tests whether executing
the semantic parse for the prediction against the
database receives the same answer as the parse
for the gold standard reference. Our cost function
c(y(z), y) = (1−BLEU(y(z), y)) is based on a ver-
sion of sentence-level BLEU Nakov et al. (2012).
Define y+ as a surrogate gold-standard translation
that receives positive feedback, has a high model
score, and a low cost of predicting y instead of
y(z):
</bodyText>
<equation confidence="0.958462">
( )
y+ = arg max s(x(z), y; w) − c(y(z), y) .
y∈Y (x(&apos;&apos;)):e(y)=1
</equation>
<bodyText confidence="0.999636">
The opposite of y+ is the translation y− that leads
to negative feedback, has a high model score, and
a high cost. It is defined as follows:
</bodyText>
<equation confidence="0.999372">
y− = arg max s(x(z), y; w) + c(y(z), y) .
y∈Y (x(&apos;&apos;)):e(y)=0 ( )
</equation>
<bodyText confidence="0.953988">
Update rules can be derived by minimization of
the following ramp loss objective:
</bodyText>
<equation confidence="0.9965405">
� ( )
min − max s(x(z), y; w) − c(y(z), y)
w y∈Y (x(&apos;&apos;)):e(y)=1
( )/
+ max s(x(z), y; w) + c(y(z), y)
y∈Y (x(&apos;&apos;)):e(y)=0
</equation>
<bodyText confidence="0.998632333333333">
Minimization of this objective using stochastic
(sub)gradient descent (McAllester and Keshet,
2011) yields the following update rule:
</bodyText>
<equation confidence="0.986961">
w = w + φ(x(z), y+) − φ(x(z), y−).
</equation>
<bodyText confidence="0.9998813">
The intuition behind this update rule is to discrim-
inate the translation y+ that leads to positive feed-
back and best approximates (or is identical to) the
reference within the means of the model from a
translation y− which is favored by the model but
does not execute and has high cost. This is done
by putting all the weight on the former.
Algorithm 1 presents pseudo-code for our
response-driven learning scenario. Upon predict-
ing translation ˆy, in case of positive feedback from
the task, we treat the prediction as surrogate refer-
ence by setting y+ ← ˆy, and by adding it to the
set of reference translations for future use. Then
we need to compute y−, and update by the differ-
ence in feature representations of y+ and y−, at
a learning rate η. If the feedback is negative, we
want to move the weights away from the predic-
tion, thus we treat it as y−. To perform an update,
we need to compute y+. If either y+ or y− cannot
be computed, the example is skipped.
</bodyText>
<figure confidence="0.822117055555556">
Algorithm 1 Response-based Online Learning
repeat
for i = 1,...,n do
Receive input string x(z)
Predict translation yˆ
Receive task feedback e(ˆy) ∈ {1, 0}
if e(ˆy) = 1 then
y+ ←yˆ
Store yˆ as reference y(z) for x(z)
Compute y−
else
y− ← yˆ
Receive reference y(z)
Compute y+
end if
w ← w + η(φ(x(z), y+) − φ(x(z), y−))
end for
until Convergence
</figure>
<sectionHeader confidence="0.97708" genericHeader="method">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.987114">
5.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.951841705882353">
In our experiments, we use the GEOQUERY
database on U.S. geography as provided by Jones
The sketched algorithm allows several varia-
tions. In the form depicted above, it allows
. to use human reference translations in addition
to task-approved surrogate references. The cost
function can be implemented by different ver-
sions of sentence-wise BLEU, or it can be omitted
completely so that learning relies on task-based
feedback alone, similar to algorithms recently
suggested for semantic parsing (Goldwasser and
Roth, 2013; Kwiatowski et al., 2013; Berant et
al., 2013). Lastly, regularization can be intro-
duced by using update rules corresponding to pri-
mal form optimization variants of support vector
machines (Collobert and Bengio, 2004; Chapelle,
2007; Shalev-Shwartz et al., 2007).
</bodyText>
<page confidence="0.980009">
885
</page>
<table confidence="0.9996706">
method precision recall F1 BLEU
1 CDEC 63.67 58.21 60.82 46.53
2 EXEC 70.36 63.57 66.791 48.001
3 RAMPION 75.58 69.64 72.4912 56.6412
4 REBOL 81.15 75.36 78.15123 55.6612
</table>
<tableCaption confidence="0.967486">
Table 1: Experimental results using extended parser for returning answers from GEOQUERY (precision,
recall, F1) and n-gram match to original English query (BLEU) on 280 re-translated test examples. Best
results for each column are highlighted in bold face. Superscripts 1234 denote a significant improvement
over the respective method.
</tableCaption>
<table confidence="0.999466">
method precision recall F1 BLEU
1 CDEC 65.59 57.86 61.48 46.53
2 EXEC 66.54 61.79 64.07 46.00
3 RAMPION 67.68 63.57 65.56 55.6712
4 REBOL 70.68 67.14 68.8612 55.6712
</table>
<tableCaption confidence="0.992018">
Table 2: Experimental results using the original parser for returning answers from GEOQUERY (preci-
</tableCaption>
<bodyText confidence="0.986528071428572">
sion, recall, F1) and n-gram match to original English query (BLEU) on 280 re-translated test examples.
et al. (2012).3 The dataset includes 880 English
questions and their logical forms. The English
strings were manually translated into German by
the authors of Jones et al. (2012)), and corrected
for typos by the authors of this paper. We follow
the provided split into 600 training examples and
280 test examples.
For response-based learning, we retrained the
semantic parser of Andreas et al. (2013)4 on the
full 880 GEOQUERY examples in order to reach
full parse coverage. This parser is itself based on
SMT, trained on parallel data consisting of English
queries and linearized logical forms, and on a lan-
guage model trained on linearized logical forms.
We used the hierarchical phrase-based variant of
the parser. Note that we do not use GEOQUERY
test data in SMT training. Parser training includes
GEOQUERY test data in order to be less depen-
dent on parse and execution failures in the eval-
uation: If a translation system, response-based or
reference-based, translates the German input into
the gold standard English query it should be re-
warded by positive task feedback. To double-
check whether including the 280 test examples
in parser training gives an unfair advantage to
response-based learning, we also present experi-
mental results using the original parser of Andreas
</bodyText>
<footnote confidence="0.9813925">
3http://homepages.inf.ed.ac.uk/
s1051107/geoquery-2012-08-27.zip
4https://github.com/jacobandreas/
smt-semparse
</footnote>
<bodyText confidence="0.997339384615385">
et al. (2013) that is trained only on the 600 GEO-
QUERY training examples.
The bilingual SMT system used in our experi-
ments is the state-of-the-art SCFG decoder CDEC
(Dyer et al., 2010)5. We built grammars us-
ing its implementation of the suffix array extrac-
tion method described in Lopez (2007). For lan-
guage modeling, we built a modified Kneser-Ney
smoothed 5-gram language model using the En-
glish side of the training data. We trained the SMT
system on the English-German parallel web data
provided in the COMMON CRAWL6 (Smith et al.,
2013) dataset.
</bodyText>
<subsectionHeader confidence="0.998867">
5.2 Compared Systems
</subsectionHeader>
<bodyText confidence="0.999651166666667">
Method 1 is the baseline system, consisting of
the CDEC SMT system trained on the COMMON
CRAWL data as described above. This system does
not use any GEOQUERY data for training. Meth-
ods 2-4 use the 600 training examples from GEO-
QUERY for discriminative training only.
Variants of the response-based learning algo-
rithm described above are implemented as a stand-
alone tool that operates on CDEC n-best lists of
10,000 translations of the GEOQUERY training
data. All variants use sparse features of CDEC as
described in Simianer et al. (2012) that extract rule
</bodyText>
<footnote confidence="0.993884">
5https://github.com/redpony/cdec
6http://www.statmt.org/wmt13/
training-parallel-commoncrawl.tgz
</footnote>
<page confidence="0.998443">
886
</page>
<bodyText confidence="0.998813357142857">
prediction: how many inhabitants has new york
reference: how many people live in new york
prediction: how big is the population of texas
reference: how many people live in texas
prediction: which are the cities of the state with the highest elevation
reference: what are the cities of the state with the highest point
prediction: how big is the population of states , through which the mississippi runs
reference: what are the populations of the states through which the mississippi river runs
prediction: what state borders california
reference: what is the adjacent state of california
prediction: what are the capitals of the states which have cities with the name durham
reference: what is the capital of states that have cities named durham
prediction: what rivers go through states with the least cities
reference: which rivers run through states with fewest cities
</bodyText>
<tableCaption confidence="0.7181465">
Table 3: Predicted translations by response-based learning (REBOL) leading to positive feedback versus
gold standard references.
</tableCaption>
<bodyText confidence="0.9999766">
shapes, rule identifiers, and bigrams in rule source
and target directly from grammar rules. Method
4, named REBOL, implements REsponse-Based
Online Learning by instantiating y+ and y− to
the form described in Section 4: In addition to
the model score s, it uses a cost function c based
on sentence-level BLEU (Nakov et al., 2012) and
tests translation hypotheses for task-based feed-
back using a binary execution function e. This
algorithm can convert predicted translations into
references by task-feedback, and additionally use
the given original English queries as references.
Method 2, named EXEC, relies on task-execution
by function e and searches for executable or non-
executable translations with highest score s to dis-
tinguish positive from negative training examples.
It does not use a cost function and thus cannot
make use of the original English queries.
We compare response-based learning with a
standard structured prediction setup that omits the
use of the execution function e in the definition
of y+ and y−. This algorithm can be seen as a
stochastic (sub)gradient descent variant of RAM-
PION (Gimpel and Smith, 2012). It does not make
use of the semantic parser, but defines positive and
negative examples based on score s and cost c with
respect to human reference translations.
We report BLEU (Papineni et al., 2001) of
translation system output measured against the
original English queries. Furthermore, we report
precision, recall, and F1-score for executing se-
mantic parses built from translation system out-
puts against the GEOQUERY database. Precision
is defined as the percentage of correctly answered
examples out of those for which a parse could be
produced; recall is defined as the percentage of to-
tal examples answered correctly; F1-score is the
harmonic mean of both. Statistical significance
is measured using Approximate Randomization
(Noreen, 1989) where result differences with a p-
value smaller than 0.05 are considered statistically
significant.
Methods 2-4 perform structured learning for
SMT on the 600 GEOQUERY training examples
and re-translate the 280 unseen GEOQUERY test
data, following the data split of Jones et al. (2012).
Training for RAMPION, REBOL and EXEC was re-
peated for 10 epochs. The learning rate q is set to
a constant that is adjusted by cross-validation on
the 600 training examples.
</bodyText>
<subsectionHeader confidence="0.990523">
5.3 Empirical Results
</subsectionHeader>
<bodyText confidence="0.9999765">
We present an experimental comparison of the
four different systems according to BLEU and
</bodyText>
<page confidence="0.994364">
887
</page>
<bodyText confidence="0.989823038461539">
reference RAMPION REBOL
how many colorado rivers are
there
what are the populations of
states which border texas
what is the biggest capital city in
the us
how many rivers with the name
colorado gives it
how big are the populations of
the states , which in texas bor-
ders
how many rivers named col-
orado are there
how big are the populations of
the states which on texas border
what is the largest city in the usa what is the largest capital in the
usa
what state borders new york what states limits of new york what states border new york
which states border the state
with the smallest area
what states boundaries of the
state with the smallest surface
area
what states border the state with
the smallest surface area
</bodyText>
<tableCaption confidence="0.995522">
Table 4: Predicted translations by response-based learning (REBOL) leading to positive feedback versus
translations by supervised structured learning (RAMPION) leading to negative feedback.
</tableCaption>
<bodyText confidence="0.999928482142858">
F1, using an extended semantic parser (trained
on 880 GEOQUERY examples) and the original
parser (trained on 600 GEOQUERY training exam-
ples). The extended parser reaches and F1-score
of 99.64% on the 280 GEOQUERY test examples;
the original parser yields an F1-score of 82.76%.
Table 1 reports results for the extended seman-
tic parser. A system ranking according to F1-
score shows about 6 points difference between the
respective methods, ranking REBOL over RAM-
PION, EXEC and CDEC. The exploitation of task-
feedback allows both EXEC and REBOL to im-
prove task-performance over the baseline. RE-
BOL’s combination of task feedback with a cost
function achieves the best results since positively
executable hypotheses and reference translations
can both be exploited to guide the learning pro-
cess. Since all English reference queries lead to
positively executable parses in the setup that uses
the extended semantic parser, RAMPION implic-
itly also has access to task feedback. This allows
RAMPION to improve F1 over the baseline. All
result differences are statistically significant.
In terms of BLEU score measured against the
original English GEOQUERY queries, the best
nominal result is obtained by RAMPION which
uses them as reference translations. REBOL per-
forms worse since BLEU performance is opti-
mized only implicitly in cases where original En-
glish queries function as positive examples. How-
ever, the result differences between these two
systems do not score as statistically significant.
Despite not optimizing for BLEU performance
against references, the fact that positively exe-
cutable translations include the references allows
even EXEC to improve BLEU over CDEC which
does not use GEOQUERY data at all in training.
This result difference is statistically significant.
Table 2 compares the same systems using the
original parser trained on 600 training examples.
The system ranking according to F1-score shows
the same ordering that is obtained when using an
extended semantic parser. However, the respec-
tive methods are separated only by 3 or less points
in F1 score such that only the result difference of
REBOL over the baseline CDEC and over EXEC is
statistically significant. We conjecture that this is
due to a higher number of empty parses on the test
set which makes this comparison unstable.
In terms of BLEU measured against the original
queries, the result differences between REBOL and
RAMPION are not statistically significant, and nei-
ther are the result differences between EXEC and
CDEC. The result differences between systems of
the former group and the systems of latter group
are statistically significant.
</bodyText>
<subsectionHeader confidence="0.950172">
5.4 Error Analysis
</subsectionHeader>
<bodyText confidence="0.996539333333333">
For a better understanding of the differences be-
tween the results produced by supervised and
response-based learning, we conducted an er-
</bodyText>
<page confidence="0.99593">
888
</page>
<sectionHeader confidence="0.504656" genericHeader="method">
reference RAMPION REBOL
</sectionHeader>
<bodyText confidence="0.998680470588236">
how many states have a higher
point than the highest point of
the state with the largest capital
city in the us
how many states have a higher
nearby point as the highest point
of the state with the largest capi-
tal in the usa
how many states have a high
point than the highest point of
the state with the largest capital
in the usa
how tall is mount mckinley how high is mount mckinley what is mount mckinley
what is the longest river that
flows through a state that borders
indiana
what states does the mississippi
river run through
which is the highest peak not in
alaska
how is the longest river, which
runs through a state, borders the
of indiana
through which states runs the
mississippi
how is the highest peaks of not
in alaska is
what is the longest river which
runs through a state of indiana
borders
through which states is the mis-
sissippi
what is the highest peak in
alaska is
</bodyText>
<tableCaption confidence="0.826487">
Table 5: Predicted translations where supervised structured learning (RAMPION) leads to positive feed-
back versus translations by response-based learning (REBOL) leading to negative feedback.
</tableCaption>
<bodyText confidence="0.999863225806452">
ror analysis on the test examples. Table 3
shows examples where the translation predicted by
response-based learning (REBOL) differs from the
gold standard reference translation, but yet leads
to positive feedback via a parse that returns the
correct answer from the database. The examples
show structural and lexical variation that leads to
differences on the string level at equivalent posi-
tive feedback from the extrinsic task. This can ex-
plain the success of response-based learning: Lex-
ical and structural variants of reference transla-
tions can be used to boost model parameters to-
wards translations with positive feedback, while
the same translations might be considered as neg-
ative examples in standard structured learning.
Table 4 shows examples where translations
from REBOL and RAMPION differ from the gold
standard reference, and predictions by REBOL
lead to positive feedback, while predictions by
RAMPION lead to negative feedback. Table 5
shows examples where translations from RAM-
PION outperform translations from REBOL in
terms of task feedback. We see that predictions
from both systems are in general grammatical.
This can be attributed to the use of sentence-
level BLEU as cost function in RAMPION and
REBOL. Translation errors of RAMPION can be
traced back to mistranslations of key terms (city
versus capital, limits or boundaries versus
border). Translation errors of REBOL more fre-
quently show missing translations of terms.
</bodyText>
<sectionHeader confidence="0.999012" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999848">
We presented a proposal for a new learning and
evaluation framework for SMT. The central idea
is to ground meaning transfer in successful in-
teraction in an extrinsic task, and use task-based
feedback for structured learning. We presented a
proof-of-concept experiment that defines the ex-
trinsic task as executing semantic parses of trans-
lated queries against the GEOQUERY database.
Our experiments show an improvement of about
6 points in F1-score for response-based learning
over structured learning from reference transla-
tions. Our error analysis shows that response-
based learning generates grammatical translations
which is due to the additional use of a cost func-
tion that boosts similarity of translations to human
reference translations.
In future work, we would like to extend our
work on embedding SMT in virtual gameplay to
larger and more diverse datasets, and involve hu-
man feedback in the response-based learning loop.
</bodyText>
<sectionHeader confidence="0.995507" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.964357">
Jacob Andreas, Andreas Vlachos, and Stephen Clark.
2013. Semantic parsing as machine translation. In
</reference>
<page confidence="0.99518">
889
</page>
<reference confidence="0.997676495412844">
Proceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics (ACL’13),
Sofia, Bulgaria.
Sergio Barrachina, Oliver Bender, Francisco Casacu-
berta, Jorge Civera, Elsa Cubel, Shahram Khadivi,
Antonio Lagarda, Hermann Ney, Jes´us Tom´as, En-
rique Vidal, and Juan-Miguel Vilar. 2008. Sta-
tistical approaches to computer-assisted translation.
Computational Linguistics, 35(1):3–28.
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on freebase from
question-answer pairs. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP’13), Seattle, WA.
Qingqing Cai and Alexander Yates. 2013. Large-scale
semantic parsing via schema matching and lexicon
extenstion. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguistics
(ACL’13), Sofia, Bulgaria.
Nicol`o Cesa-Bianchi, Gabriele Reverberi, and San-
dor Szedmak. 2008. Online learning algorithms
for computer-assisted translation. Technical report,
SMART (www.smart-project.eu).
Olivier Chapelle. 2007. Training a support vec-
tor machine in the primal. Neural Computation,
19(5):1155–1178.
David L. Chen and Raymond J. Mooney. 2011.
Learning to interpret natural language navigation
instructions from observations. In Proceedings of
the 25th AAAI Conference on Artificial Intelligence
(AAAI’11), pages 859–866, San Francisco, CA.
James Clarke, Dan Goldwasser, Wing-Wei Chang, and
Dan Roth. 2010. Driving semantic parsing from the
world’s response. In Proceedings of the 14th Con-
ference on Natural Language Learning (CoNLL’10),
pages 18–27, Uppsala, Sweden.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In Proceedings
of the conference on Empirical Methods in Nat-
ural Language Processing (EMNLP’02), Philadel-
phia, PA.
Ronan Collobert and Samy Bengio. 2004. Links be-
tween perceptrons, MLPs, and SVMs. In Proceed-
ings of the 21st International Conference on Ma-
chine Learning (ICML’04), Banff, Canada.
Michael Denkowski, Chris Dyer, and Alon Lavie.
2014. Learning from post-editing: Online model
adaptation for statistical machine translation. In
Proceedings of the 14th Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL’14), Gothenburg, Sweden.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proceedings of the ACL 2010 System Demonstra-
tions, Uppsala, Sweden.
Masaru Fuji. 1999. Evaluation experiment for reading
comprehension of machine translation outputs. In
Proceedings of the Machine Translation Summit VII,
Singapore.
Kevin Gimpel and Noah A. Smith. 2012. Structured
ramp loss minimization for machine translation.
In Proceedings of 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(NAACL-HLT 2012), Montreal, Canada.
Dan Goldwasser and Dan Roth. 2013. Learning from
natural instructions. Machine Learning, 94(2):205–
232.
Daniel Hardt and Jakob Elming. 2010. Incremental
re-training for post-editing SMT. In Proceedings of
the 9th Conference of the Association for Machine
Tranlation in the Americas (AMTA’10), Denver, CO.
Douglas Jones, Wade Shen, Neil Granoien, Martha
Herzog, and Clifford Weinstein. 2005. Measuring
translation quality by testing english speakers with
a new defense language proficiency test for arabic.
In Proceedings of 2005 International Conference on
Intelligence Analysis, McLean, VA.
Bevan K. Jones, Mark Johnson, and Sharon Goldwater.
2012. Semantic parsing with bayesion tree trans-
ducers. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguistics
(ACL’12), Jeju Island, Korea.
Philipp Koehn and Barry Haddow. 2009. Interactive
assistance to human translators using statistical ma-
chine translation methods. In Proceedings of MT
Summit XII, Ottawa, Ontario, Canada.
Tom Kwiatowski, Eunsol Choi, Yoav Artzi, and Luke
Zettlemoyer. 2013. Scaling semantic parsers with
on-the-fly ontology matching. In Proceedings of the
2013 Conference on Empirical Methods in Natural
Language Processing (EMNLP’13), Seattle, WA.
Philippe Langlais, George Foster, and Guy Lapalme.
2000. Transtype: a computer-aided translation typ-
ing system. In Proceedings of the ANLP-NAACL
2000 Workshop on Embedded Machine Translation
Systems, Seattle, WA.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In Proceedings of the Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL 2007), Prague,
Czech Republic.
Pascual Mart´ınez-G´omez, Germ´an Sanchis-Trilles, and
Francisco Casacuberta. 2012. Online adaptation
</reference>
<page confidence="0.976293">
890
</page>
<reference confidence="0.999584352941176">
strategies for statistical machine translation in post-
editing scenarios. Pattern Recognition, 45(9):3193–
3202.
David McAllester and Joseph Keshet. 2011. General-
ization bounds and consistency for latent structural
probit and ramp loss. In Proceedings of the 25th An-
nual Conference on Neural Information Processing
Sytems (NIPS 2011), Granada, Spain.
Preslav Nakov, Francisco Guzm´an, and Stephan Vogel.
2012. Optimizing for sentence-level bleu+1 yields
short translations. In Proceedings of the 24th Inter-
national Conference on Computational Linguistics
(COLING 2012), Bombay, India.
Vassilina Nikoulina, Bogomil Kovachev, Nikolaos La-
gos, and Christof Monz. 2012. Adaptation of statis-
tical machine translation model for cross-lingual in-
formation retrieval in a service context. In Proceed-
ings of the 13th Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL’12), Avignon, France.
Eric W. Noreen. 1989. Computer Intensive Meth-
ods for Testing Hypotheses. An Introduction. Wiley,
New York.
Daniel Ortiz-Martinez, Ismal Garcia-Varea, and Fran-
cisco Casacuberta. 2010. Online learning for in-
teractive statistical machine translation. In Proceed-
ings of the Human Language Technologies confer-
ence and the 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics (HLT-NAACL’10), Los Angeles,
CA.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic
evaluation of machine translation. Technical Report
IBM Research Division Technical Report, RC22176
(W0190-022), Yorktown Heights, N.Y.
Sheila M. Pfafflin. 1965. Evaluation of machine trans-
lations by reading comprehension tests and subjec-
tive judgements. Mechanical Translation and Com-
putational Linguistics, 8(2):2–8.
Akiko Sakamoto, Nayuko Watanabe, Satoshi Ka-
matani, and Kazuo Sumita. 2013. Development of a
simultaneous interpretation system for face-to-face
services and its evaluation experiment in real situ-
ation. In Proceedings of the Machine Translation
Summit XIV, Nice, France.
Avneesh Saluja, Ian Lane, and Ying Zhang. 2012.
Machine translation with binary feedback: A large-
margin approach. In Proceedings of the 10th Bi-
ennial Conference of the Association for Machine
Translation in the Americas (AMTA’12), San Diego,
CA.
Shai Shalev-Shwartz, Yoram Singer, and Nathan Sre-
bro. 2007. Pegasos: Primal Estimated sub-
GrAdient SOlver for SVM. In Proceedings of the
24th International Conference on Machine Learning
(ICML’07), Corvallis, OR.
Patrick Simianer, Stefan Riezler, and Chris Dyer.
2012. Joint feature selection in distributed stochas-
tic learning for large-scale discriminative training in
SMT. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics (ACL
2012), Jeju, Korea.
Jason R. Smith, Herve Saint-Amand, Magdalena Pla-
mada, Philipp Koehn, Chris Callison-Burch, and
Adam Lopez. 2013. Dirt cheap web-scale paral-
lel text from the common crawl. In Proceedings of
the 51st Annual Meeting of the Association for Com-
putational Linguistics (ACL’13), Sofia, Bulgaria.
Katharina W¨aschle, Patrick Simianer, Nicola Bertoldi,
Stefan Riezler, and Marcello Federico. 2013. Gen-
erative and discriminative methods for online adap-
tation in SMT. In Proceedings of the Machine
Translation Summit XIV, Nice, France.
Yuk Wah Wong and Raymond J. Mooney. 2006.
Learning for semantic parsing with statistical ma-
chine translation. In Proceedings of the Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics (HLT/NAACL’06), New York City, NY.
Luke S. Zettlemoyer and Michael Collins. 2009.
Learning context-dependent mappings from sen-
tences to logical form. In Proceedings of the 47th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL-IJCNLP’09), Singapore.
</reference>
<page confidence="0.9986">
891
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.867032">
<title confidence="0.999782">Response-based Learning for Grounded Machine Translation</title>
<author confidence="0.995788">Riezler Simianer</author>
<affiliation confidence="0.9734525">Department of Computational Heidelberg University, 69120 Heidelberg,</affiliation>
<abstract confidence="0.995631736842105">We propose a novel learning approach for statistical machine translation (SMT) that allows to extract supervision signals for structured learning from an extrinsic response to a translation input. We show how to generate responses by grounding SMT in the task of executing a semantic parse of a translated query against database. Experiments on the show an improvement of about 6 points in F1-score for responsebased learning over learning from references only on returning the correct answer from a semantic parse of a translated query. In general, our approach alleviates the dependency on human reference translations and solves the reachability problem in structured learning for SMT.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jacob Andreas</author>
<author>Andreas Vlachos</author>
<author>Stephen Clark</author>
</authors>
<title>Semantic parsing as machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL’13),</booktitle>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="20433" citStr="Andreas et al. (2013)" startWordPosition="3237" endWordPosition="3240">14 68.8612 55.6712 Table 2: Experimental results using the original parser for returning answers from GEOQUERY (precision, recall, F1) and n-gram match to original English query (BLEU) on 280 re-translated test examples. et al. (2012).3 The dataset includes 880 English questions and their logical forms. The English strings were manually translated into German by the authors of Jones et al. (2012)), and corrected for typos by the authors of this paper. We follow the provided split into 600 training examples and 280 test examples. For response-based learning, we retrained the semantic parser of Andreas et al. (2013)4 on the full 880 GEOQUERY examples in order to reach full parse coverage. This parser is itself based on SMT, trained on parallel data consisting of English queries and linearized logical forms, and on a language model trained on linearized logical forms. We used the hierarchical phrase-based variant of the parser. Note that we do not use GEOQUERY test data in SMT training. Parser training includes GEOQUERY test data in order to be less dependent on parse and execution failures in the evaluation: If a translation system, response-based or reference-based, translates the German input into the </context>
</contexts>
<marker>Andreas, Vlachos, Clark, 2013</marker>
<rawString>Jacob Andreas, Andreas Vlachos, and Stephen Clark. 2013. Semantic parsing as machine translation. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL’13), Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergio Barrachina</author>
<author>Oliver Bender</author>
<author>Francisco Casacuberta</author>
<author>Jorge Civera</author>
<author>Elsa Cubel</author>
<author>Shahram Khadivi</author>
<author>Antonio Lagarda</author>
<author>Hermann Ney</author>
<author>Jes´us Tom´as</author>
<author>Enrique Vidal</author>
<author>Juan-Miguel Vilar</author>
</authors>
<title>Statistical approaches to computer-assisted translation.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>35</volume>
<issue>1</issue>
<marker>Barrachina, Bender, Casacuberta, Civera, Cubel, Khadivi, Lagarda, Ney, Tom´as, Vidal, Vilar, 2008</marker>
<rawString>Sergio Barrachina, Oliver Bender, Francisco Casacuberta, Jorge Civera, Elsa Cubel, Shahram Khadivi, Antonio Lagarda, Hermann Ney, Jes´us Tom´as, Enrique Vidal, and Juan-Miguel Vilar. 2008. Statistical approaches to computer-assisted translation. Computational Linguistics, 35(1):3–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Berant</author>
<author>Andrew Chou</author>
<author>Roy Frostig</author>
<author>Percy Liang</author>
</authors>
<title>Semantic parsing on freebase from question-answer pairs.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP’13),</booktitle>
<location>Seattle, WA.</location>
<contexts>
<context position="6476" citStr="Berant et al. (2013)" startWordPosition="993" endWordPosition="997">, inter alia). Since there are many possible correct parses, matching against a single gold standard falls short of grounding in a non-linguistic environment. Rather, the semantic context for interpretation, as well as the success criterion in evaluation is defined by successful execution of an action in the extrinsic environment, e.g., by receiving the correct answer from the database or by successful navigation to the destination. Recent attempts to learn semantic parsing from question-answer pairs without recurring to annotated logical forms have been presented by Kwiatowski et al. (2013), Berant et al. (2013), or Goldwasser and Roth (2013). The algorithms presented in these works are variants of structured prediction that take executability of semantic parses into account. Our work builds upon these ideas, however, to our knowledge the presented work is the first to embed translations into grounded scenarios in order to use feedback from interactions in these scenarios for structured learning in SMT. A recent important research direction in SMT has focused on employing automated translation as an aid to human translators. Computer assisted translation (CAT) subsumes several modes of interaction, r</context>
<context position="18936" citStr="Berant et al., 2013" startWordPosition="3004" endWordPosition="3007">il Convergence 5 Experiments 5.1 Experimental Setup In our experiments, we use the GEOQUERY database on U.S. geography as provided by Jones The sketched algorithm allows several variations. In the form depicted above, it allows . to use human reference translations in addition to task-approved surrogate references. The cost function can be implemented by different versions of sentence-wise BLEU, or it can be omitted completely so that learning relies on task-based feedback alone, similar to algorithms recently suggested for semantic parsing (Goldwasser and Roth, 2013; Kwiatowski et al., 2013; Berant et al., 2013). Lastly, regularization can be introduced by using update rules corresponding to primal form optimization variants of support vector machines (Collobert and Bengio, 2004; Chapelle, 2007; Shalev-Shwartz et al., 2007). 885 method precision recall F1 BLEU 1 CDEC 63.67 58.21 60.82 46.53 2 EXEC 70.36 63.57 66.791 48.001 3 RAMPION 75.58 69.64 72.4912 56.6412 4 REBOL 81.15 75.36 78.15123 55.6612 Table 1: Experimental results using extended parser for returning answers from GEOQUERY (precision, recall, F1) and n-gram match to original English query (BLEU) on 280 re-translated test examples. Best resu</context>
</contexts>
<marker>Berant, Chou, Frostig, Liang, 2013</marker>
<rawString>Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on freebase from question-answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP’13), Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qingqing Cai</author>
<author>Alexander Yates</author>
</authors>
<title>Large-scale semantic parsing via schema matching and lexicon extenstion.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL’13),</booktitle>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="5856" citStr="Cai and Yates (2013)" startWordPosition="897" endWordPosition="900">resentation can be directly executed by a computer system. For example, in semantic parsing, the learning goal is to produce and successfully execute a meaning representation. Executable system actions include access to databases such as the GEOQUERY database on U.S. geography (Wong and Mooney (2006), inter alia), the ATIS travel planning database (Zettlemoyer and Collins (2009), inter alia), robotic control in simulated navigation tasks (Chen and Mooney (2011), inter alia), databases of simulated card games (Goldwasser and Roth (2013), inter alia), or the user-generated contents of FREEBASE (Cai and Yates (2013), inter alia). Since there are many possible correct parses, matching against a single gold standard falls short of grounding in a non-linguistic environment. Rather, the semantic context for interpretation, as well as the success criterion in evaluation is defined by successful execution of an action in the extrinsic environment, e.g., by receiving the correct answer from the database or by successful navigation to the destination. Recent attempts to learn semantic parsing from question-answer pairs without recurring to annotated logical forms have been presented by Kwiatowski et al. (2013), </context>
</contexts>
<marker>Cai, Yates, 2013</marker>
<rawString>Qingqing Cai and Alexander Yates. 2013. Large-scale semantic parsing via schema matching and lexicon extenstion. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL’13), Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicol`o Cesa-Bianchi</author>
<author>Gabriele Reverberi</author>
<author>Sandor Szedmak</author>
</authors>
<title>Online learning algorithms for computer-assisted translation.</title>
<date>2008</date>
<tech>Technical report, SMART (www.smart-project.eu).</tech>
<contexts>
<context position="7287" citStr="Cesa-Bianchi et al., 2008" startWordPosition="1118" endWordPosition="1121">pon these ideas, however, to our knowledge the presented work is the first to embed translations into grounded scenarios in order to use feedback from interactions in these scenarios for structured learning in SMT. A recent important research direction in SMT has focused on employing automated translation as an aid to human translators. Computer assisted translation (CAT) subsumes several modes of interaction, ranging from binary feedback on the quality of the system prediction (Saluja et al., 2012), to human post-editing operations on a system prediction resulting in a reference translation (Cesa-Bianchi et al., 2008), to human acceptance or overriding of sentence completion predictions (Langlais et al., 2000; Barrachina et al., 2008; Koehn and Haddow, 2009). In all interaction scenarios, it is important that the system learns dynamically from its errors in order to offer the user the experience of a system that adapts to the provided feedback. Since retraining the SMT model after each interaction is too costly, online adaptation after each interaction has become the learning protocol of choice for CAT. Online learning has been applied in generative SMT, e.g., using incremental versions of the EM algorithm</context>
</contexts>
<marker>Cesa-Bianchi, Reverberi, Szedmak, 2008</marker>
<rawString>Nicol`o Cesa-Bianchi, Gabriele Reverberi, and Sandor Szedmak. 2008. Online learning algorithms for computer-assisted translation. Technical report, SMART (www.smart-project.eu).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivier Chapelle</author>
</authors>
<title>Training a support vector machine in the primal.</title>
<date>2007</date>
<journal>Neural Computation,</journal>
<volume>19</volume>
<issue>5</issue>
<contexts>
<context position="19122" citStr="Chapelle, 2007" startWordPosition="3033" endWordPosition="3034"> the form depicted above, it allows . to use human reference translations in addition to task-approved surrogate references. The cost function can be implemented by different versions of sentence-wise BLEU, or it can be omitted completely so that learning relies on task-based feedback alone, similar to algorithms recently suggested for semantic parsing (Goldwasser and Roth, 2013; Kwiatowski et al., 2013; Berant et al., 2013). Lastly, regularization can be introduced by using update rules corresponding to primal form optimization variants of support vector machines (Collobert and Bengio, 2004; Chapelle, 2007; Shalev-Shwartz et al., 2007). 885 method precision recall F1 BLEU 1 CDEC 63.67 58.21 60.82 46.53 2 EXEC 70.36 63.57 66.791 48.001 3 RAMPION 75.58 69.64 72.4912 56.6412 4 REBOL 81.15 75.36 78.15123 55.6612 Table 1: Experimental results using extended parser for returning answers from GEOQUERY (precision, recall, F1) and n-gram match to original English query (BLEU) on 280 re-translated test examples. Best results for each column are highlighted in bold face. Superscripts 1234 denote a significant improvement over the respective method. method precision recall F1 BLEU 1 CDEC 65.59 57.86 61.48 </context>
</contexts>
<marker>Chapelle, 2007</marker>
<rawString>Olivier Chapelle. 2007. Training a support vector machine in the primal. Neural Computation, 19(5):1155–1178.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David L Chen</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning to interpret natural language navigation instructions from observations.</title>
<date>2011</date>
<booktitle>In Proceedings of the 25th AAAI Conference on Artificial Intelligence (AAAI’11),</booktitle>
<pages>859--866</pages>
<location>San Francisco, CA.</location>
<contexts>
<context position="5701" citStr="Chen and Mooney (2011)" startWordPosition="874" endWordPosition="877">n learning, where a learner tests her understanding in an actionable setting. Such a setting can be a simulated world environment in which the linguistic representation can be directly executed by a computer system. For example, in semantic parsing, the learning goal is to produce and successfully execute a meaning representation. Executable system actions include access to databases such as the GEOQUERY database on U.S. geography (Wong and Mooney (2006), inter alia), the ATIS travel planning database (Zettlemoyer and Collins (2009), inter alia), robotic control in simulated navigation tasks (Chen and Mooney (2011), inter alia), databases of simulated card games (Goldwasser and Roth (2013), inter alia), or the user-generated contents of FREEBASE (Cai and Yates (2013), inter alia). Since there are many possible correct parses, matching against a single gold standard falls short of grounding in a non-linguistic environment. Rather, the semantic context for interpretation, as well as the success criterion in evaluation is defined by successful execution of an action in the extrinsic environment, e.g., by receiving the correct answer from the database or by successful navigation to the destination. Recent a</context>
</contexts>
<marker>Chen, Mooney, 2011</marker>
<rawString>David L. Chen and Raymond J. Mooney. 2011. Learning to interpret natural language navigation instructions from observations. In Proceedings of the 25th AAAI Conference on Artificial Intelligence (AAAI’11), pages 859–866, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Clarke</author>
<author>Dan Goldwasser</author>
<author>Wing-Wei Chang</author>
<author>Dan Roth</author>
</authors>
<title>Driving semantic parsing from the world’s response.</title>
<date>2010</date>
<booktitle>In Proceedings of the 14th Conference on Natural Language Learning (CoNLL’10),</booktitle>
<pages>18--27</pages>
<location>Uppsala,</location>
<contexts>
<context position="14752" citStr="Clarke et al. (2010)" startWordPosition="2294" endWordPosition="2297">y the SMT system. Such “unreachable” gold-standard translations need to be replaced by “surrogate” gold-standard translations that are close to the human-generated translations and still lie within the reach of the SMT system. Computation of distance to the reference translation usually involves cost functions based on sentence-level BLEU (Nakov et al. (2012), inter alia) and incorporates the current model score, leading to various ramp loss objectives described in Gimpel and Smith (2012). An alternative approach to alleviate the dependency on labeled training data is response-based learning. Clarke et al. (2010) or Goldwasser and Roth (2013) describe a response-driven learning framework for the area of semantic parsing: Here a meaning representation is “tried out” by iteratively generating system outputs, receiving feedback from world interaction, and updating the model parameters. Applied to SMT, this means that we predict translations and use positive response from acting in the world to create “surrogate” gold-standard translations. This decreases the dependency on a few (mostly only one) reference translations and guides the learner to promote translations that perform well with respect to the ex</context>
</contexts>
<marker>Clarke, Goldwasser, Chang, Roth, 2010</marker>
<rawString>James Clarke, Dan Goldwasser, Wing-Wei Chang, and Dan Roth. 2010. Driving semantic parsing from the world’s response. In Proceedings of the 14th Conference on Natural Language Learning (CoNLL’10), pages 18–27, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the conference on Empirical Methods in Natural Language Processing (EMNLP’02),</booktitle>
<location>Philadelphia, PA.</location>
<contexts>
<context position="13384" citStr="Collins, 2002" startWordPosition="2084" endWordPosition="2085">k from parse execution against the geographical database. 4 Response-based Online Learning Recent approaches to machine learning for SMT formalize the task of discriminating good from bad translations as a structured prediction problem. Assume a joint feature representation φ(x, y) of input sentences x and output translations y E Y (x), and a linear scoring function s(x, y; w) for predicting a translation yˆ (where (,) denotes the standard vector dot product) s.t. yˆ = arg max s(x, y; w) = arg max (w, φ(x, y)) . Y∈Y (x) Y∈Y (x) 2http://translate.google.com The structured perceptron algorithm (Collins, 2002) learns an optimal weight vector w by updating w on input x(Z) by the following rule, in case the predicted translation yˆ is different from and scored higher than the reference translation y(Z): w = w + φ(x(Z), y(Z)) − φ(x(Z), ˆy). This stochastic structural update aims to demote weights of features corresponding to incorrect decisions, and to promote weights of features for correct decisions. An application of structured prediction to SMT involves more than a straightforward replacement of labeled output structures by reference translations. Firstly, update rules that require to compute a fe</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms. In Proceedings of the conference on Empirical Methods in Natural Language Processing (EMNLP’02), Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Samy Bengio</author>
</authors>
<title>Links between perceptrons, MLPs, and SVMs.</title>
<date>2004</date>
<booktitle>In Proceedings of the 21st International Conference on Machine Learning (ICML’04),</booktitle>
<location>Banff, Canada.</location>
<contexts>
<context position="19106" citStr="Collobert and Bengio, 2004" startWordPosition="3029" endWordPosition="3032">llows several variations. In the form depicted above, it allows . to use human reference translations in addition to task-approved surrogate references. The cost function can be implemented by different versions of sentence-wise BLEU, or it can be omitted completely so that learning relies on task-based feedback alone, similar to algorithms recently suggested for semantic parsing (Goldwasser and Roth, 2013; Kwiatowski et al., 2013; Berant et al., 2013). Lastly, regularization can be introduced by using update rules corresponding to primal form optimization variants of support vector machines (Collobert and Bengio, 2004; Chapelle, 2007; Shalev-Shwartz et al., 2007). 885 method precision recall F1 BLEU 1 CDEC 63.67 58.21 60.82 46.53 2 EXEC 70.36 63.57 66.791 48.001 3 RAMPION 75.58 69.64 72.4912 56.6412 4 REBOL 81.15 75.36 78.15123 55.6612 Table 1: Experimental results using extended parser for returning answers from GEOQUERY (precision, recall, F1) and n-gram match to original English query (BLEU) on 280 re-translated test examples. Best results for each column are highlighted in bold face. Superscripts 1234 denote a significant improvement over the respective method. method precision recall F1 BLEU 1 CDEC 65</context>
</contexts>
<marker>Collobert, Bengio, 2004</marker>
<rawString>Ronan Collobert and Samy Bengio. 2004. Links between perceptrons, MLPs, and SVMs. In Proceedings of the 21st International Conference on Machine Learning (ICML’04), Banff, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Denkowski</author>
<author>Chris Dyer</author>
<author>Alon Lavie</author>
</authors>
<title>Learning from post-editing: Online model adaptation for statistical machine translation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics (EACL’14),</booktitle>
<location>Gothenburg,</location>
<contexts>
<context position="8112" citStr="Denkowski et al., 2014" startWordPosition="1252" endWordPosition="1255">stem learns dynamically from its errors in order to offer the user the experience of a system that adapts to the provided feedback. Since retraining the SMT model after each interaction is too costly, online adaptation after each interaction has become the learning protocol of choice for CAT. Online learning has been applied in generative SMT, e.g., using incremental versions of the EM algorithm (Ortiz-Martinez et al., 2010; Hardt and Elming, 2010), or in discriminative SMT, e.g., using perceptron-type algorithms (Cesa-Bianchi et al., 2008; Martinez-G´omez et al., 2012; W¨aschle et al., 2013; Denkowski et al., 2014). In a similar way to deploying human feedback, extrinsic loss functions have been used to provide learning signals for SMT. For example, Nikoulina et al. (2012) propose a setup where an SMT system feeds into cross-language information retrieval, and receives feedback from the performance of translated queries with respect to cross-language retrieval performance. This feedback is used to train a reranker on an n-best list of translations order with respect to retrieval performance. In con882 Figure 1: Response-based learning cycle for grounding SMT in virtual trivia gameplay. trast to our work</context>
</contexts>
<marker>Denkowski, Dyer, Lavie, 2014</marker>
<rawString>Michael Denkowski, Chris Dyer, and Alon Lavie. 2014. Learning from post-editing: Online model adaptation for statistical machine translation. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics (EACL’14), Gothenburg, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Adam Lopez</author>
<author>Juri Ganitkevitch</author>
<author>Jonathan Weese</author>
<author>Ferhan Ture</author>
<author>Phil Blunsom</author>
<author>Hendra Setiawan</author>
<author>Vladimir Eidelman</author>
<author>Philip Resnik</author>
</authors>
<date>2010</date>
<contexts>
<context position="21608" citStr="Dyer et al., 2010" startWordPosition="3417" endWordPosition="3420">ased, translates the German input into the gold standard English query it should be rewarded by positive task feedback. To doublecheck whether including the 280 test examples in parser training gives an unfair advantage to response-based learning, we also present experimental results using the original parser of Andreas 3http://homepages.inf.ed.ac.uk/ s1051107/geoquery-2012-08-27.zip 4https://github.com/jacobandreas/ smt-semparse et al. (2013) that is trained only on the 600 GEOQUERY training examples. The bilingual SMT system used in our experiments is the state-of-the-art SCFG decoder CDEC (Dyer et al., 2010)5. We built grammars using its implementation of the suffix array extraction method described in Lopez (2007). For language modeling, we built a modified Kneser-Ney smoothed 5-gram language model using the English side of the training data. We trained the SMT system on the English-German parallel web data provided in the COMMON CRAWL6 (Smith et al., 2013) dataset. 5.2 Compared Systems Method 1 is the baseline system, consisting of the CDEC SMT system trained on the COMMON CRAWL data as described above. This system does not use any GEOQUERY data for training. Methods 2-4 use the 600 training ex</context>
</contexts>
<marker>Dyer, Lopez, Ganitkevitch, Weese, Ture, Blunsom, Setiawan, Eidelman, Resnik, 2010</marker>
<rawString>Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan, Vladimir Eidelman, and Philip Resnik. 2010.</rawString>
</citation>
<citation valid="true">
<title>cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models.</title>
<date></date>
<booktitle>In Proceedings of the ACL 2010 System Demonstrations,</booktitle>
<location>Uppsala,</location>
<marker></marker>
<rawString>cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models. In Proceedings of the ACL 2010 System Demonstrations, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaru Fuji</author>
</authors>
<title>Evaluation experiment for reading comprehension of machine translation outputs.</title>
<date>1999</date>
<booktitle>In Proceedings of the Machine Translation Summit VII,</booktitle>
<contexts>
<context position="9159" citStr="Fuji, 1999" startWordPosition="1417" endWordPosition="1418">ions order with respect to retrieval performance. In con882 Figure 1: Response-based learning cycle for grounding SMT in virtual trivia gameplay. trast to our work, all mentioned approaches to interactive or adaptive learning in SMT rely on human post-edits or human reference translations. Our work differs from these approaches in that exactly this dependency is alleviated by learning from responses in an extrinsic task. Interactive scenarios have been used for evaluation purposes of translation systems for nearly 50 years, especially using human reading comprehension testing (Pfafflin, 1965; Fuji, 1999; Jones et al., 2005), and more recently, using face-toface conversation mediated via machine translation (Sakamoto et al., 2013). However, despite offering direct and reliable prediction of translation quality, the cost and lack of reusability has confined task-based evaluations involving humans to testing scenarios, but prevented a use for interactive training of SMT systems as in our work. Lastly, our work is related to cross-lingual natural language processing such as cross-lingual question answering or cross-lingual information retrieval as conducted at recent evaluation campaigns of the </context>
</contexts>
<marker>Fuji, 1999</marker>
<rawString>Masaru Fuji. 1999. Evaluation experiment for reading comprehension of machine translation outputs. In Proceedings of the Machine Translation Summit VII, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Gimpel</author>
<author>Noah A Smith</author>
</authors>
<title>Structured ramp loss minimization for machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2012),</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="14625" citStr="Gimpel and Smith (2012)" startWordPosition="2275" endWordPosition="2278">on for the reference translation are suboptimal in SMT, because often human-generated reference translations cannot be generated by the SMT system. Such “unreachable” gold-standard translations need to be replaced by “surrogate” gold-standard translations that are close to the human-generated translations and still lie within the reach of the SMT system. Computation of distance to the reference translation usually involves cost functions based on sentence-level BLEU (Nakov et al. (2012), inter alia) and incorporates the current model score, leading to various ramp loss objectives described in Gimpel and Smith (2012). An alternative approach to alleviate the dependency on labeled training data is response-based learning. Clarke et al. (2010) or Goldwasser and Roth (2013) describe a response-driven learning framework for the area of semantic parsing: Here a meaning representation is “tried out” by iteratively generating system outputs, receiving feedback from world interaction, and updating the model parameters. Applied to SMT, this means that we predict translations and use positive response from acting in the world to create “surrogate” gold-standard translations. This decreases the dependency on a few (</context>
<context position="24788" citStr="Gimpel and Smith, 2012" startWordPosition="3915" endWordPosition="3918">nd additionally use the given original English queries as references. Method 2, named EXEC, relies on task-execution by function e and searches for executable or nonexecutable translations with highest score s to distinguish positive from negative training examples. It does not use a cost function and thus cannot make use of the original English queries. We compare response-based learning with a standard structured prediction setup that omits the use of the execution function e in the definition of y+ and y−. This algorithm can be seen as a stochastic (sub)gradient descent variant of RAMPION (Gimpel and Smith, 2012). It does not make use of the semantic parser, but defines positive and negative examples based on score s and cost c with respect to human reference translations. We report BLEU (Papineni et al., 2001) of translation system output measured against the original English queries. Furthermore, we report precision, recall, and F1-score for executing semantic parses built from translation system outputs against the GEOQUERY database. Precision is defined as the percentage of correctly answered examples out of those for which a parse could be produced; recall is defined as the percentage of total ex</context>
</contexts>
<marker>Gimpel, Smith, 2012</marker>
<rawString>Kevin Gimpel and Noah A. Smith. 2012. Structured ramp loss minimization for machine translation. In Proceedings of 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2012), Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Goldwasser</author>
<author>Dan Roth</author>
</authors>
<title>Learning from natural instructions.</title>
<date>2013</date>
<booktitle>Machine Learning,</booktitle>
<volume>94</volume>
<issue>2</issue>
<pages>232</pages>
<contexts>
<context position="5777" citStr="Goldwasser and Roth (2013)" startWordPosition="885" endWordPosition="888">ting. Such a setting can be a simulated world environment in which the linguistic representation can be directly executed by a computer system. For example, in semantic parsing, the learning goal is to produce and successfully execute a meaning representation. Executable system actions include access to databases such as the GEOQUERY database on U.S. geography (Wong and Mooney (2006), inter alia), the ATIS travel planning database (Zettlemoyer and Collins (2009), inter alia), robotic control in simulated navigation tasks (Chen and Mooney (2011), inter alia), databases of simulated card games (Goldwasser and Roth (2013), inter alia), or the user-generated contents of FREEBASE (Cai and Yates (2013), inter alia). Since there are many possible correct parses, matching against a single gold standard falls short of grounding in a non-linguistic environment. Rather, the semantic context for interpretation, as well as the success criterion in evaluation is defined by successful execution of an action in the extrinsic environment, e.g., by receiving the correct answer from the database or by successful navigation to the destination. Recent attempts to learn semantic parsing from question-answer pairs without recurri</context>
<context position="14782" citStr="Goldwasser and Roth (2013)" startWordPosition="2299" endWordPosition="2302">unreachable” gold-standard translations need to be replaced by “surrogate” gold-standard translations that are close to the human-generated translations and still lie within the reach of the SMT system. Computation of distance to the reference translation usually involves cost functions based on sentence-level BLEU (Nakov et al. (2012), inter alia) and incorporates the current model score, leading to various ramp loss objectives described in Gimpel and Smith (2012). An alternative approach to alleviate the dependency on labeled training data is response-based learning. Clarke et al. (2010) or Goldwasser and Roth (2013) describe a response-driven learning framework for the area of semantic parsing: Here a meaning representation is “tried out” by iteratively generating system outputs, receiving feedback from world interaction, and updating the model parameters. Applied to SMT, this means that we predict translations and use positive response from acting in the world to create “surrogate” gold-standard translations. This decreases the dependency on a few (mostly only one) reference translations and guides the learner to promote translations that perform well with respect to the extrinsic task. In the following</context>
<context position="18889" citStr="Goldwasser and Roth, 2013" startWordPosition="2996" endWordPosition="2999"> if w ← w + η(φ(x(z), y+) − φ(x(z), y−)) end for until Convergence 5 Experiments 5.1 Experimental Setup In our experiments, we use the GEOQUERY database on U.S. geography as provided by Jones The sketched algorithm allows several variations. In the form depicted above, it allows . to use human reference translations in addition to task-approved surrogate references. The cost function can be implemented by different versions of sentence-wise BLEU, or it can be omitted completely so that learning relies on task-based feedback alone, similar to algorithms recently suggested for semantic parsing (Goldwasser and Roth, 2013; Kwiatowski et al., 2013; Berant et al., 2013). Lastly, regularization can be introduced by using update rules corresponding to primal form optimization variants of support vector machines (Collobert and Bengio, 2004; Chapelle, 2007; Shalev-Shwartz et al., 2007). 885 method precision recall F1 BLEU 1 CDEC 63.67 58.21 60.82 46.53 2 EXEC 70.36 63.57 66.791 48.001 3 RAMPION 75.58 69.64 72.4912 56.6412 4 REBOL 81.15 75.36 78.15123 55.6612 Table 1: Experimental results using extended parser for returning answers from GEOQUERY (precision, recall, F1) and n-gram match to original English query (BLEU</context>
</contexts>
<marker>Goldwasser, Roth, 2013</marker>
<rawString>Dan Goldwasser and Dan Roth. 2013. Learning from natural instructions. Machine Learning, 94(2):205– 232.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Hardt</author>
<author>Jakob Elming</author>
</authors>
<title>Incremental re-training for post-editing SMT.</title>
<date>2010</date>
<booktitle>In Proceedings of the 9th Conference of the Association for Machine Tranlation in the Americas (AMTA’10),</booktitle>
<location>Denver, CO.</location>
<contexts>
<context position="7941" citStr="Hardt and Elming, 2010" startWordPosition="1227" endWordPosition="1231">ding of sentence completion predictions (Langlais et al., 2000; Barrachina et al., 2008; Koehn and Haddow, 2009). In all interaction scenarios, it is important that the system learns dynamically from its errors in order to offer the user the experience of a system that adapts to the provided feedback. Since retraining the SMT model after each interaction is too costly, online adaptation after each interaction has become the learning protocol of choice for CAT. Online learning has been applied in generative SMT, e.g., using incremental versions of the EM algorithm (Ortiz-Martinez et al., 2010; Hardt and Elming, 2010), or in discriminative SMT, e.g., using perceptron-type algorithms (Cesa-Bianchi et al., 2008; Martinez-G´omez et al., 2012; W¨aschle et al., 2013; Denkowski et al., 2014). In a similar way to deploying human feedback, extrinsic loss functions have been used to provide learning signals for SMT. For example, Nikoulina et al. (2012) propose a setup where an SMT system feeds into cross-language information retrieval, and receives feedback from the performance of translated queries with respect to cross-language retrieval performance. This feedback is used to train a reranker on an n-best list of </context>
</contexts>
<marker>Hardt, Elming, 2010</marker>
<rawString>Daniel Hardt and Jakob Elming. 2010. Incremental re-training for post-editing SMT. In Proceedings of the 9th Conference of the Association for Machine Tranlation in the Americas (AMTA’10), Denver, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas Jones</author>
<author>Wade Shen</author>
<author>Neil Granoien</author>
<author>Martha Herzog</author>
<author>Clifford Weinstein</author>
</authors>
<title>Measuring translation quality by testing english speakers with a new defense language proficiency test for arabic.</title>
<date>2005</date>
<booktitle>In Proceedings of 2005 International Conference on Intelligence Analysis,</booktitle>
<location>McLean, VA.</location>
<contexts>
<context position="9180" citStr="Jones et al., 2005" startWordPosition="1419" endWordPosition="1422">ith respect to retrieval performance. In con882 Figure 1: Response-based learning cycle for grounding SMT in virtual trivia gameplay. trast to our work, all mentioned approaches to interactive or adaptive learning in SMT rely on human post-edits or human reference translations. Our work differs from these approaches in that exactly this dependency is alleviated by learning from responses in an extrinsic task. Interactive scenarios have been used for evaluation purposes of translation systems for nearly 50 years, especially using human reading comprehension testing (Pfafflin, 1965; Fuji, 1999; Jones et al., 2005), and more recently, using face-toface conversation mediated via machine translation (Sakamoto et al., 2013). However, despite offering direct and reliable prediction of translation quality, the cost and lack of reusability has confined task-based evaluations involving humans to testing scenarios, but prevented a use for interactive training of SMT systems as in our work. Lastly, our work is related to cross-lingual natural language processing such as cross-lingual question answering or cross-lingual information retrieval as conducted at recent evaluation campaigns of the CLEF initiative.1 Whi</context>
</contexts>
<marker>Jones, Shen, Granoien, Herzog, Weinstein, 2005</marker>
<rawString>Douglas Jones, Wade Shen, Neil Granoien, Martha Herzog, and Clifford Weinstein. 2005. Measuring translation quality by testing english speakers with a new defense language proficiency test for arabic. In Proceedings of 2005 International Conference on Intelligence Analysis, McLean, VA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bevan K Jones</author>
<author>Mark Johnson</author>
<author>Sharon Goldwater</author>
</authors>
<title>Semantic parsing with bayesion tree transducers.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL’12),</booktitle>
<location>Jeju Island,</location>
<contexts>
<context position="20211" citStr="Jones et al. (2012)" startWordPosition="3201" endWordPosition="3204">cripts 1234 denote a significant improvement over the respective method. method precision recall F1 BLEU 1 CDEC 65.59 57.86 61.48 46.53 2 EXEC 66.54 61.79 64.07 46.00 3 RAMPION 67.68 63.57 65.56 55.6712 4 REBOL 70.68 67.14 68.8612 55.6712 Table 2: Experimental results using the original parser for returning answers from GEOQUERY (precision, recall, F1) and n-gram match to original English query (BLEU) on 280 re-translated test examples. et al. (2012).3 The dataset includes 880 English questions and their logical forms. The English strings were manually translated into German by the authors of Jones et al. (2012)), and corrected for typos by the authors of this paper. We follow the provided split into 600 training examples and 280 test examples. For response-based learning, we retrained the semantic parser of Andreas et al. (2013)4 on the full 880 GEOQUERY examples in order to reach full parse coverage. This parser is itself based on SMT, trained on parallel data consisting of English queries and linearized logical forms, and on a language model trained on linearized logical forms. We used the hierarchical phrase-based variant of the parser. Note that we do not use GEOQUERY test data in SMT training. </context>
<context position="25822" citStr="Jones et al. (2012)" startWordPosition="4074" endWordPosition="4077">EOQUERY database. Precision is defined as the percentage of correctly answered examples out of those for which a parse could be produced; recall is defined as the percentage of total examples answered correctly; F1-score is the harmonic mean of both. Statistical significance is measured using Approximate Randomization (Noreen, 1989) where result differences with a pvalue smaller than 0.05 are considered statistically significant. Methods 2-4 perform structured learning for SMT on the 600 GEOQUERY training examples and re-translate the 280 unseen GEOQUERY test data, following the data split of Jones et al. (2012). Training for RAMPION, REBOL and EXEC was repeated for 10 epochs. The learning rate q is set to a constant that is adjusted by cross-validation on the 600 training examples. 5.3 Empirical Results We present an experimental comparison of the four different systems according to BLEU and 887 reference RAMPION REBOL how many colorado rivers are there what are the populations of states which border texas what is the biggest capital city in the us how many rivers with the name colorado gives it how big are the populations of the states , which in texas borders how many rivers named colorado are the</context>
</contexts>
<marker>Jones, Johnson, Goldwater, 2012</marker>
<rawString>Bevan K. Jones, Mark Johnson, and Sharon Goldwater. 2012. Semantic parsing with bayesion tree transducers. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL’12), Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Barry Haddow</author>
</authors>
<title>Interactive assistance to human translators using statistical machine translation methods.</title>
<date>2009</date>
<booktitle>In Proceedings of MT Summit XII,</booktitle>
<location>Ottawa, Ontario, Canada.</location>
<contexts>
<context position="7430" citStr="Koehn and Haddow, 2009" startWordPosition="1141" endWordPosition="1144">from interactions in these scenarios for structured learning in SMT. A recent important research direction in SMT has focused on employing automated translation as an aid to human translators. Computer assisted translation (CAT) subsumes several modes of interaction, ranging from binary feedback on the quality of the system prediction (Saluja et al., 2012), to human post-editing operations on a system prediction resulting in a reference translation (Cesa-Bianchi et al., 2008), to human acceptance or overriding of sentence completion predictions (Langlais et al., 2000; Barrachina et al., 2008; Koehn and Haddow, 2009). In all interaction scenarios, it is important that the system learns dynamically from its errors in order to offer the user the experience of a system that adapts to the provided feedback. Since retraining the SMT model after each interaction is too costly, online adaptation after each interaction has become the learning protocol of choice for CAT. Online learning has been applied in generative SMT, e.g., using incremental versions of the EM algorithm (Ortiz-Martinez et al., 2010; Hardt and Elming, 2010), or in discriminative SMT, e.g., using perceptron-type algorithms (Cesa-Bianchi et al., </context>
</contexts>
<marker>Koehn, Haddow, 2009</marker>
<rawString>Philipp Koehn and Barry Haddow. 2009. Interactive assistance to human translators using statistical machine translation methods. In Proceedings of MT Summit XII, Ottawa, Ontario, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Kwiatowski</author>
<author>Eunsol Choi</author>
<author>Yoav Artzi</author>
<author>Luke Zettlemoyer</author>
</authors>
<title>Scaling semantic parsers with on-the-fly ontology matching.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP’13),</booktitle>
<location>Seattle, WA.</location>
<contexts>
<context position="6454" citStr="Kwiatowski et al. (2013)" startWordPosition="989" endWordPosition="992">BASE (Cai and Yates (2013), inter alia). Since there are many possible correct parses, matching against a single gold standard falls short of grounding in a non-linguistic environment. Rather, the semantic context for interpretation, as well as the success criterion in evaluation is defined by successful execution of an action in the extrinsic environment, e.g., by receiving the correct answer from the database or by successful navigation to the destination. Recent attempts to learn semantic parsing from question-answer pairs without recurring to annotated logical forms have been presented by Kwiatowski et al. (2013), Berant et al. (2013), or Goldwasser and Roth (2013). The algorithms presented in these works are variants of structured prediction that take executability of semantic parses into account. Our work builds upon these ideas, however, to our knowledge the presented work is the first to embed translations into grounded scenarios in order to use feedback from interactions in these scenarios for structured learning in SMT. A recent important research direction in SMT has focused on employing automated translation as an aid to human translators. Computer assisted translation (CAT) subsumes several m</context>
<context position="18914" citStr="Kwiatowski et al., 2013" startWordPosition="3000" endWordPosition="3003"> φ(x(z), y−)) end for until Convergence 5 Experiments 5.1 Experimental Setup In our experiments, we use the GEOQUERY database on U.S. geography as provided by Jones The sketched algorithm allows several variations. In the form depicted above, it allows . to use human reference translations in addition to task-approved surrogate references. The cost function can be implemented by different versions of sentence-wise BLEU, or it can be omitted completely so that learning relies on task-based feedback alone, similar to algorithms recently suggested for semantic parsing (Goldwasser and Roth, 2013; Kwiatowski et al., 2013; Berant et al., 2013). Lastly, regularization can be introduced by using update rules corresponding to primal form optimization variants of support vector machines (Collobert and Bengio, 2004; Chapelle, 2007; Shalev-Shwartz et al., 2007). 885 method precision recall F1 BLEU 1 CDEC 63.67 58.21 60.82 46.53 2 EXEC 70.36 63.57 66.791 48.001 3 RAMPION 75.58 69.64 72.4912 56.6412 4 REBOL 81.15 75.36 78.15123 55.6612 Table 1: Experimental results using extended parser for returning answers from GEOQUERY (precision, recall, F1) and n-gram match to original English query (BLEU) on 280 re-translated te</context>
</contexts>
<marker>Kwiatowski, Choi, Artzi, Zettlemoyer, 2013</marker>
<rawString>Tom Kwiatowski, Eunsol Choi, Yoav Artzi, and Luke Zettlemoyer. 2013. Scaling semantic parsers with on-the-fly ontology matching. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP’13), Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philippe Langlais</author>
<author>George Foster</author>
<author>Guy Lapalme</author>
</authors>
<title>Transtype: a computer-aided translation typing system.</title>
<date>2000</date>
<booktitle>In Proceedings of the ANLP-NAACL 2000 Workshop on Embedded Machine Translation Systems,</booktitle>
<location>Seattle, WA.</location>
<contexts>
<context position="7380" citStr="Langlais et al., 2000" startWordPosition="1133" endWordPosition="1136">nto grounded scenarios in order to use feedback from interactions in these scenarios for structured learning in SMT. A recent important research direction in SMT has focused on employing automated translation as an aid to human translators. Computer assisted translation (CAT) subsumes several modes of interaction, ranging from binary feedback on the quality of the system prediction (Saluja et al., 2012), to human post-editing operations on a system prediction resulting in a reference translation (Cesa-Bianchi et al., 2008), to human acceptance or overriding of sentence completion predictions (Langlais et al., 2000; Barrachina et al., 2008; Koehn and Haddow, 2009). In all interaction scenarios, it is important that the system learns dynamically from its errors in order to offer the user the experience of a system that adapts to the provided feedback. Since retraining the SMT model after each interaction is too costly, online adaptation after each interaction has become the learning protocol of choice for CAT. Online learning has been applied in generative SMT, e.g., using incremental versions of the EM algorithm (Ortiz-Martinez et al., 2010; Hardt and Elming, 2010), or in discriminative SMT, e.g., using</context>
</contexts>
<marker>Langlais, Foster, Lapalme, 2000</marker>
<rawString>Philippe Langlais, George Foster, and Guy Lapalme. 2000. Transtype: a computer-aided translation typing system. In Proceedings of the ANLP-NAACL 2000 Workshop on Embedded Machine Translation Systems, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Lopez</author>
</authors>
<title>Hierarchical phrase-based translation with suffix arrays.</title>
<date>2007</date>
<booktitle>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL 2007),</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="21717" citStr="Lopez (2007)" startWordPosition="3437" endWordPosition="3438">ack. To doublecheck whether including the 280 test examples in parser training gives an unfair advantage to response-based learning, we also present experimental results using the original parser of Andreas 3http://homepages.inf.ed.ac.uk/ s1051107/geoquery-2012-08-27.zip 4https://github.com/jacobandreas/ smt-semparse et al. (2013) that is trained only on the 600 GEOQUERY training examples. The bilingual SMT system used in our experiments is the state-of-the-art SCFG decoder CDEC (Dyer et al., 2010)5. We built grammars using its implementation of the suffix array extraction method described in Lopez (2007). For language modeling, we built a modified Kneser-Ney smoothed 5-gram language model using the English side of the training data. We trained the SMT system on the English-German parallel web data provided in the COMMON CRAWL6 (Smith et al., 2013) dataset. 5.2 Compared Systems Method 1 is the baseline system, consisting of the CDEC SMT system trained on the COMMON CRAWL data as described above. This system does not use any GEOQUERY data for training. Methods 2-4 use the 600 training examples from GEOQUERY for discriminative training only. Variants of the response-based learning algorithm desc</context>
</contexts>
<marker>Lopez, 2007</marker>
<rawString>Adam Lopez. 2007. Hierarchical phrase-based translation with suffix arrays. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL 2007), Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascual Mart´ınez-G´omez</author>
<author>Germ´an Sanchis-Trilles</author>
<author>Francisco Casacuberta</author>
</authors>
<title>Online adaptation strategies for statistical machine translation in postediting scenarios.</title>
<date>2012</date>
<journal>Pattern Recognition,</journal>
<volume>45</volume>
<issue>9</issue>
<pages>3202</pages>
<marker>Mart´ınez-G´omez, Sanchis-Trilles, Casacuberta, 2012</marker>
<rawString>Pascual Mart´ınez-G´omez, Germ´an Sanchis-Trilles, and Francisco Casacuberta. 2012. Online adaptation strategies for statistical machine translation in postediting scenarios. Pattern Recognition, 45(9):3193– 3202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McAllester</author>
<author>Joseph Keshet</author>
</authors>
<title>Generalization bounds and consistency for latent structural probit and ramp loss.</title>
<date>2011</date>
<booktitle>In Proceedings of the 25th Annual Conference on Neural Information Processing Sytems (NIPS 2011),</booktitle>
<location>Granada,</location>
<contexts>
<context position="16952" citStr="McAllester and Keshet, 2011" startWordPosition="2654" endWordPosition="2657">score, and a low cost of predicting y instead of y(z): ( ) y+ = arg max s(x(z), y; w) − c(y(z), y) . y∈Y (x(&apos;&apos;)):e(y)=1 The opposite of y+ is the translation y− that leads to negative feedback, has a high model score, and a high cost. It is defined as follows: y− = arg max s(x(z), y; w) + c(y(z), y) . y∈Y (x(&apos;&apos;)):e(y)=0 ( ) Update rules can be derived by minimization of the following ramp loss objective: � ( ) min − max s(x(z), y; w) − c(y(z), y) w y∈Y (x(&apos;&apos;)):e(y)=1 ( )/ + max s(x(z), y; w) + c(y(z), y) y∈Y (x(&apos;&apos;)):e(y)=0 Minimization of this objective using stochastic (sub)gradient descent (McAllester and Keshet, 2011) yields the following update rule: w = w + φ(x(z), y+) − φ(x(z), y−). The intuition behind this update rule is to discriminate the translation y+ that leads to positive feedback and best approximates (or is identical to) the reference within the means of the model from a translation y− which is favored by the model but does not execute and has high cost. This is done by putting all the weight on the former. Algorithm 1 presents pseudo-code for our response-driven learning scenario. Upon predicting translation ˆy, in case of positive feedback from the task, we treat the prediction as surrogate </context>
</contexts>
<marker>McAllester, Keshet, 2011</marker>
<rawString>David McAllester and Joseph Keshet. 2011. Generalization bounds and consistency for latent structural probit and ramp loss. In Proceedings of the 25th Annual Conference on Neural Information Processing Sytems (NIPS 2011), Granada, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Francisco Guzm´an</author>
<author>Stephan Vogel</author>
</authors>
<title>Optimizing for sentence-level bleu+1 yields short translations.</title>
<date>2012</date>
<booktitle>In Proceedings of the 24th International Conference on Computational Linguistics (COLING 2012),</booktitle>
<location>Bombay, India.</location>
<marker>Nakov, Guzm´an, Vogel, 2012</marker>
<rawString>Preslav Nakov, Francisco Guzm´an, and Stephan Vogel. 2012. Optimizing for sentence-level bleu+1 yields short translations. In Proceedings of the 24th International Conference on Computational Linguistics (COLING 2012), Bombay, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vassilina Nikoulina</author>
<author>Bogomil Kovachev</author>
<author>Nikolaos Lagos</author>
<author>Christof Monz</author>
</authors>
<title>Adaptation of statistical machine translation model for cross-lingual information retrieval in a service context.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics (EACL’12),</booktitle>
<location>Avignon, France.</location>
<contexts>
<context position="8273" citStr="Nikoulina et al. (2012)" startWordPosition="1280" endWordPosition="1283">l after each interaction is too costly, online adaptation after each interaction has become the learning protocol of choice for CAT. Online learning has been applied in generative SMT, e.g., using incremental versions of the EM algorithm (Ortiz-Martinez et al., 2010; Hardt and Elming, 2010), or in discriminative SMT, e.g., using perceptron-type algorithms (Cesa-Bianchi et al., 2008; Martinez-G´omez et al., 2012; W¨aschle et al., 2013; Denkowski et al., 2014). In a similar way to deploying human feedback, extrinsic loss functions have been used to provide learning signals for SMT. For example, Nikoulina et al. (2012) propose a setup where an SMT system feeds into cross-language information retrieval, and receives feedback from the performance of translated queries with respect to cross-language retrieval performance. This feedback is used to train a reranker on an n-best list of translations order with respect to retrieval performance. In con882 Figure 1: Response-based learning cycle for grounding SMT in virtual trivia gameplay. trast to our work, all mentioned approaches to interactive or adaptive learning in SMT rely on human post-edits or human reference translations. Our work differs from these appro</context>
</contexts>
<marker>Nikoulina, Kovachev, Lagos, Monz, 2012</marker>
<rawString>Vassilina Nikoulina, Bogomil Kovachev, Nikolaos Lagos, and Christof Monz. 2012. Adaptation of statistical machine translation model for cross-lingual information retrieval in a service context. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics (EACL’12), Avignon, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric W Noreen</author>
</authors>
<title>Computer Intensive Methods for Testing Hypotheses. An Introduction.</title>
<date>1989</date>
<publisher>Wiley,</publisher>
<location>New York.</location>
<contexts>
<context position="25537" citStr="Noreen, 1989" startWordPosition="4032" endWordPosition="4033">man reference translations. We report BLEU (Papineni et al., 2001) of translation system output measured against the original English queries. Furthermore, we report precision, recall, and F1-score for executing semantic parses built from translation system outputs against the GEOQUERY database. Precision is defined as the percentage of correctly answered examples out of those for which a parse could be produced; recall is defined as the percentage of total examples answered correctly; F1-score is the harmonic mean of both. Statistical significance is measured using Approximate Randomization (Noreen, 1989) where result differences with a pvalue smaller than 0.05 are considered statistically significant. Methods 2-4 perform structured learning for SMT on the 600 GEOQUERY training examples and re-translate the 280 unseen GEOQUERY test data, following the data split of Jones et al. (2012). Training for RAMPION, REBOL and EXEC was repeated for 10 epochs. The learning rate q is set to a constant that is adjusted by cross-validation on the 600 training examples. 5.3 Empirical Results We present an experimental comparison of the four different systems according to BLEU and 887 reference RAMPION REBOL </context>
</contexts>
<marker>Noreen, 1989</marker>
<rawString>Eric W. Noreen. 1989. Computer Intensive Methods for Testing Hypotheses. An Introduction. Wiley, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Ortiz-Martinez</author>
<author>Ismal Garcia-Varea</author>
<author>Francisco Casacuberta</author>
</authors>
<title>Online learning for interactive statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the Human Language Technologies conference and the 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL’10),</booktitle>
<location>Los Angeles, CA.</location>
<contexts>
<context position="7916" citStr="Ortiz-Martinez et al., 2010" startWordPosition="1223" endWordPosition="1226">to human acceptance or overriding of sentence completion predictions (Langlais et al., 2000; Barrachina et al., 2008; Koehn and Haddow, 2009). In all interaction scenarios, it is important that the system learns dynamically from its errors in order to offer the user the experience of a system that adapts to the provided feedback. Since retraining the SMT model after each interaction is too costly, online adaptation after each interaction has become the learning protocol of choice for CAT. Online learning has been applied in generative SMT, e.g., using incremental versions of the EM algorithm (Ortiz-Martinez et al., 2010; Hardt and Elming, 2010), or in discriminative SMT, e.g., using perceptron-type algorithms (Cesa-Bianchi et al., 2008; Martinez-G´omez et al., 2012; W¨aschle et al., 2013; Denkowski et al., 2014). In a similar way to deploying human feedback, extrinsic loss functions have been used to provide learning signals for SMT. For example, Nikoulina et al. (2012) propose a setup where an SMT system feeds into cross-language information retrieval, and receives feedback from the performance of translated queries with respect to cross-language retrieval performance. This feedback is used to train a reran</context>
</contexts>
<marker>Ortiz-Martinez, Garcia-Varea, Casacuberta, 2010</marker>
<rawString>Daniel Ortiz-Martinez, Ismal Garcia-Varea, and Francisco Casacuberta. 2010. Online learning for interactive statistical machine translation. In Proceedings of the Human Language Technologies conference and the 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL’10), Los Angeles, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2001</date>
<tech>Technical Report IBM Research Division Technical Report, RC22176 (W0190-022),</tech>
<location>Yorktown Heights, N.Y.</location>
<contexts>
<context position="24990" citStr="Papineni et al., 2001" startWordPosition="3950" endWordPosition="3953">core s to distinguish positive from negative training examples. It does not use a cost function and thus cannot make use of the original English queries. We compare response-based learning with a standard structured prediction setup that omits the use of the execution function e in the definition of y+ and y−. This algorithm can be seen as a stochastic (sub)gradient descent variant of RAMPION (Gimpel and Smith, 2012). It does not make use of the semantic parser, but defines positive and negative examples based on score s and cost c with respect to human reference translations. We report BLEU (Papineni et al., 2001) of translation system output measured against the original English queries. Furthermore, we report precision, recall, and F1-score for executing semantic parses built from translation system outputs against the GEOQUERY database. Precision is defined as the percentage of correctly answered examples out of those for which a parse could be produced; recall is defined as the percentage of total examples answered correctly; F1-score is the harmonic mean of both. Statistical significance is measured using Approximate Randomization (Noreen, 1989) where result differences with a pvalue smaller than </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2001. Bleu: a method for automatic evaluation of machine translation. Technical Report IBM Research Division Technical Report, RC22176 (W0190-022), Yorktown Heights, N.Y.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sheila M Pfafflin</author>
</authors>
<title>Evaluation of machine translations by reading comprehension tests and subjective judgements.</title>
<date>1965</date>
<journal>Mechanical Translation and Computational Linguistics,</journal>
<volume>8</volume>
<issue>2</issue>
<contexts>
<context position="9147" citStr="Pfafflin, 1965" startWordPosition="1415" endWordPosition="1416">list of translations order with respect to retrieval performance. In con882 Figure 1: Response-based learning cycle for grounding SMT in virtual trivia gameplay. trast to our work, all mentioned approaches to interactive or adaptive learning in SMT rely on human post-edits or human reference translations. Our work differs from these approaches in that exactly this dependency is alleviated by learning from responses in an extrinsic task. Interactive scenarios have been used for evaluation purposes of translation systems for nearly 50 years, especially using human reading comprehension testing (Pfafflin, 1965; Fuji, 1999; Jones et al., 2005), and more recently, using face-toface conversation mediated via machine translation (Sakamoto et al., 2013). However, despite offering direct and reliable prediction of translation quality, the cost and lack of reusability has confined task-based evaluations involving humans to testing scenarios, but prevented a use for interactive training of SMT systems as in our work. Lastly, our work is related to cross-lingual natural language processing such as cross-lingual question answering or cross-lingual information retrieval as conducted at recent evaluation campa</context>
</contexts>
<marker>Pfafflin, 1965</marker>
<rawString>Sheila M. Pfafflin. 1965. Evaluation of machine translations by reading comprehension tests and subjective judgements. Mechanical Translation and Computational Linguistics, 8(2):2–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akiko Sakamoto</author>
<author>Nayuko Watanabe</author>
<author>Satoshi Kamatani</author>
<author>Kazuo Sumita</author>
</authors>
<title>Development of a simultaneous interpretation system for face-to-face services and its evaluation experiment in real situation.</title>
<date>2013</date>
<booktitle>In Proceedings of the Machine Translation Summit XIV,</booktitle>
<location>Nice, France.</location>
<contexts>
<context position="9288" citStr="Sakamoto et al., 2013" startWordPosition="1435" endWordPosition="1438"> in virtual trivia gameplay. trast to our work, all mentioned approaches to interactive or adaptive learning in SMT rely on human post-edits or human reference translations. Our work differs from these approaches in that exactly this dependency is alleviated by learning from responses in an extrinsic task. Interactive scenarios have been used for evaluation purposes of translation systems for nearly 50 years, especially using human reading comprehension testing (Pfafflin, 1965; Fuji, 1999; Jones et al., 2005), and more recently, using face-toface conversation mediated via machine translation (Sakamoto et al., 2013). However, despite offering direct and reliable prediction of translation quality, the cost and lack of reusability has confined task-based evaluations involving humans to testing scenarios, but prevented a use for interactive training of SMT systems as in our work. Lastly, our work is related to cross-lingual natural language processing such as cross-lingual question answering or cross-lingual information retrieval as conducted at recent evaluation campaigns of the CLEF initiative.1 While these approaches focus on improvements of the respective natural language processing task, our goal is to</context>
</contexts>
<marker>Sakamoto, Watanabe, Kamatani, Sumita, 2013</marker>
<rawString>Akiko Sakamoto, Nayuko Watanabe, Satoshi Kamatani, and Kazuo Sumita. 2013. Development of a simultaneous interpretation system for face-to-face services and its evaluation experiment in real situation. In Proceedings of the Machine Translation Summit XIV, Nice, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avneesh Saluja</author>
<author>Ian Lane</author>
<author>Ying Zhang</author>
</authors>
<title>Machine translation with binary feedback: A largemargin approach.</title>
<date>2012</date>
<booktitle>In Proceedings of the 10th Biennial Conference of the Association for Machine Translation in the Americas (AMTA’12),</booktitle>
<location>San Diego, CA.</location>
<contexts>
<context position="7165" citStr="Saluja et al., 2012" startWordPosition="1100" endWordPosition="1103">rks are variants of structured prediction that take executability of semantic parses into account. Our work builds upon these ideas, however, to our knowledge the presented work is the first to embed translations into grounded scenarios in order to use feedback from interactions in these scenarios for structured learning in SMT. A recent important research direction in SMT has focused on employing automated translation as an aid to human translators. Computer assisted translation (CAT) subsumes several modes of interaction, ranging from binary feedback on the quality of the system prediction (Saluja et al., 2012), to human post-editing operations on a system prediction resulting in a reference translation (Cesa-Bianchi et al., 2008), to human acceptance or overriding of sentence completion predictions (Langlais et al., 2000; Barrachina et al., 2008; Koehn and Haddow, 2009). In all interaction scenarios, it is important that the system learns dynamically from its errors in order to offer the user the experience of a system that adapts to the provided feedback. Since retraining the SMT model after each interaction is too costly, online adaptation after each interaction has become the learning protocol o</context>
</contexts>
<marker>Saluja, Lane, Zhang, 2012</marker>
<rawString>Avneesh Saluja, Ian Lane, and Ying Zhang. 2012. Machine translation with binary feedback: A largemargin approach. In Proceedings of the 10th Biennial Conference of the Association for Machine Translation in the Americas (AMTA’12), San Diego, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shai Shalev-Shwartz</author>
<author>Yoram Singer</author>
<author>Nathan Srebro</author>
</authors>
<title>Pegasos: Primal Estimated subGrAdient SOlver for SVM.</title>
<date>2007</date>
<booktitle>In Proceedings of the 24th International Conference on Machine Learning (ICML’07),</booktitle>
<location>Corvallis, OR.</location>
<contexts>
<context position="19152" citStr="Shalev-Shwartz et al., 2007" startWordPosition="3035" endWordPosition="3038">ed above, it allows . to use human reference translations in addition to task-approved surrogate references. The cost function can be implemented by different versions of sentence-wise BLEU, or it can be omitted completely so that learning relies on task-based feedback alone, similar to algorithms recently suggested for semantic parsing (Goldwasser and Roth, 2013; Kwiatowski et al., 2013; Berant et al., 2013). Lastly, regularization can be introduced by using update rules corresponding to primal form optimization variants of support vector machines (Collobert and Bengio, 2004; Chapelle, 2007; Shalev-Shwartz et al., 2007). 885 method precision recall F1 BLEU 1 CDEC 63.67 58.21 60.82 46.53 2 EXEC 70.36 63.57 66.791 48.001 3 RAMPION 75.58 69.64 72.4912 56.6412 4 REBOL 81.15 75.36 78.15123 55.6612 Table 1: Experimental results using extended parser for returning answers from GEOQUERY (precision, recall, F1) and n-gram match to original English query (BLEU) on 280 re-translated test examples. Best results for each column are highlighted in bold face. Superscripts 1234 denote a significant improvement over the respective method. method precision recall F1 BLEU 1 CDEC 65.59 57.86 61.48 46.53 2 EXEC 66.54 61.79 64.07</context>
</contexts>
<marker>Shalev-Shwartz, Singer, Srebro, 2007</marker>
<rawString>Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro. 2007. Pegasos: Primal Estimated subGrAdient SOlver for SVM. In Proceedings of the 24th International Conference on Machine Learning (ICML’07), Corvallis, OR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Simianer</author>
<author>Stefan Riezler</author>
<author>Chris Dyer</author>
</authors>
<title>Joint feature selection in distributed stochastic learning for large-scale discriminative training in SMT.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<location>Jeju,</location>
<contexts>
<context position="22534" citStr="Simianer et al. (2012)" startWordPosition="3571" endWordPosition="3574">ta provided in the COMMON CRAWL6 (Smith et al., 2013) dataset. 5.2 Compared Systems Method 1 is the baseline system, consisting of the CDEC SMT system trained on the COMMON CRAWL data as described above. This system does not use any GEOQUERY data for training. Methods 2-4 use the 600 training examples from GEOQUERY for discriminative training only. Variants of the response-based learning algorithm described above are implemented as a standalone tool that operates on CDEC n-best lists of 10,000 translations of the GEOQUERY training data. All variants use sparse features of CDEC as described in Simianer et al. (2012) that extract rule 5https://github.com/redpony/cdec 6http://www.statmt.org/wmt13/ training-parallel-commoncrawl.tgz 886 prediction: how many inhabitants has new york reference: how many people live in new york prediction: how big is the population of texas reference: how many people live in texas prediction: which are the cities of the state with the highest elevation reference: what are the cities of the state with the highest point prediction: how big is the population of states , through which the mississippi runs reference: what are the populations of the states through which the mississip</context>
</contexts>
<marker>Simianer, Riezler, Dyer, 2012</marker>
<rawString>Patrick Simianer, Stefan Riezler, and Chris Dyer. 2012. Joint feature selection in distributed stochastic learning for large-scale discriminative training in SMT. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL 2012), Jeju, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason R Smith</author>
<author>Herve Saint-Amand</author>
<author>Magdalena Plamada</author>
<author>Philipp Koehn</author>
<author>Chris Callison-Burch</author>
<author>Adam Lopez</author>
</authors>
<title>Dirt cheap web-scale parallel text from the common crawl.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL’13),</booktitle>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="21965" citStr="Smith et al., 2013" startWordPosition="3477" endWordPosition="3480">051107/geoquery-2012-08-27.zip 4https://github.com/jacobandreas/ smt-semparse et al. (2013) that is trained only on the 600 GEOQUERY training examples. The bilingual SMT system used in our experiments is the state-of-the-art SCFG decoder CDEC (Dyer et al., 2010)5. We built grammars using its implementation of the suffix array extraction method described in Lopez (2007). For language modeling, we built a modified Kneser-Ney smoothed 5-gram language model using the English side of the training data. We trained the SMT system on the English-German parallel web data provided in the COMMON CRAWL6 (Smith et al., 2013) dataset. 5.2 Compared Systems Method 1 is the baseline system, consisting of the CDEC SMT system trained on the COMMON CRAWL data as described above. This system does not use any GEOQUERY data for training. Methods 2-4 use the 600 training examples from GEOQUERY for discriminative training only. Variants of the response-based learning algorithm described above are implemented as a standalone tool that operates on CDEC n-best lists of 10,000 translations of the GEOQUERY training data. All variants use sparse features of CDEC as described in Simianer et al. (2012) that extract rule 5https://git</context>
</contexts>
<marker>Smith, Saint-Amand, Plamada, Koehn, Callison-Burch, Lopez, 2013</marker>
<rawString>Jason R. Smith, Herve Saint-Amand, Magdalena Plamada, Philipp Koehn, Chris Callison-Burch, and Adam Lopez. 2013. Dirt cheap web-scale parallel text from the common crawl. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL’13), Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katharina W¨aschle</author>
<author>Patrick Simianer</author>
<author>Nicola Bertoldi</author>
<author>Stefan Riezler</author>
<author>Marcello Federico</author>
</authors>
<title>Generative and discriminative methods for online adaptation in SMT.</title>
<date>2013</date>
<booktitle>In Proceedings of the Machine Translation Summit XIV,</booktitle>
<location>Nice, France.</location>
<marker>W¨aschle, Simianer, Bertoldi, Riezler, Federico, 2013</marker>
<rawString>Katharina W¨aschle, Patrick Simianer, Nicola Bertoldi, Stefan Riezler, and Marcello Federico. 2013. Generative and discriminative methods for online adaptation in SMT. In Proceedings of the Machine Translation Summit XIV, Nice, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuk Wah Wong</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning for semantic parsing with statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT/NAACL’06),</booktitle>
<location>New York City, NY.</location>
<contexts>
<context position="5537" citStr="Wong and Mooney (2006)" startWordPosition="849" endWordPosition="852">to study natural language in the context of a non-linguistic environment, in which meaning is grounded in perception and/or action. This presents an analogy to human learning, where a learner tests her understanding in an actionable setting. Such a setting can be a simulated world environment in which the linguistic representation can be directly executed by a computer system. For example, in semantic parsing, the learning goal is to produce and successfully execute a meaning representation. Executable system actions include access to databases such as the GEOQUERY database on U.S. geography (Wong and Mooney (2006), inter alia), the ATIS travel planning database (Zettlemoyer and Collins (2009), inter alia), robotic control in simulated navigation tasks (Chen and Mooney (2011), inter alia), databases of simulated card games (Goldwasser and Roth (2013), inter alia), or the user-generated contents of FREEBASE (Cai and Yates (2013), inter alia). Since there are many possible correct parses, matching against a single gold standard falls short of grounding in a non-linguistic environment. Rather, the semantic context for interpretation, as well as the success criterion in evaluation is defined by successful e</context>
</contexts>
<marker>Wong, Mooney, 2006</marker>
<rawString>Yuk Wah Wong and Raymond J. Mooney. 2006. Learning for semantic parsing with statistical machine translation. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT/NAACL’06), New York City, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke S Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Learning context-dependent mappings from sentences to logical form.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics (ACL-IJCNLP’09),</booktitle>
<contexts>
<context position="5617" citStr="Zettlemoyer and Collins (2009)" startWordPosition="861" endWordPosition="864">, in which meaning is grounded in perception and/or action. This presents an analogy to human learning, where a learner tests her understanding in an actionable setting. Such a setting can be a simulated world environment in which the linguistic representation can be directly executed by a computer system. For example, in semantic parsing, the learning goal is to produce and successfully execute a meaning representation. Executable system actions include access to databases such as the GEOQUERY database on U.S. geography (Wong and Mooney (2006), inter alia), the ATIS travel planning database (Zettlemoyer and Collins (2009), inter alia), robotic control in simulated navigation tasks (Chen and Mooney (2011), inter alia), databases of simulated card games (Goldwasser and Roth (2013), inter alia), or the user-generated contents of FREEBASE (Cai and Yates (2013), inter alia). Since there are many possible correct parses, matching against a single gold standard falls short of grounding in a non-linguistic environment. Rather, the semantic context for interpretation, as well as the success criterion in evaluation is defined by successful execution of an action in the extrinsic environment, e.g., by receiving the corre</context>
</contexts>
<marker>Zettlemoyer, Collins, 2009</marker>
<rawString>Luke S. Zettlemoyer and Michael Collins. 2009. Learning context-dependent mappings from sentences to logical form. In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics (ACL-IJCNLP’09), Singapore.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>