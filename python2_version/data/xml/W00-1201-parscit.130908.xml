<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000461">
<title confidence="0.994977">
Two Statistical Parsing Models Applied to the Chinese Treebank
</title>
<author confidence="0.999452">
Daniel M. Bikel David Chiang
</author>
<affiliation confidence="0.998559">
Department of Computer &amp; Information Science
University of Pennsylvania
</affiliation>
<address confidence="0.9749365">
200 South 33rd Street
Philadelphia, PA 19104-6389
</address>
<email confidence="0.927868">
Idbikel,dchiangl@cis .upenn.edu
</email>
<sectionHeader confidence="0.989761" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999674384615384">
This paper presents the first-ever
results of applying statistical pars-
ing models to the newly-available
Chinese Treebank. We have em-
ployed two models, one extracted
and adapted from BBN&apos;s SIFT Sys-
tem (Miller et al., 1998) and a TAG-
based parsing model, adapted from
(Chiang, 2000). On sentences with
&lt;40 words, the former model per-
forms at 69% precision, 75% recall,
and the latter at 77% precision and
78% recall.
</bodyText>
<sectionHeader confidence="0.998795" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999987577777778">
Ever since the success of HMMs&apos; applica-
tion to part-of-speech tagging in (Church,
1988), machine learning approaches to nat-
ural language processing have steadily be-
come more widespread. This increase has of
course been due to their proven efficacy in
many tasks, but also to their engineering effi-
cacy. Many machine learning approaches let
the data speak for itself (data ipsa loguun-
tur), as it were, allowing the modeler to focus
on what features of the data are important,
rather than on the complicated interaction
of such features, as had often been the case
with hand-crafted NLP systems. The success
of statistical methods in particular has been
quite evident in the area of syntactic pars-
ing, most recently with the outstanding re-
sults of (Charniak, 2000) and (Collins, 2000)
on the now-standard English test set of the
Penn rkeebank (Marcus et al., 1993). A sig-
nificant trend in parsing models has been the
incorporation of linguistically-motivated fea-
tures; however, it is important to note that
&amp;quot;linguistically-motivated&amp;quot; does not necessarily
mean language-dependent&amp;quot;--often, it means
just the opposite. For example, almost all sta-
tistical parsers make use of lexicalized non-
terminals in some way, which allows lexical
items&apos; indiosyncratic parsing preferences to
be modeled, but the paring between head
words and their parent nonterminals is deter-
mined almost entirely by the training data,
thereby making this feature—which models
preferences of particular words of a par-
ticular language—almost entirely language-
independent. In this paper, we will explore the
use of two parsing models, which were origi-
nally designed for English parsing, on parsing
Chinese, using the newly-available Chinese
IYeebank. We will show that the language-
dependent components of these parsers are
quite compact, and that with little effort they
can be adapted to produce promising results
for Chinese parsing. We also discuss directions
for future work.
</bodyText>
<sectionHeader confidence="0.862093" genericHeader="introduction">
2 Models and Modifications
</sectionHeader>
<bodyText confidence="0.9997448">
We will briefly describe the two parsing mod-
els employed (for a full description of the BBN
model, see (Miller et al., 1998) and also (Bikel,
2000); for a full description of the TAG model,
see (Chiang, 2000)).
</bodyText>
<subsectionHeader confidence="0.984088">
2.1 Model 2 of (Collins, 1997)
</subsectionHeader>
<bodyText confidence="0.999864222222222">
Both parsing models discussed in this paper
inherit a great deal from this model, so we
briefly describe its &amp;quot;progenitive&amp;quot; features here,
describing only how each of the two models of
this paper differ in the subsequent two sec-
tions.
The lexicalized PCFG that sits behind
Model 2 of (Collins, 1997) has rules of the
form
</bodyText>
<equation confidence="0.912101571428572">
P • L1HR1- • - R1R„ (1)
1
S (will-MD)
NP(Apple--NNP)
NNP
Apple
MD
will
VB
buy
VP (buy-VB)
PRT(out-RP) NP (Microsoft -NNP)
RP NNP
out Microsoft
</equation>
<figureCaption confidence="0.999552">
Figure 1: A sample sentence with parse tree.
</figureCaption>
<bodyText confidence="0.999982692307692">
where P, L, Ri and H are all lexicalized
nonterminals, and P inherits its lexical head
from its distinguished head child, H. In this
generative model, first P is generated, then
its head-child H, then each of the left- and
right-modifying nonterminals are generated
from the head outward. The modifying non-
terminals Li and Ri are generated condition-
ing on P and H, as well as a distance met-
ric (based on what material intervenes be-
tween the currently-generated modifying non-
terminal and H) and an incremental subcat
frame feature (a multiset containing the com-
plements of H that have yet to be gener-
ated on the side of H in which the currently-
generated nonterminal falls). Note that if the
modifying nonterminals were generated com-
pletely independently, the model would be
very impoverished, but in actuality, by includ-
ing the distance and subcat frame features,
the model captures a crucial bit of linguis-
tic reality, viz., that words often have well-
defined sets of complements and adjuncts, dis-
persed with some well-defined distribution in
the right hand sides of a (context-free) rewrit-
ing system.
</bodyText>
<subsectionHeader confidence="0.977922">
2.2 BBN Model
2.2.1 Overview
</subsectionHeader>
<bodyText confidence="0.9956650625">
The BBN model is also of the lexicalized
PCFG variety. In the BBN model, as with
Model 2 of (Collins, 1997), modifying non-
terminals are generated conditioning both on
the parent P and its head child H. Unlike
Model 2 of (Collins, 1997), they are also gen-
erated conditioning on the previously gener-
ated modifying nonterminal, or
and there is no subcat frame or distance fea-
ture. While the BBN model does not per-
form at the level of Model 2 of (Collins, 1997)
on Wall Street Journal text, it is also less
language-dependent, eschewing the distance
metric (which relied on specific features of the
English Treebank) in favor of the &amp;quot;bigrams on
nonterminals&amp;quot; model.
</bodyText>
<subsectionHeader confidence="0.65033">
2.2.2 Model Parameters
</subsectionHeader>
<bodyText confidence="0.999801">
This section briefly describes the top-level
parameters used in the BBN parsing model.
We use p to denote the unlexicalized nonter-
minal corresponding to P in (1), and simi-
larly for i, ri and h. We now present the top-
level generation probabilities, along with ex-
amples from Figure 1. For brevity, we omit the
smoothing details of BBN&apos;s model (see (Miller
et al., 1998) for a complete description); we
note that all smoothing weights are computed
via the technique described in (Bikel et al.,
1997).
The probability of generating p as the
root label is predicted conditioning on only
+TOP+, which is the hidden root of all parse
trees:
</bodyText>
<equation confidence="0.991056">
P (pI + TOP+) , e.g., P(S I + TOP+). (2)
</equation>
<bodyText confidence="0.9998705">
The probability of generating a head node h
with a parent p is
</bodyText>
<equation confidence="0.846054">
P(hIp), e.g., P(VP)S). (3)
</equation>
<bodyText confidence="0.9967945">
The probability of generating a left-modifier
ii is
</bodyText>
<equation confidence="0.974734">
PL(ii h, wh), 7 e.g.) (4)
PL, (NP I + BEGIN+, S, VP, will)
</equation>
<page confidence="0.844936">
2
</page>
<bodyText confidence="0.99695">
when generating the NP for NP(Apple—NNP),
and the probability of generating a right mod-
</bodyText>
<equation confidence="0.834389">
ifier ri is
PR(ri I ri_i,p, h, wh), e.g., (5)
PR(NP I PRT, VP, VB, buy)
</equation>
<bodyText confidence="0.999829285714286">
when generating the NP for NP(Microsoft—
NNP).1
The probabilities for generating lexical ele-
ments (part-of-speech tags and words) are as
follows. The part of speech tag of the head of
the entire sentence, th, is computed condition-
ing only on the top-most symbol p:2
</bodyText>
<equation confidence="0.994206">
P(iii 1P)- (6)
</equation>
<bodyText confidence="0.9955348">
Part of speech tags of modifier constituents,
and tri, are predicted conditioning on the
modifier constituent i or ri, the tag of the
head constituent, th, and the word of the head
constituent, wh
</bodyText>
<equation confidence="0.61398">
P(tiiI l, th, wh) and P(tri ri, th, WO. (7)
</equation>
<bodyText confidence="0.99981">
The head word of the entire sentence, wh, is
predicted conditioning only on the top-most
symbol p and th:
</bodyText>
<equation confidence="0.948124">
P(wh th,P)- (8)
</equation>
<bodyText confidence="0.99999025">
Head words of modifier constituents, wii and
wri , are predicted conditioning on all the con-
text used for predicting parts of speech in (7),
as well as the parts of speech themsleves
</bodyText>
<subsubsectionHeader confidence="0.45179">
P(wis I ti, ii, th, Wh)
</subsubsectionHeader>
<bodyText confidence="0.990544269230769">
and P (wri I tri , ri, WO- (9)
The original English model also included a
word feature to help reduce part-of-speech
ambiguity for unknown words, but this com-
ponent of the model was removed for Chinese,
as it was language-dependent.
The probability of an entire parse tree is
the product of the probabilities of generat-
ing all of the elements of that parse tree,
The hidden nonterminal +BEGIN+ is used to
provide a convenient mechanism for determining the
initial probability of the underlying Markov process
generating the modifying nonterminals; the hidden
nonterminal +END+ is used to provide consistency to
the underlying Markov process, i.e., so that the proba-
bilities of all possible nonterminal sequences sum to 1.
2This is the one place where we altered the original
model, as the lexical components of the head of the
entire sentence were all being estimated incorrectly,
causing an inconsistency in the model. We corrected
the estimation of th and wh in our implementation.
where an element is either a constituent la-
bel, a part of speech tag or a word. We obtain
maximum-likelihood estimates of the param-
eters of this model using frequencies gathered
from the training data.
</bodyText>
<subsectionHeader confidence="0.804164">
2.3 TAG Model
</subsectionHeader>
<bodyText confidence="0.999979793103448">
The model of (Chiang, 2000) is based
on stochastic TAG (Resnik, 1992; Schabes,
1992). In this model a parse tree is built up not
out of lexicalized phrase-structure rules but by
tree fragments (called elementary trees) which
are lexicalized in the sense that each fragment
contains exactly one lexical item (its anchor).
In the variant of TAG we use, there are
three kinds of elementary tree: initial, (pred-
icative) auxiliary, and modifier, and three
composition operations: substitution, adjunc-
tion, and sister-adjunction. Figure 2 illus-
trates all three of these operations, al is an
initial tree which substitutes at the leftmost
node labeled NP,I.; 13 is an auxiliary tree which
adjoins at the node labeled VP. See (Joshi and
Schabes, 1997) for a more detailed explana-
tion.
Sister-adjunction is not a standard TAG op-
eration, but borrowed from D-Tree Grammar
(Rambow et al., 1995). In Figure 2 the modi-
fier tree 7 is sister adjoined between the nodes
labeled VB and NI4. Multiple modifier trees
can adjoin at the same place, in the spirit of
(Schabes and Shieber, 1994).
In stochastic TAG, the probability of gen-
erating an elementary tree depends on the el-
ementary tree itself and the elementary tree
it attaches to. The parameters are as follows:
</bodyText>
<equation confidence="0.997188">
E Pi(a) =1
EPs(a I n)
E p6,03 I + Pa(NONE 77) = 1
</equation>
<bodyText confidence="0.998899">
where a ranges over initial trees, /3 over aux-
iliary trees, 7 over modifier trees, and n over
nodes. Pi(a) is the probability of beginning
a derivation with a; Ps(a j ) is the prob-
ability of substituting a at 71; Pa(O I n) is
the probability of adjoining 13 at 77; finally,
Pa(NONE j 7) is the probability of nothing
adjoining at n.
Our variant adds another set of parameters:
</bodyText>
<page confidence="0.944842">
3
</page>
<figure confidence="0.999573035714286">
NP
NNP
Apple
(ay) (3) (7) (as)
(a2)
NPJ. VP
/
/ VB t NIPJ.
I k
buy \
VP PRT
MD VP* Rp
will out
NP
NP
Microsoft
derivation tree
al # 7 a3
NP VP
derived tree
NI4P
MD VP
Apple 1
will
VB PRT NP
1
buy RP NNP
out Microsoft
</figure>
<figureCaption confidence="0.999074">
Figure 2: Grammar and derivation for &amp;quot;Apple will buy out Microsoft.&amp;quot;
</figureCaption>
<equation confidence="0.702831">
E Psa(7 I f) +Psa(STOP I 77, f) = 1
</equation>
<bodyText confidence="0.999896740740741">
This is the probability of sister-adjoining -y
between the ith and i + 1 th children of 77 (al-
lowing for two imaginary children beyond the
leftmost and rightmost children). Since multi-
ple modifier trees can adjoin at the same lo-
cation, P.,a(-y) is also conditioned on a flag f
which indicates whether -y is the first modi-
fier tree (i.e., the one closest to the head) to
adjoin at that location.
For our model we break down these prob-
abilities further: first the elementary tree is
generated without its anchor, and then its an-
chor is generated. See (Chiang, 2000) for more
details.
During training each example is broken
into elementary trees using head rules and
argument/adjunct rules similar to those of
(Collins, 1997). The rules are interpreted as
follows: a head is kept in the same elemen-
tary tree in its parent, an argument is broken
off into a separate initial tree, leaving a sub-
stitution node, and an adjunct is broken off
into a separate modifier tree. A different rule
is used for extracting auxiliary trees; see (Chi-
ang, 2000) for details. Xia (1999) describes a
similar process, and in fact our rules for the
Xinhua corpus are based on hers.
</bodyText>
<subsectionHeader confidence="0.966067">
2.4 Modifications
</subsectionHeader>
<bodyText confidence="0.99988475">
The primary language-dependent component
that had to be changed in both models was
the head table, used to determine heads when
training. We modified the head rules described
in (Xia, 1999) for the Xinhua corpus and sub-
stituted these new rules into both models.
The (Chiang, 2000) model had the following
additional modifications.
</bodyText>
<listItem confidence="0.870839608695652">
• The new corpus had to be prepared for
use with the trainer and parser. Aside
from technicalities, this involved retrain-
ing the part-of-speech tagger described in
(Ratnaparkhi, 1997), which was used for
tagging unknown words. We also lowered
the unknown word threshold from 4 to 2
because the Xinhua corpus was smaller
than the WSJ corpus.
• In addition to the change to the head-
finding rules, we also changed the rules
for classifying modifiers as arguments or
adjuncts. In both cases the new rules were
adapted from (Xia, 1999).
• For the tests done in this paper, a beam
width of 10-4 was used.
The BBN model had the following additional
modifications:
• As with the (Chiang, 2000) model, we
similarly lowered the unknown word
threshold of the BBN model from its de-
fault 5 to 2.
• The language-dependent word-feature
</listItem>
<bodyText confidence="0.73241725">
was eliminated, causing parts of speech
for unknown words to be predicted solely
on the head relations in the model.
• The default beam size in the probabilis-
tic CKY parsing algorithm was widened.
The default beam pruned away chart en-
tries whose scores were not within a fac-
tor of e-5 of the top-ranked subtree; this
</bodyText>
<page confidence="0.995457">
4
</page>
<table confidence="0.9996689375">
Model, test set &lt;40 words
LR LP CB OCB &lt;2CB
BBN-allt, WSJ-all 84.7 86.5 1.12 60.6 83.2
BBN-smallf, , WSJ-small* 79.0 80.7 1.66 47.0 74.6
BBN, Xinhua t 69.0 74.8 2.05 45.0 68.5
Chiang-all, WSJ-all 86.9 86.6 1.09 63.2 84.3
Chiang-small, WSJ-small 78.9 79.6 1.75 44.8 72.4
Chiang, Xinhua 76.8 77.8 1.99 50.8 74.1
&lt;100 words
LR LP CB OCB &lt;2CB
BBN-allf, WSJ-all 83.9 85.7 1.31 57.8 80.8
BBN-smallf, WSJ-small* 78.4 80.0 1.92 44.3 71.3
BBN, Xinhua t 67.5 73.5 2.87 39.9 61.8
Chiang-all, WSJ-all 86.2 85.8 1.29 60.4 81.8
Chiang-small, WSJ-small 77.1 78.8 2.00 43.25 70.5
Chiang, Xinhua 73.3 74.6 3.03 44.8 66.8
</table>
<tableCaption confidence="0.785846333333333">
Table 1: Results for both parsing models on all test sets. Key: LR = labeled recall, LP = labeled
precision, CB = avg. crossing brackets, OCB = zero crossing brackets, &lt;2CB = &lt;2 crossing
brackets. All results are percentages, except for those in the CB column. t Used larger beam
</tableCaption>
<bodyText confidence="0.945174285714286">
settings and lower unknown word threshold than the defaults. *3 of the 400 sentences were not
parsed due to timeouts and/or pruning problems. t3 of the 348 sentences did not get parsed due
to pruning problems, and 2 other sentences had length mismatches (scoring program errors).
tight limit was changed to e-9. Also, the
default decoder pruned away all but the
top 25-ranked chart entries in each cell;
this limit was expanded to 50.
</bodyText>
<sectionHeader confidence="0.994896" genericHeader="related work">
3 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999941157894737">
The Chinese Treebank consists of 4185 sen-
tences of Xinhua newswire text. We blindly
separated this into training, devtest and test
sets, with a roughly 80/10/10 split, putting
files 001-270 (3484 sentences, 84,873 words)
into the training set, 301-325 (353 sentences,
6776 words) into the development test set and
reserving 271-300 (348 sentences, 7980 words)
for testing. See Table 1 for results.
In order to put the new Chinese Treebank
results into context with the unmodified (En-
glish) parsing models, we present results on
two test sets from the Wall Street Journal:
WSJ-all, which is the complete Section 23 (the
de facto standard test set for English pars-
ing), and WSJ-small, which is the first 400
sentences of Section 23 and which is roughly
comparable in size to the Chinese test set.
Furthermore, when testing on WSJ-small, we
trained on a subset of our English training
data roughly equivalent in size to our Chinese
training set (Sections 02 and 03 of the Penn
Treebank); we have indicated models trained
on all English training with &amp;quot;-all&amp;quot;, and mod-
els trained with the reduced English train-
ing set with &amp;quot;-small&amp;quot;. Therefore, by compar-
ing the WSJ-small results with the Chinese
results, one can reasonably gauge the perfor-
mance gap between English parsing on the
Penn Treebank and Chinese parsing on the
Chinese Treebank.
The reader will note that the modified BBN
model does significantly poorer than (Chiang,
2000) on Chinese. While more investigation is
required, we suspect part of the difference may
be due to the fact that currently, the BBN
model uses language-specific rules to guess
part of speech tags for unknown words.
</bodyText>
<sectionHeader confidence="0.998794" genericHeader="conclusions">
4 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999218125">
There is no question that a great deal of care
and expertise went into creating the Chinese
Treebank, and that it is a source of important
grammatical information that is unique to the
Chinese language. However, there are definite
similarities between the grammars of English
and Chinese, especially when viewed through
the lens of the statistical models we employed
</bodyText>
<page confidence="0.969276">
5
</page>
<bodyText confidence="0.999929730769231">
here. In both languages, the nouns, adjec-
tives, adverbs, and verbs have preferences for
certain arguments and adjuncts, and these
preferences—in spite of the potentially vastly-
different configurations of these items—are ef-
fectively modeled. As discussed in the intro-
duction, lexical items&apos; idiosyncratic parsing
preferences are modeled by lexicalizing the
grammar formalism, using a lexicalized PCFG
in one case and a lexicalized stochastic TAG
in the other. Linguistically-reasonable inde-
pendence assumptions are made, such as the
independence of grammar productions in the
case of the PCFG model, or the independence
of the composition operations in the case of
the LTAG model, and we would argue that
these assumptions are no less reasonable for
the Chinese grammar than they are for that
of English. While results for the two languages
are far from equal, we believe that further tun-
ing of the head rules, and analysis of develop-
ment test set errors will yield significant per-
formance gains on Chinese to close the gap.
Finally, we fully expect that absolute perfor-
mance will increase greatly as additional high-
quality Chinese parse data becomes available.
</bodyText>
<sectionHeader confidence="0.998887" genericHeader="acknowledgments">
5 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999875375">
This research was funded in part by NSF
grant SBR-89-20230-15. We would greatly
like to acknowledge the researchers at BBN
who allowed us to use their model: Ralph
Weischedel, Scott Miller, Lance Ramshaw,
Heidi Fox and Sean Boisen. We would also
like to thank Mike Collins and our advisors
Aravind Joshi and Mitch Marcus.
</bodyText>
<sectionHeader confidence="0.998263" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999414142857143">
Daniel M. Bikel, Richard Schwartz, Ralph
Weischedel, and Scott Miller. 1997. Nymble:
A high-performance learning name-finder. In
Fifth Conference on Applied Natural Language
Processing, pages 194-201„ Washington, D.C.
Daniel M. Bikel. 2000. A statistical model for
parsing and word-sense disambiguation. In
Joint SIGDAT Conference on Empirical Meth-
ods in Natural Language Processing and Very
Large Corpora, Hong Kong, October.
Eugene Charniak. 2000. A maximum entropy-
inspired parser. In Proceedings of the 1st Meet-
ing of the North American Chapter of the As-
sociation for Computational Linguistics, pages
132-139, Seattle, Washington, April 29 to May
4.
David Chiang. 2000. Statistical parsing with an
automatically-extracted tree adjoining gram-
mar. In Proceedings of the 38th Annual Meeting
of the Association for Computational Linguis-
tics.
Kenneth Church. 1988. A stochastic parts pro-
gram and noun phrase parser for unrestricted
text. In Second Conference on Applied Natural
Language Processing, pages 136-143, Austin,
Texas.
Michael Collins. 1997. Three generative, lexi-
calised models for statistical parsing. In Pro-
ceedings of ACL-EACL &apos;97, pages 16-23.
Michael Collins. 2000. Discriminative reranlcing
for natural language parsing. In International
Conference on Machine Learning. (to appear).
Aravind K. Joshi and Yves Schabes. 1997. Tree-
adjoining grammars. In A. Salomma and
G. Rosenberg, editors, Handbook of Formal
Languages and Automata, volume 3, pages 69-
124. Springer-Verlag, Heidelberg.
Mitchell P. Marcus, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building a
large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19:313-
330.
Scott Miller, Heidi Fox, Lance Ramshaw, and
Ralph Weischedel. 1998. SIFT - Statistically-
derived Information From Text. In Seventh
Message Understanding Conference (MUC-7),
Washington, D.C.
Owen Rainbow, K. Vijay-Shanker, and David
Weir. 1995. D-tree grammars. In Proceedings
of the 33rd Annual Meeting of the Assocation
for Computational Linguistics, pages 151-158.
Adwait Ratnaparkhi. 1997. A simple introduc-
tion to maximum entropy models for natural
language processing. Technical Report IRCS
Report 97-08, Institute for Research in Cogni-
tive Science, May.
Philip Resnik. 1992. Probabilistic tree-adjoining
grammar as a framework for statistical nat-
ural language processing. In Proceedings of
COLING-92, pages 418-424.
Yves Schabes and Stuart M. Shieber. 1994. An
alternative conception of tree-adjoining deriva-
tion. Computational Linguistics, 20(1):91-124.
Yves Schabes. 1992. Stochastic lexicalized
tree-adjoining grammars. In Proceedings of
COLING-92, pages 426-432.
Fel Xia. 1999. Extracting tree adjoining gram-
mars from bracketed corpora. In Proceedings
of the 5th Natural Language Processing Pacific
Rim Symposium (NLPRS-99).
</reference>
<page confidence="0.998779">
6
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.731059">
<title confidence="0.998942">Two Statistical Parsing Models Applied to the Chinese Treebank</title>
<author confidence="0.999994">Daniel M Bikel David Chiang</author>
<affiliation confidence="0.999627">Department of Computer &amp; Information University of</affiliation>
<address confidence="0.9782465">200 South 33rd Philadelphia, PA</address>
<email confidence="0.855804">Idbikel,dchiangl@cis.upenn.edu</email>
<abstract confidence="0.990630642857143">This paper presents the first-ever results of applying statistical parsing models to the newly-available Chinese Treebank. We have employed two models, one extracted and adapted from BBN&apos;s SIFT System (Miller et al., 1998) and a TAGbased parsing model, adapted from (Chiang, 2000). On sentences with &lt;40 words, the former model performs at 69% precision, 75% recall, and the latter at 77% precision and 78% recall.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
<author>Richard Schwartz</author>
<author>Ralph Weischedel</author>
<author>Scott Miller</author>
</authors>
<title>Nymble: A high-performance learning name-finder.</title>
<date>1997</date>
<booktitle>In Fifth Conference on Applied Natural Language Processing,</booktitle>
<pages>194--201</pages>
<location>Washington, D.C.</location>
<contexts>
<context position="5718" citStr="Bikel et al., 1997" startWordPosition="937" endWordPosition="940">lied on specific features of the English Treebank) in favor of the &amp;quot;bigrams on nonterminals&amp;quot; model. 2.2.2 Model Parameters This section briefly describes the top-level parameters used in the BBN parsing model. We use p to denote the unlexicalized nonterminal corresponding to P in (1), and similarly for i, ri and h. We now present the toplevel generation probabilities, along with examples from Figure 1. For brevity, we omit the smoothing details of BBN&apos;s model (see (Miller et al., 1998) for a complete description); we note that all smoothing weights are computed via the technique described in (Bikel et al., 1997). The probability of generating p as the root label is predicted conditioning on only +TOP+, which is the hidden root of all parse trees: P (pI + TOP+) , e.g., P(S I + TOP+). (2) The probability of generating a head node h with a parent p is P(hIp), e.g., P(VP)S). (3) The probability of generating a left-modifier ii is PL(ii h, wh), 7 e.g.) (4) PL, (NP I + BEGIN+, S, VP, will) 2 when generating the NP for NP(Apple—NNP), and the probability of generating a right modifier ri is PR(ri I ri_i,p, h, wh), e.g., (5) PR(NP I PRT, VP, VB, buy) when generating the NP for NP(Microsoft— NNP).1 The probabi</context>
</contexts>
<marker>Bikel, Schwartz, Weischedel, Miller, 1997</marker>
<rawString>Daniel M. Bikel, Richard Schwartz, Ralph Weischedel, and Scott Miller. 1997. Nymble: A high-performance learning name-finder. In Fifth Conference on Applied Natural Language Processing, pages 194-201„ Washington, D.C.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
</authors>
<title>A statistical model for parsing and word-sense disambiguation.</title>
<date>2000</date>
<booktitle>In Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,</booktitle>
<location>Hong Kong,</location>
<contexts>
<context position="2803" citStr="Bikel, 2000" startWordPosition="437" endWordPosition="438"> entirely languageindependent. In this paper, we will explore the use of two parsing models, which were originally designed for English parsing, on parsing Chinese, using the newly-available Chinese IYeebank. We will show that the languagedependent components of these parsers are quite compact, and that with little effort they can be adapted to produce promising results for Chinese parsing. We also discuss directions for future work. 2 Models and Modifications We will briefly describe the two parsing models employed (for a full description of the BBN model, see (Miller et al., 1998) and also (Bikel, 2000); for a full description of the TAG model, see (Chiang, 2000)). 2.1 Model 2 of (Collins, 1997) Both parsing models discussed in this paper inherit a great deal from this model, so we briefly describe its &amp;quot;progenitive&amp;quot; features here, describing only how each of the two models of this paper differ in the subsequent two sections. The lexicalized PCFG that sits behind Model 2 of (Collins, 1997) has rules of the form P • L1HR1- • - R1R„ (1) 1 S (will-MD) NP(Apple--NNP) NNP Apple MD will VB buy VP (buy-VB) PRT(out-RP) NP (Microsoft -NNP) RP NNP out Microsoft Figure 1: A sample sentence with parse tr</context>
</contexts>
<marker>Bikel, 2000</marker>
<rawString>Daniel M. Bikel. 2000. A statistical model for parsing and word-sense disambiguation. In Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, Hong Kong, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum entropyinspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st Meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<volume>29</volume>
<pages>132--139</pages>
<location>Seattle, Washington,</location>
<contexts>
<context position="1447" citStr="Charniak, 2000" startWordPosition="229" endWordPosition="230"> have steadily become more widespread. This increase has of course been due to their proven efficacy in many tasks, but also to their engineering efficacy. Many machine learning approaches let the data speak for itself (data ipsa loguuntur), as it were, allowing the modeler to focus on what features of the data are important, rather than on the complicated interaction of such features, as had often been the case with hand-crafted NLP systems. The success of statistical methods in particular has been quite evident in the area of syntactic parsing, most recently with the outstanding results of (Charniak, 2000) and (Collins, 2000) on the now-standard English test set of the Penn rkeebank (Marcus et al., 1993). A significant trend in parsing models has been the incorporation of linguistically-motivated features; however, it is important to note that &amp;quot;linguistically-motivated&amp;quot; does not necessarily mean language-dependent&amp;quot;--often, it means just the opposite. For example, almost all statistical parsers make use of lexicalized nonterminals in some way, which allows lexical items&apos; indiosyncratic parsing preferences to be modeled, but the paring between head words and their parent nonterminals is determine</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum entropyinspired parser. In Proceedings of the 1st Meeting of the North American Chapter of the Association for Computational Linguistics, pages 132-139, Seattle, Washington, April 29 to May 4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Statistical parsing with an automatically-extracted tree adjoining grammar.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2864" citStr="Chiang, 2000" startWordPosition="448" endWordPosition="449">e the use of two parsing models, which were originally designed for English parsing, on parsing Chinese, using the newly-available Chinese IYeebank. We will show that the languagedependent components of these parsers are quite compact, and that with little effort they can be adapted to produce promising results for Chinese parsing. We also discuss directions for future work. 2 Models and Modifications We will briefly describe the two parsing models employed (for a full description of the BBN model, see (Miller et al., 1998) and also (Bikel, 2000); for a full description of the TAG model, see (Chiang, 2000)). 2.1 Model 2 of (Collins, 1997) Both parsing models discussed in this paper inherit a great deal from this model, so we briefly describe its &amp;quot;progenitive&amp;quot; features here, describing only how each of the two models of this paper differ in the subsequent two sections. The lexicalized PCFG that sits behind Model 2 of (Collins, 1997) has rules of the form P • L1HR1- • - R1R„ (1) 1 S (will-MD) NP(Apple--NNP) NNP Apple MD will VB buy VP (buy-VB) PRT(out-RP) NP (Microsoft -NNP) RP NNP out Microsoft Figure 1: A sample sentence with parse tree. where P, L, Ri and H are all lexicalized nonterminals, an</context>
<context position="8337" citStr="Chiang, 2000" startWordPosition="1401" endWordPosition="1402"> underlying Markov process, i.e., so that the probabilities of all possible nonterminal sequences sum to 1. 2This is the one place where we altered the original model, as the lexical components of the head of the entire sentence were all being estimated incorrectly, causing an inconsistency in the model. We corrected the estimation of th and wh in our implementation. where an element is either a constituent label, a part of speech tag or a word. We obtain maximum-likelihood estimates of the parameters of this model using frequencies gathered from the training data. 2.3 TAG Model The model of (Chiang, 2000) is based on stochastic TAG (Resnik, 1992; Schabes, 1992). In this model a parse tree is built up not out of lexicalized phrase-structure rules but by tree fragments (called elementary trees) which are lexicalized in the sense that each fragment contains exactly one lexical item (its anchor). In the variant of TAG we use, there are three kinds of elementary tree: initial, (predicative) auxiliary, and modifier, and three composition operations: substitution, adjunction, and sister-adjunction. Figure 2 illustrates all three of these operations, al is an initial tree which substitutes at the left</context>
<context position="10874" citStr="Chiang, 2000" startWordPosition="1866" endWordPosition="1867">t Microsoft.&amp;quot; E Psa(7 I f) +Psa(STOP I 77, f) = 1 This is the probability of sister-adjoining -y between the ith and i + 1 th children of 77 (allowing for two imaginary children beyond the leftmost and rightmost children). Since multiple modifier trees can adjoin at the same location, P.,a(-y) is also conditioned on a flag f which indicates whether -y is the first modifier tree (i.e., the one closest to the head) to adjoin at that location. For our model we break down these probabilities further: first the elementary tree is generated without its anchor, and then its anchor is generated. See (Chiang, 2000) for more details. During training each example is broken into elementary trees using head rules and argument/adjunct rules similar to those of (Collins, 1997). The rules are interpreted as follows: a head is kept in the same elementary tree in its parent, an argument is broken off into a separate initial tree, leaving a substitution node, and an adjunct is broken off into a separate modifier tree. A different rule is used for extracting auxiliary trees; see (Chiang, 2000) for details. Xia (1999) describes a similar process, and in fact our rules for the Xinhua corpus are based on hers. 2.4 Mo</context>
<context position="12502" citStr="Chiang, 2000" startWordPosition="2144" endWordPosition="2145"> Aside from technicalities, this involved retraining the part-of-speech tagger described in (Ratnaparkhi, 1997), which was used for tagging unknown words. We also lowered the unknown word threshold from 4 to 2 because the Xinhua corpus was smaller than the WSJ corpus. • In addition to the change to the headfinding rules, we also changed the rules for classifying modifiers as arguments or adjuncts. In both cases the new rules were adapted from (Xia, 1999). • For the tests done in this paper, a beam width of 10-4 was used. The BBN model had the following additional modifications: • As with the (Chiang, 2000) model, we similarly lowered the unknown word threshold of the BBN model from its default 5 to 2. • The language-dependent word-feature was eliminated, causing parts of speech for unknown words to be predicted solely on the head relations in the model. • The default beam size in the probabilistic CKY parsing algorithm was widened. The default beam pruned away chart entries whose scores were not within a factor of e-5 of the top-ranked subtree; this 4 Model, test set &lt;40 words LR LP CB OCB &lt;2CB BBN-allt, WSJ-all 84.7 86.5 1.12 60.6 83.2 BBN-smallf, , WSJ-small* 79.0 80.7 1.66 47.0 74.6 BBN, Xin</context>
<context position="15730" citStr="Chiang, 2000" startWordPosition="2693" endWordPosition="2694">sting on WSJ-small, we trained on a subset of our English training data roughly equivalent in size to our Chinese training set (Sections 02 and 03 of the Penn Treebank); we have indicated models trained on all English training with &amp;quot;-all&amp;quot;, and models trained with the reduced English training set with &amp;quot;-small&amp;quot;. Therefore, by comparing the WSJ-small results with the Chinese results, one can reasonably gauge the performance gap between English parsing on the Penn Treebank and Chinese parsing on the Chinese Treebank. The reader will note that the modified BBN model does significantly poorer than (Chiang, 2000) on Chinese. While more investigation is required, we suspect part of the difference may be due to the fact that currently, the BBN model uses language-specific rules to guess part of speech tags for unknown words. 4 Conclusions and Future Work There is no question that a great deal of care and expertise went into creating the Chinese Treebank, and that it is a source of important grammatical information that is unique to the Chinese language. However, there are definite similarities between the grammars of English and Chinese, especially when viewed through the lens of the statistical models </context>
</contexts>
<marker>Chiang, 2000</marker>
<rawString>David Chiang. 2000. Statistical parsing with an automatically-extracted tree adjoining grammar. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Church</author>
</authors>
<title>A stochastic parts program and noun phrase parser for unrestricted text.</title>
<date>1988</date>
<booktitle>In Second Conference on Applied Natural Language Processing,</booktitle>
<pages>136--143</pages>
<location>Austin, Texas.</location>
<contexts>
<context position="772" citStr="Church, 1988" startWordPosition="117" endWordPosition="118">ania 200 South 33rd Street Philadelphia, PA 19104-6389 Idbikel,dchiangl@cis .upenn.edu Abstract This paper presents the first-ever results of applying statistical parsing models to the newly-available Chinese Treebank. We have employed two models, one extracted and adapted from BBN&apos;s SIFT System (Miller et al., 1998) and a TAGbased parsing model, adapted from (Chiang, 2000). On sentences with &lt;40 words, the former model performs at 69% precision, 75% recall, and the latter at 77% precision and 78% recall. 1 Introduction Ever since the success of HMMs&apos; application to part-of-speech tagging in (Church, 1988), machine learning approaches to natural language processing have steadily become more widespread. This increase has of course been due to their proven efficacy in many tasks, but also to their engineering efficacy. Many machine learning approaches let the data speak for itself (data ipsa loguuntur), as it were, allowing the modeler to focus on what features of the data are important, rather than on the complicated interaction of such features, as had often been the case with hand-crafted NLP systems. The success of statistical methods in particular has been quite evident in the area of syntac</context>
</contexts>
<marker>Church, 1988</marker>
<rawString>Kenneth Church. 1988. A stochastic parts program and noun phrase parser for unrestricted text. In Second Conference on Applied Natural Language Processing, pages 136-143, Austin, Texas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of ACL-EACL &apos;97,</booktitle>
<pages>16--23</pages>
<contexts>
<context position="2897" citStr="Collins, 1997" startWordPosition="454" endWordPosition="455"> which were originally designed for English parsing, on parsing Chinese, using the newly-available Chinese IYeebank. We will show that the languagedependent components of these parsers are quite compact, and that with little effort they can be adapted to produce promising results for Chinese parsing. We also discuss directions for future work. 2 Models and Modifications We will briefly describe the two parsing models employed (for a full description of the BBN model, see (Miller et al., 1998) and also (Bikel, 2000); for a full description of the TAG model, see (Chiang, 2000)). 2.1 Model 2 of (Collins, 1997) Both parsing models discussed in this paper inherit a great deal from this model, so we briefly describe its &amp;quot;progenitive&amp;quot; features here, describing only how each of the two models of this paper differ in the subsequent two sections. The lexicalized PCFG that sits behind Model 2 of (Collins, 1997) has rules of the form P • L1HR1- • - R1R„ (1) 1 S (will-MD) NP(Apple--NNP) NNP Apple MD will VB buy VP (buy-VB) PRT(out-RP) NP (Microsoft -NNP) RP NNP out Microsoft Figure 1: A sample sentence with parse tree. where P, L, Ri and H are all lexicalized nonterminals, and P inherits its lexical head fro</context>
<context position="4644" citStr="Collins, 1997" startWordPosition="756" endWordPosition="757"> which the currentlygenerated nonterminal falls). Note that if the modifying nonterminals were generated completely independently, the model would be very impoverished, but in actuality, by including the distance and subcat frame features, the model captures a crucial bit of linguistic reality, viz., that words often have welldefined sets of complements and adjuncts, dispersed with some well-defined distribution in the right hand sides of a (context-free) rewriting system. 2.2 BBN Model 2.2.1 Overview The BBN model is also of the lexicalized PCFG variety. In the BBN model, as with Model 2 of (Collins, 1997), modifying nonterminals are generated conditioning both on the parent P and its head child H. Unlike Model 2 of (Collins, 1997), they are also generated conditioning on the previously generated modifying nonterminal, or and there is no subcat frame or distance feature. While the BBN model does not perform at the level of Model 2 of (Collins, 1997) on Wall Street Journal text, it is also less language-dependent, eschewing the distance metric (which relied on specific features of the English Treebank) in favor of the &amp;quot;bigrams on nonterminals&amp;quot; model. 2.2.2 Model Parameters This section briefly d</context>
<context position="11033" citStr="Collins, 1997" startWordPosition="1890" endWordPosition="1891"> imaginary children beyond the leftmost and rightmost children). Since multiple modifier trees can adjoin at the same location, P.,a(-y) is also conditioned on a flag f which indicates whether -y is the first modifier tree (i.e., the one closest to the head) to adjoin at that location. For our model we break down these probabilities further: first the elementary tree is generated without its anchor, and then its anchor is generated. See (Chiang, 2000) for more details. During training each example is broken into elementary trees using head rules and argument/adjunct rules similar to those of (Collins, 1997). The rules are interpreted as follows: a head is kept in the same elementary tree in its parent, an argument is broken off into a separate initial tree, leaving a substitution node, and an adjunct is broken off into a separate modifier tree. A different rule is used for extracting auxiliary trees; see (Chiang, 2000) for details. Xia (1999) describes a similar process, and in fact our rules for the Xinhua corpus are based on hers. 2.4 Modifications The primary language-dependent component that had to be changed in both models was the head table, used to determine heads when training. We modifi</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proceedings of ACL-EACL &apos;97, pages 16-23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative reranlcing for natural language parsing.</title>
<date>2000</date>
<booktitle>In International Conference on Machine Learning.</booktitle>
<note>(to appear).</note>
<contexts>
<context position="1467" citStr="Collins, 2000" startWordPosition="232" endWordPosition="233"> more widespread. This increase has of course been due to their proven efficacy in many tasks, but also to their engineering efficacy. Many machine learning approaches let the data speak for itself (data ipsa loguuntur), as it were, allowing the modeler to focus on what features of the data are important, rather than on the complicated interaction of such features, as had often been the case with hand-crafted NLP systems. The success of statistical methods in particular has been quite evident in the area of syntactic parsing, most recently with the outstanding results of (Charniak, 2000) and (Collins, 2000) on the now-standard English test set of the Penn rkeebank (Marcus et al., 1993). A significant trend in parsing models has been the incorporation of linguistically-motivated features; however, it is important to note that &amp;quot;linguistically-motivated&amp;quot; does not necessarily mean language-dependent&amp;quot;--often, it means just the opposite. For example, almost all statistical parsers make use of lexicalized nonterminals in some way, which allows lexical items&apos; indiosyncratic parsing preferences to be modeled, but the paring between head words and their parent nonterminals is determined almost entirely by</context>
</contexts>
<marker>Collins, 2000</marker>
<rawString>Michael Collins. 2000. Discriminative reranlcing for natural language parsing. In International Conference on Machine Learning. (to appear).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
<author>Yves Schabes</author>
</authors>
<title>Treeadjoining grammars.</title>
<date>1997</date>
<booktitle>Handbook of Formal Languages and Automata,</booktitle>
<volume>3</volume>
<pages>69--124</pages>
<editor>In A. Salomma and G. Rosenberg, editors,</editor>
<publisher>Springer-Verlag,</publisher>
<location>Heidelberg.</location>
<contexts>
<context position="9053" citStr="Joshi and Schabes, 1997" startWordPosition="1514" endWordPosition="1517">lt up not out of lexicalized phrase-structure rules but by tree fragments (called elementary trees) which are lexicalized in the sense that each fragment contains exactly one lexical item (its anchor). In the variant of TAG we use, there are three kinds of elementary tree: initial, (predicative) auxiliary, and modifier, and three composition operations: substitution, adjunction, and sister-adjunction. Figure 2 illustrates all three of these operations, al is an initial tree which substitutes at the leftmost node labeled NP,I.; 13 is an auxiliary tree which adjoins at the node labeled VP. See (Joshi and Schabes, 1997) for a more detailed explanation. Sister-adjunction is not a standard TAG operation, but borrowed from D-Tree Grammar (Rambow et al., 1995). In Figure 2 the modifier tree 7 is sister adjoined between the nodes labeled VB and NI4. Multiple modifier trees can adjoin at the same place, in the spirit of (Schabes and Shieber, 1994). In stochastic TAG, the probability of generating an elementary tree depends on the elementary tree itself and the elementary tree it attaches to. The parameters are as follows: E Pi(a) =1 EPs(a I n) E p6,03 I + Pa(NONE 77) = 1 where a ranges over initial trees, /3 over </context>
</contexts>
<marker>Joshi, Schabes, 1997</marker>
<rawString>Aravind K. Joshi and Yves Schabes. 1997. Treeadjoining grammars. In A. Salomma and G. Rosenberg, editors, Handbook of Formal Languages and Automata, volume 3, pages 69-124. Springer-Verlag, Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<pages>19--313</pages>
<contexts>
<context position="1547" citStr="Marcus et al., 1993" startWordPosition="244" endWordPosition="247">icacy in many tasks, but also to their engineering efficacy. Many machine learning approaches let the data speak for itself (data ipsa loguuntur), as it were, allowing the modeler to focus on what features of the data are important, rather than on the complicated interaction of such features, as had often been the case with hand-crafted NLP systems. The success of statistical methods in particular has been quite evident in the area of syntactic parsing, most recently with the outstanding results of (Charniak, 2000) and (Collins, 2000) on the now-standard English test set of the Penn rkeebank (Marcus et al., 1993). A significant trend in parsing models has been the incorporation of linguistically-motivated features; however, it is important to note that &amp;quot;linguistically-motivated&amp;quot; does not necessarily mean language-dependent&amp;quot;--often, it means just the opposite. For example, almost all statistical parsers make use of lexicalized nonterminals in some way, which allows lexical items&apos; indiosyncratic parsing preferences to be modeled, but the paring between head words and their parent nonterminals is determined almost entirely by the training data, thereby making this feature—which models preferences of part</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19:313-330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Miller</author>
<author>Heidi Fox</author>
<author>Lance Ramshaw</author>
<author>Ralph Weischedel</author>
</authors>
<title>SIFT - Statisticallyderived Information From Text.</title>
<date>1998</date>
<booktitle>In Seventh Message Understanding Conference (MUC-7),</booktitle>
<location>Washington, D.C.</location>
<contexts>
<context position="2780" citStr="Miller et al., 1998" startWordPosition="431" endWordPosition="434">of a particular language—almost entirely languageindependent. In this paper, we will explore the use of two parsing models, which were originally designed for English parsing, on parsing Chinese, using the newly-available Chinese IYeebank. We will show that the languagedependent components of these parsers are quite compact, and that with little effort they can be adapted to produce promising results for Chinese parsing. We also discuss directions for future work. 2 Models and Modifications We will briefly describe the two parsing models employed (for a full description of the BBN model, see (Miller et al., 1998) and also (Bikel, 2000); for a full description of the TAG model, see (Chiang, 2000)). 2.1 Model 2 of (Collins, 1997) Both parsing models discussed in this paper inherit a great deal from this model, so we briefly describe its &amp;quot;progenitive&amp;quot; features here, describing only how each of the two models of this paper differ in the subsequent two sections. The lexicalized PCFG that sits behind Model 2 of (Collins, 1997) has rules of the form P • L1HR1- • - R1R„ (1) 1 S (will-MD) NP(Apple--NNP) NNP Apple MD will VB buy VP (buy-VB) PRT(out-RP) NP (Microsoft -NNP) RP NNP out Microsoft Figure 1: A sample</context>
<context position="5589" citStr="Miller et al., 1998" startWordPosition="916" endWordPosition="919">odel 2 of (Collins, 1997) on Wall Street Journal text, it is also less language-dependent, eschewing the distance metric (which relied on specific features of the English Treebank) in favor of the &amp;quot;bigrams on nonterminals&amp;quot; model. 2.2.2 Model Parameters This section briefly describes the top-level parameters used in the BBN parsing model. We use p to denote the unlexicalized nonterminal corresponding to P in (1), and similarly for i, ri and h. We now present the toplevel generation probabilities, along with examples from Figure 1. For brevity, we omit the smoothing details of BBN&apos;s model (see (Miller et al., 1998) for a complete description); we note that all smoothing weights are computed via the technique described in (Bikel et al., 1997). The probability of generating p as the root label is predicted conditioning on only +TOP+, which is the hidden root of all parse trees: P (pI + TOP+) , e.g., P(S I + TOP+). (2) The probability of generating a head node h with a parent p is P(hIp), e.g., P(VP)S). (3) The probability of generating a left-modifier ii is PL(ii h, wh), 7 e.g.) (4) PL, (NP I + BEGIN+, S, VP, will) 2 when generating the NP for NP(Apple—NNP), and the probability of generating a right modif</context>
</contexts>
<marker>Miller, Fox, Ramshaw, Weischedel, 1998</marker>
<rawString>Scott Miller, Heidi Fox, Lance Ramshaw, and Ralph Weischedel. 1998. SIFT - Statisticallyderived Information From Text. In Seventh Message Understanding Conference (MUC-7), Washington, D.C.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Owen Rainbow</author>
<author>K Vijay-Shanker</author>
<author>David Weir</author>
</authors>
<title>D-tree grammars.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the Assocation for Computational Linguistics,</booktitle>
<pages>151--158</pages>
<marker>Rainbow, Vijay-Shanker, Weir, 1995</marker>
<rawString>Owen Rainbow, K. Vijay-Shanker, and David Weir. 1995. D-tree grammars. In Proceedings of the 33rd Annual Meeting of the Assocation for Computational Linguistics, pages 151-158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A simple introduction to maximum entropy models for natural language processing.</title>
<date>1997</date>
<tech>Technical Report IRCS Report 97-08,</tech>
<institution>Institute for Research in Cognitive Science,</institution>
<contexts>
<context position="12000" citStr="Ratnaparkhi, 1997" startWordPosition="2053" endWordPosition="2054">bes a similar process, and in fact our rules for the Xinhua corpus are based on hers. 2.4 Modifications The primary language-dependent component that had to be changed in both models was the head table, used to determine heads when training. We modified the head rules described in (Xia, 1999) for the Xinhua corpus and substituted these new rules into both models. The (Chiang, 2000) model had the following additional modifications. • The new corpus had to be prepared for use with the trainer and parser. Aside from technicalities, this involved retraining the part-of-speech tagger described in (Ratnaparkhi, 1997), which was used for tagging unknown words. We also lowered the unknown word threshold from 4 to 2 because the Xinhua corpus was smaller than the WSJ corpus. • In addition to the change to the headfinding rules, we also changed the rules for classifying modifiers as arguments or adjuncts. In both cases the new rules were adapted from (Xia, 1999). • For the tests done in this paper, a beam width of 10-4 was used. The BBN model had the following additional modifications: • As with the (Chiang, 2000) model, we similarly lowered the unknown word threshold of the BBN model from its default 5 to 2. </context>
</contexts>
<marker>Ratnaparkhi, 1997</marker>
<rawString>Adwait Ratnaparkhi. 1997. A simple introduction to maximum entropy models for natural language processing. Technical Report IRCS Report 97-08, Institute for Research in Cognitive Science, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Probabilistic tree-adjoining grammar as a framework for statistical natural language processing.</title>
<date>1992</date>
<booktitle>In Proceedings of COLING-92,</booktitle>
<pages>418--424</pages>
<contexts>
<context position="8378" citStr="Resnik, 1992" startWordPosition="1408" endWordPosition="1409">the probabilities of all possible nonterminal sequences sum to 1. 2This is the one place where we altered the original model, as the lexical components of the head of the entire sentence were all being estimated incorrectly, causing an inconsistency in the model. We corrected the estimation of th and wh in our implementation. where an element is either a constituent label, a part of speech tag or a word. We obtain maximum-likelihood estimates of the parameters of this model using frequencies gathered from the training data. 2.3 TAG Model The model of (Chiang, 2000) is based on stochastic TAG (Resnik, 1992; Schabes, 1992). In this model a parse tree is built up not out of lexicalized phrase-structure rules but by tree fragments (called elementary trees) which are lexicalized in the sense that each fragment contains exactly one lexical item (its anchor). In the variant of TAG we use, there are three kinds of elementary tree: initial, (predicative) auxiliary, and modifier, and three composition operations: substitution, adjunction, and sister-adjunction. Figure 2 illustrates all three of these operations, al is an initial tree which substitutes at the leftmost node labeled NP,I.; 13 is an auxilia</context>
</contexts>
<marker>Resnik, 1992</marker>
<rawString>Philip Resnik. 1992. Probabilistic tree-adjoining grammar as a framework for statistical natural language processing. In Proceedings of COLING-92, pages 418-424.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Schabes</author>
<author>Stuart M Shieber</author>
</authors>
<title>An alternative conception of tree-adjoining derivation.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<pages>20--1</pages>
<contexts>
<context position="9381" citStr="Schabes and Shieber, 1994" startWordPosition="1572" endWordPosition="1575">three composition operations: substitution, adjunction, and sister-adjunction. Figure 2 illustrates all three of these operations, al is an initial tree which substitutes at the leftmost node labeled NP,I.; 13 is an auxiliary tree which adjoins at the node labeled VP. See (Joshi and Schabes, 1997) for a more detailed explanation. Sister-adjunction is not a standard TAG operation, but borrowed from D-Tree Grammar (Rambow et al., 1995). In Figure 2 the modifier tree 7 is sister adjoined between the nodes labeled VB and NI4. Multiple modifier trees can adjoin at the same place, in the spirit of (Schabes and Shieber, 1994). In stochastic TAG, the probability of generating an elementary tree depends on the elementary tree itself and the elementary tree it attaches to. The parameters are as follows: E Pi(a) =1 EPs(a I n) E p6,03 I + Pa(NONE 77) = 1 where a ranges over initial trees, /3 over auxiliary trees, 7 over modifier trees, and n over nodes. Pi(a) is the probability of beginning a derivation with a; Ps(a j ) is the probability of substituting a at 71; Pa(O I n) is the probability of adjoining 13 at 77; finally, Pa(NONE j 7) is the probability of nothing adjoining at n. Our variant adds another set of parame</context>
</contexts>
<marker>Schabes, Shieber, 1994</marker>
<rawString>Yves Schabes and Stuart M. Shieber. 1994. An alternative conception of tree-adjoining derivation. Computational Linguistics, 20(1):91-124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Schabes</author>
</authors>
<title>Stochastic lexicalized tree-adjoining grammars.</title>
<date>1992</date>
<booktitle>In Proceedings of COLING-92,</booktitle>
<pages>426--432</pages>
<contexts>
<context position="8394" citStr="Schabes, 1992" startWordPosition="1410" endWordPosition="1411">ies of all possible nonterminal sequences sum to 1. 2This is the one place where we altered the original model, as the lexical components of the head of the entire sentence were all being estimated incorrectly, causing an inconsistency in the model. We corrected the estimation of th and wh in our implementation. where an element is either a constituent label, a part of speech tag or a word. We obtain maximum-likelihood estimates of the parameters of this model using frequencies gathered from the training data. 2.3 TAG Model The model of (Chiang, 2000) is based on stochastic TAG (Resnik, 1992; Schabes, 1992). In this model a parse tree is built up not out of lexicalized phrase-structure rules but by tree fragments (called elementary trees) which are lexicalized in the sense that each fragment contains exactly one lexical item (its anchor). In the variant of TAG we use, there are three kinds of elementary tree: initial, (predicative) auxiliary, and modifier, and three composition operations: substitution, adjunction, and sister-adjunction. Figure 2 illustrates all three of these operations, al is an initial tree which substitutes at the leftmost node labeled NP,I.; 13 is an auxiliary tree which ad</context>
</contexts>
<marker>Schabes, 1992</marker>
<rawString>Yves Schabes. 1992. Stochastic lexicalized tree-adjoining grammars. In Proceedings of COLING-92, pages 426-432.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fel Xia</author>
</authors>
<title>Extracting tree adjoining grammars from bracketed corpora.</title>
<date>1999</date>
<booktitle>In Proceedings of the 5th Natural Language Processing Pacific Rim Symposium (NLPRS-99).</booktitle>
<contexts>
<context position="11375" citStr="Xia (1999)" startWordPosition="1952" endWordPosition="1953">rst the elementary tree is generated without its anchor, and then its anchor is generated. See (Chiang, 2000) for more details. During training each example is broken into elementary trees using head rules and argument/adjunct rules similar to those of (Collins, 1997). The rules are interpreted as follows: a head is kept in the same elementary tree in its parent, an argument is broken off into a separate initial tree, leaving a substitution node, and an adjunct is broken off into a separate modifier tree. A different rule is used for extracting auxiliary trees; see (Chiang, 2000) for details. Xia (1999) describes a similar process, and in fact our rules for the Xinhua corpus are based on hers. 2.4 Modifications The primary language-dependent component that had to be changed in both models was the head table, used to determine heads when training. We modified the head rules described in (Xia, 1999) for the Xinhua corpus and substituted these new rules into both models. The (Chiang, 2000) model had the following additional modifications. • The new corpus had to be prepared for use with the trainer and parser. Aside from technicalities, this involved retraining the part-of-speech tagger describ</context>
</contexts>
<marker>Xia, 1999</marker>
<rawString>Fel Xia. 1999. Extracting tree adjoining grammars from bracketed corpora. In Proceedings of the 5th Natural Language Processing Pacific Rim Symposium (NLPRS-99).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>