<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.074627">
<title confidence="0.982143">
Duluth: Word Sense Discrimination in the Service of Lexicography
</title>
<author confidence="0.998896">
Ted Pedersen
</author>
<affiliation confidence="0.849550666666667">
Department of Computer Science
University of Minnesota
Duluth, MN, 55812, USA
</affiliation>
<email confidence="0.998706">
tpederse@d.umn.edu
</email>
<sectionHeader confidence="0.995641" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999967285714286">
This paper describes the Duluth systems that
participated in Task 15 of SemEval 2015. The
goal of the task was to automatically construct
dictionary entries (via a series of three sub-
tasks). Our systems participated in subtask
2, which involved automatically clustering the
contexts in which a target word occurs into
its different senses. Our results are consis-
tent with previous word sense induction and
discrimination findings, where it proves diffi-
cult to beat a baseline algorithm that assigns
all instances of a target word to a single sense.
However, our method of predicting the num-
ber of senses automatically fared quite well.
</bodyText>
<sectionHeader confidence="0.998975" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99932375">
A Corpus Pattern Analysis (CPA) dictionary en-
try building task (SemEval 2015 Task 15) included
three subtasks, the combination of which creates a
dictionary entry based on CPA (Hanks, 2013). The
Duluth systems participated in the second subtask,
which sought to cluster the contexts in which tar-
get words occur based on their underlying sense or
meaning. Note that for this task all of the target
words are verbs. This is unusual for a word sense
shared task, since nouns are much more commonly
studied.
The task input includes two sets of words : the
Microcheck includes 8 target verbs, where the num-
ber of senses for each are given to task participants,
while the Wingspread includes 20 target verbs where
the number of senses are withheld. Both sets of tar-
get verbs and their frequencies are shown in Tables
3.2 and 3.2.
The CPA method is based on finding patterns of
use in corpora, and definitions of word senses re-
fer explicitly to these patterns. For example, the
verb totter has three senses, where a person (sense
1), building (sense 2), or institution (sense 3) may
be what totters. The verb undertake has two senses,
where a person or institution embarks on an activity
(sense 1) or promises to do so (sense 2).
There is certainly a role for syntactic information
in defining such senses – direct and indirect objects
are clearly important, and chunking would in gen-
eral be quite useful. It also seems that incorporating
semantic features, for example, those based on se-
lectional restrictions or constraints, might be fruit-
ful. In fact, subtask 1 focuses on shallow parsing
and is said to be similar to semantic role labeling.
Given different syntactic and semantic features dis-
covered in subtask 1, it would be possible to pursue
subtask 2 using a more rule based approach.
However, the Duluth systems do not explicitly ac-
count for syntax or semantics and do not try to iden-
tify these kinds of patterns. While we believe such
approaches are extremely useful, we are primarily
interested in exploring the limits of methods that de-
pend on purely lexical features.
As a result, the Duluth systems rely on clustering
target verbs based on the context in which they oc-
cur (e.g., (Sch¨utze, 1998), (Purandare and Pedersen,
2004), (Pedersen, 2007)). This follows from the dis-
tributional hypothesis (Harris, 1954). Simply put,
words that are used in similar contexts may often
have similar meanings. However, words with dif-
ferent meanings can also be used in similar contexts
(e.g., antonyms) so results are often noisy.
</bodyText>
<page confidence="0.980386">
438
</page>
<bodyText confidence="0.831118285714286">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 438–442,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
The Duluth systems take a knowledge-lean ap-
proach (Pedersen, 1997), and treat this task as an un-
supervised word sense discrimination or induction
problem, and use the freely available open-source
software package SenseClusters1.
</bodyText>
<sectionHeader confidence="0.976766" genericHeader="introduction">
2 Systems
</sectionHeader>
<bodyText confidence="0.999983735294118">
We submitted three runs for subtask 2 : run1, run2,
and run3. These three systems share a few basic
characteristics, but differ in important respects. All
use SenseClusters, and all utilize the same relatively
simple pre–processing. Text was converted to lower
case, and numeric values were all converted to a sin-
gle string. Also, all three runs automatically deter-
mined the number of clusters (senses) using the PK2
measure (Pedersen and Kulkarni, 2006). This mea-
sure looks at the degree of change in the clustering
criterion function, and stops the clustering process
when the criterion function begins to plateau. This
indicates that additional clustering of the data is not
improving the quality of the clusters, and that further
divisions will break apart relatively homogeneous
senses.
There are however important differences between
the systems. Runs run1 and run2 rely on second–
order co–occurrences, run1 uses words that co–
occur near the target verb as features, and run2 uses
words that occur anywhere in the contexts to be clus-
tered. Both run1 and run2 represent these features
using second–order co–occurrences, where run1 de-
rives these from the contexts to be clustered, and
run2 uses the WordNet 3.0 glosses2 as a 1.46 mil-
lion word corpus for building these features. run3
use first–order unigrams found in the contexts to be
clustered as features.
While the Microcheck data provided the number
of senses, the Duluth systems elected not to use this.
We felt that in most realistic use cases the number
of senses is not known, and we were curious to see
how well our systems could perform at identifying
the number of senses automatically.
</bodyText>
<subsectionHeader confidence="0.996949">
2.1 First and Second–Order Co–Occurrences
</subsectionHeader>
<bodyText confidence="0.9996635">
A first–order representation simply looks for fea-
tures that directly occur in the contexts to be clus-
</bodyText>
<footnote confidence="0.960815666666667">
1http://senseclusters.sourceforge.net
2http://www.d.umn.edu/˜tpederse/Code/glossExtract-
v0.03.tar.gz
</footnote>
<bodyText confidence="0.999911255319149">
tered and uses their occurrence (or not) as the ba-
sis for making clustering decisions. First–order un-
igrams depend on having multiple occurrences of
the same words in various different contexts, and as
such often do not perform well with smaller num-
bers of contexts. Among our systems, run3 is the
only to take a first order unigram approach.
A second–order representation takes a somewhat
fuzzier approach, and allows for a more flexible sort
of feature matching. Rather than looking for the
same features in multiple contexts, this represen-
tation seeks features that co–occur with the same
words in different contexts. This can be thought of
as a kind of a friend of a friend approach to feature
matching.
For example, suppose that car and auto occur in
two different contexts. They do not match (as first—
order
rstorder features) but if both are known to occur with
repairs then that second–order co–occurrence can
be the basis for considering them as matching fea-
tures that could then be used to cluster the contexts
in which car and auto occur in together.
This is operationalized by replacing words in the
context to be clustered with a co–occurrence vector.
For run1, the only word that is replaced is the tar-
get verb, which is instead represented by a vector of
words that occur within 8 positions of that target in
that particular context.
For run2, all the words in the contexts to be clus-
tered that are used in a WordNet gloss (version 3.0)
are replaced by a vector representing all the words in
WordNet glosses that immediately follow that word
in a definition.
As a simple example, imagine a gloss corpus with
two definitions : a vehicle powered by an internal
combustion engine and a medication used to speed
up the internal clock. If the word internal occurs in a
context, it would be replaced by a vector consisting
of combustion and clock.
Then, all the vectors associated with the words in
a context are averaged together (although in the case
of run1 this might just be a single vector). Each con-
text is represented now by its averaged vector, and
the closeness or distance of contexts to or from each
other is based on the number of second–order fea-
ture matches.
</bodyText>
<page confidence="0.993349">
439
</page>
<table confidence="0.9997564">
Microcheck Wingspread
run1 0.525 0.604
run2 0.440 0.581
run3 0.439 0.615
baseline 0.588 0.720
</table>
<tableCaption confidence="0.99979">
Table 1: B–Cubed F–Scores.
</tableCaption>
<subsectionHeader confidence="0.99863">
2.2 Lexical Feature Selection
</subsectionHeader>
<bodyText confidence="0.999780875">
run1 finds what are known in SenseClusters as target
co–occurrences (tco) in the contexts to be clustered,
and run2 finds bigrams in the WordNet 3.0 gloss
corpus. While there are many methods for identi-
fying statistically significant or associated pairs of
words in corpora, the number of contexts in the
Wingspread data is relatively small – 12 of 20 tar-
get verbs have fewer than 40 contexts, so we sim-
ply relied on frequency counts when selecting fea-
tures. Given this, run1 used a long distance defini-
tion of co–occurrence to help overcome the smaller
numbers of contexts, and so any word that occurs
anywhere within 8 positions of the target word 2 or
more times is considered a target co–occurrence. In
run2 any bigram that occurred 5 or more times in the
WordNet 3.0 gloss corpus was used as a feature. In
run3 any unigram that occurred 2 or more times in
the contexts to be clustered was used as a feature.
We used the nearly 400 word stoplist from the
Ngram Statistics Package3 (Banerjee and Pedersen,
2003) for all three of our runs. Any bigram or co–
occurrence where both words are stop words was not
used as a feature, and any unigram in the stoplist was
likewise discarded.
</bodyText>
<sectionHeader confidence="0.999539" genericHeader="related work">
3 Results and Analysis
</sectionHeader>
<bodyText confidence="0.9996915">
Official results from task 15 are based on the B–
cubed F–score (Bagga and Baldwin, 1998). In addi-
tion to reporting those values, we also carried out our
own analysis using the SenseClusters F–measure.
</bodyText>
<subsectionHeader confidence="0.999337">
3.1 B–cubed F–score
</subsectionHeader>
<bodyText confidence="0.971524">
Table 3.1 shows the B–Cubed F–scores as reported
by the task organizers. Note that the baseline system
assigns all contexts to a single cluster or sense.
Prior to the evaluation we designated run1 as our
official submission, since we felt that this system
</bodyText>
<footnote confidence="0.619869">
3http://ngram.sourceforge.net
</footnote>
<bodyText confidence="0.999949285714286">
was likely to be most successful with this task. This
was based on our pre–evaluation tuning with the
training data which had been made available by the
task organizers. This prediction was largely con-
firmed – run1 was easily our most accurate system
with the Microcheck data, and was only narrowly
exceeded by run3 for the Wingspread data.
There were several hundred contexts available for
each target verb in the Microcheck data. This is large
enough to generate a rich second–order representa-
tion of context. Given that we focused on somewhat
localized target co–occurrences in run1, the number
of spurious features will be somewhat less than if
we had looked more generally at features that occur
anywhere in a context (as is the case with run2 and
run3). This is why we believe that run1 had a fairly
significant advantage in the Microcheck data.
However, in the Wingspread data run3 slightly
outperformed run1, although not to a significant
degree. We believe this occurred because the
Wingspread data has a majority of target verbs with
less than 40 contexts. This small amount of data will
result in very sparse second–order co–occurrences.
Given that run1 seeks target co–occurrences, when
these are very sparse they essentially reduce to first—
order
rstorder co–occurrences, leading to very similar perfor-
mance between run1 and run3.
</bodyText>
<subsectionHeader confidence="0.998964">
3.2 SenseClusters F–Measure
</subsectionHeader>
<bodyText confidence="0.999967631578948">
Tables 3.2 and 3.2 provide results for run1 using
the SenseClusters F-Measure (F) (Pedersen, 2007).
This measure first assigns the discovered clusters to
gold standard senses in whatever way optimizes the
agreement between them using the (Munkres, 1957)
algorithm. Then any senses or clusters that are not
aligned are discarded, and precision and recall are
computed in the usual way. In these experiments all
contexts are assigned to clusters, so recall and preci-
sion are the same, and the F-measure can be viewed
as accuracy. In this case the F-measure is the per-
centage of contexts that were assigned to the correct
cluster.
These tables also show the most frequent sense
baseline (M). This is the percentage of contexts that
belong to the most frequent sense. This is a well
known baseline in supervised approaches to word
sense disambiguation, and also proves to be the
same for unsupervised approaches. Given the defini-
</bodyText>
<page confidence="0.993549">
440
</page>
<table confidence="0.9996667">
N C D M F
appreciate 215 2 2 .744 .693
apprehend 123 3 5 .626 .435
continue 203 7 4 .350 .291
crush 170 5 5 .365 .324
decline 201 3 4 .672 .439
operate 140 8 4 .286 .250
undertake 228 2 2 .895 .750
total (w) 4.1 3.5 .585 .478
total 1,280 4.3 3.7 .562 .455
</table>
<tableCaption confidence="0.99497075">
Table 2: Microcheck run1, N is number of instances, C
is number of actual clusters, D is number of discovered
clusters, M is majority sense baseline, F is SenseClusters
F-Measure, total (w) are weighted averages.
</tableCaption>
<bodyText confidence="0.999959166666667">
tion of the SenseClusters F-Measure, if all contexts
are assigned to a single cluster, then the F-Measure
will be equal to the most frequent sense percentage.
As can be seen in Tables 2 and 3, in general this
baseline outperformed the Duluth systems for nearly
every target verb.
We were pleased that in general the PK2 method
of identifying the number of clusters was reasonably
successful. While it did not always predict exactly
the same number of clusters as found in the gold
standard data, in general there were no cases where
it differed radically. On average the Microcheck data
had 4.3 senses, while run1 discovered 3.7. For the
Wingspread data there were 3.0 senses, while run1
discovered 2.7. While the results show that the clus-
ters themselves are noisy, in general we are pleased
that our ability to predict the number of clusters is
reasonably accurate.
</bodyText>
<sectionHeader confidence="0.997118" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.998295636363636">
SenseClusters has participated in numerous SensE-
val and SemEval shared tasks that have included
word sense discrimination and induction (Pedersen,
2007; Pedersen, 2010; Pedersen, 2013). In all of
these prior events, the most frequent sense baseline
has proven hard to beat. In general assigning all in-
stances of a target verb to a single cluster replicates
most frequent sense performance. The results in this
subtask are similar, and suggest that for the moment,
automatic word sense discrimination is still not a vi-
able replacement for human lexicographic expertise.
</bodyText>
<table confidence="0.999618086956522">
N C D M F
adapt 182 4 1 .539 .539
advise 230 8 2 .365 .365
afflict 179 2 2 .961 .687
ascertain 7 2 1 .571 .571
ask 573 9 2 .522 .470
attain 240 3 4 .833 .627
avert 240 2 7 .958 .374
avoid 242 3 2 .727 .566
begrudge 19 2 4 .579 .581
belch 24 3 4 .583 .468
bludgeon 32 2 2 .500 .500
bluff 25 2 2 .560 .520
boo 36 2 2 .750 .640
brag 29 2 2 .621 .586
breeze 12 2 1 .583 .583
sue 247 2 2 .980 .846
teeter 28 2 2 .821 .750
tense 37 3 2 .622 .432
totter 19 2 5 .632 .533
wing 22 2 4 .474 .864
total (w) 4.6 2.7 .694 .548
total 2,421 3.0 2.7 .659 .575
</table>
<tableCaption confidence="0.96942275">
Table 3: Wingspread run1, N is number of instances, C
is number of actual clusters, D is number of discovered
clusters, M is majority sense baseline, F is SenseClusters
F-Measure, total (w) are weighted averages.
</tableCaption>
<bodyText confidence="0.999953">
However, we are encouraged by the accurate re-
sults from the PK2 method in identifying the num-
ber of senses automatically. If the discovered clus-
ters themselves can be made less noisy (through im-
proved feature selection), our overall results could
improve significantly since we are already able to
identify the number of distinct senses accurately. We
believe that the incorporation of more grammatical
and semantic features will certainly help improve
the quality of the clustering, and so plan to pursue
that in future work.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999874333333333">
I would like to thank Bridget McInnes for her help
in understanding the task, and for very useful brain-
storming discussions.
</bodyText>
<page confidence="0.998399">
441
</page>
<sectionHeader confidence="0.989976" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998958392156863">
Amit Bagga and Breck Baldwin. 1998. Entity–based
cross–document co–referencing using the vector space
model. In Proceedings of the 17th International Con-
ference on Computational Linguistics, pages 79–85.
Satanjeev Banerjee and Ted Pedersen. 2003. Extended
gloss overlaps as a measure of semantic relatedness.
In Proceedings of the Eighteenth International Joint
Conference on Artificial Intelligence, pages 805–810,
Acapulco, August.
Patrick Hanks. 2013. Lexical Analysis : Norms and Ex-
ploitations. The MIT Press, Cambridge, MA.
Zellig Harris. 1954. Distributional structure. Word,
10(23):146–162.
James Munkres. 1957. Algorithms for the assignment
and transportation problems. Journal of the Society
of Industrial and Applied Mathematics, 5(1):32–38,
March.
Ted Pedersen and Anagha Kulkarni. 2006. Selecting the
right number of senses based on clustering criterion
functions. In Proceedings of the Posters and Demo
Program of the Eleventh Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 111–114, Trento, Italy, April.
Ted Pedersen. 1997. Knowledge lean word sense disam-
biguation. In Proceedings of the Fourteenth National
Conference on Artificial Intelligence, page 814, Provi-
dence, RI, July.
Ted Pedersen. 2007. UMND2 : SenseClusters applied
to the sense induction task of Senseval-4. In Proceed-
ings of the Fourth International Workshop on Semantic
Evaluations (SemEval-2007), pages 394–397, Prague,
Czech Republic, June.
Ted Pedersen. 2010. Duluth-WSI: SenseClusters applied
to the sense induction task of semEval-2. In Proceed-
ings of the SemEval 2010 Workshop : the 5th Interna-
tional Workshop on Semantic Evaluations, pages 363–
366, Uppsala, July.
Ted Pedersen. 2013. Duluth : Word sense induc-
tion applied to web page clustering. In Second Joint
Conference on Lexical and Computational Semantics
(*SEM), Volume 2: Proceedings of the Seventh Inter-
national Workshop on Semantic Evaluation (SemEval
2013), pages 202–206, Atlanta, Georgia, USA, June.
Association for Computational Linguistics.
Amruta Purandare and Ted Pedersen. 2004. Word sense
discrimination by clustering contexts in vector and
similarity spaces. In Proceedings of the Conference
on Computational Natural Language Learning, pages
41–48, Boston, MA.
Hinrich Sch¨utze. 1998. Automatic word sense discrimi-
nation. Computational Linguistics, 24(1):97–123.
</reference>
<page confidence="0.998456">
442
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.934493">
<title confidence="0.996374">Duluth: Word Sense Discrimination in the Service of Lexicography</title>
<author confidence="0.993324">Ted</author>
<affiliation confidence="0.9998575">Department of Computer University of</affiliation>
<address confidence="0.970402">Duluth, MN, 55812,</address>
<email confidence="0.999816">tpederse@d.umn.edu</email>
<abstract confidence="0.997725933333333">This paper describes the Duluth systems that participated in Task 15 of SemEval 2015. The goal of the task was to automatically construct dictionary entries (via a series of three subtasks). Our systems participated in subtask 2, which involved automatically clustering the contexts in which a target word occurs into its different senses. Our results are consistent with previous word sense induction and discrimination findings, where it proves difficult to beat a baseline algorithm that assigns all instances of a target word to a single sense. However, our method of predicting the number of senses automatically fared quite well.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amit Bagga</author>
<author>Breck Baldwin</author>
</authors>
<title>Entity–based cross–document co–referencing using the vector space model.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th International Conference on Computational Linguistics,</booktitle>
<pages>79--85</pages>
<contexts>
<context position="9322" citStr="Bagga and Baldwin, 1998" startWordPosition="1543" endWordPosition="1546">dered a target co–occurrence. In run2 any bigram that occurred 5 or more times in the WordNet 3.0 gloss corpus was used as a feature. In run3 any unigram that occurred 2 or more times in the contexts to be clustered was used as a feature. We used the nearly 400 word stoplist from the Ngram Statistics Package3 (Banerjee and Pedersen, 2003) for all three of our runs. Any bigram or co– occurrence where both words are stop words was not used as a feature, and any unigram in the stoplist was likewise discarded. 3 Results and Analysis Official results from task 15 are based on the B– cubed F–score (Bagga and Baldwin, 1998). In addition to reporting those values, we also carried out our own analysis using the SenseClusters F–measure. 3.1 B–cubed F–score Table 3.1 shows the B–Cubed F–scores as reported by the task organizers. Note that the baseline system assigns all contexts to a single cluster or sense. Prior to the evaluation we designated run1 as our official submission, since we felt that this system 3http://ngram.sourceforge.net was likely to be most successful with this task. This was based on our pre–evaluation tuning with the training data which had been made available by the task organizers. This predic</context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>Amit Bagga and Breck Baldwin. 1998. Entity–based cross–document co–referencing using the vector space model. In Proceedings of the 17th International Conference on Computational Linguistics, pages 79–85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Ted Pedersen</author>
</authors>
<title>Extended gloss overlaps as a measure of semantic relatedness.</title>
<date>2003</date>
<booktitle>In Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence,</booktitle>
<pages>805--810</pages>
<location>Acapulco,</location>
<contexts>
<context position="9038" citStr="Banerjee and Pedersen, 2003" startWordPosition="1491" endWordPosition="1494">ntexts, so we simply relied on frequency counts when selecting features. Given this, run1 used a long distance definition of co–occurrence to help overcome the smaller numbers of contexts, and so any word that occurs anywhere within 8 positions of the target word 2 or more times is considered a target co–occurrence. In run2 any bigram that occurred 5 or more times in the WordNet 3.0 gloss corpus was used as a feature. In run3 any unigram that occurred 2 or more times in the contexts to be clustered was used as a feature. We used the nearly 400 word stoplist from the Ngram Statistics Package3 (Banerjee and Pedersen, 2003) for all three of our runs. Any bigram or co– occurrence where both words are stop words was not used as a feature, and any unigram in the stoplist was likewise discarded. 3 Results and Analysis Official results from task 15 are based on the B– cubed F–score (Bagga and Baldwin, 1998). In addition to reporting those values, we also carried out our own analysis using the SenseClusters F–measure. 3.1 B–cubed F–score Table 3.1 shows the B–Cubed F–scores as reported by the task organizers. Note that the baseline system assigns all contexts to a single cluster or sense. Prior to the evaluation we de</context>
</contexts>
<marker>Banerjee, Pedersen, 2003</marker>
<rawString>Satanjeev Banerjee and Ted Pedersen. 2003. Extended gloss overlaps as a measure of semantic relatedness. In Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence, pages 805–810, Acapulco, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Hanks</author>
</authors>
<title>Lexical Analysis : Norms and Exploitations.</title>
<date>2013</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="1024" citStr="Hanks, 2013" startWordPosition="157" endWordPosition="158">, which involved automatically clustering the contexts in which a target word occurs into its different senses. Our results are consistent with previous word sense induction and discrimination findings, where it proves difficult to beat a baseline algorithm that assigns all instances of a target word to a single sense. However, our method of predicting the number of senses automatically fared quite well. 1 Introduction A Corpus Pattern Analysis (CPA) dictionary entry building task (SemEval 2015 Task 15) included three subtasks, the combination of which creates a dictionary entry based on CPA (Hanks, 2013). The Duluth systems participated in the second subtask, which sought to cluster the contexts in which target words occur based on their underlying sense or meaning. Note that for this task all of the target words are verbs. This is unusual for a word sense shared task, since nouns are much more commonly studied. The task input includes two sets of words : the Microcheck includes 8 target verbs, where the number of senses for each are given to task participants, while the Wingspread includes 20 target verbs where the number of senses are withheld. Both sets of target verbs and their frequencie</context>
</contexts>
<marker>Hanks, 2013</marker>
<rawString>Patrick Hanks. 2013. Lexical Analysis : Norms and Exploitations. The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig Harris</author>
</authors>
<date>1954</date>
<journal>Distributional structure. Word,</journal>
<volume>10</volume>
<issue>23</issue>
<contexts>
<context position="3152" citStr="Harris, 1954" startWordPosition="521" endWordPosition="522">ask 1, it would be possible to pursue subtask 2 using a more rule based approach. However, the Duluth systems do not explicitly account for syntax or semantics and do not try to identify these kinds of patterns. While we believe such approaches are extremely useful, we are primarily interested in exploring the limits of methods that depend on purely lexical features. As a result, the Duluth systems rely on clustering target verbs based on the context in which they occur (e.g., (Sch¨utze, 1998), (Purandare and Pedersen, 2004), (Pedersen, 2007)). This follows from the distributional hypothesis (Harris, 1954). Simply put, words that are used in similar contexts may often have similar meanings. However, words with different meanings can also be used in similar contexts (e.g., antonyms) so results are often noisy. 438 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 438–442, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics The Duluth systems take a knowledge-lean approach (Pedersen, 1997), and treat this task as an unsupervised word sense discrimination or induction problem, and use the freely available open-source software </context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Zellig Harris. 1954. Distributional structure. Word, 10(23):146–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Munkres</author>
</authors>
<title>Algorithms for the assignment and transportation problems.</title>
<date>1957</date>
<journal>Journal of the Society of Industrial and Applied Mathematics,</journal>
<volume>5</volume>
<issue>1</issue>
<contexts>
<context position="11367" citStr="Munkres, 1957" startWordPosition="1870" endWordPosition="1871">a majority of target verbs with less than 40 contexts. This small amount of data will result in very sparse second–order co–occurrences. Given that run1 seeks target co–occurrences, when these are very sparse they essentially reduce to first— order rstorder co–occurrences, leading to very similar performance between run1 and run3. 3.2 SenseClusters F–Measure Tables 3.2 and 3.2 provide results for run1 using the SenseClusters F-Measure (F) (Pedersen, 2007). This measure first assigns the discovered clusters to gold standard senses in whatever way optimizes the agreement between them using the (Munkres, 1957) algorithm. Then any senses or clusters that are not aligned are discarded, and precision and recall are computed in the usual way. In these experiments all contexts are assigned to clusters, so recall and precision are the same, and the F-measure can be viewed as accuracy. In this case the F-measure is the percentage of contexts that were assigned to the correct cluster. These tables also show the most frequent sense baseline (M). This is the percentage of contexts that belong to the most frequent sense. This is a well known baseline in supervised approaches to word sense disambiguation, and </context>
</contexts>
<marker>Munkres, 1957</marker>
<rawString>James Munkres. 1957. Algorithms for the assignment and transportation problems. Journal of the Society of Industrial and Applied Mathematics, 5(1):32–38, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Anagha Kulkarni</author>
</authors>
<title>Selecting the right number of senses based on clustering criterion functions.</title>
<date>2006</date>
<booktitle>In Proceedings of the Posters and Demo Program of the Eleventh Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>111--114</pages>
<location>Trento, Italy,</location>
<contexts>
<context position="4240" citStr="Pedersen and Kulkarni, 2006" startWordPosition="685" endWordPosition="688">), and treat this task as an unsupervised word sense discrimination or induction problem, and use the freely available open-source software package SenseClusters1. 2 Systems We submitted three runs for subtask 2 : run1, run2, and run3. These three systems share a few basic characteristics, but differ in important respects. All use SenseClusters, and all utilize the same relatively simple pre–processing. Text was converted to lower case, and numeric values were all converted to a single string. Also, all three runs automatically determined the number of clusters (senses) using the PK2 measure (Pedersen and Kulkarni, 2006). This measure looks at the degree of change in the clustering criterion function, and stops the clustering process when the criterion function begins to plateau. This indicates that additional clustering of the data is not improving the quality of the clusters, and that further divisions will break apart relatively homogeneous senses. There are however important differences between the systems. Runs run1 and run2 rely on second– order co–occurrences, run1 uses words that co– occur near the target verb as features, and run2 uses words that occur anywhere in the contexts to be clustered. Both r</context>
</contexts>
<marker>Pedersen, Kulkarni, 2006</marker>
<rawString>Ted Pedersen and Anagha Kulkarni. 2006. Selecting the right number of senses based on clustering criterion functions. In Proceedings of the Posters and Demo Program of the Eleventh Conference of the European Chapter of the Association for Computational Linguistics, pages 111–114, Trento, Italy, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
</authors>
<title>Knowledge lean word sense disambiguation.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fourteenth National Conference on Artificial Intelligence,</booktitle>
<pages>814</pages>
<location>Providence, RI,</location>
<contexts>
<context position="3613" citStr="Pedersen, 1997" startWordPosition="589" endWordPosition="590">hich they occur (e.g., (Sch¨utze, 1998), (Purandare and Pedersen, 2004), (Pedersen, 2007)). This follows from the distributional hypothesis (Harris, 1954). Simply put, words that are used in similar contexts may often have similar meanings. However, words with different meanings can also be used in similar contexts (e.g., antonyms) so results are often noisy. 438 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 438–442, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics The Duluth systems take a knowledge-lean approach (Pedersen, 1997), and treat this task as an unsupervised word sense discrimination or induction problem, and use the freely available open-source software package SenseClusters1. 2 Systems We submitted three runs for subtask 2 : run1, run2, and run3. These three systems share a few basic characteristics, but differ in important respects. All use SenseClusters, and all utilize the same relatively simple pre–processing. Text was converted to lower case, and numeric values were all converted to a single string. Also, all three runs automatically determined the number of clusters (senses) using the PK2 measure (P</context>
</contexts>
<marker>Pedersen, 1997</marker>
<rawString>Ted Pedersen. 1997. Knowledge lean word sense disambiguation. In Proceedings of the Fourteenth National Conference on Artificial Intelligence, page 814, Providence, RI, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
</authors>
<title>UMND2 : SenseClusters applied to the sense induction task of Senseval-4.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>394--397</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="3087" citStr="Pedersen, 2007" startWordPosition="512" endWordPosition="513"> Given different syntactic and semantic features discovered in subtask 1, it would be possible to pursue subtask 2 using a more rule based approach. However, the Duluth systems do not explicitly account for syntax or semantics and do not try to identify these kinds of patterns. While we believe such approaches are extremely useful, we are primarily interested in exploring the limits of methods that depend on purely lexical features. As a result, the Duluth systems rely on clustering target verbs based on the context in which they occur (e.g., (Sch¨utze, 1998), (Purandare and Pedersen, 2004), (Pedersen, 2007)). This follows from the distributional hypothesis (Harris, 1954). Simply put, words that are used in similar contexts may often have similar meanings. However, words with different meanings can also be used in similar contexts (e.g., antonyms) so results are often noisy. 438 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 438–442, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics The Duluth systems take a knowledge-lean approach (Pedersen, 1997), and treat this task as an unsupervised word sense discrimination or indu</context>
<context position="11212" citStr="Pedersen, 2007" startWordPosition="1847" endWordPosition="1848">ver, in the Wingspread data run3 slightly outperformed run1, although not to a significant degree. We believe this occurred because the Wingspread data has a majority of target verbs with less than 40 contexts. This small amount of data will result in very sparse second–order co–occurrences. Given that run1 seeks target co–occurrences, when these are very sparse they essentially reduce to first— order rstorder co–occurrences, leading to very similar performance between run1 and run3. 3.2 SenseClusters F–Measure Tables 3.2 and 3.2 provide results for run1 using the SenseClusters F-Measure (F) (Pedersen, 2007). This measure first assigns the discovered clusters to gold standard senses in whatever way optimizes the agreement between them using the (Munkres, 1957) algorithm. Then any senses or clusters that are not aligned are discarded, and precision and recall are computed in the usual way. In these experiments all contexts are assigned to clusters, so recall and precision are the same, and the F-measure can be viewed as accuracy. In this case the F-measure is the percentage of contexts that were assigned to the correct cluster. These tables also show the most frequent sense baseline (M). This is t</context>
<context position="13545" citStr="Pedersen, 2007" startWordPosition="2249" endWordPosition="2250">dict exactly the same number of clusters as found in the gold standard data, in general there were no cases where it differed radically. On average the Microcheck data had 4.3 senses, while run1 discovered 3.7. For the Wingspread data there were 3.0 senses, while run1 discovered 2.7. While the results show that the clusters themselves are noisy, in general we are pleased that our ability to predict the number of clusters is reasonably accurate. 4 Conclusions SenseClusters has participated in numerous SensEval and SemEval shared tasks that have included word sense discrimination and induction (Pedersen, 2007; Pedersen, 2010; Pedersen, 2013). In all of these prior events, the most frequent sense baseline has proven hard to beat. In general assigning all instances of a target verb to a single cluster replicates most frequent sense performance. The results in this subtask are similar, and suggest that for the moment, automatic word sense discrimination is still not a viable replacement for human lexicographic expertise. N C D M F adapt 182 4 1 .539 .539 advise 230 8 2 .365 .365 afflict 179 2 2 .961 .687 ascertain 7 2 1 .571 .571 ask 573 9 2 .522 .470 attain 240 3 4 .833 .627 avert 240 2 7 .958 .374 </context>
</contexts>
<marker>Pedersen, 2007</marker>
<rawString>Ted Pedersen. 2007. UMND2 : SenseClusters applied to the sense induction task of Senseval-4. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), pages 394–397, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
</authors>
<title>Duluth-WSI: SenseClusters applied to the sense induction task of semEval-2.</title>
<date>2010</date>
<booktitle>In Proceedings of the SemEval 2010 Workshop : the 5th International Workshop on Semantic Evaluations,</booktitle>
<pages>363--366</pages>
<location>Uppsala,</location>
<contexts>
<context position="13561" citStr="Pedersen, 2010" startWordPosition="2251" endWordPosition="2252"> same number of clusters as found in the gold standard data, in general there were no cases where it differed radically. On average the Microcheck data had 4.3 senses, while run1 discovered 3.7. For the Wingspread data there were 3.0 senses, while run1 discovered 2.7. While the results show that the clusters themselves are noisy, in general we are pleased that our ability to predict the number of clusters is reasonably accurate. 4 Conclusions SenseClusters has participated in numerous SensEval and SemEval shared tasks that have included word sense discrimination and induction (Pedersen, 2007; Pedersen, 2010; Pedersen, 2013). In all of these prior events, the most frequent sense baseline has proven hard to beat. In general assigning all instances of a target verb to a single cluster replicates most frequent sense performance. The results in this subtask are similar, and suggest that for the moment, automatic word sense discrimination is still not a viable replacement for human lexicographic expertise. N C D M F adapt 182 4 1 .539 .539 advise 230 8 2 .365 .365 afflict 179 2 2 .961 .687 ascertain 7 2 1 .571 .571 ask 573 9 2 .522 .470 attain 240 3 4 .833 .627 avert 240 2 7 .958 .374 avoid 242 3 2 .7</context>
</contexts>
<marker>Pedersen, 2010</marker>
<rawString>Ted Pedersen. 2010. Duluth-WSI: SenseClusters applied to the sense induction task of semEval-2. In Proceedings of the SemEval 2010 Workshop : the 5th International Workshop on Semantic Evaluations, pages 363– 366, Uppsala, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
</authors>
<title>Duluth : Word sense induction applied to web page clustering.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>202--206</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="13578" citStr="Pedersen, 2013" startWordPosition="2253" endWordPosition="2254">clusters as found in the gold standard data, in general there were no cases where it differed radically. On average the Microcheck data had 4.3 senses, while run1 discovered 3.7. For the Wingspread data there were 3.0 senses, while run1 discovered 2.7. While the results show that the clusters themselves are noisy, in general we are pleased that our ability to predict the number of clusters is reasonably accurate. 4 Conclusions SenseClusters has participated in numerous SensEval and SemEval shared tasks that have included word sense discrimination and induction (Pedersen, 2007; Pedersen, 2010; Pedersen, 2013). In all of these prior events, the most frequent sense baseline has proven hard to beat. In general assigning all instances of a target verb to a single cluster replicates most frequent sense performance. The results in this subtask are similar, and suggest that for the moment, automatic word sense discrimination is still not a viable replacement for human lexicographic expertise. N C D M F adapt 182 4 1 .539 .539 advise 230 8 2 .365 .365 afflict 179 2 2 .961 .687 ascertain 7 2 1 .571 .571 ask 573 9 2 .522 .470 attain 240 3 4 .833 .627 avert 240 2 7 .958 .374 avoid 242 3 2 .727 .566 begrudge </context>
</contexts>
<marker>Pedersen, 2013</marker>
<rawString>Ted Pedersen. 2013. Duluth : Word sense induction applied to web page clustering. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 202–206, Atlanta, Georgia, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amruta Purandare</author>
<author>Ted Pedersen</author>
</authors>
<title>Word sense discrimination by clustering contexts in vector and similarity spaces.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Computational Natural Language Learning,</booktitle>
<pages>41--48</pages>
<location>Boston, MA.</location>
<contexts>
<context position="3069" citStr="Purandare and Pedersen, 2004" startWordPosition="508" endWordPosition="511">milar to semantic role labeling. Given different syntactic and semantic features discovered in subtask 1, it would be possible to pursue subtask 2 using a more rule based approach. However, the Duluth systems do not explicitly account for syntax or semantics and do not try to identify these kinds of patterns. While we believe such approaches are extremely useful, we are primarily interested in exploring the limits of methods that depend on purely lexical features. As a result, the Duluth systems rely on clustering target verbs based on the context in which they occur (e.g., (Sch¨utze, 1998), (Purandare and Pedersen, 2004), (Pedersen, 2007)). This follows from the distributional hypothesis (Harris, 1954). Simply put, words that are used in similar contexts may often have similar meanings. However, words with different meanings can also be used in similar contexts (e.g., antonyms) so results are often noisy. 438 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 438–442, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics The Duluth systems take a knowledge-lean approach (Pedersen, 1997), and treat this task as an unsupervised word sense disc</context>
</contexts>
<marker>Purandare, Pedersen, 2004</marker>
<rawString>Amruta Purandare and Ted Pedersen. 2004. Word sense discrimination by clustering contexts in vector and similarity spaces. In Proceedings of the Conference on Computational Natural Language Learning, pages 41–48, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<marker>Sch¨utze, 1998</marker>
<rawString>Hinrich Sch¨utze. 1998. Automatic word sense discrimination. Computational Linguistics, 24(1):97–123.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>