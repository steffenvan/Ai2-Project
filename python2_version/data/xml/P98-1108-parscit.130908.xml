<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000163">
<title confidence="0.9975085">
Use of Mutual Information Based Character Clusters in
Dictionary-less Morphological Analysis of Japanese
</title>
<author confidence="0.8654915">
Hideki Kashioka, Yasuhiro Kawata, Yumiko Kinjo,
Andrew Finch and Ezra W. Black
</author>
<keyword confidence="0.2657325">
{kashioka, ykawata, kinjo, finch, black}tiitl.atr.co.jp
ATR Interpreting Telecommunications Reserach Laboratories
</keyword>
<sectionHeader confidence="0.931316" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99991725">
For languages whose character set is very large
and whose orthography does not require spac-
ing between words, such as Japanese, tokenizing
and part-of-speech tagging are often the diffi-
cult parts of any morphological analysis. For
practical systems to tackle this problem, un-
controlled heuristics are primarily used. The
use of information on character sorts, however,
mitigates this difficulty. This paper presents
our method of incorporating character cluster-
ing based on mutual information into Decision-
Tree Dictionary-less morphological analysis. By
using natural classes, we have confirmed that
our morphological analyzer has been signifi-
cantly improved in both tokenizing and tagging
Japanese text.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999316875">
Recent papers have reported cases of successful
part-of-speech tagging with statistical language
modeling techniques (Church 1988; Cutting et.
al. 1992; Charniak et. al. 1993; Brill 1994;
Nagata 1994; Yamamoto 1996). Morphological
analysis on Japanese, however, is more complex
because, unlike European languages, no spaces
are inserted between words. In fact, even native
Japanese speakers place word boundaries incon-
sistently. Consequently, individual researchers
have been adopting different word boundaries
and tag sets based on their own theory—internal
justifications.
For a practical system to utilize the different
word boundaries and tag sets according to the
demands of an application, it is necessary to co-
ordinate the dictionary used, tag sets, and nu-
merous other parameters. Unfortunately, such
a task is costly. Furthermore, it is difficult to
maintain the accuracy needed to regulate the
word boundaries. Also, depending on the pur-
pose, new technical terminology may have to be
collected, the dictionary has to be coordinated,
but the problem of unknown words would still
remain.
The above problems will arise so long as a
dictionary continue to play a principal role. In
analyzing Japanese, a Decision-Tree approach
with no need for a dictionary (Kashioka, et. al.
1997) has led us to employ, among other param-
eters, mutual information (MI) bits of individ-
ual characters derived from large hierarchically
clustered sets of characters in the corpus.
This paper therefore proposes a type of
Decision-Tree morphological analysis using the
MI of characters but with no need for a dic-
tionary. Next the paper describes the use of
information on character sorts in morpholog-
ical analysis involving the Japanese language,
how knowing the sort of each character is use-
ful when tokenizing a string of characters into
a string of words and when assigning parts-of-
speech to them, and our method of clustering
characters based on MI bits. Then, it proposes
a type of Decision-Tree analysis where the no-
tion of MI-based character and word clustering
is incorporated. Finally, we move on to an ex-
perimental report and discussions.
</bodyText>
<subsectionHeader confidence="0.473471">
2 Use of Information on Characters
</subsectionHeader>
<bodyText confidence="0.9994248">
Many languages in the world do not insert
a space between words in the written text.
Japanese is one of them. Moreover, the num-
ber of characters involved in Japanese is very
large. 1
</bodyText>
<footnote confidence="0.8778415">
1 Unlike English being basically written in a 26-
character alphabet, the domain of possible characters
appearing in an average Japanese text is a set involving
tens of thousands of characters,
</footnote>
<page confidence="0.990687">
658
</page>
<subsectionHeader confidence="0.984368">
2.1 Character Sort
</subsectionHeader>
<bodyText confidence="0.999878181818182">
There are three clearly identifiable character
sorts in Japanese: 2
Kanji are Chinese characters adopted for
historical reasons and deeply rooted in
Japanese. Each character carries a seman-
tic sense.
Hiragana are basic Japanese phonograms rep-
resenting syllables. About fifty of them
constitute the syllabary.
Katalcana are characters corresponding to hi-
ragana, but their use is restricted mainly
to foreign loan words.
Each character sort has a limited number of el-
ements, except for Kanji whose exhaustive list
is hard to obtain.
Identifying each character sort in a sen-
tence would help in predicting the word bound-
aries and subsequently in assigning the parts-of-
speech. For example, between characters of dif-
ferent sorts, word boundaries are highly likely.
Accordingly, in formalizing heuristics, character
sorts must be assumed.
</bodyText>
<subsectionHeader confidence="0.99951">
2.2 Character Cluster
</subsectionHeader>
<bodyText confidence="0.999974">
Apart from the distinctions mentioned above,
are there things such as natural classes with re-
spect to the distribution of characters in a cer-
tain set of sentences (therefore, the classes are
empirically learnable)? If there are, how can we
obtain such knowledge?
It seems that only a certain group of charac-
ters tends to occur in a certain restricted con-
text. For example, in Japanese, there are many
numerical classifier expressions attached imme-
diately after numericals. 3 If such is the case,
these classifiers can be clustered in terms of
their distributions with respect to a presumably
natural class called numericals. Supposing one
of a certain group of characters often occurs as
a neighbor to one of the other groups of char-
acters, and supposing characters are clustered
and organized in a hierarchical fashion, then it
is possible to refer to such groupings by pointing
</bodyText>
<footnote confidence="0.9487182">
20ther sorts found in ordinary text are Arabic nu-
merics, punctuations, other symbols, etc.
3For example, &amp;quot;3 iiif (san-satsu)&amp;quot; for bound ob-
jects &amp;quot;3 copies of&amp;quot;, &amp;quot;2 ft (ni-mai)&amp;quot; for flat objects &amp;quot;2
pieces/sheets of&amp;quot;.
</footnote>
<bodyText confidence="0.99767675">
out a certain node in the structure. Having a
way of organizing classes of characters is clearly
an advantage in describing facts in Japanese.
The next section presents such a method.
</bodyText>
<sectionHeader confidence="0.991841" genericHeader="method">
3 Mutual Information-Based
</sectionHeader>
<subsectionHeader confidence="0.761355">
Character Clustering
</subsectionHeader>
<bodyText confidence="0.999966461538462">
One idea is to sort words out in terms of neigh-
boring contexts. Accordingly research has been
carried out on n-gram models of word cluster-
ing (Brown et. al. 1992) to obtain hierarchical
clusters of words by classifying words in such a
way so as to minimizes the reduction of MI.
This idea is general in the clustering of any
kind of list of items into hierarchical classes. 4
We therefore have adopted this approach not
only to compute word classes but also to com-
pute character clusterings in Japanese.
The basic algorithm for clustering items
based on the amount of MI is as follows: 5
</bodyText>
<listItem confidence="0.996932333333333">
1) Assign a singleton class to every item in the
set.
2) Choose two appropriate classes to create a
new class which subsumes them.
3) Repeat 2) until the additional new items
include all of the items in the set.
</listItem>
<bodyText confidence="0.943427">
With this method, we conducted an experi-
mental clustering over the ATR travel conver-
sation corpus. 6 As a result, all of the charac-
ters in the corpus were hierarchically clustered
according to their distributions.
Example: A partial character clustering
0000000110in
</bodyText>
<equation confidence="0.7968424">
+-+-+-+---IJ 0000000111000000
I I +-+- it 00000001110000010
I I 4.- A 0000000i110000011
pp 000000011100001
g 00000001110001000
</equation>
<bodyText confidence="0.993120714285714">
Each node represents a subset of all of the
different characters found in the training data.
We represent tree structured clusters with bit
strings, so that we may specify any node in the
structure by using a bit substring.
4Brown, et. al. (1992) for details.
6Tliis algorithm, however, is too costly because the
amount of computation exponentially increases depend-
ing on the number of items. For practical processing,
the basic procedure is carried out over a certain limited
number of items, while a new item is supplied to the
processing set each time clustering is done.
680,000 sentences, with a total number of 1,585,009
characters and 1,831 different characters.
</bodyText>
<page confidence="0.996654">
659
</page>
<bodyText confidence="0.99967225">
Numerous significant clusters are found
among them. 7 They are all natural classes
computed based on the events in the training
set.
</bodyText>
<sectionHeader confidence="0.9466265" genericHeader="method">
4 Decision-Tree Morphological
Analysis
</sectionHeader>
<bodyText confidence="0.999959">
The Decision-Tree model consists of a set of
questions structured into a dendrogram with
a probability distribution associated with each
leaf of the tree. In general, a decision-tree is a
complex of n-ary branching trees in which ques-
tions are associated with each parent node, and
a choice or class is associated with each child
node. 8 We represent answers to questions as
bits.
Among other advantages to using decision-
trees, it is important to note that they are able
to assign integrated costs for classification by
all types of questions at different feature levels
provided each feature has a different cost.
</bodyText>
<subsectionHeader confidence="0.992097">
4.1 Model
</subsectionHeader>
<bodyText confidence="0.98088525">
Let us assume that an input sentence C =
C1 c2 en denotes a sequence of n charac-
ters that constitute words IV = Wi tv2 wm,
where each word wi is assigned a tag ti (T =
</bodyText>
<equation confidence="0.836565">
t2 tm ).
</equation>
<bodyText confidence="0.999841857142857">
The morphological analysis task can be for-
mally defined as finding a set of word segmenta-
tions and part-of-speech assignments that maxi-
mizes the joint probability of the word sequence
and tag sequence P(IV, T(C).
The joint probability P(147, TIC) is calculated
by the following formulae:
</bodyText>
<equation confidence="0.99861425">
P(TV,TIC) =
P(wi,tilwi, •-•,wi-i,ti, •••,ti-i,C)
=
P(tikvi, •••,wi,ti,
</equation>
<bodyText confidence="0.849665151515152">
The Word Model decision-tree is used as the
word tokenizer. While finding word bound-
&apos;For example, katakana, numerical classifiers, numer-
ics, postpositional case particles, and prefixes of demon-
strative pronouns.
8The work described here employs only binary
decision-trees. Multiple alternative questions are rep-
resented in more than two yes/no questions. The main
reason for this is the computational efficiency. Allowing
questions to have more answers complicates the decision-
tree growth algorithm.
&apos;We call this the &amp;quot;Word Model&amp;quot;.
&amp;quot;We call this the &amp;quot;Tagging Model&amp;quot;.
aries, we use two different labels: Word+ and
Word-. In the training data, we label Word+
to a complete word string, and Word- to ev-
ery substring of a relevant word since these sub-
strings are not in fact a word in the current con-
text. 11 The probability of a word estimates the
associated distributions of leaves with a word
decision-tree.
We use the Tagging Model decision-tree as
our part-of-speech tagger. For an input sentence
C, let us consider the character sequence from
el to cp_1 (assigned w1 wk_i) and the
following character sequence from p to p +1 to
be the word wk; also, the word wk is assumed
to be assigned the tag tk.
We approximate the probability of the word
wk assigned with tag tk as follows: P(tk) =
p(t2lw1,...,wk,ti,...,tk_1,C). This probability
estimates the associated distributions of leaves
with a part-of-speech tag decision-tree.
</bodyText>
<subsectionHeader confidence="0.987611">
4.2 Growing Decision-Trees
</subsectionHeader>
<bodyText confidence="0.999929222222222">
Growing a decision-tree requires two steps: se-
lecting a question to ask at each node; and de-
termining the probability distribution for each
leaf from the distribution of events in the train-
ing set. At each node, we choose from among all
possible questions, the question that maximizes
the reduction in entropy.
The two steps are repeated until the following
conditions are no longer satisfied:
</bodyText>
<listItem confidence="0.99773775">
• The number of leaf node events exceeds the
constant number.
• The reduction in entropy is more than the
threshold.
</listItem>
<bodyText confidence="0.966718058823529">
Consequently, the list of questions is optimally
structured in such a way that, when the data
flows in the decision-tree, at each decision point,
the most efficient question is asked.
Provided a set of training sentences with word
boundaries in which each word is assigned with
a part-of-speech tag, we have a) the neces-
sary structured character clusters, and b) the
necessary structured word clusters; 12 both of
them are based on the n-gram language model.
For instance, for the word &amp;quot;mo-shi-mo-shi&amp;quot; (hello),
&amp;quot;mo-shi-mo-shi&amp;quot; is labeled Word+, and &amp;quot;mo-shi-mo&amp;quot;,
&amp;quot;mo-shi&amp;quot;, &amp;quot;mo&amp;quot; are all labeled Word—. Note that &amp;quot;mo-
shi&amp;quot; or &amp;quot;mo-shi-mo&amp;quot; may be real words in other contexts,
e.g., &amp;quot;mo-shi/wa-ta-shi/ga ... (If I do ... )&amp;quot;.
12Here, a word token is based only on a word string,
not on a word string tagged with a part-of-speech.
</bodyText>
<page confidence="0.993501">
660
</page>
<bodyText confidence="0.9999832">
We also have c) the necessary decision-trees
for word-splitting and part-of-speech tagging,
each of which contains a set of questions about
events. We have considered the following points
in making decision-tree questions.
</bodyText>
<sectionHeader confidence="0.325576" genericHeader="method">
1) MI character bits
</sectionHeader>
<bodyText confidence="0.9986392">
We define self-organizing character classes
represented by binary trees, each of whose
nodes are significant in the n-gram lan-
guage model. We can ask which node a
character is dominated by.
</bodyText>
<sectionHeader confidence="0.728443" genericHeader="method">
2) MI word bits
</sectionHeader>
<bodyText confidence="0.949880666666667">
Likewise, MI word bits (Brown et. al.
1992) are also available so that we may ask
which node a word is dominated by.
</bodyText>
<listItem confidence="0.988162666666667">
3) Questions about the target word
These questions mostly relate to the mor-
phology of a word (e.g., Is it ending in `-
shi-i&apos; (an adjective ending)? Does it start
with `do-&apos;?).
4) Questions about the context
</listItem>
<bodyText confidence="0.99938325">
Many of these questions concern continu-
ous part-of-speech tags (e.g., Is the pre-
vious word an adjective?). However, the
questions may concern information at dif-
ferent remote locations in a sentence (e.g.,
Is the initial word in the sentence a noun?).
These questions can be combined in order to
form questions of greater complexity.
</bodyText>
<subsectionHeader confidence="0.61619">
5 Analysis with Decision-Trees
</subsectionHeader>
<bodyText confidence="0.999943875">
Our proposed morphological analyzer processes
each character in a string from left to right.
Candidates for a word are examined, and a
tag candidate is assigned to each word. When
each candidate for a word is checked, it is given
a probability by the word model decision-tree.
We can either exhaustively enumerate and score
all of the cases or use a stack decoder algorithm
(Jelinek 1969; Paul 1991) to search through the
most probable candidates.
The fact that we do not use a dictionary, 13
is one of the great advantages. By using a dic-
tionary, a morphological analyzer has to deal
with unknown words and unknown tags, 14 and
is also fooled by many words sharing common
substrings. In practical contexts, the system
</bodyText>
<tableCaption confidence="0.8715012">
&amp;quot;Here, a dictionary is a listing of words attached to
part-of-speech tags.
&amp;quot;Words that are not found in the dictionary and nec-
essary tags that are not assigned in the dictionary.
Table 1: Travel Conversation
</tableCaption>
<table confidence="0.983950266666667">
Training A (%) B (%)
1,000-FMIChr 80.67 69.93
—MIChr 70.03 62.24
2,000-1- MIChr 86.61 76.43
—MIChr 69.65 63.36
3,000-FMIChr 88.60 79.33
—MIChr 71.97 66.47
4,000+MIChr 88.26 80.11
—MIChr 72.55 67.24
5,000+MIChr 89.42 81.94
—MIChr 72.41 67.72
Training: number of sentences
with/without Character Clustering
A: Correct word/system output words
B: Correct tags/system output words
</table>
<bodyText confidence="0.999561428571428">
refers to the dictionary by using heuristic rules
to find the more likely word boundaries, e.g., the
minimum number of words, or the maximum
word length available at the minimum cost. If
the system could learn how to find word bound-
aries without a dictionary, then there would be
no need for such an extra device or process.
</bodyText>
<sectionHeader confidence="0.992658" genericHeader="evaluation">
6 Experimental Results
</sectionHeader>
<bodyText confidence="0.985941227272727">
We tested our morphological analyzer with two
different corpora: a) ATR-travel, which is a
task oriented dialogue in a travel context, and
b) EDR Corpus, (EDR 1996) which consists of
rather general written text.
For each experiment, we used the charac-
ter clustering based on MI. Each question for
the decision-trees was prepared separately, with
or without questions concerning the character
clusters. Evaluations were made with respect
to the original tagged corpora, from which both
the training and test sentences were taken.
The analyzer was trained for an incrementally
enlarged set of training data using or not us-
ing character clustering. 15 Table 1 shows re-
sults obtained from training sets of ATR-travel.
The upper figures in each box indicate the re-
sults when using the character clusters, and the
lower without using them. The actual test set of
4,147 sentences (55,544 words) was taken from
&amp;quot;Another 2,231 sentences (28,933 words) in the same
domain are used for the smoothing.
</bodyText>
<page confidence="0.991044">
661
</page>
<table confidence="0.973548666666667">
•
Training A (%) B (%)
3,000+MIChr 83.80 78.19
-MIChr 77.56 72.49
5,000+MIChr 85.50 80.42
-MIChr 78.68 73.84
7,000+MIChr 85.97 81.66
-MIChr 79.32 75.30
9,000+MIChr 86.08 81.20
-MIChr 78.59 74.05
10,000+MIChr 86.22 81.39
-MIChr 78.94 74.41
</table>
<bodyText confidence="0.996461125">
the same domain.
The MI-word clusters were constructed ac-
cording to the domain of the training set. The
tag set consisted of 209 part-of-speech tags. 16
For the word model decision-tree, three of 69
questions concerned the character clusters and
three of 63 the tagging model. Their presence
or absence was the deciding parameter.
The analyzer was also trained for the EDR
Corpus. The same character clusters as with the
conversational corpus were used. A tag set in
the corpus consisted of 15 parts-of-speech. For
the word model, 45 questions were prepared; 18
for the Tagging model. Just a couple of them
were involved in the character clusters. The re-
sults are shown in Table 2.
</bodyText>
<sectionHeader confidence="0.99484" genericHeader="conclusions">
7 Conclusion and Discussion
</sectionHeader>
<bodyText confidence="0.997044878787879">
Both results show that the use of character clus-
ters significantly improves both tokenizing and
tagging at every stage of the training. Consid-
ering the results, our model with MI characters
is useful for assigning parts of speech as well
as for finding word boundaries, and overcoming
the unknown word problem.
The consistent experimental results obtained
from the training data with different word
boundaries and different tag sets in the
Japanese text, suggests the method is generally
applicable to various different sets of corpora
constructed for different purposes. We believe
that with the appropriate number of adequate
&apos;These include common noun, verb, post-position,
auxiliary verb, adjective, adverb, etc. The purpose
of this tag set is to perform machine translation from
Japanese to English, German and Korean.
questions, the method is transferable to other
languages that have word boundaries not indi-
cated in the text.
In conclusion, we should note that our
method, which does not require a dictionary,
has been significantly improved by the charac-
ter cluster information provided.
Our plans for further research include inves-
tigating the correlation between accuracy and
the training data size, the number of questions
as well as exploring methods for factoring in-
formation from a &amp;quot;dictionary&amp;quot; into our model.
Along these lines, a fruitful approach may be
to explore methods of coordinating probabilis-
tic decision-trees to obtain a higher accuracy.
</bodyText>
<sectionHeader confidence="0.999676" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999725621621622">
Brill, E. (1994) &amp;quot;Some Advances in Transformation-
Based Part of Speech Tagging,&amp;quot; AAAI-94, pp.
722-727.
Brown, P., Della Pietra, V., de Souza, P., Lai, J., and
Mercer, R. (1992) &amp;quot;Class-based n-gram models
of natural language,&amp;quot; Computational Linguistics,
Vol. 18, No. 4, pp. 467-479.
Cutting, D., Kupiec, J., Pedersen, J., and Sibun,
P. (1992) &amp;quot;A Practical Part-of-Speech Tagger,&amp;quot;
ANLP-92, pp. 133-140.
Charniak, E., Hendrickson, C., Jacobson, N., and
Perkowits, M. (1993) &amp;quot;Equations for Part-of-
Speech Tagging,&amp;quot; AAAI-93, pp. 784-789.
Church, K. (1988) &amp;quot;A Stochastic Parts Program and
Noun Phrase Parser for Unrestricted Text,&amp;quot; Pro-
ceedings of the 2nd Conference on Applied Natu-
ral Language Processing, Austin-Marriott at the
Capitol, Austin, Texas, USA, 1988, pp. 136-143.
EDR (1996) EDR Electronic Dictionary Version 1.5
Technical Guide. EDR TR2-007.
Jelinek, F. (1969) &amp;quot;A fast sequential decoding algo-
rithm using a stack,&amp;quot; IBM Journal of Research
and Development, Vol. 13, pp. 675-685.
Kashioka, H., Black, E., and Eubank, S. (1997)
&amp;quot;Decision-Tree Morphological Analysis without a
Dictionary for Japanese,&amp;quot; Proceedings of NLPRS
97, pp. 541-544.
Nagata, M. (1994) &amp;quot;A Stochastic Japanese Morpho-
logical Analyzer Using a Forward-DP Backward-
A* N-Best Search Algorithm,&amp;quot; Proceedings of
COLING-94, pp. 201-207.
Paul, D. (1991) &amp;quot;Algorithms for an optimal a*
search and linearizing the search in the stack de-
coder,&amp;quot; Proceedings, ICASSP 91, pp. 693-696.
Yamamoto, M. (1996) &amp;quot;A Re-estimation Method for
Stochastic Language Modeling from Ambiguous
Observations,&amp;quot; WVL C-4, pp. 155-167.
</reference>
<page confidence="0.99722">
662
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.553256">
<title confidence="0.9895465">Use of Mutual Information Based Character Clusters in Dictionary-less Morphological Analysis of Japanese</title>
<author confidence="0.926975">Hideki Kashioka</author>
<author confidence="0.926975">Yasuhiro Kawata</author>
<author confidence="0.926975">Yumiko Kinjo</author>
<author confidence="0.926975">Finch W Black</author>
<email confidence="0.926125">kashiokatiitl.atr.co.jp</email>
<email confidence="0.926125">ykawatatiitl.atr.co.jp</email>
<email confidence="0.926125">kinjotiitl.atr.co.jp</email>
<email confidence="0.926125">finchtiitl.atr.co.jp</email>
<email confidence="0.926125">blacktiitl.atr.co.jp</email>
<affiliation confidence="0.797352">ATR Interpreting Telecommunications Reserach Laboratories</affiliation>
<abstract confidence="0.992253058823529">For languages whose character set is very large and whose orthography does not require spacing between words, such as Japanese, tokenizing and part-of-speech tagging are often the difficult parts of any morphological analysis. For practical systems to tackle this problem, uncontrolled heuristics are primarily used. The use of information on character sorts, however, mitigates this difficulty. This paper presents our method of incorporating character clustering based on mutual information into Decision- Tree Dictionary-less morphological analysis. By using natural classes, we have confirmed that our morphological analyzer has been significantly improved in both tokenizing and tagging Japanese text.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>Some Advances in TransformationBased Part of Speech Tagging,&amp;quot;</title>
<date>1994</date>
<booktitle>AAAI-94,</booktitle>
<pages>722--727</pages>
<contexts>
<context position="1213" citStr="Brill 1994" startWordPosition="164" endWordPosition="165"> are primarily used. The use of information on character sorts, however, mitigates this difficulty. This paper presents our method of incorporating character clustering based on mutual information into DecisionTree Dictionary-less morphological analysis. By using natural classes, we have confirmed that our morphological analyzer has been significantly improved in both tokenizing and tagging Japanese text. 1 Introduction Recent papers have reported cases of successful part-of-speech tagging with statistical language modeling techniques (Church 1988; Cutting et. al. 1992; Charniak et. al. 1993; Brill 1994; Nagata 1994; Yamamoto 1996). Morphological analysis on Japanese, however, is more complex because, unlike European languages, no spaces are inserted between words. In fact, even native Japanese speakers place word boundaries inconsistently. Consequently, individual researchers have been adopting different word boundaries and tag sets based on their own theory—internal justifications. For a practical system to utilize the different word boundaries and tag sets according to the demands of an application, it is necessary to coordinate the dictionary used, tag sets, and numerous other parameters</context>
</contexts>
<marker>Brill, 1994</marker>
<rawString>Brill, E. (1994) &amp;quot;Some Advances in TransformationBased Part of Speech Tagging,&amp;quot; AAAI-94, pp. 722-727.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>Della Pietra</author>
<author>V de Souza</author>
<author>P Lai</author>
<author>J</author>
<author>R Mercer</author>
</authors>
<title>Class-based n-gram models of natural language,&amp;quot;</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<pages>467--479</pages>
<marker>Brown, Pietra, de Souza, Lai, J, Mercer, 1992</marker>
<rawString>Brown, P., Della Pietra, V., de Souza, P., Lai, J., and Mercer, R. (1992) &amp;quot;Class-based n-gram models of natural language,&amp;quot; Computational Linguistics, Vol. 18, No. 4, pp. 467-479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Cutting</author>
<author>J Kupiec</author>
<author>J Pedersen</author>
<author>P Sibun</author>
</authors>
<title>A Practical Part-of-Speech Tagger,&amp;quot; ANLP-92,</title>
<date>1992</date>
<pages>133--140</pages>
<marker>Cutting, Kupiec, Pedersen, Sibun, 1992</marker>
<rawString>Cutting, D., Kupiec, J., Pedersen, J., and Sibun, P. (1992) &amp;quot;A Practical Part-of-Speech Tagger,&amp;quot; ANLP-92, pp. 133-140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>C Hendrickson</author>
<author>N Jacobson</author>
<author>M Perkowits</author>
</authors>
<title>Equations for Part-ofSpeech Tagging,&amp;quot; AAAI-93,</title>
<date>1993</date>
<pages>784--789</pages>
<marker>Charniak, Hendrickson, Jacobson, Perkowits, 1993</marker>
<rawString>Charniak, E., Hendrickson, C., Jacobson, N., and Perkowits, M. (1993) &amp;quot;Equations for Part-ofSpeech Tagging,&amp;quot; AAAI-93, pp. 784-789.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Church</author>
</authors>
<title>A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text,&amp;quot;</title>
<date>1988</date>
<booktitle>Proceedings of the 2nd Conference on Applied Natural Language Processing, Austin-Marriott at the Capitol,</booktitle>
<pages>136--143</pages>
<location>Austin, Texas, USA,</location>
<contexts>
<context position="1156" citStr="Church 1988" startWordPosition="154" endWordPosition="155">al systems to tackle this problem, uncontrolled heuristics are primarily used. The use of information on character sorts, however, mitigates this difficulty. This paper presents our method of incorporating character clustering based on mutual information into DecisionTree Dictionary-less morphological analysis. By using natural classes, we have confirmed that our morphological analyzer has been significantly improved in both tokenizing and tagging Japanese text. 1 Introduction Recent papers have reported cases of successful part-of-speech tagging with statistical language modeling techniques (Church 1988; Cutting et. al. 1992; Charniak et. al. 1993; Brill 1994; Nagata 1994; Yamamoto 1996). Morphological analysis on Japanese, however, is more complex because, unlike European languages, no spaces are inserted between words. In fact, even native Japanese speakers place word boundaries inconsistently. Consequently, individual researchers have been adopting different word boundaries and tag sets based on their own theory—internal justifications. For a practical system to utilize the different word boundaries and tag sets according to the demands of an application, it is necessary to coordinate the</context>
</contexts>
<marker>Church, 1988</marker>
<rawString>Church, K. (1988) &amp;quot;A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text,&amp;quot; Proceedings of the 2nd Conference on Applied Natural Language Processing, Austin-Marriott at the Capitol, Austin, Texas, USA, 1988, pp. 136-143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>EDR</author>
</authors>
<date>1996</date>
<booktitle>EDR Electronic Dictionary Version 1.5 Technical Guide. EDR</booktitle>
<pages>2--007</pages>
<contexts>
<context position="14721" citStr="EDR 1996" startWordPosition="2376" endWordPosition="2377">out Character Clustering A: Correct word/system output words B: Correct tags/system output words refers to the dictionary by using heuristic rules to find the more likely word boundaries, e.g., the minimum number of words, or the maximum word length available at the minimum cost. If the system could learn how to find word boundaries without a dictionary, then there would be no need for such an extra device or process. 6 Experimental Results We tested our morphological analyzer with two different corpora: a) ATR-travel, which is a task oriented dialogue in a travel context, and b) EDR Corpus, (EDR 1996) which consists of rather general written text. For each experiment, we used the character clustering based on MI. Each question for the decision-trees was prepared separately, with or without questions concerning the character clusters. Evaluations were made with respect to the original tagged corpora, from which both the training and test sentences were taken. The analyzer was trained for an incrementally enlarged set of training data using or not using character clustering. 15 Table 1 shows results obtained from training sets of ATR-travel. The upper figures in each box indicate the results</context>
</contexts>
<marker>EDR, 1996</marker>
<rawString>EDR (1996) EDR Electronic Dictionary Version 1.5 Technical Guide. EDR TR2-007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
</authors>
<title>A fast sequential decoding algorithm using a stack,&amp;quot;</title>
<date>1969</date>
<journal>IBM Journal of Research and Development,</journal>
<volume>13</volume>
<pages>675--685</pages>
<contexts>
<context position="13292" citStr="Jelinek 1969" startWordPosition="2142" endWordPosition="2143">rn information at different remote locations in a sentence (e.g., Is the initial word in the sentence a noun?). These questions can be combined in order to form questions of greater complexity. 5 Analysis with Decision-Trees Our proposed morphological analyzer processes each character in a string from left to right. Candidates for a word are examined, and a tag candidate is assigned to each word. When each candidate for a word is checked, it is given a probability by the word model decision-tree. We can either exhaustively enumerate and score all of the cases or use a stack decoder algorithm (Jelinek 1969; Paul 1991) to search through the most probable candidates. The fact that we do not use a dictionary, 13 is one of the great advantages. By using a dictionary, a morphological analyzer has to deal with unknown words and unknown tags, 14 and is also fooled by many words sharing common substrings. In practical contexts, the system &amp;quot;Here, a dictionary is a listing of words attached to part-of-speech tags. &amp;quot;Words that are not found in the dictionary and necessary tags that are not assigned in the dictionary. Table 1: Travel Conversation Training A (%) B (%) 1,000-FMIChr 80.67 69.93 —MIChr 70.03 6</context>
</contexts>
<marker>Jelinek, 1969</marker>
<rawString>Jelinek, F. (1969) &amp;quot;A fast sequential decoding algorithm using a stack,&amp;quot; IBM Journal of Research and Development, Vol. 13, pp. 675-685.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kashioka</author>
<author>E Black</author>
<author>S Eubank</author>
</authors>
<title>Decision-Tree Morphological Analysis without a Dictionary for Japanese,&amp;quot;</title>
<date>1997</date>
<booktitle>Proceedings of NLPRS 97,</booktitle>
<pages>541--544</pages>
<marker>Kashioka, Black, Eubank, 1997</marker>
<rawString>Kashioka, H., Black, E., and Eubank, S. (1997) &amp;quot;Decision-Tree Morphological Analysis without a Dictionary for Japanese,&amp;quot; Proceedings of NLPRS 97, pp. 541-544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Nagata</author>
</authors>
<title>A Stochastic Japanese Morphological Analyzer Using a Forward-DP BackwardA* N-Best Search Algorithm,&amp;quot;</title>
<date>1994</date>
<booktitle>Proceedings of COLING-94,</booktitle>
<pages>201--207</pages>
<contexts>
<context position="1226" citStr="Nagata 1994" startWordPosition="166" endWordPosition="167">ly used. The use of information on character sorts, however, mitigates this difficulty. This paper presents our method of incorporating character clustering based on mutual information into DecisionTree Dictionary-less morphological analysis. By using natural classes, we have confirmed that our morphological analyzer has been significantly improved in both tokenizing and tagging Japanese text. 1 Introduction Recent papers have reported cases of successful part-of-speech tagging with statistical language modeling techniques (Church 1988; Cutting et. al. 1992; Charniak et. al. 1993; Brill 1994; Nagata 1994; Yamamoto 1996). Morphological analysis on Japanese, however, is more complex because, unlike European languages, no spaces are inserted between words. In fact, even native Japanese speakers place word boundaries inconsistently. Consequently, individual researchers have been adopting different word boundaries and tag sets based on their own theory—internal justifications. For a practical system to utilize the different word boundaries and tag sets according to the demands of an application, it is necessary to coordinate the dictionary used, tag sets, and numerous other parameters. Unfortunate</context>
</contexts>
<marker>Nagata, 1994</marker>
<rawString>Nagata, M. (1994) &amp;quot;A Stochastic Japanese Morphological Analyzer Using a Forward-DP BackwardA* N-Best Search Algorithm,&amp;quot; Proceedings of COLING-94, pp. 201-207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Paul</author>
</authors>
<title>Algorithms for an optimal a* search and linearizing the search in the stack decoder,&amp;quot;</title>
<date>1991</date>
<booktitle>Proceedings, ICASSP 91,</booktitle>
<pages>693--696</pages>
<contexts>
<context position="13304" citStr="Paul 1991" startWordPosition="2144" endWordPosition="2145"> at different remote locations in a sentence (e.g., Is the initial word in the sentence a noun?). These questions can be combined in order to form questions of greater complexity. 5 Analysis with Decision-Trees Our proposed morphological analyzer processes each character in a string from left to right. Candidates for a word are examined, and a tag candidate is assigned to each word. When each candidate for a word is checked, it is given a probability by the word model decision-tree. We can either exhaustively enumerate and score all of the cases or use a stack decoder algorithm (Jelinek 1969; Paul 1991) to search through the most probable candidates. The fact that we do not use a dictionary, 13 is one of the great advantages. By using a dictionary, a morphological analyzer has to deal with unknown words and unknown tags, 14 and is also fooled by many words sharing common substrings. In practical contexts, the system &amp;quot;Here, a dictionary is a listing of words attached to part-of-speech tags. &amp;quot;Words that are not found in the dictionary and necessary tags that are not assigned in the dictionary. Table 1: Travel Conversation Training A (%) B (%) 1,000-FMIChr 80.67 69.93 —MIChr 70.03 62.24 2,000-1</context>
</contexts>
<marker>Paul, 1991</marker>
<rawString>Paul, D. (1991) &amp;quot;Algorithms for an optimal a* search and linearizing the search in the stack decoder,&amp;quot; Proceedings, ICASSP 91, pp. 693-696.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Yamamoto</author>
</authors>
<title>A Re-estimation Method for Stochastic Language Modeling from Ambiguous Observations,&amp;quot;</title>
<date>1996</date>
<booktitle>WVL C-4,</booktitle>
<pages>155--167</pages>
<contexts>
<context position="1242" citStr="Yamamoto 1996" startWordPosition="168" endWordPosition="169">use of information on character sorts, however, mitigates this difficulty. This paper presents our method of incorporating character clustering based on mutual information into DecisionTree Dictionary-less morphological analysis. By using natural classes, we have confirmed that our morphological analyzer has been significantly improved in both tokenizing and tagging Japanese text. 1 Introduction Recent papers have reported cases of successful part-of-speech tagging with statistical language modeling techniques (Church 1988; Cutting et. al. 1992; Charniak et. al. 1993; Brill 1994; Nagata 1994; Yamamoto 1996). Morphological analysis on Japanese, however, is more complex because, unlike European languages, no spaces are inserted between words. In fact, even native Japanese speakers place word boundaries inconsistently. Consequently, individual researchers have been adopting different word boundaries and tag sets based on their own theory—internal justifications. For a practical system to utilize the different word boundaries and tag sets according to the demands of an application, it is necessary to coordinate the dictionary used, tag sets, and numerous other parameters. Unfortunately, such a task </context>
</contexts>
<marker>Yamamoto, 1996</marker>
<rawString>Yamamoto, M. (1996) &amp;quot;A Re-estimation Method for Stochastic Language Modeling from Ambiguous Observations,&amp;quot; WVL C-4, pp. 155-167.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>