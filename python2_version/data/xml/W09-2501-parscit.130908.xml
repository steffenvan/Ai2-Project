<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.019629">
<bodyText confidence="0.999682333333333">
In RTE2 dev and test, we find only 81 and 60
true MWEs, respectively. Out of the 1600 sentence
pairs in the two datasets, 8.2% involve true MWEs
(73 in RTE2 dev and 58 in RTE2 test). On the level
of word alignments, the ratio is even smaller: only
1.2% of all SURE alignments involve true MWEs.
Furthermore, more than half of them are decom-
posable (MWEH/CP). Some examples from this
category are (“heads” marked in boldface):
</bodyText>
<figureCaption confidence="0.471952">
sue—* file lawsuits against
diseases —* liver cancer
Barbie —* Barbie doll
got —* was awarded with
works —* executive director
military —* naval forces
</figureCaption>
<bodyText confidence="0.998191857142857">
In particular when light verbs are involved (file
lawsuits) or when modification adds just minor
meaning aspects (executive director), we argue that
it is sufficient to align the left-hand expression to
the “head” in order to decide entailment.
Consider, in contrast, these examples from the
non-decomposable categories (MWENH/NCP):
</bodyText>
<equation confidence="0.857562">
politician —*presidential candidate
killed —* lost their lives
shipwreck —* sunken ship
ever —* in its history
widow —* late husband
sexes —* men and women
</equation>
<bodyText confidence="0.9975015">
These cases span a broad range of linguistic rela-
tions from pure associations (widow/late husband)
to collective expressions (sexes/men and women).
Arguably, in these cases aligning the left-hand word
to any single word on the right can seriously throw
off an entailment recognition system. However,
they are fairly rare, occurring only in 65 out of
1600 sentences.
</bodyText>
<subsectionHeader confidence="0.995927">
3.3 Conclusions from the MSR Analysis
</subsectionHeader>
<bodyText confidence="0.999397095238096">
Our analysis has found that 8% of the sentences
in the MSR dataset involve true MWEs. At the
word level, the fraction of true MWEs of all SURE
alignment links is just over 1%.
Of course, if errors in the alignment of these
MWEs had a high probability to lead to entailment
recognition errors, MWEs would still constitute a
major factor in determining entailment. However,
we have argued that about half of the true MWEs
are decomposable, that is, the part of the alignment
that is crucial for entailment can be recovered with
a one-to-one alignment link that can be identified
even by very limited alignment models.
This leaves considerably less than 1% of all word
alignments (or —4% of sentence pairs) where im-
perfect MWE alignments are able at all to exert a
negative influence on entailment. However, this is
just an upper bound – their impact is by no means
guaranteed. Thus, our conclusion from the annota-
tion study is that we do not expect MWEs to play a
large role in actual entailment recognition.
</bodyText>
<sectionHeader confidence="0.970174" genericHeader="abstract">
4 MWEs in Paraphrase Resources
</sectionHeader>
<bodyText confidence="0.999914897435897">
Before we come to actual experiments on the au-
tomatic recognition of MWEs in a practical RTE
system, we need to consider the prerequisites for
this task. As mentioned in Section 2, if an RTE
system is to establish multi-word alignments, it re-
quires a knowledge source that provides accurate
semantic similarity judgments for “many-to-many”
alignments (capital punishment – death penalty)
as well as for “one-to-many” alignments (vote –
cast ballots). Such similarities are not present in
standard lexical resources like WordNet or Dekang
Lin’s thesaurus (Lin, 1998).
The best class of candidate resources to provide
wide-coverage of multi-word similarities seems to
be paraphrase resources. In this section, we ex-
amine to what extent two of the most widely used
paraphrase resource types provide supporting ev-
idence for the true MWEs in the MSR data. We
deliberately use corpus-derived, noisy resources,
since we are interested in the real-world (rather
than idealized) prospects for accurate MWE align-
ment.
Dependency-based paraphrases. Lin and Pan-
tel (2002)’s DIRT model collects lexicalized de-
pendency paths with two slots at either end. Paths
with similar distributions over slot fillers count as
paraphrases, with the quality measured by a mutual
information-based similarity over the slot fillers.
The outcome of their study is the DIRT database
which lists paraphrases for around 230,000 depen-
dency paths, extracted from about 1 GB of mis-
cellaneous newswire text. We converted the DIRT
paraphrases2 into a resource of semantic similari-
ties between raw text phrases. We used a heuristic
mapping from dependency relations to word or-
der, and obtained similarity ratings by rescaling the
DIRT paraphrase ratings, which are based on a mu-
tual information-based measure of filler similarity,
onto the range [0,1].
</bodyText>
<footnote confidence="0.968291">
2We thank Patrick Pantel for granting us access to DIRT.
</footnote>
<page confidence="0.996889">
4
</page>
<bodyText confidence="0.999045477272727">
Parallel corpora-based paraphrases. An alter-
native approach to paraphrase acquisition was pro-
posed by Bannard and Callison-Burch (2005). It
exploits the variance inherent in translation to ex-
tract paraphrases from bilingual parallel corpora.
Concretely, it observes translational relationships
between a source and a target language and pairs
up source language phrases with other source lan-
guage phrases that translate into the same target
language phrases. We applied this method to
the large Chinese-English GALE MT evaluation
P3/P3.5 corpus (-2 GB text per language, mostly
newswire). The large number of translations makes
it impractical to store all observed paraphrases. We
therefore filtered the list of paraphrases against the
raw text of the RTE corpora, acquiring the 10 best
paraphrases for around 100,000 two- and three-
word phrases. The MLE conditional probabilities
were scaled onto [0,1] for each target.
Analysis. We checked the two resources for the
presence of the true MWEs identified in the MSR
data. We found that overall 34% of the MWEs ap-
pear in these resources, with more decomposable
MWEs (MWEH/CP) than non-decomposable ones
(MWENH/NCP) (42.1% vs. 24.6%). However, we
find that almost all of the MWEs that are covered
by the paraphrase resources are assigned very low
scores, while erroneous paraphrases (expressions
with clearly different meanings) have higher scores.
This is illustrated in Table 3 for the case of poorly
represented, which is aligned to very few in one
RTE2 sentence. This paraphrase is on the list, but
with a lower similarity than unsuitable paraphrases
such as representatives or good. This problem is
widespread. Other examples of low-scoring para-
phrases are: another step —* measures, quarantine
—* in isolation, punitive measures —* sanctions,
held a position —* served as, or inability —* could
not.
The noise in the rankings means that any align-
ment algorithm faces a dilemma: either it uses a
high threshold and misses valid MWE alignments,
or it lowers its threshold and risks constructing
incorrect alignments.
</bodyText>
<sectionHeader confidence="0.684495" genericHeader="categories and subject descriptors">
5 Impact of MWEs on Practical
</sectionHeader>
<subsectionHeader confidence="0.869172">
Entailment Recognition
</subsectionHeader>
<bodyText confidence="0.9077044">
This section provides the final step in our study: an
evaluation of the impact of MWEs on entailment
recognition in a current RTE system, and of the
benefits of explicit MWE alignment. While the
poorly represented
</bodyText>
<table confidence="0.98868975">
represented 0.42
poorly 0.07
rarely 0.06
good 0.05
representatives 0.04
very few 0.04
well 0.02
representative 0.01
</table>
<tableCaption confidence="0.9787885">
Table 3: Paraphrases of “poorly represented” with
scores (semantic similarities).
</tableCaption>
<bodyText confidence="0.9962426">
results of this experiment are not guaranteed to
transfer to other RTE system architectures, or to
future, improved paraphrase resources, it provides
a current snapshot of the practical impact of MWE
handling.
</bodyText>
<subsectionHeader confidence="0.991403">
5.1 The Stanford RTE System
</subsectionHeader>
<bodyText confidence="0.999980677419355">
We base our experiments on the Stanford RTE sys-
tem which uses a staged architecture (MacCartney
et al., 2006). After the linguistic analysis which
produces dependency graphs for premise and hy-
pothesis, the alignment stage creates links between
the nodes of the two dependency trees. In the infer-
ence stage, the system produces roughly 70 features
for the aligned premise-hypothesis pair, almost all
of which are implementations of “small linguistic
theories” whose activation indicates lexical, syn-
tactic and semantic matches and mismatches of
different types. The entailment decision is com-
puted using a logistic regression on these features.
The Stanford system supports the use of dif-
ferent aligners without touching the rest of the
pipeline. We compare two aligners: a one-to-one
aligner, which cannot construct MWE alignments
(UNIQ), and a many-to-many aligner (MANLI)
(MacCartney et al., 2008), which can. Both align-
ers use around 10 large-coverage lexical resources
of semantic similarities, both manually compiled
resources (such as WordNet and NomBank) and
automatically induced resources (such as Dekang
Lin’s distributional thesaurus or InfoMap).
UNIQ: A one-to-one aligner. UNIQ constructs
an alignment between dependency graphs as the
highest-scoring mapping from each word in the
hypothesis to one word in the premise, or to null.
Mappings are scored by summing the alignment
scores of all individual word pairs (provided by the
lexical resources), plus edge alignment scores that
</bodyText>
<page confidence="0.965031">
5
</page>
<bodyText confidence="0.99956655319149">
use the syntactic structure of premise and hypoth-
esis to introduce a bias for syntactic parallelism.
The large number of possible alignments (expo-
nential in the number of hypothesis words) makes
exhaustive search intractable. Instead, UNIQ uses a
stochastic search based on Gibbs sampling, a well-
known Markov Chain Monte Carlo technique (see
de Marneffe et al. (2007) for details).
Since it does not support many-to-many align-
ments, the UNIQ aligner cannot make use of the
multi-word information present in the paraphrase
resources. To be able to capture some common
MWEs, the Stanford RTE system was originally
designed with a facility to concatenate MWEs
present in WordNet into a single token (mostly
particle verbs and collocations, e.g., treat as or
foreign minister). However, we discovered that
WordNet collapsing always has a negative effect.
Inspection of the constructed alignments suggests
that the lexical resources that inform the alignment
process do not provide scores for most collapsed
tokens (such as wait for), and precision suffers.
MANLI: A phrase-to-phrase aligner. MANLI
aims at finding an optimal alignment between
phrases, defined as contiguous spans of one or mul-
tiple words. MANLI characterizes alignments as
edit scripts, sets of edits (substitutions, deletions,
and insertions) over phrases. The quality of an
edit script is the sum of the quality of the individ-
ual edit steps. Individual edits are scored using a
feature-based scoring function that takes edit type
and size into consideration.3 The score for substi-
tution edits also includes a lexical similarity score
similar to UNIQ, plus potential knowledge about
the semantic relatedness of multi-word phrases not
expressible in UNIQ. Substitution edits also use
contextual features, including a distortion score
and a matching-neighbors feature.4 Due to the
dependence between alignment and segmentation
decisions, MANLI uses a simulated annealing strat-
egy to traverse the resulting large search space.
Even though MANLI is our current best candi-
date at recovering MWE alignments, it currently
has an important architectural limitation: it works
on textual phrases rather than dependency tree frag-
ments, and therefore misses all MWEs that are not
contiguous (e.g., due to inserted articles or adver-
</bodyText>
<footnote confidence="0.9909545">
3Positive weights for all operation types ensure that
MANLI prefers small over large edits where appropriate.
4An adaptation of the averaged perceptron algorithm
(Collins, 2002) is used to tune the model parameters.
</footnote>
<table confidence="0.9993784">
micro-avg
P R F1
UNIQ w/o para 80.4 80.8 80.6
MANLI w/o para 77.0 85.5 81.0
w/ para 76.7 85.4 80.8
</table>
<tableCaption confidence="0.878385">
Table 4: Evaluation of aligners and resources
against the manual MSR RTE2 test annotations.
</tableCaption>
<bodyText confidence="0.9993825625">
bials). This accounts for roughly 9% of the MWEs
in RTE2 data. Other work on RTE has targeted
specifically this observation and has described para-
phrases on a dependency level (Marsi et al., 2007;
Dinu and Wang, 2009).
Setup. To set the parameters of the two models
(i.e., the weights for different lexical resources for
UNIQ, and the weights for the edit operation for
MANLI), we use the RTE2 development data. Test-
ing takes place on the RTE2 test and RTE4 datasets.
For MANLI, we performed this procedure twice,
with the paraphrase resources described in Sec-
tion 4 once deactivated and once activated. We
evaluated the output of the Stanford RTE system
both on the word alignment level, and on the entail-
ment decision level.
</bodyText>
<subsectionHeader confidence="0.998931">
5.2 Evaluation of Alignment Accuracy
</subsectionHeader>
<bodyText confidence="0.999871166666667">
The results for evaluating the MANLI and UNIQ
alignments against the manual alignment links in
the MSR RTE2 test set are given in Table 4. We
present micro-averaged numbers, where each align-
ment link counts equally (i.e., longer problems have
a larger impact). The overall difference is not large,
but MANLI produces a slightly better alignment.
The ability of MANLI to construct many-to-
many alignments is reflected in a different position
on the precision/recall curve: the MANLI aligner
is less precise than UNIQ, but has a higher recall.
Examples for UNIQ and MANLI alignments are
shown in Figures 1 and 2. A comparison of the
alignments shows the pattern to be expected from
Table 4: MANLI has a higher recall, but contains
occasional questionable links, such as at President
→ President in Figure 1.
However, the many-to-many alignments that
MANLI produces do not correspond well to the
MWE alignments. The overall impact of the para-
phrase resources is very small, and their addition
actually hurts MANLI’s performance slightly. A
more detailed analysis revealed two contrary trends.
On the one hand, the paraphrase resources provide
</bodyText>
<page confidence="0.999345">
6
</page>
<tableCaption confidence="0.9449195">
Table 5: Entailment recognition accuracy of the
Stanford system on RTE2 test (two-way task).
</tableCaption>
<table confidence="0.961267">
Aligner w/o para w/ para
UNIQ 63.3 –
MANLI 59.0 57.9
</table>
<tableCaption confidence="0.9267065">
Table 6: Entailment recognition accuracy of the
Stanford system on RTE4 (two-way task).
</tableCaption>
<bodyText confidence="0.998439714285714">
beneficial information, maybe surprisingly, in the
form of broad distributional similarities for single
words that were not available from the standard lex-
ical resources (e.g., the alignment “the company’s
letter” → “the company’s certificate”).
On the other hand, MANLI captures not one of
the true MWEs identified in the MSR data. It only
finds two many-to-many alignments which belong
to the CP category: aimed criticism → has criti-
cised, European currency → euro currency. We
see this as the practical consequences of our ob-
servation from Section 4: The scores in current
paraphrase resources are too noisy to support accu-
rate MWE recognition (cf. Table 3).
</bodyText>
<subsectionHeader confidence="0.997545">
5.3 Evaluation of Entailment Recognition
</subsectionHeader>
<bodyText confidence="0.999865120689655">
We finally evaluated the performance of the Stan-
ford system using UNIQ and MANLI alignments
on the entailment task. We consider two datasets:
RTE2 test, the alignment evaluation dataset, and
the most recent RTE4 dataset, where current num-
bers for the Stanford system are available from last
year’s Text Analysis Conference (TAC).
A reasonable conjecture would be that better
alignments translate into better entailment recog-
nition. However, as the results in Tables 5 and 6
show, this is not the case. Overall, UNIQ outper-
forms MANLI by several percent accuracy despite
MANLI’s better alignments. This “baseline” differ-
ence should not be overinterpreted, since it may be
setup-specific: the features computed in the infer-
ence stage of the Stanford system were developed
mainly with the UNIQ aligner in mind. A more sig-
nificant result is that the integration of paraphrase
knowledge in MANLI has no effect on RTE2 test,
and even decreases performance on RTE4.
The general picture that we observe is that
there is only a loose coupling between alignments
and the entailment decision: individual align-
ments seldom matter. This is shown, for exam-
ple, by the alignments in Figures 1 and 2. Even
though MANLI provides a better overall alignment,
UNIQ’s alignment is “good enough” for entailment
purposes. In Figure 1, the two words UNIQ leaves
unaligned are a preposition (at) and a light verb
(aimed), both of which are not critical to determine
whether or not the premise entails the hypothesis.
This interpretation is supported by another analy-
sis, where we tested whether entailments involving
at least one true MWE are more difficult to rec-
ognize. We computed the entailment accuracy for
all applicable RTE2 test pairs (7%, 58 sentences).
The accuracy on this subset is 62% for the MANLI
model without paraphrases, 64% for the MANLI
model with paraphrases, and 74% for UNIQ. The
differences from the numbers in Table 5 are not
significant due to the small size of the MWE sam-
ple, but we observe that the accuracy on the MWE
subset tends to be higher than on the whole set
(rather than lower). Futhermore, even though we fi-
nally see a small beneficial effect of paraphrases on
the MANLI aligner, the UNIQ aligner, which com-
pletely ignores MWEs, still performs substantially
better.
Our conclusion is that wrong entailment deci-
sions rarely hinge on wrongly aligned MWEs, at
least with a probabilistic architecture like the Stan-
ford system. Consequently, it suffices to recover
the most crucial alignment links to predict entail-
ment, and the benefits associated with the use of
a more restricted alignment formulation, like the
one-to-one alignment formulation of UNIQ, out-
weighs those of more powerful alignment models,
like MANLI’s phrasal alignments.
</bodyText>
<sectionHeader confidence="0.996902" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999853230769231">
We have investigated the influence of multi-word
expressions on the task of recognizing textual en-
tailment. In contrast to the widely held view that
proper treatment of MWEs could bring about a sub-
stantial improvement in NLP tasks, we found that
the importance of MWEs in RTE is rather small.
Among the MWEs that we identified in the align-
ments, more than half can be captured by one-to-
one alignments, and should not pose problems for
entailment recognition.
Furthermore, we found that the remaining
MWEs are rather difficult to model faithfully. The
MSR MWEs are poorly represented in state-of-the-
</bodyText>
<figure confidence="0.996896666666667">
UNIQ
63.8
Aligner
w/o para w/ para
MANLI
60.6 60.6
TAC system
61.4
57.0
</figure>
<page confidence="0.624508">
7
</page>
<figure confidence="0.988472555555556">
Former Former
South South
African African
President President
aimed aimed
criticism criticism
at at
President President
Bush Bush
</figure>
<figureCaption confidence="0.9959805">
Figure 1: UNIQ (left) and MANLI (right) alignments for problem 483 in RTE2 test. The rows represent
the hypothesis words, and the columns the premise words.
</figureCaption>
<figure confidence="0.9946793">
Those
who
recovered
from
Sars
might
have
permanent
lung
damage
Those
who
recovered
from
Sars
might
have
permanent
lung
damage
</figure>
<figureCaption confidence="0.999875">
Figure 2: UNIQ (left) and MANLI (right) alignments for problem 1 in RTE2 test.
</figureCaption>
<bodyText confidence="0.999985">
art lexical resources, and when they are present,
scoring issues arise. Consequently, at least in
the Stanford system, the integration of paraphrase
knowledge to enable MWE recognition has made
almost no difference either in terms of alignment
accuracy nor in entailment accuracy. Furthermore,
it is not the case that entailment recognition accu-
racy is worse for sentences with “true” MWEs. In
sum, we find that even though capturing and repre-
senting MWEs is an interesting problem in itself,
MWEs do not seem to be such a pain in the neck –
at least not for textual entailment.
Our results may seem to contradict the results
of many previous RTE studies such as (Bar-Haim
et al., 2005) which found paraphrases to make an
important contribution. However, the beneficial ef-
fect of paraphrases found in these studies refers not
to an alignment task, but to the ability of relating
lexico-syntactic reformulations such as diathesis
alternations or symmetrical predicates (buy/sell).
In the Stanford system, this kind of knowledge
is already present in the features of the inference
stage. Our results should therefore rather be seen
as a clarification of the complementary nature of
the paraphrase and MWE issues.
In our opinion, there is much more potential
for improvement from better estimates of semantic
similarity. This is true for phrasal similarity, as our
negative results for multi-word paraphrases show,
but also on the single-word level. The 2% gain
in accuracy for the Stanford system here over the
reported TAC RTE4 results stems merely from ef-
forts to clean up and rescale the lexical resources
used by the system, and outweighs the effect of
MWEs. One possible direction of research is con-
ditioning semantic similarity on context: Most cur-
rent lexical resources characterize similarity at the
lemma level, but true similarities of word or phrase
pairs are strongly context-dependent: obtain and
be awarded are much better matches in the context
of a degree than in the context of data.
</bodyText>
<sectionHeader confidence="0.998384" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999834428571429">
We thank Bill MacCartney for his help with the
MANLI aligner, and Michel Galley for the parallel
corpus-based paraphrase resource. This paper is
based on work funded in part by DARPA through
IBM. The content does not necessarily reflect the
views of the U.S. Government, and no official en-
dorsement should be inferred.
</bodyText>
<page confidence="0.99738">
8
</page>
<sectionHeader confidence="0.996394" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999648495495496">
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of the 43rd Annual Meeting of the Association
for Computational Linguistics, pages 597–604, Ann
Arbor, MI.
Roy Bar-Haim, Idan Szpecktor, and Oren Glickman.
2005. Definition and analysis of intermediate entail-
ment levels. In Proceedings of the ACL Workshop on
Empirical Modeling of Semantic Equivalence and
Entailment, pages 55–60, Ann Arbor, MI.
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,
Danilo Giampiccolo, Bernardo Magnini, and Idan
Szpektor. 2006. The second PASCAL recognising
textual entailment challenge. In Proceedings of the
Second PASCAL Challenges Workshop on Recognis-
ing Textual Entailment, Venice, Italy.
Francis Bond, Anna Korhonen, Diana McCarthy, and
Aline Villavicencio, editors. 2003. Proceedings of
the ACL 2003 workshop on multiword expressions:
Analysis, acquisition and treatment.
Chris Brockett. 2007. Aligning the RTE 2006 corpus.
Technical Report MSR-TR-2007-77, Microsoft Re-
search.
Peter F. Brown, Stephen Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathe-
matic of statistical machine translation: Parameter
estimation. Computational Linguistics, 19(2):263–
311.
Aljoscha Burchardt, Nils Reiter, Stefan Thater, and
Anette Frank. 2007. A semantic approach to tex-
tual entailment: System evaluation and task analy-
sis. In Proceedings of the ACL-PASCAL Workshop
on Textual Entailment and Paraphrasing, pages 10–
15, Prague, Czech Republic.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL recognising textual entail-
ment challenge. In J. Quinonero-Candela, I. Da-
gan, B. Magnini, and F. d’Alch Buc, editors, Ma-
chine Learning Challenges. Lecture Notes in Com-
puter Science, Vol. 3944, pages 177–190. Springer.
Marie-Catherine de Marneffe, Trond Grenager, Bill
MacCartney, Daniel Cer, Daniel Ramage, Chlo´e
Kiddon, and Christopher D. Manning. 2007. Align-
ing semantic graphs for textual inference and ma-
chine reading. In Proceedings of the AAAI Spring
Symposium.
Georgiana Dinu and Rui Wang. 2009. Inference rules
and their application to recognizing textual entail-
ment. In Proceedings of the 12th Conference of the
European Chapter of the ACL (EACL 2009), pages
211–219, Athens, Greece.
Nicole Gr´egoire, Stefan Evert, and Su Nam Kim, edi-
tors. 2007. Proceedings of the ACL workshop: A
broader perspective on multiword expressions.
Andrew Hickl and Jeremy Bensley. 2007. A discourse
commitment-based framework for recognizing tex-
tual entailment. In Proceedings of the ACL-PASCAL
Workshop on Textual Entailment and Paraphrasing,
pages 171–176, Prague, Czech Republic.
Adrian Iftene and Alexandra Balahur-Dobrescu. 2007.
Hypothesis transformation and semantic variability
rules used in recognizing textual entailment. In
Proceedings of the ACL-PASCAL Workshop on Tex-
tual Entailment and Paraphrasing, pages 125–130,
Prague, Czech Republic.
Dekang Lin and Patrick Pantel. 2002. Discovery of
inference rules for question answering. Journal of
Natural Language Engineering, 7(4):343–360.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the joint An-
nual Meeting of the Association for Computational
Linguistics and International Conference on Com-
putational Linguistics, pages 768–774, Montr´eal,
Canada.
Bill MacCartney, Trond Grenager, Marie-Catherine
de Marneffe, Daniel Cer, and Christopher D. Man-
ning. 2006. Learning to recognize features of valid
textual entailments. In Proceedings of NAACL.
Bill MacCartney, Michel Galley, and Christopher D.
Manning. 2008. A phrase-based alignment model
for natural language inference. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing, Honolulu, Hawaii.
Erwin Marsi, Emiel Krahmer, and Wauter Bosma.
2007. Dependency-based paraphrasing for recogniz-
ing textual entailment. In Proceedings of the ACL-
PASCAL Workshop on Textual Entailment and Para-
phrasing, pages 83–88, Prague, Czech Republic.
Begona Villada Moir´on, Aline Villavicencio, Diana
McCarthy, Stefan Evert, and Suzanne Stevenson, ed-
itors. 2006. Proceedings of the ACL Workshop on
Multiword Expressions: Identifying and Exploiting
Underlying Properties.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multi-word
expressions: a pain in the neck for NLP. In Proceed-
ings of CICLing.
Takaaki Tanaka, Aline Villavicencio, Francis Bond,
and Anna Korhonen, editors. 2004. Proceedings of
the second ACL workshop on multiword expressions:
Integrating processing.
Fabio Massimo Zanzotto, Marco Pennacchiotti, and
Alessandro Moschitti. 2007. Shallow semantic in
fast textual entailment rule learners. In Proceed-
ings of the ACL-PASCAL Workshop on Textual En-
tailment and Paraphrasing, pages 72–77, Prague,
Czech Republic.
</reference>
<page confidence="0.997104">
9
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.771134">In RTE2 dev and test, we find only 81 and 60</note>
<abstract confidence="0.997077582278482">true MWEs, respectively. Out of the 1600 sentence pairs in the two datasets, 8.2% involve true MWEs (73 in RTE2 dev and 58 in RTE2 test). On the level of word alignments, the ratio is even smaller: only of all involve true MWEs. Furthermore, more than half of them are decomposable (MWEH/CP). Some examples from this category are (“heads” marked in boldface): lawsuits against cancer doll awarded with director forces particular when light verbs are involved or when modification adds just minor aspects we argue that it is sufficient to align the left-hand expression to the “head” in order to decide entailment. Consider, in contrast, these examples from the non-decomposable categories (MWENH/NCP): candidate their lives ship its history husband and women These cases span a broad range of linguistic relafrom pure associations collective expressions and Arguably, in these cases aligning the left-hand word to any single word on the right can seriously throw off an entailment recognition system. However, they are fairly rare, occurring only in 65 out of 1600 sentences. 3.3 Conclusions from the MSR Analysis Our analysis has found that 8% of the sentences in the MSR dataset involve true MWEs. At the level, the fraction of true MWEs of all alignment links is just over 1%. Of course, if errors in the alignment of these MWEs had a high probability to lead to entailment recognition errors, MWEs would still constitute a major factor in determining entailment. However, we have argued that about half of the true MWEs that is, the part of the alignment that is crucial for entailment can be recovered with a one-to-one alignment link that can be identified even by very limited alignment models. This leaves considerably less than 1% of all word (or of sentence pairs) where im- MWE alignments able at all exert a negative influence on entailment. However, this is just an upper bound – their impact is by no means guaranteed. Thus, our conclusion from the annotation study is that we do not expect MWEs to play a large role in actual entailment recognition. 4 MWEs in Paraphrase Resources Before we come to actual experiments on the automatic recognition of MWEs in a practical RTE system, we need to consider the prerequisites for this task. As mentioned in Section 2, if an RTE system is to establish multi-word alignments, it requires a knowledge source that provides accurate semantic similarity judgments for “many-to-many” punishment – death well as for “one-to-many” alignments – Such similarities are not present in standard lexical resources like WordNet or Dekang Lin’s thesaurus (Lin, 1998). The best class of candidate resources to provide wide-coverage of multi-word similarities seems to In this section, we examine to what extent two of the most widely used paraphrase resource types provide supporting evidence for the true MWEs in the MSR data. We deliberately use corpus-derived, noisy resources, since we are interested in the real-world (rather than idealized) prospects for accurate MWE alignment. paraphrases. and Pantel (2002)’s DIRT model collects lexicalized dependency paths with two slots at either end. Paths with similar distributions over slot fillers count as paraphrases, with the quality measured by a mutual information-based similarity over the slot fillers. The outcome of their study is the DIRT database which lists paraphrases for around 230,000 dependency paths, extracted from about 1 GB of miscellaneous newswire text. We converted the DIRT into a resource of semantic similarities between raw text phrases. We used a heuristic mapping from dependency relations to word order, and obtained similarity ratings by rescaling the DIRT paraphrase ratings, which are based on a mutual information-based measure of filler similarity, onto the range [0,1]. thank Patrick Pantel for granting us access to DIRT. 4 corpora-based paraphrases. alternative approach to paraphrase acquisition was proposed by Bannard and Callison-Burch (2005). It exploits the variance inherent in translation to extract paraphrases from bilingual parallel corpora. Concretely, it observes translational relationships between a source and a target language and pairs up source language phrases with other source language phrases that translate into the same target language phrases. We applied this method to the large Chinese-English GALE MT evaluation corpus GB text per language, mostly newswire). The large number of translations makes it impractical to store all observed paraphrases. We therefore filtered the list of paraphrases against the raw text of the RTE corpora, acquiring the 10 best paraphrases for around 100,000 twoand threeword phrases. The MLE conditional probabilities were scaled onto [0,1] for each target. checked the two resources for the presence of the true MWEs identified in the MSR data. We found that overall 34% of the MWEs appear in these resources, with more decomposable MWEs (MWEH/CP) than non-decomposable ones (MWENH/NCP) (42.1% vs. 24.6%). However, we find that almost all of the MWEs that are covered by the paraphrase resources are assigned very low scores, while erroneous paraphrases (expressions with clearly different meanings) have higher scores. is illustrated in Table 3 for the case of which is aligned to few one RTE2 sentence. This paraphrase is on the list, but with a lower similarity than unsuitable paraphrases as This problem is widespread. Other examples of low-scoring paraare: step measures a position or The noise in the rankings means that any alignment algorithm faces a dilemma: either it uses a high threshold and misses valid MWE alignments, or it lowers its threshold and risks constructing incorrect alignments. 5 Impact of MWEs on Practical Entailment Recognition This section provides the final step in our study: an evaluation of the impact of MWEs on entailment recognition in a current RTE system, and of the benefits of explicit MWE alignment. While the poorly represented represented 0.42 poorly 0.07 rarely 0.06 good 0.05 representatives 0.04 very few 0.04 well 0.02 representative 0.01 Table 3: Paraphrases of “poorly represented” with scores (semantic similarities). results of this experiment are not guaranteed to transfer to other RTE system architectures, or to future, improved paraphrase resources, it provides a current snapshot of the practical impact of MWE handling. 5.1 The Stanford RTE System We base our experiments on the Stanford RTE system which uses a staged architecture (MacCartney et al., 2006). After the linguistic analysis which produces dependency graphs for premise and hypothesis, the alignment stage creates links between the nodes of the two dependency trees. In the inference stage, the system produces roughly 70 features for the aligned premise-hypothesis pair, almost all of which are implementations of “small linguistic theories” whose activation indicates lexical, syntactic and semantic matches and mismatches of different types. The entailment decision is computed using a logistic regression on these features. The Stanford system supports the use of different aligners without touching the rest of the pipeline. We compare two aligners: a one-to-one aligner, which cannot construct MWE alignments (UNIQ), and a many-to-many aligner (MANLI) (MacCartney et al., 2008), which can. Both aligners use around 10 large-coverage lexical resources of semantic similarities, both manually compiled resources (such as WordNet and NomBank) and automatically induced resources (such as Dekang Lin’s distributional thesaurus or InfoMap). A one-to-one aligner. constructs an alignment between dependency graphs as the highest-scoring mapping from each word in the hypothesis to one word in the premise, or to null. Mappings are scored by summing the alignment scores of all individual word pairs (provided by the lexical resources), plus edge alignment scores that 5 use the syntactic structure of premise and hypothesis to introduce a bias for syntactic parallelism. The large number of possible alignments (exponential in the number of hypothesis words) makes exhaustive search intractable. Instead, UNIQ uses a stochastic search based on Gibbs sampling, a wellknown Markov Chain Monte Carlo technique (see de Marneffe et al. (2007) for details). Since it does not support many-to-many alignments, the UNIQ aligner cannot make use of the multi-word information present in the paraphrase resources. To be able to capture some common MWEs, the Stanford RTE system was originally designed with a facility to concatenate MWEs present in WordNet into a single token (mostly verbs and collocations, e.g., as However, we discovered that WordNet collapsing always has a negative effect. Inspection of the constructed alignments suggests that the lexical resources that inform the alignment process do not provide scores for most collapsed (such as and precision suffers. A phrase-to-phrase aligner. aims at finding an optimal alignment between phrases, defined as contiguous spans of one or multiple words. MANLI characterizes alignments as sets of edits (substitutions, deletions, and insertions) over phrases. The quality of an edit script is the sum of the quality of the individual edit steps. Individual edits are scored using a feature-based scoring function that takes edit type size into The score for substitution edits also includes a lexical similarity score similar to UNIQ, plus potential knowledge about the semantic relatedness of multi-word phrases not expressible in UNIQ. Substitution edits also use contextual features, including a distortion score a matching-neighbors Due to the dependence between alignment and segmentation decisions, MANLI uses a simulated annealing strategy to traverse the resulting large search space. Even though MANLI is our current best candidate at recovering MWE alignments, it currently has an important architectural limitation: it works on textual phrases rather than dependency tree fragments, and therefore misses all MWEs that are not (e.g., due to inserted articles or adverweights for all operation types ensure that MANLI prefers small over large edits where appropriate. adaptation of the averaged perceptron algorithm (Collins, 2002) is used to tune the model parameters. micro-avg R UNIQ w/o para 80.4 80.8 80.6 MANLI w/o para w/ para 77.0 85.5 81.0 76.7 85.4 80.8 Table 4: Evaluation of aligners and resources against the manual MSR RTE2 test annotations. bials). This accounts for roughly 9% of the MWEs in RTE2 data. Other work on RTE has targeted specifically this observation and has described paraphrases on a dependency level (Marsi et al., 2007; Dinu and Wang, 2009). set the parameters of the two models (i.e., the weights for different lexical resources for UNIQ, and the weights for the edit operation for MANLI), we use the RTE2 development data. Testing takes place on the RTE2 test and RTE4 datasets. For MANLI, we performed this procedure twice, with the paraphrase resources described in Section 4 once deactivated and once activated. We evaluated the output of the Stanford RTE system both on the word alignment level, and on the entailment decision level. 5.2 Evaluation of Alignment Accuracy The results for evaluating the MANLI and UNIQ alignments against the manual alignment links in the MSR RTE2 test set are given in Table 4. We present micro-averaged numbers, where each alignment link counts equally (i.e., longer problems have a larger impact). The overall difference is not large, but MANLI produces a slightly better alignment. The ability of MANLI to construct many-tomany alignments is reflected in a different position on the precision/recall curve: the MANLI aligner is less precise than UNIQ, but has a higher recall. Examples for UNIQ and MANLI alignments are shown in Figures 1 and 2. A comparison of the alignments shows the pattern to be expected from Table 4: MANLI has a higher recall, but contains questionable links, such as President Figure 1. However, the many-to-many alignments that MANLI produces do not correspond well to the MWE alignments. The overall impact of the paraphrase resources is very small, and their addition actually hurts MANLI’s performance slightly. A more detailed analysis revealed two contrary trends. On the one hand, the paraphrase resources provide 6 Table 5: Entailment recognition accuracy of the Stanford system on RTE2 test (two-way task). Aligner w/o para w/ para UNIQ 63.3 MANLI 59.0 57.9 Table 6: Entailment recognition accuracy of the Stanford system on RTE4 (two-way task). beneficial information, maybe surprisingly, in the of broad distributional similarities for words that were not available from the standard lexical resources (e.g., the alignment “the company’s company’s On the other hand, MANLI captures not one of the true MWEs identified in the MSR data. It only finds two many-to-many alignments which belong the CP category: criticism criticurrency We see this as the practical consequences of our observation from Section 4: The scores in current paraphrase resources are too noisy to support accurate MWE recognition (cf. Table 3). 5.3 Evaluation of Entailment Recognition We finally evaluated the performance of the Stanford system using UNIQ and MANLI alignments on the entailment task. We consider two datasets: RTE2 test, the alignment evaluation dataset, and the most recent RTE4 dataset, where current numbers for the Stanford system are available from last year’s Text Analysis Conference (TAC). A reasonable conjecture would be that better alignments translate into better entailment recognition. However, as the results in Tables 5 and 6 show, this is not the case. Overall, UNIQ outperforms MANLI by several percent accuracy despite MANLI’s better alignments. This “baseline” difference should not be overinterpreted, since it may be setup-specific: the features computed in the inference stage of the Stanford system were developed mainly with the UNIQ aligner in mind. A more significant result is that the integration of paraphrase knowledge in MANLI has no effect on RTE2 test, and even decreases performance on RTE4. The general picture that we observe is that there is only a loose coupling between alignments and the entailment decision: individual alignments seldom matter. This is shown, for example, by the alignments in Figures 1 and 2. Even though MANLI provides a better overall alignment, UNIQ’s alignment is “good enough” for entailment purposes. In Figure 1, the two words UNIQ leaves are a preposition and a light verb both of which are not critical to determine whether or not the premise entails the hypothesis. This interpretation is supported by another analysis, where we tested whether entailments involving at least one true MWE are more difficult to recognize. We computed the entailment accuracy for all applicable RTE2 test pairs (7%, 58 sentences). The accuracy on this subset is 62% for the MANLI model without paraphrases, 64% for the MANLI model with paraphrases, and 74% for UNIQ. The differences from the numbers in Table 5 are not significant due to the small size of the MWE sample, but we observe that the accuracy on the MWE tends to be on the whole set (rather than lower). Futhermore, even though we finally see a small beneficial effect of paraphrases on the MANLI aligner, the UNIQ aligner, which completely ignores MWEs, still performs substantially better. Our conclusion is that wrong entailment decisions rarely hinge on wrongly aligned MWEs, at least with a probabilistic architecture like the Stanford system. Consequently, it suffices to recover the most crucial alignment links to predict entailment, and the benefits associated with the use of more formulation, like the one-to-one alignment formulation of UNIQ, outweighs those of more powerful alignment models, like MANLI’s phrasal alignments. 6 Conclusions We have investigated the influence of multi-word expressions on the task of recognizing textual entailment. In contrast to the widely held view that proper treatment of MWEs could bring about a substantial improvement in NLP tasks, we found that the importance of MWEs in RTE is rather small. Among the MWEs that we identified in the alignments, more than half can be captured by one-toone alignments, and should not pose problems for entailment recognition. Furthermore, we found that the remaining MWEs are rather difficult to model faithfully. The MWEs are poorly represented in state-of-the- UNIQ 63.8 Aligner w/o para w/ para MANLI 60.6 60.6 TAC system 61.4 57.0 7 Former South African President aimed criticism at President Bush Former South African President aimed criticism at President Bush Figure 1: UNIQ (left) and MANLI (right) alignments for problem 483 in RTE2 test. The rows represent the hypothesis words, and the columns the premise words. Those who recovered from Sars might have permanent lung damage Those who recovered from Sars might have permanent lung damage Figure 2: UNIQ (left) and MANLI (right) alignments for problem 1 in RTE2 test. art lexical resources, and when they are present, scoring issues arise. Consequently, at least in the Stanford system, the integration of paraphrase knowledge to enable MWE recognition has made almost no difference either in terms of alignment accuracy nor in entailment accuracy. Furthermore, it is not the case that entailment recognition accuracy is worse for sentences with “true” MWEs. In sum, we find that even though capturing and representing MWEs is an interesting problem in itself, MWEs do not seem to be such a pain in the neck – at least not for textual entailment. Our results may seem to contradict the results of many previous RTE studies such as (Bar-Haim et al., 2005) which found paraphrases to make an important contribution. However, the beneficial effect of paraphrases found in these studies refers not to an alignment task, but to the ability of relating such as diathesis or symmetrical predicates In the Stanford system, this kind of knowledge is already present in the features of the inference stage. Our results should therefore rather be seen as a clarification of the complementary nature of the paraphrase and MWE issues. In our opinion, there is much more potential for improvement from better estimates of semantic similarity. This is true for phrasal similarity, as our negative results for multi-word paraphrases show, but also on the single-word level. The 2% gain in accuracy for the Stanford system here over the reported TAC RTE4 results stems merely from efforts to clean up and rescale the lexical resources used by the system, and outweighs the effect of MWEs. One possible direction of research is consemantic similarity on Most current lexical resources characterize similarity at the lemma level, but true similarities of word or phrase are strongly context-dependent: awarded much better matches in the context degree in the context of Acknowledgments We thank Bill MacCartney for his help with the MANLI aligner, and Michel Galley for the parallel corpus-based paraphrase resource. This paper is based on work funded in part by DARPA through IBM. The content does not necessarily reflect the views of the U.S. Government, and no official endorsement should be inferred.</abstract>
<note confidence="0.786444066666667">8 References Colin Bannard and Chris Callison-Burch. 2005. Parawith bilingual parallel corpora. In Proceedings of the 43rd Annual Meeting of the Association Computational pages 597–604, Ann Arbor, MI. Roy Bar-Haim, Idan Szpecktor, and Oren Glickman. 2005. Definition and analysis of intermediate entaillevels. In of the ACL Workshop on Empirical Modeling of Semantic Equivalence and pages 55–60, Ann Arbor, MI. Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan Szpektor. 2006. The second PASCAL recognising entailment challenge. In of the Second PASCAL Challenges Workshop on Recognis- Textual Venice, Italy. Francis Bond, Anna Korhonen, Diana McCarthy, and Villavicencio, editors. 2003. of the ACL 2003 workshop on multiword expressions: acquisition and Chris Brockett. 2007. Aligning the RTE 2006 corpus. Technical Report MSR-TR-2007-77, Microsoft Research. Peter F. Brown, Stephen Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematic of statistical machine translation: Parameter 19(2):263– 311. Aljoscha Burchardt, Nils Reiter, Stefan Thater, and Anette Frank. 2007. A semantic approach to textual entailment: System evaluation and task analy- In of the ACL-PASCAL Workshop Textual Entailment and pages 10– 15, Prague, Czech Republic. Michael Collins. 2002. Discriminative training methfor hidden Markov models. In of the Conference on Empirical Methods in Natural Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006. The PASCAL recognising textual entailment challenge. In J. Quinonero-Candela, I. Da- B. Magnini, and F. d’Alch Buc, editors, Machine Learning Challenges. Lecture Notes in Com- Science, Vol. pages 177–190. Springer.</note>
<affiliation confidence="0.278473">Marie-Catherine de Marneffe, Trond Grenager, Bill</affiliation>
<address confidence="0.39023">MacCartney, Daniel Cer, Daniel Ramage, Chlo´e</address>
<abstract confidence="0.745043733333333">Kiddon, and Christopher D. Manning. 2007. Aligning semantic graphs for textual inference and mareading. In of the AAAI Spring Georgiana Dinu and Rui Wang. 2009. Inference rules and their application to recognizing textual entail- In of the 12th Conference of the Chapter of the ACL (EACL pages 211–219, Athens, Greece. Nicole Gr´egoire, Stefan Evert, and Su Nam Kim, edi- 2007. of the ACL workshop: A perspective on multiword Andrew Hickl and Jeremy Bensley. 2007. A discourse commitment-based framework for recognizing texentailment. In of the ACL-PASCAL on Textual Entailment and</abstract>
<note confidence="0.66426896969697">pages 171–176, Prague, Czech Republic. Adrian Iftene and Alexandra Balahur-Dobrescu. 2007. Hypothesis transformation and semantic variability rules used in recognizing textual entailment. In Proceedings of the ACL-PASCAL Workshop on Tex- Entailment and pages 125–130, Prague, Czech Republic. Dekang Lin and Patrick Pantel. 2002. Discovery of rules for question answering. of Language 7(4):343–360. Dekang Lin. 1998. Automatic retrieval and clustering similar words. In of the joint Annual Meeting of the Association for Computational Linguistics and International Conference on Compages 768–774, Montr´eal, Canada. Bill MacCartney, Trond Grenager, Marie-Catherine de Marneffe, Daniel Cer, and Christopher D. Manning. 2006. Learning to recognize features of valid entailments. In of Bill MacCartney, Michel Galley, and Christopher D. Manning. 2008. A phrase-based alignment model natural language inference. In of the Conference on Empirical Methods in Natural Honolulu, Hawaii. Erwin Marsi, Emiel Krahmer, and Wauter Bosma. 2007. Dependency-based paraphrasing for recogniztextual entailment. In of the ACL- PASCAL Workshop on Textual Entailment and Parapages 83–88, Prague, Czech Republic. Begona Villada Moir´on, Aline Villavicencio, Diana McCarthy, Stefan Evert, and Suzanne Stevenson, ed- 2006. of the ACL Workshop on</note>
<title confidence="0.968418">Multiword Expressions: Identifying and Exploiting</title>
<author confidence="0.964622">Ivan A Sag</author>
<author confidence="0.964622">Timothy Baldwin</author>
<author confidence="0.964622">Francis Bond</author>
<author confidence="0.964622">Ann</author>
<note confidence="0.9408478">Copestake, and Dan Flickinger. 2002. Multi-word a pain in the neck for NLP. In Proceedof Takaaki Tanaka, Aline Villavicencio, Francis Bond, Anna Korhonen, editors. 2004. of</note>
<title confidence="0.446475">the second ACL workshop on multiword expressions:</title>
<author confidence="0.772506">Shallow semantic in</author>
<affiliation confidence="0.608092333333333">textual entailment rule learners. In Proceedings of the ACL-PASCAL Workshop on Textual Enand pages 72–77, Prague,</affiliation>
<address confidence="0.85016">Czech Republic.</address>
<intro confidence="0.611377">9</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Colin Bannard</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Paraphrasing with bilingual parallel corpora.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>597--604</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="4517" citStr="Bannard and Callison-Burch (2005)" startWordPosition="728" endWordPosition="731"> for around 230,000 dependency paths, extracted from about 1 GB of miscellaneous newswire text. We converted the DIRT paraphrases2 into a resource of semantic similarities between raw text phrases. We used a heuristic mapping from dependency relations to word order, and obtained similarity ratings by rescaling the DIRT paraphrase ratings, which are based on a mutual information-based measure of filler similarity, onto the range [0,1]. 2We thank Patrick Pantel for granting us access to DIRT. 4 Parallel corpora-based paraphrases. An alternative approach to paraphrase acquisition was proposed by Bannard and Callison-Burch (2005). It exploits the variance inherent in translation to extract paraphrases from bilingual parallel corpora. Concretely, it observes translational relationships between a source and a target language and pairs up source language phrases with other source language phrases that translate into the same target language phrases. We applied this method to the large Chinese-English GALE MT evaluation P3/P3.5 corpus (-2 GB text per language, mostly newswire). The large number of translations makes it impractical to store all observed paraphrases. We therefore filtered the list of paraphrases against the</context>
</contexts>
<marker>Bannard, Callison-Burch, 2005</marker>
<rawString>Colin Bannard and Chris Callison-Burch. 2005. Paraphrasing with bilingual parallel corpora. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 597–604, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Bar-Haim</author>
<author>Idan Szpecktor</author>
<author>Oren Glickman</author>
</authors>
<title>Definition and analysis of intermediate entailment levels.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment,</booktitle>
<pages>55--60</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="18796" citStr="Bar-Haim et al., 2005" startWordPosition="3005" endWordPosition="3008"> arise. Consequently, at least in the Stanford system, the integration of paraphrase knowledge to enable MWE recognition has made almost no difference either in terms of alignment accuracy nor in entailment accuracy. Furthermore, it is not the case that entailment recognition accuracy is worse for sentences with “true” MWEs. In sum, we find that even though capturing and representing MWEs is an interesting problem in itself, MWEs do not seem to be such a pain in the neck – at least not for textual entailment. Our results may seem to contradict the results of many previous RTE studies such as (Bar-Haim et al., 2005) which found paraphrases to make an important contribution. However, the beneficial effect of paraphrases found in these studies refers not to an alignment task, but to the ability of relating lexico-syntactic reformulations such as diathesis alternations or symmetrical predicates (buy/sell). In the Stanford system, this kind of knowledge is already present in the features of the inference stage. Our results should therefore rather be seen as a clarification of the complementary nature of the paraphrase and MWE issues. In our opinion, there is much more potential for improvement from better es</context>
</contexts>
<marker>Bar-Haim, Szpecktor, Glickman, 2005</marker>
<rawString>Roy Bar-Haim, Idan Szpecktor, and Oren Glickman. 2005. Definition and analysis of intermediate entailment levels. In Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pages 55–60, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Bar-Haim</author>
<author>Ido Dagan</author>
<author>Bill Dolan</author>
<author>Lisa Ferro</author>
<author>Danilo Giampiccolo</author>
<author>Bernardo Magnini</author>
<author>Idan Szpektor</author>
</authors>
<title>The second PASCAL recognising textual entailment challenge.</title>
<date>2006</date>
<booktitle>In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment,</booktitle>
<location>Venice, Italy.</location>
<marker>Bar-Haim, Dagan, Dolan, Ferro, Giampiccolo, Magnini, Szpektor, 2006</marker>
<rawString>Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan Szpektor. 2006. The second PASCAL recognising textual entailment challenge. In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment, Venice, Italy.</rawString>
</citation>
<citation valid="true">
<date>2003</date>
<booktitle>Proceedings of the ACL 2003 workshop on multiword expressions: Analysis, acquisition and treatment.</booktitle>
<editor>Francis Bond, Anna Korhonen, Diana McCarthy, and Aline Villavicencio, editors.</editor>
<marker>2003</marker>
<rawString>Francis Bond, Anna Korhonen, Diana McCarthy, and Aline Villavicencio, editors. 2003. Proceedings of the ACL 2003 workshop on multiword expressions: Analysis, acquisition and treatment.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Brockett</author>
</authors>
<title>Aligning the RTE</title>
<date>2007</date>
<tech>Technical Report MSR-TR-2007-77, Microsoft Research.</tech>
<marker>Brockett, 2007</marker>
<rawString>Chris Brockett. 2007. Aligning the RTE 2006 corpus. Technical Report MSR-TR-2007-77, Microsoft Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematic of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>311</pages>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematic of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263– 311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aljoscha Burchardt</author>
<author>Nils Reiter</author>
<author>Stefan Thater</author>
<author>Anette Frank</author>
</authors>
<title>A semantic approach to textual entailment: System evaluation and task analysis.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing,</booktitle>
<pages>10--15</pages>
<location>Prague, Czech Republic.</location>
<marker>Burchardt, Reiter, Thater, Frank, 2007</marker>
<rawString>Aljoscha Burchardt, Nils Reiter, Stefan Thater, and Anette Frank. 2007. A semantic approach to textual entailment: System evaluation and task analysis. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, pages 10– 15, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden Markov models.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="11110" citStr="Collins, 2002" startWordPosition="1742" endWordPosition="1743">endence between alignment and segmentation decisions, MANLI uses a simulated annealing strategy to traverse the resulting large search space. Even though MANLI is our current best candidate at recovering MWE alignments, it currently has an important architectural limitation: it works on textual phrases rather than dependency tree fragments, and therefore misses all MWEs that are not contiguous (e.g., due to inserted articles or adver3Positive weights for all operation types ensure that MANLI prefers small over large edits where appropriate. 4An adaptation of the averaged perceptron algorithm (Collins, 2002) is used to tune the model parameters. micro-avg P R F1 UNIQ w/o para 80.4 80.8 80.6 MANLI w/o para 77.0 85.5 81.0 w/ para 76.7 85.4 80.8 Table 4: Evaluation of aligners and resources against the manual MSR RTE2 test annotations. bials). This accounts for roughly 9% of the MWEs in RTE2 data. Other work on RTE has targeted specifically this observation and has described paraphrases on a dependency level (Marsi et al., 2007; Dinu and Wang, 2009). Setup. To set the parameters of the two models (i.e., the weights for different lexical resources for UNIQ, and the weights for the edit operation for </context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden Markov models. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The PASCAL recognising textual entailment challenge.</title>
<date>2006</date>
<booktitle>Machine Learning Challenges. Lecture Notes in Computer Science,</booktitle>
<volume>3944</volume>
<pages>177--190</pages>
<editor>In J. Quinonero-Candela, I. Dagan, B. Magnini, and F. d’Alch Buc, editors,</editor>
<publisher>Springer.</publisher>
<marker>Dagan, Glickman, Magnini, 2006</marker>
<rawString>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006. The PASCAL recognising textual entailment challenge. In J. Quinonero-Candela, I. Dagan, B. Magnini, and F. d’Alch Buc, editors, Machine Learning Challenges. Lecture Notes in Computer Science, Vol. 3944, pages 177–190. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Trond Grenager</author>
<author>Bill MacCartney</author>
<author>Daniel Cer</author>
<author>Daniel Ramage</author>
<author>Chlo´e Kiddon</author>
<author>Christopher D Manning</author>
</authors>
<title>Aligning semantic graphs for textual inference and machine reading.</title>
<date>2007</date>
<booktitle>In Proceedings of the</booktitle>
<publisher>AAAI Spring Symposium.</publisher>
<marker>de Marneffe, Grenager, MacCartney, Cer, Ramage, Kiddon, Manning, 2007</marker>
<rawString>Marie-Catherine de Marneffe, Trond Grenager, Bill MacCartney, Daniel Cer, Daniel Ramage, Chlo´e Kiddon, and Christopher D. Manning. 2007. Aligning semantic graphs for textual inference and machine reading. In Proceedings of the AAAI Spring Symposium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgiana Dinu</author>
<author>Rui Wang</author>
</authors>
<title>Inference rules and their application to recognizing textual entailment.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL</booktitle>
<pages>211--219</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="11557" citStr="Dinu and Wang, 2009" startWordPosition="1820" endWordPosition="1823">itive weights for all operation types ensure that MANLI prefers small over large edits where appropriate. 4An adaptation of the averaged perceptron algorithm (Collins, 2002) is used to tune the model parameters. micro-avg P R F1 UNIQ w/o para 80.4 80.8 80.6 MANLI w/o para 77.0 85.5 81.0 w/ para 76.7 85.4 80.8 Table 4: Evaluation of aligners and resources against the manual MSR RTE2 test annotations. bials). This accounts for roughly 9% of the MWEs in RTE2 data. Other work on RTE has targeted specifically this observation and has described paraphrases on a dependency level (Marsi et al., 2007; Dinu and Wang, 2009). Setup. To set the parameters of the two models (i.e., the weights for different lexical resources for UNIQ, and the weights for the edit operation for MANLI), we use the RTE2 development data. Testing takes place on the RTE2 test and RTE4 datasets. For MANLI, we performed this procedure twice, with the paraphrase resources described in Section 4 once deactivated and once activated. We evaluated the output of the Stanford RTE system both on the word alignment level, and on the entailment decision level. 5.2 Evaluation of Alignment Accuracy The results for evaluating the MANLI and UNIQ alignme</context>
</contexts>
<marker>Dinu, Wang, 2009</marker>
<rawString>Georgiana Dinu and Rui Wang. 2009. Inference rules and their application to recognizing textual entailment. In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009), pages 211–219, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<date>2007</date>
<booktitle>Proceedings of the ACL workshop: A broader perspective on multiword expressions.</booktitle>
<editor>Nicole Gr´egoire, Stefan Evert, and Su Nam Kim, editors.</editor>
<contexts>
<context position="9017" citStr="(2007)" startWordPosition="1427" endWordPosition="1427">oring mapping from each word in the hypothesis to one word in the premise, or to null. Mappings are scored by summing the alignment scores of all individual word pairs (provided by the lexical resources), plus edge alignment scores that 5 use the syntactic structure of premise and hypothesis to introduce a bias for syntactic parallelism. The large number of possible alignments (exponential in the number of hypothesis words) makes exhaustive search intractable. Instead, UNIQ uses a stochastic search based on Gibbs sampling, a wellknown Markov Chain Monte Carlo technique (see de Marneffe et al. (2007) for details). Since it does not support many-to-many alignments, the UNIQ aligner cannot make use of the multi-word information present in the paraphrase resources. To be able to capture some common MWEs, the Stanford RTE system was originally designed with a facility to concatenate MWEs present in WordNet into a single token (mostly particle verbs and collocations, e.g., treat as or foreign minister). However, we discovered that WordNet collapsing always has a negative effect. Inspection of the constructed alignments suggests that the lexical resources that inform the alignment process do no</context>
</contexts>
<marker>2007</marker>
<rawString>Nicole Gr´egoire, Stefan Evert, and Su Nam Kim, editors. 2007. Proceedings of the ACL workshop: A broader perspective on multiword expressions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Hickl</author>
<author>Jeremy Bensley</author>
</authors>
<title>A discourse commitment-based framework for recognizing textual entailment.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing,</booktitle>
<pages>171--176</pages>
<location>Prague, Czech Republic.</location>
<marker>Hickl, Bensley, 2007</marker>
<rawString>Andrew Hickl and Jeremy Bensley. 2007. A discourse commitment-based framework for recognizing textual entailment. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, pages 171–176, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adrian Iftene</author>
<author>Alexandra Balahur-Dobrescu</author>
</authors>
<title>Hypothesis transformation and semantic variability rules used in recognizing textual entailment.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing,</booktitle>
<pages>125--130</pages>
<location>Prague, Czech Republic.</location>
<marker>Iftene, Balahur-Dobrescu, 2007</marker>
<rawString>Adrian Iftene and Alexandra Balahur-Dobrescu. 2007. Hypothesis transformation and semantic variability rules used in recognizing textual entailment. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, pages 125–130, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>Discovery of inference rules for question answering.</title>
<date>2002</date>
<journal>Journal of Natural Language Engineering,</journal>
<volume>7</volume>
<issue>4</issue>
<contexts>
<context position="3568" citStr="Lin and Pantel (2002)" startWordPosition="580" endWordPosition="584">st ballots). Such similarities are not present in standard lexical resources like WordNet or Dekang Lin’s thesaurus (Lin, 1998). The best class of candidate resources to provide wide-coverage of multi-word similarities seems to be paraphrase resources. In this section, we examine to what extent two of the most widely used paraphrase resource types provide supporting evidence for the true MWEs in the MSR data. We deliberately use corpus-derived, noisy resources, since we are interested in the real-world (rather than idealized) prospects for accurate MWE alignment. Dependency-based paraphrases. Lin and Pantel (2002)’s DIRT model collects lexicalized dependency paths with two slots at either end. Paths with similar distributions over slot fillers count as paraphrases, with the quality measured by a mutual information-based similarity over the slot fillers. The outcome of their study is the DIRT database which lists paraphrases for around 230,000 dependency paths, extracted from about 1 GB of miscellaneous newswire text. We converted the DIRT paraphrases2 into a resource of semantic similarities between raw text phrases. We used a heuristic mapping from dependency relations to word order, and obtained simi</context>
</contexts>
<marker>Lin, Pantel, 2002</marker>
<rawString>Dekang Lin and Patrick Pantel. 2002. Discovery of inference rules for question answering. Journal of Natural Language Engineering, 7(4):343–360.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of the joint Annual Meeting of the Association for Computational Linguistics and International Conference on Computational Linguistics,</booktitle>
<pages>768--774</pages>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="3074" citStr="Lin, 1998" startWordPosition="507" endWordPosition="508"> recognition. 4 MWEs in Paraphrase Resources Before we come to actual experiments on the automatic recognition of MWEs in a practical RTE system, we need to consider the prerequisites for this task. As mentioned in Section 2, if an RTE system is to establish multi-word alignments, it requires a knowledge source that provides accurate semantic similarity judgments for “many-to-many” alignments (capital punishment – death penalty) as well as for “one-to-many” alignments (vote – cast ballots). Such similarities are not present in standard lexical resources like WordNet or Dekang Lin’s thesaurus (Lin, 1998). The best class of candidate resources to provide wide-coverage of multi-word similarities seems to be paraphrase resources. In this section, we examine to what extent two of the most widely used paraphrase resource types provide supporting evidence for the true MWEs in the MSR data. We deliberately use corpus-derived, noisy resources, since we are interested in the real-world (rather than idealized) prospects for accurate MWE alignment. Dependency-based paraphrases. Lin and Pantel (2002)’s DIRT model collects lexicalized dependency paths with two slots at either end. Paths with similar distr</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the joint Annual Meeting of the Association for Computational Linguistics and International Conference on Computational Linguistics, pages 768–774, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill MacCartney</author>
<author>Trond Grenager</author>
<author>Marie-Catherine de Marneffe</author>
<author>Daniel Cer</author>
<author>Christopher D Manning</author>
</authors>
<title>Learning to recognize features of valid textual entailments.</title>
<date>2006</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<marker>MacCartney, Grenager, de Marneffe, Cer, Manning, 2006</marker>
<rawString>Bill MacCartney, Trond Grenager, Marie-Catherine de Marneffe, Daniel Cer, and Christopher D. Manning. 2006. Learning to recognize features of valid textual entailments. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill MacCartney</author>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>A phrase-based alignment model for natural language inference.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Honolulu, Hawaii.</location>
<contexts>
<context position="8052" citStr="MacCartney et al., 2008" startWordPosition="1277" endWordPosition="1280"> trees. In the inference stage, the system produces roughly 70 features for the aligned premise-hypothesis pair, almost all of which are implementations of “small linguistic theories” whose activation indicates lexical, syntactic and semantic matches and mismatches of different types. The entailment decision is computed using a logistic regression on these features. The Stanford system supports the use of different aligners without touching the rest of the pipeline. We compare two aligners: a one-to-one aligner, which cannot construct MWE alignments (UNIQ), and a many-to-many aligner (MANLI) (MacCartney et al., 2008), which can. Both aligners use around 10 large-coverage lexical resources of semantic similarities, both manually compiled resources (such as WordNet and NomBank) and automatically induced resources (such as Dekang Lin’s distributional thesaurus or InfoMap). UNIQ: A one-to-one aligner. UNIQ constructs an alignment between dependency graphs as the highest-scoring mapping from each word in the hypothesis to one word in the premise, or to null. Mappings are scored by summing the alignment scores of all individual word pairs (provided by the lexical resources), plus edge alignment scores that 5 us</context>
</contexts>
<marker>MacCartney, Galley, Manning, 2008</marker>
<rawString>Bill MacCartney, Michel Galley, and Christopher D. Manning. 2008. A phrase-based alignment model for natural language inference. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, Honolulu, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erwin Marsi</author>
<author>Emiel Krahmer</author>
<author>Wauter Bosma</author>
</authors>
<title>Dependency-based paraphrasing for recognizing textual entailment.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACLPASCAL Workshop on Textual Entailment and Paraphrasing,</booktitle>
<pages>83--88</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="11535" citStr="Marsi et al., 2007" startWordPosition="1816" endWordPosition="1819">rticles or adver3Positive weights for all operation types ensure that MANLI prefers small over large edits where appropriate. 4An adaptation of the averaged perceptron algorithm (Collins, 2002) is used to tune the model parameters. micro-avg P R F1 UNIQ w/o para 80.4 80.8 80.6 MANLI w/o para 77.0 85.5 81.0 w/ para 76.7 85.4 80.8 Table 4: Evaluation of aligners and resources against the manual MSR RTE2 test annotations. bials). This accounts for roughly 9% of the MWEs in RTE2 data. Other work on RTE has targeted specifically this observation and has described paraphrases on a dependency level (Marsi et al., 2007; Dinu and Wang, 2009). Setup. To set the parameters of the two models (i.e., the weights for different lexical resources for UNIQ, and the weights for the edit operation for MANLI), we use the RTE2 development data. Testing takes place on the RTE2 test and RTE4 datasets. For MANLI, we performed this procedure twice, with the paraphrase resources described in Section 4 once deactivated and once activated. We evaluated the output of the Stanford RTE system both on the word alignment level, and on the entailment decision level. 5.2 Evaluation of Alignment Accuracy The results for evaluating the </context>
</contexts>
<marker>Marsi, Krahmer, Bosma, 2007</marker>
<rawString>Erwin Marsi, Emiel Krahmer, and Wauter Bosma. 2007. Dependency-based paraphrasing for recognizing textual entailment. In Proceedings of the ACLPASCAL Workshop on Textual Entailment and Paraphrasing, pages 83–88, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<date>2006</date>
<booktitle>Proceedings of the ACL Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties.</booktitle>
<editor>Begona Villada Moir´on, Aline Villavicencio, Diana McCarthy, Stefan Evert, and Suzanne Stevenson, editors.</editor>
<marker>2006</marker>
<rawString>Begona Villada Moir´on, Aline Villavicencio, Diana McCarthy, Stefan Evert, and Suzanne Stevenson, editors. 2006. Proceedings of the ACL Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan A Sag</author>
<author>Timothy Baldwin</author>
<author>Francis Bond</author>
<author>Ann Copestake</author>
<author>Dan Flickinger</author>
</authors>
<title>Multi-word expressions: a pain in the neck for NLP.</title>
<date>2002</date>
<booktitle>In Proceedings of CICLing.</booktitle>
<marker>Sag, Baldwin, Bond, Copestake, Flickinger, 2002</marker>
<rawString>Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann Copestake, and Dan Flickinger. 2002. Multi-word expressions: a pain in the neck for NLP. In Proceedings of CICLing.</rawString>
</citation>
<citation valid="true">
<date>2004</date>
<booktitle>Proceedings of the second ACL workshop on multiword expressions: Integrating processing.</booktitle>
<editor>Takaaki Tanaka, Aline Villavicencio, Francis Bond, and Anna Korhonen, editors.</editor>
<marker>2004</marker>
<rawString>Takaaki Tanaka, Aline Villavicencio, Francis Bond, and Anna Korhonen, editors. 2004. Proceedings of the second ACL workshop on multiword expressions: Integrating processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Massimo Zanzotto</author>
<author>Marco Pennacchiotti</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Shallow semantic in fast textual entailment rule learners.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing,</booktitle>
<pages>72--77</pages>
<location>Prague, Czech Republic.</location>
<marker>Zanzotto, Pennacchiotti, Moschitti, 2007</marker>
<rawString>Fabio Massimo Zanzotto, Marco Pennacchiotti, and Alessandro Moschitti. 2007. Shallow semantic in fast textual entailment rule learners. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, pages 72–77, Prague, Czech Republic.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>