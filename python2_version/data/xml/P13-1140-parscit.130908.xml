<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000075">
<title confidence="0.99838">
Learning a Phrase-based Translation Model from Mon-
olingual Data with Application to Domain Adaptation
</title>
<author confidence="0.981812">
Jiajun Zhang and Chengqing Zong
</author>
<affiliation confidence="0.9429305">
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences, Beijing, China
</affiliation>
<email confidence="0.964927">
{jjzhang, cqzong}@nlpr.ia.ac.cn
</email>
<sectionHeader confidence="0.993416" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997758318181818">
Currently, almost all of the statistical ma-
chine translation (SMT) models are trained
with the parallel corpora in some specific
domains. However, when it comes to a lan-
guage pair or a different domain without
any bilingual resources, the traditional SMT
loses its power. Recently, some research
works study the unsupervised SMT for in-
ducing a simple word-based translation
model from the monolingual corpora. It
successfully bypasses the constraint of
bitext for SMT and obtains a relatively
promising result. In this paper, we take a
step forward and propose a simple but effec-
tive method to induce a phrase-based model
from the monolingual corpora given an au-
tomatically-induced translation lexicon or a
manually-edited translation dictionary. We
apply our method for the domain adaptation
task and the extensive experiments show
that our proposed method can substantially
improve the translation quality.
</bodyText>
<sectionHeader confidence="0.998977" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998047983870968">
During the last decade, statistical machine trans-
lation has made great progress. Novel translation
models, such as phrase-based models (Koehn et
a., 2007), hierarchical phrase-based models
(Chiang, 2007) and linguistically syntax-based
models (Liu et a., 2006; Huang et al., 2006; Gal-
ley, 2006; Zhang et al, 2008; Chiang, 2010;
Zhang et al., 2011; Zhai et al., 2011, 2012) have
been proposed and achieved higher and higher
translation performance. However, all of these
state-of-the-art translation models rely on the
parallel corpora to induce translation rules and
estimate the corresponding parameters.
It is unfortunate that the parallel corpora are
very expensive to collect and are usually not
available for resource-poor languages and for
many specific domains even in a resource-rich
language pair.
Recently, more and more researchers concen-
trated on taking full advantage of the monolin-
gual corpora in both source and target languages,
and proposed methods for bilingual lexicon in-
duction from non-parallel data (Rapp, 1995,
1999; Koehn and Knight, 2002; Haghighi et al.,
2008; Daumé III and Jagarlamudi, 2011) and
proposed unsupervised statistical machine trans-
lation (bilingual lexicon is a byproduct) with
only monolingual corpora (Ravi and Knight,
2011; Nuhn et al., 2012; Dou and Knight, 2012).
In the bilingual lexicon induction (Koehn and
Knight, 2002; Haghighi et al., 2008; Daumé III
and Jagarlamudi, 2011), with the help of the or-
thographic and context features, researchers
adopted an unsupervised method, such as canon-
ical correlation analysis (CCA) model, to auto-
matically induce the word translation pairs be-
tween two languages from non-parallel data only
requiring that the monolingual data in each lan-
guage are from a fairly comparable domain.
The unsupervised statistical machine transla-
tion method (Ravi and Knight, 2011; Nuhn et al.,
2012; Dou and Knight, 2012) viewed the trans-
lation task as a decipherment problem and de-
signed a generative model with the objective
function to maximize the likelihood of the
source language monolingual data. To tackle the
large-scale vocabulary, they mainly considered
the word-based model (e.g. IBM Model 3) and
applied the Bayesian method with Gibbs sam-
pling or slice sampling. Finally, they used the
learned translation model directly to translate
unseen data (Ravi and Knight, 2011; Nuhn et al.,
2012) or incorporated the learned bilingual lexi-
con as a new in-domain translation resource into
the phrase-based model which is trained with
out-of-domain data to improve the domain adap-
tation performance in machine translation (Dou
and Knight, 2012).
We can easily see that these unsupervised
methods can only induce the word-based transla-
tion rules (bilingual lexicon) at present. It is a
big challenge that whether we can induce phrase
</bodyText>
<page confidence="0.92508">
1425
</page>
<note confidence="0.538669">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1425–1434,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
1, word reordering example:
2, idiom example:
3, unknown word translation:
</note>
<tableCaption confidence="0.983167666666667">
Table 1: Examples of new translation knowledge learned with the proposed phrase pair induction method. For
the three fields separated by “ ”, the first two are respectively Chinese and English phrase, and the last one is
the word alignment between these two phrases.
</tableCaption>
<bodyText confidence="0.99999375">
level translation rules and learn a phrase-based
model from the monolingual corpora.
In this paper, we focus on exploring this di-
rection and propose a simple but effective meth-
od to induce the phrase-level translation rules
from monolingual data. The main idea of our
method is to divide the phrase-level translation
rule induction into two steps: bilingual lexicon
induction and phrase pair induction.
Since many researchers have studied the bi-
lingual lexicon induction, in this paper, we
mainly concentrate ourselves on phrase pair in-
duction given a probabilistic bilingual lexicon
and two in-domain large monolingual data
(source and target language). In addition, we
will further introduce how to refine the induced
phrase pairs and estimate the parameters of the
induced phrase pairs, such as four standard
translation features and phrase reordering feature
used in the conventional phrase-based models
(Koehn et al., 2007). The induced phrase-based
model will be used to help domain adaptation
for machine translation.
In the rest of this paper, we first explain with
examples to show what new translation
knowledge can be learned with our proposed
phrase pair induction method (Section 2), and
then we introduce the approach for probabilistic
bilingual lexicon acquisition in Section 3. In Sec-
tion 4 and 5, we respectively present our method
for phrase pair induction and introduce an ap-
proach for phrase pair refinement and parameter
estimation. Section 6 will show the detailed ex-
periments for the task of domain adaptation. We
will introduce some related work in Section 7
and conclude this paper in Section 8.
</bodyText>
<sectionHeader confidence="0.850226" genericHeader="introduction">
2 What Can We Learn with Phrase
Pair Induction?
</sectionHeader>
<bodyText confidence="0.967741882352941">
Readers may doubt that if phrase pair induction
is performed only using bilingual lexicon and
monolingual data, what new translation
knowledge can be learned?
The bilingual lexicon can only express the
translation equivalence between source- and tar-
get-side word pair and has little ability to deal
with word reordering and idiom translation. In
contrast, phrase pair induction can make up for
this deficiency to some extent. Furthermore, our
method is able to learn some unknown word
translations.
From the induced phrase pairs with our meth-
od, we have conducted a deep analysis and find
that we can learn three kinds of new translation
knowledge: 1) word reordering in a phrase pair;
2) idioms; and 3) unknown word translations.
Table 1 gives examples for each of the three
kinds. For the first example, the source and tar-
get phrase are extracted respectively from mono-
lingual data, each word in the source phrase has
a translation in the target phrase, but the word
order is different. The word order encoded in a
phrase pair is difficult to learn in a word-based
SMT. In the second example, the italic source
word corresponds to two target words (in italic),
and the phrase pair is an idiom which cannot be
learned from word-based SMT. In the third ex-
ample, as we learn from the source and target
monolingual text that the words around the italic
ones are translations with each other, thus we
cannot only extract a new phrase pair but also
learn a translation pair of unknown words in
italic.
</bodyText>
<sectionHeader confidence="0.9853325" genericHeader="method">
3 Probabilistic Bilingual Lexicon Ac-
quisition
</sectionHeader>
<bodyText confidence="0.9998887">
In order to induce the phrase pairs from the in-
domain monolingual data for domain adaptation,
the probabilistic bilingual lexicon is essential.
In this paper, we acquire the probabilistic bi-
lingual lexicon from two approaches: 1) build a
bilingual lexicon from large-scale out-of-domain
parallel data; 2) adopt a manually collected in-
domain lexicon. This paper uses Chinese-to-
English translation as a case study and electronic
data is the in-domain data we focus on.
</bodyText>
<page confidence="0.986849">
1426
</page>
<bodyText confidence="0.9994682">
In Chinese-to-English translation, there are
lots of parallel data on News. Here, we utilize
about 2.08 million sentence pairs1 in News do-
main to learn a probabilistic bilingual lexicon.
Basically, we can use GIZA++ (Och, 2003) to
get the probabilistic lexicon. However, the prob-
lem is that each source-side word associates too
many possible translations which contain much
noise. For instance, in the lexicon obtained with
GIZA++, each source-side word has about 13
translations on average. The noise of the lexicon
can influence the accuracy of the induced phrase
pairs to a large extent. To learn a lexicon with a
high precision, we follow Munteanu and Marcu
(2006) to apply Log-Likelihood-Ratios (Dunning,
1993; Melamed, 2000; Moore, 2004a, 2004b) to
estimate how strong the association is between a
source-side word and its aligned target-side word.
We employ the same algorithm used in (Munte-
anu and Marcu, 2006) which first use the GI-
ZA++ (with grow-diag-final-and heuristic) to
obtain the word alignment between source and
target words, and then calculate the association
strength between the aligned words. After using
the log-likelihood-ratios algorithm2, we obtain a
probabilistic bilingual lexicon with bidirectional
translation probabilities from the out-of-domain
data. In the final lexicon, the number of average
translations is only 5. We call this lexicon LLR-
lex.
In the electronic domain, we manually collect-
ed a lexicon which contains about 140k entries.
It should be noted that there is no translation
probability in this lexicon. In order to assign
probabilities to each entry, we apply the Corpus
Translation Probability which used in (Wu et al.,
2008): given an in-domain source language
monolingual data, we translate this data with the
phrase-based model trained on the out-of-domain
News data, the in-domain lexicon and the in-
domain target language monolingual data (for
language model estimation). With the source
language data and its translation, we estimate the
bidirectional translation probabilities for each
entry in the original lexicon. For the entries
whose translation probabilities are not estimated,
we just assign a uniform probability. That is if a
source word has n translations, then the transla-
tion probability of target word given the source
word is 1/n. We call this lexicon Domain-lex.
</bodyText>
<footnote confidence="0.9590604">
1 LDC category numbers are: LDC2000T50, LDC2003E14,
LDC2003E07, LDC2004T07, LDC2005T06, LDC2002L27,
LDC2005T10 and LDC2005T34.
2 Following Moore (2004b), we use the threshold 10 on
LLR to filter out unlikely translations.
</footnote>
<bodyText confidence="0.999598666666667">
We combine LLR-lex and Domain-lex to obtain
the final probabilistic bilingual lexicon for phrase
pair induction.
</bodyText>
<sectionHeader confidence="0.967489" genericHeader="method">
4 Phrase Pair Induction Method
</sectionHeader>
<bodyText confidence="0.833585111111111">
Given a probabilistic bilingual lexicon and two
monolingual data, we present a simple but effec-
tive method for phrase pair induction in this sec-
tion.
Input: Probabilistic bilingual lexicon V (each source word
s maps a translation set V[s])
Source language monolingual data S={sn} n=1...N
Target language monolingual data T={tm} m=1...M
Output: Phrase pairs P
</bodyText>
<listItem confidence="0.960524666666667">
1: For each distinct source-side phrase in S:
2: If each in V:
3: CollectV[sk]k=;
4: For each permutation of :
5: If in T:
6: Add phrase pair into P
</listItem>
<figureCaption confidence="0.997914">
Figure 1: a naïve algorithm for phrase pair induction.
</figureCaption>
<subsectionHeader confidence="0.997199">
4.1 A Naïve Method
</subsectionHeader>
<bodyText confidence="0.98298548">
We first introduce a relatively naïve way to ex-
tract phrase pairs from the given resources. For a
source phrase (word sequence), we can reorder
the words in the phrase (permutation) first, and
then obtain the target phrases with the bilingual
lexicon (translation), and finally check if the tar-
get phrase is in the target monolingual data. The
algorithm is given in Figure 1.
Figure 1 shows that the naïve algorithm is very
easy to implement. However, the time complexi-
ty is too high. For each source phrase (with
(j — i + 1)! permutations), suppose a source word
has C translations on average and checking
whether the target phrase in T needs time
, then, phrase pair induction for a single
source phrase needs time O(C&apos;-&apos;+&apos; T(j—i+1)!).
It is very time consuming. One may design
smarter algorithms. For example, one can collect
distinct n-grams from source and target monolin-
gual data. Then, for a source-side phrase with
length L, one can find the best translation candi-
date using the probabilistic bilingual lexicon
from the target-side phrases with the same length
L. The biggest disadvantage of these algorithms
is that they can only induce phrase pair (with the
</bodyText>
<page confidence="0.967212">
1427
</page>
<bodyText confidence="0.712290727272727">
same length) encoding word reordering, but can-
not learn phrase pairs in different length. Fur-
thermore, they cannot learn idioms and unknown
word translations from monolingual data. Obvi-
ously, these kind of approaches is not optimal.
Input: Probabilistic bilingual lexicon V (each source word s
maps a translation set V[s])
Source language monolingual data S={sn} n=1...N
Inverted index representing target language monolin-
gual data IMap
Output: Phrase pairs P
</bodyText>
<subsectionHeader confidence="0.921257">
4.2 Phrase Pair Induction with Inverted
Index
</subsectionHeader>
<bodyText confidence="0.999782111111111">
In order to make the phrase pair induction both
effective and efficient, we propose a method
using inverted index data structure which is usu-
ally a central component of a typical search en-
gine.
The inverted index is employed to represent
the target language monolingual data. For a tar-
get language word, the inverted index not only
records the sentence position in monolingual
data, but also records the word position in a sen-
tence. Some examples are shown in Table 2. By
doing this, we do not need to iterate all the per-
mutations of source language phrase to ex-
plore possible phrase pairs encoding word reor-
dering. Furthermore, it is possible to learn idiom
translation and unknown word translations. We
will elaborate how to induce phrase pairs with
the help of inverted index.
</bodyText>
<table confidence="0.896024">
Target Language Position
Word
communication (2,5), (106,20), ..., (23022, 12)
... ...
zoom (90,2), (280,21), ..., (90239,15)
</table>
<tableCaption confidence="0.915361">
Table 2: Some examples of inverted index for tar-
</tableCaption>
<bodyText confidence="0.998942909090909">
get language words, (2,5) means that “communica-
tion” occurs at the 5th word of the 2nd sentence in the
target monolingual data.
The new algorithm for phrase pair induction is
presented in Figure 2. Line 1 iterates all the dis-
tinct phrases in the source-side monolingual data.
It can be implemented by collecting all the dis-
tinct n-grams in which n is the phrase length we
are interested in (3 to 7 in this paper). For each
distinct source-side phrase, Line 2-5 efficiently
collects all the positions in the target monolin-
gual data for the translations of each word in the
source phrase. Line 6 sorts the positions so that
we can easily find the position sequence belong-
ing to a same sentence. Line 8-9 discards all the
position sub-sequences that lack translations for
more than one source-side words. That is to say
we allow at most one unknown word in an in-
duced phrase pair in order to make the induction
more accurate. Line 10 and Line 12 is the core
of this algorithm. We first define a constraint
before detailing the algorithm.
</bodyText>
<figureCaption confidence="0.989921">
Figure 2: Phrase pair induction using inverted index.
</figureCaption>
<bodyText confidence="0.982282444444444">
Constraint: we require that there exists at
most one phrase in a target sentence that is the
translation of the source-side phrase.
According to our analysis, it is not often to
find that two phrases (length larger than 2) in a
same sentence have the same meaning. Even if it
happens, it is reasonable to keep the one with the
highest probability. Given a position sequence
belonging to a same sentence, Line 10 smoothes
the probability of the single word gap according
to the probabilities of the around words. Single
word gap means that this word is not aligned but
its left and right words are aligned with the
words of the source-side phrase. Suppose the
target sub-sequence is and is the
only word that is not aligned with source-side
words. We smooth the probabilityp(ti+rInull)
as follows:
</bodyText>
<equation confidence="0.995291882352941">
 min  |, |
    
p t s p t s
i j
 i t j t
 2
p t s
 1  |1   1 
 p t s
|
i r
  t i r
  1 t
i r
  i r
 
 2
</equation>
<bodyText confidence="0.999938090909091">
The above formula means that if the left or the
right side only has one word, then the smoothed
probability is one half of the minimum of the
probabilities of the two neighbors, otherwise the
smoothed probability is the average of the prob-
abilities of the two neighbors. This smoothing
strategy encourages that if more words around
the un-aligned word are translations of the
source-side phrase, then the gap word is more
likely to belong to the translations of the source-
side phrase.
</bodyText>
<figure confidence="0.820656909090909">
1: For each distinct source-side phrase in S:
2: positionArray = []
3: For each :
4: For each :
5: add IMap[ t ] into positionArray
6: Sort positionArray
7: For each sequence in a same sentence in positionArray:
8: If more than 1 word in has no trans in the seq:
9: Discard this seq and continue
10: Probability smoothing for single word gap
11: For all continuous position sub-sequence:
12: Find the one with maximum probability
13: Add phrase pair into P
ptir  |null
 


j
, if r or r
 1  1
(1)
, otherwise
</figure>
<page confidence="0.956907">
1428
</page>
<bodyText confidence="0.990953175">
After probability smoothing of the single gap
word, we are ready to extract the candidate
translation of the source-side phrase. Similar
with Line 9 in Figure 2, we further filter the tar-
get continuous phrase if more than one word in
source-side phrase has no translation in this tar-
get phrase. After that, we just choose the contin-
uous target phrase with the largest probability if
two or more continuous target phrases exist in
the same target sentence. The probability of a
target-side phrase given the source-side phrase is
computed similar to that of (Koehn et al., 2003)
except that we impose length normalization:
(2)
where the alignment a is produced using
probabilistic bilingual lexicon. If a target word
in t is a gap word, we suppose there is a word
alignment between the target gap word and the
source-side null.
Similarly, we can compute the probability of
source-side phrase given the target-side phrase
p,. ( s I t, a) . Then, we find the target-side phrase
which has the biggest value of
p, (t I s, a) - p,� (s I t, a) . Line 13 in Figure 2 col-
lects the induced phrase pairs.
For the time complexity, it depends on the
length of positionArray, since the time complex-
ity of the core algorithm (Line 7-13) is propor-
tional to the length of positionArray. If posi-
tionArray contains almost all the positions in the
target monolingual data T, then the worst time
complexity will be (for array sort).
However, we find in the target monolingual data
(1 million sentences) that each distinct word
happens 110 times on average. Then, for a
sources-side phrase with 7 words, the average
length of positionArray will be 3850, since each
source word has averagely 5 target translations
(mentioned in Section 3). Therefore, the algo-
rithm is relatively efficient in the average case.
</bodyText>
<sectionHeader confidence="0.999347" genericHeader="method">
5 Phrase Pair Refinement and Parame-
terization
</sectionHeader>
<subsectionHeader confidence="0.990833">
5.1 Phrase Pair Refinement
</subsectionHeader>
<bodyText confidence="0.999980902439024">
Some of the phrase pairs induced in Section 4
may contain noise. According to our analysis,
we find that the biggest problem is that in the
target-side of the phrase pair, there are two or
more identical words aligned to the same source-
side word. For example, we extract a phrase pair
as follows:
In the above phrase pair, there are two words
“of&apos; in the target side and the first one is redun-
dant. The phrase pair induction algorithm pre-
sented in Section 4 cannot deal with this situa-
tion. In this section, we propose a simple ap-
proach to handle this problem. For each entry in
LLR-lex, such as (的, of), we can learn two kinds
of information from the out-of-domain word-
aligned sentence pairs: one is whether the target
translation is before or after the translation of the
preceding source-side word (Order); the other is
whether the target translation is adjacent with
the translation of the preceding source-side word
(Adjacency). If the source-side word is the be-
ginning of the phrase, we calculate the corre-
sponding information with the succeeding word
instead of the preceding word. For the entries in
Domain-lex, we constrain that the target transla-
tion should be adjacent with the translations of
its source-side neighbors and translation order is
the same with the source-side words.
With the Order and Adjacency information,
we first check the order information, and then
check the adjacency information if the dupli-
cates cannot be handled using order information.
For example, since (的, of) is an entry in LLR-
lex and we have learned that “of&apos; is much more
likely to be behind the translation of the suc-
ceeding word. Thus, the first word “of&apos; can be
discarded. This refinement can be applied before
finding the phrase pair with maximum probabil-
ity (Line 12 in Figure 2) so that the duplicate
words do not affect the calculation of translation
probability of phrase pair.
</bodyText>
<subsectionHeader confidence="0.99705">
5.2 Translation Probability Estimation
</subsectionHeader>
<bodyText confidence="0.997136846153846">
It is well known that in the phrase-based SMT
there are four translation probabilities and the
reordering probability for each phrase pair.
The translation probabilities in the traditional
phrase-based SMT include bidirectional phrase
translation probabilities and bidirectional lexical
weights. For the lexical weights, we can use the
plex s  |t, a and p, (t I s, a) computed in the
above section without length normalization.
However, for the phrase-level probability, we
cannot use maximum likelihood estimation since
the phrase pairs are not extracted from parallel
sentences.
</bodyText>
<page confidence="0.991297">
1429
</page>
<bodyText confidence="0.99565825">
In this paper, we borrow and extend the idea of
(Klementiev et al., 2012) to calculate the phrase-
level translation probability with context infor-
mation in source and target monolingual corpus.
The value is calculated using a vector space
model. With source and target vocabularies
and , the source-side
phrase s and target-side phrase t can be respec-
tively represented in an I- and M-dimensional
vector. The k-th component of s’s contextual
vector is computed using the method of (Fung
and Yee, 1998) as follows:
</bodyText>
<equation confidence="0.4238195">
(3)
wherens,k
</equation>
<bodyText confidence="0.988017666666667">
and nk denotes the number of times sk
occurs in the context of s and in the entire source
language monolingual data, and n. is the max-
imum number of occurrence of any source-side
word in the source language monolingual data.
The k-th element of t’s vector can be computed
with the same method. We finally normalize
these vectors with L2-norm.
With the s’s and t’s contextual vector represen-
tations, we calculate two similarities: 1) project
s’s vector into target side with the lexical
mapping p(t|s), and then get the similarity by
computing the cosine of two angles between t’s
and ’s vectors; 2) project t’s vector into source
side with the lexical mapping p(s|t), and then
obtain the similarity between s’s and ’s vectors.
These two contextual similarities will serve as
two phrase-level translation probabilities.
</bodyText>
<subsectionHeader confidence="0.996458">
5.3 Reordering Probability Estimation
</subsectionHeader>
<bodyText confidence="0.999646555555556">
For the reordering probabilities of newly induced
phrase pairs, we can also follow Klementiev et al.
(2012) to estimate these probabilities using
source and target monolingual data. The method
is to calculate six probabilities for monotone,
swap or discontinuous cases. For the phrase pair
(的 商 业 信 息 , business information of), we
find a source sentence containing 的 商业 信息,
and find a target sentence containing business
information of. If there is another phrase pair
 s, t, exactly follows business information of
and occurs in the same source sentence with
的 商业 信息, then we compare the position
relationship between and 的 商业 信息. We
increment the swap count if is just before 的
商业 信息. After counting, we finally use max-
imum likelihood estimation method to compute
the reordering probabilities.
</bodyText>
<sectionHeader confidence="0.99986" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999975818181818">
As far as we know, few researchers study phrase
pair induction from only monolingual data.
There are three research works that are most
related with ours. One is using an in-domain
probabilistic bilingual lexicon to extract sub-
sentential parallel fragments from comparable
corpora (Munteanu and Marcu, 2006; Quirk et al.,
2007; Cettolo et al., 2010). Munteanu and Marcu
(2006) first extract the candidate parallel sen-
tences from the comparable corpora and further
extract the accurate sub-sentential bilingual
fragments from the candidate parallel sentences
using the in-domain probabilistic bilingual lexi-
con. Compared with their work, our focus is to
induce phrase pairs directly from monolingual
data rather than comparable data. Thus, finding
the candidate parallel sentences is not possible in
our situation.
Another is to make full use of monolingual da-
ta with transductive learning (Ueffing et al., 2007;
Schwenk, 2008; Wu et al., 2008; Bertoldi and
Federico, 2009). For the target-side monolingual
data, they just use it to train language model, and
for the source-side monolingual data, they em-
ploy a baseline (word-based SMT or phrase-
based SMT trained with small-scale bitext) to
first translate the source sentences, combining
the source sentence and its target translation as a
bilingual sentence pair, and then train a new
phrase-base SMT with these pseudo sentence
pairs. This method cannot learn idiom transla-
tions and unknown word translations.
The third is to estimate the translation parame-
ters and reordering parameters using monolin-
gual data given the phrase pairs (Klementiev et
al., 2012). Their work supposes the phrase pairs
are already given and then corresponding param-
eters can be learned with monolingual data. Dif-
ferent from their work, we concentrate ourselves
on inducing phrase pairs from monolingual data
and then borrow some ideas from theirs for pa-
rameter estimation. Furthermore, we extend their
contextual similarity between source and target
phrases to both directions.
</bodyText>
<sectionHeader confidence="0.999865" genericHeader="evaluation">
7 Experiments
</sectionHeader>
<subsectionHeader confidence="0.995231">
7.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.995594">
Our purpose is to induce phrase pairs to improve
translation quality for domain adaptation. We
have introduced the out-of-domain data and the
electronic in-domain lexicon in Section 3. Here
we introduce other information about the in-
</bodyText>
<page confidence="0.977135">
1430
</page>
<bodyText confidence="0.999991138888889">
domain data. Besides the in-domain lexicon, we
have collected respectively 1 million monolin-
gual sentences in electronic area from the web.
They are neither parallel nor comparable because
we cannot even extract a small number of paral-
lel sentence pairs from this monolingual data
using the method of (Munteanu and Marcu,
2006). We further employ experts to translate
2000 Chinese electronic sentences into English.
The first half is used as the tuning set (elec1000-
tune) and the second half is employed as the test-
ing set (elec1000-test).
We construct two kinds of phrase-based mod-
els using Moses (Koehn et al., 2007): one uses
out-of-domain data and the other uses in-domain
data. For the out-of-domain data, we build the
phrase table and reordering table using the 2.08
million Chinese-to-English sentence pairs, and
we use the SRILM toolkit (Stolcke, 2002) to
train the 5-gram English language model with
the target part of the parallel sentences and the
Xinhua portion of the English Gigaword. For the
in-domain electronic data, we first consider the
lexicon as a phrase table in which we assign a
constant 1.0 for each of the four probabilities,
and then we combine this initial phrase table and
the induced phrase pairs to form the new phrase
table. The in-domain reordering table is created
for the induced phrase pairs. An in-domain 5-
gram English language model is trained with the
target 1 million monolingual data.
We use BLEU (Papineni et al., 2002) score
with shortest length penalty as the evaluation
metric and apply the pairwise re-sampling ap-
proach (Koehn, 2004) to perform the signifi-
cance test.
</bodyText>
<subsectionHeader confidence="0.998076">
7.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999965211267606">
In this section, we first conduct experiments to
figure out how the translation performance de-
grades when the domain changes. To better illus-
trate the comparison, we first use News data to
evaluate the NIST evaluation tests and then use
the same News data to evaluate the electronic
test sets. For the NIST evaluation, we employ
Chinese-to-English NIST MT03 as the tuning set
and NIST MT05 as the test set. Table 3 gives the
results. It is obvious that, it is relatively high
when using the News training data to evaluate
the same News test set. However, when the test
domain is changed, the translation performance
decreases to a large extent.
Given the in-domain bilingual lexicon and two
monolingual data, previous works also proposed
some good methods to explore the potential of
the given data to improve the translation quality.
Here, we implement their approaches and use
them as our strong baseline. Wu et al. (2008)
regards the in-domain lexicon with corpus trans-
lation probability as another phrase table and
further use the in-domain language model be-
sides the out-of-domain language model. Table 4
gives the results. We can see from the table that
the domain lexicon is much helpful and signifi-
cantly outperforms the baseline with more than
4.0 BLEU points. When it is enhanced with the
in-domain language model, it can further im-
prove the translation performance by more than
2.5 BLEU points. This method has made good
use of in-domain lexicon and the target-side in-
domain monolingual data, but it does not take
full advantage of the in-domain source-side
monolingual data.
In order to use source-side monolingual data,
Ueffing et al. (2007), Schwenk (2008), Wu et al.
(2008) and Bertoldi and Federico (2009) em-
ployed the transductive learning to first translate
the source-side monolingual data using the best
configuration (baseline+in-domain lexicon+in-
domain language model) and obtain 1-best trans-
lation for each source-side sentence. With the
source-side sentences and their translations, the
new phrase table and reordering table are built.
Then, these resources are added into the best
configuration. The experimental results are pre-
sented in the last low of Table 4. From the results,
we see that transductive learning can further im-
prove the translation performance significantly
by 0.6 BLEU points.
In tranductive learning, in-domain lexicon and
both-side monolingual data have been explored.
However, this method does not take full ad-
vantage of both-side monolingual data because it
uses source and target monolingual data individ-
ually. In our method, we explore fully the source
and target monolingual data to induce translation
equivalence on the phrase level. In order to make
the phrase pair induction more efficient, we first
sort all the sentences in the both-side monolin-
gual data according to the word hit rate in the
bilingual lexicon. Then, we conduct six sets of
experiments respectively on the first 100k, 200k,
300k, 500k and whole 1m sentences. All the ex-
periments are run based on the configuration
with BLEU 13.41 in Table 4, and we call this
configuration BestConfig. Note that the unknown
words are only allowed if the source-side of a
phrase pair has more than 3 words. Table 5
shows the results.
</bodyText>
<page confidence="0.964333">
1431
</page>
<table confidence="0.99893325">
Training Data Tune Data (NIST MT03) Test Data (NIST MT05)
2.08M sentence pairs in News 35.79 34.26
Tune Data (elec1000-tune) Test Data (elec1000-test)
7.93 6.69
</table>
<tableCaption confidence="0.962328">
Table 3: Experimental results using News training data to test NIST evaluation data and electronic data (numbers
denote BLEU score points in percent).
</tableCaption>
<table confidence="0.999717166666667">
Method Tune (elec1000-tune) Test (elec1000-test)
Baseline 7.93 6.69
baseline + in-domain lexicon 10.97 10.87
baseline + in-domain lexicon + in- 13.72 13.41++
domain language model
Transductive Learning 14.13 14.01*
</table>
<tableCaption confidence="0.9867075">
Table 4: Experimental results using News training data, in-domain lexicon, language model and transductive
learning. Bold figures mean that the results are statistically significant better than the baseline with p&lt;0.01, and
“++” denotes the result is statistically significant better than baseline+in-domain lexicon. “*” means that the
result is statistically significant better than 13.41 with p&lt;0.05.
</tableCaption>
<table confidence="0.999804">
Method Tune (BLEU %) Test (BLEU %)
BestConfig 13.72 13.41
+phrase pair induction (100k) 14.23 14.06
+phrase pair induction (200k) 14.45 14.24
+phrase pair induction (300k) 14.76 14.83++
+phrase pair induction (500k) 14.98 15.16++
+phrase pair induction (1m) 15.11 15.30++
</table>
<tableCaption confidence="0.97985525">
Table 5: Experimental results of our phrase pair induction method. Bold figures denotes the corresponding
method significantly outperform the BestConfig with p&lt;0.05. Bold and Italic figures means the results are sig-
nificantly better than that of BestConfig with p&lt;0.01. “++” denotes that the corresponding approach performs
significantly better than Transductive Learning with p&lt;0.01.
</tableCaption>
<table confidence="0.998974333333333">
Method Before Filtering After Filtering
+phrase pair induction (100k) 72,615 8,724
+phrase pair induction (200k) 108,948 12,328
+phrase pair induction (300k) 136,529 17,505
+phrase pair induction (500k) 150,263 19,862
+phrase pair induction (1m) 169,172 21,486
</table>
<tableCaption confidence="0.998747">
Table 6: the number of phrase pairs induced with different size of monolingual data.
</tableCaption>
<bodyText confidence="0.99995125">
We can see from the table that our method ob-
tains the best translation performance. When us-
ing the first 100k sentences for phrase pair induc-
tion, it obtains a significant improvement over
the BestConfig by 0.65 BLEU points and can
outperform the transductive learning method.
When we use more monolingual data, the per-
formance becomes even better. The method of
phrase pair induction using 300k sentences per-
forms quite well. It outperforms the BestConfig
significantly with an improvement of 1.42 BLEU
points and it also performs much better than
transductive learning method with gains of 0.82
BLEU points. With the monolingual data larger
and larger, the gains become smaller and smaller
because the word hit rate gets lower and lower.
These experimental results empirically show the
effectiveness of our proposed phrase pair induc-
tion method.
A question remains that how many new phrase
pairs are induced with different size of monolin-
gual data. Here, we give respectively the statis-
tics before and after filtering with the 1000 test
sentences. Table 6 shows the statistics. We can
see from the table that lots of new phrase pairs
can be induced since the source and target mono-
lingual data is in the same domain. However,
since the source and target monolingual data is
</bodyText>
<page confidence="0.983619">
1432
</page>
<bodyText confidence="0.99997575">
far from parallel, most of the phrase pairs are not
long. For example, in the 108,948 distinct phrase
pairs, we find that the phrase pair distribution
according to source-side length is (3:50.6%,
4:35.6%, 5:3.3%, 6:9.8%, 7:0.7%). It is easy to
see that the phrase pairs whose source-side
length longer than 4 account for only a very
small part.
</bodyText>
<sectionHeader confidence="0.981952" genericHeader="conclusions">
8 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999994148148148">
This paper proposes a simple but effective meth-
od to induce phrase pairs from monolingual data.
Given the probabilistic bilingual lexicon and
both-side monolingual data in the same domain,
the method employs inverted index structure to
represent the target-side monolingual data, and
induce the translations for each distinct source-
side phrase with the help of the bilingual lexicon.
We further propose an approach to refine the re-
sult phrase pairs to make them more accurate.
We also introduce how to estimate the translation
and reordering parameters for the induced phrase
pairs with monolingual data. Extensive experi-
ments on domain adaptation have shown that our
method can significantly outperform previous
methods which also focus on exploring the in-
domain lexicon and monolingual data.
However, through the analysis we find that our
induced phrase pairs still contain some noise,
such as the words in source- and target-side of
the phrase pair are all aligned but the target-side
phrase expresses the different meaning. Further-
more, our proposed method cannot learn expres-
sions which are not lexical translations but are
semantic ones. In the future, we will study fur-
ther on these phenomena and propose new meth-
ods to handle these problems.
</bodyText>
<sectionHeader confidence="0.997488" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9993309">
The research work has been funded by the Hi-
Tech Research and Development Program (“863”
Program) of China under Grant No.
2011AA01A207, 2012AA011101 and
2012AA011102, and also supported by the Key
Project of Knowledge Innovation of Program of
Chinese Academy of Sciences under Grant No.
KGZD-EW-501. We would also like to thank the
anonymous reviewers for their valuable sugges-
tions.
</bodyText>
<sectionHeader confidence="0.99886" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999440714285714">
Nicola Bertoldi and Marcello Federico, 2009. Domain
adaptation for statistical machine translation with
monolingual resources. In Proc. of the Fourth
Workshop on Statistical Machine Translation,
pages 182-189.
Mauro Cettolo, Marcello Federico and Nicola
Bertoldi, 2010. Mining parallel fragments from
comparable texts. In Proc. of the seventh
International Workshop on Spoken Language
Translation (IWSLT), pages 227-234.
David Chiang, 2007. Hierarchical phrase-based
translation. computational linguistics, 33 (2).
pages 201-228.
David Chiang, 2010. Learning to translate with source
and target syntax. In Proc. of ACL 2010, pages
1443-1452.
Hal Daumé III and Jagadeesh Jagarlamudi, 2011.
Domain adaptation for machine translation by
mining unseen words. In Proc. of ACL-HLT
2011.
Qing Dou and Kevin Knight, 2012. Large Scale
Decipherment for Out-of-Domain Machine
Translation. In Proc. of EMNLP-CONLL 2012.
Ted Dunning, 1993. Accurate methods for the
statistics of surprise and coincidence.
computational linguistics, 19 (1). pages 61-74.
Pascale Fung and Lo Yuen Yee, 1998. An IR
approach for translating new words from
nonparallel, comparable texts. In Proc. of ACL-
COLING 1998., pages 414-420.
Michel Galley, Jonathan Graehl, Kevin Knight,
Daniel Marcu, Steve DeNeefe, Wei Wang and
Ignacio Thayer, 2006. Scalable inference and
training of context-rich syntactic translation
models. In Proc. of COLING-ACL 2006, pages
961-968.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick
and Dan Klein, 2008. Learning bilingual
lexicons from monolingual corpora. In Proc. of
ACL-08: HLT, pages 771-779.
Liang Huang, Kevin Knight and Aravind Joshi, 2006.
A syntax-directed translator with extended
domain of locality. In Proc. of AMTA 2006,
pages 1-8.
Alexandre Klementiev, Ann Irvine, Chris Callison-
Burch and David Yarowsky, 2012. Toward
statistical machine translation without parallel
corpora. In Proc. of EACL 2012., pages 130-140.
Philipp Koehn, 2004. Statistical significance tests for
machine translation evaluation. In Proc. of
EMNLP 2004., pages 388-395, Barcelona, Spain,
July 25th-26th, 2004.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Marcello Federico, Nicola Bertoldi, Brooke
Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondřej Bojar, Alexandra
Constantin and Evan Herbst, 2007. Moses: Open
source toolkit for statistical machine translation.
In Proc. of ACL on Interactive Poster and
Demonstration Sessions 2007., pages 177-180,
Prague, Czech Republic, June 27th-30th, 2007.
Philipp Koehn and Kevin Knight, 2002. Learning a
translation lexicon from monolingual corpora. In
</reference>
<page confidence="0.601989">
1433
</page>
<reference confidence="0.999469961538462">
Proc. of the ACL-02 workshop on Unsupervised
lexical acquisition, pages 9-16.
Yang Liu, Qun Liu and Shouxun Lin, 2006. Tree-to-
string alignment template for statistical machine
translation. In Proc. of COLING-ACL 2006,
pages 609-616.
I. Dan Melamed, 2000. Models of translational
equivalence among words. computational
linguistics, 26 (2). pages 221-249.
Rorbert C. Moore, 2004a. Improving IBM word-
alignment model 1. In Proc. of ACL 2004.
Rorbert C. Moore, 2004b. On log-likelihood-ratios
and the significance of rare events. In Proc. of
EMNLP 2004., pages 333-340.
Dragos Stefan Munteanu and Daniel Marcu, 2006.
Extracting parallel sub-sentential fragments from
non-parallel corpora. In Proc. of ACL-COLING
2006.
Malte Nuhn, Arne Mauser and Hermann Ney, 2012.
Deciphering Foreign Language by Combining
Language Models and Context Vectors. In Proc.
of ACL 2012.
Franz Josef Och and Hermann Ney., 2003. A
systematic comparison of various statistical
alignment models. computational linguistics, 29
(1). pages 19-51.
Kishore Papineni, Salim Roukos, Todd Ward and
Wei-Jing Zhu, 2002. BLEU: a method for
automatic evaluation of machine translation. In
Proc. of ACL 2002., pages 311-318.
Chris Quirk, Raghavendra Udupa and Arul Menezes,
2007. Generative models of noisy translations
with applications to parallel fragment extraction.
In Proc. of the Machine Translation Summit XI,
pages 377-384.
Reinhard Rapp, 1995. Identifying word translations in
non-parallel texts. In Proc. of ACL 1995, pages
320-322.
Reinhard Rapp, 1999. Automatic identification of
word translations from unrelated English and
German corpora. In Proc. of ACL 1999, pages
519-526.
Sujith Ravi and Kevin Knight, 2011. Deciphering
foreign language. In Proc. of ACL 2011., pages
12-21.
Holger Schwenk, 2008. Investigations on largescale
lightly-supervised training for statistical machine
translation. In Proc. of IWSLT 2008, pages 182-
189.
Andreas Stolcke, 2002. SRILM-an extensible
language modeling toolkit. In Proc. of 7th
International Conference on Spoken Language
Processing, pages 901-904, Denver, Colorado,
USA, September 16th-20th, 2002.
Nicola Ueffing, Gholamreza Haffari and Anoop
Sarkar, 2007. Transductive learning for statistical
machine translation. In Proc. of ACL 2007.
Hua Wu, Haifeng Wang and Chengqing Zong, 2008.
Domain adaptation for statistical machine
translation with domain dictionary and
monolingual corpora. In Proc. of COLING 2008.,
pages 993-1000.
Feifei Zhai, Jiajun Zhang, Yu Zhou and Chengqing
Zong, 2011. Simple but effective approaches to
improving tree-to-tree model. In Proc. of MT
Summit XIII 2011, pages 261-268.
Feifei Zhai, Jiajun Zhang, Yu Zhou and Chengqing
Zong, 2012. Tree-based translation without using
parse trees. In Proc. of COLING 2012, pages
3037-3054.
Jiajun Zhang, Feifei Zhai and Chengqing Zong, 2011.
Augmenting string-to-tree translation models
with fuzzy use of the source-side syntax. In Proc.
of EMNLP 2011, pages 204-215.
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,
Chew Lim Tan and Sheng Li, 2008. A tree
sequence alignment-based tree-to-tree translation
model. In Proc. of ACL-08: HLT, pages 559-567.
</reference>
<page confidence="0.995078">
1434
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.875880">
<title confidence="0.998512">Learning a Phrase-based Translation Model from Monolingual Data with Application to Domain Adaptation</title>
<author confidence="0.972808">Jiajun Zhang</author>
<author confidence="0.972808">Chengqing Zong</author>
<affiliation confidence="0.998038">National Laboratory of Pattern Recognition Institute of Automation, Chinese Academy of Sciences, Beijing,</affiliation>
<email confidence="0.906308">jjzhang@nlpr.ia.ac.cn</email>
<email confidence="0.906308">cqzong@nlpr.ia.ac.cn</email>
<abstract confidence="0.999862608695652">Currently, almost all of the statistical machine translation (SMT) models are trained with the parallel corpora in some specific domains. However, when it comes to a language pair or a different domain without any bilingual resources, the traditional SMT loses its power. Recently, some research works study the unsupervised SMT for inducing a simple word-based translation model from the monolingual corpora. It successfully bypasses the constraint of bitext for SMT and obtains a relatively promising result. In this paper, we take a step forward and propose a simple but effective method to induce a phrase-based model from the monolingual corpora given an automatically-induced translation lexicon or a manually-edited translation dictionary. We apply our method for the domain adaptation task and the extensive experiments show that our proposed method can substantially improve the translation quality.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>anonymous reviewers for their valuable suggestions.</title>
<marker></marker>
<rawString>anonymous reviewers for their valuable suggestions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Bertoldi</author>
<author>Marcello Federico</author>
</authors>
<title>Domain adaptation for statistical machine translation with monolingual resources.</title>
<date>2009</date>
<booktitle>In Proc. of the Fourth Workshop on Statistical Machine Translation,</booktitle>
<pages>182--189</pages>
<contexts>
<context position="24704" citStr="Bertoldi and Federico, 2009" startWordPosition="4056" endWordPosition="4059">2010). Munteanu and Marcu (2006) first extract the candidate parallel sentences from the comparable corpora and further extract the accurate sub-sentential bilingual fragments from the candidate parallel sentences using the in-domain probabilistic bilingual lexicon. Compared with their work, our focus is to induce phrase pairs directly from monolingual data rather than comparable data. Thus, finding the candidate parallel sentences is not possible in our situation. Another is to make full use of monolingual data with transductive learning (Ueffing et al., 2007; Schwenk, 2008; Wu et al., 2008; Bertoldi and Federico, 2009). For the target-side monolingual data, they just use it to train language model, and for the source-side monolingual data, they employ a baseline (word-based SMT or phrasebased SMT trained with small-scale bitext) to first translate the source sentences, combining the source sentence and its target translation as a bilingual sentence pair, and then train a new phrase-base SMT with these pseudo sentence pairs. This method cannot learn idiom translations and unknown word translations. The third is to estimate the translation parameters and reordering parameters using monolingual data given the </context>
<context position="29379" citStr="Bertoldi and Federico (2009)" startWordPosition="4812" endWordPosition="4815">e model. Table 4 gives the results. We can see from the table that the domain lexicon is much helpful and significantly outperforms the baseline with more than 4.0 BLEU points. When it is enhanced with the in-domain language model, it can further improve the translation performance by more than 2.5 BLEU points. This method has made good use of in-domain lexicon and the target-side indomain monolingual data, but it does not take full advantage of the in-domain source-side monolingual data. In order to use source-side monolingual data, Ueffing et al. (2007), Schwenk (2008), Wu et al. (2008) and Bertoldi and Federico (2009) employed the transductive learning to first translate the source-side monolingual data using the best configuration (baseline+in-domain lexicon+indomain language model) and obtain 1-best translation for each source-side sentence. With the source-side sentences and their translations, the new phrase table and reordering table are built. Then, these resources are added into the best configuration. The experimental results are presented in the last low of Table 4. From the results, we see that transductive learning can further improve the translation performance significantly by 0.6 BLEU points.</context>
</contexts>
<marker>Bertoldi, Federico, 2009</marker>
<rawString>Nicola Bertoldi and Marcello Federico, 2009. Domain adaptation for statistical machine translation with monolingual resources. In Proc. of the Fourth Workshop on Statistical Machine Translation, pages 182-189.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mauro Cettolo</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
</authors>
<title>Mining parallel fragments from comparable texts.</title>
<date>2010</date>
<booktitle>In Proc. of the seventh International Workshop on Spoken Language Translation (IWSLT),</booktitle>
<pages>227--234</pages>
<contexts>
<context position="24081" citStr="Cettolo et al., 2010" startWordPosition="3963" endWordPosition="3966">source sentence with 的 商业 信息, then we compare the position relationship between and 的 商业 信息. We increment the swap count if is just before 的 商业 信息. After counting, we finally use maximum likelihood estimation method to compute the reordering probabilities. 6 Related Work As far as we know, few researchers study phrase pair induction from only monolingual data. There are three research works that are most related with ours. One is using an in-domain probabilistic bilingual lexicon to extract subsentential parallel fragments from comparable corpora (Munteanu and Marcu, 2006; Quirk et al., 2007; Cettolo et al., 2010). Munteanu and Marcu (2006) first extract the candidate parallel sentences from the comparable corpora and further extract the accurate sub-sentential bilingual fragments from the candidate parallel sentences using the in-domain probabilistic bilingual lexicon. Compared with their work, our focus is to induce phrase pairs directly from monolingual data rather than comparable data. Thus, finding the candidate parallel sentences is not possible in our situation. Another is to make full use of monolingual data with transductive learning (Ueffing et al., 2007; Schwenk, 2008; Wu et al., 2008; Berto</context>
</contexts>
<marker>Cettolo, Federico, Bertoldi, 2010</marker>
<rawString>Mauro Cettolo, Marcello Federico and Nicola Bertoldi, 2010. Mining parallel fragments from comparable texts. In Proc. of the seventh International Workshop on Spoken Language Translation (IWSLT), pages 227-234.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation. computational linguistics,</title>
<date>2007</date>
<volume>33</volume>
<issue>2</issue>
<pages>201--228</pages>
<contexts>
<context position="1414" citStr="Chiang, 2007" startWordPosition="203" endWordPosition="204">this paper, we take a step forward and propose a simple but effective method to induce a phrase-based model from the monolingual corpora given an automatically-induced translation lexicon or a manually-edited translation dictionary. We apply our method for the domain adaptation task and the extensive experiments show that our proposed method can substantially improve the translation quality. 1 Introduction During the last decade, statistical machine translation has made great progress. Novel translation models, such as phrase-based models (Koehn et a., 2007), hierarchical phrase-based models (Chiang, 2007) and linguistically syntax-based models (Liu et a., 2006; Huang et al., 2006; Galley, 2006; Zhang et al, 2008; Chiang, 2010; Zhang et al., 2011; Zhai et al., 2011, 2012) have been proposed and achieved higher and higher translation performance. However, all of these state-of-the-art translation models rely on the parallel corpora to induce translation rules and estimate the corresponding parameters. It is unfortunate that the parallel corpora are very expensive to collect and are usually not available for resource-poor languages and for many specific domains even in a resource-rich language pa</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang, 2007. Hierarchical phrase-based translation. computational linguistics, 33 (2). pages 201-228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Learning to translate with source and target syntax.</title>
<date>2010</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>1443--1452</pages>
<contexts>
<context position="1537" citStr="Chiang, 2010" startWordPosition="224" endWordPosition="225">gual corpora given an automatically-induced translation lexicon or a manually-edited translation dictionary. We apply our method for the domain adaptation task and the extensive experiments show that our proposed method can substantially improve the translation quality. 1 Introduction During the last decade, statistical machine translation has made great progress. Novel translation models, such as phrase-based models (Koehn et a., 2007), hierarchical phrase-based models (Chiang, 2007) and linguistically syntax-based models (Liu et a., 2006; Huang et al., 2006; Galley, 2006; Zhang et al, 2008; Chiang, 2010; Zhang et al., 2011; Zhai et al., 2011, 2012) have been proposed and achieved higher and higher translation performance. However, all of these state-of-the-art translation models rely on the parallel corpora to induce translation rules and estimate the corresponding parameters. It is unfortunate that the parallel corpora are very expensive to collect and are usually not available for resource-poor languages and for many specific domains even in a resource-rich language pair. Recently, more and more researchers concentrated on taking full advantage of the monolingual corpora in both source and</context>
</contexts>
<marker>Chiang, 2010</marker>
<rawString>David Chiang, 2010. Learning to translate with source and target syntax. In Proc. of ACL 2010, pages 1443-1452.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daumé</author>
<author>Jagadeesh Jagarlamudi</author>
</authors>
<title>Domain adaptation for machine translation by mining unseen words.</title>
<date>2011</date>
<booktitle>In Proc. of ACL-HLT</booktitle>
<marker>Daumé, Jagarlamudi, 2011</marker>
<rawString>Hal Daumé III and Jagadeesh Jagarlamudi, 2011. Domain adaptation for machine translation by mining unseen words. In Proc. of ACL-HLT 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qing Dou</author>
<author>Kevin Knight</author>
</authors>
<title>Large Scale Decipherment for Out-of-Domain Machine Translation.</title>
<date>2012</date>
<booktitle>In Proc. of EMNLP-CONLL</booktitle>
<contexts>
<context position="2518" citStr="Dou and Knight, 2012" startWordPosition="370" endWordPosition="373">e usually not available for resource-poor languages and for many specific domains even in a resource-rich language pair. Recently, more and more researchers concentrated on taking full advantage of the monolingual corpora in both source and target languages, and proposed methods for bilingual lexicon induction from non-parallel data (Rapp, 1995, 1999; Koehn and Knight, 2002; Haghighi et al., 2008; Daumé III and Jagarlamudi, 2011) and proposed unsupervised statistical machine translation (bilingual lexicon is a byproduct) with only monolingual corpora (Ravi and Knight, 2011; Nuhn et al., 2012; Dou and Knight, 2012). In the bilingual lexicon induction (Koehn and Knight, 2002; Haghighi et al., 2008; Daumé III and Jagarlamudi, 2011), with the help of the orthographic and context features, researchers adopted an unsupervised method, such as canonical correlation analysis (CCA) model, to automatically induce the word translation pairs between two languages from non-parallel data only requiring that the monolingual data in each language are from a fairly comparable domain. The unsupervised statistical machine translation method (Ravi and Knight, 2011; Nuhn et al., 2012; Dou and Knight, 2012) viewed the transl</context>
<context position="3826" citStr="Dou and Knight, 2012" startWordPosition="574" endWordPosition="577"> function to maximize the likelihood of the source language monolingual data. To tackle the large-scale vocabulary, they mainly considered the word-based model (e.g. IBM Model 3) and applied the Bayesian method with Gibbs sampling or slice sampling. Finally, they used the learned translation model directly to translate unseen data (Ravi and Knight, 2011; Nuhn et al., 2012) or incorporated the learned bilingual lexicon as a new in-domain translation resource into the phrase-based model which is trained with out-of-domain data to improve the domain adaptation performance in machine translation (Dou and Knight, 2012). We can easily see that these unsupervised methods can only induce the word-based translation rules (bilingual lexicon) at present. It is a big challenge that whether we can induce phrase 1425 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1425–1434, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics 1, word reordering example: 2, idiom example: 3, unknown word translation: Table 1: Examples of new translation knowledge learned with the proposed phrase pair induction method. For the three fields separated by “ ”, </context>
</contexts>
<marker>Dou, Knight, 2012</marker>
<rawString>Qing Dou and Kevin Knight, 2012. Large Scale Decipherment for Out-of-Domain Machine Translation. In Proc. of EMNLP-CONLL 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Dunning</author>
</authors>
<title>Accurate methods for the statistics of surprise and coincidence. computational linguistics,</title>
<date>1993</date>
<volume>19</volume>
<issue>1</issue>
<pages>61--74</pages>
<contexts>
<context position="8950" citStr="Dunning, 1993" startWordPosition="1401" endWordPosition="1402">08 million sentence pairs1 in News domain to learn a probabilistic bilingual lexicon. Basically, we can use GIZA++ (Och, 2003) to get the probabilistic lexicon. However, the problem is that each source-side word associates too many possible translations which contain much noise. For instance, in the lexicon obtained with GIZA++, each source-side word has about 13 translations on average. The noise of the lexicon can influence the accuracy of the induced phrase pairs to a large extent. To learn a lexicon with a high precision, we follow Munteanu and Marcu (2006) to apply Log-Likelihood-Ratios (Dunning, 1993; Melamed, 2000; Moore, 2004a, 2004b) to estimate how strong the association is between a source-side word and its aligned target-side word. We employ the same algorithm used in (Munteanu and Marcu, 2006) which first use the GIZA++ (with grow-diag-final-and heuristic) to obtain the word alignment between source and target words, and then calculate the association strength between the aligned words. After using the log-likelihood-ratios algorithm2, we obtain a probabilistic bilingual lexicon with bidirectional translation probabilities from the out-of-domain data. In the final lexicon, the numb</context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>Ted Dunning, 1993. Accurate methods for the statistics of surprise and coincidence. computational linguistics, 19 (1). pages 61-74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
<author>Lo Yuen Yee</author>
</authors>
<title>An IR approach for translating new words from nonparallel, comparable texts.</title>
<date>1998</date>
<booktitle>In Proc. of ACLCOLING</booktitle>
<pages>414--420</pages>
<contexts>
<context position="22034" citStr="Fung and Yee, 1998" startWordPosition="3627" endWordPosition="3630">ty, we cannot use maximum likelihood estimation since the phrase pairs are not extracted from parallel sentences. 1429 In this paper, we borrow and extend the idea of (Klementiev et al., 2012) to calculate the phraselevel translation probability with context information in source and target monolingual corpus. The value is calculated using a vector space model. With source and target vocabularies and , the source-side phrase s and target-side phrase t can be respectively represented in an I- and M-dimensional vector. The k-th component of s’s contextual vector is computed using the method of (Fung and Yee, 1998) as follows: (3) wherens,k and nk denotes the number of times sk occurs in the context of s and in the entire source language monolingual data, and n. is the maximum number of occurrence of any source-side word in the source language monolingual data. The k-th element of t’s vector can be computed with the same method. We finally normalize these vectors with L2-norm. With the s’s and t’s contextual vector representations, we calculate two similarities: 1) project s’s vector into target side with the lexical mapping p(t|s), and then get the similarity by computing the cosine of two angles betwe</context>
</contexts>
<marker>Fung, Yee, 1998</marker>
<rawString>Pascale Fung and Lo Yuen Yee, 1998. An IR approach for translating new words from nonparallel, comparable texts. In Proc. of ACLCOLING 1998., pages 414-420.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In Proc. of COLING-ACL</booktitle>
<pages>961--968</pages>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang and Ignacio Thayer, 2006. Scalable inference and training of context-rich syntactic translation models. In Proc. of COLING-ACL 2006, pages 961-968.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Percy Liang</author>
<author>Taylor Berg-Kirkpatrick</author>
<author>Dan Klein</author>
</authors>
<title>Learning bilingual lexicons from monolingual corpora.</title>
<date>2008</date>
<booktitle>In Proc. of ACL-08: HLT,</booktitle>
<pages>771--779</pages>
<contexts>
<context position="2296" citStr="Haghighi et al., 2008" startWordPosition="337" endWordPosition="340">these state-of-the-art translation models rely on the parallel corpora to induce translation rules and estimate the corresponding parameters. It is unfortunate that the parallel corpora are very expensive to collect and are usually not available for resource-poor languages and for many specific domains even in a resource-rich language pair. Recently, more and more researchers concentrated on taking full advantage of the monolingual corpora in both source and target languages, and proposed methods for bilingual lexicon induction from non-parallel data (Rapp, 1995, 1999; Koehn and Knight, 2002; Haghighi et al., 2008; Daumé III and Jagarlamudi, 2011) and proposed unsupervised statistical machine translation (bilingual lexicon is a byproduct) with only monolingual corpora (Ravi and Knight, 2011; Nuhn et al., 2012; Dou and Knight, 2012). In the bilingual lexicon induction (Koehn and Knight, 2002; Haghighi et al., 2008; Daumé III and Jagarlamudi, 2011), with the help of the orthographic and context features, researchers adopted an unsupervised method, such as canonical correlation analysis (CCA) model, to automatically induce the word translation pairs between two languages from non-parallel data only requir</context>
</contexts>
<marker>Haghighi, Liang, Berg-Kirkpatrick, Klein, 2008</marker>
<rawString>Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick and Dan Klein, 2008. Learning bilingual lexicons from monolingual corpora. In Proc. of ACL-08: HLT, pages 771-779.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kevin Knight</author>
<author>Aravind Joshi</author>
</authors>
<title>A syntax-directed translator with extended domain of locality.</title>
<date>2006</date>
<booktitle>In Proc. of AMTA</booktitle>
<pages>1--8</pages>
<contexts>
<context position="1490" citStr="Huang et al., 2006" startWordPosition="213" endWordPosition="216">ethod to induce a phrase-based model from the monolingual corpora given an automatically-induced translation lexicon or a manually-edited translation dictionary. We apply our method for the domain adaptation task and the extensive experiments show that our proposed method can substantially improve the translation quality. 1 Introduction During the last decade, statistical machine translation has made great progress. Novel translation models, such as phrase-based models (Koehn et a., 2007), hierarchical phrase-based models (Chiang, 2007) and linguistically syntax-based models (Liu et a., 2006; Huang et al., 2006; Galley, 2006; Zhang et al, 2008; Chiang, 2010; Zhang et al., 2011; Zhai et al., 2011, 2012) have been proposed and achieved higher and higher translation performance. However, all of these state-of-the-art translation models rely on the parallel corpora to induce translation rules and estimate the corresponding parameters. It is unfortunate that the parallel corpora are very expensive to collect and are usually not available for resource-poor languages and for many specific domains even in a resource-rich language pair. Recently, more and more researchers concentrated on taking full advantag</context>
</contexts>
<marker>Huang, Knight, Joshi, 2006</marker>
<rawString>Liang Huang, Kevin Knight and Aravind Joshi, 2006. A syntax-directed translator with extended domain of locality. In Proc. of AMTA 2006, pages 1-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandre Klementiev</author>
<author>Ann Irvine</author>
<author>Chris CallisonBurch</author>
<author>David Yarowsky</author>
</authors>
<title>Toward statistical machine translation without parallel corpora.</title>
<date>2012</date>
<booktitle>In Proc. of EACL</booktitle>
<pages>130--140</pages>
<contexts>
<context position="21607" citStr="Klementiev et al., 2012" startWordPosition="3558" endWordPosition="3561">based SMT there are four translation probabilities and the reordering probability for each phrase pair. The translation probabilities in the traditional phrase-based SMT include bidirectional phrase translation probabilities and bidirectional lexical weights. For the lexical weights, we can use the plex s |t, a and p, (t I s, a) computed in the above section without length normalization. However, for the phrase-level probability, we cannot use maximum likelihood estimation since the phrase pairs are not extracted from parallel sentences. 1429 In this paper, we borrow and extend the idea of (Klementiev et al., 2012) to calculate the phraselevel translation probability with context information in source and target monolingual corpus. The value is calculated using a vector space model. With source and target vocabularies and , the source-side phrase s and target-side phrase t can be respectively represented in an I- and M-dimensional vector. The k-th component of s’s contextual vector is computed using the method of (Fung and Yee, 1998) as follows: (3) wherens,k and nk denotes the number of times sk occurs in the context of s and in the entire source language monolingual data, and n. is the maximum number </context>
<context position="23025" citStr="Klementiev et al. (2012)" startWordPosition="3788" endWordPosition="3791">m. With the s’s and t’s contextual vector representations, we calculate two similarities: 1) project s’s vector into target side with the lexical mapping p(t|s), and then get the similarity by computing the cosine of two angles between t’s and ’s vectors; 2) project t’s vector into source side with the lexical mapping p(s|t), and then obtain the similarity between s’s and ’s vectors. These two contextual similarities will serve as two phrase-level translation probabilities. 5.3 Reordering Probability Estimation For the reordering probabilities of newly induced phrase pairs, we can also follow Klementiev et al. (2012) to estimate these probabilities using source and target monolingual data. The method is to calculate six probabilities for monotone, swap or discontinuous cases. For the phrase pair (的 商 业 信 息 , business information of), we find a source sentence containing 的 商业 信息, and find a target sentence containing business information of. If there is another phrase pair  s, t, exactly follows business information of and occurs in the same source sentence with 的 商业 信息, then we compare the position relationship between and 的 商业 信息. We increment the swap count if is just before 的 商业 信息. After counting, w</context>
<context position="25342" citStr="Klementiev et al., 2012" startWordPosition="4156" endWordPosition="4159">et-side monolingual data, they just use it to train language model, and for the source-side monolingual data, they employ a baseline (word-based SMT or phrasebased SMT trained with small-scale bitext) to first translate the source sentences, combining the source sentence and its target translation as a bilingual sentence pair, and then train a new phrase-base SMT with these pseudo sentence pairs. This method cannot learn idiom translations and unknown word translations. The third is to estimate the translation parameters and reordering parameters using monolingual data given the phrase pairs (Klementiev et al., 2012). Their work supposes the phrase pairs are already given and then corresponding parameters can be learned with monolingual data. Different from their work, we concentrate ourselves on inducing phrase pairs from monolingual data and then borrow some ideas from theirs for parameter estimation. Furthermore, we extend their contextual similarity between source and target phrases to both directions. 7 Experiments 7.1 Experimental Setup Our purpose is to induce phrase pairs to improve translation quality for domain adaptation. We have introduced the out-of-domain data and the electronic in-domain le</context>
</contexts>
<marker>Klementiev, Irvine, CallisonBurch, Yarowsky, 2012</marker>
<rawString>Alexandre Klementiev, Ann Irvine, Chris CallisonBurch and David Yarowsky, 2012. Toward statistical machine translation without parallel corpora. In Proc. of EACL 2012., pages 130-140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>388--395</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="27598" citStr="Koehn, 2004" startWordPosition="4519" endWordPosition="4520">tion of the English Gigaword. For the in-domain electronic data, we first consider the lexicon as a phrase table in which we assign a constant 1.0 for each of the four probabilities, and then we combine this initial phrase table and the induced phrase pairs to form the new phrase table. The in-domain reordering table is created for the induced phrase pairs. An in-domain 5- gram English language model is trained with the target 1 million monolingual data. We use BLEU (Papineni et al., 2002) score with shortest length penalty as the evaluation metric and apply the pairwise re-sampling approach (Koehn, 2004) to perform the significance test. 7.2 Experimental Results In this section, we first conduct experiments to figure out how the translation performance degrades when the domain changes. To better illustrate the comparison, we first use News data to evaluate the NIST evaluation tests and then use the same News data to evaluate the electronic test sets. For the NIST evaluation, we employ Chinese-to-English NIST MT03 as the tuning set and NIST MT05 as the test set. Table 3 gives the results. It is obvious that, it is relatively high when using the News training data to evaluate the same News test</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn, 2004. Statistical significance tests for machine translation evaluation. In Proc. of EMNLP 2004., pages 388-395, Barcelona, Spain, July 25th-26th, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Chris Dyer, Ondřej Bojar, Alexandra Constantin and Evan Herbst,</title>
<date>2007</date>
<booktitle>In Proc. of ACL on Interactive Poster and Demonstration Sessions</booktitle>
<pages>177--180</pages>
<contexts>
<context position="5479" citStr="Koehn et al., 2007" startWordPosition="827" endWordPosition="830">slation rule induction into two steps: bilingual lexicon induction and phrase pair induction. Since many researchers have studied the bilingual lexicon induction, in this paper, we mainly concentrate ourselves on phrase pair induction given a probabilistic bilingual lexicon and two in-domain large monolingual data (source and target language). In addition, we will further introduce how to refine the induced phrase pairs and estimate the parameters of the induced phrase pairs, such as four standard translation features and phrase reordering feature used in the conventional phrase-based models (Koehn et al., 2007). The induced phrase-based model will be used to help domain adaptation for machine translation. In the rest of this paper, we first explain with examples to show what new translation knowledge can be learned with our proposed phrase pair induction method (Section 2), and then we introduce the approach for probabilistic bilingual lexicon acquisition in Section 3. In Section 4 and 5, we respectively present our method for phrase pair induction and introduce an approach for phrase pair refinement and parameter estimation. Section 6 will show the detailed experiments for the task of domain adapta</context>
<context position="26635" citStr="Koehn et al., 2007" startWordPosition="4359" endWordPosition="4362">omain data. Besides the in-domain lexicon, we have collected respectively 1 million monolingual sentences in electronic area from the web. They are neither parallel nor comparable because we cannot even extract a small number of parallel sentence pairs from this monolingual data using the method of (Munteanu and Marcu, 2006). We further employ experts to translate 2000 Chinese electronic sentences into English. The first half is used as the tuning set (elec1000- tune) and the second half is employed as the testing set (elec1000-test). We construct two kinds of phrase-based models using Moses (Koehn et al., 2007): one uses out-of-domain data and the other uses in-domain data. For the out-of-domain data, we build the phrase table and reordering table using the 2.08 million Chinese-to-English sentence pairs, and we use the SRILM toolkit (Stolcke, 2002) to train the 5-gram English language model with the target part of the parallel sentences and the Xinhua portion of the English Gigaword. For the in-domain electronic data, we first consider the lexicon as a phrase table in which we assign a constant 1.0 for each of the four probabilities, and then we combine this initial phrase table and the induced phra</context>
</contexts>
<marker>Koehn, Hoang, Birch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondřej Bojar, Alexandra Constantin and Evan Herbst, 2007. Moses: Open source toolkit for statistical machine translation. In Proc. of ACL on Interactive Poster and Demonstration Sessions 2007., pages 177-180,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Kevin Knight</author>
</authors>
<title>Learning a translation lexicon from monolingual corpora.</title>
<date>2002</date>
<booktitle>In Proc. of the ACL-02 workshop on Unsupervised lexical acquisition,</booktitle>
<pages>9--16</pages>
<contexts>
<context position="2273" citStr="Koehn and Knight, 2002" startWordPosition="333" endWordPosition="336">rmance. However, all of these state-of-the-art translation models rely on the parallel corpora to induce translation rules and estimate the corresponding parameters. It is unfortunate that the parallel corpora are very expensive to collect and are usually not available for resource-poor languages and for many specific domains even in a resource-rich language pair. Recently, more and more researchers concentrated on taking full advantage of the monolingual corpora in both source and target languages, and proposed methods for bilingual lexicon induction from non-parallel data (Rapp, 1995, 1999; Koehn and Knight, 2002; Haghighi et al., 2008; Daumé III and Jagarlamudi, 2011) and proposed unsupervised statistical machine translation (bilingual lexicon is a byproduct) with only monolingual corpora (Ravi and Knight, 2011; Nuhn et al., 2012; Dou and Knight, 2012). In the bilingual lexicon induction (Koehn and Knight, 2002; Haghighi et al., 2008; Daumé III and Jagarlamudi, 2011), with the help of the orthographic and context features, researchers adopted an unsupervised method, such as canonical correlation analysis (CCA) model, to automatically induce the word translation pairs between two languages from non-pa</context>
</contexts>
<marker>Koehn, Knight, 2002</marker>
<rawString>Philipp Koehn and Kevin Knight, 2002. Learning a translation lexicon from monolingual corpora. In Proc. of the ACL-02 workshop on Unsupervised lexical acquisition, pages 9-16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Tree-tostring alignment template for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proc. of COLING-ACL</booktitle>
<pages>609--616</pages>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu and Shouxun Lin, 2006. Tree-tostring alignment template for statistical machine translation. In Proc. of COLING-ACL 2006, pages 609-616.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
</authors>
<title>Models of translational equivalence among words. computational linguistics,</title>
<date>2000</date>
<volume>26</volume>
<issue>2</issue>
<pages>221--249</pages>
<contexts>
<context position="8965" citStr="Melamed, 2000" startWordPosition="1403" endWordPosition="1404">ence pairs1 in News domain to learn a probabilistic bilingual lexicon. Basically, we can use GIZA++ (Och, 2003) to get the probabilistic lexicon. However, the problem is that each source-side word associates too many possible translations which contain much noise. For instance, in the lexicon obtained with GIZA++, each source-side word has about 13 translations on average. The noise of the lexicon can influence the accuracy of the induced phrase pairs to a large extent. To learn a lexicon with a high precision, we follow Munteanu and Marcu (2006) to apply Log-Likelihood-Ratios (Dunning, 1993; Melamed, 2000; Moore, 2004a, 2004b) to estimate how strong the association is between a source-side word and its aligned target-side word. We employ the same algorithm used in (Munteanu and Marcu, 2006) which first use the GIZA++ (with grow-diag-final-and heuristic) to obtain the word alignment between source and target words, and then calculate the association strength between the aligned words. After using the log-likelihood-ratios algorithm2, we obtain a probabilistic bilingual lexicon with bidirectional translation probabilities from the out-of-domain data. In the final lexicon, the number of average t</context>
</contexts>
<marker>Melamed, 2000</marker>
<rawString>I. Dan Melamed, 2000. Models of translational equivalence among words. computational linguistics, 26 (2). pages 221-249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rorbert C Moore</author>
</authors>
<title>Improving IBM wordalignment model 1. In</title>
<date>2004</date>
<booktitle>Proc. of ACL</booktitle>
<contexts>
<context position="8978" citStr="Moore, 2004" startWordPosition="1405" endWordPosition="1406">News domain to learn a probabilistic bilingual lexicon. Basically, we can use GIZA++ (Och, 2003) to get the probabilistic lexicon. However, the problem is that each source-side word associates too many possible translations which contain much noise. For instance, in the lexicon obtained with GIZA++, each source-side word has about 13 translations on average. The noise of the lexicon can influence the accuracy of the induced phrase pairs to a large extent. To learn a lexicon with a high precision, we follow Munteanu and Marcu (2006) to apply Log-Likelihood-Ratios (Dunning, 1993; Melamed, 2000; Moore, 2004a, 2004b) to estimate how strong the association is between a source-side word and its aligned target-side word. We employ the same algorithm used in (Munteanu and Marcu, 2006) which first use the GIZA++ (with grow-diag-final-and heuristic) to obtain the word alignment between source and target words, and then calculate the association strength between the aligned words. After using the log-likelihood-ratios algorithm2, we obtain a probabilistic bilingual lexicon with bidirectional translation probabilities from the out-of-domain data. In the final lexicon, the number of average translations i</context>
<context position="10719" citStr="Moore (2004" startWordPosition="1667" endWordPosition="1668">ge monolingual data (for language model estimation). With the source language data and its translation, we estimate the bidirectional translation probabilities for each entry in the original lexicon. For the entries whose translation probabilities are not estimated, we just assign a uniform probability. That is if a source word has n translations, then the translation probability of target word given the source word is 1/n. We call this lexicon Domain-lex. 1 LDC category numbers are: LDC2000T50, LDC2003E14, LDC2003E07, LDC2004T07, LDC2005T06, LDC2002L27, LDC2005T10 and LDC2005T34. 2 Following Moore (2004b), we use the threshold 10 on LLR to filter out unlikely translations. We combine LLR-lex and Domain-lex to obtain the final probabilistic bilingual lexicon for phrase pair induction. 4 Phrase Pair Induction Method Given a probabilistic bilingual lexicon and two monolingual data, we present a simple but effective method for phrase pair induction in this section. Input: Probabilistic bilingual lexicon V (each source word s maps a translation set V[s]) Source language monolingual data S={sn} n=1...N Target language monolingual data T={tm} m=1...M Output: Phrase pairs P 1: For each distinct sour</context>
</contexts>
<marker>Moore, 2004</marker>
<rawString>Rorbert C. Moore, 2004a. Improving IBM wordalignment model 1. In Proc. of ACL 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rorbert C Moore</author>
</authors>
<title>On log-likelihood-ratios and the significance of rare events.</title>
<date>2004</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>333--340</pages>
<contexts>
<context position="8978" citStr="Moore, 2004" startWordPosition="1405" endWordPosition="1406">News domain to learn a probabilistic bilingual lexicon. Basically, we can use GIZA++ (Och, 2003) to get the probabilistic lexicon. However, the problem is that each source-side word associates too many possible translations which contain much noise. For instance, in the lexicon obtained with GIZA++, each source-side word has about 13 translations on average. The noise of the lexicon can influence the accuracy of the induced phrase pairs to a large extent. To learn a lexicon with a high precision, we follow Munteanu and Marcu (2006) to apply Log-Likelihood-Ratios (Dunning, 1993; Melamed, 2000; Moore, 2004a, 2004b) to estimate how strong the association is between a source-side word and its aligned target-side word. We employ the same algorithm used in (Munteanu and Marcu, 2006) which first use the GIZA++ (with grow-diag-final-and heuristic) to obtain the word alignment between source and target words, and then calculate the association strength between the aligned words. After using the log-likelihood-ratios algorithm2, we obtain a probabilistic bilingual lexicon with bidirectional translation probabilities from the out-of-domain data. In the final lexicon, the number of average translations i</context>
<context position="10719" citStr="Moore (2004" startWordPosition="1667" endWordPosition="1668">ge monolingual data (for language model estimation). With the source language data and its translation, we estimate the bidirectional translation probabilities for each entry in the original lexicon. For the entries whose translation probabilities are not estimated, we just assign a uniform probability. That is if a source word has n translations, then the translation probability of target word given the source word is 1/n. We call this lexicon Domain-lex. 1 LDC category numbers are: LDC2000T50, LDC2003E14, LDC2003E07, LDC2004T07, LDC2005T06, LDC2002L27, LDC2005T10 and LDC2005T34. 2 Following Moore (2004b), we use the threshold 10 on LLR to filter out unlikely translations. We combine LLR-lex and Domain-lex to obtain the final probabilistic bilingual lexicon for phrase pair induction. 4 Phrase Pair Induction Method Given a probabilistic bilingual lexicon and two monolingual data, we present a simple but effective method for phrase pair induction in this section. Input: Probabilistic bilingual lexicon V (each source word s maps a translation set V[s]) Source language monolingual data S={sn} n=1...N Target language monolingual data T={tm} m=1...M Output: Phrase pairs P 1: For each distinct sour</context>
</contexts>
<marker>Moore, 2004</marker>
<rawString>Rorbert C. Moore, 2004b. On log-likelihood-ratios and the significance of rare events. In Proc. of EMNLP 2004., pages 333-340.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragos Stefan Munteanu</author>
<author>Daniel Marcu</author>
</authors>
<title>Extracting parallel sub-sentential fragments from non-parallel corpora.</title>
<date>2006</date>
<booktitle>In Proc. of ACL-COLING</booktitle>
<contexts>
<context position="8904" citStr="Munteanu and Marcu (2006)" startWordPosition="1394" endWordPosition="1397">e lots of parallel data on News. Here, we utilize about 2.08 million sentence pairs1 in News domain to learn a probabilistic bilingual lexicon. Basically, we can use GIZA++ (Och, 2003) to get the probabilistic lexicon. However, the problem is that each source-side word associates too many possible translations which contain much noise. For instance, in the lexicon obtained with GIZA++, each source-side word has about 13 translations on average. The noise of the lexicon can influence the accuracy of the induced phrase pairs to a large extent. To learn a lexicon with a high precision, we follow Munteanu and Marcu (2006) to apply Log-Likelihood-Ratios (Dunning, 1993; Melamed, 2000; Moore, 2004a, 2004b) to estimate how strong the association is between a source-side word and its aligned target-side word. We employ the same algorithm used in (Munteanu and Marcu, 2006) which first use the GIZA++ (with grow-diag-final-and heuristic) to obtain the word alignment between source and target words, and then calculate the association strength between the aligned words. After using the log-likelihood-ratios algorithm2, we obtain a probabilistic bilingual lexicon with bidirectional translation probabilities from the out-</context>
<context position="24038" citStr="Munteanu and Marcu, 2006" startWordPosition="3955" endWordPosition="3958">usiness information of and occurs in the same source sentence with 的 商业 信息, then we compare the position relationship between and 的 商业 信息. We increment the swap count if is just before 的 商业 信息. After counting, we finally use maximum likelihood estimation method to compute the reordering probabilities. 6 Related Work As far as we know, few researchers study phrase pair induction from only monolingual data. There are three research works that are most related with ours. One is using an in-domain probabilistic bilingual lexicon to extract subsentential parallel fragments from comparable corpora (Munteanu and Marcu, 2006; Quirk et al., 2007; Cettolo et al., 2010). Munteanu and Marcu (2006) first extract the candidate parallel sentences from the comparable corpora and further extract the accurate sub-sentential bilingual fragments from the candidate parallel sentences using the in-domain probabilistic bilingual lexicon. Compared with their work, our focus is to induce phrase pairs directly from monolingual data rather than comparable data. Thus, finding the candidate parallel sentences is not possible in our situation. Another is to make full use of monolingual data with transductive learning (Ueffing et al., </context>
<context position="26342" citStr="Munteanu and Marcu, 2006" startWordPosition="4310" endWordPosition="4313">es to both directions. 7 Experiments 7.1 Experimental Setup Our purpose is to induce phrase pairs to improve translation quality for domain adaptation. We have introduced the out-of-domain data and the electronic in-domain lexicon in Section 3. Here we introduce other information about the in1430 domain data. Besides the in-domain lexicon, we have collected respectively 1 million monolingual sentences in electronic area from the web. They are neither parallel nor comparable because we cannot even extract a small number of parallel sentence pairs from this monolingual data using the method of (Munteanu and Marcu, 2006). We further employ experts to translate 2000 Chinese electronic sentences into English. The first half is used as the tuning set (elec1000- tune) and the second half is employed as the testing set (elec1000-test). We construct two kinds of phrase-based models using Moses (Koehn et al., 2007): one uses out-of-domain data and the other uses in-domain data. For the out-of-domain data, we build the phrase table and reordering table using the 2.08 million Chinese-to-English sentence pairs, and we use the SRILM toolkit (Stolcke, 2002) to train the 5-gram English language model with the target part </context>
</contexts>
<marker>Munteanu, Marcu, 2006</marker>
<rawString>Dragos Stefan Munteanu and Daniel Marcu, 2006. Extracting parallel sub-sentential fragments from non-parallel corpora. In Proc. of ACL-COLING 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Malte Nuhn</author>
<author>Arne Mauser</author>
<author>Hermann Ney</author>
</authors>
<title>Deciphering Foreign Language by Combining Language Models and Context Vectors.</title>
<date>2012</date>
<booktitle>In Proc. of ACL</booktitle>
<contexts>
<context position="2495" citStr="Nuhn et al., 2012" startWordPosition="366" endWordPosition="369">e to collect and are usually not available for resource-poor languages and for many specific domains even in a resource-rich language pair. Recently, more and more researchers concentrated on taking full advantage of the monolingual corpora in both source and target languages, and proposed methods for bilingual lexicon induction from non-parallel data (Rapp, 1995, 1999; Koehn and Knight, 2002; Haghighi et al., 2008; Daumé III and Jagarlamudi, 2011) and proposed unsupervised statistical machine translation (bilingual lexicon is a byproduct) with only monolingual corpora (Ravi and Knight, 2011; Nuhn et al., 2012; Dou and Knight, 2012). In the bilingual lexicon induction (Koehn and Knight, 2002; Haghighi et al., 2008; Daumé III and Jagarlamudi, 2011), with the help of the orthographic and context features, researchers adopted an unsupervised method, such as canonical correlation analysis (CCA) model, to automatically induce the word translation pairs between two languages from non-parallel data only requiring that the monolingual data in each language are from a fairly comparable domain. The unsupervised statistical machine translation method (Ravi and Knight, 2011; Nuhn et al., 2012; Dou and Knight, </context>
</contexts>
<marker>Nuhn, Mauser, Ney, 2012</marker>
<rawString>Malte Nuhn, Arne Mauser and Hermann Ney, 2012. Deciphering Foreign Language by Combining Language Models and Context Vectors. In Proc. of ACL 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models. computational linguistics,</title>
<date>2003</date>
<volume>29</volume>
<issue>1</issue>
<pages>pages</pages>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney., 2003. A systematic comparison of various statistical alignment models. computational linguistics, 29 (1). pages 19-51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>311--318</pages>
<contexts>
<context position="27480" citStr="Papineni et al., 2002" startWordPosition="4499" endWordPosition="4502">kit (Stolcke, 2002) to train the 5-gram English language model with the target part of the parallel sentences and the Xinhua portion of the English Gigaword. For the in-domain electronic data, we first consider the lexicon as a phrase table in which we assign a constant 1.0 for each of the four probabilities, and then we combine this initial phrase table and the induced phrase pairs to form the new phrase table. The in-domain reordering table is created for the induced phrase pairs. An in-domain 5- gram English language model is trained with the target 1 million monolingual data. We use BLEU (Papineni et al., 2002) score with shortest length penalty as the evaluation metric and apply the pairwise re-sampling approach (Koehn, 2004) to perform the significance test. 7.2 Experimental Results In this section, we first conduct experiments to figure out how the translation performance degrades when the domain changes. To better illustrate the comparison, we first use News data to evaluate the NIST evaluation tests and then use the same News data to evaluate the electronic test sets. For the NIST evaluation, we employ Chinese-to-English NIST MT03 as the tuning set and NIST MT05 as the test set. Table 3 gives t</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward and Wei-Jing Zhu, 2002. BLEU: a method for automatic evaluation of machine translation. In Proc. of ACL 2002., pages 311-318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
</authors>
<title>Raghavendra Udupa and Arul Menezes,</title>
<date>2007</date>
<booktitle>In Proc. of the Machine Translation Summit XI,</booktitle>
<pages>377--384</pages>
<marker>Quirk, 2007</marker>
<rawString>Chris Quirk, Raghavendra Udupa and Arul Menezes, 2007. Generative models of noisy translations with applications to parallel fragment extraction. In Proc. of the Machine Translation Summit XI, pages 377-384.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Rapp</author>
</authors>
<title>Identifying word translations in non-parallel texts.</title>
<date>1995</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>320--322</pages>
<contexts>
<context position="2243" citStr="Rapp, 1995" startWordPosition="330" endWordPosition="331"> translation performance. However, all of these state-of-the-art translation models rely on the parallel corpora to induce translation rules and estimate the corresponding parameters. It is unfortunate that the parallel corpora are very expensive to collect and are usually not available for resource-poor languages and for many specific domains even in a resource-rich language pair. Recently, more and more researchers concentrated on taking full advantage of the monolingual corpora in both source and target languages, and proposed methods for bilingual lexicon induction from non-parallel data (Rapp, 1995, 1999; Koehn and Knight, 2002; Haghighi et al., 2008; Daumé III and Jagarlamudi, 2011) and proposed unsupervised statistical machine translation (bilingual lexicon is a byproduct) with only monolingual corpora (Ravi and Knight, 2011; Nuhn et al., 2012; Dou and Knight, 2012). In the bilingual lexicon induction (Koehn and Knight, 2002; Haghighi et al., 2008; Daumé III and Jagarlamudi, 2011), with the help of the orthographic and context features, researchers adopted an unsupervised method, such as canonical correlation analysis (CCA) model, to automatically induce the word translation pairs bet</context>
</contexts>
<marker>Rapp, 1995</marker>
<rawString>Reinhard Rapp, 1995. Identifying word translations in non-parallel texts. In Proc. of ACL 1995, pages 320-322.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Rapp</author>
</authors>
<title>Automatic identification of word translations from unrelated English and German corpora.</title>
<date>1999</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>519--526</pages>
<marker>Rapp, 1999</marker>
<rawString>Reinhard Rapp, 1999. Automatic identification of word translations from unrelated English and German corpora. In Proc. of ACL 1999, pages 519-526.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujith Ravi</author>
<author>Kevin Knight</author>
</authors>
<title>Deciphering foreign language.</title>
<date>2011</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>12--21</pages>
<contexts>
<context position="2476" citStr="Ravi and Knight, 2011" startWordPosition="362" endWordPosition="365">rpora are very expensive to collect and are usually not available for resource-poor languages and for many specific domains even in a resource-rich language pair. Recently, more and more researchers concentrated on taking full advantage of the monolingual corpora in both source and target languages, and proposed methods for bilingual lexicon induction from non-parallel data (Rapp, 1995, 1999; Koehn and Knight, 2002; Haghighi et al., 2008; Daumé III and Jagarlamudi, 2011) and proposed unsupervised statistical machine translation (bilingual lexicon is a byproduct) with only monolingual corpora (Ravi and Knight, 2011; Nuhn et al., 2012; Dou and Knight, 2012). In the bilingual lexicon induction (Koehn and Knight, 2002; Haghighi et al., 2008; Daumé III and Jagarlamudi, 2011), with the help of the orthographic and context features, researchers adopted an unsupervised method, such as canonical correlation analysis (CCA) model, to automatically induce the word translation pairs between two languages from non-parallel data only requiring that the monolingual data in each language are from a fairly comparable domain. The unsupervised statistical machine translation method (Ravi and Knight, 2011; Nuhn et al., 201</context>
</contexts>
<marker>Ravi, Knight, 2011</marker>
<rawString>Sujith Ravi and Kevin Knight, 2011. Deciphering foreign language. In Proc. of ACL 2011., pages 12-21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
</authors>
<title>Investigations on largescale lightly-supervised training for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proc. of IWSLT</booktitle>
<pages>182--189</pages>
<contexts>
<context position="24657" citStr="Schwenk, 2008" startWordPosition="4050" endWordPosition="4051">k et al., 2007; Cettolo et al., 2010). Munteanu and Marcu (2006) first extract the candidate parallel sentences from the comparable corpora and further extract the accurate sub-sentential bilingual fragments from the candidate parallel sentences using the in-domain probabilistic bilingual lexicon. Compared with their work, our focus is to induce phrase pairs directly from monolingual data rather than comparable data. Thus, finding the candidate parallel sentences is not possible in our situation. Another is to make full use of monolingual data with transductive learning (Ueffing et al., 2007; Schwenk, 2008; Wu et al., 2008; Bertoldi and Federico, 2009). For the target-side monolingual data, they just use it to train language model, and for the source-side monolingual data, they employ a baseline (word-based SMT or phrasebased SMT trained with small-scale bitext) to first translate the source sentences, combining the source sentence and its target translation as a bilingual sentence pair, and then train a new phrase-base SMT with these pseudo sentence pairs. This method cannot learn idiom translations and unknown word translations. The third is to estimate the translation parameters and reorderi</context>
<context position="29328" citStr="Schwenk (2008)" startWordPosition="4805" endWordPosition="4806">del besides the out-of-domain language model. Table 4 gives the results. We can see from the table that the domain lexicon is much helpful and significantly outperforms the baseline with more than 4.0 BLEU points. When it is enhanced with the in-domain language model, it can further improve the translation performance by more than 2.5 BLEU points. This method has made good use of in-domain lexicon and the target-side indomain monolingual data, but it does not take full advantage of the in-domain source-side monolingual data. In order to use source-side monolingual data, Ueffing et al. (2007), Schwenk (2008), Wu et al. (2008) and Bertoldi and Federico (2009) employed the transductive learning to first translate the source-side monolingual data using the best configuration (baseline+in-domain lexicon+indomain language model) and obtain 1-best translation for each source-side sentence. With the source-side sentences and their translations, the new phrase table and reordering table are built. Then, these resources are added into the best configuration. The experimental results are presented in the last low of Table 4. From the results, we see that transductive learning can further improve the transl</context>
</contexts>
<marker>Schwenk, 2008</marker>
<rawString>Holger Schwenk, 2008. Investigations on largescale lightly-supervised training for statistical machine translation. In Proc. of IWSLT 2008, pages 182-189.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM-an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proc. of 7th International Conference on Spoken Language Processing,</booktitle>
<pages>901--904</pages>
<location>Denver, Colorado, USA,</location>
<contexts>
<context position="26877" citStr="Stolcke, 2002" startWordPosition="4398" endWordPosition="4399">pairs from this monolingual data using the method of (Munteanu and Marcu, 2006). We further employ experts to translate 2000 Chinese electronic sentences into English. The first half is used as the tuning set (elec1000- tune) and the second half is employed as the testing set (elec1000-test). We construct two kinds of phrase-based models using Moses (Koehn et al., 2007): one uses out-of-domain data and the other uses in-domain data. For the out-of-domain data, we build the phrase table and reordering table using the 2.08 million Chinese-to-English sentence pairs, and we use the SRILM toolkit (Stolcke, 2002) to train the 5-gram English language model with the target part of the parallel sentences and the Xinhua portion of the English Gigaword. For the in-domain electronic data, we first consider the lexicon as a phrase table in which we assign a constant 1.0 for each of the four probabilities, and then we combine this initial phrase table and the induced phrase pairs to form the new phrase table. The in-domain reordering table is created for the induced phrase pairs. An in-domain 5- gram English language model is trained with the target 1 million monolingual data. We use BLEU (Papineni et al., 20</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke, 2002. SRILM-an extensible language modeling toolkit. In Proc. of 7th International Conference on Spoken Language Processing, pages 901-904, Denver, Colorado, USA, September 16th-20th, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Ueffing</author>
</authors>
<title>Gholamreza Haffari and Anoop Sarkar,</title>
<date>2007</date>
<booktitle>In Proc. of ACL</booktitle>
<marker>Ueffing, 2007</marker>
<rawString>Nicola Ueffing, Gholamreza Haffari and Anoop Sarkar, 2007. Transductive learning for statistical machine translation. In Proc. of ACL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hua Wu</author>
</authors>
<title>Haifeng Wang and Chengqing Zong,</title>
<date>2008</date>
<booktitle>In Proc. of COLING</booktitle>
<pages>993--1000</pages>
<marker>Wu, 2008</marker>
<rawString>Hua Wu, Haifeng Wang and Chengqing Zong, 2008. Domain adaptation for statistical machine translation with domain dictionary and monolingual corpora. In Proc. of COLING 2008., pages 993-1000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Feifei Zhai</author>
<author>Jiajun Zhang</author>
<author>Yu Zhou</author>
<author>Chengqing Zong</author>
</authors>
<title>Simple but effective approaches to improving tree-to-tree model.</title>
<date>2011</date>
<booktitle>In Proc. of MT Summit XIII</booktitle>
<pages>261--268</pages>
<contexts>
<context position="1576" citStr="Zhai et al., 2011" startWordPosition="230" endWordPosition="233">y-induced translation lexicon or a manually-edited translation dictionary. We apply our method for the domain adaptation task and the extensive experiments show that our proposed method can substantially improve the translation quality. 1 Introduction During the last decade, statistical machine translation has made great progress. Novel translation models, such as phrase-based models (Koehn et a., 2007), hierarchical phrase-based models (Chiang, 2007) and linguistically syntax-based models (Liu et a., 2006; Huang et al., 2006; Galley, 2006; Zhang et al, 2008; Chiang, 2010; Zhang et al., 2011; Zhai et al., 2011, 2012) have been proposed and achieved higher and higher translation performance. However, all of these state-of-the-art translation models rely on the parallel corpora to induce translation rules and estimate the corresponding parameters. It is unfortunate that the parallel corpora are very expensive to collect and are usually not available for resource-poor languages and for many specific domains even in a resource-rich language pair. Recently, more and more researchers concentrated on taking full advantage of the monolingual corpora in both source and target languages, and proposed methods</context>
</contexts>
<marker>Zhai, Zhang, Zhou, Zong, 2011</marker>
<rawString>Feifei Zhai, Jiajun Zhang, Yu Zhou and Chengqing Zong, 2011. Simple but effective approaches to improving tree-to-tree model. In Proc. of MT Summit XIII 2011, pages 261-268.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Feifei Zhai</author>
<author>Jiajun Zhang</author>
<author>Yu Zhou</author>
<author>Chengqing Zong</author>
</authors>
<title>Tree-based translation without using parse trees.</title>
<date>2012</date>
<booktitle>In Proc. of COLING 2012,</booktitle>
<pages>3037--3054</pages>
<marker>Zhai, Zhang, Zhou, Zong, 2012</marker>
<rawString>Feifei Zhai, Jiajun Zhang, Yu Zhou and Chengqing Zong, 2012. Tree-based translation without using parse trees. In Proc. of COLING 2012, pages 3037-3054.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiajun Zhang</author>
<author>Feifei Zhai</author>
<author>Chengqing Zong</author>
</authors>
<title>Augmenting string-to-tree translation models with fuzzy use of the source-side syntax.</title>
<date>2011</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>204--215</pages>
<contexts>
<context position="1557" citStr="Zhang et al., 2011" startWordPosition="226" endWordPosition="229">iven an automatically-induced translation lexicon or a manually-edited translation dictionary. We apply our method for the domain adaptation task and the extensive experiments show that our proposed method can substantially improve the translation quality. 1 Introduction During the last decade, statistical machine translation has made great progress. Novel translation models, such as phrase-based models (Koehn et a., 2007), hierarchical phrase-based models (Chiang, 2007) and linguistically syntax-based models (Liu et a., 2006; Huang et al., 2006; Galley, 2006; Zhang et al, 2008; Chiang, 2010; Zhang et al., 2011; Zhai et al., 2011, 2012) have been proposed and achieved higher and higher translation performance. However, all of these state-of-the-art translation models rely on the parallel corpora to induce translation rules and estimate the corresponding parameters. It is unfortunate that the parallel corpora are very expensive to collect and are usually not available for resource-poor languages and for many specific domains even in a resource-rich language pair. Recently, more and more researchers concentrated on taking full advantage of the monolingual corpora in both source and target languages, a</context>
</contexts>
<marker>Zhang, Zhai, Zong, 2011</marker>
<rawString>Jiajun Zhang, Feifei Zhai and Chengqing Zong, 2011. Augmenting string-to-tree translation models with fuzzy use of the source-side syntax. In Proc. of EMNLP 2011, pages 204-215.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Min Zhang</author>
</authors>
<title>Hongfei Jiang, Aiti Aw,</title>
<location>Haizhou Li,</location>
<marker>Zhang, </marker>
<rawString>Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>