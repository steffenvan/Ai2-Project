<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.052574">
<title confidence="0.932747">
ULisboa: Identification and Classification of Medical Concepts
</title>
<author confidence="0.948991">
Andr´e Leal+, Diogo Gonc¸alves+, Bruno Martins∗, and Francisco M. Couto+
</author>
<affiliation confidence="0.635655">
+LASIGE, Faculdade de Ciˆencias, Universidade de Lisboa, 1749-016 Lisboa, Portugal.
∗INESC-ID, Instituto Superior T´ecnico, Universidade de Lisboa, Portugal
</affiliation>
<email confidence="0.890278">
faleal,dgoncalvesl@lasige.di.fc.ul.pt,bruno.g.martins@ist.ul.pt,fcouto@di.fc.ul.pt
</email>
<sectionHeader confidence="0.990001" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999973142857143">
This paper describes our participation on
Task 7 of SemEval 2014, which fo-
cused on the recognition and disambigua-
tion of medical concepts. We used an
adapted version of the Stanford NER sys-
tem to train CRF models to recognize tex-
tual spans denoting diseases and disor-
ders, within clinical notes. We consid-
ered an encoding that accounts with non-
continuous entities, together with a rich
set of features (i) based on domain spe-
cific lexicons like SNOMED CT, or (ii)
leveraging Brown clusters inferred from a
large collection of clinical texts. Together
with this recognition mechanism, we used
a heuristic similarity search method, to
assign an unambiguous identifier to each
concept recognized in the text.
Our best run on Task A (i.e., in the recog-
nition of medical concepts in the text)
achieved an F-measure of 0.705 in the
strict evaluation mode, and a promising
F-measure of 0.862 in the relaxed mode,
with a precision of 0.914. For Task B (i.e.,
the disambiguation of the recognized con-
cepts), we achieved less promising results,
with an accuracy of 0.405 in the strict
mode, and of 0.615 in the relaxed mode.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.934027117647059">
Currently, many off-the-shelf named entity recog-
nition solutions are available, and these can be
used to recognize mentions in clinical notes de-
noting diseases and disorders. We decided to use
the Stanford NER tool (Finkel et al., 2005) to train
CRF models based on annotated biomedical text.
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
The use of unsupervised methods for inferring
word representations is nowadays also known to
increase the accuracy of entity recognition mod-
els (Turian et al., 2010). Thus, we also used Brown
clusters (Brown et al., 1992; Turian et al., 2009)
inferred from a large collection of non-annotated
clinical texts, together with domain specific lexi-
cons, to build features for our CRF models.
An important challenge in entity recognition re-
lates to the recognition of overlapping and non-
continuous entities (Alex et al., 2007). In this
paper, we describe how we modified the Stan-
ford NER system to be able to recognize non-
continuous entities, through an adapted version of
the SBIEO scheme (Ratinov and Roth, 2009).
Besides the recognition of medical concepts, we
also present the strategy used to map each of the
recognized concepts into a SNOMED CT identi-
fier (Cornet and de Keizer, 2008). This task is
particularly challenging, since there are many am-
biguous cases. We describe our general approach
to address the aforementioned CUI mapping prob-
lem, based on similarity search and on the infor-
mation content of SNOMED CT concept names.
</bodyText>
<sectionHeader confidence="0.886597" genericHeader="introduction">
2 Task and Datasets
</sectionHeader>
<bodyText confidence="0.991870666666667">
Task 7 of SemEval 2014 actually consisted of two
smaller tasks: recognition of mentions of medi-
cal concepts (Task A) and mapping each medical
concept, recognized in clinical notes, to a unique
UMLS CUI (Task B). In the first task, recogni-
tion of medical concepts, systems have to detect
continuous and discontinuous medical concepts
that belong to the UMLS semantic group disor-
ders. The second task, concerning with normal-
ization and mapping, is limited to UMLS CUIs
of SNOMED CT codes (i.e., although the UMLS
meta-thesaurus integrates several resources, we
are only interested in SNOMED CT). Each con-
cept that was previously recognized can have a
unique CUI associated to it, or none at all (CUI-
</bodyText>
<page confidence="0.971555">
711
</page>
<note confidence="0.729803">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 711–715,
Dublin, Ireland, August 23-24, 2014.
</note>
<bodyText confidence="0.999922538461538">
LESS). The goal here is to disambiguate the con-
cepts and choose the right CUI for each case. For
supporting the recognition and CUI mapping of
medical concepts, we retrieved the disorders sub-
set of SNOMED CT directly from UMLS1.
The evaluation can be done in a strict or a in
a relaxed way. For the case of strict evaluation,
an exact match must be achieved in the recogni-
tion, by having correct start and end offsets, within
the text, for the continuous concepts, and a cor-
rect set of start and end offsets for the discontin-
uous concepts. In the relaxed evaluation, there is
some space for errors in the offset values from the
recognition task. If there is some overlap between
the concepts, then the result is considered a partial
match, otherwise it is a recognition error.
A set of annotated biomedical texts was given
to the participants, separated in three categories:
trial, development and training. We also received a
final test set, and a large set of non-annotated texts.
All the provided texts were initially converted into
a common tokenized format, to be used as input
to the tools that we considered for developing our
approach. After processing, we converted the re-
sults back into the format used by SemEval 2014,
this way generating the official runs.
</bodyText>
<sectionHeader confidence="0.994776" genericHeader="method">
3 Entity Recognition
</sectionHeader>
<bodyText confidence="0.998636476190476">
Our entity recognition approach was based on the
usage of the Stanford NER software, which em-
ploys a linear chain Conditional Random Field
(CRF) approach for building probabilistic mod-
els based on training data (Finkel et al., 2005).
In Stanford NER, model training is based on the
L-BFGS algorithm, and model decoding is made
through the Viterbi algorithm.
This tool requires all input texts to be tokenized
and encoded according to a named entity recog-
nition scheme such as SBIEO (Ratinov and Roth,
2009), characterized by only being able to recog-
nize continuous entities. As we also need to rec-
ognize non-continuous entities, we modified the
Stanford NER software to use a SBIEON encod-
ing scheme. This new scheme has the following
specific token-tag associations:
S: Single, that indicates if the token individually
constitutes an entity to be recognized.
B: Begin, identifying the beginning of the entity.
This tag is only given to the first word of the
</bodyText>
<footnote confidence="0.90533">
1http://www.nlm.nih.gov/research/umls/
</footnote>
<bodyText confidence="0.996587473684211">
entity, being followed is most cases by tokens
labeled as being inside the entity.
I: Inside, representing the continuation of a non
single word entity (i.e., the middle tokens).
E: Ending, representing the last word in the case
of entities composed by more than one word.
N: Non-Continuous, which identifies all the
words that are between the beginning and the
end of an entity, but that do not belong to it.
This label specifically allows us to model the
in-between tokens of non-continuous entities.
O: Other, which is associated to all other words
that are not part of entities.
We developed a Java parser that converts the
biomedical text, provided to the participants, into
a tokenized format. This tokenized format, in the
case of the annotated texts, associates individual
tokens to their SBIEON or SBIEO tags, so that the
datasets can be used as input to train CRF models.
</bodyText>
<subsectionHeader confidence="0.998212">
3.1 Concept Recognition Models
</subsectionHeader>
<bodyText confidence="0.997705533333333">
As we said, SBIEON tokenization differs from
SBIEO by the fact that the first one gives support
to non-continuous entities. Based on these two in-
put schemes, we generated two different models:
Only continuous entities: A 2nd-order CRF
model was trained based on the SBIOE entity en-
coding scheme, which only recognizes continuous
entities. Non-continuous and overlapping entities
will thus, in this case, only be partially modeled
(i.e., we only considered the initial span of text as-
sociated to the non-continuous entities).
Non-continuous entities: A 2nd-order CRF
model was trained based on the SBIOEN entity
encoding scheme, accepting continuous and non-
continuous entities, although still not supporting
the case of overlapping entities. In these last cases,
only the first entity in each of the overlapping
groups will be modeled correctly, while the oth-
ers will only be partially modeled (i.e., by only
considering the non-overlapping spans).
Our CRF models relied on a standard set of fea-
ture templates that includes (i) word tokens within
a window of size 2, (ii) the token shape (e.g., if it
is uppercased, capitalized, numeric, etc.), (iii) to-
ken prefixes and suffixes, (iv) token position (e.g.,
at the beginning or ending of a sentence), and (v)
conjunctions of the current token with the previ-
ous 2 tags. Besides these standard features, we
also considered (a) domain-specific lexicons, and
(b) word representations based on Brown clusters.
</bodyText>
<page confidence="0.982458">
712
</page>
<subsectionHeader confidence="0.997538">
3.2 Word Clusters
</subsectionHeader>
<bodyText confidence="0.999982305555556">
In addition to the annotated training dataset, par-
ticipants were also provided with 403876 non-
annotated texts, containing a total of 828509 to-
kens. We used this information to induce general-
ized cluster-based word representations.
Brown et al. proposed a greedy agglomera-
tive hierarchical clustering procedure that groups
words to maximize the mutual information of bi-
grams (Brown et al., 1992). According to Brown’s
clustering procedure, clusters are initialized as
consisting of a single word each, and are then
greedily merged according to a mutual informa-
tion criterion, based on bi-grams, to form a lower-
dimensional representation of a vocabulary that
can mitigate the feature sparseness problem. In
the context of named entity recognition studies,
several authors have previously noted that using
these types of cluster-based word representations
can indeed result in improvements (Turian et al.,
2009). The hierarchical nature of the clustering
allows words to be represented at different levels
in the hierarchy and, in our case, we considered
1000 different clusters of similar words.
We specifically used the set of training docu-
ments, together with the non-annotated documents
that were provided by the organizers, to induce
word representations based on Brown’s clustering
procedure, using an open-source implementation
that follows the description given by (Turian et al.,
2010). The word clusters are latter used as features
within the Stanford NER package, by considering
that each word can be replaced by the correspond-
ing cluster, this way adding some other back-off
features to the models (i.e., features that are less
sparse, in the sense that they will appear more fre-
quently associated to some of the instances).
</bodyText>
<sectionHeader confidence="0.996489" genericHeader="method">
4 Disambiguating Concepts
</sectionHeader>
<bodyText confidence="0.998548625">
For mapping entities to concept IDs (Task B), we
used a heuristic method based on similarity search,
supported on Lucene indexes (MacCandless et al.,
2010). We look for SNOMED CT concepts that
have a high n-gram overlap with the entity name
occurring in the text, together with the information
content of each SNOMED CT concept.
In our implementation, we used Lucene to re-
trieve candidate SNOMED CT concepts according
to different string distance algorithms: the NGram
distance (Kondrak, 2005) first, then according to
the Jaro-Winkler distance (Winkler, 1990), and fi-
nally according to the Levenshtein distance. The
most similar candidate is chosen as the disam-
biguation. The specific order for the similar-
ity metrics was based on the intuition that met-
rics based on individual character-level matches
are probably not as informative as metrics based
on longer sequences of characters, although they
can be useful for dealing with spelling variations.
However, for future work, we plan to explore more
systematic approaches (e.g., based on learning to
rank) for combining multiple similarity metrics.
Additionally to the aforementioned similarity
metrics, a measure of the Information Content (IC)
of each SNOMED CT concept was also employed,
to further disambiguate the mappings (i.e., to se-
lect the SNOMED CT identifier that is more gen-
eral, and thus more likely to be associated to a par-
ticular concept descriptor). Notice that the IC of
a concept corresponds to a measure of its speci-
ficity, where higher values correspond to more
specific concepts, and lower values to more gen-
eral ones. Given the frequency freq(c) for each
concept c in a corpus (i.e., the same corpus that
was used to infer the word clusters), the informa-
tion content of this concept can be computed from
the ratio between its frequency (including its de-
scendants) and the maximum frequency of all con-
cepts (Resnik, 1995):
(freq(c) 1
maxFreq )
In the formula, maxFreq represents the maximum
frequency of a concept, i.e. the frequency of the
root concept, when it exists. The frequency of a
concept can be computed using an extrinsic ap-
proach that counts the exact matches of the con-
cept names on a large text corpus.
</bodyText>
<sectionHeader confidence="0.994347" genericHeader="method">
5 Evaluation Experiments
</sectionHeader>
<bodyText confidence="0.974924666666667">
We submitted three distinct runs to the SemEval
competition. These runs were as follows:
Run 1: A SBIOEN model was used to recog-
nize non-continuous entities. This model was
trained using only the annotated texts from
the provided training set. We also used some
domain specific lexicons like SNOMED CT,
or lists with names for drugs and diseases
retrieved from DBPedia. Finally, the recog-
nition model also used Brown clusters gen-
erated from the non-annotated datasets pro-
vided in the competition.
</bodyText>
<equation confidence="0.988008">
IC(c) = − log
</equation>
<page confidence="0.984911">
713
</page>
<bodyText confidence="0.986032272727273">
For assigning a SNOMED CT identifier to
each entity, we used the disambiguation tech-
nique supported by Lucene indexes. In
this specific run we used all the considered
heuristics for similarity search.
Run 2: A simpler model based on the SBIOE
scheme was used in this case, which can only
recognize continuous entities. The same fea-
tures from Run 1 were used for training the
recognition model.
For assigning the SNOMED CT identifier to
each entity, we also used the same strategy
that was presented for Run 1.
Run 3: A similar SBIOE model to that from Run
2 was used for the recognition.
For assigning the corresponding SNOMED
CT identifier to each entity, we in this case
limited the heuristic rules that were used.
Instead of using the string similarity algo-
rithms, we used only exact matches, together
with the information content measure and the
neighboring terms for disambiguation.
</bodyText>
<sectionHeader confidence="0.999059" genericHeader="evaluation">
6 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999804214285715">
We present our official results in Table 1, which
highlights our best results for each task.
Specifically in Task A, we achieved encourag-
ing results. Run 1 achieved an F-measure of 0.705
in the strict evaluation, and of 0.862 in the relaxed
evaluation. Since Runs 2 and 3 used the same
recognition strategy (i.e., models that attempted to
capture only the continuous entities), we obtained
the same results for Task A in both these runs. Ta-
ble 1 also shows that our performance in Task B
was significantly lower than in Task A.
As we can see in the table, our first run was the
one with the best results for Task A. The model
used on this run recognizes non-continuous enti-
ties, and this is perhaps the main reason for the
higher results (i.e., the other two runs used the
same features for the recognition models).
On what concerns the results of Task B, it is im-
portant to notice the distinct results from the first
and second runs, which used exactly the same dis-
ambiguation strategy. The differences in the re-
sults are a consequence from the use of a different
recognition model in Task A. We can see that the
ability to recognize non-continuous entities leads
to the generation of worse mappings, when con-
sidering our specific disambiguation strategy. Our
last run is the best in terms of the performance over
Task B, but the difference is subtle.
</bodyText>
<sectionHeader confidence="0.960485" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999968063829787">
This paper described our participation in Task 7 of
the SemEval 2014 competition, which was divided
into two subtasks, namely (i) the recognition of
continuous and non-continuous medical concepts,
and (ii) the mapping of each recognized concept to
a SNOMED CT identifier.
For the first task, we used the Stanford NER
software (Finkel et al., 2005), modified by us
to recognize not only continuous, but also non-
continuous entities. This was possible by intro-
ducing the SBIEON scheme, derived from the tra-
ditional SBIEO encoding. To increase the accu-
racy and precision of the recognition we have also
used domain specific lexicons and Brown clusters
inferred from non-annotated documents.
For the second task, we used a heuristic method
based on similarity search, for matching concepts
in the text against concepts from SNOMED CT,
together with a measure of information content
to disambiguate the cases of term polysemy in
SNOMED CT. We implemented our disambigua-
tion approach through the Lucene software frame-
work (MacCandless et al., 2010).
In the first task (Task A) we achieved some par-
ticularly encouraging results, showing that an off-
the-shelf NER system can be easily adapted to
the recognition of medical concepts in biomedi-
cal text. Our specific modifications to the Stanford
NER system, in order to support the recognition of
non-continuous entity names, indeed increased the
precision and recall on Task A. However, our ap-
proach for the disambiguation of the recognized
concepts (Task B) performed much worse, achiev-
ing an accuracy of 0.615 in the case of the relaxed
evaluation. Future developments will therefore fo-
cus on improving the component that addressed
the entity disambiguation subtask.
Specifically on what regards future work, we
plan to experiment with the usage of machine
learning methods for the disambiguation subtask,
instead of relying on a purely heuristic approach.
We are interested in experimenting with the us-
age of Learning to Rank (L2R) methods, similar to
those employed on the DNorm system (Leaman et
al., 2013), to optimally combine different heuris-
tics such as the ones that were used in our current
approach. A L2R model can be used to rank candi-
</bodyText>
<page confidence="0.994331">
714
</page>
<table confidence="0.981209833333333">
Task A Task B
Strict Evaluation Relaxed Evaluation Strict Relaxed
Run Precision Recall F-measure Precision Recall F-measure Accuracy Accuracy
1 0.753 0.663 0.705 0.914 0.815 0.862 0.402 0.606
2 0.752 0.660 0.703 0.909 0.806 0.855 0.404 0.612
3 0.752 0.660 0.703 0.909 0.806 0.855 0.405 0.615
</table>
<tableCaption confidence="0.999867">
Table 1: Our official results for Tasks A and B of the SemEval challenge focusing on clinical text.
</tableCaption>
<bodyText confidence="0.999905272727273">
date disambiguations (e.g., retrieved through sim-
ilarity search with basis on Lucene) according to a
combination of multiple criteria, and we can then
choose the top candidate as the disambiguation.
Additionally, we plan to use ontology-based
similarity measures to validate and improve the
mappings (Couto and Pinto, 2013). For example,
by assuming that all entities in a given span of text
are semantically related with each other, we can
use ontology relations to filter likely misannota-
tions (Grego and Couto, 2013; Grego et al., 2013).
</bodyText>
<sectionHeader confidence="0.998561" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999731333333333">
The authors would like to thank Fundac¸˜ao para a
Ciˆencia e Tecnologia (FCT) for the financial sup-
port of SOMER (PTDC/EIA-EIA/119119/2010),
LASIGE (PEst-OE/EEI/UI0408/2014) and
INESC-ID (Pest-OE/EEI/LA0021/2013).
We also would like to thank our colleagues
Berta Alves, for her support in evaluating devel-
opment errors, and Lu´ıs Atalaya, for the develop-
ment of some of the data pre-processing scripts.
</bodyText>
<sectionHeader confidence="0.998574" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999884253731343">
Beatrice Alex, Barry Haddow, and Claire Grover.
2007. Recognising nested named entities in biomed-
ical text. In Proceedings of the ACL-07 Workshop
on Biological, Translational, and Clinical Language
Processing, pages 65–72.
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational linguistics, 18(4):467–479.
Ronald Cornet and Nicolette de Keizer. 2008. Forty
years of SNOMED: A literature review. BMC
Medical Informatics and Decision Making, 8(Suppl
1:S2):1–6.
Francisco M. Couto and Helena Sofia Pinto. 2013. The
next generation of similarity measures that fully ex-
plore the semantics in biomedical ontologies. Jour-
nal of Bioinformatics and Computational Biology,
11(05):1–11.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 363–370.
Tiago Grego and Francisco M Couto. 2013. Enhance-
ment of chemical entity identification in text using
semantic similarity validation. PloS ONE, 8(5):1–9.
Tiago Grego, Francisco Pinto, and Francisco Couto.
2013. LASIGE: using conditional random fields and
chebi ontology. In Proceedings of the 7th Inter-
national Workshop on Semantic Evaluation, pages
660–666.
Grzegorz Kondrak. 2005. N-gram similarity and dis-
tance. In Proceedings of the 12th International
Conference String Processing and Information Re-
trieval, pages 115–126.
Robert Leaman, Rezarta Islamaj Do˘gan, and Zhiy-
ong Lu. 2013. DNorm: disease name normaliza-
tion with pairwise learning to rank. Bioinformatics,
29(22):2909–2917.
Michael MacCandless, Erik Hatcher, and Otis Gospod-
neti´c. 2010. Lucene in Action. Manning Publica-
tions Company.
Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
Proceedings of the 13th Conference on Computa-
tional Natural Language Learning, pages 147–155.
Philip Resnik. 1995. Using information content to
evaluate semantic similarity in a taxonomy. In Pro-
ceedings of the 14th International Joint Conference
on Artificial Intelligence, pages 448–453.
Joseph Turian, Lev Ratinov, Yoshua Bengio, and Dan
Roth. 2009. A preliminary evaluation of word rep-
resentations for named-entity recognition. In Pro-
ceedings of the NIPS-09 Workshop on Grammar In-
duction, Representation of Language and Language
Learning, pages 1–8.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 384–394.
William E Winkler. 1990. String comparator metrics
and enhanced decision rules in the Fellegi-Sunter
model of record linkage. In Proceedings of the Sec-
tion on Survey Research of the American Statistical
Association, pages 354–359.
</reference>
<page confidence="0.998554">
715
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.819782">
<title confidence="0.996936">ULisboa: Identification and Classification of Medical Concepts</title>
<author confidence="0.970485">Diogo Bruno</author>
<author confidence="0.970485">M Francisco</author>
<affiliation confidence="0.944551">Faculdade de Ciˆencias, Universidade de Lisboa, 1749-016 Lisboa,</affiliation>
<address confidence="0.862053">Instituto Superior T´ecnico, Universidade de Lisboa, Portugal</address>
<abstract confidence="0.998719655172413">This paper describes our participation on Task 7 of SemEval 2014, which focused on the recognition and disambiguation of medical concepts. We used an adapted version of the Stanford NER system to train CRF models to recognize textual spans denoting diseases and disorders, within clinical notes. We considered an encoding that accounts with noncontinuous entities, together with a rich set of features (i) based on domain specific lexicons like SNOMED CT, or (ii) leveraging Brown clusters inferred from a large collection of clinical texts. Together with this recognition mechanism, we used a heuristic similarity search method, to assign an unambiguous identifier to each concept recognized in the text. Our best run on Task A (i.e., in the recognition of medical concepts in the text) achieved an F-measure of 0.705 in the strict evaluation mode, and a promising F-measure of 0.862 in the relaxed mode, with a precision of 0.914. For Task B (i.e., the disambiguation of the recognized concepts), we achieved less promising results, with an accuracy of 0.405 in the strict mode, and of 0.615 in the relaxed mode.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Beatrice Alex</author>
<author>Barry Haddow</author>
<author>Claire Grover</author>
</authors>
<title>Recognising nested named entities in biomedical text.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL-07 Workshop on Biological, Translational, and Clinical Language Processing,</booktitle>
<pages>65--72</pages>
<contexts>
<context position="2540" citStr="Alex et al., 2007" startWordPosition="382" endWordPosition="385"> footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ The use of unsupervised methods for inferring word representations is nowadays also known to increase the accuracy of entity recognition models (Turian et al., 2010). Thus, we also used Brown clusters (Brown et al., 1992; Turian et al., 2009) inferred from a large collection of non-annotated clinical texts, together with domain specific lexicons, to build features for our CRF models. An important challenge in entity recognition relates to the recognition of overlapping and noncontinuous entities (Alex et al., 2007). In this paper, we describe how we modified the Stanford NER system to be able to recognize noncontinuous entities, through an adapted version of the SBIEO scheme (Ratinov and Roth, 2009). Besides the recognition of medical concepts, we also present the strategy used to map each of the recognized concepts into a SNOMED CT identifier (Cornet and de Keizer, 2008). This task is particularly challenging, since there are many ambiguous cases. We describe our general approach to address the aforementioned CUI mapping problem, based on similarity search and on the information content of SNOMED CT co</context>
</contexts>
<marker>Alex, Haddow, Grover, 2007</marker>
<rawString>Beatrice Alex, Barry Haddow, and Claire Grover. 2007. Recognising nested named entities in biomedical text. In Proceedings of the ACL-07 Workshop on Biological, Translational, and Clinical Language Processing, pages 65–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Peter V deSouza</author>
<author>Robert L Mercer</author>
<author>Vincent J Della Pietra</author>
<author>Jenifer C Lai</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="2240" citStr="Brown et al., 1992" startWordPosition="335" endWordPosition="338">cognize mentions in clinical notes denoting diseases and disorders. We decided to use the Stanford NER tool (Finkel et al., 2005) to train CRF models based on annotated biomedical text. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ The use of unsupervised methods for inferring word representations is nowadays also known to increase the accuracy of entity recognition models (Turian et al., 2010). Thus, we also used Brown clusters (Brown et al., 1992; Turian et al., 2009) inferred from a large collection of non-annotated clinical texts, together with domain specific lexicons, to build features for our CRF models. An important challenge in entity recognition relates to the recognition of overlapping and noncontinuous entities (Alex et al., 2007). In this paper, we describe how we modified the Stanford NER system to be able to recognize noncontinuous entities, through an adapted version of the SBIEO scheme (Ratinov and Roth, 2009). Besides the recognition of medical concepts, we also present the strategy used to map each of the recognized c</context>
<context position="9080" citStr="Brown et al., 1992" startWordPosition="1454" endWordPosition="1457">nd (v) conjunctions of the current token with the previous 2 tags. Besides these standard features, we also considered (a) domain-specific lexicons, and (b) word representations based on Brown clusters. 712 3.2 Word Clusters In addition to the annotated training dataset, participants were also provided with 403876 nonannotated texts, containing a total of 828509 tokens. We used this information to induce generalized cluster-based word representations. Brown et al. proposed a greedy agglomerative hierarchical clustering procedure that groups words to maximize the mutual information of bigrams (Brown et al., 1992). According to Brown’s clustering procedure, clusters are initialized as consisting of a single word each, and are then greedily merged according to a mutual information criterion, based on bi-grams, to form a lowerdimensional representation of a vocabulary that can mitigate the feature sparseness problem. In the context of named entity recognition studies, several authors have previously noted that using these types of cluster-based word representations can indeed result in improvements (Turian et al., 2009). The hierarchical nature of the clustering allows words to be represented at differen</context>
</contexts>
<marker>Brown, deSouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Class-based n-gram models of natural language. Computational linguistics, 18(4):467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Cornet</author>
<author>Nicolette de Keizer</author>
</authors>
<title>Forty years of SNOMED: A literature review.</title>
<date>2008</date>
<booktitle>BMC Medical Informatics and Decision Making, 8(Suppl 1:S2):1–6.</booktitle>
<marker>Cornet, de Keizer, 2008</marker>
<rawString>Ronald Cornet and Nicolette de Keizer. 2008. Forty years of SNOMED: A literature review. BMC Medical Informatics and Decision Making, 8(Suppl 1:S2):1–6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francisco M Couto</author>
<author>Helena Sofia Pinto</author>
</authors>
<title>The next generation of similarity measures that fully explore the semantics in biomedical ontologies.</title>
<date>2013</date>
<journal>Journal of Bioinformatics and Computational Biology,</journal>
<volume>11</volume>
<issue>05</issue>
<contexts>
<context position="18382" citStr="Couto and Pinto, 2013" startWordPosition="2980" endWordPosition="2983">ure Precision Recall F-measure Accuracy Accuracy 1 0.753 0.663 0.705 0.914 0.815 0.862 0.402 0.606 2 0.752 0.660 0.703 0.909 0.806 0.855 0.404 0.612 3 0.752 0.660 0.703 0.909 0.806 0.855 0.405 0.615 Table 1: Our official results for Tasks A and B of the SemEval challenge focusing on clinical text. date disambiguations (e.g., retrieved through similarity search with basis on Lucene) according to a combination of multiple criteria, and we can then choose the top candidate as the disambiguation. Additionally, we plan to use ontology-based similarity measures to validate and improve the mappings (Couto and Pinto, 2013). For example, by assuming that all entities in a given span of text are semantically related with each other, we can use ontology relations to filter likely misannotations (Grego and Couto, 2013; Grego et al., 2013). Acknowledgments The authors would like to thank Fundac¸˜ao para a Ciˆencia e Tecnologia (FCT) for the financial support of SOMER (PTDC/EIA-EIA/119119/2010), LASIGE (PEst-OE/EEI/UI0408/2014) and INESC-ID (Pest-OE/EEI/LA0021/2013). We also would like to thank our colleagues Berta Alves, for her support in evaluating development errors, and Lu´ıs Atalaya, for the development of some</context>
</contexts>
<marker>Couto, Pinto, 2013</marker>
<rawString>Francisco M. Couto and Helena Sofia Pinto. 2013. The next generation of similarity measures that fully explore the semantics in biomedical ontologies. Journal of Bioinformatics and Computational Biology, 11(05):1–11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by Gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>363--370</pages>
<contexts>
<context position="1751" citStr="Finkel et al., 2005" startWordPosition="263" endWordPosition="266">ognition of medical concepts in the text) achieved an F-measure of 0.705 in the strict evaluation mode, and a promising F-measure of 0.862 in the relaxed mode, with a precision of 0.914. For Task B (i.e., the disambiguation of the recognized concepts), we achieved less promising results, with an accuracy of 0.405 in the strict mode, and of 0.615 in the relaxed mode. 1 Introduction Currently, many off-the-shelf named entity recognition solutions are available, and these can be used to recognize mentions in clinical notes denoting diseases and disorders. We decided to use the Stanford NER tool (Finkel et al., 2005) to train CRF models based on annotated biomedical text. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ The use of unsupervised methods for inferring word representations is nowadays also known to increase the accuracy of entity recognition models (Turian et al., 2010). Thus, we also used Brown clusters (Brown et al., 1992; Turian et al., 2009) inferred from a large collection of non-annotated clinical texts, together with domain s</context>
<context position="5533" citStr="Finkel et al., 2005" startWordPosition="885" endWordPosition="888">ning. We also received a final test set, and a large set of non-annotated texts. All the provided texts were initially converted into a common tokenized format, to be used as input to the tools that we considered for developing our approach. After processing, we converted the results back into the format used by SemEval 2014, this way generating the official runs. 3 Entity Recognition Our entity recognition approach was based on the usage of the Stanford NER software, which employs a linear chain Conditional Random Field (CRF) approach for building probabilistic models based on training data (Finkel et al., 2005). In Stanford NER, model training is based on the L-BFGS algorithm, and model decoding is made through the Viterbi algorithm. This tool requires all input texts to be tokenized and encoded according to a named entity recognition scheme such as SBIEO (Ratinov and Roth, 2009), characterized by only being able to recognize continuous entities. As we also need to recognize non-continuous entities, we modified the Stanford NER software to use a SBIEON encoding scheme. This new scheme has the following specific token-tag associations: S: Single, that indicates if the token individually constitutes a</context>
<context position="15821" citStr="Finkel et al., 2005" startWordPosition="2569" endWordPosition="2572">the ability to recognize non-continuous entities leads to the generation of worse mappings, when considering our specific disambiguation strategy. Our last run is the best in terms of the performance over Task B, but the difference is subtle. 7 Conclusions and Future Work This paper described our participation in Task 7 of the SemEval 2014 competition, which was divided into two subtasks, namely (i) the recognition of continuous and non-continuous medical concepts, and (ii) the mapping of each recognized concept to a SNOMED CT identifier. For the first task, we used the Stanford NER software (Finkel et al., 2005), modified by us to recognize not only continuous, but also noncontinuous entities. This was possible by introducing the SBIEON scheme, derived from the traditional SBIEO encoding. To increase the accuracy and precision of the recognition we have also used domain specific lexicons and Brown clusters inferred from non-annotated documents. For the second task, we used a heuristic method based on similarity search, for matching concepts in the text against concepts from SNOMED CT, together with a measure of information content to disambiguate the cases of term polysemy in SNOMED CT. We implemente</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by Gibbs sampling. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 363–370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tiago Grego</author>
<author>Francisco M Couto</author>
</authors>
<title>Enhancement of chemical entity identification in text using semantic similarity validation.</title>
<date>2013</date>
<journal>PloS ONE,</journal>
<volume>8</volume>
<issue>5</issue>
<marker>Grego, Couto, 2013</marker>
<rawString>Tiago Grego and Francisco M Couto. 2013. Enhancement of chemical entity identification in text using semantic similarity validation. PloS ONE, 8(5):1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tiago Grego</author>
<author>Francisco Pinto</author>
<author>Francisco Couto</author>
</authors>
<title>LASIGE: using conditional random fields and chebi ontology.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th International Workshop on Semantic Evaluation,</booktitle>
<pages>660--666</pages>
<marker>Grego, Pinto, Couto, 2013</marker>
<rawString>Tiago Grego, Francisco Pinto, and Francisco Couto. 2013. LASIGE: using conditional random fields and chebi ontology. In Proceedings of the 7th International Workshop on Semantic Evaluation, pages 660–666.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grzegorz Kondrak</author>
</authors>
<title>N-gram similarity and distance.</title>
<date>2005</date>
<booktitle>In Proceedings of the 12th International Conference String Processing and Information Retrieval,</booktitle>
<pages>115--126</pages>
<contexts>
<context position="10939" citStr="Kondrak, 2005" startWordPosition="1744" endWordPosition="1745">arse, in the sense that they will appear more frequently associated to some of the instances). 4 Disambiguating Concepts For mapping entities to concept IDs (Task B), we used a heuristic method based on similarity search, supported on Lucene indexes (MacCandless et al., 2010). We look for SNOMED CT concepts that have a high n-gram overlap with the entity name occurring in the text, together with the information content of each SNOMED CT concept. In our implementation, we used Lucene to retrieve candidate SNOMED CT concepts according to different string distance algorithms: the NGram distance (Kondrak, 2005) first, then according to the Jaro-Winkler distance (Winkler, 1990), and finally according to the Levenshtein distance. The most similar candidate is chosen as the disambiguation. The specific order for the similarity metrics was based on the intuition that metrics based on individual character-level matches are probably not as informative as metrics based on longer sequences of characters, although they can be useful for dealing with spelling variations. However, for future work, we plan to explore more systematic approaches (e.g., based on learning to rank) for combining multiple similarity </context>
</contexts>
<marker>Kondrak, 2005</marker>
<rawString>Grzegorz Kondrak. 2005. N-gram similarity and distance. In Proceedings of the 12th International Conference String Processing and Information Retrieval, pages 115–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Leaman</author>
<author>Rezarta Islamaj Do˘gan</author>
<author>Zhiyong Lu</author>
</authors>
<title>DNorm: disease name normalization with pairwise learning to rank.</title>
<date>2013</date>
<journal>Bioinformatics,</journal>
<volume>29</volume>
<issue>22</issue>
<marker>Leaman, Do˘gan, Lu, 2013</marker>
<rawString>Robert Leaman, Rezarta Islamaj Do˘gan, and Zhiyong Lu. 2013. DNorm: disease name normalization with pairwise learning to rank. Bioinformatics, 29(22):2909–2917.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael MacCandless</author>
<author>Erik Hatcher</author>
<author>Otis Gospodneti´c</author>
</authors>
<title>Lucene in Action.</title>
<date>2010</date>
<publisher>Manning Publications Company.</publisher>
<marker>MacCandless, Hatcher, Gospodneti´c, 2010</marker>
<rawString>Michael MacCandless, Erik Hatcher, and Otis Gospodneti´c. 2010. Lucene in Action. Manning Publications Company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Ratinov</author>
<author>Dan Roth</author>
</authors>
<title>Design challenges and misconceptions in named entity recognition.</title>
<date>2009</date>
<booktitle>In Proceedings of the 13th Conference on Computational Natural Language Learning,</booktitle>
<pages>147--155</pages>
<contexts>
<context position="2728" citStr="Ratinov and Roth, 2009" startWordPosition="415" endWordPosition="418">nown to increase the accuracy of entity recognition models (Turian et al., 2010). Thus, we also used Brown clusters (Brown et al., 1992; Turian et al., 2009) inferred from a large collection of non-annotated clinical texts, together with domain specific lexicons, to build features for our CRF models. An important challenge in entity recognition relates to the recognition of overlapping and noncontinuous entities (Alex et al., 2007). In this paper, we describe how we modified the Stanford NER system to be able to recognize noncontinuous entities, through an adapted version of the SBIEO scheme (Ratinov and Roth, 2009). Besides the recognition of medical concepts, we also present the strategy used to map each of the recognized concepts into a SNOMED CT identifier (Cornet and de Keizer, 2008). This task is particularly challenging, since there are many ambiguous cases. We describe our general approach to address the aforementioned CUI mapping problem, based on similarity search and on the information content of SNOMED CT concept names. 2 Task and Datasets Task 7 of SemEval 2014 actually consisted of two smaller tasks: recognition of mentions of medical concepts (Task A) and mapping each medical concept, reco</context>
<context position="5807" citStr="Ratinov and Roth, 2009" startWordPosition="931" endWordPosition="934">d the results back into the format used by SemEval 2014, this way generating the official runs. 3 Entity Recognition Our entity recognition approach was based on the usage of the Stanford NER software, which employs a linear chain Conditional Random Field (CRF) approach for building probabilistic models based on training data (Finkel et al., 2005). In Stanford NER, model training is based on the L-BFGS algorithm, and model decoding is made through the Viterbi algorithm. This tool requires all input texts to be tokenized and encoded according to a named entity recognition scheme such as SBIEO (Ratinov and Roth, 2009), characterized by only being able to recognize continuous entities. As we also need to recognize non-continuous entities, we modified the Stanford NER software to use a SBIEON encoding scheme. This new scheme has the following specific token-tag associations: S: Single, that indicates if the token individually constitutes an entity to be recognized. B: Begin, identifying the beginning of the entity. This tag is only given to the first word of the 1http://www.nlm.nih.gov/research/umls/ entity, being followed is most cases by tokens labeled as being inside the entity. I: Inside, representing th</context>
</contexts>
<marker>Ratinov, Roth, 2009</marker>
<rawString>Lev Ratinov and Dan Roth. 2009. Design challenges and misconceptions in named entity recognition. In Proceedings of the 13th Conference on Computational Natural Language Learning, pages 147–155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Using information content to evaluate semantic similarity in a taxonomy.</title>
<date>1995</date>
<booktitle>In Proceedings of the 14th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>448--453</pages>
<contexts>
<context position="12342" citStr="Resnik, 1995" startWordPosition="1973" endWordPosition="1974">.e., to select the SNOMED CT identifier that is more general, and thus more likely to be associated to a particular concept descriptor). Notice that the IC of a concept corresponds to a measure of its specificity, where higher values correspond to more specific concepts, and lower values to more general ones. Given the frequency freq(c) for each concept c in a corpus (i.e., the same corpus that was used to infer the word clusters), the information content of this concept can be computed from the ratio between its frequency (including its descendants) and the maximum frequency of all concepts (Resnik, 1995): (freq(c) 1 maxFreq ) In the formula, maxFreq represents the maximum frequency of a concept, i.e. the frequency of the root concept, when it exists. The frequency of a concept can be computed using an extrinsic approach that counts the exact matches of the concept names on a large text corpus. 5 Evaluation Experiments We submitted three distinct runs to the SemEval competition. These runs were as follows: Run 1: A SBIOEN model was used to recognize non-continuous entities. This model was trained using only the annotated texts from the provided training set. We also used some domain specific l</context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>Philip Resnik. 1995. Using information content to evaluate semantic similarity in a taxonomy. In Proceedings of the 14th International Joint Conference on Artificial Intelligence, pages 448–453.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
<author>Dan Roth</author>
</authors>
<title>A preliminary evaluation of word representations for named-entity recognition.</title>
<date>2009</date>
<booktitle>In Proceedings of the NIPS-09 Workshop on Grammar Induction, Representation of Language and Language Learning,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="2262" citStr="Turian et al., 2009" startWordPosition="339" endWordPosition="342">clinical notes denoting diseases and disorders. We decided to use the Stanford NER tool (Finkel et al., 2005) to train CRF models based on annotated biomedical text. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ The use of unsupervised methods for inferring word representations is nowadays also known to increase the accuracy of entity recognition models (Turian et al., 2010). Thus, we also used Brown clusters (Brown et al., 1992; Turian et al., 2009) inferred from a large collection of non-annotated clinical texts, together with domain specific lexicons, to build features for our CRF models. An important challenge in entity recognition relates to the recognition of overlapping and noncontinuous entities (Alex et al., 2007). In this paper, we describe how we modified the Stanford NER system to be able to recognize noncontinuous entities, through an adapted version of the SBIEO scheme (Ratinov and Roth, 2009). Besides the recognition of medical concepts, we also present the strategy used to map each of the recognized concepts into a SNOMED </context>
<context position="9594" citStr="Turian et al., 2009" startWordPosition="1530" endWordPosition="1533"> clustering procedure that groups words to maximize the mutual information of bigrams (Brown et al., 1992). According to Brown’s clustering procedure, clusters are initialized as consisting of a single word each, and are then greedily merged according to a mutual information criterion, based on bi-grams, to form a lowerdimensional representation of a vocabulary that can mitigate the feature sparseness problem. In the context of named entity recognition studies, several authors have previously noted that using these types of cluster-based word representations can indeed result in improvements (Turian et al., 2009). The hierarchical nature of the clustering allows words to be represented at different levels in the hierarchy and, in our case, we considered 1000 different clusters of similar words. We specifically used the set of training documents, together with the non-annotated documents that were provided by the organizers, to induce word representations based on Brown’s clustering procedure, using an open-source implementation that follows the description given by (Turian et al., 2010). The word clusters are latter used as features within the Stanford NER package, by considering that each word can be</context>
</contexts>
<marker>Turian, Ratinov, Bengio, Roth, 2009</marker>
<rawString>Joseph Turian, Lev Ratinov, Yoshua Bengio, and Dan Roth. 2009. A preliminary evaluation of word representations for named-entity recognition. In Proceedings of the NIPS-09 Workshop on Grammar Induction, Representation of Language and Language Learning, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: A simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>384--394</pages>
<contexts>
<context position="2185" citStr="Turian et al., 2010" startWordPosition="325" endWordPosition="328">tion solutions are available, and these can be used to recognize mentions in clinical notes denoting diseases and disorders. We decided to use the Stanford NER tool (Finkel et al., 2005) to train CRF models based on annotated biomedical text. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ The use of unsupervised methods for inferring word representations is nowadays also known to increase the accuracy of entity recognition models (Turian et al., 2010). Thus, we also used Brown clusters (Brown et al., 1992; Turian et al., 2009) inferred from a large collection of non-annotated clinical texts, together with domain specific lexicons, to build features for our CRF models. An important challenge in entity recognition relates to the recognition of overlapping and noncontinuous entities (Alex et al., 2007). In this paper, we describe how we modified the Stanford NER system to be able to recognize noncontinuous entities, through an adapted version of the SBIEO scheme (Ratinov and Roth, 2009). Besides the recognition of medical concepts, we also pr</context>
<context position="10077" citStr="Turian et al., 2010" startWordPosition="1602" endWordPosition="1605">ave previously noted that using these types of cluster-based word representations can indeed result in improvements (Turian et al., 2009). The hierarchical nature of the clustering allows words to be represented at different levels in the hierarchy and, in our case, we considered 1000 different clusters of similar words. We specifically used the set of training documents, together with the non-annotated documents that were provided by the organizers, to induce word representations based on Brown’s clustering procedure, using an open-source implementation that follows the description given by (Turian et al., 2010). The word clusters are latter used as features within the Stanford NER package, by considering that each word can be replaced by the corresponding cluster, this way adding some other back-off features to the models (i.e., features that are less sparse, in the sense that they will appear more frequently associated to some of the instances). 4 Disambiguating Concepts For mapping entities to concept IDs (Task B), we used a heuristic method based on similarity search, supported on Lucene indexes (MacCandless et al., 2010). We look for SNOMED CT concepts that have a high n-gram overlap with the en</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 384–394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William E Winkler</author>
</authors>
<title>String comparator metrics and enhanced decision rules in the Fellegi-Sunter model of record linkage.</title>
<date>1990</date>
<booktitle>In Proceedings of the Section on Survey Research of the American Statistical Association,</booktitle>
<pages>354--359</pages>
<contexts>
<context position="11006" citStr="Winkler, 1990" startWordPosition="1753" endWordPosition="1754"> to some of the instances). 4 Disambiguating Concepts For mapping entities to concept IDs (Task B), we used a heuristic method based on similarity search, supported on Lucene indexes (MacCandless et al., 2010). We look for SNOMED CT concepts that have a high n-gram overlap with the entity name occurring in the text, together with the information content of each SNOMED CT concept. In our implementation, we used Lucene to retrieve candidate SNOMED CT concepts according to different string distance algorithms: the NGram distance (Kondrak, 2005) first, then according to the Jaro-Winkler distance (Winkler, 1990), and finally according to the Levenshtein distance. The most similar candidate is chosen as the disambiguation. The specific order for the similarity metrics was based on the intuition that metrics based on individual character-level matches are probably not as informative as metrics based on longer sequences of characters, although they can be useful for dealing with spelling variations. However, for future work, we plan to explore more systematic approaches (e.g., based on learning to rank) for combining multiple similarity metrics. Additionally to the aforementioned similarity metrics, a m</context>
</contexts>
<marker>Winkler, 1990</marker>
<rawString>William E Winkler. 1990. String comparator metrics and enhanced decision rules in the Fellegi-Sunter model of record linkage. In Proceedings of the Section on Survey Research of the American Statistical Association, pages 354–359.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>