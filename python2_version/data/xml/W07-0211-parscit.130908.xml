<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000890">
<title confidence="0.9940135">
DLSITE-2: Semantic Similarity Based on Syntactic
Dependency Trees Applied to Textual Entailment
</title>
<author confidence="0.998896">
Daniel Micol, ´Oscar Ferr´andez, Rafael Mu˜noz, and Manuel Palomar
</author>
<affiliation confidence="0.996069">
Natural Language Processing and Information Systems Group
Department of Computing Languages and Systems
University of Alicante
</affiliation>
<address confidence="0.839572">
San Vicente del Raspeig, Alicante 03690, Spain
</address>
<email confidence="0.982852">
{dmicol, ofe, rafael, mpalomar}@dlsi.ua.es
</email>
<sectionHeader confidence="0.995514" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9899085">
In this paper we attempt to deduce tex-
tual entailment based on syntactic depen-
dency trees of a given text-hypothesis pair.
The goals of this project are to provide an
accurate and fast system, which we have
called DLSITE-2, that can be applied in
software systems that require a near-real-
time interaction with the user. To accom-
plish this we use MINIPAR to parse the
phrases and construct their correspond-
ing trees. Later on we apply syntactic-
based techniques to calculate the seman-
tic similarity between text and hypothe-
sis. To measure our method’s precision we
used the test text corpus set from Second
PASCAL Recognising Textual Entailment
Challenge (RTE-2), obtaining an accuracy
rate of 60.75%.
</bodyText>
<sectionHeader confidence="0.999114" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999983142857143">
There are several methods used to determine tex-
tual entailment for a given text-hypothesis pair. The
one described in this paper uses the information
contained in the syntactic dependency trees of such
phrases to deduce whether there is entailment or
not. In addition, semantic knowledge extracted from
WordNet (Miller et al., 1990) has been added to
achieve higher accuracy rates.
It has been proven in several competitions and
other workshops that textual entailment is a complex
task. One of these competitions is PASCAL Recog-
nising Textual Entailment Challenge (Bar-Haim et
al., 2006), where each participating group develops a
textual entailment recognizing system attempting to
accomplish the best accuracy rate of all competitors.
Such complexity is the reason why we use a combi-
nation of various techniques to deduce whether en-
tailment is produced.
Currently there are few research projects related
to the topic discussed in this paper. Some systems
use syntactic tree matching as the textual entailment
decision core module, such as (Katrenko and Adri-
aans, 2006). It is based on maximal embedded syn-
tactic subtrees to analyze the semantic relation be-
tween text and hypothesis. Other systems use syn-
tactic trees as a collaborative module, not being the
core, such as (Herrera et al., 2006). The application
discussed in this paper belongs to the first set of sys-
tems, since syntactic matching is its main module.
The remainder of this paper is structured as fol-
lows. In the second section we will describe the
methods implemented in our system. The third one
contains the experimental results, and the fourth and
last discusses such results and proposes future work
based on our actual research.
</bodyText>
<sectionHeader confidence="0.997459" genericHeader="introduction">
2 Methods
</sectionHeader>
<bodyText confidence="0.999956555555556">
The system we have built aims to provide a good
accuracy rate in a short lapse of time, making it
feasible to be included in applications that require
near-real-time responses due to their interaction with
the user. Such a system is composed of few mod-
ules that behave collaboratively. These include tree
construction, filtering, embedded subtree search and
graph node matching. A schematic representation of
the system architecture is shown in Figure 1.
</bodyText>
<page confidence="0.982085">
73
</page>
<note confidence="0.9674065">
TextGraphs-2: Graph-Based Al orithms for Natural Language Processing, pages 73–80,
Rochester, April 2007 (c 2007 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999958">
Figure 1: DLSITE-2 system architecture.
</figureCaption>
<bodyText confidence="0.9995655">
Each of the steps or modules of DLSITE-2 is de-
scribed in the following subsections, that are num-
bered sequentially according to their execution or-
der.
</bodyText>
<subsectionHeader confidence="0.995855">
2.1 Tree generation
</subsectionHeader>
<bodyText confidence="0.999911875">
The first module constructs the corresponding syn-
tactic dependency trees. For this purpose, MINI-
PAR (Lin, 1998) output is generated and afterwards
parsed for each text and hypothesis of our corpus.
Phrase tokens, along with their grammatical infor-
mation, are stored in an on-memory data structure
that represents a tree, which is equivalent to the men-
tioned syntactic dependency tree.
</bodyText>
<subsectionHeader confidence="0.99943">
2.2 Tree filtering
</subsectionHeader>
<bodyText confidence="0.947083105263158">
Once the tree has been constructed, we may want
to discard irrelevant data in order to reduce our sys-
tem’s response time and noise. For this purpose we
have generated a database of relevant grammatical
categories, represented in Table 1, that will allow
us to remove from the tree all those tokens whose
category does not belong to such list. The result-
ing tree will have the same structure as the original,
but will not contain any stop words nor irrelevant to-
kens, such as determinants or auxiliary verbs. The
whole list of ignored grammatical categories is rep-
resented in Table 2.
We have performed tests taking into account and
discarding each grammatical category, which has al-
lowed us to generate both lists of relevant and ig-
nored grammatical categories.
Verbs, verbs with one argument, verbs with two ar-
guments, verbs taking clause as complement, verb
Have, verb Be
</bodyText>
<figure confidence="0.8011016">
Nouns
Numbers
Adjectives
Adverbs
Noun-noun modifiers
</figure>
<tableCaption confidence="0.992667">
Table 1: Relevant grammatical categories.
</tableCaption>
<subsectionHeader confidence="0.997702">
2.3 Graph embedding detection
</subsectionHeader>
<bodyText confidence="0.999734">
The next step of our system consists in determining
whether the hypothesis’ tree is embedded into the
text’s. Let us first define the concept of embedded
tree (Katrenko and Adriaans, 2006).
</bodyText>
<equation confidence="0.9850416">
Definition 1: Embedded tree A tree
T1 = (V1, E1) is embedded into another
one T2 = (V2, E2) iff
1. V1 V2, and
2. E1 C E2
</equation>
<bodyText confidence="0.999747875">
where V1 and V2 represent the vertices,
and E1 and E2 the edges.
In other words, a tree, T1, is embedded into an-
other one, T2, if all nodes and branches of T1 are
presentin T2.
We believe that it makes sense to reduce the strict-
ness of such a definition to allow the appearance
of intermediate nodes in the text’s branches that are
</bodyText>
<page confidence="0.994091">
74
</page>
<figure confidence="0.951555555555556">
Determiners
Pre-determiners
Post-determiners
Clauses
Inflectional phrases
Preposition and preposition phrases
Specifiers of preposition phrases
Auxiliary verbs
Complementizers
</figure>
<tableCaption confidence="0.973959">
Table 2: Ignored grammatical categories.
</tableCaption>
<bodyText confidence="0.998159307692308">
not present in the corresponding hypothesis’ branch,
which means that we allow partial matching. There-
fore, a match between two branches will be pro-
duced if all nodes of the first one, namely 01 C E1,
are present in the second, namely 02 C E2, and their
respective order is the same, allowing the possibil-
ity of appearance of intermediate nodes that are not
present in both branches. This is also described in
(Katrenko and Adriaans, 2006).
To determine whether the hypothesis’ tree is em-
bedded into the text’s, we perform a top-down
matching process. For this purpose we first compare
the roots of both trees. If they coincide, we then pro-
ceed to compare their respective child nodes, which
are the tokens that have some sort of dependency
with their respective root token.
In order to add more flexibility to our system,
we do not require the pair of tokens to be ex-
actly the same, but rather set a threshold that rep-
resents the minimum similarity value between them.
This is a difference between our approach and the
one described in (Katrenko and Adriaans, 2006).
Such a similarity is calculated by using the Word-
Net::Similarity tool (Pedersen et al., 2004), and,
concretely, the Wu-Palmer measure, as defined in
Equation 1 (Wu and Palmer, 1994).
</bodyText>
<equation confidence="0.958135666666667">
2N3
Sim(C1, C2) � (1)
N1 + N2 + 2N3
</equation>
<bodyText confidence="0.995595571428571">
where C1 and C2 are the synsets whose similarity
we want to calculate, C3 is their least common su-
perconcept, N1 is the number of nodes on the path
from C1 to C3, N2 is the number of nodes on the
path from C2 to C3, and N3 is the number of nodes
on the path from C3 to the root. All these synsets
and distances can be observed in Figure 2.
</bodyText>
<figureCaption confidence="0.990133">
Figure 2: Distance between two synsets.
</figureCaption>
<bodyText confidence="0.99962752">
If the similarity rate is greater or equal than the
established threshold, which we have set empirically
to 80%, we will consider the corresponding hypoth-
esis’ token as suitable to have the same meaning
as the text’s token, and will proceed to compare its
child nodes in the hypothesis’ tree. On the other
hand, if such similarity value is less than the cor-
responding threshold, we will proceed to compare
the children of such text’s tree node with the actual
hypothesis’ node that was being analyzed.
The comparison between the syntactic depen-
dency trees of both text and hypothesis will be com-
pleted when all nodes of either tree have been pro-
cessed. If we have been able to find a match for all
the tokens within the hypothesis, the corresponding
tree will be embedded into the text’s and we will be-
lieve that there is entailment. If not, we will not be
able to assure that such an implication is produced
and will proceed to execute the next module of our
system.
Next, we will present a text-hypothesis pair sam-
ple where the syntactic dependency tree of the hy-
pothesis (Figure 3(b)) is embedded into the text’s
(Figure 3(a)). The mentioned text-hypothesis pair
is the following:
</bodyText>
<construct confidence="0.7055795">
Text: Mossad is one of the world’s most
well-known intelligence agencies, and is
often viewed in the same regard as the CIA
and MI6.
Hypothesis: Mossad is an intelligence
agency.
</construct>
<page confidence="0.982419">
75
</page>
<figure confidence="0.968321">
(a) Mossad is one of the world’s most well-known intelligence agencies, and is often viewed (b) Mossad is an intelligence
in the same regard as the CIA and MI6. agency.
</figure>
<figureCaption confidence="0.999951">
Figure 3: Representation of a hypothesis’ syntactic dependency tree that is embedded into the text’s.
</figureCaption>
<bodyText confidence="0.999944888888889">
As one can see in Figure 3, the hypothesis’ syn-
tactic dependency tree represented is embedded into
the text’s because all of its nodes are present in
the text in the same order. There is one exception
though, that is the word an. However, since it is a
determinant, the filtering module will have deleted
it before the graph embedding test is performed.
Therefore, in this example the entailment would be
recognized.
</bodyText>
<subsectionHeader confidence="0.997363">
2.4 Graph node matching
</subsectionHeader>
<bodyText confidence="0.997052">
Once the embedded subtree comparison has fin-
ished, and if its result is negative, we proceed to per-
form a graph node matching process, termed align-
ment, between both the text and the hypothesis. This
operation consists in finding pairs of tokens in both
trees whose lemmas are identical, no matter whether
they are in the same position within the tree. We
would like to point out that in this step we do not
use the WordNet::Similarity tool.
Some authors have already designed similar
matching techniques, such as the ones described in
(MacCartney et al., 2006) and (Snow et al., 2006).
However, these include semantic constraints that we
have decided not to consider. The reason of this
decision is that we desired to overcome the textual
entailment recognition from an exclusively syntactic
perspective. Therefore, we did not want this module
to include any kind of semantic knowledge.
The weight given to a token that has been found
in both trees will depend on the depth in the hypoth-
esis’ tree and the token’s grammatical relevance.
The first of these factors depends on an empirically-
calculated weight that assigns less importance to a
node the deeper it is located in the tree. This weight
is defined in Equation 2. The second factor gives
different relevance depending on the grammatical
category and relationship. For instance, a verb will
have the highest weight, while an adverb or an ad-
jective will have less relevance. The values assigned
to each grammatical category and relationship are
also empirically-calculated and are shown in Tables
3 and 4, respectively.
</bodyText>
<table confidence="0.931405444444445">
Grammatical category Weight
Verbs, verbs with one argument, verbs 1.0
with two arguments, verbs taking
clause as complement
Nouns, numbers 0.75
Be used as a linking verb 0.7
Adjectives, adverbs, noun-noun mod- 0.5
ifiers
Verbs Have and Be 0.3
</table>
<tableCaption confidence="0.9771115">
Table 3: Weights assigned to the grammatical cate-
gories.
</tableCaption>
<page confidence="0.712167">
76
</page>
<table confidence="0.908125">
Grammatical relationship Weight
Subject of verbs, surface subject, ob- 1.0
ject of verbs, second object of ditran-
sitive verbs
The rest 0.5
</table>
<tableCaption confidence="0.8703485">
Table 4: Weights assigned to the grammatical rela-
tionships.
</tableCaption>
<bodyText confidence="0.999819363636363">
Let τ and λ represent the text’s and hypothesis’
syntactic dependency trees, respectively. We as-
sume we have found members of a synset, namely β,
present in both τ and λ. Now let γ be the weight as-
signed to β’s grammatical category (defined in Table
3), σ the weight of β’s grammatical relationship (de-
fined in Table 4), µ an empirically-calculated value
that represents the weight difference between tree
levels, and δp the depth of the node that contains
the synset β in λ. We define the function φ(β) as
represented in Equation 2.
</bodyText>
<equation confidence="0.999346">
φ(β) = γ - σ - µ−bp (2)
</equation>
<bodyText confidence="0.999923166666667">
The value obtained by calculating the expression
of Equation 2 would represent the relevance of a
synset in our system. The experiments performed
reveal that the optimal value for µ is 1.1.
For a given pair (τ, λ), we define the set ξ as the
one that contains the synsets present in both trees:
</bodyText>
<equation confidence="0.945589">
ξ = τ n λ Vα E τ,β E λ (3)
</equation>
<bodyText confidence="0.9976735">
Therefore, the similarity rate between τ and λ, de-
noted by the symbol ψ, would be defined as:
</bodyText>
<equation confidence="0.9607695">
ψ(τ, λ) = E φ(ν) (4)
&apos;EC
</equation>
<bodyText confidence="0.9992195">
One should note that a requirement of our sys-
tem’s similarity measure would be to be independent
of the hypothesis length. Thus, we must define the
normalized similarity rate, as shown in Equation 5.
</bodyText>
<equation confidence="0.956789166666667">
E
ψ(τ, λ) = ψ(τ, λ)
φ(β) E
&apos;EC
φ(ν)
pEa
</equation>
<bodyText confidence="0.999853181818182">
Once the similarity value, ψ(τ, λ), has been cal-
culated, it will be provided to the user together with
the corresponding text-hypothesis pair identifier. It
will be his responsibility to choose an appropriate
threshold that will represent the minimum similarity
rate to be considered as entailment between text and
hypothesis. All values that are under such a thresh-
old will be marked as not entailed. For this purpose,
we suggest using a development corpus in order to
obtain the optimal threshold value, as it is done in
the RTE challenges.
</bodyText>
<sectionHeader confidence="0.996742" genericHeader="method">
3 Experimental results
</sectionHeader>
<bodyText confidence="0.991536878787879">
The experimental results shown in this paper were
obtained processing a set of text-hypothesis pairs
from RTE-2. The organizers of this challenge pro-
vide development and test corpora to the partic-
ipants, both of them containing 800 pairs manu-
ally annotated for logical entailment. It is com-
posed of four subsets, each of them correspond-
ing to typical true and false entailments in different
tasks, such as Information Extraction (IE), Informa-
tion Retrieval (IR), Question Answering (QA), and
Multi-document Summarization (SUM). For each
task, the annotators selected the same amount of true
entailments as negative ones (50%-50% split).
The organizers have also defined two measures to
evaluate the participating systems. All judgments
returned by the systems will be compared to those
manually assigned by the human annotators. The
percentage of matching judgments will provide the
accuracy of the system, i.e. the percentage of cor-
rect responses. As a second measure, the average
precision will be computed. This measure evaluates
the ability of the systems to rank all the pairs in the
corpus according to their entailment confidence, in
decreasing order from the most certain entailment to
the least. Average precision is a common evaluation
measure for system rankings that is defined as shown
in Equation 6.
E(i)#correct up to pair i (6)
i
where n is the amount of the pairs in the test corpus,
R is the total number of positive pairs in it, i ranges
over the pairs, ordered by their ranking, and E(i) is
defined as follows:
</bodyText>
<figure confidence="0.965424875">
E
pEa
φ(β)
1
AP = R
(5)
n
���
</figure>
<page confidence="0.875037">
77
</page>
<equation confidence="0.847446333333333">
1 if the i − th pair is positive,
(7)
0 otherwise.
</equation>
<bodyText confidence="0.999734454545455">
As we previously mentioned, we tested our sys-
tem against RTE-2 development corpus, and used
the test one to evaluate it.
First, Table 5 shows the accuracy (ACC) and av-
erage precision (AP), both as a percentage, obtained
processing the development corpus from RTE-2 for
a threshold value of 68.9%, which corresponds to
the highest accuracy that can be obtained using our
system for the mentioned corpus. It also provides
the rate of correctly predicted true and false entail-
ments.
</bodyText>
<table confidence="0.9848905">
Task ACC AP TRUE FALSE
IE 52.00 51.49 54.00 50.00
IR 55.50 58.99 32.00 79.00
QA 57.50 54.72 53.00 62.00
SUM 65.00 81.35 39.00 91.00
Overall 57.50 58.96 44.50 70.50
</table>
<tableCaption confidence="0.8232605">
Table 5: Results obtained for the development cor-
pus.
</tableCaption>
<bodyText confidence="0.99909575">
Next, let us show in Table 6 the results obtained
processing the test corpus, which is the one used
to compare the different systems that participated in
RTE-2, with the same threshold as before.
</bodyText>
<table confidence="0.998315166666667">
Task ACC AP TRUE FALSE
IE 50.50 47.33 75.00 26.00
IR 64.50 67.67 59.00 70.00
QA 59.50 58.16 80.00 39.00
SUM 68.50 75.86 49.00 88.00
Overall 60.75 57.91 65.75 55.75
</table>
<tableCaption confidence="0.999727">
Table 6: Results obtained for the test corpus.
</tableCaption>
<bodyText confidence="0.999785181818182">
As one can observe in the previous table, our
system provides a high accuracy rate by using
mainly syntactical measures. The number of text-
hypothesis pairs that succeeded the graph embed-
ding evaluation was three for the development cor-
pus and one for the test set, which reflects the strict-
ness of such module. However, we would like to
point out that the amount of pairs affected by the
mentioned module will depend on the corpus na-
ture, so it can vary significantly between different
corpora.
Let us now compare our results with the ones that
were achieved by the systems that participated in
RTE-2. One should note that the criteria for such
ranking is based exclusively on the accuracy, ignor-
ing the average precision value. In addition, each
participating group was allowed to submit two dif-
ferent systems to RTE-2. We will consider here the
best result of both systems for each group. The men-
tioned comparison is shown in Table 7, and contains
only the systems that had higher accuracy rates than
our approach.
</bodyText>
<table confidence="0.998617571428571">
Participant Accuracy
(Hickl et al., 2006) 75.38
(Tatu et al., 2006) 73.75
(Zanzotto et al., 2006) 63.88
(Adams, 2006) 62.62
(Bos and Markert, 2006) 61.62
DLSITE-2 60.75
</table>
<tableCaption confidence="0.9551015">
Table 7: Comparison of some of the teams that par-
ticipated in RTE-2.
</tableCaption>
<bodyText confidence="0.999807095238095">
As it is reflected in Table 7, our system would
have obtained the sixth position out of twenty-four
participants, which is an accomplishment consider-
ing the limited number of resources that it has built-
in.
Since one of our system’s modules is based on
(Katrenko and Adriaans, 2006), we will compare
their results with ours to analyze whether the modi-
fications we introduced perform correctly. In RTE-
2, they obtained an accuracy rate of 59.00% for the
test corpus. The reason why we believe we have
achieved better results than their system is due to
the fact that we added semantic knowledge to our
graph embedding module. In addition, the syntactic
dependency trees to which we have applied such a
module have been previously filtered to ensure that
they do not contain irrelevant words. This reduces
the system’s noise and allows us to achieve higher
accuracy rates.
In the introduction of this paper we mentioned
that one of the goals of our system was to provide
</bodyText>
<equation confidence="0.990373">
E(i) = {
</equation>
<page confidence="0.980091">
78
</page>
<bodyText confidence="0.999873">
a high accuracy rate in a short lapse of time. This is
one of the reasons why we chose to construct a light
system where one of the aspects to minimize was its
response time. Table 8 shows the execution times1
of our system for both development and test text cor-
pora from RTE-2. These include total and average2
response times.
structure X, Y, Z into X is Y, and Z. For the shown
example, the resulting text and hypothesis would be
as follows:
</bodyText>
<note confidence="0.963817">
Text: Tony Blair is the British Prime Min-
ister, and met Jacques Chirac in London.
Hypothesis: Tony Blair is the British
</note>
<table confidence="0.6408335">
Prime Minister.
Development Test
Total 1045 1023
Average 1.30625 1.27875
</table>
<tableCaption confidence="0.993633">
Table 8: DLSITE-2 response times (in seconds).
</tableCaption>
<bodyText confidence="0.992968379310345">
As we can see, accurate results can be obtained
using syntactic dependency trees in a short lapse of
time. However, there are some limitations that our
system does not avoid. For instance, the tree em-
bedding test is not applicable when there is no verb
entailment. This is reflected in the following pair:
Text: Tony Blair, the British Prime Minis-
ter, met Jacques Chirac in London.
Hypothesis: Tony Blair is the British
Prime Minister.
The root node of the hypothesis’ tree would be
the one corresponding to the verb is. Since the en-
tailment here is implicit, there is no need for such a
verb to appear in the text. However, this is not com-
patible with our system, since is would not match
any node of the text’s tree, and thus the hypothesis’
tree would not be found embedded into the text’s.
The graph matching process would not behave
correctly either. This is due to the fact that the main
verb, which has the maximum weight because it is
the root of the hypothesis’ tree and its grammatical
category has the maximum relevance, is not present
in the text, so the overall similarity score would have
a considerable handicap.
The example of limitation of our system that we
have presented is an apposition. To avoid this spe-
cific kind of situations that produce an undesired be-
havior in our system, we could add a preprocess-
ing module that transforms the phrases that have the
</bodyText>
<footnote confidence="0.98650225">
1The machine we used to measure the response times had an
Intel Core 2 Duo processor at 2GHz.
2Average response times are calculated diving the totals by
the number of pairs in the corpus.
</footnote>
<bodyText confidence="0.99985975">
The transformed text would still be syntactically
correct, and the entailment would be detected since
the hypothesis’ syntactic dependency tree is embed-
ded into the text’s.
</bodyText>
<sectionHeader confidence="0.997218" genericHeader="conclusions">
4 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.999988303030303">
The experimental results obtained from this research
demonstrate that it is possible to apply a syntactic-
based approach to deduce textual entailment from a
text-hypothesis pair. We can obtain good accuracy
rates using the discussed techniques with very short
response times, which is very useful for assisting
different kinds of tasks that demand near-real-time
responses to user interaction.
The baseline we set for our system was to achieve
better results than the ones we obtained with our last
participation in RTE-2. As it is stated in (Ferr´andez
et al., 2006), the maximum accuracy value obtained
by then was 55.63% for the test corpus. Therefore,
our system is 9.20% more accurate compared to the
one that participated in RTE-2, which represents a
considerable improvement.
The authors of this paper believe that if higher ac-
curacy rates are desired, a step-based system must be
constructed. This would have several preprocessing
units, such as negation detectors, multi-word associ-
ators and so on. The addition of these units would
definitely increase the response time preventing the
system from being used in real-time tasks.
Future work can be related to the cases where no
verb entailment is produced. For this purpose we
propose to extract a higher amount of semantic in-
formation that would allow us to construct a charac-
terized representation based on the input text, so that
we can deduce entailment even if there is no appar-
ent structure similarity between text and hypothesis.
This would mean to create an abstract conceptual-
ization of the information contained in the analyzed
phrases, allowing us to deduce ideas that are not
</bodyText>
<page confidence="0.994933">
79
</page>
<bodyText confidence="0.9994999">
explicitly mentioned in the parsed text-hypothesis
pairs.
In addition, the weights and thresholds defined
in our system have been established empirically. It
would be interesting to calculate those values by
means of a machine learning algorithm and com-
pare them to the ones we have obtained empirically.
Some authors have already performed this compari-
son, being one example the work described in (Mac-
Cartney et al., 2006).
</bodyText>
<sectionHeader confidence="0.997385" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999870555555556">
The authors of this paper would like to thank pro-
fessors Borja Navarro and Rafael M. Terol for their
help and critical comments.
This research has been supported by the under-
graduate research fellowships financed by the Span-
ish Ministry of Education and Science, the project
TIN2006-15265-C06-01 financed by such ministry,
and the project ACOM06/90 financed by the Span-
ish Generalitat Valenciana.
</bodyText>
<sectionHeader confidence="0.999104" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999935153846154">
Rod Adams. 2006. Textual Entailment Through Ex-
tended Lexical Overlap. In Proceedings of the Second
PASCAL Challenges Workshop on Recognising Tex-
tual Entailment, Venice, Italy.
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo
Giampiccolo, Bernardo Magnini, and Idan Szpektor.
2006. The Second PASCAL Recognising Textual En-
tailment Challenge. In Proceedings of the Second
PASCAL Challenges Workshop on Recognising Tex-
tual Entailment, Venice, Italy.
Johan Bos, and Katja Markert. 2006. When logical infer-
ence helps determining textual entailment (and when it
doesnt). In Proceedings of the Second PASCAL Chal-
lenges Workshop on Recognising Textual Entailment,
Venice, Italy.
´Oscar Ferr´andez, Rafael M. Terol, Rafael Mu˜noz, Patri-
cio Martinez-Barco, and Manuel Palomar. 2006. An
approach based on Logic Forms and WordNet relation-
ships to Textual Entailment performance. In Proceed-
ings of the Second PASCAL Challenges Workshop on
Recognising Textual Entailment, Venice, Italy.
Jes´us Herrera, Anselmo Pe˜nas, ´Alvaro Rodrigo, and Fe-
lisa Verdejo. 2006. UNED at PASCAL RTE-2 Chal-
lenge. In Proceedings of the Second PASCAL Chal-
lenges Workshop on Recognising Textual Entailment,
Venice, Italy.
Andrew Hickl, John Williams, Jeremy Bensley, Kirk
Roberts, Bryan Rink, and Ying Shi. 2006. Recog-
nizing Textual Entailment with LCC’s GROUNDHOG
System. In Proceedings of the Second PASCAL Chal-
lenges Workshop on Recognising Textual Entailment,
Venice, Italy.
Sophia Katrenko, and Pieter Adriaans. 2006. Using
Maximal Embedded Syntactic Subtrees for Textual En-
tailment Recognition. In Proceedings of the Second
PASCAL Challenges Workshop on Recognising Tex-
tual Entailment, Venice, Italy.
Dekang Lin. 1998. Dependency-based Evaluation of
MINIPAR. In Workshop on the Evaluation of Parsing
Systems, Granada, Spain.
Bill MacCartney, Trond Grenager, Marie-Catherine de
Marneffe, Daniel Cer, and Christopher D. Manning.
2006. Learning to recognize features of valid textual
entailments. In Proceedings of the North American
Association of Computational Linguistics (NAACL-
06), New York City, New York, United States of Amer-
ica.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine J. Miller. 1990. In-
troduction to WordNet: An On-line Lexical Database.
International Journal of Lexicography 1990 3(4):235-
244.
Ted Pedersen, Siddhart Patwardhan, and Jason Miche-
lizzi. 2004. WordNet::Similarity - Measuring the Re-
latedness of Concepts. In Proceedings of the North
American Chapter of the Association for Computa-
tional Linguistics (NAACL-04), Boston, Massachus-
sets, United States of America.
Rion Snow, Lucy Vanderwende, and Arul Menezes.
2006. Effectively using syntax for recognizing false
entailment. In Proceedings of the North American As-
sociation of Computational Linguistics (NAACL-06),
New York City, New York, United States of America.
Marta Tatu, Brandon Iles, John Slavick, Adrian Novischi,
and Dan Moldovan. 2006. COGEX at the Second Rec-
ognizing Textual Entailment Challenge. In Proceed-
ings of the Second PASCAL Challenges Workshop on
Recognising Textual Entailment, Venice, Italy.
Zhibiao Wu, and Martha Palmer. 1994. Verb Semantics
and Lexical Selection. In Proceedings of the 32nd An-
nual Meeting of the Associations for Computational
Linguistics, pages 133-138, Las Cruces, New Mexico,
United States of America.
Fabio M. Zanzotto, Alessandro Moschitti, Marco Pen-
nacchiotti, and Maria T. Pazienza. 2006. Learning
textual entailment from examples. In Proceedings of
the Second PASCAL Challenges Workshop on Recog-
nising Textual Entailment, Venice, Italy.
</reference>
<page confidence="0.998248">
80
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.464857">
<title confidence="0.80437825">DLSITE-2: Semantic Similarity Based on Dependency Trees Applied to Textual Entailment Micol, ´Oscar Ferr´andez, Rafael and Manuel Natural Language Processing and Information Systems</title>
<affiliation confidence="0.999151">Department of Computing Languages and University of</affiliation>
<address confidence="0.941712">San Vicente del Raspeig, Alicante 03690,</address>
<email confidence="0.86344">ofe,rafael,</email>
<abstract confidence="0.998902684210526">In this paper we attempt to deduce textual entailment based on syntactic dependency trees of a given text-hypothesis pair. The goals of this project are to provide an accurate and fast system, which we have that can be applied in software systems that require a near-realtime interaction with the user. To accomthis we use parse the phrases and construct their corresponding trees. Later on we apply syntacticbased techniques to calculate the semantic similarity between text and hypothesis. To measure our method’s precision we the test text corpus set from PASCAL Recognising Textual Entailment obtaining an accuracy rate of 60.75%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rod Adams</author>
</authors>
<title>Textual Entailment Through Extended Lexical Overlap.</title>
<date>2006</date>
<booktitle>In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment,</booktitle>
<location>Venice, Italy.</location>
<contexts>
<context position="17391" citStr="Adams, 2006" startWordPosition="2936" endWordPosition="2937">lts with the ones that were achieved by the systems that participated in RTE-2. One should note that the criteria for such ranking is based exclusively on the accuracy, ignoring the average precision value. In addition, each participating group was allowed to submit two different systems to RTE-2. We will consider here the best result of both systems for each group. The mentioned comparison is shown in Table 7, and contains only the systems that had higher accuracy rates than our approach. Participant Accuracy (Hickl et al., 2006) 75.38 (Tatu et al., 2006) 73.75 (Zanzotto et al., 2006) 63.88 (Adams, 2006) 62.62 (Bos and Markert, 2006) 61.62 DLSITE-2 60.75 Table 7: Comparison of some of the teams that participated in RTE-2. As it is reflected in Table 7, our system would have obtained the sixth position out of twenty-four participants, which is an accomplishment considering the limited number of resources that it has builtin. Since one of our system’s modules is based on (Katrenko and Adriaans, 2006), we will compare their results with ours to analyze whether the modifications we introduced perform correctly. In RTE2, they obtained an accuracy rate of 59.00% for the test corpus. The reason why </context>
</contexts>
<marker>Adams, 2006</marker>
<rawString>Rod Adams. 2006. Textual Entailment Through Extended Lexical Overlap. In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment, Venice, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Bar-Haim</author>
<author>Ido Dagan</author>
<author>Bill Dolan</author>
<author>Lisa Ferro</author>
<author>Danilo Giampiccolo</author>
<author>Bernardo Magnini</author>
<author>Idan Szpektor</author>
</authors>
<title>The Second PASCAL Recognising Textual Entailment Challenge.</title>
<date>2006</date>
<booktitle>In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment,</booktitle>
<location>Venice, Italy.</location>
<contexts>
<context position="1692" citStr="Bar-Haim et al., 2006" startWordPosition="255" endWordPosition="258">cy rate of 60.75%. 1 Introduction There are several methods used to determine textual entailment for a given text-hypothesis pair. The one described in this paper uses the information contained in the syntactic dependency trees of such phrases to deduce whether there is entailment or not. In addition, semantic knowledge extracted from WordNet (Miller et al., 1990) has been added to achieve higher accuracy rates. It has been proven in several competitions and other workshops that textual entailment is a complex task. One of these competitions is PASCAL Recognising Textual Entailment Challenge (Bar-Haim et al., 2006), where each participating group develops a textual entailment recognizing system attempting to accomplish the best accuracy rate of all competitors. Such complexity is the reason why we use a combination of various techniques to deduce whether entailment is produced. Currently there are few research projects related to the topic discussed in this paper. Some systems use syntactic tree matching as the textual entailment decision core module, such as (Katrenko and Adriaans, 2006). It is based on maximal embedded syntactic subtrees to analyze the semantic relation between text and hypothesis. Ot</context>
</contexts>
<marker>Bar-Haim, Dagan, Dolan, Ferro, Giampiccolo, Magnini, Szpektor, 2006</marker>
<rawString>Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan Szpektor. 2006. The Second PASCAL Recognising Textual Entailment Challenge. In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment, Venice, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bos</author>
<author>Katja Markert</author>
</authors>
<title>When logical inference helps determining textual entailment (and when it doesnt).</title>
<date>2006</date>
<booktitle>In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment,</booktitle>
<location>Venice, Italy.</location>
<contexts>
<context position="17421" citStr="Bos and Markert, 2006" startWordPosition="2939" endWordPosition="2942">at were achieved by the systems that participated in RTE-2. One should note that the criteria for such ranking is based exclusively on the accuracy, ignoring the average precision value. In addition, each participating group was allowed to submit two different systems to RTE-2. We will consider here the best result of both systems for each group. The mentioned comparison is shown in Table 7, and contains only the systems that had higher accuracy rates than our approach. Participant Accuracy (Hickl et al., 2006) 75.38 (Tatu et al., 2006) 73.75 (Zanzotto et al., 2006) 63.88 (Adams, 2006) 62.62 (Bos and Markert, 2006) 61.62 DLSITE-2 60.75 Table 7: Comparison of some of the teams that participated in RTE-2. As it is reflected in Table 7, our system would have obtained the sixth position out of twenty-four participants, which is an accomplishment considering the limited number of resources that it has builtin. Since one of our system’s modules is based on (Katrenko and Adriaans, 2006), we will compare their results with ours to analyze whether the modifications we introduced perform correctly. In RTE2, they obtained an accuracy rate of 59.00% for the test corpus. The reason why we believe we have achieved be</context>
</contexts>
<marker>Bos, Markert, 2006</marker>
<rawString>Johan Bos, and Katja Markert. 2006. When logical inference helps determining textual entailment (and when it doesnt). In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment, Venice, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>´Oscar Ferr´andez</author>
<author>Rafael M Terol</author>
<author>Rafael Mu˜noz</author>
<author>Patricio Martinez-Barco</author>
<author>Manuel Palomar</author>
</authors>
<title>An approach based on Logic Forms and WordNet relationships to Textual Entailment performance.</title>
<date>2006</date>
<booktitle>In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment,</booktitle>
<location>Venice, Italy.</location>
<marker>Ferr´andez, Terol, Mu˜noz, Martinez-Barco, Palomar, 2006</marker>
<rawString>´Oscar Ferr´andez, Rafael M. Terol, Rafael Mu˜noz, Patricio Martinez-Barco, and Manuel Palomar. 2006. An approach based on Logic Forms and WordNet relationships to Textual Entailment performance. In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment, Venice, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Herrera</author>
<author>Anselmo Pe˜nas</author>
<author>´Alvaro Rodrigo</author>
<author>Felisa Verdejo</author>
</authors>
<title>UNED at PASCAL RTE-2 Challenge.</title>
<date>2006</date>
<booktitle>In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment,</booktitle>
<location>Venice, Italy.</location>
<marker>Herrera, Pe˜nas, Rodrigo, Verdejo, 2006</marker>
<rawString>Jes´us Herrera, Anselmo Pe˜nas, ´Alvaro Rodrigo, and Felisa Verdejo. 2006. UNED at PASCAL RTE-2 Challenge. In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment, Venice, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Hickl</author>
<author>John Williams</author>
<author>Jeremy Bensley</author>
<author>Kirk Roberts</author>
<author>Bryan Rink</author>
<author>Ying Shi</author>
</authors>
<title>Recognizing Textual Entailment with LCC’s GROUNDHOG System.</title>
<date>2006</date>
<booktitle>In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment,</booktitle>
<location>Venice, Italy.</location>
<contexts>
<context position="17315" citStr="Hickl et al., 2006" startWordPosition="2921" endWordPosition="2924">so it can vary significantly between different corpora. Let us now compare our results with the ones that were achieved by the systems that participated in RTE-2. One should note that the criteria for such ranking is based exclusively on the accuracy, ignoring the average precision value. In addition, each participating group was allowed to submit two different systems to RTE-2. We will consider here the best result of both systems for each group. The mentioned comparison is shown in Table 7, and contains only the systems that had higher accuracy rates than our approach. Participant Accuracy (Hickl et al., 2006) 75.38 (Tatu et al., 2006) 73.75 (Zanzotto et al., 2006) 63.88 (Adams, 2006) 62.62 (Bos and Markert, 2006) 61.62 DLSITE-2 60.75 Table 7: Comparison of some of the teams that participated in RTE-2. As it is reflected in Table 7, our system would have obtained the sixth position out of twenty-four participants, which is an accomplishment considering the limited number of resources that it has builtin. Since one of our system’s modules is based on (Katrenko and Adriaans, 2006), we will compare their results with ours to analyze whether the modifications we introduced perform correctly. In RTE2, t</context>
</contexts>
<marker>Hickl, Williams, Bensley, Roberts, Rink, Shi, 2006</marker>
<rawString>Andrew Hickl, John Williams, Jeremy Bensley, Kirk Roberts, Bryan Rink, and Ying Shi. 2006. Recognizing Textual Entailment with LCC’s GROUNDHOG System. In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment, Venice, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sophia Katrenko</author>
<author>Pieter Adriaans</author>
</authors>
<title>Using Maximal Embedded Syntactic Subtrees for Textual Entailment Recognition.</title>
<date>2006</date>
<booktitle>In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment,</booktitle>
<location>Venice, Italy.</location>
<contexts>
<context position="2175" citStr="Katrenko and Adriaans, 2006" startWordPosition="329" endWordPosition="333">s that textual entailment is a complex task. One of these competitions is PASCAL Recognising Textual Entailment Challenge (Bar-Haim et al., 2006), where each participating group develops a textual entailment recognizing system attempting to accomplish the best accuracy rate of all competitors. Such complexity is the reason why we use a combination of various techniques to deduce whether entailment is produced. Currently there are few research projects related to the topic discussed in this paper. Some systems use syntactic tree matching as the textual entailment decision core module, such as (Katrenko and Adriaans, 2006). It is based on maximal embedded syntactic subtrees to analyze the semantic relation between text and hypothesis. Other systems use syntactic trees as a collaborative module, not being the core, such as (Herrera et al., 2006). The application discussed in this paper belongs to the first set of systems, since syntactic matching is its main module. The remainder of this paper is structured as follows. In the second section we will describe the methods implemented in our system. The third one contains the experimental results, and the fourth and last discusses such results and proposes future wo</context>
<context position="5233" citStr="Katrenko and Adriaans, 2006" startWordPosition="824" endWordPosition="827"> represented in Table 2. We have performed tests taking into account and discarding each grammatical category, which has allowed us to generate both lists of relevant and ignored grammatical categories. Verbs, verbs with one argument, verbs with two arguments, verbs taking clause as complement, verb Have, verb Be Nouns Numbers Adjectives Adverbs Noun-noun modifiers Table 1: Relevant grammatical categories. 2.3 Graph embedding detection The next step of our system consists in determining whether the hypothesis’ tree is embedded into the text’s. Let us first define the concept of embedded tree (Katrenko and Adriaans, 2006). Definition 1: Embedded tree A tree T1 = (V1, E1) is embedded into another one T2 = (V2, E2) iff 1. V1 V2, and 2. E1 C E2 where V1 and V2 represent the vertices, and E1 and E2 the edges. In other words, a tree, T1, is embedded into another one, T2, if all nodes and branches of T1 are presentin T2. We believe that it makes sense to reduce the strictness of such a definition to allow the appearance of intermediate nodes in the text’s branches that are 74 Determiners Pre-determiners Post-determiners Clauses Inflectional phrases Preposition and preposition phrases Specifiers of preposition phrase</context>
<context position="6974" citStr="Katrenko and Adriaans, 2006" startWordPosition="1126" endWordPosition="1129">). To determine whether the hypothesis’ tree is embedded into the text’s, we perform a top-down matching process. For this purpose we first compare the roots of both trees. If they coincide, we then proceed to compare their respective child nodes, which are the tokens that have some sort of dependency with their respective root token. In order to add more flexibility to our system, we do not require the pair of tokens to be exactly the same, but rather set a threshold that represents the minimum similarity value between them. This is a difference between our approach and the one described in (Katrenko and Adriaans, 2006). Such a similarity is calculated by using the WordNet::Similarity tool (Pedersen et al., 2004), and, concretely, the Wu-Palmer measure, as defined in Equation 1 (Wu and Palmer, 1994). 2N3 Sim(C1, C2) � (1) N1 + N2 + 2N3 where C1 and C2 are the synsets whose similarity we want to calculate, C3 is their least common superconcept, N1 is the number of nodes on the path from C1 to C3, N2 is the number of nodes on the path from C2 to C3, and N3 is the number of nodes on the path from C3 to the root. All these synsets and distances can be observed in Figure 2. Figure 2: Distance between two synsets.</context>
<context position="17793" citStr="Katrenko and Adriaans, 2006" startWordPosition="3003" endWordPosition="3006">parison is shown in Table 7, and contains only the systems that had higher accuracy rates than our approach. Participant Accuracy (Hickl et al., 2006) 75.38 (Tatu et al., 2006) 73.75 (Zanzotto et al., 2006) 63.88 (Adams, 2006) 62.62 (Bos and Markert, 2006) 61.62 DLSITE-2 60.75 Table 7: Comparison of some of the teams that participated in RTE-2. As it is reflected in Table 7, our system would have obtained the sixth position out of twenty-four participants, which is an accomplishment considering the limited number of resources that it has builtin. Since one of our system’s modules is based on (Katrenko and Adriaans, 2006), we will compare their results with ours to analyze whether the modifications we introduced perform correctly. In RTE2, they obtained an accuracy rate of 59.00% for the test corpus. The reason why we believe we have achieved better results than their system is due to the fact that we added semantic knowledge to our graph embedding module. In addition, the syntactic dependency trees to which we have applied such a module have been previously filtered to ensure that they do not contain irrelevant words. This reduces the system’s noise and allows us to achieve higher accuracy rates. In the intro</context>
</contexts>
<marker>Katrenko, Adriaans, 2006</marker>
<rawString>Sophia Katrenko, and Pieter Adriaans. 2006. Using Maximal Embedded Syntactic Subtrees for Textual Entailment Recognition. In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment, Venice, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Dependency-based Evaluation of MINIPAR.</title>
<date>1998</date>
<booktitle>In Workshop on the Evaluation of Parsing Systems,</booktitle>
<location>Granada,</location>
<contexts>
<context position="3753" citStr="Lin, 1998" startWordPosition="585" endWordPosition="586">ing, embedded subtree search and graph node matching. A schematic representation of the system architecture is shown in Figure 1. 73 TextGraphs-2: Graph-Based Al orithms for Natural Language Processing, pages 73–80, Rochester, April 2007 (c 2007 Association for Computational Linguistics Figure 1: DLSITE-2 system architecture. Each of the steps or modules of DLSITE-2 is described in the following subsections, that are numbered sequentially according to their execution order. 2.1 Tree generation The first module constructs the corresponding syntactic dependency trees. For this purpose, MINIPAR (Lin, 1998) output is generated and afterwards parsed for each text and hypothesis of our corpus. Phrase tokens, along with their grammatical information, are stored in an on-memory data structure that represents a tree, which is equivalent to the mentioned syntactic dependency tree. 2.2 Tree filtering Once the tree has been constructed, we may want to discard irrelevant data in order to reduce our system’s response time and noise. For this purpose we have generated a database of relevant grammatical categories, represented in Table 1, that will allow us to remove from the tree all those tokens whose cat</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Dependency-based Evaluation of MINIPAR. In Workshop on the Evaluation of Parsing Systems, Granada, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill MacCartney</author>
<author>Trond Grenager</author>
<author>Marie-Catherine de Marneffe</author>
<author>Daniel Cer</author>
<author>Christopher D Manning</author>
</authors>
<title>Learning to recognize features of valid textual entailments.</title>
<date>2006</date>
<booktitle>In Proceedings of the North American Association of Computational Linguistics (NAACL06),</booktitle>
<publisher>States of America.</publisher>
<location>New York City, New York, United</location>
<marker>MacCartney, Grenager, de Marneffe, Cer, Manning, 2006</marker>
<rawString>Bill MacCartney, Trond Grenager, Marie-Catherine de Marneffe, Daniel Cer, and Christopher D. Manning. 2006. Learning to recognize features of valid textual entailments. In Proceedings of the North American Association of Computational Linguistics (NAACL06), New York City, New York, United States of America.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Richard Beckwith</author>
<author>Christiane Fellbaum</author>
<author>Derek Gross</author>
<author>Katherine J Miller</author>
</authors>
<title>Introduction to WordNet: An On-line Lexical Database.</title>
<date>1990</date>
<journal>International Journal of Lexicography</journal>
<pages>3--4</pages>
<contexts>
<context position="1436" citStr="Miller et al., 1990" startWordPosition="215" endWordPosition="218"> apply syntacticbased techniques to calculate the semantic similarity between text and hypothesis. To measure our method’s precision we used the test text corpus set from Second PASCAL Recognising Textual Entailment Challenge (RTE-2), obtaining an accuracy rate of 60.75%. 1 Introduction There are several methods used to determine textual entailment for a given text-hypothesis pair. The one described in this paper uses the information contained in the syntactic dependency trees of such phrases to deduce whether there is entailment or not. In addition, semantic knowledge extracted from WordNet (Miller et al., 1990) has been added to achieve higher accuracy rates. It has been proven in several competitions and other workshops that textual entailment is a complex task. One of these competitions is PASCAL Recognising Textual Entailment Challenge (Bar-Haim et al., 2006), where each participating group develops a textual entailment recognizing system attempting to accomplish the best accuracy rate of all competitors. Such complexity is the reason why we use a combination of various techniques to deduce whether entailment is produced. Currently there are few research projects related to the topic discussed in</context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1990</marker>
<rawString>George A. Miller, Richard Beckwith, Christiane Fellbaum, Derek Gross, and Katherine J. Miller. 1990. Introduction to WordNet: An On-line Lexical Database. International Journal of Lexicography 1990 3(4):235-244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Siddhart Patwardhan</author>
<author>Jason Michelizzi</author>
</authors>
<title>WordNet::Similarity - Measuring the Relatedness of Concepts.</title>
<date>2004</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL-04),</booktitle>
<publisher>States of America.</publisher>
<location>Boston, Massachussets, United</location>
<contexts>
<context position="7069" citStr="Pedersen et al., 2004" startWordPosition="1141" endWordPosition="1144">ing process. For this purpose we first compare the roots of both trees. If they coincide, we then proceed to compare their respective child nodes, which are the tokens that have some sort of dependency with their respective root token. In order to add more flexibility to our system, we do not require the pair of tokens to be exactly the same, but rather set a threshold that represents the minimum similarity value between them. This is a difference between our approach and the one described in (Katrenko and Adriaans, 2006). Such a similarity is calculated by using the WordNet::Similarity tool (Pedersen et al., 2004), and, concretely, the Wu-Palmer measure, as defined in Equation 1 (Wu and Palmer, 1994). 2N3 Sim(C1, C2) � (1) N1 + N2 + 2N3 where C1 and C2 are the synsets whose similarity we want to calculate, C3 is their least common superconcept, N1 is the number of nodes on the path from C1 to C3, N2 is the number of nodes on the path from C2 to C3, and N3 is the number of nodes on the path from C3 to the root. All these synsets and distances can be observed in Figure 2. Figure 2: Distance between two synsets. If the similarity rate is greater or equal than the established threshold, which we have set e</context>
</contexts>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>Ted Pedersen, Siddhart Patwardhan, and Jason Michelizzi. 2004. WordNet::Similarity - Measuring the Relatedness of Concepts. In Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL-04), Boston, Massachussets, United States of America.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Lucy Vanderwende</author>
<author>Arul Menezes</author>
</authors>
<title>Effectively using syntax for recognizing false entailment.</title>
<date>2006</date>
<booktitle>In Proceedings of the North American Association of Computational Linguistics (NAACL-06),</booktitle>
<publisher>States of America.</publisher>
<location>New York City, New York, United</location>
<contexts>
<context position="10238" citStr="Snow et al., 2006" startWordPosition="1703" endWordPosition="1706">ognized. 2.4 Graph node matching Once the embedded subtree comparison has finished, and if its result is negative, we proceed to perform a graph node matching process, termed alignment, between both the text and the hypothesis. This operation consists in finding pairs of tokens in both trees whose lemmas are identical, no matter whether they are in the same position within the tree. We would like to point out that in this step we do not use the WordNet::Similarity tool. Some authors have already designed similar matching techniques, such as the ones described in (MacCartney et al., 2006) and (Snow et al., 2006). However, these include semantic constraints that we have decided not to consider. The reason of this decision is that we desired to overcome the textual entailment recognition from an exclusively syntactic perspective. Therefore, we did not want this module to include any kind of semantic knowledge. The weight given to a token that has been found in both trees will depend on the depth in the hypothesis’ tree and the token’s grammatical relevance. The first of these factors depends on an empiricallycalculated weight that assigns less importance to a node the deeper it is located in the tree. </context>
</contexts>
<marker>Snow, Vanderwende, Menezes, 2006</marker>
<rawString>Rion Snow, Lucy Vanderwende, and Arul Menezes. 2006. Effectively using syntax for recognizing false entailment. In Proceedings of the North American Association of Computational Linguistics (NAACL-06), New York City, New York, United States of America.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta Tatu</author>
<author>Brandon Iles</author>
<author>John Slavick</author>
<author>Adrian Novischi</author>
<author>Dan Moldovan</author>
</authors>
<title>COGEX at the Second Recognizing Textual Entailment Challenge.</title>
<date>2006</date>
<booktitle>In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment,</booktitle>
<location>Venice, Italy.</location>
<contexts>
<context position="17341" citStr="Tatu et al., 2006" startWordPosition="2926" endWordPosition="2929">y between different corpora. Let us now compare our results with the ones that were achieved by the systems that participated in RTE-2. One should note that the criteria for such ranking is based exclusively on the accuracy, ignoring the average precision value. In addition, each participating group was allowed to submit two different systems to RTE-2. We will consider here the best result of both systems for each group. The mentioned comparison is shown in Table 7, and contains only the systems that had higher accuracy rates than our approach. Participant Accuracy (Hickl et al., 2006) 75.38 (Tatu et al., 2006) 73.75 (Zanzotto et al., 2006) 63.88 (Adams, 2006) 62.62 (Bos and Markert, 2006) 61.62 DLSITE-2 60.75 Table 7: Comparison of some of the teams that participated in RTE-2. As it is reflected in Table 7, our system would have obtained the sixth position out of twenty-four participants, which is an accomplishment considering the limited number of resources that it has builtin. Since one of our system’s modules is based on (Katrenko and Adriaans, 2006), we will compare their results with ours to analyze whether the modifications we introduced perform correctly. In RTE2, they obtained an accuracy r</context>
</contexts>
<marker>Tatu, Iles, Slavick, Novischi, Moldovan, 2006</marker>
<rawString>Marta Tatu, Brandon Iles, John Slavick, Adrian Novischi, and Dan Moldovan. 2006. COGEX at the Second Recognizing Textual Entailment Challenge. In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment, Venice, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhibiao Wu</author>
<author>Martha Palmer</author>
</authors>
<title>Verb Semantics and Lexical Selection.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Meeting of the Associations for Computational Linguistics,</booktitle>
<pages>133--138</pages>
<institution>Mexico, United States of America.</institution>
<location>Las Cruces, New</location>
<contexts>
<context position="7157" citStr="Wu and Palmer, 1994" startWordPosition="1155" endWordPosition="1158">we then proceed to compare their respective child nodes, which are the tokens that have some sort of dependency with their respective root token. In order to add more flexibility to our system, we do not require the pair of tokens to be exactly the same, but rather set a threshold that represents the minimum similarity value between them. This is a difference between our approach and the one described in (Katrenko and Adriaans, 2006). Such a similarity is calculated by using the WordNet::Similarity tool (Pedersen et al., 2004), and, concretely, the Wu-Palmer measure, as defined in Equation 1 (Wu and Palmer, 1994). 2N3 Sim(C1, C2) � (1) N1 + N2 + 2N3 where C1 and C2 are the synsets whose similarity we want to calculate, C3 is their least common superconcept, N1 is the number of nodes on the path from C1 to C3, N2 is the number of nodes on the path from C2 to C3, and N3 is the number of nodes on the path from C3 to the root. All these synsets and distances can be observed in Figure 2. Figure 2: Distance between two synsets. If the similarity rate is greater or equal than the established threshold, which we have set empirically to 80%, we will consider the corresponding hypothesis’ token as suitable to h</context>
</contexts>
<marker>Wu, Palmer, 1994</marker>
<rawString>Zhibiao Wu, and Martha Palmer. 1994. Verb Semantics and Lexical Selection. In Proceedings of the 32nd Annual Meeting of the Associations for Computational Linguistics, pages 133-138, Las Cruces, New Mexico, United States of America.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio M Zanzotto</author>
<author>Alessandro Moschitti</author>
<author>Marco Pennacchiotti</author>
<author>Maria T Pazienza</author>
</authors>
<title>Learning textual entailment from examples.</title>
<date>2006</date>
<booktitle>In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment,</booktitle>
<location>Venice, Italy.</location>
<contexts>
<context position="17371" citStr="Zanzotto et al., 2006" startWordPosition="2931" endWordPosition="2934">a. Let us now compare our results with the ones that were achieved by the systems that participated in RTE-2. One should note that the criteria for such ranking is based exclusively on the accuracy, ignoring the average precision value. In addition, each participating group was allowed to submit two different systems to RTE-2. We will consider here the best result of both systems for each group. The mentioned comparison is shown in Table 7, and contains only the systems that had higher accuracy rates than our approach. Participant Accuracy (Hickl et al., 2006) 75.38 (Tatu et al., 2006) 73.75 (Zanzotto et al., 2006) 63.88 (Adams, 2006) 62.62 (Bos and Markert, 2006) 61.62 DLSITE-2 60.75 Table 7: Comparison of some of the teams that participated in RTE-2. As it is reflected in Table 7, our system would have obtained the sixth position out of twenty-four participants, which is an accomplishment considering the limited number of resources that it has builtin. Since one of our system’s modules is based on (Katrenko and Adriaans, 2006), we will compare their results with ours to analyze whether the modifications we introduced perform correctly. In RTE2, they obtained an accuracy rate of 59.00% for the test cor</context>
</contexts>
<marker>Zanzotto, Moschitti, Pennacchiotti, Pazienza, 2006</marker>
<rawString>Fabio M. Zanzotto, Alessandro Moschitti, Marco Pennacchiotti, and Maria T. Pazienza. 2006. Learning textual entailment from examples. In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment, Venice, Italy.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>