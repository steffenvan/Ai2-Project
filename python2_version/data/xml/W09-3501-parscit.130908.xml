<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.958271">
Report of NEWS 2009 Machine Transliteration Shared Task
</title>
<author confidence="0.986829">
Haizhou Li†, A Kumaran‡, Vladimir Pervouchine† and Min Zhang†
</author>
<affiliation confidence="0.972371">
†Institute for Infocomm Research, A*STAR, Singapore 138632
</affiliation>
<email confidence="0.925543">
{hli,vpervouchine,mzhang}@i2r.a-star.edu.sg
</email>
<note confidence="0.474005">
‡Multilingual Systems Research, Microsoft Research India
</note>
<email confidence="0.99511">
A.Kumaran@microsoft.com
</email>
<sectionHeader confidence="0.993791" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999924810810811">
This report documents the details of the
Machine Transliteration Shared Task con-
ducted as a part of the Named Enti-
ties Workshop (NEWS), an ACL-IJCNLP
2009 workshop. The shared task features
machine transliteration of proper names
from English to a set of languages. This
shared task has witnessed enthusiastic par-
ticipation of 31 teams from all over the
world, with diversity of participation for
a given system and wide coverage for a
given language pair (more than a dozen
participants per language pair). Diverse
transliteration methodologies are repre-
sented adequately in the shared task for a
given language pair, thus underscoring the
fact that the workshop may truly indicate
the state of the art in machine transliter-
ation in these language pairs. We mea-
sure and report 6 performance metrics on
the submitted results. We believe that the
shared task has successfully achieved the
following objectives: (i) bringing together
the community of researchers in the area
of Machine Transliteration to focus on var-
ious research avenues, (ii) Calibrating sys-
tems on common corpora, using common
metrics, thus creating a reasonable base-
line for the state-of-the-art of translitera-
tion systems, and (iii) providing a quan-
titative basis for meaningful comparison
and analysis between various algorithmic
approaches used in machine translitera-
tion. We believe that the results of this
shared task would uncover a host of inter-
esting research problems, giving impetus
to research in this significant research area.
</bodyText>
<sectionHeader confidence="0.998999" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999734">
Names play a significant role in many Natural
Language Processing (NLP) and Information Re-
trieval (IR) systems. They have a critical role
in Cross Language Information Retrieval (CLIR)
and Machine Translation (MT) systems as the sys-
tems’ performances are shown to positively cor-
relate with the correct conversion of names be-
tween the languages in several studies (Demner-
Fushman and Oard, 2002; Mandl and Womser-
Hacker, 2005; Hermjakob et al., 2008; Udupa et
al., 2009). The traditional source for name equiva-
lence, the bilingual dictionaries — whether hand-
crafted or statistical — offer only limited support
as they do not have sufficient coverage of names.
New names are introduced to the vocabulary of a
language every day.
All of the above point to the critical need for ro-
bust Machine Transliteration technology and sys-
tems. This has attracted attention from the re-
search community. Over the last decade scores of
papers on Machine Transliteration have appeared
in the top Computational Linguistics, Information
Retrieval and Data Management conferences, ex-
ploring diverse algorithmic approaches in a wide
variety of different languages (Knight and Graehl,
1998; Li et al., 2004; Zelenko and Aone, 2006;
Sproat et al., 2006; Sherif and Kondrak, 2007;
Hermjakob et al., 2008; Goldwasser and Roth,
2008; Goldberg and Elhadad, 2008; Klementiev
and Roth, 2006). However, there has not been
any coordinated effort in calibrating the state-of-
the-art technical capabilities of machine translit-
eration: the studies explore different algorithmic
approaches in different language pairs and report
their performance in different metrics and tested
on different corpora.
The overarching objective of this shared task
is to drive the machine transliteration technology
forward, to measure and baseline the state-of-the-
</bodyText>
<page confidence="0.822333">
1
</page>
<note confidence="0.9989285">
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 1–18,
Suntec, Singapore, 7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.998486266666667">
art and to provide a meaningful comparison be-
tween the most promising algorithmic approaches
in order to stimulate the discussions among the re-
searchers. The NLP community in Asia is espe-
cially interested in transliteration as several major
Asian languages do not use Latin script in their na-
tive writing systems. The Named Entity Workshop
(NEWS 2009) in ACL-IJCNLP 2009 in Singapore
provides an ideal platform for the shared task to
take off. This is precisely what we address in this
shared task on machine transliteration that is con-
ducted as a part of the Named Entity Workshop
(NEWS-2009), an ACL-IJCNLP 2009 workshop.
The shared task aims at achieving the following
objectives:
</bodyText>
<listItem confidence="0.974542833333333">
• Providing a forum to bring together the com-
munity of researchers in the area of Machine
Transliteration to focus on various research
avenues in this important research area.
• Calibrating systems on common hand-crafted
corpora, using common metrics, in many dif-
ferent languages, thus creating a reasonable
baseline for the state-of-the-art of translitera-
tion systems.
• Analysing the results so that a reason-
able comparison of different algorithmic
approaches and their trade-offs (such as,
</listItem>
<bodyText confidence="0.96730628">
transliteration quality vs. generality of ap-
proach across languages vs. training data
size, etc.) may be explored.
We believe that a substantial part of what we have
set out to achieve has been accomplished, and we
present this report as a record of the task pro-
cess, system participation and results and our find-
ings. It is our hope that this reporting will generate
lively discussions during the NEWS workshop and
subsequent research in this important area.
This introduction outlines the purpose of the
transliteration shared task conducted as a part of
the NEWS workshop. Section 2 outlines the ma-
chine transliteration task and the corpora used and
Section 3 discusses the metrics chosen for evalua-
tion, along with the rationale for choosing them.
Section 4 sketches the participation. Section 5
presents the results of the shared task and the anal-
ysis of the results. Section 6, summarises the
queries and feedback we have received from the
participants and Section 7 concludes, presenting
some lessons learnt from the current edition of the
shared task, and some ideas we want to pursue
in the future plan for the Machine Transliteration
tasks.
</bodyText>
<sectionHeader confidence="0.848584" genericHeader="method">
2 Transliteration Shared Task
</sectionHeader>
<bodyText confidence="0.999622666666667">
In this section, we outline the definition of the task,
the process followed and the rationale for the de-
cisions.
</bodyText>
<subsectionHeader confidence="0.92887">
2.1 “Transliteration”: A definition
</subsectionHeader>
<bodyText confidence="0.95873943902439">
There exists several terms that are used inter-
changeably in the contemporary research litera-
ture for the conversion of names between two
languages, such as, transliteration, transcription,
and sometimes Romanisation, especially if Latin
scripts are used for target strings (Halpern, 2007).
Our aim is not only at capturing the name con-
version process from a source to a target language,
but also at its ultimate utility for downstream ap-
plications, such as CLIR and MT. We have nar-
rowed down to three specific requirements for the
task, as follows: “Transliteration is the conver-
sion of a given name in the source language (a
text string in the source writing system or orthog-
raphy) to a name in the target language (another
text string in the target writing system or orthog-
raphy), such that the target language name is:
(i) phonemically equivalent to the source name
(ii) conforms to the phonology of the target lan-
guage and (iii) matches the user intuition of the
equivalent of the source language name in the tar-
get language.”
Given that the phoneme set of languages may
not be exactly the same, the first requirement must
be diluted to “close to”, instead of “equivalent”.
The second requirement is needed to ensure that
the target string is a valid string as per the target
language phonology. The third requirement is in-
troduced to produce what a normal user would ex-
pect (at least for the popular names), and in or-
der to make it useful for downstream applications
like MT or CLIR systems. Though the third re-
quirement make systems produce target language
strings that marginally violate the first or second
requirements, it ensures that such transliteration
system is of value to downstream systems. All the
above requirements are implicitly enforced by the
choice of name pairs used to define the training
and test corpora in a given language pair. In cases
where multiple equivalent target language names
are possible for a source language name, we in-
</bodyText>
<page confidence="0.988765">
2
</page>
<bodyText confidence="0.988038428571429">
clude all of them.
After much debate, we have also retained the
task name as “transliteration”, though our defi-
nition may be closest to the “popular transcrip-
tion” (Halpern, 2007), due to the popularity of
term “Machine Transliteration” among the lan-
guage technology researchers.
</bodyText>
<subsectionHeader confidence="0.998267">
2.2 Shared Task Description
</subsectionHeader>
<bodyText confidence="0.99955775">
The shared task is specified as development of ma-
chine transliteration systems in one or more of the
specified language pairs. Each language pair of
the shared task consists of a source and a target
language, implicitly specifying the transliteration
direction. Training and development data in each
of the language pairs have been made available to
all registered participants for developing a translit-
eration system for that specific language pair using
any approach that they find appropriate.
At the evaluation time, a standard hand-crafted
test set consisting of between 1,000 and 3,000
source names (approximately 10% of the train-
ing data size) have been released, on which the
participants are required to produce a ranked list
of transliteration candidates in the target language
for each source name. The system output is
tested against a reference set (which may include
multiple correct transliterations for some source
names), and the performance of a system is cap-
tured in multiple metrics (defined in Section 3),
each designed to capture a specific performance
dimension.
For every language pair every participant is re-
quired to submit one run (designated as a “stan-
dard” run) that uses only the data provided by the
NEWS workshop organisers in that language pair,
and no other data or linguistic resources. This
standard run ensures parity between systems and
enables meaningful comparison of performance
of various algorithmic approaches in a given lan-
guage pair. Participants are allowed to submit
more runs (designated as “non-standard”) for ev-
ery language pair using either data beyond that
provided by the shared task organisers or linguis-
tic resources in a specific language, or both. This
essentially may enable any participant to demon-
strate the limits of performance of their system in
a given language pair.
The shared task timelines provide adequate time
for development, testing (approximately 2 months
after the release of the training data) and the final
result submission (5 days after the release of the
test data).
</bodyText>
<subsectionHeader confidence="0.998881">
2.3 Shared Task Corpora
</subsectionHeader>
<bodyText confidence="0.999852046511628">
We have had two specific constraints in selecting
languages for the shared task: language diversity
and data availability. To make the shared task in-
teresting and to attract wider participation, it is
important to ensure a reasonable variety among
the languages in terms of linguistic diversity, or-
thography and geography. Clearly, the ability of
procuring and distributing a reasonably large (ap-
proximately 10K paired names for training and
testing together) hand-crafted corpora consisting
primarily of paired names is critical for this pro-
cess. At the end of the planning stage and after
discussion with the data providers, we have cho-
sen the set of 7 languages shown in Table 1 for the
task (Li et al., 2004; Kumaran and Kellner, 2007;
MSRI, 2009; CJKI, 2009).
For all of the languages chosen, we have been
able to procure paired names data between En-
glish and the respective languages and were able
to make them available to the participants. In ad-
dition, we have been able to procure a specific
corpus of about 40K Romanised Japanese names
and their Kanji counterparts, and the correspond-
ing language pair (Japanese names from their Ro-
manised form to Kanji) has been included as one
of the task language pair.
It should be noted here that each corpus has a
definite skew in its characteristics: the names in
the Chinese, Japanese and Korean (CJK) language
corpora are Western names; the Indic languages
(Hindi, Kannada and Tamil) corpora consists of a
mix of Indian and Western names. The Roman-
ised Kanji to Kanji corpus consists only of native
Japanese names. While such characteristics may
have provided us an opportunity to specifically
measure the performance for forward translitera-
tions (in CJK) and backward transliterations (in
Romanised Kanji), we do not highlight such fine
distinctions in this edition.
Finally, it should be noted here that the corpora
procured and released for NEWS 2009 represent
perhaps the most diverse and largest corpora to be
used for any common transliteration tasks today.
</bodyText>
<sectionHeader confidence="0.968474" genericHeader="method">
3 Evaluation Metrics and Rationale
</sectionHeader>
<bodyText confidence="0.998806">
The participants have been asked to submit re-
sults of one standard and up to four non-standard
</bodyText>
<page confidence="0.99842">
3
</page>
<table confidence="0.999493">
Source language Target language Data Source Data Size (No. source names) Task ID
Training Development Testing
English Hindi Microsoft Research India 9,975 974 1,000 EnHi
English Tamil Microsoft Research India 7,974 987 1,000 EnTa
English Kannada Microsoft Research India 7,990 968 1,000 EnKa
English Russian Microsoft Research India 5,977 943 1,000 EnRu
English Chinese Institute for Infocomm Research 31,961 2,896 2,896 EnCh
English Korean Hangul CJK Institute 4,785 987 989 EnKo
English Japanese Katakana CJK Institute 23,225 1,492 1,489 EnJa
Japanese name (in English) Japanese Kanji CJK Institute 6,785 1,500 1,500 JnJk
</table>
<tableCaption confidence="0.999843">
Table 1: Source and target languages for the shared task on transliteration.
</tableCaption>
<subsectionHeader confidence="0.997728">
3.2 Fuzziness in Top-1 (Mean F-score)
</subsectionHeader>
<bodyText confidence="0.990868">
The mean F-score measures how different, on av-
erage, the top transliteration candidate is from its
closest reference. F-score for each source word
is a function of Precision and Recall and equals 1
when the top candidate matches one of the refer-
ences, and 0 when there are no common characters
between the candidate and any of the references.
Precision and Recall are calculated based on
the length of the Longest Common Subsequence
(LCS) between a candidate and a reference:
</bodyText>
<equation confidence="0.9954885">
1
LC5(c, r) = 2 (|c |+ |r |− ED(c, r)) (2)
</equation>
<bodyText confidence="0.999778285714286">
where ED is the edit distance and |x |is the length
of x. For example, the longest common subse-
quence between “abcd” and “afcde” is “acd” and
its length is 3. The best matching reference, that
is, the reference for which the edit distance has
the minimum, is taken for calculation. If the best
matching reference is given by
</bodyText>
<equation confidence="0.901253928571428">
ri,m = arg min (ED(ci,1, ri,j)) (3)
j
then Recall, Precision and F-score for i-th word
are calculated as
Ri = =
Pi LC5(ci,1, ri,m)
F.
Z
=
|ri,m|
LC5(ci,1, ri,m)
|ci,1|
2 Ri x Pi
Ri + Pi
</equation>
<listItem confidence="0.9935622">
• The length is computed in distinct Unicode
characters.
• No distinction is made on different character
types of a language (e.g., vowel vs. conso-
nants vs. combining diereses’ etc.)
</listItem>
<bodyText confidence="0.920209583333333">
runs. Each run contains a ranked list of up to
10 candidate transliterations for each source name.
The submitted results are compared to the ground
truth (reference transliterations) using 6 evaluation
metrics capturing different aspects of translitera-
tion performance. Since a name may have mul-
tiple correct transliterations, all these alternatives
are treated equally in the evaluation, that is, any
of these alternatives is considered as a correct
transliteration, and all candidates matching any of
the reference transliterations are accepted as cor-
rect ones.
The following notation is further assumed:
N : Total number of names (source
words) in the test set
ni : Number of reference transliterations
for i-th name in the test set (ni &gt; 1)
ri,j : j-th reference transliteration for i-th
name in the test set
ci,k : k-th candidate transliteration (system
output) for i-th name in the test set
(1 &lt; k &lt; 10)
Ki : Number of candidate transliterations
produced by a transliteration system
</bodyText>
<subsectionHeader confidence="0.995425">
3.1 Word Accuracy in Top-1(ACC)
</subsectionHeader>
<bodyText confidence="0.998034571428572">
Also known as Word Error Rate, it measures cor-
rectness of the first transliteration candidate in the
candidate list produced by a transliteration system.
ACC = 1 means that all top candidates are cor-
rect transliterations i.e. they match one of the ref-
erences, and ACC = 0 means that none of the top
candidates are correct.
</bodyText>
<equation confidence="0.59596175">
1 �N 1 if Iri ,j : ri,j = ci,1 (1)
ACC = N { }
0 otherwise
i=1
</equation>
<page confidence="0.971327">
4
</page>
<subsectionHeader confidence="0.998772">
3.3 Mean Reciprocal Rank (MRR)
</subsectionHeader>
<bodyText confidence="0.999956333333333">
Measures traditional MRR for any right answer
produced by the system, from among the candi-
dates. 1/MRR tells approximately the average
rank of the correct transliteration. MRR closer to 1
implies that the correct answer is mostly produced
close to the top of the n-best lists.
</bodyText>
<equation confidence="0.8555986">
minj 1 if Iri,j,ci,k : ri,j = ci,k;
{ 0 otherwise I
(7)
1
MRR = �
</equation>
<sectionHeader confidence="0.747699" genericHeader="method">
4 Participation in Shared Task
</sectionHeader>
<bodyText confidence="0.999438583333333">
There have been 31 systems from around the
world that participated in the shared task and sub-
mitted the transliteration results for a common test
data, produced by their systems trained on the
common training corpora.
A few teams have participated in all or almost
all tasks (that is, language pairs); most others par-
ticipated in 3 tasks on average. Each language pair
has attracted on average around 13 teams. The par-
ticipation details are shown in Table 3 and the de-
mographics of the participating teams by country
is shown in Figure 1.
</bodyText>
<equation confidence="0.99719375">
RRi =
N
RRi (8)
i=1
</equation>
<bodyText confidence="0.8907413">
3.4 MAPref The Netherlands
Measures tightly the precision in the n-best can- Canada
didates for i-th source name, for which reference /reland
transliterations are available. If all of the refer- Egypt
ences are produced, then the MAP is 1. Let’s de- Taiwan
note the number of correct candidates for the i-th Korea
source word in k-best list as num(i, k). MAPref China &amp; Hong Kong
is then given by Japan
/ndia
USA
</bodyText>
<equation confidence="0.960418">
0 1 2 3 4 5 6 7 8 9 10
� � � � � � � � � � ��
�num(i, k) (9)
3.5 MAP10
</equation>
<bodyText confidence="0.9999222">
MAP10 measures the precision in the 10-best can-
didates for i-th source name provided by the can-
didate system. In general, the higher MAP10 is,
the better is the quality of the transliteration sys-
tem in capturing the multiple references.
</bodyText>
<equation confidence="0.5773135">
�num(i, k) (10)
3.6 MAPsys
</equation>
<bodyText confidence="0.999461571428571">
MAPsys measures the precision in the top Ki-best
candidates produced by the system for i-th source
name, for which ni reference transliterations are
available. This measure allows the systems to pro-
duce variable number of transliterations, based on
their confidence in identifying and producing cor-
rect transliterations.
</bodyText>
<equation confidence="0.560846">
�num(i, k) (11)
</equation>
<figureCaption confidence="0.994753">
Figure 1: Participation by country.
</figureCaption>
<bodyText confidence="0.999954545454545">
Teams are required to submit at least one stan-
dard run for every task they participated in. In total
104 standard and 86 non-standard runs have been
submitted. Table 2 shows the number of standard
and non-standard runs submitted for each task. It
is clear that the most “popular” tasks are translit-
eration from English to Hindi and from English to
Chinese, attempted by 21 and 18 participants re-
spectively. Overall, as can be noted from the re-
sults, each task has received significant participa-
tion.
</bodyText>
<sectionHeader confidence="0.636327" genericHeader="method">
5 Task Results and Analysis
</sectionHeader>
<subsectionHeader confidence="0.998296">
5.1 Standard runs
</subsectionHeader>
<bodyText confidence="0.999976545454545">
The 8 individual plots in Figure 2 summarise (for
each task) the results of standard runs via 3 mea-
sured metrics concerning output of at least one
correct candidate per source word, namely, ac-
curacy in top-1, F-score and Mean Reciprocal
Rank (MRR). The plots in Figure 3 summarise (for
each task) the results for 3 metrics on ranked or-
dered transliteration output of the systems, namely
MAPref, MAP10 and MAPsys metrics. All the
results are presented numerically in Tables 8–11,
for all evaluation metrics. These are the official
</bodyText>
<equation confidence="0.993605523809524">
�N
1
MAPref = �
i
1 ni
ni k=1
1
MAP10 = �
1
1 0
10
N
i=1
�
k=1
�N
1
MAPsys = �
1 Ki
Ki k=1
i=1
</equation>
<page confidence="0.978233">
5
</page>
<table confidence="0.97370825">
English English English English English English English Japanese
to Hindi to Tamil to Kan- to Rus- to Chi- to Ko- to translit-
nada sian nese rean Japanese erated to
Katakana Japanese
Kanji
Language pair code EnHi EnTa EnKa EnRu EnCh EnKo EnJa JnJk
Standard runs 21 13 14 13 18 8 10 7
Non-standard runs 18 5 5 16 20 9 5 8
</table>
<tableCaption confidence="0.9642455">
Table 2: Number of runs submitted for each task. Number of participants coincides with the number of
standard runs submitted.
</tableCaption>
<bodyText confidence="0.999524">
evaluation results published for this edition of the
transliteration shared task. Note that two teams
have updated their results (after fixing bugs in their
systems) after the deadline; their results are iden-
tified specifically.
We find that two approaches to transliteration
are most popular in the shared task submissions.
One of these approaches is Phrase-based statis-
tical machine transliteration (Finch and Sumita,
2008), an approach initially developed for ma-
chine translation (Koehn et al., 2003). Systems
that adopted this approach are (Song, 2009; Haque
et al., 2009; Noeman, 2009; Rama and Gali, 2009;
Chinnakotla and Damani, 2009).1 The other is
Conditional Random Fields(Lafferty et al., 2001)
(CRF), adopted by (Aramaki and Abekawa, 2009;
Shishtla et al., 2009). With only a few exceptions,
most implementations are based on approaches
that are language-independent. Indeed, many of
the participants fielded their systems on multiple
languages, as can be seen from Table 3.
We also note that combination of several differ-
ent models via re-ranking of their outputs (CRF,
Maximum Entropy Model, Margin Infused Re-
laxed Algorithm) proves to be very successful (Oh
et al., 2009); their system (reported as Team ID
6) produced the best or second-best transliteration
performance consistently across all metrics, in all
tasks, except Japanese back-transliteration. Exam-
ples of other model combinations are (Das et al.,
2009).
At least two teams (reported as Team IDs 14
and 27) incorporate language origin detection in
their system (Bose and Sarkar, 2009; Khapra and
Bhattacharyya, 2009). The Indian language cor-
pora contains names of both English and Indic ori-
gin. Khapra and Bhattacharyya (2009) demon-
strate how much the transliteration performance
can be improved when language of origin detec-
</bodyText>
<footnote confidence="0.781922">
1To maintain anonymity, papers of the teams that submit-
ted anonymous results are not cited in this report.
</footnote>
<bodyText confidence="0.993774527777778">
tion is employed, followed by a language-specific
transliteration model for decoding.
Some systems merit specific mention as they
adopt are rather unique approaches. Jiampoja-
marn et al. (2009) propose DirectTL discrimina-
tive sequence prediction model that is language-
independent (reported as Team ID 7). Their
transliteration accuracy is among the highest in
several tasks (EnCh, EnHi and EnRu). Zelenko
(2009) present an approach to the transliteration
problem based on Minimum Description Length
(MDL) principle. Freitag and Wang (2009) ap-
proach the problem of transliteration with bidirec-
tional perceptron edit models.
Finally, in Figure 4 we present a plot where
each point represents a standard run by a system,
with different tasks marked with specific shape
and colour. This plot gives a bird-eye-view of
the system performances across two most uncorre-
lated evaluation metrics, namely accuracy in top-1
(ACC) and Mean F-score. Not surprisingly, we
notice very high performance in terms of F-score
for English to Russian transliteration task, likely
because Russian orthography follows pronuncia-
tion very closely, except for characters like soft
and hard signs that can hardly be recovered from
English words.
We also observe that Japanese back-
transliteration has proven to be much harder
than other (forward-transliteration) tasks. In
general, we note that a well-performing translit-
eration system performs well across all metrics.
We are curious about the correlation between
different metrics, and the results (specifically,
the Spearman’s rank correlation coefficient) are
presented below:
</bodyText>
<listItem confidence="0.999959333333333">
• Accuracy in top-1 vs. F-score: 0.40
• Accuracy in top-1 vs. MRR: 0.97
• Accuracy in top-1 vs. MAPTef: 0.997
</listItem>
<page confidence="0.763738">
6
</page>
<listItem confidence="0.9997275">
• Accuracy in top-1 vs. MAPio: 0.89
• Accuracy in top-1 vs. MAPsys: 0.80
</listItem>
<bodyText confidence="0.969154125">
We find that F-score is the most uncorrelated met-
ric: the Spearman’s rank correlation coefficient
between F-score and accuracy in top-1 is 0.40 and
between F-score and MRR it is 0.44. This is likely
because all metrics, except for F-score, are based
on word accuracy, while F-score is based on word
similarity allowing non-matching words to have
scores well above 0.
</bodyText>
<figure confidence="0.89597">
00 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
Accuracy in top-1
</figure>
<figureCaption confidence="0.995083">
Figure 4: Accuracy in top-1 vs. F-score for dif-
ferent tasks.
</figureCaption>
<subsectionHeader confidence="0.976669">
5.2 Non-standard runs
</subsectionHeader>
<bodyText confidence="0.996801285714286">
For the non-standard runs there exist no restric-
tions for the teams on the use of more data or other
linguistic resources. The purpose of non-standard
runs is to see how accurate personal name translit-
eration can be, for a given language pair. The ap-
proaches used in non-standard runs are typical and
may be summarised as follows:
</bodyText>
<listItem confidence="0.994551769230769">
• Dictionary lookup.
• Pronunciation dictionaries to convert words
to their phonetic transcription.
• Additional corpora for training and dictio-
nary lookup, such as LDC English-Chinese
named entity list LDC2005T34 (Linguistic
Data Consortium, 2005).
• Web search, and in particular, Wikipedia
search. First, transliteration candidates are
generated. Then a Web search is performed
to see if any of the candidates appear in the
search results. Based on the results, the can-
didates are re-ranked.
</listItem>
<bodyText confidence="0.9998728">
The results are shown in Tables 16–19. For En-
glish to Chinese and English to Russian transliter-
ation tasks the accuracy in top-1 can go as high as
0.909 and 0.955 respectively when Web search is
used to aid transliteration.
</bodyText>
<subsectionHeader confidence="0.996964">
5.3 Post-evaluation
</subsectionHeader>
<bodyText confidence="0.99997925">
Two participants have found a bug in their system
implementation and re-evaluated the results after
the deadline. Their results are marked specifically
in Tables 4–8 and 16.
</bodyText>
<sectionHeader confidence="0.881721" genericHeader="method">
6 Process Analysis and Fine-tuning
</sectionHeader>
<bodyText confidence="0.999211323529412">
In this section we highlight some of the sugges-
tions and feedback that we have received from the
participants during the course of this shared task.
While a few of them have been implemented in the
current edition, many of these may be considered
in the future editions of the shared task.
More or different languages There is quite a
bit of interest in enhancing the list of language
pairs short-listed. While we are constrained (in
this edition) due to the availability of manually
verified data, certainly more languages will be in-
cluded in the future editions, as some specific data
have already been promised for future editions.
Bidirectional transliteration Many partic-
ipants express interest in transliterations into
English; and this reflexive task will be added in
the future editions. We believe it will encourage
more participation as it will be easy to read and
verify system output in English for those teams
not familiar with the non-English side of the
language.
Forward vs. backward transliteration There
is quite a bit of interest expressed in specifically
separating forward and backward transliteration
tasks. However, such separation requires specific
corpora with known origin for each name pair, and
clearly we are constrained by the availability of
corpora. When corpora is available, the task may
be designated explicitly in future editions.
Number of standard runs The number of stan-
dard runs that may be submitted may be increased
in the future editions, as many participants would
like to submit many standard runs, trained with
different parameters.
</bodyText>
<figure confidence="0.944071277777778">
English to Chinese
English to Hindi
English to Tamil
English to Kannada
English to Russian
English to Korean
English to Japanese
Japanese transliterated to Japanese Kanji
F-score 1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
</figure>
<page confidence="0.995994">
7
</page>
<bodyText confidence="0.9985845">
Errors in training and development corpora
While we have taken all precautions in acquiring
and creating the corpora, some errors still remain.
We thank those who have sent us the errata. How-
ever, since the affected part is less than 0.5% of
the data, we believe that the effect on final results
is minimal. The errata will be made available to
all participants.
</bodyText>
<sectionHeader confidence="0.995179" genericHeader="conclusions">
7 Conclusions and Future Plans
</sectionHeader>
<bodyText confidence="0.99954704">
We are pleased to report a comprehensive cal-
ibration and baselining of machine translitera-
tion apporaches as most state-of-the-art machine
transliteration techniques are represented in the
shared task. The most popular techniques such as
Phrase-Based Machine Transliteration (Koehn et
al., 2003), and Conditional Random Fields (Laf-
ferty et al., 2001) are inspired by recent progress in
machine translation. As the standard runs are lim-
ited by the use of corpus, most of the systems are
implemented under the direct orthographic map-
ping (DOM) framework (Li et al., 2004). While
the standard runs allow us to conduct meaning-
ful comparison across different algorithms, we
recognise that the non-standard runs open up more
opportunities for exploiting larger linguistic cor-
pora. It is also noted that several systems have re-
ported improved performance over any previously
reported results on similar corpora.
NEWS 2009 Shared Task represents a suc-
cessful debut of a community effort in driving
machine transliteration techniques forward. The
overwhelming responses in the first shared task
also warrant continuation of such an effort in fu-
ture ACL or IJCNLP events.
</bodyText>
<sectionHeader confidence="0.989525" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99969125">
The organisers of the NEWS 2009 Shared Task
would like to thank the Institute for Infocomm Re-
search (Singapore), Microsoft Research India and
CJK Institute (Japan) for providing the corpora
and technical support. Without those, the Shared
Task would not be possible. We thank those par-
ticipants who identified errors in the data and sent
us the errata. We want to thank Monojit Choud-
hury for his contribution to metrics defined for the
shared task. We also want to thank the members
of programme committee for their invaluable com-
ments that improve the quality of the shared task
papers. Finally, we wish to thank all the partici-
pants for their active participation that have made
this first machine transliteration shared task a com-
prehensive one.
</bodyText>
<page confidence="0.996612">
8
</page>
<sectionHeader confidence="0.779388" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.588999333333333">
Eiji Aramaki and Takeshi Abekawa. 2009. Fast de-
coding and easy implementation: Transliteration as
a sequential labeling. In Proc. ACL/IJCNLP Named
</bodyText>
<subsubsectionHeader confidence="0.353709">
Entities Workshop Shared Task.
</subsubsectionHeader>
<bodyText confidence="0.372675">
Dipankar Bose and Sudeshna Sarkar. 2009. Learn-
ing multi character alignment rules and classifica-
tion of training data for transliteration. In Proc.
</bodyText>
<figure confidence="0.749663571428571">
ACL/IJCNLP Named Entities Workshop Shared
Task.
Manoj Kumar Chinnakotla and Om P. Damani. 2009.
Experiences with English-Hindi, English-Tamil and
English-Kannada transliteration tasks at NEWS
2009. In Proc. ACL/IJCNLP Named Entities Work-
shop Shared Task.
</figure>
<figureCaption confidence="0.768684197368421">
CJKI. 2009. CJK Institute. http://www.cjk.org/.
Amitava Das, Asif Ekbal, Tapabrata Mondal, and
Sivaji Bandyopadhyay. 2009. English to Hindi
machine transliteration system at NEWS 2009.
In Proc. ACL/IJCNLP Named Entities Workshop
Shared Task.
D. Demner-Fushman and D. W. Oard. 2002. The ef-
fect of bilingual term list size on dictionary-based
cross-language information retrieval. In Proc. 36-th
Hawaii Int’l. Conf. System Sciences, volume 4, page
108.2.
Andrew Finch and Eiichiro Sumita. 2008. Phrase-
based machine transliteration. In Proc. 3rd Int’l.
Joint Conf NLP, volume 1, Hyderabad, India, Jan-
uary.
Dayne Freitag and Zhiqiang Wang. 2009. Name
transliteration with bidirectional perceptron edit
models. In Proc. ACL/IJCNLP Named Entities
Workshop Shared Task.
Yoav Goldberg and Michael Elhadad. 2008. Identifica-
tion of transliterated foreign words in Hebrew script.
In Proc. CICLing, volume LNCS 4919, pages 466–
477.
Dan Goldwasser and Dan Roth. 2008. Translitera-
tion as constrained optimization. In Proc. EMNLP,
pages 353–362.
Jack Halpern. 2007. The challenges and pitfalls
of Arabic romanization and arabization. In Proc.
Workshop on Comp. Approaches to Arabic Script-
based Lang.
Rejwanul Haque, Sandipan Dandapat, Ankit Kumar
Srivastava, Sudip Kumar Naskar, and Andy Way.
2009. English-Hindi transliteration using context-
informed PB-SMT. In Proc. ACL/IJCNLP Named
Entities Workshop Shared Task.
Ulf Hermjakob, Kevin Knight, and Hal Daum´e. 2008.
Name translation in statistical machine translation:
Learning when to transliterate. In Proc. ACL,
Columbus, OH, USA, June.
Sittichai Jiampojamarn, Aditya Bhargava, Qing Dou,
Kenneth Dwyer, and Grzegorz Kondrak. 2009. Di-
recTL: a language-independent approach to translit-
eration. In Proc. ACL/IJCNLP Named Entities
Workshop Shared Task.
Mitesh Khapra and Pushpak Bhattacharyya. 2009. Im-
proving transliteration accuracy using word-origin
detection and lexicon lookup. In Proc. ACL/IJCNLP
Named Entities Workshop Shared Task.
Alexandre Klementiev and Dan Roth. 2006. Weakly
supervised named entity transliteration and discov-
ery from multilingual comparable corpora. In Proc.
21st Int’l Conf Computational Linguistics and 44th
Annual Meeting of ACL, pages 817–824, Sydney,
Australia, July.
Kevin Knight and Jonathan Graehl. 1998. Machine
transliteration. Computational Linguistics, 24(4).
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. HLT-NAACL.
A Kumaran and T. Kellner. 2007. A generic frame-
work for machine transliteration. In Proc. SIGIR,
pages 721–722.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. Int’l.
Conf. Machine Learning, pages 282–289.
Haizhou Li, Min Zhang, and Jian Su. 2004. A joint
source-channel model for machine transliteration.
In Proc. 42nd ACL Annual Meeting, pages 159–166,
Barcelona, Spain.
Linguistic Data Consortium. 2005. LDC Chinese-
English name entity lists LDC2005T34.
T. Mandl and C. Womser-Hacker. 2005. The effect of
named entities on effectiveness in cross-language in-
formation retrieval evaluation. In Proc. ACM Symp.
Applied Comp., pages 1059–1064.
MSRI. 2009. Microsoft Research India.
</figureCaption>
<bodyText confidence="0.904751">
http://research.microsoft.com/india.
Sara Noeman. 2009. Language independent translit-
eration system using phrase based SMT approach
on substring. In Proc. ACL/IJCNLP Named Entities
</bodyText>
<subsubsectionHeader confidence="0.567809">
Workshop Shared Task.
</subsubsectionHeader>
<figureCaption confidence="0.66426525">
Jong-Hoon Oh, Kiyotaka Uchimoto, and Kentaro Tori-
sawa. 2009. Machine transliteration with target-
language grapheme and phoneme: Multi-engine
transliteration approach. In Proc. ACL/IJCNLP
</figureCaption>
<subsubsectionHeader confidence="0.429896">
Named Entities Workshop Shared Task.
</subsubsectionHeader>
<bodyText confidence="0.8535225">
Taraka Rama and Karthik Gali. 2009. Modeling ma-
chine transliteration as a phrase based statistical ma-
chine translation problem. In Proc. ACL/IJCNLP
Named Entities Workshop Shared Task.
</bodyText>
<page confidence="0.99531">
9
</page>
<figureCaption confidence="0.885259566666666">
Tarek Sherif and Grzegorz Kondrak. 2007. Substring-
based transliteration. In Proc. 45th Annual Meeting
of the ACL, pages 944–951, Prague, Czech Repub-
lic, June.
Praneeth Shishtla, V Surya Ganesh, S Sethurama-
lingam, and Vasudeva Varma. 2009. A language-
independent transliteration schema using character
aligned models. In Proc. ACL/IJCNLP Named Enti-
ties Workshop Shared Task.
Yan Song. 2009. Name entities transliteration via
improved statistical translation on character-level
chunks. In Proc. ACL/IJCNLP Named Entities
Workshop Shared Task.
Richard Sproat, Tao Tao, and ChengXiang Zhai. 2006.
Named entity transliteration with comparable cor-
pora. In Proc. 21st Int’l Conf Computational Lin-
guistics and 44th Annual Meeting ofACL, pages 73–
80, Sydney, Australia.
Raghavendra Udupa, K. Saravanan, Anton Bakalov,
and Abhijit Bhole. 2009. “They are out there, if
you know where to look”: Mining transliterations
of OOV query terms for cross-language informa-
tion retrieval. In LNCS: Advances in Information
Retrieval, volume 5478, pages 437–448. Springer
Berlin / Heidelberg.
Dmitry Zelenko and Chinatsu Aone. 2006. Discrimi-
native methods for transliteration. In Proc. EMNLP,
pages 612–617, Sydney, Australia, July.
Dmitry Zelenko. 2009. Combining MDL translitera-
tion training with discriminative modeling. In Proc.
</figureCaption>
<figure confidence="0.3378205">
ACL/IJCNLP Named Entities Workshop Shared
Task.
</figure>
<page confidence="0.793568">
10
</page>
<table confidence="0.986024813953488">
Team ID Organisation English to English to English to English to English to English English Japanese
Hindi Tamil Kannada Russian Chinese to Ko- to translit-
rean Japanese erated to
Katakana Japanese
Kanji
EnHi EnTa EnKa EnRu EnCh EnKo EnJa JnJk
1 IIT Bombay x x x
2 Institution of Computational x
Linguistics Peking Univer-
sity
3 University of Tokyo x x x x x x x
4* University of Illinois, x x
Urbana-Champaign
5 IIT Bombay x x
6 NICT x x x x x x x x
7 University of Alberta x x x x x x
8 x x x x x x x x
9 x x x x x x x x
10 Johns Hopkins University x x x x x
11 x x x
12 x x
13 Jadavpur University x
14 IIIT Hyderabad x
15 x x x
16* ARL-CACI x
17 x x x x x x x x
18 x
19* Chaoyang University of x
Technology
20 Pondicherry University x x x
21 Microsoft Research x x
22 SRI International x x x x x
23 IBM Cairo TDC x x
24 SRA x x x x x x x x
25 IIT Kharagpur x x x
26 Institute of Software Chinese x
Academy of Sciences
27 x
28 George Washington Univer- x
sity
29* x
30 Dublin City University x
31 IIIT x x x x x
</table>
<tableCaption confidence="0.999885">
Table 3: Participation of teams in different tasks. *Participants without a system paper.
</tableCaption>
<page confidence="0.98247">
11
</page>
<figure confidence="0.999729308943089">
ACC
F-score
MRR
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
ACC
F-score
MRR
7 6 13 14 8 1 11 21 17 24 5 31 16 30 10 25 3 9 22 29 20 Site ID 6 17 11 1 31 24 8 5 25 3 10 9 22 20 Site ID
(a) English to Hindi (b) English to Kannada
ACC
F-score
MRR
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
ACC
F-score
MRR
6 17 11 31 1 25 24 3 8 10 9 22 20 Site ID 7 6 17 24 8 31 23 3 10 4 9 22 27 Site ID
(c) English to Tamil (d) English to Russian
ACC
F-score
MRR
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
ACC
F-score
MRR
6 7 15 8 2 17 9 18 24 4 3 26 31 22 28 10 23 19 Site ID 17 6 12 24 7 8 9 3 Site ID
(e) English to Chinese (f) English to Korean
ACC
F-score
MRR
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
ACC
F- core
MRR
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
6 15 17 7 21 3 8 24 12 9 Site ID 15 17 8 7 9 6 24 Site ID
6 15 17 7 21 3 8 24 12 9
Site ID
15 17 8 7 9 6 24
Site ID
(g) English to Japanese Katakana
(h) Japanese transliterated to Japanese Kanji
(g) English to Japanese Katakana (h) Japanese transliterated to Japanese Kanji
</figure>
<figureCaption confidence="0.99943">
Figure 2: Accuracy in top-1, F-score and MRR for standard runs.
</figureCaption>
<page confidence="0.746534">
12
</page>
<figure confidence="0.999439075471698">
0.6
0.5
0.4
0.3
0.2
0.1
0
MAP_ref
MAP_10
MAP_sys
Site ID
6 17 11 1 31 24 8 5 25 3 10 9 22 20
0.6
0.5
0.4
0.3
0.2
0.1
0
MAP_ref
MAP_10
MAP_sys
Site ID
7 6 17 24 8 31 23 3 10 4 9 22 27
0.6
0.5
0.4
0.3
0.2
0.1
0
MAP_ref
MAP_10
MAP_sys
Site ID
17 6 12 24 7 8 9 3
0.6
0.5
0.4
0.3
0.2
0.1
0
MAP_ref
MAP_10
MAP_sys
Site ID
15 17 8 7 9 6 24
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
6 7 15 8 2 17 9 18 24 4 3 26 31 22 28 10 23 19
0.6
0.5
0.4
0.3
0.2
0.1
0
6 15 17 7 21 3 8 24 12 9
(c) English to Tamil
(e) English to Chinese
0.6
0.5
7 6 13 14 8 1 11 21 17 24 5 31 16 30 10 25 3 9 22 29 20
(a) English to Hindi
6 17 11 31 1 25 24 3 8 10 9 22 20
0.6
0.5
0.4
0.3
0.2
0.1
0
0.4
0.3
0.2
0.1
0.0
(h) Japanese transliterated to Japanese Kanji
(g) English to Japanese Katakana
(b) English to Kannada
(d) English to Russian
(f) English to Korean
MAP_ref
MAP_10
MAP_sys
Site ID
MAP_ref
MAP_10
MAP_sys
Site ID
MAP_ref
MAP_10
MAP_sys
Site ID
Site ID
MAP_ref
MAP_10
MAP_sys
</figure>
<table confidence="0.947740916666667">
Team ID ACC F-score MRR MAPref MAP10 MAPsys Organisation
7 0.498 0.890 0.603 0.488 0.195 0.195 University of Alberta
6 0.483 0.892 0.607 0.477 0.202 0.202 1VICT
13 0.471 0.861 0.519 0.463 0.162 0.383 Jadavpur University
14 0.463 0.876 0.573 0.454 0.201 0.201 IIIT Hyderabad
8 0.462 0.876 0.576 0.454 0.189 0.189
1 0.423 0.863 0.544 0.417 0.179 0.202 IIT Bombay
11 0.418 0.879 0.546 0.412 0.183 0.240
21 0.418 0.864 0.522 0.409 0.170 0.170 Microsoft Research
17 0.415 0.858 0.505 0.406 0.164 0.168
24 0.409 0.864 0.527 0.402 0.174 0.176 SRA
5 0.409 0.881 0.546 0.400 0.184 0.184 IIT Bombay
31 0.407 0.877 0.544 0.402 0.195 0.195 IIIT
16 0.406 0.863 0.514 0.397 0.170 0.280 ARL-CACI
30 0.399 0.863 0.488 0.392 0.157 0.157 Dublin City University
10 0.398 0.855 0.515 0.389 0.170 0.170 Johns Hopkins University
25 0.366 0.854 0.493 0.360 0.164 0.164 IIT Kharagpur
3 0.363 0.864 0.503 0.360 0.170 0.170 University of Tokyo
9 0.349 0.829 0.455 0.341 0.151 0.151
22 0.212 0.788 0.317 0.207 0.106 0.106 SRI International
29 0.053 0.664 0.089 0.053 0.037 0.037
20 0.004 0.012 0.004 0.004 0.001 0.004 Pondicherry University
21 0.466 0.881 0.567 0.457 0.183 0.183 Microsoft Research (post-evaluation)
22 0.465 0.886 0.567 0.458 0.185 0.185 SRI International (post-evaluation)
</table>
<tableCaption confidence="0.984157">
Table 4: Standard runs for English to Hindi task.
</tableCaption>
<table confidence="0.9997026">
TeamID ACC F-score MRR MAPref MAP10 MAPsys Organisation
6 0.474 0.910 0.608 0.465 0.204 0.204 1VICT
17 0.436 0.894 0.551 0.427 0.184 0.189
11 0.435 0.902 0.572 0.430 0.195 0.265
31 0.406 0.894 0.542 0.399 0.193 0.193 IIIT
1 0.405 0.892 0.542 0.397 0.181 0.184 IIT Bombay
25 0.404 0.883 0.539 0.398 0.182 0.182 IIT Kharagpur
24 0.374 0.880 0.512 0.369 0.174 0.174 SRA
3 0.365 0.884 0.504 0.360 0.172 0.172 University of Tokyo
8 0.361 0.883 0.510 0.354 0.174 0.174
10 0.327 0.870 0.458 0.317 0.156 0.156 Johns Hopkins University
9 0.316 0.848 0.451 0.307 0.154 0.154
22 0.141 0.760 0.256 0.139 0.090 0.090 SRI International
20 0.061 0.131 0.068 0.059 0.021 0.056 Pondicherry University
22 0.475 0.909 0.581 0.466 0.193 0.193 SRI International (post-evaluation)
</table>
<tableCaption confidence="0.967937">
Table 5: Standard runs for English to Tamil task.
</tableCaption>
<table confidence="0.999732875">
Team ID ACC F-score MRR MAPref MAP10 MAPsys Organisation
6 0.398 0.880 0.526 0.391 0.178 0.178 1VICT
17 0.370 0.867 0.499 0.362 0.170 0.175
11 0.363 0.870 0.482 0.355 0.164 0.218
1 0.360 0.861 0.479 0.351 0.161 0.164 IIT Bombay
31 0.350 0.864 0.482 0.344 0.175 0.175 IIIT
24 0.345 0.854 0.462 0.336 0.157 0.157 SRA
8 0.343 0.855 0.458 0.334 0.155 0.155
5 0.335 0.859 0.453 0.327 0.154 0.154 IIT Bombay
25 0.335 0.856 0.457 0.328 0.154 0.154 IIT Kharagpur
3 0.324 0.856 0.438 0.315 0.148 0.148 University of Tokyo
10 0.235 0.817 0.353 0.229 0.121 0.121 Johns Hopkins University
9 0.177 0.799 0.307 0.178 0.109 0.109
22 0.091 0.735 0.180 0.090 0.064 0.064 SRI International
20 0.004 0.009 0.004 0.004 0.001 0.004 Pondicherry University
22 0.396 0.874 0.494 0.385 0.161 0.161 SRI International (post-evaluation)
</table>
<tableCaption confidence="0.995894">
Table 6: Standard runs for English to Kannada task.
</tableCaption>
<page confidence="0.935164">
14
</page>
<table confidence="0.9993314">
Team ID ACC F-score MRR MAP,,f MAP10 MAP3y3 Organisation
7 0.613 0.928 0.696 0.613 0.212 0.212 University of Alberta
6 0.605 0.926 0.701 0.605 0.215 0.215 1VICT
17 0.597 0.925 0.691 0.597 0.212 0.255
24 0.566 0.919 0.662 0.566 0.203 0.216 SRA
8 0.564 0.917 0.677 0.564 0.210 0.210
31 0.548 0.916 0.640 0.548 0.210 0.210 IIIT
23 0.545 0.917 0.596 0.545 0.286 0.299 IBM Cairo TDC
3 0.531 0.912 0.635 0.531 0.219 0.219 University of Tokyo
10 0.506 0.901 0.609 0.506 0.204 0.204 Johns Hopkins University
4 0.504 0.909 0.618 0.504 0.193 0.193 University of Illinois, Urbana-Champaign
9 0.500 0.906 0.613 0.500 0.192 0.192
22 0.364 0.876 0.440 0.364 0.136 0.136 SRI International
27 0.354 0.869 0.394 0.354 0.134 0.134
22 0.609 0.928 0.686 0.609 0.209 0.209 SRI International (post-evaluation)
</table>
<tableCaption confidence="0.984219">
Table 7: Standard runs for English to Russian task.
</tableCaption>
<table confidence="0.992318409090909">
Team ID ACC F-score MRR MAP,,f MAP10 MAP3y3 Organisation
6 0.731 0.895 0.812 0.731 0.246 0.246 1VICT
7 0.717 0.890 0.785 0.717 0.237 0.237 University of Alberta
15 0.713 0.883 0.794 0.713 0.241 0.241
8 0.666 0.864 0.765 0.666 0.234 0.234
2 0.652 0.858 0.755 0.652 0.232 0.232 Institution of Computational Linguistics Peking
University China
17 0.646 0.867 0.747 0.646 0.229 0.229
9 0.643 0.854 0.745 0.643 0.228 0.229
18 0.621 0.852 0.718 0.621 0.220 0.222
24 0.619 0.847 0.711 0.619 0.217 0.217 SRA
4 0.607 0.840 0.695 0.607 0.213 0.213 University of Illinois, Urbana-Champaign
3 0.580 0.826 0.653 0.580 0.199 0.199 University of Tokyo
26 0.498 0.786 0.603 0.498 0.187 0.189 Institute of Software Chinese Academy of Sci-
ences
31 0.493 0.804 0.600 0.493 0.192 0.192 IIIT
22 0.468 0.768 0.546 0.468 0.168 0.168 SRI International
28 0.456 0.763 0.587 0.456 0.185 0.185 George Washington University
10 0.450 0.755 0.514 0.450 0.157 0.166 Johns Hopkins University
23 0.411 0.737 0.464 0.411 0.141 0.173 IBM Cairo TDC
19 0.199 0.606 0.229 0.199 0.070 0.070 Chaoyang University of Technology
22 0.671 0.872 0.725 0.672 0.218 0.218 SRI International (post-evaluation)
</table>
<tableCaption confidence="0.985883">
Table 8: Standard runs for English to Chinese task.
</tableCaption>
<table confidence="0.757337222222222">
Team ID ACC F-score MRR MAP,,f MAP10 MAP3y3 Organisation
17 0.476 0.742 0.596 0.477 0.187 0.199
6 0.473 0.740 0.584 0.473 0.182 0.182 1VICT
12 0.451 0.720 0.576 0.451 0.181 0.181
24 0.413 0.702 0.524 0.412 0.165 0.165 SRA
7 0.387 0.693 0.469 0.387 0.146 0.146 University of Alberta
8 0.362 0.662 0.460 0.362 0.144 0.144
9 0.332 0.648 0.425 0.331 0.134 0.135
3 0.170 0.512 0.218 0.170 0.069 0.069 University of Tokyo
</table>
<tableCaption confidence="0.995886">
Table 9: Standard runs for English to Korean task.
</tableCaption>
<page confidence="0.919377">
15
</page>
<table confidence="0.982916333333333">
Team ID ACC F-score MRR MAP,,f MAP10 MAP3y3 Organisation
6 0.537 0.858 0.657 0.529 0.223 0.223 NICT
15 0.510 0.838 0.624 0.498 0.209 0.209
17 0.503 0.843 0.627 0.491 0.212 0.212
7 0.500 0.847 0.604 0.487 0.199 0.199 University of Alberta
21 0.465 0.827 0.559 0.454 0.183 0.183 Microsoft Research
3 0.457 0.828 0.576 0.445 0.194 0.194 University of Tokyo
8 0.449 0.816 0.571 0.436 0.192 0.192
24 0.420 0.807 0.541 0.410 0.182 0.184 SRA
12 0.408 0.808 0.537 0.398 0.182 0.182
9 0.406 0.800 0.529 0.393 0.180 0.180
21 0.469 0.834 0.567 0.454 0.186 0.186 Microsoft Research (post-evaluation)
</table>
<tableCaption confidence="0.988061">
Table 10: Standard runs for English to Japanese Katakana task.
</tableCaption>
<table confidence="0.686000375">
Team ID ACC F-score MRR MAP,,f MAP10 MAP3y3 Organisation
15 0.627 0.763 0.706 0.605 0.292 0.292
17 0.606 0.749 0.695 0.586 0.287 0.288
8 0.596 0.741 0.687 0.575 0.282 0.282
7 0.560 0.730 0.644 0.525 0.244 0.244 University of Alberta
9 0.555 0.708 0.653 0.538 0.261 0.261
6 0.532 0.716 0.583 0.485 0.214 0.218 NICT
24 0.509 0.675 0.600 0.491 0.226 0.226 SRA
</table>
<tableCaption confidence="0.80831">
Table 11: Standard runs for Japanese Transliterated to Japanese Kanji task.
</tableCaption>
<figure confidence="0.86970575">
Team ID ACC F-score MRR MAP,,f MAP10 MAP3y3 Organisation
7 0.509 0.893 0.610 0.498 0.198 0.198 University of Alberta
1 0.487 0.873 0.594 0.481 0.195 0.229 IIT Bombay
6 0.475 0.893 0.601 0.469 0.200 0.200 NICT
6 0.469 0.884 0.581 0.464 0.192 0.193 NICT
6 0.455 0.888 0.575 0.448 0.191 0.191 NICT
5 0.448 0.885 0.570 0.439 0.190 0.190 IIT Bombay
6 0.443 0.879 0.555 0.437 0.184 0.191 NICT
</figure>
<table confidence="0.965861363636364">
17 0.424 0.862 0.513 0.415 0.166 0.174
30 0.421 0.864 0.519 0.415 0.171 0.171 Dublin City University
30 0.420 0.867 0.519 0.413 0.170 0.170 Dublin City University
30 0.419 0.868 0.464 0.419 0.338 0.338 Dublin City University
16 0.407 0.862 0.528 0.399 0.175 0.289 ARL-CACI
16 0.407 0.862 0.528 0.399 0.175 0.289 ARL-CACI
30 0.407 0.856 0.507 0.399 0.168 0.168 Dublin City University
16 0.400 0.864 0.516 0.391 0.171 0.212 ARL-CACI
13 0.389 0.831 0.487 0.385 0.160 0.328 Jadavpur University
13 0.384 0.828 0.485 0.380 0.160 0.325 Jadavpur University
16 0.273 0.796 0.358 0.266 0.119 0.193 ARL-CACI
</table>
<tableCaption confidence="0.999038">
Table 12: Non-standard runs for English to Hindi task.
</tableCaption>
<figure confidence="0.9915505">
Team ID ACC F-score MRR MAP,,f MAP10 MAP3y3 Organisation
6 0.478 0.910 0.606 0.472 0.203 0.203 NICT
6 0.459 0.906 0.583 0.453 0.195 0.196 NICT
6 0.459 0.906 0.583 0.453 0.195 0.196 NICT
6 0.453 0.907 0.584 0.446 0.196 0.196 NICT
17 0.437 0.894 0.555 0.426 0.185 0.193
</figure>
<tableCaption confidence="0.968608">
Table 13: Non-standard runs for English to Tamil task.
</tableCaption>
<page confidence="0.903424">
16
</page>
<figure confidence="0.970041470588235">
Team ID ACC F-score MRR MAP,,f MAP10 MAP3y3 Organisation
6 0.399 0.881 0.522 0.391 0.176 0.176 NICT
6 0.386 0.877 0.503 0.379 0.169 0.169 NICT
6 0.380 0.869 0.488 0.370 0.163 0.163 NICT
17 0.374 0.868 0.502 0.366 0.170 0.176
6 0.373 0.869 0.485 0.362 0.162 0.168 NICT
Table 14: Non-standard runs for English to Kannada task.
Team ID ACC F-score MRR MAP,,f MAP10 MAP3y3 Organisation
17 0.955 0.989 0.966 0.955 0.284 0.504
17 0.609 0.928 0.701 0.609 0.214 0.263
7 0.608 0.927 0.694 0.608 0.212 0.212 University of Alberta
7 0.607 0.927 0.690 0.607 0.211 0.211 University of Alberta
6 0.600 0.927 0.634 0.600 0.189 0.189 NICT
6 0.600 0.926 0.699 0.600 0.214 0.214 NICT
7 0.591 0.928 0.679 0.591 0.208 0.208 University of Alberta
6 0.561 0.918 0.595 0.561 0.178 0.182 NICT
6 0.557 0.920 0.596 0.557 0.179 0.233 NICT
</figure>
<table confidence="0.965446714285714">
23 0.545 0.917 0.618 0.545 0.188 0.206 IBM Cairo TDC
23 0.524 0.913 0.602 0.524 0.184 0.203 IBM Cairo TDC
23 0.524 0.913 0.579 0.524 0.277 0.291 IBM Cairo TDC
4 0.496 0.908 0.613 0.496 0.191 0.191 University of Illinois, Urbana-Champaign
27 0.338 0.872 0.408 0.338 0.128 0.128
27 0.293 0.845 0.325 0.293 0.099 0.099
27 0.162 0.849 0.298 0.162 0.188 0.188
</table>
<tableCaption confidence="0.983583">
Table 15: Non-standard runs for English to Russian task.
</tableCaption>
<figure confidence="0.750237333333333">
Team ID ACC F-score MRR MAP,,f MAP10 MAP3y3 Organisation
17 0.909 0.960 0.933 0.909 0.276 0.276
7 0.746 0.900 0.814 0.746 0.245 0.245 University of Alberta
7 0.734 0.895 0.807 0.734 0.244 0.244 University of Alberta
7 0.732 0.895 0.803 0.732 0.242 0.242 University of Alberta
6 0.731 0.894 0.812 0.731 0.246 0.246 NICT
6 0.715 0.890 0.741 0.715 0.220 0.231 NICT
6 0.699 0.884 0.729 0.699 0.216 0.232 NICT
6 0.684 0.873 0.711 0.684 0.211 0.211 NICT
</figure>
<table confidence="0.984458142857143">
22 0.663 0.867 0.754 0.663 0.230 0.230 SRI International
17 0.658 0.865 0.752 0.658 0.230 0.230
18 0.587 0.834 0.665 0.587 0.203 0.330
26 0.500 0.786 0.607 0.500 0.189 0.191 Institute of Software Chinese Academy of Sciences
22 0.487 0.787 0.622 0.487 0.196 0.196 SRI International
28 0.462 0.764 0.564 0.462 0.175 0.175 George Washington University
28 0.458 0.763 0.602 0.458 0.191 0.191 George Washington University
23 0.411 0.737 0.464 0.411 0.141 0.173 IBM Cairo TDC
19 0.279 0.668 0.351 0.279 0.110 0.110 Chaoyang University of Technology
28 0.058 0.353 0.269 0.058 0.101 0.101 George Washington University
28 0.050 0.359 0.260 0.050 0.098 0.098 George Washington University
4 0.001 0.249 0.001 0.001 0.000 0.000 University of Illinois, Urbana-Champaign
22 0.674 0.873 0.763 0.674 0.232 0.232 SRI International (post-evaluation)
22 0.500 0.793 0.636 0.500 0.200 0.200 SRI International (post-evaluation)
</table>
<tableCaption confidence="0.99906">
Table 16: Non-standard runs for English to Chinese task.
</tableCaption>
<page confidence="0.965216">
17
</page>
<figure confidence="0.926675529411765">
Team ID ACC F-score MRR MAP,,f MAP10 MAP3y3 Organisation
17 0.794 0.894 0.836 0.793 0.249 0.323
12 0.785 0.887 0.840 0.785 0.252 0.441
12 0.784 0.889 0.840 0.784 0.252 0.484
12 0.781 0.885 0.839 0.781 0.252 0.460
12 0.740 0.868 0.806 0.740 0.243 0.243
6 0.461 0.737 0.576 0.461 0.180 0.180 NICT
6 0.457 0.734 0.506 0.457 0.153 0.153 NICT
6 0.447 0.718 0.493 0.447 0.149 0.149 NICT
6 0.369 0.679 0.406 0.369 0.123 0.123 NICT
Table 17: Non-standard runs for English to Korean task.
Team ID ACC F-score MRR MAP,,f MAP10 MAP3y3 Organisation
6 0.535 0.858 0.656 0.526 0.222 0.222 NICT
6 0.517 0.850 0.567 0.495 0.177 0.188 NICT
6 0.513 0.854 0.567 0.495 0.178 0.178 NICT
7 0.510 0.848 0.614 0.496 0.202 0.202 University of Alberta
6 0.500 0.842 0.547 0.480 0.170 0.196 NICT
</figure>
<tableCaption confidence="0.579141">
Table 18: Non-standard runs for English to Japanese Katakana task.
</tableCaption>
<figure confidence="0.979156555555556">
Team ID ACC F-score MRR MAP,,f MAP10 MAP3y3 Organisation
17 0.717 0.818 0.784 0.691 0.319 0.319
17 0.703 0.805 0.768 0.673 0.311 0.311
17 0.698 0.805 0.774 0.676 0.317 0.317
17 0.681 0.790 0.755 0.657 0.308 0.309
6 0.525 0.713 0.607 0.503 0.248 0.249 NICT
6 0.525 0.712 0.606 0.502 0.248 0.248 NICT
6 0.523 0.712 0.572 0.479 0.211 0.213 NICT
6 0.517 0.705 0.603 0.496 0.248 0.249 NICT
</figure>
<tableCaption confidence="0.983712">
Table 19: Non-standard runs for Japanese Transliterated to Japanese Kanji task.
</tableCaption>
<page confidence="0.998267">
18
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.240755">
<title confidence="0.939558">Report of NEWS 2009 Machine Transliteration Shared Task</title>
<author confidence="0.544842">A Vladimir Min</author>
<affiliation confidence="0.4951995">for Infocomm Research, A*STAR, Singapore Systems Research, Microsoft Research</affiliation>
<email confidence="0.995933">A.Kumaran@microsoft.com</email>
<abstract confidence="0.995397578947368">This report documents the details of the Machine Transliteration Shared Task conducted as a part of the Named Entities Workshop (NEWS), an ACL-IJCNLP 2009 workshop. The shared task features machine transliteration of proper names from English to a set of languages. This shared task has witnessed enthusiastic participation of 31 teams from all over the world, with diversity of participation for a given system and wide coverage for a given language pair (more than a dozen participants per language pair). Diverse transliteration methodologies are represented adequately in the shared task for a given language pair, thus underscoring the fact that the workshop may truly indicate the state of the art in machine transliteration in these language pairs. We measure and report 6 performance metrics on the submitted results. We believe that the shared task has successfully achieved the following objectives: (i) bringing together the community of researchers in the area of Machine Transliteration to focus on various research avenues, (ii) Calibrating systems on common corpora, using common metrics, thus creating a reasonable baseline for the state-of-the-art of transliteration systems, and (iii) providing a quantitative basis for meaningful comparison and analysis between various algorithmic approaches used in machine transliteration. We believe that the results of this shared task would uncover a host of interesting research problems, giving impetus to research in this significant research area.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>