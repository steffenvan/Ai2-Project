<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000075">
<title confidence="0.991977">
Efficient Learning for Undirected Topic Models
</title>
<author confidence="0.980012">
Jiatao Gu and Victor O.K. Li
</author>
<affiliation confidence="0.9729805">
Department of Electrical and Electronic Engineering
The University of Hong Kong
</affiliation>
<email confidence="0.984385">
{jiataogu, vli}@eee.hku.hk
</email>
<sectionHeader confidence="0.994474" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999166">
Replicated Softmax model, a well-known
undirected topic model, is powerful in ex-
tracting semantic representations of docu-
ments. Traditional learning strategies such
as Contrastive Divergence are very inef-
ficient. This paper provides a novel esti-
mator to speed up the learning based on
Noise Contrastive Estimate, extended for
documents of variant lengths and weighted
inputs. Experiments on two benchmarks
show that the new estimator achieves great
learning efficiency and high accuracy on
document retrieval and classification.
</bodyText>
<sectionHeader confidence="0.998426" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999968333333333">
Topic models are powerful probabilistic graphical
approaches to analyze document semantics in dif-
ferent applications such as document categoriza-
tion and information retrieval. They are mainly
constructed by directed structure like pLSA (Hof-
mann, 2000) and LDA (Blei et al., 2003). Accom-
panied by the vast developments in deep learn-
ing, several undirected topic models, such as
(Salakhutdinov and Hinton, 2009; Srivastava et
al., 2013), have recently been reported to achieve
great improvements in efficiency and accuracy.
Replicated Softmax model (RSM) (Hinton and
Salakhutdinov, 2009), a kind of typical undirected
topic model, is composed of a family of Restricted
Boltzmann Machines (RBMs). Commonly, RSM
is learned like standard RBMs using approximate
methods like Contrastive Divergence (CD). How-
ever, CD is not really designed for RSM. Different
from RBMs with binary input, RSM adopts soft-
max units to represent words, resulting in great in-
efficiency with sampling inside CD, especially for
a large vocabulary. Yet, NLP systems usually re-
quire vocabulary sizes of tens to hundreds of thou-
sands, thus seriously limiting its application.
Dealing with the large vocabulary size of the in-
puts is a serious problem in deep-learning-based
NLP systems. Bengio et al. (2003) pointed this
problem out when normalizing the softmax proba-
bility in the neural language model (NNLM), and
Morin and Bengio (2005) solved it based on a hi-
erarchical binary tree. A similar architecture was
used in word representations like (Mnih and Hin-
ton, 2009; Mikolov et al., 2013a). Directed tree
structures cannot be applied to undirected mod-
els like RSM, but stochastic approaches can work
well. For instance, Dahl et al. (2012) found that
several Metropolis Hastings sampling (MH) ap-
proaches approximate the softmax distribution in
CD well, although MH requires additional com-
plexity in computation. Hyv¨arinen (2007) pro-
posed Ratio Matching (RM) to train unnormal-
ized models, and Dauphin and Bengio (2013)
added stochastic approaches in RM to accommo-
date high-dimensional inputs. Recently, a new es-
timator Noise Contrastive Estimate (NCE) (Gut-
mann and Hyv¨arinen, 2010) is proposed for un-
normalized models, and shows great efficiency in
learning word representations such as in (Mnih
and Teh, 2012; Mikolov et al., 2013b).
In this paper, we propose an efficient learning
strategy for RSM named α-NCE, applying NCE as
the basic estimator. Different from most related ef-
forts that use NCE for predicting single word, our
method extends NCE to generate noise for doc-
uments in variant lengths. It also enables RSM to
use weighted inputs to improve the modelling abil-
ity. As RSM is usually used as the first layer in
many deeper undirected models like Deep Boltz-
mann Machines (Srivastava et al., 2013), α-NCE
can be readily extended to learn them efficiently.
</bodyText>
<sectionHeader confidence="0.99653" genericHeader="method">
2 Replicated Softmax Model
</sectionHeader>
<bodyText confidence="0.912607333333333">
RSM is a typical undirected topic model, which is
based on bag-of-words (BoW) to represent docu-
ments. In general, it consists of a series of RBMs,
</bodyText>
<page confidence="0.887901">
162
</page>
<bodyText confidence="0.882423941176471">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 162–167,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
each of which contains variant softmax visible
units but the same binary hidden units.
Suppose K is the vocabulary size. For a docu-
ment with D words, if the ith word in the docu-
ment equals the kth word of the dictionary, a vec-
tor vi ∈ {0,1}K is assigned, only with the kth
element vik = 1. An RBM is formed by assign-
ing a hidden state h ∈ {0,1}H to this document
V = {v1, ..., vD}, where the energy function is:
Ee(V , h) = −hT W vˆ− bT vˆ− D · aTh (1)
where θ = {W, b, a} are parameters shared by
all the RBMs, and vˆ = PDi=1 vi is commonly re-
ferred to as the word count vector of a document.
The probability for the document V is given by:
</bodyText>
<equation confidence="0.999012">
Pe(V ) = 1 e—Fθ(V) = X e−Fθ(V )
Z ZDV
D (2)
XFe(V ) = log h e−Eθ(V ,h)
</equation>
<bodyText confidence="0.999869833333333">
where Fe(V ) is the “free energy”, which can be
analytically integrated easily, and ZD is the “par-
tition function” for normalization, only associated
with the document length D. As the hidden state
and document are conditionally independent, the
conditional distributions are derived:
</bodyText>
<equation confidence="0.888992714285714">
exp �WT �
k h + bk
Pe (vik = 1|h) = K ex WTh + bk)
k=1p (3)
� ( k
Pe (hj = 1|V ) = σ (Wjˆv + D · aj) (4)
where σ(x) = 1
</equation>
<bodyText confidence="0.994604833333333">
1+e−x. Equation (3) is the soft-
max units describing the multinomial distribution
of the words, and Equation (4) serves as an effi-
cient inference from words to semantic meanings,
where we adopt the probabilities of each hidden
unit “activated” as the topic features.
</bodyText>
<subsectionHeader confidence="0.999501">
2.1 Learning Strategies for RSM
</subsectionHeader>
<bodyText confidence="0.995904">
RSM is naturally learned by minimizing the nega-
tive log-likelihood function (ML) as follows:
</bodyText>
<equation confidence="0.98895">
L(θ) = −EV ∼Pdata [log Pe(V )] (5)
</equation>
<bodyText confidence="0.9998905">
However, the gradient is intractable for the combi-
natorial normalization term ZD. Common strate-
gies to overcome this intractability are MCMC-
based approaches such as Contrastive Divergence
(CD) (Hinton, 2002) and Persistent CD (PCD)
(Tieleman, 2008), both of which require repeating
Gibbs steps of h(i) ∼ Pe(h|V (i)) and V (i+1) ∼
Pe(V |h(i)) to generate model samples to approx-
imate the gradient. Typically, the performance and
consistency improve when more steps are adopted.
Notwithstanding, even one Gibbs step is time con-
suming for RSM, since the multinomial sampling
normally requires linear time computations. The
“alias method” (Kronmal and Peterson Jr, 1979)
speeds up multinomial sampling to constant time
while linear time is required for processing the dis-
tribution. Since Pe(V |h) changes at every itera-
tion in CD, such methods cannot be used.
</bodyText>
<sectionHeader confidence="0.947495" genericHeader="method">
3 Efficient Learning for RSM
</sectionHeader>
<bodyText confidence="0.999846">
Unlike (Dahl et al., 2012) that retains CD, we
adopted NCE as the basic learning strategy. Con-
sidering RSM is designed for documents, we fur-
ther modified NCE with two novel heuristics,
developing the approach “Partial Noise Uniform
Contrastive Estimate” (or α-NCE for short).
</bodyText>
<subsectionHeader confidence="0.989357">
3.1 Noise Contrastive Estimate
</subsectionHeader>
<bodyText confidence="0.999979833333334">
Noise Contrastive Estimate (NCE), similar to CD,
is another estimator for training models with in-
tractable partition functions. NCE solves the in-
tractability through treating the partition function
ZD as an additional parameter ZcD added to θ,
which makes the likelihood computable. Yet, the
model cannot be trained through ML as the likeli-
hood tends to be arbitrarily large by setting ZcD to
huge numbers. Instead, NCE learns the model in a
proxy classification problem with noise samples.
another collection (noise) {Vn}Tn with Tn = kTd,
Given a document collection (data) {Vd}Td, and
NCE distinguishes these (1+k)Td documents sim-
ply based on Bayes’ Theorem, where we assumed
data samples matched by our model, indicating
Pe &apos; Pdata, and noise samples generated from an
artificial distribution Pn. Parameters are learned
by minimizing the cross-entropy function:
</bodyText>
<equation confidence="0.935057">
J(θ) = −EVd∼Pθ [log σk(X(Vd))] (6)
−kEVn∼Pn [log σk−1(−X (Vn))]
</equation>
<bodyText confidence="0.843087181818182">
and the gradient is derived as follows,
−∇eJ(θ) =EVd∼Pθ [σk−1(−X)∇eX(Vd)]
−kEVn∼Pn [σk(X)∇eX(Vn)] (7)
where σk(x) = 1
1+ke−x , and the “log-ratio” is:
X(V ) = log [Pe(V )/Pn(V )] (8)
J(θ) can be optimized efficiently with stochastic
gradient descent (SGD). Gutmann and Hyv¨arinen
(2010) showed that the NCE gradient ∇eJ(θ) will
reach the ML gradient when k → ∞. In practice,
a larger k tends to train the model better.
</bodyText>
<page confidence="0.994255">
163
</page>
<subsectionHeader confidence="0.997382">
3.2 Partial Noise Sampling
</subsectionHeader>
<bodyText confidence="0.9999364">
Different from (Mnih and Teh, 2012), which gen-
erates noise per word, RSM requires the estimator
to sample the noise at the document level. An in-
tuitive approach is to sample from the empirical
distribution p˜ for D times, where the log probabil-
</bodyText>
<equation confidence="0.624913">
ity is computed: log Pn(V ) = E [vT log ˜p].
v∈V
</equation>
<bodyText confidence="0.950149125">
For a fixed k, Gutmann and Hyv¨arinen (2010)
suggested choosing the noise close to the data for
a sufficient learning result, indicating full noise
might not be satisfactory. We proposed an alter-
native “Partial Noise Sampling (PNS)” to gener-
ate noise by replacing part of the data with sam-
pled words. See Algorithm 1, where we fixed the
Algorithm 1 Partial Noise Sampling
</bodyText>
<listItem confidence="0.79144925">
1: Initialize: k, α E (0, 1)
2: for each Vd = {v}D E {Vd}Td do
3: Set: Dr = Fα · D]
4: Draw: Vr = {vr}Dr C V uniformly
5: for j = 1, ..., k do
Draw: V (j)
n = {v(j)
6: n }D−Dr ∼ p˜
V (j)
n = V (j)
7: n ∪ Vr
8: end for
9: Bind: (Vd, Vr), (V (1)
n , Vr), ..., (V (k)
n ,Vr)
10: end for
</listItem>
<bodyText confidence="0.9980355">
proportion of remaining words at α, named “noise
level” of PNS. However, traversing all the condi-
tions to guess the remaining words requires O(D!)
computations. To avoid this, we simply bound the
remaining words with the data and noise in ad-
vance and the noise log Pn(V ) is derived readily:
</bodyText>
<equation confidence="0.994045">
� log Pθ (Vr) + v∈V \V[vT log p] (9)
r
</equation>
<bodyText confidence="0.99997">
where the remaining words Vr are still assumed
to be described by RSM with a smaller document
length. In this way, it also strengthens the robust-
ness of RSM towards incomplete data.
Sampling the noise normally requires additional
computational load. Fortunately, since p˜ is fixed,
sampling is efficient using the “alias method”. It
also allows storing the noise for subsequent use,
yielding much faster computation than CD.
</bodyText>
<subsectionHeader confidence="0.993474">
3.3 Uniform Contrastive Estimate
</subsectionHeader>
<bodyText confidence="0.971131214285714">
When we initially implemented NCE for RSM,
we found the document lengths terribly biased the
log-ratio, resulting in bad parameters. Therefore
“Uniform Contrastive Estimate (UCE)” was pro-
posed to accommodate variant document lengths
by adding the uniform assumption:
¯X(V ) = D−1 log [Pθ(V )/Pn(V )] (10)
where UCE adopts the uniform probabilities D√Pθ
and D√Pn for classification to average the mod-
elling ability at word-level. Note that D is not
necessarily an integer in UCE, and allows choos-
ing a real-valued weights on the document such as
idf-weighting (Salton and McGill, 1983). Typi-
cally, it is defined as a weighting vector w, where
</bodyText>
<equation confidence="0.587961">
wk = log |V ∈{Vd}:vik=1,vi∈V  |is multiplied to the
Td
</equation>
<bodyText confidence="0.8823805">
kth word in the dictionary. Thus for a weighted in-
put Vw and corresponding length Dw, we derive:
</bodyText>
<equation confidence="0.987344666666667">
˜X(Vw) = Dw−1 log [Pθ(V w)/Pn(Vw)] (11)
where log Pn(Vw) = E [vwT log ˜p]. A
vw∈V w
</equation>
<bodyText confidence="0.987976333333333">
specific ZcDw will be assigned to Pθ(Vw).
Combining PNS and UCE yields a new estima-
tor for RSM, which we simply call α-NCE1.
</bodyText>
<sectionHeader confidence="0.99985" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.996992">
4.1 Datasets and Details of Learning
</subsectionHeader>
<bodyText confidence="0.999990083333333">
We evaluated the new estimator to train RSMs on
two text datasets: 20 Newsgroups and IMDB.
The 20 Newsgroups2 dataset is a collection of
the Usenet posts, which contains 11,345 training
and 7,531 testing instances. Both the training and
testing sets are labeled into 20 classes. Removing
stop words as well as stemming were performed.
The IMDB dataset3 is a benchmark for senti-
ment analysis, which consists of 100,000 movie
reviews taken from IMDB. The dataset is divided
into 75,000 training instances (1/3 labeled and
2/3 unlabeled) and 25,000 testing instances. Two
types of labels, positive and negative, are given to
show sentiment. Following (Maas et al., 2011), no
stop words are removed from this dataset.
For each dataset, we randomly selected 10% of
the training set for validation, and the idf-weight
vector is computed in advance. In addition, replac-
ing the word count vˆ by Flog (1 + ˆv)] slightly im-
proved the modelling performance for all models.
We implemented α-NCE according to the pa-
rameter settings in (Hinton, 2010) using SGD in
minibatches of size 128 and an initialized learning
rate of 0.1. The number of hidden units was fixed
</bodyText>
<footnote confidence="0.99991975">
1α comes from the noise level in PNS, but UCE is also
the vital part of this estimator, which is absorbed in α-NCE.
2Available at http://qwone.com/˜jason/20Newsgroups
3Available at http://ai.stanford.edu/˜amaas/data/sentiment
</footnote>
<page confidence="0.998246">
164
</page>
<bodyText confidence="0.999722875">
at 128 for all models. Although learning the parti-
tion function ZD separately for every length D is
nearly impossible, as in (Mnih and Teh, 2012) we
also surprisingly found freezing ZD as a constant
function of D without updating never harmed but
actually enhanced the performance. It is proba-
bly because the large number of free parameters
in RSM are forced to learn better when ZD is a
constant. In practise, we set this constant function
as ZD = 2H · (Ek ebk)D. It can readily extend to
learn RSM for real-valued weighted length D&apos;.
We also implemented CD with the same set-
tings. All the experiments were run on a single
GPU GTX970 using the library Theano (Bergstra
et al., 2010). To make the comparison fair, both
α-NCE and CD share the same implementation.
</bodyText>
<subsectionHeader confidence="0.994508">
4.2 Evaluation of Efficiency
</subsectionHeader>
<bodyText confidence="0.9951095">
To evaluate the efficiency in learning, we used
the most frequent words as dictionaries with sizes
ranging from 100 to 20, 000 for both datasets, and
test the computation time both for CD of vari-
ant Gibbs steps and α-NCE of variant noise sam-
ple sizes. The comparison of the mean running
</bodyText>
<figureCaption confidence="0.999722">
Figure 1: Comparison of running time
</figureCaption>
<bodyText confidence="0.9999597">
time per minibatch is clearly shown in Figure 1,
which is averaged on both datasets. Typically,
α-NCE achieves 10 to 500 times speed-up com-
pared to CD. Although both CD and α-NCE run
slower when the input dimension increases, CD
tends to take much more time due to the multino-
mial sampling at each iteration, especially when
more Gibbs steps are used. In contrast, running
time stays reasonable in α-NCE even if a larger
noise size or a larger dimension is applied.
</bodyText>
<subsectionHeader confidence="0.998381">
4.3 Evaluation of Performance
</subsectionHeader>
<bodyText confidence="0.975884675">
One direct measure to evaluate the modelling per-
formance is to assess RSM as a generative model
to estimate the log-probability per word as per-
plexity. However, as α-NCE learns RSM by dis-
tinguishing the data and noise from their respec-
tive features, parameters are trained more like a
feature extractor than a generative model. It is not
fair to use perplexity to evaluate the performance.
For this reason, we evaluated the modelling per-
formance with some indirect measures.
Figure 2: Precision-Recall curves for the retrieval
task on the 20 Newsgroups dataset using RSMs.
For 20 Newsgroups, we trained RSMs on the
training set, and reported the results on docu-
ment retrieval and document classification. For
retrieval, we treated the testing set as queries, and
retrieved documents with the same labels in the
training set by cosine-similarity. Precision-recall
(P-R) curves and mean average precision (MAP)
are two metrics we used for evaluation. For clas-
sification, we trained a softmax regression on the
training set, and checked the accuracy on the test-
ing set. We use this dataset to show the modelling
ability of RSM with different estimators.
For IMDB, the whole training set is used for
learning RSMs, and an L2-regularized logistic re-
gression is trained on the labeled training set. The
error rate of sentiment classification on the testing
set is reported, compared with several BoW-based
baselines. We use this dataset to show the general
modelling ability of RSM compared with others.
We trained both α-NCE and CD, and naturally
NCE (without UCE) at a fixed vocabulary size
(2000 for 20 Newsgroups, and 5000 for IMDB).
Posteriors of the hidden units were used as topic
features. For α-NCE , we fixed noise level at 0.5
for 20 Newsgroups and 0.3 for IMDB. In compar-
ison, we trained CD from 1 up to 5 Gibbs steps.
Figure 2 and Table 1 show that a larger noise
size in α-NCE achieves better modelling perfor-
</bodyText>
<page confidence="0.987972">
165
</page>
<figure confidence="0.982431">
(a) MAP for document retrieval (b) Document classification accuracy (c) Sentiment classification accuracy
</figure>
<figureCaption confidence="0.7976">
Figure 3: Tracking the modelling performance with variant α using α-NCE to learn RSMs. CD is also
reported as the baseline. (a) (b) are performed on 20 Newsgroups, and (c) is performed on IMDB.
</figureCaption>
<bodyText confidence="0.999853727272727">
mance, and α-NCE greatly outperforms CD on re-
trieval tasks especially around large recall values.
The classification results of α-NCE is also compa-
rable or slightly better than CD. Simultaneously,
it is gratifying to find that the idf-weighting in-
puts achieve the best results both in retrieval and
classification tasks, as idf-weighting is known to
extract information better than word count. In ad-
dition, naturally NCE performs poorly compared
to others in Figure 2, indicating variant document
lengths actually bias the learning greatly.
</bodyText>
<equation confidence="0.862657">
α-NCE
CD
k=1 k=5 k=25 k=25 (idf)
64.1% 61.8% 63.6% 64.8% 65.6%
</equation>
<tableCaption confidence="0.8154935">
Table 1: Comparison of classification accuracy on
the 20 Newsgroups dataset using RSMs.
</tableCaption>
<table confidence="0.999733111111111">
Models Accuracy
Bag of Words (BoW) (Maas and Ng, 2010) 86.75%
LDA (Maas et al., 2011) 67.42%
LSA (Maas et al., 2011) 83.96%
Maas et al. (2011)’s “full” model 87.44%
WRRBM (Dahl et al., 2012) 87.42%
RSM:CD 86.22%
RSM:α-NCE-5 87.09%
RSM:α-NCE-5 (idf) 87.81%
</table>
<tableCaption confidence="0.997943">
Table 2: The performance of sentiment classifica-
</tableCaption>
<bodyText confidence="0.949417055555556">
tion accuracy on the IMDB dataset using RSMs
compared to other BoW-based approaches.
On the other hand, Table 2 shows the perfor-
mance of RSM in sentiment classification, where
model combinations reported in previous efforts
are not considered. It is clear that α-NCE learns
RSM better than CD, and outperforms BoW and
other BoW-based models4 such as LDA. The idf-
4Accurately, WRRBM uses “bag of n-grams” assumption.
weighting inputs also achieve the best perfor-
mance. Note that RSM is also based on BoW, in-
dicating α-NCE has arguably reached the limits of
learning BoW-based models. In future work, RSM
can be extended to more powerful undirected topic
models, by considering more syntactic informa-
tion such as word-order or dependency relation-
ship in representation. α-NCE can be used to learn
them efficiently and achieve better performance.
</bodyText>
<subsectionHeader confidence="0.999551">
4.4 Choice of Noise Level-α
</subsectionHeader>
<bodyText confidence="0.999987705882353">
In order to decide the best noise level (α) for PNS,
we learned RSMs using α-NCE with different
noise levels for both word count and idf-weighting
inputs on the two datasets. Figure 3 shows that
α-NCE learning with partial noise (α &gt; 0) out-
performs full noise (α = 0) in most situations,
and achieves better results than CD in retrieval and
classification on both datasets. However, learning
tends to become extremely difficult if the noise
becomes too close to the data, and this explains
why the performance drops rapidly when α → 1.
Furthermore, curves in Figure 3 also imply the
choice of α might be problem-dependent, with
larger sets like IMDB requiring relatively smaller
α. Nonetheless, a systematic strategy for choos-
ing optimal α will be explored in future work. In
practise, a range from 0.3 ∼ 0.5 is recommended.
</bodyText>
<sectionHeader confidence="0.999504" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999991375">
We propose a novel approach α-NCE for learning
undirected topic models such as RSM efficiently,
allowing large vocabulary sizes. It is new a es-
timator based on NCE, and adapted to documents
with variant lengths and weighted inputs. We learn
RSMs with α-NCE on two classic benchmarks,
where it achieves both efficiency in learning and
accuracy in retrieval and classification tasks.
</bodyText>
<page confidence="0.998078">
166
</page>
<sectionHeader confidence="0.983427" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999862086956522">
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. The Journal of Machine Learning Re-
search, 3:1137–1155.
James Bergstra, Olivier Breuleux, Fr´ed´eric Bastien,
Pascal Lamblin, Razvan Pascanu, Guillaume Des-
jardins, Joseph Turian, David Warde-Farley, and
Yoshua Bengio. 2010. Theano: a CPU and
GPU math expression compiler. In Proceedings
of the Python for Scientific Computing Conference
(SciPy), June. Oral Presentation.
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. the Journal of ma-
chine Learning research, 3:993–1022.
George E Dahl, Ryan P Adams, and Hugo Larochelle.
2012. Training restricted boltzmann machines on
word observations. arXiv preprint arXiv:1202.5695.
Yann Dauphin and Yoshua Bengio. 2013. Stochastic
ratio matching of rbms for sparse high-dimensional
inputs. In Advances in Neural Information Process-
ing Systems, pages 1340–1348.
Michael Gutmann and Aapo Hyv¨arinen. 2010. Noise-
contrastive estimation: A new estimation princi-
ple for unnormalized statistical models. In Inter-
national Conference on Artificial Intelligence and
Statistics, pages 297–304.
Geoffrey E Hinton and Ruslan R Salakhutdinov. 2009.
Replicated softmax: an undirected topic model. In
Advances in neural information processing systems,
pages 1607–1614.
Geoffrey Hinton. 2002. Training products of experts
by minimizing contrastive divergence. Neural com-
putation, 14(8):1771–1800.
Geoffrey Hinton. 2010. A practical guide to train-
ing restricted boltzmann machines. Momentum,
9(1):926.
Thomas Hofmann. 2000. Learning the similarity of
documents: An information-geometric approach to
document retrieval and categorization.
Aapo Hyv¨arinen. 2007. Some extensions of score
matching. Computational statistics &amp; data analysis,
51(5):2499–2512.
Richard A Kronmal and Arthur V Peterson Jr. 1979.
On the alias method for generating random variables
from a discrete distribution. The American Statisti-
cian, 33(4):214–218.
Andrew L Maas and Andrew Y Ng. 2010. A prob-
abilistic model for semantic word vectors. In NIPS
Workshop on Deep Learning and Unsupervised Fea-
ture Learning.
Andrew L Maas, Raymond E Daly, Peter T Pham, Dan
Huang, Andrew Y Ng, and Christopher Potts. 2011.
Learning word vectors for sentiment analysis. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies-Volume 1, pages 142–150. As-
sociation for Computational Linguistics.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111–3119.
Andriy Mnih and Geoffrey E Hinton. 2009. A scal-
able hierarchical distributed language model. In
Advances in neural information processing systems,
pages 1081–1088.
Andriy Mnih and Yee Whye Teh. 2012. A fast and
simple algorithm for training neural probabilistic
language models. arXiv preprint arXiv:1206.6426.
Frederic Morin and Yoshua Bengio. 2005. Hierarchi-
cal probabilistic neural network language model. In
Proceedings of the international workshop on artifi-
cial intelligence and statistics, pages 246–252. Cite-
seer.
Ruslan Salakhutdinov and Geoffrey Hinton. 2009. Se-
mantic hashing. International Journal of Approxi-
mate Reasoning, 50(7):969–978.
Gerard Salton and Michael J McGill. 1983. Introduc-
tion to modern information retrieval.
Nitish Srivastava, Ruslan R Salakhutdinov, and Ge-
offrey E Hinton. 2013. Modeling documents
with deep boltzmann machines. arXiv preprint
arXiv:1309.6865.
Tijmen Tieleman. 2008. Training restricted boltz-
mann machines using approximations to the likeli-
hood gradient. In Proceedings of the 25th interna-
tional conference on Machine learning, pages 1064–
1071. ACM.
</reference>
<page confidence="0.997756">
167
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.960792">
<title confidence="0.999911">Efficient Learning for Undirected Topic Models</title>
<author confidence="0.999611">Jiatao Gu</author>
<author confidence="0.999611">O K Victor</author>
<affiliation confidence="0.9922185">Department of Electrical and Electronic The University of Hong</affiliation>
<abstract confidence="0.9981955">Replicated Softmax model, a well-known undirected topic model, is powerful in extracting semantic representations of documents. Traditional learning strategies such as Contrastive Divergence are very inefficient. This paper provides a novel estimator to speed up the learning based on Noise Contrastive Estimate, extended for documents of variant lengths and weighted inputs. Experiments on two benchmarks show that the new estimator achieves great learning efficiency and high accuracy on document retrieval and classification.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Janvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="2009" citStr="Bengio et al. (2003)" startWordPosition="295" endWordPosition="298">of Restricted Boltzmann Machines (RBMs). Commonly, RSM is learned like standard RBMs using approximate methods like Contrastive Divergence (CD). However, CD is not really designed for RSM. Different from RBMs with binary input, RSM adopts softmax units to represent words, resulting in great inefficiency with sampling inside CD, especially for a large vocabulary. Yet, NLP systems usually require vocabulary sizes of tens to hundreds of thousands, thus seriously limiting its application. Dealing with the large vocabulary size of the inputs is a serious problem in deep-learning-based NLP systems. Bengio et al. (2003) pointed this problem out when normalizing the softmax probability in the neural language model (NNLM), and Morin and Bengio (2005) solved it based on a hierarchical binary tree. A similar architecture was used in word representations like (Mnih and Hinton, 2009; Mikolov et al., 2013a). Directed tree structures cannot be applied to undirected models like RSM, but stochastic approaches can work well. For instance, Dahl et al. (2012) found that several Metropolis Hastings sampling (MH) approaches approximate the softmax distribution in CD well, although MH requires additional complexity in compu</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. The Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Bergstra</author>
<author>Olivier Breuleux</author>
<author>Fr´ed´eric Bastien</author>
<author>Pascal Lamblin</author>
<author>Razvan Pascanu</author>
<author>Guillaume Desjardins</author>
<author>Joseph Turian</author>
<author>David Warde-Farley</author>
<author>Yoshua Bengio</author>
</authors>
<title>Theano: a CPU and GPU math expression compiler.</title>
<date>2010</date>
<booktitle>In Proceedings of the Python for Scientific Computing Conference (SciPy),</booktitle>
<tech>Oral Presentation.</tech>
<contexts>
<context position="12998" citStr="Bergstra et al., 2010" startWordPosition="2188" endWordPosition="2191">on ZD separately for every length D is nearly impossible, as in (Mnih and Teh, 2012) we also surprisingly found freezing ZD as a constant function of D without updating never harmed but actually enhanced the performance. It is probably because the large number of free parameters in RSM are forced to learn better when ZD is a constant. In practise, we set this constant function as ZD = 2H · (Ek ebk)D. It can readily extend to learn RSM for real-valued weighted length D&apos;. We also implemented CD with the same settings. All the experiments were run on a single GPU GTX970 using the library Theano (Bergstra et al., 2010). To make the comparison fair, both α-NCE and CD share the same implementation. 4.2 Evaluation of Efficiency To evaluate the efficiency in learning, we used the most frequent words as dictionaries with sizes ranging from 100 to 20, 000 for both datasets, and test the computation time both for CD of variant Gibbs steps and α-NCE of variant noise sample sizes. The comparison of the mean running Figure 1: Comparison of running time time per minibatch is clearly shown in Figure 1, which is averaged on both datasets. Typically, α-NCE achieves 10 to 500 times speed-up compared to CD. Although both C</context>
</contexts>
<marker>Bergstra, Breuleux, Bastien, Lamblin, Pascanu, Desjardins, Turian, Warde-Farley, Bengio, 2010</marker>
<rawString>James Bergstra, Olivier Breuleux, Fr´ed´eric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume Desjardins, Joseph Turian, David Warde-Farley, and Yoshua Bengio. 2010. Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientific Computing Conference (SciPy), June. Oral Presentation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>the Journal of machine Learning research,</journal>
<pages>3--993</pages>
<contexts>
<context position="1015" citStr="Blei et al., 2003" startWordPosition="141" endWordPosition="144">icient. This paper provides a novel estimator to speed up the learning based on Noise Contrastive Estimate, extended for documents of variant lengths and weighted inputs. Experiments on two benchmarks show that the new estimator achieves great learning efficiency and high accuracy on document retrieval and classification. 1 Introduction Topic models are powerful probabilistic graphical approaches to analyze document semantics in different applications such as document categorization and information retrieval. They are mainly constructed by directed structure like pLSA (Hofmann, 2000) and LDA (Blei et al., 2003). Accompanied by the vast developments in deep learning, several undirected topic models, such as (Salakhutdinov and Hinton, 2009; Srivastava et al., 2013), have recently been reported to achieve great improvements in efficiency and accuracy. Replicated Softmax model (RSM) (Hinton and Salakhutdinov, 2009), a kind of typical undirected topic model, is composed of a family of Restricted Boltzmann Machines (RBMs). Commonly, RSM is learned like standard RBMs using approximate methods like Contrastive Divergence (CD). However, CD is not really designed for RSM. Different from RBMs with binary input</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet allocation. the Journal of machine Learning research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George E Dahl</author>
<author>Ryan P Adams</author>
<author>Hugo Larochelle</author>
</authors>
<title>Training restricted boltzmann machines on word observations. arXiv preprint arXiv:1202.5695.</title>
<date>2012</date>
<contexts>
<context position="2444" citStr="Dahl et al. (2012)" startWordPosition="367" endWordPosition="370"> thousands, thus seriously limiting its application. Dealing with the large vocabulary size of the inputs is a serious problem in deep-learning-based NLP systems. Bengio et al. (2003) pointed this problem out when normalizing the softmax probability in the neural language model (NNLM), and Morin and Bengio (2005) solved it based on a hierarchical binary tree. A similar architecture was used in word representations like (Mnih and Hinton, 2009; Mikolov et al., 2013a). Directed tree structures cannot be applied to undirected models like RSM, but stochastic approaches can work well. For instance, Dahl et al. (2012) found that several Metropolis Hastings sampling (MH) approaches approximate the softmax distribution in CD well, although MH requires additional complexity in computation. Hyv¨arinen (2007) proposed Ratio Matching (RM) to train unnormalized models, and Dauphin and Bengio (2013) added stochastic approaches in RM to accommodate high-dimensional inputs. Recently, a new estimator Noise Contrastive Estimate (NCE) (Gutmann and Hyv¨arinen, 2010) is proposed for unnormalized models, and shows great efficiency in learning word representations such as in (Mnih and Teh, 2012; Mikolov et al., 2013b). In </context>
<context position="6482" citStr="Dahl et al., 2012" startWordPosition="1068" endWordPosition="1071"> of h(i) ∼ Pe(h|V (i)) and V (i+1) ∼ Pe(V |h(i)) to generate model samples to approximate the gradient. Typically, the performance and consistency improve when more steps are adopted. Notwithstanding, even one Gibbs step is time consuming for RSM, since the multinomial sampling normally requires linear time computations. The “alias method” (Kronmal and Peterson Jr, 1979) speeds up multinomial sampling to constant time while linear time is required for processing the distribution. Since Pe(V |h) changes at every iteration in CD, such methods cannot be used. 3 Efficient Learning for RSM Unlike (Dahl et al., 2012) that retains CD, we adopted NCE as the basic learning strategy. Considering RSM is designed for documents, we further modified NCE with two novel heuristics, developing the approach “Partial Noise Uniform Contrastive Estimate” (or α-NCE for short). 3.1 Noise Contrastive Estimate Noise Contrastive Estimate (NCE), similar to CD, is another estimator for training models with intractable partition functions. NCE solves the intractability through treating the partition function ZD as an additional parameter ZcD added to θ, which makes the likelihood computable. Yet, the model cannot be trained thr</context>
<context position="17030" citStr="Dahl et al., 2012" startWordPosition="2863" endWordPosition="2866">best results both in retrieval and classification tasks, as idf-weighting is known to extract information better than word count. In addition, naturally NCE performs poorly compared to others in Figure 2, indicating variant document lengths actually bias the learning greatly. α-NCE CD k=1 k=5 k=25 k=25 (idf) 64.1% 61.8% 63.6% 64.8% 65.6% Table 1: Comparison of classification accuracy on the 20 Newsgroups dataset using RSMs. Models Accuracy Bag of Words (BoW) (Maas and Ng, 2010) 86.75% LDA (Maas et al., 2011) 67.42% LSA (Maas et al., 2011) 83.96% Maas et al. (2011)’s “full” model 87.44% WRRBM (Dahl et al., 2012) 87.42% RSM:CD 86.22% RSM:α-NCE-5 87.09% RSM:α-NCE-5 (idf) 87.81% Table 2: The performance of sentiment classification accuracy on the IMDB dataset using RSMs compared to other BoW-based approaches. On the other hand, Table 2 shows the performance of RSM in sentiment classification, where model combinations reported in previous efforts are not considered. It is clear that α-NCE learns RSM better than CD, and outperforms BoW and other BoW-based models4 such as LDA. The idf4Accurately, WRRBM uses “bag of n-grams” assumption. weighting inputs also achieve the best performance. Note that RSM is al</context>
</contexts>
<marker>Dahl, Adams, Larochelle, 2012</marker>
<rawString>George E Dahl, Ryan P Adams, and Hugo Larochelle. 2012. Training restricted boltzmann machines on word observations. arXiv preprint arXiv:1202.5695.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yann Dauphin</author>
<author>Yoshua Bengio</author>
</authors>
<title>Stochastic ratio matching of rbms for sparse high-dimensional inputs.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>1340--1348</pages>
<contexts>
<context position="2723" citStr="Dauphin and Bengio (2013)" startWordPosition="408" endWordPosition="411"> model (NNLM), and Morin and Bengio (2005) solved it based on a hierarchical binary tree. A similar architecture was used in word representations like (Mnih and Hinton, 2009; Mikolov et al., 2013a). Directed tree structures cannot be applied to undirected models like RSM, but stochastic approaches can work well. For instance, Dahl et al. (2012) found that several Metropolis Hastings sampling (MH) approaches approximate the softmax distribution in CD well, although MH requires additional complexity in computation. Hyv¨arinen (2007) proposed Ratio Matching (RM) to train unnormalized models, and Dauphin and Bengio (2013) added stochastic approaches in RM to accommodate high-dimensional inputs. Recently, a new estimator Noise Contrastive Estimate (NCE) (Gutmann and Hyv¨arinen, 2010) is proposed for unnormalized models, and shows great efficiency in learning word representations such as in (Mnih and Teh, 2012; Mikolov et al., 2013b). In this paper, we propose an efficient learning strategy for RSM named α-NCE, applying NCE as the basic estimator. Different from most related efforts that use NCE for predicting single word, our method extends NCE to generate noise for documents in variant lengths. It also enables</context>
</contexts>
<marker>Dauphin, Bengio, 2013</marker>
<rawString>Yann Dauphin and Yoshua Bengio. 2013. Stochastic ratio matching of rbms for sparse high-dimensional inputs. In Advances in Neural Information Processing Systems, pages 1340–1348.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Gutmann</author>
<author>Aapo Hyv¨arinen</author>
</authors>
<title>Noisecontrastive estimation: A new estimation principle for unnormalized statistical models.</title>
<date>2010</date>
<booktitle>In International Conference on Artificial Intelligence and Statistics,</booktitle>
<pages>297--304</pages>
<marker>Gutmann, Hyv¨arinen, 2010</marker>
<rawString>Michael Gutmann and Aapo Hyv¨arinen. 2010. Noisecontrastive estimation: A new estimation principle for unnormalized statistical models. In International Conference on Artificial Intelligence and Statistics, pages 297–304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey E Hinton</author>
<author>Ruslan R Salakhutdinov</author>
</authors>
<title>Replicated softmax: an undirected topic model. In Advances in neural information processing systems,</title>
<date>2009</date>
<pages>1607--1614</pages>
<contexts>
<context position="1321" citStr="Hinton and Salakhutdinov, 2009" startWordPosition="185" endWordPosition="188"> retrieval and classification. 1 Introduction Topic models are powerful probabilistic graphical approaches to analyze document semantics in different applications such as document categorization and information retrieval. They are mainly constructed by directed structure like pLSA (Hofmann, 2000) and LDA (Blei et al., 2003). Accompanied by the vast developments in deep learning, several undirected topic models, such as (Salakhutdinov and Hinton, 2009; Srivastava et al., 2013), have recently been reported to achieve great improvements in efficiency and accuracy. Replicated Softmax model (RSM) (Hinton and Salakhutdinov, 2009), a kind of typical undirected topic model, is composed of a family of Restricted Boltzmann Machines (RBMs). Commonly, RSM is learned like standard RBMs using approximate methods like Contrastive Divergence (CD). However, CD is not really designed for RSM. Different from RBMs with binary input, RSM adopts softmax units to represent words, resulting in great inefficiency with sampling inside CD, especially for a large vocabulary. Yet, NLP systems usually require vocabulary sizes of tens to hundreds of thousands, thus seriously limiting its application. Dealing with the large vocabulary size of </context>
</contexts>
<marker>Hinton, Salakhutdinov, 2009</marker>
<rawString>Geoffrey E Hinton and Ruslan R Salakhutdinov. 2009. Replicated softmax: an undirected topic model. In Advances in neural information processing systems, pages 1607–1614.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Hinton</author>
</authors>
<title>Training products of experts by minimizing contrastive divergence.</title>
<date>2002</date>
<booktitle>Neural computation,</booktitle>
<pages>14--8</pages>
<contexts>
<context position="5778" citStr="Hinton, 2002" startWordPosition="956" endWordPosition="957">is the softmax units describing the multinomial distribution of the words, and Equation (4) serves as an efficient inference from words to semantic meanings, where we adopt the probabilities of each hidden unit “activated” as the topic features. 2.1 Learning Strategies for RSM RSM is naturally learned by minimizing the negative log-likelihood function (ML) as follows: L(θ) = −EV ∼Pdata [log Pe(V )] (5) However, the gradient is intractable for the combinatorial normalization term ZD. Common strategies to overcome this intractability are MCMCbased approaches such as Contrastive Divergence (CD) (Hinton, 2002) and Persistent CD (PCD) (Tieleman, 2008), both of which require repeating Gibbs steps of h(i) ∼ Pe(h|V (i)) and V (i+1) ∼ Pe(V |h(i)) to generate model samples to approximate the gradient. Typically, the performance and consistency improve when more steps are adopted. Notwithstanding, even one Gibbs step is time consuming for RSM, since the multinomial sampling normally requires linear time computations. The “alias method” (Kronmal and Peterson Jr, 1979) speeds up multinomial sampling to constant time while linear time is required for processing the distribution. Since Pe(V |h) changes at eve</context>
</contexts>
<marker>Hinton, 2002</marker>
<rawString>Geoffrey Hinton. 2002. Training products of experts by minimizing contrastive divergence. Neural computation, 14(8):1771–1800.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Hinton</author>
</authors>
<title>A practical guide to training restricted boltzmann machines.</title>
<date>2010</date>
<journal>Momentum,</journal>
<volume>9</volume>
<issue>1</issue>
<contexts>
<context position="11969" citStr="Hinton, 2010" startWordPosition="2016" endWordPosition="2017"> reviews taken from IMDB. The dataset is divided into 75,000 training instances (1/3 labeled and 2/3 unlabeled) and 25,000 testing instances. Two types of labels, positive and negative, are given to show sentiment. Following (Maas et al., 2011), no stop words are removed from this dataset. For each dataset, we randomly selected 10% of the training set for validation, and the idf-weight vector is computed in advance. In addition, replacing the word count vˆ by Flog (1 + ˆv)] slightly improved the modelling performance for all models. We implemented α-NCE according to the parameter settings in (Hinton, 2010) using SGD in minibatches of size 128 and an initialized learning rate of 0.1. The number of hidden units was fixed 1α comes from the noise level in PNS, but UCE is also the vital part of this estimator, which is absorbed in α-NCE. 2Available at http://qwone.com/˜jason/20Newsgroups 3Available at http://ai.stanford.edu/˜amaas/data/sentiment 164 at 128 for all models. Although learning the partition function ZD separately for every length D is nearly impossible, as in (Mnih and Teh, 2012) we also surprisingly found freezing ZD as a constant function of D without updating never harmed but actuall</context>
</contexts>
<marker>Hinton, 2010</marker>
<rawString>Geoffrey Hinton. 2010. A practical guide to training restricted boltzmann machines. Momentum, 9(1):926.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
</authors>
<title>Learning the similarity of documents: An information-geometric approach to document retrieval and categorization.</title>
<date>2000</date>
<contexts>
<context position="987" citStr="Hofmann, 2000" startWordPosition="136" endWordPosition="138">ivergence are very inefficient. This paper provides a novel estimator to speed up the learning based on Noise Contrastive Estimate, extended for documents of variant lengths and weighted inputs. Experiments on two benchmarks show that the new estimator achieves great learning efficiency and high accuracy on document retrieval and classification. 1 Introduction Topic models are powerful probabilistic graphical approaches to analyze document semantics in different applications such as document categorization and information retrieval. They are mainly constructed by directed structure like pLSA (Hofmann, 2000) and LDA (Blei et al., 2003). Accompanied by the vast developments in deep learning, several undirected topic models, such as (Salakhutdinov and Hinton, 2009; Srivastava et al., 2013), have recently been reported to achieve great improvements in efficiency and accuracy. Replicated Softmax model (RSM) (Hinton and Salakhutdinov, 2009), a kind of typical undirected topic model, is composed of a family of Restricted Boltzmann Machines (RBMs). Commonly, RSM is learned like standard RBMs using approximate methods like Contrastive Divergence (CD). However, CD is not really designed for RSM. Different</context>
</contexts>
<marker>Hofmann, 2000</marker>
<rawString>Thomas Hofmann. 2000. Learning the similarity of documents: An information-geometric approach to document retrieval and categorization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aapo Hyv¨arinen</author>
</authors>
<title>Some extensions of score matching.</title>
<date>2007</date>
<booktitle>Computational statistics &amp; data analysis,</booktitle>
<pages>51--5</pages>
<marker>Hyv¨arinen, 2007</marker>
<rawString>Aapo Hyv¨arinen. 2007. Some extensions of score matching. Computational statistics &amp; data analysis, 51(5):2499–2512.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard A Kronmal</author>
<author>Arthur V Peterson Jr</author>
</authors>
<title>On the alias method for generating random variables from a discrete distribution.</title>
<date>1979</date>
<journal>The American Statistician,</journal>
<volume>33</volume>
<issue>4</issue>
<marker>Kronmal, Jr, 1979</marker>
<rawString>Richard A Kronmal and Arthur V Peterson Jr. 1979. On the alias method for generating random variables from a discrete distribution. The American Statistician, 33(4):214–218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew L Maas</author>
<author>Andrew Y Ng</author>
</authors>
<title>A probabilistic model for semantic word vectors.</title>
<date>2010</date>
<booktitle>In NIPS Workshop on Deep Learning and Unsupervised Feature Learning.</booktitle>
<contexts>
<context position="16894" citStr="Maas and Ng, 2010" startWordPosition="2838" endWordPosition="2841">α-NCE is also comparable or slightly better than CD. Simultaneously, it is gratifying to find that the idf-weighting inputs achieve the best results both in retrieval and classification tasks, as idf-weighting is known to extract information better than word count. In addition, naturally NCE performs poorly compared to others in Figure 2, indicating variant document lengths actually bias the learning greatly. α-NCE CD k=1 k=5 k=25 k=25 (idf) 64.1% 61.8% 63.6% 64.8% 65.6% Table 1: Comparison of classification accuracy on the 20 Newsgroups dataset using RSMs. Models Accuracy Bag of Words (BoW) (Maas and Ng, 2010) 86.75% LDA (Maas et al., 2011) 67.42% LSA (Maas et al., 2011) 83.96% Maas et al. (2011)’s “full” model 87.44% WRRBM (Dahl et al., 2012) 87.42% RSM:CD 86.22% RSM:α-NCE-5 87.09% RSM:α-NCE-5 (idf) 87.81% Table 2: The performance of sentiment classification accuracy on the IMDB dataset using RSMs compared to other BoW-based approaches. On the other hand, Table 2 shows the performance of RSM in sentiment classification, where model combinations reported in previous efforts are not considered. It is clear that α-NCE learns RSM better than CD, and outperforms BoW and other BoW-based models4 such as </context>
</contexts>
<marker>Maas, Ng, 2010</marker>
<rawString>Andrew L Maas and Andrew Y Ng. 2010. A probabilistic model for semantic word vectors. In NIPS Workshop on Deep Learning and Unsupervised Feature Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew L Maas</author>
<author>Raymond E Daly</author>
<author>Peter T Pham</author>
<author>Dan Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Learning word vectors for sentiment analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume</booktitle>
<volume>1</volume>
<pages>142--150</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="11600" citStr="Maas et al., 2011" startWordPosition="1951" endWordPosition="1954">wo text datasets: 20 Newsgroups and IMDB. The 20 Newsgroups2 dataset is a collection of the Usenet posts, which contains 11,345 training and 7,531 testing instances. Both the training and testing sets are labeled into 20 classes. Removing stop words as well as stemming were performed. The IMDB dataset3 is a benchmark for sentiment analysis, which consists of 100,000 movie reviews taken from IMDB. The dataset is divided into 75,000 training instances (1/3 labeled and 2/3 unlabeled) and 25,000 testing instances. Two types of labels, positive and negative, are given to show sentiment. Following (Maas et al., 2011), no stop words are removed from this dataset. For each dataset, we randomly selected 10% of the training set for validation, and the idf-weight vector is computed in advance. In addition, replacing the word count vˆ by Flog (1 + ˆv)] slightly improved the modelling performance for all models. We implemented α-NCE according to the parameter settings in (Hinton, 2010) using SGD in minibatches of size 128 and an initialized learning rate of 0.1. The number of hidden units was fixed 1α comes from the noise level in PNS, but UCE is also the vital part of this estimator, which is absorbed in α-NCE.</context>
<context position="16925" citStr="Maas et al., 2011" startWordPosition="2844" endWordPosition="2847">ghtly better than CD. Simultaneously, it is gratifying to find that the idf-weighting inputs achieve the best results both in retrieval and classification tasks, as idf-weighting is known to extract information better than word count. In addition, naturally NCE performs poorly compared to others in Figure 2, indicating variant document lengths actually bias the learning greatly. α-NCE CD k=1 k=5 k=25 k=25 (idf) 64.1% 61.8% 63.6% 64.8% 65.6% Table 1: Comparison of classification accuracy on the 20 Newsgroups dataset using RSMs. Models Accuracy Bag of Words (BoW) (Maas and Ng, 2010) 86.75% LDA (Maas et al., 2011) 67.42% LSA (Maas et al., 2011) 83.96% Maas et al. (2011)’s “full” model 87.44% WRRBM (Dahl et al., 2012) 87.42% RSM:CD 86.22% RSM:α-NCE-5 87.09% RSM:α-NCE-5 (idf) 87.81% Table 2: The performance of sentiment classification accuracy on the IMDB dataset using RSMs compared to other BoW-based approaches. On the other hand, Table 2 shows the performance of RSM in sentiment classification, where model combinations reported in previous efforts are not considered. It is clear that α-NCE learns RSM better than CD, and outperforms BoW and other BoW-based models4 such as LDA. The idf4Accurately, WRRBM </context>
</contexts>
<marker>Maas, Daly, Pham, Huang, Ng, Potts, 2011</marker>
<rawString>Andrew L Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 142–150. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</title>
<date>2013</date>
<contexts>
<context position="2293" citStr="Mikolov et al., 2013" startWordPosition="343" endWordPosition="346">n great inefficiency with sampling inside CD, especially for a large vocabulary. Yet, NLP systems usually require vocabulary sizes of tens to hundreds of thousands, thus seriously limiting its application. Dealing with the large vocabulary size of the inputs is a serious problem in deep-learning-based NLP systems. Bengio et al. (2003) pointed this problem out when normalizing the softmax probability in the neural language model (NNLM), and Morin and Bengio (2005) solved it based on a hierarchical binary tree. A similar architecture was used in word representations like (Mnih and Hinton, 2009; Mikolov et al., 2013a). Directed tree structures cannot be applied to undirected models like RSM, but stochastic approaches can work well. For instance, Dahl et al. (2012) found that several Metropolis Hastings sampling (MH) approaches approximate the softmax distribution in CD well, although MH requires additional complexity in computation. Hyv¨arinen (2007) proposed Ratio Matching (RM) to train unnormalized models, and Dauphin and Bengio (2013) added stochastic approaches in RM to accommodate high-dimensional inputs. Recently, a new estimator Noise Contrastive Estimate (NCE) (Gutmann and Hyv¨arinen, 2010) is pr</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="2293" citStr="Mikolov et al., 2013" startWordPosition="343" endWordPosition="346">n great inefficiency with sampling inside CD, especially for a large vocabulary. Yet, NLP systems usually require vocabulary sizes of tens to hundreds of thousands, thus seriously limiting its application. Dealing with the large vocabulary size of the inputs is a serious problem in deep-learning-based NLP systems. Bengio et al. (2003) pointed this problem out when normalizing the softmax probability in the neural language model (NNLM), and Morin and Bengio (2005) solved it based on a hierarchical binary tree. A similar architecture was used in word representations like (Mnih and Hinton, 2009; Mikolov et al., 2013a). Directed tree structures cannot be applied to undirected models like RSM, but stochastic approaches can work well. For instance, Dahl et al. (2012) found that several Metropolis Hastings sampling (MH) approaches approximate the softmax distribution in CD well, although MH requires additional complexity in computation. Hyv¨arinen (2007) proposed Ratio Matching (RM) to train unnormalized models, and Dauphin and Bengio (2013) added stochastic approaches in RM to accommodate high-dimensional inputs. Recently, a new estimator Noise Contrastive Estimate (NCE) (Gutmann and Hyv¨arinen, 2010) is pr</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013b. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>A scalable hierarchical distributed language model.</title>
<date>2009</date>
<booktitle>In Advances in neural information processing systems,</booktitle>
<pages>1081--1088</pages>
<contexts>
<context position="2271" citStr="Mnih and Hinton, 2009" startWordPosition="338" endWordPosition="342">sent words, resulting in great inefficiency with sampling inside CD, especially for a large vocabulary. Yet, NLP systems usually require vocabulary sizes of tens to hundreds of thousands, thus seriously limiting its application. Dealing with the large vocabulary size of the inputs is a serious problem in deep-learning-based NLP systems. Bengio et al. (2003) pointed this problem out when normalizing the softmax probability in the neural language model (NNLM), and Morin and Bengio (2005) solved it based on a hierarchical binary tree. A similar architecture was used in word representations like (Mnih and Hinton, 2009; Mikolov et al., 2013a). Directed tree structures cannot be applied to undirected models like RSM, but stochastic approaches can work well. For instance, Dahl et al. (2012) found that several Metropolis Hastings sampling (MH) approaches approximate the softmax distribution in CD well, although MH requires additional complexity in computation. Hyv¨arinen (2007) proposed Ratio Matching (RM) to train unnormalized models, and Dauphin and Bengio (2013) added stochastic approaches in RM to accommodate high-dimensional inputs. Recently, a new estimator Noise Contrastive Estimate (NCE) (Gutmann and H</context>
</contexts>
<marker>Mnih, Hinton, 2009</marker>
<rawString>Andriy Mnih and Geoffrey E Hinton. 2009. A scalable hierarchical distributed language model. In Advances in neural information processing systems, pages 1081–1088.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Yee Whye Teh</author>
</authors>
<title>A fast and simple algorithm for training neural probabilistic language models. arXiv preprint arXiv:1206.6426.</title>
<date>2012</date>
<contexts>
<context position="3015" citStr="Mnih and Teh, 2012" startWordPosition="453" endWordPosition="456">can work well. For instance, Dahl et al. (2012) found that several Metropolis Hastings sampling (MH) approaches approximate the softmax distribution in CD well, although MH requires additional complexity in computation. Hyv¨arinen (2007) proposed Ratio Matching (RM) to train unnormalized models, and Dauphin and Bengio (2013) added stochastic approaches in RM to accommodate high-dimensional inputs. Recently, a new estimator Noise Contrastive Estimate (NCE) (Gutmann and Hyv¨arinen, 2010) is proposed for unnormalized models, and shows great efficiency in learning word representations such as in (Mnih and Teh, 2012; Mikolov et al., 2013b). In this paper, we propose an efficient learning strategy for RSM named α-NCE, applying NCE as the basic estimator. Different from most related efforts that use NCE for predicting single word, our method extends NCE to generate noise for documents in variant lengths. It also enables RSM to use weighted inputs to improve the modelling ability. As RSM is usually used as the first layer in many deeper undirected models like Deep Boltzmann Machines (Srivastava et al., 2013), α-NCE can be readily extended to learn them efficiently. 2 Replicated Softmax Model RSM is a typica</context>
<context position="8177" citStr="Mnih and Teh, 2012" startWordPosition="1337" endWordPosition="1340"> Pn. Parameters are learned by minimizing the cross-entropy function: J(θ) = −EVd∼Pθ [log σk(X(Vd))] (6) −kEVn∼Pn [log σk−1(−X (Vn))] and the gradient is derived as follows, −∇eJ(θ) =EVd∼Pθ [σk−1(−X)∇eX(Vd)] −kEVn∼Pn [σk(X)∇eX(Vn)] (7) where σk(x) = 1 1+ke−x , and the “log-ratio” is: X(V ) = log [Pe(V )/Pn(V )] (8) J(θ) can be optimized efficiently with stochastic gradient descent (SGD). Gutmann and Hyv¨arinen (2010) showed that the NCE gradient ∇eJ(θ) will reach the ML gradient when k → ∞. In practice, a larger k tends to train the model better. 163 3.2 Partial Noise Sampling Different from (Mnih and Teh, 2012), which generates noise per word, RSM requires the estimator to sample the noise at the document level. An intuitive approach is to sample from the empirical distribution p˜ for D times, where the log probability is computed: log Pn(V ) = E [vT log ˜p]. v∈V For a fixed k, Gutmann and Hyv¨arinen (2010) suggested choosing the noise close to the data for a sufficient learning result, indicating full noise might not be satisfactory. We proposed an alternative “Partial Noise Sampling (PNS)” to generate noise by replacing part of the data with sampled words. See Algorithm 1, where we fixed the Algor</context>
<context position="12460" citStr="Mnih and Teh, 2012" startWordPosition="2091" endWordPosition="2094">ly improved the modelling performance for all models. We implemented α-NCE according to the parameter settings in (Hinton, 2010) using SGD in minibatches of size 128 and an initialized learning rate of 0.1. The number of hidden units was fixed 1α comes from the noise level in PNS, but UCE is also the vital part of this estimator, which is absorbed in α-NCE. 2Available at http://qwone.com/˜jason/20Newsgroups 3Available at http://ai.stanford.edu/˜amaas/data/sentiment 164 at 128 for all models. Although learning the partition function ZD separately for every length D is nearly impossible, as in (Mnih and Teh, 2012) we also surprisingly found freezing ZD as a constant function of D without updating never harmed but actually enhanced the performance. It is probably because the large number of free parameters in RSM are forced to learn better when ZD is a constant. In practise, we set this constant function as ZD = 2H · (Ek ebk)D. It can readily extend to learn RSM for real-valued weighted length D&apos;. We also implemented CD with the same settings. All the experiments were run on a single GPU GTX970 using the library Theano (Bergstra et al., 2010). To make the comparison fair, both α-NCE and CD share the sam</context>
</contexts>
<marker>Mnih, Teh, 2012</marker>
<rawString>Andriy Mnih and Yee Whye Teh. 2012. A fast and simple algorithm for training neural probabilistic language models. arXiv preprint arXiv:1206.6426.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederic Morin</author>
<author>Yoshua Bengio</author>
</authors>
<title>Hierarchical probabilistic neural network language model.</title>
<date>2005</date>
<booktitle>In Proceedings of the international workshop on artificial intelligence and statistics,</booktitle>
<pages>246--252</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="2140" citStr="Morin and Bengio (2005)" startWordPosition="316" endWordPosition="319">Divergence (CD). However, CD is not really designed for RSM. Different from RBMs with binary input, RSM adopts softmax units to represent words, resulting in great inefficiency with sampling inside CD, especially for a large vocabulary. Yet, NLP systems usually require vocabulary sizes of tens to hundreds of thousands, thus seriously limiting its application. Dealing with the large vocabulary size of the inputs is a serious problem in deep-learning-based NLP systems. Bengio et al. (2003) pointed this problem out when normalizing the softmax probability in the neural language model (NNLM), and Morin and Bengio (2005) solved it based on a hierarchical binary tree. A similar architecture was used in word representations like (Mnih and Hinton, 2009; Mikolov et al., 2013a). Directed tree structures cannot be applied to undirected models like RSM, but stochastic approaches can work well. For instance, Dahl et al. (2012) found that several Metropolis Hastings sampling (MH) approaches approximate the softmax distribution in CD well, although MH requires additional complexity in computation. Hyv¨arinen (2007) proposed Ratio Matching (RM) to train unnormalized models, and Dauphin and Bengio (2013) added stochastic</context>
</contexts>
<marker>Morin, Bengio, 2005</marker>
<rawString>Frederic Morin and Yoshua Bengio. 2005. Hierarchical probabilistic neural network language model. In Proceedings of the international workshop on artificial intelligence and statistics, pages 246–252. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruslan Salakhutdinov</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Semantic hashing.</title>
<date>2009</date>
<journal>International Journal of Approximate Reasoning,</journal>
<volume>50</volume>
<issue>7</issue>
<contexts>
<context position="1144" citStr="Salakhutdinov and Hinton, 2009" startWordPosition="161" endWordPosition="164">for documents of variant lengths and weighted inputs. Experiments on two benchmarks show that the new estimator achieves great learning efficiency and high accuracy on document retrieval and classification. 1 Introduction Topic models are powerful probabilistic graphical approaches to analyze document semantics in different applications such as document categorization and information retrieval. They are mainly constructed by directed structure like pLSA (Hofmann, 2000) and LDA (Blei et al., 2003). Accompanied by the vast developments in deep learning, several undirected topic models, such as (Salakhutdinov and Hinton, 2009; Srivastava et al., 2013), have recently been reported to achieve great improvements in efficiency and accuracy. Replicated Softmax model (RSM) (Hinton and Salakhutdinov, 2009), a kind of typical undirected topic model, is composed of a family of Restricted Boltzmann Machines (RBMs). Commonly, RSM is learned like standard RBMs using approximate methods like Contrastive Divergence (CD). However, CD is not really designed for RSM. Different from RBMs with binary input, RSM adopts softmax units to represent words, resulting in great inefficiency with sampling inside CD, especially for a large vo</context>
</contexts>
<marker>Salakhutdinov, Hinton, 2009</marker>
<rawString>Ruslan Salakhutdinov and Geoffrey Hinton. 2009. Semantic hashing. International Journal of Approximate Reasoning, 50(7):969–978.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Michael J McGill</author>
</authors>
<title>Introduction to modern information retrieval.</title>
<date>1983</date>
<contexts>
<context position="10464" citStr="Salton and McGill, 1983" startWordPosition="1755" endWordPosition="1758">an CD. 3.3 Uniform Contrastive Estimate When we initially implemented NCE for RSM, we found the document lengths terribly biased the log-ratio, resulting in bad parameters. Therefore “Uniform Contrastive Estimate (UCE)” was proposed to accommodate variant document lengths by adding the uniform assumption: ¯X(V ) = D−1 log [Pθ(V )/Pn(V )] (10) where UCE adopts the uniform probabilities D√Pθ and D√Pn for classification to average the modelling ability at word-level. Note that D is not necessarily an integer in UCE, and allows choosing a real-valued weights on the document such as idf-weighting (Salton and McGill, 1983). Typically, it is defined as a weighting vector w, where wk = log |V ∈{Vd}:vik=1,vi∈V |is multiplied to the Td kth word in the dictionary. Thus for a weighted input Vw and corresponding length Dw, we derive: ˜X(Vw) = Dw−1 log [Pθ(V w)/Pn(Vw)] (11) where log Pn(Vw) = E [vwT log ˜p]. A vw∈V w specific ZcDw will be assigned to Pθ(Vw). Combining PNS and UCE yields a new estimator for RSM, which we simply call α-NCE1. 4 Experiments 4.1 Datasets and Details of Learning We evaluated the new estimator to train RSMs on two text datasets: 20 Newsgroups and IMDB. The 20 Newsgroups2 dataset is a collecti</context>
</contexts>
<marker>Salton, McGill, 1983</marker>
<rawString>Gerard Salton and Michael J McGill. 1983. Introduction to modern information retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitish Srivastava</author>
<author>Ruslan R Salakhutdinov</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>Modeling documents with deep boltzmann machines. arXiv preprint arXiv:1309.6865.</title>
<date>2013</date>
<contexts>
<context position="1170" citStr="Srivastava et al., 2013" startWordPosition="165" endWordPosition="168"> and weighted inputs. Experiments on two benchmarks show that the new estimator achieves great learning efficiency and high accuracy on document retrieval and classification. 1 Introduction Topic models are powerful probabilistic graphical approaches to analyze document semantics in different applications such as document categorization and information retrieval. They are mainly constructed by directed structure like pLSA (Hofmann, 2000) and LDA (Blei et al., 2003). Accompanied by the vast developments in deep learning, several undirected topic models, such as (Salakhutdinov and Hinton, 2009; Srivastava et al., 2013), have recently been reported to achieve great improvements in efficiency and accuracy. Replicated Softmax model (RSM) (Hinton and Salakhutdinov, 2009), a kind of typical undirected topic model, is composed of a family of Restricted Boltzmann Machines (RBMs). Commonly, RSM is learned like standard RBMs using approximate methods like Contrastive Divergence (CD). However, CD is not really designed for RSM. Different from RBMs with binary input, RSM adopts softmax units to represent words, resulting in great inefficiency with sampling inside CD, especially for a large vocabulary. Yet, NLP systems</context>
<context position="3514" citStr="Srivastava et al., 2013" startWordPosition="539" endWordPosition="542"> proposed for unnormalized models, and shows great efficiency in learning word representations such as in (Mnih and Teh, 2012; Mikolov et al., 2013b). In this paper, we propose an efficient learning strategy for RSM named α-NCE, applying NCE as the basic estimator. Different from most related efforts that use NCE for predicting single word, our method extends NCE to generate noise for documents in variant lengths. It also enables RSM to use weighted inputs to improve the modelling ability. As RSM is usually used as the first layer in many deeper undirected models like Deep Boltzmann Machines (Srivastava et al., 2013), α-NCE can be readily extended to learn them efficiently. 2 Replicated Softmax Model RSM is a typical undirected topic model, which is based on bag-of-words (BoW) to represent documents. In general, it consists of a series of RBMs, 162 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 162–167, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics each of which contains variant softmax visible units but the same binary hidden units. </context>
</contexts>
<marker>Srivastava, Salakhutdinov, Hinton, 2013</marker>
<rawString>Nitish Srivastava, Ruslan R Salakhutdinov, and Geoffrey E Hinton. 2013. Modeling documents with deep boltzmann machines. arXiv preprint arXiv:1309.6865.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tijmen Tieleman</author>
</authors>
<title>Training restricted boltzmann machines using approximations to the likelihood gradient.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th international conference on Machine learning,</booktitle>
<pages>1064--1071</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="5819" citStr="Tieleman, 2008" startWordPosition="962" endWordPosition="963">tinomial distribution of the words, and Equation (4) serves as an efficient inference from words to semantic meanings, where we adopt the probabilities of each hidden unit “activated” as the topic features. 2.1 Learning Strategies for RSM RSM is naturally learned by minimizing the negative log-likelihood function (ML) as follows: L(θ) = −EV ∼Pdata [log Pe(V )] (5) However, the gradient is intractable for the combinatorial normalization term ZD. Common strategies to overcome this intractability are MCMCbased approaches such as Contrastive Divergence (CD) (Hinton, 2002) and Persistent CD (PCD) (Tieleman, 2008), both of which require repeating Gibbs steps of h(i) ∼ Pe(h|V (i)) and V (i+1) ∼ Pe(V |h(i)) to generate model samples to approximate the gradient. Typically, the performance and consistency improve when more steps are adopted. Notwithstanding, even one Gibbs step is time consuming for RSM, since the multinomial sampling normally requires linear time computations. The “alias method” (Kronmal and Peterson Jr, 1979) speeds up multinomial sampling to constant time while linear time is required for processing the distribution. Since Pe(V |h) changes at every iteration in CD, such methods cannot b</context>
</contexts>
<marker>Tieleman, 2008</marker>
<rawString>Tijmen Tieleman. 2008. Training restricted boltzmann machines using approximations to the likelihood gradient. In Proceedings of the 25th international conference on Machine learning, pages 1064– 1071. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>