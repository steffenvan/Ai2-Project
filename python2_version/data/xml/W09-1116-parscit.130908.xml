<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000155">
<title confidence="0.993715">
Glen, Glenda or Glendale:
Unsupervised and Semi-supervised Learning of English Noun Gender
</title>
<author confidence="0.998498">
Shane Bergsma Dekang Lin Randy Goebel
</author>
<affiliation confidence="0.99751">
Department of Computing Science Google, Inc. Department of Computing Science
University of Alberta 1600 Amphitheatre Parkway University of Alberta
</affiliation>
<address confidence="0.755618">
Edmonton, Alberta Mountain View Edmonton, Alberta
Canada, T6G 2E8 California, 94301 Canada, T6G 2E8
</address>
<email confidence="0.996917">
bergsma@cs.ualberta.ca lindek@google.com goebel@cs.ualberta.ca
</email>
<sectionHeader confidence="0.994722" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99988576">
English pronouns like he and they reliably re-
flect the gender and number of the entities to
which they refer. Pronoun resolution systems
can use this fact to filter noun candidates that
do not agree with the pronoun gender. In-
deed, broad-coverage models of noun gender
have proved to be the most important source
of world knowledge in automatic pronoun res-
olution systems.
Previous approaches predict gender by count-
ing the co-occurrence of nouns with pronouns
of each gender class. While this provides use-
ful statistics for frequent nouns, many infre-
quent nouns cannot be classified using this
method. Rather than using co-occurrence in-
formation directly, we use it to automatically
annotate training examples for a large-scale
discriminative gender model. Our model col-
lectively classifies all occurrences of a noun
in a document using a wide variety of con-
textual, morphological, and categorical gender
features. By leveraging large volumes of un-
labeled data, our full semi-supervised system
reduces error by 50% over the existing state-
of-the-art in gender classification.
</bodyText>
<sectionHeader confidence="0.998853" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.994182659090909">
Pronoun resolution is the process of determining
which preceding nouns are referred to by a partic-
ular pronoun in text. Consider the sentence:
(1) Glen told Glenda that she was wrong about
Glendale.
A pronoun resolution system should determine that
the pronoun she refers to the noun Glenda. Pro-
noun resolution is challenging because it requires a
lot of world knowledge (general knowledge of word
types). If she is replaced with the pronoun he in (1),
Glen becomes the antecedent. Pronoun resolution
systems need the knowledge of noun gender that ad-
vises that Glen is usually masculine (and thus re-
ferred to by he) while Glenda is feminine.
English third-person pronouns are grouped in four
gender/number categories: masculine (he, his, him,
himself), feminine (she, her, herself), neutral (it, its,
itself), and plural (they, their, them, themselves). We
broadly refer to these gender and number classes
simply as gender. The objective of our work is to
correctly assign gender to English noun tokens, in
context; to determine which class of pronoun will
refer to a given noun.
One successful approach to this problem is to
build a statistical gender model from a noun’s asso-
ciation with pronouns in text. For example, Ge et al.
(1998) learn Ford has a 94% chance of being neu-
tral, based on its frequent co-occurrence with neu-
tral pronouns in text. Such estimates are noisy but
useful. Both Ge et al. (1998) and Bergsma and Lin
(2006) show that learned gender is the most impor-
tant feature in their pronoun resolution systems.
English differs from other languages like French
and German in that gender is not an inherent gram-
matical property of an English noun, but rather a
property of a real-world entity that is being referred
to. A common noun like lawyer can be (semanti-
cally) masculine in one document and feminine in
another. While previous statistical gender models
learn gender for noun types only, we use document
context to correctly determine the current gender
class of noun tokens, making dynamic decisions on
common nouns like lawyer and ambiguous names
like Ford. Furthermore, if a noun type has not yet
</bodyText>
<page confidence="0.947279">
120
</page>
<note confidence="0.989879">
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 120–128,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999940428571429">
been observed (an unknown word), previous ap-
proaches cannot estimate the gender. Our system,
on the other hand, is able to correctly determine that
unknown words corroborators and propeller-heads
are plural, while Pope Formosus is masculine, using
learned contextual and morphological cues.
Our approach is based on the key observation that
while gender information from noun-pronoun co-
occurrence provides imperfect noun coverage, it can
nevertheless provide rich and accurate training data
for a large-scale discriminative classifier. The clas-
sifier leverages a wide variety of noun properties to
generalize from the automatically-labeled examples.
The steps in our approach are:
</bodyText>
<listItem confidence="0.997587227272727">
1. Training:
(a) Automatically extract a set of seed
(noun,gender) pairs from high-quality in-
stances in a statistical gender database.
(b) In a large corpus of text, find documents con-
taining these nouns.
(c) For all instances of each noun in each document,
create a single, composite feature vector repre-
senting all the contexts of the noun in the docu-
ment, as well as encoding other selected proper-
ties of the noun type.
(d) Label each feature vector with the seed noun’s
corresponding gender.
(e) Train a 4-way gender classifier (masculine, fem-
inine, neutral, plural) from the automatically-
labeled vectors.
2. Testing:
(a) Given a new document, create a composite fea-
ture vector for all occurrences of each noun.
(b) Use the learned classifier to assign gender to
each feature vector, and thus all occurrences of
all nouns in the document.
</listItem>
<bodyText confidence="0.9998511">
This algorithm achieves significantly better per-
formance than the existing state-of-the-art statisti-
cal gender classifier, while requiring no manually-
labeled examples to train. Furthermore, by training
on a small number of manually-labeled examples,
we can combine the predictions of this system with
the counts from the original gender database. This
semi-supervised extension achieves 95.5% accuracy
on final unseen test data, an impressive 50% reduc-
tion in error over previous work.
</bodyText>
<sectionHeader confidence="0.833795" genericHeader="method">
2 Path-based Statistical Noun Gender
</sectionHeader>
<bodyText confidence="0.997768136363636">
Seed (noun,gender) examples can be extracted re-
liably and automatically from raw text, providing
the training data for our discriminative classifier.
We call these examples pseudo-seeds because they
are created fully automatically, unlike the small set
of manually-created seeds used to initialize other
bootstrapping approaches (cf. the bootstrapping ap-
proaches discussed in Section 6).
We adopt a statistical approach to acquire the
pseudo-seed (noun,gender) pairs. All previous sta-
tistical approaches rely on a similar observation: if
a noun like Glen is often referred to by masculine
pronouns, like he or his, then Glen is likely a mas-
culine noun. But for most nouns we have no an-
notated data recording their coreference with pro-
nouns, and thus no data from which we can ex-
tract the co-occurrence statistics. Thus previous ap-
proaches rely on either hand-crafted coreference-
indicating patterns (Bergsma, 2005), or iteratively
guess and improve gender models through expec-
tation maximization of pronoun resolution (Cherry
and Bergsma, 2005; Charniak and Elsner, 2009). In
statistical approaches, the more frequent the noun,
the more accurate the assignment of gender.
We use the approach of Bergsma and Lin (2006),
both because it achieves state-of-the-art gender
classification performance, and because a database
of the obtained noun genders is available online.1
Bergsma and Lin (2006) use an unsupervised
algorithm to identify syntactic paths along which a
noun and pronoun are highly likely to corefer. To
extract gender information, they processed a large
corpus of news text, and obtained co-occurrence
counts for nouns and pronouns connected with these
paths in the corpus. In their database, each noun is
listed with its corresponding masculine, feminine,
neutral, and plural pronoun co-occurrence counts,
e.g.:
glen 555 42 32 34
glenda 8 102 0 11
glendale 24 2 167 18
glendalians 0 0 0 1
glenn 3182 207 95 54
glenna 0 6 0 0
</bodyText>
<footnote confidence="0.999488">
1Available at http://www.cs.ualberta.ca/˜bergsma/Gender/
</footnote>
<page confidence="0.997526">
121
</page>
<bodyText confidence="0.994714393442623">
This sample of the gender data shows that the
noun glenda, for example, occurs 8 times with mas-
culine pronouns, 102 times with feminine pronouns,
0 times with neutral pronouns, and 11 times with
plural pronouns; 84% of the time glenda co-occurs
with a feminine pronoun. Note that all nouns in the
data have been converted to lower-case.2
There are gender counts for 3.1 million English
nouns in the online database. These counts form the
basis for the state-of-the-art gender classifier. We
can either take the most-frequent pronoun-gender
(MFPG) as the class (e.g. feminine for glenda), or
we can supply the logarithm of the counts as features
in a 4-way multi-class classifier. We implement the
latter approach as a comparison system and refer to
it as PATHGENDER in our experiments.
In our approach, rather than using these counts
directly, we process the database to automatically
extract a high-coverage but also high-quality set of
pseudo-seed (noun,gender) pairs. First, we filter
nouns that occur less than fifty times and whose
MFPG accounts for less than 85% of counts. Next,
we note that the most reliable nouns should occur
relatively often in a coreferent path. For exam-
ple, note that importance occurs twice as often on
the web as Clinton, but has twenty-four times less
counts in the gender database. This is because im-
portance is unlikely to be a pronoun’s antecedent.
We plan to investigate this idea further in future
work as a possible filter on antecedent candidates
for pronoun resolution. For the present work, sim-
ply note that a high ratio of database-count to web-
count provides a good indication of the reliability of
a noun’s gender counts, and thus we filter nouns that
have such ratios below a threshold.3 After this fil-
tering, we have about 45 thousand nouns to which
we automatically assign gender according to their
MFPG. These (noun,gender) pairs provide the seed
examples for the training process described in the
2Statistical approaches can adapt to the idiosyncrasies of the
particular text domain. In the news text from which this data
was generated, for example, both the word ships and specific
instances of ships (the USS Cole, the Titanic, etc.) are neutral.
In Wikipedia, on the other hand, feminine pronouns are often
used for ships. Such differences can be learned automatically.
3We roughly tuned all the thresholds to obtain the highest
number of seeds such that almost all of them looked correct
(e.g. Figure 1). Further work is needed to determine whether a
different precision/recall tradeoff can improve performance.
. . .
stefanie
steffi graf
steinem
stella mccartney
stellar jayne
stepdaughter
stephanie
stephanie herseth
stephanie white
stepmother
stewardess
</bodyText>
<figure confidence="0.568174">
. . .
</figure>
<figureCaption confidence="0.998923">
Figure 1: Sample feminine seed nouns
</figureCaption>
<bodyText confidence="0.9986155">
following section. Figure 1 provides a portion of the
ordered feminine seed nouns that we extracted.
</bodyText>
<sectionHeader confidence="0.992469" genericHeader="method">
3 Discriminative Learning of Gender
</sectionHeader>
<bodyText confidence="0.99998405">
Once we have extracted a number of pseudo-seed
(noun,gender) pairs, we use them to automatically-
label nouns (in context) in raw text. The auto-
labeled examples provide training data for discrimi-
native learning of noun gender.
Since the training pairs are acquired from a
sparse and imperfect model of gender, what can
we gain by training over them? We can regard the
Bergsma and Lin (2006) approach and our discrim-
inative system as two orthogonal views of gender,
in a co-training sense (Blum and Mitchell, 1998).
Some nouns can be accurately labeled by noun-
pronoun co-occurrence (a view based on pronoun
co-occurrence), and these examples can be used to
deduce other gender-indicating regularities (a view
based on other features, described below).
We presently explain how examples are extracted
using our pseudo-seed pairs, turned into auto-
labeled feature vectors, and then used to train a su-
pervised classifier.
</bodyText>
<subsectionHeader confidence="0.998997">
3.1 Automatic example extraction
</subsectionHeader>
<bodyText confidence="0.9998008">
Our example-extraction module processes a large
collection of documents (roughly a million docu-
ments in our experiments). For each document, we
extract all the nouns, including context words within
±5 tokens of each noun. We then group the nouns by
</bodyText>
<page confidence="0.934446">
122
</page>
<figure confidence="0.410947">
Class=masculine String=“Lee”
Contexts =
“led some to suggest that ⋆ , who was born in”
“⋆ also downloaded secret files to”
“⋆ says he was just making”
“by mishandling the investigation of ⋆ .”
. . .
</figure>
<figureCaption confidence="0.997366">
Figure 2: Sample noun training instance
</figureCaption>
<bodyText confidence="0.999798333333333">
their (lower-case) string. If a group’s noun-string is
in our set of seed (noun,gender) pairs, we assign the
corresponding gender to be the class of the group.
Otherwise, we discard the group. To prevent fre-
quent nouns from dominating our training data, we
only keep the first 200 groups corresponding to each
noun string. Figure 2 gives an example training noun
group with some (selected) context sentences. At
test time, all nouns in the test documents are con-
verted to this format for further processing.
We group nouns because there is a strong ten-
dency for nouns to have only one sense (and hence
gender) per discourse. We extract contexts because
nearby words provide good clues about which gen-
der is being used. The notion that nouns have only
one sense per discourse/collocation was also ex-
ploited by Yarowsky (1995) in his seminal work on
bootstrapping for word sense disambiguation.
</bodyText>
<subsectionHeader confidence="0.999321">
3.2 Feature vectors
</subsectionHeader>
<bodyText confidence="0.999982857142857">
Once the training instances are extracted, they are
converted to labeled feature vectors for supervised
learning. The automatically-determined gender pro-
vides the class label (e.g., masculine for the group
in Figure 2). The features identify properties of the
noun and its context that potentially correlate with a
particular gender category. We divide the features
into two sets: those that depend on the contexts
within the document (Context features: features of
the tokens in the document), and those that depend
on the noun string only (Type features). In both
cases we induce the feature space from the train-
ing examples, keeping only those features that occur
more than 5 times.
</bodyText>
<subsubsectionHeader confidence="0.565958">
3.2.1 Context features
</subsubsectionHeader>
<bodyText confidence="0.999986545454545">
The first set of features represent the contexts of
the word, using all the contexts in the noun group.
To illustrate the potential utility of the context infor-
mation, consider the context sentences for the mas-
culine noun in Figure 2. Even if these snippets were
all the information we were given, it would be easy
to guess the gender of the noun.
We use binary attribute-value features to flag, for
any of the contexts, the presence of all words at con-
text positions ±1, ±2, etc. (sometimes called col-
location features (Golding and Roth, 1999)). For
example, feature 255920 flags that the word two-to-
the-right of the noun is he. We also provide fea-
tures for the presence of all words anywhere within
±5 tokens of the noun (sometimes called context
words). We also parse the sentence and provide a
feature for the noun’s parent (and relationship with
the parent) in the parse tree. For example, the in-
stance in Figure 2 has features downloaded(subject),
says(subject), etc. Since plural nouns should be gov-
erned by plural verbs, this feature is likely to be es-
pecially helpful for number classification.
</bodyText>
<subsectionHeader confidence="0.491454">
3.2.2 Type features
</subsectionHeader>
<bodyText confidence="0.999981190476191">
The next group of features represent morpholog-
ical properties of the noun. Binary features flag the
presence of all prefixes and suffixes of one-to-four
characters. For multi-token nouns, we have features
for the first and last token in the noun. Thus we hope
to learn that Bob begins masculine nouns while inc.
ends neutral ones.
Finally, we have features that indicate if the noun
or parts of the noun occur on various lists. Indica-
tor features specify if any token occurs on in-house
lists of given names, family names, cities, provinces,
countries, corporations, languages, etc. A feature
also indicates if a token is a corporate designation
(like inc. or ltd.) or a human one (like Mr. or Sheik).
We also made use of the person-name/instance
pairs automatically extracted by Fleischman et al.
(2003).4 This data provides counts for pairs such
as (Zhang Qiyue, spokeswoman) and (Thorvald
Stoltenberg, mediator). We have features for all con-
cepts (like spokeswoman and mediator) and there-
fore learn their association with each gender.
</bodyText>
<subsectionHeader confidence="0.999193">
3.3 Supervised learning and classification
</subsectionHeader>
<bodyText confidence="0.8574225">
Once all the feature vectors have been extracted,
they are passed to a supervised machine learn-
</bodyText>
<footnote confidence="0.999268">
4Available at http://www.mit.edu/˜mbf/instances.txt.gz
</footnote>
<page confidence="0.997292">
123
</page>
<bodyText confidence="0.999320166666667">
ing algorithm. We train and classify using a
multi-class linear-kernel Support Vector Machine
(SVM) (Crammer and Singer, 2001). SVMs are
maximum-margin classifiers that achieve good per-
formance on a range of tasks. At test time, nouns in
test documents are processed exactly as the training
instances described above, converting them to fea-
ture vectors. The test vectors are classified by the
SVM, providing gender classes for all the nouns in
the test document. Since all training examples are
labeled automatically (auto-trained), we denote sys-
tems using this approach as -AUTO.
</bodyText>
<subsectionHeader confidence="0.739785">
3.4 Semi-supervised extension
</subsectionHeader>
<bodyText confidence="0.999969142857143">
Although a good gender classifier can be learned
from the automatically-labeled examples alone, we
can also use a small quantity of gold-standard la-
beled examples to achieve better performance.
Combining information from our two sets of la-
beled data is akin to a domain adaptation prob-
lem. The gold-standard data can be regarded as
high-quality in-domain data, and the automatically-
labeled examples can be regarded as the weaker, but
larger, out-of-domain evidence.
There is a simple but effective method for com-
bining information from two domains using predic-
tions as features. We train a classifier on the full set
of automatically-labeled data (as described in Sec-
tion 3.3), and then use this classifier’s predictions as
features in a separate classifier, which is trained on
the gold-standard data. This is like the competitive
Feats domain-adaptation system in Daum´e III and
Marcu (2006).
For our particular SVM classifier (Section 4.1),
predictions take the form of four numerical scores
corresponding to the four different genders. Our
gold-standard classifier has features for these four
predictions plus features for the original path-based
gender counts (Section 2).5 Since this approach uses
both automatically-labeled and gold-standard data in
a semi-supervised learning framework, we denote
systems using this approach as -SEMI.
</bodyText>
<footnote confidence="0.990579333333333">
5We actually use 12 features for the path-based counts: the
4 original, and then 4 each for counts for the first and last token
in the noun string. See PATHGENDER+ in Section 4.2.
</footnote>
<sectionHeader confidence="0.993974" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.994179">
4.1 Set-up
</subsectionHeader>
<bodyText confidence="0.999989307692308">
We parsed the 3 GB AQUAINT corpus (Vorhees,
2002) using Minipar (Lin, 1998) to create our un-
labeled data. We process this data as described in
Section 3, making feature vectors from the first 4
million noun groups. We train from these exam-
ples using a linear-kernel SVM via the the efficient
SVMmulticlass instance of the SVM struct software
package (Tsochantaridis et al., 2004).
To create our gold-standard gender data, we fol-
low Bergsma (2005) in extracting gender informa-
tion from the anaphora-annotated portion6 of the
American National Corpus (ANC) (Ide and Sud-
erman, 2004). In each document, we first group
all nouns with a common lower-case string (exactly
as done for our example extraction (Section 3.1)).
Next, for each group we determine if a third-person
pronoun refers to any noun in that group. If so, we
label all nouns in the group with the gender of the
referring pronoun. For example, if the pronoun he
refers to a noun Brown, then all instances of Brown
in the document are labeled as masculine. We ex-
tract the genders for 2794 nouns in the ANC train-
ing set (in 798 noun groups) and 2596 nouns in the
ANC test set (in 642 groups). We apply this method
to other annotated corpora (including MUC corpora)
to create a development set.
The gold standard ANC training set is used to
set the weights on the counts in the PATHGENDER
classifiers, and to train the semi-supervised ap-
proaches. We also use an SVM to learn these
weights. We use the development set to tune the
SVM’s regularization parameter, both for systems
trained on automatically-generated data, and for sys-
tems trained on gold-standard data. We also opti-
mize each automatically-trained system on the de-
velopment set when we include this system’s pre-
dictions as features in the semi-supervised exten-
sion. We evaluate and state performance for all ap-
proaches on the final unseen ANC test set.
</bodyText>
<subsectionHeader confidence="0.915259">
4.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.998542333333333">
The primary purpose of our experiments is to de-
termine if we can improve on the existing state-of-
the-art in gender classification (path-based gender
</bodyText>
<footnote confidence="0.999246">
6Available at http://www.cs.ualberta.ca/˜bergsma/CorefTags/
</footnote>
<page confidence="0.998032">
124
</page>
<bodyText confidence="0.9995115">
counts). We test systems both trained purely on
automatically-labeled data (Section 3.3), and those
that leverage some gold-standard annotations in a
semi-supervised setting (Section 3.4). Another pur-
pose of our experiments is to investigate the relative
value of our context-based features and type-based
features. We accomplish these objectives by imple-
menting and evaluating the following systems:
</bodyText>
<listItem confidence="0.919302">
1. PATHGENDER:
</listItem>
<bodyText confidence="0.9127105">
A classifier with the four path-based gender
counts as features (Section 2).
</bodyText>
<sectionHeader confidence="0.5571" genericHeader="method">
2. PATHGENDER+:
</sectionHeader>
<bodyText confidence="0.997499833333333">
A method of back-off to help classify unseen
nouns: For multi-token nouns (like Bob John-
son), we also include the four gender counts
aggregated over all nouns sharing the first to-
ken (Bob .*), and the four gender counts over
all nouns sharing the last token (.* Johnson).
</bodyText>
<listItem confidence="0.8829035">
3. CONTEXT-AUTO:
Auto-trained system using only context fea-
tures (Section 3.2.1).
4. TYPE-AUTO:
Auto-trained system using only type features
(Section 3.2.2).
5. FULL-AUTO:
Auto-trained system using all features.
6. CONTEXT-SEMI:
Semi-sup. combination of the PATHGENDER+
features and the CONTEXT-AUTO predictions.
7. TYPE-SEMI:
</listItem>
<bodyText confidence="0.934591333333334">
Semi-sup. combination of the PATHGENDER+
features and the TYPE-AUTO predictions.
8. FULL-SEMI:
Semi-sup. combination of the PATHGENDER+
features and the FULL-AUTO predictions.
We evaluate using accuracy: the percentage of
labeled nouns that are correctly assigned a gender
class. As a baseline, note that always choosing
neutral achieves 38.1% accuracy on our test data.
</bodyText>
<sectionHeader confidence="0.999581" genericHeader="evaluation">
5 Results and Discussion
</sectionHeader>
<subsectionHeader confidence="0.987839">
5.1 Main results
</subsectionHeader>
<bodyText confidence="0.998568">
Table 1 provides our experimental results. The orig-
inal gender counts already do an excellent job clas-
sifying the nouns; PATHGENDER achieves 91.0%
accuracy by looking for exact noun matches. Our
</bodyText>
<table confidence="0.98600675">
1. PATHGENDER 91.0
2. PATHGENDER+ 92.1
3. CONTEXT-AUTO 79.1
4. TYPE-AUTO 89.1
5. FULL-AUTO 92.6
6. CONTEXT-SEMI 92.4
7. TYPE-SEMI 91.3
8. FULL-SEMI 95.5
</table>
<tableCaption confidence="0.999895">
Table 1: Noun gender classification accuracy (%)
</tableCaption>
<bodyText confidence="0.999795444444444">
simple method of using back-off counts for the first
and last token, PATHGENDER+, achieves 92.1%.
While PATHGENDER+ uses gold standard data to
determine optimum weights on the twelve counts,
FULL-AUTO achieves 92.6% accuracy using no
gold standard training data. This confirms that our
algorithm, using no manually-labeled training data,
can produce a competitive gender classifier.
Both PATHGENDER and PATHGENDER+ do
poorly on the noun types that have low counts in
the gender database, achieving only 63% and 66%
on nouns with less than ten counts. On these
same nouns, FULL-AUTO achieves 88% perfor-
mance, demonstrating the robustness of the learned
classifier on the most difficult examples for previ-
ous approaches (FULL-SEMI achieves 94% on these
nouns).
If we break down the contribution of the two fea-
ture types in FULL-AUTO, we find that we achieve
89.1% accuracy by only using type features, while
we achieve 79.1% with only context features. While
not as high as the type-based accuracy, it is impres-
sive that almost four out of five nouns can be classi-
fied correctly based purely on the document context,
using no information about the noun itself. This is
information that has not previously been systemati-
cally exploited in gender classification models.
We examine the relationship between training
data size and accuracy by plotting a (logarithmic-
scale) learning curve for FULL-AUTO (Figure 3).
Although using four million noun groups originally
seemed sufficient, performance appears to still be in-
creasing. Since more training data can be generated
automatically, it appears we have not yet reached the
full power of the FULL-AUTO system. Of course,
even with orders of magnitude more data, the system
</bodyText>
<page confidence="0.992178">
125
</page>
<figure confidence="0.996904222222222">
100
95
90
85
80
75
70
1000 10000 100000 1e+06 1e+07
Number of training examples
</figure>
<figureCaption confidence="0.996183">
Figure 3: Noun gender classification learning curve for
FULL-AUTO
</figureCaption>
<bodyText confidence="0.999121741935484">
does not appear destined to reach the performance
obtained through other means described below.
We achieve even higher accuracy when the output
of the -AUTO systems are combined with the orig-
inal gender counts (the semi-supervised extension).
The relative value of the context and type-based fea-
tures is now reversed: using only context-based fea-
tures (CONTEXT-SEMI) achieves 92.4%, while us-
ing only type-based features (TYPE-SEMI) achieves
91.3%. This is because much of the type informa-
tion is already implicit in the PATHGENDER counts.
The TYPE-AUTO predictions contribute little infor-
mation, only fragmenting the data and leading to
over-training and lower accuracy. On the other hand,
the CONTEXT-AUTO predictions improve accuracy,
as these scores provide orthogonal and hence helpful
information for the semi-supervised classifier.
Combining FULL-AUTO with our enhanced path
gender counts, PATHGENDER+, results in the over-
all best performance, 95.5% for FULL-SEMI, signif-
icantly better than PATHGENDER+ alone.7 This is
a 50% error reduction over the PATHGENDER sys-
tem, strongly confirming the benefit of our semi-
supervised approach.
To illustrate the importance of the unlabeled data,
we created a system that uses all features, including
the PATHGENDER+ counts, and trained this system
using only the gold standard training data. This sys-
tem was unable to leverage the extra features to im-
prove performance; its accuracy was 92.0%, roughly
equal to PATHGENDER+ alone. While SVMs work
</bodyText>
<footnote confidence="0.998226333333333">
7We evaluate significance using McNemar’s test, p&lt;0.01.
Since McNemar’s test assumes independent classifications, we
apply the test to the classification of noun groups, not instances.
</footnote>
<bodyText confidence="0.917241">
well with high-dimensional data, they simply cannot
exploit features that do not occur in the training set.
</bodyText>
<subsectionHeader confidence="0.996055">
5.2 Further improvements
</subsectionHeader>
<bodyText confidence="0.999985945945946">
We can improve performance further by doing some
simple coreference before assigning gender. Cur-
rently, we only group nouns with the same string,
and then decide gender collectively for the group.
There are a few cases, however, where an ambiguous
surname, such as Willey, can only be classified cor-
rectly if we link the surname to an earlier instance of
the full name, e.g. Katherine Willey. We thus added
the following simple post-processing rule: If a noun
is classified as masculine or feminine (like the am-
biguous Willey), and it was observed earlier as the
last part of a larger noun, then re-assign the gender
to masculine or feminine if one of these is the most
common path-gender count for the larger noun. We
back off to counts for the first name (e.g. Kathleen
.*) if the full name is unobserved.
This enhancement improved the PATHGENDER
and PATHGENDER+ systems to 93.3% and 94.3%,
respectively, while raising the accuracy of our
FULL-SEMI system to 96.7%. This demonstrates
that the surname-matching post-processor is a sim-
ple but worthwhile extension to a gender predictor.8
The remaining errors represent a number of chal-
lenging cases: United States, group, and public la-
beled as plural but classified as neutral; spectator
classified as neutral, etc. Some of these may yield
to more sophisticated joint classification of corefer-
ence and gender, perhaps along the lines of work in
named-entity classification (Bunescu and Mooney,
2004) or anaphoricity (Denis and Baldridge, 2007).
While gender has been shown to be the key fea-
ture for statistical pronoun resolution (Ge et al.,
1998; Bergsma and Lin, 2006), it remains to be
seen whether the exceptional accuracy obtained here
will translate into improvements in resolution per-
formance. However, given the clear utility of gender
in coreference, substantial error reductions in gender
</bodyText>
<footnote confidence="0.982759571428572">
8One might wonder, why not provide special features so that
the system can learn how to handle ambiguous nouns that oc-
curred as sub-phrases in earlier names? The nature of our train-
ing data precludes this approach. We only include unambiguous
examples as pseudo-seeds in the learning process. Without
providing ambiguous (but labeled) surnames in some way, the
learner will not take advantage of features to help classify them.
</footnote>
<note confidence="0.247992">
Accuracy (%)
</note>
<page confidence="0.98678">
126
</page>
<bodyText confidence="0.883629">
assignment will likely be a helpful contribution.
</bodyText>
<sectionHeader confidence="0.999688" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999981014084507">
Most coreference and pronoun resolution papers
mention that they use gender information, but few
explain how it is acquired. Kennedy and Boguraev
(1996) use gender information produced by their en-
hanced part-of-speech tagger. Gender mistakes ac-
count for 35% of their system’s errors. Gender is
less crucial in some genres, like computer manuals;
most nouns are either neutral or plural and gender
can be determined accurately based solely on mor-
phological information (Lappin and Leass, 1994).
A number of researchers (Evans and Or˘asan,
2000; Soon et al., 2001; Harabagiu et al., 2001) use
WordNet classes to infer gender knowledge. Unfor-
tunately, manually-constructed databases like Word-
Net suffer from both low coverage and rare senses.
Pantel and Ravichandran (2004) note that the nouns
computer and company both have a WordNet sense
that is a hyponym of person, falsely indicating these
nouns would be compatible with pronouns like he
or she. In addition to using WordNet classes, Soon
et al. (2001) assign gender if the noun has a gen-
dered designator (like Mr. or Mrs.) or if the first
token is present on a list of common human first
names. Note that we incorporate such contextual
and categorical information (among many other in-
formation sources) automatically in our discrimina-
tive classifier, while they manually specify a few
high-precision rules for particular gender cues.
Ge et al. (1998) pioneered the statistical approach
to gender determination. Like others, they consider
gender and number separately, only learning statis-
tical gender for the masculine, feminine, and neu-
tral classes. While gender and number can be han-
dled together for pronoun resolution, it might be use-
ful to learn them separately for other applications.
Other statistical approaches to English noun gender
are discussed in Section 2.
In languages with ‘grammatical’ gender and plen-
tiful gold standard data, gender can be tagged along
with other word properties using standard super-
vised tagging techniques (Hajiˇc and Hladk´a, 1997).
While our approach is the first to exploit a dual
or orthogonal representation of English noun gen-
der, a bootstrapping approach has been applied to
determining grammatical gender in other languages
by Cucerzan and Yarowsky (2003). In their work,
the two orthogonal views are: 1) the context of the
noun, and 2) the noun’s morphological properties.
Bootstrapping with these views is possible in other
languages where context is highly predictive of gen-
der class, since contextual words like adjectives and
determiners inflect to agree with the grammatical
noun gender. We initially attempted a similar system
for English noun gender but found context alone to
be insufficiently predictive.
Bootstrapping is also used in general information
extraction. Brin (1998) shows how to alternate be-
tween extracting instances of a class and inducing
new instance-extracting patterns. Collins and Singer
(1999) and Cucerzan and Yarowsky (1999) apply
bootstrapping to the related task of named-entity
recognition. Our approach was directly influenced
by the hypernym-extractor of Snow et al. (2005) and
we provided an analogous summary in Section 1.
While their approach uses WordNet to label hyper-
nyms in raw text, our initial labels are generated au-
tomatically. Etzioni et al. (2005) also require no la-
beled data or hand-labeled seeds for their named-
entity extractor, but by comparison their classifier
only uses a very small number of both features and
automatically-generated training examples.
</bodyText>
<sectionHeader confidence="0.998452" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999923">
We have shown how noun-pronoun co-occurrence
counts can be used to automatically annotate the
gender of millions of nouns in unlabeled text. Train-
ing from these examples produced a classifier that
clearly exceeds the state-of-the-art in gender classi-
fication. We incorporated thousands of useful but
previously unexplored indicators of noun gender as
features in our classifier. By combining the pre-
dictions of this classifier with the original gender
counts, we were able to produce a gender predic-
tor that achieves 95.5% classification accuracy on
2596 test nouns, a 50% reduction in error over the
current state-of-the-art. A further name-matching
post-processor reduced error even further, resulting
in 96.7% accuracy on the test data. Our final system
is the broadest and most accurate gender model yet
created, and should be of value to many pronoun and
coreference resolution systems.
</bodyText>
<page confidence="0.996414">
127
</page>
<sectionHeader confidence="0.993864" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999914153061225">
Shane Bergsma and Dekang Lin. 2006. Bootstrap-
ping path-based pronoun resolution. In COLING-
ACL, pages 33–40.
Shane Bergsma. 2005. Automatic acquisition of gen-
der information for anaphora resolution. In Canadian
Conference on Artificial Intelligence, pages 342–353.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In COLT,
pages 92–100.
Sergey Brin. 1998. Extracting patterns and relations
from the world wide web. In WebDB Workshop at
6th International Conference on Extending Database
Technology, pages 172–183.
Razvan Bunescu and Raymond J. Mooney. 2004. Col-
lective information extraction with relational Markov
networks. In ACL, pages 438–445.
Eugene Charniak and Micha Elsner. 2009. EM works for
pronoun anaphora resolution. In EACL.
Colin Cherry and Shane Bergsma. 2005. An expecta-
tion maximization approach to pronoun resolution. In
CoNLL, pages 88–95.
Michael Collins and Yoram Singer. 1999. Unsupervised
models for named entity classification. In EMNLP-
VLC, pages 100–110.
Koby Crammer and Yoram Singer. 2001. On the al-
gorithmic implementation of multiclass kernel-based
vector machines. Journal of Machine Learning Re-
search, 2:265–292.
Silviu Cucerzan and David Yarowsky. 1999. Language
independent named entity recognition combining mor-
phological and contextual evidence. In EMNLP-VLC,
pages 90–99.
Silviu Cucerzan and David Yarowsky. 2003. Mini-
mally supervised induction of grammatical gender. In
NAACL, pages 40–47.
Hal Daum´e III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. Journal of Artificial
Intelligence Research, 26:101–126.
Pascal Denis and Jason Baldridge. 2007. Joint determi-
nation of anaphoricity and coreference using integer
programming. In NAACL-HLT, pages 236–243.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Unsu-
pervised named-entity extraction from the web: an ex-
perimental study. Artif. Intell., 165(1):91–134.
Richard Evans and Constantin Or˘asan. 2000. Improving
anaphora resolution by identifying animate entities in
texts. In DAARC, pages 154–162.
Michael Fleischman, Eduard Hovy, and Abdessamad
Echihabi. 2003. Offline strategies for online question
answering: answering questions before they are asked.
In ACL, pages 1–7.
Niyu Ge, John Hale, and Eugene Charniak. 1998. A sta-
tistical approach to anaphora resolution. In Proceed-
ings of the Sixth Workshop on Very Large Corpora,
pages 161–171.
Andrew R. Golding and Dan Roth. 1999. A Winnow-
based approach to context-sensitive spelling correc-
tion. Machine Learning, 34(1-3):107–130.
Jan Hajiˇc and Barbora Hladk´a. 1997. Probabilistic and
rule-based tagger of an inflective language: a compar-
ison. In ANLP, pages 111–118.
Sanda Harabagiu, Razvan Bunescu, and Steven Maio-
rano. 2001. Text and knowledge mining for coref-
erence resolution. In NAACL, pages 55–62.
Nancy Ide and Keith Suderman. 2004. The American
National Corpus first release. In LREC, pages 1681–
84.
Christopher Kennedy and Branimir Boguraev. 1996.
Anaphora for everyone: Pronominal anaphora resolu-
tion without a parser. In COLING, pages 113–118.
Shalom Lappin and Herbert J. Leass. 1994. An algo-
rithm for pronominal anaphora resolution. Computa-
tional Linguistics, 20(4):535–561.
Dekang Lin. 1998. Dependency-based evaluation of
MINIPAR. In LREC Workshop on the Evaluation of
Parsing Systems.
Patrick Pantel and Deepak Ravichandran. 2004. Auto-
matically labeling semantic classes. In HLT-NAACL,
pages 321–328.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005.
Learning syntactic patterns for automatic hypernym
discovery. In NIPS, pages 1297–1304.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to corefer-
ence resolution of noun phrases. Computational Lin-
guistics, 27(4):521–544.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemin Altun. 2004. Support vec-
tor machine learning for interdependent and structured
output spaces. In ICML.
Ellen Vorhees. 2002. Overview of the TREC 2002 ques-
tion answering track. In Proceedings of the Eleventh
Text REtrieval Conference (TREC).
David Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In ACL, pages
189–196.
</reference>
<page confidence="0.996743">
128
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.755250">
<title confidence="0.986627">Glen, Glenda or Glendale: Unsupervised and Semi-supervised Learning of English Noun Gender</title>
<author confidence="0.999992">Shane Bergsma Dekang Lin Randy Goebel</author>
<affiliation confidence="0.999757">Department of Computing Science Google, Inc. Department of Computing Science University of Alberta 1600 Amphitheatre Parkway University of Alberta</affiliation>
<address confidence="0.944244">Edmonton, Alberta Mountain View Edmonton, Alberta Canada, T6G 2E8 California, 94301 Canada, T6G</address>
<email confidence="0.973772">bergsma@cs.ualberta.calindek@google.comgoebel@cs.ualberta.ca</email>
<abstract confidence="0.995889653846154">pronouns like reflect the gender and number of the entities to which they refer. Pronoun resolution systems can use this fact to filter noun candidates that do not agree with the pronoun gender. Indeed, broad-coverage models of noun gender have proved to be the most important source of world knowledge in automatic pronoun resolution systems. Previous approaches predict gender by counting the co-occurrence of nouns with pronouns of each gender class. While this provides useful statistics for frequent nouns, many infrequent nouns cannot be classified using this method. Rather than using co-occurrence information directly, we use it to automatically annotate training examples for a large-scale discriminative gender model. Our model collectively classifies all occurrences of a noun in a document using a wide variety of contextual, morphological, and categorical gender features. By leveraging large volumes of unlabeled data, our full semi-supervised system reduces error by 50% over the existing stateof-the-art in gender classification.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Dekang Lin</author>
</authors>
<title>Bootstrapping path-based pronoun resolution.</title>
<date>2006</date>
<booktitle>In COLINGACL,</booktitle>
<pages>33--40</pages>
<contexts>
<context position="2975" citStr="Bergsma and Lin (2006)" startWordPosition="463" endWordPosition="466">plural (they, their, them, themselves). We broadly refer to these gender and number classes simply as gender. The objective of our work is to correctly assign gender to English noun tokens, in context; to determine which class of pronoun will refer to a given noun. One successful approach to this problem is to build a statistical gender model from a noun’s association with pronouns in text. For example, Ge et al. (1998) learn Ford has a 94% chance of being neutral, based on its frequent co-occurrence with neutral pronouns in text. Such estimates are noisy but useful. Both Ge et al. (1998) and Bergsma and Lin (2006) show that learned gender is the most important feature in their pronoun resolution systems. English differs from other languages like French and German in that gender is not an inherent grammatical property of an English noun, but rather a property of a real-world entity that is being referred to. A common noun like lawyer can be (semantically) masculine in one document and feminine in another. While previous statistical gender models learn gender for noun types only, we use document context to correctly determine the current gender class of noun tokens, making dynamic decisions on common nou</context>
<context position="7115" citStr="Bergsma and Lin (2006)" startWordPosition="1101" endWordPosition="1104">uns, like he or his, then Glen is likely a masculine noun. But for most nouns we have no annotated data recording their coreference with pronouns, and thus no data from which we can extract the co-occurrence statistics. Thus previous approaches rely on either hand-crafted coreferenceindicating patterns (Bergsma, 2005), or iteratively guess and improve gender models through expectation maximization of pronoun resolution (Cherry and Bergsma, 2005; Charniak and Elsner, 2009). In statistical approaches, the more frequent the noun, the more accurate the assignment of gender. We use the approach of Bergsma and Lin (2006), both because it achieves state-of-the-art gender classification performance, and because a database of the obtained noun genders is available online.1 Bergsma and Lin (2006) use an unsupervised algorithm to identify syntactic paths along which a noun and pronoun are highly likely to corefer. To extract gender information, they processed a large corpus of news text, and obtained co-occurrence counts for nouns and pronouns connected with these paths in the corpus. In their database, each noun is listed with its corresponding masculine, feminine, neutral, and plural pronoun co-occurrence counts</context>
<context position="11175" citStr="Bergsma and Lin (2006)" startWordPosition="1760" endWordPosition="1763">ephanie herseth stephanie white stepmother stewardess . . . Figure 1: Sample feminine seed nouns following section. Figure 1 provides a portion of the ordered feminine seed nouns that we extracted. 3 Discriminative Learning of Gender Once we have extracted a number of pseudo-seed (noun,gender) pairs, we use them to automaticallylabel nouns (in context) in raw text. The autolabeled examples provide training data for discriminative learning of noun gender. Since the training pairs are acquired from a sparse and imperfect model of gender, what can we gain by training over them? We can regard the Bergsma and Lin (2006) approach and our discriminative system as two orthogonal views of gender, in a co-training sense (Blum and Mitchell, 1998). Some nouns can be accurately labeled by nounpronoun co-occurrence (a view based on pronoun co-occurrence), and these examples can be used to deduce other gender-indicating regularities (a view based on other features, described below). We presently explain how examples are extracted using our pseudo-seed pairs, turned into autolabeled feature vectors, and then used to train a supervised classifier. 3.1 Automatic example extraction Our example-extraction module processes </context>
<context position="27681" citStr="Bergsma and Lin, 2006" startWordPosition="4382" endWordPosition="4385">name-matching post-processor is a simple but worthwhile extension to a gender predictor.8 The remaining errors represent a number of challenging cases: United States, group, and public labeled as plural but classified as neutral; spectator classified as neutral, etc. Some of these may yield to more sophisticated joint classification of coreference and gender, perhaps along the lines of work in named-entity classification (Bunescu and Mooney, 2004) or anaphoricity (Denis and Baldridge, 2007). While gender has been shown to be the key feature for statistical pronoun resolution (Ge et al., 1998; Bergsma and Lin, 2006), it remains to be seen whether the exceptional accuracy obtained here will translate into improvements in resolution performance. However, given the clear utility of gender in coreference, substantial error reductions in gender 8One might wonder, why not provide special features so that the system can learn how to handle ambiguous nouns that occurred as sub-phrases in earlier names? The nature of our training data precludes this approach. We only include unambiguous examples as pseudo-seeds in the learning process. Without providing ambiguous (but labeled) surnames in some way, the learner wi</context>
</contexts>
<marker>Bergsma, Lin, 2006</marker>
<rawString>Shane Bergsma and Dekang Lin. 2006. Bootstrapping path-based pronoun resolution. In COLINGACL, pages 33–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
</authors>
<title>Automatic acquisition of gender information for anaphora resolution.</title>
<date>2005</date>
<booktitle>In Canadian Conference on Artificial Intelligence,</booktitle>
<pages>342--353</pages>
<contexts>
<context position="6812" citStr="Bergsma, 2005" startWordPosition="1057" endWordPosition="1058">ther bootstrapping approaches (cf. the bootstrapping approaches discussed in Section 6). We adopt a statistical approach to acquire the pseudo-seed (noun,gender) pairs. All previous statistical approaches rely on a similar observation: if a noun like Glen is often referred to by masculine pronouns, like he or his, then Glen is likely a masculine noun. But for most nouns we have no annotated data recording their coreference with pronouns, and thus no data from which we can extract the co-occurrence statistics. Thus previous approaches rely on either hand-crafted coreferenceindicating patterns (Bergsma, 2005), or iteratively guess and improve gender models through expectation maximization of pronoun resolution (Cherry and Bergsma, 2005; Charniak and Elsner, 2009). In statistical approaches, the more frequent the noun, the more accurate the assignment of gender. We use the approach of Bergsma and Lin (2006), both because it achieves state-of-the-art gender classification performance, and because a database of the obtained noun genders is available online.1 Bergsma and Lin (2006) use an unsupervised algorithm to identify syntactic paths along which a noun and pronoun are highly likely to corefer. To</context>
<context position="18813" citStr="Bergsma (2005)" startWordPosition="2990" endWordPosition="2991">path-based counts: the 4 original, and then 4 each for counts for the first and last token in the noun string. See PATHGENDER+ in Section 4.2. 4 Experiments 4.1 Set-up We parsed the 3 GB AQUAINT corpus (Vorhees, 2002) using Minipar (Lin, 1998) to create our unlabeled data. We process this data as described in Section 3, making feature vectors from the first 4 million noun groups. We train from these examples using a linear-kernel SVM via the the efficient SVMmulticlass instance of the SVM struct software package (Tsochantaridis et al., 2004). To create our gold-standard gender data, we follow Bergsma (2005) in extracting gender information from the anaphora-annotated portion6 of the American National Corpus (ANC) (Ide and Suderman, 2004). In each document, we first group all nouns with a common lower-case string (exactly as done for our example extraction (Section 3.1)). Next, for each group we determine if a third-person pronoun refers to any noun in that group. If so, we label all nouns in the group with the gender of the referring pronoun. For example, if the pronoun he refers to a noun Brown, then all instances of Brown in the document are labeled as masculine. We extract the genders for 279</context>
</contexts>
<marker>Bergsma, 2005</marker>
<rawString>Shane Bergsma. 2005. Automatic acquisition of gender information for anaphora resolution. In Canadian Conference on Artificial Intelligence, pages 342–353.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Tom Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training.</title>
<date>1998</date>
<booktitle>In COLT,</booktitle>
<pages>92--100</pages>
<contexts>
<context position="11298" citStr="Blum and Mitchell, 1998" startWordPosition="1780" endWordPosition="1783">e 1 provides a portion of the ordered feminine seed nouns that we extracted. 3 Discriminative Learning of Gender Once we have extracted a number of pseudo-seed (noun,gender) pairs, we use them to automaticallylabel nouns (in context) in raw text. The autolabeled examples provide training data for discriminative learning of noun gender. Since the training pairs are acquired from a sparse and imperfect model of gender, what can we gain by training over them? We can regard the Bergsma and Lin (2006) approach and our discriminative system as two orthogonal views of gender, in a co-training sense (Blum and Mitchell, 1998). Some nouns can be accurately labeled by nounpronoun co-occurrence (a view based on pronoun co-occurrence), and these examples can be used to deduce other gender-indicating regularities (a view based on other features, described below). We presently explain how examples are extracted using our pseudo-seed pairs, turned into autolabeled feature vectors, and then used to train a supervised classifier. 3.1 Automatic example extraction Our example-extraction module processes a large collection of documents (roughly a million documents in our experiments). For each document, we extract all the nou</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>Avrim Blum and Tom Mitchell. 1998. Combining labeled and unlabeled data with co-training. In COLT, pages 92–100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergey Brin</author>
</authors>
<title>Extracting patterns and relations from the world wide web.</title>
<date>1998</date>
<booktitle>In WebDB Workshop at 6th International Conference on Extending Database Technology,</booktitle>
<pages>172--183</pages>
<contexts>
<context position="31206" citStr="Brin (1998)" startWordPosition="4935" endWordPosition="4936">ermining grammatical gender in other languages by Cucerzan and Yarowsky (2003). In their work, the two orthogonal views are: 1) the context of the noun, and 2) the noun’s morphological properties. Bootstrapping with these views is possible in other languages where context is highly predictive of gender class, since contextual words like adjectives and determiners inflect to agree with the grammatical noun gender. We initially attempted a similar system for English noun gender but found context alone to be insufficiently predictive. Bootstrapping is also used in general information extraction. Brin (1998) shows how to alternate between extracting instances of a class and inducing new instance-extracting patterns. Collins and Singer (1999) and Cucerzan and Yarowsky (1999) apply bootstrapping to the related task of named-entity recognition. Our approach was directly influenced by the hypernym-extractor of Snow et al. (2005) and we provided an analogous summary in Section 1. While their approach uses WordNet to label hypernyms in raw text, our initial labels are generated automatically. Etzioni et al. (2005) also require no labeled data or hand-labeled seeds for their namedentity extractor, but b</context>
</contexts>
<marker>Brin, 1998</marker>
<rawString>Sergey Brin. 1998. Extracting patterns and relations from the world wide web. In WebDB Workshop at 6th International Conference on Extending Database Technology, pages 172–183.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan Bunescu</author>
<author>Raymond J Mooney</author>
</authors>
<title>Collective information extraction with relational Markov networks.</title>
<date>2004</date>
<booktitle>In ACL,</booktitle>
<pages>438--445</pages>
<contexts>
<context position="27510" citStr="Bunescu and Mooney, 2004" startWordPosition="4353" endWordPosition="4356"> improved the PATHGENDER and PATHGENDER+ systems to 93.3% and 94.3%, respectively, while raising the accuracy of our FULL-SEMI system to 96.7%. This demonstrates that the surname-matching post-processor is a simple but worthwhile extension to a gender predictor.8 The remaining errors represent a number of challenging cases: United States, group, and public labeled as plural but classified as neutral; spectator classified as neutral, etc. Some of these may yield to more sophisticated joint classification of coreference and gender, perhaps along the lines of work in named-entity classification (Bunescu and Mooney, 2004) or anaphoricity (Denis and Baldridge, 2007). While gender has been shown to be the key feature for statistical pronoun resolution (Ge et al., 1998; Bergsma and Lin, 2006), it remains to be seen whether the exceptional accuracy obtained here will translate into improvements in resolution performance. However, given the clear utility of gender in coreference, substantial error reductions in gender 8One might wonder, why not provide special features so that the system can learn how to handle ambiguous nouns that occurred as sub-phrases in earlier names? The nature of our training data precludes </context>
</contexts>
<marker>Bunescu, Mooney, 2004</marker>
<rawString>Razvan Bunescu and Raymond J. Mooney. 2004. Collective information extraction with relational Markov networks. In ACL, pages 438–445.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Micha Elsner</author>
</authors>
<title>EM works for pronoun anaphora resolution.</title>
<date>2009</date>
<booktitle>In EACL.</booktitle>
<contexts>
<context position="6969" citStr="Charniak and Elsner, 2009" startWordPosition="1077" endWordPosition="1080">ed (noun,gender) pairs. All previous statistical approaches rely on a similar observation: if a noun like Glen is often referred to by masculine pronouns, like he or his, then Glen is likely a masculine noun. But for most nouns we have no annotated data recording their coreference with pronouns, and thus no data from which we can extract the co-occurrence statistics. Thus previous approaches rely on either hand-crafted coreferenceindicating patterns (Bergsma, 2005), or iteratively guess and improve gender models through expectation maximization of pronoun resolution (Cherry and Bergsma, 2005; Charniak and Elsner, 2009). In statistical approaches, the more frequent the noun, the more accurate the assignment of gender. We use the approach of Bergsma and Lin (2006), both because it achieves state-of-the-art gender classification performance, and because a database of the obtained noun genders is available online.1 Bergsma and Lin (2006) use an unsupervised algorithm to identify syntactic paths along which a noun and pronoun are highly likely to corefer. To extract gender information, they processed a large corpus of news text, and obtained co-occurrence counts for nouns and pronouns connected with these paths </context>
</contexts>
<marker>Charniak, Elsner, 2009</marker>
<rawString>Eugene Charniak and Micha Elsner. 2009. EM works for pronoun anaphora resolution. In EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>Shane Bergsma</author>
</authors>
<title>An expectation maximization approach to pronoun resolution. In CoNLL,</title>
<date>2005</date>
<pages>88--95</pages>
<contexts>
<context position="6941" citStr="Cherry and Bergsma, 2005" startWordPosition="1073" endWordPosition="1076">h to acquire the pseudo-seed (noun,gender) pairs. All previous statistical approaches rely on a similar observation: if a noun like Glen is often referred to by masculine pronouns, like he or his, then Glen is likely a masculine noun. But for most nouns we have no annotated data recording their coreference with pronouns, and thus no data from which we can extract the co-occurrence statistics. Thus previous approaches rely on either hand-crafted coreferenceindicating patterns (Bergsma, 2005), or iteratively guess and improve gender models through expectation maximization of pronoun resolution (Cherry and Bergsma, 2005; Charniak and Elsner, 2009). In statistical approaches, the more frequent the noun, the more accurate the assignment of gender. We use the approach of Bergsma and Lin (2006), both because it achieves state-of-the-art gender classification performance, and because a database of the obtained noun genders is available online.1 Bergsma and Lin (2006) use an unsupervised algorithm to identify syntactic paths along which a noun and pronoun are highly likely to corefer. To extract gender information, they processed a large corpus of news text, and obtained co-occurrence counts for nouns and pronouns</context>
</contexts>
<marker>Cherry, Bergsma, 2005</marker>
<rawString>Colin Cherry and Shane Bergsma. 2005. An expectation maximization approach to pronoun resolution. In CoNLL, pages 88–95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Yoram Singer</author>
</authors>
<title>Unsupervised models for named entity classification.</title>
<date>1999</date>
<booktitle>In EMNLPVLC,</booktitle>
<pages>100--110</pages>
<contexts>
<context position="31342" citStr="Collins and Singer (1999)" startWordPosition="4953" endWordPosition="4956">e: 1) the context of the noun, and 2) the noun’s morphological properties. Bootstrapping with these views is possible in other languages where context is highly predictive of gender class, since contextual words like adjectives and determiners inflect to agree with the grammatical noun gender. We initially attempted a similar system for English noun gender but found context alone to be insufficiently predictive. Bootstrapping is also used in general information extraction. Brin (1998) shows how to alternate between extracting instances of a class and inducing new instance-extracting patterns. Collins and Singer (1999) and Cucerzan and Yarowsky (1999) apply bootstrapping to the related task of named-entity recognition. Our approach was directly influenced by the hypernym-extractor of Snow et al. (2005) and we provided an analogous summary in Section 1. While their approach uses WordNet to label hypernyms in raw text, our initial labels are generated automatically. Etzioni et al. (2005) also require no labeled data or hand-labeled seeds for their namedentity extractor, but by comparison their classifier only uses a very small number of both features and automatically-generated training examples. 7 Conclusion</context>
</contexts>
<marker>Collins, Singer, 1999</marker>
<rawString>Michael Collins and Yoram Singer. 1999. Unsupervised models for named entity classification. In EMNLPVLC, pages 100–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Yoram Singer</author>
</authors>
<title>On the algorithmic implementation of multiclass kernel-based vector machines.</title>
<date>2001</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>2--265</pages>
<contexts>
<context position="16334" citStr="Crammer and Singer, 2001" startWordPosition="2598" endWordPosition="2601">e/instance pairs automatically extracted by Fleischman et al. (2003).4 This data provides counts for pairs such as (Zhang Qiyue, spokeswoman) and (Thorvald Stoltenberg, mediator). We have features for all concepts (like spokeswoman and mediator) and therefore learn their association with each gender. 3.3 Supervised learning and classification Once all the feature vectors have been extracted, they are passed to a supervised machine learn4Available at http://www.mit.edu/˜mbf/instances.txt.gz 123 ing algorithm. We train and classify using a multi-class linear-kernel Support Vector Machine (SVM) (Crammer and Singer, 2001). SVMs are maximum-margin classifiers that achieve good performance on a range of tasks. At test time, nouns in test documents are processed exactly as the training instances described above, converting them to feature vectors. The test vectors are classified by the SVM, providing gender classes for all the nouns in the test document. Since all training examples are labeled automatically (auto-trained), we denote systems using this approach as -AUTO. 3.4 Semi-supervised extension Although a good gender classifier can be learned from the automatically-labeled examples alone, we can also use a s</context>
</contexts>
<marker>Crammer, Singer, 2001</marker>
<rawString>Koby Crammer and Yoram Singer. 2001. On the algorithmic implementation of multiclass kernel-based vector machines. Journal of Machine Learning Research, 2:265–292.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silviu Cucerzan</author>
<author>David Yarowsky</author>
</authors>
<title>Language independent named entity recognition combining morphological and contextual evidence.</title>
<date>1999</date>
<booktitle>In EMNLP-VLC,</booktitle>
<pages>90--99</pages>
<contexts>
<context position="31375" citStr="Cucerzan and Yarowsky (1999)" startWordPosition="4958" endWordPosition="4961"> and 2) the noun’s morphological properties. Bootstrapping with these views is possible in other languages where context is highly predictive of gender class, since contextual words like adjectives and determiners inflect to agree with the grammatical noun gender. We initially attempted a similar system for English noun gender but found context alone to be insufficiently predictive. Bootstrapping is also used in general information extraction. Brin (1998) shows how to alternate between extracting instances of a class and inducing new instance-extracting patterns. Collins and Singer (1999) and Cucerzan and Yarowsky (1999) apply bootstrapping to the related task of named-entity recognition. Our approach was directly influenced by the hypernym-extractor of Snow et al. (2005) and we provided an analogous summary in Section 1. While their approach uses WordNet to label hypernyms in raw text, our initial labels are generated automatically. Etzioni et al. (2005) also require no labeled data or hand-labeled seeds for their namedentity extractor, but by comparison their classifier only uses a very small number of both features and automatically-generated training examples. 7 Conclusion We have shown how noun-pronoun c</context>
</contexts>
<marker>Cucerzan, Yarowsky, 1999</marker>
<rawString>Silviu Cucerzan and David Yarowsky. 1999. Language independent named entity recognition combining morphological and contextual evidence. In EMNLP-VLC, pages 90–99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silviu Cucerzan</author>
<author>David Yarowsky</author>
</authors>
<title>Minimally supervised induction of grammatical gender.</title>
<date>2003</date>
<booktitle>In NAACL,</booktitle>
<pages>40--47</pages>
<contexts>
<context position="30673" citStr="Cucerzan and Yarowsky (2003)" startWordPosition="4852" endWordPosition="4855">er can be handled together for pronoun resolution, it might be useful to learn them separately for other applications. Other statistical approaches to English noun gender are discussed in Section 2. In languages with ‘grammatical’ gender and plentiful gold standard data, gender can be tagged along with other word properties using standard supervised tagging techniques (Hajiˇc and Hladk´a, 1997). While our approach is the first to exploit a dual or orthogonal representation of English noun gender, a bootstrapping approach has been applied to determining grammatical gender in other languages by Cucerzan and Yarowsky (2003). In their work, the two orthogonal views are: 1) the context of the noun, and 2) the noun’s morphological properties. Bootstrapping with these views is possible in other languages where context is highly predictive of gender class, since contextual words like adjectives and determiners inflect to agree with the grammatical noun gender. We initially attempted a similar system for English noun gender but found context alone to be insufficiently predictive. Bootstrapping is also used in general information extraction. Brin (1998) shows how to alternate between extracting instances of a class and</context>
</contexts>
<marker>Cucerzan, Yarowsky, 2003</marker>
<rawString>Silviu Cucerzan and David Yarowsky. 2003. Minimally supervised induction of grammatical gender. In NAACL, pages 40–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>Domain adaptation for statistical classifiers.</title>
<date>2006</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>26--101</pages>
<marker>Daum´e, Marcu, 2006</marker>
<rawString>Hal Daum´e III and Daniel Marcu. 2006. Domain adaptation for statistical classifiers. Journal of Artificial Intelligence Research, 26:101–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascal Denis</author>
<author>Jason Baldridge</author>
</authors>
<title>Joint determination of anaphoricity and coreference using integer programming.</title>
<date>2007</date>
<booktitle>In NAACL-HLT,</booktitle>
<pages>236--243</pages>
<contexts>
<context position="27554" citStr="Denis and Baldridge, 2007" startWordPosition="4359" endWordPosition="4362">stems to 93.3% and 94.3%, respectively, while raising the accuracy of our FULL-SEMI system to 96.7%. This demonstrates that the surname-matching post-processor is a simple but worthwhile extension to a gender predictor.8 The remaining errors represent a number of challenging cases: United States, group, and public labeled as plural but classified as neutral; spectator classified as neutral, etc. Some of these may yield to more sophisticated joint classification of coreference and gender, perhaps along the lines of work in named-entity classification (Bunescu and Mooney, 2004) or anaphoricity (Denis and Baldridge, 2007). While gender has been shown to be the key feature for statistical pronoun resolution (Ge et al., 1998; Bergsma and Lin, 2006), it remains to be seen whether the exceptional accuracy obtained here will translate into improvements in resolution performance. However, given the clear utility of gender in coreference, substantial error reductions in gender 8One might wonder, why not provide special features so that the system can learn how to handle ambiguous nouns that occurred as sub-phrases in earlier names? The nature of our training data precludes this approach. We only include unambiguous e</context>
</contexts>
<marker>Denis, Baldridge, 2007</marker>
<rawString>Pascal Denis and Jason Baldridge. 2007. Joint determination of anaphoricity and coreference using integer programming. In NAACL-HLT, pages 236–243.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Etzioni</author>
<author>Michael Cafarella</author>
<author>Doug Downey</author>
<author>AnaMaria Popescu</author>
<author>Tal Shaked</author>
<author>Stephen Soderland</author>
<author>Daniel S Weld</author>
<author>Alexander Yates</author>
</authors>
<title>Unsupervised named-entity extraction from the web: an experimental study.</title>
<date>2005</date>
<journal>Artif. Intell.,</journal>
<volume>165</volume>
<issue>1</issue>
<contexts>
<context position="31716" citStr="Etzioni et al. (2005)" startWordPosition="5012" endWordPosition="5015">lone to be insufficiently predictive. Bootstrapping is also used in general information extraction. Brin (1998) shows how to alternate between extracting instances of a class and inducing new instance-extracting patterns. Collins and Singer (1999) and Cucerzan and Yarowsky (1999) apply bootstrapping to the related task of named-entity recognition. Our approach was directly influenced by the hypernym-extractor of Snow et al. (2005) and we provided an analogous summary in Section 1. While their approach uses WordNet to label hypernyms in raw text, our initial labels are generated automatically. Etzioni et al. (2005) also require no labeled data or hand-labeled seeds for their namedentity extractor, but by comparison their classifier only uses a very small number of both features and automatically-generated training examples. 7 Conclusion We have shown how noun-pronoun co-occurrence counts can be used to automatically annotate the gender of millions of nouns in unlabeled text. Training from these examples produced a classifier that clearly exceeds the state-of-the-art in gender classification. We incorporated thousands of useful but previously unexplored indicators of noun gender as features in our classi</context>
</contexts>
<marker>Etzioni, Cafarella, Downey, Popescu, Shaked, Soderland, Weld, Yates, 2005</marker>
<rawString>Oren Etzioni, Michael Cafarella, Doug Downey, AnaMaria Popescu, Tal Shaked, Stephen Soderland, Daniel S. Weld, and Alexander Yates. 2005. Unsupervised named-entity extraction from the web: an experimental study. Artif. Intell., 165(1):91–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Evans</author>
<author>Constantin Or˘asan</author>
</authors>
<title>Improving anaphora resolution by identifying animate entities in texts.</title>
<date>2000</date>
<booktitle>In DAARC,</booktitle>
<pages>154--162</pages>
<marker>Evans, Or˘asan, 2000</marker>
<rawString>Richard Evans and Constantin Or˘asan. 2000. Improving anaphora resolution by identifying animate entities in texts. In DAARC, pages 154–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Fleischman</author>
<author>Eduard Hovy</author>
<author>Abdessamad Echihabi</author>
</authors>
<title>Offline strategies for online question answering: answering questions before they are asked.</title>
<date>2003</date>
<booktitle>In ACL,</booktitle>
<pages>1--7</pages>
<contexts>
<context position="15777" citStr="Fleischman et al. (2003)" startWordPosition="2520" endWordPosition="2523"> nouns, we have features for the first and last token in the noun. Thus we hope to learn that Bob begins masculine nouns while inc. ends neutral ones. Finally, we have features that indicate if the noun or parts of the noun occur on various lists. Indicator features specify if any token occurs on in-house lists of given names, family names, cities, provinces, countries, corporations, languages, etc. A feature also indicates if a token is a corporate designation (like inc. or ltd.) or a human one (like Mr. or Sheik). We also made use of the person-name/instance pairs automatically extracted by Fleischman et al. (2003).4 This data provides counts for pairs such as (Zhang Qiyue, spokeswoman) and (Thorvald Stoltenberg, mediator). We have features for all concepts (like spokeswoman and mediator) and therefore learn their association with each gender. 3.3 Supervised learning and classification Once all the feature vectors have been extracted, they are passed to a supervised machine learn4Available at http://www.mit.edu/˜mbf/instances.txt.gz 123 ing algorithm. We train and classify using a multi-class linear-kernel Support Vector Machine (SVM) (Crammer and Singer, 2001). SVMs are maximum-margin classifiers that </context>
</contexts>
<marker>Fleischman, Hovy, Echihabi, 2003</marker>
<rawString>Michael Fleischman, Eduard Hovy, and Abdessamad Echihabi. 2003. Offline strategies for online question answering: answering questions before they are asked. In ACL, pages 1–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Niyu Ge</author>
<author>John Hale</author>
<author>Eugene Charniak</author>
</authors>
<title>A statistical approach to anaphora resolution.</title>
<date>1998</date>
<booktitle>In Proceedings of the Sixth Workshop on Very Large Corpora,</booktitle>
<pages>161--171</pages>
<contexts>
<context position="2776" citStr="Ge et al. (1998)" startWordPosition="426" endWordPosition="429"> Glenda is feminine. English third-person pronouns are grouped in four gender/number categories: masculine (he, his, him, himself), feminine (she, her, herself), neutral (it, its, itself), and plural (they, their, them, themselves). We broadly refer to these gender and number classes simply as gender. The objective of our work is to correctly assign gender to English noun tokens, in context; to determine which class of pronoun will refer to a given noun. One successful approach to this problem is to build a statistical gender model from a noun’s association with pronouns in text. For example, Ge et al. (1998) learn Ford has a 94% chance of being neutral, based on its frequent co-occurrence with neutral pronouns in text. Such estimates are noisy but useful. Both Ge et al. (1998) and Bergsma and Lin (2006) show that learned gender is the most important feature in their pronoun resolution systems. English differs from other languages like French and German in that gender is not an inherent grammatical property of an English noun, but rather a property of a real-world entity that is being referred to. A common noun like lawyer can be (semantically) masculine in one document and feminine in another. Wh</context>
<context position="27657" citStr="Ge et al., 1998" startWordPosition="4378" endWordPosition="4381">ates that the surname-matching post-processor is a simple but worthwhile extension to a gender predictor.8 The remaining errors represent a number of challenging cases: United States, group, and public labeled as plural but classified as neutral; spectator classified as neutral, etc. Some of these may yield to more sophisticated joint classification of coreference and gender, perhaps along the lines of work in named-entity classification (Bunescu and Mooney, 2004) or anaphoricity (Denis and Baldridge, 2007). While gender has been shown to be the key feature for statistical pronoun resolution (Ge et al., 1998; Bergsma and Lin, 2006), it remains to be seen whether the exceptional accuracy obtained here will translate into improvements in resolution performance. However, given the clear utility of gender in coreference, substantial error reductions in gender 8One might wonder, why not provide special features so that the system can learn how to handle ambiguous nouns that occurred as sub-phrases in earlier names? The nature of our training data precludes this approach. We only include unambiguous examples as pseudo-seeds in the learning process. Without providing ambiguous (but labeled) surnames in </context>
<context position="29823" citStr="Ge et al. (1998)" startWordPosition="4722" endWordPosition="4725">uns computer and company both have a WordNet sense that is a hyponym of person, falsely indicating these nouns would be compatible with pronouns like he or she. In addition to using WordNet classes, Soon et al. (2001) assign gender if the noun has a gendered designator (like Mr. or Mrs.) or if the first token is present on a list of common human first names. Note that we incorporate such contextual and categorical information (among many other information sources) automatically in our discriminative classifier, while they manually specify a few high-precision rules for particular gender cues. Ge et al. (1998) pioneered the statistical approach to gender determination. Like others, they consider gender and number separately, only learning statistical gender for the masculine, feminine, and neutral classes. While gender and number can be handled together for pronoun resolution, it might be useful to learn them separately for other applications. Other statistical approaches to English noun gender are discussed in Section 2. In languages with ‘grammatical’ gender and plentiful gold standard data, gender can be tagged along with other word properties using standard supervised tagging techniques (Hajiˇc</context>
</contexts>
<marker>Ge, Hale, Charniak, 1998</marker>
<rawString>Niyu Ge, John Hale, and Eugene Charniak. 1998. A statistical approach to anaphora resolution. In Proceedings of the Sixth Workshop on Very Large Corpora, pages 161–171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew R Golding</author>
<author>Dan Roth</author>
</authors>
<title>A Winnowbased approach to context-sensitive spelling correction.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="14393" citStr="Golding and Roth, 1999" startWordPosition="2290" endWordPosition="2293">eping only those features that occur more than 5 times. 3.2.1 Context features The first set of features represent the contexts of the word, using all the contexts in the noun group. To illustrate the potential utility of the context information, consider the context sentences for the masculine noun in Figure 2. Even if these snippets were all the information we were given, it would be easy to guess the gender of the noun. We use binary attribute-value features to flag, for any of the contexts, the presence of all words at context positions ±1, ±2, etc. (sometimes called collocation features (Golding and Roth, 1999)). For example, feature 255920 flags that the word two-tothe-right of the noun is he. We also provide features for the presence of all words anywhere within ±5 tokens of the noun (sometimes called context words). We also parse the sentence and provide a feature for the noun’s parent (and relationship with the parent) in the parse tree. For example, the instance in Figure 2 has features downloaded(subject), says(subject), etc. Since plural nouns should be governed by plural verbs, this feature is likely to be especially helpful for number classification. 3.2.2 Type features The next group of fe</context>
</contexts>
<marker>Golding, Roth, 1999</marker>
<rawString>Andrew R. Golding and Dan Roth. 1999. A Winnowbased approach to context-sensitive spelling correction. Machine Learning, 34(1-3):107–130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
<author>Barbora Hladk´a</author>
</authors>
<title>Probabilistic and rule-based tagger of an inflective language: a comparison. In ANLP,</title>
<date>1997</date>
<pages>111--118</pages>
<marker>Hajiˇc, Hladk´a, 1997</marker>
<rawString>Jan Hajiˇc and Barbora Hladk´a. 1997. Probabilistic and rule-based tagger of an inflective language: a comparison. In ANLP, pages 111–118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda Harabagiu</author>
<author>Razvan Bunescu</author>
<author>Steven Maiorano</author>
</authors>
<title>Text and knowledge mining for coreference resolution. In</title>
<date>2001</date>
<booktitle>NAACL,</booktitle>
<pages>55--62</pages>
<contexts>
<context position="29006" citStr="Harabagiu et al., 2001" startWordPosition="4589" endWordPosition="4592">helpful contribution. 6 Related Work Most coreference and pronoun resolution papers mention that they use gender information, but few explain how it is acquired. Kennedy and Boguraev (1996) use gender information produced by their enhanced part-of-speech tagger. Gender mistakes account for 35% of their system’s errors. Gender is less crucial in some genres, like computer manuals; most nouns are either neutral or plural and gender can be determined accurately based solely on morphological information (Lappin and Leass, 1994). A number of researchers (Evans and Or˘asan, 2000; Soon et al., 2001; Harabagiu et al., 2001) use WordNet classes to infer gender knowledge. Unfortunately, manually-constructed databases like WordNet suffer from both low coverage and rare senses. Pantel and Ravichandran (2004) note that the nouns computer and company both have a WordNet sense that is a hyponym of person, falsely indicating these nouns would be compatible with pronouns like he or she. In addition to using WordNet classes, Soon et al. (2001) assign gender if the noun has a gendered designator (like Mr. or Mrs.) or if the first token is present on a list of common human first names. Note that we incorporate such contextu</context>
</contexts>
<marker>Harabagiu, Bunescu, Maiorano, 2001</marker>
<rawString>Sanda Harabagiu, Razvan Bunescu, and Steven Maiorano. 2001. Text and knowledge mining for coreference resolution. In NAACL, pages 55–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Ide</author>
<author>Keith Suderman</author>
</authors>
<title>The American National Corpus first release.</title>
<date>2004</date>
<booktitle>In LREC,</booktitle>
<pages>1681--84</pages>
<contexts>
<context position="18946" citStr="Ide and Suderman, 2004" startWordPosition="3007" endWordPosition="3011">R+ in Section 4.2. 4 Experiments 4.1 Set-up We parsed the 3 GB AQUAINT corpus (Vorhees, 2002) using Minipar (Lin, 1998) to create our unlabeled data. We process this data as described in Section 3, making feature vectors from the first 4 million noun groups. We train from these examples using a linear-kernel SVM via the the efficient SVMmulticlass instance of the SVM struct software package (Tsochantaridis et al., 2004). To create our gold-standard gender data, we follow Bergsma (2005) in extracting gender information from the anaphora-annotated portion6 of the American National Corpus (ANC) (Ide and Suderman, 2004). In each document, we first group all nouns with a common lower-case string (exactly as done for our example extraction (Section 3.1)). Next, for each group we determine if a third-person pronoun refers to any noun in that group. If so, we label all nouns in the group with the gender of the referring pronoun. For example, if the pronoun he refers to a noun Brown, then all instances of Brown in the document are labeled as masculine. We extract the genders for 2794 nouns in the ANC training set (in 798 noun groups) and 2596 nouns in the ANC test set (in 642 groups). We apply this method to othe</context>
</contexts>
<marker>Ide, Suderman, 2004</marker>
<rawString>Nancy Ide and Keith Suderman. 2004. The American National Corpus first release. In LREC, pages 1681– 84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Kennedy</author>
<author>Branimir Boguraev</author>
</authors>
<title>Anaphora for everyone: Pronominal anaphora resolution without a parser. In</title>
<date>1996</date>
<booktitle>COLING,</booktitle>
<pages>113--118</pages>
<contexts>
<context position="28572" citStr="Kennedy and Boguraev (1996)" startWordPosition="4520" endWordPosition="4523">special features so that the system can learn how to handle ambiguous nouns that occurred as sub-phrases in earlier names? The nature of our training data precludes this approach. We only include unambiguous examples as pseudo-seeds in the learning process. Without providing ambiguous (but labeled) surnames in some way, the learner will not take advantage of features to help classify them. Accuracy (%) 126 assignment will likely be a helpful contribution. 6 Related Work Most coreference and pronoun resolution papers mention that they use gender information, but few explain how it is acquired. Kennedy and Boguraev (1996) use gender information produced by their enhanced part-of-speech tagger. Gender mistakes account for 35% of their system’s errors. Gender is less crucial in some genres, like computer manuals; most nouns are either neutral or plural and gender can be determined accurately based solely on morphological information (Lappin and Leass, 1994). A number of researchers (Evans and Or˘asan, 2000; Soon et al., 2001; Harabagiu et al., 2001) use WordNet classes to infer gender knowledge. Unfortunately, manually-constructed databases like WordNet suffer from both low coverage and rare senses. Pantel and R</context>
</contexts>
<marker>Kennedy, Boguraev, 1996</marker>
<rawString>Christopher Kennedy and Branimir Boguraev. 1996. Anaphora for everyone: Pronominal anaphora resolution without a parser. In COLING, pages 113–118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shalom Lappin</author>
<author>Herbert J Leass</author>
</authors>
<title>An algorithm for pronominal anaphora resolution.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>4</issue>
<contexts>
<context position="28912" citStr="Lappin and Leass, 1994" startWordPosition="4573" endWordPosition="4576">ake advantage of features to help classify them. Accuracy (%) 126 assignment will likely be a helpful contribution. 6 Related Work Most coreference and pronoun resolution papers mention that they use gender information, but few explain how it is acquired. Kennedy and Boguraev (1996) use gender information produced by their enhanced part-of-speech tagger. Gender mistakes account for 35% of their system’s errors. Gender is less crucial in some genres, like computer manuals; most nouns are either neutral or plural and gender can be determined accurately based solely on morphological information (Lappin and Leass, 1994). A number of researchers (Evans and Or˘asan, 2000; Soon et al., 2001; Harabagiu et al., 2001) use WordNet classes to infer gender knowledge. Unfortunately, manually-constructed databases like WordNet suffer from both low coverage and rare senses. Pantel and Ravichandran (2004) note that the nouns computer and company both have a WordNet sense that is a hyponym of person, falsely indicating these nouns would be compatible with pronouns like he or she. In addition to using WordNet classes, Soon et al. (2001) assign gender if the noun has a gendered designator (like Mr. or Mrs.) or if the first </context>
</contexts>
<marker>Lappin, Leass, 1994</marker>
<rawString>Shalom Lappin and Herbert J. Leass. 1994. An algorithm for pronominal anaphora resolution. Computational Linguistics, 20(4):535–561.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Dependency-based evaluation of MINIPAR.</title>
<date>1998</date>
<booktitle>In LREC Workshop on the Evaluation of Parsing Systems.</booktitle>
<contexts>
<context position="18442" citStr="Lin, 1998" startWordPosition="2928" endWordPosition="2929">he four different genders. Our gold-standard classifier has features for these four predictions plus features for the original path-based gender counts (Section 2).5 Since this approach uses both automatically-labeled and gold-standard data in a semi-supervised learning framework, we denote systems using this approach as -SEMI. 5We actually use 12 features for the path-based counts: the 4 original, and then 4 each for counts for the first and last token in the noun string. See PATHGENDER+ in Section 4.2. 4 Experiments 4.1 Set-up We parsed the 3 GB AQUAINT corpus (Vorhees, 2002) using Minipar (Lin, 1998) to create our unlabeled data. We process this data as described in Section 3, making feature vectors from the first 4 million noun groups. We train from these examples using a linear-kernel SVM via the the efficient SVMmulticlass instance of the SVM struct software package (Tsochantaridis et al., 2004). To create our gold-standard gender data, we follow Bergsma (2005) in extracting gender information from the anaphora-annotated portion6 of the American National Corpus (ANC) (Ide and Suderman, 2004). In each document, we first group all nouns with a common lower-case string (exactly as done fo</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Dependency-based evaluation of MINIPAR. In LREC Workshop on the Evaluation of Parsing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Deepak Ravichandran</author>
</authors>
<title>Automatically labeling semantic classes.</title>
<date>2004</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>321--328</pages>
<contexts>
<context position="29190" citStr="Pantel and Ravichandran (2004)" startWordPosition="4615" endWordPosition="4618">uraev (1996) use gender information produced by their enhanced part-of-speech tagger. Gender mistakes account for 35% of their system’s errors. Gender is less crucial in some genres, like computer manuals; most nouns are either neutral or plural and gender can be determined accurately based solely on morphological information (Lappin and Leass, 1994). A number of researchers (Evans and Or˘asan, 2000; Soon et al., 2001; Harabagiu et al., 2001) use WordNet classes to infer gender knowledge. Unfortunately, manually-constructed databases like WordNet suffer from both low coverage and rare senses. Pantel and Ravichandran (2004) note that the nouns computer and company both have a WordNet sense that is a hyponym of person, falsely indicating these nouns would be compatible with pronouns like he or she. In addition to using WordNet classes, Soon et al. (2001) assign gender if the noun has a gendered designator (like Mr. or Mrs.) or if the first token is present on a list of common human first names. Note that we incorporate such contextual and categorical information (among many other information sources) automatically in our discriminative classifier, while they manually specify a few high-precision rules for particu</context>
</contexts>
<marker>Pantel, Ravichandran, 2004</marker>
<rawString>Patrick Pantel and Deepak Ravichandran. 2004. Automatically labeling semantic classes. In HLT-NAACL, pages 321–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Learning syntactic patterns for automatic hypernym discovery.</title>
<date>2005</date>
<booktitle>In NIPS,</booktitle>
<pages>1297--1304</pages>
<contexts>
<context position="31529" citStr="Snow et al. (2005)" startWordPosition="4980" endWordPosition="4983"> contextual words like adjectives and determiners inflect to agree with the grammatical noun gender. We initially attempted a similar system for English noun gender but found context alone to be insufficiently predictive. Bootstrapping is also used in general information extraction. Brin (1998) shows how to alternate between extracting instances of a class and inducing new instance-extracting patterns. Collins and Singer (1999) and Cucerzan and Yarowsky (1999) apply bootstrapping to the related task of named-entity recognition. Our approach was directly influenced by the hypernym-extractor of Snow et al. (2005) and we provided an analogous summary in Section 1. While their approach uses WordNet to label hypernyms in raw text, our initial labels are generated automatically. Etzioni et al. (2005) also require no labeled data or hand-labeled seeds for their namedentity extractor, but by comparison their classifier only uses a very small number of both features and automatically-generated training examples. 7 Conclusion We have shown how noun-pronoun co-occurrence counts can be used to automatically annotate the gender of millions of nouns in unlabeled text. Training from these examples produced a class</context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2005</marker>
<rawString>Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005. Learning syntactic patterns for automatic hypernym discovery. In NIPS, pages 1297–1304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wee Meng Soon</author>
<author>Hwee Tou Ng</author>
<author>Daniel Chung Yong Lim</author>
</authors>
<title>A machine learning approach to coreference resolution of noun phrases.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>4</issue>
<contexts>
<context position="28981" citStr="Soon et al., 2001" startWordPosition="4585" endWordPosition="4588">t will likely be a helpful contribution. 6 Related Work Most coreference and pronoun resolution papers mention that they use gender information, but few explain how it is acquired. Kennedy and Boguraev (1996) use gender information produced by their enhanced part-of-speech tagger. Gender mistakes account for 35% of their system’s errors. Gender is less crucial in some genres, like computer manuals; most nouns are either neutral or plural and gender can be determined accurately based solely on morphological information (Lappin and Leass, 1994). A number of researchers (Evans and Or˘asan, 2000; Soon et al., 2001; Harabagiu et al., 2001) use WordNet classes to infer gender knowledge. Unfortunately, manually-constructed databases like WordNet suffer from both low coverage and rare senses. Pantel and Ravichandran (2004) note that the nouns computer and company both have a WordNet sense that is a hyponym of person, falsely indicating these nouns would be compatible with pronouns like he or she. In addition to using WordNet classes, Soon et al. (2001) assign gender if the noun has a gendered designator (like Mr. or Mrs.) or if the first token is present on a list of common human first names. Note that we </context>
</contexts>
<marker>Soon, Ng, Lim, 2001</marker>
<rawString>Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong Lim. 2001. A machine learning approach to coreference resolution of noun phrases. Computational Linguistics, 27(4):521–544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ioannis Tsochantaridis</author>
<author>Thomas Hofmann</author>
<author>Thorsten Joachims</author>
<author>Yasemin Altun</author>
</authors>
<title>Support vector machine learning for interdependent and structured output spaces.</title>
<date>2004</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="18746" citStr="Tsochantaridis et al., 2004" startWordPosition="2977" endWordPosition="2980">enote systems using this approach as -SEMI. 5We actually use 12 features for the path-based counts: the 4 original, and then 4 each for counts for the first and last token in the noun string. See PATHGENDER+ in Section 4.2. 4 Experiments 4.1 Set-up We parsed the 3 GB AQUAINT corpus (Vorhees, 2002) using Minipar (Lin, 1998) to create our unlabeled data. We process this data as described in Section 3, making feature vectors from the first 4 million noun groups. We train from these examples using a linear-kernel SVM via the the efficient SVMmulticlass instance of the SVM struct software package (Tsochantaridis et al., 2004). To create our gold-standard gender data, we follow Bergsma (2005) in extracting gender information from the anaphora-annotated portion6 of the American National Corpus (ANC) (Ide and Suderman, 2004). In each document, we first group all nouns with a common lower-case string (exactly as done for our example extraction (Section 3.1)). Next, for each group we determine if a third-person pronoun refers to any noun in that group. If so, we label all nouns in the group with the gender of the referring pronoun. For example, if the pronoun he refers to a noun Brown, then all instances of Brown in th</context>
</contexts>
<marker>Tsochantaridis, Hofmann, Joachims, Altun, 2004</marker>
<rawString>Ioannis Tsochantaridis, Thomas Hofmann, Thorsten Joachims, and Yasemin Altun. 2004. Support vector machine learning for interdependent and structured output spaces. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Vorhees</author>
</authors>
<title>Overview of the TREC</title>
<date>2002</date>
<booktitle>In Proceedings of the Eleventh Text REtrieval Conference (TREC).</booktitle>
<contexts>
<context position="18416" citStr="Vorhees, 2002" startWordPosition="2924" endWordPosition="2925">ical scores corresponding to the four different genders. Our gold-standard classifier has features for these four predictions plus features for the original path-based gender counts (Section 2).5 Since this approach uses both automatically-labeled and gold-standard data in a semi-supervised learning framework, we denote systems using this approach as -SEMI. 5We actually use 12 features for the path-based counts: the 4 original, and then 4 each for counts for the first and last token in the noun string. See PATHGENDER+ in Section 4.2. 4 Experiments 4.1 Set-up We parsed the 3 GB AQUAINT corpus (Vorhees, 2002) using Minipar (Lin, 1998) to create our unlabeled data. We process this data as described in Section 3, making feature vectors from the first 4 million noun groups. We train from these examples using a linear-kernel SVM via the the efficient SVMmulticlass instance of the SVM struct software package (Tsochantaridis et al., 2004). To create our gold-standard gender data, we follow Bergsma (2005) in extracting gender information from the anaphora-annotated portion6 of the American National Corpus (ANC) (Ide and Suderman, 2004). In each document, we first group all nouns with a common lower-case </context>
</contexts>
<marker>Vorhees, 2002</marker>
<rawString>Ellen Vorhees. 2002. Overview of the TREC 2002 question answering track. In Proceedings of the Eleventh Text REtrieval Conference (TREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In ACL,</booktitle>
<pages>189--196</pages>
<contexts>
<context position="13051" citStr="Yarowsky (1995)" startWordPosition="2071" endWordPosition="2072">ent nouns from dominating our training data, we only keep the first 200 groups corresponding to each noun string. Figure 2 gives an example training noun group with some (selected) context sentences. At test time, all nouns in the test documents are converted to this format for further processing. We group nouns because there is a strong tendency for nouns to have only one sense (and hence gender) per discourse. We extract contexts because nearby words provide good clues about which gender is being used. The notion that nouns have only one sense per discourse/collocation was also exploited by Yarowsky (1995) in his seminal work on bootstrapping for word sense disambiguation. 3.2 Feature vectors Once the training instances are extracted, they are converted to labeled feature vectors for supervised learning. The automatically-determined gender provides the class label (e.g., masculine for the group in Figure 2). The features identify properties of the noun and its context that potentially correlate with a particular gender category. We divide the features into two sets: those that depend on the contexts within the document (Context features: features of the tokens in the document), and those that d</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>David Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In ACL, pages 189–196.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>