<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000439">
<note confidence="0.6108834">
TUGAS: Exploiting Unlabelled Data for Twitter Sentiment Analysis
Silvio Amir+, Miguel Almeida*†, Bruno Martins+, Jo˜ao Filgueiras+, and M´ario J. Silva+
+INESC-ID, Instituto Superior T´ecnico, Universidade de Lisboa, Portugal
*Priberam Labs, Alameda D. Afonso Henriques, 41, 2o, 1000-123 Lisboa, Portugal
†Instituto de Telecomunicac¸˜oes, Instituto Superior T´ecnico, Universidade de Lisboa, Portugal
</note>
<keyword confidence="0.416976">
samir@inesc-id.pt, miguel.almeida@priberam.pt, bruno.g.martins@tecnico.ulisboa.pt
jfilgueiras@inesc-id.pt,mjs@inesc-id.pt
</keyword>
<sectionHeader confidence="0.986028" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9992196">
This paper describes our participation in
the message polarity classification task of
SemEval 2014. We focused on exploiting
unlabeled data to improve accuracy, com-
bining features leveraging word represen-
tations with other, more common features,
based on word tokens or lexicons. We
analyse the contribution of the different
features, concluding that unlabeled data
yields significant improvements.
</bodyText>
<sectionHeader confidence="0.998409" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99858475">
Research in exploiting social media for mea-
suring public opinion, evaluating popularity of
products and brands, anticipating stock-market
trends, or predicting elections showed promising
results (O’Connor et al., 2010; Mitchell et al.,
2013). However, this type of content poses a par-
ticularly challenging problem for text analysis sys-
tems. Typical messages show heavy use of Inter-
net slang, emoticons and other abbreviations and
discourse conventions. The lexical variation intro-
duced by this creative use of language, together
with the unconventional spelling and occasional
typos, leads to very large vocabularies. On the
other hand, messages are very short, and there-
fore word feature representations tend to become
very sparse, degrading the performance of ma-
chine learned classifiers.
The growing interest in this problem motivated
the creation of a shared task for Twitter Sentiment
Analysis in the 2013 edition of SemEval. The
Message Polarity Classification task was formal-
ized as follows: Given a message, decide whether
the message is ofpositive, negative, or neutral sen-
timent. For messages conveying both a positive
</bodyText>
<footnote confidence="0.96437825">
This work is licensed under a Creative Commons Attribution
4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http:
//creativecommons.org/licenses/by/4.0/
</footnote>
<table confidence="0.999069857142857">
Positive Neutral Negative
Train 2014 3230 4109 1265
Tweets 2013 1572 1640 601
Tweets 2014 982 669 202
SMS 2013 492 1207 394
Tweets Sarcasm 2014 33 13 40
LiveJournal2014 427 411 304
</table>
<tableCaption confidence="0.659372">
Table 1: Number of examples per class in each
SemEval dataset. The first row represents all train-
ing data; the other rows are sets used for testing.
</tableCaption>
<bodyText confidence="0.999799033333333">
and negative sentiment, whichever is the stronger
sentiment should be chosen (Nakov et al., 2013).
We describe our participation on the 2014 edi-
tion of this task, for which a set of manually la-
belled messages was created. Complying with the
Twitter policies for data access, the corpus was
distributed as a list of message IDs and each par-
ticipant was responsible for downloading the ac-
tual tweets. Using the provided script, we col-
lected a training set with 8604 tweets. After sub-
mission, the 2014 test sets were also made avail-
able. Along with the Tweets 2014 test set, evalu-
ation was also performed on a set of tweets with
sarcasm, on a set of LiveJournal blog entries, and
on sets of tweets and SMS messages from the 2013
edition of the task. Table 1 shows the class distri-
bution for each of these datasets.
In the 2013 edition (task 2B), the NRC-Canada
system (Mohammad et al., 2013) earned first place
by scoring 69.02% on the Official SemEval metric
(see Section 4) with a significant margin with re-
spect to the other systems: the second (G¨unther
and Furrer, 2013) and third (Reckman et al., 2013)
best systems scored 65.27% and 64.86%, respec-
tively. The main novelty in the NRC-Canada sys-
tem was the use of sentiment lexicons, specific
for the Twitter domain, generated from unlabeled
tweets using emoticons and hashtags as indicators
of sentiment. They found that these lexicons had a
strong impact on the results – more than word and
</bodyText>
<page confidence="0.990417">
673
</page>
<note confidence="0.731296">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 673–677,
Dublin, Ireland, August 23-24, 2014.
</note>
<bodyText confidence="0.998113777777778">
character n-grams.
The automatically induced lexicons are a way
to use information from unlabeled data to aid in
the classification task. In our approach, we take
this reasoning further, and focus on the impact of
various ways to incorporate knowledge from un-
labeled data. This allows us to mimic many real-
world scenarios where labelled data is scarce but
unlabeled data is plentiful.
</bodyText>
<sectionHeader confidence="0.901318" genericHeader="method">
2 Word Representations
</sectionHeader>
<bodyText confidence="0.9999585">
In text classification it is common to represent doc-
uments as bags-of-words, i.e., as unordered col-
lections of words. However, in the case of very
short social media texts, these representations be-
come less effective, as they lead to increased data
sparseness. We focused our experiments in com-
paring and complementing these approaches with
denser representations, which we now describe.
</bodyText>
<subsectionHeader confidence="0.963797">
2.1 Bag-Of-Words and ΔBM25
</subsectionHeader>
<bodyText confidence="0.999712142857143">
In a representation based on bags-of-words,
each message is represented as a vector m =
{w1, w2, ..., wn} ∈ RV , where V is the size of
the vocabulary. In order to have weights that re-
flect how relevant a word is to each of the classes,
we weighted the individual terms according to the
ΔBM25 heuristic (Paltoglou and Thelwall, 2010):
</bodyText>
<equation confidence="0.997534">
ΔBM25(wi) = tfi × log ((Nn−dfi,n+sj·dfi,p+3), (1)
</equation>
<bodyText confidence="0.999991625">
where tfi represents the frequency of term i in the
message, Na is the size of corpus a, dfi,a is the
document frequency of term i in the corpus a (i.e.,
in one of two subsets for the training data, corre-
sponding to either positive or negative messages),
and s is a smoothing constant, which we set to
0.5. This term weighting function was previously
shown to be effective for sentiment analysis.
</bodyText>
<subsectionHeader confidence="0.999503">
2.2 Brown Clusters
</subsectionHeader>
<bodyText confidence="0.99998875">
Brown et al. (1992) proposed a greedy agglomer-
ative hierarchical clustering procedure that groups
words to maximize the mutual information of bi-
grams. Clusters are initialized as consisting of a
single word each, and are then greedily merged ac-
cording to a mutual information criterion, to form
a lower-dimensional representation of a vocabu-
lary. The hierarchical nature of the clustering al-
lows words to be represented at different levels in
the hierarchy. This approach provides a denser
representation of the messages, mitigating the fea-
ture sparseness problem. We used a publicly avail-
able1 set of 1000 Brown clusters induced from a
corpus of 56 million Twitter messages.
We leveraged the word clusters by mapping
each word to the corresponding cluster, and we
then represented each message as a bag-of-clusters
vector in RK, where K = 1000 is the number
of clusters. These word cluster features were also
weighted with the ΔBM25 scheme.
</bodyText>
<subsectionHeader confidence="0.999537">
2.3 Concise Semantic Analysis
</subsectionHeader>
<bodyText confidence="0.9991105">
Concise Semantic Analysis is a form of term
and document representation that assigns, to each
term, its weight on each of the classes (Li et al.,
2011). These weights, computed from the fre-
quencies of the term on the training data, reflect
how associated the term is to each class. The
weight of term j in class c is given by (Lopez-
Monroy et al., 2013):
</bodyText>
<equation confidence="0.99680075">
� �
1 + tfkj
log2 (2)
len(k) ,
</equation>
<bodyText confidence="0.999968">
where Pc is the set of documents with label c
and tfkj is the term frequency of term j in doc-
ument k. To prevent labels with a higher number
of examples, or terms with higher frequencies, to
have stronger weights, an additional normalization
step is performed to obtain nwcj, the normalized
weight of term j in class c:
</bodyText>
<equation confidence="0.932004">
. (3)
</equation>
<bodyText confidence="0.999145285714286">
In the formula, L is the set of class labels and T is
the set of terms, making wlj the weight of term
j for a class l, and wct the weight of a term t
in class c. After defining every term as a vector
tj = {nw1j, ... , nwCj} ∈ RC, where Cis the
number of classes, each message m is represented
by summing each of its terms’ weight vectors:
</bodyText>
<equation confidence="0.978901">
tfj × tj. (4)
len(m)
</equation>
<bodyText confidence="0.97901">
In the formula, tfj is the frequency of term j in m.
</bodyText>
<subsectionHeader confidence="0.979254">
2.4 Dense Word Vectors
</subsectionHeader>
<bodyText confidence="0.996279">
Efficient approaches have recently been intro-
duced to train neural networks capable of produc-
ing continuous representations of words (Mikolov
</bodyText>
<footnote confidence="0.582971">
1http://www.ark.cs.cmu.edu/TweetNLP/
</footnote>
<figure confidence="0.456579">
�wcj =
k∈Pc
nwcj = wcj
E wlj × E wct
l∈L t∈T
�mcsa =
j∈m
</figure>
<page confidence="0.988106">
674
</page>
<table confidence="0.999930714285714">
Lexicon #1-grams #2-grams #pairs
Bing Liu 6789 - -
MPQA 8222 - -
SentiStrength 2546 - -
NRC EmoLex 14177 - -
Sentiment140 62468 677698 480010
NRC HashSent 54129 316531 308808
</table>
<tableCaption confidence="0.8896175">
Table 2: Number of unigrams, bigrams, and collo-
cation pairs, in the lexicons used in our system.
</tableCaption>
<bodyText confidence="0.9997864">
et al., 2013). These approaches allow fast train-
ing of projections from a representation based on
bags-of-words, where vectors have very high di-
mension (of the order of 104), but are also very
sparse and integer-valued, to vectors of much
lower dimensions (of the order of 102), with full
density and continuous values.
To induce word embeddings, a corpus of 17 mil-
lion Twitter messages was collected with the Twit-
ter crawler of Boanjak et al. (2012). Then, us-
ing word2vec2, we induced representations for the
word tokens occurring in the messages. All the to-
kens were represented as vectors wj E Rn, with
n = 100. A message was modeled as the sum of
the vector representations of the individual words:
</bodyText>
<equation confidence="0.9466235">
�mvec = wj. (5)
j∈m
</equation>
<bodyText confidence="0.8439525">
We also created a polarity class vector pC for each
class c, defined as:
</bodyText>
<equation confidence="0.841914">
mvec, (6)
</equation>
<bodyText confidence="0.99981125">
where m is a message of class c and Nc is the total
number of instances in class c. These vectors can
be interpreted as prototypes of their classes, and
are used in the classVec features described below.
</bodyText>
<sectionHeader confidence="0.992886" genericHeader="method">
3 The TUGAS System
</sectionHeader>
<bodyText confidence="0.999929">
We now describe the TUGAS approach, detailing
the considered features and our modeling choices.
</bodyText>
<subsectionHeader confidence="0.997805">
3.1 Word Features
</subsectionHeader>
<bodyText confidence="0.998082666666667">
To reduce the feature space of the model,
messages were lower-cased, Twitter user men-
tions (@username) were replaced with the to-
ken &lt;USER&gt; and URLs were replaced with
the &lt;URL&gt; token. We also normalized words
to include at most 3 repeated characters (e.g.,
</bodyText>
<footnote confidence="0.843682">
2https://code.google.com/p/word2vec/
</footnote>
<bodyText confidence="0.996835833333333">
“helloooooo!” to “hellooo!”). Following Pang et
al. (2002), negation was directly integrated into
the word representations. All the tokens occurring
between a negation word and the next punctuation
mark, were suffixed with the NEG annotation.
We used the following groups of features:
</bodyText>
<listItem confidence="0.999130666666667">
• bow-uni: vector of word unigrams
• bow-bc: vector of Brown word clusters
• csa: Concise Semantic Analysis vector mcsa
• wordVec: word2vec message vector mvec
• classVec: Euclidean distance between mes-
sage vector mvec and each class vector pc
</listItem>
<subsectionHeader confidence="0.99953">
3.2 Lexicon Features
</subsectionHeader>
<bodyText confidence="0.9999154375">
The document model was enriched with features
that take into account the presence of words with a
known prior polarity, such as happy or sad. We in-
cluded words from manually annotated sentiment
lexicons: Bing Liu Opinion Lexicon (Hu and Liu,
2004), MPQA (Wilson et al., 2005) and the NRC
Emotion Lexicon (Mohammad and Turney, 2013).
We also used the two automatically generated lex-
icons from Mohammad et al. (2013): the NRC
Hashtag Sentiment Lexicon and the Sentiment140
Lexicon. Table 2 summarizes the number of terms
of each lexicon.
As Mohammad et al. (2013), we added the fol-
lowing set of lexicon features, for each lexicon,
and for each combination of negated/non-negated
words and positive/negative polarity.
</bodyText>
<listItem confidence="0.996413428571429">
• The sum of the sentiment scores of all
(negated/non-negated) terms with (posi-
tive/negative) sentiment
• The largest of those scores
• The sentiment score of the last word in the
message that is also present in the lexicon
• The number of terms within the lexicon
</listItem>
<bodyText confidence="0.684692333333333">
Notice that terms can be unigrams, bigrams, and
collocations pairs. A group of these features was
computed for each of the sentiment lexicons.
</bodyText>
<subsectionHeader confidence="0.997771">
3.3 Syntactic Features
</subsectionHeader>
<bodyText confidence="0.9807254">
We extracted syntactic features aimed at the Twit-
ter domain, such as the use of heavy punctuation,
emoticons and character repetition. Concretely,
the following features were computed from the
original Twitter messages:
</bodyText>
<listItem confidence="0.9980625">
• Number of words originally with more than 3
repeated characters
• Number of sequences of exclamation marks
and/or question marks
</listItem>
<figure confidence="0.49727375">
1
Nc
m∈c
pc =
</figure>
<page confidence="0.996211">
675
</page>
<table confidence="0.999535363636364">
Features Tweets Test 2013 Tweets Test 2014 Acc SMS 2013 Live Journal 2014 Tweets Sarcasm 2014
Acc F1 Official Acc F1 Official F1 Official Acc F1 Official Acc F1 Official
bow-uni 65.62 59.30 54.60 69.94 66.30 65.60 68.80 62.40 54.90 60.42 58.30 56.60 47.67 43.90 41.50
submitted 69.55 67.50 65.60 71.45 69.00 69.00 70.57 67.60 62.70 68.21 68.20 69.80 53.49 50.10 52.90
- lexicons 66.90 64.30 61.70 70.37 67.00 66.40 66.46 63.50 58.30 64.27 64.20 65.50 48.84 45.10 47.00
- classVec 69.37 67.30 65.40 71.83 69.30 69.60 69.14 66.60 62.10 67.51 67.50 69.30 53.49 50.10 52.90
- wordVec 69.63 67.70 66.00 70.32 67.70 68.00 66.79 64.90 60.90 68.04 68.00 69.70 53.49 50.50 53.50
- bow-bc 68.06 66.40 65.10 67.40 64.30 65.30 67.89 65.20 60.40 68.30 68.30 70.00 52.33 49.90 49.90
+ syntactic 69.58 67.60 65.70 71.24 68.30 68.50 70.38 67.40 62.40 67.95 68.00 69.70 52.33 48.80 50.00
+ csa 67.45 63.70 60.50 70.10 67.30 67.50 71.48 67.60 62.10 66.11 66.00 68.30 53.49 51.30 50.30
+ bow-uni 67.69 62.50 58.50 70.64 67.30 66.70 72.77 67.10 60.40 67.60 67.20 67.10 51.16 48.00 43.90
</table>
<tableCaption confidence="0.863746">
Table 3: Impact of removing or adding groups of features. The row marked as submitted, in bold, is the
one that we submitted to the shared task. The bold column is the official score used to rank participants.
</tableCaption>
<listItem confidence="0.998537666666667">
• Number of positive/negative emoticons, de-
tected with a pre-existing regular expression3
• Number of capitalized words
</listItem>
<subsectionHeader confidence="0.993013">
3.4 Model Training
</subsectionHeader>
<bodyText confidence="0.999703">
We used the L2-regularized logistic regression im-
plementation from scikit-learn4. Given a set of m
instance-label pairs (xi, yi), with i = 1, ... , m,
xi E Rn, and yi E {−1, +1}, learning the clas-
sifier involves solving the following optimization
problem, where C &gt; 0 is a penalty parameter.
</bodyText>
<equation confidence="0.998523">
log(1 + e−yiw&apos;xi). (7)
</equation>
<bodyText confidence="0.999985153846154">
In scikit-learn, the problem is solved through
a trust region Newton method, using a wrapper
over the implementation available in the liblin-
ear5 package. For multi-class problems, scikit-
learn uses the one-vs-the-rest strategy. This par-
ticular implementation also suports the introduc-
tion of class weights, which we set to be inversely
proportional to the class frequency in the training
data, thus making each class equally important.
The selection of groups of features to be in-
cluded in the submitted run, as well as the tun-
ing of the regularization constant, were obtained
by cross-validation on the training dataset.
</bodyText>
<sectionHeader confidence="0.999818" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.993367">
We report results using the following metrics:
</bodyText>
<listItem confidence="0.999123285714286">
• Accuracy, defined as the percentage of
tweets correctly classified.
• Overall F1, computed by averaging the F1
score of all three classes.
• The Official SemEval score, computed by
averaging the F1 scores of the positive and
negative classes (Nakov et al., 2013).
</listItem>
<footnote confidence="0.999953">
3http://sentiment.christopherpotts.net/
4http://scikit-learn.org/
5http://www.csie.ntu.edu.tw/˜cjlin/liblinear/
</footnote>
<table confidence="0.9996162">
Feature group Acc F1 Official
bow-bc 66.33 63.30 60.30
wordVec 62.34 60.00 57.90
bow-uni 65.62 59.30 54.60
csa 61.58 56.70 52.90
</table>
<tableCaption confidence="0.9953455">
Table 4: Performance comparison using different
word representations in isolation.
</tableCaption>
<bodyText confidence="0.99994664516129">
We tried including or excluding various groups
of features, and obtained the best results on the
training set using Brown clusters (bow-bc), lexi-
con features (lexicon), word2vec word represen-
tations (wordVec), and the Euclidean distance be-
tween the word2vec representation and each class
vector (classVec). These features were the ones
used in our submission. Inclusion of syntactic
features (syntactic), Concise Semantic Analysis
(csa), and word unigrams (bow-uni) was found to
decrease performance during cross-validation, and
thus these features were not included.
Table 4 shows the results on the Twitter 2014
test set using only a single group of word represen-
tation features to train the model, from each of the
techniques introduced in Section 2. This table sug-
gests that exploiting unlabeled data is beneficial,
as representing words through their Brown clus-
ters (bow-bc) or through word2vec (wordVec)
yields better results than unigrams or CSA.
Table 3 shows results on five different test sets,
including two from the 2013 challenge (Nakov et
al., 2013), when features are added or removed
from the official submission, one group at a time.
Adding representations like bow-uni or csa actu-
ally hurts the performance, suggesting that, given
the relatively small set of training instances, using
coarse-level features in isolation, such as Brown
clusters, can yield better results.
More importantly, we verify that lexicon-based
and Brown cluster features have the largest impact
</bodyText>
<equation confidence="0.815223333333333">
min
w 1 2wIw + C �m
i=1
</equation>
<page confidence="0.992548">
676
</page>
<bodyText confidence="0.999778333333333">
(2.6% and 3.7%, respectively, in the official met-
ric). These results indicate that leveraging unla-
beled data yields significant improvements.
</bodyText>
<sectionHeader confidence="0.998923" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999969571428571">
This paper describes the participation of the
TUGAS team in the message polarity classifica-
tion task of SemEval 2014. We showed that there
are significant gains in leveraging unlabeled data
for the task of classifying the sentiment of Twit-
ter texts. Our score of 69% ranks at fifth place in
42 submissions, roughly 2% points below the top
score of 70.96%. We believe that the direction of
leveraging unlabeled data is still vastly unexplored
and, for future work, we intend to: (a) experi-
ment with semi-supervised learning approaches,
further exploiting unlabeled tweets; and (b) make
use of domain adaptation strategies to leverage on
labelled non-Twitter data.
</bodyText>
<sectionHeader confidence="0.996945" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.934256818181818">
This work was partially supported by the
EU/FEDER programme, QREN/POR Lis-
boa (Portugal), under the Intelligo project
(contract 2012/24803). The researchers from
INESC-ID were supported by Fundac¸˜ao para
a Ciˆencia e Tecnologia (FCT), through con-
tracts Pest-OE/EEI/LA0021/2013, EXCL/EEI-
ESS/0257/2012 (DataStorm), project grants
PTDC/CPJ-CPO/116888/2010 (POPSTAR), and
EXPL/EEI-ESS/0427/2013 (KD-LBSN), and
Ph.D. scholarship SFRH/BD/89020/2012.
</bodyText>
<sectionHeader confidence="0.992491" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997825544303798">
Matko Boanjak, Eduardo Oliveira, Jos´e Martins, Ed-
uarda Mendes Rodrigues, and Lu´ıs Sarmento. 2012.
Twitterecho: a distributed focused crawler to sup-
port open research with twitter data. In 21st Interna-
tional Conference Companion on World Wide Web,
pages 1233–1240.
Peter F. Brown, Peter V. Desouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics, 18(4):467–479.
Tobias G¨unther and Lenz Furrer. 2013. GU-MLT-
LT: Sentiment analysis of short messages using lin-
guistic features and stochastic gradient descent. In
7th International Workshop on Semantic Evaluation,
pages 328–332.
Minqing Hu and Bing Liu. 2004. Mining and sum-
marizing customer reviews. In 10th ACM SIGKDD
International Conference on Knowledge Discovery
and Data Mining, pages 168–177.
Zhixing Li, Zhongyang Xiong, Yufang Zhang, Chuny-
ong Liu, and Kuan Li. 2011. Fast text categorization
using concise semantic analysis. Pattern Recogni-
tion Letters, 32(3):441–448.
´Adrian Pastor Lopez-Monroy, Manuel Montes-y
Gomez, Hugo Jair Escalante, Luis Villasenor-
Pineda, and Esa´u Villatoro-Tello. 2013. INAOE’s
participation at PAN’13: Author profiling task. In
CLEF 2013 Evaluation Labs and Workshop.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013. Distributed represen-
tations of words and phrases and their composition-
ality. In 27th Annual Conference on Neural Infor-
mation Processing Systems, pages 3111–3119.
Lewis Mitchell, Kameron Decker Harris, Morgan R
Frank, Peter Sheridan Dodds, and Christopher M
Danforth. 2013. The geography of happiness:
connecting twitter sentiment and expression, de-
mographics, and objective characteristics of place.
PLoS ONE, 8(5):e64417.
Saif M Mohammad and Peter D Turney. 2013. Crowd-
sourcing a word–emotion association lexicon. Com-
putational Intelligence, 29(3):436–465.
Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. NRC-Canada: building the state-of-
the-art in sentiment analysis of tweets. In 7th Inter-
national Workshop on Semantic Evaluation, pages
321–327.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. SemEval-2013 task 2: Sentiment analysis in
Twitter. In 7th International Workshop on Semantic
Evaluation, pages 312–320.
Brendan O’Connor, Ramnath Balasubramanyan,
Bryan R Routledge, and Noah A Smith. 2010.
From tweets to polls: Linking text sentiment to
public opinion time series. In 4th International
AAAI Conference on Weblogs and Social Media,
pages 122–129.
Georgios Paltoglou and Mike Thelwall. 2010. A study
of information retrieval weighting schemes for sen-
timent analysis. In 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 1386–
1395.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using
machine learning techniques. In ACL-02 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 79–86.
Hilke Reckman, Baird Cheyanne, Jean Crawford,
Richard Crowell, Linnea Micciulla, Saratendu Sethi,
and Fruzsina Veress. 2013. teragram: Rule-based
detection of sentiment phrases using SAS sentiment
analysis. In 7th International Workshop on Seman-
tic Evaluation, pages 513–519.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing, pages 347–354.
</reference>
<page confidence="0.998055">
677
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.282906">
<title confidence="0.999671">TUGAS: Exploiting Unlabelled Data for Twitter Sentiment Analysis</title>
<author confidence="0.999875">Miguel Bruno</author>
<author confidence="0.999875">J M´ario</author>
<affiliation confidence="0.600046666666667">Instituto Superior T´ecnico, Universidade de Lisboa, Labs, Alameda D. Afonso Henriques, 41, 1000-123 Lisboa, de Instituto Superior T´ecnico, Universidade de Lisboa, Portugal</affiliation>
<abstract confidence="0.999425909090909">This paper describes our participation in the message polarity classification task of SemEval 2014. We focused on exploiting unlabeled data to improve accuracy, combining features leveraging word representations with other, more common features, based on word tokens or lexicons. We analyse the contribution of the different features, concluding that unlabeled data yields significant improvements.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Matko Boanjak</author>
<author>Eduardo Oliveira</author>
<author>Jos´e Martins</author>
<author>Eduarda Mendes Rodrigues</author>
<author>Lu´ıs Sarmento</author>
</authors>
<title>Twitterecho: a distributed focused crawler to support open research with twitter data.</title>
<date>2012</date>
<booktitle>In 21st International Conference Companion on World Wide Web,</booktitle>
<pages>1233--1240</pages>
<contexts>
<context position="8909" citStr="Boanjak et al. (2012)" startWordPosition="1457" endWordPosition="1460">177 - - Sentiment140 62468 677698 480010 NRC HashSent 54129 316531 308808 Table 2: Number of unigrams, bigrams, and collocation pairs, in the lexicons used in our system. et al., 2013). These approaches allow fast training of projections from a representation based on bags-of-words, where vectors have very high dimension (of the order of 104), but are also very sparse and integer-valued, to vectors of much lower dimensions (of the order of 102), with full density and continuous values. To induce word embeddings, a corpus of 17 million Twitter messages was collected with the Twitter crawler of Boanjak et al. (2012). Then, using word2vec2, we induced representations for the word tokens occurring in the messages. All the tokens were represented as vectors wj E Rn, with n = 100. A message was modeled as the sum of the vector representations of the individual words: �mvec = wj. (5) j∈m We also created a polarity class vector pC for each class c, defined as: mvec, (6) where m is a message of class c and Nc is the total number of instances in class c. These vectors can be interpreted as prototypes of their classes, and are used in the classVec features described below. 3 The TUGAS System We now describe the T</context>
</contexts>
<marker>Boanjak, Oliveira, Martins, Rodrigues, Sarmento, 2012</marker>
<rawString>Matko Boanjak, Eduardo Oliveira, Jos´e Martins, Eduarda Mendes Rodrigues, and Lu´ıs Sarmento. 2012. Twitterecho: a distributed focused crawler to support open research with twitter data. In 21st International Conference Companion on World Wide Web, pages 1233–1240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Peter V Desouza</author>
<author>Robert L Mercer</author>
<author>Vincent J Della Pietra</author>
<author>Jenifer C Lai</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="5847" citStr="Brown et al. (1992)" startWordPosition="912" endWordPosition="915">t a word is to each of the classes, we weighted the individual terms according to the ΔBM25 heuristic (Paltoglou and Thelwall, 2010): ΔBM25(wi) = tfi × log ((Nn−dfi,n+sj·dfi,p+3), (1) where tfi represents the frequency of term i in the message, Na is the size of corpus a, dfi,a is the document frequency of term i in the corpus a (i.e., in one of two subsets for the training data, corresponding to either positive or negative messages), and s is a smoothing constant, which we set to 0.5. This term weighting function was previously shown to be effective for sentiment analysis. 2.2 Brown Clusters Brown et al. (1992) proposed a greedy agglomerative hierarchical clustering procedure that groups words to maximize the mutual information of bigrams. Clusters are initialized as consisting of a single word each, and are then greedily merged according to a mutual information criterion, to form a lower-dimensional representation of a vocabulary. The hierarchical nature of the clustering allows words to be represented at different levels in the hierarchy. This approach provides a denser representation of the messages, mitigating the feature sparseness problem. We used a publicly available1 set of 1000 Brown cluste</context>
</contexts>
<marker>Brown, Desouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>Peter F. Brown, Peter V. Desouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18(4):467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tobias G¨unther</author>
<author>Lenz Furrer</author>
</authors>
<title>GU-MLTLT: Sentiment analysis of short messages using linguistic features and stochastic gradient descent.</title>
<date>2013</date>
<booktitle>In 7th International Workshop on Semantic Evaluation,</booktitle>
<pages>328--332</pages>
<marker>G¨unther, Furrer, 2013</marker>
<rawString>Tobias G¨unther and Lenz Furrer. 2013. GU-MLTLT: Sentiment analysis of short messages using linguistic features and stochastic gradient descent. In 7th International Workshop on Semantic Evaluation, pages 328–332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>168--177</pages>
<contexts>
<context position="10693" citStr="Hu and Liu, 2004" startWordPosition="1753" endWordPosition="1756"> next punctuation mark, were suffixed with the NEG annotation. We used the following groups of features: • bow-uni: vector of word unigrams • bow-bc: vector of Brown word clusters • csa: Concise Semantic Analysis vector mcsa • wordVec: word2vec message vector mvec • classVec: Euclidean distance between message vector mvec and each class vector pc 3.2 Lexicon Features The document model was enriched with features that take into account the presence of words with a known prior polarity, such as happy or sad. We included words from manually annotated sentiment lexicons: Bing Liu Opinion Lexicon (Hu and Liu, 2004), MPQA (Wilson et al., 2005) and the NRC Emotion Lexicon (Mohammad and Turney, 2013). We also used the two automatically generated lexicons from Mohammad et al. (2013): the NRC Hashtag Sentiment Lexicon and the Sentiment140 Lexicon. Table 2 summarizes the number of terms of each lexicon. As Mohammad et al. (2013), we added the following set of lexicon features, for each lexicon, and for each combination of negated/non-negated words and positive/negative polarity. • The sum of the sentiment scores of all (negated/non-negated) terms with (positive/negative) sentiment • The largest of those score</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 168–177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhixing Li</author>
<author>Zhongyang Xiong</author>
<author>Yufang Zhang</author>
<author>Chunyong Liu</author>
<author>Kuan Li</author>
</authors>
<title>Fast text categorization using concise semantic analysis.</title>
<date>2011</date>
<journal>Pattern Recognition Letters,</journal>
<volume>32</volume>
<issue>3</issue>
<contexts>
<context position="6951" citStr="Li et al., 2011" startWordPosition="1091" endWordPosition="1094"> the messages, mitigating the feature sparseness problem. We used a publicly available1 set of 1000 Brown clusters induced from a corpus of 56 million Twitter messages. We leveraged the word clusters by mapping each word to the corresponding cluster, and we then represented each message as a bag-of-clusters vector in RK, where K = 1000 is the number of clusters. These word cluster features were also weighted with the ΔBM25 scheme. 2.3 Concise Semantic Analysis Concise Semantic Analysis is a form of term and document representation that assigns, to each term, its weight on each of the classes (Li et al., 2011). These weights, computed from the frequencies of the term on the training data, reflect how associated the term is to each class. The weight of term j in class c is given by (LopezMonroy et al., 2013): � � 1 + tfkj log2 (2) len(k) , where Pc is the set of documents with label c and tfkj is the term frequency of term j in document k. To prevent labels with a higher number of examples, or terms with higher frequencies, to have stronger weights, an additional normalization step is performed to obtain nwcj, the normalized weight of term j in class c: . (3) In the formula, L is the set of class la</context>
</contexts>
<marker>Li, Xiong, Zhang, Liu, Li, 2011</marker>
<rawString>Zhixing Li, Zhongyang Xiong, Yufang Zhang, Chunyong Liu, and Kuan Li. 2011. Fast text categorization using concise semantic analysis. Pattern Recognition Letters, 32(3):441–448.</rawString>
</citation>
<citation valid="true">
<authors>
<author>´Adrian Pastor Lopez-Monroy</author>
<author>Manuel Montes-y Gomez</author>
<author>Hugo Jair Escalante</author>
<author>Luis VillasenorPineda</author>
<author>Esa´u Villatoro-Tello</author>
</authors>
<title>INAOE’s participation at PAN’13: Author profiling task.</title>
<date>2013</date>
<booktitle>In CLEF 2013 Evaluation Labs and Workshop.</booktitle>
<marker>Lopez-Monroy, Gomez, Escalante, VillasenorPineda, Villatoro-Tello, 2013</marker>
<rawString>´Adrian Pastor Lopez-Monroy, Manuel Montes-y Gomez, Hugo Jair Escalante, Luis VillasenorPineda, and Esa´u Villatoro-Tello. 2013. INAOE’s participation at PAN’13: Author profiling task. In CLEF 2013 Evaluation Labs and Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In 27th Annual Conference on Neural Information Processing Systems,</booktitle>
<pages>3111--3119</pages>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Distributed representations of words and phrases and their compositionality. In 27th Annual Conference on Neural Information Processing Systems, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lewis Mitchell</author>
<author>Kameron Decker Harris</author>
<author>Morgan R Frank</author>
<author>Peter Sheridan Dodds</author>
<author>Christopher M Danforth</author>
</authors>
<title>The geography of happiness: connecting twitter sentiment and expression, demographics, and objective characteristics of place.</title>
<date>2013</date>
<journal>PLoS ONE,</journal>
<volume>8</volume>
<issue>5</issue>
<contexts>
<context position="1187" citStr="Mitchell et al., 2013" startWordPosition="141" endWordPosition="144">ation in the message polarity classification task of SemEval 2014. We focused on exploiting unlabeled data to improve accuracy, combining features leveraging word representations with other, more common features, based on word tokens or lexicons. We analyse the contribution of the different features, concluding that unlabeled data yields significant improvements. 1 Introduction Research in exploiting social media for measuring public opinion, evaluating popularity of products and brands, anticipating stock-market trends, or predicting elections showed promising results (O’Connor et al., 2010; Mitchell et al., 2013). However, this type of content poses a particularly challenging problem for text analysis systems. Typical messages show heavy use of Internet slang, emoticons and other abbreviations and discourse conventions. The lexical variation introduced by this creative use of language, together with the unconventional spelling and occasional typos, leads to very large vocabularies. On the other hand, messages are very short, and therefore word feature representations tend to become very sparse, degrading the performance of machine learned classifiers. The growing interest in this problem motivated the</context>
</contexts>
<marker>Mitchell, Harris, Frank, Dodds, Danforth, 2013</marker>
<rawString>Lewis Mitchell, Kameron Decker Harris, Morgan R Frank, Peter Sheridan Dodds, and Christopher M Danforth. 2013. The geography of happiness: connecting twitter sentiment and expression, demographics, and objective characteristics of place. PLoS ONE, 8(5):e64417.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif M Mohammad</author>
<author>Peter D Turney</author>
</authors>
<title>Crowdsourcing a word–emotion association lexicon.</title>
<date>2013</date>
<journal>Computational Intelligence,</journal>
<volume>29</volume>
<issue>3</issue>
<contexts>
<context position="10777" citStr="Mohammad and Turney, 2013" startWordPosition="1767" endWordPosition="1770">following groups of features: • bow-uni: vector of word unigrams • bow-bc: vector of Brown word clusters • csa: Concise Semantic Analysis vector mcsa • wordVec: word2vec message vector mvec • classVec: Euclidean distance between message vector mvec and each class vector pc 3.2 Lexicon Features The document model was enriched with features that take into account the presence of words with a known prior polarity, such as happy or sad. We included words from manually annotated sentiment lexicons: Bing Liu Opinion Lexicon (Hu and Liu, 2004), MPQA (Wilson et al., 2005) and the NRC Emotion Lexicon (Mohammad and Turney, 2013). We also used the two automatically generated lexicons from Mohammad et al. (2013): the NRC Hashtag Sentiment Lexicon and the Sentiment140 Lexicon. Table 2 summarizes the number of terms of each lexicon. As Mohammad et al. (2013), we added the following set of lexicon features, for each lexicon, and for each combination of negated/non-negated words and positive/negative polarity. • The sum of the sentiment scores of all (negated/non-negated) terms with (positive/negative) sentiment • The largest of those scores • The sentiment score of the last word in the message that is also present in the </context>
</contexts>
<marker>Mohammad, Turney, 2013</marker>
<rawString>Saif M Mohammad and Peter D Turney. 2013. Crowdsourcing a word–emotion association lexicon. Computational Intelligence, 29(3):436–465.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammad</author>
<author>Svetlana Kiritchenko</author>
<author>Xiaodan Zhu</author>
</authors>
<title>NRC-Canada: building the state-ofthe-art in sentiment analysis of tweets.</title>
<date>2013</date>
<booktitle>In 7th International Workshop on Semantic Evaluation,</booktitle>
<pages>321--327</pages>
<contexts>
<context position="3506" citStr="Mohammad et al., 2013" startWordPosition="522" endWordPosition="525"> access, the corpus was distributed as a list of message IDs and each participant was responsible for downloading the actual tweets. Using the provided script, we collected a training set with 8604 tweets. After submission, the 2014 test sets were also made available. Along with the Tweets 2014 test set, evaluation was also performed on a set of tweets with sarcasm, on a set of LiveJournal blog entries, and on sets of tweets and SMS messages from the 2013 edition of the task. Table 1 shows the class distribution for each of these datasets. In the 2013 edition (task 2B), the NRC-Canada system (Mohammad et al., 2013) earned first place by scoring 69.02% on the Official SemEval metric (see Section 4) with a significant margin with respect to the other systems: the second (G¨unther and Furrer, 2013) and third (Reckman et al., 2013) best systems scored 65.27% and 64.86%, respectively. The main novelty in the NRC-Canada system was the use of sentiment lexicons, specific for the Twitter domain, generated from unlabeled tweets using emoticons and hashtags as indicators of sentiment. They found that these lexicons had a strong impact on the results – more than word and 673 Proceedings of the 8th International Wo</context>
<context position="10860" citStr="Mohammad et al. (2013)" startWordPosition="1781" endWordPosition="1784">own word clusters • csa: Concise Semantic Analysis vector mcsa • wordVec: word2vec message vector mvec • classVec: Euclidean distance between message vector mvec and each class vector pc 3.2 Lexicon Features The document model was enriched with features that take into account the presence of words with a known prior polarity, such as happy or sad. We included words from manually annotated sentiment lexicons: Bing Liu Opinion Lexicon (Hu and Liu, 2004), MPQA (Wilson et al., 2005) and the NRC Emotion Lexicon (Mohammad and Turney, 2013). We also used the two automatically generated lexicons from Mohammad et al. (2013): the NRC Hashtag Sentiment Lexicon and the Sentiment140 Lexicon. Table 2 summarizes the number of terms of each lexicon. As Mohammad et al. (2013), we added the following set of lexicon features, for each lexicon, and for each combination of negated/non-negated words and positive/negative polarity. • The sum of the sentiment scores of all (negated/non-negated) terms with (positive/negative) sentiment • The largest of those scores • The sentiment score of the last word in the message that is also present in the lexicon • The number of terms within the lexicon Notice that terms can be unigrams,</context>
</contexts>
<marker>Mohammad, Kiritchenko, Zhu, 2013</marker>
<rawString>Saif Mohammad, Svetlana Kiritchenko, and Xiaodan Zhu. 2013. NRC-Canada: building the state-ofthe-art in sentiment analysis of tweets. In 7th International Workshop on Semantic Evaluation, pages 321–327.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Sara Rosenthal</author>
<author>Zornitsa Kozareva</author>
<author>Veselin Stoyanov</author>
<author>Alan Ritter</author>
<author>Theresa Wilson</author>
</authors>
<date>2013</date>
<booktitle>SemEval-2013 task 2: Sentiment analysis in Twitter. In 7th International Workshop on Semantic Evaluation,</booktitle>
<pages>312--320</pages>
<contexts>
<context position="2715" citStr="Nakov et al., 2013" startWordPosition="378" endWordPosition="381"> under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http: //creativecommons.org/licenses/by/4.0/ Positive Neutral Negative Train 2014 3230 4109 1265 Tweets 2013 1572 1640 601 Tweets 2014 982 669 202 SMS 2013 492 1207 394 Tweets Sarcasm 2014 33 13 40 LiveJournal2014 427 411 304 Table 1: Number of examples per class in each SemEval dataset. The first row represents all training data; the other rows are sets used for testing. and negative sentiment, whichever is the stronger sentiment should be chosen (Nakov et al., 2013). We describe our participation on the 2014 edition of this task, for which a set of manually labelled messages was created. Complying with the Twitter policies for data access, the corpus was distributed as a list of message IDs and each participant was responsible for downloading the actual tweets. Using the provided script, we collected a training set with 8604 tweets. After submission, the 2014 test sets were also made available. Along with the Tweets 2014 test set, evaluation was also performed on a set of tweets with sarcasm, on a set of LiveJournal blog entries, and on sets of tweets an</context>
<context position="14634" citStr="Nakov et al., 2013" startWordPosition="2405" endWordPosition="2408">e inversely proportional to the class frequency in the training data, thus making each class equally important. The selection of groups of features to be included in the submitted run, as well as the tuning of the regularization constant, were obtained by cross-validation on the training dataset. 4 Results We report results using the following metrics: • Accuracy, defined as the percentage of tweets correctly classified. • Overall F1, computed by averaging the F1 score of all three classes. • The Official SemEval score, computed by averaging the F1 scores of the positive and negative classes (Nakov et al., 2013). 3http://sentiment.christopherpotts.net/ 4http://scikit-learn.org/ 5http://www.csie.ntu.edu.tw/˜cjlin/liblinear/ Feature group Acc F1 Official bow-bc 66.33 63.30 60.30 wordVec 62.34 60.00 57.90 bow-uni 65.62 59.30 54.60 csa 61.58 56.70 52.90 Table 4: Performance comparison using different word representations in isolation. We tried including or excluding various groups of features, and obtained the best results on the training set using Brown clusters (bow-bc), lexicon features (lexicon), word2vec word representations (wordVec), and the Euclidean distance between the word2vec representation a</context>
<context position="16023" citStr="Nakov et al., 2013" startWordPosition="2603" endWordPosition="2606">rd unigrams (bow-uni) was found to decrease performance during cross-validation, and thus these features were not included. Table 4 shows the results on the Twitter 2014 test set using only a single group of word representation features to train the model, from each of the techniques introduced in Section 2. This table suggests that exploiting unlabeled data is beneficial, as representing words through their Brown clusters (bow-bc) or through word2vec (wordVec) yields better results than unigrams or CSA. Table 3 shows results on five different test sets, including two from the 2013 challenge (Nakov et al., 2013), when features are added or removed from the official submission, one group at a time. Adding representations like bow-uni or csa actually hurts the performance, suggesting that, given the relatively small set of training instances, using coarse-level features in isolation, such as Brown clusters, can yield better results. More importantly, we verify that lexicon-based and Brown cluster features have the largest impact min w 1 2wIw + C �m i=1 676 (2.6% and 3.7%, respectively, in the official metric). These results indicate that leveraging unlabeled data yields significant improvements. 5 Conc</context>
</contexts>
<marker>Nakov, Rosenthal, Kozareva, Stoyanov, Ritter, Wilson, 2013</marker>
<rawString>Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva, Veselin Stoyanov, Alan Ritter, and Theresa Wilson. 2013. SemEval-2013 task 2: Sentiment analysis in Twitter. In 7th International Workshop on Semantic Evaluation, pages 312–320.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brendan O’Connor</author>
<author>Ramnath Balasubramanyan</author>
<author>Bryan R Routledge</author>
<author>Noah A Smith</author>
</authors>
<title>From tweets to polls: Linking text sentiment to public opinion time series.</title>
<date>2010</date>
<booktitle>In 4th International AAAI Conference on Weblogs and Social Media,</booktitle>
<pages>122--129</pages>
<marker>O’Connor, Balasubramanyan, Routledge, Smith, 2010</marker>
<rawString>Brendan O’Connor, Ramnath Balasubramanyan, Bryan R Routledge, and Noah A Smith. 2010. From tweets to polls: Linking text sentiment to public opinion time series. In 4th International AAAI Conference on Weblogs and Social Media, pages 122–129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgios Paltoglou</author>
<author>Mike Thelwall</author>
</authors>
<title>A study of information retrieval weighting schemes for sentiment analysis.</title>
<date>2010</date>
<booktitle>In 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1386--1395</pages>
<contexts>
<context position="5360" citStr="Paltoglou and Thelwall, 2010" startWordPosition="826" endWordPosition="829">owever, in the case of very short social media texts, these representations become less effective, as they lead to increased data sparseness. We focused our experiments in comparing and complementing these approaches with denser representations, which we now describe. 2.1 Bag-Of-Words and ΔBM25 In a representation based on bags-of-words, each message is represented as a vector m = {w1, w2, ..., wn} ∈ RV , where V is the size of the vocabulary. In order to have weights that reflect how relevant a word is to each of the classes, we weighted the individual terms according to the ΔBM25 heuristic (Paltoglou and Thelwall, 2010): ΔBM25(wi) = tfi × log ((Nn−dfi,n+sj·dfi,p+3), (1) where tfi represents the frequency of term i in the message, Na is the size of corpus a, dfi,a is the document frequency of term i in the corpus a (i.e., in one of two subsets for the training data, corresponding to either positive or negative messages), and s is a smoothing constant, which we set to 0.5. This term weighting function was previously shown to be effective for sentiment analysis. 2.2 Brown Clusters Brown et al. (1992) proposed a greedy agglomerative hierarchical clustering procedure that groups words to maximize the mutual infor</context>
</contexts>
<marker>Paltoglou, Thelwall, 2010</marker>
<rawString>Georgios Paltoglou and Mike Thelwall. 2010. A study of information retrieval weighting schemes for sentiment analysis. In 48th Annual Meeting of the Association for Computational Linguistics, pages 1386– 1395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up?: sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In ACL-02 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>79--86</pages>
<contexts>
<context position="9954" citStr="Pang et al. (2002)" startWordPosition="1634" endWordPosition="1637"> in class c. These vectors can be interpreted as prototypes of their classes, and are used in the classVec features described below. 3 The TUGAS System We now describe the TUGAS approach, detailing the considered features and our modeling choices. 3.1 Word Features To reduce the feature space of the model, messages were lower-cased, Twitter user mentions (@username) were replaced with the token &lt;USER&gt; and URLs were replaced with the &lt;URL&gt; token. We also normalized words to include at most 3 repeated characters (e.g., 2https://code.google.com/p/word2vec/ “helloooooo!” to “hellooo!”). Following Pang et al. (2002), negation was directly integrated into the word representations. All the tokens occurring between a negation word and the next punctuation mark, were suffixed with the NEG annotation. We used the following groups of features: • bow-uni: vector of word unigrams • bow-bc: vector of Brown word clusters • csa: Concise Semantic Analysis vector mcsa • wordVec: word2vec message vector mvec • classVec: Euclidean distance between message vector mvec and each class vector pc 3.2 Lexicon Features The document model was enriched with features that take into account the presence of words with a known prio</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up?: sentiment classification using machine learning techniques. In ACL-02 Conference on Empirical Methods in Natural Language Processing, pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hilke Reckman</author>
<author>Baird Cheyanne</author>
<author>Jean Crawford</author>
<author>Richard Crowell</author>
<author>Linnea Micciulla</author>
<author>Saratendu Sethi</author>
<author>Fruzsina Veress</author>
</authors>
<title>teragram: Rule-based detection of sentiment phrases using SAS sentiment analysis.</title>
<date>2013</date>
<booktitle>In 7th International Workshop on Semantic Evaluation,</booktitle>
<pages>513--519</pages>
<contexts>
<context position="3723" citStr="Reckman et al., 2013" startWordPosition="559" endWordPosition="562">sion, the 2014 test sets were also made available. Along with the Tweets 2014 test set, evaluation was also performed on a set of tweets with sarcasm, on a set of LiveJournal blog entries, and on sets of tweets and SMS messages from the 2013 edition of the task. Table 1 shows the class distribution for each of these datasets. In the 2013 edition (task 2B), the NRC-Canada system (Mohammad et al., 2013) earned first place by scoring 69.02% on the Official SemEval metric (see Section 4) with a significant margin with respect to the other systems: the second (G¨unther and Furrer, 2013) and third (Reckman et al., 2013) best systems scored 65.27% and 64.86%, respectively. The main novelty in the NRC-Canada system was the use of sentiment lexicons, specific for the Twitter domain, generated from unlabeled tweets using emoticons and hashtags as indicators of sentiment. They found that these lexicons had a strong impact on the results – more than word and 673 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 673–677, Dublin, Ireland, August 23-24, 2014. character n-grams. The automatically induced lexicons are a way to use information from unlabeled data to aid in the cl</context>
</contexts>
<marker>Reckman, Cheyanne, Crawford, Crowell, Micciulla, Sethi, Veress, 2013</marker>
<rawString>Hilke Reckman, Baird Cheyanne, Jean Crawford, Richard Crowell, Linnea Micciulla, Saratendu Sethi, and Fruzsina Veress. 2013. teragram: Rule-based detection of sentiment phrases using SAS sentiment analysis. In 7th International Workshop on Semantic Evaluation, pages 513–519.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phraselevel sentiment analysis.</title>
<date>2005</date>
<booktitle>In Conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>347--354</pages>
<contexts>
<context position="10721" citStr="Wilson et al., 2005" startWordPosition="1758" endWordPosition="1761">ere suffixed with the NEG annotation. We used the following groups of features: • bow-uni: vector of word unigrams • bow-bc: vector of Brown word clusters • csa: Concise Semantic Analysis vector mcsa • wordVec: word2vec message vector mvec • classVec: Euclidean distance between message vector mvec and each class vector pc 3.2 Lexicon Features The document model was enriched with features that take into account the presence of words with a known prior polarity, such as happy or sad. We included words from manually annotated sentiment lexicons: Bing Liu Opinion Lexicon (Hu and Liu, 2004), MPQA (Wilson et al., 2005) and the NRC Emotion Lexicon (Mohammad and Turney, 2013). We also used the two automatically generated lexicons from Mohammad et al. (2013): the NRC Hashtag Sentiment Lexicon and the Sentiment140 Lexicon. Table 2 summarizes the number of terms of each lexicon. As Mohammad et al. (2013), we added the following set of lexicon features, for each lexicon, and for each combination of negated/non-negated words and positive/negative polarity. • The sum of the sentiment scores of all (negated/non-negated) terms with (positive/negative) sentiment • The largest of those scores • The sentiment score of t</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phraselevel sentiment analysis. In Conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 347–354.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>