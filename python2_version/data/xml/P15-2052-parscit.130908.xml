<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005041">
<title confidence="0.875632">
The Users Who Say ‘Ni’:
Audience Identification in Chinese-language Restaurant Reviews
</title>
<author confidence="0.988099">
Rob Voigt Dan Jurafsky
</author>
<affiliation confidence="0.981478">
Stanford University
</affiliation>
<email confidence="0.995767">
{robvoigt,jurafsky}@stanford.edu
</email>
<sectionHeader confidence="0.994732" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999886428571429">
We give an algorithm for disambiguating
generic versus referential uses of second-
person pronouns in restaurant reviews in
Chinese. Reviews in this domain use the
‘you’ pronoun ` either generically or to
refer to shopkeepers, readers, or for self-
reference in reported conversation. We
first show that linguistic features of the lo-
cal context (drawn from prior literature)
help in disambigation. We then show
that document-level features (n-grams and
document-level embeddings)— not previ-
ously used in the referentiality literature—
actually give the largest gain in perfor-
mance, and suggest this is because pro-
nouns in this domain exhibit ‘one-sense-
per-discourse’. Our work highlights an
important case of discourse effects on pro-
noun use, and may suggest practical impli-
cations for audience extraction and other
sentiment tasks in online reviews.
</bodyText>
<sectionHeader confidence="0.972538" genericHeader="keywords">
1 Introduction and Task Description
</sectionHeader>
<bodyText confidence="0.999752058823529">
Detecting whether a given entity is referential is
an important question in computational discourse
processing. Linguistic features in the local con-
text of a given mention have been successfully
used for determining whether a second-person
pronoun (you) in dialogue is referential (Gupta et
al., 2007b; Frampton et al., 2009; Purver et al.,
2009). The related task of anaphoricity detection
is an important subtask of coreference resolution
(Ng and Cardie, 2002; Ng, 2004; Luo, 2007; Zhou
and Kong, 2009; Recasens et al., 2013).
In this paper we consider the task of audience
identification in review texts, using restaurant re-
views written in Chinese. Our task is to disam-
biguate a mention of the Chinese second-person
pronoun ` (ni, “you”) into the following four la-
bels that we found to occur commonly in reviews:
</bodyText>
<sectionHeader confidence="0.421963" genericHeader="introduction">
Generic
</sectionHeader>
<bodyText confidence="0.99446475">
������y�P�����{��`
For drinks they only have Sprite and Coke,
and you have to order before they’ll give
them to you.
</bodyText>
<sectionHeader confidence="0.516023" genericHeader="method">
Referential - Shop
</sectionHeader>
<bodyText confidence="0.867249333333333">
�H��������`��
With such good service, I’ll definitely come
back to your shop next time!
</bodyText>
<subsectionHeader confidence="0.672859">
Referential - Reader
</subsectionHeader>
<bodyText confidence="0.968111666666667">
Tá`ì �`ì �%M�!
Go and try it if you don’t believe me - your
whole body will feel regret!
</bodyText>
<equation confidence="0.4174815">
Referential - Writer / Self
���“`ì�������”
</equation>
<bodyText confidence="0.999881727272727">
The shop employee said, “You only want the
stone-bowl chicken?”
We aim to gain insight into the linguistics of
narrative by distinguishing the types of discourse
contexts in which different referential senses are
found. Restaurant reviews provide an important
new test case, and resolving who a reviewer wants
to address could have important implications for
coreference resolution or sentiment analysis of re-
views, as well as downstream tasks like informa-
tion extraction.
</bodyText>
<sectionHeader confidence="0.999856" genericHeader="method">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9999392">
A number of closely related earlier papers have fo-
cused on disambiguating ‘you’ in English. Gupta
et al. (2007b) annotated the Switchboard cor-
pus of telephone dialogue, showing that features
based on specific lexical patterns, adjacent parts-
of-speech, punctuation, and dialog acts are suf-
ficient to achieve performance of 84.39% at the
binary generic/referential prediction task. Gupta
et al. (2007a) show that similar features gener-
alize to addressee prediction for multi-party in-
</bodyText>
<page confidence="0.959554">
314
</page>
<bodyText confidence="0.961550363636364">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 314–319,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
teractions significantly better than a simple base-
line. Frampton et al. (2009) combine discourse
features with acoustic and visual information for
four-way interactions to resolve participant refer-
ence, and in the same setting Purver et al. (2009)
employ cascaded classifiers that first establish ref-
erentiality and then attempt to resolve the refer-
ent. They show that utterance-level lexical fea-
tures help, suggesting that different uses of ‘you’
are associated with distinct vocabularies.
Reiter and Frank (2010) investigate the more
general question of identifying genericity for noun
phrases, showing the usefuleness of linguistic fea-
tures such as syntactic dependency relations. Sim-
ilar local structural cues like phrase-structure po-
sitioning, head word identity, and distance to sur-
rounding clauses have been used as features in ma-
chine learning approaches for anaphoricity detec-
tion as one stage in a coreference resolution (Kong
and Zhou, 2010; Zhou and Kong, 2011; Kong and
Ng, 2013).
Prior work has also shown improvements in per-
formance in the dialogue domain from incorporat-
ing features having to do with acoustic prosody,
gaze, and head movements (Jovanovi´c et al., 2006;
Takemae and Ozawa, 2006; Gupta et al., 2007b;
Frampton et al., 2009). Of course in the review
domain we have no access to such information; as
we’ll see, however, we can exploit other unique
properties of reviews to make up for this lack.
</bodyText>
<sectionHeader confidence="0.996269" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.993944">
We scrape reviews from dianping.com, a Chinese-
language restaurant review site, from the ten cities
with the most reviews. We randomly sample 750
restaurants within each city and randomly sample
reviews of those restaurants.
We scraped 346,381 reviews, including all as-
sociated metadata (city, restaurant category, and
cost) for each restaurant, as well as the provided
ratings (service, taste, ambience, and overall stars)
for each review. Of these reviews only 6,704
(less than 2%) have the second-person pronominal
character ni, highlighting another particular inter-
est of this task: explicit second-person pronomi-
nals are quite rare in Chinese, at least in this genre,
making the reviews in which they appear linguis-
tically marked.
Summary statistics for this dataset are given in
Table 1. We release all our data and annotations at
nlp.stanford.edu/robvoigt/nis.
</bodyText>
<subsectionHeader confidence="0.999608">
3.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.9999803">
We apply the Stanford CRF Word Segmenter
(Tseng et al., 2005) to segment the text of each re-
view into words, and use simple heuristics based
on whitespace and punctuation to extract sen-
tences or sentence fragments. The Stanford Parser
(Klein and Manning, 2003; Levy and Manning,
2003) is then run on each extracted sentence or
fragment containing a ni to produce a dependency
graph and set of part-of-speech (POS) tags for
later use in feature extraction.
</bodyText>
<subsectionHeader confidence="0.999714">
3.2 Annotation
</subsectionHeader>
<bodyText confidence="0.999985766666666">
We hand-annotated 701 examples of ni tokens (in-
cluding both singular and plural cases), placing
them into one of seven categories: generic, writer-
referential, reader-referential, shop-referential, id-
iomatic, non-“you”, and other. The idiomatic and
non-“you” cases are commonly comprised of set
phrases such as 1//F,ff (nihao, “hello”) or A1//F,
(mini, “mini”) and are therefore relatively trivial to
filter; and the “other” class is both rare and varied,
including cases such as direct reference to prior
review-writers.
We therefore only consider the generic and
large-class referential cases, leaving us with 636
examples for our task; the distribution of anno-
tated nis is shown in Table 2.
The approximately half-and-half split between
generic and referential tokens is surprisingly sim-
ilar to that found by studies on English dialogue
like Gupta et al. (2007b), in spite of the large di-
vergence in language and genre.
We also found an unexpected word-sense prop-
erty of second-person pronouns in this genre: of
the 122 annotated reviews which contain more
than one ni, 83.6% use ni with the same sense
in each occurrence in the review, recalling the
one-sense-per-discourse hypothesis of Gale et al.
(1992). Finding that this discourse property—
normally predicated of word-sense in common
nouns—occurs in pronouns suggests the use of
features of the entire discourse in this task.
</bodyText>
<sectionHeader confidence="0.999481" genericHeader="method">
4 Features
</sectionHeader>
<bodyText confidence="0.999827">
We consider two primary types of features: “lo-
cal” and “discourse”.
</bodyText>
<subsectionHeader confidence="0.998417">
4.1 Local Features
</subsectionHeader>
<bodyText confidence="0.9997975">
“Local” features model textual and linguistic
properties of the immediate context of a given ni
</bodyText>
<page confidence="0.997193">
315
</page>
<table confidence="0.998706666666667">
SUBSET small REVIEWS CHARACTERS WORDS CHARS / REVIEW WORDS / REVIEW
Total 346,381 15,010,375 10,112,722 43.33 29.20
Containing ni 6,704 1,099,597 748,683 164.02 111.68
</table>
<tableCaption confidence="0.997895">
Table 1: Summary statistics for the dataset collected for this paper; 701 cases of ni in 472 documents were annotated.
</tableCaption>
<table confidence="0.99971">
TYPE ADDRESSEE COUNT
Generic - 296
Referential Shop 256
Referential Reader 48
Referential Writer 36
Idiomatic - 25
Non-“you” - 26
Other - 14
</table>
<tableCaption confidence="0.990191">
Table 2: Distribution of relevant types of Trh (ni, “you”) in
our annotated data.
</tableCaption>
<bodyText confidence="0.999870772727273">
mention, and were drawn from the large litera-
ture on referentiality, anaphoricity, and singleton-
detection:
Word Identity This feature simply encodes the
word-segmented identity of the word in which the
current ni token is found, capturing cases such as
the second-person plural &apos;fr//F f f ] (nimen, “you [plu-
ral]”).
Adjacent POS Tags Following Gupta et al.
(2007b), we include POS tag features for the sin-
gle words immediately following and preceeding
the ni token.
Dependencies We include binary features for
the presence or absence of lexicalized dependency
relations in which the given ni participates. As an
example, for the phrase J//F,-1M (“if you want
to sell dishes”), we extract a feature for NSUBJ(�
M, 1/&apos;F,) – you is the subject of the verb sell.
Lexical Context This feature set fires binary
features for the presence or absence of words in
the vocabulary within a three-word window on ei-
ther side of the given ni token.
</bodyText>
<subsectionHeader confidence="0.96353">
4.2 Discourse Features
</subsectionHeader>
<bodyText confidence="0.999787351351351">
The “discourse” category considers features that
characterize the entire review, capturing the intu-
ition that the classic one-sense-per-discourse prop-
erty is likely to hold for a given review, so we ex-
pect that features on the entire text of the review
will be relevant for prediction.
This is a novel contribution of this work: we
propose that in certain contexts (such as reviews),
referentiality resolution can be interpreted in part
as a text classification task.
Review N-grams These are binary features for
the presence or absence of n-grams in the entire
text of the review. We found that using a larger n
than 1 caused overfitting on our relatively small
dataset and reduced performance; therefore, re-
sults are reported using unigram features.
Review Vector Embedding To see if we can
induce higher-level representations of the review
text than simply binary n-gram features, we also
train a document-level distributed vector represen-
tation (Le and Mikolov, 2014) on the entire corpus
of reviews using the “doc2vec” implementation in
GENSIM (ˇReh˚uˇrek and Sojka, 2010), and include
200 vector features per review: a 100-dimensional
embedding learned on the entire document, as well
as a 100-dimensional average embedding calcu-
lated by averaging the vectors for each word in the
document. In experiments we found using both
the document and the average vectors combined
resulted in higher performance than either alone,
so we report results in this setting.
Metadata In addition to discourse features, we
also included features that encode the category,
city, and estimated cost for each restaurant, as well
as the service, taste, environment, and overall star
rank ratings associated with a given review on a
5-point scale.
</bodyText>
<sectionHeader confidence="0.999539" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999665538461538">
We tested the effectiveness of these features at
predicting genericity and reference for each ni to-
ken with multinomial logistic regression, as imple-
mented in SCIKIT-LEARN (Pedregosa et al., 2011).
We used two classification settings: a binary pre-
diction of whether a given ni is referential or not,
and a four-way prediction including distinctions
between the three annotated referential targets.
The results for each task are shown in Table 3.
In each case, we compare the performance of
all local and discourse features, as well as several
relevant subsets. One question we aim to address
is whether our discourse-level n-gram and embed-
</bodyText>
<page confidence="0.9925">
316
</page>
<table confidence="0.999892071428571">
FEATURES BINARY FOUR-WAY
Baseline 53.44% 46.56%
Word ID 67.19% 62.19%
+ POS 74.84% 64.38%
+ Deps 75.00% 69.38%
+ Context 78.44% 72.19%
N-grams 81.72% 77.03%
Vectors 74.84% 66.56%
N-grams + Vectors 81.25% 76.72%
Local + N-grams 84.38% 79.84%
Local + Vectors 86.56% 78.91%
Local + Discourse 85.78% 80.63%
Local + Discourse
+ Meta 88.21% 81.45%
</table>
<tableCaption confidence="0.919223">
Table 3: Average ten-fold cross-validation classification ac-
curacy for different feature sets on two tasks. “Local” refers
to all feature sets described in Section 4.1. BINARY distin-
guishes generic and referential ni, FOUR-WAY distinguishes
between generic and three referential senses.
</tableCaption>
<bodyText confidence="0.999931076923077">
ding features contribute similar information, so we
test them both separately and together. We com-
pare our results to a baseline of choosing the most
common class for either task.
We train and test models with ten-fold cross-
validation. In each fold, we use 80% of the data
for training, 10% for development, and 10% for
testing. For each feature set, we set the 12 regu-
larization strength as a hyperparameter based on
average cross-validation accuracy on the develop-
ment data in each fold. All reported results are
average cross-validation accuracy at that regular-
ization strength on the test set in each fold.
</bodyText>
<subsectionHeader confidence="0.998447">
5.1 Meta-analysis
</subsectionHeader>
<bodyText confidence="0.9999608">
To better understand the effectiveness of each fea-
ture set for this task, we perform a full ablation
study by training a classifier on all 127 (27 −1, ig-
noring the empty set) possible combinations of our
7 feature sets, and run a linear regression predict-
ing the classification score from the feature sets
used. This allows us to obtain estimates of the ef-
fect size and statistical significance for each set of
features with reference to all the others. These re-
sults are shown in Table 4.
</bodyText>
<sectionHeader confidence="0.999527" genericHeader="conclusions">
6 Discussion
</sectionHeader>
<bodyText confidence="0.9996748">
These results show that on the task of detecting
genericity and reference for second-person pro-
nouns in our annotated set of Chinese-language
restaurant reviews, both discourse-level features as
well as local, contextual features significantly im-
</bodyText>
<figure confidence="0.953268777777778">
BINARY FOUR-WAY
estimate p estimate p
2.5%
1.0%
0.2%
4.2%
7.8% ***
3.8% ***
2.6% ***
</figure>
<tableCaption confidence="0.95057725">
Table 4: Meta-analysis results for both tasks: effect size es-
timates from linear regressions (n = 127) predicting cross-
validation scores from feature set. the p column denotes sta-
tistical significance; . is p &lt; 0.1 and *** is p &lt; 0.001.
</tableCaption>
<bodyText confidence="0.998530447368421">
pact classification performance.
Simple word identity features alone already
provide surprising performance: the classifier
learns that the singular ni is more likely to be
generic while the plural (rjC (f ] often refers to peo-
ple affiliated with the shop.
While local features alone achieve respectable
performance (78.44% for binary genericity detec-
tion and 72.19% for four-way classification), we
show that in the review context significant gains
can be made from using a combination of local
and discourse-level features, exploiting discourse-
level indicators of referentiality and the fact that a
one-sense-per-discourse assumption tends to hold
with regards to the use of ni.
Analysis of learned feature weights in our
highest-performing model also provides some in-
teresting social insights. Reviews with a high
overall star rank were more likely to use generic
ni, and reviewers who thought highly of the restau-
rant’s service as indicated by their quality-of-
service rating were more likely to use reader-
directed referential ni.
Reviews with shop-directed referential ni were
likely to use emotive sentence-final particles like
IBJ (a), exclamation points, and question marks,
just as question marks were among the strongest
indicators of referential uses in the English “you”s
in Gupta et al. (2007b). We also found that other
pronouns like R (wo, “I”) and R fl (women,
“we”), as well as words of temporal sequencing Z
--- (diyi, “the first”), R (you, “again”), and ?,&apos;k&amp;quot; (ci,
“[one] time”) receive high weights for referential
classes.
Combined with the observation that reviews
containing ni simply tend to be much longer than
those without (see Table 1), these results suggest a
link to the narrative work of Jurafsky et al. (2014),
</bodyText>
<figure confidence="0.998803714285714">
LOCAL
DISCOURSE
MIXED
ALL
FEATURE
ID
POS
Deps
Context
N-grams
Vectors
Metadata
6.6%
3.8%
2.7%
***
.
***
***
***
***
1.9%
1.0%
0.6%
4.2%
***
.
***
</figure>
<page confidence="0.99554">
317
</page>
<bodyText confidence="0.9994995">
who characterize negative reviews as narrative ex-
positions of an individual bad experience.
For example, consider the following review
containing a referential ni:
</bodyText>
<equation confidence="0.9742625">
�qq,ý21 TrAo1� 1&amp;14�oIRR
ì-����,&amp;��Y�1º*31UkT,�X
)MAN-��I&apos;wTF1//rìWq†,MUIRAR
ì���å������
</equation>
<bodyText confidence="0.999987069767442">
The food and quantity was fine. The ambi-
ence need not be mentioned. But in spite of
having been a bit late for lunch, we wouldn’t
have imagined you’d first turn off the lights,
and then turn off the air conditioner. I’d like
to ask: saving money on electricity like this,
do you mean to imply that there’s no need for
us to pay for our meal?
While the immediate context suggests a refer-
ential interpretation (tff�-F&apos;f�//Mì�Wq†, literally
“want to ask you [plural], saving electricity”), it is
only when this mention is connected to elements
of the entire discourse (the sequence of events, the
first-person pronouns) that it becomes completely
clear first that the mention is referential and sec-
ond that it refers to the shop owner.
Furthermore, we found that when combined
with local features, features derived from dis-
tributed representations of each document per-
form at least as well for this task as document-
level n-grams, but at a much lower dimensional-
ity. This suggests that these embeddings do suc-
cessfully encode the information necessary to re-
produce document-level distinctions in discourse
types, such as between the personal narratives that
often surround referential uses of ni and the ab-
stract descriptions of generic uses.
Our meta-analysis shows that more linguisti-
cally motivated local features such as POS tags
and dependency relations are substantially over-
shadowed in effectiveness by lexical and discourse
features, although this may be due in part to re-
duced performance of these automatic taggers on
the more colloquial language in online reviews.
Finally, this work challenges prior claims that
spoken language is “more complex” than other
genres with regards to referentiality. On the con-
trary: whereas in a spoken discourse the poten-
tial addressees are by default the participants, web
texts such as the reviews studied here have no such
default, and may include complex, creative, and
domain-specific deictic reference that can be im-
portant for computational systems to address.
</bodyText>
<sectionHeader confidence="0.979121" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995706648148148">
Matthew Frampton, Raquel Fern´andez, Patrick Ehlen,
Mario Christoudias, Trevor Darrell, and Stanley Pe-
ters. 2009. Who is you?: combining linguistic and
gaze features to resolve second-person references in
dialogue. In Proceedings of the 12th Conference of
the European Chapter of the Association for Com-
putational Linguistics, pages 273–281. Association
for Computational Linguistics.
William A Gale, Kenneth W Church, and David
Yarowsky. 1992. One sense per discourse. In Pro-
ceedings of the workshop on Speech and Natural
Language, pages 233–237. Association for Compu-
tational Linguistics.
Surabhi Gupta, John Niekrasz, Matthew Purver, and
Daniel Jurafsky. 2007a. Resolving “you” in mul-
tiparty dialog. In Proceedings of the 8th SIGdial
Workshop on Discourse and Dialogue, pages 227–
30.
Surabhi Gupta, Matthew Purver, and Dan Jurafsky.
2007b. Disambiguating between generic and refer-
ential you in dialog. In Proceedings of the 45th An-
nual Meeting of the ACL on Interactive Poster and
Demonstration Sessions, pages 105–108. Associa-
tion for Computational Linguistics.
Natasa Jovanovi´c, Anton Nijholt, et al. 2006. Ad-
dressee identification in face-to-face meetings. As-
sociation for Computational Linguistics.
Dan Jurafsky, Victor Chahuneau, Bryan R Routledge,
and Noah A Smith. 2014. Narrative framing of con-
sumer sentiment in online restaurant reviews. First
Monday, 19(4).
Dan Klein and Christopher D Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics-Volume 1, pages 423–430. Asso-
ciation for Computational Linguistics.
Fang Kong and Hwee Tou Ng. 2013. Exploiting zero
pronouns to improve chinese coreference resolution.
In EMNLP, pages 278–288.
Fang Kong and Guodong Zhou. 2010. A tree kernel-
based unified framework for chinese zero anaphora
resolution. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 882–891. Association for Computa-
tional Linguistics.
Quoc V Le and Tomas Mikolov. 2014. Distributed
representations of sentences and documents. arXiv
preprint arXiv:1405.4053.
Roger Levy and Christopher Manning. 2003. Is it
harder to parse chinese, or the chinese treebank?
In Proceedings of the 41st Annual Meeting on As-
sociation for Computational Linguistics-Volume 1,
pages 439–446. Association for Computational Lin-
guistics.
</reference>
<page confidence="0.986173">
318
</page>
<reference confidence="0.99971763076923">
Xiaoqiang Luo. 2007. Coreference or not: A twin
model for coreference resolution. In HLT-NAACL,
pages 73–80.
Vincent Ng and Claire Cardie. 2002. Identifying
anaphoric and non-anaphoric noun phrases to im-
prove coreference resolution. In Proceedings of
the 19th international conference on Computational
linguistics-Volume 1, pages 1–7. Association for
Computational Linguistics.
Vincent Ng. 2004. Learning noun phrase anaphoricity
to improve coreference resolution: Issues in repre-
sentation and optimization. In Proceedings of the
42nd Annual Meeting on Association for Computa-
tional Linguistics, page 151. Association for Com-
putational Linguistics.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learn-
ing in Python. Journal of Machine Learning Re-
search, 12:2825–2830.
Matthew Purver, Raquel Fern´andez, Matthew Framp-
ton, and Stanley Peters. 2009. Cascaded lexi-
calised classifiers for second-person reference reso-
lution. In Proceedings of the SIGDIAL 2009 Confer-
ence: The 10th Annual Meeting of the Special Inter-
est Group on Discourse and Dialogue, pages 306–
309. Association for Computational Linguistics.
Marta Recasens, Marie-Catherine de Marneffe, and
Christopher Potts. 2013. The life and death of dis-
course entities: Identifying singleton mentions. In
HLT-NAACL, pages 627–633.
Radim ˇReh˚uˇrek and Petr Sojka. 2010. Software
Framework for Topic Modelling with Large Cor-
pora. In Proceedings of the LREC 2010 Workshop
on New Challenges for NLP Frameworks, pages 45–
50, Valletta, Malta, May. ELRA. http://is.
muni.cz/publication/884893/en.
Nils Reiter and Anette Frank. 2010. Identifying
generic noun phrases. In Proceedings of the 48th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 40–49. Association for
Computational Linguistics.
Yoshinao Takemae and Shinji Ozawa. 2006. Auto-
matic addressee identification based on participants’
head orientation and utterances for multiparty con-
versations. In Multimedia and Expo, 2006 IEEE In-
ternational Conference on, pages 1285–1288. IEEE.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A con-
ditional random field word segmenter. In Proceed-
ings of the fourth SIGHAN workshop on Chinese
language Processing, volume 171.
Guodong Zhou and Fang Kong. 2009. Global learn-
ing of noun phrase anaphoricity in coreference res-
olution via label propagation. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing: Volume 2-Volume 2, pages
978–986. Association for Computational Linguis-
tics.
Guodong Zhou and Fang Kong. 2011. Learning noun
phrase anaphoricity in coreference resolution via la-
bel propagation. Journal of Computer Science and
Technology, 26(1):34–44.
</reference>
<page confidence="0.999345">
319
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.915364">
<title confidence="0.9612255">The Users Who Say ‘Ni’: Audience Identification in Chinese-language Restaurant Reviews</title>
<author confidence="0.999744">Rob Voigt Dan Jurafsky</author>
<affiliation confidence="0.999556">Stanford University</affiliation>
<abstract confidence="0.999597954545455">We give an algorithm for disambiguating generic versus referential uses of secondperson pronouns in restaurant reviews in Chinese. Reviews in this domain use the pronoun generically or to refer to shopkeepers, readers, or for selfreference in reported conversation. We first show that linguistic features of the local context (drawn from prior literature) help in disambigation. We then show that document-level features (n-grams and document-level embeddings)— not previously used in the referentiality literature— actually give the largest gain in performance, and suggest this is because pronouns in this domain exhibit ‘one-senseper-discourse’. Our work highlights an important case of discourse effects on pronoun use, and may suggest practical implications for audience extraction and other sentiment tasks in online reviews.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Matthew Frampton</author>
<author>Raquel Fern´andez</author>
<author>Patrick Ehlen</author>
<author>Mario Christoudias</author>
<author>Trevor Darrell</author>
<author>Stanley Peters</author>
</authors>
<title>Who is you?: combining linguistic and gaze features to resolve second-person references in dialogue.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>273--281</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Frampton, Fern´andez, Ehlen, Christoudias, Darrell, Peters, 2009</marker>
<rawString>Matthew Frampton, Raquel Fern´andez, Patrick Ehlen, Mario Christoudias, Trevor Darrell, and Stanley Peters. 2009. Who is you?: combining linguistic and gaze features to resolve second-person references in dialogue. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, pages 273–281. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Gale</author>
<author>Kenneth W Church</author>
<author>David Yarowsky</author>
</authors>
<title>One sense per discourse.</title>
<date>1992</date>
<booktitle>In Proceedings of the workshop on Speech and Natural Language,</booktitle>
<pages>233--237</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7503" citStr="Gale et al. (1992)" startWordPosition="1158" endWordPosition="1161">es, leaving us with 636 examples for our task; the distribution of annotated nis is shown in Table 2. The approximately half-and-half split between generic and referential tokens is surprisingly similar to that found by studies on English dialogue like Gupta et al. (2007b), in spite of the large divergence in language and genre. We also found an unexpected word-sense property of second-person pronouns in this genre: of the 122 annotated reviews which contain more than one ni, 83.6% use ni with the same sense in each occurrence in the review, recalling the one-sense-per-discourse hypothesis of Gale et al. (1992). Finding that this discourse property— normally predicated of word-sense in common nouns—occurs in pronouns suggests the use of features of the entire discourse in this task. 4 Features We consider two primary types of features: “local” and “discourse”. 4.1 Local Features “Local” features model textual and linguistic properties of the immediate context of a given ni 315 SUBSET small REVIEWS CHARACTERS WORDS CHARS / REVIEW WORDS / REVIEW Total 346,381 15,010,375 10,112,722 43.33 29.20 Containing ni 6,704 1,099,597 748,683 164.02 111.68 Table 1: Summary statistics for the dataset collected for </context>
</contexts>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>William A Gale, Kenneth W Church, and David Yarowsky. 1992. One sense per discourse. In Proceedings of the workshop on Speech and Natural Language, pages 233–237. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Surabhi Gupta</author>
<author>John Niekrasz</author>
<author>Matthew Purver</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Resolving “you” in multiparty dialog.</title>
<date>2007</date>
<booktitle>In Proceedings of the 8th SIGdial Workshop on Discourse and Dialogue,</booktitle>
<pages>227--30</pages>
<contexts>
<context position="1355" citStr="Gupta et al., 2007" startWordPosition="194" endWordPosition="197">performance, and suggest this is because pronouns in this domain exhibit ‘one-senseper-discourse’. Our work highlights an important case of discourse effects on pronoun use, and may suggest practical implications for audience extraction and other sentiment tasks in online reviews. 1 Introduction and Task Description Detecting whether a given entity is referential is an important question in computational discourse processing. Linguistic features in the local context of a given mention have been successfully used for determining whether a second-person pronoun (you) in dialogue is referential (Gupta et al., 2007b; Frampton et al., 2009; Purver et al., 2009). The related task of anaphoricity detection is an important subtask of coreference resolution (Ng and Cardie, 2002; Ng, 2004; Luo, 2007; Zhou and Kong, 2009; Recasens et al., 2013). In this paper we consider the task of audience identification in review texts, using restaurant reviews written in Chinese. Our task is to disambiguate a mention of the Chinese second-person pronoun ` (ni, “you”) into the following four labels that we found to occur commonly in reviews: Generic ������y�P�����{��` For drinks they only have Sprite and Coke, and you have </context>
<context position="2855" citStr="Gupta et al. (2007" startWordPosition="441" endWordPosition="444">Self ���“`ì�������” The shop employee said, “You only want the stone-bowl chicken?” We aim to gain insight into the linguistics of narrative by distinguishing the types of discourse contexts in which different referential senses are found. Restaurant reviews provide an important new test case, and resolving who a reviewer wants to address could have important implications for coreference resolution or sentiment analysis of reviews, as well as downstream tasks like information extraction. 2 Related Work A number of closely related earlier papers have focused on disambiguating ‘you’ in English. Gupta et al. (2007b) annotated the Switchboard corpus of telephone dialogue, showing that features based on specific lexical patterns, adjacent partsof-speech, punctuation, and dialog acts are sufficient to achieve performance of 84.39% at the binary generic/referential prediction task. Gupta et al. (2007a) show that similar features generalize to addressee prediction for multi-party in314 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 314–319, Beijing, China, July 26-31, 2015</context>
<context position="4735" citStr="Gupta et al., 2007" startWordPosition="720" endWordPosition="723">efuleness of linguistic features such as syntactic dependency relations. Similar local structural cues like phrase-structure positioning, head word identity, and distance to surrounding clauses have been used as features in machine learning approaches for anaphoricity detection as one stage in a coreference resolution (Kong and Zhou, 2010; Zhou and Kong, 2011; Kong and Ng, 2013). Prior work has also shown improvements in performance in the dialogue domain from incorporating features having to do with acoustic prosody, gaze, and head movements (Jovanovi´c et al., 2006; Takemae and Ozawa, 2006; Gupta et al., 2007b; Frampton et al., 2009). Of course in the review domain we have no access to such information; as we’ll see, however, we can exploit other unique properties of reviews to make up for this lack. 3 Data We scrape reviews from dianping.com, a Chineselanguage restaurant review site, from the ten cities with the most reviews. We randomly sample 750 restaurants within each city and randomly sample reviews of those restaurants. We scraped 346,381 reviews, including all associated metadata (city, restaurant category, and cost) for each restaurant, as well as the provided ratings (service, taste, amb</context>
<context position="7156" citStr="Gupta et al. (2007" startWordPosition="1100" endWordPosition="1103">and non-“you” cases are commonly comprised of set phrases such as 1//F,ff (nihao, “hello”) or A1//F, (mini, “mini”) and are therefore relatively trivial to filter; and the “other” class is both rare and varied, including cases such as direct reference to prior review-writers. We therefore only consider the generic and large-class referential cases, leaving us with 636 examples for our task; the distribution of annotated nis is shown in Table 2. The approximately half-and-half split between generic and referential tokens is surprisingly similar to that found by studies on English dialogue like Gupta et al. (2007b), in spite of the large divergence in language and genre. We also found an unexpected word-sense property of second-person pronouns in this genre: of the 122 annotated reviews which contain more than one ni, 83.6% use ni with the same sense in each occurrence in the review, recalling the one-sense-per-discourse hypothesis of Gale et al. (1992). Finding that this discourse property— normally predicated of word-sense in common nouns—occurs in pronouns suggests the use of features of the entire discourse in this task. 4 Features We consider two primary types of features: “local” and “discourse”</context>
<context position="8749" citStr="Gupta et al. (2007" startWordPosition="1356" endWordPosition="1359">in 472 documents were annotated. TYPE ADDRESSEE COUNT Generic - 296 Referential Shop 256 Referential Reader 48 Referential Writer 36 Idiomatic - 25 Non-“you” - 26 Other - 14 Table 2: Distribution of relevant types of Trh (ni, “you”) in our annotated data. mention, and were drawn from the large literature on referentiality, anaphoricity, and singletondetection: Word Identity This feature simply encodes the word-segmented identity of the word in which the current ni token is found, capturing cases such as the second-person plural &apos;fr//F f f ] (nimen, “you [plural]”). Adjacent POS Tags Following Gupta et al. (2007b), we include POS tag features for the single words immediately following and preceeding the ni token. Dependencies We include binary features for the presence or absence of lexicalized dependency relations in which the given ni participates. As an example, for the phrase J//F,-1M (“if you want to sell dishes”), we extract a feature for NSUBJ(� M, 1/&apos;F,) – you is the subject of the verb sell. Lexical Context This feature set fires binary features for the presence or absence of words in the vocabulary within a three-word window on either side of the given ni token. 4.2 Discourse Features The “</context>
<context position="15367" citStr="Gupta et al. (2007" startWordPosition="2418" endWordPosition="2421">sis of learned feature weights in our highest-performing model also provides some interesting social insights. Reviews with a high overall star rank were more likely to use generic ni, and reviewers who thought highly of the restaurant’s service as indicated by their quality-ofservice rating were more likely to use readerdirected referential ni. Reviews with shop-directed referential ni were likely to use emotive sentence-final particles like IBJ (a), exclamation points, and question marks, just as question marks were among the strongest indicators of referential uses in the English “you”s in Gupta et al. (2007b). We also found that other pronouns like R (wo, “I”) and R fl (women, “we”), as well as words of temporal sequencing Z --- (diyi, “the first”), R (you, “again”), and ?,&apos;k&amp;quot; (ci, “[one] time”) receive high weights for referential classes. Combined with the observation that reviews containing ni simply tend to be much longer than those without (see Table 1), these results suggest a link to the narrative work of Jurafsky et al. (2014), LOCAL DISCOURSE MIXED ALL FEATURE ID POS Deps Context N-grams Vectors Metadata 6.6% 3.8% 2.7% *** . *** *** *** *** 1.9% 1.0% 0.6% 4.2% *** . *** 317 who characte</context>
</contexts>
<marker>Gupta, Niekrasz, Purver, Jurafsky, 2007</marker>
<rawString>Surabhi Gupta, John Niekrasz, Matthew Purver, and Daniel Jurafsky. 2007a. Resolving “you” in multiparty dialog. In Proceedings of the 8th SIGdial Workshop on Discourse and Dialogue, pages 227– 30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Surabhi Gupta</author>
<author>Matthew Purver</author>
<author>Dan Jurafsky</author>
</authors>
<title>Disambiguating between generic and referential you in dialog.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions,</booktitle>
<pages>105--108</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1355" citStr="Gupta et al., 2007" startWordPosition="194" endWordPosition="197">performance, and suggest this is because pronouns in this domain exhibit ‘one-senseper-discourse’. Our work highlights an important case of discourse effects on pronoun use, and may suggest practical implications for audience extraction and other sentiment tasks in online reviews. 1 Introduction and Task Description Detecting whether a given entity is referential is an important question in computational discourse processing. Linguistic features in the local context of a given mention have been successfully used for determining whether a second-person pronoun (you) in dialogue is referential (Gupta et al., 2007b; Frampton et al., 2009; Purver et al., 2009). The related task of anaphoricity detection is an important subtask of coreference resolution (Ng and Cardie, 2002; Ng, 2004; Luo, 2007; Zhou and Kong, 2009; Recasens et al., 2013). In this paper we consider the task of audience identification in review texts, using restaurant reviews written in Chinese. Our task is to disambiguate a mention of the Chinese second-person pronoun ` (ni, “you”) into the following four labels that we found to occur commonly in reviews: Generic ������y�P�����{��` For drinks they only have Sprite and Coke, and you have </context>
<context position="2855" citStr="Gupta et al. (2007" startWordPosition="441" endWordPosition="444">Self ���“`ì�������” The shop employee said, “You only want the stone-bowl chicken?” We aim to gain insight into the linguistics of narrative by distinguishing the types of discourse contexts in which different referential senses are found. Restaurant reviews provide an important new test case, and resolving who a reviewer wants to address could have important implications for coreference resolution or sentiment analysis of reviews, as well as downstream tasks like information extraction. 2 Related Work A number of closely related earlier papers have focused on disambiguating ‘you’ in English. Gupta et al. (2007b) annotated the Switchboard corpus of telephone dialogue, showing that features based on specific lexical patterns, adjacent partsof-speech, punctuation, and dialog acts are sufficient to achieve performance of 84.39% at the binary generic/referential prediction task. Gupta et al. (2007a) show that similar features generalize to addressee prediction for multi-party in314 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 314–319, Beijing, China, July 26-31, 2015</context>
<context position="4735" citStr="Gupta et al., 2007" startWordPosition="720" endWordPosition="723">efuleness of linguistic features such as syntactic dependency relations. Similar local structural cues like phrase-structure positioning, head word identity, and distance to surrounding clauses have been used as features in machine learning approaches for anaphoricity detection as one stage in a coreference resolution (Kong and Zhou, 2010; Zhou and Kong, 2011; Kong and Ng, 2013). Prior work has also shown improvements in performance in the dialogue domain from incorporating features having to do with acoustic prosody, gaze, and head movements (Jovanovi´c et al., 2006; Takemae and Ozawa, 2006; Gupta et al., 2007b; Frampton et al., 2009). Of course in the review domain we have no access to such information; as we’ll see, however, we can exploit other unique properties of reviews to make up for this lack. 3 Data We scrape reviews from dianping.com, a Chineselanguage restaurant review site, from the ten cities with the most reviews. We randomly sample 750 restaurants within each city and randomly sample reviews of those restaurants. We scraped 346,381 reviews, including all associated metadata (city, restaurant category, and cost) for each restaurant, as well as the provided ratings (service, taste, amb</context>
<context position="7156" citStr="Gupta et al. (2007" startWordPosition="1100" endWordPosition="1103">and non-“you” cases are commonly comprised of set phrases such as 1//F,ff (nihao, “hello”) or A1//F, (mini, “mini”) and are therefore relatively trivial to filter; and the “other” class is both rare and varied, including cases such as direct reference to prior review-writers. We therefore only consider the generic and large-class referential cases, leaving us with 636 examples for our task; the distribution of annotated nis is shown in Table 2. The approximately half-and-half split between generic and referential tokens is surprisingly similar to that found by studies on English dialogue like Gupta et al. (2007b), in spite of the large divergence in language and genre. We also found an unexpected word-sense property of second-person pronouns in this genre: of the 122 annotated reviews which contain more than one ni, 83.6% use ni with the same sense in each occurrence in the review, recalling the one-sense-per-discourse hypothesis of Gale et al. (1992). Finding that this discourse property— normally predicated of word-sense in common nouns—occurs in pronouns suggests the use of features of the entire discourse in this task. 4 Features We consider two primary types of features: “local” and “discourse”</context>
<context position="8749" citStr="Gupta et al. (2007" startWordPosition="1356" endWordPosition="1359">in 472 documents were annotated. TYPE ADDRESSEE COUNT Generic - 296 Referential Shop 256 Referential Reader 48 Referential Writer 36 Idiomatic - 25 Non-“you” - 26 Other - 14 Table 2: Distribution of relevant types of Trh (ni, “you”) in our annotated data. mention, and were drawn from the large literature on referentiality, anaphoricity, and singletondetection: Word Identity This feature simply encodes the word-segmented identity of the word in which the current ni token is found, capturing cases such as the second-person plural &apos;fr//F f f ] (nimen, “you [plural]”). Adjacent POS Tags Following Gupta et al. (2007b), we include POS tag features for the single words immediately following and preceeding the ni token. Dependencies We include binary features for the presence or absence of lexicalized dependency relations in which the given ni participates. As an example, for the phrase J//F,-1M (“if you want to sell dishes”), we extract a feature for NSUBJ(� M, 1/&apos;F,) – you is the subject of the verb sell. Lexical Context This feature set fires binary features for the presence or absence of words in the vocabulary within a three-word window on either side of the given ni token. 4.2 Discourse Features The “</context>
<context position="15367" citStr="Gupta et al. (2007" startWordPosition="2418" endWordPosition="2421">sis of learned feature weights in our highest-performing model also provides some interesting social insights. Reviews with a high overall star rank were more likely to use generic ni, and reviewers who thought highly of the restaurant’s service as indicated by their quality-ofservice rating were more likely to use readerdirected referential ni. Reviews with shop-directed referential ni were likely to use emotive sentence-final particles like IBJ (a), exclamation points, and question marks, just as question marks were among the strongest indicators of referential uses in the English “you”s in Gupta et al. (2007b). We also found that other pronouns like R (wo, “I”) and R fl (women, “we”), as well as words of temporal sequencing Z --- (diyi, “the first”), R (you, “again”), and ?,&apos;k&amp;quot; (ci, “[one] time”) receive high weights for referential classes. Combined with the observation that reviews containing ni simply tend to be much longer than those without (see Table 1), these results suggest a link to the narrative work of Jurafsky et al. (2014), LOCAL DISCOURSE MIXED ALL FEATURE ID POS Deps Context N-grams Vectors Metadata 6.6% 3.8% 2.7% *** . *** *** *** *** 1.9% 1.0% 0.6% 4.2% *** . *** 317 who characte</context>
</contexts>
<marker>Gupta, Purver, Jurafsky, 2007</marker>
<rawString>Surabhi Gupta, Matthew Purver, and Dan Jurafsky. 2007b. Disambiguating between generic and referential you in dialog. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, pages 105–108. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Natasa Jovanovi´c</author>
<author>Anton Nijholt</author>
</authors>
<title>Addressee identification in face-to-face meetings. Association for Computational Linguistics.</title>
<date>2006</date>
<marker>Jovanovi´c, Nijholt, 2006</marker>
<rawString>Natasa Jovanovi´c, Anton Nijholt, et al. 2006. Addressee identification in face-to-face meetings. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Jurafsky</author>
<author>Victor Chahuneau</author>
<author>Bryan R Routledge</author>
<author>Noah A Smith</author>
</authors>
<title>Narrative framing of consumer sentiment in online restaurant reviews.</title>
<date>2014</date>
<journal>First Monday,</journal>
<volume>19</volume>
<issue>4</issue>
<contexts>
<context position="15803" citStr="Jurafsky et al. (2014)" startWordPosition="2493" endWordPosition="2496">particles like IBJ (a), exclamation points, and question marks, just as question marks were among the strongest indicators of referential uses in the English “you”s in Gupta et al. (2007b). We also found that other pronouns like R (wo, “I”) and R fl (women, “we”), as well as words of temporal sequencing Z --- (diyi, “the first”), R (you, “again”), and ?,&apos;k&amp;quot; (ci, “[one] time”) receive high weights for referential classes. Combined with the observation that reviews containing ni simply tend to be much longer than those without (see Table 1), these results suggest a link to the narrative work of Jurafsky et al. (2014), LOCAL DISCOURSE MIXED ALL FEATURE ID POS Deps Context N-grams Vectors Metadata 6.6% 3.8% 2.7% *** . *** *** *** *** 1.9% 1.0% 0.6% 4.2% *** . *** 317 who characterize negative reviews as narrative expositions of an individual bad experience. For example, consider the following review containing a referential ni: �qq,ý21 TrAo1� 1&amp;14�oIRR ì-����,&amp;��Y�1º*31UkT,�X )MAN-��I&apos;wTF1//rìWq†,MUIRAR ì���å������ The food and quantity was fine. The ambience need not be mentioned. But in spite of having been a bit late for lunch, we wouldn’t have imagined you’d first turn off the lights, and then turn off </context>
</contexts>
<marker>Jurafsky, Chahuneau, Routledge, Smith, 2014</marker>
<rawString>Dan Jurafsky, Victor Chahuneau, Bryan R Routledge, and Noah A Smith. 2014. Narrative framing of consumer sentiment in online restaurant reviews. First Monday, 19(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1,</booktitle>
<pages>423--430</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6082" citStr="Klein and Manning, 2003" startWordPosition="934" endWordPosition="937">acter ni, highlighting another particular interest of this task: explicit second-person pronominals are quite rare in Chinese, at least in this genre, making the reviews in which they appear linguistically marked. Summary statistics for this dataset are given in Table 1. We release all our data and annotations at nlp.stanford.edu/robvoigt/nis. 3.1 Preprocessing We apply the Stanford CRF Word Segmenter (Tseng et al., 2005) to segment the text of each review into words, and use simple heuristics based on whitespace and punctuation to extract sentences or sentence fragments. The Stanford Parser (Klein and Manning, 2003; Levy and Manning, 2003) is then run on each extracted sentence or fragment containing a ni to produce a dependency graph and set of part-of-speech (POS) tags for later use in feature extraction. 3.2 Annotation We hand-annotated 701 examples of ni tokens (including both singular and plural cases), placing them into one of seven categories: generic, writerreferential, reader-referential, shop-referential, idiomatic, non-“you”, and other. The idiomatic and non-“you” cases are commonly comprised of set phrases such as 1//F,ff (nihao, “hello”) or A1//F, (mini, “mini”) and are therefore relatively</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 423–430. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fang Kong</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Exploiting zero pronouns to improve chinese coreference resolution.</title>
<date>2013</date>
<booktitle>In EMNLP,</booktitle>
<pages>278--288</pages>
<contexts>
<context position="4498" citStr="Kong and Ng, 2013" startWordPosition="681" endWordPosition="684">ance-level lexical features help, suggesting that different uses of ‘you’ are associated with distinct vocabularies. Reiter and Frank (2010) investigate the more general question of identifying genericity for noun phrases, showing the usefuleness of linguistic features such as syntactic dependency relations. Similar local structural cues like phrase-structure positioning, head word identity, and distance to surrounding clauses have been used as features in machine learning approaches for anaphoricity detection as one stage in a coreference resolution (Kong and Zhou, 2010; Zhou and Kong, 2011; Kong and Ng, 2013). Prior work has also shown improvements in performance in the dialogue domain from incorporating features having to do with acoustic prosody, gaze, and head movements (Jovanovi´c et al., 2006; Takemae and Ozawa, 2006; Gupta et al., 2007b; Frampton et al., 2009). Of course in the review domain we have no access to such information; as we’ll see, however, we can exploit other unique properties of reviews to make up for this lack. 3 Data We scrape reviews from dianping.com, a Chineselanguage restaurant review site, from the ten cities with the most reviews. We randomly sample 750 restaurants wit</context>
</contexts>
<marker>Kong, Ng, 2013</marker>
<rawString>Fang Kong and Hwee Tou Ng. 2013. Exploiting zero pronouns to improve chinese coreference resolution. In EMNLP, pages 278–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fang Kong</author>
<author>Guodong Zhou</author>
</authors>
<title>A tree kernelbased unified framework for chinese zero anaphora resolution.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>882--891</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4457" citStr="Kong and Zhou, 2010" startWordPosition="673" endWordPosition="676">resolve the referent. They show that utterance-level lexical features help, suggesting that different uses of ‘you’ are associated with distinct vocabularies. Reiter and Frank (2010) investigate the more general question of identifying genericity for noun phrases, showing the usefuleness of linguistic features such as syntactic dependency relations. Similar local structural cues like phrase-structure positioning, head word identity, and distance to surrounding clauses have been used as features in machine learning approaches for anaphoricity detection as one stage in a coreference resolution (Kong and Zhou, 2010; Zhou and Kong, 2011; Kong and Ng, 2013). Prior work has also shown improvements in performance in the dialogue domain from incorporating features having to do with acoustic prosody, gaze, and head movements (Jovanovi´c et al., 2006; Takemae and Ozawa, 2006; Gupta et al., 2007b; Frampton et al., 2009). Of course in the review domain we have no access to such information; as we’ll see, however, we can exploit other unique properties of reviews to make up for this lack. 3 Data We scrape reviews from dianping.com, a Chineselanguage restaurant review site, from the ten cities with the most review</context>
</contexts>
<marker>Kong, Zhou, 2010</marker>
<rawString>Fang Kong and Guodong Zhou. 2010. A tree kernelbased unified framework for chinese zero anaphora resolution. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 882–891. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quoc V Le and Tomas Mikolov</author>
</authors>
<title>Distributed representations of sentences and documents. arXiv preprint arXiv:1405.4053.</title>
<date>2014</date>
<contexts>
<context position="10315" citStr="Mikolov, 2014" startWordPosition="1614" endWordPosition="1615">ontexts (such as reviews), referentiality resolution can be interpreted in part as a text classification task. Review N-grams These are binary features for the presence or absence of n-grams in the entire text of the review. We found that using a larger n than 1 caused overfitting on our relatively small dataset and reduced performance; therefore, results are reported using unigram features. Review Vector Embedding To see if we can induce higher-level representations of the review text than simply binary n-gram features, we also train a document-level distributed vector representation (Le and Mikolov, 2014) on the entire corpus of reviews using the “doc2vec” implementation in GENSIM (ˇReh˚uˇrek and Sojka, 2010), and include 200 vector features per review: a 100-dimensional embedding learned on the entire document, as well as a 100-dimensional average embedding calculated by averaging the vectors for each word in the document. In experiments we found using both the document and the average vectors combined resulted in higher performance than either alone, so we report results in this setting. Metadata In addition to discourse features, we also included features that encode the category, city, and</context>
</contexts>
<marker>Mikolov, 2014</marker>
<rawString>Quoc V Le and Tomas Mikolov. 2014. Distributed representations of sentences and documents. arXiv preprint arXiv:1405.4053.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Levy</author>
<author>Christopher Manning</author>
</authors>
<title>Is it harder to parse chinese, or the chinese treebank?</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1,</booktitle>
<pages>439--446</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6107" citStr="Levy and Manning, 2003" startWordPosition="938" endWordPosition="941">other particular interest of this task: explicit second-person pronominals are quite rare in Chinese, at least in this genre, making the reviews in which they appear linguistically marked. Summary statistics for this dataset are given in Table 1. We release all our data and annotations at nlp.stanford.edu/robvoigt/nis. 3.1 Preprocessing We apply the Stanford CRF Word Segmenter (Tseng et al., 2005) to segment the text of each review into words, and use simple heuristics based on whitespace and punctuation to extract sentences or sentence fragments. The Stanford Parser (Klein and Manning, 2003; Levy and Manning, 2003) is then run on each extracted sentence or fragment containing a ni to produce a dependency graph and set of part-of-speech (POS) tags for later use in feature extraction. 3.2 Annotation We hand-annotated 701 examples of ni tokens (including both singular and plural cases), placing them into one of seven categories: generic, writerreferential, reader-referential, shop-referential, idiomatic, non-“you”, and other. The idiomatic and non-“you” cases are commonly comprised of set phrases such as 1//F,ff (nihao, “hello”) or A1//F, (mini, “mini”) and are therefore relatively trivial to filter; and t</context>
</contexts>
<marker>Levy, Manning, 2003</marker>
<rawString>Roger Levy and Christopher Manning. 2003. Is it harder to parse chinese, or the chinese treebank? In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 439–446. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Luo</author>
</authors>
<title>Coreference or not: A twin model for coreference resolution. In</title>
<date>2007</date>
<booktitle>HLT-NAACL,</booktitle>
<pages>73--80</pages>
<contexts>
<context position="1537" citStr="Luo, 2007" startWordPosition="225" endWordPosition="226">ractical implications for audience extraction and other sentiment tasks in online reviews. 1 Introduction and Task Description Detecting whether a given entity is referential is an important question in computational discourse processing. Linguistic features in the local context of a given mention have been successfully used for determining whether a second-person pronoun (you) in dialogue is referential (Gupta et al., 2007b; Frampton et al., 2009; Purver et al., 2009). The related task of anaphoricity detection is an important subtask of coreference resolution (Ng and Cardie, 2002; Ng, 2004; Luo, 2007; Zhou and Kong, 2009; Recasens et al., 2013). In this paper we consider the task of audience identification in review texts, using restaurant reviews written in Chinese. Our task is to disambiguate a mention of the Chinese second-person pronoun ` (ni, “you”) into the following four labels that we found to occur commonly in reviews: Generic ������y�P�����{��` For drinks they only have Sprite and Coke, and you have to order before they’ll give them to you. Referential - Shop �H��������`�� With such good service, I’ll definitely come back to your shop next time! Referential - Reader Tá`ì �`ì �%M</context>
</contexts>
<marker>Luo, 2007</marker>
<rawString>Xiaoqiang Luo. 2007. Coreference or not: A twin model for coreference resolution. In HLT-NAACL, pages 73–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
<author>Claire Cardie</author>
</authors>
<title>Identifying anaphoric and non-anaphoric noun phrases to improve coreference resolution.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th international conference on Computational linguistics-Volume 1,</booktitle>
<pages>1--7</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1516" citStr="Ng and Cardie, 2002" startWordPosition="219" endWordPosition="222"> pronoun use, and may suggest practical implications for audience extraction and other sentiment tasks in online reviews. 1 Introduction and Task Description Detecting whether a given entity is referential is an important question in computational discourse processing. Linguistic features in the local context of a given mention have been successfully used for determining whether a second-person pronoun (you) in dialogue is referential (Gupta et al., 2007b; Frampton et al., 2009; Purver et al., 2009). The related task of anaphoricity detection is an important subtask of coreference resolution (Ng and Cardie, 2002; Ng, 2004; Luo, 2007; Zhou and Kong, 2009; Recasens et al., 2013). In this paper we consider the task of audience identification in review texts, using restaurant reviews written in Chinese. Our task is to disambiguate a mention of the Chinese second-person pronoun ` (ni, “you”) into the following four labels that we found to occur commonly in reviews: Generic ������y�P�����{��` For drinks they only have Sprite and Coke, and you have to order before they’ll give them to you. Referential - Shop �H��������`�� With such good service, I’ll definitely come back to your shop next time! Referential </context>
</contexts>
<marker>Ng, Cardie, 2002</marker>
<rawString>Vincent Ng and Claire Cardie. 2002. Identifying anaphoric and non-anaphoric noun phrases to improve coreference resolution. In Proceedings of the 19th international conference on Computational linguistics-Volume 1, pages 1–7. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
</authors>
<title>Learning noun phrase anaphoricity to improve coreference resolution: Issues in representation and optimization.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>151</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1526" citStr="Ng, 2004" startWordPosition="223" endWordPosition="224"> suggest practical implications for audience extraction and other sentiment tasks in online reviews. 1 Introduction and Task Description Detecting whether a given entity is referential is an important question in computational discourse processing. Linguistic features in the local context of a given mention have been successfully used for determining whether a second-person pronoun (you) in dialogue is referential (Gupta et al., 2007b; Frampton et al., 2009; Purver et al., 2009). The related task of anaphoricity detection is an important subtask of coreference resolution (Ng and Cardie, 2002; Ng, 2004; Luo, 2007; Zhou and Kong, 2009; Recasens et al., 2013). In this paper we consider the task of audience identification in review texts, using restaurant reviews written in Chinese. Our task is to disambiguate a mention of the Chinese second-person pronoun ` (ni, “you”) into the following four labels that we found to occur commonly in reviews: Generic ������y�P�����{��` For drinks they only have Sprite and Coke, and you have to order before they’ll give them to you. Referential - Shop �H��������`�� With such good service, I’ll definitely come back to your shop next time! Referential - Reader T</context>
</contexts>
<marker>Ng, 2004</marker>
<rawString>Vincent Ng. 2004. Learning noun phrase anaphoricity to improve coreference resolution: Issues in representation and optimization. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, page 151. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pedregosa</author>
<author>G Varoquaux</author>
<author>A Gramfort</author>
<author>V Michel</author>
<author>B Thirion</author>
<author>O Grisel</author>
<author>M Blondel</author>
<author>P Prettenhofer</author>
<author>R Weiss</author>
<author>V Dubourg</author>
<author>J Vanderplas</author>
<author>A Passos</author>
<author>D Cournapeau</author>
<author>M Brucher</author>
<author>M Perrot</author>
<author>E Duchesnay</author>
</authors>
<title>Scikit-learn: Machine learning in Python.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2825</pages>
<contexts>
<context position="11287" citStr="Pedregosa et al., 2011" startWordPosition="1763" endWordPosition="1766"> using both the document and the average vectors combined resulted in higher performance than either alone, so we report results in this setting. Metadata In addition to discourse features, we also included features that encode the category, city, and estimated cost for each restaurant, as well as the service, taste, environment, and overall star rank ratings associated with a given review on a 5-point scale. 5 Experiments We tested the effectiveness of these features at predicting genericity and reference for each ni token with multinomial logistic regression, as implemented in SCIKIT-LEARN (Pedregosa et al., 2011). We used two classification settings: a binary prediction of whether a given ni is referential or not, and a four-way prediction including distinctions between the three annotated referential targets. The results for each task are shown in Table 3. In each case, we compare the performance of all local and discourse features, as well as several relevant subsets. One question we aim to address is whether our discourse-level n-gram and embed316 FEATURES BINARY FOUR-WAY Baseline 53.44% 46.56% Word ID 67.19% 62.19% + POS 74.84% 64.38% + Deps 75.00% 69.38% + Context 78.44% 72.19% N-grams 81.72% 77.</context>
</contexts>
<marker>Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos, Cournapeau, Brucher, Perrot, Duchesnay, 2011</marker>
<rawString>F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Purver</author>
<author>Raquel Fern´andez</author>
<author>Matthew Frampton</author>
<author>Stanley Peters</author>
</authors>
<title>Cascaded lexicalised classifiers for second-person reference resolution.</title>
<date>2009</date>
<booktitle>In Proceedings of the SIGDIAL 2009 Conference: The 10th Annual Meeting of the Special Interest Group on Discourse and Dialogue,</booktitle>
<pages>306--309</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Purver, Fern´andez, Frampton, Peters, 2009</marker>
<rawString>Matthew Purver, Raquel Fern´andez, Matthew Frampton, and Stanley Peters. 2009. Cascaded lexicalised classifiers for second-person reference resolution. In Proceedings of the SIGDIAL 2009 Conference: The 10th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 306– 309. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta Recasens</author>
<author>Marie-Catherine de Marneffe</author>
<author>Christopher Potts</author>
</authors>
<title>The life and death of discourse entities: Identifying singleton mentions.</title>
<date>2013</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>627--633</pages>
<marker>Recasens, de Marneffe, Potts, 2013</marker>
<rawString>Marta Recasens, Marie-Catherine de Marneffe, and Christopher Potts. 2013. The life and death of discourse entities: Identifying singleton mentions. In HLT-NAACL, pages 627–633.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radim ˇReh˚uˇrek</author>
<author>Petr Sojka</author>
</authors>
<title>Software Framework for Topic Modelling with Large Corpora.</title>
<date>2010</date>
<booktitle>In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks,</booktitle>
<pages>45--50</pages>
<location>Valletta, Malta,</location>
<note>ELRA. http://is. muni.cz/publication/884893/en.</note>
<marker>ˇReh˚uˇrek, Sojka, 2010</marker>
<rawString>Radim ˇReh˚uˇrek and Petr Sojka. 2010. Software Framework for Topic Modelling with Large Corpora. In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45– 50, Valletta, Malta, May. ELRA. http://is. muni.cz/publication/884893/en.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nils Reiter</author>
<author>Anette Frank</author>
</authors>
<title>Identifying generic noun phrases.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>40--49</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4020" citStr="Reiter and Frank (2010)" startWordPosition="607" endWordPosition="610">t Papers), pages 314–319, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics teractions significantly better than a simple baseline. Frampton et al. (2009) combine discourse features with acoustic and visual information for four-way interactions to resolve participant reference, and in the same setting Purver et al. (2009) employ cascaded classifiers that first establish referentiality and then attempt to resolve the referent. They show that utterance-level lexical features help, suggesting that different uses of ‘you’ are associated with distinct vocabularies. Reiter and Frank (2010) investigate the more general question of identifying genericity for noun phrases, showing the usefuleness of linguistic features such as syntactic dependency relations. Similar local structural cues like phrase-structure positioning, head word identity, and distance to surrounding clauses have been used as features in machine learning approaches for anaphoricity detection as one stage in a coreference resolution (Kong and Zhou, 2010; Zhou and Kong, 2011; Kong and Ng, 2013). Prior work has also shown improvements in performance in the dialogue domain from incorporating features having to do wi</context>
</contexts>
<marker>Reiter, Frank, 2010</marker>
<rawString>Nils Reiter and Anette Frank. 2010. Identifying generic noun phrases. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 40–49. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshinao Takemae</author>
<author>Shinji Ozawa</author>
</authors>
<title>Automatic addressee identification based on participants’ head orientation and utterances for multiparty conversations. In Multimedia and Expo,</title>
<date>2006</date>
<booktitle>IEEE International Conference on,</booktitle>
<pages>1285--1288</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="4715" citStr="Takemae and Ozawa, 2006" startWordPosition="716" endWordPosition="719">n phrases, showing the usefuleness of linguistic features such as syntactic dependency relations. Similar local structural cues like phrase-structure positioning, head word identity, and distance to surrounding clauses have been used as features in machine learning approaches for anaphoricity detection as one stage in a coreference resolution (Kong and Zhou, 2010; Zhou and Kong, 2011; Kong and Ng, 2013). Prior work has also shown improvements in performance in the dialogue domain from incorporating features having to do with acoustic prosody, gaze, and head movements (Jovanovi´c et al., 2006; Takemae and Ozawa, 2006; Gupta et al., 2007b; Frampton et al., 2009). Of course in the review domain we have no access to such information; as we’ll see, however, we can exploit other unique properties of reviews to make up for this lack. 3 Data We scrape reviews from dianping.com, a Chineselanguage restaurant review site, from the ten cities with the most reviews. We randomly sample 750 restaurants within each city and randomly sample reviews of those restaurants. We scraped 346,381 reviews, including all associated metadata (city, restaurant category, and cost) for each restaurant, as well as the provided ratings </context>
</contexts>
<marker>Takemae, Ozawa, 2006</marker>
<rawString>Yoshinao Takemae and Shinji Ozawa. 2006. Automatic addressee identification based on participants’ head orientation and utterances for multiparty conversations. In Multimedia and Expo, 2006 IEEE International Conference on, pages 1285–1288. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huihsin Tseng</author>
<author>Pichuan Chang</author>
<author>Galen Andrew</author>
<author>Daniel Jurafsky</author>
<author>Christopher Manning</author>
</authors>
<title>A conditional random field word segmenter.</title>
<date>2005</date>
<booktitle>In Proceedings of the fourth SIGHAN workshop on Chinese language Processing,</booktitle>
<volume>171</volume>
<contexts>
<context position="5884" citStr="Tseng et al., 2005" startWordPosition="901" endWordPosition="904"> each restaurant, as well as the provided ratings (service, taste, ambience, and overall stars) for each review. Of these reviews only 6,704 (less than 2%) have the second-person pronominal character ni, highlighting another particular interest of this task: explicit second-person pronominals are quite rare in Chinese, at least in this genre, making the reviews in which they appear linguistically marked. Summary statistics for this dataset are given in Table 1. We release all our data and annotations at nlp.stanford.edu/robvoigt/nis. 3.1 Preprocessing We apply the Stanford CRF Word Segmenter (Tseng et al., 2005) to segment the text of each review into words, and use simple heuristics based on whitespace and punctuation to extract sentences or sentence fragments. The Stanford Parser (Klein and Manning, 2003; Levy and Manning, 2003) is then run on each extracted sentence or fragment containing a ni to produce a dependency graph and set of part-of-speech (POS) tags for later use in feature extraction. 3.2 Annotation We hand-annotated 701 examples of ni tokens (including both singular and plural cases), placing them into one of seven categories: generic, writerreferential, reader-referential, shop-refere</context>
</contexts>
<marker>Tseng, Chang, Andrew, Jurafsky, Manning, 2005</marker>
<rawString>Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel Jurafsky, and Christopher Manning. 2005. A conditional random field word segmenter. In Proceedings of the fourth SIGHAN workshop on Chinese language Processing, volume 171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guodong Zhou</author>
<author>Fang Kong</author>
</authors>
<title>Global learning of noun phrase anaphoricity in coreference resolution via label propagation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume</booktitle>
<volume>2</volume>
<pages>978--986</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1558" citStr="Zhou and Kong, 2009" startWordPosition="227" endWordPosition="230">plications for audience extraction and other sentiment tasks in online reviews. 1 Introduction and Task Description Detecting whether a given entity is referential is an important question in computational discourse processing. Linguistic features in the local context of a given mention have been successfully used for determining whether a second-person pronoun (you) in dialogue is referential (Gupta et al., 2007b; Frampton et al., 2009; Purver et al., 2009). The related task of anaphoricity detection is an important subtask of coreference resolution (Ng and Cardie, 2002; Ng, 2004; Luo, 2007; Zhou and Kong, 2009; Recasens et al., 2013). In this paper we consider the task of audience identification in review texts, using restaurant reviews written in Chinese. Our task is to disambiguate a mention of the Chinese second-person pronoun ` (ni, “you”) into the following four labels that we found to occur commonly in reviews: Generic ������y�P�����{��` For drinks they only have Sprite and Coke, and you have to order before they’ll give them to you. Referential - Shop �H��������`�� With such good service, I’ll definitely come back to your shop next time! Referential - Reader Tá`ì �`ì �%M�! Go and try it if y</context>
</contexts>
<marker>Zhou, Kong, 2009</marker>
<rawString>Guodong Zhou and Fang Kong. 2009. Global learning of noun phrase anaphoricity in coreference resolution via label propagation. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2-Volume 2, pages 978–986. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guodong Zhou</author>
<author>Fang Kong</author>
</authors>
<title>Learning noun phrase anaphoricity in coreference resolution via label propagation.</title>
<date>2011</date>
<journal>Journal of Computer Science and Technology,</journal>
<volume>26</volume>
<issue>1</issue>
<contexts>
<context position="4478" citStr="Zhou and Kong, 2011" startWordPosition="677" endWordPosition="680"> They show that utterance-level lexical features help, suggesting that different uses of ‘you’ are associated with distinct vocabularies. Reiter and Frank (2010) investigate the more general question of identifying genericity for noun phrases, showing the usefuleness of linguistic features such as syntactic dependency relations. Similar local structural cues like phrase-structure positioning, head word identity, and distance to surrounding clauses have been used as features in machine learning approaches for anaphoricity detection as one stage in a coreference resolution (Kong and Zhou, 2010; Zhou and Kong, 2011; Kong and Ng, 2013). Prior work has also shown improvements in performance in the dialogue domain from incorporating features having to do with acoustic prosody, gaze, and head movements (Jovanovi´c et al., 2006; Takemae and Ozawa, 2006; Gupta et al., 2007b; Frampton et al., 2009). Of course in the review domain we have no access to such information; as we’ll see, however, we can exploit other unique properties of reviews to make up for this lack. 3 Data We scrape reviews from dianping.com, a Chineselanguage restaurant review site, from the ten cities with the most reviews. We randomly sample</context>
</contexts>
<marker>Zhou, Kong, 2011</marker>
<rawString>Guodong Zhou and Fang Kong. 2011. Learning noun phrase anaphoricity in coreference resolution via label propagation. Journal of Computer Science and Technology, 26(1):34–44.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>