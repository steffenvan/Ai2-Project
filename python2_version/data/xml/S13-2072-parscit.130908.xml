<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.149365">
<title confidence="0.972149">
SAIL: A hybrid approach to sentiment analysis
</title>
<author confidence="0.998575">
Nikolaos Malandrakis1, Abe Kazemzadeh2, Alexandros Potamianos3, Shrikanth Narayanan1
</author>
<affiliation confidence="0.985549333333333">
1 Signal Analysis and Interpretation Laboratory (SAIL), USC, Los Angeles, CA 90089, USA
2 Annenberg Innovation Laboratory (AIL), USC, Los Angeles, CA 90089, USA
3Department of ECE, Technical University of Crete, 73100 Chania, Greece
</affiliation>
<email confidence="0.994096">
malandra@usc.edu, kazemzad@usc.edu, potam@telecom.tuc.gr, shri@sipi.usc.edu
</email>
<sectionHeader confidence="0.995622" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999748263157895">
This paper describes our submission for Se-
mEval2013 Task 2: Sentiment Analysis in
Twitter. For the limited data condition we use
a lexicon-based model. The model uses an af-
fective lexicon automatically generated from a
very large corpus of raw web data. Statistics
are calculated over the word and bigram af-
fective ratings and used as features of a Naive
Bayes tree model. For the unconstrained data
scenario we combine the lexicon-based model
with a classifier built on maximum entropy
language models and trained on a large exter-
nal dataset. The two models are fused at the
posterior level to produce a final output. The
approach proved successful, reaching rank-
ings of 9th and 4th in the twitter sentiment
analysis constrained and unconstrained sce-
nario respectively, despite using only lexical
features.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999981934782609">
The analysis of the emotional content of text, is
relevant to numerous natural language processing
(NLP), web and multi-modal dialogue applications.
To that end there has been a significant scientific
effort towards tasks like product review analysis
(Wiebe and Mihalcea, 2006; Hu and Liu, 2004),
speech emotion extraction (Lee and Narayanan,
2005; Lee et al., 2002; Ang et al., 2002) and pure
text word (Esuli and Sebastiani, 2006; Strappar-
ava and Valitutti, 2004) and sentence (Turney and
Littman, 2002; Turney and Littman, 2003) level
emotion extraction.
The rise of social media in recent years has seen
a shift in research focus towards them, particularly
twitter. The large volume of text data available is
particularly useful, since it allows the use of com-
plex machine learning methods. Also important is
the interest on the part of companies that are actively
looking for ways to mine social media for opinions
and attitudes towards them and their products. Sim-
ilarly, in journalism there is interest in sentiment
analysis for a way to process and report on the public
opinion about current events (Petulla, 2013).
Analyzing emotion expressed in twitter borrows
from other tasks related to affective analysis, but
also presents unique challenges. One common is-
sue is the breadth of content available in twitter: a
more limited domain would make the task easier,
however there are no such bounds. There is also a
significant difference in the form of language used
in tweets. The tone is informal and typographical
and grammatical errors are very common, making
even simple tasks, like Part-of-Speech tagging much
harder. Features like hashtags and emoticons can
also be helpful (Davidov et al., 2010).
This paper describes our submissions for Se-
mEval 2013 task 2, subtask B, which deals pri-
marily with sentiment analysis in twitter. For the
constrained condition (using only the organizer-
provided twitter sentences) we implemented a sys-
tem based on the use of an affective lexicon and part-
of-speech tag information, which has been shown
relevant to the task (Pak and Paroubek, 2010).
For the unconstrained condition (including external
sources of twitter sentences) we combine the con-
strained model with a maximum entropy language
</bodyText>
<page confidence="0.976231">
438
</page>
<bodyText confidence="0.924883333333333">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 438–442, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics
model trained on external data.
</bodyText>
<sectionHeader confidence="0.855361" genericHeader="method">
2 Experimental procedure
</sectionHeader>
<bodyText confidence="0.999924">
We use two separate models, one for the constrained
condition and a combination for the unconstrained
condition. Following are short descriptions.
</bodyText>
<subsectionHeader confidence="0.911816">
2.1 Lexicon-based model
</subsectionHeader>
<bodyText confidence="0.999952">
The method used for the constrained condition is
based on an affective lexicon containing out-of-
context affective ratings for all terms contained in
each sentence. We use an automated algorithm of
affective lexicon expansion based on the one pre-
sented in (Malandrakis et al., 2011), which in turn
is an expansion of (Turney and Littman, 2002).
We assume that the continuous (in [−1, 1]) va-
lence and arousal ratings of any term can be repre-
sented as a linear combination of its semantic simi-
larities to a set of seed words and the affective rat-
ings of these words, as follows:
</bodyText>
<equation confidence="0.990148666666667">
N
v(wj) = a0 + az v(wz) dzj, (1)
z=1
</equation>
<bodyText confidence="0.999977480769231">
where wj is the term we mean to characterize,
w1...wN are the seed words, v(wz) is the valence rat-
ing for seed word wz, az is the weight corresponding
to seed word wz (that is estimated as described next),
dzj is a measure of semantic similarity between wz
and wj. For the purposes of this work, the seman-
tic similarity metric is the cosine similarity between
context vectors computed over a corpus of 116 mil-
lion web snippets collected by posing one query for
every word in the Aspell spellchecker’s vocabulary
to the Yahoo! search engine and collecting up to 500
of the top results.
Given a starting, manually annotated, lexicon we
can select part of it to serve as seed words and then
use 1 to create a system of linear equations where
the only unknowns are the weights az. The system
is solved using Least Squares Estimation. That pro-
vides us with an equation that can generate affective
ratings for every term (not limited to words), as long
as we can estimate the semantic similarity between
it and the seed words.
Seed word selection is performed by a simple
heuristic (though validated through experiments):
we want seed words to have extreme affective rat-
ings (maximum absolute value) and we want the set
to be as closed to balanced as possible (sum of seed
ratings equal to zero).
Given these term ratings, the next step is combin-
ing them through statistics. To do that we use sim-
ple statistics (mean, min, max) and group by part
of speech tags. The results are statistics like “max-
imum valence among adjectives”, “mean arousal
among proper nouns” and “number of verbs and
nouns”. The dimensions used are: valence, absolute
valence and arousal. The grouping factors are the 39
Penn treebank pos tags plus higher order tags (adjec-
tives, verbs, nouns, adverbs and combinations of 2,3
and 4 of them). The statistics extracted are: mean,
min, max, most extreme, sum, number, percentage
of sentence coverage. In the case of bigram terms no
part-of-speech filtering/grouping is applied. These
statistics form the feature vectors.
Finally we perform feature selection on the mas-
sive set of candidates and use them to train a model.
The model selected is a Naive Bayes tree, a tree with
Naive Bayes classifiers on each leaf. The motivation
comes by considering this a two stage problem: sub-
jectivity detection and polarity classification, mak-
ing a hierarchical model a natural choice. NB trees
proved superior to other types of trees during our
testing, presumably due to the smoothing of obser-
vation distributions.
</bodyText>
<subsectionHeader confidence="0.998531">
2.2 N-gram language model
</subsectionHeader>
<bodyText confidence="0.999994176470588">
The method used for the unconstrained condition
is based on a combination of the automatically ex-
panded affective lexicon described in the previ-
ous section together with a bigram language model
based on the work of (Wang et al., 2012), which
uses a large set of twitter data from the U.S. 2012
Presidential election. As a part of the unconstrained
system, we were able to leverage external annotated
data apart from those provided by the SEMEVAL
2013 sentiment task dataset. Of the 315 million
tweets we collected about the election, we anno-
tated a subset of 40 thousand tweets using Ama-
zon Mechanical Turk. The annotation labels that
we used were “positive”, “negative”, “neutral”, and
“unsure”, and additionally raters could mark tweets
for sarcasm and humor. We excluded tweets marked
as “unsure” as well as tweets that had disagree-
</bodyText>
<page confidence="0.997061">
439
</page>
<bodyText confidence="0.999915071428571">
ment in labels if they were annotated by more than
one annotator. To extract the bigram features, we
used a twitter-specific tokenizer (Potts, 2011), which
marked uniform resource locators (URLs), emoti-
cons, and repeated characters, and which lowercased
words that began with capital letters followed by
lowercase letters (but left words in all capitals). The
bigram features were computed as presence or ab-
sense in the tweet rather than counts due to the small
number of words in tweets. The machine learning
model used to classify the tweets was the Megam
maximum entropy classifier (Daum´e III, 2004) in
the Natural Language Toolkit (NLTK) (Bird et al.,
2009).
</bodyText>
<subsectionHeader confidence="0.966675">
2.3 Fusion
</subsectionHeader>
<bodyText confidence="0.9999712">
The submitted system for the unconstrained condi-
tion leverages both the lexicon-based and bigram
language models. Due to the very different nature
of the models we opt to not fuse them at the feature
level, using a late fusion scheme instead. Both par-
tial models are probabilistic, therefore we can use
their per-class posterior probabilities as features of
a fusion model. The fusion model is a linear kernel
SVM using six features, the three posteriors from
each partial model, and trained on held out data.
</bodyText>
<sectionHeader confidence="0.999967" genericHeader="evaluation">
3 Results
</sectionHeader>
<bodyText confidence="0.999765857142857">
Following are results from our method, evaluated
on the testing sets (of sms and twitter posts) of
SemEval2013 task 2. We evaluate in terms of 3-
class classification, polarity classification (positive
vs. negative) and subjectivity detection (neutral vs.
other). Results shown in terms of per category f-
measure.
</bodyText>
<subsectionHeader confidence="0.991111">
3.1 Constrained
</subsectionHeader>
<bodyText confidence="0.9997462">
The preprocessing required for the lexicon-based
model is just part-of-speech tagging using Treetag-
ger (Schmid, 1994). The lexicon expansion method
is used to generate valence and arousal ratings for
all words and ngrams in all datasets and the part of
speech tags are used as grouping criteria to gener-
ate statistics. Finally, feature selection is performed
using a correlation criterion (Hall, 1999) and the re-
sulting feature set is used to train a Naive Bayes
tree model. The feature selection and model train-
</bodyText>
<tableCaption confidence="0.72033">
Table 1: F-measure results for the lexicon-based model,
using different machine learning methods, evaluated on
the 3-class twitter testing data.
</tableCaption>
<table confidence="0.999463166666667">
model per-class F-measure
neg neu pos
Nbayes 0.494 0.652 0.614
SVM 0.369 0.677 0.583
CART 0.430 0.676 0.593
NBTree 0.561 0.662 0.643
</table>
<tableCaption confidence="0.9985385">
Table 2: F-measure results for the constrained condition,
evaluated on the testing data.
</tableCaption>
<table confidence="0.87504625">
set classes per-class F-measure
neg neu pos/other
twitter 3-class 0.561 0.662 0.643
pos vs neg 0.679 0.858
neu vs other 0.685 0.699
sms 3-class 0.506 0.709 0.531
pos vs neg 0.688 0.755
neu vs other 0.730 0.628
</table>
<bodyText confidence="0.998876826086956">
ing/classification was conducted using Weka (Wit-
ten and Frank, 2000).
The final model uses a total of 72 features, which
can not be listed here due to space constraints. The
vast majority of these features are necessary to de-
tect the neutral category: positive-negative separa-
tion can be achieved with under 30 features.
One aspect of the model we felt worth investigat-
ing, was the type of model to be used. Using a multi-
stage model, performing subjectivity detection be-
fore positive-negative classification, has been shown
to provide an improvement, however single models
have also been used extensively. We compared some
popular models: Naive Bayes, linear kernel SVM,
CART-trained tree and Naive Bayes tree, all using
the same features, on the twitter part of the SemEval
testing data. The results are shown in Table 1. The
two Naive Bayes-based models proved significantly
better, with NBTree being clearly the best model for
these features.
Results from the submitted constrained model are
shown in Table 2. Looking at the twitter data re-
sults and comparing the positive-negative vs the
</bodyText>
<page confidence="0.99446">
440
</page>
<bodyText confidence="0.999727588235294">
3-class results, it appears the main weakness of
this model is subjectivity detection, mostly on the
neutral-negative side. It is not entirely clear to us
whether that is an artifact of the model (the nega-
tive class has the lowest prior probability, thus may
suffer compared to neutral) or of the more complex
forms of negativity (sarcasm, irony) which we do not
directly address. There is a definite drop in perfor-
mance when using the same twitter-trained model on
sms data, which we would not expect, given that the
features used are not twitter-specific. We believe this
gap is caused by lower part-of-speech tagger perfor-
mance: visual inspection reveals the output on twit-
ter data is fairly bad.
Overall this model ranked 9th out of 35 in the
twitter set and 11th out of 28 in the sms set, among
all constrained submissions.
</bodyText>
<subsectionHeader confidence="0.983251">
3.2 Unconstrained
</subsectionHeader>
<tableCaption confidence="0.992437">
Table 3: F-measure results for the maximum entropy
model with bigram features, evaluated on the testing data.
</tableCaption>
<table confidence="0.996289">
set classes per-class F-measure
neg neu pos/other
twitter 3-class 0.403 0.661 0.623
pos vs neg 0.586 0.804
neu vs other 0.661 0.704
sms 3-class 0.390 0.587 0.542
pos vs neg 0.710 0.648
neu vs other 0.587 0.641
</table>
<tableCaption confidence="0.981286">
Table 4: F-measure results for the unconstrained condi-
tion, evaluated on the testing data.
</tableCaption>
<bodyText confidence="0.949011470588235">
set classes per-class F-measure
neg neu pos/other
twitter 3-class 0.565 0.679 0.655
pos vs neg 0.672 0.881
neu vs other 0.667 0.732
sms 3-class 0.502 0.723 0.538
pos vs neg 0.625 0.772
neu vs other 0.710 0.637
In order to create the submitted unconstrained
model we train an SVM model using the lexicon-
based and bigram language model posterior proba-
bilities as features. This fusion model is trained on
held-out data (the development set of the SemEval
data). The results of classification using the bigram
language model alone are shown in Table 3 and the
results from the final fused model are shown in Ta-
ble 4. Looking at relative per-class performance, the
results follow a form most similar to the constrained
model, though there are gains in all cases. These
gains are less significant when evaluated on the sms
data, resulting in a fair drop in ranks: the bigram lan-
guage model (expectedly) suffers more when mov-
ing to a different domain, since it uses words as
features rather than the more abstract affective rat-
ings used by the lexicon-based model. Also, because
the external data used to train the bigram language
model was from discussions of politics on Twitter,
the subject matter also varied in terms of prior senti-
ment distribution in that the negative class was pre-
dominant in politics, which resulted in high recall
but low precision for the negative class.
This model ranked 4th out of 16 in the twitter set
and 7th out of 17 in the sms set, among all uncon-
strained submissions.
</bodyText>
<sectionHeader confidence="0.999685" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999991722222222">
We presented a system of twitter sentiment analy-
sis combining two approaches: a hierarchical model
based on an affective lexicon and a language model-
ing approach, fused at the posterior level. The hier-
archical lexicon-based model proved very successful
despite using only n-gram affective ratings and part-
of-speech information. The language model was
not as good individually, but provided a noticeable
improvement to the lexicon-based model. Overall
the models achieved good performance, ranking 9th
of 35 and 4th of 16 in the constrained and uncon-
strained twitter experiments respectively, despite us-
ing only lexical information.
Future work will focus on incorporating im-
proved tokenization (including part-of-speech tag-
ging), making better use of twitter-specific features
like emoticons and hashtags, and performing affec-
tive lexicon generation on twitter data.
</bodyText>
<page confidence="0.998398">
441
</page>
<sectionHeader confidence="0.989779" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999414203125">
J. Ang, R. Dhillon, A. Krupski, E. Shriberg, and A. Stol-
cke. 2002. Prosody-based automatic detection of an-
noyance and frustration in human-computer dialog. In
Proc. ICSLP, pages 2037–2040.
S. Bird, E. Klein, and E. Loper. 2009. Natural Language
Processing with Python. O’Reilly Media.
H. Daum´e III. 2004. Notes on cg and lm-bfgs op-
timization of logistic regression. Paper available at
http://pub. hal3. name# daume04cg-bfgs, implementa-
tion available at http://hal3. name/megam.
D. Davidov, O. Tsur, and A. Rappoport. 2010. Enhanced
sentiment learning using twitter hashtags and smileys.
In Proc. COLING, pages 241–249.
A. Esuli and F. Sebastiani. 2006. Sentiwordnet: A pub-
licly available lexical resource for opinion mining. In
Proc. LREC, pages 417–422.
M. A. Hall. 1999. Correlation-based feature selection
for machine learning. Ph.D. thesis, The University of
Waikato.
M. Hu and B. Liu. 2004. Mining and summarizing cus-
tomer reviews. In Proc. SIGKDD, KDD ’04, pages
168–177. ACM.
C. M. Lee and S. Narayanan. 2005. Toward detecting
emotions in spoken dialogs. IEEE Transactions on
Speech and Audio Processing, 13(2):293–303.
C. M. Lee, S. Narayanan, and R. Pieraccini. 2002. Com-
bining acoustic and language information for emotion
recognition. In Proc. ICSLP, pages 873–876.
N. Malandrakis, A. Potamianos, E. Iosif, and
S. Narayanan. 2011. Kernel models for affec-
tive lexicon creation. In Proc. Interspeech, pages
2977–2980.
A. Pak and P. Paroubek. 2010. Twitter as a corpus
for sentiment analysis and opinion mining. In Proc.
LREC, pages 1320–1326.
S. Petulla. 2013. Feelings, nothing more than feelings:
The measured rise of sentiment analysis in journalism.
Neiman Journalism Lab, January.
C. Potts. 2011. Sentiment symposium tutorial: Tokeniz-
ing. Technical report, Stanford Linguistics.
H. Schmid. 1994. Probabilistic part-of-speech tagging
using decision trees. In Proc. International Confer-
ence on New Methods in Language Processing, vol-
ume 12, pages 44–49.
C. Strapparava and A. Valitutti. 2004. WordNet-Affect:
an affective extension of WordNet. In Proc. LREC,
volume 4, pages 1083–1086.
P. Turney and M. L. Littman. 2002. Unsupervised
Learning of Semantic Orientation from a Hundred-
Billion-Word Corpus. Technical report ERC-1094
(NRC 44929). National Research Council of Canada.
P. Turney and M. L. Littman. 2003. Measuring praise
and criticism: Inference of semantic orientation from
association. ACM Transactions on Information Sys-
tems, 21:315–346.
H. Wang, D. Can, A. Kazemzadeh, F. Bar, and
S. Narayanan. 2012. A system for real-time twitter
sentiment analysis of 2012 u.s. presidential election
cycle. In Proc. ACL, pages 115–120.
J. Wiebe and R. Mihalcea. 2006. Word sense and subjec-
tivity. In Proc. COLING/ACL, pages 1065–1072.
Ian H. Witten and Eibe Frank. 2000. Data Mining: Prac-
tical Machine Learning Tools and Techniques. Mor-
gan Kaufmann.
</reference>
<page confidence="0.998449">
442
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.461288">
<title confidence="0.999608">SAIL: A hybrid approach to sentiment analysis</title>
<author confidence="0.972171">Abe Alexandros Shrikanth</author>
<address confidence="0.832907">Analysis and Interpretation Laboratory (SAIL), USC, Los Angeles, CA 90089, Innovation Laboratory (AIL), USC, Los Angeles, CA 90089, of ECE, Technical University of Crete, 73100 Chania, Greece</address>
<email confidence="0.99981">malandra@usc.edu,kazemzad@usc.edu,potam@telecom.tuc.gr,shri@sipi.usc.edu</email>
<abstract confidence="0.9958815">This paper describes our submission for SemEval2013 Task 2: Sentiment Analysis in Twitter. For the limited data condition we use a lexicon-based model. The model uses an affective lexicon automatically generated from a very large corpus of raw web data. Statistics are calculated over the word and bigram affective ratings and used as features of a Naive Bayes tree model. For the unconstrained data scenario we combine the lexicon-based model with a classifier built on maximum entropy language models and trained on a large external dataset. The two models are fused at the posterior level to produce a final output. The approach proved successful, reaching rankings of 9th and 4th in the twitter sentiment analysis constrained and unconstrained scenario respectively, despite using only lexical features.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Ang</author>
<author>R Dhillon</author>
<author>A Krupski</author>
<author>E Shriberg</author>
<author>A Stolcke</author>
</authors>
<title>Prosody-based automatic detection of annoyance and frustration in human-computer dialog.</title>
<date>2002</date>
<booktitle>In Proc. ICSLP,</booktitle>
<pages>2037--2040</pages>
<contexts>
<context position="1656" citStr="Ang et al., 2002" startWordPosition="243" endWordPosition="246">l to produce a final output. The approach proved successful, reaching rankings of 9th and 4th in the twitter sentiment analysis constrained and unconstrained scenario respectively, despite using only lexical features. 1 Introduction The analysis of the emotional content of text, is relevant to numerous natural language processing (NLP), web and multi-modal dialogue applications. To that end there has been a significant scientific effort towards tasks like product review analysis (Wiebe and Mihalcea, 2006; Hu and Liu, 2004), speech emotion extraction (Lee and Narayanan, 2005; Lee et al., 2002; Ang et al., 2002) and pure text word (Esuli and Sebastiani, 2006; Strapparava and Valitutti, 2004) and sentence (Turney and Littman, 2002; Turney and Littman, 2003) level emotion extraction. The rise of social media in recent years has seen a shift in research focus towards them, particularly twitter. The large volume of text data available is particularly useful, since it allows the use of complex machine learning methods. Also important is the interest on the part of companies that are actively looking for ways to mine social media for opinions and attitudes towards them and their products. Similarly, in jou</context>
</contexts>
<marker>Ang, Dhillon, Krupski, Shriberg, Stolcke, 2002</marker>
<rawString>J. Ang, R. Dhillon, A. Krupski, E. Shriberg, and A. Stolcke. 2002. Prosody-based automatic detection of annoyance and frustration in human-computer dialog. In Proc. ICSLP, pages 2037–2040.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bird</author>
<author>E Klein</author>
<author>E Loper</author>
</authors>
<date>2009</date>
<booktitle>Natural Language Processing with Python. O’Reilly Media.</booktitle>
<contexts>
<context position="8653" citStr="Bird et al., 2009" startWordPosition="1397" endWordPosition="1400">e than one annotator. To extract the bigram features, we used a twitter-specific tokenizer (Potts, 2011), which marked uniform resource locators (URLs), emoticons, and repeated characters, and which lowercased words that began with capital letters followed by lowercase letters (but left words in all capitals). The bigram features were computed as presence or absense in the tweet rather than counts due to the small number of words in tweets. The machine learning model used to classify the tweets was the Megam maximum entropy classifier (Daum´e III, 2004) in the Natural Language Toolkit (NLTK) (Bird et al., 2009). 2.3 Fusion The submitted system for the unconstrained condition leverages both the lexicon-based and bigram language models. Due to the very different nature of the models we opt to not fuse them at the feature level, using a late fusion scheme instead. Both partial models are probabilistic, therefore we can use their per-class posterior probabilities as features of a fusion model. The fusion model is a linear kernel SVM using six features, the three posteriors from each partial model, and trained on held out data. 3 Results Following are results from our method, evaluated on the testing set</context>
</contexts>
<marker>Bird, Klein, Loper, 2009</marker>
<rawString>S. Bird, E. Klein, and E. Loper. 2009. Natural Language Processing with Python. O’Reilly Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Daum´e</author>
</authors>
<title>Notes on cg and lm-bfgs optimization of logistic regression. Paper available at http://pub. hal3. name# daume04cg-bfgs, implementation available at http://hal3. name/megam.</title>
<date>2004</date>
<marker>Daum´e, 2004</marker>
<rawString>H. Daum´e III. 2004. Notes on cg and lm-bfgs optimization of logistic regression. Paper available at http://pub. hal3. name# daume04cg-bfgs, implementation available at http://hal3. name/megam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Davidov</author>
<author>O Tsur</author>
<author>A Rappoport</author>
</authors>
<title>Enhanced sentiment learning using twitter hashtags and smileys.</title>
<date>2010</date>
<booktitle>In Proc. COLING,</booktitle>
<pages>241--249</pages>
<contexts>
<context position="2981" citStr="Davidov et al., 2010" startWordPosition="457" endWordPosition="460">out current events (Petulla, 2013). Analyzing emotion expressed in twitter borrows from other tasks related to affective analysis, but also presents unique challenges. One common issue is the breadth of content available in twitter: a more limited domain would make the task easier, however there are no such bounds. There is also a significant difference in the form of language used in tweets. The tone is informal and typographical and grammatical errors are very common, making even simple tasks, like Part-of-Speech tagging much harder. Features like hashtags and emoticons can also be helpful (Davidov et al., 2010). This paper describes our submissions for SemEval 2013 task 2, subtask B, which deals primarily with sentiment analysis in twitter. For the constrained condition (using only the organizerprovided twitter sentences) we implemented a system based on the use of an affective lexicon and partof-speech tag information, which has been shown relevant to the task (Pak and Paroubek, 2010). For the unconstrained condition (including external sources of twitter sentences) we combine the constrained model with a maximum entropy language 438 Second Joint Conference on Lexical and Computational Semantics (*</context>
</contexts>
<marker>Davidov, Tsur, Rappoport, 2010</marker>
<rawString>D. Davidov, O. Tsur, and A. Rappoport. 2010. Enhanced sentiment learning using twitter hashtags and smileys. In Proc. COLING, pages 241–249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Esuli</author>
<author>F Sebastiani</author>
</authors>
<title>Sentiwordnet: A publicly available lexical resource for opinion mining.</title>
<date>2006</date>
<booktitle>In Proc. LREC,</booktitle>
<pages>417--422</pages>
<contexts>
<context position="1703" citStr="Esuli and Sebastiani, 2006" startWordPosition="251" endWordPosition="254">ach proved successful, reaching rankings of 9th and 4th in the twitter sentiment analysis constrained and unconstrained scenario respectively, despite using only lexical features. 1 Introduction The analysis of the emotional content of text, is relevant to numerous natural language processing (NLP), web and multi-modal dialogue applications. To that end there has been a significant scientific effort towards tasks like product review analysis (Wiebe and Mihalcea, 2006; Hu and Liu, 2004), speech emotion extraction (Lee and Narayanan, 2005; Lee et al., 2002; Ang et al., 2002) and pure text word (Esuli and Sebastiani, 2006; Strapparava and Valitutti, 2004) and sentence (Turney and Littman, 2002; Turney and Littman, 2003) level emotion extraction. The rise of social media in recent years has seen a shift in research focus towards them, particularly twitter. The large volume of text data available is particularly useful, since it allows the use of complex machine learning methods. Also important is the interest on the part of companies that are actively looking for ways to mine social media for opinions and attitudes towards them and their products. Similarly, in journalism there is interest in sentiment analysis</context>
</contexts>
<marker>Esuli, Sebastiani, 2006</marker>
<rawString>A. Esuli and F. Sebastiani. 2006. Sentiwordnet: A publicly available lexical resource for opinion mining. In Proc. LREC, pages 417–422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Hall</author>
</authors>
<title>Correlation-based feature selection for machine learning.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>The University of Waikato.</institution>
<contexts>
<context position="9916" citStr="Hall, 1999" startWordPosition="1600" endWordPosition="1601">We evaluate in terms of 3- class classification, polarity classification (positive vs. negative) and subjectivity detection (neutral vs. other). Results shown in terms of per category fmeasure. 3.1 Constrained The preprocessing required for the lexicon-based model is just part-of-speech tagging using Treetagger (Schmid, 1994). The lexicon expansion method is used to generate valence and arousal ratings for all words and ngrams in all datasets and the part of speech tags are used as grouping criteria to generate statistics. Finally, feature selection is performed using a correlation criterion (Hall, 1999) and the resulting feature set is used to train a Naive Bayes tree model. The feature selection and model trainTable 1: F-measure results for the lexicon-based model, using different machine learning methods, evaluated on the 3-class twitter testing data. model per-class F-measure neg neu pos Nbayes 0.494 0.652 0.614 SVM 0.369 0.677 0.583 CART 0.430 0.676 0.593 NBTree 0.561 0.662 0.643 Table 2: F-measure results for the constrained condition, evaluated on the testing data. set classes per-class F-measure neg neu pos/other twitter 3-class 0.561 0.662 0.643 pos vs neg 0.679 0.858 neu vs other 0.</context>
</contexts>
<marker>Hall, 1999</marker>
<rawString>M. A. Hall. 1999. Correlation-based feature selection for machine learning. Ph.D. thesis, The University of Waikato.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hu</author>
<author>B Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proc. SIGKDD, KDD ’04,</booktitle>
<pages>168--177</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1567" citStr="Hu and Liu, 2004" startWordPosition="228" endWordPosition="231">s and trained on a large external dataset. The two models are fused at the posterior level to produce a final output. The approach proved successful, reaching rankings of 9th and 4th in the twitter sentiment analysis constrained and unconstrained scenario respectively, despite using only lexical features. 1 Introduction The analysis of the emotional content of text, is relevant to numerous natural language processing (NLP), web and multi-modal dialogue applications. To that end there has been a significant scientific effort towards tasks like product review analysis (Wiebe and Mihalcea, 2006; Hu and Liu, 2004), speech emotion extraction (Lee and Narayanan, 2005; Lee et al., 2002; Ang et al., 2002) and pure text word (Esuli and Sebastiani, 2006; Strapparava and Valitutti, 2004) and sentence (Turney and Littman, 2002; Turney and Littman, 2003) level emotion extraction. The rise of social media in recent years has seen a shift in research focus towards them, particularly twitter. The large volume of text data available is particularly useful, since it allows the use of complex machine learning methods. Also important is the interest on the part of companies that are actively looking for ways to mine s</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>M. Hu and B. Liu. 2004. Mining and summarizing customer reviews. In Proc. SIGKDD, KDD ’04, pages 168–177. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C M Lee</author>
<author>S Narayanan</author>
</authors>
<title>Toward detecting emotions in spoken dialogs.</title>
<date>2005</date>
<journal>IEEE Transactions on Speech and Audio Processing,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="1619" citStr="Lee and Narayanan, 2005" startWordPosition="235" endWordPosition="238"> two models are fused at the posterior level to produce a final output. The approach proved successful, reaching rankings of 9th and 4th in the twitter sentiment analysis constrained and unconstrained scenario respectively, despite using only lexical features. 1 Introduction The analysis of the emotional content of text, is relevant to numerous natural language processing (NLP), web and multi-modal dialogue applications. To that end there has been a significant scientific effort towards tasks like product review analysis (Wiebe and Mihalcea, 2006; Hu and Liu, 2004), speech emotion extraction (Lee and Narayanan, 2005; Lee et al., 2002; Ang et al., 2002) and pure text word (Esuli and Sebastiani, 2006; Strapparava and Valitutti, 2004) and sentence (Turney and Littman, 2002; Turney and Littman, 2003) level emotion extraction. The rise of social media in recent years has seen a shift in research focus towards them, particularly twitter. The large volume of text data available is particularly useful, since it allows the use of complex machine learning methods. Also important is the interest on the part of companies that are actively looking for ways to mine social media for opinions and attitudes towards them </context>
</contexts>
<marker>Lee, Narayanan, 2005</marker>
<rawString>C. M. Lee and S. Narayanan. 2005. Toward detecting emotions in spoken dialogs. IEEE Transactions on Speech and Audio Processing, 13(2):293–303.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C M Lee</author>
<author>S Narayanan</author>
<author>R Pieraccini</author>
</authors>
<title>Combining acoustic and language information for emotion recognition.</title>
<date>2002</date>
<booktitle>In Proc. ICSLP,</booktitle>
<pages>873--876</pages>
<contexts>
<context position="1637" citStr="Lee et al., 2002" startWordPosition="239" endWordPosition="242">the posterior level to produce a final output. The approach proved successful, reaching rankings of 9th and 4th in the twitter sentiment analysis constrained and unconstrained scenario respectively, despite using only lexical features. 1 Introduction The analysis of the emotional content of text, is relevant to numerous natural language processing (NLP), web and multi-modal dialogue applications. To that end there has been a significant scientific effort towards tasks like product review analysis (Wiebe and Mihalcea, 2006; Hu and Liu, 2004), speech emotion extraction (Lee and Narayanan, 2005; Lee et al., 2002; Ang et al., 2002) and pure text word (Esuli and Sebastiani, 2006; Strapparava and Valitutti, 2004) and sentence (Turney and Littman, 2002; Turney and Littman, 2003) level emotion extraction. The rise of social media in recent years has seen a shift in research focus towards them, particularly twitter. The large volume of text data available is particularly useful, since it allows the use of complex machine learning methods. Also important is the interest on the part of companies that are actively looking for ways to mine social media for opinions and attitudes towards them and their products</context>
</contexts>
<marker>Lee, Narayanan, Pieraccini, 2002</marker>
<rawString>C. M. Lee, S. Narayanan, and R. Pieraccini. 2002. Combining acoustic and language information for emotion recognition. In Proc. ICSLP, pages 873–876.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Malandrakis</author>
<author>A Potamianos</author>
<author>E Iosif</author>
<author>S Narayanan</author>
</authors>
<title>Kernel models for affective lexicon creation.</title>
<date>2011</date>
<booktitle>In Proc. Interspeech,</booktitle>
<pages>2977--2980</pages>
<contexts>
<context position="4276" citStr="Malandrakis et al., 2011" startWordPosition="650" endWordPosition="653">(SemEval 2013), pages 438–442, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics model trained on external data. 2 Experimental procedure We use two separate models, one for the constrained condition and a combination for the unconstrained condition. Following are short descriptions. 2.1 Lexicon-based model The method used for the constrained condition is based on an affective lexicon containing out-ofcontext affective ratings for all terms contained in each sentence. We use an automated algorithm of affective lexicon expansion based on the one presented in (Malandrakis et al., 2011), which in turn is an expansion of (Turney and Littman, 2002). We assume that the continuous (in [−1, 1]) valence and arousal ratings of any term can be represented as a linear combination of its semantic similarities to a set of seed words and the affective ratings of these words, as follows: N v(wj) = a0 + az v(wz) dzj, (1) z=1 where wj is the term we mean to characterize, w1...wN are the seed words, v(wz) is the valence rating for seed word wz, az is the weight corresponding to seed word wz (that is estimated as described next), dzj is a measure of semantic similarity between wz and wj. For</context>
</contexts>
<marker>Malandrakis, Potamianos, Iosif, Narayanan, 2011</marker>
<rawString>N. Malandrakis, A. Potamianos, E. Iosif, and S. Narayanan. 2011. Kernel models for affective lexicon creation. In Proc. Interspeech, pages 2977–2980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Pak</author>
<author>P Paroubek</author>
</authors>
<title>Twitter as a corpus for sentiment analysis and opinion mining.</title>
<date>2010</date>
<booktitle>In Proc. LREC,</booktitle>
<pages>1320--1326</pages>
<contexts>
<context position="3363" citStr="Pak and Paroubek, 2010" startWordPosition="520" endWordPosition="523">used in tweets. The tone is informal and typographical and grammatical errors are very common, making even simple tasks, like Part-of-Speech tagging much harder. Features like hashtags and emoticons can also be helpful (Davidov et al., 2010). This paper describes our submissions for SemEval 2013 task 2, subtask B, which deals primarily with sentiment analysis in twitter. For the constrained condition (using only the organizerprovided twitter sentences) we implemented a system based on the use of an affective lexicon and partof-speech tag information, which has been shown relevant to the task (Pak and Paroubek, 2010). For the unconstrained condition (including external sources of twitter sentences) we combine the constrained model with a maximum entropy language 438 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 438–442, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics model trained on external data. 2 Experimental procedure We use two separate models, one for the constrained condition and a combination for the unconstrained condition. Following are short descri</context>
</contexts>
<marker>Pak, Paroubek, 2010</marker>
<rawString>A. Pak and P. Paroubek. 2010. Twitter as a corpus for sentiment analysis and opinion mining. In Proc. LREC, pages 1320–1326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petulla</author>
</authors>
<title>Feelings, nothing more than feelings: The measured rise of sentiment analysis in journalism. Neiman Journalism Lab,</title>
<date>2013</date>
<contexts>
<context position="2394" citStr="Petulla, 2013" startWordPosition="366" endWordPosition="367">rney and Littman, 2003) level emotion extraction. The rise of social media in recent years has seen a shift in research focus towards them, particularly twitter. The large volume of text data available is particularly useful, since it allows the use of complex machine learning methods. Also important is the interest on the part of companies that are actively looking for ways to mine social media for opinions and attitudes towards them and their products. Similarly, in journalism there is interest in sentiment analysis for a way to process and report on the public opinion about current events (Petulla, 2013). Analyzing emotion expressed in twitter borrows from other tasks related to affective analysis, but also presents unique challenges. One common issue is the breadth of content available in twitter: a more limited domain would make the task easier, however there are no such bounds. There is also a significant difference in the form of language used in tweets. The tone is informal and typographical and grammatical errors are very common, making even simple tasks, like Part-of-Speech tagging much harder. Features like hashtags and emoticons can also be helpful (Davidov et al., 2010). This paper </context>
</contexts>
<marker>Petulla, 2013</marker>
<rawString>S. Petulla. 2013. Feelings, nothing more than feelings: The measured rise of sentiment analysis in journalism. Neiman Journalism Lab, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Potts</author>
</authors>
<title>Sentiment symposium tutorial: Tokenizing.</title>
<date>2011</date>
<tech>Technical report, Stanford Linguistics.</tech>
<contexts>
<context position="8139" citStr="Potts, 2011" startWordPosition="1316" endWordPosition="1317">erage external annotated data apart from those provided by the SEMEVAL 2013 sentiment task dataset. Of the 315 million tweets we collected about the election, we annotated a subset of 40 thousand tweets using Amazon Mechanical Turk. The annotation labels that we used were “positive”, “negative”, “neutral”, and “unsure”, and additionally raters could mark tweets for sarcasm and humor. We excluded tweets marked as “unsure” as well as tweets that had disagree439 ment in labels if they were annotated by more than one annotator. To extract the bigram features, we used a twitter-specific tokenizer (Potts, 2011), which marked uniform resource locators (URLs), emoticons, and repeated characters, and which lowercased words that began with capital letters followed by lowercase letters (but left words in all capitals). The bigram features were computed as presence or absense in the tweet rather than counts due to the small number of words in tweets. The machine learning model used to classify the tweets was the Megam maximum entropy classifier (Daum´e III, 2004) in the Natural Language Toolkit (NLTK) (Bird et al., 2009). 2.3 Fusion The submitted system for the unconstrained condition leverages both the l</context>
</contexts>
<marker>Potts, 2011</marker>
<rawString>C. Potts. 2011. Sentiment symposium tutorial: Tokenizing. Technical report, Stanford Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In Proc. International Conference on New Methods in Language Processing,</booktitle>
<volume>12</volume>
<pages>44--49</pages>
<contexts>
<context position="9632" citStr="Schmid, 1994" startWordPosition="1554" endWordPosition="1555">s of a fusion model. The fusion model is a linear kernel SVM using six features, the three posteriors from each partial model, and trained on held out data. 3 Results Following are results from our method, evaluated on the testing sets (of sms and twitter posts) of SemEval2013 task 2. We evaluate in terms of 3- class classification, polarity classification (positive vs. negative) and subjectivity detection (neutral vs. other). Results shown in terms of per category fmeasure. 3.1 Constrained The preprocessing required for the lexicon-based model is just part-of-speech tagging using Treetagger (Schmid, 1994). The lexicon expansion method is used to generate valence and arousal ratings for all words and ngrams in all datasets and the part of speech tags are used as grouping criteria to generate statistics. Finally, feature selection is performed using a correlation criterion (Hall, 1999) and the resulting feature set is used to train a Naive Bayes tree model. The feature selection and model trainTable 1: F-measure results for the lexicon-based model, using different machine learning methods, evaluated on the 3-class twitter testing data. model per-class F-measure neg neu pos Nbayes 0.494 0.652 0.6</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>H. Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proc. International Conference on New Methods in Language Processing, volume 12, pages 44–49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Strapparava</author>
<author>A Valitutti</author>
</authors>
<title>WordNet-Affect: an affective extension of WordNet.</title>
<date>2004</date>
<booktitle>In Proc. LREC,</booktitle>
<volume>4</volume>
<pages>1083--1086</pages>
<contexts>
<context position="1737" citStr="Strapparava and Valitutti, 2004" startWordPosition="255" endWordPosition="259">ing rankings of 9th and 4th in the twitter sentiment analysis constrained and unconstrained scenario respectively, despite using only lexical features. 1 Introduction The analysis of the emotional content of text, is relevant to numerous natural language processing (NLP), web and multi-modal dialogue applications. To that end there has been a significant scientific effort towards tasks like product review analysis (Wiebe and Mihalcea, 2006; Hu and Liu, 2004), speech emotion extraction (Lee and Narayanan, 2005; Lee et al., 2002; Ang et al., 2002) and pure text word (Esuli and Sebastiani, 2006; Strapparava and Valitutti, 2004) and sentence (Turney and Littman, 2002; Turney and Littman, 2003) level emotion extraction. The rise of social media in recent years has seen a shift in research focus towards them, particularly twitter. The large volume of text data available is particularly useful, since it allows the use of complex machine learning methods. Also important is the interest on the part of companies that are actively looking for ways to mine social media for opinions and attitudes towards them and their products. Similarly, in journalism there is interest in sentiment analysis for a way to process and report o</context>
</contexts>
<marker>Strapparava, Valitutti, 2004</marker>
<rawString>C. Strapparava and A. Valitutti. 2004. WordNet-Affect: an affective extension of WordNet. In Proc. LREC, volume 4, pages 1083–1086.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Turney</author>
<author>M L Littman</author>
</authors>
<title>Unsupervised Learning of Semantic Orientation from a HundredBillion-Word Corpus.</title>
<date>2002</date>
<tech>Technical report ERC-1094 (NRC 44929).</tech>
<institution>National Research Council of Canada.</institution>
<contexts>
<context position="1776" citStr="Turney and Littman, 2002" startWordPosition="262" endWordPosition="265">iment analysis constrained and unconstrained scenario respectively, despite using only lexical features. 1 Introduction The analysis of the emotional content of text, is relevant to numerous natural language processing (NLP), web and multi-modal dialogue applications. To that end there has been a significant scientific effort towards tasks like product review analysis (Wiebe and Mihalcea, 2006; Hu and Liu, 2004), speech emotion extraction (Lee and Narayanan, 2005; Lee et al., 2002; Ang et al., 2002) and pure text word (Esuli and Sebastiani, 2006; Strapparava and Valitutti, 2004) and sentence (Turney and Littman, 2002; Turney and Littman, 2003) level emotion extraction. The rise of social media in recent years has seen a shift in research focus towards them, particularly twitter. The large volume of text data available is particularly useful, since it allows the use of complex machine learning methods. Also important is the interest on the part of companies that are actively looking for ways to mine social media for opinions and attitudes towards them and their products. Similarly, in journalism there is interest in sentiment analysis for a way to process and report on the public opinion about current even</context>
<context position="4337" citStr="Turney and Littman, 2002" startWordPosition="661" endWordPosition="664">2013. c�2013 Association for Computational Linguistics model trained on external data. 2 Experimental procedure We use two separate models, one for the constrained condition and a combination for the unconstrained condition. Following are short descriptions. 2.1 Lexicon-based model The method used for the constrained condition is based on an affective lexicon containing out-ofcontext affective ratings for all terms contained in each sentence. We use an automated algorithm of affective lexicon expansion based on the one presented in (Malandrakis et al., 2011), which in turn is an expansion of (Turney and Littman, 2002). We assume that the continuous (in [−1, 1]) valence and arousal ratings of any term can be represented as a linear combination of its semantic similarities to a set of seed words and the affective ratings of these words, as follows: N v(wj) = a0 + az v(wz) dzj, (1) z=1 where wj is the term we mean to characterize, w1...wN are the seed words, v(wz) is the valence rating for seed word wz, az is the weight corresponding to seed word wz (that is estimated as described next), dzj is a measure of semantic similarity between wz and wj. For the purposes of this work, the semantic similarity metric is</context>
</contexts>
<marker>Turney, Littman, 2002</marker>
<rawString>P. Turney and M. L. Littman. 2002. Unsupervised Learning of Semantic Orientation from a HundredBillion-Word Corpus. Technical report ERC-1094 (NRC 44929). National Research Council of Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Turney</author>
<author>M L Littman</author>
</authors>
<title>Measuring praise and criticism: Inference of semantic orientation from association.</title>
<date>2003</date>
<journal>ACM Transactions on Information Systems,</journal>
<pages>21--315</pages>
<contexts>
<context position="1803" citStr="Turney and Littman, 2003" startWordPosition="266" endWordPosition="269"> and unconstrained scenario respectively, despite using only lexical features. 1 Introduction The analysis of the emotional content of text, is relevant to numerous natural language processing (NLP), web and multi-modal dialogue applications. To that end there has been a significant scientific effort towards tasks like product review analysis (Wiebe and Mihalcea, 2006; Hu and Liu, 2004), speech emotion extraction (Lee and Narayanan, 2005; Lee et al., 2002; Ang et al., 2002) and pure text word (Esuli and Sebastiani, 2006; Strapparava and Valitutti, 2004) and sentence (Turney and Littman, 2002; Turney and Littman, 2003) level emotion extraction. The rise of social media in recent years has seen a shift in research focus towards them, particularly twitter. The large volume of text data available is particularly useful, since it allows the use of complex machine learning methods. Also important is the interest on the part of companies that are actively looking for ways to mine social media for opinions and attitudes towards them and their products. Similarly, in journalism there is interest in sentiment analysis for a way to process and report on the public opinion about current events (Petulla, 2013). Analyzi</context>
</contexts>
<marker>Turney, Littman, 2003</marker>
<rawString>P. Turney and M. L. Littman. 2003. Measuring praise and criticism: Inference of semantic orientation from association. ACM Transactions on Information Systems, 21:315–346.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Wang</author>
<author>D Can</author>
<author>A Kazemzadeh</author>
<author>F Bar</author>
<author>S Narayanan</author>
</authors>
<title>A system for real-time twitter sentiment analysis of 2012 u.s. presidential election cycle.</title>
<date>2012</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>115--120</pages>
<contexts>
<context position="7386" citStr="Wang et al., 2012" startWordPosition="1190" endWordPosition="1193">aive Bayes tree, a tree with Naive Bayes classifiers on each leaf. The motivation comes by considering this a two stage problem: subjectivity detection and polarity classification, making a hierarchical model a natural choice. NB trees proved superior to other types of trees during our testing, presumably due to the smoothing of observation distributions. 2.2 N-gram language model The method used for the unconstrained condition is based on a combination of the automatically expanded affective lexicon described in the previous section together with a bigram language model based on the work of (Wang et al., 2012), which uses a large set of twitter data from the U.S. 2012 Presidential election. As a part of the unconstrained system, we were able to leverage external annotated data apart from those provided by the SEMEVAL 2013 sentiment task dataset. Of the 315 million tweets we collected about the election, we annotated a subset of 40 thousand tweets using Amazon Mechanical Turk. The annotation labels that we used were “positive”, “negative”, “neutral”, and “unsure”, and additionally raters could mark tweets for sarcasm and humor. We excluded tweets marked as “unsure” as well as tweets that had disagre</context>
</contexts>
<marker>Wang, Can, Kazemzadeh, Bar, Narayanan, 2012</marker>
<rawString>H. Wang, D. Can, A. Kazemzadeh, F. Bar, and S. Narayanan. 2012. A system for real-time twitter sentiment analysis of 2012 u.s. presidential election cycle. In Proc. ACL, pages 115–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wiebe</author>
<author>R Mihalcea</author>
</authors>
<title>Word sense and subjectivity.</title>
<date>2006</date>
<booktitle>In Proc. COLING/ACL,</booktitle>
<pages>1065--1072</pages>
<contexts>
<context position="1548" citStr="Wiebe and Mihalcea, 2006" startWordPosition="224" endWordPosition="227">mum entropy language models and trained on a large external dataset. The two models are fused at the posterior level to produce a final output. The approach proved successful, reaching rankings of 9th and 4th in the twitter sentiment analysis constrained and unconstrained scenario respectively, despite using only lexical features. 1 Introduction The analysis of the emotional content of text, is relevant to numerous natural language processing (NLP), web and multi-modal dialogue applications. To that end there has been a significant scientific effort towards tasks like product review analysis (Wiebe and Mihalcea, 2006; Hu and Liu, 2004), speech emotion extraction (Lee and Narayanan, 2005; Lee et al., 2002; Ang et al., 2002) and pure text word (Esuli and Sebastiani, 2006; Strapparava and Valitutti, 2004) and sentence (Turney and Littman, 2002; Turney and Littman, 2003) level emotion extraction. The rise of social media in recent years has seen a shift in research focus towards them, particularly twitter. The large volume of text data available is particularly useful, since it allows the use of complex machine learning methods. Also important is the interest on the part of companies that are actively looking</context>
</contexts>
<marker>Wiebe, Mihalcea, 2006</marker>
<rawString>J. Wiebe and R. Mihalcea. 2006. Word sense and subjectivity. In Proc. COLING/ACL, pages 1065–1072.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Eibe Frank</author>
</authors>
<title>Data Mining: Practical Machine Learning Tools and Techniques.</title>
<date>2000</date>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="10672" citStr="Witten and Frank, 2000" startWordPosition="1720" endWordPosition="1724">ults for the lexicon-based model, using different machine learning methods, evaluated on the 3-class twitter testing data. model per-class F-measure neg neu pos Nbayes 0.494 0.652 0.614 SVM 0.369 0.677 0.583 CART 0.430 0.676 0.593 NBTree 0.561 0.662 0.643 Table 2: F-measure results for the constrained condition, evaluated on the testing data. set classes per-class F-measure neg neu pos/other twitter 3-class 0.561 0.662 0.643 pos vs neg 0.679 0.858 neu vs other 0.685 0.699 sms 3-class 0.506 0.709 0.531 pos vs neg 0.688 0.755 neu vs other 0.730 0.628 ing/classification was conducted using Weka (Witten and Frank, 2000). The final model uses a total of 72 features, which can not be listed here due to space constraints. The vast majority of these features are necessary to detect the neutral category: positive-negative separation can be achieved with under 30 features. One aspect of the model we felt worth investigating, was the type of model to be used. Using a multistage model, performing subjectivity detection before positive-negative classification, has been shown to provide an improvement, however single models have also been used extensively. We compared some popular models: Naive Bayes, linear kernel SV</context>
</contexts>
<marker>Witten, Frank, 2000</marker>
<rawString>Ian H. Witten and Eibe Frank. 2000. Data Mining: Practical Machine Learning Tools and Techniques. Morgan Kaufmann.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>