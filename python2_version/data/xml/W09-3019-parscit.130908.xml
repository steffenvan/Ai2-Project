<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.361656">
<title confidence="0.983288">
Transducing Logical Relations from Automatic and Manual GLARF
</title>
<author confidence="0.819189333333333">
Adam Meyers†, Michiko Kosaka$, Heng Ji*, Nianwen Xue*,
Mary Harper°, Ang Sun†, Wei Xu† and Shasha Liao†† New York Univ., $Monmouth Univ., *Brandeis Univ,, *City Univ. of New York, °Johns
Hopkins Human Lang. Tech. Ctr. of Excellence &amp; U. of Maryland, College Park
</author>
<sectionHeader confidence="0.955584" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9993758">
GLARF relations are generated from tree-
bank and parses for English, Chinese and
Japanese. Our evaluation of system out-
put for these input types requires consid-
eration of multiple correct answers.1
</bodyText>
<sectionHeader confidence="0.999263" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999715476190476">
Systems, such as treebank-based parsers (Char-
niak, 2001; Collins, 1999) and semantic role la-
belers (Gildea and Jurafsky, 2002; Xue, 2008), are
trained and tested on hand-annotated data. Evalu-
ation is based on differences between system out-
put and test data. Other systems use these pro-
grams to perform tasks unrelated to the original
annotation. For example, participating systems in
CONLL (Surdeanu et al., 2008; Hajiˇc et al., 2009),
ACE and GALE tasks merged the results of sev-
eral processors (parsers, named entity recognizers,
etc.) not initially designed for the task at hand.
This paper discusses differences between hand-
annotated data and automatically generated data
with respect to our GLARFers, systems for gen-
erating Grammatical and Logical Representation
Framework (GLARF) for English, Chinese and
Japanese sentences. The paper describes GLARF
(Meyers et al., 2001; Meyers et al., 2009) and
GLARFers and compares GLARF produced from
treebank and parses.
</bodyText>
<sectionHeader confidence="0.997625" genericHeader="introduction">
2 GLARF
</sectionHeader>
<bodyText confidence="0.9967056">
Figure 1 includes simplified GLARF analyses for
English, Chinese and Japanese sentences. For
each sentence, a GLARFer constructs both a Fea-
ture Structure (FS) representing a constituency
analysis and a set of 31-tuples, each representing
</bodyText>
<footnote confidence="0.97454725">
1Support includes: NSF IIS-0534700 &amp; IIS-0534325
Structure Alignment-based MT; DARPA HR0011-06-C-
0023 &amp; HR0011-06-C-0023; CUNY REP &amp; GRTI Program.
This work does not necessarily reflect views of sponsors.
</footnote>
<bodyText confidence="0.998759585365854">
up to three dependency relations between pairs of
words. Due to space limitations, we will focus on
the 6 fields of the 31-tuple represented in Figure 1.
These include: (1) a functor (func); (2) the de-
pending argument (Arg); (3) a surface (Surf) la-
bel based on the position in the parse tree with no
regularizations; (4) a logic1 label (L¯1) for a re-
lation that reflects grammar-based regularizations
of the surface level. This marks relations for fill-
ing gaps in relative clauses or missing infinitival
subjects, represents passives as paraphrases as ac-
tives, etc. While the general framework supports
many regularizations, the relations actually repre-
sented depends on the implemented grammar, e.g.,
our current grammar of English regularizes across
passives and relative clauses, but our grammars
of Japanese and Chinese do not currently.; (5) a
logic2 label (L2) for Chinese and English, which
represents PropBank, NomBank and Penn Dis-
course Treebank relations; and (6) Asterisks (*)
indicate transparent relations, relations where the
functor inherits semantic properties of certain spe-
cial arguments (*CONJ, *OBJ, *PRD, *COMP).
Figure 1 contains several transparent relations.
The interpretation of the *CONJ relations in the
Japanese example, include not only that the nouns
[zaisan] (assets) and [seimei] (lives) are con-
joined, but also that these two nouns, together
form the object of the Japanese verb [mamoru]
(protect). Thus, for example, semantic selection
patterns should treat these nouns as possible ob-
jects for this verb. Transparent relations may serve
to neutralize some of the problematic cases of at-
tachment ambiguity. For example, in the English
sentence A number ofphrases with modifiers are
not ambiguous, there is a transparent *COMP re-
lation between numbers and of and a transpar-
ent *OBJ relation between of and phrases. Thus,
high attachment of the PP with modifiers, would
have the same interpretation as low attachment
since phrases is the underlying head of number of
</bodyText>
<page confidence="0.986097">
116
</page>
<note confidence="0.979965">
Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 116–120,
Suntec, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<figureCaption confidence="0.999903">
Figure 1: GLARF 5-tuples for 3 languages
</figureCaption>
<bodyText confidence="0.999962235294118">
phrases. In this same example, the adverb not can
be attached to either the copula are or the pred-
icative adjective, with no discernible difference in
meaning–this factor is indicated by the transparent
designation of the relations where the copula is a
functor. Transparent features also provide us with
a simple way of handling certain function words,
such as the Chinese word De which inherits the
function of its underlying head, connecting a vari-
ety of such modifiers to head nouns (an adjective
in the Chinese example.). For conjunction cases,
the number of underlying relations would multi-
ply, e.g., Mary and John bought and sold stock
would (underlyingly) have four subject relations
derived by pairing each of the underlying subject
nouns Mary and John with each of the underlying
main predicate verbs bought and sold.
</bodyText>
<sectionHeader confidence="0.978173" genericHeader="method">
3 Automatic vs. Manual Annotation
</sectionHeader>
<bodyText confidence="0.999970510638298">
Apart from accuracy, there are several other ways
that automatic and manual annotation differs. For
Penn-treebank (PTB) parsing, for example, most
parsers (not all) leave out function tags and empty
categories. Consistency is an important goal for
manual annotation for many reasons including: (1)
in the absence of a clear correct answer, consis-
tency helps clarify measures of annotation quality
(inter-annotator agreement scores); and (2) consis-
tent annotation is better training data for machine
learning. Thus, annotation specifications use de-
faults to ensure the consistent handling of spurious
ambiguity. For example, given a sentence like I
bought three acres of land in California, the PP in
California can be attached to either acres or land
with no difference in meaning. While annotation
guidelines may direct a human annotator to prefer,
for example, high attachment, systems output may
have other preferences, e.g., the probability that
land is modified by a PP (headed by in) versus the
probability that acres can be so modified.
Even if the manual annotation for a particular
corpus is consistent when it comes to other factors
such as tokenization or part of speech, developers
of parsers sometimes change these guidelines to
suit their needs. For example, users of the Char-
niak parser (Charniak, 2001) should add the AUX
category to the PTB parts of speech and adjust
their systems to account for the conversion of the
word ain’t into the tokens IS and n’t. Similarly, to-
kenization decisions with respect to hyphens vary
among different versions of the Penn Treebank, as
well as different parsers based on these treebanks.
Thus if a system uses multiple parsers, such differ-
ences must be accounted for. Differences that are
not important for a particular application should
be ignored (e.g., by merging alternative analyses).
For example, in the case of spurious attachment
ambiguity, a system may need to either accept both
as right answers or derive a common representa-
tion for both. Of course, many of the particular
problems that result from spurious ambiguity can
be accounted for in hind sight. Nevertheless, it
is precisely this lack of a controlled environment
which adds elements of spurious ambiguity. Us-
ing new processors or training on new treebanks
can bring new instances of spurious ambiguity.
</bodyText>
<sectionHeader confidence="0.996511" genericHeader="method">
4 Experiments and Evaluation
</sectionHeader>
<bodyText confidence="0.99995075">
We ran GLARFers on both manually created tree-
banks and automatically produced parses for En-
glish, Chinese and Japanese. For each corpus, we
created one or more answer keys by correcting
</bodyText>
<page confidence="0.990529">
117
</page>
<bodyText confidence="0.98404480952381">
system output. For this paper, we evaluate solely
on the logic1 relations (the second column in fig-
ure 1.) Figure 2 lists our results for all three lan-
guages, based on treebank and parser input.
As in (Meyers et al., 2009), we generated 4-
tuples consisting of the following for each depen-
dency: (A) the logic1 label (SBJ, OBJ, etc.), (B)
its transparency (True or False), (C) The functor (a
single word or a named entity); and (D) the argu-
ment (a single word or a named entity). In the case
of conjunction where there was no lexical con-
junction word, we used either punctuation (com-
mas or semi-colons) or the placeholder *NULL*.
We then corrected these results by hand to produce
the answer key–an answer was correct if all four
members of the tuple were correct and incorrect
otherwise. Table 2 provides the Precision, Recall
and F-scores for our output. The F-T columns
indicates a modified F-score derived by ignoring
the +/-Transparent distinction (resulting changes
in precision, recall and F-score are the same).
For English and Japanese, an expert native
speaking linguist corrected the output. For Chi-
nese, several native speaking computational lin-
guists shared the task. By checking compatibil-
ity of the answer keys with outputs derived from
different sources (parser, treebank), we could de-
tect errors and inconsistencies. We processed the
following corpora. English: 86 sentence article
(wsj 2300) from the Wall Street Journal PTB test
corpus (WSJ); 46 sentence letter from Good Will
(LET), the first 100 sentences of a switchboard
telephone transcript (TEL) and the first 100 sen-
tences of a narrative from the Charlotte Narra-
tive and Conversation (NAR). These samples are
taken from the PTB WSJ Corpus and the SIGANN
shared subcorpus of the OANC. The filenames are:
110CYL067, NapierDianne and sw2014. Chi-
nese: a 20 sentence sample of text from the
Penn Chinese Treebank (CTB) (Xue et al., 2005).
Japanese: 20 sentences from the Kyoto Corpus
(KYO) (Kurohashi and Nagao, 1998)
</bodyText>
<sectionHeader confidence="0.964205" genericHeader="method">
5 Running the GLARFer Programs
</sectionHeader>
<bodyText confidence="0.999917655172414">
We use Charniak, UMD and KNP parsers (Char-
niak, 2001; Huang and Harper, 2009; Kurohashi
and Nagao, 1998), JET Named Entity tagger (Gr-
ishman et al., 2005; Ji and Grishman, 2006)
and other resources in conjunction with language-
specific GLARFers that incorporate hand-written
rules to convert output of these processors into
a final representation, including logic1 struc-
ture, the focus of this paper. English GLAR-
Fer rules use Comlex (Macleod et al., 1998a)
and the various NomBank lexicons (http://
nlp.cs.nyu.edu/meyers/nombank/) for
lexical lookup. The GLARF rules implemented
vary by language as follows. English: cor-
recting/standardizing phrase boundaries and part
of speech (POS); recognizing multiword expres-
sions; marking subconstituents; labeling rela-
tions; incorporating NEs; regularizing infiniti-
val, passives, relatives, VP deletion, predica-
tive and numerous other constructions. Chi-
nese: correcting/standardizing phrase boundaries
and POS, marking subconstituents, labeling rela-
tions; regularizing copula constructions; incorpo-
rating NEs; recognizing dates and number expres-
sions. Japanese: converting to PTB format; cor-
recting/standardizing phrase boundaries and POS;
labeling relations; processing NEs, double quote
constructions, number phrases, common idioms,
light verbs and copula constructions.
</bodyText>
<sectionHeader confidence="0.99943" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999893814814815">
Naturally, the treebank-based system out-
performed parse-based system. The Charniak
parser for English was trained on the Wall Street
Journal corpus and can achieve about 90% accu-
racy on similar corpora, but lower accuracy on
other genres. Differences between treebank and
parser results for English were higher for LET and
NAR genres than for the TEL because the system
is not currently designed to handle TEL-specific
features like disfluencies. All processors were
trained on or initially designed for news corpora.
Thus corpora out of this domain usually produce
lower results. LET was easier as it consisted
mainly of short simple sentences. In (Meyers et
al., 2009), we evaluated our results on 40 Japanese
sentences from the JENAAD corpus (Utiyama
and Isahara, 2003) and achieved a higher F-score
(90.6%) relative to the Kyoto corpus, as JENAAD
tends to have fewer long complex sentences.
By using our answer key for multiple inputs, we
discovered errors and consequently improved the
quality of the answer keys. However, at times we
were also compelled to fork the answer keys–given
multiple correct answers, we needed to allow dif-
ferent answer keys corresponding to different in-
puts. For English, these items represent approxi-
mately 2% of the answer keys (there were a total
</bodyText>
<page confidence="0.994067">
118
</page>
<table confidence="0.999297476190476">
Treebank Parser
ID % Prec % Rec F F-T % Prec % Rec F F-T
WSJ 1238 1238 83.6 87.1 1164 1164 79.5 81.8
= 83.0 = 84.2 = 80.2 = 78.9
1491 1471 1452 1475
LET 419 419 92.6 93.3 390 390 87.8 87.8
= 92.9 = 92.3 = 89.9 = 85.9
451 454 434 454
TEL 478 478 78.6 82.2 439 439 74.7 77.4
= 76.2 = 81.2 = 74.8 = 74.5
627 589 587 589
NAR 817 817 82.3 84.1 724 724 75.2 76.1
=84.0
= 80.7 973 = 75.7 = 74.7
1013 957 969
CTB 351 351 88.4 88.7 352 352 83.7 83.7
= 87.8 = 89.1 = 87.3 = 80.4
400 394 403 438
KYO 525 525 91.1 91.1 493 493 85.5 87.8
= 91.3 = 91.0 = 84.9 = 86.2
575 577 581 572
</table>
<figureCaption confidence="0.994381714285714">
Figure 2: Logic1 Scores
Figure 3: Examples of Answer Key Divergences
of 74 4-tuples out of a total of 3487). Figure 3 lists
examples of answer key divergences that we have
found: (1) alternative tokenizations; (2) spurious
differences in attachment and conjunction scope;
and (3) ambiguities specific to our framework.
</figureCaption>
<bodyText confidence="0.999893651162791">
Examples 1 and 2 reflect different treatments of
hyphenation and contractions in treebank specifi-
cations over time. Parsers trained on different tree-
banks will either keep hyphenated words together
or separate more words at hyphens. The Treebank
treatment of can’t regularizes so that (can need
not be differentiated from ca), whereas the parser
treatment makes maintaining character offsets eas-
ier. In example 3, the Japanese parser recognizes
a single word whereas the treebank divides it into
a prefix plus stem. Example 4 is a case of differ-
ences in character encoding (zero).
Example 5 is a common case of spurious attach-
ment ambiguity for English, where a transparent
noun takes an of PP complement–nouns such as
form, variety and thousands bear the feature trans-
parent in the NOMLEX-PLUS dictionary (a Nom-
Bank dictionary based on NOMLEX (Macleod et
al., 1998b)). The relative clause attaches either
to the noun thousands or people and, therefore,
the subject gap of the relative is filled by either
thousands or people. This ambiguity is spurious
since there is no meaningful distinction between
these two attachments. Example 6 is a case of
attachment ambiguity due to a support construc-
tion (Meyers et al., 2004). The recipient of the
gift will be Goodwill regardless of whether the
PP is attached to give or gift. Thus there is not
much sense in marking one attachment more cor-
rect than the other. Example 7 is a case of conjunc-
tion ambiguity–the context does not make it clear
whether or not the pearls are part of a necklace or
just the beads are. The distinction is of little con-
sequence to the understanding of the narrative.
Example 8 is a case in which our grammar han-
dles a case ambiguously: the prenominal adjective
can be analyzed either as a simple noun plus ad-
jective phrase meaning various businesses or as a
noun plus relative clause meaning businesses that
are varied. Example 9 is a common case in Chi-
nese where the verb/noun distinction, while un-
clear, is not crucial to the meaning of the phrase –
under either interpretation, 5 billion was exported.
</bodyText>
<sectionHeader confidence="0.975727" genericHeader="conclusions">
7 Concluding Remarks
</sectionHeader>
<bodyText confidence="0.999968153846154">
We have discussed challenges of automatic an-
notation when transducers of other annotation
schemata are used as input. Models underly-
ing different transducers approximate the origi-
nal annotation in different ways, as do transduc-
ers trained on different corpora. We have found
it necessary to allow for multiple correct answers,
due to such differences, as well as, genuine and
spurious ambiguities. In the future, we intend to
investigate automatic ways of identifying and han-
dling spurious ambiguities which are predictable,
including examples like 5,6 and 7 in figure 3 in-
volving transparent functors.
</bodyText>
<page confidence="0.998688">
119
</page>
<sectionHeader confidence="0.989403" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999836873015873">
E. Charniak. 2001. Immediate-head parsing for lan-
guage models. In ACL 2001, pages 116–123.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
D. Gildea and D. Jurafsky. 2002. Automatic Label-
ing of Semantic Roles. Computational Linguistics,
28:245–288.
R. Grishman, D. Westbrook, and A. Meyers. 2005.
Nyu’s english ace 2005 system description. In ACE
2005 Evaluation Workshop.
J. Hajiˇc, M. Ciaramita, R. Johansson, D. Kawahara,
M. A. Marti, L. M`arquez, A. Meyers, J. Nivre,
S. Pad´o, J. ˇStˇep´anek, P. Straˇn´ak, M. Surdeanu,
N. Xue, and Y. Zhang. 2009. The CoNLL-2009
shared task: Syntactic and semantic dependencies in
multiple languages. In CoNLL-2009, Boulder, Col-
orado, USA.
Z. Huang and M. Harper. 2009. Self-training PCFG
Grammars with Latent Annotations across Lan-
guages. In EMNLP 2009.
H. Ji and R. Grishman. 2006. Analysis and Repair of
Name Tagger Errors. In COLING/ACL 2006, Syd-
ney, Australia.
S. Kurohashi and M. Nagao. 1998. Building a
Japanese parsed corpus while improving the pars-
ing system. In Proceedings of The 1st International
Conference on Language Resources &amp; Evaluation,
pages 719–724.
C. Macleod, R. Grishman, and A. Meyers. 1998a.
COMLEX Syntax. Computers and the Humanities,
31:459–481.
C. Macleod, R. Grishman, A. Meyers, L. Barrett, and
R. Reeves. 1998b. Nomlex: A lexicon of nominal-
izations. In Proceedings of Euralex98.
A. Meyers, M. Kosaka, S. Sekine, R. Grishman, and
S. Zhao. 2001. Parsing and GLARFing. In Pro-
ceedings ofRANLP-2001, Tzigov Chark, Bulgaria.
A. Meyers, R. Reeves, and Catherine Macleod. 2004.
NP-External Arguments: A Study of Argument
Sharing in English. In The ACL 2004 Workshop
on Multiword Expressions: Integrating Processing,
Barcelona, Spain.
A. Meyers, M. Kosaka, N. Xue, H. Ji, A. Sun, S. Liao,
and W. Xu. 2009. Automatic Recognition of Log-
ical Relations for English, Chinese and Japanese in
the GLARF Framework. In SEW-2009 at NAACL-
HLT-2009.
M. Surdeanu, R. Johansson, A. Meyers, L. M´arquez,
and J. Nivre. 2008. The CoNLL-2008 Shared Task
on Joint Parsing of Syntactic and Semantic Depen-
dencies. In Proceedings of the CoNLL-2008 Shared
Task, Manchester, GB.
M. Utiyama and H. Isahara. 2003. Reliable Mea-
sures for Aligning Japanese-English News Articles
and Sentences. In ACL-2003, pages 72–79.
N. Xue, F. Xia, F. Chiou, and M. Palmer. 2005. The
Penn Chinese Treebank: Phrase Structure Annota-
tion of a Large Corpus. Natural Language Engi-
neering.
N. Xue. 2008. Labeling Chinese Predicates with Se-
mantic roles. Computational Linguistics, 34:225–
255.
</reference>
<page confidence="0.996259">
120
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.490438">
<title confidence="0.999003">Transducing Logical Relations from Automatic and Manual GLARF</title>
<author confidence="0.917178333333333">of New York</author>
<author confidence="0.917178333333333">Hopkins Human Lang Tech Ctr of Excellence</author>
<author confidence="0.917178333333333">U of Maryland</author>
<author confidence="0.917178333333333">College Park</author>
<abstract confidence="0.927601166666667">GLARF relations are generated from treebank and parses for English, Chinese and Japanese. Our evaluation of system output for these input types requires considof multiple correct</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>Immediate-head parsing for language models.</title>
<date>2001</date>
<booktitle>In ACL</booktitle>
<pages>116--123</pages>
<contexts>
<context position="6333" citStr="Charniak, 2001" startWordPosition="985" endWordPosition="986">ifornia can be attached to either acres or land with no difference in meaning. While annotation guidelines may direct a human annotator to prefer, for example, high attachment, systems output may have other preferences, e.g., the probability that land is modified by a PP (headed by in) versus the probability that acres can be so modified. Even if the manual annotation for a particular corpus is consistent when it comes to other factors such as tokenization or part of speech, developers of parsers sometimes change these guidelines to suit their needs. For example, users of the Charniak parser (Charniak, 2001) should add the AUX category to the PTB parts of speech and adjust their systems to account for the conversion of the word ain’t into the tokens IS and n’t. Similarly, tokenization decisions with respect to hyphens vary among different versions of the Penn Treebank, as well as different parsers based on these treebanks. Thus if a system uses multiple parsers, such differences must be accounted for. Differences that are not important for a particular application should be ignored (e.g., by merging alternative analyses). For example, in the case of spurious attachment ambiguity, a system may nee</context>
<context position="9632" citStr="Charniak, 2001" startWordPosition="1529" endWordPosition="1531">J); 46 sentence letter from Good Will (LET), the first 100 sentences of a switchboard telephone transcript (TEL) and the first 100 sentences of a narrative from the Charlotte Narrative and Conversation (NAR). These samples are taken from the PTB WSJ Corpus and the SIGANN shared subcorpus of the OANC. The filenames are: 110CYL067, NapierDianne and sw2014. Chinese: a 20 sentence sample of text from the Penn Chinese Treebank (CTB) (Xue et al., 2005). Japanese: 20 sentences from the Kyoto Corpus (KYO) (Kurohashi and Nagao, 1998) 5 Running the GLARFer Programs We use Charniak, UMD and KNP parsers (Charniak, 2001; Huang and Harper, 2009; Kurohashi and Nagao, 1998), JET Named Entity tagger (Grishman et al., 2005; Ji and Grishman, 2006) and other resources in conjunction with languagespecific GLARFers that incorporate hand-written rules to convert output of these processors into a final representation, including logic1 structure, the focus of this paper. English GLARFer rules use Comlex (Macleod et al., 1998a) and the various NomBank lexicons (http:// nlp.cs.nyu.edu/meyers/nombank/) for lexical lookup. The GLARF rules implemented vary by language as follows. English: correcting/standardizing phrase boun</context>
</contexts>
<marker>Charniak, 2001</marker>
<rawString>E. Charniak. 2001. Immediate-head parsing for language models. In ACL 2001, pages 116–123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="617" citStr="Collins, 1999" startWordPosition="93" endWordPosition="94">nsducing Logical Relations from Automatic and Manual GLARF Adam Meyers†, Michiko Kosaka$, Heng Ji*, Nianwen Xue*, Mary Harper°, Ang Sun†, Wei Xu† and Shasha Liao†† New York Univ., $Monmouth Univ., *Brandeis Univ,, *City Univ. of New York, °Johns Hopkins Human Lang. Tech. Ctr. of Excellence &amp; U. of Maryland, College Park Abstract GLARF relations are generated from treebank and parses for English, Chinese and Japanese. Our evaluation of system output for these input types requires consideration of multiple correct answers.1 1 Introduction Systems, such as treebank-based parsers (Charniak, 2001; Collins, 1999) and semantic role labelers (Gildea and Jurafsky, 2002; Xue, 2008), are trained and tested on hand-annotated data. Evaluation is based on differences between system output and test data. Other systems use these programs to perform tasks unrelated to the original annotation. For example, participating systems in CONLL (Surdeanu et al., 2008; Hajiˇc et al., 2009), ACE and GALE tasks merged the results of several processors (parsers, named entity recognizers, etc.) not initially designed for the task at hand. This paper discusses differences between handannotated data and automatically generated </context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>M. Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
<author>D Jurafsky</author>
</authors>
<title>Automatic Labeling of Semantic Roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<pages>28--245</pages>
<contexts>
<context position="671" citStr="Gildea and Jurafsky, 2002" startWordPosition="100" endWordPosition="103">nd Manual GLARF Adam Meyers†, Michiko Kosaka$, Heng Ji*, Nianwen Xue*, Mary Harper°, Ang Sun†, Wei Xu† and Shasha Liao†† New York Univ., $Monmouth Univ., *Brandeis Univ,, *City Univ. of New York, °Johns Hopkins Human Lang. Tech. Ctr. of Excellence &amp; U. of Maryland, College Park Abstract GLARF relations are generated from treebank and parses for English, Chinese and Japanese. Our evaluation of system output for these input types requires consideration of multiple correct answers.1 1 Introduction Systems, such as treebank-based parsers (Charniak, 2001; Collins, 1999) and semantic role labelers (Gildea and Jurafsky, 2002; Xue, 2008), are trained and tested on hand-annotated data. Evaluation is based on differences between system output and test data. Other systems use these programs to perform tasks unrelated to the original annotation. For example, participating systems in CONLL (Surdeanu et al., 2008; Hajiˇc et al., 2009), ACE and GALE tasks merged the results of several processors (parsers, named entity recognizers, etc.) not initially designed for the task at hand. This paper discusses differences between handannotated data and automatically generated data with respect to our GLARFers, systems for generat</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>D. Gildea and D. Jurafsky. 2002. Automatic Labeling of Semantic Roles. Computational Linguistics, 28:245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Grishman</author>
<author>D Westbrook</author>
<author>A Meyers</author>
</authors>
<title>Nyu’s english ace 2005 system description.</title>
<date>2005</date>
<booktitle>In ACE 2005 Evaluation Workshop.</booktitle>
<contexts>
<context position="9732" citStr="Grishman et al., 2005" startWordPosition="1544" endWordPosition="1548">ne transcript (TEL) and the first 100 sentences of a narrative from the Charlotte Narrative and Conversation (NAR). These samples are taken from the PTB WSJ Corpus and the SIGANN shared subcorpus of the OANC. The filenames are: 110CYL067, NapierDianne and sw2014. Chinese: a 20 sentence sample of text from the Penn Chinese Treebank (CTB) (Xue et al., 2005). Japanese: 20 sentences from the Kyoto Corpus (KYO) (Kurohashi and Nagao, 1998) 5 Running the GLARFer Programs We use Charniak, UMD and KNP parsers (Charniak, 2001; Huang and Harper, 2009; Kurohashi and Nagao, 1998), JET Named Entity tagger (Grishman et al., 2005; Ji and Grishman, 2006) and other resources in conjunction with languagespecific GLARFers that incorporate hand-written rules to convert output of these processors into a final representation, including logic1 structure, the focus of this paper. English GLARFer rules use Comlex (Macleod et al., 1998a) and the various NomBank lexicons (http:// nlp.cs.nyu.edu/meyers/nombank/) for lexical lookup. The GLARF rules implemented vary by language as follows. English: correcting/standardizing phrase boundaries and part of speech (POS); recognizing multiword expressions; marking subconstituents; labelin</context>
</contexts>
<marker>Grishman, Westbrook, Meyers, 2005</marker>
<rawString>R. Grishman, D. Westbrook, and A. Meyers. 2005. Nyu’s english ace 2005 system description. In ACE 2005 Evaluation Workshop.</rawString>
</citation>
<citation valid="false">
<authors>
<author>J Hajiˇc</author>
<author>M Ciaramita</author>
<author>R Johansson</author>
<author>D Kawahara</author>
<author>M A Marti</author>
<author>L M`arquez</author>
<author>A Meyers</author>
<author>J Nivre</author>
<author>S Pad´o</author>
<author>J ˇStˇep´anek</author>
<author>P Straˇn´ak</author>
<author>M Surdeanu</author>
</authors>
<marker>Hajiˇc, Ciaramita, Johansson, Kawahara, Marti, M`arquez, Meyers, Nivre, Pad´o, ˇStˇep´anek, Straˇn´ak, Surdeanu, </marker>
<rawString>J. Hajiˇc, M. Ciaramita, R. Johansson, D. Kawahara, M. A. Marti, L. M`arquez, A. Meyers, J. Nivre, S. Pad´o, J. ˇStˇep´anek, P. Straˇn´ak, M. Surdeanu,</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Xue</author>
<author>Y Zhang</author>
</authors>
<title>The CoNLL-2009 shared task: Syntactic and semantic dependencies in multiple languages.</title>
<date>2009</date>
<booktitle>In CoNLL-2009,</booktitle>
<location>Boulder, Colorado, USA.</location>
<marker>Xue, Zhang, 2009</marker>
<rawString>N. Xue, and Y. Zhang. 2009. The CoNLL-2009 shared task: Syntactic and semantic dependencies in multiple languages. In CoNLL-2009, Boulder, Colorado, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Huang</author>
<author>M Harper</author>
</authors>
<title>Self-training PCFG Grammars with Latent Annotations across Languages. In EMNLP</title>
<date>2009</date>
<contexts>
<context position="9656" citStr="Huang and Harper, 2009" startWordPosition="1532" endWordPosition="1535">letter from Good Will (LET), the first 100 sentences of a switchboard telephone transcript (TEL) and the first 100 sentences of a narrative from the Charlotte Narrative and Conversation (NAR). These samples are taken from the PTB WSJ Corpus and the SIGANN shared subcorpus of the OANC. The filenames are: 110CYL067, NapierDianne and sw2014. Chinese: a 20 sentence sample of text from the Penn Chinese Treebank (CTB) (Xue et al., 2005). Japanese: 20 sentences from the Kyoto Corpus (KYO) (Kurohashi and Nagao, 1998) 5 Running the GLARFer Programs We use Charniak, UMD and KNP parsers (Charniak, 2001; Huang and Harper, 2009; Kurohashi and Nagao, 1998), JET Named Entity tagger (Grishman et al., 2005; Ji and Grishman, 2006) and other resources in conjunction with languagespecific GLARFers that incorporate hand-written rules to convert output of these processors into a final representation, including logic1 structure, the focus of this paper. English GLARFer rules use Comlex (Macleod et al., 1998a) and the various NomBank lexicons (http:// nlp.cs.nyu.edu/meyers/nombank/) for lexical lookup. The GLARF rules implemented vary by language as follows. English: correcting/standardizing phrase boundaries and part of speec</context>
</contexts>
<marker>Huang, Harper, 2009</marker>
<rawString>Z. Huang and M. Harper. 2009. Self-training PCFG Grammars with Latent Annotations across Languages. In EMNLP 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ji</author>
<author>R Grishman</author>
</authors>
<title>Analysis and Repair of Name Tagger Errors.</title>
<date>2006</date>
<booktitle>In COLING/ACL 2006,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="9756" citStr="Ji and Grishman, 2006" startWordPosition="1549" endWordPosition="1552"> the first 100 sentences of a narrative from the Charlotte Narrative and Conversation (NAR). These samples are taken from the PTB WSJ Corpus and the SIGANN shared subcorpus of the OANC. The filenames are: 110CYL067, NapierDianne and sw2014. Chinese: a 20 sentence sample of text from the Penn Chinese Treebank (CTB) (Xue et al., 2005). Japanese: 20 sentences from the Kyoto Corpus (KYO) (Kurohashi and Nagao, 1998) 5 Running the GLARFer Programs We use Charniak, UMD and KNP parsers (Charniak, 2001; Huang and Harper, 2009; Kurohashi and Nagao, 1998), JET Named Entity tagger (Grishman et al., 2005; Ji and Grishman, 2006) and other resources in conjunction with languagespecific GLARFers that incorporate hand-written rules to convert output of these processors into a final representation, including logic1 structure, the focus of this paper. English GLARFer rules use Comlex (Macleod et al., 1998a) and the various NomBank lexicons (http:// nlp.cs.nyu.edu/meyers/nombank/) for lexical lookup. The GLARF rules implemented vary by language as follows. English: correcting/standardizing phrase boundaries and part of speech (POS); recognizing multiword expressions; marking subconstituents; labeling relations; incorporati</context>
</contexts>
<marker>Ji, Grishman, 2006</marker>
<rawString>H. Ji and R. Grishman. 2006. Analysis and Repair of Name Tagger Errors. In COLING/ACL 2006, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kurohashi</author>
<author>M Nagao</author>
</authors>
<title>Building a Japanese parsed corpus while improving the parsing system.</title>
<date>1998</date>
<booktitle>In Proceedings of The 1st International Conference on Language Resources &amp; Evaluation,</booktitle>
<pages>719--724</pages>
<contexts>
<context position="9548" citStr="Kurohashi and Nagao, 1998" startWordPosition="1513" endWordPosition="1516">orpora. English: 86 sentence article (wsj 2300) from the Wall Street Journal PTB test corpus (WSJ); 46 sentence letter from Good Will (LET), the first 100 sentences of a switchboard telephone transcript (TEL) and the first 100 sentences of a narrative from the Charlotte Narrative and Conversation (NAR). These samples are taken from the PTB WSJ Corpus and the SIGANN shared subcorpus of the OANC. The filenames are: 110CYL067, NapierDianne and sw2014. Chinese: a 20 sentence sample of text from the Penn Chinese Treebank (CTB) (Xue et al., 2005). Japanese: 20 sentences from the Kyoto Corpus (KYO) (Kurohashi and Nagao, 1998) 5 Running the GLARFer Programs We use Charniak, UMD and KNP parsers (Charniak, 2001; Huang and Harper, 2009; Kurohashi and Nagao, 1998), JET Named Entity tagger (Grishman et al., 2005; Ji and Grishman, 2006) and other resources in conjunction with languagespecific GLARFers that incorporate hand-written rules to convert output of these processors into a final representation, including logic1 structure, the focus of this paper. English GLARFer rules use Comlex (Macleod et al., 1998a) and the various NomBank lexicons (http:// nlp.cs.nyu.edu/meyers/nombank/) for lexical lookup. The GLARF rules im</context>
</contexts>
<marker>Kurohashi, Nagao, 1998</marker>
<rawString>S. Kurohashi and M. Nagao. 1998. Building a Japanese parsed corpus while improving the parsing system. In Proceedings of The 1st International Conference on Language Resources &amp; Evaluation, pages 719–724.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Macleod</author>
<author>R Grishman</author>
<author>A Meyers</author>
</authors>
<date>1998</date>
<booktitle>COMLEX Syntax. Computers and the Humanities,</booktitle>
<pages>31--459</pages>
<contexts>
<context position="10033" citStr="Macleod et al., 1998" startWordPosition="1591" endWordPosition="1594">from the Penn Chinese Treebank (CTB) (Xue et al., 2005). Japanese: 20 sentences from the Kyoto Corpus (KYO) (Kurohashi and Nagao, 1998) 5 Running the GLARFer Programs We use Charniak, UMD and KNP parsers (Charniak, 2001; Huang and Harper, 2009; Kurohashi and Nagao, 1998), JET Named Entity tagger (Grishman et al., 2005; Ji and Grishman, 2006) and other resources in conjunction with languagespecific GLARFers that incorporate hand-written rules to convert output of these processors into a final representation, including logic1 structure, the focus of this paper. English GLARFer rules use Comlex (Macleod et al., 1998a) and the various NomBank lexicons (http:// nlp.cs.nyu.edu/meyers/nombank/) for lexical lookup. The GLARF rules implemented vary by language as follows. English: correcting/standardizing phrase boundaries and part of speech (POS); recognizing multiword expressions; marking subconstituents; labeling relations; incorporating NEs; regularizing infinitival, passives, relatives, VP deletion, predicative and numerous other constructions. Chinese: correcting/standardizing phrase boundaries and POS, marking subconstituents, labeling relations; regularizing copula constructions; incorporating NEs; rec</context>
<context position="13946" citStr="Macleod et al., 1998" startWordPosition="2234" endWordPosition="2237"> treatment of can’t regularizes so that (can need not be differentiated from ca), whereas the parser treatment makes maintaining character offsets easier. In example 3, the Japanese parser recognizes a single word whereas the treebank divides it into a prefix plus stem. Example 4 is a case of differences in character encoding (zero). Example 5 is a common case of spurious attachment ambiguity for English, where a transparent noun takes an of PP complement–nouns such as form, variety and thousands bear the feature transparent in the NOMLEX-PLUS dictionary (a NomBank dictionary based on NOMLEX (Macleod et al., 1998b)). The relative clause attaches either to the noun thousands or people and, therefore, the subject gap of the relative is filled by either thousands or people. This ambiguity is spurious since there is no meaningful distinction between these two attachments. Example 6 is a case of attachment ambiguity due to a support construction (Meyers et al., 2004). The recipient of the gift will be Goodwill regardless of whether the PP is attached to give or gift. Thus there is not much sense in marking one attachment more correct than the other. Example 7 is a case of conjunction ambiguity–the context </context>
</contexts>
<marker>Macleod, Grishman, Meyers, 1998</marker>
<rawString>C. Macleod, R. Grishman, and A. Meyers. 1998a. COMLEX Syntax. Computers and the Humanities, 31:459–481.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Macleod</author>
<author>R Grishman</author>
<author>A Meyers</author>
<author>L Barrett</author>
<author>R Reeves</author>
</authors>
<title>Nomlex: A lexicon of nominalizations.</title>
<date>1998</date>
<booktitle>In Proceedings of Euralex98.</booktitle>
<contexts>
<context position="10033" citStr="Macleod et al., 1998" startWordPosition="1591" endWordPosition="1594">from the Penn Chinese Treebank (CTB) (Xue et al., 2005). Japanese: 20 sentences from the Kyoto Corpus (KYO) (Kurohashi and Nagao, 1998) 5 Running the GLARFer Programs We use Charniak, UMD and KNP parsers (Charniak, 2001; Huang and Harper, 2009; Kurohashi and Nagao, 1998), JET Named Entity tagger (Grishman et al., 2005; Ji and Grishman, 2006) and other resources in conjunction with languagespecific GLARFers that incorporate hand-written rules to convert output of these processors into a final representation, including logic1 structure, the focus of this paper. English GLARFer rules use Comlex (Macleod et al., 1998a) and the various NomBank lexicons (http:// nlp.cs.nyu.edu/meyers/nombank/) for lexical lookup. The GLARF rules implemented vary by language as follows. English: correcting/standardizing phrase boundaries and part of speech (POS); recognizing multiword expressions; marking subconstituents; labeling relations; incorporating NEs; regularizing infinitival, passives, relatives, VP deletion, predicative and numerous other constructions. Chinese: correcting/standardizing phrase boundaries and POS, marking subconstituents, labeling relations; regularizing copula constructions; incorporating NEs; rec</context>
<context position="13946" citStr="Macleod et al., 1998" startWordPosition="2234" endWordPosition="2237"> treatment of can’t regularizes so that (can need not be differentiated from ca), whereas the parser treatment makes maintaining character offsets easier. In example 3, the Japanese parser recognizes a single word whereas the treebank divides it into a prefix plus stem. Example 4 is a case of differences in character encoding (zero). Example 5 is a common case of spurious attachment ambiguity for English, where a transparent noun takes an of PP complement–nouns such as form, variety and thousands bear the feature transparent in the NOMLEX-PLUS dictionary (a NomBank dictionary based on NOMLEX (Macleod et al., 1998b)). The relative clause attaches either to the noun thousands or people and, therefore, the subject gap of the relative is filled by either thousands or people. This ambiguity is spurious since there is no meaningful distinction between these two attachments. Example 6 is a case of attachment ambiguity due to a support construction (Meyers et al., 2004). The recipient of the gift will be Goodwill regardless of whether the PP is attached to give or gift. Thus there is not much sense in marking one attachment more correct than the other. Example 7 is a case of conjunction ambiguity–the context </context>
</contexts>
<marker>Macleod, Grishman, Meyers, Barrett, Reeves, 1998</marker>
<rawString>C. Macleod, R. Grishman, A. Meyers, L. Barrett, and R. Reeves. 1998b. Nomlex: A lexicon of nominalizations. In Proceedings of Euralex98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Meyers</author>
<author>M Kosaka</author>
<author>S Sekine</author>
<author>R Grishman</author>
<author>S Zhao</author>
</authors>
<title>Parsing and GLARFing.</title>
<date>2001</date>
<booktitle>In Proceedings ofRANLP-2001, Tzigov Chark,</booktitle>
<contexts>
<context position="1423" citStr="Meyers et al., 2001" startWordPosition="215" endWordPosition="218">Other systems use these programs to perform tasks unrelated to the original annotation. For example, participating systems in CONLL (Surdeanu et al., 2008; Hajiˇc et al., 2009), ACE and GALE tasks merged the results of several processors (parsers, named entity recognizers, etc.) not initially designed for the task at hand. This paper discusses differences between handannotated data and automatically generated data with respect to our GLARFers, systems for generating Grammatical and Logical Representation Framework (GLARF) for English, Chinese and Japanese sentences. The paper describes GLARF (Meyers et al., 2001; Meyers et al., 2009) and GLARFers and compares GLARF produced from treebank and parses. 2 GLARF Figure 1 includes simplified GLARF analyses for English, Chinese and Japanese sentences. For each sentence, a GLARFer constructs both a Feature Structure (FS) representing a constituency analysis and a set of 31-tuples, each representing 1Support includes: NSF IIS-0534700 &amp; IIS-0534325 Structure Alignment-based MT; DARPA HR0011-06-C0023 &amp; HR0011-06-C-0023; CUNY REP &amp; GRTI Program. This work does not necessarily reflect views of sponsors. up to three dependency relations between pairs of words. Due</context>
</contexts>
<marker>Meyers, Kosaka, Sekine, Grishman, Zhao, 2001</marker>
<rawString>A. Meyers, M. Kosaka, S. Sekine, R. Grishman, and S. Zhao. 2001. Parsing and GLARFing. In Proceedings ofRANLP-2001, Tzigov Chark, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Meyers</author>
<author>R Reeves</author>
<author>Catherine Macleod</author>
</authors>
<title>NP-External Arguments: A Study of Argument Sharing in English.</title>
<date>2004</date>
<booktitle>In The ACL 2004 Workshop on Multiword Expressions: Integrating Processing,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="14302" citStr="Meyers et al., 2004" startWordPosition="2292" endWordPosition="2295">n case of spurious attachment ambiguity for English, where a transparent noun takes an of PP complement–nouns such as form, variety and thousands bear the feature transparent in the NOMLEX-PLUS dictionary (a NomBank dictionary based on NOMLEX (Macleod et al., 1998b)). The relative clause attaches either to the noun thousands or people and, therefore, the subject gap of the relative is filled by either thousands or people. This ambiguity is spurious since there is no meaningful distinction between these two attachments. Example 6 is a case of attachment ambiguity due to a support construction (Meyers et al., 2004). The recipient of the gift will be Goodwill regardless of whether the PP is attached to give or gift. Thus there is not much sense in marking one attachment more correct than the other. Example 7 is a case of conjunction ambiguity–the context does not make it clear whether or not the pearls are part of a necklace or just the beads are. The distinction is of little consequence to the understanding of the narrative. Example 8 is a case in which our grammar handles a case ambiguously: the prenominal adjective can be analyzed either as a simple noun plus adjective phrase meaning various businesse</context>
</contexts>
<marker>Meyers, Reeves, Macleod, 2004</marker>
<rawString>A. Meyers, R. Reeves, and Catherine Macleod. 2004. NP-External Arguments: A Study of Argument Sharing in English. In The ACL 2004 Workshop on Multiword Expressions: Integrating Processing, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Meyers</author>
<author>M Kosaka</author>
<author>N Xue</author>
<author>H Ji</author>
<author>A Sun</author>
<author>S Liao</author>
<author>W Xu</author>
</authors>
<date>2009</date>
<booktitle>Automatic Recognition of Logical Relations for English, Chinese and Japanese in the GLARF Framework. In SEW-2009 at NAACLHLT-2009.</booktitle>
<contexts>
<context position="1445" citStr="Meyers et al., 2009" startWordPosition="219" endWordPosition="222">se programs to perform tasks unrelated to the original annotation. For example, participating systems in CONLL (Surdeanu et al., 2008; Hajiˇc et al., 2009), ACE and GALE tasks merged the results of several processors (parsers, named entity recognizers, etc.) not initially designed for the task at hand. This paper discusses differences between handannotated data and automatically generated data with respect to our GLARFers, systems for generating Grammatical and Logical Representation Framework (GLARF) for English, Chinese and Japanese sentences. The paper describes GLARF (Meyers et al., 2001; Meyers et al., 2009) and GLARFers and compares GLARF produced from treebank and parses. 2 GLARF Figure 1 includes simplified GLARF analyses for English, Chinese and Japanese sentences. For each sentence, a GLARFer constructs both a Feature Structure (FS) representing a constituency analysis and a set of 31-tuples, each representing 1Support includes: NSF IIS-0534700 &amp; IIS-0534325 Structure Alignment-based MT; DARPA HR0011-06-C0023 &amp; HR0011-06-C-0023; CUNY REP &amp; GRTI Program. This work does not necessarily reflect views of sponsors. up to three dependency relations between pairs of words. Due to space limitations,</context>
<context position="7782" citStr="Meyers et al., 2009" startWordPosition="1225" endWordPosition="1228"> this lack of a controlled environment which adds elements of spurious ambiguity. Using new processors or training on new treebanks can bring new instances of spurious ambiguity. 4 Experiments and Evaluation We ran GLARFers on both manually created treebanks and automatically produced parses for English, Chinese and Japanese. For each corpus, we created one or more answer keys by correcting 117 system output. For this paper, we evaluate solely on the logic1 relations (the second column in figure 1.) Figure 2 lists our results for all three languages, based on treebank and parser input. As in (Meyers et al., 2009), we generated 4- tuples consisting of the following for each dependency: (A) the logic1 label (SBJ, OBJ, etc.), (B) its transparency (True or False), (C) The functor (a single word or a named entity); and (D) the argument (a single word or a named entity). In the case of conjunction where there was no lexical conjunction word, we used either punctuation (commas or semi-colons) or the placeholder *NULL*. We then corrected these results by hand to produce the answer key–an answer was correct if all four members of the tuple were correct and incorrect otherwise. Table 2 provides the Precision, R</context>
<context position="11576" citStr="Meyers et al., 2009" startWordPosition="1803" endWordPosition="1806">parse-based system. The Charniak parser for English was trained on the Wall Street Journal corpus and can achieve about 90% accuracy on similar corpora, but lower accuracy on other genres. Differences between treebank and parser results for English were higher for LET and NAR genres than for the TEL because the system is not currently designed to handle TEL-specific features like disfluencies. All processors were trained on or initially designed for news corpora. Thus corpora out of this domain usually produce lower results. LET was easier as it consisted mainly of short simple sentences. In (Meyers et al., 2009), we evaluated our results on 40 Japanese sentences from the JENAAD corpus (Utiyama and Isahara, 2003) and achieved a higher F-score (90.6%) relative to the Kyoto corpus, as JENAAD tends to have fewer long complex sentences. By using our answer key for multiple inputs, we discovered errors and consequently improved the quality of the answer keys. However, at times we were also compelled to fork the answer keys–given multiple correct answers, we needed to allow different answer keys corresponding to different inputs. For English, these items represent approximately 2% of the answer keys (there </context>
</contexts>
<marker>Meyers, Kosaka, Xue, Ji, Sun, Liao, Xu, 2009</marker>
<rawString>A. Meyers, M. Kosaka, N. Xue, H. Ji, A. Sun, S. Liao, and W. Xu. 2009. Automatic Recognition of Logical Relations for English, Chinese and Japanese in the GLARF Framework. In SEW-2009 at NAACLHLT-2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Surdeanu</author>
<author>R Johansson</author>
<author>A Meyers</author>
<author>L M´arquez</author>
<author>J Nivre</author>
</authors>
<date>2008</date>
<booktitle>The CoNLL-2008 Shared Task on Joint Parsing of Syntactic and Semantic Dependencies. In Proceedings of the CoNLL-2008 Shared Task,</booktitle>
<location>Manchester, GB.</location>
<marker>Surdeanu, Johansson, Meyers, M´arquez, Nivre, 2008</marker>
<rawString>M. Surdeanu, R. Johansson, A. Meyers, L. M´arquez, and J. Nivre. 2008. The CoNLL-2008 Shared Task on Joint Parsing of Syntactic and Semantic Dependencies. In Proceedings of the CoNLL-2008 Shared Task, Manchester, GB.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Utiyama</author>
<author>H Isahara</author>
</authors>
<title>Reliable Measures for Aligning Japanese-English News Articles and Sentences. In</title>
<date>2003</date>
<booktitle>ACL-2003,</booktitle>
<pages>72--79</pages>
<contexts>
<context position="11678" citStr="Utiyama and Isahara, 2003" startWordPosition="1819" endWordPosition="1822">s and can achieve about 90% accuracy on similar corpora, but lower accuracy on other genres. Differences between treebank and parser results for English were higher for LET and NAR genres than for the TEL because the system is not currently designed to handle TEL-specific features like disfluencies. All processors were trained on or initially designed for news corpora. Thus corpora out of this domain usually produce lower results. LET was easier as it consisted mainly of short simple sentences. In (Meyers et al., 2009), we evaluated our results on 40 Japanese sentences from the JENAAD corpus (Utiyama and Isahara, 2003) and achieved a higher F-score (90.6%) relative to the Kyoto corpus, as JENAAD tends to have fewer long complex sentences. By using our answer key for multiple inputs, we discovered errors and consequently improved the quality of the answer keys. However, at times we were also compelled to fork the answer keys–given multiple correct answers, we needed to allow different answer keys corresponding to different inputs. For English, these items represent approximately 2% of the answer keys (there were a total 118 Treebank Parser ID % Prec % Rec F F-T % Prec % Rec F F-T WSJ 1238 1238 83.6 87.1 1164</context>
</contexts>
<marker>Utiyama, Isahara, 2003</marker>
<rawString>M. Utiyama and H. Isahara. 2003. Reliable Measures for Aligning Japanese-English News Articles and Sentences. In ACL-2003, pages 72–79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Xue</author>
<author>F Xia</author>
<author>F Chiou</author>
<author>M Palmer</author>
</authors>
<title>The Penn Chinese Treebank: Phrase Structure Annotation of a Large Corpus. Natural Language Engineering.</title>
<date>2005</date>
<contexts>
<context position="9468" citStr="Xue et al., 2005" startWordPosition="1501" endWordPosition="1504">e could detect errors and inconsistencies. We processed the following corpora. English: 86 sentence article (wsj 2300) from the Wall Street Journal PTB test corpus (WSJ); 46 sentence letter from Good Will (LET), the first 100 sentences of a switchboard telephone transcript (TEL) and the first 100 sentences of a narrative from the Charlotte Narrative and Conversation (NAR). These samples are taken from the PTB WSJ Corpus and the SIGANN shared subcorpus of the OANC. The filenames are: 110CYL067, NapierDianne and sw2014. Chinese: a 20 sentence sample of text from the Penn Chinese Treebank (CTB) (Xue et al., 2005). Japanese: 20 sentences from the Kyoto Corpus (KYO) (Kurohashi and Nagao, 1998) 5 Running the GLARFer Programs We use Charniak, UMD and KNP parsers (Charniak, 2001; Huang and Harper, 2009; Kurohashi and Nagao, 1998), JET Named Entity tagger (Grishman et al., 2005; Ji and Grishman, 2006) and other resources in conjunction with languagespecific GLARFers that incorporate hand-written rules to convert output of these processors into a final representation, including logic1 structure, the focus of this paper. English GLARFer rules use Comlex (Macleod et al., 1998a) and the various NomBank lexicons</context>
</contexts>
<marker>Xue, Xia, Chiou, Palmer, 2005</marker>
<rawString>N. Xue, F. Xia, F. Chiou, and M. Palmer. 2005. The Penn Chinese Treebank: Phrase Structure Annotation of a Large Corpus. Natural Language Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Xue</author>
</authors>
<title>Labeling Chinese Predicates with Semantic roles.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<pages>255</pages>
<contexts>
<context position="683" citStr="Xue, 2008" startWordPosition="104" endWordPosition="105">†, Michiko Kosaka$, Heng Ji*, Nianwen Xue*, Mary Harper°, Ang Sun†, Wei Xu† and Shasha Liao†† New York Univ., $Monmouth Univ., *Brandeis Univ,, *City Univ. of New York, °Johns Hopkins Human Lang. Tech. Ctr. of Excellence &amp; U. of Maryland, College Park Abstract GLARF relations are generated from treebank and parses for English, Chinese and Japanese. Our evaluation of system output for these input types requires consideration of multiple correct answers.1 1 Introduction Systems, such as treebank-based parsers (Charniak, 2001; Collins, 1999) and semantic role labelers (Gildea and Jurafsky, 2002; Xue, 2008), are trained and tested on hand-annotated data. Evaluation is based on differences between system output and test data. Other systems use these programs to perform tasks unrelated to the original annotation. For example, participating systems in CONLL (Surdeanu et al., 2008; Hajiˇc et al., 2009), ACE and GALE tasks merged the results of several processors (parsers, named entity recognizers, etc.) not initially designed for the task at hand. This paper discusses differences between handannotated data and automatically generated data with respect to our GLARFers, systems for generating Grammati</context>
</contexts>
<marker>Xue, 2008</marker>
<rawString>N. Xue. 2008. Labeling Chinese Predicates with Semantic roles. Computational Linguistics, 34:225– 255.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>