<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004540">
<title confidence="0.917357">
SemEval-2012 Task 1: English Lexical Simplification
</title>
<author confidence="0.994347">
Lucia Specia Sujay Kumar Jauhar
</author>
<affiliation confidence="0.997905">
Department of Computer Science Research Group in Computational Linguistics
University of Sheffield University of Wolverhampton
</affiliation>
<email confidence="0.955846">
L.Specia@sheffield.ac.uk Sujay.KumarJauhar@wlv.ac.uk
</email>
<author confidence="0.99734">
Rada Mihalcea
</author>
<affiliation confidence="0.999362">
Department of Computer Science and Engineering
University of North Texas
</affiliation>
<email confidence="0.998472">
rada@cs.unt.edu
</email>
<sectionHeader confidence="0.995637" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999877333333333">
We describe the English Lexical Simplifica-
tion task at SemEval-2012. This is the first
time such a shared task has been organized
and its goal is to provide a framework for the
evaluation of systems for lexical simplification
and foster research on context-aware lexical
simplification approaches. The task requires
that annotators and systems rank a number of
alternative substitutes – all deemed adequate –
for a target word in context, according to how
“simple” these substitutes are. The notion of
simplicity is biased towards non-native speak-
ers of English. Out of nine participating sys-
tems, the best scoring ones combine context-
dependent and context-independent informa-
tion, with the strongest individual contribution
given by the frequency of the substitute re-
gardless of its context.
</bodyText>
<sectionHeader confidence="0.998987" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997783291666667">
Lexical Simplification is a subtask of Text Simpli-
fication (Siddharthan, 2006) concerned with replac-
ing words or short phrases by simpler variants in a
context aware fashion (generally synonyms), which
can be understood by a wider range of readers. It
generally envisages a certain human target audience
that may find it difficult or impossible to understand
complex words or phrases, e.g., children, people
with poor literacy levels or cognitive disabilities, or
second language learners. It is similar in many re-
spects to the task of Lexical Substitution (McCarthy
and Navigli, 2007) in that it involves determining
adequate substitutes in context, but in this case on
the basis of a predefined criterion: simplicity.
A common pipeline for a Lexical Simplification
system includes at least three major components: (i)
complexity analysis: selection of words or phrases
in a text that are considered complex for the reader
and/or task at hand; (ii) substitute lookup: search
for adequate replacement words or phrases deemed
complex in context, e.g., taking synonyms (with
the same sense) from a thesaurus or finding similar
words/phrases in a corpus using distributional simi-
larity metrics; and (iii) context-based ranking: rank-
ing of substitutes according to how simple they are
to the reader/task at hand.
As an example take the sentence: “Hitler com-
mitted terrible atrocities during the second World
War.” The system would first identify complex
words, e.g. atrocities, then search for substitutes
that might adequately replace it. A thesaurus lookup
would yield the following synonyms: abomination,
cruelty, enormity and violation, but enormity should
be dropped as it does not fit the context appropri-
ately. Finally, the system would determine the sim-
plest of these substitutes, e.g., cruelty, and use it
to replace the complex word, yielding the sentence:
“Hitler committed terrible cruelties during the sec-
ond World War.”.
Different from other subtasks of Text Simplifica-
tion like Syntactic Simplification, which have been
relatively well studied, Lexical Simplification has
received less attention. Although a few recent at-
tempts explicitly address dependency on context (de
Belder et al., 2010; Yatskar et al., 2010; Biran et al.,
2011; Specia, 2010), most approaches are context-
independent (Candido et al., 2009; Devlin and Tait,
1998). In addition, a general deeper understanding
</bodyText>
<page confidence="0.971439">
347
</page>
<note confidence="0.9727425">
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 347–355,
Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999930458333333">
of the problem is yet to be gained. As a first attempt
to address this problem in the shape of a shared task,
the English Simplification task at SemEval-2012 fo-
cuses on the third component, which we believe is
the core of the Lexical Simplification problem.
The SemEval-2012 shared task on English Lexi-
cal Simplification has been conceived with the fol-
lowing main purposes: advancing the state-of-the-
art Lexical Simplification approaches, and provid-
ing a common framework for evaluation of Lexical
Simplification systems for participants and other re-
searchers interested in the field. Another central mo-
tive of such a shared task is to bring awareness to the
general vagueness associated with the notion of lex-
ical simplicity. Our hypothesis is that in addition to
the notion of a target application/reader, the notion
of simplicity is highly context-dependent. In other
words, given the same list of substitutes for a given
target word with the same sense, we expect different
orderings of these substitutes in different contexts.
We hope that participation in this shared task will
help discover some underlying traits of lexical sim-
plicity and furthermore shed some light on how this
may be leveraged in future work.
</bodyText>
<sectionHeader confidence="0.919605" genericHeader="method">
2 Task definition
</sectionHeader>
<bodyText confidence="0.990436307692308">
Given a short context, a target word in English,
and several substitutes for the target word that are
deemed adequate for that context, the goal of the
English Simplification task at SemEval-2012 is to
rank these substitutes according to how “simple”
they are, allowing ties. Simple words/phrases are
loosely defined as those which can be understood by
a wide range of people, including those with low lit-
eracy levels or some cognitive disability, children,
and non-native speakers of English. In particular,
the data provided as part of the task is annotated by
fluent but non-native speakers of English.
The task thus essentially involves comparing
words or phrases and determining their order of
complexity. By ranking the candidates, as opposed
to categorizing them into specific labels (simple,
moderate, complex, etc.), we avoid the need for a
fixed number of categories and for more subjective
judgments. Also ranking enables a more natural and
intuitive way for humans (and systems) to perform
annotations by preventing them from treating each
individual case in isolation, as opposed to relative
to each other. However, the inherent subjectivity
introduced by ranking entails higher disagreement
among human annotators, and more complexity for
systems to tackle.
</bodyText>
<sectionHeader confidence="0.952712" genericHeader="method">
3 Corpus compilation
</sectionHeader>
<bodyText confidence="0.999880454545455">
The trial and test corpora were created from the cor-
pus of SemEval-2007 shared task on Lexical Sub-
stitution (McCarthy and Navigli, 2007). This de-
cision was motivated by the similarity between the
two tasks. Moreover the existing corpus provided an
adequate solution given time and cost constraints for
our corpus creation. Given existing contexts with the
original target word replaced by a placeholder and
the lists of substitutes (including the target word),
annotators (and systems) are required to rank substi-
tutes in order of simplicity for each context.
</bodyText>
<subsectionHeader confidence="0.999253">
3.1 SemEval-2007 - LS corpus
</subsectionHeader>
<bodyText confidence="0.9999312">
The corpus from the shared task on Lexical Substi-
tution (LS) at SemEval-2007 is a selection of sen-
tences, or contexts, extracted from the English Inter-
net Corpus of English (Sharoff, 2006). It contains
samples of English texts crawled from the web.
This selection makes up the dataset of a total of
2, 010 contexts which are divided into Trial and Test
sets, consisting of 300 and 1710 contexts respec-
tively. It covers a total of 201 (mostly polysemous)
target words, including nouns, verbs, adjectives and
adverbs, and each of the target words is shown in
10 different contexts. Annotators had been asked to
suggest up to three different substitutes (words or
short phrases) for each of the target words within
their contexts. The substitutes were lemmatized un-
less it was deemed that the lemmatization would al-
ter the meaning of the substitute. Annotators were
all native English speakers and each annotated the
entire dataset. Here is an example of a context for
the target word “bright”:
</bodyText>
<construct confidence="0.49134525">
&lt;lexelt item=&amp;quot;bright.a&amp;quot;&gt;
&lt;instance id=&amp;quot;1&amp;quot;&gt;
&lt;context&gt;During the siege, George
Robertson had appointed Shuja-ul-Mulk,
who was a &lt;head&gt;bright&lt;/head&gt; boy
only 12 years old and the youngest surviv-
ing son of Aman-ul-Mulk, as the ruler of
Chitral.&lt;/context&gt;
</construct>
<page confidence="0.987702">
348
</page>
<bodyText confidence="0.86594975">
&lt;/instance&gt; ... &lt;/lexelt&gt;
The gold-standard document contains each target
word along with a ranked list of its possible substi-
tutes, e.g., for the context above, three annotators
suggested “intelligent” and “clever” as substitutes
for “bright”, while only one annotator came up with
“smart”:
bright.a 1:: intelligent 3; clever 3; smart 1;
</bodyText>
<subsectionHeader confidence="0.9828305">
3.2 SemEval-2012 Lexical Simplification
corpus
</subsectionHeader>
<bodyText confidence="0.999989876923077">
Given the list of contexts and each respective list
of substitutes we asked annotators to rank substi-
tutes for each individual context in ascending order
of complexity. Since the notion of textual simplic-
ity varies from individual to individual, we carefully
chose a group of annotators in an attempt to cap-
ture as much of a common notion of simplicity as
possible. For practical reasons, we selected annota-
tors with high proficiency levels in English as sec-
ond language learners - all with a university first de-
gree in different subjects.
The Trial dataset was annotated by four people
while the Test dataset was annotated by five peo-
ple. In both cases each annotator tagged the com-
plete dataset.
Inter-annotator agreement was computed using an
adaptation of the kappa index with pairwise rank
comparisons (Callison-Burch et al., 2011). This is
also the primary evaluation metric for participating
systems in the shared task, and it is covered in more
detail in Section 4.
The inter-annotator agreement was computed for
each pair of annotators and averaged over all possi-
ble pairs for a final agreement score. On the Trial
dataset, a kappa index of 0.386 was found, while
for the Test dataset, a kappa index of 0.398 was
found. It may be noted that certain annotators dis-
agreed considerably with all others. For example,
on the Test set, if annotations from one judge are re-
moved, the average inter-annotator agreement rises
to 0.443. While these scores are apparently low, the
highly subjective nature of the annotation task must
be taken into account. According to the reference
values for other tasks, this level of agreement is con-
sidered “moderate” (Callison-Burch et al., 2011).
It is interesting to note that higher inter-annotator
agreement scores were achieved between annota-
tors with similar language and/or educational back-
grounds. The highest of any pairwise annotator
agreement (0.52) was achieved between annotators
of identical language and educational background,
as well as very similar levels of English proficiency.
High agreement scores were also achieved between
annotators with first languages belonging to the
same language family.
Finally, it is also worth noticing that this agree-
ment metric is highly sensitive to small differences
in annotation, thus leading to overly pessimistic
scores. A brief analysis reveals that annotators often
agree on clusters of simplicity and the source of the
disagreement comes from the rankings within these
clusters.
Finally, the gold-standard annotations for the
Trial and Test datasets – against which systems are
to be evaluated – were generated by averaging the
annotations from all annotators. This was done
context by context where each substitution was at-
tributed a score based upon the average of the rank-
ings it was ascribed. The substitutions were then
sorted in ascending order of scores, i.e., lowest score
(highest average ranking) first. Tied scores were
grouped together to form a single rank. For exam-
ple, assume that for a certain context, four annota-
tors provided rankings as given below, where multi-
ple candidates between {} indicate ties:
</bodyText>
<construct confidence="0.75735925">
Annotator 1: {clear} {light} {bright} {lumi-
nous} {well-lit}
Annotator 2: {well-lit} {clear} {light}
{bright} {luminous}
Annotator 3: {clear} {bright} {light} {lumi-
nous} {well-lit}
Annotator 4: {bright} {well-lit} {luminous}
{clear} {light}
</construct>
<bodyText confidence="0.98213725">
Thus the word “clear”, having been ranked 1st,
2nd, 1st and 4th by each of the annotators respec-
tively is given an averaged ranking score of 2. Sim-
ilarly “light” = 3.25, “bright” = 2.5, “luminous” =
4 and “well-lit” = 3.25. Consequently the gold-
standard ranking for this context is:
Gold: {clear} {bright} {light, well-lit} {lumi-
nous}
</bodyText>
<page confidence="0.995258">
349
</page>
<subsectionHeader confidence="0.999205">
3.3 Context-dependency
</subsectionHeader>
<bodyText confidence="0.999236947368421">
As mentioned in Section 1, one of our hypothe-
ses was that the notion of simplicity is context-
dependent. In other words, that the ordering of sub-
stitutes for different occurrences of a target word
with a given sense is highly dependent on the con-
texts in which such a target word appears. In order
to verify this hypothesis quantitatively, we further
analyzed the gold-standard annotations of the Trial
and Test datasets. We assume that identical lists of
substitutes for different occurrences of a given tar-
get word ensure that such a target word has the same
sense in all these occurrences. For every target word,
we then generate all pairs of contexts containing the
exact same initial list of substitutes and check the
proportion of these contexts for which human an-
notators ranked the substitutes differently. We also
check for cases where only the top-ranked substitute
is different. The numbers obtained are shown in Ta-
ble 1.
</bodyText>
<figure confidence="0.918664">
Trial Test
1) # context pairs 1350 7695
2) # 1) with same list 60 242
3) # 2) with different rankings 24 139
4) # 2) with different top substitute 19 38
</figure>
<tableCaption confidence="0.966901">
Table 1: Analysis on the context-dependency of the no-
tion of simplicity.
</tableCaption>
<bodyText confidence="0.999739636363636">
Although the proportion of pairs of contexts with
the same list of substitutes is very low (less than
5%), it is likely that there are many other occur-
rences of a target word with the same sense and
slightly different lists of substitutes. Further man-
ual inspection is necessary to determine the actual
numbers. Nevertheless, from the observed sample
it is possible to conclude that humans will, in fact,
rank the same set of words (with the same sense)
differently depending on the context (on an average
in 40-57% of the instances).
</bodyText>
<sectionHeader confidence="0.998407" genericHeader="method">
4 Evaluation metric
</sectionHeader>
<bodyText confidence="0.9993596">
No standard metric has yet been defined for eval-
uating Lexical Simplification systems. Evaluating
such systems is a challenging problem due to the
aforementioned subjectivity of the task. Since this
is a ranking task, rank correlation metrics are desir-
able. However, metrics such as Spearman’s Rank
Correlation are not reliable on the limited number of
data points available for comparison on each rank-
ing (note that the nature of the problem enforces a
context-by-context ranking, as opposed to a global
score), Other metrics for localized, pairwise rank
correlation, such as Kendall’s Tau, disregard ties, –
which are important for our purposes – and are thus
not suitable.
The main evaluation metric proposed for this
shared task is in fact a measure of inter-annotator
agreement, which is used for both contrasting two
human annotators (Section 3.2) and contrasting a
system output to the average of human annotations
that together forms the gold-standard.
Out metric is based on the kappa index (Cohen,
1960) which in spite of many criticisms is widely
used for its simplicity and adaptability for different
applications. The generalized form of the kappa in-
dex is
</bodyText>
<equation confidence="0.9954495">
P(A) − P(E)
1 − P(E)
</equation>
<bodyText confidence="0.997204388888889">
where P(A) denotes the proportion of times two
annotators agree and P(E) gives the probability of
agreement by chance between them.
In order to apply the kappa index for a ranking
task, we follow the method proposed by (Callison-
Burch et al., 2011) for measuring agreement over
judgments of translation quality. This method de-
fines P(A) and P(E) in such a way that it now
counts agreement whenever annotators concur upon
the order of pairwise ranks. Thus, if one annotator
ranked two given words 1 and 3, and the second an-
notator ranked them 3 and 7 respectively, they are
still in agreement. Formally, assume that two anno-
tators A1 and A2 rank two instance a and b. Then
P(A) = the proportion of times A1 and A2 agree
on a ranking, where an occurrence of agreement is
counted whenever rank(a &lt; b) or rank(a = b) or
rank(a &gt; b).
</bodyText>
<listItem confidence="0.996142375">
P(E) (the likelihood that annotators A1 and A2
agree by chance) is based upon the probability that
both of them assign the same ranking order to a and
b. Given that the probability of getting rank(a &lt;
b) by any annotator is P(a &lt; b), the probability
that both annotators get rank(a &lt; b) is P(a &lt; b)2
(agreement is achieved when A1 assigns a &lt; b by
chance and A2 also assigns a &lt; b). Similarly, the
</listItem>
<equation confidence="0.987106">
K =
</equation>
<page confidence="0.918461">
350
</page>
<listItem confidence="0.569538333333333">
probability of chance agreement for rank(a = b)
and rank(a &gt; b) are P(a = b)2 and P(a &gt; b)2
respectively. Thus:
</listItem>
<equation confidence="0.988721">
P(E) = P(a &lt; b)2 + P(a = b)2 + P(a &gt; b)2
</equation>
<bodyText confidence="0.999361555555556">
However, the counts of rank(a &lt; b) and
rank(a &gt; b) are inextricably linked, since for any
particular case of a1 &lt; b1, it follows that b1 &gt;
a1, and thus the two counts must be incremented
equally. Therefore, over the entire space of ranked
pairs, the probabilities remain exactly the same. In
essence, after counting for P(a = b), the remaining
probability mass is equally split between P(a &lt; b)
and P(a &gt; b). Therefore:
</bodyText>
<equation confidence="0.992786333333333">
1 − P(a = b)
P (a &lt; b) = P (a &gt; b) =
2
</equation>
<bodyText confidence="0.997558">
Kappa is calculated for every pair of ranked items
for a given context, and then averaged to get an over-
all kappa score:
</bodyText>
<equation confidence="0.994148">
Pn(A) − Pn(E)
1 − Pn(E)
|N|
</equation>
<bodyText confidence="0.978083214285714">
where N is the total number of contexts, and Pn(A)
and Pn(E) are calculated based on counts extracted
from the data on the particular context n.
The functioning of this evaluation metric is illus-
trated by the following example:
Context: During the siege, George Robert-
son had appointed Shuja-ul-Mulk, who was a
_____ boy only 12 years old and the youngest
surviving son of Aman-ul-Mulk, as the ruler
of Chitral.
Gold: {intelligent} {clever} {smart} {bright}
System: {intelligent} {bright} {clever,
smart}
Out of the 6 distinct unordered pairs of lexical
items, system and gold agreed 3 times. Conse-
quently, Pn(A) = 3�. In addition, count(a =
b) = 1. Thus, Pn(a = b) = 112. Which gives a
P(E) = 41
96 and the final kappa score for this partic-
ular context of 0.13.
The statistical significance of the results from two
systems A and B is measured using the method
of Approximate Randomization, which has been
shown to be a robust approach for several NLP tasks
(Noreen, 1989). The randomization is run 1, 000
times and if the p-value is &lt; 0.05 the difference be-
tween systems A and B is asserted as being statisti-
cally significance.
</bodyText>
<sectionHeader confidence="0.992185" genericHeader="method">
5 Baselines
</sectionHeader>
<bodyText confidence="0.999494810810811">
We defined three baseline lexical simplification sys-
tems for this task, as follows.
L-Sub Gold: This baseline uses the gold-standard
annotations from the Lexical Substitution cor-
pus of SemEval-2007 as is. In other words, the
ranking is based on the goodness offit of sub-
stitutes for a context, as judged by human anno-
tators. This method also serves to show that the
Lexical Substitution and Lexical Simplification
tasks are indeed different.
Random: This baseline provides a randomized or-
der of the substitutes for every context. The
process of randomization is such that is allows
the occurrence of ties.
Simple Freq.: This simple frequency baseline uses
the frequency of the substitutes as extracted
from the Google Web 1T Corpus (Brants and
Franz, 2006) to rank candidate substitutes
within each context.
The results in Table 2 show that the “L-Sub Gold”
and “Random” baselines perform very poorly on
both Trial and Test sets. In particular, the reason for
the poor scores for “L-Sub Gold” can be attributed
to the fact that it yields many ties, whereas the gold-
standard presents almost no ties. Our kappa met-
ric tends to penalize system outputs with too many
ties, since the probability of agreement by chance is
primarily computed on the basis of the number of
ties present in the two rankings being compared (see
Section 4).
The “Simple Freq.” baseline, on the other hand,
performs very strongly, in spite of its simplistic ap-
proach, which is entirely agnostic to context. In fact
it surpasses the average inter-annotator agreement
on both Trial and Test datasets. Indeed, the scores on
the Test set approach the best inter-annotator agree-
ment scores between any two annotators.
</bodyText>
<equation confidence="0.952178333333333">
κ =
� INI
n=1
</equation>
<page confidence="0.995512">
351
</page>
<table confidence="0.99930025">
Trial Test
L-Sub Gold 0.050 0.106
Random 0.016 0.012
Simple Freq. 0.397 0.471
</table>
<tableCaption confidence="0.998663">
Table 2: Baseline kappa scores on trial and test sets
</tableCaption>
<sectionHeader confidence="0.998588" genericHeader="evaluation">
6 Results and Discussion
</sectionHeader>
<subsectionHeader confidence="0.959052">
6.1 Participants
</subsectionHeader>
<bodyText confidence="0.999724914634147">
Five sites submitted one or more systems to the task,
totaling nine systems:
ANNLOR-lmbing: This system (Ligozat et al.,
2012) relies on language models probabili-
ties, and builds on the principle of the Sim-
ple Frequency baseline. While the baseline
uses Google n-grams to rank substitutes, this
approach uses Microsoft Web n-grams in the
same way. Additionally characteristics, such
as the contexts of each term to be substituted,
were integrated into the system. Microsoft Web
N-gram Service was used to obtain log likeli-
hood probabilities for text units, composed of
the lexical item and 4 words to the left and right
from the surrounding context.
ANNLOR-simple: The system (Ligozat et al.,
2012) is based on Simple English Wikipedia
frequencies, with the motivation that the lan-
guage used in this version of Wikipedia is
targeted towards people who are not first-
language English speakers. Word n-grams (n =
1-3) and their frequencies were extracted from
this corpus using the Text-NSP Perl module
and a ranking of the possible substitutes of a
target word according to these frequencies in
descending order was produced.
EMNLPCPH-ORD1: The system performs a se-
ries of pairwise comparisons between candi-
dates. A binary classifier is learned purpose
using the Trial dataset and artificial unlabeled
data extracted based on Wordnet and a corpus
in a semi-supervised fashion. A co-training
procedure that lets each classifier increase the
other classifier’s training set with selected in-
stances from the unlabeled dataset is used. The
features include word and character n-gram
probabilities of candidates and contexts using
web corpora, distributional differences of can-
didate in a corpus of “easy” sentences and a
corpus of normal sentences, syntactic complex-
ity of documents that are similar to the given
context, candidate length, and letter-wise rec-
ognizability of candidate as measured by a tri-
gram LM. The first feature sets for co-training
combines the syntactic complexity, character
trigram LM and basic word length features, re-
sulting in 29 features against the remaining 21.
EMNLPCPH-ORD2: This is a variant of the
EMNLPCPH-ORD1 system where the first fea-
ture set pools all syntactic complexity fea-
tures and Wikipedia-based features (28 fea-
tures) against all the remaining 22 features in
the second group.
SB-mmSystem: The approach (Amoia and Ro-
manelli, 2012) builds on the baseline defini-
tion of simplicity using word frequencies but
attempt at defining a more linguistically mo-
tivated notion of simplicity based on lexical
semantics considerations. It adopts different
strategies depending on the syntactic complex-
ity of the substitute. For one-word substitutes
or common collocations, the system uses its
frequency from Wordnet as a metric. In the
case of multi-words substitutes the system uses
“relevance” rules that apply (de)compositional
semantic criteria and attempts to identify a
unique content word in the substitute that might
better approximate the whole expression. The
expression is then assigned the frequency asso-
ciated to this content word for the ranking. Af-
ter POS tagging and sense disambiguating all
substitutes, hand-written rules are used to de-
compose the meaning of a complex phrase and
identify the most relevant word conveying the
semantics of the whole.
UNT-SimpRank: The system (Sinha, 2012) uses
external resources, including the Simple En-
glish Wikipedia corpus, a set of Spoken En-
glish dialogues, transcribed into machine read-
able form, WordNet, and unigram frequencies
(Google Web1T data). SimpRank scores each
substitute by a sum of its unigram frequency, its
</bodyText>
<page confidence="0.995928">
352
</page>
<bodyText confidence="0.99988040625">
frequency in the Simple English Wikipedia, its
frequency in the spoken corpus, the inverse of
its length, and the number of senses the sub-
stitute has in WordNet. For a given context,
the substitutes are then reverse-ranked based on
their simplicity scores.
UNT-SimpRankLight: This is a variant of Sim-
pRank which does not use unigram frequen-
cies. The goal of this system is to check
whether a memory and time-intensive and non-
free resource such as the Web1T corpus makes
a difference over other free and lightweight re-
sources.
UNT-SaLSA: The only resource SaLSA depends
on is the Web1T data, and in particular only
3-grams from this corpus. It leverages the con-
text provided with the dataset by replacing the
target placeholder one by one with each of the
substitutes and their inflections thus building
sets of 3-grams for each substitute in a given
instance. The score of any substitute is then the
sum of the 3-gram frequencies of all the gener-
ated 3-grams for that substitute.
UOW-SHEF-SimpLex: The system (Jauhar and
Specia, 2012) uses a linear weighted ranking
function composed of three features to pro-
duce a ranking. These include a context sen-
sitive n-gram frequency model, a bag-of-words
model and a feature composed of simplicity
oriented psycholinguistic features. These three
features are combined using an SVM ranker
that is trained and tuned on the Trial dataset.
</bodyText>
<subsectionHeader confidence="0.999663">
6.2 Pairwise kappa
</subsectionHeader>
<bodyText confidence="0.999944">
The official task results and the ranking of the sys-
tems are shown in Table 3.
Firstly, it is worthwhile to note that all the top
ranking systems include features that use frequency
as a surrogate measure for lexical simplicity. This
indicates a very high correlation between distribu-
tional frequency of a given word and its perceived
complexity level. Additionally, the top two systems
involve context-dependent and context-independent
features, thus supporting our hypothesis of the com-
posite nature of the lexical simplification problem.
</bodyText>
<table confidence="0.999816615384615">
Rank Team - System Kappa
1 UOW-SHEF-SimpLex 0.496
UNT-SimpRank 0.471
2 Baseline-Simple Freq. 0.471
ANNLOR-simple 0.465
3 UNT-SimpRankL 0.449
4 EMNLPCPH-ORD1 0.405
5 EMNLPCPH-ORD2 0.393
6 SB-mmSystem 0.289
7 ANNLOR-lmbing 0.199
8 Baseline-L-Sub Gold 0.106
9 Baseline-Random 0.013
10 UNT-SaLSA -0.082
</table>
<tableCaption confidence="0.95300375">
Table 3: Official results and ranking according to the pair-
wise kappa metric. Systems are ranked together when the
difference in their kappa score is not statistically signifi-
cant.
</tableCaption>
<bodyText confidence="0.999892695652174">
Few of the systems opted to use some form of
supervised learning for the task, due to the limited
number of training examples given. As pointed out
by some participants who checked learning curves
for their systems, the performance is likely to im-
prove with larger training sets. Without enough
training data, context agnostic approaches such as
the “Simple Freq.” baseline become very hard to
beat.
We speculate that the reason why the effects of
context-aware approaches are somewhat mitigated is
because of the isolated setup of the shared task. In
practice, humans produce language at an even level
of complexity, i.e. consistently simple, or consis-
tently complex. In the shared task’s setup, systems
are expected to simplify a single target word in a
context, ignoring the possibility that sometimes sim-
ple words may not be contextually associated with
complex surrounding words. This not only explains
why context-aware approaches are less successful
than was originally expected, but also gives a reason
for the good performance of context-agnostic sys-
tems.
</bodyText>
<subsectionHeader confidence="0.996468">
6.3 Recall and top-rank
</subsectionHeader>
<bodyText confidence="0.9998824">
As previously noted, the primary evaluation met-
ric is very susceptible to penalize slight changes,
making it overly pessimistic about systems’ perfor-
mance. Hence, while it may be an efficient way to
compare and rank systems within the framework of
</bodyText>
<page confidence="0.99746">
353
</page>
<bodyText confidence="0.999695">
a shared task, it may be unnecessarily devaluing the
practical viability of approaches. We performed two
post hoc evaluations that assess system output from
a practical point of view. We check how well the
top-ranked substitute, i.e., the simplest substitute ac-
cording to a given system (which is most likely to
be used in a real simplification task) compares to the
top-ranked candidate from the gold standard. This is
reported in the TRnk column of Table 4: the percent-
age of contexts in which the intersection between the
simplest substitute set from a system’s output and
the gold standard contained at least one element.
We note that while ties are virtually inexistent in the
gold standard data, ties in the system output can af-
fect this metric: a system that naively predicts all
substitutes as the simplest (i.e., a single tie includ-
ing all candidates) will score 100% in this metric.
We also measured the “recall-at-n&amp;quot; values for 1 &lt;
n &lt; 3, which gives the ratio of candidates from the
top n substitute sets to those from the gold-standard.
For a given n, we only consider contexts that have
at least n+1 candidates in the gold-standard (so that
there is some ranking to be done). Table 4 shows the
results of this additional analysis.
</bodyText>
<table confidence="0.998536615384615">
Team - System TRnk n=1 n=2 n=3
UOW-SHEF-SimpLex 0.602 0.575 0.689 0.769
UNT-SimpRank 0.585 0.559 0.681 0.760
Baseline-Simple Freq. 0.585 0.559 0.681 0.760
ANNLOR-simple 0.564 0.538 0.674 0.768
UNT-SimpRankL 0.567 0.541 0.674 0.753
EMNLPCPH-ORD1 0.539 0.513 0.645 0.727
EMNLPCPH-ORD2 0.530 0.503 0.637 0.722
SB-mmSystem 0.477 0.452 0.632 0.748
ANNLOR-lmbing 0.336 0.316 0.494 0.647
Baseline-L-Sub Gold 0.454 0.427 0.667 0.959
Baseline-Random 0.340 0.321 0.612 0.825
UNT-SaLSA 0.146 0.137 0.364 0.532
</table>
<tableCaption confidence="0.9914155">
Table 4: Additional results according to the top-rank
(TRnk) and recall-at-n metrics.
</tableCaption>
<bodyText confidence="0.99993645">
These evaluation metrics favour systems that pro-
duce many ties. Consequently the baselines “L-Sub
Gold&amp;quot; and “Random&amp;quot; yield overly high scores for
recall-at-n for n=2 and n= 3. Nevertheless the rest
of the results are by and large consistent with the
rankings from the kappa metric.
The results for recall-at-2, e.g., show that most
systems, on average 70% of the time, are able to
find the simplest 2 substitute sets that correspond
to the gold standard. This indicates that most ap-
proaches are reasonably good at distinguishing very
simple substitutes from very complex ones, and that
the top few substitutes will most often produce ef-
fective simplifications.
These results correspond to our experience from
the comparison of human annotators, who are easily
able to form clusters of simplicity with high agree-
ment, but who strongly disagree (based on personal
biases towards perceptions of lexical simplicity) on
the internal rankings of these clusters.
</bodyText>
<sectionHeader confidence="0.999086" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999985482758621">
We have presented the organization and findings of
the first English Lexical Simplification shared task.
This was a first attempt at garnering interest in the
NLP community for research focused on the lexical
aspects of Text Simplification.
Our analysis has shown that there is a very strong
relation between distributional frequency of words
and their perceived simplicity. The best systems on
the shared task were those that relied on this asso-
ciation, and integrated both context-dependent and
context-independent features. Further analysis re-
vealed that while context-dependent features are im-
portant in principle, their applied efficacy is some-
what lessened due to the setup of the shared task,
which treats simplification as an isolated problem.
Future work would involve evaluating the im-
portance of context for lexical simplification in the
scope of a simultaneous simplification to all the
words in a context. In addition, the annotation of
the gold-standard datasets could be re-done taking
into consideration some of the features that are now
known to have clearly influenced the large variance
observed in the rankings of different annotators,
such as their background language and the educa-
tion level. One option would be to select annotators
that conform a specific instantiation of these fea-
tures. This should result in a higher inter-annotator
agreement and hence a simpler task for simplifica-
tion systems.
</bodyText>
<sectionHeader confidence="0.998843" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9996885">
We would like to thank the annotators for their hard
work in delivering the corpus on time.
</bodyText>
<page confidence="0.998707">
354
</page>
<sectionHeader confidence="0.995874" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999870950617284">
Marilisa Amoia and Massimo Romanelli. 2012. SB-
mmSystem: Using Decompositional Semantics for
Lexical Simplification. In English Lexical Simplifica-
tion. Proceedings of the 6th International Workshop
on Semantic Evaluation (SemEval 2012), Montreal,
Canada.
Or Biran, Samuel Brody, and Noemie Elhadad. 2011.
Putting it simply: a context-aware approach to lexi-
cal simplification. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, pages 496–501,
Portland, Oregon.
Thorsten Brants and Alex Franz. 2006. The google web
1t 5-gram corpus version 1.1.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011 work-
shop on statistical machine translation. In Proceedings
of the Sixth Workshop on Statistical Machine Transla-
tion, pages 22–64, Edinburgh, Scotland.
Arnaldo Candido, Jr., Erick Maziero, Caroline Gasperin,
Thiago A. S. Pardo, Lucia Specia, and Sandra M.
Aluisio. 2009. Supporting the adaptation of texts for
poor literacy readers: a text simplification editor for
Brazilian Portuguese. In Proceedings of the Fourth
Workshop on Innovative Use of NLP for Building Ed-
ucational Applications, pages 34–42, Boulder, Col-
orado.
J Cohen. 1960. A coefficient of agreement for nominal
scales. Educational and Psychological Measurement,
20:37–46, April.
Jan de Belder, Koen Deschacht, and Marie-Francine
Moens. 2010. Lexical simplification. In Proceedings
of Itec2010: 1st International Conference on Inter-
disciplinary Research on Technology, Education and
Communication, Kortrijk, Belgium.
Siobhan Devlin and John Tait. 1998. The use of a psy-
cholinguistic database in the simplification of text for
aphasic readers. Linguistic Databases, pages 161–
173.
Sujay Kumar Jauhar and Lucia Specia. 2012. UOW-
SHEF: SimpLex - Lexical Simplicity Ranking based
on Contextual and Psycholinguistic Features. In En-
glish Lexical Simplification. Proceedings of the 6th
International Workshop on Semantic Evaluation (Se-
mEval 2012), Montreal, Canada.
Anne-Laure Ligozat, Cyril Grouin, Anne Garcia-
Fernandez, and Delphine Bernhard. 2012. ANNLOR:
A Naive Notation-system for Lexical Outputs Rank-
ing. In English Lexical Simplification. Proceedings of
the 6th International Workshop on Semantic Evalua-
tion (SemEval 2012), Montreal, Canada.
Diana McCarthy and Roberto Navigli. 2007. Semeval-
2007 task 10: English lexical substitution task. In Pro-
ceedings of the 4th International Workshop on Seman-
tic Evaluations (SemEval-2007), Prague, Czech Re-
public, pages 48–53.
E. Noreen. 1989. Computer-intensive methods for test-
ing hypotheses. New York: Wiley.
Serge Sharoff. 2006. Open-source corpora: Using the
net to fish for linguistic data. International Journal of
Corpus Linguistics, 11(4):435–462.
Advaith Siddharthan. 2006. Syntactic simplification and
text cohesion. Research on Language and Computa-
tion, 4:77–109.
Ravi Sinha. 2012. UNT-SimpRank: Systems for Lex-
ical Simplification Ranking. In English Lexical Sim-
plification. Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval 2012), Mon-
treal, Canada.
Lucia Specia. 2010. Translating from complex to simpli-
fied sentences. In Proceedings of the 9th international
conference on Computational Processing of the Por-
tuguese Language, PROPOR’10, pages 30–39, Berlin,
Heidelberg. Springer-Verlag.
Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-
Mizil, and Lillian Lee. 2010. For the sake of simplic-
ity: unsupervised extraction of lexical simplifications
from Wikipedia. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, pages 365–368, Los Angeles, California.
</reference>
<page confidence="0.999003">
355
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.348932">
<title confidence="0.997393">SemEval-2012 Task 1: English Lexical Simplification</title>
<author confidence="0.99997">Lucia Specia Sujay Kumar Jauhar</author>
<affiliation confidence="0.9998955">Department of Computer Science Research Group in Computational Linguistics University of Sheffield University of Wolverhampton</affiliation>
<title confidence="0.370953">L.Specia@sheffield.ac.uk Sujay.KumarJauhar@wlv.ac.uk</title>
<author confidence="0.948422">Rada</author>
<affiliation confidence="0.999863">Department of Computer Science and University of North</affiliation>
<email confidence="0.999641">rada@cs.unt.edu</email>
<abstract confidence="0.997040684210526">We describe the English Lexical Simplification task at SemEval-2012. This is the first time such a shared task has been organized and its goal is to provide a framework for the evaluation of systems for lexical simplification and foster research on context-aware lexical simplification approaches. The task requires that annotators and systems rank a number of alternative substitutes – all deemed adequate – for a target word in context, according to how “simple” these substitutes are. The notion of simplicity is biased towards non-native speakers of English. Out of nine participating systems, the best scoring ones combine contextdependent and context-independent information, with the strongest individual contribution given by the frequency of the substitute regardless of its context.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marilisa Amoia</author>
<author>Massimo Romanelli</author>
</authors>
<title>SBmmSystem: Using Decompositional Semantics for Lexical Simplification.</title>
<date>2012</date>
<booktitle>In English Lexical Simplification. Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="22511" citStr="Amoia and Romanelli, 2012" startWordPosition="3660" endWordPosition="3664">yntactic complexity of documents that are similar to the given context, candidate length, and letter-wise recognizability of candidate as measured by a trigram LM. The first feature sets for co-training combines the syntactic complexity, character trigram LM and basic word length features, resulting in 29 features against the remaining 21. EMNLPCPH-ORD2: This is a variant of the EMNLPCPH-ORD1 system where the first feature set pools all syntactic complexity features and Wikipedia-based features (28 features) against all the remaining 22 features in the second group. SB-mmSystem: The approach (Amoia and Romanelli, 2012) builds on the baseline definition of simplicity using word frequencies but attempt at defining a more linguistically motivated notion of simplicity based on lexical semantics considerations. It adopts different strategies depending on the syntactic complexity of the substitute. For one-word substitutes or common collocations, the system uses its frequency from Wordnet as a metric. In the case of multi-words substitutes the system uses “relevance” rules that apply (de)compositional semantic criteria and attempts to identify a unique content word in the substitute that might better approximate </context>
</contexts>
<marker>Amoia, Romanelli, 2012</marker>
<rawString>Marilisa Amoia and Massimo Romanelli. 2012. SBmmSystem: Using Decompositional Semantics for Lexical Simplification. In English Lexical Simplification. Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Or Biran</author>
<author>Samuel Brody</author>
<author>Noemie Elhadad</author>
</authors>
<title>Putting it simply: a context-aware approach to lexical simplification.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>496--501</pages>
<location>Portland, Oregon.</location>
<contexts>
<context position="3429" citStr="Biran et al., 2011" startWordPosition="508" endWordPosition="511">iolation, but enormity should be dropped as it does not fit the context appropriately. Finally, the system would determine the simplest of these substitutes, e.g., cruelty, and use it to replace the complex word, yielding the sentence: “Hitler committed terrible cruelties during the second World War.”. Different from other subtasks of Text Simplification like Syntactic Simplification, which have been relatively well studied, Lexical Simplification has received less attention. Although a few recent attempts explicitly address dependency on context (de Belder et al., 2010; Yatskar et al., 2010; Biran et al., 2011; Specia, 2010), most approaches are contextindependent (Candido et al., 2009; Devlin and Tait, 1998). In addition, a general deeper understanding 347 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 347–355, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics of the problem is yet to be gained. As a first attempt to address this problem in the shape of a shared task, the English Simplification task at SemEval-2012 focuses on the third component, which we believe is the core of the Lexical Simplification problem. The SemEval-2012 share</context>
</contexts>
<marker>Biran, Brody, Elhadad, 2011</marker>
<rawString>Or Biran, Samuel Brody, and Noemie Elhadad. 2011. Putting it simply: a context-aware approach to lexical simplification. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 496–501, Portland, Oregon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Alex Franz</author>
</authors>
<title>The google web 1t 5-gram corpus version 1.1.</title>
<date>2006</date>
<contexts>
<context position="19020" citStr="Brants and Franz, 2006" startWordPosition="3097" endWordPosition="3100">annotations from the Lexical Substitution corpus of SemEval-2007 as is. In other words, the ranking is based on the goodness offit of substitutes for a context, as judged by human annotators. This method also serves to show that the Lexical Substitution and Lexical Simplification tasks are indeed different. Random: This baseline provides a randomized order of the substitutes for every context. The process of randomization is such that is allows the occurrence of ties. Simple Freq.: This simple frequency baseline uses the frequency of the substitutes as extracted from the Google Web 1T Corpus (Brants and Franz, 2006) to rank candidate substitutes within each context. The results in Table 2 show that the “L-Sub Gold” and “Random” baselines perform very poorly on both Trial and Test sets. In particular, the reason for the poor scores for “L-Sub Gold” can be attributed to the fact that it yields many ties, whereas the goldstandard presents almost no ties. Our kappa metric tends to penalize system outputs with too many ties, since the probability of agreement by chance is primarily computed on the basis of the number of ties present in the two rankings being compared (see Section 4). The “Simple Freq.” baseli</context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>Thorsten Brants and Alex Franz. 2006. The google web 1t 5-gram corpus version 1.1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Omar Zaidan</author>
</authors>
<title>Findings of the 2011 workshop on statistical machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>22--64</pages>
<location>Edinburgh, Scotland.</location>
<contexts>
<context position="9338" citStr="Callison-Burch et al., 2011" startWordPosition="1442" endWordPosition="1445">varies from individual to individual, we carefully chose a group of annotators in an attempt to capture as much of a common notion of simplicity as possible. For practical reasons, we selected annotators with high proficiency levels in English as second language learners - all with a university first degree in different subjects. The Trial dataset was annotated by four people while the Test dataset was annotated by five people. In both cases each annotator tagged the complete dataset. Inter-annotator agreement was computed using an adaptation of the kappa index with pairwise rank comparisons (Callison-Burch et al., 2011). This is also the primary evaluation metric for participating systems in the shared task, and it is covered in more detail in Section 4. The inter-annotator agreement was computed for each pair of annotators and averaged over all possible pairs for a final agreement score. On the Trial dataset, a kappa index of 0.386 was found, while for the Test dataset, a kappa index of 0.398 was found. It may be noted that certain annotators disagreed considerably with all others. For example, on the Test set, if annotations from one judge are removed, the average inter-annotator agreement rises to 0.443. </context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Zaidan, 2011</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, and Omar Zaidan. 2011. Findings of the 2011 workshop on statistical machine translation. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 22–64, Edinburgh, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arnaldo Candido</author>
<author>Erick Maziero</author>
<author>Caroline Gasperin</author>
<author>Thiago A S Pardo</author>
<author>Lucia Specia</author>
<author>Sandra M Aluisio</author>
</authors>
<title>Supporting the adaptation of texts for poor literacy readers: a text simplification editor for Brazilian Portuguese.</title>
<date>2009</date>
<booktitle>In Proceedings of the Fourth Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>34--42</pages>
<location>Boulder, Colorado.</location>
<contexts>
<context position="3506" citStr="Candido et al., 2009" startWordPosition="519" endWordPosition="522">ropriately. Finally, the system would determine the simplest of these substitutes, e.g., cruelty, and use it to replace the complex word, yielding the sentence: “Hitler committed terrible cruelties during the second World War.”. Different from other subtasks of Text Simplification like Syntactic Simplification, which have been relatively well studied, Lexical Simplification has received less attention. Although a few recent attempts explicitly address dependency on context (de Belder et al., 2010; Yatskar et al., 2010; Biran et al., 2011; Specia, 2010), most approaches are contextindependent (Candido et al., 2009; Devlin and Tait, 1998). In addition, a general deeper understanding 347 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 347–355, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics of the problem is yet to be gained. As a first attempt to address this problem in the shape of a shared task, the English Simplification task at SemEval-2012 focuses on the third component, which we believe is the core of the Lexical Simplification problem. The SemEval-2012 shared task on English Lexical Simplification has been conceived with the followin</context>
</contexts>
<marker>Candido, Maziero, Gasperin, Pardo, Specia, Aluisio, 2009</marker>
<rawString>Arnaldo Candido, Jr., Erick Maziero, Caroline Gasperin, Thiago A. S. Pardo, Lucia Specia, and Sandra M. Aluisio. 2009. Supporting the adaptation of texts for poor literacy readers: a text simplification editor for Brazilian Portuguese. In Proceedings of the Fourth Workshop on Innovative Use of NLP for Building Educational Applications, pages 34–42, Boulder, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cohen</author>
</authors>
<title>A coefficient of agreement for nominal scales.</title>
<date>1960</date>
<booktitle>Educational and Psychological Measurement,</booktitle>
<pages>20--37</pages>
<contexts>
<context position="14950" citStr="Cohen, 1960" startWordPosition="2357" endWordPosition="2358">king (note that the nature of the problem enforces a context-by-context ranking, as opposed to a global score), Other metrics for localized, pairwise rank correlation, such as Kendall’s Tau, disregard ties, – which are important for our purposes – and are thus not suitable. The main evaluation metric proposed for this shared task is in fact a measure of inter-annotator agreement, which is used for both contrasting two human annotators (Section 3.2) and contrasting a system output to the average of human annotations that together forms the gold-standard. Out metric is based on the kappa index (Cohen, 1960) which in spite of many criticisms is widely used for its simplicity and adaptability for different applications. The generalized form of the kappa index is P(A) − P(E) 1 − P(E) where P(A) denotes the proportion of times two annotators agree and P(E) gives the probability of agreement by chance between them. In order to apply the kappa index for a ranking task, we follow the method proposed by (CallisonBurch et al., 2011) for measuring agreement over judgments of translation quality. This method defines P(A) and P(E) in such a way that it now counts agreement whenever annotators concur upon th</context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>J Cohen. 1960. A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20:37–46, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan de Belder</author>
<author>Koen Deschacht</author>
<author>Marie-Francine Moens</author>
</authors>
<title>Lexical simplification.</title>
<date>2010</date>
<booktitle>In Proceedings of Itec2010: 1st International Conference on Interdisciplinary Research on Technology, Education and Communication,</booktitle>
<location>Kortrijk, Belgium.</location>
<marker>de Belder, Deschacht, Moens, 2010</marker>
<rawString>Jan de Belder, Koen Deschacht, and Marie-Francine Moens. 2010. Lexical simplification. In Proceedings of Itec2010: 1st International Conference on Interdisciplinary Research on Technology, Education and Communication, Kortrijk, Belgium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siobhan Devlin</author>
<author>John Tait</author>
</authors>
<title>The use of a psycholinguistic database in the simplification of text for aphasic readers. Linguistic Databases,</title>
<date>1998</date>
<pages>161--173</pages>
<contexts>
<context position="3530" citStr="Devlin and Tait, 1998" startWordPosition="523" endWordPosition="526">he system would determine the simplest of these substitutes, e.g., cruelty, and use it to replace the complex word, yielding the sentence: “Hitler committed terrible cruelties during the second World War.”. Different from other subtasks of Text Simplification like Syntactic Simplification, which have been relatively well studied, Lexical Simplification has received less attention. Although a few recent attempts explicitly address dependency on context (de Belder et al., 2010; Yatskar et al., 2010; Biran et al., 2011; Specia, 2010), most approaches are contextindependent (Candido et al., 2009; Devlin and Tait, 1998). In addition, a general deeper understanding 347 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 347–355, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics of the problem is yet to be gained. As a first attempt to address this problem in the shape of a shared task, the English Simplification task at SemEval-2012 focuses on the third component, which we believe is the core of the Lexical Simplification problem. The SemEval-2012 shared task on English Lexical Simplification has been conceived with the following main purposes: advanci</context>
</contexts>
<marker>Devlin, Tait, 1998</marker>
<rawString>Siobhan Devlin and John Tait. 1998. The use of a psycholinguistic database in the simplification of text for aphasic readers. Linguistic Databases, pages 161– 173.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujay Kumar Jauhar</author>
<author>Lucia Specia</author>
</authors>
<title>UOWSHEF: SimpLex - Lexical Simplicity Ranking based on Contextual and Psycholinguistic Features.</title>
<date>2012</date>
<booktitle>In English Lexical Simplification. Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="24784" citStr="Jauhar and Specia, 2012" startWordPosition="4024" endWordPosition="4027">tensive and nonfree resource such as the Web1T corpus makes a difference over other free and lightweight resources. UNT-SaLSA: The only resource SaLSA depends on is the Web1T data, and in particular only 3-grams from this corpus. It leverages the context provided with the dataset by replacing the target placeholder one by one with each of the substitutes and their inflections thus building sets of 3-grams for each substitute in a given instance. The score of any substitute is then the sum of the 3-gram frequencies of all the generated 3-grams for that substitute. UOW-SHEF-SimpLex: The system (Jauhar and Specia, 2012) uses a linear weighted ranking function composed of three features to produce a ranking. These include a context sensitive n-gram frequency model, a bag-of-words model and a feature composed of simplicity oriented psycholinguistic features. These three features are combined using an SVM ranker that is trained and tuned on the Trial dataset. 6.2 Pairwise kappa The official task results and the ranking of the systems are shown in Table 3. Firstly, it is worthwhile to note that all the top ranking systems include features that use frequency as a surrogate measure for lexical simplicity. This ind</context>
</contexts>
<marker>Jauhar, Specia, 2012</marker>
<rawString>Sujay Kumar Jauhar and Lucia Specia. 2012. UOWSHEF: SimpLex - Lexical Simplicity Ranking based on Contextual and Psycholinguistic Features. In English Lexical Simplification. Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anne-Laure Ligozat</author>
<author>Cyril Grouin</author>
<author>Anne GarciaFernandez</author>
<author>Delphine Bernhard</author>
</authors>
<title>ANNLOR: A Naive Notation-system for Lexical Outputs Ranking.</title>
<date>2012</date>
<booktitle>In English Lexical Simplification. Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="20267" citStr="Ligozat et al., 2012" startWordPosition="3308" endWordPosition="3311">forms very strongly, in spite of its simplistic approach, which is entirely agnostic to context. In fact it surpasses the average inter-annotator agreement on both Trial and Test datasets. Indeed, the scores on the Test set approach the best inter-annotator agreement scores between any two annotators. κ = � INI n=1 351 Trial Test L-Sub Gold 0.050 0.106 Random 0.016 0.012 Simple Freq. 0.397 0.471 Table 2: Baseline kappa scores on trial and test sets 6 Results and Discussion 6.1 Participants Five sites submitted one or more systems to the task, totaling nine systems: ANNLOR-lmbing: This system (Ligozat et al., 2012) relies on language models probabilities, and builds on the principle of the Simple Frequency baseline. While the baseline uses Google n-grams to rank substitutes, this approach uses Microsoft Web n-grams in the same way. Additionally characteristics, such as the contexts of each term to be substituted, were integrated into the system. Microsoft Web N-gram Service was used to obtain log likelihood probabilities for text units, composed of the lexical item and 4 words to the left and right from the surrounding context. ANNLOR-simple: The system (Ligozat et al., 2012) is based on Simple English </context>
</contexts>
<marker>Ligozat, Grouin, GarciaFernandez, Bernhard, 2012</marker>
<rawString>Anne-Laure Ligozat, Cyril Grouin, Anne GarciaFernandez, and Delphine Bernhard. 2012. ANNLOR: A Naive Notation-system for Lexical Outputs Ranking. In English Lexical Simplification. Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Roberto Navigli</author>
</authors>
<title>Semeval2007 task 10: English lexical substitution task.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>48--53</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="1769" citStr="McCarthy and Navigli, 2007" startWordPosition="254" endWordPosition="257">regardless of its context. 1 Introduction Lexical Simplification is a subtask of Text Simplification (Siddharthan, 2006) concerned with replacing words or short phrases by simpler variants in a context aware fashion (generally synonyms), which can be understood by a wider range of readers. It generally envisages a certain human target audience that may find it difficult or impossible to understand complex words or phrases, e.g., children, people with poor literacy levels or cognitive disabilities, or second language learners. It is similar in many respects to the task of Lexical Substitution (McCarthy and Navigli, 2007) in that it involves determining adequate substitutes in context, but in this case on the basis of a predefined criterion: simplicity. A common pipeline for a Lexical Simplification system includes at least three major components: (i) complexity analysis: selection of words or phrases in a text that are considered complex for the reader and/or task at hand; (ii) substitute lookup: search for adequate replacement words or phrases deemed complex in context, e.g., taking synonyms (with the same sense) from a thesaurus or finding similar words/phrases in a corpus using distributional similarity me</context>
<context position="6416" citStr="McCarthy and Navigli, 2007" startWordPosition="975" endWordPosition="978">derate, complex, etc.), we avoid the need for a fixed number of categories and for more subjective judgments. Also ranking enables a more natural and intuitive way for humans (and systems) to perform annotations by preventing them from treating each individual case in isolation, as opposed to relative to each other. However, the inherent subjectivity introduced by ranking entails higher disagreement among human annotators, and more complexity for systems to tackle. 3 Corpus compilation The trial and test corpora were created from the corpus of SemEval-2007 shared task on Lexical Substitution (McCarthy and Navigli, 2007). This decision was motivated by the similarity between the two tasks. Moreover the existing corpus provided an adequate solution given time and cost constraints for our corpus creation. Given existing contexts with the original target word replaced by a placeholder and the lists of substitutes (including the target word), annotators (and systems) are required to rank substitutes in order of simplicity for each context. 3.1 SemEval-2007 - LS corpus The corpus from the shared task on Lexical Substitution (LS) at SemEval-2007 is a selection of sentences, or contexts, extracted from the English I</context>
</contexts>
<marker>McCarthy, Navigli, 2007</marker>
<rawString>Diana McCarthy and Roberto Navigli. 2007. Semeval2007 task 10: English lexical substitution task. In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), Prague, Czech Republic, pages 48–53.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Noreen</author>
</authors>
<title>Computer-intensive methods for testing hypotheses.</title>
<date>1989</date>
<publisher>Wiley.</publisher>
<location>New York:</location>
<contexts>
<context position="18095" citStr="Noreen, 1989" startWordPosition="2946" endWordPosition="2947">viving son of Aman-ul-Mulk, as the ruler of Chitral. Gold: {intelligent} {clever} {smart} {bright} System: {intelligent} {bright} {clever, smart} Out of the 6 distinct unordered pairs of lexical items, system and gold agreed 3 times. Consequently, Pn(A) = 3�. In addition, count(a = b) = 1. Thus, Pn(a = b) = 112. Which gives a P(E) = 41 96 and the final kappa score for this particular context of 0.13. The statistical significance of the results from two systems A and B is measured using the method of Approximate Randomization, which has been shown to be a robust approach for several NLP tasks (Noreen, 1989). The randomization is run 1, 000 times and if the p-value is &lt; 0.05 the difference between systems A and B is asserted as being statistically significance. 5 Baselines We defined three baseline lexical simplification systems for this task, as follows. L-Sub Gold: This baseline uses the gold-standard annotations from the Lexical Substitution corpus of SemEval-2007 as is. In other words, the ranking is based on the goodness offit of substitutes for a context, as judged by human annotators. This method also serves to show that the Lexical Substitution and Lexical Simplification tasks are indeed </context>
</contexts>
<marker>Noreen, 1989</marker>
<rawString>E. Noreen. 1989. Computer-intensive methods for testing hypotheses. New York: Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Serge Sharoff</author>
</authors>
<title>Open-source corpora: Using the net to fish for linguistic data.</title>
<date>2006</date>
<journal>International Journal of Corpus Linguistics,</journal>
<volume>11</volume>
<issue>4</issue>
<contexts>
<context position="7057" citStr="Sharoff, 2006" startWordPosition="1080" endWordPosition="1081">ed by the similarity between the two tasks. Moreover the existing corpus provided an adequate solution given time and cost constraints for our corpus creation. Given existing contexts with the original target word replaced by a placeholder and the lists of substitutes (including the target word), annotators (and systems) are required to rank substitutes in order of simplicity for each context. 3.1 SemEval-2007 - LS corpus The corpus from the shared task on Lexical Substitution (LS) at SemEval-2007 is a selection of sentences, or contexts, extracted from the English Internet Corpus of English (Sharoff, 2006). It contains samples of English texts crawled from the web. This selection makes up the dataset of a total of 2, 010 contexts which are divided into Trial and Test sets, consisting of 300 and 1710 contexts respectively. It covers a total of 201 (mostly polysemous) target words, including nouns, verbs, adjectives and adverbs, and each of the target words is shown in 10 different contexts. Annotators had been asked to suggest up to three different substitutes (words or short phrases) for each of the target words within their contexts. The substitutes were lemmatized unless it was deemed that th</context>
</contexts>
<marker>Sharoff, 2006</marker>
<rawString>Serge Sharoff. 2006. Open-source corpora: Using the net to fish for linguistic data. International Journal of Corpus Linguistics, 11(4):435–462.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Advaith Siddharthan</author>
</authors>
<title>Syntactic simplification and text cohesion.</title>
<date>2006</date>
<booktitle>Research on Language and Computation,</booktitle>
<pages>4--77</pages>
<contexts>
<context position="1262" citStr="Siddharthan, 2006" startWordPosition="176" endWordPosition="177">al simplification approaches. The task requires that annotators and systems rank a number of alternative substitutes – all deemed adequate – for a target word in context, according to how “simple” these substitutes are. The notion of simplicity is biased towards non-native speakers of English. Out of nine participating systems, the best scoring ones combine contextdependent and context-independent information, with the strongest individual contribution given by the frequency of the substitute regardless of its context. 1 Introduction Lexical Simplification is a subtask of Text Simplification (Siddharthan, 2006) concerned with replacing words or short phrases by simpler variants in a context aware fashion (generally synonyms), which can be understood by a wider range of readers. It generally envisages a certain human target audience that may find it difficult or impossible to understand complex words or phrases, e.g., children, people with poor literacy levels or cognitive disabilities, or second language learners. It is similar in many respects to the task of Lexical Substitution (McCarthy and Navigli, 2007) in that it involves determining adequate substitutes in context, but in this case on the bas</context>
</contexts>
<marker>Siddharthan, 2006</marker>
<rawString>Advaith Siddharthan. 2006. Syntactic simplification and text cohesion. Research on Language and Computation, 4:77–109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ravi Sinha</author>
</authors>
<title>UNT-SimpRank: Systems for Lexical Simplification Ranking.</title>
<date>2012</date>
<booktitle>In English Lexical Simplification. Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="23473" citStr="Sinha, 2012" startWordPosition="3809" endWordPosition="3810">requency from Wordnet as a metric. In the case of multi-words substitutes the system uses “relevance” rules that apply (de)compositional semantic criteria and attempts to identify a unique content word in the substitute that might better approximate the whole expression. The expression is then assigned the frequency associated to this content word for the ranking. After POS tagging and sense disambiguating all substitutes, hand-written rules are used to decompose the meaning of a complex phrase and identify the most relevant word conveying the semantics of the whole. UNT-SimpRank: The system (Sinha, 2012) uses external resources, including the Simple English Wikipedia corpus, a set of Spoken English dialogues, transcribed into machine readable form, WordNet, and unigram frequencies (Google Web1T data). SimpRank scores each substitute by a sum of its unigram frequency, its 352 frequency in the Simple English Wikipedia, its frequency in the spoken corpus, the inverse of its length, and the number of senses the substitute has in WordNet. For a given context, the substitutes are then reverse-ranked based on their simplicity scores. UNT-SimpRankLight: This is a variant of SimpRank which does not us</context>
</contexts>
<marker>Sinha, 2012</marker>
<rawString>Ravi Sinha. 2012. UNT-SimpRank: Systems for Lexical Simplification Ranking. In English Lexical Simplification. Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucia Specia</author>
</authors>
<title>Translating from complex to simplified sentences.</title>
<date>2010</date>
<booktitle>In Proceedings of the 9th international conference on Computational Processing of the Portuguese Language, PROPOR’10,</booktitle>
<pages>30--39</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="3444" citStr="Specia, 2010" startWordPosition="512" endWordPosition="513">ty should be dropped as it does not fit the context appropriately. Finally, the system would determine the simplest of these substitutes, e.g., cruelty, and use it to replace the complex word, yielding the sentence: “Hitler committed terrible cruelties during the second World War.”. Different from other subtasks of Text Simplification like Syntactic Simplification, which have been relatively well studied, Lexical Simplification has received less attention. Although a few recent attempts explicitly address dependency on context (de Belder et al., 2010; Yatskar et al., 2010; Biran et al., 2011; Specia, 2010), most approaches are contextindependent (Candido et al., 2009; Devlin and Tait, 1998). In addition, a general deeper understanding 347 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 347–355, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics of the problem is yet to be gained. As a first attempt to address this problem in the shape of a shared task, the English Simplification task at SemEval-2012 focuses on the third component, which we believe is the core of the Lexical Simplification problem. The SemEval-2012 shared task on Engli</context>
</contexts>
<marker>Specia, 2010</marker>
<rawString>Lucia Specia. 2010. Translating from complex to simplified sentences. In Proceedings of the 9th international conference on Computational Processing of the Portuguese Language, PROPOR’10, pages 30–39, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Yatskar</author>
<author>Bo Pang</author>
<author>Cristian Danescu-NiculescuMizil</author>
<author>Lillian Lee</author>
</authors>
<title>For the sake of simplicity: unsupervised extraction of lexical simplifications from Wikipedia.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>365--368</pages>
<location>Los Angeles, California.</location>
<contexts>
<context position="3409" citStr="Yatskar et al., 2010" startWordPosition="504" endWordPosition="507">ruelty, enormity and violation, but enormity should be dropped as it does not fit the context appropriately. Finally, the system would determine the simplest of these substitutes, e.g., cruelty, and use it to replace the complex word, yielding the sentence: “Hitler committed terrible cruelties during the second World War.”. Different from other subtasks of Text Simplification like Syntactic Simplification, which have been relatively well studied, Lexical Simplification has received less attention. Although a few recent attempts explicitly address dependency on context (de Belder et al., 2010; Yatskar et al., 2010; Biran et al., 2011; Specia, 2010), most approaches are contextindependent (Candido et al., 2009; Devlin and Tait, 1998). In addition, a general deeper understanding 347 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 347–355, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics of the problem is yet to be gained. As a first attempt to address this problem in the shape of a shared task, the English Simplification task at SemEval-2012 focuses on the third component, which we believe is the core of the Lexical Simplification problem. Th</context>
</contexts>
<marker>Yatskar, Pang, Danescu-NiculescuMizil, Lee, 2010</marker>
<rawString>Mark Yatskar, Bo Pang, Cristian Danescu-NiculescuMizil, and Lillian Lee. 2010. For the sake of simplicity: unsupervised extraction of lexical simplifications from Wikipedia. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 365–368, Los Angeles, California.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>