<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000113">
<title confidence="0.976952">
Hypertext Authoring for Linking Relevant Segments of
Related Instruction Manuals
</title>
<author confidence="0.976115">
Hiroshi Nakagawa and Tatsunori Mori and Nobuyuki Omori and Jun Okamura
</author>
<affiliation confidence="0.7676425">
Department of Computer and Electronic Engineering, Yokohama National University
Tokiwadai 79-5, Hodogaya, Yokohama, 240-8501, JAPAN
</affiliation>
<email confidence="0.933238">
E-mail: nakagawa©naklab.dnj.ynu.ac.jp,{mori,ohmori,jun} ©forest .dnj .ynu .ac.j p
</email>
<sectionHeader confidence="0.992474" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998898666666667">
Recently manuals of industrial products become
large and often consist of separated volumes. In
reading such individual but related manuals, we
must consider the relation among segments, which
contain explanations of sequences of operation. In
this paper, we propose methods for linking relevant
segments in hypertext authoring of a set of related
manuals. Our method is based on the similarity
calculation between two segments. Our experimen-
tal results show that the proposed method improves
both recall and precision comparing with the con-
ventional if. idf based method.
</bodyText>
<sectionHeader confidence="0.99843" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999780866666667">
In reading traditional paper based manuals, we
should use their indices and table of contents in or-
der to know where the contents we want to know are
written. In fact, it is not an easy task especially for
novices. Recent years, electronic manuals in a form
of hypertext like Help of Microsoft Windows became
widely used. Unfortunately it is very expensive to
make a hypertext manual by hand especially in case
of a large volume of manual which consists of sev-
eral separated volumes. In a case of such a large
manual, the same topic appears at several places in
different volumes. One of them is an introductory
explanation for a novice. Another is a precise ex-
planation for an advanced user. It is very useful to
jump from one of them to another of them directly
by just clicking a button of mouse in reading a man-
ual text on a browser like NetScape. This type of
access is realized by linking them in hypertext for-
mat by hypertext authoring.
Automatic hypertext authoring has been focused
on in these years, and much work has been done. For
instance, Basili et al. (1994) use document struc-
tures and semantic information by means of natural
language processing technique to set hyperlinks on
plain texts.
The essential point in the research of automatic
hypertext authoring is the way to find semantically
relevant parts where each part is characterized by
a number of key words. Actually it is very similar
with information retrieval, IR henceforth, especially
with the so called passage retrieval (Salton et al.,
1993). J.Green (1996) does hypertext authoring of
newspaper articles by word&apos;s lexical chains which are
calculated using WordNet. Kurohashi et al. (1992)
made a hypertext dictionary of the field of infor-
mation science. They use linguistic patterns that
are used for definition of terminology as well as the-
saurus based on words&apos; similarity. Furner-Hines and
Willett (1994) experimentally evaluate and compare
the performance of several human hyper linkers. In
general, however, we have not yet paid enough at-
tention to a full-automatic hyper linker system, that
is what we pursue in this paper.
The new ideas in our system are the following
points:
</bodyText>
<listItem confidence="0.954295111111111">
1. Our target is a multi-volume manual that de-
scribes the same hardware or software but is dif-
ferent in their granularity of descriptions from
volume to volume.
2. In our system, hyper links are set not between
an anchor word and a certain part of text but
between two segments, where a segment is a
smallest formal unit in document, like a sub-
subsection of 14c1bX if no smaller units like
subsubsubsection are used.
3. We find pairs of relevant segments over two
volumes, for instance, between an introductory
manual for novices and a reference manual for
advanced level users about the same software or
hardware.
4. We use not only t f •idf based vector space model
but also words&apos; co-occurrence information to
measure the similarity between segments.
</listItem>
<sectionHeader confidence="0.899658" genericHeader="method">
2 Similarity Calculation
</sectionHeader>
<bodyText confidence="0.985731666666667">
We need to calculate a semantic similarity between
two segments in order to decide whether two of them
are linked, automatically. The most well known
method to calculate similarity in IR is a vector space
model based on if • idf value. As for idf, namely
inverse document frequency, we adopt a segment in-
</bodyText>
<page confidence="0.997615">
929
</page>
<bodyText confidence="0.999529">
stead of document in the definition of idf. The def-
inition of idf in our system is the following.
</bodyText>
<equation confidence="0.805008">
of segments in the manual
idf (t) = log + 1it of segments in which t occurs
</equation>
<bodyText confidence="0.9993191875">
For example, suppose a sentence &amp;quot;An end user
learns the programming language.&amp;quot; Then in ad-
dition to dimensions corresponding to every noun
phrase like &amp;quot;end user&amp;quot;, we introduce the new di-
mensions corresponding to co-occurrence informa-
tion such as:
Then a segment is described as a vector in a vector
space. Each dimension of the vector space consists
of each term used in the manual. A vector&apos;s value
of each dimension corresponding to the term t is
its if • idf value. The similarity of two segments is
a cosine of two vectors corresponding to these two
segments respectively. Actually the cosine measure
similarity based on if • idf is a baseline in evaluation
of similarity measures we propose in the rest of this
section.
As the first expansion of definition of if • idf, we
use case information of each noun. In Japanese, case
information is easily identified by the case particle
like ga( nominal marker ), o( accusative marker ),
ni( dative marker ) etc. which are attached just af-
ter a noun. As the second expansion, we use not only
nouns (-I- case information) but also verbs because
verbs give important information about an action a
user does in operating a system. As the third expan-
sion, we use co-occurrence information of nouns and
verbs in a sentence because combination of nouns
and a verb gives us an outline of what the sentence
describes. The problem at this moment is the way
to reflect co-occurrence information in If • idf based
vector space model. We investigate two methods for
this, namely,
</bodyText>
<listItem confidence="0.9994855">
1. Dimension expansion of vector space, and
2. Modification of if value within a segment.
</listItem>
<bodyText confidence="0.944303">
In the following, we describe the detail of these two
methods.
</bodyText>
<subsectionHeader confidence="0.992887">
2.1 Dimension Expansion
</subsectionHeader>
<bodyText confidence="0.99973375">
This method is adding extra-dimensions into the
vector space in order to express co-occurrence in-
formation. It is described more precisely as the fol-
lowing procedure.
</bodyText>
<listItem confidence="0.95305">
1. Extracting a case information (case particle in
Japanese) from each noun phrase. Extracting a
verb from a clause.
2. Suppose be there n noun phrases with a case
particle in a clause. Enumerating every combi-
nation of 1 to n noun phrases with case particle.
</listItem>
<bodyText confidence="0.978896">
Then we have EnCk combinations.
</bodyText>
<page confidence="0.345478">
k=1
</page>
<listItem confidence="0.999064">
3. Calculating if • idf for every combination with
the corresponding verb. And using them as new
extra dimensions of the original vector space.
• (VERB, learn) (NOMNINAL end user) (AC-
CUSATIVE programming language)
• (VERB, learn) (NOMNINAL end user)
• (VERB, learn) (ACCUSATIVE programming
language)
</listItem>
<bodyText confidence="0.9754796">
We calculate if • idf of each of these combinations
that is a value of vector corresponding to each of
these combinations. The similarity calculation based
on cosine measure is done on this expanded vector
space.
</bodyText>
<subsectionHeader confidence="0.999916">
2.2 Modification of if value
</subsectionHeader>
<bodyText confidence="0.999981583333333">
Another method we propose for reflecting co-
occurrence information to similarity is modification
of if value within a segment. (Takaki and Kitani,
1996) reports that co-occurrence of word pairs con-
tributes to the IR performance for Japanese news
paper articles.
In our method, we modify if of pairs of co-
occurred words that occur in both of two segments,
say dA and dB, in the following way. Suppose that a
term tk, namely noun or verb, occurs f times in the
segment dA. Then the modified tf(dA, tk) is defined
as the following formula.
</bodyText>
<equation confidence="0.9868326">
tt(dA,tk) = tf(dA,tk)
• E E CW(CIA,ik,p,t,)
tcETc(ik,dA,cia)p=1
• E EcodA,tk,p,t,)
icET,(tk,dA,dB)p.i
</equation>
<bodyText confidence="0.9997896">
where cw and cw&apos; are scores of importance for co-
occurrence of words, tk and tc. Intuitively, cw and
cw&apos; are counter parts of if. idf for co-occurrence of
words and co-occurrence of (noun case-information),
respectively. cw is defined by the following formula.
</bodyText>
<equation confidence="0.944425333333333">
cw(dA,tk,p,t,)
ct(dA,tk,p,tc) x /3(tk,te) x -y(tk,tc) x C
M(dA )
</equation>
<bodyText confidence="0.999767">
where a(dA,tk,p,tc) is a function expressing how
near tk and tc occur, p denotes that pth tk &apos;s occur-
rence in the segment dA, and fl(tk, tc) is a normal-
ized frequency of co-occurrence of tk and tc. Each
of them is defined as follows.
</bodyText>
<equation confidence="0.9814848">
a(dA,tk,Atc) — d(dA,tk,P)
CI(CIAlik)P) — diSi(dA,ik)Atc)
930
rtf(tk,t,)
13(tk&apos;tcl = at f(tk)
</equation>
<bodyText confidence="0.999483666666667">
where the function dist(dA,tk,p,te) is a distance
between pth tk within dA and te counted by word.
d(dA,tk,p) shows the threshold of distance within
which two words are regarded as a co-occurrence.
Since, in our system, we only focus on co-occurrences
within a sentence, ce(dA,tk,p,t,) is calculated for
pairs of word occurrences within a sentence. As a
result, d(dA,tk,p) is a number of words in a sen-
tence we focus on. au f (1k) is a total number of
tk&apos;s occurrences within the manual we deal with.
rtf(tk,t,) is a total number of co-occurrences of tk
and 1, within a sentence. 7(tk,t,) is an inverse doc-
ument frequency ( in this case &amp;quot;inverse segment fre-
quency&amp;quot;) of te which co-occurs with tk, and defined
as follows.
</bodyText>
<equation confidence="0.54296">
= 1o9( df))
</equation>
<bodyText confidence="0.998794923076923">
where N is a number of segments in a manual,
and df(i) is a number segments in which te occurs
with tk.
M(dA) is a length of segment dA counted in mor-
phological unit, and used to normalize cw. C is a
weight parameter for cw. Actually we adopt the
value of C which optimizes 11point precision as de-
scribed later.
The other modification factor cw&apos; is defined in al-
most the same way as cw is. The difference between
cw and cw&apos; is the following. cw is calculated for
each noun. On the other hand, cw&apos; is calculated for
each combination of noun and its case information.
Therefore, cw&apos; is calculated for each ( noun, case )
like (user, NOMINAL). In other words, in calcula-
tion of cw&apos;, only when (noun-i, case-1 ) and ( noun-
2, case-2 ), like (user NOMINAL) and (program AC-
CUSATIVE), occur within the same sentence, they
are regarded as a co-occurrence.
Now we have defined cw and cw&apos;. Then back to
the formula which defines if&apos;. In the definition of
if&apos;, Tc(tk,dA,dB) is a set of word which occur in
both of dA and dB. Therefore cws and cw&apos;s are
summed up for all occurrences of tk in dA. Namely
we add up all cws and cw&apos;s whose t, is included in
Tc(tk,dA,dB) to calculate if&apos;.
</bodyText>
<sectionHeader confidence="0.9991105" genericHeader="method">
3 Implementation and Experimental
Results
</sectionHeader>
<bodyText confidence="0.85429775">
Our system has the following inputs and outputs.
Input is an electronic manual text which can be
written in plain textOTEXor HTML)
Output is a hypertext in HTML format.
</bodyText>
<figureCaption confidence="0.998362">
Figure 1: Overview of our hypertext generator
</figureCaption>
<bodyText confidence="0.907905">
We need a browser like NetScape that can display
a text written in HTML. Our system consists of four
sub-systems shown in Figure 1.
Keyword Extraction Sub-System In this sub-
system, a morphological analyzer segments out
the input text, and extract all nouns and verbs
that are to be keywords. We use Chasen 1.04b
(Matsumoto et al., 1996) as a morphological
analyzer for Japanese texts. Noun and Case-
information pairs are also made in this sub-
system. If you use the dimension expansion de-
scribed in 2.1, you introduce new dimensions
here.
tf idf Calculation Sub-System
This sub-system calculates if • idf of extracted
keywords by Keyword Extraction Sub-System.
</bodyText>
<subsubsectionHeader confidence="0.658372">
Similarity Calculation Sub-System This sub-
</subsubsectionHeader>
<bodyText confidence="0.986084923076923">
system calculates the similarity that is repre-
sented by cosine of every pair of segments based
on if idf values calculated above. If you use
modifications of if values described in 2.2, you
calculated modified if, namely if&apos; in this sub-
system.
Hypertext Generator This sub-system trans-
lates the given input text into a hypertext in
which pairs of segments having high similarity,
say high cosine value, are linked. The similarity
of those pairs are associated with their links for
user friendly display described in the following
We show an example of display on a browser in
</bodyText>
<figureCaption confidence="0.926063333333333">
Figure 2. The display screen is divided into four
parts. The upper left and upper right parts show
a distinct part of manual text respectively. In the
lower left (right) part, the title of segments that
are relevant to the segment displayed on the upper
left (right) part are displayed in descending order of
</figureCaption>
<figure confidence="0.999503277777778">
INPUT
utomadt Hyper
.. -
text Generator
Word-co-occurrences
and Case information
Morphological Analysis
System
Keyword Extract
Electronic Manuals
manual A manual Es
t f iatcutation
Similasity Calculation
based on Vector Space Model
Hypertext Unk Genarator
manual A manual B
OUTPUT
HYPERTEXT
</figure>
<page confidence="0.985102">
931
</page>
<table confidence="0.998748">
Elle Edit View Go Bocknarks Options Directory Window ii:71
.&apos;1. . L Y..
: J.. . . LI]. . . 1 4 I&apos; .. 1. PA i j..
..f .. . ._6._1.. .&apos; .:•gl .
Location: linty:now. forest. dn.i. mu. ac. iPriunrina_chafrarne. htaLl 14%J
What&apos;s New? What&apos;s Cool?I Destiostionsi Net Ssarchl Pe__9211e Software
E JUMAN 2.0 b. r- &apos; F JUMAN 2.0 lx. t›
ChaSen 1.0 &apos;•.a)trag.4 JUMAN 3.0 f\CT)11/30.A
NE i r NN .5 f e..03 . )-. 1 cisE1.9,0).):, ,
1. gr. &lt; 0375 , I- 7 ..- -.1...&amp;quot;&amp;quot;ce&apos; 1.•
7... I- -JI:.11C Z,..k 3 L: A ws■
el:Alt-5077&apos; 0 2OoEill/a,
(111./Z.
2 riqiNtiNaTh-&amp;quot;M &apos;10..3/AdONIce hi
1k.,-Clitftights:TstlE4111153t
Zif.fr ,... 19Ig Nino), v
I-- ...1 ri 77Ic ,141. ,t, t. 03 V.:RN
1.-71. Mtn tzeZTEtailio3.7-f
X /MON Y031 t: ti e r:. sr:.
PM a&gt; &amp;quot;.:.,I r -f limmu0*,3&amp;quot;-303).
MI L.Zti e r--
S. latif:..- L. 03.R.ai 1.... -7-tit. •
, aCialtelt1 --, 7.-- WrIiIC71.&apos;
1P111--11Mr7.2 -3717- (JUMAN2.0
glIMIk UM Liat St, ,,cb r5 WO te.)
1.).-Fs&apos;AMAD1L Fi.k ill1140014-1,47 ,
r:.
I AMUR
s.110110,ilitaMs.
a. IgS01. M0.9313§01301,1
. 13 5, /31-28a3t/F,
s. 1.1MSAJR031‘5,T.4-77:, a %MA
NO
4. 8881649410321.81
7. 11812PSCM03066.
s. 11911.1.--2,-7-./..moDemi C- f.; 3
--&apos; iMilli/0
a • -21..• Ao - I.,&apos; it:
..1 ,
0 F.5 188,72rAAT,F7&apos;,.., 3 l&apos;,7) 3.658 teem AAA: P JUNIAN as I
imktoau 6.6 &amp;quot;..dOlgiilda
0 ASUMAN 20 iro-3 00.5,0 60 0.03
0 P /OMAN 2.0 1)`&amp;quot;i 11/16AN 3D &apos;,AD
ittarsat
. r &apos;LIMAN 3.0 e xu€ emaIAL:
• P.3 M. -
. Mi. jr.e_jaittin
, - •---1,-
zeal mr-ii
</table>
<figureCaption confidence="0.998784">
Figure 2: The use of this system
</figureCaption>
<bodyText confidence="0.999942785714286">
similarity. Since these titles are linked to the cor-
responding segment text, if we click one of them in
the lower left (right) part, the hyperlinked segment&apos;s
text is instantly displayed on the upper right (left)
part, and its relevant segments&apos; title are displayed
on the lower right (left) part. By this type of brows-
ing along with links displayed on the lower parts,
if a user wants to know relevant information about
what she/he is reading on the text displayed on the
upper part, a user can easily access the segments in
which what she/he wants to know might be written
in high probability.
Now we describe the evaluation of our proposed
methods with recall and precision defined as follows.
</bodyText>
<listItem confidence="0.9536326">
0 of retrieved pairs of relevant segments
recall -
0 of pairs of relevant segments
0 of retrieved pairs of relevant segments
0 of retrieved pairs of segments
</listItem>
<bodyText confidence="0.999920933333333">
The first experiment is done for a large manual
of APPGALLARY(Hitachi, 1995) which is 2.5MB
large. This manual is divided into two volumes. One
is a tutorial manual for novices that contains 65 seg-
ments. The other is a help manual for advanced
users that contains 2479 segments. If we try to find
the relevant segments between ones in the tutorial
manual and ones in the help manual, the number of
possible pairs of segments is 161135. This number
is too big for human to extract all relevant segment
manually. Then we investigate highest 200 pairs of
segments by hand, actually by two students in the
engineering department of our university to extract
pairs of relevant segments. The guideline of selection
of pairs of relevant segments is:
</bodyText>
<figure confidence="0.986804">
20 40 60 80 100 120 140 160 180 200
Rank-
</figure>
<figureCaption confidence="0.9897845">
Figure 3: Recall and precision of generated hyper-
links on large-scale manuals
</figureCaption>
<tableCaption confidence="0.9511615">
Table 1: Manual combinations and number of right
correspondences of segments
</tableCaption>
<table confidence="0.708355">
pair of manuals A&lt;#.13 AC Bt*C
0 of all pairs 1056 896 924
0 of relevant pairs 65 60 47
</table>
<listItem confidence="0.98341">
1. Two segments explain the same operation or the
same terminology.
2. One segment explains an abstract concept and
the other explains that concept in concrete op-
eration.
</listItem>
<bodyText confidence="0.999947107142857">
Figure 3 shows the recall and precision for num-
bers of selected pairs of segments where those pairs
are sorted in descending order of cosine similarity
value using normal if • idf of all nouns. This result
indicates that pairs of relevant segments are concen-
trated in high similarity area. In fact, the pairs of
segments within top 200 pairs are almost all relevant
ones.
The second experiment is done for three
small manuals of three models of video cas-
sette recorder(MITSUBISHI, 1995c; MITSUBISHI,
1995a; MITSUBISHI, 1995b) produced by the same
company. We investigate all pairs of segments
that appear in the distinct manuals respectively,
and extract relevant pairs of segment according
to the same guideline we did in the first experi-
ment by two students of the engineering depart-
ment of our university. The numbers of segments
are 32 for manual A(MITSUBISHI, 1995c), 33 for
manual B(MITSUBISHI, 1995a) and 28 for manual
C(MITSUBISHI, 1995b), respectively. The number
of relevant pairs of segments are shown in Table 1.
We show the 11 points precision averages for these
methods in Table 2. Each recall-precision curve,
say Keyword, dimension N, cw-Fcw&apos; tf, and Normal
Query, corresponds to the methods described in the
previous section. We describe the more precise defi-
nition of each in the following.
</bodyText>
<equation confidence="0.476532">
precision -
</equation>
<page confidence="0.992801">
932
</page>
<tableCaption confidence="0.999145">
Table 2: 11 point average of precision for each
method and combination
</tableCaption>
<table confidence="0.999093833333333">
Method AB AC B &lt;#. C
Keyword 0.678 0.589 0.549
cw+cw&apos; tf 0.683 0.625 0.582
C 0.1 0.6 1.3
dimension N 0.684 0.597 0.556
Normal Query 0.692 0.532 0.395
</table>
<bodyText confidence="0.973617142857143">
Keyword: Using if. idf for all nouns and verbs
occuring in a pair of manuals. This is the baseline
data.
dimension N: Dimension Expansion method de-
scribed in section 2.1. In this experiment, we use
only noun-noun co-occurrences.
cw+cw&apos; tf: Modification of if value method de-
scribed in section2.2. In this experiment, we use
only noun-verb co-occurrences.
Normal Query: This is the same as Keyword ex-
cept that vector values in one manual are all set to
0 or 1, and vector values of the other manual are
t f • idf
In the rest of this section, we consider the results
shown above point by point.
The effect of using if • idf information of both
segments
We consider the effect of using if • idf of two seg-
ments that we calculate similarity. For comparison,
we did the experiment Normal Query where t f • idf
is used as vector value for one segment and 1 or 0
is used as vector value for the other segment. This
is a typical situation in IR. In our system, we calcu-
late similarity of two segments .already given. That
makes us possible using if • idf for both segments.
As shown in Table 2, Keyword outperforms Nor-
mal Query.
The effect of using co-occurrence information
The same types of operation are generally de-
scribed in relevant segments. The same type of op-
eration consists of the same action and equipment
in high probability. This is why using co-occurrence
information in similarity calculation magnifies sim-
ilarities between relevant segments. Comparing di-
mension expansion and modification of if, the latter
outperforms the former in precision for almost all
recall rates. Modification of if value method also
shows better results than dimension expansion in 11
point precision average shown in Table 2 for A-C
and B-C manual pairs. As for normalization factor
C of modification of if value method, the smaller
C becomes, the less tf value changes and the more
similar the result becomes with the baseline case in
which only if is used. On the contrary, the bigger C
becomes, the more incorrect pairs get high similar-
ity and the precision deteriorates in low recall area.
As a result, there is an optimum C value, which we
selected experimentally for each pair of manuals and
is shown in Table 2 respectively.
</bodyText>
<sectionHeader confidence="0.999661" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999983777777778">
We proposed two methods for calculating similarity
of a pair of segments appearing in distinct manuals.
One is Dimension Expansion method, and the other
is Modification of if value method. Both of them
improve the recall and precision in searching pairs of
relevant segment .This type of calculation of similar-
ity between two segments is useful in implementing
a user friendly manual browsing system that is also
proposed and implemented in this research.
</bodyText>
<sectionHeader confidence="0.999151" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997209805555555">
Roberto Basili, Fabrizio Grisoli, and Maria Teresa
Pazienza. 1994. Might a semantic lexicon support
hypertextual authoring? In 4th ANLP, pages
174-179.
David Elhs. Jonathan Furner-Hines and Peter Wil-
lett. 1994. On the measurement of inter-linker
consistency and retrieval effectiveness in hyper-
text databases. In SIGIR &apos;94, pages 51-60.
Hitachi, 1995. How to use the APPGALLERY,
APPGALLERY On-Line Help. Hitachi Limited.
Stephen J.Green. 1996. Using lexcal chains to build
hypertext links in newspaper articles. In Proceed-
ings of AAAI Workshop on Knowledge Discovery
in Databases, Portland, Oregon.
S. Kurohashi, M. Nagao, S. Sato, and M. Murakami.
1992. A method of automatic hypertext construc-
tion from an encyclopedic dictionary of a specific
field. In 3rd ANLP, pages 239-240.
Yuji Matsumoto, Osamu Imaichi, Tatsuo Ya-
mashita, Akira Kitauchi, and Tomoaki Imamura.
1996. Japanese morphological analysis system
ChaSen manual (version 1.0b4). Nara Institute of
Science and Technology, Nov.
MITSUBISHI, 1995a. MITSUBISHI Video Tape
Recorder HV-BZ66 Instruction Manual.
MITSUBISHI, 1995b. MITSUBISHI Video Tape
Recorder HV-F93 Instruction Manual.
MITSUBISHI, 1995c. MITSUBISHI Video Tape
Recorder HV-FZ62 Instruction Manual.
Gerard Salton, J. Allan, and Chris Buckley. 1993.
Approaches to passage retrieval in full text infor-
mation systems. In SIGIR &apos;93, pages 49-58.
Toru Takaki and Tsuyoshi Kitani. 1996. Rele-
vance ranking of documents using query word co-
occurrences (in Japanese). IPSJ SIG Notes 96-FI-
41-8, IPS Japan, April.
</reference>
<page confidence="0.999084">
933
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.888235">
<title confidence="0.997719">Hypertext Authoring for Linking Relevant Segments of Related Instruction Manuals</title>
<author confidence="0.998633">Nakagawa Mori Omori Okamura</author>
<affiliation confidence="0.999999">Department of Computer and Electronic Engineering, Yokohama National University</affiliation>
<address confidence="0.997166">Tokiwadai 79-5, Hodogaya, Yokohama, 240-8501, JAPAN</address>
<email confidence="0.982767">E-mail:nakagawa©naklab.dnj.ynu.ac.jp,{mori,ohmori,jun}©forest.dnj.ynu.ac.jp</email>
<abstract confidence="0.993131923076923">Recently manuals of industrial products become large and often consist of separated volumes. In reading such individual but related manuals, we must consider the relation among segments, which contain explanations of sequences of operation. In this paper, we propose methods for linking relevant segments in hypertext authoring of a set of related manuals. Our method is based on the similarity calculation between two segments. Our experimental results show that the proposed method improves both recall and precision comparing with the conidf method.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Roberto Basili</author>
<author>Fabrizio Grisoli</author>
<author>Maria Teresa Pazienza</author>
</authors>
<title>Might a semantic lexicon support hypertextual authoring?</title>
<date>1994</date>
<booktitle>In 4th ANLP,</booktitle>
<pages>174--179</pages>
<contexts>
<context position="2036" citStr="Basili et al. (1994)" startWordPosition="323" endWordPosition="326"> of several separated volumes. In a case of such a large manual, the same topic appears at several places in different volumes. One of them is an introductory explanation for a novice. Another is a precise explanation for an advanced user. It is very useful to jump from one of them to another of them directly by just clicking a button of mouse in reading a manual text on a browser like NetScape. This type of access is realized by linking them in hypertext format by hypertext authoring. Automatic hypertext authoring has been focused on in these years, and much work has been done. For instance, Basili et al. (1994) use document structures and semantic information by means of natural language processing technique to set hyperlinks on plain texts. The essential point in the research of automatic hypertext authoring is the way to find semantically relevant parts where each part is characterized by a number of key words. Actually it is very similar with information retrieval, IR henceforth, especially with the so called passage retrieval (Salton et al., 1993). J.Green (1996) does hypertext authoring of newspaper articles by word&apos;s lexical chains which are calculated using WordNet. Kurohashi et al. (1992) ma</context>
</contexts>
<marker>Basili, Grisoli, Pazienza, 1994</marker>
<rawString>Roberto Basili, Fabrizio Grisoli, and Maria Teresa Pazienza. 1994. Might a semantic lexicon support hypertextual authoring? In 4th ANLP, pages 174-179.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Furner-Hines</author>
<author>Peter Willett</author>
</authors>
<title>On the measurement of inter-linker consistency and retrieval effectiveness in hypertext databases.</title>
<date>1994</date>
<booktitle>In SIGIR &apos;94,</booktitle>
<pages>51--60</pages>
<contexts>
<context position="2852" citStr="Furner-Hines and Willett (1994)" startWordPosition="449" endWordPosition="452">pertext authoring is the way to find semantically relevant parts where each part is characterized by a number of key words. Actually it is very similar with information retrieval, IR henceforth, especially with the so called passage retrieval (Salton et al., 1993). J.Green (1996) does hypertext authoring of newspaper articles by word&apos;s lexical chains which are calculated using WordNet. Kurohashi et al. (1992) made a hypertext dictionary of the field of information science. They use linguistic patterns that are used for definition of terminology as well as thesaurus based on words&apos; similarity. Furner-Hines and Willett (1994) experimentally evaluate and compare the performance of several human hyper linkers. In general, however, we have not yet paid enough attention to a full-automatic hyper linker system, that is what we pursue in this paper. The new ideas in our system are the following points: 1. Our target is a multi-volume manual that describes the same hardware or software but is different in their granularity of descriptions from volume to volume. 2. In our system, hyper links are set not between an anchor word and a certain part of text but between two segments, where a segment is a smallest formal unit in</context>
</contexts>
<marker>Furner-Hines, Willett, 1994</marker>
<rawString>David Elhs. Jonathan Furner-Hines and Peter Willett. 1994. On the measurement of inter-linker consistency and retrieval effectiveness in hypertext databases. In SIGIR &apos;94, pages 51-60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hitachi</author>
</authors>
<title>How to use the APPGALLERY, APPGALLERY On-Line Help.</title>
<date>1995</date>
<publisher>Hitachi Limited.</publisher>
<contexts>
<context position="14810" citStr="Hitachi, 1995" startWordPosition="2520" endWordPosition="2521">wsing along with links displayed on the lower parts, if a user wants to know relevant information about what she/he is reading on the text displayed on the upper part, a user can easily access the segments in which what she/he wants to know might be written in high probability. Now we describe the evaluation of our proposed methods with recall and precision defined as follows. 0 of retrieved pairs of relevant segments recall - 0 of pairs of relevant segments 0 of retrieved pairs of relevant segments 0 of retrieved pairs of segments The first experiment is done for a large manual of APPGALLARY(Hitachi, 1995) which is 2.5MB large. This manual is divided into two volumes. One is a tutorial manual for novices that contains 65 segments. The other is a help manual for advanced users that contains 2479 segments. If we try to find the relevant segments between ones in the tutorial manual and ones in the help manual, the number of possible pairs of segments is 161135. This number is too big for human to extract all relevant segment manually. Then we investigate highest 200 pairs of segments by hand, actually by two students in the engineering department of our university to extract pairs of relevant segm</context>
</contexts>
<marker>Hitachi, 1995</marker>
<rawString>Hitachi, 1995. How to use the APPGALLERY, APPGALLERY On-Line Help. Hitachi Limited.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen J Green</author>
</authors>
<title>Using lexcal chains to build hypertext links in newspaper articles.</title>
<date>1996</date>
<booktitle>In Proceedings of AAAI Workshop on Knowledge Discovery in Databases,</booktitle>
<location>Portland, Oregon.</location>
<contexts>
<context position="2501" citStr="Green (1996)" startWordPosition="397" endWordPosition="398">ertext authoring. Automatic hypertext authoring has been focused on in these years, and much work has been done. For instance, Basili et al. (1994) use document structures and semantic information by means of natural language processing technique to set hyperlinks on plain texts. The essential point in the research of automatic hypertext authoring is the way to find semantically relevant parts where each part is characterized by a number of key words. Actually it is very similar with information retrieval, IR henceforth, especially with the so called passage retrieval (Salton et al., 1993). J.Green (1996) does hypertext authoring of newspaper articles by word&apos;s lexical chains which are calculated using WordNet. Kurohashi et al. (1992) made a hypertext dictionary of the field of information science. They use linguistic patterns that are used for definition of terminology as well as thesaurus based on words&apos; similarity. Furner-Hines and Willett (1994) experimentally evaluate and compare the performance of several human hyper linkers. In general, however, we have not yet paid enough attention to a full-automatic hyper linker system, that is what we pursue in this paper. The new ideas in our syste</context>
</contexts>
<marker>Green, 1996</marker>
<rawString>Stephen J.Green. 1996. Using lexcal chains to build hypertext links in newspaper articles. In Proceedings of AAAI Workshop on Knowledge Discovery in Databases, Portland, Oregon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kurohashi</author>
<author>M Nagao</author>
<author>S Sato</author>
<author>M Murakami</author>
</authors>
<title>A method of automatic hypertext construction from an encyclopedic dictionary of a specific field.</title>
<date>1992</date>
<booktitle>In 3rd ANLP,</booktitle>
<pages>239--240</pages>
<contexts>
<context position="2633" citStr="Kurohashi et al. (1992)" startWordPosition="414" endWordPosition="417">ance, Basili et al. (1994) use document structures and semantic information by means of natural language processing technique to set hyperlinks on plain texts. The essential point in the research of automatic hypertext authoring is the way to find semantically relevant parts where each part is characterized by a number of key words. Actually it is very similar with information retrieval, IR henceforth, especially with the so called passage retrieval (Salton et al., 1993). J.Green (1996) does hypertext authoring of newspaper articles by word&apos;s lexical chains which are calculated using WordNet. Kurohashi et al. (1992) made a hypertext dictionary of the field of information science. They use linguistic patterns that are used for definition of terminology as well as thesaurus based on words&apos; similarity. Furner-Hines and Willett (1994) experimentally evaluate and compare the performance of several human hyper linkers. In general, however, we have not yet paid enough attention to a full-automatic hyper linker system, that is what we pursue in this paper. The new ideas in our system are the following points: 1. Our target is a multi-volume manual that describes the same hardware or software but is different in </context>
</contexts>
<marker>Kurohashi, Nagao, Sato, Murakami, 1992</marker>
<rawString>S. Kurohashi, M. Nagao, S. Sato, and M. Murakami. 1992. A method of automatic hypertext construction from an encyclopedic dictionary of a specific field. In 3rd ANLP, pages 239-240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuji Matsumoto</author>
<author>Osamu Imaichi</author>
<author>Tatsuo Yamashita</author>
<author>Akira Kitauchi</author>
<author>Tomoaki Imamura</author>
</authors>
<date>1996</date>
<booktitle>Japanese morphological analysis system ChaSen manual (version 1.0b4). Nara Institute of Science and Technology,</booktitle>
<contexts>
<context position="10874" citStr="Matsumoto et al., 1996" startWordPosition="1844" endWordPosition="1847">c(tk,dA,dB) to calculate if&apos;. 3 Implementation and Experimental Results Our system has the following inputs and outputs. Input is an electronic manual text which can be written in plain textOTEXor HTML) Output is a hypertext in HTML format. Figure 1: Overview of our hypertext generator We need a browser like NetScape that can display a text written in HTML. Our system consists of four sub-systems shown in Figure 1. Keyword Extraction Sub-System In this subsystem, a morphological analyzer segments out the input text, and extract all nouns and verbs that are to be keywords. We use Chasen 1.04b (Matsumoto et al., 1996) as a morphological analyzer for Japanese texts. Noun and Caseinformation pairs are also made in this subsystem. If you use the dimension expansion described in 2.1, you introduce new dimensions here. tf idf Calculation Sub-System This sub-system calculates if • idf of extracted keywords by Keyword Extraction Sub-System. Similarity Calculation Sub-System This subsystem calculates the similarity that is represented by cosine of every pair of segments based on if idf values calculated above. If you use modifications of if values described in 2.2, you calculated modified if, namely if&apos; in this su</context>
</contexts>
<marker>Matsumoto, Imaichi, Yamashita, Kitauchi, Imamura, 1996</marker>
<rawString>Yuji Matsumoto, Osamu Imaichi, Tatsuo Yamashita, Akira Kitauchi, and Tomoaki Imamura. 1996. Japanese morphological analysis system ChaSen manual (version 1.0b4). Nara Institute of Science and Technology, Nov.</rawString>
</citation>
<citation valid="true">
<authors>
<author>MITSUBISHI</author>
</authors>
<title>MITSUBISHI Video Tape Recorder</title>
<date>1995</date>
<booktitle>HV-BZ66 Instruction Manual. MITSUBISHI, 1995b. MITSUBISHI Video Tape Recorder HV-F93 Instruction Manual. MITSUBISHI, 1995c. MITSUBISHI Video Tape Recorder HV-FZ62 Instruction Manual.</booktitle>
<contexts>
<context position="16416" citStr="MITSUBISHI, 1995" startWordPosition="2800" endWordPosition="2801">on or the same terminology. 2. One segment explains an abstract concept and the other explains that concept in concrete operation. Figure 3 shows the recall and precision for numbers of selected pairs of segments where those pairs are sorted in descending order of cosine similarity value using normal if • idf of all nouns. This result indicates that pairs of relevant segments are concentrated in high similarity area. In fact, the pairs of segments within top 200 pairs are almost all relevant ones. The second experiment is done for three small manuals of three models of video cassette recorder(MITSUBISHI, 1995c; MITSUBISHI, 1995a; MITSUBISHI, 1995b) produced by the same company. We investigate all pairs of segments that appear in the distinct manuals respectively, and extract relevant pairs of segment according to the same guideline we did in the first experiment by two students of the engineering department of our university. The numbers of segments are 32 for manual A(MITSUBISHI, 1995c), 33 for manual B(MITSUBISHI, 1995a) and 28 for manual C(MITSUBISHI, 1995b), respectively. The number of relevant pairs of segments are shown in Table 1. We show the 11 points precision averages for these methods i</context>
</contexts>
<marker>MITSUBISHI, 1995</marker>
<rawString>MITSUBISHI, 1995a. MITSUBISHI Video Tape Recorder HV-BZ66 Instruction Manual. MITSUBISHI, 1995b. MITSUBISHI Video Tape Recorder HV-F93 Instruction Manual. MITSUBISHI, 1995c. MITSUBISHI Video Tape Recorder HV-FZ62 Instruction Manual.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>J Allan</author>
<author>Chris Buckley</author>
</authors>
<title>Approaches to passage retrieval in full text information systems.</title>
<date>1993</date>
<booktitle>In SIGIR &apos;93,</booktitle>
<pages>49--58</pages>
<contexts>
<context position="2485" citStr="Salton et al., 1993" startWordPosition="393" endWordPosition="396"> hypertext format by hypertext authoring. Automatic hypertext authoring has been focused on in these years, and much work has been done. For instance, Basili et al. (1994) use document structures and semantic information by means of natural language processing technique to set hyperlinks on plain texts. The essential point in the research of automatic hypertext authoring is the way to find semantically relevant parts where each part is characterized by a number of key words. Actually it is very similar with information retrieval, IR henceforth, especially with the so called passage retrieval (Salton et al., 1993). J.Green (1996) does hypertext authoring of newspaper articles by word&apos;s lexical chains which are calculated using WordNet. Kurohashi et al. (1992) made a hypertext dictionary of the field of information science. They use linguistic patterns that are used for definition of terminology as well as thesaurus based on words&apos; similarity. Furner-Hines and Willett (1994) experimentally evaluate and compare the performance of several human hyper linkers. In general, however, we have not yet paid enough attention to a full-automatic hyper linker system, that is what we pursue in this paper. The new id</context>
</contexts>
<marker>Salton, Allan, Buckley, 1993</marker>
<rawString>Gerard Salton, J. Allan, and Chris Buckley. 1993. Approaches to passage retrieval in full text information systems. In SIGIR &apos;93, pages 49-58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Toru Takaki</author>
<author>Tsuyoshi Kitani</author>
</authors>
<title>Relevance ranking of documents using query word cooccurrences (in Japanese).</title>
<date>1996</date>
<booktitle>IPSJ SIG Notes 96-FI41-8, IPS Japan,</booktitle>
<contexts>
<context position="7237" citStr="Takaki and Kitani, 1996" startWordPosition="1200" endWordPosition="1203">verb. And using them as new extra dimensions of the original vector space. • (VERB, learn) (NOMNINAL end user) (ACCUSATIVE programming language) • (VERB, learn) (NOMNINAL end user) • (VERB, learn) (ACCUSATIVE programming language) We calculate if • idf of each of these combinations that is a value of vector corresponding to each of these combinations. The similarity calculation based on cosine measure is done on this expanded vector space. 2.2 Modification of if value Another method we propose for reflecting cooccurrence information to similarity is modification of if value within a segment. (Takaki and Kitani, 1996) reports that co-occurrence of word pairs contributes to the IR performance for Japanese news paper articles. In our method, we modify if of pairs of cooccurred words that occur in both of two segments, say dA and dB, in the following way. Suppose that a term tk, namely noun or verb, occurs f times in the segment dA. Then the modified tf(dA, tk) is defined as the following formula. tt(dA,tk) = tf(dA,tk) • E E CW(CIA,ik,p,t,) tcETc(ik,dA,cia)p=1 • E EcodA,tk,p,t,) icET,(tk,dA,dB)p.i where cw and cw&apos; are scores of importance for cooccurrence of words, tk and tc. Intuitively, cw and cw&apos; are count</context>
</contexts>
<marker>Takaki, Kitani, 1996</marker>
<rawString>Toru Takaki and Tsuyoshi Kitani. 1996. Relevance ranking of documents using query word cooccurrences (in Japanese). IPSJ SIG Notes 96-FI41-8, IPS Japan, April.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>