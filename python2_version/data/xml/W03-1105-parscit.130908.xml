<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000280">
<title confidence="0.996313">
Poisson Naive Bayes for Text Classification with Feature Weighting
</title>
<author confidence="0.995481">
Sang-Bum Kim, Hee-Cheol Seo and Hae-Chang Rim
</author>
<affiliation confidence="0.99316">
Dept. of CSE., Korea University
</affiliation>
<address confidence="0.942869">
5-ka Anamdong, SungPuk-ku, SEOUL 136-701, KOREA
</address>
<email confidence="0.999645">
{sbkim,hcseo,rim}@nlp.korea.ac.kr
</email>
<sectionHeader confidence="0.998605" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999945565217391">
In this paper, we investigate the use of
multivariate Poisson model and feature
weighting to learn naive Bayes text clas-
sifier. Our new naive Bayes text classifi-
cation model assumes that a document is
generated by a multivariate Poisson model
while the previous works consider a doc-
ument as a vector of binary term features
based on the presence or absence of each
term. We also explore the use of feature
weighting for the naive Bayes text classifi-
cation rather than feature selection, which
is a quite costly process when a small
number of the new training documents are
continuously provided.
Experimental results on the two test col-
lections indicate that our new model with
the proposed parameter estimation and the
feature weighting technique leads to sub-
stantial improvements compared to the
unigram language model classifiers that
are known to outperform the original pure
naive Bayes text classifiers.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998947195652174">
The naive Bayes classifier has been one of the core
frameworks in the information retrieval research for
many years. Recently, naive Bayes is emerged as a
research topic itself because it sometimes achieves
good performances on various tasks, compared to
more complex learning algorithms, in spite of the
wrong independence assumptions on naive Bayes.
Similarly, naive Bayes is also an attractive ap-
proach in the text classification task because it is
simple enough to be practically implemented even
with a great number of features. This simplicity en-
ables us to integrate the text classification and filter-
ing modules with the existing information retrieval
systems easily. It is because that the frequency re-
lated information stored in the general text retrieval
systems is all the required information in naive
Bayes learning. No further complex generaliza-
tion processes are required unlike the other machine
learning methods such as SVM or boosting. More-
over, incremental adaptation using a small number
of new training documents can be performed by just
adding or updating frequencies.
Several earlier works have extensively studied the
naive Bayes text classification (Lewis, 1992; Lewis,
1998; McCallum and Nigam, 1998). However,
their pure naive Bayes classifiers considered a doc-
ument as a binary feature vector, and so they can’t
utilize the term frequencies in a document, result-
ing the poor performances. For that reason, the
unigram language model classifier (or multinomial
naive Bayes text classifier) has been referred as an
alternative and promising naive Bayes by a num-
ber of researchers(McCallum and Nigam, 1998; Du-
mais et al., 1998; Yang and Liu, 1999; Nigam et
al., 2000). Although the unigram language model
classifiers usually outperform the pure naive Bayes,
they also have given the disappointing results com-
pared to many other statistical learning methods
such as nearest neighbor classifiers(Yang and Chute,
1994), support vector machines(Joachims, 1998),
and boosting(Schapire and Singer, 2000), etc.
mains(Domingos and Pazzani, 1997) in spite of its
wrong independent assumption.
In the context of text classification, the probabil-
ity of class c given a document dj is calculated by
Bayes’ theorem as follows:
</bodyText>
<equation confidence="0.9985514">
p(cldj) = p(djlc)p(c)
p(dj)
= p(dj c)p(c)
p(djIc)p(c) + p(djI�c)p(�c)
p(dj 1 c) P(c)
p(dj 1 c) =(1)
~~~~1~~~ ~p(c) + p(�c)
~~~~1~~
Now, if we define a new function zj,,
zi, = log p(dj c) (2)
p(dj c)
then, Equation (1) can be rewritten as
���� � p(c)
p(c�d�) = (3)
cz,, p(c) + p(�c)
</equation>
<bodyText confidence="0.999911444444445">
Using Equation (3), we can get the posterior prob-
ability p(cIdj) by obtaining zj,, which is a form of
log ratio similar to the BIM retrieval model(Jones et
al., 2000). It means that the linked independence as-
sumption(Cooper et al., 1992), which explains that
the strong independent assumption can be relaxed
in the BIM model, is sufficient for the use of naive
Bayes text classification model.
With this framework, two representative naive
Bayes text classification approaches are well intro-
duced in (McCallum and Nigam, 1998). They desig-
nated the pure naive Bayes as multivariate Bernoulli
model, and the unigram language model classifier as
multinomial model. Instead, we introduce multivari-
ate Poisson model to improve the pure naive Bayes
text classification in the next section.
In the real world, an operational text classifica-
tion system is usually placed in the environment
where the amount of human-annotated training doc-
uments is small in spite of the hundreds of thousands
classes. Moreover, re-training of the text classifiers
is required since a small number of new training
documents are continuously provided. In this envi-
ronment, naive Bayes is probably the most appropri-
ate model for the practical systems rather than other
complex learning models. Therefore, more inten-
sive studies about the naive Bayes text classification
model are required.
In this paper, we revisit the naive Bayes frame-
work, and propose a Poisson naive Bayes model for
text classification with a statistical feature weight-
ing method. Feature weighting has many advan-
tages compared to the previous feature selection ap-
proaches, especially when the new training exam-
ples are continuously provided. Our new model as-
sumes that a document is generated by a multivari-
ate Poisson model, and their parameters are esti-
mated by weighted averaging of the normalized and
smoothed term frequencies over all the training doc-
uments. Under the assumption, we have tested the
feature weighting approach with three measures: in-
formation gain, x2-statistic, and newly introduced
probability ratio. With the proposed model and fea-
ture weighting techniques, we can get much better
performance without losing the simplicity and effi-
ciency of the naive Bayes model.
The remainder of this paper is organized as fol-
lows. The next section presents a naive Bayes frame-
work for the text classification briefly. Section 3
describes our new naive Bayes model and the pro-
posed technique, and the experimental results are
presented in Section 4. Finally, we conclude the pa-
per by suggesting possible directions for future work
in Section 5.
</bodyText>
<sectionHeader confidence="0.787836" genericHeader="method">
2 Naive Bayes Text Classification
3 Poisson Naive Bayes Text Classification
</sectionHeader>
<bodyText confidence="0.999940571428572">
A naive Bayes classifier is a well-known and highly
practical probabilistic classifier, and has been em-
ployed in many applications. It assumes that all
attributes of the examples are independent of each
other given the context of the class, that is, an in-
dependent assumption. Several studies show that
naive Bayes performs surprisingly well in many do-
</bodyText>
<subsectionHeader confidence="0.992054">
3.1 Overview
</subsectionHeader>
<bodyText confidence="0.998956476190476">
The Poisson distribution is most commonly used to
model the number of random occurrences of some
phenomenon in a specified unit of space or time,
for example, the number of phone calls received by
a telephone operator in a 10-minute period. If we
think that the occurrence of each term is a random
occurrence in a fixed unit of space (i.e. a length
of document) the Poisson distribution is intuitively
suitable to model the term frequencies in a given
document. For that reason, the use of Poisson model
is widely investigated in the IR literature, but it is
rarely used for the text classification task(Lewis,
1998). It motivates us to adopt the Poisson model
for learning the naive Bayes text classification.
Our model assumes that dj is generated by multi-
variate Poisson model. In other words, a document
dj is a random vector which consists of the Poisson
random variables Xi, and Xi has the value of within-
term-frequency fig for the i-th term ti. Thus, if we
assume the independence among the terms in d-, a
probability of dj is represented by,
</bodyText>
<equation confidence="0.956433125">
IVI
p(di) = P(Xi =fij) (4)
���
where, IV l is a vocabulary size, and each P(Xi =
fij) is given by,
�—�����
P(Xi = fig) = (5)
fig!
</equation>
<bodyText confidence="0.980536333333333">
where, A is the Poisson mean.
As a result, the zj, function of Equation (2) is
rewritten using Equations (4) and (5) as follows:
</bodyText>
<equation confidence="0.980649444444445">
IVI
P(Xi = figlc)
log
P(Xi = fijIc)
���
�—�������
���
(6)
c—µiµ2fi.
</equation>
<bodyText confidence="0.99802175">
where, Ai and µi is the Poisson mean for ti in class
c and class c, respectively.
The most important issues of this work are as fol-
lows:
</bodyText>
<listItem confidence="0.95009675">
• How to decide the frequency fig representing
the characteristic of document dj?
• How to estimate the model parameter A, and µi
representing the characteristic of each class?
</listItem>
<bodyText confidence="0.998891">
We propose the possible answers in the next subsec-
tion.
</bodyText>
<subsectionHeader confidence="0.998968">
3.2 Parameter Estimation
</subsectionHeader>
<bodyText confidence="0.998751363636364">
Since fig is a frequency of a term i in a document
d, with a fixed length according to the definition of
Poisson distribution, we should normalize the actual
term frequencies in the documents with the different
length. In addition, many earlier works in NLP and
IR fields recommend that smoothing term frequen-
cies is necessary in order to build a more accurate
model.
Thus, we estimate fig as the normalized and
smoothed frequency of actual term frequency Aj,
represented by,
</bodyText>
<equation confidence="0.9989345">
(7)
��� + � � �� �
</equation>
<bodyText confidence="0.999830692307692">
where B is a laplace smoothing parameter, T is any
huge value which makes all the fig in our model an
integer value1, and dlj is the length of dj.
Conceptually, fig can be regarded as the value es-
timated by the following steps : 1) Add B of all IV l
terms to the document dj, 2) Scale dj up to dj&apos; whose
total length is T without changing the proportion of
frequency for each term ti, 3) Count t2 in d&apos;j.
Then, Poisson mean Ai, which represents an aver-
age number of occurrence of ti in the documents be-
longing to class c, is estimated using the normalized
and smoothed f ij values over the training documents
as follows:
</bodyText>
<equation confidence="0.865171">
�, = E
djED,
</equation>
<bodyText confidence="0.9996116">
where D, is the set of training documents belonging
to class c, and g(djlc)2 is the interpolation of the
uniform probability and the probability proportional
to the length of the document, and the interpolation
is calculated as follows:
</bodyText>
<equation confidence="0.955582333333333">
1
�(����) = � + (1 —a) dh (9)
lDcl Ed,ED,dlj
</equation>
<bodyText confidence="0.9947335">
Simple averaging of fig, the case of a=1, seems to
be correct to estimate A. However, the statistics
</bodyText>
<footnote confidence="0.874358125">
1Since fid is a value of random variable Xis representing
the frequency in our Poisson distribution, we multiply the nor-
malized frequency with some unnatural constant T to make f�d
integer value. However, T is dropped in the final induced func-
tion.
2We use the notation g(djIc) for the distribution defined
only in the training documents, to distinguish it from the no-
tation p(djIc) used in the Section 2.
</footnote>
<equation confidence="0.987411636363636">
zic =
=
IVI
���
�
=
f2�
xis + B
T
�
g(djlc) • fij (8)
</equation>
<bodyText confidence="0.999967285714286">
from the long documents can be more reliable than
those in the short documents, hence we try to inter-
polate between the two different probabilities with
the parameter a ranging from 0 to 1. Consequently,
Ai is a weighted average over all training documents
belonging to the class c, and M for the class c can be
estimated in the same manner.
</bodyText>
<subsectionHeader confidence="0.999599">
3.3 Feature Weighting
</subsectionHeader>
<bodyText confidence="0.999870371428571">
Feature selection is often performed as a preprocess-
ing step for the purpose of both reducing the fea-
ture space and improving the classification perfor-
mance. Text classifiers are then trained with various
machine learning algorithms in the resulting feature
space. (Yang and Pedersen, 1997) investigated some
measures to select useful term features including
mutual information(MI), information gain(IG), and
x2-statistics(CHI), etc. On the contrary, (Joachims,
1998) claimed that there is no useless term features,
and it is preferable to use all term features. It is
clear that learning and classification become very
efficient when the feature space is considerably re-
duced. However, there is no definite conclusion
about the contribution of feature selection to im-
prove overall performances of the text classification
systems. It may considerably depend on the em-
ployed learning algorithm. We believe that proper
external feature selection or weighting is required to
improve the performances of naive Bayes since the
naive Bayes has no framework of the discriminative
optimizing process in itself. Of the two possible ap-
proaches, feature selection is very inefficient in case
that the additional training documents are provided
continuously. It is because the feature set should
be redefined according to the modified term statis-
tics in the new training document set, and classifiers
should be trained again with this new feature set. For
that reason, we prefer to use feature weighting to
improve naive Bayes rather than feature selection.
With the feature weighting method, our z,, is rede-
fined as follows:
where, wic is the weight of feature for the class c,
and W, is the normalization factor, that is,EZ1 wi,
In our work, three measures are used to weight
</bodyText>
<tableCaption confidence="0.993315">
Table 1: Two-way contingency table
</tableCaption>
<bodyText confidence="0.976345454545454">
presence of t2 absence of t2
labeled as c a b
not labeled as c c d
each term feature: information gain, x2-statistics
and probability ratio. Information gain (or aver-
age mutual information) is an information-theoretic
measure defined by the amount of reduced uncer-
tainty given a piece of information. We use the in-
formation gain value as the weight of each term for
the class c, and the value is calculated using a docu-
ment event model as follows:
</bodyText>
<equation confidence="0.999531666666667">
wio = H(C) — W1wi) (11)
P(C&amp;quot;W&apos;)log p(C&amp;quot;wt)
P(C&apos;)P(Wt)
</equation>
<bodyText confidence="0.9999344">
where, for example, p(c) is the number of docu-
ments belonging to the class c divided by the total
number of documents, and p( w) is the number of
documents without the term w divided by the total
number of documents, etc.
Second measure we used is x2- statistics devel-
oped for the statistical test of the hypothesis. In the
text classification, given a two-way contingency ta-
ble for each term ti and class c as represented in Ta-
ble 1, wic is calculated as follows:
</bodyText>
<equation confidence="0.9993485">
(ad — bc)2
wic = (a + b)(a + c)(b + d)(c + d) (12)
</equation>
<bodyText confidence="0.999865375">
where, a,b,c and d indicate the number of documents
for each cell in the above contingency table.
(Yang and Pedersen, 1997) compared the various
feature selection methods, and concluded that these
two measures are most effective for their kNN and
LLSF classification models.
Finally, we introduce a new measure - probability
ratio. Probability ratio is defined by,
</bodyText>
<equation confidence="0.9996945">
wic = p(wi1 C) + P(wi1 C) (13)
P(wi1�e) p(wi1C)
</equation>
<bodyText confidence="0.999661">
This measure calculates the sum of the ratio of two
class-conditional probabilities from each class and
its reciprocal. The former term and the latter term
</bodyText>
<figure confidence="0.970337625">
�
• ���
We e
fz,
(10)
fz,
IVI
zjC =
���
wic
Az
i
�
µz/ii
� �
csE{c,cl wtE{wz, wzl
</figure>
<bodyText confidence="0.999750714285714">
are representing the degree of predicting positive
and negative class respectively. The weight using
this measure always has a positive value higher than
2.
We have conducted the experiments with these
three measures for the feature weighting test, and
the results are given in Section 4.
</bodyText>
<subsectionHeader confidence="0.975064">
3.4 Implementation Issues
</subsectionHeader>
<bodyText confidence="0.9987565">
By a couple of simple arithmetic operations, our fi-
nal zjc function can be rewritten as follows:
</bodyText>
<figure confidence="0.9962198125">
Reuters21578
0 0.2 0.4 0.6 0.8 1
alpha
MicroF1
0.79
0.78
0.77
0.76
0.75
0.74
0.73
0.72
0.71
0.7
PNB
Unigram Model
</figure>
<bodyText confidence="0.702866">
where, zjc = Wd1cj(Ac + (Bc + zjc) &apos; ) (14) Figure 1: MicroF1 Performances for Reuters21578
according to interpolation parameter a for estimat-
ing A and µ(without feature weighting)
</bodyText>
<equation confidence="0.983728533333334">
IVI 4 Experimental Result
Ac =
i�� wic( liiI — �i ) 4.1 Data and Evaluation Measure
Bc = B IVI wic log �i i
i�� Iii�
zjc = E wicxij log Ai �
di,tzEdj µi
��i i =1
T
��i =
djED,
dl&apos;j = dlj +BIVI
fij
g(djlc) �
T
</equation>
<bodyText confidence="0.989515333333333">
In this equation, �i
iI and µi&apos; are just weighted av-
erage of T-dropped nfij, that is, h~xzj~0 Wc, Ac and
</bodyText>
<equation confidence="0.844061">
d01V �.
</equation>
<bodyText confidence="0.99994425">
Bc are the class-specific constants, and T is a con-
stant over all the classes and documents. If the class
c is fixed, Wc, Ac and T can be dropped, and the
ranking function zjc is defined as follows:
</bodyText>
<equation confidence="0.957296">
zjc = (Bc + zjc) 1(15)
~~j~
</equation>
<bodyText confidence="0.999955866666667">
When we use this ranking function z*jc, the calcu-
lation of the exact posterior probability p(cIdj) pre-
sented in Section 2 becomes impossible. However,
it is trivial since most of IR systems do not have in-
terest on exact posterior probability. In addition, all
the parameters in our model is guaranteed to be cal-
culated by the incremental way.
Our experiments were performed on the two
datasets: Reuters21578 and KoreanNews2002 col-
lection. Reuters21578 collection is the most widely
used benchmark dataset for the text categorization
research. We have used “ModApte” split version,
which consists of 9603 training documents and 3299
test documents. There are 90 categories, and each
document has one or more of the categories.
We have built another benchmark collection - Ko-
reanNews2002 collection. KoreanNews2002 collec-
tion is composed of 15,000 news articles published
during the year of 2002. The articles are collected
from a number of Korean news portal websites, and
each article is labeled with exactly one of the 46
classes. All the documents have date stamps at-
tached and have been ordered according to their date
stamps. With this date order, we divided them into
the former 10,000 documents for training and the
latter 5,000 documents for testing.
The performances are evaluated using popular F1
measure, and the F1 values for each class are micro-
averaged(MicroF1) and macro-averaged(MacroF1)
to examine the general classification performances.
</bodyText>
<page confidence="0.260542">
4.2 Proposed Model: PNB (vs. UM)
</page>
<figureCaption confidence="0.81995">
Figure 1 shows the performances of our new model
named Poisson naive Bayes(PNB) classifiers ac-
</figureCaption>
<tableCaption confidence="0.964962">
Table 2: Performances of UM and PNB on the
Reuters21578 collection
</tableCaption>
<table confidence="0.999041333333333">
UM PNB(min) PNB(max)
MicroF1 0.7212 0.7644 0.7706
MacroF1 0.3214 0.4227 0.4358
</table>
<tableCaption confidence="0.9866325">
Table 3: Performances of UM and PNB on the Ko-
reanNews2002 collection
</tableCaption>
<table confidence="0.998542333333333">
UM PNB(min) PNB(max)
MicroF1 0.6502 0.7031 0.7094
MacroF1 0.5208 0.5859 0.5949
</table>
<bodyText confidence="0.999812625">
cording to the interpolation parameter a for estimat-
ing Poisson mean A and µ. The baseline method
is a unigram model classifier (UM) which is also
referred to multinomial naive Bayes classifier de-
scribed in (McCallum and Nigam, 1998). Our pro-
posed PNB clearly outperforms the UM.
Although there is no significant difference of Mi-
croF1 values among the various a values, the F1
value of each class is considerably affected by the
a values. Figure 2 presents the fluctuations of the
F1 values for 4 classes in Reuters21578 collection.
From this result, we can assume that there is no
global optimal value of a, but each class has its own
optimal a. In our experiments, many of the classes
have the highest F1 value when a is about 0.8 or
0.9 except some classes such as corn class which
shows the highest F1 value at a = 0 .3. Similar
results are obtained in the KoreanNews2002 collec-
tions.
Table 2 and 3 shows the MicroF1 and MacroF1
values of the unigram model classifiers and our
PNB on the two collections, where PNB(min) and
PNB(max) are the highest and lowest values at dif-
ferent a. In any cases, PNB is superior to UM.
</bodyText>
<subsectionHeader confidence="0.999327">
4.3 Feature Weighting: PNB-{IG,CHI,PrR}
</subsectionHeader>
<bodyText confidence="0.999963625">
We have fixed the interpolation parameter a at 0.8,
and evaluated the following feature weighting meth-
ods: PNB-IG with information gain, PNB-CHI with
X2-statistic, and PNB-PrR with probability ratio. In
these experiments, some important behaviors of fea-
ture weighted PNB classifiers are observed from the
results. In order to explain the phenomenon, we
have grouped the classes into the bins according to
</bodyText>
<equation confidence="0.6673338">
Reuters21578 - acq
Reuters21578 - interest
0 0.2 0.4 0.6 0.8 1
alpha
Reuters21578 - corn
</equation>
<figureCaption confidence="0.658406">
Figure 2: Performances for 4 categories in
Reuters21578 according to interpolation parameter
a for estimating A and µ(without feature weighting)
</figureCaption>
<figure confidence="0.995671087719298">
0.78
0.76
0.74
F1
0.72
0.7
0.68
0.66
0 0.2 0.4 0.6 0.8 1
alpha
PNB
Unigram Model
0 0.2 0.4 0.6 0.8 1
alpha
0.96
PNB
Unigram Model
0.94
0.92
F1
0.9
0.88
0.86
0.84
Reuters21578 - grain
F1
0.72
0.68
0.66
0.64
0.62
0.7
0.6
PNB
Unigram Model
0 0.2 0.4 0.6 0.8 1
alpha
F1
0.56
0.54
0.52
0.48
0.46
0.44
0.5
PNB
Unigram Model
Reuters21578
PNB
PNB-IG
PNB-CHI
PNB-PrR
10 100 1000
avg # of train doc for each bean
KoreanNews
0 100 200 300 400 500 600 700
Avg # of train docs for each bean
</figure>
<figureCaption confidence="0.7782404">
Figure 3: MacroF1 performances of the bins on Re-
tures21578 and KoreanNews2002
the number of training documents for each class. 5
bins are generated in both Reuters21578 and Kore-
anNews2002 collection.
</figureCaption>
<bodyText confidence="0.999965173913043">
The different average F1 performance of each bin
is shown in Figure 3. The clear observation from
this result is that feature weighting is highly effec-
tive in the bins of the classes with a small num-
ber of training documents, but hardly contributes
the performances for the bins of the classes with
sufficiently many training documents. In the bins
with enough training documents, simple PNB clas-
sifiers show the similar performances to the PNB
with feature weighting methods. This tendency is
more clearly captured in the Reuters21578 collec-
tion, where a third of the classes have fewer than
10 training documents. In contrast, two thirds of
the classes in the KoreanNews2002 collection have
more than a hundred of training documents.
Among the feature weighting methods, PNB-
PrR performs stably than PNB-IG and PNB-CHI.
PNB-IG or PNB-CHI somewhat degrades the per-
formance in the classes with the large number of
training documents, while PNB-PrR maintains the
good performances in those classes on both of the
collections. On the other hand, PNB-IG and PNB-
CHI considerably improve the performances in the
rare categories though the improvement is some-
what different from the two collections. For ex-
ample, PNB-CHI significantly improves the simple
PNB on the Reuters21578 collection while PNB-
IG is very effective on the KoreanNews2002 collec-
tion. Thus, we can realize that the proper feature
weighting method depends on the characteristics of
the collection, and different feature weighting strate-
gies should be adopted to improve the naive Bayes
text classification.
From these observations, we tested another clas-
sifier PNB* which employ different feature weight-
ing method for each bin to obtain the near opti-
mal performances. Table 4 and 5 show the sum-
mary of the performances including PNB* on the
both collections. Our proposed model with feature
weighting methods are very effective compared to
the baseline UM method. Moreover, the perfor-
mance of bin-optimized PNB* in Reuters21578 col-
lection shows that Poisson naive Bayes with feature
weighting methods can achieve the state-of-the-art
performances achieved by SVM or kNN which are
reported in (Yang and Liu, 1999; Joachims, 1998).
</bodyText>
<sectionHeader confidence="0.997487" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999835333333333">
In this paper, we propose a Poisson naive Bayes text
classification model with feature weighting. Our
new model uses the normalized and smoothed term
frequencies for each document, and Poisson param-
eters are calculated by weighted averaging the fre-
quencies over all training documents. Experimental
results show that the proposed model is quite use-
ful to build probabilistic text classification systems
without requiring any extra cost compared to the
traditional simple naive Bayes or unigram language
model classifiers.
Further improvement is achieved by a feature
weighting technique. In our experiments, three
measures including chi-square statistics, informa-
tion gain, and newly introduced probability ratio are
</bodyText>
<figure confidence="0.999188291666667">
PNB
PNB-IG
PNB-CHI
PNB-PrR
MacroF1 for each bean 1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
MacroF1 for each bean 0.9
0.85
0.8
0.75
0.7
0.65
0.6
0.55
0.5
0.45
0.4
</figure>
<tableCaption confidence="0.993178">
Table 4: Summary of the performances on the Reuters21578 collection
</tableCaption>
<table confidence="0.965301">
UM PNB PNB-IG PNB-CHI PNB-PrR PNB*
MicroF1 0.7212 0.7690 0.7971 0.8167 0.8190(+13.56%) 0.8341
MacroF1 0.3414 0.4307 0.5800 0.6601(+93.35%) 0.5899 0.6645
</table>
<tableCaption confidence="0.994694">
Table 5: Summary of the performances on the KoreanNews2002 collection
</tableCaption>
<table confidence="0.389391">
UM PNB PNB-IG PNB-CHI PNB-PrR PNB*
MicroF1 0.6502 0.7056 0.7114 0.7122 0.7409(+13.95%) 0.7438
MacroF1 0.5208 0.5906 0.6305(+21.06%) 0.5748 0.6119 0.6662
</table>
<bodyText confidence="0.999941625">
adopted to weigh each term feature. The results
show that feature weighting considerably improves
the performances for the classes with a small num-
ber of training documents, but not for the classes
with the sufficient training documents. Probability
ratio also performs well, especially in the classes
with the great number of training documents where
other feature weighting methods show the unsatis-
factory performances.
For the future work, we will try to develop
some automatic methods of selecting proper feature
weighting measures and determining the interpola-
tion parameters for the different classes. Further-
more, we will explore applications of our approach
in other tasks such as adaptive filtering and relevance
feedback.
</bodyText>
<sectionHeader confidence="0.999554" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999459964285714">
William S. Cooper, Fredric C. Gey, and Daniel P. Dabney.
1992. Probabilistic retrieval based on staged logsitic
regression. Proceedings of SIGIR-92, 15th ACM In-
ternational Conference on Research and Development
in Information Retrieval, pages 198–210.
Pedro Domingos and Michael J. Pazzani. 1997. On the
optimality of the simple bayesian classifier under zero-
one loss. Machine Learning, 29(2/3):103–130.
Susan Dumais, John Plat, David Heckerman, and Mehran
Sahami. 1998. Inductive learning algorithms and
representation for text categorization. Proceedings of
CIKM-98, 7th ACM International Conference on In-
formation and Knowledge Management, pages 148–
155.
Thorsten Joachims. 1998. Text categorization with sup-
port vector machines: learning with many relevant fea-
tures. Proceedings of ECML-98, 10th European Con-
ference on Machine Learning, pages 137–142.
Karen Sparck Jones, Steve Walker, and Stephen E.
Robertson. 2000. A probabilistic model of informa-
tion retrieval: development and comparative experi-
ments -part 1. Information Processing and Manage-
ment, 36(6):779–808.
David D. Lewis. 1992. Representation and learning
in information retrieval. Ph.D. thesis, Department
of Computer Science, University of Massachusetts,
Amherst, US.
David D. Lewis. 1998. Naive (Bayes) at forty: The in-
dependence assumption in information retrieval. Pro-
ceedings of ECML-98, 10th European Conference on
Machine Learning, pages 4–15.
Andrew K. McCallum and Kamal Nigam. 1998. Em-
ploying EM in pool-based active learning for text clas-
sification. Proceedings of ICML-98, 15th Interna-
tional Conference on Machine Learning, pages 350–
358.
Kamal Nigam, Andrew K. McCallum, Sebastian Thrun,
and Tom M. Mitchell. 2000. Text classification from
labeled and unlabeled documents using EM. Machine
Learning, 39(2/3):103–134.
Robert E. Schapire and Yoram Singer. 2000. BOOSTEX-
TER: a boosting-based system for text categorization.
Machine Learning, 39(2/3):135–168.
Yiming Yang and Christopher G. Chute. 1994. An
example-based mapping method for text categoriza-
tion and retrieval. ACM Transactions on Information
Systems, 12(3):252–277.
Yiming Yang and Xin Liu. 1999. A re-examination of
text categorization methods. Proceedings of SIGIR-
99, 22nd ACM International Conference on Research
and Development in Information Retrieval, pages 42–
49.
Yiming Yang and Jan O. Pedersen. 1997. A comparative
study on feature selection in text categorization. Pro-
ceedings of ICML-97, 14th International Conference
on Machine Learning, pages 412–420.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.712114">
<title confidence="0.999352">Poisson Naive Bayes for Text Classification with Feature Weighting</title>
<author confidence="0.967967">Sang-Bum Kim</author>
<author confidence="0.967967">Hee-Cheol Seo</author>
<author confidence="0.967967">Hae-Chang</author>
<affiliation confidence="0.988221">Dept. of CSE., Korea</affiliation>
<address confidence="0.743378">5-ka Anamdong, SungPuk-ku, SEOUL 136-701,</address>
<abstract confidence="0.999481375">In this paper, we investigate the use of multivariate Poisson model and feature weighting to learn naive Bayes text classifier. Our new naive Bayes text classification model assumes that a document is generated by a multivariate Poisson model while the previous works consider a document as a vector of binary term features based on the presence or absence of each term. We also explore the use of feature weighting for the naive Bayes text classification rather than feature selection, which is a quite costly process when a small number of the new training documents are continuously provided. Experimental results on the two test collections indicate that our new model with the proposed parameter estimation and the feature weighting technique leads to substantial improvements compared to the unigram language model classifiers that are known to outperform the original pure naive Bayes text classifiers.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>William S Cooper</author>
<author>Fredric C Gey</author>
<author>Daniel P Dabney</author>
</authors>
<title>Probabilistic retrieval based on staged logsitic regression.</title>
<date>1992</date>
<booktitle>Proceedings of SIGIR-92, 15th ACM International Conference on Research and Development in Information Retrieval,</booktitle>
<pages>198--210</pages>
<contexts>
<context position="3920" citStr="Cooper et al., 1992" startWordPosition="616" endWordPosition="620">fication, the probability of class c given a document dj is calculated by Bayes’ theorem as follows: p(cldj) = p(djlc)p(c) p(dj) = p(dj c)p(c) p(djIc)p(c) + p(djI�c)p(�c) p(dj 1 c) P(c) p(dj 1 c) =(1) ~~~~1~~~ ~p(c) + p(�c) ~~~~1~~ Now, if we define a new function zj,, zi, = log p(dj c) (2) p(dj c) then, Equation (1) can be rewritten as ���� � p(c) p(c�d�) = (3) cz,, p(c) + p(�c) Using Equation (3), we can get the posterior probability p(cIdj) by obtaining zj,, which is a form of log ratio similar to the BIM retrieval model(Jones et al., 2000). It means that the linked independence assumption(Cooper et al., 1992), which explains that the strong independent assumption can be relaxed in the BIM model, is sufficient for the use of naive Bayes text classification model. With this framework, two representative naive Bayes text classification approaches are well introduced in (McCallum and Nigam, 1998). They designated the pure naive Bayes as multivariate Bernoulli model, and the unigram language model classifier as multinomial model. Instead, we introduce multivariate Poisson model to improve the pure naive Bayes text classification in the next section. In the real world, an operational text classification</context>
</contexts>
<marker>Cooper, Gey, Dabney, 1992</marker>
<rawString>William S. Cooper, Fredric C. Gey, and Daniel P. Dabney. 1992. Probabilistic retrieval based on staged logsitic regression. Proceedings of SIGIR-92, 15th ACM International Conference on Research and Development in Information Retrieval, pages 198–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pedro Domingos</author>
<author>Michael J Pazzani</author>
</authors>
<title>On the optimality of the simple bayesian classifier under zeroone loss.</title>
<date>1997</date>
<booktitle>Machine Learning,</booktitle>
<pages>29--2</pages>
<contexts>
<context position="3224" citStr="Domingos and Pazzani, 1997" startWordPosition="490" endWordPosition="493">he unigram language model classifier (or multinomial naive Bayes text classifier) has been referred as an alternative and promising naive Bayes by a number of researchers(McCallum and Nigam, 1998; Dumais et al., 1998; Yang and Liu, 1999; Nigam et al., 2000). Although the unigram language model classifiers usually outperform the pure naive Bayes, they also have given the disappointing results compared to many other statistical learning methods such as nearest neighbor classifiers(Yang and Chute, 1994), support vector machines(Joachims, 1998), and boosting(Schapire and Singer, 2000), etc. mains(Domingos and Pazzani, 1997) in spite of its wrong independent assumption. In the context of text classification, the probability of class c given a document dj is calculated by Bayes’ theorem as follows: p(cldj) = p(djlc)p(c) p(dj) = p(dj c)p(c) p(djIc)p(c) + p(djI�c)p(�c) p(dj 1 c) P(c) p(dj 1 c) =(1) ~~~~1~~~ ~p(c) + p(�c) ~~~~1~~ Now, if we define a new function zj,, zi, = log p(dj c) (2) p(dj c) then, Equation (1) can be rewritten as ���� � p(c) p(c�d�) = (3) cz,, p(c) + p(�c) Using Equation (3), we can get the posterior probability p(cIdj) by obtaining zj,, which is a form of log ratio similar to the BIM retrieval </context>
</contexts>
<marker>Domingos, Pazzani, 1997</marker>
<rawString>Pedro Domingos and Michael J. Pazzani. 1997. On the optimality of the simple bayesian classifier under zeroone loss. Machine Learning, 29(2/3):103–130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan Dumais</author>
<author>John Plat</author>
<author>David Heckerman</author>
<author>Mehran Sahami</author>
</authors>
<title>Inductive learning algorithms and representation for text categorization.</title>
<date>1998</date>
<booktitle>Proceedings of CIKM-98, 7th ACM International Conference on Information and Knowledge Management,</booktitle>
<pages>148--155</pages>
<contexts>
<context position="2813" citStr="Dumais et al., 1998" startWordPosition="432" endWordPosition="436">s can be performed by just adding or updating frequencies. Several earlier works have extensively studied the naive Bayes text classification (Lewis, 1992; Lewis, 1998; McCallum and Nigam, 1998). However, their pure naive Bayes classifiers considered a document as a binary feature vector, and so they can’t utilize the term frequencies in a document, resulting the poor performances. For that reason, the unigram language model classifier (or multinomial naive Bayes text classifier) has been referred as an alternative and promising naive Bayes by a number of researchers(McCallum and Nigam, 1998; Dumais et al., 1998; Yang and Liu, 1999; Nigam et al., 2000). Although the unigram language model classifiers usually outperform the pure naive Bayes, they also have given the disappointing results compared to many other statistical learning methods such as nearest neighbor classifiers(Yang and Chute, 1994), support vector machines(Joachims, 1998), and boosting(Schapire and Singer, 2000), etc. mains(Domingos and Pazzani, 1997) in spite of its wrong independent assumption. In the context of text classification, the probability of class c given a document dj is calculated by Bayes’ theorem as follows: p(cldj) = p(</context>
</contexts>
<marker>Dumais, Plat, Heckerman, Sahami, 1998</marker>
<rawString>Susan Dumais, John Plat, David Heckerman, and Mehran Sahami. 1998. Inductive learning algorithms and representation for text categorization. Proceedings of CIKM-98, 7th ACM International Conference on Information and Knowledge Management, pages 148– 155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Text categorization with support vector machines: learning with many relevant features.</title>
<date>1998</date>
<booktitle>Proceedings of ECML-98, 10th European Conference on Machine Learning,</booktitle>
<pages>137--142</pages>
<contexts>
<context position="3143" citStr="Joachims, 1998" startWordPosition="482" endWordPosition="483">es in a document, resulting the poor performances. For that reason, the unigram language model classifier (or multinomial naive Bayes text classifier) has been referred as an alternative and promising naive Bayes by a number of researchers(McCallum and Nigam, 1998; Dumais et al., 1998; Yang and Liu, 1999; Nigam et al., 2000). Although the unigram language model classifiers usually outperform the pure naive Bayes, they also have given the disappointing results compared to many other statistical learning methods such as nearest neighbor classifiers(Yang and Chute, 1994), support vector machines(Joachims, 1998), and boosting(Schapire and Singer, 2000), etc. mains(Domingos and Pazzani, 1997) in spite of its wrong independent assumption. In the context of text classification, the probability of class c given a document dj is calculated by Bayes’ theorem as follows: p(cldj) = p(djlc)p(c) p(dj) = p(dj c)p(c) p(djIc)p(c) + p(djI�c)p(�c) p(dj 1 c) P(c) p(dj 1 c) =(1) ~~~~1~~~ ~p(c) + p(�c) ~~~~1~~ Now, if we define a new function zj,, zi, = log p(dj c) (2) p(dj c) then, Equation (1) can be rewritten as ���� � p(c) p(c�d�) = (3) cz,, p(c) + p(�c) Using Equation (3), we can get the posterior probability p(c</context>
<context position="11298" citStr="Joachims, 1998" startWordPosition="1878" endWordPosition="1879">hted average over all training documents belonging to the class c, and M for the class c can be estimated in the same manner. 3.3 Feature Weighting Feature selection is often performed as a preprocessing step for the purpose of both reducing the feature space and improving the classification performance. Text classifiers are then trained with various machine learning algorithms in the resulting feature space. (Yang and Pedersen, 1997) investigated some measures to select useful term features including mutual information(MI), information gain(IG), and x2-statistics(CHI), etc. On the contrary, (Joachims, 1998) claimed that there is no useless term features, and it is preferable to use all term features. It is clear that learning and classification become very efficient when the feature space is considerably reduced. However, there is no definite conclusion about the contribution of feature selection to improve overall performances of the text classification systems. It may considerably depend on the employed learning algorithm. We believe that proper external feature selection or weighting is required to improve the performances of naive Bayes since the naive Bayes has no framework of the discrimin</context>
<context position="22103" citStr="Joachims, 1998" startWordPosition="3717" endWordPosition="3718">hese observations, we tested another classifier PNB* which employ different feature weighting method for each bin to obtain the near optimal performances. Table 4 and 5 show the summary of the performances including PNB* on the both collections. Our proposed model with feature weighting methods are very effective compared to the baseline UM method. Moreover, the performance of bin-optimized PNB* in Reuters21578 collection shows that Poisson naive Bayes with feature weighting methods can achieve the state-of-the-art performances achieved by SVM or kNN which are reported in (Yang and Liu, 1999; Joachims, 1998). 5 Conclusion and Future Work In this paper, we propose a Poisson naive Bayes text classification model with feature weighting. Our new model uses the normalized and smoothed term frequencies for each document, and Poisson parameters are calculated by weighted averaging the frequencies over all training documents. Experimental results show that the proposed model is quite useful to build probabilistic text classification systems without requiring any extra cost compared to the traditional simple naive Bayes or unigram language model classifiers. Further improvement is achieved by a feature we</context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>Thorsten Joachims. 1998. Text categorization with support vector machines: learning with many relevant features. Proceedings of ECML-98, 10th European Conference on Machine Learning, pages 137–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Sparck Jones</author>
<author>Steve Walker</author>
<author>Stephen E Robertson</author>
</authors>
<title>A probabilistic model of information retrieval: development and comparative experiments -part 1. Information Processing and</title>
<date>2000</date>
<journal>Management,</journal>
<volume>36</volume>
<issue>6</issue>
<contexts>
<context position="3849" citStr="Jones et al., 2000" startWordPosition="606" endWordPosition="609">ite of its wrong independent assumption. In the context of text classification, the probability of class c given a document dj is calculated by Bayes’ theorem as follows: p(cldj) = p(djlc)p(c) p(dj) = p(dj c)p(c) p(djIc)p(c) + p(djI�c)p(�c) p(dj 1 c) P(c) p(dj 1 c) =(1) ~~~~1~~~ ~p(c) + p(�c) ~~~~1~~ Now, if we define a new function zj,, zi, = log p(dj c) (2) p(dj c) then, Equation (1) can be rewritten as ���� � p(c) p(c�d�) = (3) cz,, p(c) + p(�c) Using Equation (3), we can get the posterior probability p(cIdj) by obtaining zj,, which is a form of log ratio similar to the BIM retrieval model(Jones et al., 2000). It means that the linked independence assumption(Cooper et al., 1992), which explains that the strong independent assumption can be relaxed in the BIM model, is sufficient for the use of naive Bayes text classification model. With this framework, two representative naive Bayes text classification approaches are well introduced in (McCallum and Nigam, 1998). They designated the pure naive Bayes as multivariate Bernoulli model, and the unigram language model classifier as multinomial model. Instead, we introduce multivariate Poisson model to improve the pure naive Bayes text classification in </context>
</contexts>
<marker>Jones, Walker, Robertson, 2000</marker>
<rawString>Karen Sparck Jones, Steve Walker, and Stephen E. Robertson. 2000. A probabilistic model of information retrieval: development and comparative experiments -part 1. Information Processing and Management, 36(6):779–808.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Lewis</author>
</authors>
<title>Representation and learning in information retrieval.</title>
<date>1992</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer Science, University of Massachusetts,</institution>
<location>Amherst, US.</location>
<contexts>
<context position="2348" citStr="Lewis, 1992" startWordPosition="360" endWordPosition="361">e text classification and filtering modules with the existing information retrieval systems easily. It is because that the frequency related information stored in the general text retrieval systems is all the required information in naive Bayes learning. No further complex generalization processes are required unlike the other machine learning methods such as SVM or boosting. Moreover, incremental adaptation using a small number of new training documents can be performed by just adding or updating frequencies. Several earlier works have extensively studied the naive Bayes text classification (Lewis, 1992; Lewis, 1998; McCallum and Nigam, 1998). However, their pure naive Bayes classifiers considered a document as a binary feature vector, and so they can’t utilize the term frequencies in a document, resulting the poor performances. For that reason, the unigram language model classifier (or multinomial naive Bayes text classifier) has been referred as an alternative and promising naive Bayes by a number of researchers(McCallum and Nigam, 1998; Dumais et al., 1998; Yang and Liu, 1999; Nigam et al., 2000). Although the unigram language model classifiers usually outperform the pure naive Bayes, the</context>
</contexts>
<marker>Lewis, 1992</marker>
<rawString>David D. Lewis. 1992. Representation and learning in information retrieval. Ph.D. thesis, Department of Computer Science, University of Massachusetts, Amherst, US.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Lewis</author>
</authors>
<title>Naive (Bayes) at forty: The independence assumption in information retrieval.</title>
<date>1998</date>
<booktitle>Proceedings of ECML-98, 10th European Conference on Machine Learning,</booktitle>
<pages>4--15</pages>
<contexts>
<context position="2361" citStr="Lewis, 1998" startWordPosition="362" endWordPosition="363">fication and filtering modules with the existing information retrieval systems easily. It is because that the frequency related information stored in the general text retrieval systems is all the required information in naive Bayes learning. No further complex generalization processes are required unlike the other machine learning methods such as SVM or boosting. Moreover, incremental adaptation using a small number of new training documents can be performed by just adding or updating frequencies. Several earlier works have extensively studied the naive Bayes text classification (Lewis, 1992; Lewis, 1998; McCallum and Nigam, 1998). However, their pure naive Bayes classifiers considered a document as a binary feature vector, and so they can’t utilize the term frequencies in a document, resulting the poor performances. For that reason, the unigram language model classifier (or multinomial naive Bayes text classifier) has been referred as an alternative and promising naive Bayes by a number of researchers(McCallum and Nigam, 1998; Dumais et al., 1998; Yang and Liu, 1999; Nigam et al., 2000). Although the unigram language model classifiers usually outperform the pure naive Bayes, they also have g</context>
<context position="7339" citStr="Lewis, 1998" startWordPosition="1168" endWordPosition="1169">Poisson distribution is most commonly used to model the number of random occurrences of some phenomenon in a specified unit of space or time, for example, the number of phone calls received by a telephone operator in a 10-minute period. If we think that the occurrence of each term is a random occurrence in a fixed unit of space (i.e. a length of document) the Poisson distribution is intuitively suitable to model the term frequencies in a given document. For that reason, the use of Poisson model is widely investigated in the IR literature, but it is rarely used for the text classification task(Lewis, 1998). It motivates us to adopt the Poisson model for learning the naive Bayes text classification. Our model assumes that dj is generated by multivariate Poisson model. In other words, a document dj is a random vector which consists of the Poisson random variables Xi, and Xi has the value of withinterm-frequency fig for the i-th term ti. Thus, if we assume the independence among the terms in d-, a probability of dj is represented by, IVI p(di) = P(Xi =fij) (4) ��� where, IV l is a vocabulary size, and each P(Xi = fij) is given by, �—����� P(Xi = fig) = (5) fig! where, A is the Poisson mean. As a r</context>
</contexts>
<marker>Lewis, 1998</marker>
<rawString>David D. Lewis. 1998. Naive (Bayes) at forty: The independence assumption in information retrieval. Proceedings of ECML-98, 10th European Conference on Machine Learning, pages 4–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew K McCallum</author>
<author>Kamal Nigam</author>
</authors>
<title>Employing EM in pool-based active learning for text classification.</title>
<date>1998</date>
<booktitle>Proceedings of ICML-98, 15th International Conference on Machine Learning,</booktitle>
<pages>350--358</pages>
<contexts>
<context position="2388" citStr="McCallum and Nigam, 1998" startWordPosition="364" endWordPosition="367">filtering modules with the existing information retrieval systems easily. It is because that the frequency related information stored in the general text retrieval systems is all the required information in naive Bayes learning. No further complex generalization processes are required unlike the other machine learning methods such as SVM or boosting. Moreover, incremental adaptation using a small number of new training documents can be performed by just adding or updating frequencies. Several earlier works have extensively studied the naive Bayes text classification (Lewis, 1992; Lewis, 1998; McCallum and Nigam, 1998). However, their pure naive Bayes classifiers considered a document as a binary feature vector, and so they can’t utilize the term frequencies in a document, resulting the poor performances. For that reason, the unigram language model classifier (or multinomial naive Bayes text classifier) has been referred as an alternative and promising naive Bayes by a number of researchers(McCallum and Nigam, 1998; Dumais et al., 1998; Yang and Liu, 1999; Nigam et al., 2000). Although the unigram language model classifiers usually outperform the pure naive Bayes, they also have given the disappointing resu</context>
<context position="4209" citStr="McCallum and Nigam, 1998" startWordPosition="661" endWordPosition="664"> c) (2) p(dj c) then, Equation (1) can be rewritten as ���� � p(c) p(c�d�) = (3) cz,, p(c) + p(�c) Using Equation (3), we can get the posterior probability p(cIdj) by obtaining zj,, which is a form of log ratio similar to the BIM retrieval model(Jones et al., 2000). It means that the linked independence assumption(Cooper et al., 1992), which explains that the strong independent assumption can be relaxed in the BIM model, is sufficient for the use of naive Bayes text classification model. With this framework, two representative naive Bayes text classification approaches are well introduced in (McCallum and Nigam, 1998). They designated the pure naive Bayes as multivariate Bernoulli model, and the unigram language model classifier as multinomial model. Instead, we introduce multivariate Poisson model to improve the pure naive Bayes text classification in the next section. In the real world, an operational text classification system is usually placed in the environment where the amount of human-annotated training documents is small in spite of the hundreds of thousands classes. Moreover, re-training of the text classifiers is required since a small number of new training documents are continuously provided. I</context>
<context position="17666" citStr="McCallum and Nigam, 1998" startWordPosition="2968" endWordPosition="2971">UM) Figure 1 shows the performances of our new model named Poisson naive Bayes(PNB) classifiers acTable 2: Performances of UM and PNB on the Reuters21578 collection UM PNB(min) PNB(max) MicroF1 0.7212 0.7644 0.7706 MacroF1 0.3214 0.4227 0.4358 Table 3: Performances of UM and PNB on the KoreanNews2002 collection UM PNB(min) PNB(max) MicroF1 0.6502 0.7031 0.7094 MacroF1 0.5208 0.5859 0.5949 cording to the interpolation parameter a for estimating Poisson mean A and µ. The baseline method is a unigram model classifier (UM) which is also referred to multinomial naive Bayes classifier described in (McCallum and Nigam, 1998). Our proposed PNB clearly outperforms the UM. Although there is no significant difference of MicroF1 values among the various a values, the F1 value of each class is considerably affected by the a values. Figure 2 presents the fluctuations of the F1 values for 4 classes in Reuters21578 collection. From this result, we can assume that there is no global optimal value of a, but each class has its own optimal a. In our experiments, many of the classes have the highest F1 value when a is about 0.8 or 0.9 except some classes such as corn class which shows the highest F1 value at a = 0 .3. Similar </context>
</contexts>
<marker>McCallum, Nigam, 1998</marker>
<rawString>Andrew K. McCallum and Kamal Nigam. 1998. Employing EM in pool-based active learning for text classification. Proceedings of ICML-98, 15th International Conference on Machine Learning, pages 350– 358.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kamal Nigam</author>
<author>Andrew K McCallum</author>
<author>Sebastian Thrun</author>
<author>Tom M Mitchell</author>
</authors>
<title>Text classification from labeled and unlabeled documents using EM.</title>
<date>2000</date>
<booktitle>Machine Learning,</booktitle>
<pages>39--2</pages>
<contexts>
<context position="2854" citStr="Nigam et al., 2000" startWordPosition="441" endWordPosition="444">ting frequencies. Several earlier works have extensively studied the naive Bayes text classification (Lewis, 1992; Lewis, 1998; McCallum and Nigam, 1998). However, their pure naive Bayes classifiers considered a document as a binary feature vector, and so they can’t utilize the term frequencies in a document, resulting the poor performances. For that reason, the unigram language model classifier (or multinomial naive Bayes text classifier) has been referred as an alternative and promising naive Bayes by a number of researchers(McCallum and Nigam, 1998; Dumais et al., 1998; Yang and Liu, 1999; Nigam et al., 2000). Although the unigram language model classifiers usually outperform the pure naive Bayes, they also have given the disappointing results compared to many other statistical learning methods such as nearest neighbor classifiers(Yang and Chute, 1994), support vector machines(Joachims, 1998), and boosting(Schapire and Singer, 2000), etc. mains(Domingos and Pazzani, 1997) in spite of its wrong independent assumption. In the context of text classification, the probability of class c given a document dj is calculated by Bayes’ theorem as follows: p(cldj) = p(djlc)p(c) p(dj) = p(dj c)p(c) p(djIc)p(c)</context>
</contexts>
<marker>Nigam, McCallum, Thrun, Mitchell, 2000</marker>
<rawString>Kamal Nigam, Andrew K. McCallum, Sebastian Thrun, and Tom M. Mitchell. 2000. Text classification from labeled and unlabeled documents using EM. Machine Learning, 39(2/3):103–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert E Schapire</author>
<author>Yoram Singer</author>
</authors>
<title>BOOSTEXTER: a boosting-based system for text categorization.</title>
<date>2000</date>
<booktitle>Machine Learning,</booktitle>
<pages>39--2</pages>
<contexts>
<context position="3184" citStr="Schapire and Singer, 2000" startWordPosition="485" endWordPosition="488">e poor performances. For that reason, the unigram language model classifier (or multinomial naive Bayes text classifier) has been referred as an alternative and promising naive Bayes by a number of researchers(McCallum and Nigam, 1998; Dumais et al., 1998; Yang and Liu, 1999; Nigam et al., 2000). Although the unigram language model classifiers usually outperform the pure naive Bayes, they also have given the disappointing results compared to many other statistical learning methods such as nearest neighbor classifiers(Yang and Chute, 1994), support vector machines(Joachims, 1998), and boosting(Schapire and Singer, 2000), etc. mains(Domingos and Pazzani, 1997) in spite of its wrong independent assumption. In the context of text classification, the probability of class c given a document dj is calculated by Bayes’ theorem as follows: p(cldj) = p(djlc)p(c) p(dj) = p(dj c)p(c) p(djIc)p(c) + p(djI�c)p(�c) p(dj 1 c) P(c) p(dj 1 c) =(1) ~~~~1~~~ ~p(c) + p(�c) ~~~~1~~ Now, if we define a new function zj,, zi, = log p(dj c) (2) p(dj c) then, Equation (1) can be rewritten as ���� � p(c) p(c�d�) = (3) cz,, p(c) + p(�c) Using Equation (3), we can get the posterior probability p(cIdj) by obtaining zj,, which is a form of</context>
</contexts>
<marker>Schapire, Singer, 2000</marker>
<rawString>Robert E. Schapire and Yoram Singer. 2000. BOOSTEXTER: a boosting-based system for text categorization. Machine Learning, 39(2/3):135–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yiming Yang</author>
<author>Christopher G Chute</author>
</authors>
<title>An example-based mapping method for text categorization and retrieval.</title>
<date>1994</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>12</volume>
<issue>3</issue>
<contexts>
<context position="3102" citStr="Yang and Chute, 1994" startWordPosition="476" endWordPosition="479">r, and so they can’t utilize the term frequencies in a document, resulting the poor performances. For that reason, the unigram language model classifier (or multinomial naive Bayes text classifier) has been referred as an alternative and promising naive Bayes by a number of researchers(McCallum and Nigam, 1998; Dumais et al., 1998; Yang and Liu, 1999; Nigam et al., 2000). Although the unigram language model classifiers usually outperform the pure naive Bayes, they also have given the disappointing results compared to many other statistical learning methods such as nearest neighbor classifiers(Yang and Chute, 1994), support vector machines(Joachims, 1998), and boosting(Schapire and Singer, 2000), etc. mains(Domingos and Pazzani, 1997) in spite of its wrong independent assumption. In the context of text classification, the probability of class c given a document dj is calculated by Bayes’ theorem as follows: p(cldj) = p(djlc)p(c) p(dj) = p(dj c)p(c) p(djIc)p(c) + p(djI�c)p(�c) p(dj 1 c) P(c) p(dj 1 c) =(1) ~~~~1~~~ ~p(c) + p(�c) ~~~~1~~ Now, if we define a new function zj,, zi, = log p(dj c) (2) p(dj c) then, Equation (1) can be rewritten as ���� � p(c) p(c�d�) = (3) cz,, p(c) + p(�c) Using Equation (3),</context>
</contexts>
<marker>Yang, Chute, 1994</marker>
<rawString>Yiming Yang and Christopher G. Chute. 1994. An example-based mapping method for text categorization and retrieval. ACM Transactions on Information Systems, 12(3):252–277.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yiming Yang</author>
<author>Xin Liu</author>
</authors>
<title>A re-examination of text categorization methods.</title>
<date>1999</date>
<booktitle>Proceedings of SIGIR99, 22nd ACM International Conference on Research and Development in Information Retrieval,</booktitle>
<pages>42--49</pages>
<contexts>
<context position="2833" citStr="Yang and Liu, 1999" startWordPosition="437" endWordPosition="440"> just adding or updating frequencies. Several earlier works have extensively studied the naive Bayes text classification (Lewis, 1992; Lewis, 1998; McCallum and Nigam, 1998). However, their pure naive Bayes classifiers considered a document as a binary feature vector, and so they can’t utilize the term frequencies in a document, resulting the poor performances. For that reason, the unigram language model classifier (or multinomial naive Bayes text classifier) has been referred as an alternative and promising naive Bayes by a number of researchers(McCallum and Nigam, 1998; Dumais et al., 1998; Yang and Liu, 1999; Nigam et al., 2000). Although the unigram language model classifiers usually outperform the pure naive Bayes, they also have given the disappointing results compared to many other statistical learning methods such as nearest neighbor classifiers(Yang and Chute, 1994), support vector machines(Joachims, 1998), and boosting(Schapire and Singer, 2000), etc. mains(Domingos and Pazzani, 1997) in spite of its wrong independent assumption. In the context of text classification, the probability of class c given a document dj is calculated by Bayes’ theorem as follows: p(cldj) = p(djlc)p(c) p(dj) = p(</context>
<context position="22086" citStr="Yang and Liu, 1999" startWordPosition="3713" endWordPosition="3716">assification. From these observations, we tested another classifier PNB* which employ different feature weighting method for each bin to obtain the near optimal performances. Table 4 and 5 show the summary of the performances including PNB* on the both collections. Our proposed model with feature weighting methods are very effective compared to the baseline UM method. Moreover, the performance of bin-optimized PNB* in Reuters21578 collection shows that Poisson naive Bayes with feature weighting methods can achieve the state-of-the-art performances achieved by SVM or kNN which are reported in (Yang and Liu, 1999; Joachims, 1998). 5 Conclusion and Future Work In this paper, we propose a Poisson naive Bayes text classification model with feature weighting. Our new model uses the normalized and smoothed term frequencies for each document, and Poisson parameters are calculated by weighted averaging the frequencies over all training documents. Experimental results show that the proposed model is quite useful to build probabilistic text classification systems without requiring any extra cost compared to the traditional simple naive Bayes or unigram language model classifiers. Further improvement is achieve</context>
</contexts>
<marker>Yang, Liu, 1999</marker>
<rawString>Yiming Yang and Xin Liu. 1999. A re-examination of text categorization methods. Proceedings of SIGIR99, 22nd ACM International Conference on Research and Development in Information Retrieval, pages 42– 49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yiming Yang</author>
<author>Jan O Pedersen</author>
</authors>
<title>A comparative study on feature selection in text categorization.</title>
<date>1997</date>
<booktitle>Proceedings of ICML-97, 14th International Conference on Machine Learning,</booktitle>
<pages>412--420</pages>
<contexts>
<context position="11121" citStr="Yang and Pedersen, 1997" startWordPosition="1855" endWordPosition="1858"> more reliable than those in the short documents, hence we try to interpolate between the two different probabilities with the parameter a ranging from 0 to 1. Consequently, Ai is a weighted average over all training documents belonging to the class c, and M for the class c can be estimated in the same manner. 3.3 Feature Weighting Feature selection is often performed as a preprocessing step for the purpose of both reducing the feature space and improving the classification performance. Text classifiers are then trained with various machine learning algorithms in the resulting feature space. (Yang and Pedersen, 1997) investigated some measures to select useful term features including mutual information(MI), information gain(IG), and x2-statistics(CHI), etc. On the contrary, (Joachims, 1998) claimed that there is no useless term features, and it is preferable to use all term features. It is clear that learning and classification become very efficient when the feature space is considerably reduced. However, there is no definite conclusion about the contribution of feature selection to improve overall performances of the text classification systems. It may considerably depend on the employed learning algorit</context>
<context position="13773" citStr="Yang and Pedersen, 1997" startWordPosition="2303" endWordPosition="2306">ple, p(c) is the number of documents belonging to the class c divided by the total number of documents, and p( w) is the number of documents without the term w divided by the total number of documents, etc. Second measure we used is x2- statistics developed for the statistical test of the hypothesis. In the text classification, given a two-way contingency table for each term ti and class c as represented in Table 1, wic is calculated as follows: (ad — bc)2 wic = (a + b)(a + c)(b + d)(c + d) (12) where, a,b,c and d indicate the number of documents for each cell in the above contingency table. (Yang and Pedersen, 1997) compared the various feature selection methods, and concluded that these two measures are most effective for their kNN and LLSF classification models. Finally, we introduce a new measure - probability ratio. Probability ratio is defined by, wic = p(wi1 C) + P(wi1 C) (13) P(wi1�e) p(wi1C) This measure calculates the sum of the ratio of two class-conditional probabilities from each class and its reciprocal. The former term and the latter term � • ��� We e fz, (10) fz, IVI zjC = ��� wic Az i � µz/ii � � csE{c,cl wtE{wz, wzl are representing the degree of predicting positive and negative class re</context>
</contexts>
<marker>Yang, Pedersen, 1997</marker>
<rawString>Yiming Yang and Jan O. Pedersen. 1997. A comparative study on feature selection in text categorization. Proceedings of ICML-97, 14th International Conference on Machine Learning, pages 412–420.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>