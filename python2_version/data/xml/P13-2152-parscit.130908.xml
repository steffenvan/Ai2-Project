<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.018862">
<title confidence="0.960696">
Word surprisal predicts N400 amplitude during reading
</title>
<author confidence="0.923437">
Stefan L. Frank1,2 Leun J. Otten3 Giulia Galli3 Gabriella Vigliocco2
</author>
<email confidence="0.613058">
{s.frank, l.otten, g.galli, g.vigliocco}@ucl.ac.uk
</email>
<affiliation confidence="0.991183666666667">
1Centre for Language Studies, Radboud University Nijmegen
2Department of Cognitive, Perceptual and Brain Sciences, University College London
3Institute of Cognitive Neuroscience, University College London
</affiliation>
<sectionHeader confidence="0.978801" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999962578947369">
We investigated the effect of word sur-
prisal on the EEG signal during sen-
tence reading. On each word of 205 ex-
perimental sentences, surprisal was esti-
mated by three types of language model:
Markov models, probabilistic phrase-
structure grammars, and recurrent neu-
ral networks. Four event-related poten-
tial components were extracted from the
EEG of 24 readers of the same sentences.
Surprisal estimates under each model type
formed a significant predictor of the am-
plitude of the N400 component only, with
more surprising words resulting in more
negative N400s. This effect was mostly
due to content words. These findings
provide support for surprisal as a gener-
ally applicable measure of processing dif-
ficulty during language comprehension.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999678219512195">
Many studies of human language comprehension
measure the brain’s electrical activity during read-
ing. Such electroencephalography (EEG) experi-
ments have revealed that the EEG signal displays
systematic variation in response to the appearance
of each word. The different components that can
be observed in this signal are known as event-
related potentials (ERPs). Probably the most reli-
ably observed (and most studied) of these compo-
nents is a negative-going deflection at centropari-
etal electrodes that peaks at around 400 ms after
word onset and is therefore referred to as the N400
component.
It is well known that the N400 increases in am-
plitude (i.e., becomes more negative) when the
word leads to comprehension difficulty. To study
the general relation between word predictability
and the N400, Dambacher et al. (2006) obtained
subjective word-probability estimates (so-called
cloze probabilities) by asking participants to pre-
dict the upcoming word at each point in a large
number of sentences. A different group of subjects
read these same sentences while their EEG signal
was recorded. Results showed a correlation be-
tween N400 amplitude and cloze probability: Less
predictable words yielded stronger N400s.
We investigated whether similar results can be
obtained using more objective, model-based word
probabilities. For each word in a collection of
English sentences, estimates of its surprisal (i.e.,
its negative log-transformed conditional probabil-
ity: −log P(wt|w1, ... , wt−1)) were generated
by three types of language model: Markov (i.e., n-
gram) models, phrase-structure grammars (PSGs),
and recurrent neural networks (RNNs). Next, EEG
signals of participants reading the same sentences
were recorded. A comparison of word surprisal to
different ERP components revealed that, indeed,
N400 amplitude was predicted by surprisal values:
More surprising words resulted in more negative
N400s, at least for content words.
</bodyText>
<sectionHeader confidence="0.967043" genericHeader="method">
2 Language models
</sectionHeader>
<bodyText confidence="0.999797909090909">
A range of models of each type was trained, al-
lowing to investigate whether models that capture
the language statistics more accurately also yield
better predictions of ERP size. Such a relation is
generally found in studies that use word-reading
time as the dependent variable (Fernandez Mon-
salve et al., 2012; Frank and Bod, 2011; Frank
and Thompson, 2012), providing additional sup-
port that these psychological data are indeed ex-
plained by the surprisal values and not by some
confounding variable.
</bodyText>
<subsectionHeader confidence="0.997488">
2.1 Corpus data
</subsectionHeader>
<bodyText confidence="0.992045666666667">
All models were trained on sentences from the
written texts in the British National Corpus
(BNC). First, the 10,000 word types with highest
</bodyText>
<page confidence="0.96696">
878
</page>
<bodyText confidence="0.892415076923077">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 878–883,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
frequency were selected from the BNC. Next, all
sentences were extracted that contained only those
words. This resulted in a training corpus of 1.06
million sentences (12.6 million word tokens).
Each trained model estimated a surprisal value
for each word of the 205 sentences (1931 word to-
kens) for which eye-tracking data are available in
the UCL corpus of reading times (Frank et al., in
press). These sentences, which were selected from
three unpublished novels, only contained words
from the 10,000 high-frequency word list.
</bodyText>
<subsectionHeader confidence="0.999041">
2.2 Markov models
</subsectionHeader>
<bodyText confidence="0.999957">
Markov models were trained with modified
Kneser-Ney smoothing (Chen and Goodman,
1999) as implemented in SRILM (Stolcke, 2002).
Model order was varied: n = 2, 3, 4. No unigram
model was computed because word frequency was
factored out during data analysis (see Section 4.2).
</bodyText>
<subsectionHeader confidence="0.999217">
2.3 Recurrent neural networks
</subsectionHeader>
<bodyText confidence="0.999931666666667">
The RNN model architecture has been thoroughly
described elsewhere (Fernandez Monsalve et al.,
2012; Frank, in press) so it is not discussed here.
The only difference with previous versions was
that the current RNN was trained on a substantially
larger data set with more word types. A range of
RNN models was obtained by training on nine in-
creasingly large subsets of the BNC data, compris-
ing 2K, 5K, 10K, 20K, 50K, 100K, 200K, 400K,
and all 1.06M sentences. In addition, the network
was trained on the full set twice, making a total of
ten instantiations of the RNN model.
</bodyText>
<subsectionHeader confidence="0.992284">
2.4 Phrase-structure grammars
</subsectionHeader>
<bodyText confidence="0.999929357142857">
To prepare data for PSG training, the selected
BNC sentences were parsed by the Stanford parser
(Klein and Manning, 2003). The resulting tree-
bank was divided into nine increasingly large sub-
sets, equal to those used for RNN training.1 Gram-
mars were induced from these subsets using the
algorithm by Roark (2001) with its standard set-
tings. Next, surprisal values on the experimental
sentences were generated by Roark’s incremental
parser. Since increasing the parser’s beam width
has been shown to improve both word-probability
estimates and the fit to word-reading times (Frank,
2009), the parser’s ‘base beam threshold’ parame-
ter was reduced to 10−20.
</bodyText>
<footnote confidence="0.987124333333333">
1Because not all experimental sentences could be parsed
when the treebank comprised only 2K sentences, 1K sen-
tences were added to the smallest subset.
</footnote>
<sectionHeader confidence="0.941372" genericHeader="method">
3 EEG data collection
</sectionHeader>
<bodyText confidence="0.999985615384616">
Twenty-four healthy, adult volunteers from the
UCL Psychology subject pool took part in the
reading study. Their EEG was recorded contin-
uously from 32 channels during the presentation
of 5 practice sentences and the 205 experimental
items. Participants were asked to minimise blinks,
eye movements, and head movements during sen-
tence presentation.
Each sentence was preceded by a centrally pre-
sented fixation cross. As soon as the partici-
pant pressed a key, the cross was replaced by the
sentence’s first word, which was then automati-
cally replaced by each subsequent word. Word
presentation duration (in milliseconds) equalled
190 + 20k, where k is the number of characters
in the word (including any attached punctuation).
After the word disappeared, there was a 390 ms
interval before the next word appeared.
The sentences were presented in random or-
der, one word at a time, always centrally located
on the monitor. One-hundred and ten of the ex-
perimental sentences were followed by a yes/no-
comprehension question, to ensure that partici-
pants tried to understand the sentences. All par-
ticipants answered at least 80% of the comprehen-
sion questions correctly.
</bodyText>
<sectionHeader confidence="0.980625" genericHeader="method">
4 Data analysis
</sectionHeader>
<subsectionHeader confidence="0.99861">
4.1 ERP components
</subsectionHeader>
<bodyText confidence="0.999940333333333">
Four ERP components of interest were identified
from the literature on EEG and sentence reading:
Early Left Anterior Negativity (ELAN), P200,
N400, and a post-N400 positivity (PNP). Table 1
lists the corresponding time windows and approx-
imate electrode sites.2 For each component, the
average electrode potential over the corresponding
time window and electrodes was computed. These
average ERP amplitudes served as the four depen-
dent variables for data analysis.
The ELAN component is generally thought of
as indicative of difficulty with constructing syntac-
tic phrase structure (Friederici et al., 1999; Gunter
et al., 1999; Neville et al., 1991). Hence, if any
of the model types predicts ELAN size, we would
expect this to be the PSG.
Dambacher et al. (2006) found effects of word
frequency or length (which are strongly correlated
</bodyText>
<footnote confidence="0.747937">
2The P600 component (Osterhout and Holcomb, 1992)
was not included because the shortest interval between con-
secutive word onsets was only 600 ms.
</footnote>
<page confidence="0.992866">
879
</page>
<table confidence="0.9988482">
Component Time window Location
ELAN 125–175 ms left anterior
P200 140–200 ms frontocentral
N400 300–500 ms centroparietal
PNP 400–600 ms frontopolar
</table>
<tableCaption confidence="0.983691">
Table 1: Investigated ERP components, their time
windows, and approximate scalp locations.
</tableCaption>
<bodyText confidence="0.999972863636364">
and therefore difficult to tease apart) on the P200
amplitude. Since we factor out these two lexical
factors in the analysis, we expect no additional ef-
fect of surprisal on P200.
If any of the components is sensitive to word
surprisal, this is most likely to be the N400 as
many studies have already shown that N400 am-
plitude depends on subjective word predictabil-
ity (Dambacher et al., 2006; Kutas and Hillyard,
1984; Moreno et al., 2002). Whether an effect
will appear on the PNP is more doubtful. Van
Petten and Luka (2012) argue that word expec-
tations that are confirmed result in reduced N400
size, whereas expectations that are disconfirmed
increase the PNP. However, in a probabilistic set-
ting, expectations are not all-or-nothing so there
is no strict distinction between confirmation and
disconfirmation. Nevertheless, surprisal effects on
PNP may occur. Since the PNP has received rel-
atively little attention, the component may not be
such a reliable index of comprehension difficulty
as the N400 has proven to be.
</bodyText>
<subsectionHeader confidence="0.998042">
4.2 Regression analysis
</subsectionHeader>
<bodyText confidence="0.99999325">
Data were discarded on words attached to a
comma, clitics, sentence-initial, and sentence-
final words. Moreover, artifacts in the EEG data
(mostly due to eye blinks) were identified and re-
moved, leaving 32,010 analysed data points per in-
vestigated ERP component. For each data point
and ERP component, a baseline potential was de-
termined by averaging over the component’s elec-
trodes in the 100 ms leading up to word onset.
In order to quantify the fit of surprisal to ERP
size, a linear mixed-effects regression model was
fitted to each of the four ERPs, using the pre-
dictors: baseline potential, log-transformed word
frequency, word length (number of characters),
word position in the sentence, and sentence po-
sition in the experiment.3 Also, all significant
</bodyText>
<footnote confidence="0.8506835">
3For word and sentence position, both linear and squared
factors were included in order to capture possible non-linear
</footnote>
<bodyText confidence="0.999918555555556">
two-way interactions were included (main effects
were removed if they were not significant and did
not appear in any interaction). In addition, there
were by-subject and by-item random intervals, as
well as significant by-subject and by-item random
slopes. Parameters for the correlation between
random intercept and slope where also estimated,
if they significantly contributed to model fit.
When the surprisal estimates by a particular lan-
guage model are included in the analysis, the re-
gression model’s deviance decreases. The size of
this decrease is the x2-statistic of a likelihood-
ratio test for significance of the surprisal effect,
and was taken as the measure of the surprisal val-
ues’ fit to the ERP data.4 Negative values will be
used to indicate effects in the negative direction,
that is, when higher surprisal results in more neg-
ative (or less positive) going ERP deflections.
</bodyText>
<sectionHeader confidence="0.999982" genericHeader="method">
5 Results
</sectionHeader>
<subsectionHeader confidence="0.997323">
5.1 Surprisal effects
</subsectionHeader>
<bodyText confidence="0.9930135">
Figure 1 plots the fit of each model’s surprisal esti-
mates to ERP amplitude as a function of the aver-
age natural log P(wt|w1,... , wt−1), which quan-
tifies to what extent the model has acquired accu-
rate language statistics.5 For the ELAN, P200 and
PNP components, there were no significant effects
after correcting for multiple comparisons. In con-
trast, effects on the N400 were highly significant.
</bodyText>
<subsectionHeader confidence="0.998741">
5.2 Model comparison
</subsectionHeader>
<bodyText confidence="0.9999267">
Table 2 shows results of pairwise comparisons be-
tween the best models of each type (i.e., those
whose surprisal estimates fit the N400 data best).
Clearly, RNN-based surprisal explains variance
over and above each of the other two models
whereas neither the n-gram nor the PSG model
outperforms the RNN. Moreover, the RNN’s sur-
prisals explain a marginally significant (x2 =
3.47; p &lt; .07) amount of variance over and above
the combined PSG and n-gram surprisals.
</bodyText>
<footnote confidence="0.966756888888889">
changes over the course of the sentence or experiment.
4This definition equals what Frank and Bod (2011) call
‘psychological accuracy’ in an analysis of reading times.
5This measure, which Frank and Bod (2011) call ‘linguis-
tic accuracy’, equals the negative logarithm of the model’s
perplexity. Increasing the amount of training data (or the
value of n) resulted in higher linguistic accuracy, except for
the three PSG models trained on the smallest amounts of data.
This shows that the models did not suffer from overfitting.
</footnote>
<page confidence="0.989437">
880
</page>
<figureCaption confidence="0.982668571428571">
Figure 1: Fit to surprisal of ERP amplitude (for ELAN, P200, N400, and PNP components) as a function
of average log P(wt|w1, ... , wt−1). Each plotted point corresponds to predictions by one of the trained
models. Dotted lines indicate x2 = ±3.84, beyond which effects are statistically significant (p &lt; .05)
if no correction for multiple comparisons is applied. The dashed line indicates the level below which
effects are significant after applying the correction proposed by Benjamini and Hochberg (1995), on
each ERP component separately because of our prior expectation that effects would occur mostly (if not
exclusively) on the N400 component.
</figureCaption>
<figure confidence="0.9953489375">
ELAN
P200
N400
PNP
−6.5 −6 −5.5 −5
av. log P(wt|w1,...,wt−1)
−6.5 −6 −5.5 −5
av. log P(wt|w1,...,wt−1)
−6.5 −6 −5.5 −5
av. log P(wt|w1,...,wt−1)
−6.5 −6 −5.5 −5
av. log P(wt|w1,...,wt−1)
10
0
−10
−20
10
10
RNN
PSG
n−gram
−10
−20
0
0
fit of surprisal (±x2)
−10
−20
10
0
−10
−20
</figure>
<table confidence="0.95125">
Model n-gram RII PSG
n-gram x2 = 1.34 x2 = 1.66
p &gt; .2 p &gt; .1
RII x2 = 6.52 x2 = 4.78
p &lt; .02 p &lt; .05
PSG x2 = 4.20 x2 = 2.14
p &lt; .05 p &gt; .1
</table>
<tableCaption confidence="0.985745">
Table 2: Pairwise comparisons between surprisal
</tableCaption>
<bodyText confidence="0.9526705">
estimates by the best models of each type. Shown
are the results of likelihood-ratio tests for the ef-
fect of one set of surprisal estimates (rows) over
and above the other (columns).
</bodyText>
<subsectionHeader confidence="0.998689">
5.3 Comparing word classes
</subsectionHeader>
<bodyText confidence="0.999975166666667">
N400 effects are nearly exclusively investigated
on content (i.e., open-class) words. Dambacher et
al. (2006), too, investigated the relation between
ERP amplitudes and cloze probabilities on con-
tent words only. When running separate analyses
on content and function words (constituting 53.2%
and 46.8% of the data, respectively), we found that
the N400 effect of Figure 1 is nearly fully driven
by content words (see Figure 2). None of the mod-
els’ surprisal estimates formed a significant pre-
dictor of N400 amplitude on function words, after
correction for multiple comparisons.
</bodyText>
<sectionHeader confidence="0.999637" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.997668892857143">
We demonstrated a clear effect of word surprisal,
as estimated by different language models, on the
EEG signal: The larger a (content) word’s sur-
prisal value, the more negative the resulting N400.
Figure 2: Fit to surprisal of N400 amplitude, for
content words (left) and function words (right).
Dotted lines indicate x2 = ±3.84, beyond which
effects are statistically significant (p &lt; .05) with-
out correcting for multiple comparisons. Dashed
lines indicates the levels beyond which effects
are significant after multiple-comparison correc-
tion (Benjamini and Hochberg, 1995).
The N400 component is generally viewed as in-
dicative of lexical rather than syntactic processing
(Kaan, 2007), which may explain why surprisal
under the PSG model did not have any significant
explanatory value over and above RNN-based sur-
prisal. The relatively weak performance of our
Markov models is most likely due to their strict
(and cognitively unrealistic) limit on the size of
the prior context upon which word-probability es-
timates are conditioned.
Unlike the ELAN, P200, and PNP components,
the N400 is known to be sensitive to the cloze
probability of content words. The fact that sur-
prisal effects were found on the N400 only, there-
fore suggests that subjective predictability scores
and model-based surprisal estimates form opera-
</bodyText>
<figure confidence="0.997288230769231">
content words function words
−8.5 −8 −7.5 −7 −6.5 −4.5 −4 −3.5
av. log P(wt|w1,...,wt−1) av. log P(wt|w1,...,wt−1)
fit of surprisal (±x2)
−10
−20
0
−10
−20
0
RNN
PSG
n−gram
</figure>
<page confidence="0.995306">
881
</page>
<bodyText confidence="0.99998875">
tionalisations of one and the same underlying cog-
nitive factor. Needless to say, our statistical mod-
els fail to capture many information sources, such
as semantics and discourse, that do affect cloze
probabilities. However, it is possible in principle
to integrate these into probabilistic language mod-
els (Dubey et al., 2011; Mitchell et al., 2010).
To the best of our knowledge, only one other
published study relates language model predic-
tions to the N400: Parviz et al. (2011) found that
surprisal estimates (corrected for word frequency)
from an n = 4 Markov model predicted N400 size
as measured by magnetoencephalography (rather
than EEG). Although their PSG-based surprisals
did not correlate with N400 size, a related measure
derived from the PSG –lexical entropy– did. How-
ever, Parviz et al. (2011) only looked at effects
on the sentence-final content word of items con-
structed for a speech perception experiment (Ka-
likow et al., 1977), rather than investigating sur-
prisal’s general predictive value across words of
naturally occurring sentences, as we did here.
Our experimental design was parametric rather
than factorial, which allowed us to study the effect
of surprisal over a sample of English sentences
rather than carefully manipulating surprisal while
holding other factors constant. This has the ad-
vantage that our findings are likely to generalise to
other sentence stimuli, but it can also raise a pos-
sible concern: The N400 effect may not be due
to surprisal itself, but to an unknown confound-
ing variable that was not included in the regression
analysis. However, this seems unlikely because of
two additional findings that only follow naturally
if surprisal is indeed the relevant predictor: Signif-
icant results only appeared where they were most
expected a priori (i.e., on N400 but not on other
components) and there was a nearly monotonic re-
lation between the models’ word-prediction accu-
racy and their ability to account for N400 size.
</bodyText>
<sectionHeader confidence="0.998485" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999632555555556">
Although word surprisal has often been shown
to be predictive of word-reading time (Fernan-
dez Monsalve et al., 2012; Frank and Thompson,
2012; Smith and Levy, in press), a general effect
on the EEG signal has not before been demon-
strated. Hence, these results provide additional ev-
idence in support of surprisal as a reliable measure
of cognitive processing difficulty during sentence
comprehension (Hale, 2001; Levy, 2008).
</bodyText>
<sectionHeader confidence="0.995499" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999918857142857">
The research presented here was funded by the
European Union Seventh Framework Programme
(FP7/2007-2013) under grant number 253803.
The authors acknowledge the use of the UCL Le-
gion High Performance Computing Facility, and
associated support services, in the completion of
this work.
</bodyText>
<sectionHeader confidence="0.997938" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.994156071428571">
Y. Benjamini and Y. Hochberg. 1995. Controlling the
false discovery rate: a practical and powerful ap-
proach to multiple testing. Journal of the Royal Sta-
tistical Society B, 57:289–300.
S. F. Chen and J. Goodman. 1999. An empirical
study of smoothing techniques for language model-
ing. Computer Speech and Language, 13:359–394.
M. Dambacher, R. Kliegl, M. Hofmann, and A. M. Ja-
cobs. 2006. Frequency and predictability effect on
event-related potentials during reading. Brain Re-
search, 1084:89–103.
A. Dubey, F. Keller, and P. Sturt. 2011. A model of
discourse predictions in human sentence processing.
In Proceedings of the 2011 Conference on Empiri-
cal Methods in Natural Language Processing, pages
304–312. Edinburgh, UK: Association for Compu-
tational Linguistics.
I. Fernandez Monsalve, S. L. Frank, and G. Vigliocco.
2012. Lexical surprisal as a general predictor of
reading time. In Proceedings of the 13th Confer-
ence of the European Chapter of the Association
for Computational Linguistics, pages 398–408. Avi-
gnon, France: Association for Computational Lin-
guistics.
S. L. Frank and R. Bod. 2011. Insensitivity of the
human sentence-processing system to hierarchical
structure. Psychological Science, 22:829–834.
S. L. Frank and R. L. Thompson. 2012. Early ef-
fects of word surprisal on pupil size during read-
ing. In Proceedings of the 34th Annual Conference
of the Cognitive Science Society, pages 1554–1559.
Austin, TX: Cognitive Science Society.
S. L. Frank, I. Fernandez Monsalve, R. L. Thompson,
and G. Vigliocco. in press. Reading time data for
evaluating broad-coverage models of English sen-
tence processing. Behavior Research Methods.
S. L. Frank. 2009. Surprisal-based comparison be-
tween a symbolic and a connectionist model of sen-
tence processing. In N. A. Taatgen and H. van Rijn,
editors, Proceedings of the 31st Annual Conference
of the Cognitive Science Society, pages 1139–1144.
Austin, TX: Cognitive Science Society.
</reference>
<page confidence="0.990735">
882
</page>
<reference confidence="0.999848351351351">
S. L. Frank. in press. Uncertainty reduction as a mea-
sure of cognitive processing load in sentence com-
prehension. Topics in Cognitive Science.
A. D. Friederici, K. Steinhauer, and S. Frisch. 1999.
Lexical integration: sequential effects of syntactic
and semantic information. Memory &amp; Cognition,
27:438–453.
T. C. Gunter, A. D. Friederici, and A. Hahne. 1999.
Brain responses during sentence reading: Visual
input affects central processes. NeuroReport,
10:3175–3178.
J. T. Hale. 2001. A probabilistic Early parser as
a psycholinguistic model. In Proceedings of the
2nd Conference of the North American Chapter of
the Association for Computational Linguistics, vol-
ume 2, pages 159–166. Pittsburgh, PA: Association
for Computational Linguistics.
E. Kaan. 2007. Event-related potentials and language
processing: a brief overview. Language and Lin-
guistics Compass, 1:571–591.
D. N. Kalikow, K. N. Stevens, and L. L. Elliott. 1977.
Development of a test of speech intelligibility in
noise using sentence materials with controlled word
predictability. Journal of the Acoustical Society of
America, 61:1337–1351.
D. Klein and C. D. Manning. 2003. Accurate unlex-
icalized parsing. In Proceedings of the 41st Meet-
ing of the Association for Computational Linguis-
tics, pages 423–430. Sapporo, Japan: Association
for Computational Linguistics.
M. Kutas and S. A. Hillyard. 1984. Brain potentials
during reading reflect word expectancy and semantic
association. Nature, 307:161–163.
R. Levy. 2008. Expectation-based syntactic compre-
hension. Cognition, 106:1126–1177.
J. Mitchell, M. Lapata, V. Demberg, and F. Keller.
2010. Syntactic and semantic factors in process-
ing difficulty: An integrated measure. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 196–206. Up-
psala, Sweden: Association for Computational Lin-
guistics.
E. M. Moreno, K. D. Federmeier, and M. Kutas. 2002.
Switching languages, switching palabras (words):
an electrophysiological study of code switching.
Brain and Language, 80:188–207.
H. Neville, J. L. Nicol, A. Barss, K. I. Forster, and M. F.
Garrett. 1991. Syntactically based sentence pro-
cessing classes: evidence from event-related brain
potentials. Journal of Cognitive Neuroscience,
3:151–165.
L. Osterhout and P. J. Holcomb. 1992. Event-related
brain potentials elicited by syntactic anomaly. Jour-
nal ofMemory and Language, 31:785–806.
M. Parviz, M. Johnson, B. Johnson, and J. Brock.
2011. Using language models and Latent Semantic
Analysis to characterise the N400m neural response.
In Proceedings of the Australasian Language Tech-
nology Association Workshop 2011, pages 38–46.
Canberra, Australia.
B. Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27:249–276.
N. J. Smith and R. Levy. in press. The effect of word
predictability on reading time is logarithmic. Cog-
nition.
A. Stolcke. 2002. SRILM – an extensible language
modeling toolkit. In Proceedings of the Interna-
tional Conference on Spoken Language Processing,
pages 901–904. Denver, Colorado.
C. Van Petten and B. J. Luka. 2012. Prediction during
language comprehension: benefits, costs, and ERP
components. International Journal of Psychophysi-
ology, 83:176–190.
</reference>
<page confidence="0.999217">
883
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.810517">
<title confidence="0.974319">Word surprisal predicts N400 amplitude during reading</title>
<author confidence="0.994271">L Leun J Giulia Gabriella</author>
<email confidence="0.920155">l.otten,g.galli,</email>
<affiliation confidence="0.981499333333333">for Language Studies, Radboud University of Cognitive, Perceptual and Brain Sciences, University College of Cognitive Neuroscience, University College London</affiliation>
<abstract confidence="0.99739905">We investigated the effect of word surprisal on the EEG signal during sentence reading. On each word of 205 experimental sentences, surprisal was estimated by three types of language model: Markov models, probabilistic phrasestructure grammars, and recurrent neural networks. Four event-related potential components were extracted from the EEG of 24 readers of the same sentences. Surprisal estimates under each model type formed a significant predictor of the amplitude of the N400 component only, with more surprising words resulting in more negative N400s. This effect was mostly due to content words. These findings provide support for surprisal as a generally applicable measure of processing difficulty during language comprehension.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Y Benjamini</author>
<author>Y Hochberg</author>
</authors>
<title>Controlling the false discovery rate: a practical and powerful approach to multiple testing.</title>
<date>1995</date>
<journal>Journal of the Royal Statistical Society B,</journal>
<pages>57--289</pages>
<contexts>
<context position="13418" citStr="Benjamini and Hochberg (1995)" startWordPosition="2106" endWordPosition="2109"> three PSG models trained on the smallest amounts of data. This shows that the models did not suffer from overfitting. 880 Figure 1: Fit to surprisal of ERP amplitude (for ELAN, P200, N400, and PNP components) as a function of average log P(wt|w1, ... , wt−1). Each plotted point corresponds to predictions by one of the trained models. Dotted lines indicate x2 = ±3.84, beyond which effects are statistically significant (p &lt; .05) if no correction for multiple comparisons is applied. The dashed line indicates the level below which effects are significant after applying the correction proposed by Benjamini and Hochberg (1995), on each ERP component separately because of our prior expectation that effects would occur mostly (if not exclusively) on the N400 component. ELAN P200 N400 PNP −6.5 −6 −5.5 −5 av. log P(wt|w1,...,wt−1) −6.5 −6 −5.5 −5 av. log P(wt|w1,...,wt−1) −6.5 −6 −5.5 −5 av. log P(wt|w1,...,wt−1) −6.5 −6 −5.5 −5 av. log P(wt|w1,...,wt−1) 10 0 −10 −20 10 10 RNN PSG n−gram −10 −20 0 0 fit of surprisal (±x2) −10 −20 10 0 −10 −20 Model n-gram RII PSG n-gram x2 = 1.34 x2 = 1.66 p &gt; .2 p &gt; .1 RII x2 = 6.52 x2 = 4.78 p &lt; .02 p &lt; .05 PSG x2 = 4.20 x2 = 2.14 p &lt; .05 p &gt; .1 Table 2: Pairwise comparisons between </context>
<context position="15405" citStr="Benjamini and Hochberg, 1995" startWordPosition="2446" endWordPosition="2449">fter correction for multiple comparisons. 6 Discussion We demonstrated a clear effect of word surprisal, as estimated by different language models, on the EEG signal: The larger a (content) word’s surprisal value, the more negative the resulting N400. Figure 2: Fit to surprisal of N400 amplitude, for content words (left) and function words (right). Dotted lines indicate x2 = ±3.84, beyond which effects are statistically significant (p &lt; .05) without correcting for multiple comparisons. Dashed lines indicates the levels beyond which effects are significant after multiple-comparison correction (Benjamini and Hochberg, 1995). The N400 component is generally viewed as indicative of lexical rather than syntactic processing (Kaan, 2007), which may explain why surprisal under the PSG model did not have any significant explanatory value over and above RNN-based surprisal. The relatively weak performance of our Markov models is most likely due to their strict (and cognitively unrealistic) limit on the size of the prior context upon which word-probability estimates are conditioned. Unlike the ELAN, P200, and PNP components, the N400 is known to be sensitive to the cloze probability of content words. The fact that surpri</context>
</contexts>
<marker>Benjamini, Hochberg, 1995</marker>
<rawString>Y. Benjamini and Y. Hochberg. 1995. Controlling the false discovery rate: a practical and powerful approach to multiple testing. Journal of the Royal Statistical Society B, 57:289–300.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S F Chen</author>
<author>J Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling. Computer Speech and Language,</title>
<date>1999</date>
<pages>13--359</pages>
<contexts>
<context position="4576" citStr="Chen and Goodman, 1999" startWordPosition="684" endWordPosition="687">ere selected from the BNC. Next, all sentences were extracted that contained only those words. This resulted in a training corpus of 1.06 million sentences (12.6 million word tokens). Each trained model estimated a surprisal value for each word of the 205 sentences (1931 word tokens) for which eye-tracking data are available in the UCL corpus of reading times (Frank et al., in press). These sentences, which were selected from three unpublished novels, only contained words from the 10,000 high-frequency word list. 2.2 Markov models Markov models were trained with modified Kneser-Ney smoothing (Chen and Goodman, 1999) as implemented in SRILM (Stolcke, 2002). Model order was varied: n = 2, 3, 4. No unigram model was computed because word frequency was factored out during data analysis (see Section 4.2). 2.3 Recurrent neural networks The RNN model architecture has been thoroughly described elsewhere (Fernandez Monsalve et al., 2012; Frank, in press) so it is not discussed here. The only difference with previous versions was that the current RNN was trained on a substantially larger data set with more word types. A range of RNN models was obtained by training on nine increasingly large subsets of the BNC data</context>
</contexts>
<marker>Chen, Goodman, 1999</marker>
<rawString>S. F. Chen and J. Goodman. 1999. An empirical study of smoothing techniques for language modeling. Computer Speech and Language, 13:359–394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Dambacher</author>
<author>R Kliegl</author>
<author>M Hofmann</author>
<author>A M Jacobs</author>
</authors>
<title>Frequency and predictability effect on event-related potentials during reading.</title>
<date>2006</date>
<journal>Brain Research,</journal>
<pages>1084--89</pages>
<contexts>
<context position="1964" citStr="Dambacher et al. (2006)" startWordPosition="290" endWordPosition="293">ic variation in response to the appearance of each word. The different components that can be observed in this signal are known as eventrelated potentials (ERPs). Probably the most reliably observed (and most studied) of these components is a negative-going deflection at centroparietal electrodes that peaks at around 400 ms after word onset and is therefore referred to as the N400 component. It is well known that the N400 increases in amplitude (i.e., becomes more negative) when the word leads to comprehension difficulty. To study the general relation between word predictability and the N400, Dambacher et al. (2006) obtained subjective word-probability estimates (so-called cloze probabilities) by asking participants to predict the upcoming word at each point in a large number of sentences. A different group of subjects read these same sentences while their EEG signal was recorded. Results showed a correlation between N400 amplitude and cloze probability: Less predictable words yielded stronger N400s. We investigated whether similar results can be obtained using more objective, model-based word probabilities. For each word in a collection of English sentences, estimates of its surprisal (i.e., its negativ</context>
<context position="8186" citStr="Dambacher et al. (2006)" startWordPosition="1266" endWordPosition="1269">0, N400, and a post-N400 positivity (PNP). Table 1 lists the corresponding time windows and approximate electrode sites.2 For each component, the average electrode potential over the corresponding time window and electrodes was computed. These average ERP amplitudes served as the four dependent variables for data analysis. The ELAN component is generally thought of as indicative of difficulty with constructing syntactic phrase structure (Friederici et al., 1999; Gunter et al., 1999; Neville et al., 1991). Hence, if any of the model types predicts ELAN size, we would expect this to be the PSG. Dambacher et al. (2006) found effects of word frequency or length (which are strongly correlated 2The P600 component (Osterhout and Holcomb, 1992) was not included because the shortest interval between consecutive word onsets was only 600 ms. 879 Component Time window Location ELAN 125–175 ms left anterior P200 140–200 ms frontocentral N400 300–500 ms centroparietal PNP 400–600 ms frontopolar Table 1: Investigated ERP components, their time windows, and approximate scalp locations. and therefore difficult to tease apart) on the P200 amplitude. Since we factor out these two lexical factors in the analysis, we expect </context>
<context position="14347" citStr="Dambacher et al. (2006)" startWordPosition="2283" endWordPosition="2286">v. log P(wt|w1,...,wt−1) 10 0 −10 −20 10 10 RNN PSG n−gram −10 −20 0 0 fit of surprisal (±x2) −10 −20 10 0 −10 −20 Model n-gram RII PSG n-gram x2 = 1.34 x2 = 1.66 p &gt; .2 p &gt; .1 RII x2 = 6.52 x2 = 4.78 p &lt; .02 p &lt; .05 PSG x2 = 4.20 x2 = 2.14 p &lt; .05 p &gt; .1 Table 2: Pairwise comparisons between surprisal estimates by the best models of each type. Shown are the results of likelihood-ratio tests for the effect of one set of surprisal estimates (rows) over and above the other (columns). 5.3 Comparing word classes N400 effects are nearly exclusively investigated on content (i.e., open-class) words. Dambacher et al. (2006), too, investigated the relation between ERP amplitudes and cloze probabilities on content words only. When running separate analyses on content and function words (constituting 53.2% and 46.8% of the data, respectively), we found that the N400 effect of Figure 1 is nearly fully driven by content words (see Figure 2). None of the models’ surprisal estimates formed a significant predictor of N400 amplitude on function words, after correction for multiple comparisons. 6 Discussion We demonstrated a clear effect of word surprisal, as estimated by different language models, on the EEG signal: The </context>
</contexts>
<marker>Dambacher, Kliegl, Hofmann, Jacobs, 2006</marker>
<rawString>M. Dambacher, R. Kliegl, M. Hofmann, and A. M. Jacobs. 2006. Frequency and predictability effect on event-related potentials during reading. Brain Research, 1084:89–103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Dubey</author>
<author>F Keller</author>
<author>P Sturt</author>
</authors>
<title>A model of discourse predictions in human sentence processing.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>304--312</pages>
<publisher>Association for</publisher>
<institution>Computational Linguistics.</institution>
<location>Edinburgh, UK:</location>
<contexts>
<context position="16651" citStr="Dubey et al., 2011" startWordPosition="2647" endWordPosition="2650">e N400 only, therefore suggests that subjective predictability scores and model-based surprisal estimates form operacontent words function words −8.5 −8 −7.5 −7 −6.5 −4.5 −4 −3.5 av. log P(wt|w1,...,wt−1) av. log P(wt|w1,...,wt−1) fit of surprisal (±x2) −10 −20 0 −10 −20 0 RNN PSG n−gram 881 tionalisations of one and the same underlying cognitive factor. Needless to say, our statistical models fail to capture many information sources, such as semantics and discourse, that do affect cloze probabilities. However, it is possible in principle to integrate these into probabilistic language models (Dubey et al., 2011; Mitchell et al., 2010). To the best of our knowledge, only one other published study relates language model predictions to the N400: Parviz et al. (2011) found that surprisal estimates (corrected for word frequency) from an n = 4 Markov model predicted N400 size as measured by magnetoencephalography (rather than EEG). Although their PSG-based surprisals did not correlate with N400 size, a related measure derived from the PSG –lexical entropy– did. However, Parviz et al. (2011) only looked at effects on the sentence-final content word of items constructed for a speech perception experiment (K</context>
</contexts>
<marker>Dubey, Keller, Sturt, 2011</marker>
<rawString>A. Dubey, F. Keller, and P. Sturt. 2011. A model of discourse predictions in human sentence processing. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 304–312. Edinburgh, UK: Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Fernandez Monsalve</author>
<author>S L Frank</author>
<author>G Vigliocco</author>
</authors>
<title>Lexical surprisal as a general predictor of reading time.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>398--408</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Avignon, France:</location>
<contexts>
<context position="3403" citStr="Monsalve et al., 2012" startWordPosition="503" endWordPosition="507">. Next, EEG signals of participants reading the same sentences were recorded. A comparison of word surprisal to different ERP components revealed that, indeed, N400 amplitude was predicted by surprisal values: More surprising words resulted in more negative N400s, at least for content words. 2 Language models A range of models of each type was trained, allowing to investigate whether models that capture the language statistics more accurately also yield better predictions of ERP size. Such a relation is generally found in studies that use word-reading time as the dependent variable (Fernandez Monsalve et al., 2012; Frank and Bod, 2011; Frank and Thompson, 2012), providing additional support that these psychological data are indeed explained by the surprisal values and not by some confounding variable. 2.1 Corpus data All models were trained on sentences from the written texts in the British National Corpus (BNC). First, the 10,000 word types with highest 878 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 878–883, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics frequency were selected from the BNC. Next, all sentences wer</context>
<context position="4894" citStr="Monsalve et al., 2012" startWordPosition="734" endWordPosition="737">ilable in the UCL corpus of reading times (Frank et al., in press). These sentences, which were selected from three unpublished novels, only contained words from the 10,000 high-frequency word list. 2.2 Markov models Markov models were trained with modified Kneser-Ney smoothing (Chen and Goodman, 1999) as implemented in SRILM (Stolcke, 2002). Model order was varied: n = 2, 3, 4. No unigram model was computed because word frequency was factored out during data analysis (see Section 4.2). 2.3 Recurrent neural networks The RNN model architecture has been thoroughly described elsewhere (Fernandez Monsalve et al., 2012; Frank, in press) so it is not discussed here. The only difference with previous versions was that the current RNN was trained on a substantially larger data set with more word types. A range of RNN models was obtained by training on nine increasingly large subsets of the BNC data, comprising 2K, 5K, 10K, 20K, 50K, 100K, 200K, 400K, and all 1.06M sentences. In addition, the network was trained on the full set twice, making a total of ten instantiations of the RNN model. 2.4 Phrase-structure grammars To prepare data for PSG training, the selected BNC sentences were parsed by the Stanford parse</context>
<context position="18419" citStr="Monsalve et al., 2012" startWordPosition="2932" endWordPosition="2935">surprisal itself, but to an unknown confounding variable that was not included in the regression analysis. However, this seems unlikely because of two additional findings that only follow naturally if surprisal is indeed the relevant predictor: Significant results only appeared where they were most expected a priori (i.e., on N400 but not on other components) and there was a nearly monotonic relation between the models’ word-prediction accuracy and their ability to account for N400 size. 7 Conclusion Although word surprisal has often been shown to be predictive of word-reading time (Fernandez Monsalve et al., 2012; Frank and Thompson, 2012; Smith and Levy, in press), a general effect on the EEG signal has not before been demonstrated. Hence, these results provide additional evidence in support of surprisal as a reliable measure of cognitive processing difficulty during sentence comprehension (Hale, 2001; Levy, 2008). Acknowledgments The research presented here was funded by the European Union Seventh Framework Programme (FP7/2007-2013) under grant number 253803. The authors acknowledge the use of the UCL Legion High Performance Computing Facility, and associated support services, in the completion of t</context>
</contexts>
<marker>Monsalve, Frank, Vigliocco, 2012</marker>
<rawString>I. Fernandez Monsalve, S. L. Frank, and G. Vigliocco. 2012. Lexical surprisal as a general predictor of reading time. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 398–408. Avignon, France: Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S L Frank</author>
<author>R Bod</author>
</authors>
<title>Insensitivity of the human sentence-processing system to hierarchical structure.</title>
<date>2011</date>
<pages>22--829</pages>
<publisher>Psychological Science,</publisher>
<contexts>
<context position="3424" citStr="Frank and Bod, 2011" startWordPosition="508" endWordPosition="511">participants reading the same sentences were recorded. A comparison of word surprisal to different ERP components revealed that, indeed, N400 amplitude was predicted by surprisal values: More surprising words resulted in more negative N400s, at least for content words. 2 Language models A range of models of each type was trained, allowing to investigate whether models that capture the language statistics more accurately also yield better predictions of ERP size. Such a relation is generally found in studies that use word-reading time as the dependent variable (Fernandez Monsalve et al., 2012; Frank and Bod, 2011; Frank and Thompson, 2012), providing additional support that these psychological data are indeed explained by the surprisal values and not by some confounding variable. 2.1 Corpus data All models were trained on sentences from the written texts in the British National Corpus (BNC). First, the 10,000 word types with highest 878 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 878–883, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics frequency were selected from the BNC. Next, all sentences were extracted that cont</context>
<context position="12485" citStr="Frank and Bod (2011)" startWordPosition="1957" endWordPosition="1960">e N400 were highly significant. 5.2 Model comparison Table 2 shows results of pairwise comparisons between the best models of each type (i.e., those whose surprisal estimates fit the N400 data best). Clearly, RNN-based surprisal explains variance over and above each of the other two models whereas neither the n-gram nor the PSG model outperforms the RNN. Moreover, the RNN’s surprisals explain a marginally significant (x2 = 3.47; p &lt; .07) amount of variance over and above the combined PSG and n-gram surprisals. changes over the course of the sentence or experiment. 4This definition equals what Frank and Bod (2011) call ‘psychological accuracy’ in an analysis of reading times. 5This measure, which Frank and Bod (2011) call ‘linguistic accuracy’, equals the negative logarithm of the model’s perplexity. Increasing the amount of training data (or the value of n) resulted in higher linguistic accuracy, except for the three PSG models trained on the smallest amounts of data. This shows that the models did not suffer from overfitting. 880 Figure 1: Fit to surprisal of ERP amplitude (for ELAN, P200, N400, and PNP components) as a function of average log P(wt|w1, ... , wt−1). Each plotted point corresponds to p</context>
</contexts>
<marker>Frank, Bod, 2011</marker>
<rawString>S. L. Frank and R. Bod. 2011. Insensitivity of the human sentence-processing system to hierarchical structure. Psychological Science, 22:829–834.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S L Frank</author>
<author>R L Thompson</author>
</authors>
<title>Early effects of word surprisal on pupil size during reading.</title>
<date>2012</date>
<booktitle>In Proceedings of the 34th Annual Conference of the Cognitive Science Society,</booktitle>
<pages>1554--1559</pages>
<publisher>Cognitive Science Society.</publisher>
<location>Austin, TX:</location>
<contexts>
<context position="3451" citStr="Frank and Thompson, 2012" startWordPosition="512" endWordPosition="515">the same sentences were recorded. A comparison of word surprisal to different ERP components revealed that, indeed, N400 amplitude was predicted by surprisal values: More surprising words resulted in more negative N400s, at least for content words. 2 Language models A range of models of each type was trained, allowing to investigate whether models that capture the language statistics more accurately also yield better predictions of ERP size. Such a relation is generally found in studies that use word-reading time as the dependent variable (Fernandez Monsalve et al., 2012; Frank and Bod, 2011; Frank and Thompson, 2012), providing additional support that these psychological data are indeed explained by the surprisal values and not by some confounding variable. 2.1 Corpus data All models were trained on sentences from the written texts in the British National Corpus (BNC). First, the 10,000 word types with highest 878 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 878–883, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics frequency were selected from the BNC. Next, all sentences were extracted that contained only those words. Thi</context>
</contexts>
<marker>Frank, Thompson, 2012</marker>
<rawString>S. L. Frank and R. L. Thompson. 2012. Early effects of word surprisal on pupil size during reading. In Proceedings of the 34th Annual Conference of the Cognitive Science Society, pages 1554–1559. Austin, TX: Cognitive Science Society.</rawString>
</citation>
<citation valid="false">
<authors>
<author>S L Frank</author>
<author>I Fernandez Monsalve</author>
<author>R L Thompson</author>
<author>G Vigliocco</author>
</authors>
<title>in press. Reading time data for evaluating broad-coverage models of English sentence processing. Behavior Research Methods.</title>
<marker>Frank, Monsalve, Thompson, Vigliocco, </marker>
<rawString>S. L. Frank, I. Fernandez Monsalve, R. L. Thompson, and G. Vigliocco. in press. Reading time data for evaluating broad-coverage models of English sentence processing. Behavior Research Methods.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S L Frank</author>
</authors>
<title>Surprisal-based comparison between a symbolic and a connectionist model of sentence processing.</title>
<date>2009</date>
<booktitle>Proceedings of the 31st Annual Conference of the Cognitive Science Society,</booktitle>
<pages>1139--1144</pages>
<editor>In N. A. Taatgen and H. van Rijn, editors,</editor>
<publisher>Cognitive Science Society.</publisher>
<location>Austin, TX:</location>
<contexts>
<context position="5985" citStr="Frank, 2009" startWordPosition="917" endWordPosition="918"> Phrase-structure grammars To prepare data for PSG training, the selected BNC sentences were parsed by the Stanford parser (Klein and Manning, 2003). The resulting treebank was divided into nine increasingly large subsets, equal to those used for RNN training.1 Grammars were induced from these subsets using the algorithm by Roark (2001) with its standard settings. Next, surprisal values on the experimental sentences were generated by Roark’s incremental parser. Since increasing the parser’s beam width has been shown to improve both word-probability estimates and the fit to word-reading times (Frank, 2009), the parser’s ‘base beam threshold’ parameter was reduced to 10−20. 1Because not all experimental sentences could be parsed when the treebank comprised only 2K sentences, 1K sentences were added to the smallest subset. 3 EEG data collection Twenty-four healthy, adult volunteers from the UCL Psychology subject pool took part in the reading study. Their EEG was recorded continuously from 32 channels during the presentation of 5 practice sentences and the 205 experimental items. Participants were asked to minimise blinks, eye movements, and head movements during sentence presentation. Each sente</context>
</contexts>
<marker>Frank, 2009</marker>
<rawString>S. L. Frank. 2009. Surprisal-based comparison between a symbolic and a connectionist model of sentence processing. In N. A. Taatgen and H. van Rijn, editors, Proceedings of the 31st Annual Conference of the Cognitive Science Society, pages 1139–1144. Austin, TX: Cognitive Science Society.</rawString>
</citation>
<citation valid="false">
<authors>
<author>S L Frank</author>
</authors>
<title>in press. Uncertainty reduction as a measure of cognitive processing load in sentence comprehension. Topics in Cognitive Science.</title>
<marker>Frank, </marker>
<rawString>S. L. Frank. in press. Uncertainty reduction as a measure of cognitive processing load in sentence comprehension. Topics in Cognitive Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A D Friederici</author>
<author>K Steinhauer</author>
<author>S Frisch</author>
</authors>
<title>Lexical integration: sequential effects of syntactic and semantic information.</title>
<date>1999</date>
<journal>Memory &amp; Cognition,</journal>
<pages>27--438</pages>
<contexts>
<context position="8028" citStr="Friederici et al., 1999" startWordPosition="1236" endWordPosition="1239">4.1 ERP components Four ERP components of interest were identified from the literature on EEG and sentence reading: Early Left Anterior Negativity (ELAN), P200, N400, and a post-N400 positivity (PNP). Table 1 lists the corresponding time windows and approximate electrode sites.2 For each component, the average electrode potential over the corresponding time window and electrodes was computed. These average ERP amplitudes served as the four dependent variables for data analysis. The ELAN component is generally thought of as indicative of difficulty with constructing syntactic phrase structure (Friederici et al., 1999; Gunter et al., 1999; Neville et al., 1991). Hence, if any of the model types predicts ELAN size, we would expect this to be the PSG. Dambacher et al. (2006) found effects of word frequency or length (which are strongly correlated 2The P600 component (Osterhout and Holcomb, 1992) was not included because the shortest interval between consecutive word onsets was only 600 ms. 879 Component Time window Location ELAN 125–175 ms left anterior P200 140–200 ms frontocentral N400 300–500 ms centroparietal PNP 400–600 ms frontopolar Table 1: Investigated ERP components, their time windows, and approxi</context>
</contexts>
<marker>Friederici, Steinhauer, Frisch, 1999</marker>
<rawString>A. D. Friederici, K. Steinhauer, and S. Frisch. 1999. Lexical integration: sequential effects of syntactic and semantic information. Memory &amp; Cognition, 27:438–453.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T C Gunter</author>
<author>A D Friederici</author>
<author>A Hahne</author>
</authors>
<title>Brain responses during sentence reading: Visual input affects central processes.</title>
<date>1999</date>
<tech>NeuroReport,</tech>
<pages>10--3175</pages>
<contexts>
<context position="8049" citStr="Gunter et al., 1999" startWordPosition="1240" endWordPosition="1243">RP components of interest were identified from the literature on EEG and sentence reading: Early Left Anterior Negativity (ELAN), P200, N400, and a post-N400 positivity (PNP). Table 1 lists the corresponding time windows and approximate electrode sites.2 For each component, the average electrode potential over the corresponding time window and electrodes was computed. These average ERP amplitudes served as the four dependent variables for data analysis. The ELAN component is generally thought of as indicative of difficulty with constructing syntactic phrase structure (Friederici et al., 1999; Gunter et al., 1999; Neville et al., 1991). Hence, if any of the model types predicts ELAN size, we would expect this to be the PSG. Dambacher et al. (2006) found effects of word frequency or length (which are strongly correlated 2The P600 component (Osterhout and Holcomb, 1992) was not included because the shortest interval between consecutive word onsets was only 600 ms. 879 Component Time window Location ELAN 125–175 ms left anterior P200 140–200 ms frontocentral N400 300–500 ms centroparietal PNP 400–600 ms frontopolar Table 1: Investigated ERP components, their time windows, and approximate scalp locations.</context>
</contexts>
<marker>Gunter, Friederici, Hahne, 1999</marker>
<rawString>T. C. Gunter, A. D. Friederici, and A. Hahne. 1999. Brain responses during sentence reading: Visual input affects central processes. NeuroReport, 10:3175–3178.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J T Hale</author>
</authors>
<title>A probabilistic Early parser as a psycholinguistic model.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2nd Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<volume>2</volume>
<pages>159--166</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Pittsburgh, PA:</location>
<marker>Hale, 2001</marker>
<rawString>J. T. Hale. 2001. A probabilistic Early parser as a psycholinguistic model. In Proceedings of the 2nd Conference of the North American Chapter of the Association for Computational Linguistics, volume 2, pages 159–166. Pittsburgh, PA: Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Kaan</author>
</authors>
<title>Event-related potentials and language processing: a brief overview. Language and Linguistics Compass,</title>
<date>2007</date>
<pages>1--571</pages>
<contexts>
<context position="15516" citStr="Kaan, 2007" startWordPosition="2465" endWordPosition="2466">t language models, on the EEG signal: The larger a (content) word’s surprisal value, the more negative the resulting N400. Figure 2: Fit to surprisal of N400 amplitude, for content words (left) and function words (right). Dotted lines indicate x2 = ±3.84, beyond which effects are statistically significant (p &lt; .05) without correcting for multiple comparisons. Dashed lines indicates the levels beyond which effects are significant after multiple-comparison correction (Benjamini and Hochberg, 1995). The N400 component is generally viewed as indicative of lexical rather than syntactic processing (Kaan, 2007), which may explain why surprisal under the PSG model did not have any significant explanatory value over and above RNN-based surprisal. The relatively weak performance of our Markov models is most likely due to their strict (and cognitively unrealistic) limit on the size of the prior context upon which word-probability estimates are conditioned. Unlike the ELAN, P200, and PNP components, the N400 is known to be sensitive to the cloze probability of content words. The fact that surprisal effects were found on the N400 only, therefore suggests that subjective predictability scores and model-bas</context>
</contexts>
<marker>Kaan, 2007</marker>
<rawString>E. Kaan. 2007. Event-related potentials and language processing: a brief overview. Language and Linguistics Compass, 1:571–591.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D N Kalikow</author>
<author>K N Stevens</author>
<author>L L Elliott</author>
</authors>
<title>Development of a test of speech intelligibility in noise using sentence materials with controlled word predictability.</title>
<date>1977</date>
<journal>Journal of the Acoustical Society of America,</journal>
<pages>61--1337</pages>
<contexts>
<context position="17271" citStr="Kalikow et al., 1977" startWordPosition="2747" endWordPosition="2751">1; Mitchell et al., 2010). To the best of our knowledge, only one other published study relates language model predictions to the N400: Parviz et al. (2011) found that surprisal estimates (corrected for word frequency) from an n = 4 Markov model predicted N400 size as measured by magnetoencephalography (rather than EEG). Although their PSG-based surprisals did not correlate with N400 size, a related measure derived from the PSG –lexical entropy– did. However, Parviz et al. (2011) only looked at effects on the sentence-final content word of items constructed for a speech perception experiment (Kalikow et al., 1977), rather than investigating surprisal’s general predictive value across words of naturally occurring sentences, as we did here. Our experimental design was parametric rather than factorial, which allowed us to study the effect of surprisal over a sample of English sentences rather than carefully manipulating surprisal while holding other factors constant. This has the advantage that our findings are likely to generalise to other sentence stimuli, but it can also raise a possible concern: The N400 effect may not be due to surprisal itself, but to an unknown confounding variable that was not inc</context>
</contexts>
<marker>Kalikow, Stevens, Elliott, 1977</marker>
<rawString>D. N. Kalikow, K. N. Stevens, and L. L. Elliott. 1977. Development of a test of speech intelligibility in noise using sentence materials with controlled word predictability. Journal of the Acoustical Society of America, 61:1337–1351.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Meeting of the Association for Computational Linguistics,</booktitle>
<pages>423--430</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sapporo, Japan:</location>
<contexts>
<context position="5521" citStr="Klein and Manning, 2003" startWordPosition="843" endWordPosition="846">rank, in press) so it is not discussed here. The only difference with previous versions was that the current RNN was trained on a substantially larger data set with more word types. A range of RNN models was obtained by training on nine increasingly large subsets of the BNC data, comprising 2K, 5K, 10K, 20K, 50K, 100K, 200K, 400K, and all 1.06M sentences. In addition, the network was trained on the full set twice, making a total of ten instantiations of the RNN model. 2.4 Phrase-structure grammars To prepare data for PSG training, the selected BNC sentences were parsed by the Stanford parser (Klein and Manning, 2003). The resulting treebank was divided into nine increasingly large subsets, equal to those used for RNN training.1 Grammars were induced from these subsets using the algorithm by Roark (2001) with its standard settings. Next, surprisal values on the experimental sentences were generated by Roark’s incremental parser. Since increasing the parser’s beam width has been shown to improve both word-probability estimates and the fit to word-reading times (Frank, 2009), the parser’s ‘base beam threshold’ parameter was reduced to 10−20. 1Because not all experimental sentences could be parsed when the tr</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>D. Klein and C. D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Meeting of the Association for Computational Linguistics, pages 423–430. Sapporo, Japan: Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kutas</author>
<author>S A Hillyard</author>
</authors>
<title>Brain potentials during reading reflect word expectancy and semantic association.</title>
<date>1984</date>
<journal>Nature,</journal>
<pages>307--161</pages>
<contexts>
<context position="9067" citStr="Kutas and Hillyard, 1984" startWordPosition="1407" endWordPosition="1410">AN 125–175 ms left anterior P200 140–200 ms frontocentral N400 300–500 ms centroparietal PNP 400–600 ms frontopolar Table 1: Investigated ERP components, their time windows, and approximate scalp locations. and therefore difficult to tease apart) on the P200 amplitude. Since we factor out these two lexical factors in the analysis, we expect no additional effect of surprisal on P200. If any of the components is sensitive to word surprisal, this is most likely to be the N400 as many studies have already shown that N400 amplitude depends on subjective word predictability (Dambacher et al., 2006; Kutas and Hillyard, 1984; Moreno et al., 2002). Whether an effect will appear on the PNP is more doubtful. Van Petten and Luka (2012) argue that word expectations that are confirmed result in reduced N400 size, whereas expectations that are disconfirmed increase the PNP. However, in a probabilistic setting, expectations are not all-or-nothing so there is no strict distinction between confirmation and disconfirmation. Nevertheless, surprisal effects on PNP may occur. Since the PNP has received relatively little attention, the component may not be such a reliable index of comprehension difficulty as the N400 has proven</context>
</contexts>
<marker>Kutas, Hillyard, 1984</marker>
<rawString>M. Kutas and S. A. Hillyard. 1984. Brain potentials during reading reflect word expectancy and semantic association. Nature, 307:161–163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Levy</author>
</authors>
<title>Expectation-based syntactic comprehension.</title>
<date>2008</date>
<journal>Cognition,</journal>
<pages>106--1126</pages>
<marker>Levy, 2008</marker>
<rawString>R. Levy. 2008. Expectation-based syntactic comprehension. Cognition, 106:1126–1177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Mitchell</author>
<author>M Lapata</author>
<author>V Demberg</author>
<author>F Keller</author>
</authors>
<title>Syntactic and semantic factors in processing difficulty: An integrated measure.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>196--206</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden:</location>
<contexts>
<context position="16675" citStr="Mitchell et al., 2010" startWordPosition="2651" endWordPosition="2654">re suggests that subjective predictability scores and model-based surprisal estimates form operacontent words function words −8.5 −8 −7.5 −7 −6.5 −4.5 −4 −3.5 av. log P(wt|w1,...,wt−1) av. log P(wt|w1,...,wt−1) fit of surprisal (±x2) −10 −20 0 −10 −20 0 RNN PSG n−gram 881 tionalisations of one and the same underlying cognitive factor. Needless to say, our statistical models fail to capture many information sources, such as semantics and discourse, that do affect cloze probabilities. However, it is possible in principle to integrate these into probabilistic language models (Dubey et al., 2011; Mitchell et al., 2010). To the best of our knowledge, only one other published study relates language model predictions to the N400: Parviz et al. (2011) found that surprisal estimates (corrected for word frequency) from an n = 4 Markov model predicted N400 size as measured by magnetoencephalography (rather than EEG). Although their PSG-based surprisals did not correlate with N400 size, a related measure derived from the PSG –lexical entropy– did. However, Parviz et al. (2011) only looked at effects on the sentence-final content word of items constructed for a speech perception experiment (Kalikow et al., 1977), ra</context>
</contexts>
<marker>Mitchell, Lapata, Demberg, Keller, 2010</marker>
<rawString>J. Mitchell, M. Lapata, V. Demberg, and F. Keller. 2010. Syntactic and semantic factors in processing difficulty: An integrated measure. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 196–206. Uppsala, Sweden: Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Moreno</author>
<author>K D Federmeier</author>
<author>M Kutas</author>
</authors>
<title>Switching languages, switching palabras (words): an electrophysiological study of code switching.</title>
<date>2002</date>
<journal>Brain and Language,</journal>
<pages>80--188</pages>
<contexts>
<context position="9089" citStr="Moreno et al., 2002" startWordPosition="1411" endWordPosition="1414">r P200 140–200 ms frontocentral N400 300–500 ms centroparietal PNP 400–600 ms frontopolar Table 1: Investigated ERP components, their time windows, and approximate scalp locations. and therefore difficult to tease apart) on the P200 amplitude. Since we factor out these two lexical factors in the analysis, we expect no additional effect of surprisal on P200. If any of the components is sensitive to word surprisal, this is most likely to be the N400 as many studies have already shown that N400 amplitude depends on subjective word predictability (Dambacher et al., 2006; Kutas and Hillyard, 1984; Moreno et al., 2002). Whether an effect will appear on the PNP is more doubtful. Van Petten and Luka (2012) argue that word expectations that are confirmed result in reduced N400 size, whereas expectations that are disconfirmed increase the PNP. However, in a probabilistic setting, expectations are not all-or-nothing so there is no strict distinction between confirmation and disconfirmation. Nevertheless, surprisal effects on PNP may occur. Since the PNP has received relatively little attention, the component may not be such a reliable index of comprehension difficulty as the N400 has proven to be. 4.2 Regression</context>
</contexts>
<marker>Moreno, Federmeier, Kutas, 2002</marker>
<rawString>E. M. Moreno, K. D. Federmeier, and M. Kutas. 2002. Switching languages, switching palabras (words): an electrophysiological study of code switching. Brain and Language, 80:188–207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Neville</author>
<author>J L Nicol</author>
<author>A Barss</author>
<author>K I Forster</author>
<author>M F Garrett</author>
</authors>
<title>Syntactically based sentence processing classes: evidence from event-related brain potentials.</title>
<date>1991</date>
<journal>Journal of Cognitive Neuroscience,</journal>
<pages>3--151</pages>
<contexts>
<context position="8072" citStr="Neville et al., 1991" startWordPosition="1244" endWordPosition="1247">rest were identified from the literature on EEG and sentence reading: Early Left Anterior Negativity (ELAN), P200, N400, and a post-N400 positivity (PNP). Table 1 lists the corresponding time windows and approximate electrode sites.2 For each component, the average electrode potential over the corresponding time window and electrodes was computed. These average ERP amplitudes served as the four dependent variables for data analysis. The ELAN component is generally thought of as indicative of difficulty with constructing syntactic phrase structure (Friederici et al., 1999; Gunter et al., 1999; Neville et al., 1991). Hence, if any of the model types predicts ELAN size, we would expect this to be the PSG. Dambacher et al. (2006) found effects of word frequency or length (which are strongly correlated 2The P600 component (Osterhout and Holcomb, 1992) was not included because the shortest interval between consecutive word onsets was only 600 ms. 879 Component Time window Location ELAN 125–175 ms left anterior P200 140–200 ms frontocentral N400 300–500 ms centroparietal PNP 400–600 ms frontopolar Table 1: Investigated ERP components, their time windows, and approximate scalp locations. and therefore difficul</context>
</contexts>
<marker>Neville, Nicol, Barss, Forster, Garrett, 1991</marker>
<rawString>H. Neville, J. L. Nicol, A. Barss, K. I. Forster, and M. F. Garrett. 1991. Syntactically based sentence processing classes: evidence from event-related brain potentials. Journal of Cognitive Neuroscience, 3:151–165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Osterhout</author>
<author>P J Holcomb</author>
</authors>
<title>Event-related brain potentials elicited by syntactic anomaly.</title>
<date>1992</date>
<journal>Journal ofMemory and Language,</journal>
<pages>31--785</pages>
<contexts>
<context position="8309" citStr="Osterhout and Holcomb, 1992" startWordPosition="1284" endWordPosition="1287">s.2 For each component, the average electrode potential over the corresponding time window and electrodes was computed. These average ERP amplitudes served as the four dependent variables for data analysis. The ELAN component is generally thought of as indicative of difficulty with constructing syntactic phrase structure (Friederici et al., 1999; Gunter et al., 1999; Neville et al., 1991). Hence, if any of the model types predicts ELAN size, we would expect this to be the PSG. Dambacher et al. (2006) found effects of word frequency or length (which are strongly correlated 2The P600 component (Osterhout and Holcomb, 1992) was not included because the shortest interval between consecutive word onsets was only 600 ms. 879 Component Time window Location ELAN 125–175 ms left anterior P200 140–200 ms frontocentral N400 300–500 ms centroparietal PNP 400–600 ms frontopolar Table 1: Investigated ERP components, their time windows, and approximate scalp locations. and therefore difficult to tease apart) on the P200 amplitude. Since we factor out these two lexical factors in the analysis, we expect no additional effect of surprisal on P200. If any of the components is sensitive to word surprisal, this is most likely to </context>
</contexts>
<marker>Osterhout, Holcomb, 1992</marker>
<rawString>L. Osterhout and P. J. Holcomb. 1992. Event-related brain potentials elicited by syntactic anomaly. Journal ofMemory and Language, 31:785–806.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Parviz</author>
<author>M Johnson</author>
<author>B Johnson</author>
<author>J Brock</author>
</authors>
<title>Using language models and Latent Semantic Analysis to characterise the N400m neural response.</title>
<date>2011</date>
<booktitle>In Proceedings of the Australasian Language Technology Association Workshop</booktitle>
<pages>38--46</pages>
<location>Canberra, Australia.</location>
<contexts>
<context position="16806" citStr="Parviz et al. (2011)" startWordPosition="2674" endWordPosition="2677">−7.5 −7 −6.5 −4.5 −4 −3.5 av. log P(wt|w1,...,wt−1) av. log P(wt|w1,...,wt−1) fit of surprisal (±x2) −10 −20 0 −10 −20 0 RNN PSG n−gram 881 tionalisations of one and the same underlying cognitive factor. Needless to say, our statistical models fail to capture many information sources, such as semantics and discourse, that do affect cloze probabilities. However, it is possible in principle to integrate these into probabilistic language models (Dubey et al., 2011; Mitchell et al., 2010). To the best of our knowledge, only one other published study relates language model predictions to the N400: Parviz et al. (2011) found that surprisal estimates (corrected for word frequency) from an n = 4 Markov model predicted N400 size as measured by magnetoencephalography (rather than EEG). Although their PSG-based surprisals did not correlate with N400 size, a related measure derived from the PSG –lexical entropy– did. However, Parviz et al. (2011) only looked at effects on the sentence-final content word of items constructed for a speech perception experiment (Kalikow et al., 1977), rather than investigating surprisal’s general predictive value across words of naturally occurring sentences, as we did here. Our exp</context>
</contexts>
<marker>Parviz, Johnson, Johnson, Brock, 2011</marker>
<rawString>M. Parviz, M. Johnson, B. Johnson, and J. Brock. 2011. Using language models and Latent Semantic Analysis to characterise the N400m neural response. In Proceedings of the Australasian Language Technology Association Workshop 2011, pages 38–46. Canberra, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Roark</author>
</authors>
<title>Probabilistic top-down parsing and language modeling.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<pages>27--249</pages>
<contexts>
<context position="5711" citStr="Roark (2001)" startWordPosition="877" endWordPosition="878">s was obtained by training on nine increasingly large subsets of the BNC data, comprising 2K, 5K, 10K, 20K, 50K, 100K, 200K, 400K, and all 1.06M sentences. In addition, the network was trained on the full set twice, making a total of ten instantiations of the RNN model. 2.4 Phrase-structure grammars To prepare data for PSG training, the selected BNC sentences were parsed by the Stanford parser (Klein and Manning, 2003). The resulting treebank was divided into nine increasingly large subsets, equal to those used for RNN training.1 Grammars were induced from these subsets using the algorithm by Roark (2001) with its standard settings. Next, surprisal values on the experimental sentences were generated by Roark’s incremental parser. Since increasing the parser’s beam width has been shown to improve both word-probability estimates and the fit to word-reading times (Frank, 2009), the parser’s ‘base beam threshold’ parameter was reduced to 10−20. 1Because not all experimental sentences could be parsed when the treebank comprised only 2K sentences, 1K sentences were added to the smallest subset. 3 EEG data collection Twenty-four healthy, adult volunteers from the UCL Psychology subject pool took part</context>
</contexts>
<marker>Roark, 2001</marker>
<rawString>B. Roark. 2001. Probabilistic top-down parsing and language modeling. Computational Linguistics, 27:249–276.</rawString>
</citation>
<citation valid="false">
<authors>
<author>N J Smith</author>
<author>R Levy</author>
</authors>
<title>in press. The effect of word predictability on reading time is logarithmic.</title>
<journal>Cognition.</journal>
<marker>Smith, Levy, </marker>
<rawString>N. J. Smith and R. Levy. in press. The effect of word predictability on reading time is logarithmic. Cognition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM – an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing,</booktitle>
<pages>901--904</pages>
<location>Denver, Colorado.</location>
<contexts>
<context position="4616" citStr="Stolcke, 2002" startWordPosition="692" endWordPosition="693">re extracted that contained only those words. This resulted in a training corpus of 1.06 million sentences (12.6 million word tokens). Each trained model estimated a surprisal value for each word of the 205 sentences (1931 word tokens) for which eye-tracking data are available in the UCL corpus of reading times (Frank et al., in press). These sentences, which were selected from three unpublished novels, only contained words from the 10,000 high-frequency word list. 2.2 Markov models Markov models were trained with modified Kneser-Ney smoothing (Chen and Goodman, 1999) as implemented in SRILM (Stolcke, 2002). Model order was varied: n = 2, 3, 4. No unigram model was computed because word frequency was factored out during data analysis (see Section 4.2). 2.3 Recurrent neural networks The RNN model architecture has been thoroughly described elsewhere (Fernandez Monsalve et al., 2012; Frank, in press) so it is not discussed here. The only difference with previous versions was that the current RNN was trained on a substantially larger data set with more word types. A range of RNN models was obtained by training on nine increasingly large subsets of the BNC data, comprising 2K, 5K, 10K, 20K, 50K, 100K</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. SRILM – an extensible language modeling toolkit. In Proceedings of the International Conference on Spoken Language Processing, pages 901–904. Denver, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Van Petten</author>
<author>B J Luka</author>
</authors>
<title>Prediction during language comprehension: benefits, costs, and ERP components.</title>
<date>2012</date>
<journal>International Journal of Psychophysiology,</journal>
<pages>83--176</pages>
<marker>Van Petten, Luka, 2012</marker>
<rawString>C. Van Petten and B. J. Luka. 2012. Prediction during language comprehension: benefits, costs, and ERP components. International Journal of Psychophysiology, 83:176–190.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>