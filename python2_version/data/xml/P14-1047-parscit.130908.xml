<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002795">
<title confidence="0.997039">
Single-Agent vs. Multi-Agent Techniques for Concurrent
Reinforcement Learning of Negotiation Dialogue Policies
</title>
<author confidence="0.998538">
Kallirroi Georgila, Claire Nelson, David Traum
</author>
<affiliation confidence="0.999465">
University of Southern California Institute for Creative Technologies
</affiliation>
<address confidence="0.940952">
12015 Waterfront Drive, Playa Vista, CA 90094, USA
</address>
<email confidence="0.999829">
{kgeorgila,traum}@ict.usc.edu
</email>
<sectionHeader confidence="0.994825" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999965869565217">
We use single-agent and multi-agent Rein-
forcement Learning (RL) for learning dia-
logue policies in a resource allocation ne-
gotiation scenario. Two agents learn con-
currently by interacting with each other
without any need for simulated users
(SUs) to train against or corpora to learn
from. In particular, we compare the Q-
learning, Policy Hill-Climbing (PHC) and
Win or Learn Fast Policy Hill-Climbing
(PHC-WoLF) algorithms, varying the sce-
nario complexity (state space size), the
number of training episodes, the learning
rate, and the exploration rate. Our re-
sults show that generally Q-learning fails
to converge whereas PHC and PHC-WoLF
always converge and perform similarly.
We also show that very high gradually
decreasing exploration rates are required
for convergence. We conclude that multi-
agent RL of dialogue policies is a promis-
ing alternative to using single-agent RL
and SUs or learning directly from corpora.
</bodyText>
<sectionHeader confidence="0.998883" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999820480769231">
The dialogue policy of a dialogue system decides
on which actions the system should perform given
a particular dialogue state (i.e., dialogue context).
Building a dialogue policy can be a challenging
task especially for complex applications. For this
reason, recently much attention has been drawn
to machine learning approaches to dialogue man-
agement and in particular Reinforcement Learning
(RL) of dialogue policies (Williams and Young,
2007; Rieser et al., 2011; Jurˇc´ıˇcek et al., 2012).
Typically there are three main approaches to
the problem of learning dialogue policies using
RL: (1) learn against a simulated user (SU), i.e.,
a model that simulates the behavior of a real user
(Georgila et al., 2006; Schatzmann et al., 2006);
(2) learn directly from a corpus (Henderson et al.,
2008; Li et al., 2009); or (3) learn via live interac-
tion with human users (Singh et al., 2002; Gaˇsi´c et
al., 2011; Gaˇsi´c et al., 2013).
We propose a fourth approach: concurrent
learning of the system policy and the SU policy
using multi-agent RL techniques. Both agents are
trained simultaneously and there is no need for
building a SU separately or having access to a cor-
pus.1 As we discuss below, concurrent learning
could potentially be used for learning via live in-
teraction with human users. Moreover, for negoti-
ation in particular there is one more reason in fa-
vor of concurrent learning as opposed to learning
against a SU. Unlike slot-filling domains, in nego-
tiation the behaviors of the system and the user are
symmetric. They are both negotiators, thus build-
ing a good SU is as difficult as building a good
system policy.
So far research on using RL for dialogue pol-
icy learning has focused on single-agent RL tech-
niques. Single-agent RL methods make the as-
sumption that the system learns by interacting with
a stationary environment, i.e., an environment that
does not change over time. Here the environ-
ment is the user. Generally the assumption that
users do not significantly change their behavior
over time holds for simple information providing
tasks (e.g., reserving a flight). But this is not nec-
essarily the case for other genres of dialogue, in-
cluding negotiation. Imagine a situation where a
negotiator is so uncooperative and arrogant that
the other negotiators decide to completely change
their negotiation strategy in order to punish her.
Therefore it is important to investigate RL ap-
proaches that do not make such assumptions about
the user/environment.
</bodyText>
<footnote confidence="0.940592666666667">
1Though corpora or SUs may still be useful for bootstrap-
ping the policies and encoding real user behavior (see sec-
tion 6).
</footnote>
<page confidence="0.831519">
500
</page>
<note confidence="0.8367915">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 500–510,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999362392156863">
Multi-agent RL is designed to work for non-
stationary environments. In this case the envi-
ronment of a learning agent is one or more other
agents that can also be learning at the same time.
Therefore, unlike single-agent RL, multi-agent RL
can handle changes in user behavior or in the be-
havior of other agents participating in the inter-
action, and thus potentially lead to more realis-
tic dialogue policies in complex dialogue scenar-
ios. This ability of multi-agent RL can also have
important implications for learning via live inter-
action with human users. Imagine a system that
learns to change its strategy as it realizes that a
particular user is no longer a novice user, or that a
user no longer cares about five star restaurants.
We apply multi-agent RL to a resource alloca-
tion negotiation scenario. Two agents with dif-
ferent preferences negotiate about how to share
resources. We compare Q-learning (a single-
agent RL algorithm) with two multi-agent RL al-
gorithms: Policy Hill-Climbing (PHC) and Win
or Learn Fast Policy Hill-Climbing (PHC-WoLF)
(Bowling and Veloso, 2002). We vary the scenario
complexity (i.e., the quantity of resources to be
shared and consequently the state space size), the
number of training episodes, the learning rate, and
the exploration rate.
Our research contributions are as follows: (1)
we propose concurrent learning using multi-agent
RL as a way to deal with some of the issues of cur-
rent approaches to dialogue policy learning (i.e.,
the need for SUs and corpora), which may also
potentially prove useful for learning via live inter-
action with human users; (2) we show that concur-
rent learning can address changes in user behav-
ior over time, and requires multi-agent RL tech-
niques and variable exploration rates; (3) to our
knowledge this is the first time that PHC and PHC-
WoLF are used for learning dialogue policies; (4)
for the first time, the above techniques are applied
to a negotiation domain; and (5) this is the first
study that compares Q-learning, PHC, and PHC-
WoLF in such a variety of situations (varying a
large number of parameters).
The paper is structured as follows. Section 2
presents related work. Section 3 provides a brief
introduction to single-agent RL and multi-agent
RL. Section 4 describes our negotiation domain
and experimental setup. In section 5 we present
our results. Finally, section 6 concludes and pro-
vides some ideas for future work.
</bodyText>
<sectionHeader confidence="0.999416" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9998558">
Most research in RL for dialogue management has
been done in the framework of slot-filling applica-
tions such as restaurant recommendations (Lemon
et al., 2006; Thomson and Young, 2010; Gaˇsi´c
et al., 2012; Daubigney et al., 2012), flight reser-
vations (Henderson et al., 2008), sightseeing rec-
ommendations (Misu et al., 2010), appointment
scheduling (Georgila et al., 2010), etc. RL has
also been applied to question-answering (Misu et
al., 2012), tutoring domains (Tetreault and Litman,
2008; Chi et al., 2011), and learning negotiation
dialogue policies (Heeman, 2009; Georgila and
Traum, 2011; Georgila, 2013).
As mentioned in section 1, there are three main
approaches to the problem of learning dialogue
policies using RL.
In the first approach, a SU is hand-crafted or
learned from a small corpus of human-human or
human-machine dialogues. Then the dialogue pol-
icy can be learned by having the system interact
with the SU for a large number of dialogues (usu-
ally thousands of dialogues). Depending on the
application, building a realistic SU can be just as
difficult as building a good dialogue policy. Fur-
thermore, it is not clear what constitutes a good
SU for dialogue policy learning. Should the SU
resemble real user behavior as closely as possi-
ble, or should it exhibit some degree of random-
ness to explore a variety of interaction patterns?
Despite much research on the issue, these are still
open questions (Schatzmann et al., 2006; Ai and
Litman, 2008; Pietquin and Hastie, 2013).
In the second approach, no SUs are required.
Instead the dialogue policy is learned directly from
a corpus of human-human or human-machine dia-
logues. For example, Henderson et al. (2008) used
a combination of RL and supervised learning to
learn a dialogue policy in a flight reservation do-
main, whereas Li et al. (2009) used Least-Squares
Policy Iteration (Lagoudakis and Parr, 2003), an
RL-based technique that can learn directly from
corpora, in a voice dialer application. However,
collecting such corpora is not trivial, especially in
new domains. Typically, data are collected in a
Wizard-of-Oz setup where human users think that
they interact with a system while in fact they inter-
act with a human pretending to be the system, or
by having human users interact with a preliminary
version of the dialogue system. In both cases the
resulting interactions are expected to be quite dif-
</bodyText>
<page confidence="0.995514">
501
</page>
<bodyText confidence="0.999921892156863">
ferent from the interactions of human users with
the final system. In practice this means that dia-
logue policies learned from such data could be far
from optimal.
The first experiment on learning via live inter-
action with human users (third approach) was re-
ported by Singh et al. (2002). They used RL to
help the system with two choices: how much ini-
tiative it should allow the user, and whether or not
to confirm information provided by the user. Re-
cently, learning of “full” dialogue policies (not just
choices at specific points in the dialogue) via live
interaction with human users has become possi-
ble with the use of Gaussian processes (Engel et
al., 2005; Rasmussen and Williams, 2006). Typi-
cally learning a dialogue policy is a slow process
requiring thousands of dialogues, hence the need
for SUs. Gaussian processes have been shown to
speed up learning. This fact together with easy
access to a large number of human users through
crowd-sourcing has allowed dialogue policy learn-
ing via live interaction with human users (Gaˇsi´c et
al., 2011; Gaˇsi´c et al., 2013).
Space constraints prevent us from providing an
exhaustive list of previous work on using RL for
dialogue management. Thus below we focus only
on research that is directly related to our work,
specifically research on concurrent learning of the
policies of multiple agents, and the application of
RL to negotiation domains.
So far research on RL in the dialogue commu-
nity has focused on using single-agent RL tech-
niques where the stationary environment is the
user. Most approaches assume that the user goal
is fixed and that the behavior of the user is ratio-
nal. Other approaches account for changes in user
goals (Ma, 2013). In either case, one can build a
user simulation model that is the average of dif-
ferent user behaviors or learn a policy from a cor-
pus that contains a variety of interaction patterns,
and thus safely assume that single-agent RL tech-
niques will work. However, in the latter case if
the behavior of the user changes significantly over
time then the assumption that the environment is
stationary will no longer hold.
There has been a lot of research on multi-agent
RL in the optimal control and robotics communi-
ties (Littman, 1994; Hu and Wellman, 1998; Buso-
niu et al., 2008). Here two or more agents learn si-
multaneously. Thus the environment of an agent is
one or more other agents that continuously change
their behavior because they are also learning at the
same time. Therefore the environment is no longer
stationary and single-agent RL techniques do not
work well or do not work at all. We are particu-
larly interested in the work of Bowling and Veloso
(2002) who proposed the PHC and PHC-WoLF al-
gorithms that we use in this paper. We chose these
two algorithms because, unlike other multi-agent
RL methods (Littman, 1994; Hu and Wellman,
1998), they do not make assumptions that do not
always hold and do not require quadratic or linear
programming that does not always scale.
English and Heeman (2005) were the first in the
dialogue community to explore the idea of con-
current learning of dialogue policies. However,
English and Heeman (2005) did not use multi-
agent RL but only standard single-agent RL, in
particular an on-policy Monte Carlo method (Sut-
ton and Barto, 1998). But single-agent RL tech-
niques are not well suited for concurrent learning
where each agent is trained against a continuously
changing environment. Indeed, English and Hee-
man (2005) reported problems with convergence.
Chandramohan et al. (2012) proposed a frame-
work for co-adaptation of the dialogue policy and
the SU using single-agent RL. They applied In-
verse Reinforcement Learning (IRL) (Abbeel and
Ng, 2004) to a corpus in order to learn the reward
functions of both the system and the SU. Further-
more, Cuay´ahuitl and Dethlefs (2012) used hier-
archical multi-agent RL for co-ordinating the ver-
bal and non-verbal actions of a robot. Cuay´ahuitl
and Dethlefs (2012) did not use PHC or PHC-
WoLF and did not compare against single-agent
RL methods.
With regard to using RL for learning negotia-
tion policies, the amount of research that has been
performed is very limited compared to slot-filling.
English and Heeman (2005) learned negotiation
policies for a furniture layout task. Then Hee-
man (2009) extended this work by experiment-
ing with different representations of the RL state
in the same domain (this time learning against
a hand-crafted SU). In both cases, to reduce the
search space, the RL state included only infor-
mation about e.g., whether there was a pending
proposal rather than the actual value of this pro-
posal. Paruchuri et al. (2009) performed a theo-
retical study on how Partially Observable Markov
Decision Processes (POMDPs) can be applied to
negotiation domains.
</bodyText>
<page confidence="0.985035">
502
</page>
<bodyText confidence="0.999965">
Georgila and Traum (2011) built argumentation
dialogue policies for negotiation against users of
different cultural norms in a one-issue negotiation
scenario. To learn these policies they trained SUs
on a spoken dialogue corpus in a florist-grocer
negotiation domain, and then tweaked these SUs
towards a particular cultural norm using hand-
crafted rules. Georgila (2013) learned argumen-
tation dialogue policies from a simulated corpus
in a two-issue negotiation scenario (organizing a
party). Finally, Nouri et al. (2012) used IRL to
learn a model for cultural decision-making in a
simple negotiation game (the Ultimatum Game).
</bodyText>
<sectionHeader confidence="0.7429395" genericHeader="method">
3 Single-Agent vs. Multi-Agent
Reinforcement Learning
</sectionHeader>
<bodyText confidence="0.998535650793651">
Reinforcement Learning (RL) is a machine learn-
ing technique used to learn the policy of an agent,
i.e., which action the agent should perform given
its current state (Sutton and Barto, 1998). The goal
of an RL-based agent is to maximize the reward it
gets during an interaction. Because it is very dif-
ficult for the agent to know what will happen in
the rest of the interaction, the agent must select an
action based on the average reward it has previ-
ously observed after having performed that action
in similar contexts. This average reward is called
expected future reward. Single-agent RL is used
in the framework of Markov Decision Processes
(MDPs) (Sutton and Barto, 1998) or Partially Ob-
servable Markov Decision Processes (POMDPs)
(Williams and Young, 2007). Here we focus on
MDPs.
An MDP is defined as a tuple (5, A, T, R, y)
where 5 is the set of states (representing different
contexts) which the agent may be in, A is the set
of actions of the agent, T is the transition func-
tion 5 x A x 5 —* [0, 1] which defines a set of
transition probabilities between states after taking
an action, R is the reward function 5 x A —* R
which defines the reward received when taking an
action from the given state, and y is a factor that
discounts future rewards. Solving the MDP means
finding a policy 7r : 5 —* A. The quality of the
policy 7r is measured by the expected discounted
(with discount factor y) future reward also called
Q-value, Qπ : 5 x A —* R.
A stochastic game is defined as a tuple (n, 5,
A1...n, T, R1...n, y) where n is the number of
agents, 5 is the set of states, AZ is the set of ac-
tions available for agent i (and A is the joint ac-
tion space A1 x A2 x ... x An), T is the transi-
tion function 5 x A x 5 —* [0, 1] which defines
a set of transition probabilities between states af-
ter taking a joint action, RZ is the reward function
for the ith agent 5 x A —* R, and y is a factor
that discounts future rewards. The goal is for each
agent i to learn a mixed policy 7rZ : 5 x AZ —* [0,
1] that maps states to mixed strategies, which are
probability distributions over the agent’s actions,
so that the agent’s expected discounted (with dis-
count factory) future reward is maximized.
Stochastic games are a generalization of MDPs
for multi-agent RL. In stochastic games there are
many agents that select actions and the next state
and rewards depend on the joint action of all these
agents. The agents can have different reward
functions. Partially Observable Stochastic Games
(POSGs) are the equivalent of POMDPs for multi-
agent RL. In POSGs, the agents have different ob-
servations, and uncertainty about the state they are
in and the beliefs of their interlocutors. POSGs
are very hard to solve but new algorithms continu-
ously emerge in the literature.
In this paper we use three algorithms: Q-
learning, Policy Hill-Climbing (PHC), and Win
or Learn Fast Policy Hill-Climbing (PHC-WoLF).
PHC is an extension of Q-learning. For all three
algorithms, Q-values are updated as follows:
</bodyText>
<equation confidence="0.995792333333333">
� �
Q(s, a) +— (1−α)Q(s, a)+α r + ymaxa0 Q(s0, a0)
(1)
</equation>
<bodyText confidence="0.993571125">
In Q-learning, for a given state s, the agent
performs the action with the highest Q-value for
that state. In addition to Q-values, PHC and
PHC-WoLF also maintain the current mixed pol-
icy 7r(s, a). In each step the mixed policy is up-
dated by increasing the probability of selecting the
highest valued action according to a learning rate
S (see equations (2), (3), and (4) below).
</bodyText>
<equation confidence="0.9870595">
7r(s, a) +— 7r(s, a) + Osa (2)
(3)
Ssa = min (7r (s, a), |AZ |− 1
S ) (4)
</equation>
<bodyText confidence="0.987137">
The difference between PHC and PHC-WoLF is
that PHC uses a constant learning rate S whereas
</bodyText>
<equation confidence="0.576316">
1 Osa = Ea0�aSsa0 otherwise
−Ssa if a =� argmaxa0Q(s, a0)
</equation>
<page confidence="0.990147">
503
</page>
<bodyText confidence="0.999834875">
PHC-WoLF uses a variable learning rate (see
equation (5) below). The main idea is that when
the agent is “winning” the learning rate SW should
be low so that the opponents have more time to
adapt to the agent’s policy, which helps with con-
vergence. On the other hand when the agent is
“losing” the learning rate SLF should be high so
that the agent has more time to adapt to the other
agents’ policies, which also facilitates conver-
gence. Thus PHC-WoLF uses two learning rates
SW and SLF. PHC-WoLF determines whether the
agent is “winning” or “losing” by comparing the
current policy’s 7r(s, a) expected payoff with that
of the average policy ˜7r(s, a) over time. If the cur-
rent policy’s expected payoff is greater then the
agent is “winning”, otherwise it is “losing”.
</bodyText>
<table confidence="0.982714">
Agent 1 Agent 2
apple 300 200
orange 200 300
</table>
<tableCaption confidence="0.648390125">
Table 1: Points earned by Agents 1 and 2 for each
apple and each orange that they have at the end of
the negotiation.
Agent 1: offer-2-2 (I offer you 2 A and 2 O)
Agent 2: offer-3-0 (I offer you 3 A and 0 O)
Agent 1: offer-0-3 (I offer you 0 A and 3 O)
Agent 2: offer-4-0 (I offer you 4 A and 0 O)
Agent 1: accept (I accept your offer)
</tableCaption>
<figureCaption confidence="0.9977375">
Figure 1: Example interaction between Agents 1
and 2 (A: apples, O: oranges).
</figureCaption>
<equation confidence="0.971448">
SW if � Σα� 7r(s, α�)Q(s, α�) &gt;
Σα� ˜7r(s, α�)Q(s, α)
SLF otherwise
S = {
(5)
</equation>
<bodyText confidence="0.999511307692308">
More details about Q-learning, PHC, and PHC-
WoLF can be found in (Sutton and Barto, 1998;
Bowling and Veloso, 2002).
As discussed in sections 1 and 2, single-agent
RL techniques, such as Q-learning, are not suit-
able for multi-agent RL. Nevertheless, despite its
shortcomings Q-learning has been used success-
fully for multi-agent RL (Claus and Boutilier,
1998). Indeed, as we see in section 5, Q-learning
can converge to the optimal policy for small state
spaces. However, as the state space size increases
the performance of Q-learning drops (compared to
PHC and PHC-WoLF).
</bodyText>
<sectionHeader confidence="0.994015" genericHeader="method">
4 Domain and Experimental Setup
</sectionHeader>
<bodyText confidence="0.998088875">
Our domain is a resource allocation negotiation
scenario. Two agents negotiate about how to share
resources. For the sake of readability from now on
we will refer to apples and oranges.
The two agents have different goals. Also,
they have human-like constraints of imperfect in-
formation about each other; they do not know
each other’s reward function or degree of rational-
ity (during learning our agents can be irrational).
Thus a Nash equilibrium (if there exists one) can-
not be computed in advance. Agent 1 cares more
about apples and Agent 2 cares more about or-
anges. Table 1 shows the points that Agents 1
and 2 earn for each apple and each orange that they
have at the end of the negotiation.
We use a simplified dialogue model with two
types of speech acts: offers and acceptances. The
dialogue proceeds as follows: one agent makes an
offer, e.g., “I give you 3 apples and 1 orange”, and
the other agent may choose to accept it or make a
new offer. The negotiation finishes when one of
the agents accepts the other agent’s offer or time
runs out.
We compare Q-learning with PHC and PHC-
WoLF. For all algorithms and experiments each
agent is rewarded only at the end of the dialogue
based on the negotiation outcome (see Table 1).
Thus the two agents have different reward func-
tions. There is also a penalty of -10 for each agent
action to ensure that dialogues are not too long.
Also, to avoid long dialogues, if none of the agents
accepts the other agent’s offers, the negotiation
finishes after 20 pairs of exchanges between the
two agents (20 offers from Agent 1 and 20 offers
from Agent 2).
An example interaction between the two agents
is shown in Figure 1. As we can see, each agent
can offer any combination of apples and oranges.
So if we have X apples and Y oranges for sharing,
there can be (X + 1) × (Y + 1) possible offers.
For example if we have 2 apples and 2 oranges
for sharing, there can be 9 possible offers: “offer-
0-0”, “offer-0-1”, ..., “offer-2-2”. For our exper-
iments we vary the number of fruits to be shared
and choose to keep X equal to Y .
Table 2 shows our state representation, i.e., the
state variables that we keep track of with all the
possible values they can take, where X is the num-
</bodyText>
<page confidence="0.989068">
504
</page>
<bodyText confidence="0.7882916">
Current offer: (X + 1) × (Y + 1) possible
values
How many times the current offer has already
been rejected: (0, 1, 2, 3, or 4)
Is the current offer accepted: yes, no
</bodyText>
<tableCaption confidence="0.988627">
Table 2: State variables.
</tableCaption>
<bodyText confidence="0.999798162790698">
ber of apples and Y is the number of oranges to be
shared. The third variable is always set to “no” un-
til one of the agents accepts the other agent’s offer.
Table 3 shows the state and action space sizes
for different numbers of apples and oranges to be
shared used in our experiments below. The num-
ber of actions includes the acceptance of an of-
fer. Table 3 also shows the number of state-action
pairs (Q-values). As we will see in section 5, even
though the number of states for each agent is not
large, it takes many iterations and high exploration
rates for convergence due to the fact that both
agents are learning at the same time and the as-
sumption of interacting with a stationary environ-
ment no longer holds. For comparison, in (English
and Heeman, 2005) the state specification for each
agent included 5 binary variables resulting in 32
possible states. English and Heeman (2005) kept
track of whether there was an offer on the table but
not of the actual value of the offer. For our task it
is essential to keep track of the offer values, which
of course results in much larger state spaces. Also,
in (English and Heeman, 2005) there were 5 possi-
ble actions resulting in 160 state-action pairs. Our
state and action spaces are much larger and fur-
thermore we explore the effect of different state
and action space sizes on convergence.
During learning the two agents interact for
5 epochs. Each epoch contains N number of
episodes. We vary N from 25,000 up to 400,000
with a step of 25,000 episodes. English and Hee-
man (2005) trained their agents for 200 epochs,
where each epoch contained 200 episodes.
We also vary the exploration rate per epoch.
In particular, in the experiments reported in sec-
tion 5.1 the exploration rate is set as follows: 0.95
for epoch 1, 0.8 for epoch 2, 0.5 for epoch 3, 0.3
for epoch 4, and 0.1 for epoch 5. Section 5.2 re-
ports results again with 5 epochs of training but a
constant exploration rate per epoch set to 0.3. An
exploration rate of 0.3 means that 30% of the time
the agent will select an action randomly.
Finally, we vary the learning rate. For PHC-
</bodyText>
<figure confidence="0.613713444444444">
#States #Actions #State-Action
Pairs
1 A &amp; O 40 5 200
2 A &amp; O 90 10 900
3 A &amp; O 160 17 2720
4 A &amp; O 250 26 6500
5 A &amp; O 360 37 13320
6 A &amp; O 490 50 24500
7 A &amp; O 640 65 41600
</figure>
<tableCaption confidence="0.938837666666667">
Table 3: State space, action space, and state-action
space sizes for different numbers of apples and or-
anges to be shared (A: apples, O: oranges).
</tableCaption>
<bodyText confidence="0.999880407407407">
WoLF we set SW = 0.05 and SLF = 0.2 (see sec-
tion 3). These values were chosen with exper-
imentation and the basic idea is that the agent
should learn faster when “losing” and slower when
“winning”. For PHC we explore two cases. In the
first case which from now on will be referred to
as PHC-W, we set S to be equal to SW (also used
for PHC-WoLF). In the second case which from
now on will be referred to as PHC-LF, we set S
to be equal to SLF (also used for PHC-WoLF). So
unlike PHC-WoLF, PHC-W and PHC-LF do not
use a variable learning rate. PHC-W always learns
slowly and PHC-LF always learns fast.
In all the above cases, training stops after 5
epochs. Then we test the learned policies against
each other for one more epoch the size of which is
the same as the size of the epochs used for train-
ing. For example, if the policies were learned
for 5 epochs with each epoch containing 25,000
episodes, then for testing the two policies will in-
teract for another 25,000 episodes. For compari-
son, English and Heeman (2005) had their agents
interact for 5,000 dialogues during testing. To en-
sure that the policies do not converge by chance,
we run the training and test sessions 20 times each
and we report averages. Thus all results presented
in section 5 are averages of 20 runs.
</bodyText>
<sectionHeader confidence="0.999941" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.99933975">
Given that Agent 1 is more interested in apples
and Agent 2 cares more about oranges, the maxi-
mum total utility solution would be the case where
each agent offers to get all the fruits it cares about
and to give its interlocutor all the fruits it does not
care about, and the other agent accepts this of-
fer. Thus, when converging to the maximum to-
tal utility solution, in the case of 4 fruits (4 ap-
</bodyText>
<page confidence="0.996048">
505
</page>
<bodyText confidence="0.999986269230769">
ples and 4 oranges), the average reward of the
two agents should be 1200 minus 10 for making
or accepting an offer. For 5 fruits the average re-
ward should be 1500 minus 10, and so forth. We
call 1200 (or 1500) the convergence reward, i.e.,
the reward after converging to the maximum to-
tal utility solution if we do not take into account
the action penalty. For example, in the case of 4
fruits, if Agent 1 starts the negotiation, after con-
verging to the maximum total utility solution the
optimal interaction should be: Agent 1 makes an
offer to Agent 2, namely 0 apples and 4 oranges,
and Agent 2 accepts. Thus the reward for Agent 1
is 1190, the reward for Agent 2 is 1190, and the av-
erage reward of the two agents is also 1190. Also,
the convergence reward for Agent 1 is 1200 and
the convergence reward for Agent 2 is also 1200.
Below, in all the graphs that we provide, we
show the average distance from the convergence
reward. This is to make all graphs comparable
because in all cases the optimal average distance
from the convergence reward of the two agents
should be equal to 10 (make the optimal offer
or accept the optimal offer that the other agent
makes). The formulas for calculating the average
distance from the convergence reward are:
</bodyText>
<equation confidence="0.994792285714286">
AD1 = En r 1 |CR1 − R1j
nr
AD2 = Ej r 1  |CR2 − R2j
nr
AD1 + AD2
AD = (8)
2
</equation>
<bodyText confidence="0.999938333333333">
where CR1 is the convergence reward for Agent 1,
R1j is the reward of Agent 1 for run j, CR2 is the
convergence reward for Agent 2, and R2j is the
reward of Agent 2 for run j. Moreover, AD1 is
the average distance from the convergence reward
for Agent 1, AD2 is the average distance from the
convergence reward for Agent 2, and AD is the
average of AD1 and AD2. All graphs of section 5
show AD values. Also, nr is the number of runs
(in our case always equal to 20). Thus in the case
of 4 fruits, we will have CR1=CR2=1200, and if
for all runs R1j=R2j=1190, then AD=10.
</bodyText>
<subsectionHeader confidence="0.995057">
5.1 Variable Exploration Rate
</subsectionHeader>
<bodyText confidence="0.9904275">
In this section we report results with different ex-
ploration rates per training epoch (see section 4).
</bodyText>
<table confidence="0.609257">
Q- PHC- PHC- PHC-
learning LF W WoLF
1 A &amp; O 10.5 10 10 10
</table>
<listItem confidence="0.909969166666667">
2 A &amp; O 10.3 10.3 10 10
3 A &amp; O 11.7 10 10 10
4 A &amp; O 15 11.8 11.7 11.7
5 A &amp; O 45.4 29.5 26.5 22.9
6 A &amp; O 60.8 33.4 46.1 33.9
7 A &amp; O 95 56 187.8 88.6
</listItem>
<tableCaption confidence="0.91050275">
Table 4: Average distance from convergence re-
ward over 20 runs for 100,000 episodes per epoch
and for different numbers of fruits to be shared (A:
apples, O: oranges). The best possible value is 10.
</tableCaption>
<bodyText confidence="0.988693342857143">
Table 4 shows the average distance from the con-
vergence reward over 20 runs for 100,000 episodes
per epoch, for different numbers of fruits, and
for all four methods (Q-learning, PHC-LF, PHC-
W, and PHC-WoLF). It is clear that as the state
space becomes larger 100,000 training episodes
per epoch are not enough for convergence. Also,
for 1, 2, and 3 fruits all algorithms converge and
perform comparably. As the number of fruits in-
creases, Q-learning starts performing worse than
the multi-agent RL algorithms. For 7 fruits PHC-
W appears to perform worse than Q-learning but
this is because, as we can see in Figure 5, in this
case more than 400,000 episodes per epoch are re-
quired for convergence. Thus after only 100,000
episodes per epoch all policies still behave some-
what randomly.
Figures 2, 3, 4, and 5 show the average distance
from the convergence reward as a function of the
number of episodes per epoch during training, for
4, 5, 6, and 7 fruits respectively. For 4 fruits it
takes about 125,000 episodes per epoch and for 5
fruits it takes about 225,000 episodes per epoch for
the policies to converge. This number rises to ap-
proximately 350,000 for 6 fruits and becomes even
higher for 7 fruits. Q-learning consistently per-
forms worse than the rest of the algorithms. The
differences between PHC-LF, PHC-W, and PHC-
WoLF are insignificant, which is a bit surprising
given that Bowling and Veloso (2002) showed that
PHC-WoLF performed better than PHC in a series
of benchmark tasks. In Figures 2 and 3, PHC-LF
appears to be reaching convergence slightly faster
than PHC-W and PHC-WoLF but this is not statis-
tically significant.
</bodyText>
<page confidence="0.997545">
506
</page>
<figureCaption confidence="0.99615675">
Figure 2: 4 fruits and variable exploration rate:
Average distance from convergence reward during
testing (20 runs). The best possible value is 10.
Figure 4: 6 fruits and variable exploration rate:
Average distance from convergence reward during
testing (20 runs). The best possible value is 10.
Figure 3: 5 fruits and variable exploration rate:
Average distance from convergence reward during
testing (20 runs). The best possible value is 10.
Figure 5: 7 fruits and variable exploration rate:
Average distance from convergence reward during
testing (20 runs). The best possible value is 10.
</figureCaption>
<subsectionHeader confidence="0.99942">
5.2 Constant Exploration Rate
</subsectionHeader>
<bodyText confidence="0.999973578947369">
In this section we report results with a constant
exploration rate for all training epochs (see sec-
tion 4). Figures 6 and 7 show the average dis-
tance from the convergence reward as a function of
the number of episodes per epoch during training,
for 4 and 5 fruits respectively. Clearly having a
constant exploration rate in all epochs is problem-
atic. For 4 fruits, after 225,000 episodes per epoch
there is still no convergence. For comparison, with
a variable exploration rate it took about 125,000
episodes per epoch for the policies to converge.
Likewise for 5 fruits. After 400,000 episodes per
epoch there is still no convergence. For compari-
son, with a variable exploration rate it took about
225,000 episodes per epoch for convergence.
The above results show that, unlike single-agent
RL where having a constant exploration rate is
perfectly acceptable, here a constant exploration
rate does not work.
</bodyText>
<sectionHeader confidence="0.980278" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.9985407">
We used single-agent RL and multi-agent RL for
learning dialogue policies in a resource allocation
negotiation scenario. Two agents interacted with
each other and both learned at the same time. The
advantage of this approach is that it does not re-
quire SUs to train against or corpora to learn from.
We compared a traditional single-agent RL al-
gorithm (Q-learning) against two multi-agent RL
algorithms (PHC and PHC-WoLF) varying the
scenario complexity (state space size), the number
</bodyText>
<page confidence="0.995033">
507
</page>
<figureCaption confidence="0.982619666666667">
Figure 6: 4 fruits and constant exploration rate:
Average distance from convergence reward during
testing (20 runs). The best possible value is 10.
Figure 7: 5 fruits and constant exploration rate:
Average distance from convergence reward during
testing (20 runs). The best possible value is 10.
</figureCaption>
<bodyText confidence="0.9999691">
of training episodes, and the learning and explo-
ration rates. Our results showed that Q-learning
is not suitable for concurrent learning given that
it is designed for learning against a stationary en-
vironment. Q-learning failed to converge in all
cases, except for very small state space sizes. On
the other hand, both PHC and PHC-WoLF always
converged (or in the case of 7 fruits they needed
more training episodes) and performed similarly.
We also showed that in concurrent learning very
high gradually decreasing exploration rates are re-
quired for convergence. We conclude that multi-
agent RL of dialogue policies is a promising alter-
native to using single-agent RL and SUs or learn-
ing directly from corpora.
The focus of this paper is on comparing single-
agent RL and multi-agent RL for concurrent learn-
ing, and studying the implications for convergence
and exploration/learning rates. Our next step is
testing with human users. We are particularly in-
terested in users whose behavior changes during
the interaction and continuous testing against ex-
pert repeat users, which has never been done be-
fore. Another interesting question is whether cor-
pora or SUs may still be required for designing
the state and action spaces and the reward func-
tions of the interlocutors, bootstrapping the poli-
cies, and ensuring that information about the be-
havior of human users is encoded in the resulting
learned policies. Gaˇsi´c et al. (2013) showed that it
is possible to learn “full” dialogue policies just via
interaction with human users (without any boot-
strapping using corpora or SUs). Similarly, con-
current learning could be used in an on-line fash-
ion via live interaction with human users. Or al-
ternatively concurrent learning could be used off-
line to bootstrap the policies and then these poli-
cies could be improved via live interaction with
human users (again using concurrent learning to
address possible changes in user behavior). These
are open research questions for future work.
Furthermore, we intend to apply multi-agent RL
to more complex negotiation domains, e.g., exper-
iment with more than two types of resources (not
just apples and oranges) and more types of actions
(not just offers and acceptances). We would also
like to compare policies learned with multi-agent
RL techniques with policies learned with SUs or
from corpora both in simulation and with human
users. Finally, we aim to experiment with differ-
ent feature-based representations of the state and
action spaces. Currently all possible deal combi-
nations are listed as possible actions and as ele-
ments of the state, which can quickly lead to very
large state and action spaces as the application be-
comes more complex (in our case as the number of
fruits increases). However, abstraction is not triv-
ial because the agents have no guarantee that the
value of a deal is a simple function of the value of
its parts, and values may differ for different agents.
</bodyText>
<sectionHeader confidence="0.998031" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99915725">
Claire Nelson sadly died in May 2013. We con-
tinued and completed this work after her pass-
ing away. She is greatly missed. This work was
funded by the NSF grant #1117313.
</bodyText>
<page confidence="0.996314">
508
</page>
<sectionHeader confidence="0.981749" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999265924528301">
Pieter Abbeel and Andrew Y. Ng. 2004. Apprentice-
ship learning via inverse reinforcement learning. In
Proc. of the International Conference on Machine
Learning, Bannf, Alberta, Canada.
Hua Ai and Diane Litman. 2008. Assessing dialog sys-
tem user simulation evaluation measures using hu-
man judges. In Proc. of the Annual Meeting of the
Association for Computational Linguistics, Colum-
bus, Ohio, USA.
Michael Bowling and Manuela Veloso. 2002. Multi-
agent learning using a variable learning rate. Artifi-
cial Intelligence, 136(2):215–250.
L. Busoniu, R. Babuska, and B. De Schutter. 2008.
A comprehensive survey of multiagent reinforce-
ment learning. IEEE Transactions on Systems, Man,
and Cybernetics, Part C: Applications and Reviews,
38(2):156–172.
Senthilkumar Chandramohan, Matthieu Geist, Fabrice
Lef`evre, and Olivier Pietquin. 2012. Co-adaptation
in spoken dialogue systems. In Proc. of the Interna-
tional Workshop on Spoken Dialogue Systems, Paris,
France.
Min Chi, Kurt VanLehn, Diane Litman, and Pamela
Jordan. 2011. Empirically evaluating the ap-
plication of reinforcement learning to the induc-
tion of effective and adaptive pedagogical strategies.
User Modeling and User-Adapted Interaction, 21(1-
2):137–180.
Caroline Claus and Craig Boutilier. 1998. The dynam-
ics of reinforcement learning in cooperative multia-
gent systems. In Proc. of the National Conference
on Artificial Intelligence.
Heriberto Cuay´ahuitl and Nina Dethlefs. 2012. Hier-
archical multiagent reinforcement learning for coor-
dinating verbal and nonverbal actions in robots. In
Proc. of the ECAI Workshop on Machine Learning
for Interactive Systems, Montpellier, France.
Lucie Daubigney, Matthieu Geist, Senthilkumar Chan-
dramohan, and Olivier Pietquin. 2012. A compre-
hensive reinforcement learning framework for dia-
logue management optimization. IEEE Journal of
Selected Topics in Signal Processing, 6(8):891–902.
Yaakov Engel, Shie Mannor, and Ron Meir. 2005. Re-
inforcement learning with Gaussian processes. In
Proc. of the International Conference on Machine
Learning, Bonn, Germany.
Michael S. English and Peter A. Heeman. 2005.
Learning mixed initiative dialogue strategies by us-
ing reinforcement learning on both conversants. In
Proc. of the Conference on Empirical Methods in
Natural Language Processing, Vancouver, Canada.
M. Ga&amp;quot;si´c, Filip Jur&amp;quot;c´ı&amp;quot;cek, Blaise Thomson, Kai Yu, and
Steve Young. 2011. On-line policy optimisation
of spoken dialogue systems via live interaction with
human subjects. In Proc. of the IEEE Automatic
Speech Recognition and Understanding Workshop,
Big Island, Hawaii, USA.
Milica Ga&amp;quot;si´c, Matthew Henderson, Blaise Thomson,
Pirros Tsiakoulis, and Steve Young. 2012. Pol-
icy optimisation of POMDP-based dialogue systems
without state space compression. In Proc. of the
IEEE Workshop on Spoken Language Technology,
Miami, Florida, USA.
M. Ga&amp;quot;si´c, C. Breslin, M. Henderson, D. Kim,
M. Szummer, B. Thomson, P. Tsiakoulis, and
S. Young. 2013. On-line policy optimisation of
Bayesian spoken dialogue systems via human inter-
action. In Proc. of the International Conference on
Acoustics, Speech and Signal Processing, Vancou-
ver, Canada.
Kallirroi Georgila and David Traum. 2011. Reinforce-
ment learning of argumentation dialogue policies in
negotiation. In Proc. of Interspeech, Florence, Italy.
Kallirroi Georgila, James Henderson, and Oliver
Lemon. 2006. User simulation for spoken dialogue
systems: Learning and evaluation. In Proc. of Inter-
speech, Pittsburgh, Pennsylvania, USA.
Kallirroi Georgila, Maria K. Wolters, and Johanna D.
Moore. 2010. Learning dialogue strategies from
older and younger simulated users. In Proc. of
the Annual SIGdial Meeting on Discourse and Di-
alogue, Tokyo, Japan.
Kallirroi Georgila. 2013. Reinforcement learning of
two-issue negotiation dialogue policies. In Proc. of
the Annual SIGdial Meeting on Discourse and Dia-
logue, Metz, France.
Peter A. Heeman. 2009. Representing the reinforce-
ment learning state in a negotiation dialogue. In
Proc. of the IEEE Automatic Speech Recognition
and Understanding Workshop, Merano, Italy.
James Henderson, Oliver Lemon, and Kallirroi
Georgila. 2008. Hybrid reinforcement/supervised
learning of dialogue policies from fixed datasets.
Computational Linguistics, 34(4):487–511.
Junling Hu and Michael P. Wellman. 1998. Multia-
gent reinforcement learning: Theoretical framework
and an algorithm. In Proc. of the International Con-
ference on Machine Learning, Madison, Wisconsin,
USA.
Filip Jur&amp;quot;c´ı&amp;quot;cek, Blaise Thomson, and Steve Young.
2012. Reinforcement learning for parameter esti-
mation in statistical spoken dialogue systems. Com-
puter Speech and Language, 26(3):168–192.
Michail G. Lagoudakis and Ronald Parr. 2003. Least-
squares policy iteration. Journal of Machine Learn-
ing Research, 4:1107–1149.
</reference>
<page confidence="0.98273">
509
</page>
<reference confidence="0.999815">
Oliver Lemon, Kallirroi Georgila, and James Hender-
son. 2006. Evaluating effectiveness and portabil-
ity of reinforcement learned dialogue strategies with
real users: The TALK TownInfo evaluation. In Proc.
of the IEEE Workshop on Spoken Language Technol-
ogy, Palm Beach, Aruba.
Lihong Li, Jason D. Williams, and Suhrid Balakrish-
nan. 2009. Reinforcement learning for dialog man-
agement using least-squares policy iteration and fast
feature selection. In Proc. of Interspeech, Brighton,
United Kingdom.
Michael L. Littman. 1994. Markov games as a frame-
work for multi-agent reinforcement learning. In
Proc. of the International Conference on Machine
Learning, New Brunswick, New Jersey, USA.
Yi Ma. 2013. User goal change model for spoken dia-
log state tracking. In Proc. of the NAACL-HLT Stu-
dent Research Workshop, Atlanta, Georgia, USA.
Teruhisa Misu, Komei Sugiura, Kiyonori Ohtake,
Chiori Hori, Hideki Kashioka, Hisashi Kawai, and
Satoshi Nakamura. 2010. Modeling spoken deci-
sion making dialogue and optimization of its dia-
logue strategy. In Proc. of the Annual SIGdial Meet-
ing on Discourse and Dialogue, Tokyo, Japan.
Teruhisa Misu, Kallirroi Georgila, Anton Leuski, and
David Traum. 2012. Reinforcement learning of
question-answering dialogue policies for virtual mu-
seum guides. In Proc. of the Annual SIGdial Meet-
ing on Discourse and Dialogue, Seoul, South Korea.
Elnaz Nouri, Kallirroi Georgila, and David Traum.
2012. A cultural decision-making model for nego-
tiation based on inverse reinforcement learning. In
Proc. of the Cognitive Science Conference, Sapporo,
Japan.
P. Paruchuri, N. Chakraborty, R. Zivan, K. Sycara,
M. Dudik, and G. Gordon. 2009. POMDP based
negotiation modeling. In IJCAI Workshop on Mod-
eling Intercultural Collaboration and Negotiation,
Pasadena, California, USA.
Olivier Pietquin and Helen Hastie. 2013. A survey
on metrics for the evaluation of user simulations.
Knowledge Engineering Review, 28(1):59–73.
Carl Edward Rasmussen and Christopher K. I.
Williams. 2006. Gaussian Processes for Machine
Learning. MIT Press.
Verena Rieser, Simon Keizer, Xingkun Liu, and Oliver
Lemon. 2011. Adaptive information presentation
for spoken dialogue systems: Evaluation with hu-
man subjects. In Proc. of the European Workshop
on Natural Language Generation, Nancy, France.
Jost Schatzmann, Karl Weilhammer, Matt Stuttle, and
Steve Young. 2006. A survey of statistical user sim-
ulation techniques for reinforcement-learning of dia-
logue management strategies. Knowledge Engineer-
ing Review, 21(2):97–126.
Satinder Singh, Diane Litman, Michael Kearns, and
Marilyn Walker. 2002. Optimizing dialogue man-
agement with reinforcement learning: Experiments
with the NJFun system. Journal of Artificial Intelli-
gence Research, 16:105–133.
Richard S. Sutton and Andrew G. Barto. 1998. Rein-
forcement Learning: An Introduction. MIT Press.
Joel R. Tetreault and Diane J. Litman. 2008. A rein-
forcement learning approach to evaluating state rep-
resentations in spoken dialogue systems. Speech
Communication, 50(8-9):683–696.
Blaise Thomson and Steve Young. 2010. Bayesian
update of dialogue state: A POMDP framework for
spoken dialogue systems. Computer Speech and
Language, 24(4):562–588.
Jason D. Williams and Steve Young. 2007. Scal-
ing POMDPs for spoken dialog management. IEEE
Transactions on Audio, Speech, and Language Pro-
cessing, 15(7):2116–2129.
</reference>
<page confidence="0.996644">
510
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.950816">
<title confidence="0.9976845">Single-Agent vs. Multi-Agent Techniques for Reinforcement Learning of Negotiation Dialogue Policies</title>
<author confidence="0.99211">Kallirroi Georgila</author>
<author confidence="0.99211">Claire Nelson</author>
<author confidence="0.99211">David</author>
<affiliation confidence="0.999942">University of Southern California Institute for Creative</affiliation>
<address confidence="0.998814">12015 Waterfront Drive, Playa Vista, CA 90094,</address>
<abstract confidence="0.998304083333334">We use single-agent and multi-agent Reinforcement Learning (RL) for learning dialogue policies in a resource allocation negotiation scenario. Two agents learn concurrently by interacting with each other without any need for simulated users (SUs) to train against or corpora to learn from. In particular, we compare the Qlearning, Policy Hill-Climbing (PHC) and Win or Learn Fast Policy Hill-Climbing (PHC-WoLF) algorithms, varying the scenario complexity (state space size), the number of training episodes, the learning rate, and the exploration rate. Our results show that generally Q-learning fails to converge whereas PHC and PHC-WoLF always converge and perform similarly. We also show that very high gradually decreasing exploration rates are required for convergence. We conclude that multiagent RL of dialogue policies is a promising alternative to using single-agent RL and SUs or learning directly from corpora.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Pieter Abbeel</author>
<author>Andrew Y Ng</author>
</authors>
<title>Apprenticeship learning via inverse reinforcement learning.</title>
<date>2004</date>
<booktitle>In Proc. of the International Conference on Machine Learning,</booktitle>
<location>Bannf, Alberta, Canada.</location>
<contexts>
<context position="12558" citStr="Abbeel and Ng, 2004" startWordPosition="2046" endWordPosition="2049">ncurrent learning of dialogue policies. However, English and Heeman (2005) did not use multiagent RL but only standard single-agent RL, in particular an on-policy Monte Carlo method (Sutton and Barto, 1998). But single-agent RL techniques are not well suited for concurrent learning where each agent is trained against a continuously changing environment. Indeed, English and Heeman (2005) reported problems with convergence. Chandramohan et al. (2012) proposed a framework for co-adaptation of the dialogue policy and the SU using single-agent RL. They applied Inverse Reinforcement Learning (IRL) (Abbeel and Ng, 2004) to a corpus in order to learn the reward functions of both the system and the SU. Furthermore, Cuay´ahuitl and Dethlefs (2012) used hierarchical multi-agent RL for co-ordinating the verbal and non-verbal actions of a robot. Cuay´ahuitl and Dethlefs (2012) did not use PHC or PHCWoLF and did not compare against single-agent RL methods. With regard to using RL for learning negotiation policies, the amount of research that has been performed is very limited compared to slot-filling. English and Heeman (2005) learned negotiation policies for a furniture layout task. Then Heeman (2009) extended thi</context>
</contexts>
<marker>Abbeel, Ng, 2004</marker>
<rawString>Pieter Abbeel and Andrew Y. Ng. 2004. Apprenticeship learning via inverse reinforcement learning. In Proc. of the International Conference on Machine Learning, Bannf, Alberta, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hua Ai</author>
<author>Diane Litman</author>
</authors>
<title>Assessing dialog system user simulation evaluation measures using human judges.</title>
<date>2008</date>
<booktitle>In Proc. of the Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Columbus, Ohio, USA.</location>
<contexts>
<context position="7942" citStr="Ai and Litman, 2008" startWordPosition="1272" endWordPosition="1275">hen the dialogue policy can be learned by having the system interact with the SU for a large number of dialogues (usually thousands of dialogues). Depending on the application, building a realistic SU can be just as difficult as building a good dialogue policy. Furthermore, it is not clear what constitutes a good SU for dialogue policy learning. Should the SU resemble real user behavior as closely as possible, or should it exhibit some degree of randomness to explore a variety of interaction patterns? Despite much research on the issue, these are still open questions (Schatzmann et al., 2006; Ai and Litman, 2008; Pietquin and Hastie, 2013). In the second approach, no SUs are required. Instead the dialogue policy is learned directly from a corpus of human-human or human-machine dialogues. For example, Henderson et al. (2008) used a combination of RL and supervised learning to learn a dialogue policy in a flight reservation domain, whereas Li et al. (2009) used Least-Squares Policy Iteration (Lagoudakis and Parr, 2003), an RL-based technique that can learn directly from corpora, in a voice dialer application. However, collecting such corpora is not trivial, especially in new domains. Typically, data ar</context>
</contexts>
<marker>Ai, Litman, 2008</marker>
<rawString>Hua Ai and Diane Litman. 2008. Assessing dialog system user simulation evaluation measures using human judges. In Proc. of the Annual Meeting of the Association for Computational Linguistics, Columbus, Ohio, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Bowling</author>
<author>Manuela Veloso</author>
</authors>
<title>Multiagent learning using a variable learning rate.</title>
<date>2002</date>
<journal>Artificial Intelligence,</journal>
<volume>136</volume>
<issue>2</issue>
<contexts>
<context position="5130" citStr="Bowling and Veloso, 2002" startWordPosition="809" endWordPosition="812">multi-agent RL can also have important implications for learning via live interaction with human users. Imagine a system that learns to change its strategy as it realizes that a particular user is no longer a novice user, or that a user no longer cares about five star restaurants. We apply multi-agent RL to a resource allocation negotiation scenario. Two agents with different preferences negotiate about how to share resources. We compare Q-learning (a singleagent RL algorithm) with two multi-agent RL algorithms: Policy Hill-Climbing (PHC) and Win or Learn Fast Policy Hill-Climbing (PHC-WoLF) (Bowling and Veloso, 2002). We vary the scenario complexity (i.e., the quantity of resources to be shared and consequently the state space size), the number of training episodes, the learning rate, and the exploration rate. Our research contributions are as follows: (1) we propose concurrent learning using multi-agent RL as a way to deal with some of the issues of current approaches to dialogue policy learning (i.e., the need for SUs and corpora), which may also potentially prove useful for learning via live interaction with human users; (2) we show that concurrent learning can address changes in user behavior over tim</context>
<context position="11527" citStr="Bowling and Veloso (2002)" startWordPosition="1879" endWordPosition="1882"> then the assumption that the environment is stationary will no longer hold. There has been a lot of research on multi-agent RL in the optimal control and robotics communities (Littman, 1994; Hu and Wellman, 1998; Busoniu et al., 2008). Here two or more agents learn simultaneously. Thus the environment of an agent is one or more other agents that continuously change their behavior because they are also learning at the same time. Therefore the environment is no longer stationary and single-agent RL techniques do not work well or do not work at all. We are particularly interested in the work of Bowling and Veloso (2002) who proposed the PHC and PHC-WoLF algorithms that we use in this paper. We chose these two algorithms because, unlike other multi-agent RL methods (Littman, 1994; Hu and Wellman, 1998), they do not make assumptions that do not always hold and do not require quadratic or linear programming that does not always scale. English and Heeman (2005) were the first in the dialogue community to explore the idea of concurrent learning of dialogue policies. However, English and Heeman (2005) did not use multiagent RL but only standard single-agent RL, in particular an on-policy Monte Carlo method (Sutton</context>
<context position="19395" citStr="Bowling and Veloso, 2002" startWordPosition="3273" endWordPosition="3276">200 orange 200 300 Table 1: Points earned by Agents 1 and 2 for each apple and each orange that they have at the end of the negotiation. Agent 1: offer-2-2 (I offer you 2 A and 2 O) Agent 2: offer-3-0 (I offer you 3 A and 0 O) Agent 1: offer-0-3 (I offer you 0 A and 3 O) Agent 2: offer-4-0 (I offer you 4 A and 0 O) Agent 1: accept (I accept your offer) Figure 1: Example interaction between Agents 1 and 2 (A: apples, O: oranges). SW if � Σα� 7r(s, α�)Q(s, α�) &gt; Σα� ˜7r(s, α�)Q(s, α) SLF otherwise S = { (5) More details about Q-learning, PHC, and PHCWoLF can be found in (Sutton and Barto, 1998; Bowling and Veloso, 2002). As discussed in sections 1 and 2, single-agent RL techniques, such as Q-learning, are not suitable for multi-agent RL. Nevertheless, despite its shortcomings Q-learning has been used successfully for multi-agent RL (Claus and Boutilier, 1998). Indeed, as we see in section 5, Q-learning can converge to the optimal policy for small state spaces. However, as the state space size increases the performance of Q-learning drops (compared to PHC and PHC-WoLF). 4 Domain and Experimental Setup Our domain is a resource allocation negotiation scenario. Two agents negotiate about how to share resources. </context>
<context position="30267" citStr="Bowling and Veloso (2002)" startWordPosition="5309" endWordPosition="5312">igures 2, 3, 4, and 5 show the average distance from the convergence reward as a function of the number of episodes per epoch during training, for 4, 5, 6, and 7 fruits respectively. For 4 fruits it takes about 125,000 episodes per epoch and for 5 fruits it takes about 225,000 episodes per epoch for the policies to converge. This number rises to approximately 350,000 for 6 fruits and becomes even higher for 7 fruits. Q-learning consistently performs worse than the rest of the algorithms. The differences between PHC-LF, PHC-W, and PHCWoLF are insignificant, which is a bit surprising given that Bowling and Veloso (2002) showed that PHC-WoLF performed better than PHC in a series of benchmark tasks. In Figures 2 and 3, PHC-LF appears to be reaching convergence slightly faster than PHC-W and PHC-WoLF but this is not statistically significant. 506 Figure 2: 4 fruits and variable exploration rate: Average distance from convergence reward during testing (20 runs). The best possible value is 10. Figure 4: 6 fruits and variable exploration rate: Average distance from convergence reward during testing (20 runs). The best possible value is 10. Figure 3: 5 fruits and variable exploration rate: Average distance from con</context>
</contexts>
<marker>Bowling, Veloso, 2002</marker>
<rawString>Michael Bowling and Manuela Veloso. 2002. Multiagent learning using a variable learning rate. Artificial Intelligence, 136(2):215–250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Busoniu</author>
<author>R Babuska</author>
<author>B De Schutter</author>
</authors>
<title>A comprehensive survey of multiagent reinforcement learning.</title>
<date>2008</date>
<journal>IEEE Transactions on Systems, Man, and Cybernetics, Part C: Applications and Reviews,</journal>
<volume>38</volume>
<issue>2</issue>
<marker>Busoniu, Babuska, De Schutter, 2008</marker>
<rawString>L. Busoniu, R. Babuska, and B. De Schutter. 2008. A comprehensive survey of multiagent reinforcement learning. IEEE Transactions on Systems, Man, and Cybernetics, Part C: Applications and Reviews, 38(2):156–172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Senthilkumar Chandramohan</author>
<author>Matthieu Geist</author>
<author>Fabrice Lef`evre</author>
<author>Olivier Pietquin</author>
</authors>
<title>Co-adaptation in spoken dialogue systems.</title>
<date>2012</date>
<booktitle>In Proc. of the International Workshop on Spoken Dialogue Systems,</booktitle>
<location>Paris, France.</location>
<marker>Chandramohan, Geist, Lef`evre, Pietquin, 2012</marker>
<rawString>Senthilkumar Chandramohan, Matthieu Geist, Fabrice Lef`evre, and Olivier Pietquin. 2012. Co-adaptation in spoken dialogue systems. In Proc. of the International Workshop on Spoken Dialogue Systems, Paris, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Chi</author>
<author>Kurt VanLehn</author>
<author>Diane Litman</author>
<author>Pamela Jordan</author>
</authors>
<title>Empirically evaluating the application of reinforcement learning to the induction of effective and adaptive pedagogical strategies. User Modeling and User-Adapted Interaction,</title>
<date>2011</date>
<pages>21--1</pages>
<contexts>
<context position="6987" citStr="Chi et al., 2011" startWordPosition="1113" endWordPosition="1116">present our results. Finally, section 6 concludes and provides some ideas for future work. 2 Related Work Most research in RL for dialogue management has been done in the framework of slot-filling applications such as restaurant recommendations (Lemon et al., 2006; Thomson and Young, 2010; Gaˇsi´c et al., 2012; Daubigney et al., 2012), flight reservations (Henderson et al., 2008), sightseeing recommendations (Misu et al., 2010), appointment scheduling (Georgila et al., 2010), etc. RL has also been applied to question-answering (Misu et al., 2012), tutoring domains (Tetreault and Litman, 2008; Chi et al., 2011), and learning negotiation dialogue policies (Heeman, 2009; Georgila and Traum, 2011; Georgila, 2013). As mentioned in section 1, there are three main approaches to the problem of learning dialogue policies using RL. In the first approach, a SU is hand-crafted or learned from a small corpus of human-human or human-machine dialogues. Then the dialogue policy can be learned by having the system interact with the SU for a large number of dialogues (usually thousands of dialogues). Depending on the application, building a realistic SU can be just as difficult as building a good dialogue policy. Fu</context>
</contexts>
<marker>Chi, VanLehn, Litman, Jordan, 2011</marker>
<rawString>Min Chi, Kurt VanLehn, Diane Litman, and Pamela Jordan. 2011. Empirically evaluating the application of reinforcement learning to the induction of effective and adaptive pedagogical strategies. User Modeling and User-Adapted Interaction, 21(1-2):137–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Caroline Claus</author>
<author>Craig Boutilier</author>
</authors>
<title>The dynamics of reinforcement learning in cooperative multiagent systems.</title>
<date>1998</date>
<booktitle>In Proc. of the National Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="19639" citStr="Claus and Boutilier, 1998" startWordPosition="3310" endWordPosition="3313">-3 (I offer you 0 A and 3 O) Agent 2: offer-4-0 (I offer you 4 A and 0 O) Agent 1: accept (I accept your offer) Figure 1: Example interaction between Agents 1 and 2 (A: apples, O: oranges). SW if � Σα� 7r(s, α�)Q(s, α�) &gt; Σα� ˜7r(s, α�)Q(s, α) SLF otherwise S = { (5) More details about Q-learning, PHC, and PHCWoLF can be found in (Sutton and Barto, 1998; Bowling and Veloso, 2002). As discussed in sections 1 and 2, single-agent RL techniques, such as Q-learning, are not suitable for multi-agent RL. Nevertheless, despite its shortcomings Q-learning has been used successfully for multi-agent RL (Claus and Boutilier, 1998). Indeed, as we see in section 5, Q-learning can converge to the optimal policy for small state spaces. However, as the state space size increases the performance of Q-learning drops (compared to PHC and PHC-WoLF). 4 Domain and Experimental Setup Our domain is a resource allocation negotiation scenario. Two agents negotiate about how to share resources. For the sake of readability from now on we will refer to apples and oranges. The two agents have different goals. Also, they have human-like constraints of imperfect information about each other; they do not know each other’s reward function or</context>
</contexts>
<marker>Claus, Boutilier, 1998</marker>
<rawString>Caroline Claus and Craig Boutilier. 1998. The dynamics of reinforcement learning in cooperative multiagent systems. In Proc. of the National Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heriberto Cuay´ahuitl</author>
<author>Nina Dethlefs</author>
</authors>
<title>Hierarchical multiagent reinforcement learning for coordinating verbal and nonverbal actions in robots.</title>
<date>2012</date>
<booktitle>In Proc. of the ECAI Workshop on Machine Learning for Interactive Systems,</booktitle>
<location>Montpellier, France.</location>
<marker>Cuay´ahuitl, Dethlefs, 2012</marker>
<rawString>Heriberto Cuay´ahuitl and Nina Dethlefs. 2012. Hierarchical multiagent reinforcement learning for coordinating verbal and nonverbal actions in robots. In Proc. of the ECAI Workshop on Machine Learning for Interactive Systems, Montpellier, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucie Daubigney</author>
<author>Matthieu Geist</author>
<author>Senthilkumar Chandramohan</author>
<author>Olivier Pietquin</author>
</authors>
<title>A comprehensive reinforcement learning framework for dialogue management optimization.</title>
<date>2012</date>
<journal>IEEE Journal of Selected Topics in Signal Processing,</journal>
<volume>6</volume>
<issue>8</issue>
<contexts>
<context position="6706" citStr="Daubigney et al., 2012" startWordPosition="1071" endWordPosition="1074">y of situations (varying a large number of parameters). The paper is structured as follows. Section 2 presents related work. Section 3 provides a brief introduction to single-agent RL and multi-agent RL. Section 4 describes our negotiation domain and experimental setup. In section 5 we present our results. Finally, section 6 concludes and provides some ideas for future work. 2 Related Work Most research in RL for dialogue management has been done in the framework of slot-filling applications such as restaurant recommendations (Lemon et al., 2006; Thomson and Young, 2010; Gaˇsi´c et al., 2012; Daubigney et al., 2012), flight reservations (Henderson et al., 2008), sightseeing recommendations (Misu et al., 2010), appointment scheduling (Georgila et al., 2010), etc. RL has also been applied to question-answering (Misu et al., 2012), tutoring domains (Tetreault and Litman, 2008; Chi et al., 2011), and learning negotiation dialogue policies (Heeman, 2009; Georgila and Traum, 2011; Georgila, 2013). As mentioned in section 1, there are three main approaches to the problem of learning dialogue policies using RL. In the first approach, a SU is hand-crafted or learned from a small corpus of human-human or human-mac</context>
</contexts>
<marker>Daubigney, Geist, Chandramohan, Pietquin, 2012</marker>
<rawString>Lucie Daubigney, Matthieu Geist, Senthilkumar Chandramohan, and Olivier Pietquin. 2012. A comprehensive reinforcement learning framework for dialogue management optimization. IEEE Journal of Selected Topics in Signal Processing, 6(8):891–902.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yaakov Engel</author>
<author>Shie Mannor</author>
<author>Ron Meir</author>
</authors>
<title>Reinforcement learning with Gaussian processes.</title>
<date>2005</date>
<booktitle>In Proc. of the International Conference on Machine Learning,</booktitle>
<location>Bonn, Germany.</location>
<contexts>
<context position="9524" citStr="Engel et al., 2005" startWordPosition="1538" endWordPosition="1541"> with the final system. In practice this means that dialogue policies learned from such data could be far from optimal. The first experiment on learning via live interaction with human users (third approach) was reported by Singh et al. (2002). They used RL to help the system with two choices: how much initiative it should allow the user, and whether or not to confirm information provided by the user. Recently, learning of “full” dialogue policies (not just choices at specific points in the dialogue) via live interaction with human users has become possible with the use of Gaussian processes (Engel et al., 2005; Rasmussen and Williams, 2006). Typically learning a dialogue policy is a slow process requiring thousands of dialogues, hence the need for SUs. Gaussian processes have been shown to speed up learning. This fact together with easy access to a large number of human users through crowd-sourcing has allowed dialogue policy learning via live interaction with human users (Gaˇsi´c et al., 2011; Gaˇsi´c et al., 2013). Space constraints prevent us from providing an exhaustive list of previous work on using RL for dialogue management. Thus below we focus only on research that is directly related to ou</context>
</contexts>
<marker>Engel, Mannor, Meir, 2005</marker>
<rawString>Yaakov Engel, Shie Mannor, and Ron Meir. 2005. Reinforcement learning with Gaussian processes. In Proc. of the International Conference on Machine Learning, Bonn, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael S English</author>
<author>Peter A Heeman</author>
</authors>
<title>Learning mixed initiative dialogue strategies by using reinforcement learning on both conversants.</title>
<date>2005</date>
<booktitle>In Proc. of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Vancouver, Canada.</location>
<contexts>
<context position="11871" citStr="English and Heeman (2005)" startWordPosition="1937" endWordPosition="1940">that continuously change their behavior because they are also learning at the same time. Therefore the environment is no longer stationary and single-agent RL techniques do not work well or do not work at all. We are particularly interested in the work of Bowling and Veloso (2002) who proposed the PHC and PHC-WoLF algorithms that we use in this paper. We chose these two algorithms because, unlike other multi-agent RL methods (Littman, 1994; Hu and Wellman, 1998), they do not make assumptions that do not always hold and do not require quadratic or linear programming that does not always scale. English and Heeman (2005) were the first in the dialogue community to explore the idea of concurrent learning of dialogue policies. However, English and Heeman (2005) did not use multiagent RL but only standard single-agent RL, in particular an on-policy Monte Carlo method (Sutton and Barto, 1998). But single-agent RL techniques are not well suited for concurrent learning where each agent is trained against a continuously changing environment. Indeed, English and Heeman (2005) reported problems with convergence. Chandramohan et al. (2012) proposed a framework for co-adaptation of the dialogue policy and the SU using s</context>
<context position="23067" citStr="English and Heeman, 2005" startWordPosition="3938" endWordPosition="3941">her agent’s offer. Table 3 shows the state and action space sizes for different numbers of apples and oranges to be shared used in our experiments below. The number of actions includes the acceptance of an offer. Table 3 also shows the number of state-action pairs (Q-values). As we will see in section 5, even though the number of states for each agent is not large, it takes many iterations and high exploration rates for convergence due to the fact that both agents are learning at the same time and the assumption of interacting with a stationary environment no longer holds. For comparison, in (English and Heeman, 2005) the state specification for each agent included 5 binary variables resulting in 32 possible states. English and Heeman (2005) kept track of whether there was an offer on the table but not of the actual value of the offer. For our task it is essential to keep track of the offer values, which of course results in much larger state spaces. Also, in (English and Heeman, 2005) there were 5 possible actions resulting in 160 state-action pairs. Our state and action spaces are much larger and furthermore we explore the effect of different state and action space sizes on convergence. During learning t</context>
<context position="25745" citStr="English and Heeman (2005)" startWordPosition="4452" endWordPosition="4455">erred to as PHC-LF, we set S to be equal to SLF (also used for PHC-WoLF). So unlike PHC-WoLF, PHC-W and PHC-LF do not use a variable learning rate. PHC-W always learns slowly and PHC-LF always learns fast. In all the above cases, training stops after 5 epochs. Then we test the learned policies against each other for one more epoch the size of which is the same as the size of the epochs used for training. For example, if the policies were learned for 5 epochs with each epoch containing 25,000 episodes, then for testing the two policies will interact for another 25,000 episodes. For comparison, English and Heeman (2005) had their agents interact for 5,000 dialogues during testing. To ensure that the policies do not converge by chance, we run the training and test sessions 20 times each and we report averages. Thus all results presented in section 5 are averages of 20 runs. 5 Results Given that Agent 1 is more interested in apples and Agent 2 cares more about oranges, the maximum total utility solution would be the case where each agent offers to get all the fruits it cares about and to give its interlocutor all the fruits it does not care about, and the other agent accepts this offer. Thus, when converging t</context>
</contexts>
<marker>English, Heeman, 2005</marker>
<rawString>Michael S. English and Peter A. Heeman. 2005. Learning mixed initiative dialogue strategies by using reinforcement learning on both conversants. In Proc. of the Conference on Empirical Methods in Natural Language Processing, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gasi´c</author>
<author>Filip Jurc´ıcek</author>
<author>Blaise Thomson</author>
<author>Kai Yu</author>
<author>Steve Young</author>
</authors>
<title>On-line policy optimisation of spoken dialogue systems via live interaction with human subjects.</title>
<date>2011</date>
<booktitle>In Proc. of the IEEE Automatic Speech Recognition and Understanding Workshop,</booktitle>
<location>Big Island, Hawaii, USA.</location>
<marker>Gasi´c, Jurc´ıcek, Thomson, Yu, Young, 2011</marker>
<rawString>M. Ga&amp;quot;si´c, Filip Jur&amp;quot;c´ı&amp;quot;cek, Blaise Thomson, Kai Yu, and Steve Young. 2011. On-line policy optimisation of spoken dialogue systems via live interaction with human subjects. In Proc. of the IEEE Automatic Speech Recognition and Understanding Workshop, Big Island, Hawaii, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Milica Gasi´c</author>
<author>Matthew Henderson</author>
<author>Blaise Thomson</author>
<author>Pirros Tsiakoulis</author>
<author>Steve Young</author>
</authors>
<title>Policy optimisation of POMDP-based dialogue systems without state space compression.</title>
<date>2012</date>
<booktitle>In Proc. of the IEEE Workshop on Spoken Language Technology,</booktitle>
<location>Miami, Florida, USA.</location>
<marker>Gasi´c, Henderson, Thomson, Tsiakoulis, Young, 2012</marker>
<rawString>Milica Ga&amp;quot;si´c, Matthew Henderson, Blaise Thomson, Pirros Tsiakoulis, and Steve Young. 2012. Policy optimisation of POMDP-based dialogue systems without state space compression. In Proc. of the IEEE Workshop on Spoken Language Technology, Miami, Florida, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gasi´c</author>
<author>C Breslin</author>
<author>M Henderson</author>
<author>D Kim</author>
<author>M Szummer</author>
<author>B Thomson</author>
<author>P Tsiakoulis</author>
<author>S Young</author>
</authors>
<title>On-line policy optimisation of Bayesian spoken dialogue systems via human interaction.</title>
<date>2013</date>
<booktitle>In Proc. of the International Conference on Acoustics, Speech and Signal Processing,</booktitle>
<location>Vancouver, Canada.</location>
<marker>Gasi´c, Breslin, Henderson, Kim, Szummer, Thomson, Tsiakoulis, Young, 2013</marker>
<rawString>M. Ga&amp;quot;si´c, C. Breslin, M. Henderson, D. Kim, M. Szummer, B. Thomson, P. Tsiakoulis, and S. Young. 2013. On-line policy optimisation of Bayesian spoken dialogue systems via human interaction. In Proc. of the International Conference on Acoustics, Speech and Signal Processing, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kallirroi Georgila</author>
<author>David Traum</author>
</authors>
<title>Reinforcement learning of argumentation dialogue policies in negotiation.</title>
<date>2011</date>
<booktitle>In Proc. of Interspeech,</booktitle>
<location>Florence, Italy.</location>
<contexts>
<context position="7071" citStr="Georgila and Traum, 2011" startWordPosition="1124" endWordPosition="1127"> future work. 2 Related Work Most research in RL for dialogue management has been done in the framework of slot-filling applications such as restaurant recommendations (Lemon et al., 2006; Thomson and Young, 2010; Gaˇsi´c et al., 2012; Daubigney et al., 2012), flight reservations (Henderson et al., 2008), sightseeing recommendations (Misu et al., 2010), appointment scheduling (Georgila et al., 2010), etc. RL has also been applied to question-answering (Misu et al., 2012), tutoring domains (Tetreault and Litman, 2008; Chi et al., 2011), and learning negotiation dialogue policies (Heeman, 2009; Georgila and Traum, 2011; Georgila, 2013). As mentioned in section 1, there are three main approaches to the problem of learning dialogue policies using RL. In the first approach, a SU is hand-crafted or learned from a small corpus of human-human or human-machine dialogues. Then the dialogue policy can be learned by having the system interact with the SU for a large number of dialogues (usually thousands of dialogues). Depending on the application, building a realistic SU can be just as difficult as building a good dialogue policy. Furthermore, it is not clear what constitutes a good SU for dialogue policy learning. </context>
<context position="13659" citStr="Georgila and Traum (2011)" startWordPosition="2228" endWordPosition="2231">filling. English and Heeman (2005) learned negotiation policies for a furniture layout task. Then Heeman (2009) extended this work by experimenting with different representations of the RL state in the same domain (this time learning against a hand-crafted SU). In both cases, to reduce the search space, the RL state included only information about e.g., whether there was a pending proposal rather than the actual value of this proposal. Paruchuri et al. (2009) performed a theoretical study on how Partially Observable Markov Decision Processes (POMDPs) can be applied to negotiation domains. 502 Georgila and Traum (2011) built argumentation dialogue policies for negotiation against users of different cultural norms in a one-issue negotiation scenario. To learn these policies they trained SUs on a spoken dialogue corpus in a florist-grocer negotiation domain, and then tweaked these SUs towards a particular cultural norm using handcrafted rules. Georgila (2013) learned argumentation dialogue policies from a simulated corpus in a two-issue negotiation scenario (organizing a party). Finally, Nouri et al. (2012) used IRL to learn a model for cultural decision-making in a simple negotiation game (the Ultimatum Game</context>
</contexts>
<marker>Georgila, Traum, 2011</marker>
<rawString>Kallirroi Georgila and David Traum. 2011. Reinforcement learning of argumentation dialogue policies in negotiation. In Proc. of Interspeech, Florence, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kallirroi Georgila</author>
<author>James Henderson</author>
<author>Oliver Lemon</author>
</authors>
<title>User simulation for spoken dialogue systems: Learning and evaluation.</title>
<date>2006</date>
<booktitle>In Proc. of Interspeech,</booktitle>
<location>Pittsburgh, Pennsylvania, USA.</location>
<contexts>
<context position="1966" citStr="Georgila et al., 2006" startWordPosition="288" endWordPosition="291">m given a particular dialogue state (i.e., dialogue context). Building a dialogue policy can be a challenging task especially for complex applications. For this reason, recently much attention has been drawn to machine learning approaches to dialogue management and in particular Reinforcement Learning (RL) of dialogue policies (Williams and Young, 2007; Rieser et al., 2011; Jurˇc´ıˇcek et al., 2012). Typically there are three main approaches to the problem of learning dialogue policies using RL: (1) learn against a simulated user (SU), i.e., a model that simulates the behavior of a real user (Georgila et al., 2006; Schatzmann et al., 2006); (2) learn directly from a corpus (Henderson et al., 2008; Li et al., 2009); or (3) learn via live interaction with human users (Singh et al., 2002; Gaˇsi´c et al., 2011; Gaˇsi´c et al., 2013). We propose a fourth approach: concurrent learning of the system policy and the SU policy using multi-agent RL techniques. Both agents are trained simultaneously and there is no need for building a SU separately or having access to a corpus.1 As we discuss below, concurrent learning could potentially be used for learning via live interaction with human users. Moreover, for nego</context>
</contexts>
<marker>Georgila, Henderson, Lemon, 2006</marker>
<rawString>Kallirroi Georgila, James Henderson, and Oliver Lemon. 2006. User simulation for spoken dialogue systems: Learning and evaluation. In Proc. of Interspeech, Pittsburgh, Pennsylvania, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kallirroi Georgila</author>
<author>Maria K Wolters</author>
<author>Johanna D Moore</author>
</authors>
<title>Learning dialogue strategies from older and younger simulated users.</title>
<date>2010</date>
<booktitle>In Proc. of the Annual SIGdial Meeting on Discourse and Dialogue,</booktitle>
<location>Tokyo, Japan.</location>
<contexts>
<context position="6849" citStr="Georgila et al., 2010" startWordPosition="1091" endWordPosition="1094">a brief introduction to single-agent RL and multi-agent RL. Section 4 describes our negotiation domain and experimental setup. In section 5 we present our results. Finally, section 6 concludes and provides some ideas for future work. 2 Related Work Most research in RL for dialogue management has been done in the framework of slot-filling applications such as restaurant recommendations (Lemon et al., 2006; Thomson and Young, 2010; Gaˇsi´c et al., 2012; Daubigney et al., 2012), flight reservations (Henderson et al., 2008), sightseeing recommendations (Misu et al., 2010), appointment scheduling (Georgila et al., 2010), etc. RL has also been applied to question-answering (Misu et al., 2012), tutoring domains (Tetreault and Litman, 2008; Chi et al., 2011), and learning negotiation dialogue policies (Heeman, 2009; Georgila and Traum, 2011; Georgila, 2013). As mentioned in section 1, there are three main approaches to the problem of learning dialogue policies using RL. In the first approach, a SU is hand-crafted or learned from a small corpus of human-human or human-machine dialogues. Then the dialogue policy can be learned by having the system interact with the SU for a large number of dialogues (usually thou</context>
</contexts>
<marker>Georgila, Wolters, Moore, 2010</marker>
<rawString>Kallirroi Georgila, Maria K. Wolters, and Johanna D. Moore. 2010. Learning dialogue strategies from older and younger simulated users. In Proc. of the Annual SIGdial Meeting on Discourse and Dialogue, Tokyo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kallirroi Georgila</author>
</authors>
<title>Reinforcement learning of two-issue negotiation dialogue policies.</title>
<date>2013</date>
<booktitle>In Proc. of the Annual SIGdial Meeting on Discourse and Dialogue,</booktitle>
<location>Metz, France.</location>
<contexts>
<context position="7088" citStr="Georgila, 2013" startWordPosition="1128" endWordPosition="1129">rk Most research in RL for dialogue management has been done in the framework of slot-filling applications such as restaurant recommendations (Lemon et al., 2006; Thomson and Young, 2010; Gaˇsi´c et al., 2012; Daubigney et al., 2012), flight reservations (Henderson et al., 2008), sightseeing recommendations (Misu et al., 2010), appointment scheduling (Georgila et al., 2010), etc. RL has also been applied to question-answering (Misu et al., 2012), tutoring domains (Tetreault and Litman, 2008; Chi et al., 2011), and learning negotiation dialogue policies (Heeman, 2009; Georgila and Traum, 2011; Georgila, 2013). As mentioned in section 1, there are three main approaches to the problem of learning dialogue policies using RL. In the first approach, a SU is hand-crafted or learned from a small corpus of human-human or human-machine dialogues. Then the dialogue policy can be learned by having the system interact with the SU for a large number of dialogues (usually thousands of dialogues). Depending on the application, building a realistic SU can be just as difficult as building a good dialogue policy. Furthermore, it is not clear what constitutes a good SU for dialogue policy learning. Should the SU res</context>
<context position="14004" citStr="Georgila (2013)" startWordPosition="2280" endWordPosition="2281"> whether there was a pending proposal rather than the actual value of this proposal. Paruchuri et al. (2009) performed a theoretical study on how Partially Observable Markov Decision Processes (POMDPs) can be applied to negotiation domains. 502 Georgila and Traum (2011) built argumentation dialogue policies for negotiation against users of different cultural norms in a one-issue negotiation scenario. To learn these policies they trained SUs on a spoken dialogue corpus in a florist-grocer negotiation domain, and then tweaked these SUs towards a particular cultural norm using handcrafted rules. Georgila (2013) learned argumentation dialogue policies from a simulated corpus in a two-issue negotiation scenario (organizing a party). Finally, Nouri et al. (2012) used IRL to learn a model for cultural decision-making in a simple negotiation game (the Ultimatum Game). 3 Single-Agent vs. Multi-Agent Reinforcement Learning Reinforcement Learning (RL) is a machine learning technique used to learn the policy of an agent, i.e., which action the agent should perform given its current state (Sutton and Barto, 1998). The goal of an RL-based agent is to maximize the reward it gets during an interaction. Because i</context>
</contexts>
<marker>Georgila, 2013</marker>
<rawString>Kallirroi Georgila. 2013. Reinforcement learning of two-issue negotiation dialogue policies. In Proc. of the Annual SIGdial Meeting on Discourse and Dialogue, Metz, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter A Heeman</author>
</authors>
<title>Representing the reinforcement learning state in a negotiation dialogue.</title>
<date>2009</date>
<booktitle>In Proc. of the IEEE Automatic Speech Recognition and Understanding Workshop,</booktitle>
<location>Merano, Italy.</location>
<contexts>
<context position="7045" citStr="Heeman, 2009" startWordPosition="1122" endWordPosition="1123">some ideas for future work. 2 Related Work Most research in RL for dialogue management has been done in the framework of slot-filling applications such as restaurant recommendations (Lemon et al., 2006; Thomson and Young, 2010; Gaˇsi´c et al., 2012; Daubigney et al., 2012), flight reservations (Henderson et al., 2008), sightseeing recommendations (Misu et al., 2010), appointment scheduling (Georgila et al., 2010), etc. RL has also been applied to question-answering (Misu et al., 2012), tutoring domains (Tetreault and Litman, 2008; Chi et al., 2011), and learning negotiation dialogue policies (Heeman, 2009; Georgila and Traum, 2011; Georgila, 2013). As mentioned in section 1, there are three main approaches to the problem of learning dialogue policies using RL. In the first approach, a SU is hand-crafted or learned from a small corpus of human-human or human-machine dialogues. Then the dialogue policy can be learned by having the system interact with the SU for a large number of dialogues (usually thousands of dialogues). Depending on the application, building a realistic SU can be just as difficult as building a good dialogue policy. Furthermore, it is not clear what constitutes a good SU for </context>
<context position="13145" citStr="Heeman (2009)" startWordPosition="2145" endWordPosition="2147">IRL) (Abbeel and Ng, 2004) to a corpus in order to learn the reward functions of both the system and the SU. Furthermore, Cuay´ahuitl and Dethlefs (2012) used hierarchical multi-agent RL for co-ordinating the verbal and non-verbal actions of a robot. Cuay´ahuitl and Dethlefs (2012) did not use PHC or PHCWoLF and did not compare against single-agent RL methods. With regard to using RL for learning negotiation policies, the amount of research that has been performed is very limited compared to slot-filling. English and Heeman (2005) learned negotiation policies for a furniture layout task. Then Heeman (2009) extended this work by experimenting with different representations of the RL state in the same domain (this time learning against a hand-crafted SU). In both cases, to reduce the search space, the RL state included only information about e.g., whether there was a pending proposal rather than the actual value of this proposal. Paruchuri et al. (2009) performed a theoretical study on how Partially Observable Markov Decision Processes (POMDPs) can be applied to negotiation domains. 502 Georgila and Traum (2011) built argumentation dialogue policies for negotiation against users of different cult</context>
</contexts>
<marker>Heeman, 2009</marker>
<rawString>Peter A. Heeman. 2009. Representing the reinforcement learning state in a negotiation dialogue. In Proc. of the IEEE Automatic Speech Recognition and Understanding Workshop, Merano, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Henderson</author>
<author>Oliver Lemon</author>
<author>Kallirroi Georgila</author>
</authors>
<title>Hybrid reinforcement/supervised learning of dialogue policies from fixed datasets.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>4</issue>
<contexts>
<context position="2050" citStr="Henderson et al., 2008" startWordPosition="302" endWordPosition="305">olicy can be a challenging task especially for complex applications. For this reason, recently much attention has been drawn to machine learning approaches to dialogue management and in particular Reinforcement Learning (RL) of dialogue policies (Williams and Young, 2007; Rieser et al., 2011; Jurˇc´ıˇcek et al., 2012). Typically there are three main approaches to the problem of learning dialogue policies using RL: (1) learn against a simulated user (SU), i.e., a model that simulates the behavior of a real user (Georgila et al., 2006; Schatzmann et al., 2006); (2) learn directly from a corpus (Henderson et al., 2008; Li et al., 2009); or (3) learn via live interaction with human users (Singh et al., 2002; Gaˇsi´c et al., 2011; Gaˇsi´c et al., 2013). We propose a fourth approach: concurrent learning of the system policy and the SU policy using multi-agent RL techniques. Both agents are trained simultaneously and there is no need for building a SU separately or having access to a corpus.1 As we discuss below, concurrent learning could potentially be used for learning via live interaction with human users. Moreover, for negotiation in particular there is one more reason in favor of concurrent learning as op</context>
<context position="6752" citStr="Henderson et al., 2008" startWordPosition="1078" endWordPosition="1081">ameters). The paper is structured as follows. Section 2 presents related work. Section 3 provides a brief introduction to single-agent RL and multi-agent RL. Section 4 describes our negotiation domain and experimental setup. In section 5 we present our results. Finally, section 6 concludes and provides some ideas for future work. 2 Related Work Most research in RL for dialogue management has been done in the framework of slot-filling applications such as restaurant recommendations (Lemon et al., 2006; Thomson and Young, 2010; Gaˇsi´c et al., 2012; Daubigney et al., 2012), flight reservations (Henderson et al., 2008), sightseeing recommendations (Misu et al., 2010), appointment scheduling (Georgila et al., 2010), etc. RL has also been applied to question-answering (Misu et al., 2012), tutoring domains (Tetreault and Litman, 2008; Chi et al., 2011), and learning negotiation dialogue policies (Heeman, 2009; Georgila and Traum, 2011; Georgila, 2013). As mentioned in section 1, there are three main approaches to the problem of learning dialogue policies using RL. In the first approach, a SU is hand-crafted or learned from a small corpus of human-human or human-machine dialogues. Then the dialogue policy can b</context>
<context position="8158" citStr="Henderson et al. (2008)" startWordPosition="1306" endWordPosition="1309"> as difficult as building a good dialogue policy. Furthermore, it is not clear what constitutes a good SU for dialogue policy learning. Should the SU resemble real user behavior as closely as possible, or should it exhibit some degree of randomness to explore a variety of interaction patterns? Despite much research on the issue, these are still open questions (Schatzmann et al., 2006; Ai and Litman, 2008; Pietquin and Hastie, 2013). In the second approach, no SUs are required. Instead the dialogue policy is learned directly from a corpus of human-human or human-machine dialogues. For example, Henderson et al. (2008) used a combination of RL and supervised learning to learn a dialogue policy in a flight reservation domain, whereas Li et al. (2009) used Least-Squares Policy Iteration (Lagoudakis and Parr, 2003), an RL-based technique that can learn directly from corpora, in a voice dialer application. However, collecting such corpora is not trivial, especially in new domains. Typically, data are collected in a Wizard-of-Oz setup where human users think that they interact with a system while in fact they interact with a human pretending to be the system, or by having human users interact with a preliminary </context>
</contexts>
<marker>Henderson, Lemon, Georgila, 2008</marker>
<rawString>James Henderson, Oliver Lemon, and Kallirroi Georgila. 2008. Hybrid reinforcement/supervised learning of dialogue policies from fixed datasets. Computational Linguistics, 34(4):487–511.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Junling Hu</author>
<author>Michael P Wellman</author>
</authors>
<title>Multiagent reinforcement learning: Theoretical framework and an algorithm.</title>
<date>1998</date>
<booktitle>In Proc. of the International Conference on Machine Learning,</booktitle>
<location>Madison, Wisconsin, USA.</location>
<contexts>
<context position="11114" citStr="Hu and Wellman, 1998" startWordPosition="1806" endWordPosition="1809">ational. Other approaches account for changes in user goals (Ma, 2013). In either case, one can build a user simulation model that is the average of different user behaviors or learn a policy from a corpus that contains a variety of interaction patterns, and thus safely assume that single-agent RL techniques will work. However, in the latter case if the behavior of the user changes significantly over time then the assumption that the environment is stationary will no longer hold. There has been a lot of research on multi-agent RL in the optimal control and robotics communities (Littman, 1994; Hu and Wellman, 1998; Busoniu et al., 2008). Here two or more agents learn simultaneously. Thus the environment of an agent is one or more other agents that continuously change their behavior because they are also learning at the same time. Therefore the environment is no longer stationary and single-agent RL techniques do not work well or do not work at all. We are particularly interested in the work of Bowling and Veloso (2002) who proposed the PHC and PHC-WoLF algorithms that we use in this paper. We chose these two algorithms because, unlike other multi-agent RL methods (Littman, 1994; Hu and Wellman, 1998), </context>
</contexts>
<marker>Hu, Wellman, 1998</marker>
<rawString>Junling Hu and Michael P. Wellman. 1998. Multiagent reinforcement learning: Theoretical framework and an algorithm. In Proc. of the International Conference on Machine Learning, Madison, Wisconsin, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Filip Jurc´ıcek</author>
<author>Blaise Thomson</author>
<author>Steve Young</author>
</authors>
<title>Reinforcement learning for parameter estimation in statistical spoken dialogue systems.</title>
<date>2012</date>
<journal>Computer Speech and Language,</journal>
<volume>26</volume>
<issue>3</issue>
<marker>Jurc´ıcek, Thomson, Young, 2012</marker>
<rawString>Filip Jur&amp;quot;c´ı&amp;quot;cek, Blaise Thomson, and Steve Young. 2012. Reinforcement learning for parameter estimation in statistical spoken dialogue systems. Computer Speech and Language, 26(3):168–192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michail G Lagoudakis</author>
<author>Ronald Parr</author>
</authors>
<title>Leastsquares policy iteration.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>4--1107</pages>
<contexts>
<context position="8355" citStr="Lagoudakis and Parr, 2003" startWordPosition="1338" endWordPosition="1341">ssible, or should it exhibit some degree of randomness to explore a variety of interaction patterns? Despite much research on the issue, these are still open questions (Schatzmann et al., 2006; Ai and Litman, 2008; Pietquin and Hastie, 2013). In the second approach, no SUs are required. Instead the dialogue policy is learned directly from a corpus of human-human or human-machine dialogues. For example, Henderson et al. (2008) used a combination of RL and supervised learning to learn a dialogue policy in a flight reservation domain, whereas Li et al. (2009) used Least-Squares Policy Iteration (Lagoudakis and Parr, 2003), an RL-based technique that can learn directly from corpora, in a voice dialer application. However, collecting such corpora is not trivial, especially in new domains. Typically, data are collected in a Wizard-of-Oz setup where human users think that they interact with a system while in fact they interact with a human pretending to be the system, or by having human users interact with a preliminary version of the dialogue system. In both cases the resulting interactions are expected to be quite dif501 ferent from the interactions of human users with the final system. In practice this means th</context>
</contexts>
<marker>Lagoudakis, Parr, 2003</marker>
<rawString>Michail G. Lagoudakis and Ronald Parr. 2003. Leastsquares policy iteration. Journal of Machine Learning Research, 4:1107–1149.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oliver Lemon</author>
<author>Kallirroi Georgila</author>
<author>James Henderson</author>
</authors>
<title>Evaluating effectiveness and portability of reinforcement learned dialogue strategies with real users: The TALK TownInfo evaluation.</title>
<date>2006</date>
<booktitle>In Proc. of the IEEE Workshop on Spoken Language Technology,</booktitle>
<location>Palm Beach, Aruba.</location>
<contexts>
<context position="6634" citStr="Lemon et al., 2006" startWordPosition="1059" endWordPosition="1062">t study that compares Q-learning, PHC, and PHCWoLF in such a variety of situations (varying a large number of parameters). The paper is structured as follows. Section 2 presents related work. Section 3 provides a brief introduction to single-agent RL and multi-agent RL. Section 4 describes our negotiation domain and experimental setup. In section 5 we present our results. Finally, section 6 concludes and provides some ideas for future work. 2 Related Work Most research in RL for dialogue management has been done in the framework of slot-filling applications such as restaurant recommendations (Lemon et al., 2006; Thomson and Young, 2010; Gaˇsi´c et al., 2012; Daubigney et al., 2012), flight reservations (Henderson et al., 2008), sightseeing recommendations (Misu et al., 2010), appointment scheduling (Georgila et al., 2010), etc. RL has also been applied to question-answering (Misu et al., 2012), tutoring domains (Tetreault and Litman, 2008; Chi et al., 2011), and learning negotiation dialogue policies (Heeman, 2009; Georgila and Traum, 2011; Georgila, 2013). As mentioned in section 1, there are three main approaches to the problem of learning dialogue policies using RL. In the first approach, a SU is</context>
</contexts>
<marker>Lemon, Georgila, Henderson, 2006</marker>
<rawString>Oliver Lemon, Kallirroi Georgila, and James Henderson. 2006. Evaluating effectiveness and portability of reinforcement learned dialogue strategies with real users: The TALK TownInfo evaluation. In Proc. of the IEEE Workshop on Spoken Language Technology, Palm Beach, Aruba.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lihong Li</author>
<author>Jason D Williams</author>
<author>Suhrid Balakrishnan</author>
</authors>
<title>Reinforcement learning for dialog management using least-squares policy iteration and fast feature selection.</title>
<date>2009</date>
<booktitle>In Proc. of Interspeech,</booktitle>
<location>Brighton, United Kingdom.</location>
<contexts>
<context position="2068" citStr="Li et al., 2009" startWordPosition="306" endWordPosition="309">ng task especially for complex applications. For this reason, recently much attention has been drawn to machine learning approaches to dialogue management and in particular Reinforcement Learning (RL) of dialogue policies (Williams and Young, 2007; Rieser et al., 2011; Jurˇc´ıˇcek et al., 2012). Typically there are three main approaches to the problem of learning dialogue policies using RL: (1) learn against a simulated user (SU), i.e., a model that simulates the behavior of a real user (Georgila et al., 2006; Schatzmann et al., 2006); (2) learn directly from a corpus (Henderson et al., 2008; Li et al., 2009); or (3) learn via live interaction with human users (Singh et al., 2002; Gaˇsi´c et al., 2011; Gaˇsi´c et al., 2013). We propose a fourth approach: concurrent learning of the system policy and the SU policy using multi-agent RL techniques. Both agents are trained simultaneously and there is no need for building a SU separately or having access to a corpus.1 As we discuss below, concurrent learning could potentially be used for learning via live interaction with human users. Moreover, for negotiation in particular there is one more reason in favor of concurrent learning as opposed to learning </context>
<context position="8291" citStr="Li et al. (2009)" startWordPosition="1330" endWordPosition="1333">ld the SU resemble real user behavior as closely as possible, or should it exhibit some degree of randomness to explore a variety of interaction patterns? Despite much research on the issue, these are still open questions (Schatzmann et al., 2006; Ai and Litman, 2008; Pietquin and Hastie, 2013). In the second approach, no SUs are required. Instead the dialogue policy is learned directly from a corpus of human-human or human-machine dialogues. For example, Henderson et al. (2008) used a combination of RL and supervised learning to learn a dialogue policy in a flight reservation domain, whereas Li et al. (2009) used Least-Squares Policy Iteration (Lagoudakis and Parr, 2003), an RL-based technique that can learn directly from corpora, in a voice dialer application. However, collecting such corpora is not trivial, especially in new domains. Typically, data are collected in a Wizard-of-Oz setup where human users think that they interact with a system while in fact they interact with a human pretending to be the system, or by having human users interact with a preliminary version of the dialogue system. In both cases the resulting interactions are expected to be quite dif501 ferent from the interactions</context>
</contexts>
<marker>Li, Williams, Balakrishnan, 2009</marker>
<rawString>Lihong Li, Jason D. Williams, and Suhrid Balakrishnan. 2009. Reinforcement learning for dialog management using least-squares policy iteration and fast feature selection. In Proc. of Interspeech, Brighton, United Kingdom.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael L Littman</author>
</authors>
<title>Markov games as a framework for multi-agent reinforcement learning.</title>
<date>1994</date>
<booktitle>In Proc. of the International Conference on Machine Learning,</booktitle>
<location>New Brunswick, New Jersey, USA.</location>
<contexts>
<context position="11092" citStr="Littman, 1994" startWordPosition="1804" endWordPosition="1805">f the user is rational. Other approaches account for changes in user goals (Ma, 2013). In either case, one can build a user simulation model that is the average of different user behaviors or learn a policy from a corpus that contains a variety of interaction patterns, and thus safely assume that single-agent RL techniques will work. However, in the latter case if the behavior of the user changes significantly over time then the assumption that the environment is stationary will no longer hold. There has been a lot of research on multi-agent RL in the optimal control and robotics communities (Littman, 1994; Hu and Wellman, 1998; Busoniu et al., 2008). Here two or more agents learn simultaneously. Thus the environment of an agent is one or more other agents that continuously change their behavior because they are also learning at the same time. Therefore the environment is no longer stationary and single-agent RL techniques do not work well or do not work at all. We are particularly interested in the work of Bowling and Veloso (2002) who proposed the PHC and PHC-WoLF algorithms that we use in this paper. We chose these two algorithms because, unlike other multi-agent RL methods (Littman, 1994; H</context>
</contexts>
<marker>Littman, 1994</marker>
<rawString>Michael L. Littman. 1994. Markov games as a framework for multi-agent reinforcement learning. In Proc. of the International Conference on Machine Learning, New Brunswick, New Jersey, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yi Ma</author>
</authors>
<title>User goal change model for spoken dialog state tracking.</title>
<date>2013</date>
<booktitle>In Proc. of the NAACL-HLT Student Research Workshop,</booktitle>
<location>Atlanta, Georgia, USA.</location>
<contexts>
<context position="10564" citStr="Ma, 2013" startWordPosition="1712" endWordPosition="1713">ts prevent us from providing an exhaustive list of previous work on using RL for dialogue management. Thus below we focus only on research that is directly related to our work, specifically research on concurrent learning of the policies of multiple agents, and the application of RL to negotiation domains. So far research on RL in the dialogue community has focused on using single-agent RL techniques where the stationary environment is the user. Most approaches assume that the user goal is fixed and that the behavior of the user is rational. Other approaches account for changes in user goals (Ma, 2013). In either case, one can build a user simulation model that is the average of different user behaviors or learn a policy from a corpus that contains a variety of interaction patterns, and thus safely assume that single-agent RL techniques will work. However, in the latter case if the behavior of the user changes significantly over time then the assumption that the environment is stationary will no longer hold. There has been a lot of research on multi-agent RL in the optimal control and robotics communities (Littman, 1994; Hu and Wellman, 1998; Busoniu et al., 2008). Here two or more agents l</context>
</contexts>
<marker>Ma, 2013</marker>
<rawString>Yi Ma. 2013. User goal change model for spoken dialog state tracking. In Proc. of the NAACL-HLT Student Research Workshop, Atlanta, Georgia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Teruhisa Misu</author>
<author>Komei Sugiura</author>
<author>Kiyonori Ohtake</author>
<author>Chiori Hori</author>
<author>Hideki Kashioka</author>
<author>Hisashi Kawai</author>
<author>Satoshi Nakamura</author>
</authors>
<title>Modeling spoken decision making dialogue and optimization of its dialogue strategy.</title>
<date>2010</date>
<booktitle>In Proc. of the Annual SIGdial Meeting on Discourse and Dialogue,</booktitle>
<location>Tokyo, Japan.</location>
<contexts>
<context position="6801" citStr="Misu et al., 2010" startWordPosition="1085" endWordPosition="1088">2 presents related work. Section 3 provides a brief introduction to single-agent RL and multi-agent RL. Section 4 describes our negotiation domain and experimental setup. In section 5 we present our results. Finally, section 6 concludes and provides some ideas for future work. 2 Related Work Most research in RL for dialogue management has been done in the framework of slot-filling applications such as restaurant recommendations (Lemon et al., 2006; Thomson and Young, 2010; Gaˇsi´c et al., 2012; Daubigney et al., 2012), flight reservations (Henderson et al., 2008), sightseeing recommendations (Misu et al., 2010), appointment scheduling (Georgila et al., 2010), etc. RL has also been applied to question-answering (Misu et al., 2012), tutoring domains (Tetreault and Litman, 2008; Chi et al., 2011), and learning negotiation dialogue policies (Heeman, 2009; Georgila and Traum, 2011; Georgila, 2013). As mentioned in section 1, there are three main approaches to the problem of learning dialogue policies using RL. In the first approach, a SU is hand-crafted or learned from a small corpus of human-human or human-machine dialogues. Then the dialogue policy can be learned by having the system interact with the </context>
</contexts>
<marker>Misu, Sugiura, Ohtake, Hori, Kashioka, Kawai, Nakamura, 2010</marker>
<rawString>Teruhisa Misu, Komei Sugiura, Kiyonori Ohtake, Chiori Hori, Hideki Kashioka, Hisashi Kawai, and Satoshi Nakamura. 2010. Modeling spoken decision making dialogue and optimization of its dialogue strategy. In Proc. of the Annual SIGdial Meeting on Discourse and Dialogue, Tokyo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Teruhisa Misu</author>
<author>Kallirroi Georgila</author>
<author>Anton Leuski</author>
<author>David Traum</author>
</authors>
<title>Reinforcement learning of question-answering dialogue policies for virtual museum guides.</title>
<date>2012</date>
<booktitle>In Proc. of the Annual SIGdial Meeting on Discourse and Dialogue,</booktitle>
<location>Seoul, South</location>
<contexts>
<context position="6922" citStr="Misu et al., 2012" startWordPosition="1103" endWordPosition="1106">es our negotiation domain and experimental setup. In section 5 we present our results. Finally, section 6 concludes and provides some ideas for future work. 2 Related Work Most research in RL for dialogue management has been done in the framework of slot-filling applications such as restaurant recommendations (Lemon et al., 2006; Thomson and Young, 2010; Gaˇsi´c et al., 2012; Daubigney et al., 2012), flight reservations (Henderson et al., 2008), sightseeing recommendations (Misu et al., 2010), appointment scheduling (Georgila et al., 2010), etc. RL has also been applied to question-answering (Misu et al., 2012), tutoring domains (Tetreault and Litman, 2008; Chi et al., 2011), and learning negotiation dialogue policies (Heeman, 2009; Georgila and Traum, 2011; Georgila, 2013). As mentioned in section 1, there are three main approaches to the problem of learning dialogue policies using RL. In the first approach, a SU is hand-crafted or learned from a small corpus of human-human or human-machine dialogues. Then the dialogue policy can be learned by having the system interact with the SU for a large number of dialogues (usually thousands of dialogues). Depending on the application, building a realistic S</context>
</contexts>
<marker>Misu, Georgila, Leuski, Traum, 2012</marker>
<rawString>Teruhisa Misu, Kallirroi Georgila, Anton Leuski, and David Traum. 2012. Reinforcement learning of question-answering dialogue policies for virtual museum guides. In Proc. of the Annual SIGdial Meeting on Discourse and Dialogue, Seoul, South Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elnaz Nouri</author>
<author>Kallirroi Georgila</author>
<author>David Traum</author>
</authors>
<title>A cultural decision-making model for negotiation based on inverse reinforcement learning.</title>
<date>2012</date>
<booktitle>In Proc. of the Cognitive Science Conference,</booktitle>
<location>Sapporo, Japan.</location>
<contexts>
<context position="14155" citStr="Nouri et al. (2012)" startWordPosition="2300" endWordPosition="2303">artially Observable Markov Decision Processes (POMDPs) can be applied to negotiation domains. 502 Georgila and Traum (2011) built argumentation dialogue policies for negotiation against users of different cultural norms in a one-issue negotiation scenario. To learn these policies they trained SUs on a spoken dialogue corpus in a florist-grocer negotiation domain, and then tweaked these SUs towards a particular cultural norm using handcrafted rules. Georgila (2013) learned argumentation dialogue policies from a simulated corpus in a two-issue negotiation scenario (organizing a party). Finally, Nouri et al. (2012) used IRL to learn a model for cultural decision-making in a simple negotiation game (the Ultimatum Game). 3 Single-Agent vs. Multi-Agent Reinforcement Learning Reinforcement Learning (RL) is a machine learning technique used to learn the policy of an agent, i.e., which action the agent should perform given its current state (Sutton and Barto, 1998). The goal of an RL-based agent is to maximize the reward it gets during an interaction. Because it is very difficult for the agent to know what will happen in the rest of the interaction, the agent must select an action based on the average reward </context>
</contexts>
<marker>Nouri, Georgila, Traum, 2012</marker>
<rawString>Elnaz Nouri, Kallirroi Georgila, and David Traum. 2012. A cultural decision-making model for negotiation based on inverse reinforcement learning. In Proc. of the Cognitive Science Conference, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Paruchuri</author>
<author>N Chakraborty</author>
<author>R Zivan</author>
<author>K Sycara</author>
<author>M Dudik</author>
<author>G Gordon</author>
</authors>
<title>POMDP based negotiation modeling.</title>
<date>2009</date>
<booktitle>In IJCAI Workshop on Modeling Intercultural Collaboration and Negotiation,</booktitle>
<location>Pasadena, California, USA.</location>
<contexts>
<context position="13497" citStr="Paruchuri et al. (2009)" startWordPosition="2204" endWordPosition="2207">le-agent RL methods. With regard to using RL for learning negotiation policies, the amount of research that has been performed is very limited compared to slot-filling. English and Heeman (2005) learned negotiation policies for a furniture layout task. Then Heeman (2009) extended this work by experimenting with different representations of the RL state in the same domain (this time learning against a hand-crafted SU). In both cases, to reduce the search space, the RL state included only information about e.g., whether there was a pending proposal rather than the actual value of this proposal. Paruchuri et al. (2009) performed a theoretical study on how Partially Observable Markov Decision Processes (POMDPs) can be applied to negotiation domains. 502 Georgila and Traum (2011) built argumentation dialogue policies for negotiation against users of different cultural norms in a one-issue negotiation scenario. To learn these policies they trained SUs on a spoken dialogue corpus in a florist-grocer negotiation domain, and then tweaked these SUs towards a particular cultural norm using handcrafted rules. Georgila (2013) learned argumentation dialogue policies from a simulated corpus in a two-issue negotiation s</context>
</contexts>
<marker>Paruchuri, Chakraborty, Zivan, Sycara, Dudik, Gordon, 2009</marker>
<rawString>P. Paruchuri, N. Chakraborty, R. Zivan, K. Sycara, M. Dudik, and G. Gordon. 2009. POMDP based negotiation modeling. In IJCAI Workshop on Modeling Intercultural Collaboration and Negotiation, Pasadena, California, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivier Pietquin</author>
<author>Helen Hastie</author>
</authors>
<title>A survey on metrics for the evaluation of user simulations.</title>
<date>2013</date>
<journal>Knowledge Engineering Review,</journal>
<volume>28</volume>
<issue>1</issue>
<contexts>
<context position="7970" citStr="Pietquin and Hastie, 2013" startWordPosition="1276" endWordPosition="1279">cy can be learned by having the system interact with the SU for a large number of dialogues (usually thousands of dialogues). Depending on the application, building a realistic SU can be just as difficult as building a good dialogue policy. Furthermore, it is not clear what constitutes a good SU for dialogue policy learning. Should the SU resemble real user behavior as closely as possible, or should it exhibit some degree of randomness to explore a variety of interaction patterns? Despite much research on the issue, these are still open questions (Schatzmann et al., 2006; Ai and Litman, 2008; Pietquin and Hastie, 2013). In the second approach, no SUs are required. Instead the dialogue policy is learned directly from a corpus of human-human or human-machine dialogues. For example, Henderson et al. (2008) used a combination of RL and supervised learning to learn a dialogue policy in a flight reservation domain, whereas Li et al. (2009) used Least-Squares Policy Iteration (Lagoudakis and Parr, 2003), an RL-based technique that can learn directly from corpora, in a voice dialer application. However, collecting such corpora is not trivial, especially in new domains. Typically, data are collected in a Wizard-of-O</context>
</contexts>
<marker>Pietquin, Hastie, 2013</marker>
<rawString>Olivier Pietquin and Helen Hastie. 2013. A survey on metrics for the evaluation of user simulations. Knowledge Engineering Review, 28(1):59–73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Edward Rasmussen</author>
<author>Christopher K I Williams</author>
</authors>
<title>Gaussian Processes for Machine Learning.</title>
<date>2006</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="9555" citStr="Rasmussen and Williams, 2006" startWordPosition="1542" endWordPosition="1545">em. In practice this means that dialogue policies learned from such data could be far from optimal. The first experiment on learning via live interaction with human users (third approach) was reported by Singh et al. (2002). They used RL to help the system with two choices: how much initiative it should allow the user, and whether or not to confirm information provided by the user. Recently, learning of “full” dialogue policies (not just choices at specific points in the dialogue) via live interaction with human users has become possible with the use of Gaussian processes (Engel et al., 2005; Rasmussen and Williams, 2006). Typically learning a dialogue policy is a slow process requiring thousands of dialogues, hence the need for SUs. Gaussian processes have been shown to speed up learning. This fact together with easy access to a large number of human users through crowd-sourcing has allowed dialogue policy learning via live interaction with human users (Gaˇsi´c et al., 2011; Gaˇsi´c et al., 2013). Space constraints prevent us from providing an exhaustive list of previous work on using RL for dialogue management. Thus below we focus only on research that is directly related to our work, specifically research o</context>
</contexts>
<marker>Rasmussen, Williams, 2006</marker>
<rawString>Carl Edward Rasmussen and Christopher K. I. Williams. 2006. Gaussian Processes for Machine Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Verena Rieser</author>
<author>Simon Keizer</author>
<author>Xingkun Liu</author>
<author>Oliver Lemon</author>
</authors>
<title>Adaptive information presentation for spoken dialogue systems: Evaluation with human subjects.</title>
<date>2011</date>
<booktitle>In Proc. of the European Workshop on Natural Language Generation,</booktitle>
<location>Nancy, France.</location>
<contexts>
<context position="1720" citStr="Rieser et al., 2011" startWordPosition="247" endWordPosition="250">ude that multiagent RL of dialogue policies is a promising alternative to using single-agent RL and SUs or learning directly from corpora. 1 Introduction The dialogue policy of a dialogue system decides on which actions the system should perform given a particular dialogue state (i.e., dialogue context). Building a dialogue policy can be a challenging task especially for complex applications. For this reason, recently much attention has been drawn to machine learning approaches to dialogue management and in particular Reinforcement Learning (RL) of dialogue policies (Williams and Young, 2007; Rieser et al., 2011; Jurˇc´ıˇcek et al., 2012). Typically there are three main approaches to the problem of learning dialogue policies using RL: (1) learn against a simulated user (SU), i.e., a model that simulates the behavior of a real user (Georgila et al., 2006; Schatzmann et al., 2006); (2) learn directly from a corpus (Henderson et al., 2008; Li et al., 2009); or (3) learn via live interaction with human users (Singh et al., 2002; Gaˇsi´c et al., 2011; Gaˇsi´c et al., 2013). We propose a fourth approach: concurrent learning of the system policy and the SU policy using multi-agent RL techniques. Both agents</context>
</contexts>
<marker>Rieser, Keizer, Liu, Lemon, 2011</marker>
<rawString>Verena Rieser, Simon Keizer, Xingkun Liu, and Oliver Lemon. 2011. Adaptive information presentation for spoken dialogue systems: Evaluation with human subjects. In Proc. of the European Workshop on Natural Language Generation, Nancy, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jost Schatzmann</author>
<author>Karl Weilhammer</author>
<author>Matt Stuttle</author>
<author>Steve Young</author>
</authors>
<title>A survey of statistical user simulation techniques for reinforcement-learning of dialogue management strategies.</title>
<date>2006</date>
<journal>Knowledge Engineering Review,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="1992" citStr="Schatzmann et al., 2006" startWordPosition="292" endWordPosition="295">alogue state (i.e., dialogue context). Building a dialogue policy can be a challenging task especially for complex applications. For this reason, recently much attention has been drawn to machine learning approaches to dialogue management and in particular Reinforcement Learning (RL) of dialogue policies (Williams and Young, 2007; Rieser et al., 2011; Jurˇc´ıˇcek et al., 2012). Typically there are three main approaches to the problem of learning dialogue policies using RL: (1) learn against a simulated user (SU), i.e., a model that simulates the behavior of a real user (Georgila et al., 2006; Schatzmann et al., 2006); (2) learn directly from a corpus (Henderson et al., 2008; Li et al., 2009); or (3) learn via live interaction with human users (Singh et al., 2002; Gaˇsi´c et al., 2011; Gaˇsi´c et al., 2013). We propose a fourth approach: concurrent learning of the system policy and the SU policy using multi-agent RL techniques. Both agents are trained simultaneously and there is no need for building a SU separately or having access to a corpus.1 As we discuss below, concurrent learning could potentially be used for learning via live interaction with human users. Moreover, for negotiation in particular ther</context>
<context position="7921" citStr="Schatzmann et al., 2006" startWordPosition="1268" endWordPosition="1271">uman-machine dialogues. Then the dialogue policy can be learned by having the system interact with the SU for a large number of dialogues (usually thousands of dialogues). Depending on the application, building a realistic SU can be just as difficult as building a good dialogue policy. Furthermore, it is not clear what constitutes a good SU for dialogue policy learning. Should the SU resemble real user behavior as closely as possible, or should it exhibit some degree of randomness to explore a variety of interaction patterns? Despite much research on the issue, these are still open questions (Schatzmann et al., 2006; Ai and Litman, 2008; Pietquin and Hastie, 2013). In the second approach, no SUs are required. Instead the dialogue policy is learned directly from a corpus of human-human or human-machine dialogues. For example, Henderson et al. (2008) used a combination of RL and supervised learning to learn a dialogue policy in a flight reservation domain, whereas Li et al. (2009) used Least-Squares Policy Iteration (Lagoudakis and Parr, 2003), an RL-based technique that can learn directly from corpora, in a voice dialer application. However, collecting such corpora is not trivial, especially in new domain</context>
</contexts>
<marker>Schatzmann, Weilhammer, Stuttle, Young, 2006</marker>
<rawString>Jost Schatzmann, Karl Weilhammer, Matt Stuttle, and Steve Young. 2006. A survey of statistical user simulation techniques for reinforcement-learning of dialogue management strategies. Knowledge Engineering Review, 21(2):97–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satinder Singh</author>
<author>Diane Litman</author>
<author>Michael Kearns</author>
<author>Marilyn Walker</author>
</authors>
<title>Optimizing dialogue management with reinforcement learning: Experiments with the NJFun system.</title>
<date>2002</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>16--105</pages>
<contexts>
<context position="2140" citStr="Singh et al., 2002" startWordPosition="320" endWordPosition="323"> much attention has been drawn to machine learning approaches to dialogue management and in particular Reinforcement Learning (RL) of dialogue policies (Williams and Young, 2007; Rieser et al., 2011; Jurˇc´ıˇcek et al., 2012). Typically there are three main approaches to the problem of learning dialogue policies using RL: (1) learn against a simulated user (SU), i.e., a model that simulates the behavior of a real user (Georgila et al., 2006; Schatzmann et al., 2006); (2) learn directly from a corpus (Henderson et al., 2008; Li et al., 2009); or (3) learn via live interaction with human users (Singh et al., 2002; Gaˇsi´c et al., 2011; Gaˇsi´c et al., 2013). We propose a fourth approach: concurrent learning of the system policy and the SU policy using multi-agent RL techniques. Both agents are trained simultaneously and there is no need for building a SU separately or having access to a corpus.1 As we discuss below, concurrent learning could potentially be used for learning via live interaction with human users. Moreover, for negotiation in particular there is one more reason in favor of concurrent learning as opposed to learning against a SU. Unlike slot-filling domains, in negotiation the behaviors </context>
<context position="9149" citStr="Singh et al. (2002)" startWordPosition="1472" endWordPosition="1475">lly, data are collected in a Wizard-of-Oz setup where human users think that they interact with a system while in fact they interact with a human pretending to be the system, or by having human users interact with a preliminary version of the dialogue system. In both cases the resulting interactions are expected to be quite dif501 ferent from the interactions of human users with the final system. In practice this means that dialogue policies learned from such data could be far from optimal. The first experiment on learning via live interaction with human users (third approach) was reported by Singh et al. (2002). They used RL to help the system with two choices: how much initiative it should allow the user, and whether or not to confirm information provided by the user. Recently, learning of “full” dialogue policies (not just choices at specific points in the dialogue) via live interaction with human users has become possible with the use of Gaussian processes (Engel et al., 2005; Rasmussen and Williams, 2006). Typically learning a dialogue policy is a slow process requiring thousands of dialogues, hence the need for SUs. Gaussian processes have been shown to speed up learning. This fact together wit</context>
</contexts>
<marker>Singh, Litman, Kearns, Walker, 2002</marker>
<rawString>Satinder Singh, Diane Litman, Michael Kearns, and Marilyn Walker. 2002. Optimizing dialogue management with reinforcement learning: Experiments with the NJFun system. Journal of Artificial Intelligence Research, 16:105–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard S Sutton</author>
<author>Andrew G Barto</author>
</authors>
<title>Reinforcement Learning: An Introduction.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="12144" citStr="Sutton and Barto, 1998" startWordPosition="1982" endWordPosition="1986">(2002) who proposed the PHC and PHC-WoLF algorithms that we use in this paper. We chose these two algorithms because, unlike other multi-agent RL methods (Littman, 1994; Hu and Wellman, 1998), they do not make assumptions that do not always hold and do not require quadratic or linear programming that does not always scale. English and Heeman (2005) were the first in the dialogue community to explore the idea of concurrent learning of dialogue policies. However, English and Heeman (2005) did not use multiagent RL but only standard single-agent RL, in particular an on-policy Monte Carlo method (Sutton and Barto, 1998). But single-agent RL techniques are not well suited for concurrent learning where each agent is trained against a continuously changing environment. Indeed, English and Heeman (2005) reported problems with convergence. Chandramohan et al. (2012) proposed a framework for co-adaptation of the dialogue policy and the SU using single-agent RL. They applied Inverse Reinforcement Learning (IRL) (Abbeel and Ng, 2004) to a corpus in order to learn the reward functions of both the system and the SU. Furthermore, Cuay´ahuitl and Dethlefs (2012) used hierarchical multi-agent RL for co-ordinating the ver</context>
<context position="14506" citStr="Sutton and Barto, 1998" startWordPosition="2355" endWordPosition="2358">egotiation domain, and then tweaked these SUs towards a particular cultural norm using handcrafted rules. Georgila (2013) learned argumentation dialogue policies from a simulated corpus in a two-issue negotiation scenario (organizing a party). Finally, Nouri et al. (2012) used IRL to learn a model for cultural decision-making in a simple negotiation game (the Ultimatum Game). 3 Single-Agent vs. Multi-Agent Reinforcement Learning Reinforcement Learning (RL) is a machine learning technique used to learn the policy of an agent, i.e., which action the agent should perform given its current state (Sutton and Barto, 1998). The goal of an RL-based agent is to maximize the reward it gets during an interaction. Because it is very difficult for the agent to know what will happen in the rest of the interaction, the agent must select an action based on the average reward it has previously observed after having performed that action in similar contexts. This average reward is called expected future reward. Single-agent RL is used in the framework of Markov Decision Processes (MDPs) (Sutton and Barto, 1998) or Partially Observable Markov Decision Processes (POMDPs) (Williams and Young, 2007). Here we focus on MDPs. An</context>
<context position="19368" citStr="Sutton and Barto, 1998" startWordPosition="3269" endWordPosition="3272">ent 1 Agent 2 apple 300 200 orange 200 300 Table 1: Points earned by Agents 1 and 2 for each apple and each orange that they have at the end of the negotiation. Agent 1: offer-2-2 (I offer you 2 A and 2 O) Agent 2: offer-3-0 (I offer you 3 A and 0 O) Agent 1: offer-0-3 (I offer you 0 A and 3 O) Agent 2: offer-4-0 (I offer you 4 A and 0 O) Agent 1: accept (I accept your offer) Figure 1: Example interaction between Agents 1 and 2 (A: apples, O: oranges). SW if � Σα� 7r(s, α�)Q(s, α�) &gt; Σα� ˜7r(s, α�)Q(s, α) SLF otherwise S = { (5) More details about Q-learning, PHC, and PHCWoLF can be found in (Sutton and Barto, 1998; Bowling and Veloso, 2002). As discussed in sections 1 and 2, single-agent RL techniques, such as Q-learning, are not suitable for multi-agent RL. Nevertheless, despite its shortcomings Q-learning has been used successfully for multi-agent RL (Claus and Boutilier, 1998). Indeed, as we see in section 5, Q-learning can converge to the optimal policy for small state spaces. However, as the state space size increases the performance of Q-learning drops (compared to PHC and PHC-WoLF). 4 Domain and Experimental Setup Our domain is a resource allocation negotiation scenario. Two agents negotiate abo</context>
</contexts>
<marker>Sutton, Barto, 1998</marker>
<rawString>Richard S. Sutton and Andrew G. Barto. 1998. Reinforcement Learning: An Introduction. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel R Tetreault</author>
<author>Diane J Litman</author>
</authors>
<title>A reinforcement learning approach to evaluating state representations in spoken dialogue systems.</title>
<date>2008</date>
<journal>Speech Communication,</journal>
<pages>50--8</pages>
<contexts>
<context position="6968" citStr="Tetreault and Litman, 2008" startWordPosition="1109" endWordPosition="1112">ntal setup. In section 5 we present our results. Finally, section 6 concludes and provides some ideas for future work. 2 Related Work Most research in RL for dialogue management has been done in the framework of slot-filling applications such as restaurant recommendations (Lemon et al., 2006; Thomson and Young, 2010; Gaˇsi´c et al., 2012; Daubigney et al., 2012), flight reservations (Henderson et al., 2008), sightseeing recommendations (Misu et al., 2010), appointment scheduling (Georgila et al., 2010), etc. RL has also been applied to question-answering (Misu et al., 2012), tutoring domains (Tetreault and Litman, 2008; Chi et al., 2011), and learning negotiation dialogue policies (Heeman, 2009; Georgila and Traum, 2011; Georgila, 2013). As mentioned in section 1, there are three main approaches to the problem of learning dialogue policies using RL. In the first approach, a SU is hand-crafted or learned from a small corpus of human-human or human-machine dialogues. Then the dialogue policy can be learned by having the system interact with the SU for a large number of dialogues (usually thousands of dialogues). Depending on the application, building a realistic SU can be just as difficult as building a good </context>
</contexts>
<marker>Tetreault, Litman, 2008</marker>
<rawString>Joel R. Tetreault and Diane J. Litman. 2008. A reinforcement learning approach to evaluating state representations in spoken dialogue systems. Speech Communication, 50(8-9):683–696.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Blaise Thomson</author>
<author>Steve Young</author>
</authors>
<title>Bayesian update of dialogue state: A POMDP framework for spoken dialogue systems.</title>
<date>2010</date>
<journal>Computer Speech and Language,</journal>
<volume>24</volume>
<issue>4</issue>
<contexts>
<context position="6659" citStr="Thomson and Young, 2010" startWordPosition="1063" endWordPosition="1066">s Q-learning, PHC, and PHCWoLF in such a variety of situations (varying a large number of parameters). The paper is structured as follows. Section 2 presents related work. Section 3 provides a brief introduction to single-agent RL and multi-agent RL. Section 4 describes our negotiation domain and experimental setup. In section 5 we present our results. Finally, section 6 concludes and provides some ideas for future work. 2 Related Work Most research in RL for dialogue management has been done in the framework of slot-filling applications such as restaurant recommendations (Lemon et al., 2006; Thomson and Young, 2010; Gaˇsi´c et al., 2012; Daubigney et al., 2012), flight reservations (Henderson et al., 2008), sightseeing recommendations (Misu et al., 2010), appointment scheduling (Georgila et al., 2010), etc. RL has also been applied to question-answering (Misu et al., 2012), tutoring domains (Tetreault and Litman, 2008; Chi et al., 2011), and learning negotiation dialogue policies (Heeman, 2009; Georgila and Traum, 2011; Georgila, 2013). As mentioned in section 1, there are three main approaches to the problem of learning dialogue policies using RL. In the first approach, a SU is hand-crafted or learned </context>
</contexts>
<marker>Thomson, Young, 2010</marker>
<rawString>Blaise Thomson and Steve Young. 2010. Bayesian update of dialogue state: A POMDP framework for spoken dialogue systems. Computer Speech and Language, 24(4):562–588.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason D Williams</author>
<author>Steve Young</author>
</authors>
<title>Scaling POMDPs for spoken dialog management.</title>
<date>2007</date>
<journal>IEEE Transactions on Audio, Speech, and Language Processing,</journal>
<volume>15</volume>
<issue>7</issue>
<contexts>
<context position="1699" citStr="Williams and Young, 2007" startWordPosition="243" endWordPosition="246"> for convergence. We conclude that multiagent RL of dialogue policies is a promising alternative to using single-agent RL and SUs or learning directly from corpora. 1 Introduction The dialogue policy of a dialogue system decides on which actions the system should perform given a particular dialogue state (i.e., dialogue context). Building a dialogue policy can be a challenging task especially for complex applications. For this reason, recently much attention has been drawn to machine learning approaches to dialogue management and in particular Reinforcement Learning (RL) of dialogue policies (Williams and Young, 2007; Rieser et al., 2011; Jurˇc´ıˇcek et al., 2012). Typically there are three main approaches to the problem of learning dialogue policies using RL: (1) learn against a simulated user (SU), i.e., a model that simulates the behavior of a real user (Georgila et al., 2006; Schatzmann et al., 2006); (2) learn directly from a corpus (Henderson et al., 2008; Li et al., 2009); or (3) learn via live interaction with human users (Singh et al., 2002; Gaˇsi´c et al., 2011; Gaˇsi´c et al., 2013). We propose a fourth approach: concurrent learning of the system policy and the SU policy using multi-agent RL te</context>
<context position="15079" citStr="Williams and Young, 2007" startWordPosition="2451" endWordPosition="2454">orm given its current state (Sutton and Barto, 1998). The goal of an RL-based agent is to maximize the reward it gets during an interaction. Because it is very difficult for the agent to know what will happen in the rest of the interaction, the agent must select an action based on the average reward it has previously observed after having performed that action in similar contexts. This average reward is called expected future reward. Single-agent RL is used in the framework of Markov Decision Processes (MDPs) (Sutton and Barto, 1998) or Partially Observable Markov Decision Processes (POMDPs) (Williams and Young, 2007). Here we focus on MDPs. An MDP is defined as a tuple (5, A, T, R, y) where 5 is the set of states (representing different contexts) which the agent may be in, A is the set of actions of the agent, T is the transition function 5 x A x 5 —* [0, 1] which defines a set of transition probabilities between states after taking an action, R is the reward function 5 x A —* R which defines the reward received when taking an action from the given state, and y is a factor that discounts future rewards. Solving the MDP means finding a policy 7r : 5 —* A. The quality of the policy 7r is measured by the exp</context>
</contexts>
<marker>Williams, Young, 2007</marker>
<rawString>Jason D. Williams and Steve Young. 2007. Scaling POMDPs for spoken dialog management. IEEE Transactions on Audio, Speech, and Language Processing, 15(7):2116–2129.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>