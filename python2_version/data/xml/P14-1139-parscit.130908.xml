<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.976699">
A Constrained Viterbi Relaxation for Bidirectional Word Alignment
</title>
<note confidence="0.791518888888889">
Yin-Wen Chang Alexander M. Rush
MIT CSAIL,
Cambridge, MA 02139
{yinwen,srush}@
csail.mit.edu
John DeNero
UC Berkeley,
Berkeley, CA 94720
denero@
</note>
<email confidence="0.636354">
cs.berkeley.edu
</email>
<author confidence="0.699947">
Michael Collins
</author>
<affiliation confidence="0.688295">
Columbia University,
</affiliation>
<address confidence="0.6363635">
New York, NY 10027
mcollins@
</address>
<email confidence="0.853642">
cs.columbia.edu
</email>
<sectionHeader confidence="0.990562" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997504">
Bidirectional models of word alignment
are an appealing alternative to post-hoc
combinations of directional word align-
ers. Unfortunately, most bidirectional
formulations are NP-Hard to solve, and
a previous attempt to use a relaxation-
based decoder yielded few exact solu-
tions (6%). We present a novel relax-
ation for decoding the bidirectional model
of DeNero and Macherey (2011). The
relaxation can be solved with a mod-
ified version of the Viterbi algorithm.
To find optimal solutions on difficult
instances, we alternate between incre-
mentally adding constraints and applying
optimality-preserving coarse-to-fine prun-
ing. The algorithm finds provably ex-
act solutions on 86% of sentence pairs
and shows improvements over directional
models.
</bodyText>
<sectionHeader confidence="0.998778" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999938966666667">
Word alignment is a critical first step for build-
ing statistical machine translation systems. In or-
der to ensure accurate word alignments, most sys-
tems employ a post-hoc symmetrization step to
combine directional word aligners, such as IBM
Model 4 (Brown et al., 1993) or hidden Markov
model (HMM) based aligners (Vogel et al., 1996).
Several authors have proposed bidirectional mod-
els that incorporate this step directly, but decoding
under many bidirectional models is NP-Hard and
finding exact solutions has proven difficult.
In this paper, we describe a novel Lagrangian-
relaxation based decoder for the bidirectional
model proposed by DeNero and Macherey (2011),
with the goal of improving search accuracy.
In that work, the authors implement a dual
decomposition-based decoder for the problem, but
are only able to find exact solutions for around 6%
of instances.
Our decoder uses a simple variant of the Viterbi
algorithm for solving a relaxed version of this
model. The algorithm makes it easy to re-
introduce constraints for difficult instances, at the
cost of increasing run-time complexity. To offset
this cost, we employ optimality-preserving coarse-
to-fine pruning to reduce the search space. The
pruning method utilizes lower bounds on the cost
of valid bidirectional alignments, which we obtain
from a fast, greedy decoder.
The method has the following properties:
</bodyText>
<listItem confidence="0.968962">
• It is based on a novel relaxation for the model
of DeNero and Macherey (2011), solvable
with a variant of the Viterbi algorithm.
• To find optimal solutions, it employs an effi-
cient strategy that alternates between adding
constraints and applying pruning.
• Empirically, it is able to find exact solutions
on 86% of sentence pairs and is significantly
faster than general-purpose solvers.
</listItem>
<bodyText confidence="0.9998603125">
We begin in Section 2 by formally describing
the directional word alignment problem. Section 3
describes a preliminary bidirectional model us-
ing full agreement constraints and a Lagrangian
relaxation-based solver. Section 4 modifies this
model to include adjacency constraints. Section 5
describes an extension to the relaxed algorithm to
explicitly enforce constraints, and Section 6 gives
a pruning method for improving the efficiency of
the algorithm.
Experiments compare the search error and accu-
racy of the new bidirectional algorithm to several
directional combiners and other bidirectional al-
gorithms. Results show that the new relaxation is
much more effective at finding exact solutions and
is able to produce comparable alignment accuracy.
</bodyText>
<page confidence="0.934829">
1481
</page>
<note confidence="0.8386205">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1481–1490,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<figure confidence="0.995521375">
⎧
⎨⎪⎪⎪⎪⎪ ⎪
⎪⎪⎪⎪⎪⎪⎩
let
us
see
the
documents
</figure>
<figureCaption confidence="0.579822">
Figure 1: An example e--+f directional alignment for the sen-
tences let us see the documents and montrez -
nous les documents, with I = 5 and J = 5. The in-
dices i E [I]0 are rows, and the indices j E [J]0 are columns.
The HMM alignment shown has transitions x(0, 1, 1) =
x(1, 2, 3) = x(3, 3, 1) = x(1, 4, 4) = x(4, 5, 5) = 1.
</figureCaption>
<bodyText confidence="0.999881222222222">
Notation We use lower- and upper-case letters
for scalars and vectors, and script-case for sets
e.g. X. For vectors, such as v E 10,1}(I×J)∪J ,
where I and J are finite sets, we use the notation
v(i, j) and v(j) to represent elements of the vec-
tor. Define d = δ(i) to be the indicator vector with
d(i) = 1 and d(i0) = 0 for all i0 =� i. Finally de-
fine the notation [J] to refer to 11... J} and [J]0
to refer to 10 ... J}.
</bodyText>
<sectionHeader confidence="0.987623" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999756428571429">
The focus of this work is on the word alignment
decoding problem. Given a sentence e of length
�eI = I and a sentence f of length IfI = J, our
goal is to find the best bidirectional alignment be-
tween the two sentences under a given objective
function. Before turning to the model of interest,
we first introduce directional word alignment.
</bodyText>
<subsectionHeader confidence="0.972242">
2.1 Word Alignment
</subsectionHeader>
<bodyText confidence="0.999931846153846">
In the e—*f word alignment problem, each word
in e is aligned to a word in f or to the null word E.
This alignment is a mapping from each index i E
[I] to an index j E [J]0 (where j = 0 represents
alignment to E). We refer to a single word align-
ment as a link.
A first-order HMM alignment model (Vogel et
al., 1996) is an HMM of length I + 1 where the
hidden state at position i E [I]0 is the aligned in-
dex j E [J]0, and the transition score takes into
account the previously aligned index j0 E [J]0.1
Formally, define the set of possible HMM align-
ments as X C 10, 1}([I]0×[J]0)∪([I]×[J]0×[J]0) with
</bodyText>
<footnote confidence="0.9245435">
1Our definition differs slightly from other HMM-based
aligners in that it does not track the last a alignment.
</footnote>
<equation confidence="0.99302875">
X=
J
x(i, j) = E x(j, i + 1, j0) `di E [I − 1]0, j E [J]0
j�=0
</equation>
<bodyText confidence="0.9990645">
where x(i, j) = 1 indicates that there is a link
between index i and index j, and x(j0, i, j) = 1
indicates that index i − 1 aligns to index j0 and
index i aligns to j. Figure 1 shows an example
member of X.
The constraints of X enforce backward and for-
ward consistency respectively. If x(i, j) = 1,
backward consistency enforces that there is a tran-
sition from (i − 1, j0) to (i, j) for some j0 E [J]0,
whereas forward consistency enforces a transition
from (i, j) to (i + 1, j0) for some j0 E [J]0. Infor-
mally the constraints “chain” together the links.
The HMM objective function f : X —* R can
be written as a linear function of x
</bodyText>
<equation confidence="0.991381333333333">
J
0(j0,i,j)x(j0,i,j)
j0=0
</equation>
<bodyText confidence="0.99889025">
where the vector 0 E R[I]×[J]0×[J]0 includes the
transition and alignment scores. For a generative
model of alignment, we might define 0(j0, i, j) =
log(p(eiIfj)p(jIj0)). For a discriminative model
of alignment, we might define 0(j0, i, j) = w �
φ(i, j0, j, f, e) for a feature function φ and weights
w (Moore, 2005; Lacoste-Julien et al., 2006).
Now reverse the direction of the model and
consider the f—*e alignment problem. An f—*e
alignment is a binary vector y E Y where for
each j E [J], y(i, j) = 1 for exactly one i E
[I]0. Define the set of HMM alignments Y C
</bodyText>
<equation confidence="0.991904571428571">
10, 1}([I]0×[J]0)∪([I]0×[I]0×[J]) as
with vector ω E R[I]0×[I]0×[J].
y : y(0, 0) = 1,
I
y(i0, i, j) `di E [I]0, j E [J],
I
y(i, i0, j + 1) `di E [I]0, j E [J − 1]0
</equation>
<bodyText confidence="0.705338">
Similarly define the objective function
</bodyText>
<equation confidence="0.998347681818182">
g(y; ω) = J I I ω(i0, i, j)y(i0, i, j)
j=1 i=0 �
i0=0
⎧
⎨⎪⎪⎪⎪⎪ ⎪
⎪⎪⎪⎪⎪⎪⎩
Y =
E
i�=0
y(i, j) =
E
i�=0
y(i, j) =
x : x(0, 0) = 1,
J
x(i, j) = E x(j0, i, j) `di E [I], j E [J]0,
j�=0
I
i=1
f(x;0) =
J
j=0
</equation>
<page confidence="0.976505">
1482
</page>
<figure confidence="0.996072666666667">
let let
us us
see see
the the
documents documents
(a) (b)
</figure>
<figureCaption confidence="0.902715">
Figure 2: (a) An example alignment pair (x, y) satisfying the
full agreement conditions. The x alignment is represented
with circles and the y alignment with triangles. (b) An exam-
</figureCaption>
<bodyText confidence="0.853392888888889">
ple f→e alignment y E Y0 with relaxed forward constraints.
Note that unlike an alignment from Y multiple words may
be aligned in a column and words may transition from non-
aligned positions.
Note that for both of these models we can solve
the optimization problem exactly using the stan-
dard Viterbi algorithm for HMM decoding. The
first can be solved in O(IJ2) time and the second
in O(I2J) time.
</bodyText>
<sectionHeader confidence="0.982993" genericHeader="method">
3 Bidirectional Alignment
</sectionHeader>
<bodyText confidence="0.999985555555556">
The directional bias of the e-*f and f-*e align-
ment models may cause them to produce differing
alignments. To obtain the best single alignment,
it is common practice to use a post-hoc algorithm
to merge these directional alignments (Och et al.,
1999). First, a directional alignment is found from
each word in e to a word f. Next an alignment is
produced in the reverse direction from f to e. Fi-
nally, these alignments are merged, either through
intersection, union, or with an interpolation algo-
rithm such as grow-diag-final (Koehn et al., 2003).
In this work, we instead consider a bidirectional
alignment model that jointly considers both direc-
tional models. We begin in this section by in-
troducing a simple bidirectional model that en-
forces full agreement between directional models
and giving a relaxation for decoding. Section 4
loosens this model to adjacent agreement.
</bodyText>
<subsectionHeader confidence="0.998612">
3.1 Enforcing Full Agreement
</subsectionHeader>
<bodyText confidence="0.9990318">
Perhaps the simplest post-hoc merging strategy is
to retain the intersection of the two directional
models. The analogous bidirectional model en-
forces full agreement to ensure the two alignments
select the same non-null links i.e.
</bodyText>
<equation confidence="0.932464">
x*, y* = arg max f(x) + g(y) s.t.
xEX,yEY
x(i, j) = y(i, j) Vi E [I], j E [J]
</equation>
<bodyText confidence="0.999275321428572">
We refer to the optimal alignments for this prob-
lem as x* and y*.
Unfortunately this bidirectional decoding
model is NP-Hard (a proof is given in Ap-
pendix A). As it is common for alignment pairs to
have |f |or |e |over 40, exact decoding algorithms
are intractable in the worst-case.
Instead we will use Lagrangian relaxation for
this model. At a high level, we will remove a
subset of the constraints from the original problem
and replace them with Lagrange multipliers. If we
can solve this new problem efficiently, we may be
able to get optimal solutions to the original prob-
lem. (See the tutorial by Rush and Collins (2012)
describing the method.)
There are many possible subsets of constraints
to consider relaxing. The relaxation we use pre-
serves the agreement constraints while relaxing
the Markov structure of the f-*e alignment. This
relaxation will make it simple to later re-introduce
constraints in Section 5.
We relax the forward constraints of set Y. With-
out these constraints the y links are no longer
chained together. This has two consequences: (1)
for index j there may be any number of indices i,
such that y(i, j) = 1, (2) if y(i&apos;, i, j) = 1 it is no
longer required that y(i&apos;, j − 1) = 1. This gives a
set Y&apos; which is a superset of Y
</bodyText>
<equation confidence="0.994937">
J y : y(0, 0) = 1,
Y = l y(i, j) = EIi0=0 y(i0, i, j) ∀i E [I]0, j E [J]
</equation>
<bodyText confidence="0.994520714285714">
Figure 2(b) shows a possible y E Y&apos; and a valid
unchained structure.
To form the Lagrangian dual with relaxed for-
ward constraints, we introduce a vector of La-
grange multipliers, A E RII−1]0xIJ]0, with one
multiplier for each original constraint. The La-
grangian dual L(A) is defined as
</bodyText>
<equation confidence="0.974853933333333">
= max
x∈X,y∈Y0,
x(i,j)=y(i,j)
�
I
Ey(i, i0, j + 1)
f(x) + g0(y; ω, λ) (4)
max
x∈X,y∈Y0,
x(i,j)=y(i,j)
− EI J−1E
i=0 j=0
=max
x∈X,y∈Y0,
x(i,j)=y(i,j)
f(x) + EI J I y(i0, i, j)ω(i0, i, j)(1)
i=1 E E
j=0 i0=0
λ(i, j) (y(i, j) −
=
0i0
f(x) + EI J I y(i0, i, j)ω0(i0, i, j)(2)
i=1 E E
j=0 i0=0
f(x) + EI J y(i, j) max ω0(i0, i, j)(3)
i=1 E i0∈[I]0
j=0
= max
x∈X,y∈Y0,
x(i,j)=y(i,j)
</equation>
<page confidence="0.851514">
1483
</page>
<bodyText confidence="0.998209">
Line 2 distributes the A’s and introduces a modi-
fied potential vector w0 defined as
</bodyText>
<equation confidence="0.999026">
w0(i0,i,j) = w(i0,i,j) − A(i,j) + A(i0,j − 1)
</equation>
<bodyText confidence="0.9803544">
for all i0 ∈ [I]0, i ∈ [I]0, j ∈ [J]. Line 3 uti-
lizes the relaxed set Y0 which allows each y(i, j)
to select the best possible previous link (i0, j − 1).
Line 4 introduces the modified directional objec-
tive
</bodyText>
<equation confidence="0.926673">
y(i, j) max
i0∈[I]0
</equation>
<bodyText confidence="0.987243625">
The Lagrangian dual is guaranteed to be an up-
per bound on the optimal solution, i.e. for all A,
L(A) ≥ f(x∗) + g(y∗). Lagrangian relaxation
attempts to find the tighest possible upper bound
by minimizing the Lagrangian dual, minλ L(A),
using subgradient descent. Briefly, subgradient
descent is an iterative algorithm, with two steps.
Starting with A = 0, we iteratively
</bodyText>
<listItem confidence="0.9968615">
1. Set (x, y) to the arg max of L(A).
2. Update A(i, j) for all i ∈ [I − 1]0, j ∈ [J]0,
</listItem>
<equation confidence="0.960335">
I
λ(i, j) — λ(i, j) — ηt (y(i, j) — E y(i, i0, j + 1))
i&apos;=0
.
</equation>
<bodyText confidence="0.999548578947368">
where qt &gt; 0 is a step size for the t’th update. If
at any iteration of the algorithm the forward con-
straints are satisfied for (x, y), then f(x)+g(y) =
f(x∗) + g(x∗) and we say this gives a certificate
of optimality for the underlying problem.
To run this algorithm, we need to be able to effi-
ciently compute the (x, y) pair that is the arg max
of L(A) for any value of A. Fortunately, since the y
alignments are no longer constrained to valid tran-
sitions, we can compute these alignments by first
picking the best f→e transitions for each possible
link, and then running an e→f Viterbi-style algo-
rithm to find the bidirectional alignment.
The max version of this algorithm is shown in
Figure 3. It consists of two steps. We first compute
the score for each y(i, j) variable. We then use the
standard Viterbi update for computing the x vari-
ables, adding in the score of the y(i, j) necessary
to satisfy the constraints.
</bodyText>
<equation confidence="0.934225">
procedure VITERBIFULL(θ, ω0)
Let π, ρ be dynamic programming charts.
ω0(i0, i, j) `d i E [I], j E [J]0
π[0, 0] — EJj=1 max{0, ρ[0, j]1
for i E [I], j E [J]0 in order do
θ(j0, i, j) + π[i — 1, j0]
if j =� 0 then π[i, j] +-- π[i, j] + ρ[i, j]
return maxj∈[J]0 π[I, j]
</equation>
<figureCaption confidence="0.988564333333333">
Figure 3: Viterbi-style algorithm for computing L(λ). For
simplicity the algorithm shows the max version of the algo-
rithm, arg max can be computed with back-pointers.
</figureCaption>
<sectionHeader confidence="0.952646" genericHeader="method">
4 Adjacent Agreement
</sectionHeader>
<bodyText confidence="0.999876857142857">
Enforcing full agreement can be too strict an align-
ment criteria. DeNero and Macherey (2011) in-
stead propose a model that allows near matches,
which we call adjacent agreement. Adjacent
agreement allows links from one direction to agree
with adjacent links from the reverse alignment for
a small penalty. Figure 4(a) shows an example
of a valid bidirectional alignment under adjacent
agreement.
In this section we formally introduce adjacent
agreement, and propose a relaxation algorithm for
this model. The key algorithmic idea is to extend
the Viterbi algorithm in order to consider possible
adjacent links in the reverse direction.
</bodyText>
<subsectionHeader confidence="0.997625">
4.1 Enforcing Adjacency
</subsectionHeader>
<bodyText confidence="0.992442333333333">
Define the adjacency set K = {−1, 0, 1}. A bidi-
rectional alignment satisfies adjacency if for all
i ∈ [I], j ∈ [J],
</bodyText>
<listItem confidence="0.99207025">
• If x(i, j) = 1, it is required that y(i + k, j) =
1 for exactly one k ∈ K (i.e. either above,
center, or below). We indicate which position
with variables zli,j ∈ {0, 1}K
• If x(i, j) = 1, it is allowed that y(i, j + k) =
1 for any k ∈ K (i.e. either left, center, or
right) and all other y(i, j0) = 0. We indicate
which positions with variables z↔i,j ∈ {0,1}K
</listItem>
<bodyText confidence="0.803245">
Formally for x ∈ X and y ∈ Y, the pair (x, y) is
feasible if there exists a z from the set Z(x, y) ⊂
{0,1}K2×[I]×[J] defined as
</bodyText>
<equation confidence="0.99617752173913">
k∈K k∈K
zi,j(k) &lt; y(i + k, j) `dkE)C:i+k&gt;0,
l
x(i, j) &gt; z↔i,j−k(k) `dk E )C : j + k &gt; 0
g0(y; w, A) =
I
i=1
J
j=0
w0(i0, i, j)
ρ[i, j] — .max
V ∈[I]0
π[i, j] +— max
j&apos;∈[J]0
Z(x, y) = I
z : `di E [I], j E [J]
zi,j E {0, 11K,
l z↔�i,j E {0, 11K
zi,j(k),
l
�
x(i, j) =
z↔i,j(k) = y(i, j),
</equation>
<page confidence="0.42374">
1484
</page>
<figure confidence="0.7921575">
procedure VITERBIADJ(θ, ω0)
ω0(i0, i, j) ∀ i ∈ [I], j ∈ [J]0
ρ[i, j] ← max
i&apos;∈[I]0
let let
us us
see see
the the
documents documents
(a) (b)
</figure>
<figureCaption confidence="0.8478992">
Figure 4: (a) An alignment satisfying the adjacency con-
straints. Note that x(2, 1) = 1 is allowed because of
y(1, 1) = 1, x(4, 3) = 1 because of y(3, 3), and y(3, 1)
because of x(3, 2). (b) An adjacent bidirectional alignment
in progress. Currently x(2, 2) = 1 with zl(−1) = 1 and
</figureCaption>
<equation confidence="0.9187162">
z↔(−1) = 1. The last transition was from x(1, 3) with
z↔0(−1) = 1, z↔0(0) = 1, zl0(0) = 1.
Additionally adjacent, non-overlapping
matches are assessed a penalty α calculated as
α|k|(z&amp;quot;j(k) + z i,j(k))
</equation>
<bodyText confidence="0.999808">
where α &lt; 0 is a parameter of the model. The
example in Figure 4(a) includes a 3α penalty.
Adding these penalties gives the complete adja-
cent agreement problem
</bodyText>
<equation confidence="0.987454875">
π[0, 0] ← EJj=1 max{0, ρ[0, j]}
for i ∈ [I], j ∈ [J]0, zl, z↔ ∈ {0, 1}|K |do
π[i, j, z] ←
max
j&apos;∈[J]0,
z&apos;∈N(z,j−j&apos;)
+zl(k)α|k|
return maxj∈[J]0,z∈{0,1}|zxz |π[I, j, z]
</equation>
<figureCaption confidence="0.8647345">
Figure 5: Modified Viterbi algorithm for computing the adja-
cent agreement L(λ).
</figureCaption>
<bodyText confidence="0.9947165">
the proposed zi,j in the inner loop, we can include
the scores of the adjacent y alignments that are
in neighboring columns, as well as the possible
penalty for matching x(i, j) to a y(i + k, j) in a
different row. Figure 4(b) gives an example set-
ting of z.
In the dynamic program, we need to ensure that
the transitions between the z’s are consistent. The
vector z&apos; indicates the y links adjacent to x(i −
1, j&apos;). If j&apos; is near to j, z&apos; may overlap with z
and vice-versa. The transition set N ensures these
indicators match up
</bodyText>
<equation confidence="0.423685733333333">
I
i=1
h(z) =
�
kE1c
J
j=1
θ(j0, i, j) + π[i − 1, j0, z0]
z↔(k)(ρ[i, j + k] + α|k|)
�
+
k∈K
arg max f(x) + g(y) + h(z) N(z, k0) = I z0 : (zl(−1) ∧ k0 ∈ K) ⇒ z↔0(k0),
zEZ(x,y) (zl0(1) ∧ k0 ∈ K) ⇒ z↔(−k0),
xEX,yEY Ek∈K zl(k) = 1
</equation>
<bodyText confidence="0.99951725">
Next, apply the same relaxation from Sec-
tion 3.1, i.e. we relax the forward constraints of
the f—*e set. This yields the following Lagrangian
dual
</bodyText>
<equation confidence="0.815829">
L(A) = max f(x) + g&apos;(y; w, A) + h(z)
zEZ(x,y)
xEX,yEY0
</equation>
<bodyText confidence="0.919303538461538">
Despite the new constraints, we can still com-
pute L(A) in O(IJ(I + J)) time using a variant
of the Viterbi algorithm. The main idea will be to
consider possible adjacent settings for each link.
Since each z;,j and z i,j only have a constant num-
ber of settings, this does not increase the asymp-
totic complexity of the algorithm.
Figure 5 shows the algorithm for computing
L(A). The main loop of the algorithm is similar to
Figure 3. It proceeds row-by-row, picking the best
alignment x(i, j) = 1. The major change is that
the chart π also stores a value z E {0,1}1cx1c rep-
resenting a possible zI,j, z i,j pair. Since we have
</bodyText>
<sectionHeader confidence="0.979483" genericHeader="method">
5 Adding Back Constraints
</sectionHeader>
<bodyText confidence="0.999940176470588">
In general, it can be shown that Lagrangian relax-
ation is only guaranteed to solve a linear program-
ming relaxation of the underlying combinatorial
problem. For difficult instances, we will see that
this relaxation often does not yield provably exact
solutions. However, it is possible to “tighten” the
relaxation by re-introducing constraints from the
original problem.
In this section, we extend the algorithm to al-
low incrementally re-introducing constraints. In
particular we track which constraints are most of-
ten violated in order to explicitly enforce them in
the algorithm.
Define a binary vector p E {0,1}[I−1]0x[J]0
where p(i, j) = 1 indicates a previously re-
laxed constraint on link y(i, j) that should be re-
introduced into the problem. Let the new partially
</bodyText>
<page confidence="0.976426">
1485
</page>
<bodyText confidence="0.905057">
constrained Lagrangian dual be defined as
</bodyText>
<equation confidence="0.9351422">
L(A; p) = max f(x) + g0(y; w, A) + h(z)
z∈Z(x,y)
x∈X,y∈Y&apos;
y(i,j) = � y(i, i0, j + 1) bi, j : p(i, j) = 1
i&apos;
</equation>
<bodyText confidence="0.998996233333333">
If p = 1, the problem includes all of the original
constraints, whereas p = 0� gives our original La-
grangian dual. In between we have progressively
more constrained variants.
In order to compute the arg max of this op-
timization problem, we need to satisfy the con-
straints within the Viterbi algorithm. We augment
the Viterbi chart with a count vector d E D where
D C Z||p||1 and d(i, j) is a count for the (i, j)’th
constraint, i.e. d(i, j) = y(i, j) − Ei&apos; y(i0, i, j).
Only solutions with count 0 at the final position
satisfy the active constraints. Additionally de-
fine a helper function [·]D as the projection from
Z[I−1]0×[J] —* D, which truncates dimensions
without constraints.
Figure 6 shows this constrained Viterbi relax-
ation approach. It takes p as an argument and en-
forces the active constraints. For simplicity, we
show the full agreement version, but the adjacent
agreement version is similar. The main new addi-
tion is that the inner loop of the algorithm ensures
that the count vector d is the sum of the counts of
its children d0 and d − d0.
Since each additional constraint adds a dimen-
sion to d, adding constraints has a multiplicative
impact on running time. Asymptotically the new
algorithm requires O(2||p||1IJ(I + J)) time. This
is a problem in practice as even adding a few con-
straints can make the problem intractable. We ad-
dress this issue in the next section.
</bodyText>
<sectionHeader confidence="0.990782" genericHeader="method">
6 Pruning
</sectionHeader>
<bodyText confidence="0.869296647058824">
Re-introducing constraints can lead to an expo-
nential blow-up in the search space of the Viterbi
algorithm. In practice though, many alignments
in this space are far from optimal, e.g. align-
ing a common word like the to nous instead
of les. Since Lagrangian relaxation re-computes
the alignment many times, it would be preferable
to skip these links in later rounds, particularly after
re-introducing constraints.
In this section we describe an optimality pre-
serving coarse-to-fine algorithm for pruning. Ap-
proximate coarse-to-fine pruning algorithms are
procedure CONSVITERBIFULL(θ, ω&apos;, p)
for i ∈ [I], j ∈ [J]0, i&apos; ∈ [I] do
d ← |δ(i, j) − δ(i&apos;, j − 1)|D
ρ[i, j, d] ← ω&apos;(i&apos;, i, j)
for j ∈ [J], d ∈ D do
</bodyText>
<equation confidence="0.990371727272727">
π[0, 0, d] ← max π[0, 0, d&apos;] + ρ[0, j, d − d&apos;]
d&apos;ED
for i ∈ [I], j ∈ [J]0, d ∈ D do
if j = 0 then
π[i, j, d] ← max
j&apos;E[J]0
else
π[i, j, d] ←
θ(j&apos;, i, j) + π[i − 1, j&apos;, d&apos;]
+ρ[i, j, d − d&apos;]
return maxjE[J]0 π[I, j, 0]
</equation>
<figureCaption confidence="0.998753666666667">
Figure 6: Constrained Viterbi algorithm for finding partially-
constrained, full-agreement alignments. The argument p in-
dicates which constraints to enforce.
</figureCaption>
<bodyText confidence="0.999648285714286">
widely used within NLP, but exact pruning is
less common. Our method differs in that it
only eliminates non-optimal transitions based on
a lower-bound score. After introducing the prun-
ing method, we present an algorithm to make this
method effective in practice by producing high-
scoring lower bounds for adjacent agreement.
</bodyText>
<subsectionHeader confidence="0.995186">
6.1 Thresholding Max-Marginals
</subsectionHeader>
<bodyText confidence="0.999384">
Our pruning method is based on removing transi-
tions with low max-marginal values. Define the
max-marginal value of an e—*f transition in our
Lagrangian dual as
</bodyText>
<equation confidence="0.9973005">
M(j0, i, j; A) = max
z∈Z(x,y), f(x) + g0(y;A) + h(z)
x∈X,y∈Y&apos;
s.t. x(j0, i, j) = 1
</equation>
<bodyText confidence="0.9508795">
where M gives the value of the best dual align-
ment that transitions from (i − 1, j0) to (i, j).
These max-marginals can be computed by running
a forward-backward variant of any of the algo-
rithms described thus far.
We make the following claim about max-
marginal values and any lower-bound score
Lemma 1 (Safe Pruning). For any valid con-
strained alignment x E X, y E Y, z E i(x, y)
and for any dual vector A E R[I−1]0×[J]0, if there
exists a transition j0, i, j with max-marginal value
M(j0, i, j; A) &lt; f(x) + g(y) + h(z) then the tran-
sition will not be in the optimal alignment, i.e.
x∗(j0,i,j) = 0.
This lemma tells us that we can prune transi-
tions whose dual max-marginal value falls below
</bodyText>
<equation confidence="0.924722">
θ(j&apos;, i, j) + π[i − 1, j&apos;, d]
max
j&apos;E[J]o,d&apos;ED
</equation>
<page confidence="0.921463">
1486
</page>
<bodyText confidence="0.9997475">
a threshold without pruning possibly optimal tran-
sitions. Pruning these transitions can speed up La-
grangian relaxation without altering its properties.
Furthermore, the threshold is determined by any
feasible lower bound on the optimal score, which
means that better bounds can lead to more pruning.
</bodyText>
<subsectionHeader confidence="0.999859">
6.2 Finding Lower Bounds
</subsectionHeader>
<bodyText confidence="0.998686153846154">
Since the effectiveness of pruning is dependent on
the lower bound, it is crucial to be able to produce
high-scoring alignments that satisfy the agreement
constraints. Unfortunately, this problem is non-
trivial. For instance, taking the union of direc-
tional alignments does not guarantee a feasible so-
lution; whereas taking the intersection is trivially
feasible but often not high-scoring.
To produce higher-scoring feasible bidirectional
alignments we introduce a greedy heuristic al-
gorithm. The algorithm starts with any feasible
alignment (x, y, z). It runs the following greedy
loop:
</bodyText>
<listItem confidence="0.997256666666667">
1. Repeat until there exists no x(i, 0) = 1 or
y(0, j) = 1, or there is no score increase.
(a) For each i E [I], j E [J]0, k E K :
x(i, 0) = 1, check if x(i, j) &lt;-- 1 and
y(i, j + k) &lt;-- 1 is feasible, remember
score.
(b) For each i E [I]0, j E [J], k E K :
y(0, j) = 1, check if y(i, j) &lt;-- 1 and
x(i + k, j) &lt;-- 1 is feasible, remember
score.
(c) Let (x, y, z) be the highest-scoring fea-
sible solution produced.
</listItem>
<bodyText confidence="0.999939769230769">
This algorithm produces feasible alignments with
monotonically increasing score, starting from the
intersection of the alignments. It has run-time of
O(IJ(I + J)) since each inner loop enumerates
IJ possible updates and assigns at least one index
a non-zero value, limiting the outer loop to I + J
iterations.
In practice we initialize the heuristic based on
the intersection of x and y at the current round
of Lagrangian relaxation. Experiments show that
running this algorithm significantly improves the
lower bound compared to just taking the intersec-
tion, and consequently helps pruning significantly.
</bodyText>
<sectionHeader confidence="0.999807" genericHeader="method">
7 Related Work
</sectionHeader>
<bodyText confidence="0.999988210526316">
The most common techniques for bidirectional
alignment are post-hoc combinations, such as
union or intersection, of directional models, (Och
et al., 1999), or more complex heuristic combiners
such as grow-diag-final (Koehn et al., 2003).
Several authors have explored explicit bidirec-
tional models in the literature. Cromieres and
Kurohashi (2009) use belief propagation on a fac-
tor graph to train and decode a one-to-one word
alignment problem. Qualitatively this method is
similar to ours, although the model and decoding
algorithm are different, and their method is not
able to provide certificates of optimality.
A series of papers by Ganchev et al. (2010),
Graca et al. (2008), and Ganchev et al. (2008) use
posterior regularization to constrain the posterior
probability of the word alignment problem to be
symmetric and bijective. This work acheives state-
of-the-art performance for alignment. Instead of
utilizing posteriors our model tries to decode a sin-
gle best one-to-one word alignment.
A different approach is to use constraints at
training time to obtain models that favor bidi-
rectional properties. Liang et al. (2006) propose
agreement-based learning, which jointly learns
probabilities by maximizing a combination of
likelihood and agreement between two directional
models.
General linear programming approaches have
also been applied to word alignment problems.
Lacoste-Julien et al. (2006) formulate the word
alignment problem as quadratic assignment prob-
lem and solve it using an integer linear program-
ming solver.
Our work is most similar to DeNero and
Macherey (2011), which uses dual decomposition
to encourage agreement between two directional
HMM aligners during decoding time.
</bodyText>
<sectionHeader confidence="0.996299" genericHeader="evaluation">
8 Experiments
</sectionHeader>
<bodyText confidence="0.999276923076923">
Our experimental results compare the accuracy
and optimality of our decoding algorithm to direc-
tional alignment models and previous work on this
bidirectional model.
Data and Setup The experimental setup is iden-
tical to DeNero and Macherey (2011). Evalu-
ation is performed on a hand-aligned subset of
the NIST 2002 Chinese-English dataset (Ayan and
Dorr, 2006). Following past work, the first 150
sentence pairs of the training section are used for
evaluation. The potential parameters θ and w are
set based on unsupervised HMM models trained
on the LDC FBIS corpus (6.2 million words).
</bodyText>
<page confidence="0.97856">
1487
</page>
<table confidence="0.999817333333333">
1-20 (28%) 21-40 (45%) 41-60 (27%) all
time cert exact time cert exact time cert exact time cert exact
ILP 15.12 100.0 100.0 364.94 100.0 100.0 2,829.64 100.0 100.0 924.24 100.0 100.0
LR 0.55 97.6 97.6 4.76 55.9 55.9 15.06 7.5 7.5 6.33 54.7 54.7
CONS 0.43 100.0 100.0 9.86 95.6 95.6 61.86 55.0 62.5 21.08 86.0 88.0
D&amp;M - 6.2 - - 0.0 - - 0.0 - - 6.2 -
</table>
<tableCaption confidence="0.985392666666667">
Table 1: Experimental results for model accuracy of bilingual alignment. Column time is the mean time per sentence pair in
seconds; cert is the percentage of sentence pairs solved with a certificate of optimality; exact is the percentage of sentence pairs
solved exactly. Results are grouped by sentence length. The percentage of sentence pairs in each group is shown in parentheses.
</tableCaption>
<bodyText confidence="0.999696071428571">
Training is performed using the agreement-based
learning method which encourages the directional
models to overlap (Liang et al., 2006). This direc-
tional model has been shown produce state-of-the-
art results with this setup (Haghighi et al., 2009).
Baselines We compare the algorithm described
in this paper with several baseline methods. DIR
includes post-hoc combinations of the e→f and
f→e HMM-based aligners. Variants include
union, intersection, and grow-diag-final. D&amp;M is
the dual decomposition algorithm for bidirectional
alignment as presented by DeNero and Macherey
(2011) with different final combinations. LR is the
Lagrangian relaxation algorithm applied to the ad-
jacent agreement problem without the additional
constraints described in Section 5. CONS is our
full Lagrangian relaxation algorithm including in-
cremental constraint addition. ILP uses a highly-
optimized general-purpose integer linear program-
ming solver to solve the lattice with the constraints
described (Gurobi Optimization, 2013).
Implementation The main task of the decoder
is to repeatedly compute the arg max of L(A).
To speed up decoding, our implementation fully
instantiates the Viterbi lattice for a problem in-
stance. This approach has several benefits: each
iteration can reuse the same lattice structure; max-
marginals can be easily computed with a gen-
eral forward-backward algorithm; pruning corre-
sponds to removing lattice edges; and adding con-
straints can be done through lattice intersection.
For consistency, we implement each baseline (ex-
cept for D&amp;M) through the same lattice.
Parameter Settings We run 400 iterations of
the subgradient algorithm using the rate schedule
nt = 0.95t0 where t&apos; is the count of updates for
which the dual value did not improve. Every 10
iterations we run the greedy decoder to compute
a lower bound. If the gap between our current
dual value L(A) and the lower bound improves
significantly we run coarse-to-fine pruning as de-
scribed in Section 6 with the best lower bound. For
</bodyText>
<table confidence="0.999523222222222">
Model Combiner alignment phrase pair
Prec Rec AER Prec Rec F1
union 57.6 80.0 33.4 75.1 33.5 46.3
DIR intersection 86.2 62.9 27.0 64.3 43.5 51.9
grow-diag 59.7 79.5 32.1 70.1 36.9 48.4
union 63.3 81.5 29.1 63.2 44.9 52.5
D&amp;M intersection 77.5 75.1 23.6 57.1 53.6 55.3
grow-diag 65.6 80.6 28.0 60.2 47.4 53.0
CONS 72.5 74.9 26.4 53.0 52.4 52.7
</table>
<tableCaption confidence="0.997372">
Table 2: Alignment accuracy and phrase pair extraction ac-
</tableCaption>
<bodyText confidence="0.960356857142857">
curacy for directional and bidirectional models. Prec is the
precision. Rec is the recall. AER is alignment error rate and
F1 is the phrase pair extraction F1 score.
CONS, if the algorithm does not find an optimal
solution we run 400 more iterations and incremen-
tally add the 5 most violated constraints every 25
iterations.
Results Our first set of experiments looks at the
model accuracy and the decoding time of various
methods that can produce optimal solutions. Re-
sults are shown in Table 1. D&amp;M is only able to
find the optimal solution with certificate on 6% of
instances. The relaxation algorithm used in this
work is able to increase that number to 54.7%.
With incremental constraints and pruning, we are
able to solve over 86% of sentence pairs includ-
ing many longer and more difficult pairs. Addi-
tionally the method finds these solutions with only
a small increase in running time over Lagrangian
relaxation, and is significantly faster than using an
ILP solver.
Next we compare the models in terms of align-
ment accuracy. Table 2 shows the precision, recall
and alignment error rate (AER) for word align-
ment. We consider union, intersection and grow-
diag-final as combination procedures. The com-
bination procedures are applied to D&amp;M in the
case when the algorithm does not converge. For
CONS, we use the optimal solution for the 86%
of instances that converge and the highest-scoring
greedy solution for those that do not. The pro-
posed method has an AER of 26.4, which outper-
forms each of the directional models. However,
although CONS achieves a higher model score
than D&amp;M, it performs worse in accuracy. Ta-
</bodyText>
<page confidence="0.971748">
1488
</page>
<table confidence="0.726199">
1-20 21-40 41-60 all
# cons. 20.0 32.1 39.5 35.9
</table>
<tableCaption confidence="0.992482666666667">
Table 3: The average number of constraints added for sen-
tence pairs where Lagrangian relaxation is not able to find an
exact solution.
</tableCaption>
<bodyText confidence="0.99997047826087">
ble 2 also compares the models in terms of phrase-
extraction accuracy (Ayan and Dorr, 2006). We
use the phrase extraction algorithm described by
DeNero and Klein (2010), accounting for possi-
ble links and c alignments. CONS performs bet-
ter than each of the directional models, but worse
than the best D&amp;M model.
Finally we consider the impact of constraint ad-
dition, pruning, and use of a lower bound. Table 3
gives the average number of constraints added for
sentence pairs for which Lagrangian relaxation
alone does not produce a certificate. Figure 7(a)
shows the average over all sentence pairs of the
best dual and best primal scores. The graph com-
pares the use of the greedy algorithm from Sec-
tion 6.2 with the simple intersection of x and y.
The difference between these curves illustrates the
benefit of the greedy algorithm. This is reflected
in Figure 7(b) which shows the effectiveness of
coarse-to-fine pruning over time. On average, the
pruning reduces the search space of each sentence
pair to 20% of the initial search space after 200
iterations.
</bodyText>
<sectionHeader confidence="0.995751" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.999966529411765">
We have introduced a novel Lagrangian relaxation
algorithm for a bidirectional alignment model that
uses incremental constraint addition and coarse-
to-fine pruning to find exact solutions. The algo-
rithm increases the number of exact solution found
on the model of DeNero and Macherey (2011)
from 6% to 86%.
Unfortunately despite achieving higher model
score, this approach does not produce more accu-
rate alignments than the previous algorithm. This
suggests that the adjacent agreement model may
still be too constrained for this underlying task.
Implicitly, an approach with fewer exact solu-
tions may allow for useful violations of these con-
straints. In future work, we hope to explore bidi-
rectional models with soft-penalties to explicitly
permit these violations.
</bodyText>
<subsectionHeader confidence="0.728961">
A Proof of NP-Hardness
</subsectionHeader>
<bodyText confidence="0.854419142857143">
We can show that the bidirectional alignment
problem is NP-hard by reduction from the trav-
(a) The best dual and the best primal score, relative to the
optimal score, averaged over all sentence pairs. The best
primal curve uses a feasible greedy algorithm, whereas the
intersection curve is calculated by taking the intersec-
tion of x and y.
</bodyText>
<figure confidence="0.973926">
1.0
0.8
0.6
0.4
0.2
0 50 100 150 200 250 300 350 400
number of iterations
(b) A graph showing the effectiveness of coarse-to-fine prun-
</figure>
<figureCaption confidence="0.7290735">
ing. Relative search space size is the size of the pruned lattice
compared to the initial size. The plot shows an average over
all sentence pairs.
Figure 7
</figureCaption>
<bodyText confidence="0.8923135">
eling salesman problem (TSP). A TSP instance
with N cities has distance c(i&apos;, i) for each (i&apos;, i) ∈
[N]&apos;. We can construct a sentence pair in which
I = J = N and c-alignments have infinite cost.
</bodyText>
<equation confidence="0.9988895">
w(i0, i, j) = −c(i0, i) `di0 E [N]o, i E [N], j E [N]
0(j0, i, j) = 0 `dj0 E [N]o, i E [N], j E [N]
w(i0, 0, j) = −oo `di0 E [N]o, j E [N]
0(j0, i, 0) = −oo `dj0 E [N]o, i E [N]
</equation>
<bodyText confidence="0.9706555">
Every bidirectional alignment with finite objec-
tive score must align exactly one word in e to each
word in f, encoding a permutation a. Moreover,
each possible permutation has a finite score: the
negation of the total distance to traverse the N
cities in order a under distance c. Therefore, solv-
ing such a bidirectional alignment problem would
find a minimal Hamiltonian path of the TSP en-
coded in this way, concluding the reduction.
Acknowledgments Alexander Rush, Yin-Wen
Chang and Michael Collins were all supported
by NSF grant IIS-1161814. Alexander Rush was
partially supported by an NSF Graduate Research
Fellowship.
</bodyText>
<figure confidence="0.999411416666667">
0 50 100 150 200 250 300 350 400
iteration
score relative to optimal 100
50
0
50
100
best dual
best primal
intersection
relative search space size
0.0
</figure>
<page confidence="0.988384">
1489
</page>
<sectionHeader confidence="0.99244" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999610462365592">
Necip Fazil Ayan and Bonnie J Dorr. 2006. Going
beyond aer: An extensive analysis of word align-
ments and their impact on mt. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the Asso-
ciation for Computational Linguistics, pages 9–16.
Association for Computational Linguistics.
Peter F Brown, Vincent J Della Pietra, Stephen A Della
Pietra, and Robert L Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational linguistics, 19(2):263–311.
Fabien Cromieres and Sadao Kurohashi. 2009. An
alignment algorithm using belief propagation and a
structure-based distortion model. In Proceedings
of the 12th Conference of the European Chapter
of the Association for Computational Linguistics,
pages 166–174. Association for Computational Lin-
guistics.
John DeNero and Dan Klein. 2010. Discriminative
modeling of extraction sets for machine translation.
In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics, pages
1453–1463. Association for Computational Linguis-
tics.
John DeNero and Klaus Macherey. 2011. Model-
based aligner combination using dual decomposi-
tion. In ACL, pages 420–429.
Kuzman Ganchev, Jo˜ao V. Grac¸a, and Ben Taskar.
2008. Better alignments = better translations?
In Proceedings of ACL-08: HLT, pages 986–993,
Columbus, Ohio, June. Association for Computa-
tional Linguistics.
K. Ganchev, J. Grac¸a, J. Gillenwater, and B. Taskar.
2010. Posterior Regularization for Structured La-
tent Variable Models. Journal of Machine Learning
Research, 11:2001–2049.
Joao Graca, Kuzman Ganchev, and Ben Taskar. 2008.
Expectation maximization and posterior constraints.
In J.C. Platt, D. Koller, Y. Singer, and S. Roweis,
editors, Advances in Neural Information Processing
Systems 20, pages 569–576. MIT Press, Cambridge,
MA.
Inc. Gurobi Optimization. 2013. Gurobi optimizer ref-
erence manual.
Aria Haghighi, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with su-
pervised itg models. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP: Volume 2-
Volume 2, pages 923–931. Association for Compu-
tational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 48–54. Association for Computa-
tional Linguistics.
Simon Lacoste-Julien, Ben Taskar, Dan Klein, and
Michael I Jordan. 2006. Word alignment via
quadratic assignment. In Proceedings of the main
conference on Human Language Technology Con-
ference of the North American Chapter of the As-
sociation of Computational Linguistics, pages 112–
119. Association for Computational Linguistics.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the main
conference on Human Language Technology Con-
ference of the North American Chapter of the As-
sociation of Computational Linguistics, pages 104–
111. Association for Computational Linguistics.
Robert C Moore. 2005. A discriminative framework
for bilingual word alignment. In Proceedings of
the conference on Human Language Technology and
Empirical Methods in Natural Language Process-
ing, pages 81–88. Association for Computational
Linguistics.
Franz Josef Och, Christoph Tillmann, Hermann Ney,
et al. 1999. Improved alignment models for statis-
tical machine translation. In Proc. of the Joint SIG-
DAT Conf. on Empirical Methods in Natural Lan-
guage Processing and Very Large Corpora, pages
20–28.
Alexander M Rush and Michael Collins. 2012. A tuto-
rial on dual decomposition and lagrangian relaxation
for inference in natural language processing. Jour-
nal of Artificial Intelligence Research, 45:305–362.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. Hmm-based word alignment in statistical
translation. In Proceedings of the 16th conference
on Computational linguistics-Volume 2, pages 836–
841. Association for Computational Linguistics.
</reference>
<page confidence="0.990025">
1490
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.256057">
<title confidence="0.99989">A Constrained Viterbi Relaxation for Bidirectional Word Alignment</title>
<author confidence="0.998329">Yin-Wen Chang Alexander M</author>
<affiliation confidence="0.993587">MIT</affiliation>
<address confidence="0.98075">Cambridge, MA</address>
<email confidence="0.997665">csail.mit.edu</email>
<author confidence="0.493951">John</author>
<affiliation confidence="0.996373">UC</affiliation>
<address confidence="0.941847">Berkeley, CA</address>
<email confidence="0.999112">cs.berkeley.edu</email>
<author confidence="0.84185">Michael</author>
<affiliation confidence="0.708456">Columbia</affiliation>
<address confidence="0.883126">New York, NY</address>
<email confidence="0.999291">cs.columbia.edu</email>
<abstract confidence="0.996969952380953">Bidirectional models of word alignment are an appealing alternative to post-hoc combinations of directional word aligners. Unfortunately, most bidirectional formulations are NP-Hard to solve, and a previous attempt to use a relaxationbased decoder yielded few exact solutions (6%). We present a novel relaxation for decoding the bidirectional model of DeNero and Macherey (2011). The relaxation can be solved with a modified version of the Viterbi algorithm. To find optimal solutions on difficult instances, we alternate between incrementally adding constraints and applying optimality-preserving coarse-to-fine pruning. The algorithm finds provably exsolutions on of sentence pairs and shows improvements over directional models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Necip Fazil Ayan</author>
<author>Bonnie J Dorr</author>
</authors>
<title>Going beyond aer: An extensive analysis of word alignments and their impact on mt.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>9--16</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="26448" citStr="Ayan and Dorr, 2006" startWordPosition="4767" endWordPosition="4770">ic assignment problem and solve it using an integer linear programming solver. Our work is most similar to DeNero and Macherey (2011), which uses dual decomposition to encourage agreement between two directional HMM aligners during decoding time. 8 Experiments Our experimental results compare the accuracy and optimality of our decoding algorithm to directional alignment models and previous work on this bidirectional model. Data and Setup The experimental setup is identical to DeNero and Macherey (2011). Evaluation is performed on a hand-aligned subset of the NIST 2002 Chinese-English dataset (Ayan and Dorr, 2006). Following past work, the first 150 sentence pairs of the training section are used for evaluation. The potential parameters θ and w are set based on unsupervised HMM models trained on the LDC FBIS corpus (6.2 million words). 1487 1-20 (28%) 21-40 (45%) 41-60 (27%) all time cert exact time cert exact time cert exact time cert exact ILP 15.12 100.0 100.0 364.94 100.0 100.0 2,829.64 100.0 100.0 924.24 100.0 100.0 LR 0.55 97.6 97.6 4.76 55.9 55.9 15.06 7.5 7.5 6.33 54.7 54.7 CONS 0.43 100.0 100.0 9.86 95.6 95.6 61.86 55.0 62.5 21.08 86.0 88.0 D&amp;M - 6.2 - - 0.0 - - 0.0 - - 6.2 - Table 1: Experime</context>
<context position="31715" citStr="Ayan and Dorr, 2006" startWordPosition="5640" endWordPosition="5643"> not converge. For CONS, we use the optimal solution for the 86% of instances that converge and the highest-scoring greedy solution for those that do not. The proposed method has an AER of 26.4, which outperforms each of the directional models. However, although CONS achieves a higher model score than D&amp;M, it performs worse in accuracy. Ta1488 1-20 21-40 41-60 all # cons. 20.0 32.1 39.5 35.9 Table 3: The average number of constraints added for sentence pairs where Lagrangian relaxation is not able to find an exact solution. ble 2 also compares the models in terms of phraseextraction accuracy (Ayan and Dorr, 2006). We use the phrase extraction algorithm described by DeNero and Klein (2010), accounting for possible links and c alignments. CONS performs better than each of the directional models, but worse than the best D&amp;M model. Finally we consider the impact of constraint addition, pruning, and use of a lower bound. Table 3 gives the average number of constraints added for sentence pairs for which Lagrangian relaxation alone does not produce a certificate. Figure 7(a) shows the average over all sentence pairs of the best dual and best primal scores. The graph compares the use of the greedy algorithm f</context>
</contexts>
<marker>Ayan, Dorr, 2006</marker>
<rawString>Necip Fazil Ayan and Bonnie J Dorr. 2006. Going beyond aer: An extensive analysis of word alignments and their impact on mt. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 9–16. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1341" citStr="Brown et al., 1993" startWordPosition="191" endWordPosition="194">lved with a modified version of the Viterbi algorithm. To find optimal solutions on difficult instances, we alternate between incrementally adding constraints and applying optimality-preserving coarse-to-fine pruning. The algorithm finds provably exact solutions on 86% of sentence pairs and shows improvements over directional models. 1 Introduction Word alignment is a critical first step for building statistical machine translation systems. In order to ensure accurate word alignments, most systems employ a post-hoc symmetrization step to combine directional word aligners, such as IBM Model 4 (Brown et al., 1993) or hidden Markov model (HMM) based aligners (Vogel et al., 1996). Several authors have proposed bidirectional models that incorporate this step directly, but decoding under many bidirectional models is NP-Hard and finding exact solutions has proven difficult. In this paper, we describe a novel Lagrangianrelaxation based decoder for the bidirectional model proposed by DeNero and Macherey (2011), with the goal of improving search accuracy. In that work, the authors implement a dual decomposition-based decoder for the problem, but are only able to find exact solutions for around 6% of instances.</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F Brown, Vincent J Della Pietra, Stephen A Della Pietra, and Robert L Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabien Cromieres</author>
<author>Sadao Kurohashi</author>
</authors>
<title>An alignment algorithm using belief propagation and a structure-based distortion model.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>166--174</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="24720" citStr="Cromieres and Kurohashi (2009)" startWordPosition="4504" endWordPosition="4507">e heuristic based on the intersection of x and y at the current round of Lagrangian relaxation. Experiments show that running this algorithm significantly improves the lower bound compared to just taking the intersection, and consequently helps pruning significantly. 7 Related Work The most common techniques for bidirectional alignment are post-hoc combinations, such as union or intersection, of directional models, (Och et al., 1999), or more complex heuristic combiners such as grow-diag-final (Koehn et al., 2003). Several authors have explored explicit bidirectional models in the literature. Cromieres and Kurohashi (2009) use belief propagation on a factor graph to train and decode a one-to-one word alignment problem. Qualitatively this method is similar to ours, although the model and decoding algorithm are different, and their method is not able to provide certificates of optimality. A series of papers by Ganchev et al. (2010), Graca et al. (2008), and Ganchev et al. (2008) use posterior regularization to constrain the posterior probability of the word alignment problem to be symmetric and bijective. This work acheives stateof-the-art performance for alignment. Instead of utilizing posteriors our model tries</context>
</contexts>
<marker>Cromieres, Kurohashi, 2009</marker>
<rawString>Fabien Cromieres and Sadao Kurohashi. 2009. An alignment algorithm using belief propagation and a structure-based distortion model. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, pages 166–174. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>Discriminative modeling of extraction sets for machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1453--1463</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="31792" citStr="DeNero and Klein (2010)" startWordPosition="5652" endWordPosition="5655">ces that converge and the highest-scoring greedy solution for those that do not. The proposed method has an AER of 26.4, which outperforms each of the directional models. However, although CONS achieves a higher model score than D&amp;M, it performs worse in accuracy. Ta1488 1-20 21-40 41-60 all # cons. 20.0 32.1 39.5 35.9 Table 3: The average number of constraints added for sentence pairs where Lagrangian relaxation is not able to find an exact solution. ble 2 also compares the models in terms of phraseextraction accuracy (Ayan and Dorr, 2006). We use the phrase extraction algorithm described by DeNero and Klein (2010), accounting for possible links and c alignments. CONS performs better than each of the directional models, but worse than the best D&amp;M model. Finally we consider the impact of constraint addition, pruning, and use of a lower bound. Table 3 gives the average number of constraints added for sentence pairs for which Lagrangian relaxation alone does not produce a certificate. Figure 7(a) shows the average over all sentence pairs of the best dual and best primal scores. The graph compares the use of the greedy algorithm from Section 6.2 with the simple intersection of x and y. The difference betwe</context>
</contexts>
<marker>DeNero, Klein, 2010</marker>
<rawString>John DeNero and Dan Klein. 2010. Discriminative modeling of extraction sets for machine translation. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1453–1463. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Klaus Macherey</author>
</authors>
<title>Modelbased aligner combination using dual decomposition.</title>
<date>2011</date>
<booktitle>In ACL,</booktitle>
<pages>420--429</pages>
<contexts>
<context position="696" citStr="DeNero and Macherey (2011)" startWordPosition="93" endWordPosition="96">Wen Chang Alexander M. Rush MIT CSAIL, Cambridge, MA 02139 {yinwen,srush}@ csail.mit.edu John DeNero UC Berkeley, Berkeley, CA 94720 denero@ cs.berkeley.edu Michael Collins Columbia University, New York, NY 10027 mcollins@ cs.columbia.edu Abstract Bidirectional models of word alignment are an appealing alternative to post-hoc combinations of directional word aligners. Unfortunately, most bidirectional formulations are NP-Hard to solve, and a previous attempt to use a relaxationbased decoder yielded few exact solutions (6%). We present a novel relaxation for decoding the bidirectional model of DeNero and Macherey (2011). The relaxation can be solved with a modified version of the Viterbi algorithm. To find optimal solutions on difficult instances, we alternate between incrementally adding constraints and applying optimality-preserving coarse-to-fine pruning. The algorithm finds provably exact solutions on 86% of sentence pairs and shows improvements over directional models. 1 Introduction Word alignment is a critical first step for building statistical machine translation systems. In order to ensure accurate word alignments, most systems employ a post-hoc symmetrization step to combine directional word align</context>
<context position="2529" citStr="DeNero and Macherey (2011)" startWordPosition="378" endWordPosition="381"> solutions for around 6% of instances. Our decoder uses a simple variant of the Viterbi algorithm for solving a relaxed version of this model. The algorithm makes it easy to reintroduce constraints for difficult instances, at the cost of increasing run-time complexity. To offset this cost, we employ optimality-preserving coarseto-fine pruning to reduce the search space. The pruning method utilizes lower bounds on the cost of valid bidirectional alignments, which we obtain from a fast, greedy decoder. The method has the following properties: • It is based on a novel relaxation for the model of DeNero and Macherey (2011), solvable with a variant of the Viterbi algorithm. • To find optimal solutions, it employs an efficient strategy that alternates between adding constraints and applying pruning. • Empirically, it is able to find exact solutions on 86% of sentence pairs and is significantly faster than general-purpose solvers. We begin in Section 2 by formally describing the directional word alignment problem. Section 3 describes a preliminary bidirectional model using full agreement constraints and a Lagrangian relaxation-based solver. Section 4 modifies this model to include adjacency constraints. Section 5 </context>
<context position="13586" citStr="DeNero and Macherey (2011)" startWordPosition="2454" endWordPosition="2457">ables, adding in the score of the y(i, j) necessary to satisfy the constraints. procedure VITERBIFULL(θ, ω0) Let π, ρ be dynamic programming charts. ω0(i0, i, j) `d i E [I], j E [J]0 π[0, 0] — EJj=1 max{0, ρ[0, j]1 for i E [I], j E [J]0 in order do θ(j0, i, j) + π[i — 1, j0] if j =� 0 then π[i, j] +-- π[i, j] + ρ[i, j] return maxj∈[J]0 π[I, j] Figure 3: Viterbi-style algorithm for computing L(λ). For simplicity the algorithm shows the max version of the algorithm, arg max can be computed with back-pointers. 4 Adjacent Agreement Enforcing full agreement can be too strict an alignment criteria. DeNero and Macherey (2011) instead propose a model that allows near matches, which we call adjacent agreement. Adjacent agreement allows links from one direction to agree with adjacent links from the reverse alignment for a small penalty. Figure 4(a) shows an example of a valid bidirectional alignment under adjacent agreement. In this section we formally introduce adjacent agreement, and propose a relaxation algorithm for this model. The key algorithmic idea is to extend the Viterbi algorithm in order to consider possible adjacent links in the reverse direction. 4.1 Enforcing Adjacency Define the adjacency set K = {−1,</context>
<context position="25961" citStr="DeNero and Macherey (2011)" startWordPosition="4694" endWordPosition="4697">ngle best one-to-one word alignment. A different approach is to use constraints at training time to obtain models that favor bidirectional properties. Liang et al. (2006) propose agreement-based learning, which jointly learns probabilities by maximizing a combination of likelihood and agreement between two directional models. General linear programming approaches have also been applied to word alignment problems. Lacoste-Julien et al. (2006) formulate the word alignment problem as quadratic assignment problem and solve it using an integer linear programming solver. Our work is most similar to DeNero and Macherey (2011), which uses dual decomposition to encourage agreement between two directional HMM aligners during decoding time. 8 Experiments Our experimental results compare the accuracy and optimality of our decoding algorithm to directional alignment models and previous work on this bidirectional model. Data and Setup The experimental setup is identical to DeNero and Macherey (2011). Evaluation is performed on a hand-aligned subset of the NIST 2002 Chinese-English dataset (Ayan and Dorr, 2006). Following past work, the first 150 sentence pairs of the training section are used for evaluation. The potentia</context>
<context position="27996" citStr="DeNero and Macherey (2011)" startWordPosition="5021" endWordPosition="5024">pairs in each group is shown in parentheses. Training is performed using the agreement-based learning method which encourages the directional models to overlap (Liang et al., 2006). This directional model has been shown produce state-of-theart results with this setup (Haghighi et al., 2009). Baselines We compare the algorithm described in this paper with several baseline methods. DIR includes post-hoc combinations of the e→f and f→e HMM-based aligners. Variants include union, intersection, and grow-diag-final. D&amp;M is the dual decomposition algorithm for bidirectional alignment as presented by DeNero and Macherey (2011) with different final combinations. LR is the Lagrangian relaxation algorithm applied to the adjacent agreement problem without the additional constraints described in Section 5. CONS is our full Lagrangian relaxation algorithm including incremental constraint addition. ILP uses a highlyoptimized general-purpose integer linear programming solver to solve the lattice with the constraints described (Gurobi Optimization, 2013). Implementation The main task of the decoder is to repeatedly compute the arg max of L(A). To speed up decoding, our implementation fully instantiates the Viterbi lattice f</context>
<context position="32987" citStr="DeNero and Macherey (2011)" startWordPosition="5849" endWordPosition="5852">f x and y. The difference between these curves illustrates the benefit of the greedy algorithm. This is reflected in Figure 7(b) which shows the effectiveness of coarse-to-fine pruning over time. On average, the pruning reduces the search space of each sentence pair to 20% of the initial search space after 200 iterations. 9 Conclusion We have introduced a novel Lagrangian relaxation algorithm for a bidirectional alignment model that uses incremental constraint addition and coarseto-fine pruning to find exact solutions. The algorithm increases the number of exact solution found on the model of DeNero and Macherey (2011) from 6% to 86%. Unfortunately despite achieving higher model score, this approach does not produce more accurate alignments than the previous algorithm. This suggests that the adjacent agreement model may still be too constrained for this underlying task. Implicitly, an approach with fewer exact solutions may allow for useful violations of these constraints. In future work, we hope to explore bidirectional models with soft-penalties to explicitly permit these violations. A Proof of NP-Hardness We can show that the bidirectional alignment problem is NP-hard by reduction from the trav(a) The be</context>
</contexts>
<marker>DeNero, Macherey, 2011</marker>
<rawString>John DeNero and Klaus Macherey. 2011. Modelbased aligner combination using dual decomposition. In ACL, pages 420–429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kuzman Ganchev</author>
<author>Jo˜ao V Grac¸a</author>
<author>Ben Taskar</author>
</authors>
<title>Better alignments = better translations?</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>986--993</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<marker>Ganchev, Grac¸a, Taskar, 2008</marker>
<rawString>Kuzman Ganchev, Jo˜ao V. Grac¸a, and Ben Taskar. 2008. Better alignments = better translations? In Proceedings of ACL-08: HLT, pages 986–993, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Ganchev</author>
<author>J Grac¸a</author>
<author>J Gillenwater</author>
<author>B Taskar</author>
</authors>
<title>Posterior Regularization for Structured Latent Variable Models.</title>
<date>2010</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>11--2001</pages>
<marker>Ganchev, Grac¸a, Gillenwater, Taskar, 2010</marker>
<rawString>K. Ganchev, J. Grac¸a, J. Gillenwater, and B. Taskar. 2010. Posterior Regularization for Structured Latent Variable Models. Journal of Machine Learning Research, 11:2001–2049.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joao Graca</author>
<author>Kuzman Ganchev</author>
<author>Ben Taskar</author>
</authors>
<title>Expectation maximization and posterior constraints.</title>
<date>2008</date>
<booktitle>Advances in Neural Information Processing Systems 20,</booktitle>
<pages>569--576</pages>
<editor>In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="25054" citStr="Graca et al. (2008)" startWordPosition="4560" endWordPosition="4563">t-hoc combinations, such as union or intersection, of directional models, (Och et al., 1999), or more complex heuristic combiners such as grow-diag-final (Koehn et al., 2003). Several authors have explored explicit bidirectional models in the literature. Cromieres and Kurohashi (2009) use belief propagation on a factor graph to train and decode a one-to-one word alignment problem. Qualitatively this method is similar to ours, although the model and decoding algorithm are different, and their method is not able to provide certificates of optimality. A series of papers by Ganchev et al. (2010), Graca et al. (2008), and Ganchev et al. (2008) use posterior regularization to constrain the posterior probability of the word alignment problem to be symmetric and bijective. This work acheives stateof-the-art performance for alignment. Instead of utilizing posteriors our model tries to decode a single best one-to-one word alignment. A different approach is to use constraints at training time to obtain models that favor bidirectional properties. Liang et al. (2006) propose agreement-based learning, which jointly learns probabilities by maximizing a combination of likelihood and agreement between two directional</context>
</contexts>
<marker>Graca, Ganchev, Taskar, 2008</marker>
<rawString>Joao Graca, Kuzman Ganchev, and Ben Taskar. 2008. Expectation maximization and posterior constraints. In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20, pages 569–576. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gurobi Optimization</author>
</authors>
<date>2013</date>
<note>Gurobi optimizer reference manual.</note>
<contexts>
<context position="28423" citStr="Optimization, 2013" startWordPosition="5082" endWordPosition="5083">→e HMM-based aligners. Variants include union, intersection, and grow-diag-final. D&amp;M is the dual decomposition algorithm for bidirectional alignment as presented by DeNero and Macherey (2011) with different final combinations. LR is the Lagrangian relaxation algorithm applied to the adjacent agreement problem without the additional constraints described in Section 5. CONS is our full Lagrangian relaxation algorithm including incremental constraint addition. ILP uses a highlyoptimized general-purpose integer linear programming solver to solve the lattice with the constraints described (Gurobi Optimization, 2013). Implementation The main task of the decoder is to repeatedly compute the arg max of L(A). To speed up decoding, our implementation fully instantiates the Viterbi lattice for a problem instance. This approach has several benefits: each iteration can reuse the same lattice structure; maxmarginals can be easily computed with a general forward-backward algorithm; pruning corresponds to removing lattice edges; and adding constraints can be done through lattice intersection. For consistency, we implement each baseline (except for D&amp;M) through the same lattice. Parameter Settings We run 400 iterati</context>
</contexts>
<marker>Optimization, 2013</marker>
<rawString>Inc. Gurobi Optimization. 2013. Gurobi optimizer reference manual.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>John Blitzer</author>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>Better word alignments with supervised itg models.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume</booktitle>
<volume>2</volume>
<pages>923--931</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="27661" citStr="Haghighi et al., 2009" startWordPosition="4975" endWordPosition="4978"> Experimental results for model accuracy of bilingual alignment. Column time is the mean time per sentence pair in seconds; cert is the percentage of sentence pairs solved with a certificate of optimality; exact is the percentage of sentence pairs solved exactly. Results are grouped by sentence length. The percentage of sentence pairs in each group is shown in parentheses. Training is performed using the agreement-based learning method which encourages the directional models to overlap (Liang et al., 2006). This directional model has been shown produce state-of-theart results with this setup (Haghighi et al., 2009). Baselines We compare the algorithm described in this paper with several baseline methods. DIR includes post-hoc combinations of the e→f and f→e HMM-based aligners. Variants include union, intersection, and grow-diag-final. D&amp;M is the dual decomposition algorithm for bidirectional alignment as presented by DeNero and Macherey (2011) with different final combinations. LR is the Lagrangian relaxation algorithm applied to the adjacent agreement problem without the additional constraints described in Section 5. CONS is our full Lagrangian relaxation algorithm including incremental constraint addi</context>
</contexts>
<marker>Haghighi, Blitzer, DeNero, Klein, 2009</marker>
<rawString>Aria Haghighi, John Blitzer, John DeNero, and Dan Klein. 2009. Better word alignments with supervised itg models. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2, pages 923–931. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language TechnologyVolume 1,</booktitle>
<pages>48--54</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8577" citStr="Koehn et al., 2003" startWordPosition="1509" endWordPosition="1512"> in O(IJ2) time and the second in O(I2J) time. 3 Bidirectional Alignment The directional bias of the e-*f and f-*e alignment models may cause them to produce differing alignments. To obtain the best single alignment, it is common practice to use a post-hoc algorithm to merge these directional alignments (Och et al., 1999). First, a directional alignment is found from each word in e to a word f. Next an alignment is produced in the reverse direction from f to e. Finally, these alignments are merged, either through intersection, union, or with an interpolation algorithm such as grow-diag-final (Koehn et al., 2003). In this work, we instead consider a bidirectional alignment model that jointly considers both directional models. We begin in this section by introducing a simple bidirectional model that enforces full agreement between directional models and giving a relaxation for decoding. Section 4 loosens this model to adjacent agreement. 3.1 Enforcing Full Agreement Perhaps the simplest post-hoc merging strategy is to retain the intersection of the two directional models. The analogous bidirectional model enforces full agreement to ensure the two alignments select the same non-null links i.e. x*, y* = </context>
<context position="24609" citStr="Koehn et al., 2003" startWordPosition="4489" endWordPosition="4492">ne index a non-zero value, limiting the outer loop to I + J iterations. In practice we initialize the heuristic based on the intersection of x and y at the current round of Lagrangian relaxation. Experiments show that running this algorithm significantly improves the lower bound compared to just taking the intersection, and consequently helps pruning significantly. 7 Related Work The most common techniques for bidirectional alignment are post-hoc combinations, such as union or intersection, of directional models, (Och et al., 1999), or more complex heuristic combiners such as grow-diag-final (Koehn et al., 2003). Several authors have explored explicit bidirectional models in the literature. Cromieres and Kurohashi (2009) use belief propagation on a factor graph to train and decode a one-to-one word alignment problem. Qualitatively this method is similar to ours, although the model and decoding algorithm are different, and their method is not able to provide certificates of optimality. A series of papers by Ganchev et al. (2010), Graca et al. (2008), and Ganchev et al. (2008) use posterior regularization to constrain the posterior probability of the word alignment problem to be symmetric and bijective</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language TechnologyVolume 1, pages 48–54. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Lacoste-Julien</author>
<author>Ben Taskar</author>
<author>Dan Klein</author>
<author>Michael I Jordan</author>
</authors>
<title>Word alignment via quadratic assignment.</title>
<date>2006</date>
<booktitle>In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics,</booktitle>
<pages>112--119</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6737" citStr="Lacoste-Julien et al., 2006" startWordPosition="1152" endWordPosition="1155">i, j) for some j0 E [J]0, whereas forward consistency enforces a transition from (i, j) to (i + 1, j0) for some j0 E [J]0. Informally the constraints “chain” together the links. The HMM objective function f : X —* R can be written as a linear function of x J 0(j0,i,j)x(j0,i,j) j0=0 where the vector 0 E R[I]×[J]0×[J]0 includes the transition and alignment scores. For a generative model of alignment, we might define 0(j0, i, j) = log(p(eiIfj)p(jIj0)). For a discriminative model of alignment, we might define 0(j0, i, j) = w � φ(i, j0, j, f, e) for a feature function φ and weights w (Moore, 2005; Lacoste-Julien et al., 2006). Now reverse the direction of the model and consider the f—*e alignment problem. An f—*e alignment is a binary vector y E Y where for each j E [J], y(i, j) = 1 for exactly one i E [I]0. Define the set of HMM alignments Y C 10, 1}([I]0×[J]0)∪([I]0×[I]0×[J]) as with vector ω E R[I]0×[I]0×[J]. y : y(0, 0) = 1, I y(i0, i, j) `di E [I]0, j E [J], I y(i, i0, j + 1) `di E [I]0, j E [J − 1]0 Similarly define the objective function g(y; ω) = J I I ω(i0, i, j)y(i0, i, j) j=1 i=0 � i0=0 ⎧ ⎨⎪⎪⎪⎪⎪ ⎪ ⎪⎪⎪⎪⎪⎪⎩ Y = E i�=0 y(i, j) = E i�=0 y(i, j) = x : x(0, 0) = 1, J x(i, j) = E x(j0, i, j) `di E [I], j E [J]</context>
<context position="25780" citStr="Lacoste-Julien et al. (2006)" startWordPosition="4664" endWordPosition="4667"> the word alignment problem to be symmetric and bijective. This work acheives stateof-the-art performance for alignment. Instead of utilizing posteriors our model tries to decode a single best one-to-one word alignment. A different approach is to use constraints at training time to obtain models that favor bidirectional properties. Liang et al. (2006) propose agreement-based learning, which jointly learns probabilities by maximizing a combination of likelihood and agreement between two directional models. General linear programming approaches have also been applied to word alignment problems. Lacoste-Julien et al. (2006) formulate the word alignment problem as quadratic assignment problem and solve it using an integer linear programming solver. Our work is most similar to DeNero and Macherey (2011), which uses dual decomposition to encourage agreement between two directional HMM aligners during decoding time. 8 Experiments Our experimental results compare the accuracy and optimality of our decoding algorithm to directional alignment models and previous work on this bidirectional model. Data and Setup The experimental setup is identical to DeNero and Macherey (2011). Evaluation is performed on a hand-aligned s</context>
</contexts>
<marker>Lacoste-Julien, Taskar, Klein, Jordan, 2006</marker>
<rawString>Simon Lacoste-Julien, Ben Taskar, Dan Klein, and Michael I Jordan. 2006. Word alignment via quadratic assignment. In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, pages 112– 119. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Ben Taskar</author>
<author>Dan Klein</author>
</authors>
<title>Alignment by agreement.</title>
<date>2006</date>
<booktitle>In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics,</booktitle>
<pages>104--111</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="25505" citStr="Liang et al. (2006)" startWordPosition="4629" endWordPosition="4632">d decoding algorithm are different, and their method is not able to provide certificates of optimality. A series of papers by Ganchev et al. (2010), Graca et al. (2008), and Ganchev et al. (2008) use posterior regularization to constrain the posterior probability of the word alignment problem to be symmetric and bijective. This work acheives stateof-the-art performance for alignment. Instead of utilizing posteriors our model tries to decode a single best one-to-one word alignment. A different approach is to use constraints at training time to obtain models that favor bidirectional properties. Liang et al. (2006) propose agreement-based learning, which jointly learns probabilities by maximizing a combination of likelihood and agreement between two directional models. General linear programming approaches have also been applied to word alignment problems. Lacoste-Julien et al. (2006) formulate the word alignment problem as quadratic assignment problem and solve it using an integer linear programming solver. Our work is most similar to DeNero and Macherey (2011), which uses dual decomposition to encourage agreement between two directional HMM aligners during decoding time. 8 Experiments Our experimental</context>
<context position="27550" citStr="Liang et al., 2006" startWordPosition="4957" endWordPosition="4960">0.43 100.0 100.0 9.86 95.6 95.6 61.86 55.0 62.5 21.08 86.0 88.0 D&amp;M - 6.2 - - 0.0 - - 0.0 - - 6.2 - Table 1: Experimental results for model accuracy of bilingual alignment. Column time is the mean time per sentence pair in seconds; cert is the percentage of sentence pairs solved with a certificate of optimality; exact is the percentage of sentence pairs solved exactly. Results are grouped by sentence length. The percentage of sentence pairs in each group is shown in parentheses. Training is performed using the agreement-based learning method which encourages the directional models to overlap (Liang et al., 2006). This directional model has been shown produce state-of-theart results with this setup (Haghighi et al., 2009). Baselines We compare the algorithm described in this paper with several baseline methods. DIR includes post-hoc combinations of the e→f and f→e HMM-based aligners. Variants include union, intersection, and grow-diag-final. D&amp;M is the dual decomposition algorithm for bidirectional alignment as presented by DeNero and Macherey (2011) with different final combinations. LR is the Lagrangian relaxation algorithm applied to the adjacent agreement problem without the additional constraints</context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, pages 104– 111. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
</authors>
<title>A discriminative framework for bilingual word alignment.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>81--88</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6707" citStr="Moore, 2005" startWordPosition="1150" endWordPosition="1151">− 1, j0) to (i, j) for some j0 E [J]0, whereas forward consistency enforces a transition from (i, j) to (i + 1, j0) for some j0 E [J]0. Informally the constraints “chain” together the links. The HMM objective function f : X —* R can be written as a linear function of x J 0(j0,i,j)x(j0,i,j) j0=0 where the vector 0 E R[I]×[J]0×[J]0 includes the transition and alignment scores. For a generative model of alignment, we might define 0(j0, i, j) = log(p(eiIfj)p(jIj0)). For a discriminative model of alignment, we might define 0(j0, i, j) = w � φ(i, j0, j, f, e) for a feature function φ and weights w (Moore, 2005; Lacoste-Julien et al., 2006). Now reverse the direction of the model and consider the f—*e alignment problem. An f—*e alignment is a binary vector y E Y where for each j E [J], y(i, j) = 1 for exactly one i E [I]0. Define the set of HMM alignments Y C 10, 1}([I]0×[J]0)∪([I]0×[I]0×[J]) as with vector ω E R[I]0×[I]0×[J]. y : y(0, 0) = 1, I y(i0, i, j) `di E [I]0, j E [J], I y(i, i0, j + 1) `di E [I]0, j E [J − 1]0 Similarly define the objective function g(y; ω) = J I I ω(i0, i, j)y(i0, i, j) j=1 i=0 � i0=0 ⎧ ⎨⎪⎪⎪⎪⎪ ⎪ ⎪⎪⎪⎪⎪⎪⎩ Y = E i�=0 y(i, j) = E i�=0 y(i, j) = x : x(0, 0) = 1, J x(i, j) = E </context>
</contexts>
<marker>Moore, 2005</marker>
<rawString>Robert C Moore. 2005. A discriminative framework for bilingual word alignment. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 81–88. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Christoph Tillmann</author>
<author>Hermann Ney</author>
</authors>
<title>Improved alignment models for statistical machine translation.</title>
<date>1999</date>
<booktitle>In Proc. of the Joint SIGDAT Conf. on Empirical Methods in Natural Language Processing and Very Large Corpora,</booktitle>
<pages>20--28</pages>
<contexts>
<context position="8281" citStr="Och et al., 1999" startWordPosition="1458" endWordPosition="1461">ts. Note that unlike an alignment from Y multiple words may be aligned in a column and words may transition from nonaligned positions. Note that for both of these models we can solve the optimization problem exactly using the standard Viterbi algorithm for HMM decoding. The first can be solved in O(IJ2) time and the second in O(I2J) time. 3 Bidirectional Alignment The directional bias of the e-*f and f-*e alignment models may cause them to produce differing alignments. To obtain the best single alignment, it is common practice to use a post-hoc algorithm to merge these directional alignments (Och et al., 1999). First, a directional alignment is found from each word in e to a word f. Next an alignment is produced in the reverse direction from f to e. Finally, these alignments are merged, either through intersection, union, or with an interpolation algorithm such as grow-diag-final (Koehn et al., 2003). In this work, we instead consider a bidirectional alignment model that jointly considers both directional models. We begin in this section by introducing a simple bidirectional model that enforces full agreement between directional models and giving a relaxation for decoding. Section 4 loosens this mo</context>
<context position="24527" citStr="Och et al., 1999" startWordPosition="4477" endWordPosition="4480"> J)) since each inner loop enumerates IJ possible updates and assigns at least one index a non-zero value, limiting the outer loop to I + J iterations. In practice we initialize the heuristic based on the intersection of x and y at the current round of Lagrangian relaxation. Experiments show that running this algorithm significantly improves the lower bound compared to just taking the intersection, and consequently helps pruning significantly. 7 Related Work The most common techniques for bidirectional alignment are post-hoc combinations, such as union or intersection, of directional models, (Och et al., 1999), or more complex heuristic combiners such as grow-diag-final (Koehn et al., 2003). Several authors have explored explicit bidirectional models in the literature. Cromieres and Kurohashi (2009) use belief propagation on a factor graph to train and decode a one-to-one word alignment problem. Qualitatively this method is similar to ours, although the model and decoding algorithm are different, and their method is not able to provide certificates of optimality. A series of papers by Ganchev et al. (2010), Graca et al. (2008), and Ganchev et al. (2008) use posterior regularization to constrain the</context>
</contexts>
<marker>Och, Tillmann, Ney, 1999</marker>
<rawString>Franz Josef Och, Christoph Tillmann, Hermann Ney, et al. 1999. Improved alignment models for statistical machine translation. In Proc. of the Joint SIGDAT Conf. on Empirical Methods in Natural Language Processing and Very Large Corpora, pages 20–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander M Rush</author>
<author>Michael Collins</author>
</authors>
<title>A tutorial on dual decomposition and lagrangian relaxation for inference in natural language processing.</title>
<date>2012</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>45--305</pages>
<contexts>
<context position="9873" citStr="Rush and Collins (2012)" startWordPosition="1731" endWordPosition="1734">efer to the optimal alignments for this problem as x* and y*. Unfortunately this bidirectional decoding model is NP-Hard (a proof is given in Appendix A). As it is common for alignment pairs to have |f |or |e |over 40, exact decoding algorithms are intractable in the worst-case. Instead we will use Lagrangian relaxation for this model. At a high level, we will remove a subset of the constraints from the original problem and replace them with Lagrange multipliers. If we can solve this new problem efficiently, we may be able to get optimal solutions to the original problem. (See the tutorial by Rush and Collins (2012) describing the method.) There are many possible subsets of constraints to consider relaxing. The relaxation we use preserves the agreement constraints while relaxing the Markov structure of the f-*e alignment. This relaxation will make it simple to later re-introduce constraints in Section 5. We relax the forward constraints of set Y. Without these constraints the y links are no longer chained together. This has two consequences: (1) for index j there may be any number of indices i, such that y(i, j) = 1, (2) if y(i&apos;, i, j) = 1 it is no longer required that y(i&apos;, j − 1) = 1. This gives a set </context>
</contexts>
<marker>Rush, Collins, 2012</marker>
<rawString>Alexander M Rush and Michael Collins. 2012. A tutorial on dual decomposition and lagrangian relaxation for inference in natural language processing. Journal of Artificial Intelligence Research, 45:305–362.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>Hmm-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th conference on Computational linguistics-Volume 2,</booktitle>
<pages>836--841</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1406" citStr="Vogel et al., 1996" startWordPosition="202" endWordPosition="205">timal solutions on difficult instances, we alternate between incrementally adding constraints and applying optimality-preserving coarse-to-fine pruning. The algorithm finds provably exact solutions on 86% of sentence pairs and shows improvements over directional models. 1 Introduction Word alignment is a critical first step for building statistical machine translation systems. In order to ensure accurate word alignments, most systems employ a post-hoc symmetrization step to combine directional word aligners, such as IBM Model 4 (Brown et al., 1993) or hidden Markov model (HMM) based aligners (Vogel et al., 1996). Several authors have proposed bidirectional models that incorporate this step directly, but decoding under many bidirectional models is NP-Hard and finding exact solutions has proven difficult. In this paper, we describe a novel Lagrangianrelaxation based decoder for the bidirectional model proposed by DeNero and Macherey (2011), with the goal of improving search accuracy. In that work, the authors implement a dual decomposition-based decoder for the problem, but are only able to find exact solutions for around 6% of instances. Our decoder uses a simple variant of the Viterbi algorithm for s</context>
<context position="5272" citStr="Vogel et al., 1996" startWordPosition="867" endWordPosition="870">g problem. Given a sentence e of length �eI = I and a sentence f of length IfI = J, our goal is to find the best bidirectional alignment between the two sentences under a given objective function. Before turning to the model of interest, we first introduce directional word alignment. 2.1 Word Alignment In the e—*f word alignment problem, each word in e is aligned to a word in f or to the null word E. This alignment is a mapping from each index i E [I] to an index j E [J]0 (where j = 0 represents alignment to E). We refer to a single word alignment as a link. A first-order HMM alignment model (Vogel et al., 1996) is an HMM of length I + 1 where the hidden state at position i E [I]0 is the aligned index j E [J]0, and the transition score takes into account the previously aligned index j0 E [J]0.1 Formally, define the set of possible HMM alignments as X C 10, 1}([I]0×[J]0)∪([I]×[J]0×[J]0) with 1Our definition differs slightly from other HMM-based aligners in that it does not track the last a alignment. X= J x(i, j) = E x(j, i + 1, j0) `di E [I − 1]0, j E [J]0 j�=0 where x(i, j) = 1 indicates that there is a link between index i and index j, and x(j0, i, j) = 1 indicates that index i − 1 aligns to index </context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1996. Hmm-based word alignment in statistical translation. In Proceedings of the 16th conference on Computational linguistics-Volume 2, pages 836– 841. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>