<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002727">
<title confidence="0.9937725">
Exploiting Qualitative Information from Automatic Word Alignment
for Cross-lingual NLP Tasks
</title>
<note confidence="0.562313">
Jos´e G.C. de Souza Miquel Espl`a-Gomis Marco Turchi Matteo Negri
FBK-irst, Universitat d’Alacant FBK-irst FBK-irst
University of Trento Alacant, Spain Trento, Italy Trento, Italy
Trento, Italy mespla@dlsi.ua.es turchi@fbk.eu negri@fbk.eu
</note>
<email confidence="0.70152">
desouza@fbk.eu
</email>
<sectionHeader confidence="0.986278" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999880421052632">
The use of automatic word alignment to
capture sentence-level semantic relations
is common to a number of cross-lingual
NLP applications. Despite its proved
usefulness, however, word alignment in-
formation is typically considered from a
quantitative point of view (e.g. the number
of alignments), disregarding qualitative
aspects (the importance of aligned terms).
In this paper we demonstrate that integrat-
ing qualitative information can bring sig-
nificant performance improvements with
negligible impact on system complexity.
Focusing on the cross-lingual textual en-
tailment task, we contribute with a novel
method that: i) significantly outperforms
the state of the art, and ii) is portable, with
limited loss in performance, to language
pairs where training data are not available.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99878396969697">
Meaning representation, comparison and projec-
tion across sentences are major challenges for a
variety of cross-lingual applications. So far, de-
spite the relevance of the problem, research on
multilingual applications has either circumvented
the issue, or proposed partial solutions.
When possible, the typical approach builds on
the reduction to a monolingual task, burdening the
process with dependencies from machine transla-
tion (MT) components. For instance, in cross-
lingual question answering and cross-lingual tex-
tual entailment (CLTE), intermediate MT steps
are respectively performed to ease answer re-
trieval/presentation (Parton, 2012; Tanev et al.,
2006) and semantic inference (Mehdad et al.,
2010). Direct solutions that avoid such pivot-
ing strategies typically exploit similarity measures
that rely on bag-of-words representations. As an
example, most supervised approaches to MT qual-
ity estimation (Blatz et al., 2003; Callison-Burch
et al., 2012) and CLTE (W¨aschle and Fendrich,
2012) include features that consider the amount of
equivalent terms that are found in the input sen-
tence pairs. Such simplification, however, disre-
gards the fact that semantic equivalence is not only
proportional to the number of equivalent terms,
but also to their importance. In other words, in-
stead of checking what of a given sentence can be
found in the other, current approaches limit the
analysis to the amount of lexical elements they
share, under the rough assumption that the more
the better.
In this paper we argue that:
</bodyText>
<listItem confidence="0.980533666666667">
(1) Considering qualitative aspects of word align-
ments to identify sentence-level semantic relations
can bring significant performance improvements
in cross-lingual NLP tasks.
(2) Shallow linguistic processing techniques (of-
ten a constraint in real cross-lingual scenarios due
to limited resources availability) can be leveraged
to set up portable solutions that still outperform
current bag-of-words methods.
</listItem>
<bodyText confidence="0.9918998">
To support our claims we experiment with the
CLTE task, which allows us to perform exhaus-
tive comparative experiments due to the availabil-
ity of comparable benchmarks for different lan-
guage pairs. In the remainder of the paper, we:
</bodyText>
<listItem confidence="0.895122">
(1) Prove the effectiveness of our method over
datasets for four language combinations;
(2) Assess the portability of our models across lan-
guages in different testing conditions.
</listItem>
<sectionHeader confidence="0.925646" genericHeader="introduction">
2 Objectives and Method
</sectionHeader>
<bodyText confidence="0.999918">
We propose a supervised learning approach for
identifying and classifying semantic relations be-
tween two sentences T1 and T2 written in different
languages. Beyond semantic equivalence, which
is relevant to applications such as MT quality es-
</bodyText>
<page confidence="0.973044">
771
</page>
<note confidence="0.4601485">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 771–776,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<figure confidence="0.999911307692308">
(c)
(b)
(a)
Parallel data
for L1-L2
Word alignment
algorithm
Word alignment
algorithm
Parallel data
for L3-L4
Parallel data
for L3-L4
Word alignment
algorithm
Unlabeled
CLTE data
for L1 -L2
Word alignment
model for L1-L2
CLTE model
for L1-L2
CLTE
annotation
Learning
algorithm
Labeled
CLTE data
for L1-L2
Unlabeled
CLTE data
for L3-L4
Word alignment
model for L3-L4
CLTE model
for L1-L2
CLTE
annotation
Unlabeled
CLTE data
for L3 -L4
Word alignment
model for L3-L4
CLTE
annotation
Combination
CLTE model
for L1-L2
CLTE model
for L5-L6
CLTE model
for L7-L8
</figure>
<figureCaption confidence="0.978317666666667">
Figure 1: System architecture in different training/evaluation conditions. (a): parallel data and CLTE
labeled data are available for language pair L1-L2. (b): the L1-L2 CLTE model is used to cope with the
unavailability of labeled data for L3-L4. (c): the same problem is tackled by combining multiple models.
</figureCaption>
<bodyText confidence="0.9996231">
timation (Mehdad et al., 2012b),1 we aim to cap-
ture a richer set of relations potentially relevant to
other tasks. For instance, recognizing unrelated-
ness, forward and backward entailment relations,
represents a core problem in cross-lingual docu-
ment summarization (Lenci et al., 2002) and con-
tent synchronization (Monz et al., 2011; Mehdad
et al., 2012a). CLTE, as proposed within the Se-
mEval evaluation exercises (Negri et al., 2012;
Negri et al., 2013), represents an ideal framework
to evaluate such capabilities. Within this frame-
work, our goal is to automatically identify the fol-
lowing entailment relations between T1 and T2:
forward (T1 → T2), backward (T1 ← T2), bidi-
rectional (T1 ↔ T2) and no entailment.
Our approach (see Figure 1) involves two core
components: i) a word alignment model, and ii) a
CLTE classifier. The former is trained on a par-
allel corpus, and associates equivalent terms in T1
and T2. The information about word alignments
is used to extract quantitative (amount and dis-
tribution of the alignments) and qualitative fea-
tures (importance of the aligned terms) to train the
CLTE classifier. Although in principle both com-
ponents need training data (respectively a paral-
lel corpus and labeled CLTE data), our goal is to
develop a method that is also portable across lan-
guages. To this aim, while the parallel corpus is
necessary to train the word aligner for any lan-
guage pair we want to deal with, the CLTE clas-
</bodyText>
<footnote confidence="0.895197">
1A translation has to be semantically equivalent to the
source sentence.
</footnote>
<bodyText confidence="0.999717142857143">
sifier can be designed to learn from features that
capture language independent knowledge.2 This
allows us to experiment in different testing con-
ditions, namely: i) when CLTE training data are
available for a given language pair (Figure 1a),
and ii) when CLTE training data are missing, and
a model trained on other language pairs has to be
reused (Figure 1b-c).
Features. Considering word alignment informa-
tion, we extract three different groups of features:
AL, POS, and IDF.
The AL group provides quantitative informa-
tion about the aligned/unaligned words in each
sentence T∗ of the pair. These features are:
</bodyText>
<listItem confidence="0.9965356">
1. proportion of aligned words in T∗. We use
this indicator as our baseline (B henceforth);
2. number of sequences of unaligned words,
normalized by the length of T∗;
3. length of the longest a) sequence of aligned
words, and b) sequence of unaligned words,
both normalized by the length of T∗;
4. average length of a) the aligned word se-
quences, and b) the unaligned word se-
quences;
5. position of a) the first unaligned word, and
b) the last unaligned word, both normalized
by the lenght of T∗;
6. proportion of word n-grams in T∗ contain-
ing only aligned words (the feature was com-
</listItem>
<footnote confidence="0.973288333333333">
2For instance, the fact that aligning all nouns and the most
relevant terms in T1 and T2 is a good indicator of semantic
equivalence.
</footnote>
<page confidence="0.993162">
772
</page>
<bodyText confidence="0.994975615384615">
puted separately for values of n = 1... 5).
The POS group considers the part of speech
(PoS) of the words in T∗ as a source of qualitative
information about their importance. To compute
these features we use the TreeTagger (Schmid,
1995), manually mapping the fine-grained set of
assigned PoS labels into a more general set of tags
(P) based on the universal PoS tag set by Petrov
et al. (2012). POS features differentiate between
aligned words (words in T1 that are aligned to one
or more words in T2) and alignments (the edges
connecting words in T1 and T2). Features consid-
ering the aligned words in T∗ are:
</bodyText>
<listItem confidence="0.942766714285714">
7. for each PoS tag p E P, proportion of aligned
words in T∗ tagged with p;
8. proportion of words in T1 aligned with words
with the same PoS tag in T2 (and vice-versa);
9. for each PoS tag p E P, proportion of words
in T1 tagged as p which are aligned to words
with the same tag in T2 (and vice-versa).
</listItem>
<subsectionHeader confidence="0.373909">
Features considering the alignments are:
</subsectionHeader>
<bodyText confidence="0.92791825">
10. proportion of alignments connecting words
with the same PoS tag p;
11. for each PoS tag p E P, proportion of align-
ments connecting two words tagged as p.
IDF, the last feature, uses the inverse docu-
ment frequency (Salton and Buckley, 1988) as an-
other source of qualitative information under the
assumption that rare words (and, therefore, with
higher IDF) are more informative:
12. summation of all the IDF scores of the
aligned words in T∗ over the summation of
the IDF scores of all words in T∗.
</bodyText>
<sectionHeader confidence="0.999685" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999969444444444">
Our experiments cover two different scenarios.
First, the typical one, in which the CLTE model
is trained on labeled data for the same pair of lan-
guages L1–L2 of the test set. Then, simulating
the less favorable situation in which labeled train-
ing data for L1–L2 are missing, we investigate the
possibility to use existing CLTE models trained on
labeled data for a different language pair L3–L4.
The SemEval 2012 CLTE datasets used in our
experiments are available for four language pairs:
Es–En, De–En, Fr–En, and It–En. Each dataset
was created with the crowdsourcing-based method
described in Negri et al. (2011), and consists of
1000 T1–T2 pairs (500 for training, 500 for test).
To train the word alignment models we used
the Europarl parallel corpus (Koehn, 2005), con-
catenated with the News Commentary corpus3
for three language pairs: De–En (2,079,049
sentences), Es–En (2,123,036 sentences), Fr–En
(2,144,820 sentences). For It–En we only used
the parallel data available in Europarl (1,909,115
sentences) since this language pair is not covered
by the News Commentary corpus. IDF values for
the words in each language were calculated on the
monolingual part of these corpora, using the aver-
age IDF value of each language for unseen terms.
To build the word alignment models we used the
MGIZA++ package (Gao and Vogel, 2008). Ex-
periments have been carried out with the hidden
Markov model (HMM) (Vogel et al., 1996) and
IBM models 3 and 4 (Brown et al., 1993).4 We also
explored three symmetrization techniques (Koehn
et al., 2005): union, intersection, and grow-diag-
final-and. A greedy feature selection process on
training data, with different combinations of word
alignment models and symmetrization methods,
indicated HMM/intersection as the best perform-
ing combination. For this reason, all our experi-
ments use this setting.
The SVM implementation of Weka (Hall et
al., 2009) was used to build the CLTE model.5
Two binary classifiers were trained to separately
check T1 → T2 and T1 ← T2, merging
their output to obtain the 4-class judgments (e.g.
yes/yes=bidirectional, yes/no=forward).
</bodyText>
<subsectionHeader confidence="0.998755">
3.1 Evaluation with CLTE training data
</subsectionHeader>
<bodyText confidence="0.999878666666666">
Figure 2 shows the accuracy obtained by the dif-
ferent feature groups.6 For the sake of compari-
son, state-of-the-art results achieved for each lan-
guage combination at SemEval 2012 are also re-
ported. As regards Es–En (63.2% accuracy) and
De–En (55.8%), the top scores were obtained by
the system described in (W¨aschle and Fendrich,
2012), where a combination of binary classifiers
for each entailment direction is trained with a mix-
</bodyText>
<footnote confidence="0.984690555555555">
3http://www.statmt.org/wmt11/
translation-task.html#download
4Five iterations of HMM, and three iterations of IBM
models 3 and 4 have been performed on the training corpora.
5The polynomial kernel was used with parameters empir-
ically estimated on the training set (C = 2.0, and d = 1)
6In Figures 2 and 3, the “*” indicates statistically signif-
icant improvements over the state of the art at p ≤ 0.05,
calculated with approximate randomization (Pad´o, 2006).
</footnote>
<page confidence="0.99717">
773
</page>
<bodyText confidence="0.999753142857143">
ture of monolingual (i.e. with the input sentences
translated in the same language using Google
Translate7) and cross-lingual features. Although
such system exploits word-alignment information
to some extent, this is only done at quantitative
level (e.g. number of unaligned words, percentage
of aligned words, length of the longest unaligned
subsequence). As regards It–En, the state of the
art (56.6%) is represented by the system described
in (Jimenez et al., 2012), which uses a pure pivot-
ing method (using Google Translate) and adaptive
similarity functions based on “soft” cardinality for
flexible term comparisons. The two systems ob-
tained the same result on Fr–En (57.0%).
</bodyText>
<figure confidence="0.712054">
Es-En De-En Fr-En It-En
</figure>
<figureCaption confidence="0.9597415">
Figure 2: Accuracy obtained by each feature
group on four language combinations.
</figureCaption>
<bodyText confidence="0.9996805">
As can be seen in Figure 2, the combination of
all our features outperforms the state of the art
for each language pair. The accuracy improve-
ment ranges from 6.6% for Es–En (from 63.2% to
67.4%) to 14.6% for De–En (from 55.8% to 64%).
Except for Es–En, that has very competitive state-
of-the-art results, the combination of AL with POS
or IDF feature groups always outperforms the best
systems. Furthermore, the performance increase
with qualitative features (POS and IDF) shows co-
herent trends across all language pairs. It is worth
noting that, while we rely on a pure cross-lingual
approach, both the state-of-the-art CLTE systems
include features from the translation of T1 into the
language of T2. For De–En, quantitative features
alone achieve lower results compared to the other
languages. This can be motivated by the higher
difficulty in aligning De–En pairs (this hypothesis
is supported by the fact that the average number
of alignments per sentence pair is 18 for De–En,
and &gt;22 for the other combinations). Neverthe-
less, qualitative features lead to results comparable
</bodyText>
<footnote confidence="0.798455">
7http://translate.google.com/
</footnote>
<bodyText confidence="0.999933088235294">
with the other language pairs.
The selection of the best performing features
for each language pair produces further improve-
ments of varying degrees in Es–En (from 67.4%
to 68%), De–En (64% – 64.8%) and It–En (63.4%
– 66.8%), while performance remains stable for
Fr–En (63%). All these configurations include
the IDF feature (12) and the proportion of aligned
words for each PoS category (7), proving the ef-
fectiveness of qualitative word alignment features.
The fact that HMM/intersection is the best com-
bination of alignment model and symmetrization
method is interesting, since it contradicts the gen-
eral notion that IBM models 3 and 4 perform bet-
ter than HMM (Och and Ney, 2003). A possible
explanation is that, while word alignment models
are usually trained on parallel corpora, the major-
ity of CLTE sentence pairs are not parallel. In
this setting, where producing reliable alignments
is more difficult, IBM models are less effective for
at least two reasons. First, including a word fertil-
ity model, IBM 3 and 4 limit (typically to the half
of the source sentence length) the number of tar-
get words that can be aligned with the null word.
Therefore, when such limit is reached, these mod-
els tend to force low probability, hence less reli-
able, word alignments. Second, in IBM model 4,
the larger distortion limit makes it possible to align
distant words. In the case of non-parallel sen-
tences, this often results in wrong or noisy align-
ments that affect final results. For these reasons,
CLTE data seem more suitable for the simpler and
more conservative HMM model, and a precision-
oriented symmetrization method like intersection.
</bodyText>
<subsectionHeader confidence="0.9988">
3.2 Evaluation without CLTE training data
</subsectionHeader>
<bodyText confidence="0.9997468">
The goal of our second round of experiments is to
investigate if, and to what extent, our approach can
be considered as language-independent. Confirm-
ing this would allow to reuse models trained for
a given language pair in situations where CLTE
training data is missing. This is a rather realistic
situation since, while bitexts to train word aligners
are easier to find, the availability of labeled CLTE
data is far from being guaranteed.
Our experiments have been carried out, over the
same SemEval datasets, with two methods that do
not use labeled data for the target language com-
bination. The first one (method b in Figure 1)
uses a CLTE model trained for a language pair
L1–L2 for which labeled training data are avail-
</bodyText>
<figure confidence="0.9966861875">
Accuracy (%)
75
70
65
60
55
50
*
B
B+AL
B+AL+IDF
B+AL+POS
B+AL+IDF+POS
state-of-the-art
* * * * * *
*
</figure>
<page confidence="0.996283">
774
</page>
<bodyText confidence="0.99985925">
able, and applies this model to a language pair
L3–L4 for which only parallel corpora are avail-
able. The second method (c in Figure 1) addresses
the same problem, but exploits a combination of
CLTE models trained for different language pairs.
For each test set, the models trained for the other
three language pairs are used in a voting scheme,
in order to check whether they can complement
each other to increase final results.
All the experiments have been performed using
the best CLTE model for each language pair, com-
paring results with those presented in Section 3.1.
</bodyText>
<figure confidence="0.584223">
Es-En De-En Fr-En It-En
</figure>
<figureCaption confidence="0.99153">
Figure 3: Accuracy obtained by reusing CLTE
models (alone and in a voting scheme).
</figureCaption>
<bodyText confidence="0.999987193548387">
As shown in Figure 3, reusing models for a new
language pair leads to results that still outperform
the state of the art.6 Remarkably, when used for
other language combinations, the Es–En, It–En,
and Fr–En models always lead to results above,
or equal to the state of the art. For similar lan-
guages such as Spanish, French, and Italian, the
accuracy increase over the state of the art is up to
14.8% (from 56.6% to 65.0%) and 13.4% (from
56.6% to 64.2%) when the Fr–En and Es–En mod-
els are respectively used to label the It–En dataset.
Although not always statistically significant and
below the performance obtained in the ideal sce-
nario where CLTE training data are available (full
sys.), such improvements suggest that our features
can be re-used, at least to some extent, across dif-
ferent language settings. As expected, the major
incompatibilities arise between German and the
other languages due to the linguistic differences
between this language and the others. However, it
is interesting to note that: i) at least in one case
(i.e. when tested on It–En) the De–En model still
achieves results above the state of the art, and ii)
on the De–En evaluation setting the worst model
(Fr–En) still achieves state of the art results.
The results obtained with the voting scheme
suggest that our models can complement each
other when used on a new language pair. Although
statistically significant only over It–En data, vot-
ing results both outperform the state of the art and
the results achieved by single models.
</bodyText>
<sectionHeader confidence="0.993263" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999924642857143">
We investigated the usefulness of qualitative infor-
mation from automatic word alignment to iden-
tify semantic relations between sentences in dif-
ferent languages. With coherent results in CLTE,
we demonstrated that features considering the im-
portance of aligned terms can successfully inte-
grate the quantitative evidence (number and pro-
portion of aligned terms) used by previous su-
pervised learning approaches. A study on the
portability across languages of the learned mod-
els demonstrated that word alignment information
can be exploited to train reusable models for new
language combinations where bitexts are available
but CLTE labeled data are not.
</bodyText>
<sectionHeader confidence="0.985629" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9988996">
This work has been partially supported by the EC-
funded projects CoSyne (FP7-ICT-4-248531) and
MateCat (ICT-2011.4.2–287688), and by Span-
ish Government through projects TIN2009-14009-
C02-01 and TIN2012-32615.
</bodyText>
<sectionHeader confidence="0.998567" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996226">
John Blatz, Erin Fitzgerald, George Foster, Simona
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto
Sanchis, and Nicola Ueffing. 2003. Confidence Es-
timation for Machine Translation. Summer work-
shop final report, JHU/CLSP.
Peter F. Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993. The
Mathematics of Statistical Machine Translation: Pa-
rameter Estimation. Computational Linguistics,
19(2):263–311.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Sev-
enth Workshop on Statistical Machine Translation
(WMT’12), pages 10–51, Montr´eal, Canada.
Qin Gao and Stephan Vogel. 2008. Parallel Implemen-
tations of Word Alignment Tool. In Software En-
gineering, Testing, and Quality Assurance for Natu-
ral Language Processing, pages 49–57, Columbus,
Ohio, USA.
</reference>
<figure confidence="0.998561590909091">
Accuracy (%)
85
80
75
70
65
60
55
50
*
*
state-of-the-art
*
Es-En
De-En
Fr-En
It-En
Voting
*
*
*
*
</figure>
<page confidence="0.981508">
775
</page>
<reference confidence="0.999605688073394">
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: an Up-
date. SIGKDD Explorations, 11(1):10–18.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2012. Soft Cardinality + ML: Learning Adap-
tive Similarity Functions for Cross-lingual Textual
Entailment. In Proceedings of the 6th International
Workshop on Semantic Evaluation (SemEval 2012),
pages 684–688, Montr´eal, Canada.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh System Descrip-
tion for the 2005 IWSLT Speech Translation Evalu-
ation. In Proceedings of the International Workshop
on Spoken Language Translation, Pittsburgh, Penn-
sylvania, USA.
Philip Koehn. 2005. Europarl: a Parallel Corpus for
Statistical Machine Translation. In Proceedings of
MT Summit X, pages 79–86, Phuket, Thailand.
Alessandro Lenci, Roberto Bartolini, Nicoletta Cal-
zolari, Ana Agua, Stephan Busemann, Emmanuel
Cartier, Karine Chevreau, and Jos´e Coch. 2002.
Multilingual summarization by integrating linguistic
resources in the MLIS-MUSI Project. In Proceed-
ings of the Third International Conference on Lan-
guage Resources and Evaluation (LREC’02), pages
1464–1471, Las Palmas de Gran Canaria, Spain.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2010. Towards Cross-Lingual Textual Entailment.
In Proceedings of the Eleventh Annual Conference
of the North American Chapter of the Association
for Computational Linguistics (NAACL HLT 2010),
pages 321–324, Los Angeles, California, USA.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2012a. Detecting Semantic Equivalence and Infor-
mation Disparity in Cross–lingual Documents. In
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics (ACL’12),
pages 120–124, Jeju Island, Korea.
Yashar Mehdad, Matteo Negri, and Marcello Fed-
erico. 2012b. Match without a Referee: Evaluating
MT Adequacy without Reference Translations. In
Proceedings of the Machine Translation Workshop
(WMT2012), Montr´eal, Canada.
Christoph Monz, Vivi Nastase, Matteo Negri, Angela
Fahrni, Yashar Mehdad, and Michael Strube. 2011.
CoSyne: a Framework for Multilingual Content
Synchronization of Wikis. In Proceedings of Wik-
iSym 2011, the International Symposium on Wikis
and Open Collaboration, pages 217–218, Mountain
View, California, USA.
Matteo Negri, Luisa Bentivogli, Yashar Mehdad,
Danilo Giampiccolo, and Alessandro Marchetti.
2011. Divide and Conquer: Crowdsourcing the Cre-
ation of Cross-Lingual Textual Entailment Corpora.
In Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing (EMNLP
2011), Edinburgh, Scotland.
Matteo Negri, Alessandro Marchetti, Yashar Mehdad,
Luisa Bentivogli, and Danilo Giampiccolo. 2012.
Semeval-2012 Task 8: Cross-Lingual Textual En-
tailment for Content Synchronization. In Proceed-
ings of the 6th International Workshop on Seman-
tic Evaluation (SemEval 2012), pages 399–407,
Montr´eal, Canada.
Matteo Negri, Alessandro Marchetti, Yashar Mehdad,
Luisa Bentivogli, and Danilo Giampiccolo. 2013.
Semeval-2013 Task 8: Cross-Lingual Textual En-
tailment for Content Synchronization. In Proceed-
ings of the 7th International Workshop on Semantic
Evaluation (SemEval 2013), Atlanta, GA.
Franz J. Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Mod-
els. Computational Linguistics, 29(1):19–51.
Sebastian Pad´o, 2006. User’s guide to sigf: Signifi-
cance testing by approximate randomisation.
Kristen Parton. 2012. Lost and Found in Transla-
tion: Cross-Lingual Question Answering with Result
Translation. Ph.D. thesis, Columbia University.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceedings
of the Eight International Conference on Language
Resources and Evaluation (LREC’12), pages 2089–
2096, Istanbul, Turkey.
Gerard Salton and Christopher Buckley. 1988.
Term-weighting Approaches in Automatic Text Re-
trieval. Information Processing and Management,
24(5):513–523.
Helmut Schmid. 1995. Improvements in Part-of-
Speech Tagging with an Application to German. In
Proceedings of the ACL SIGDAT-Workshop, pages
47–50, Dublin, Ireland.
Hristo Tanev, Milen Kouylekov, Bernardo Magnini,
Matteo Negri, and Kiril Simov. 2006. Exploit-
ing Linguistic Indices and Syntactic Structures for
Multilingual Question Answering: ITC-irst at CLEF
2005. Accessing Multilingual Information Reposito-
ries, pages 390–399.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based Word Alignment in Statisti-
cal Translation. In Proceedings of the 16th Inter-
national Conference on Computational Linguistics
(ACL’96), pages 836–841, Copenhagen, Denmark.
Katharina W¨aschle and Sascha Fendrich. 2012. HDU:
Cross-lingual Textual Entailment with SMT Fea-
tures. In Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval 2012), pages
467–471, Montr´eal, Canada.
</reference>
<page confidence="0.998531">
776
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.369339">
<title confidence="0.999467">Exploiting Qualitative Information from Automatic Word for Cross-lingual NLP Tasks</title>
<author confidence="0.999973">Jos´e G C de_Souza Miquel Espl`a-Gomis Marco Turchi Matteo Negri</author>
<affiliation confidence="0.946329">FBK-irst, Universitat d’Alacant FBK-irst FBK-irst University of Trento Alacant, Spain Trento, Italy Trento, Italy</affiliation>
<address confidence="0.424278">Trento, mespla@dlsi.ua.es turchi@fbk.eu negri@fbk.eu</address>
<email confidence="0.934864">desouza@fbk.eu</email>
<abstract confidence="0.9992662">The use of automatic word alignment to capture sentence-level semantic relations is common to a number of cross-lingual NLP applications. Despite its proved usefulness, however, word alignment information is typically considered from a point of view number of alignments), disregarding qualitative aspects (the importance of aligned terms). In this paper we demonstrate that integrating qualitative information can bring significant performance improvements with negligible impact on system complexity. Focusing on the cross-lingual textual entailment task, we contribute with a novel that: outperforms state of the art, and portable, with limited loss in performance, to language pairs where training data are not available.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>John Blatz</author>
<author>Erin Fitzgerald</author>
<author>George Foster</author>
<author>Simona Gandrabur</author>
<author>Cyril Goutte</author>
<author>Alex Kulesza</author>
<author>Alberto Sanchis</author>
<author>Nicola Ueffing</author>
</authors>
<title>Confidence Estimation for Machine Translation. Summer workshop final report,</title>
<date>2003</date>
<publisher>JHU/CLSP.</publisher>
<contexts>
<context position="2085" citStr="Blatz et al., 2003" startWordPosition="286" endWordPosition="289">proach builds on the reduction to a monolingual task, burdening the process with dependencies from machine translation (MT) components. For instance, in crosslingual question answering and cross-lingual textual entailment (CLTE), intermediate MT steps are respectively performed to ease answer retrieval/presentation (Parton, 2012; Tanev et al., 2006) and semantic inference (Mehdad et al., 2010). Direct solutions that avoid such pivoting strategies typically exploit similarity measures that rely on bag-of-words representations. As an example, most supervised approaches to MT quality estimation (Blatz et al., 2003; Callison-Burch et al., 2012) and CLTE (W¨aschle and Fendrich, 2012) include features that consider the amount of equivalent terms that are found in the input sentence pairs. Such simplification, however, disregards the fact that semantic equivalence is not only proportional to the number of equivalent terms, but also to their importance. In other words, instead of checking what of a given sentence can be found in the other, current approaches limit the analysis to the amount of lexical elements they share, under the rough assumption that the more the better. In this paper we argue that: (1) </context>
</contexts>
<marker>Blatz, Fitzgerald, Foster, Gandrabur, Goutte, Kulesza, Sanchis, Ueffing, 2003</marker>
<rawString>John Blatz, Erin Fitzgerald, George Foster, Simona Gandrabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, and Nicola Ueffing. 2003. Confidence Estimation for Machine Translation. Summer workshop final report, JHU/CLSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The Mathematics of Statistical Machine Translation: Parameter Estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="10611" citStr="Brown et al., 1993" startWordPosition="1701" endWordPosition="1704">(2,079,049 sentences), Es–En (2,123,036 sentences), Fr–En (2,144,820 sentences). For It–En we only used the parallel data available in Europarl (1,909,115 sentences) since this language pair is not covered by the News Commentary corpus. IDF values for the words in each language were calculated on the monolingual part of these corpora, using the average IDF value of each language for unseen terms. To build the word alignment models we used the MGIZA++ package (Gao and Vogel, 2008). Experiments have been carried out with the hidden Markov model (HMM) (Vogel et al., 1996) and IBM models 3 and 4 (Brown et al., 1993).4 We also explored three symmetrization techniques (Koehn et al., 2005): union, intersection, and grow-diagfinal-and. A greedy feature selection process on training data, with different combinations of word alignment models and symmetrization methods, indicated HMM/intersection as the best performing combination. For this reason, all our experiments use this setting. The SVM implementation of Weka (Hall et al., 2009) was used to build the CLTE model.5 Two binary classifiers were trained to separately check T1 → T2 and T1 ← T2, merging their output to obtain the 4-class judgments (e.g. yes/yes</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<title>Findings of the 2012 workshop on statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation (WMT’12),</booktitle>
<pages>10--51</pages>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="2115" citStr="Callison-Burch et al., 2012" startWordPosition="290" endWordPosition="293"> reduction to a monolingual task, burdening the process with dependencies from machine translation (MT) components. For instance, in crosslingual question answering and cross-lingual textual entailment (CLTE), intermediate MT steps are respectively performed to ease answer retrieval/presentation (Parton, 2012; Tanev et al., 2006) and semantic inference (Mehdad et al., 2010). Direct solutions that avoid such pivoting strategies typically exploit similarity measures that rely on bag-of-words representations. As an example, most supervised approaches to MT quality estimation (Blatz et al., 2003; Callison-Burch et al., 2012) and CLTE (W¨aschle and Fendrich, 2012) include features that consider the amount of equivalent terms that are found in the input sentence pairs. Such simplification, however, disregards the fact that semantic equivalence is not only proportional to the number of equivalent terms, but also to their importance. In other words, instead of checking what of a given sentence can be found in the other, current approaches limit the analysis to the amount of lexical elements they share, under the rough assumption that the more the better. In this paper we argue that: (1) Considering qualitative aspect</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Post, Soricut, Specia, 2012</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2012. Findings of the 2012 workshop on statistical machine translation. In Proceedings of the Seventh Workshop on Statistical Machine Translation (WMT’12), pages 10–51, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qin Gao</author>
<author>Stephan Vogel</author>
</authors>
<title>Parallel Implementations of Word Alignment Tool.</title>
<date>2008</date>
<booktitle>In Software Engineering, Testing, and Quality Assurance for Natural Language Processing,</booktitle>
<pages>49--57</pages>
<location>Columbus, Ohio, USA.</location>
<contexts>
<context position="10476" citStr="Gao and Vogel, 2008" startWordPosition="1675" endWordPosition="1678">odels we used the Europarl parallel corpus (Koehn, 2005), concatenated with the News Commentary corpus3 for three language pairs: De–En (2,079,049 sentences), Es–En (2,123,036 sentences), Fr–En (2,144,820 sentences). For It–En we only used the parallel data available in Europarl (1,909,115 sentences) since this language pair is not covered by the News Commentary corpus. IDF values for the words in each language were calculated on the monolingual part of these corpora, using the average IDF value of each language for unseen terms. To build the word alignment models we used the MGIZA++ package (Gao and Vogel, 2008). Experiments have been carried out with the hidden Markov model (HMM) (Vogel et al., 1996) and IBM models 3 and 4 (Brown et al., 1993).4 We also explored three symmetrization techniques (Koehn et al., 2005): union, intersection, and grow-diagfinal-and. A greedy feature selection process on training data, with different combinations of word alignment models and symmetrization methods, indicated HMM/intersection as the best performing combination. For this reason, all our experiments use this setting. The SVM implementation of Weka (Hall et al., 2009) was used to build the CLTE model.5 Two bina</context>
</contexts>
<marker>Gao, Vogel, 2008</marker>
<rawString>Qin Gao and Stephan Vogel. 2008. Parallel Implementations of Word Alignment Tool. In Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 49–57, Columbus, Ohio, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The WEKA Data Mining Software: an Update.</title>
<date>2009</date>
<journal>SIGKDD Explorations,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="11032" citStr="Hall et al., 2009" startWordPosition="1761" endWordPosition="1764">nment models we used the MGIZA++ package (Gao and Vogel, 2008). Experiments have been carried out with the hidden Markov model (HMM) (Vogel et al., 1996) and IBM models 3 and 4 (Brown et al., 1993).4 We also explored three symmetrization techniques (Koehn et al., 2005): union, intersection, and grow-diagfinal-and. A greedy feature selection process on training data, with different combinations of word alignment models and symmetrization methods, indicated HMM/intersection as the best performing combination. For this reason, all our experiments use this setting. The SVM implementation of Weka (Hall et al., 2009) was used to build the CLTE model.5 Two binary classifiers were trained to separately check T1 → T2 and T1 ← T2, merging their output to obtain the 4-class judgments (e.g. yes/yes=bidirectional, yes/no=forward). 3.1 Evaluation with CLTE training data Figure 2 shows the accuracy obtained by the different feature groups.6 For the sake of comparison, state-of-the-art results achieved for each language combination at SemEval 2012 are also reported. As regards Es–En (63.2% accuracy) and De–En (55.8%), the top scores were obtained by the system described in (W¨aschle and Fendrich, 2012), where a com</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The WEKA Data Mining Software: an Update. SIGKDD Explorations, 11(1):10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergio Jimenez</author>
<author>Claudia Becerra</author>
<author>Alexander Gelbukh</author>
</authors>
<title>Soft Cardinality + ML: Learning Adaptive Similarity Functions for Cross-lingual Textual Entailment.</title>
<date>2012</date>
<booktitle>In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>684--688</pages>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="12645" citStr="Jimenez et al., 2012" startWordPosition="2012" endWordPosition="2015">“*” indicates statistically significant improvements over the state of the art at p ≤ 0.05, calculated with approximate randomization (Pad´o, 2006). 773 ture of monolingual (i.e. with the input sentences translated in the same language using Google Translate7) and cross-lingual features. Although such system exploits word-alignment information to some extent, this is only done at quantitative level (e.g. number of unaligned words, percentage of aligned words, length of the longest unaligned subsequence). As regards It–En, the state of the art (56.6%) is represented by the system described in (Jimenez et al., 2012), which uses a pure pivoting method (using Google Translate) and adaptive similarity functions based on “soft” cardinality for flexible term comparisons. The two systems obtained the same result on Fr–En (57.0%). Es-En De-En Fr-En It-En Figure 2: Accuracy obtained by each feature group on four language combinations. As can be seen in Figure 2, the combination of all our features outperforms the state of the art for each language pair. The accuracy improvement ranges from 6.6% for Es–En (from 63.2% to 67.4%) to 14.6% for De–En (from 55.8% to 64%). Except for Es–En, that has very competitive sta</context>
</contexts>
<marker>Jimenez, Becerra, Gelbukh, 2012</marker>
<rawString>Sergio Jimenez, Claudia Becerra, and Alexander Gelbukh. 2012. Soft Cardinality + ML: Learning Adaptive Similarity Functions for Cross-lingual Textual Entailment. In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), pages 684–688, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Amittai Axelrod</author>
<author>Alexandra Birch Mayne</author>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>David Talbot</author>
</authors>
<title>Edinburgh System Description for the 2005 IWSLT Speech Translation Evaluation.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation,</booktitle>
<location>Pittsburgh, Pennsylvania, USA.</location>
<contexts>
<context position="10683" citStr="Koehn et al., 2005" startWordPosition="1711" endWordPosition="1714">tences). For It–En we only used the parallel data available in Europarl (1,909,115 sentences) since this language pair is not covered by the News Commentary corpus. IDF values for the words in each language were calculated on the monolingual part of these corpora, using the average IDF value of each language for unseen terms. To build the word alignment models we used the MGIZA++ package (Gao and Vogel, 2008). Experiments have been carried out with the hidden Markov model (HMM) (Vogel et al., 1996) and IBM models 3 and 4 (Brown et al., 1993).4 We also explored three symmetrization techniques (Koehn et al., 2005): union, intersection, and grow-diagfinal-and. A greedy feature selection process on training data, with different combinations of word alignment models and symmetrization methods, indicated HMM/intersection as the best performing combination. For this reason, all our experiments use this setting. The SVM implementation of Weka (Hall et al., 2009) was used to build the CLTE model.5 Two binary classifiers were trained to separately check T1 → T2 and T1 ← T2, merging their output to obtain the 4-class judgments (e.g. yes/yes=bidirectional, yes/no=forward). 3.1 Evaluation with CLTE training data </context>
</contexts>
<marker>Koehn, Axelrod, Mayne, Callison-Burch, Osborne, Talbot, 2005</marker>
<rawString>Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne, Chris Callison-Burch, Miles Osborne, and David Talbot. 2005. Edinburgh System Description for the 2005 IWSLT Speech Translation Evaluation. In Proceedings of the International Workshop on Spoken Language Translation, Pittsburgh, Pennsylvania, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Koehn</author>
</authors>
<title>Europarl: a Parallel Corpus for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In Proceedings of MT Summit X,</booktitle>
<pages>79--86</pages>
<location>Phuket, Thailand.</location>
<contexts>
<context position="9912" citStr="Koehn, 2005" startWordPosition="1588" endWordPosition="1589">2 of the test set. Then, simulating the less favorable situation in which labeled training data for L1–L2 are missing, we investigate the possibility to use existing CLTE models trained on labeled data for a different language pair L3–L4. The SemEval 2012 CLTE datasets used in our experiments are available for four language pairs: Es–En, De–En, Fr–En, and It–En. Each dataset was created with the crowdsourcing-based method described in Negri et al. (2011), and consists of 1000 T1–T2 pairs (500 for training, 500 for test). To train the word alignment models we used the Europarl parallel corpus (Koehn, 2005), concatenated with the News Commentary corpus3 for three language pairs: De–En (2,079,049 sentences), Es–En (2,123,036 sentences), Fr–En (2,144,820 sentences). For It–En we only used the parallel data available in Europarl (1,909,115 sentences) since this language pair is not covered by the News Commentary corpus. IDF values for the words in each language were calculated on the monolingual part of these corpora, using the average IDF value of each language for unseen terms. To build the word alignment models we used the MGIZA++ package (Gao and Vogel, 2008). Experiments have been carried out </context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philip Koehn. 2005. Europarl: a Parallel Corpus for Statistical Machine Translation. In Proceedings of MT Summit X, pages 79–86, Phuket, Thailand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Lenci</author>
<author>Roberto Bartolini</author>
<author>Nicoletta Calzolari</author>
<author>Ana Agua</author>
<author>Stephan Busemann</author>
<author>Emmanuel Cartier</author>
<author>Karine Chevreau</author>
<author>Jos´e Coch</author>
</authors>
<title>Multilingual summarization by integrating linguistic resources in the MLIS-MUSI Project.</title>
<date>2002</date>
<booktitle>In Proceedings of the Third International Conference on Language Resources and Evaluation (LREC’02),</booktitle>
<pages>1464--1471</pages>
<contexts>
<context position="5110" citStr="Lenci et al., 2002" startWordPosition="751" endWordPosition="754">6 CLTE model for L7-L8 Figure 1: System architecture in different training/evaluation conditions. (a): parallel data and CLTE labeled data are available for language pair L1-L2. (b): the L1-L2 CLTE model is used to cope with the unavailability of labeled data for L3-L4. (c): the same problem is tackled by combining multiple models. timation (Mehdad et al., 2012b),1 we aim to capture a richer set of relations potentially relevant to other tasks. For instance, recognizing unrelatedness, forward and backward entailment relations, represents a core problem in cross-lingual document summarization (Lenci et al., 2002) and content synchronization (Monz et al., 2011; Mehdad et al., 2012a). CLTE, as proposed within the SemEval evaluation exercises (Negri et al., 2012; Negri et al., 2013), represents an ideal framework to evaluate such capabilities. Within this framework, our goal is to automatically identify the following entailment relations between T1 and T2: forward (T1 → T2), backward (T1 ← T2), bidirectional (T1 ↔ T2) and no entailment. Our approach (see Figure 1) involves two core components: i) a word alignment model, and ii) a CLTE classifier. The former is trained on a parallel corpus, and associates</context>
</contexts>
<marker>Lenci, Bartolini, Calzolari, Agua, Busemann, Cartier, Chevreau, Coch, 2002</marker>
<rawString>Alessandro Lenci, Roberto Bartolini, Nicoletta Calzolari, Ana Agua, Stephan Busemann, Emmanuel Cartier, Karine Chevreau, and Jos´e Coch. 2002. Multilingual summarization by integrating linguistic resources in the MLIS-MUSI Project. In Proceedings of the Third International Conference on Language Resources and Evaluation (LREC’02), pages 1464–1471, Las Palmas de Gran Canaria, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yashar Mehdad</author>
<author>Matteo Negri</author>
<author>Marcello Federico</author>
</authors>
<title>Towards Cross-Lingual Textual Entailment.</title>
<date>2010</date>
<booktitle>In Proceedings of the Eleventh Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL HLT 2010),</booktitle>
<pages>321--324</pages>
<location>Los Angeles, California, USA.</location>
<contexts>
<context position="1863" citStr="Mehdad et al., 2010" startWordPosition="254" endWordPosition="257">for a variety of cross-lingual applications. So far, despite the relevance of the problem, research on multilingual applications has either circumvented the issue, or proposed partial solutions. When possible, the typical approach builds on the reduction to a monolingual task, burdening the process with dependencies from machine translation (MT) components. For instance, in crosslingual question answering and cross-lingual textual entailment (CLTE), intermediate MT steps are respectively performed to ease answer retrieval/presentation (Parton, 2012; Tanev et al., 2006) and semantic inference (Mehdad et al., 2010). Direct solutions that avoid such pivoting strategies typically exploit similarity measures that rely on bag-of-words representations. As an example, most supervised approaches to MT quality estimation (Blatz et al., 2003; Callison-Burch et al., 2012) and CLTE (W¨aschle and Fendrich, 2012) include features that consider the amount of equivalent terms that are found in the input sentence pairs. Such simplification, however, disregards the fact that semantic equivalence is not only proportional to the number of equivalent terms, but also to their importance. In other words, instead of checking </context>
</contexts>
<marker>Mehdad, Negri, Federico, 2010</marker>
<rawString>Yashar Mehdad, Matteo Negri, and Marcello Federico. 2010. Towards Cross-Lingual Textual Entailment. In Proceedings of the Eleventh Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL HLT 2010), pages 321–324, Los Angeles, California, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yashar Mehdad</author>
<author>Matteo Negri</author>
<author>Marcello Federico</author>
</authors>
<title>Detecting Semantic Equivalence and Information Disparity in Cross–lingual Documents.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL’12),</booktitle>
<pages>120--124</pages>
<location>Jeju Island,</location>
<contexts>
<context position="4854" citStr="Mehdad et al., 2012" startWordPosition="713" endWordPosition="716">Labeled CLTE data for L1-L2 Unlabeled CLTE data for L3-L4 Word alignment model for L3-L4 CLTE model for L1-L2 CLTE annotation Unlabeled CLTE data for L3 -L4 Word alignment model for L3-L4 CLTE annotation Combination CLTE model for L1-L2 CLTE model for L5-L6 CLTE model for L7-L8 Figure 1: System architecture in different training/evaluation conditions. (a): parallel data and CLTE labeled data are available for language pair L1-L2. (b): the L1-L2 CLTE model is used to cope with the unavailability of labeled data for L3-L4. (c): the same problem is tackled by combining multiple models. timation (Mehdad et al., 2012b),1 we aim to capture a richer set of relations potentially relevant to other tasks. For instance, recognizing unrelatedness, forward and backward entailment relations, represents a core problem in cross-lingual document summarization (Lenci et al., 2002) and content synchronization (Monz et al., 2011; Mehdad et al., 2012a). CLTE, as proposed within the SemEval evaluation exercises (Negri et al., 2012; Negri et al., 2013), represents an ideal framework to evaluate such capabilities. Within this framework, our goal is to automatically identify the following entailment relations between T1 and </context>
</contexts>
<marker>Mehdad, Negri, Federico, 2012</marker>
<rawString>Yashar Mehdad, Matteo Negri, and Marcello Federico. 2012a. Detecting Semantic Equivalence and Information Disparity in Cross–lingual Documents. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL’12), pages 120–124, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yashar Mehdad</author>
<author>Matteo Negri</author>
<author>Marcello Federico</author>
</authors>
<title>Match without a Referee: Evaluating MT Adequacy without Reference Translations.</title>
<date>2012</date>
<booktitle>In Proceedings of the Machine Translation Workshop (WMT2012),</booktitle>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="4854" citStr="Mehdad et al., 2012" startWordPosition="713" endWordPosition="716">Labeled CLTE data for L1-L2 Unlabeled CLTE data for L3-L4 Word alignment model for L3-L4 CLTE model for L1-L2 CLTE annotation Unlabeled CLTE data for L3 -L4 Word alignment model for L3-L4 CLTE annotation Combination CLTE model for L1-L2 CLTE model for L5-L6 CLTE model for L7-L8 Figure 1: System architecture in different training/evaluation conditions. (a): parallel data and CLTE labeled data are available for language pair L1-L2. (b): the L1-L2 CLTE model is used to cope with the unavailability of labeled data for L3-L4. (c): the same problem is tackled by combining multiple models. timation (Mehdad et al., 2012b),1 we aim to capture a richer set of relations potentially relevant to other tasks. For instance, recognizing unrelatedness, forward and backward entailment relations, represents a core problem in cross-lingual document summarization (Lenci et al., 2002) and content synchronization (Monz et al., 2011; Mehdad et al., 2012a). CLTE, as proposed within the SemEval evaluation exercises (Negri et al., 2012; Negri et al., 2013), represents an ideal framework to evaluate such capabilities. Within this framework, our goal is to automatically identify the following entailment relations between T1 and </context>
</contexts>
<marker>Mehdad, Negri, Federico, 2012</marker>
<rawString>Yashar Mehdad, Matteo Negri, and Marcello Federico. 2012b. Match without a Referee: Evaluating MT Adequacy without Reference Translations. In Proceedings of the Machine Translation Workshop (WMT2012), Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Monz</author>
<author>Vivi Nastase</author>
<author>Matteo Negri</author>
<author>Angela Fahrni</author>
<author>Yashar Mehdad</author>
<author>Michael Strube</author>
</authors>
<title>CoSyne: a Framework for Multilingual Content Synchronization of Wikis.</title>
<date>2011</date>
<booktitle>In Proceedings of WikiSym 2011, the International Symposium on Wikis and Open Collaboration,</booktitle>
<pages>217--218</pages>
<location>Mountain View, California, USA.</location>
<contexts>
<context position="5157" citStr="Monz et al., 2011" startWordPosition="759" endWordPosition="762">ure in different training/evaluation conditions. (a): parallel data and CLTE labeled data are available for language pair L1-L2. (b): the L1-L2 CLTE model is used to cope with the unavailability of labeled data for L3-L4. (c): the same problem is tackled by combining multiple models. timation (Mehdad et al., 2012b),1 we aim to capture a richer set of relations potentially relevant to other tasks. For instance, recognizing unrelatedness, forward and backward entailment relations, represents a core problem in cross-lingual document summarization (Lenci et al., 2002) and content synchronization (Monz et al., 2011; Mehdad et al., 2012a). CLTE, as proposed within the SemEval evaluation exercises (Negri et al., 2012; Negri et al., 2013), represents an ideal framework to evaluate such capabilities. Within this framework, our goal is to automatically identify the following entailment relations between T1 and T2: forward (T1 → T2), backward (T1 ← T2), bidirectional (T1 ↔ T2) and no entailment. Our approach (see Figure 1) involves two core components: i) a word alignment model, and ii) a CLTE classifier. The former is trained on a parallel corpus, and associates equivalent terms in T1 and T2. The information</context>
</contexts>
<marker>Monz, Nastase, Negri, Fahrni, Mehdad, Strube, 2011</marker>
<rawString>Christoph Monz, Vivi Nastase, Matteo Negri, Angela Fahrni, Yashar Mehdad, and Michael Strube. 2011. CoSyne: a Framework for Multilingual Content Synchronization of Wikis. In Proceedings of WikiSym 2011, the International Symposium on Wikis and Open Collaboration, pages 217–218, Mountain View, California, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matteo Negri</author>
<author>Luisa Bentivogli</author>
<author>Yashar Mehdad</author>
<author>Danilo Giampiccolo</author>
<author>Alessandro Marchetti</author>
</authors>
<title>Divide and Conquer: Crowdsourcing the Creation of Cross-Lingual Textual Entailment Corpora.</title>
<date>2011</date>
<contexts>
<context position="9758" citStr="Negri et al. (2011)" startWordPosition="1560" endWordPosition="1563">nts Our experiments cover two different scenarios. First, the typical one, in which the CLTE model is trained on labeled data for the same pair of languages L1–L2 of the test set. Then, simulating the less favorable situation in which labeled training data for L1–L2 are missing, we investigate the possibility to use existing CLTE models trained on labeled data for a different language pair L3–L4. The SemEval 2012 CLTE datasets used in our experiments are available for four language pairs: Es–En, De–En, Fr–En, and It–En. Each dataset was created with the crowdsourcing-based method described in Negri et al. (2011), and consists of 1000 T1–T2 pairs (500 for training, 500 for test). To train the word alignment models we used the Europarl parallel corpus (Koehn, 2005), concatenated with the News Commentary corpus3 for three language pairs: De–En (2,079,049 sentences), Es–En (2,123,036 sentences), Fr–En (2,144,820 sentences). For It–En we only used the parallel data available in Europarl (1,909,115 sentences) since this language pair is not covered by the News Commentary corpus. IDF values for the words in each language were calculated on the monolingual part of these corpora, using the average IDF value o</context>
</contexts>
<marker>Negri, Bentivogli, Mehdad, Giampiccolo, Marchetti, 2011</marker>
<rawString>Matteo Negri, Luisa Bentivogli, Yashar Mehdad, Danilo Giampiccolo, and Alessandro Marchetti. 2011. Divide and Conquer: Crowdsourcing the Creation of Cross-Lingual Textual Entailment Corpora.</rawString>
</citation>
<citation valid="false">
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP 2011),</booktitle>
<location>Edinburgh, Scotland.</location>
<marker></marker>
<rawString>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP 2011), Edinburgh, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matteo Negri</author>
<author>Alessandro Marchetti</author>
<author>Yashar Mehdad</author>
<author>Luisa Bentivogli</author>
<author>Danilo Giampiccolo</author>
</authors>
<title>Semeval-2012 Task 8: Cross-Lingual Textual Entailment for Content Synchronization.</title>
<date>2012</date>
<booktitle>In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>399--407</pages>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="5259" citStr="Negri et al., 2012" startWordPosition="776" endWordPosition="779">le for language pair L1-L2. (b): the L1-L2 CLTE model is used to cope with the unavailability of labeled data for L3-L4. (c): the same problem is tackled by combining multiple models. timation (Mehdad et al., 2012b),1 we aim to capture a richer set of relations potentially relevant to other tasks. For instance, recognizing unrelatedness, forward and backward entailment relations, represents a core problem in cross-lingual document summarization (Lenci et al., 2002) and content synchronization (Monz et al., 2011; Mehdad et al., 2012a). CLTE, as proposed within the SemEval evaluation exercises (Negri et al., 2012; Negri et al., 2013), represents an ideal framework to evaluate such capabilities. Within this framework, our goal is to automatically identify the following entailment relations between T1 and T2: forward (T1 → T2), backward (T1 ← T2), bidirectional (T1 ↔ T2) and no entailment. Our approach (see Figure 1) involves two core components: i) a word alignment model, and ii) a CLTE classifier. The former is trained on a parallel corpus, and associates equivalent terms in T1 and T2. The information about word alignments is used to extract quantitative (amount and distribution of the alignments) and</context>
</contexts>
<marker>Negri, Marchetti, Mehdad, Bentivogli, Giampiccolo, 2012</marker>
<rawString>Matteo Negri, Alessandro Marchetti, Yashar Mehdad, Luisa Bentivogli, and Danilo Giampiccolo. 2012. Semeval-2012 Task 8: Cross-Lingual Textual Entailment for Content Synchronization. In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), pages 399–407, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matteo Negri</author>
<author>Alessandro Marchetti</author>
<author>Yashar Mehdad</author>
<author>Luisa Bentivogli</author>
<author>Danilo Giampiccolo</author>
</authors>
<title>Semeval-2013 Task 8: Cross-Lingual Textual Entailment for Content Synchronization.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval</booktitle>
<location>Atlanta, GA.</location>
<contexts>
<context position="5280" citStr="Negri et al., 2013" startWordPosition="780" endWordPosition="783"> L1-L2. (b): the L1-L2 CLTE model is used to cope with the unavailability of labeled data for L3-L4. (c): the same problem is tackled by combining multiple models. timation (Mehdad et al., 2012b),1 we aim to capture a richer set of relations potentially relevant to other tasks. For instance, recognizing unrelatedness, forward and backward entailment relations, represents a core problem in cross-lingual document summarization (Lenci et al., 2002) and content synchronization (Monz et al., 2011; Mehdad et al., 2012a). CLTE, as proposed within the SemEval evaluation exercises (Negri et al., 2012; Negri et al., 2013), represents an ideal framework to evaluate such capabilities. Within this framework, our goal is to automatically identify the following entailment relations between T1 and T2: forward (T1 → T2), backward (T1 ← T2), bidirectional (T1 ↔ T2) and no entailment. Our approach (see Figure 1) involves two core components: i) a word alignment model, and ii) a CLTE classifier. The former is trained on a parallel corpus, and associates equivalent terms in T1 and T2. The information about word alignments is used to extract quantitative (amount and distribution of the alignments) and qualitative features</context>
</contexts>
<marker>Negri, Marchetti, Mehdad, Bentivogli, Giampiccolo, 2013</marker>
<rawString>Matteo Negri, Alessandro Marchetti, Yashar Mehdad, Luisa Bentivogli, and Danilo Giampiccolo. 2013. Semeval-2013 Task 8: Cross-Lingual Textual Entailment for Content Synchronization. In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013), Atlanta, GA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="14756" citStr="Och and Ney, 2003" startWordPosition="2350" endWordPosition="2353">ing features for each language pair produces further improvements of varying degrees in Es–En (from 67.4% to 68%), De–En (64% – 64.8%) and It–En (63.4% – 66.8%), while performance remains stable for Fr–En (63%). All these configurations include the IDF feature (12) and the proportion of aligned words for each PoS category (7), proving the effectiveness of qualitative word alignment features. The fact that HMM/intersection is the best combination of alignment model and symmetrization method is interesting, since it contradicts the general notion that IBM models 3 and 4 perform better than HMM (Och and Ney, 2003). A possible explanation is that, while word alignment models are usually trained on parallel corpora, the majority of CLTE sentence pairs are not parallel. In this setting, where producing reliable alignments is more difficult, IBM models are less effective for at least two reasons. First, including a word fertility model, IBM 3 and 4 limit (typically to the half of the source sentence length) the number of target words that can be aligned with the null word. Therefore, when such limit is reached, these models tend to force low probability, hence less reliable, word alignments. Second, in IBM</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz J. Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
</authors>
<title>User’s guide to sigf: Significance testing by approximate randomisation.</title>
<date>2006</date>
<marker>Pad´o, 2006</marker>
<rawString>Sebastian Pad´o, 2006. User’s guide to sigf: Significance testing by approximate randomisation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristen Parton</author>
</authors>
<title>Lost and Found in Translation: Cross-Lingual Question Answering with Result Translation.</title>
<date>2012</date>
<tech>Ph.D. thesis,</tech>
<institution>Columbia University.</institution>
<contexts>
<context position="1797" citStr="Parton, 2012" startWordPosition="245" endWordPosition="246">ison and projection across sentences are major challenges for a variety of cross-lingual applications. So far, despite the relevance of the problem, research on multilingual applications has either circumvented the issue, or proposed partial solutions. When possible, the typical approach builds on the reduction to a monolingual task, burdening the process with dependencies from machine translation (MT) components. For instance, in crosslingual question answering and cross-lingual textual entailment (CLTE), intermediate MT steps are respectively performed to ease answer retrieval/presentation (Parton, 2012; Tanev et al., 2006) and semantic inference (Mehdad et al., 2010). Direct solutions that avoid such pivoting strategies typically exploit similarity measures that rely on bag-of-words representations. As an example, most supervised approaches to MT quality estimation (Blatz et al., 2003; Callison-Burch et al., 2012) and CLTE (W¨aschle and Fendrich, 2012) include features that consider the amount of equivalent terms that are found in the input sentence pairs. Such simplification, however, disregards the fact that semantic equivalence is not only proportional to the number of equivalent terms, </context>
</contexts>
<marker>Parton, 2012</marker>
<rawString>Kristen Parton. 2012. Lost and Found in Translation: Cross-Lingual Question Answering with Result Translation. Ph.D. thesis, Columbia University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dipanjan Das</author>
<author>Ryan McDonald</author>
</authors>
<title>A universal part-of-speech tagset.</title>
<date>2012</date>
<booktitle>In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12),</booktitle>
<pages>pages</pages>
<location>Istanbul, Turkey.</location>
<contexts>
<context position="8065" citStr="Petrov et al. (2012)" startWordPosition="1260" endWordPosition="1263">; 6. proportion of word n-grams in T∗ containing only aligned words (the feature was com2For instance, the fact that aligning all nouns and the most relevant terms in T1 and T2 is a good indicator of semantic equivalence. 772 puted separately for values of n = 1... 5). The POS group considers the part of speech (PoS) of the words in T∗ as a source of qualitative information about their importance. To compute these features we use the TreeTagger (Schmid, 1995), manually mapping the fine-grained set of assigned PoS labels into a more general set of tags (P) based on the universal PoS tag set by Petrov et al. (2012). POS features differentiate between aligned words (words in T1 that are aligned to one or more words in T2) and alignments (the edges connecting words in T1 and T2). Features considering the aligned words in T∗ are: 7. for each PoS tag p E P, proportion of aligned words in T∗ tagged with p; 8. proportion of words in T1 aligned with words with the same PoS tag in T2 (and vice-versa); 9. for each PoS tag p E P, proportion of words in T1 tagged as p which are aligned to words with the same tag in T2 (and vice-versa). Features considering the alignments are: 10. proportion of alignments connectin</context>
</contexts>
<marker>Petrov, Das, McDonald, 2012</marker>
<rawString>Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012. A universal part-of-speech tagset. In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12), pages 2089– 2096, Istanbul, Turkey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Christopher Buckley</author>
</authors>
<date>1988</date>
<booktitle>Term-weighting Approaches in Automatic Text Retrieval. Information Processing and Management,</booktitle>
<pages>24--5</pages>
<contexts>
<context position="8870" citStr="Salton and Buckley, 1988" startWordPosition="1411" endWordPosition="1414">idering the aligned words in T∗ are: 7. for each PoS tag p E P, proportion of aligned words in T∗ tagged with p; 8. proportion of words in T1 aligned with words with the same PoS tag in T2 (and vice-versa); 9. for each PoS tag p E P, proportion of words in T1 tagged as p which are aligned to words with the same tag in T2 (and vice-versa). Features considering the alignments are: 10. proportion of alignments connecting words with the same PoS tag p; 11. for each PoS tag p E P, proportion of alignments connecting two words tagged as p. IDF, the last feature, uses the inverse document frequency (Salton and Buckley, 1988) as another source of qualitative information under the assumption that rare words (and, therefore, with higher IDF) are more informative: 12. summation of all the IDF scores of the aligned words in T∗ over the summation of the IDF scores of all words in T∗. 3 Experiments Our experiments cover two different scenarios. First, the typical one, in which the CLTE model is trained on labeled data for the same pair of languages L1–L2 of the test set. Then, simulating the less favorable situation in which labeled training data for L1–L2 are missing, we investigate the possibility to use existing CLTE</context>
</contexts>
<marker>Salton, Buckley, 1988</marker>
<rawString>Gerard Salton and Christopher Buckley. 1988. Term-weighting Approaches in Automatic Text Retrieval. Information Processing and Management, 24(5):513–523.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Improvements in Part-ofSpeech Tagging with an Application to German.</title>
<date>1995</date>
<booktitle>In Proceedings of the ACL SIGDAT-Workshop,</booktitle>
<pages>47--50</pages>
<location>Dublin, Ireland.</location>
<contexts>
<context position="7908" citStr="Schmid, 1995" startWordPosition="1233" endWordPosition="1234">, and b) the unaligned word sequences; 5. position of a) the first unaligned word, and b) the last unaligned word, both normalized by the lenght of T∗; 6. proportion of word n-grams in T∗ containing only aligned words (the feature was com2For instance, the fact that aligning all nouns and the most relevant terms in T1 and T2 is a good indicator of semantic equivalence. 772 puted separately for values of n = 1... 5). The POS group considers the part of speech (PoS) of the words in T∗ as a source of qualitative information about their importance. To compute these features we use the TreeTagger (Schmid, 1995), manually mapping the fine-grained set of assigned PoS labels into a more general set of tags (P) based on the universal PoS tag set by Petrov et al. (2012). POS features differentiate between aligned words (words in T1 that are aligned to one or more words in T2) and alignments (the edges connecting words in T1 and T2). Features considering the aligned words in T∗ are: 7. for each PoS tag p E P, proportion of aligned words in T∗ tagged with p; 8. proportion of words in T1 aligned with words with the same PoS tag in T2 (and vice-versa); 9. for each PoS tag p E P, proportion of words in T1 tag</context>
</contexts>
<marker>Schmid, 1995</marker>
<rawString>Helmut Schmid. 1995. Improvements in Part-ofSpeech Tagging with an Application to German. In Proceedings of the ACL SIGDAT-Workshop, pages 47–50, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hristo Tanev</author>
<author>Milen Kouylekov</author>
<author>Bernardo Magnini</author>
<author>Matteo Negri</author>
<author>Kiril Simov</author>
</authors>
<title>Exploiting Linguistic Indices and Syntactic Structures for Multilingual Question Answering: ITC-irst at CLEF 2005. Accessing Multilingual Information Repositories,</title>
<date>2006</date>
<pages>390--399</pages>
<contexts>
<context position="1818" citStr="Tanev et al., 2006" startWordPosition="247" endWordPosition="250">ction across sentences are major challenges for a variety of cross-lingual applications. So far, despite the relevance of the problem, research on multilingual applications has either circumvented the issue, or proposed partial solutions. When possible, the typical approach builds on the reduction to a monolingual task, burdening the process with dependencies from machine translation (MT) components. For instance, in crosslingual question answering and cross-lingual textual entailment (CLTE), intermediate MT steps are respectively performed to ease answer retrieval/presentation (Parton, 2012; Tanev et al., 2006) and semantic inference (Mehdad et al., 2010). Direct solutions that avoid such pivoting strategies typically exploit similarity measures that rely on bag-of-words representations. As an example, most supervised approaches to MT quality estimation (Blatz et al., 2003; Callison-Burch et al., 2012) and CLTE (W¨aschle and Fendrich, 2012) include features that consider the amount of equivalent terms that are found in the input sentence pairs. Such simplification, however, disregards the fact that semantic equivalence is not only proportional to the number of equivalent terms, but also to their imp</context>
</contexts>
<marker>Tanev, Kouylekov, Magnini, Negri, Simov, 2006</marker>
<rawString>Hristo Tanev, Milen Kouylekov, Bernardo Magnini, Matteo Negri, and Kiril Simov. 2006. Exploiting Linguistic Indices and Syntactic Structures for Multilingual Question Answering: ITC-irst at CLEF 2005. Accessing Multilingual Information Repositories, pages 390–399.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>HMM-based Word Alignment in Statistical Translation.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th International Conference on Computational Linguistics (ACL’96),</booktitle>
<pages>836--841</pages>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="10567" citStr="Vogel et al., 1996" startWordPosition="1691" endWordPosition="1694">ary corpus3 for three language pairs: De–En (2,079,049 sentences), Es–En (2,123,036 sentences), Fr–En (2,144,820 sentences). For It–En we only used the parallel data available in Europarl (1,909,115 sentences) since this language pair is not covered by the News Commentary corpus. IDF values for the words in each language were calculated on the monolingual part of these corpora, using the average IDF value of each language for unseen terms. To build the word alignment models we used the MGIZA++ package (Gao and Vogel, 2008). Experiments have been carried out with the hidden Markov model (HMM) (Vogel et al., 1996) and IBM models 3 and 4 (Brown et al., 1993).4 We also explored three symmetrization techniques (Koehn et al., 2005): union, intersection, and grow-diagfinal-and. A greedy feature selection process on training data, with different combinations of word alignment models and symmetrization methods, indicated HMM/intersection as the best performing combination. For this reason, all our experiments use this setting. The SVM implementation of Weka (Hall et al., 2009) was used to build the CLTE model.5 Two binary classifiers were trained to separately check T1 → T2 and T1 ← T2, merging their output t</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1996. HMM-based Word Alignment in Statistical Translation. In Proceedings of the 16th International Conference on Computational Linguistics (ACL’96), pages 836–841, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katharina W¨aschle</author>
<author>Sascha Fendrich</author>
</authors>
<title>HDU: Cross-lingual Textual Entailment with SMT Features.</title>
<date>2012</date>
<booktitle>In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>467--471</pages>
<location>Montr´eal, Canada.</location>
<marker>W¨aschle, Fendrich, 2012</marker>
<rawString>Katharina W¨aschle and Sascha Fendrich. 2012. HDU: Cross-lingual Textual Entailment with SMT Features. In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), pages 467–471, Montr´eal, Canada.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>