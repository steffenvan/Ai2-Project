<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000064">
<title confidence="0.954015">
Evaluating Response Strategies in a Web-Based Spoken Dialogue Agent
</title>
<author confidence="0.710119">
Diane J. Litman
</author>
<affiliation confidence="0.593529">
AT&amp;T Labs - Research
</affiliation>
<address confidence="0.776742">
180 Park Avenue
Florham Park, NJ 07932 USA
</address>
<email confidence="0.994144">
diane@research.att.com
</email>
<author confidence="0.994284">
Shimei Pan
</author>
<affiliation confidence="0.9964125">
Computer Science Department
Columbia University
</affiliation>
<address confidence="0.981749">
New York, NY 10027 USA
</address>
<email confidence="0.99822">
pan@cs.columbia.edu
</email>
<note confidence="0.4471175">
Marilyn A. Walker
AT&amp;T Labs - Research
180 Park Avenue
Florham Park, NJ 07932 USA
</note>
<email confidence="0.996185">
walker@research.att.com
</email>
<sectionHeader confidence="0.993834" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999977136363636">
While the notion of a cooperative response has been
the focus of considerable research in natural lan-
guage dialogue systems, there has been little empir-
ical work demonstrating how such responses lead
to more efficient, natural, or successful dialogues.
This paper presents an experimental evaluation of
two alternative response strategies in TOOT, a spo-
ken dialogue agent that allows users to access train
schedules stored on the web via a telephone conver-
sation. We compare the performance of two ver-
sions of TOOT (literal and cooperative), by hav-
ing users carry out a set of tasks with each ver-
sion. By using hypothesis testing methods, we show
that a combination of response strategy, application
task, and task/strategy interactions account for var-
ious types of performance differences. By using
the PARADISE evaluation framework to estimate
an overall performance function, we identify inter-
dependencies that exist between speech recognition
and response strategy. Our results elaborate the con-
ditions under which TOOT&apos; s cooperative rather than
literal strategy contributes to greater performance.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999992102564102">
The notion of a cooperative response has been the
focus of considerable research in natural language
and spoken dialogue systems (Allen and Perrault,
1980; Mays, 1980; Kaplan, 1981; Joshi et al., 1984;
McCoy, 1989; Pao and Wilpon, 1992; Moore, 1994;
Seneff et al., 1995; Goddeau et al., 1996; Pierac-
cini et al., 1997). However, despite the existence
of many algorithms for generating cooperative re-
sponses, there has been little empirical work ad-
dressing the evaluation of such algorithms in the
context of real-time natural language dialogue sys-
tems with human users. Thus it is unclear un-
der what conditions cooperative responses result in
more efficient or efficacious dialogues.
This paper presents an empirical evaluation
of two alternative algorithms for responding to
database queries in TOOT, a spoken dialogue agent
for accessing online train schedules via a telephone
conversation. We conduct an experiment in which
12 users carry out 4 tasks of varying difficulty with
one of two versions of TOOT (literal and coopera-
tive TOOT), resulting in a corpus of 48 dialogues.
The values for a wide range of evaluation measures
are then extracted from this corpus. We analyze
our data using both traditional hypothesis testing
methods and the PARADISE (Walker et al., 1997;
Walker et al., 1998) methodology for estimating a
performance function. Hypothesis testing shows
that while differences among some evaluation mea-
sures depend on the response strategy (literal or co-
operative), other differences are a function of appli-
cation task and task/strategy interactions. A PAR-
ADISE assessment of the contribution of each eval-
uation measure to overall performance shows that
strategy-dependent dialogue phenomena as well as
phenomena associated with speech recognition sig-
nificantly predict performance. Our results identify
the conditions under which TOOT&apos; s cooperative re-
sponse strategy leads to greater agent performance.
</bodyText>
<sectionHeader confidence="0.996054" genericHeader="introduction">
2 TOOT
</sectionHeader>
<bodyText confidence="0.999681222222222">
TOOT allows users to access online AMTRAK train
schedules via a telephone dialogue, as in Figure 1.1
(All examples are from the experiment in Section 3.)
We have built two versions of TOOT: literal TOOT
(LT) and cooperative TOOT (CT). LT and CT have
equivalent functionality, but use different response
strategies to present tabular results of web queries
in a displayless environment.2 LT and CT incorpo-
rate many of the types of database responses in the
</bodyText>
<footnote confidence="0.998917">
&apos;Our domain was chosen to afford future comparison with
similar systems, e.g., (Danieli and Gerbino, 1995).
2Initiative (Walker et al., 1998) and confirmation (Danieli
and Gerbino, 1995) strategies are equivalent in LT and CT, and
were chosen to reduce speech recognition errors.
</footnote>
<page confidence="0.990194">
780
</page>
<listItem confidence="0.803941965517241">
T: Hi, this is the Amtrak train schedule system.
Which city do you want to go to?
U: Philadelphia.
T: What city do you want to go to Philadelphia from?
U: New York City.
T: On what day of the week do you want to go to
Philadelphia from New York City?
U: On the weekend.
T: What time do you want to leave on the weekend?
U: 4 o&apos;clock pm.
T: Do you want me to find the trains going to
Philadelphia from New York City on the
weekend around 4 o&apos;clock pm now?.
U: Yes.
T: I am going to query the Amtrak website for you.
This may take some time if the internet traffic is
heavy. Please wait patiently. [30 second pause]
There are 7 trains available around 4 o&apos;clock
pm. Here are the first 3 trains. The 1st train
leaves at 3:00 pm on Saturday and Sunday; and
it takes I hour 12 mins. The 2nd train leaves at
3:20 pm every da); and it takes 1 hour 22 mins.
The 3rd train leaves at 4:00 pm on Sunda); and
it takes 1 hour 12 mins. Do you want to hear the
rest of the trains?
U: No.
T: Do you want to continue and find a new set of
trains?
U: No.
</listItem>
<bodyText confidence="0.7805515">
T: Thank you for using the Amtrak schedule system.
See you next time.
</bodyText>
<figureCaption confidence="0.999473">
Figure 1: Example dialogue with (literal) TOOT.
</figureCaption>
<bodyText confidence="0.958551272727273">
literature into relatively literal and cooperative re-
sponse strategies. (More sophisticated cooperative
strategies could be imagined.) When there is too
much information to present in a single utterance,
LT groups the information into units of 3 trains,
then presents each unit, as in the italicized portion
of Figure 1. In contrast, CT summarizes the range
of trains available, then tells the user to either list the
trains or further constrain the query. In CT, the ital-
icized portion of Figure 1 would be replaced with
the following response:
</bodyText>
<footnote confidence="0.9556323">
(1) There are 7 trains available around 4 o&apos;clock pm.
Here is the earliest train we have. The first train
leaves at 3:00 pm on Saturday and Sunday, and it
takes I hour 12 mins. Here is the latest train we
have. The seventh train leaves at 5:00 pm on Satur-
da); and it takes 1 hour 12 mins. Please say &amp;quot;list&amp;quot;
to hear trains 3 at a time, or say &amp;quot;add constraint&amp;quot;
to constrain your departure time or travel da); or
say &amp;quot;continue&amp;quot; if my answer was sufficient, or say
&amp;quot;repeat&amp;quot; to hear this message again.
</footnote>
<bodyText confidence="0.989278357142857">
LT&apos;s response incrementally presents the set of
trains that match the query, until the user tells LT to
stop. Enumerating large lists, even incrementally,
can lead to information overload. CT&apos;s response
is more cooperative because it better respects the
resource limitations of the listener. CT presents a
subset of the matching trains using a summary re-
sponse (Pao and Wilpon, 1992), followed by an op-
tion to reduce the information to be retrieved (Pier-
accini et al., 1997; Goddeau et al., 1996; Seneff et
al., 1995; Pao and Wilpon, 1992).
If there is no information that matches a query,
LT reports only the lack of an answer to the query,
as in the following dialogue excerpt:
</bodyText>
<listItem confidence="0.837457428571429">
(2) There are no trains going to Chicago from
Philadelphia on Sunday around 10:30 am. Do you
want to continue and find a new set of trains?
CT automatically relaxes the user&apos;s time constraint
and allows the user to perform other relaxations:
(3) There are no trains going to Chicago from
Philadelphia on Sunday around 10:30 am. The
closest earlier train leaves at 9:28 am every day,
and it takes 1 day 3 hours 36 mins. The closest later
train leaves at 11:45 am on Saturday and Sunday,
and it takes 22 hours 5 mins. Please say &amp;quot;relax&amp;quot;
to change your departure time or travel da); or say
&amp;quot;continue&amp;quot; if my answer was sufficient, or say &amp;quot;re-
peat&amp;quot; to hear this message again.
</listItem>
<bodyText confidence="0.976344375">
CT&apos;s response is more cooperative since identify-
ing the source of a query failure can help block in-
correct user inferences (Pieraccini et al., 1997; Pao
and Wilpon, 1992; Joshi et al., 1984; Kaplan, 1981;
Mays, 1980). LT&apos;s response could lead the user to
believe that there are no trains on Sunday.
When there are 1-3 trains that match a query, both
LT and CT list the trains:
</bodyText>
<listItem confidence="0.8625964">
(4) There are 2 trains available around 6 pm. The first
train leaves at 6:05 pm every day, and it takes 5
hours 10 mins. The second train leaves at 6:30 pm
every day, and it takes 2 days 11 hours 30 mins. Do
you want to continue and find a new set of trains?
</listItem>
<bodyText confidence="0.998938125">
TOOT is implemented using a platform for spo-
ken dialogue agents (Kamm et al., 1997) that com-
bines automatic speech recognition (ASR), text-
to-speech (TTS), a phone interface, and modules
for specifying a dialogue manager and application
functions. ASR in our platform supports barge-in,
an advanced functionality which allows users to in-
terrupt an agent when it is speaking.
</bodyText>
<page confidence="0.986637">
781
</page>
<bodyText confidence="0.999398166666667">
The dialogue manager uses a finite state machine
to implement dialogue strategies. Each state spec-
ifies 1) an initial prompt (or response) which the
agent says upon entering the state (such prompts of-
ten elicit parameter values); 2) a help prompt which
the agent says if the user says help; 3) rejection
prompts which the agent says if the confidence level
of ASR is too low (rejection prompts typically ask
the user to repeat or paraphrase their utterance); and
4) timeout prompts which the agent says if the user
doesn&apos;t say anything within a specified time frame
(timeout prompts are often suggestions about what
to say). A context-free grammar specifies what ASR
can recognize in each state. Transitions between
states are driven by semantic interpretation.
TOOT&apos; s application functions access and process
information on AMTRAK&apos;S web site. Given a set of
constraints, the functions return a table listing all
matching trains in a specified temporal interval, or
within an hour of a specified timepoint. This table is
converted to a natural language response which can
be realized by TI&apos;S through the use of templates for
either the LT or the CT response type; values in the
table instantiate template variables.
</bodyText>
<sectionHeader confidence="0.997439" genericHeader="method">
3 Experimental Design
</sectionHeader>
<bodyText confidence="0.966907615384615">
The experimental instructions were given on a web
page, which consisted of a description of TOOT&apos; s
functionality, hints for talking to TOOT, and links
to 4 task pages. Each task page contained a task
scenario, the hints, instructions for calling TOOT,
and a web survey designed to ascertain the depart
and travel times obtained by the user and to measure
user perceptions of task success and agent usability.
Users were 12 researchers not involved with the de-
sign or implementation of TOOT; 6 users were ran-
domly assigned to LT and 6 to CT. Users read the in-
structions in their office and then called TOOT from
their phone. Our experiment yielded a corpus of 48
dialogues (1344 total turns; 214 minutes of speech).
Users were provided with task scenarios for two
reasons. First, our hypothesis was that performance
depended not only on response strategy, but also on
task difficulty. To include the task as a factor in our
experiment, we needed to ensure that users executed
the same tasks and that they varied in difficulty.
Figure 2 shows the task scenarios used in our ex-
periment. Our hypotheses about agent performance
are summarized in Table 1. We predicted that op-
timal performance would occur whenever the cor-
rect task solution was included in TOOT&apos; s initial re-
Task 1 (Exact-Match): Try to find a train going to
Boston from New York City on Saturday at 6:00
pm. If you cannot find an exact match, find the one
with the closest departure time. Write down the ex-
act departure time of the train you found as well
as the total travel time.
Task 2 (No-Match-1): Try to find a train going to
Chicago from Philadelphia on Sunday at 10:30
am. If you cannot find an exact match, find the one
with the closest departure time. Write down the ex-
act departure time of the train you found as well
as the total travel time.
Task 3 (No-Match-2): Try to find a train going to
Boston from Washington D.C. on Thursday at
3:30 pm. If you cannot find an exact match, find
the one between 12:00 pm and 5:00 pm that has
the shortest travel time. Write down the exact de-
parture time of the train you found as well as the
total travel time.
Task 4 (Too-Much-Info/Early-Answer): Try to find a
train going to Philadelphia from New York City
on the weekend at 4:00 pm. If you cannot find
an exact match, find the one with the closest de-
parture time. Please write down the exact depar-
ture time of the train you found as well as the total
travel time. (&amp;quot;weekend&amp;quot; means the train departure
date includes either Saturday or Sunday)
</bodyText>
<figureCaption confidence="0.998838">
Figure 2: Task scenarios.
</figureCaption>
<bodyText confidence="0.995803714285714">
sponse to a web query (i.e., when the task was easy).
Task 1 (dialogue fragment (4) above) produced
a query that resulted in 2 matching trains, one of
which was the train requested in the scenario. Since
the response strategies of LT and CT were identical
under this condition, we predicted identical LT and
CT performance, as shown in Table 1.3
Tasks 2 (dialogue fragments (2) and (3)) and 3 led
to queries that yielded no matching trains. In Task 2
users were told to find the closest train. Since only
CT included this extra information in its response,
we predicted that it would perform better than LT.
In Task 3 users were told to find the shortest
train within a new departure interval. Since neither
LT nor CT provided this information initially, we
hypothesized comparable LT and CT performance.
However, since CT allowed users to change just
their departure time while LT required users to con-
struct a whole new query, we also thought it possible
that CT might perform slightly better than LT.
Task 4 (Figure 1 and dialogue fragment (1)) led to
</bodyText>
<footnote confidence="0.8420545">
3Since Task I was the easiest, it was always performed first.
The order of the remaining tasks was randomized across users.
</footnote>
<page confidence="0.952663">
782
</page>
<table confidence="0.9899506">
Task LT Strategy CT Strategy Hypothesis
Exact-Match Say it Say it LT equal to CT
No-Match-1 Say No Match Relax Time Constraint LT worse than CT
No-Match-2 Say No Match Relax Time Constraint LT equal to or worse than CT
Too-Much-Info/Early-Answer List 3 then more? Summarize; Give Options LT better than CT
</table>
<tableCaption confidence="0.999951">
Table 1: Hypothesized performance of literal TOOT (LT) versus cooperative TOOT (CT).
</tableCaption>
<bodyText confidence="0.99427775">
a query where the 3rd of 7 matching trains was the
desired answer. Since only LT included this train in
its initial response (by luck, due to the train&apos;s po-
sition in the list of matches), we predicted that LT
would perform better than CT. Note that this pre-
diction is highly dependent on the database. If the
desired train had been last in the list, we would have
predicted that CT would perform better than LT.
</bodyText>
<table confidence="0.997417142857143">
attribute value
arrival-city Philadelphia
depart-city New York City
depart-day weekend
depart-range 4:00 pm
exact-depart-time 4:00 pm
total-travel-time 1 hour 12 mins
</table>
<tableCaption confidence="0.999349">
Table 2: Scenano key, Task 4.
</tableCaption>
<bodyText confidence="0.999966363636364">
A second reason for having task scenarios
was that it allowed us to objectively determine
whether users achieved their tasks. Following PAR-
ADISE (Walker et al., 1997), we defined a &amp;quot;key&amp;quot; for
each scenario using an attribute value matrix (AVM)
task representation, as in Table 2. The key indicates
the attribute values that must be exchanged between
the agent and user by the end of the dialogue. If
the task is successfully completed in a scenario ex-
ecution (as in Figure 1), the AVM representing the
dialogue is identical to the key.
</bodyText>
<sectionHeader confidence="0.990724" genericHeader="method">
4 Measuring Aspects of Performance
</sectionHeader>
<bodyText confidence="0.999982083333333">
Once the experiment was completed, values for a
range of evaluation measures were extracted from
the resulting data (dialogue recordings, system logs,
and web survey responses). Following PARADISE,
we organize our measures along four performance
dimensions, as shown in Figure 3.
To measure task success, we compared the sce-
nario key and scenario execution AVMs for each
dialogue, using the Kappa statistic (Walker et al.,
1997). For the scenario execution AVM, the values
for arrival-city, depart-city, depart-day, and depart-
range were extracted from system logs of ASR re-
</bodyText>
<listItem confidence="0.898315666666667">
• Task Success: Kappa, Completed
• Dialogue Quality: Help Requests, ASR Rejec-
tions, Timeouts, Mean Recognition, Barge Ins
• Dialogue Efficiency: System Turns, User Turns,
Elapsed Time
• User Satisfaction: User Satisfaction (based on
TTS Performance, ASR Performance, Task Ease,
Interaction Pace, User Expertise, System Response,
Expected Behavior, Future Use)
</listItem>
<figureCaption confidence="0.998813">
Figure 3: Measures used to evaluate TOOT.
</figureCaption>
<bodyText confidence="0.999961892857143">
sults. The exact-depart-time and total-travel-time
were extracted from the web survey. To measure
users&apos; perceptions of task success, the survey also
asked users whether they had successfully Com-
pleted the task.
To measure dialogue quality or naturalness, we
logged the dialogue manager&apos;s behavior on entering
and exiting each state in the finite state machine (re-
call Section 2). We then extracted the number of
prompts per dialogue due to Help Requests, ASR
Rejections, and Timeouts. Obtaining the values
for other quality measures required manual analysis.
We listened to the recordings and compared them to
the logged ASR results, to calculate concept accu-
racy (intuitively, semantic interpretation accuracy)
for each utterance. This was then used, in com-
bination with ASR rejections, to compute a Mean
Recognition score per dialogue. We also listened
to the recordings to determine how many times the
user interrupted the agent (Barge Ins).
To measure dialogue efficiency, the number of
System Turns and User Turns were extracted from
the dialogue manager log, and the total Elapsed
Time was determined from the recording.
To measure user satisfaction4, users responded to
the web survey in Figure 4, which assessed their
subjective evaluation of the agent&apos;s performance.
Each question was designed to measure a partic-
</bodyText>
<footnote confidence="0.911788">
4Questionnaire-based user satisfaction ratings (Shriberg et
al., 1992; Polifroni et al., 1992) have been frequently used in
the literature as an external indicator of agent usability.
</footnote>
<page confidence="0.996558">
783
</page>
<listItem confidence="0.980973157894737">
• Was the system easy to understand in this conver-
sation? (TTS Performance)
• In this conversation, did the system understand
what you said? (ASR Performance)
• In this conversation, was it easy to find the schedule
you wanted? (Task Ease)
• Was the pace of interaction with the system appro-
priate in this conversation? (Interaction Pace)
• In this conversation, did you know what you could
say at each point of the dialogue? (User Expertise)
• How often was the system sluggish and slow to
reply to you in this conversation? (System Re-
sponse)
• Did the system work the way you expected it to in
this conversation? (Expected Behavior)
• From your current experience with using our sys-
tem, do you think you&apos;d use this regularly to access
train schedules when you are away from your desk?
(Future Use)
</listItem>
<figureCaption confidence="0.9812044">
Figure 4: User satisfaction survey and associated
evaluation measures.
ular factor, e.g., System Response. Responses
ranged over n pre-defined values (e.g., almost never,
rarely, sometimes, often, almost always), which
</figureCaption>
<bodyText confidence="0.736396666666667">
were mapped to an integer in 1 n. Cumulative
User Satisfaction was computed by summing each
question&apos;s score.
</bodyText>
<sectionHeader confidence="0.941001" genericHeader="method">
5 Strategy and Task Differences
</sectionHeader>
<bodyText confidence="0.99091334375">
To test the hypotheses in Table 1 we use analysis
of variance (ANOVA) (Cohen, 1995) to determine
whether the values of any of the evaluation mea-
sures in Figure 3 significantly differ as a function
of response strategy and task scenario.
First, for each task scenario (4 sets of 12 dia-
logues, 6 per agent and 1 per user), we perform
an ANOVA for each evaluation measure as a func-
tion of response strategy. For Task 1, there are
no significant differences between the 6 LT and 6
CT dialogues for any evaluation measure, which is
consistent with Table 1. For Task 2, mean Com-
pleted (perceived task success rate) is 50% for LT
and 100% for CT (p &lt; .05). In addition, the aver-
age number of Help Requests per LT dialogue is
0, while for CT the average is 2.2 (p &lt; .05). Thus,
for Task 2, CT has a better perceived task success
rate than LT, despite the fact that users needed more
help to use CT. Only the perceived task success dif-
ference is consistent with the Task 2 prediction in
Table 1.5 For Task 3, there are no significant differ-
ences between LT and CT, which again matches our
predictions. Finally, for Task 4, mean Kappa (ac-
tual task success rate) is 100% for LT but only 65%
for CT (p &lt; .01).6 Like Task 2, this result suggests
that some type of task success measure is an impor-
tant predictor of agent performance. Surprisingly,
we found that LT and CT did not differ with respect
to any efficiency measure, in any task.7
Next, we combine all of our data (48 dialogues),
and perform a two-way ANOVA for each evaluation
measure as a function of strategy and task. An inter-
action between response strategy and task scenario
is significant for Future Use (p &lt; .03). For task 1,
the likelihood of Future Use is the same for LT and
CT; for task 2, the likelihood is higher for CT; for
tasks 3 and 4, the likelihood is higher for LT. Thus,
the results for tasks 1, 2, and 4, but not for Task 3,
are consistent with the predictions in Table 1. How-
ever, Task 3 was the most difficult task (see below),
and sometimes led to unexpected user behavior with
both agents. A strategy/task interaction is also sig-
nificant for Help Requests (p &lt; .02). For tasks 1
and 3, the number of requests is higher for LT; for
tasks 2 and 4, the number is higher for CT.
No evaluation measures significantly differ as a
function of response strategy, which is consistent
with Table 1. Since the task scenarios were con-
structed to yield comparable performance in Tasks
1 and 3, better CT performance in Task 2, and better
LT performance in Task 4, we expected that overall,
LT and CT performance would be comparable.
In contrast, many measures (User Satisfaction,
Elapsed Time, System Turns, User Turns, ASR
Performance, and Task Ease) differ as a function
of task scenario (p &lt; .03), confirming that our tasks
vary with respect to difficulty. Our results suggest
that the ordering of the tasks from easiest to most
difficult is 1, 4, 2, and 3,8 which is consistent with
our predictions. Recall that for Task 1, the initial
query was designed to yield the correct train for
both LT and CT. For tasks 4 and 2, the initial query
was designed to yield the correct train for only one
agent, and to require a follow-up query for the other.
</bodyText>
<footnote confidence="0.994643625">
5 However, the analysis in Section 6 suggests that Help Re-
quests is not a good predictor of performance.
6In our data, actual task success implies perceived task suc-
cess, but not vice-versa.
7However, our &amp;quot;difficult&amp;quot; tasks were not that difficult (we
wanted to minimize subjects&apos; time commitment).
8This ordering is observed for all the listed measures except
User Turns, which reverses tasks 4 and 1.
</footnote>
<page confidence="0.996654">
784
</page>
<bodyText confidence="0.9953255">
For Task 3, the initial query was designed to require
a follow-up query for both agents.
</bodyText>
<sectionHeader confidence="0.974691" genericHeader="method">
6 Performance Function Estimation
</sectionHeader>
<bodyText confidence="0.999749">
While hypothesis testing tells us how each evalua-
tion measure differs as a function of strategy and/or
task, it does not tell us how to tradeoff or com-
bine results from multiple measures. Understand-
ing such tradeoffs is especially important when dif-
ferent measures yield different performance predic-
tions (e.g., recall the Task 2 hypothesis testing re-
sults for Completed and Help Requests).
</bodyText>
<figure confidence="0.904048333333333">
I MAXIMIZE USER SATISFACTIONI
MAXIMIZE TASK
SUCCESS
</figure>
<figureCaption confidence="0.926408">
Figure 5: PARADISE&apos; s structure of objectives for
spoken dialogue performance.
</figureCaption>
<bodyText confidence="0.8827842">
• To assess the relative contribution of each eval-
uation measure to performance, we use PAR-
ADISE (Walker et al., 1997) to derive a perfor-
mance function from our data. PARADISE draws
on ideas in multi-attribute decision theory (Keeney
and Raiffa, 1976) to posit the model shown in Fig-
ure 5, then uses multivariate linear regression to es-
timate a quantitative performance function based on
this model. Linear regression produces coefficients
describing the relative contribution of predictor fac-
tors in accounting for the variance in a predicted fac-
tor. In PARADISE, the success and cost measures
are predictors, while user satisfaction is predicted.
Figure 3 showed how the measures used to evaluate
TOOT instantiate the PARADISE model.
The application of PARADISE to the TOOT data
shows that the only significant contributors to User
Satisfaction are Completed (Comp), Mean Recog-
nition (MR) and Barge Ins (BI), and yields the fol-
lowing performance function:
</bodyText>
<equation confidence="0.878763">
Perf = .45N(Comp) .35N(MR) — .42Ar(B/)
</equation>
<bodyText confidence="0.994374">
Completed is significant at p &lt; .0002, Mean
Recognition9 at p &lt; .003, and Barge Ins at p &lt;
.0004; these account for 47% of the variance in User
Satisfaction. Al is a Z score normalization func-
tion (Cohen, 1995) and guarantees that the coeffi-
</bodyText>
<footnote confidence="0.5472035">
9Since we measure recognition rather than misrecognition,
this &amp;quot;cost&amp;quot; factor has a positive coefficient.
</footnote>
<bodyText confidence="0.999920470588235">
cients directly indicate the relative contribution of
each factor to performance.
Our performance function demonstrates that
TOOT performance involves task success and di-
alogue quality factors. Analysis of variance sug-
gested that task success was a likely performance
factor. PARADISE confirms this hypothesis, and
demonstrates that perceived rather than actual task
success is the useful predictor. While 39 dialogues
were perceived to have been successful, only 27
were actually successful.
Results that were not apparent from the analysis
of variance are that Mean Recognition and Barge
Ins are also predictors of performance. The mean
recognition for our corpus is 85%. Apparently,
users of both LT and CT are bothered by dialogue
phenomena associated with poor recognition. For
example, system misunderstandings (which result
from ASR misrecognitions) and system requests to
repeat what users have said (which result from ASR
rejections) both make dialogues seem less natural.
While barge-in is usually considered an advanced
(and desirable) ASR capability, our performance
function suggests that in TOOT, allowing users to
interrupt actually degrades performance. Examina-
tion of our transcripts shows that users sometimes
use barge-in to shorten TOOT&apos; s prompts. This often
circumvents TOOT&apos; s confirmation strategy, which
incorporates speech recognition results into prompts
to make the user aware of misrecognitions.
Surprisingly, no efficiency measures are signif-
icant predictors of performance. This draws into
question the frequently made assumption that ef-
ficiency is one of the most important measures of
system performance, and instead suggests that users
are more attuned to both task success and qualitative
aspects of the dialogue, or that efficiency is highly
correlated with some of these factors.
However, analysis of subsets of our data suggests
that efficiency measures can become important per-
formance predictors when the more primary effects
are factored out. For example, when a regression
is performed on the 11 TOOT dialogues with per-
fect Mean Recognition, the significant contribu-
tors to performance become Completed (p &lt; .05),
Elapsed time (p &lt; .04), User Turns (p &lt; .03) and
Barge Ins (p &lt; 0.0007) (accounting for 87% of the
variance). Thus, in the presence of perfect ASR,
efficiency becomes important. When a regression
is performed using the 39 dialogues where users
thought they had successfully completed the task
</bodyText>
<sectionHeader confidence="0.733562333333333" genericHeader="method">
MINIMIZE COSTS
EFFICIENCY QUALITATIVE
MEASURES MEASURES
</sectionHeader>
<page confidence="0.996073">
785
</page>
<bodyText confidence="0.9997328">
(perfect Completed), the significant factors become
Elapsed time (p &lt; .002), Timeouts (p &lt; .002), and
Barge Ins (p &lt; .02) (58% of the variance).
Applying the performance function to each of our
48 dialogues yields a performance estimate for each
dialogue. Analysis with these estimates shows no
significant differences for mean LT and CT perfor-
mance. This result is consistent with the ANOVA
result, where only one of the three (comparably
weighted) factors in the performance function de-
pends on response strategy (Completed). Note that
for Tasks 2 and 4, the predictions in Table 1 do not
hold for overall performance, despite the ANOVA
results that the predictions do hold for some evalua-
tion measures (e.g., Completed in Task 2).
</bodyText>
<sectionHeader confidence="0.999046" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999978911764706">
We have presented an empirical comparison of lit-
eral and cooperative query response strategies in
TOOT, illustrating the advantages of combining hy-
pothesis testing and PARADISE. By using hypoth-
esis testing to examine how a set of evaluation mea-
sures differ as a function of response strategy and
task, we show that TOOT&apos; s cooperative and literal
responses can both lead to greater task success, like-
lihood of future use, and user need for help, de-
pending on task. By using PARADISE to derive a
performance function, we show that a combination
of strategy-dependent (perceived task success) and
strategy-independent (number of barge-ins, mean
recognition score) evaluation measures best predicts
overall TOOT performance. Our results elaborate
the conditions under which TOOT&apos; s response strate-
gies lead to greater performance, and allow us to
make predictions. For example, our performance
equation predicts that improving mean recognition
and/or judiciously restricting the use of barge-in
will enhance performance. Our current research is
aimed at automatically adapting dialogue behavior
in TOOT, to increase mean recognition and thus
overall agent performance (Walker et al., 1998).
Future work utilizing PARADISE will attempt to
generalize our results, to make a more predictive
model of agent performance. Performance function
estimation needs to be done iteratively over different
tasks and dialogue strategies. We plan to evaluate
additional cooperative response strategies in TOOT
(e.g., intensional summaries (Kalita et al., 1986),
summarization and constraint elicitation in isola-
tion), and to combine TOOT data with data from
other agents (Walker et al., 1998).
</bodyText>
<sectionHeader confidence="0.997703" genericHeader="acknowledgments">
8 Acknowledgments
</sectionHeader>
<bodyText confidence="0.914904">
Thanks to J. Chu-Carroll, T. Dasu, W. DuMouchel,
J. Fromer, D. Hindle, J. Hirschberg, C. Kamm, J.
Kang, A. Levy, C. Nakatani, S. Whittaker and J.
Wilpon for help with this research and/or paper.
</bodyText>
<sectionHeader confidence="0.979209" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99957658490566">
J. Allen and C. Perrault. 1980. Analyzing intention in utter-
ances. Artificial Intelligence, 15.
P. Cohen. 1995. Empirical Methods for Artificial Intelligence.
MIT Press, Boston.
M. Danieli and E. Gerbino. 1995. Metrics for evaluating dia-
logue strategies in a spoken language system. In Proc. AAAI
Spring Symposium on Empirical Methods in Discourse In-
terpretation and Generation.
D. Goddeau, H. Meng, J. Polifroni, S. Seneff, and
S. Busayapongchai. 1996. A form-based dialogue manager
for spoken language applications. In Proc. ICSLP.
A. Joshi, B. Webber, and R. Weischedel. 1984. Preventing
false inferences. In Proc. COLING.
J. Kalita, M. Jones, and G. McCalla. 1986. Summarizing nat-
ural language database responses. Computational Linguis-
tics, 12(2).
C. Kamm, S. Narayanan, D. Dutton, and R. Ritenour. 1997.
Evaluating spoken dialog systems for telecommunication
services. In Proc. EUROSPEECH.
S. Kaplan. 1981. Appropriate responses to inappropnate ques-
tions. In A. Joshi, B. Webber, and I. Sag, editors, Elements
of Discourse Understanding. Cambridge University Press.
R. Keeney and H. Raiffa. 1976. Decisions with Multiple Ob-
jectives: Preferences and Value Tradeoffs. Wiley.
E. Mays. 1980. Failures in natural language systems: Applica-
tions to data base query systems. In Proc. AAAI.
K. McCoy. 1989. Generating context-sensitive responses to
object related misconceptions. Artificial Intelligence, 41(2).
J. Moore. 1994. Participating in Explanatory Dialogues. MIT
Press.
C. Pao and J. Wilpon. 1992. Spontaneous speech collection
for the ATIS domain with an aural user feedback paradigm.
Technical report, AT&amp;T
R. Pieraccini, E. Levin, and W. Eckert. 1997. AMICA: The
AT&amp;T mixed initiative conversational architecture. In Proc.
EUROSPEECH.
J. Polifroni, L. Hirschman, S. Seneff, and V. Zue. 1992. Exper-
iments in evaluating interactive spoken language systems.
In Proc. DARPA Speech and NL Workshop.
S. Seneff, V. Zue, J. Polifroni, C. Pao, L. Hetherington, D. God-
deau, and J. Glass. 1995. The preliminary development of a
displayless PEGASUS system. In Proc. ARPA Spoken Lan-
guage Technology Workshop.
E. Shriberg, E. Wade, and P. Price. 1992. Human-machine
problem solving using spoken language systems (SLS): Fac-
tors affecting performance and user satisfaction. In Proc.
DARPA Speech and NL Workshop.
M. Walker, D. Litman, C. Kamm, and A. Abella. 1997. PAR-
ADISE: A general framework for evaluating spoken dia-
logue agents. In Proc. ACL/EACL.
M. Walker, D. Litman, C. Kamm, and A. Abella. 1998. Eval-
uating spoken dialogue agents with PARADISE: Two case
studies. Computer Speech and Language.
</reference>
<page confidence="0.998388">
786
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.974126">
<title confidence="0.999468">Evaluating Response Strategies in a Web-Based Spoken Dialogue Agent</title>
<author confidence="0.999997">Diane J Litman</author>
<affiliation confidence="0.99943">AT&amp;T Labs - Research</affiliation>
<address confidence="0.9989075">180 Park Avenue Florham Park, NJ 07932 USA</address>
<email confidence="0.999784">diane@research.att.com</email>
<author confidence="0.999705">Shimei Pan</author>
<affiliation confidence="0.999934">Computer Science Department Columbia University</affiliation>
<address confidence="0.999067">New York, NY 10027 USA</address>
<email confidence="0.998892">pan@cs.columbia.edu</email>
<author confidence="0.999971">Marilyn A Walker</author>
<affiliation confidence="0.99775">AT&amp;T Labs - Research</affiliation>
<address confidence="0.9986755">180 Park Avenue Florham Park, NJ 07932 USA</address>
<email confidence="0.999912">walker@research.att.com</email>
<abstract confidence="0.999288869565217">While the notion of a cooperative response has been the focus of considerable research in natural language dialogue systems, there has been little empirical work demonstrating how such responses lead to more efficient, natural, or successful dialogues. This paper presents an experimental evaluation of two alternative response strategies in TOOT, a spoken dialogue agent that allows users to access train schedules stored on the web via a telephone conversation. We compare the performance of two versions of TOOT (literal and cooperative), by having users carry out a set of tasks with each version. By using hypothesis testing methods, we show that a combination of response strategy, application task, and task/strategy interactions account for various types of performance differences. By using the PARADISE evaluation framework to estimate an overall performance function, we identify interdependencies that exist between speech recognition and response strategy. Our results elaborate the conditions under which TOOT&apos; s cooperative rather than literal strategy contributes to greater performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Allen</author>
<author>C Perrault</author>
</authors>
<title>Analyzing intention in utterances.</title>
<date>1980</date>
<journal>Artificial Intelligence,</journal>
<volume>15</volume>
<contexts>
<context position="1661" citStr="Allen and Perrault, 1980" startWordPosition="246" endWordPosition="249">at a combination of response strategy, application task, and task/strategy interactions account for various types of performance differences. By using the PARADISE evaluation framework to estimate an overall performance function, we identify interdependencies that exist between speech recognition and response strategy. Our results elaborate the conditions under which TOOT&apos; s cooperative rather than literal strategy contributes to greater performance. 1 Introduction The notion of a cooperative response has been the focus of considerable research in natural language and spoken dialogue systems (Allen and Perrault, 1980; Mays, 1980; Kaplan, 1981; Joshi et al., 1984; McCoy, 1989; Pao and Wilpon, 1992; Moore, 1994; Seneff et al., 1995; Goddeau et al., 1996; Pieraccini et al., 1997). However, despite the existence of many algorithms for generating cooperative responses, there has been little empirical work addressing the evaluation of such algorithms in the context of real-time natural language dialogue systems with human users. Thus it is unclear under what conditions cooperative responses result in more efficient or efficacious dialogues. This paper presents an empirical evaluation of two alternative algorith</context>
</contexts>
<marker>Allen, Perrault, 1980</marker>
<rawString>J. Allen and C. Perrault. 1980. Analyzing intention in utterances. Artificial Intelligence, 15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Cohen</author>
</authors>
<title>Empirical Methods for Artificial Intelligence.</title>
<date>1995</date>
<publisher>MIT Press,</publisher>
<location>Boston.</location>
<contexts>
<context position="18999" citStr="Cohen, 1995" startWordPosition="3202" endWordPosition="3203">r) • From your current experience with using our system, do you think you&apos;d use this regularly to access train schedules when you are away from your desk? (Future Use) Figure 4: User satisfaction survey and associated evaluation measures. ular factor, e.g., System Response. Responses ranged over n pre-defined values (e.g., almost never, rarely, sometimes, often, almost always), which were mapped to an integer in 1 n. Cumulative User Satisfaction was computed by summing each question&apos;s score. 5 Strategy and Task Differences To test the hypotheses in Table 1 we use analysis of variance (ANOVA) (Cohen, 1995) to determine whether the values of any of the evaluation measures in Figure 3 significantly differ as a function of response strategy and task scenario. First, for each task scenario (4 sets of 12 dialogues, 6 per agent and 1 per user), we perform an ANOVA for each evaluation measure as a function of response strategy. For Task 1, there are no significant differences between the 6 LT and 6 CT dialogues for any evaluation measure, which is consistent with Table 1. For Task 2, mean Completed (perceived task success rate) is 50% for LT and 100% for CT (p &lt; .05). In addition, the average number o</context>
<context position="24385" citStr="Cohen, 1995" startWordPosition="4145" endWordPosition="4146">are predictors, while user satisfaction is predicted. Figure 3 showed how the measures used to evaluate TOOT instantiate the PARADISE model. The application of PARADISE to the TOOT data shows that the only significant contributors to User Satisfaction are Completed (Comp), Mean Recognition (MR) and Barge Ins (BI), and yields the following performance function: Perf = .45N(Comp) .35N(MR) — .42Ar(B/) Completed is significant at p &lt; .0002, Mean Recognition9 at p &lt; .003, and Barge Ins at p &lt; .0004; these account for 47% of the variance in User Satisfaction. Al is a Z score normalization function (Cohen, 1995) and guarantees that the coeffi9Since we measure recognition rather than misrecognition, this &amp;quot;cost&amp;quot; factor has a positive coefficient. cients directly indicate the relative contribution of each factor to performance. Our performance function demonstrates that TOOT performance involves task success and dialogue quality factors. Analysis of variance suggested that task success was a likely performance factor. PARADISE confirms this hypothesis, and demonstrates that perceived rather than actual task success is the useful predictor. While 39 dialogues were perceived to have been successful, only </context>
</contexts>
<marker>Cohen, 1995</marker>
<rawString>P. Cohen. 1995. Empirical Methods for Artificial Intelligence. MIT Press, Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Danieli</author>
<author>E Gerbino</author>
</authors>
<title>Metrics for evaluating dialogue strategies in a spoken language system.</title>
<date>1995</date>
<booktitle>In Proc. AAAI Spring Symposium on Empirical Methods in Discourse Interpretation and Generation.</booktitle>
<contexts>
<context position="4000" citStr="Danieli and Gerbino, 1995" startWordPosition="612" endWordPosition="615">ve response strategy leads to greater agent performance. 2 TOOT TOOT allows users to access online AMTRAK train schedules via a telephone dialogue, as in Figure 1.1 (All examples are from the experiment in Section 3.) We have built two versions of TOOT: literal TOOT (LT) and cooperative TOOT (CT). LT and CT have equivalent functionality, but use different response strategies to present tabular results of web queries in a displayless environment.2 LT and CT incorporate many of the types of database responses in the &apos;Our domain was chosen to afford future comparison with similar systems, e.g., (Danieli and Gerbino, 1995). 2Initiative (Walker et al., 1998) and confirmation (Danieli and Gerbino, 1995) strategies are equivalent in LT and CT, and were chosen to reduce speech recognition errors. 780 T: Hi, this is the Amtrak train schedule system. Which city do you want to go to? U: Philadelphia. T: What city do you want to go to Philadelphia from? U: New York City. T: On what day of the week do you want to go to Philadelphia from New York City? U: On the weekend. T: What time do you want to leave on the weekend? U: 4 o&apos;clock pm. T: Do you want me to find the trains going to Philadelphia from New York City on the </context>
</contexts>
<marker>Danieli, Gerbino, 1995</marker>
<rawString>M. Danieli and E. Gerbino. 1995. Metrics for evaluating dialogue strategies in a spoken language system. In Proc. AAAI Spring Symposium on Empirical Methods in Discourse Interpretation and Generation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Goddeau</author>
<author>H Meng</author>
<author>J Polifroni</author>
<author>S Seneff</author>
<author>S Busayapongchai</author>
</authors>
<title>A form-based dialogue manager for spoken language applications. In</title>
<date>1996</date>
<booktitle>Proc. ICSLP.</booktitle>
<contexts>
<context position="1798" citStr="Goddeau et al., 1996" startWordPosition="270" endWordPosition="273"> By using the PARADISE evaluation framework to estimate an overall performance function, we identify interdependencies that exist between speech recognition and response strategy. Our results elaborate the conditions under which TOOT&apos; s cooperative rather than literal strategy contributes to greater performance. 1 Introduction The notion of a cooperative response has been the focus of considerable research in natural language and spoken dialogue systems (Allen and Perrault, 1980; Mays, 1980; Kaplan, 1981; Joshi et al., 1984; McCoy, 1989; Pao and Wilpon, 1992; Moore, 1994; Seneff et al., 1995; Goddeau et al., 1996; Pieraccini et al., 1997). However, despite the existence of many algorithms for generating cooperative responses, there has been little empirical work addressing the evaluation of such algorithms in the context of real-time natural language dialogue systems with human users. Thus it is unclear under what conditions cooperative responses result in more efficient or efficacious dialogues. This paper presents an empirical evaluation of two alternative algorithms for responding to database queries in TOOT, a spoken dialogue agent for accessing online train schedules via a telephone conversation.</context>
<context position="6872" citStr="Goddeau et al., 1996" startWordPosition="1143" endWordPosition="1146">our departure time or travel da); or say &amp;quot;continue&amp;quot; if my answer was sufficient, or say &amp;quot;repeat&amp;quot; to hear this message again. LT&apos;s response incrementally presents the set of trains that match the query, until the user tells LT to stop. Enumerating large lists, even incrementally, can lead to information overload. CT&apos;s response is more cooperative because it better respects the resource limitations of the listener. CT presents a subset of the matching trains using a summary response (Pao and Wilpon, 1992), followed by an option to reduce the information to be retrieved (Pieraccini et al., 1997; Goddeau et al., 1996; Seneff et al., 1995; Pao and Wilpon, 1992). If there is no information that matches a query, LT reports only the lack of an answer to the query, as in the following dialogue excerpt: (2) There are no trains going to Chicago from Philadelphia on Sunday around 10:30 am. Do you want to continue and find a new set of trains? CT automatically relaxes the user&apos;s time constraint and allows the user to perform other relaxations: (3) There are no trains going to Chicago from Philadelphia on Sunday around 10:30 am. The closest earlier train leaves at 9:28 am every day, and it takes 1 day 3 hours 36 mi</context>
</contexts>
<marker>Goddeau, Meng, Polifroni, Seneff, Busayapongchai, 1996</marker>
<rawString>D. Goddeau, H. Meng, J. Polifroni, S. Seneff, and S. Busayapongchai. 1996. A form-based dialogue manager for spoken language applications. In Proc. ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Joshi</author>
<author>B Webber</author>
<author>R Weischedel</author>
</authors>
<title>Preventing false inferences.</title>
<date>1984</date>
<booktitle>In Proc. COLING.</booktitle>
<contexts>
<context position="1707" citStr="Joshi et al., 1984" startWordPosition="254" endWordPosition="257">ask, and task/strategy interactions account for various types of performance differences. By using the PARADISE evaluation framework to estimate an overall performance function, we identify interdependencies that exist between speech recognition and response strategy. Our results elaborate the conditions under which TOOT&apos; s cooperative rather than literal strategy contributes to greater performance. 1 Introduction The notion of a cooperative response has been the focus of considerable research in natural language and spoken dialogue systems (Allen and Perrault, 1980; Mays, 1980; Kaplan, 1981; Joshi et al., 1984; McCoy, 1989; Pao and Wilpon, 1992; Moore, 1994; Seneff et al., 1995; Goddeau et al., 1996; Pieraccini et al., 1997). However, despite the existence of many algorithms for generating cooperative responses, there has been little empirical work addressing the evaluation of such algorithms in the context of real-time natural language dialogue systems with human users. Thus it is unclear under what conditions cooperative responses result in more efficient or efficacious dialogues. This paper presents an empirical evaluation of two alternative algorithms for responding to database queries in TOOT,</context>
<context position="7917" citStr="Joshi et al., 1984" startWordPosition="1333" endWordPosition="1336">(3) There are no trains going to Chicago from Philadelphia on Sunday around 10:30 am. The closest earlier train leaves at 9:28 am every day, and it takes 1 day 3 hours 36 mins. The closest later train leaves at 11:45 am on Saturday and Sunday, and it takes 22 hours 5 mins. Please say &amp;quot;relax&amp;quot; to change your departure time or travel da); or say &amp;quot;continue&amp;quot; if my answer was sufficient, or say &amp;quot;repeat&amp;quot; to hear this message again. CT&apos;s response is more cooperative since identifying the source of a query failure can help block incorrect user inferences (Pieraccini et al., 1997; Pao and Wilpon, 1992; Joshi et al., 1984; Kaplan, 1981; Mays, 1980). LT&apos;s response could lead the user to believe that there are no trains on Sunday. When there are 1-3 trains that match a query, both LT and CT list the trains: (4) There are 2 trains available around 6 pm. The first train leaves at 6:05 pm every day, and it takes 5 hours 10 mins. The second train leaves at 6:30 pm every day, and it takes 2 days 11 hours 30 mins. Do you want to continue and find a new set of trains? TOOT is implemented using a platform for spoken dialogue agents (Kamm et al., 1997) that combines automatic speech recognition (ASR), textto-speech (TTS)</context>
</contexts>
<marker>Joshi, Webber, Weischedel, 1984</marker>
<rawString>A. Joshi, B. Webber, and R. Weischedel. 1984. Preventing false inferences. In Proc. COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kalita</author>
<author>M Jones</author>
<author>G McCalla</author>
</authors>
<title>Summarizing natural language database responses.</title>
<date>1986</date>
<journal>Computational Linguistics,</journal>
<volume>12</volume>
<issue>2</issue>
<marker>Kalita, Jones, McCalla, 1986</marker>
<rawString>J. Kalita, M. Jones, and G. McCalla. 1986. Summarizing natural language database responses. Computational Linguistics, 12(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Kamm</author>
<author>S Narayanan</author>
<author>D Dutton</author>
<author>R Ritenour</author>
</authors>
<title>Evaluating spoken dialog systems for telecommunication services. In</title>
<date>1997</date>
<booktitle>Proc. EUROSPEECH.</booktitle>
<contexts>
<context position="8447" citStr="Kamm et al., 1997" startWordPosition="1438" endWordPosition="1441">rrect user inferences (Pieraccini et al., 1997; Pao and Wilpon, 1992; Joshi et al., 1984; Kaplan, 1981; Mays, 1980). LT&apos;s response could lead the user to believe that there are no trains on Sunday. When there are 1-3 trains that match a query, both LT and CT list the trains: (4) There are 2 trains available around 6 pm. The first train leaves at 6:05 pm every day, and it takes 5 hours 10 mins. The second train leaves at 6:30 pm every day, and it takes 2 days 11 hours 30 mins. Do you want to continue and find a new set of trains? TOOT is implemented using a platform for spoken dialogue agents (Kamm et al., 1997) that combines automatic speech recognition (ASR), textto-speech (TTS), a phone interface, and modules for specifying a dialogue manager and application functions. ASR in our platform supports barge-in, an advanced functionality which allows users to interrupt an agent when it is speaking. 781 The dialogue manager uses a finite state machine to implement dialogue strategies. Each state specifies 1) an initial prompt (or response) which the agent says upon entering the state (such prompts often elicit parameter values); 2) a help prompt which the agent says if the user says help; 3) rejection p</context>
</contexts>
<marker>Kamm, Narayanan, Dutton, Ritenour, 1997</marker>
<rawString>C. Kamm, S. Narayanan, D. Dutton, and R. Ritenour. 1997. Evaluating spoken dialog systems for telecommunication services. In Proc. EUROSPEECH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kaplan</author>
</authors>
<title>Appropriate responses to inappropnate questions.</title>
<date>1981</date>
<booktitle>Elements of Discourse Understanding.</booktitle>
<editor>In A. Joshi, B. Webber, and I. Sag, editors,</editor>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="1687" citStr="Kaplan, 1981" startWordPosition="252" endWordPosition="253"> application task, and task/strategy interactions account for various types of performance differences. By using the PARADISE evaluation framework to estimate an overall performance function, we identify interdependencies that exist between speech recognition and response strategy. Our results elaborate the conditions under which TOOT&apos; s cooperative rather than literal strategy contributes to greater performance. 1 Introduction The notion of a cooperative response has been the focus of considerable research in natural language and spoken dialogue systems (Allen and Perrault, 1980; Mays, 1980; Kaplan, 1981; Joshi et al., 1984; McCoy, 1989; Pao and Wilpon, 1992; Moore, 1994; Seneff et al., 1995; Goddeau et al., 1996; Pieraccini et al., 1997). However, despite the existence of many algorithms for generating cooperative responses, there has been little empirical work addressing the evaluation of such algorithms in the context of real-time natural language dialogue systems with human users. Thus it is unclear under what conditions cooperative responses result in more efficient or efficacious dialogues. This paper presents an empirical evaluation of two alternative algorithms for responding to datab</context>
<context position="7931" citStr="Kaplan, 1981" startWordPosition="1337" endWordPosition="1338">ins going to Chicago from Philadelphia on Sunday around 10:30 am. The closest earlier train leaves at 9:28 am every day, and it takes 1 day 3 hours 36 mins. The closest later train leaves at 11:45 am on Saturday and Sunday, and it takes 22 hours 5 mins. Please say &amp;quot;relax&amp;quot; to change your departure time or travel da); or say &amp;quot;continue&amp;quot; if my answer was sufficient, or say &amp;quot;repeat&amp;quot; to hear this message again. CT&apos;s response is more cooperative since identifying the source of a query failure can help block incorrect user inferences (Pieraccini et al., 1997; Pao and Wilpon, 1992; Joshi et al., 1984; Kaplan, 1981; Mays, 1980). LT&apos;s response could lead the user to believe that there are no trains on Sunday. When there are 1-3 trains that match a query, both LT and CT list the trains: (4) There are 2 trains available around 6 pm. The first train leaves at 6:05 pm every day, and it takes 5 hours 10 mins. The second train leaves at 6:30 pm every day, and it takes 2 days 11 hours 30 mins. Do you want to continue and find a new set of trains? TOOT is implemented using a platform for spoken dialogue agents (Kamm et al., 1997) that combines automatic speech recognition (ASR), textto-speech (TTS), a phone inte</context>
</contexts>
<marker>Kaplan, 1981</marker>
<rawString>S. Kaplan. 1981. Appropriate responses to inappropnate questions. In A. Joshi, B. Webber, and I. Sag, editors, Elements of Discourse Understanding. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Keeney</author>
<author>H Raiffa</author>
</authors>
<title>Decisions with Multiple Objectives: Preferences and Value Tradeoffs.</title>
<date>1976</date>
<publisher>Wiley.</publisher>
<contexts>
<context position="23429" citStr="Keeney and Raiffa, 1976" startWordPosition="3987" endWordPosition="3990">ombine results from multiple measures. Understanding such tradeoffs is especially important when different measures yield different performance predictions (e.g., recall the Task 2 hypothesis testing results for Completed and Help Requests). I MAXIMIZE USER SATISFACTIONI MAXIMIZE TASK SUCCESS Figure 5: PARADISE&apos; s structure of objectives for spoken dialogue performance. • To assess the relative contribution of each evaluation measure to performance, we use PARADISE (Walker et al., 1997) to derive a performance function from our data. PARADISE draws on ideas in multi-attribute decision theory (Keeney and Raiffa, 1976) to posit the model shown in Figure 5, then uses multivariate linear regression to estimate a quantitative performance function based on this model. Linear regression produces coefficients describing the relative contribution of predictor factors in accounting for the variance in a predicted factor. In PARADISE, the success and cost measures are predictors, while user satisfaction is predicted. Figure 3 showed how the measures used to evaluate TOOT instantiate the PARADISE model. The application of PARADISE to the TOOT data shows that the only significant contributors to User Satisfaction are </context>
</contexts>
<marker>Keeney, Raiffa, 1976</marker>
<rawString>R. Keeney and H. Raiffa. 1976. Decisions with Multiple Objectives: Preferences and Value Tradeoffs. Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Mays</author>
</authors>
<title>Failures in natural language systems: Applications to data base query systems.</title>
<date>1980</date>
<booktitle>In Proc. AAAI.</booktitle>
<contexts>
<context position="1673" citStr="Mays, 1980" startWordPosition="250" endWordPosition="251">se strategy, application task, and task/strategy interactions account for various types of performance differences. By using the PARADISE evaluation framework to estimate an overall performance function, we identify interdependencies that exist between speech recognition and response strategy. Our results elaborate the conditions under which TOOT&apos; s cooperative rather than literal strategy contributes to greater performance. 1 Introduction The notion of a cooperative response has been the focus of considerable research in natural language and spoken dialogue systems (Allen and Perrault, 1980; Mays, 1980; Kaplan, 1981; Joshi et al., 1984; McCoy, 1989; Pao and Wilpon, 1992; Moore, 1994; Seneff et al., 1995; Goddeau et al., 1996; Pieraccini et al., 1997). However, despite the existence of many algorithms for generating cooperative responses, there has been little empirical work addressing the evaluation of such algorithms in the context of real-time natural language dialogue systems with human users. Thus it is unclear under what conditions cooperative responses result in more efficient or efficacious dialogues. This paper presents an empirical evaluation of two alternative algorithms for respo</context>
<context position="7944" citStr="Mays, 1980" startWordPosition="1339" endWordPosition="1340">hicago from Philadelphia on Sunday around 10:30 am. The closest earlier train leaves at 9:28 am every day, and it takes 1 day 3 hours 36 mins. The closest later train leaves at 11:45 am on Saturday and Sunday, and it takes 22 hours 5 mins. Please say &amp;quot;relax&amp;quot; to change your departure time or travel da); or say &amp;quot;continue&amp;quot; if my answer was sufficient, or say &amp;quot;repeat&amp;quot; to hear this message again. CT&apos;s response is more cooperative since identifying the source of a query failure can help block incorrect user inferences (Pieraccini et al., 1997; Pao and Wilpon, 1992; Joshi et al., 1984; Kaplan, 1981; Mays, 1980). LT&apos;s response could lead the user to believe that there are no trains on Sunday. When there are 1-3 trains that match a query, both LT and CT list the trains: (4) There are 2 trains available around 6 pm. The first train leaves at 6:05 pm every day, and it takes 5 hours 10 mins. The second train leaves at 6:30 pm every day, and it takes 2 days 11 hours 30 mins. Do you want to continue and find a new set of trains? TOOT is implemented using a platform for spoken dialogue agents (Kamm et al., 1997) that combines automatic speech recognition (ASR), textto-speech (TTS), a phone interface, and mo</context>
</contexts>
<marker>Mays, 1980</marker>
<rawString>E. Mays. 1980. Failures in natural language systems: Applications to data base query systems. In Proc. AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K McCoy</author>
</authors>
<title>Generating context-sensitive responses to object related misconceptions.</title>
<date>1989</date>
<journal>Artificial Intelligence,</journal>
<volume>41</volume>
<issue>2</issue>
<publisher>MIT Press.</publisher>
<contexts>
<context position="1720" citStr="McCoy, 1989" startWordPosition="258" endWordPosition="259">gy interactions account for various types of performance differences. By using the PARADISE evaluation framework to estimate an overall performance function, we identify interdependencies that exist between speech recognition and response strategy. Our results elaborate the conditions under which TOOT&apos; s cooperative rather than literal strategy contributes to greater performance. 1 Introduction The notion of a cooperative response has been the focus of considerable research in natural language and spoken dialogue systems (Allen and Perrault, 1980; Mays, 1980; Kaplan, 1981; Joshi et al., 1984; McCoy, 1989; Pao and Wilpon, 1992; Moore, 1994; Seneff et al., 1995; Goddeau et al., 1996; Pieraccini et al., 1997). However, despite the existence of many algorithms for generating cooperative responses, there has been little empirical work addressing the evaluation of such algorithms in the context of real-time natural language dialogue systems with human users. Thus it is unclear under what conditions cooperative responses result in more efficient or efficacious dialogues. This paper presents an empirical evaluation of two alternative algorithms for responding to database queries in TOOT, a spoken dia</context>
</contexts>
<marker>McCoy, 1989</marker>
<rawString>K. McCoy. 1989. Generating context-sensitive responses to object related misconceptions. Artificial Intelligence, 41(2). J. Moore. 1994. Participating in Explanatory Dialogues. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Pao</author>
<author>J Wilpon</author>
</authors>
<title>Spontaneous speech collection for the ATIS domain with an aural user feedback paradigm.</title>
<date>1992</date>
<tech>Technical report, AT&amp;T</tech>
<contexts>
<context position="1742" citStr="Pao and Wilpon, 1992" startWordPosition="260" endWordPosition="263">ns account for various types of performance differences. By using the PARADISE evaluation framework to estimate an overall performance function, we identify interdependencies that exist between speech recognition and response strategy. Our results elaborate the conditions under which TOOT&apos; s cooperative rather than literal strategy contributes to greater performance. 1 Introduction The notion of a cooperative response has been the focus of considerable research in natural language and spoken dialogue systems (Allen and Perrault, 1980; Mays, 1980; Kaplan, 1981; Joshi et al., 1984; McCoy, 1989; Pao and Wilpon, 1992; Moore, 1994; Seneff et al., 1995; Goddeau et al., 1996; Pieraccini et al., 1997). However, despite the existence of many algorithms for generating cooperative responses, there has been little empirical work addressing the evaluation of such algorithms in the context of real-time natural language dialogue systems with human users. Thus it is unclear under what conditions cooperative responses result in more efficient or efficacious dialogues. This paper presents an empirical evaluation of two alternative algorithms for responding to database queries in TOOT, a spoken dialogue agent for access</context>
<context position="6760" citStr="Pao and Wilpon, 1992" startWordPosition="1122" endWordPosition="1125">and it takes 1 hour 12 mins. Please say &amp;quot;list&amp;quot; to hear trains 3 at a time, or say &amp;quot;add constraint&amp;quot; to constrain your departure time or travel da); or say &amp;quot;continue&amp;quot; if my answer was sufficient, or say &amp;quot;repeat&amp;quot; to hear this message again. LT&apos;s response incrementally presents the set of trains that match the query, until the user tells LT to stop. Enumerating large lists, even incrementally, can lead to information overload. CT&apos;s response is more cooperative because it better respects the resource limitations of the listener. CT presents a subset of the matching trains using a summary response (Pao and Wilpon, 1992), followed by an option to reduce the information to be retrieved (Pieraccini et al., 1997; Goddeau et al., 1996; Seneff et al., 1995; Pao and Wilpon, 1992). If there is no information that matches a query, LT reports only the lack of an answer to the query, as in the following dialogue excerpt: (2) There are no trains going to Chicago from Philadelphia on Sunday around 10:30 am. Do you want to continue and find a new set of trains? CT automatically relaxes the user&apos;s time constraint and allows the user to perform other relaxations: (3) There are no trains going to Chicago from Philadelphia on</context>
</contexts>
<marker>Pao, Wilpon, 1992</marker>
<rawString>C. Pao and J. Wilpon. 1992. Spontaneous speech collection for the ATIS domain with an aural user feedback paradigm. Technical report, AT&amp;T</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Pieraccini</author>
<author>E Levin</author>
<author>W Eckert</author>
</authors>
<title>AMICA: The AT&amp;T mixed initiative conversational architecture. In</title>
<date>1997</date>
<booktitle>Proc. EUROSPEECH.</booktitle>
<contexts>
<context position="1824" citStr="Pieraccini et al., 1997" startWordPosition="274" endWordPosition="278"> evaluation framework to estimate an overall performance function, we identify interdependencies that exist between speech recognition and response strategy. Our results elaborate the conditions under which TOOT&apos; s cooperative rather than literal strategy contributes to greater performance. 1 Introduction The notion of a cooperative response has been the focus of considerable research in natural language and spoken dialogue systems (Allen and Perrault, 1980; Mays, 1980; Kaplan, 1981; Joshi et al., 1984; McCoy, 1989; Pao and Wilpon, 1992; Moore, 1994; Seneff et al., 1995; Goddeau et al., 1996; Pieraccini et al., 1997). However, despite the existence of many algorithms for generating cooperative responses, there has been little empirical work addressing the evaluation of such algorithms in the context of real-time natural language dialogue systems with human users. Thus it is unclear under what conditions cooperative responses result in more efficient or efficacious dialogues. This paper presents an empirical evaluation of two alternative algorithms for responding to database queries in TOOT, a spoken dialogue agent for accessing online train schedules via a telephone conversation. We conduct an experiment </context>
<context position="6850" citStr="Pieraccini et al., 1997" startWordPosition="1138" endWordPosition="1142">onstraint&amp;quot; to constrain your departure time or travel da); or say &amp;quot;continue&amp;quot; if my answer was sufficient, or say &amp;quot;repeat&amp;quot; to hear this message again. LT&apos;s response incrementally presents the set of trains that match the query, until the user tells LT to stop. Enumerating large lists, even incrementally, can lead to information overload. CT&apos;s response is more cooperative because it better respects the resource limitations of the listener. CT presents a subset of the matching trains using a summary response (Pao and Wilpon, 1992), followed by an option to reduce the information to be retrieved (Pieraccini et al., 1997; Goddeau et al., 1996; Seneff et al., 1995; Pao and Wilpon, 1992). If there is no information that matches a query, LT reports only the lack of an answer to the query, as in the following dialogue excerpt: (2) There are no trains going to Chicago from Philadelphia on Sunday around 10:30 am. Do you want to continue and find a new set of trains? CT automatically relaxes the user&apos;s time constraint and allows the user to perform other relaxations: (3) There are no trains going to Chicago from Philadelphia on Sunday around 10:30 am. The closest earlier train leaves at 9:28 am every day, and it tak</context>
</contexts>
<marker>Pieraccini, Levin, Eckert, 1997</marker>
<rawString>R. Pieraccini, E. Levin, and W. Eckert. 1997. AMICA: The AT&amp;T mixed initiative conversational architecture. In Proc. EUROSPEECH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Polifroni</author>
<author>L Hirschman</author>
<author>S Seneff</author>
<author>V Zue</author>
</authors>
<title>Experiments in evaluating interactive spoken language systems.</title>
<date>1992</date>
<booktitle>In Proc. DARPA Speech and NL Workshop.</booktitle>
<contexts>
<context position="17661" citStr="Polifroni et al., 1992" startWordPosition="2977" endWordPosition="2980">mpute a Mean Recognition score per dialogue. We also listened to the recordings to determine how many times the user interrupted the agent (Barge Ins). To measure dialogue efficiency, the number of System Turns and User Turns were extracted from the dialogue manager log, and the total Elapsed Time was determined from the recording. To measure user satisfaction4, users responded to the web survey in Figure 4, which assessed their subjective evaluation of the agent&apos;s performance. Each question was designed to measure a partic4Questionnaire-based user satisfaction ratings (Shriberg et al., 1992; Polifroni et al., 1992) have been frequently used in the literature as an external indicator of agent usability. 783 • Was the system easy to understand in this conversation? (TTS Performance) • In this conversation, did the system understand what you said? (ASR Performance) • In this conversation, was it easy to find the schedule you wanted? (Task Ease) • Was the pace of interaction with the system appropriate in this conversation? (Interaction Pace) • In this conversation, did you know what you could say at each point of the dialogue? (User Expertise) • How often was the system sluggish and slow to reply to you in</context>
</contexts>
<marker>Polifroni, Hirschman, Seneff, Zue, 1992</marker>
<rawString>J. Polifroni, L. Hirschman, S. Seneff, and V. Zue. 1992. Experiments in evaluating interactive spoken language systems. In Proc. DARPA Speech and NL Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Seneff</author>
<author>V Zue</author>
<author>J Polifroni</author>
<author>C Pao</author>
<author>L Hetherington</author>
<author>D Goddeau</author>
<author>J Glass</author>
</authors>
<title>The preliminary development of a displayless PEGASUS system.</title>
<date>1995</date>
<booktitle>In Proc. ARPA Spoken Language Technology Workshop.</booktitle>
<contexts>
<context position="1776" citStr="Seneff et al., 1995" startWordPosition="266" endWordPosition="269">formance differences. By using the PARADISE evaluation framework to estimate an overall performance function, we identify interdependencies that exist between speech recognition and response strategy. Our results elaborate the conditions under which TOOT&apos; s cooperative rather than literal strategy contributes to greater performance. 1 Introduction The notion of a cooperative response has been the focus of considerable research in natural language and spoken dialogue systems (Allen and Perrault, 1980; Mays, 1980; Kaplan, 1981; Joshi et al., 1984; McCoy, 1989; Pao and Wilpon, 1992; Moore, 1994; Seneff et al., 1995; Goddeau et al., 1996; Pieraccini et al., 1997). However, despite the existence of many algorithms for generating cooperative responses, there has been little empirical work addressing the evaluation of such algorithms in the context of real-time natural language dialogue systems with human users. Thus it is unclear under what conditions cooperative responses result in more efficient or efficacious dialogues. This paper presents an empirical evaluation of two alternative algorithms for responding to database queries in TOOT, a spoken dialogue agent for accessing online train schedules via a t</context>
<context position="6893" citStr="Seneff et al., 1995" startWordPosition="1147" endWordPosition="1150">travel da); or say &amp;quot;continue&amp;quot; if my answer was sufficient, or say &amp;quot;repeat&amp;quot; to hear this message again. LT&apos;s response incrementally presents the set of trains that match the query, until the user tells LT to stop. Enumerating large lists, even incrementally, can lead to information overload. CT&apos;s response is more cooperative because it better respects the resource limitations of the listener. CT presents a subset of the matching trains using a summary response (Pao and Wilpon, 1992), followed by an option to reduce the information to be retrieved (Pieraccini et al., 1997; Goddeau et al., 1996; Seneff et al., 1995; Pao and Wilpon, 1992). If there is no information that matches a query, LT reports only the lack of an answer to the query, as in the following dialogue excerpt: (2) There are no trains going to Chicago from Philadelphia on Sunday around 10:30 am. Do you want to continue and find a new set of trains? CT automatically relaxes the user&apos;s time constraint and allows the user to perform other relaxations: (3) There are no trains going to Chicago from Philadelphia on Sunday around 10:30 am. The closest earlier train leaves at 9:28 am every day, and it takes 1 day 3 hours 36 mins. The closest later</context>
</contexts>
<marker>Seneff, Zue, Polifroni, Pao, Hetherington, Goddeau, Glass, 1995</marker>
<rawString>S. Seneff, V. Zue, J. Polifroni, C. Pao, L. Hetherington, D. Goddeau, and J. Glass. 1995. The preliminary development of a displayless PEGASUS system. In Proc. ARPA Spoken Language Technology Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Shriberg</author>
<author>E Wade</author>
<author>P Price</author>
</authors>
<title>Human-machine problem solving using spoken language systems (SLS): Factors affecting performance and user satisfaction.</title>
<date>1992</date>
<booktitle>In Proc. DARPA Speech and NL Workshop.</booktitle>
<contexts>
<context position="17636" citStr="Shriberg et al., 1992" startWordPosition="2973" endWordPosition="2976">h ASR rejections, to compute a Mean Recognition score per dialogue. We also listened to the recordings to determine how many times the user interrupted the agent (Barge Ins). To measure dialogue efficiency, the number of System Turns and User Turns were extracted from the dialogue manager log, and the total Elapsed Time was determined from the recording. To measure user satisfaction4, users responded to the web survey in Figure 4, which assessed their subjective evaluation of the agent&apos;s performance. Each question was designed to measure a partic4Questionnaire-based user satisfaction ratings (Shriberg et al., 1992; Polifroni et al., 1992) have been frequently used in the literature as an external indicator of agent usability. 783 • Was the system easy to understand in this conversation? (TTS Performance) • In this conversation, did the system understand what you said? (ASR Performance) • In this conversation, was it easy to find the schedule you wanted? (Task Ease) • Was the pace of interaction with the system appropriate in this conversation? (Interaction Pace) • In this conversation, did you know what you could say at each point of the dialogue? (User Expertise) • How often was the system sluggish an</context>
</contexts>
<marker>Shriberg, Wade, Price, 1992</marker>
<rawString>E. Shriberg, E. Wade, and P. Price. 1992. Human-machine problem solving using spoken language systems (SLS): Factors affecting performance and user satisfaction. In Proc. DARPA Speech and NL Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Walker</author>
<author>D Litman</author>
<author>C Kamm</author>
<author>A Abella</author>
</authors>
<title>PARADISE: A general framework for evaluating spoken dialogue agents.</title>
<date>1997</date>
<booktitle>In Proc. ACL/EACL.</booktitle>
<contexts>
<context position="2781" citStr="Walker et al., 1997" startWordPosition="426" endWordPosition="429">t or efficacious dialogues. This paper presents an empirical evaluation of two alternative algorithms for responding to database queries in TOOT, a spoken dialogue agent for accessing online train schedules via a telephone conversation. We conduct an experiment in which 12 users carry out 4 tasks of varying difficulty with one of two versions of TOOT (literal and cooperative TOOT), resulting in a corpus of 48 dialogues. The values for a wide range of evaluation measures are then extracted from this corpus. We analyze our data using both traditional hypothesis testing methods and the PARADISE (Walker et al., 1997; Walker et al., 1998) methodology for estimating a performance function. Hypothesis testing shows that while differences among some evaluation measures depend on the response strategy (literal or cooperative), other differences are a function of application task and task/strategy interactions. A PARADISE assessment of the contribution of each evaluation measure to overall performance shows that strategy-dependent dialogue phenomena as well as phenomena associated with speech recognition significantly predict performance. Our results identify the conditions under which TOOT&apos; s cooperative resp</context>
<context position="14865" citStr="Walker et al., 1997" startWordPosition="2545" endWordPosition="2548">osition in the list of matches), we predicted that LT would perform better than CT. Note that this prediction is highly dependent on the database. If the desired train had been last in the list, we would have predicted that CT would perform better than LT. attribute value arrival-city Philadelphia depart-city New York City depart-day weekend depart-range 4:00 pm exact-depart-time 4:00 pm total-travel-time 1 hour 12 mins Table 2: Scenano key, Task 4. A second reason for having task scenarios was that it allowed us to objectively determine whether users achieved their tasks. Following PARADISE (Walker et al., 1997), we defined a &amp;quot;key&amp;quot; for each scenario using an attribute value matrix (AVM) task representation, as in Table 2. The key indicates the attribute values that must be exchanged between the agent and user by the end of the dialogue. If the task is successfully completed in a scenario execution (as in Figure 1), the AVM representing the dialogue is identical to the key. 4 Measuring Aspects of Performance Once the experiment was completed, values for a range of evaluation measures were extracted from the resulting data (dialogue recordings, system logs, and web survey responses). Following PARADISE</context>
<context position="23296" citStr="Walker et al., 1997" startWordPosition="3966" endWordPosition="3969">ting tells us how each evaluation measure differs as a function of strategy and/or task, it does not tell us how to tradeoff or combine results from multiple measures. Understanding such tradeoffs is especially important when different measures yield different performance predictions (e.g., recall the Task 2 hypothesis testing results for Completed and Help Requests). I MAXIMIZE USER SATISFACTIONI MAXIMIZE TASK SUCCESS Figure 5: PARADISE&apos; s structure of objectives for spoken dialogue performance. • To assess the relative contribution of each evaluation measure to performance, we use PARADISE (Walker et al., 1997) to derive a performance function from our data. PARADISE draws on ideas in multi-attribute decision theory (Keeney and Raiffa, 1976) to posit the model shown in Figure 5, then uses multivariate linear regression to estimate a quantitative performance function based on this model. Linear regression produces coefficients describing the relative contribution of predictor factors in accounting for the variance in a predicted factor. In PARADISE, the success and cost measures are predictors, while user satisfaction is predicted. Figure 3 showed how the measures used to evaluate TOOT instantiate th</context>
</contexts>
<marker>Walker, Litman, Kamm, Abella, 1997</marker>
<rawString>M. Walker, D. Litman, C. Kamm, and A. Abella. 1997. PARADISE: A general framework for evaluating spoken dialogue agents. In Proc. ACL/EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Walker</author>
<author>D Litman</author>
<author>C Kamm</author>
<author>A Abella</author>
</authors>
<title>Evaluating spoken dialogue agents with PARADISE: Two case studies. Computer Speech and Language.</title>
<date>1998</date>
<contexts>
<context position="2803" citStr="Walker et al., 1998" startWordPosition="430" endWordPosition="433">ogues. This paper presents an empirical evaluation of two alternative algorithms for responding to database queries in TOOT, a spoken dialogue agent for accessing online train schedules via a telephone conversation. We conduct an experiment in which 12 users carry out 4 tasks of varying difficulty with one of two versions of TOOT (literal and cooperative TOOT), resulting in a corpus of 48 dialogues. The values for a wide range of evaluation measures are then extracted from this corpus. We analyze our data using both traditional hypothesis testing methods and the PARADISE (Walker et al., 1997; Walker et al., 1998) methodology for estimating a performance function. Hypothesis testing shows that while differences among some evaluation measures depend on the response strategy (literal or cooperative), other differences are a function of application task and task/strategy interactions. A PARADISE assessment of the contribution of each evaluation measure to overall performance shows that strategy-dependent dialogue phenomena as well as phenomena associated with speech recognition significantly predict performance. Our results identify the conditions under which TOOT&apos; s cooperative response strategy leads to</context>
<context position="4035" citStr="Walker et al., 1998" startWordPosition="617" endWordPosition="620">nt performance. 2 TOOT TOOT allows users to access online AMTRAK train schedules via a telephone dialogue, as in Figure 1.1 (All examples are from the experiment in Section 3.) We have built two versions of TOOT: literal TOOT (LT) and cooperative TOOT (CT). LT and CT have equivalent functionality, but use different response strategies to present tabular results of web queries in a displayless environment.2 LT and CT incorporate many of the types of database responses in the &apos;Our domain was chosen to afford future comparison with similar systems, e.g., (Danieli and Gerbino, 1995). 2Initiative (Walker et al., 1998) and confirmation (Danieli and Gerbino, 1995) strategies are equivalent in LT and CT, and were chosen to reduce speech recognition errors. 780 T: Hi, this is the Amtrak train schedule system. Which city do you want to go to? U: Philadelphia. T: What city do you want to go to Philadelphia from? U: New York City. T: On what day of the week do you want to go to Philadelphia from New York City? U: On the weekend. T: What time do you want to leave on the weekend? U: 4 o&apos;clock pm. T: Do you want me to find the trains going to Philadelphia from New York City on the weekend around 4 o&apos;clock pm now?. U</context>
<context position="28963" citStr="Walker et al., 1998" startWordPosition="4841" endWordPosition="4844">(perceived task success) and strategy-independent (number of barge-ins, mean recognition score) evaluation measures best predicts overall TOOT performance. Our results elaborate the conditions under which TOOT&apos; s response strategies lead to greater performance, and allow us to make predictions. For example, our performance equation predicts that improving mean recognition and/or judiciously restricting the use of barge-in will enhance performance. Our current research is aimed at automatically adapting dialogue behavior in TOOT, to increase mean recognition and thus overall agent performance (Walker et al., 1998). Future work utilizing PARADISE will attempt to generalize our results, to make a more predictive model of agent performance. Performance function estimation needs to be done iteratively over different tasks and dialogue strategies. We plan to evaluate additional cooperative response strategies in TOOT (e.g., intensional summaries (Kalita et al., 1986), summarization and constraint elicitation in isolation), and to combine TOOT data with data from other agents (Walker et al., 1998). 8 Acknowledgments Thanks to J. Chu-Carroll, T. Dasu, W. DuMouchel, J. Fromer, D. Hindle, J. Hirschberg, C. Kamm</context>
</contexts>
<marker>Walker, Litman, Kamm, Abella, 1998</marker>
<rawString>M. Walker, D. Litman, C. Kamm, and A. Abella. 1998. Evaluating spoken dialogue agents with PARADISE: Two case studies. Computer Speech and Language.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>