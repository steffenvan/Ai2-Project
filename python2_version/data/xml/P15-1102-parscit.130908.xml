<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000018">
<title confidence="0.9951235">
Co-training for Semi-supervised Sentiment Classification Based on
Dual-view Bags-of-words Representation
</title>
<author confidence="0.999893">
Rui Xia&apos;,&apos;, Cheng Wang&apos;, Xinyu Dai&apos;, and Tao Li3,4
</author>
<affiliation confidence="0.99802475">
&apos;School of Computer Science, Nanjing University of Science &amp; Technology, China
&apos;State Key Laboratory for Novel Software Technology, Nanjing University, China
3School of Computer Science, Florida International University, USA
4School of Computer Science, Nanjing University of Posts &amp; Telecommunications, China
</affiliation>
<email confidence="0.9828995">
rxia@njust.edu.cn, wangcheng1022@gmail.com,
daixinyu@nju.edu.cn, taoli@cs.fiu.edu
</email>
<sectionHeader confidence="0.99385" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999905727272727">
A review text is normally represented as
a bag-of-words (BOW) in sentiment clas-
sification. Such a simplified BOW model
has fundamental deficiencies in modeling
some complex linguistic phenomena such
as negation. In this work, we propose a
dual-view co-training algorithm based on
dual-view BOW representation for semi-
supervised sentiment classification. In
dual-view BOW, we automatically con-
struct antonymous reviews and model a
review text by a pair of bags-of-words
with opposite views. We make use of the
original and antonymous views in pairs,
in the training, bootstrapping and test-
ing process, all based on a joint observa-
tion of two views. The experimental re-
sults demonstrate the advantages of our ap-
proach, in meeting the two co-training re-
quirements, addressing the negation prob-
lem, and enhancing the semi-supervised
sentiment classification efficiency.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999953230769231">
In the past decade, there has been an explosion
of user-generated subjective texts on the Internet
in forms of online reviews, blogs and microblogs.
With the need of automatically identifying senti-
ments and opinions from those online texts, senti-
ment classification has attracted much attention in
the field of natural language processing.
Lots of previous research focused on the task
of supervised sentiment classification. However,
in some domains, it is hard to obtain a sufficient
amount of labeled training data. Manual annota-
tion is also very expensive and time-consuming.
To address this problem, semi-supervised learning
approaches were employed in sentiment classifica-
tion, to reduce the need for labeled reviews by tak-
ing advantage of unlabeled reviews.
The dominating text representation method in
both supervised and semi-supervised sentiment
classification is known as the bag-of-words (BOW)
model, which is difficult to meet the requirements
for understanding the review text and dealing with
complex linguistic structures such as negation. For
example, the BOW representations of two opposite
reviews “It works well” and “It doesn’t work well”
are considered to be very similar by most statistical
learning algorithms.
In supervised sentiment classification, many ap-
proaches have been proposed in addressing the
negation problem (Pang et al., 2002; Na et al.,
2004; Polanyi and Zaenen , 2004; Kennedy and
Inkpen, 2006; Ikeda et al., 2008; Li et al., 2010b;
Orimaye et al., 2012; Xia et al., 2013). Nev-
ertheless, in semi-supervised sentiment classifica-
tion, most of the current approaches directly ap-
ply standard semi-supervised learning algorithms,
without paying attention to appropriate representa-
tion for review texts. For example, Aue and Ga-
mon (2005) applied the naive Bayes EM algorithm
(Nigam et al., 2000). Goldberg and Zhu (2006) ap-
plied a graph-based semi-supervised learning algo-
rithm by (Zhu et al., 2003). Wan (2009) employed
a co-training approach for cross-language senti-
ment classification. Li et al. (2010a) employed co-
training with personal and impersonal views. Ren
et al. (2011) explored the use of label propagation
(Zhu and Ghahramani, 2002).
As pointed by (Goldberg and Zhu, 2006): it is
necessary to investigate better review text represen-
tations and similarity measures based on linguis-
tic knowledge, as well as reviews’ sentiment pat-
terns. However, to the best knowledge, such inves-
tigations are very scarce in the research of semi-
</bodyText>
<page confidence="0.966073">
1054
</page>
<note confidence="0.977567">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1054–1063,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.998130333333333">
supervised sentiment classification.
In (Xia et al., 2013), we have developed a
dual sentiment analysis approach, which creates
antonymous reviews and makes use of original and
antonymous reviews together for supervised sen-
timent classification. In this work, we propose
a dual-view co-training approach based on dual-
view BOW representation for semi-supervised sen-
timent classification. Specifically, we model both
the original and antonymous reviews by a pair of
bags-of-words with opposite views. Based on such
a dual-view representation, we design a dual-view
co-training approach. The training, bootstrapping
and testing processes are all performed by observ-
ing two opposite sides of one review. That is, we
consider not only how positive/negative the orig-
inal review is, but also how negative/positive the
antonymous review is.
In comparison with traditional methods, our
dual-view co-training approach has the following
advantages:
</bodyText>
<listItem confidence="0.9983824">
• Effectively address the negation problem;
• Automatically learn the associations among
antonyms;
• Better meet the two co-training requirements
in (Blum and Mitchell, 1998).
</listItem>
<sectionHeader confidence="0.999097" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999086045454546">
The mainstream of the research in sentiment clas-
sification focused on supervised and unsupervised
learning tasks. In comparison, semi-supervised
sentiment classification has much less related stud-
ies. In this section, we focus on reviewing the work
of semi-supervised sentiment classification.
Aue and Gamon (2005) combined a small
amount of labeled data with a large amount of
unlabeled data in target domain for cross-domain
sentiment classification based on the EM algo-
rithm. Goldberg and Zhu (2006) presented a graph-
based semi-supervised learning algorithm (Zhu et
al., 2003) for the sentiment analysis task of rat-
ing inference. Dasgupta and Ng (2009) proposed
a semi-supervised approach to mine the unambigu-
ous reviews at first and then exploiting them to
classify the ambiguous reviews, via a combination
of active learning, transductive learning and en-
semble learning. Ren et al. (2011) explored the
use of label propagation (LP) (Zhu and Ghahra-
mani, 2002) in building a semi-supervised senti-
ment classifier, and compared their results with
Transductive SVMs(T-SVM). LP and T-SVM are
transductive learning methods where the test data
should participate in the training process.
Zhou et al. (2010) proposed a deep learning
approach called active deep networks to address
semi-supervised sentiment classification with ac-
tive learning. Socher et al. (2012) introduced a
deep learning framework called semi-supervised
recursive autoencoders for predicting sentence-
level sentiment distributions. The limitation of
deep learning approaches might be their depen-
dence on a considerable amount of unlabeled data
to learn the representations and the inability to ex-
plicitly model the negation problem.
One line of semi-supervised learning research
is to bootstrap class labels using techniques like
self-training, co-training and their variations. Wan
(2009) proposed a co-training approach to address
the cross-lingual sentiment classification problem.
They made use of the machine translation service
to produce two views (a English view and a Chi-
nese view) for co-training a Chinese review senti-
ment classifier, based on English corpus and unla-
beled Chinese corpus. Li et al. (2010a) proposed
an unsupervised method at first to automatically
separate the review text into a personal view and an
impersonal view, based on which the standard co-
training algorithm is then applied to build a semi-
supervised sentiment classifier. Li et al. (2011)
further studied semi-supervised learning for imbal-
anced sentiment classification by using a dynamic
co-training approach. Su et al. (2012) proposed
a multi-view learning approach to semi-supervised
sentiment classification with both feature partition
and language translation strategies (Wan , 2009).
Following (Li et al., 2010a), Li (2013) proposed
a co-training approach which exploits subjective
and objective views for semi-supervised sentiment
classification. Our approach can also be viewed as
a variation of co-training. The innovation of our
approach is the dual-view construction technique
by incorporating antonymous reviews and the boot-
strapping mechanism by observing two opposite
sides of one review.
</bodyText>
<sectionHeader confidence="0.998904" genericHeader="method">
3 The Proposed Approach
</sectionHeader>
<subsectionHeader confidence="0.9935115">
3.1 Dual-view BOW Representation for
Review Texts
</subsectionHeader>
<bodyText confidence="0.9997175">
Every coin has two sizes. In this work, we are mo-
tivated to automatically construct the antonymous
reviews, consider the original and antonymous re-
views as two opposite sides of one review, and rep-
</bodyText>
<page confidence="0.996878">
1055
</page>
<figureCaption confidence="0.600083666666667">
Figure 1: An illustration of the dual-view BOW
representation. The feature vector with black font
color and grey background denotes the original
view; while the one with white font color and
black background denotes the reversed antony-
mous view.
</figureCaption>
<bodyText confidence="0.986881173913044">
resent them in pairs by a dual-view BOW model.
Look at the following example:
Original Review: “The app doesn’t
work well on my phone. Disappointing.
Don’t recommend it.”
Antonymous Review: “The app works
well on my phone. Satisfactory. Recom-
mend it.”
Given an original review, its antonymous review
is automatically created as follows1: 1) We first de-
tect the negations in each subsentence of the review
text; 2) If there is a negation, we remove negators
in that subsentence; 3) Otherwise, we reverse all
the sentiment words in the subsentence into their
antonyms, according to a pre-defined antonym dic-
tionary2.
We subsequently use a dual-view BOW model to
represent such a pair of reviews, as shown in Fig-
ure 1. The original and antonymous reviews will
be used in pairs in our dual-view semi-supervised
learning approach. As we determine the sentiment
of one review, we could observe not only the orig-
inal view, but also the antonymous view.
</bodyText>
<footnote confidence="0.99759025">
1It is worth noting that our emphasis here is not to generate
natural-language-like review texts. Since either the original or
the created antonymous review will be represented as a vector
of independent words in the BOW model, the grammatical
requirement is not as strict as that in human languages.
2In our experiments, we extract the antonym dic-
tionary from the WordNet lexicon http://wordnet.
princeton.edu/.
</footnote>
<figureCaption confidence="0.788366333333333">
Figure 2: The process of dual-view co-training.
Again, the white font color and black background
are used to denote the antonymous view.
</figureCaption>
<bodyText confidence="0.998964166666667">
It is important to notice that the antony-
mous view removes all negations and incorporates
antonymous features. On this basis, we design a
dual-view co-training approach. We will introduce
our approach in detail in Section 3.2, and analyze
its potential advantages in Section 3.3.
</bodyText>
<subsectionHeader confidence="0.99965">
3.2 The Dual-view Co-training Approach
</subsectionHeader>
<bodyText confidence="0.999856666666667">
Since the original and antonymous views form two
different views of one review text, it is natural to
employ the co-training algorithm, which requires
two views for semi-supervised classification.
Co-training is a typical bootstrapping algorithm
that first learns a separate classifier for each view
using the labeled data. The most confident predic-
tions of each classifier on the unlabeled data are
then used to construct additional labeled training
data iteratively. Co-training has been extensively
used in NLP, including statistical parsing (Sarkar ,
2001), reference resolution (Ng and Cardie, 2003),
part-of-speech tagging (Clark et al., 2003), word
sense disambiguation (Mihalcea, 2004), and senti-
ment classification (Wan , 2009; Li et al., 2010a).
But it should be noted that the dual views in
our approach are different from traditional views.
One important property of our approach is that
two views are opposite and therefore associated
with opposite class labels. Figure 2 illustrates the
process of dual-view co-training.
</bodyText>
<figure confidence="0.981319135593221">
Feature Original Antonymous
Space View View
1
app
phone
1
didn&apos;t
work
well
1
1
1
1
1
0
disappointing
recommend
satisfactory
1
1
0
1
1
0
1
1
Bootstrapping
Original View Antonymous iView
Unlabeled
Original
Reviews
Original
Sentiment
Classifier
Labeled
Original
Reviews
Dual-view
Sentiment
Consensus
Dual
Sentiment
Classifier
Review
Reversion
Review
Reversion
Unlabeled
Antonymous
Reviews
Antonymous
Sentiment
Classifier
Labeled
Antonymous
Reviews
1056
.
(1) Dual-view training
</figure>
<bodyText confidence="0.999719823529412">
For each instance in the initial labeled set, we con-
struct the dual-view representations. Let xlo and
xl a denote the bags of words in the original view
and the antonymous view, respectively. Note that
the class labels in two views are kept opposite:
yla = 1 − ylo (y ∈ {0,1}). That is, we reverse
the class label in the original view (i.e., positive to
negative, or vice versa), as the class label of the
created antonymous view.
Suppose L is the labeled set, with Lo and La
denoting the original-view and antonymous-view
labeled sets, respectively. We train two distinct
classifiers: the original-view classifier ho and the
antonymous-view classifier ha , based on Lo and
La, respectively. We further train a joint classifier
by using Lo and La together as the training data,
and refer to it as hd.
</bodyText>
<listItem confidence="0.544776">
(2) Dual-view bootstrapping
</listItem>
<bodyText confidence="0.999931409090909">
In standard co-training, we allow each classifier to
examine the unlabeled set U and select the most
confidently predicted examples in each category.
The selected examples are then added into L ,
along with the predicted class labels.
In this work, we design a dual-view co-training
algorithm to bootstrap the class labels by a joint ob-
servation of two sides of one review. Specifically,
we propose a new bootstrapping mechanism, based
on a principle called dual-view sentiment consen-
sus. Given an unlabeled instance {xuo , xua}, dual
view sentiment consensus requires that, the orig-
inal prediction yuo and the antonymous prediction
should be opposite: yua = 1 − yuo . In other words,
we only select the instances of which the original
prediction is positive/negative, and the same time
the antonymous prediction is negative/positive. To
increase the degree of sentiment consensus, we fur-
ther require that the predition yud of hd should be
the same as you.
We sort all unlabeled instances according to the
dual-view predictions in each class, filter the list
according to the dual-view sentiment consensus
principle, and add the top-ranked s instances in
each class to the labeled set. For each selected un-
labeled instance, its original view xuo is added into
Lo with class label you; and the antonymous view
xu a is added into La, with an opposite class label
yua = 1 − yuo . When Lo and La receive the supple-
mental labeled instances, we update ho and ha.
Our bootstrapping mechanism differs from the
traditional methods in two major aspects: First, in
traditional co-training, given the same instance,
the class labels in two views are the same. But in
our approach, the class labels in two views need
to be opposite. Second, in traditional co-training,
the most confidently predicted examples in each
view are selected to extend the amount of labeled
data. It is dangerous to believe the confident but
incorrect predictions. While in our approach, the
candidates are further filtered by the principle of
dual-view sentiment consensus. In this way, the
labeling accuracy and learning efficiency can be
improved.
</bodyText>
<sectionHeader confidence="0.680904" genericHeader="method">
(3) Dual-view testing
</sectionHeader>
<bodyText confidence="0.999596923076923">
Finally, in the testing stage, standard co-training
uses a joint set of features in two views to train the
classifier. In dual-view testing, we use ho and ha
to predict the test example in two views, and make
the final prediction by considering both sizes of the
review.
Given a test example xte with its original view
denoted by xteo and antonymous view denoted
by xtea , let po(·|xteo ) be the posterior probability
predicted by the original-view classifier ho, and
pa(·|xtea ) be the posterior probability predicted by
ha. The dual-view testing process can be formu-
lated as follows:
</bodyText>
<equation confidence="0.945334666666667">
p(+|xte) = p(+|xteo , xtea ) =
p(− |xte) = p(—|xoe� xtae) = po(− |xoe) + pa(+|xae)
l 2
</equation>
<bodyText confidence="0.999948833333333">
That is, the final positive score is assigned by
measuring not only how positive the original re-
view is, but also how negative the antonymous one
is; the negative score is assigned by measuring not
only how positive the original review is, but also
how negative the antonymous one is.
</bodyText>
<subsectionHeader confidence="0.999891">
3.3 Advantages of Dual-view Co-training
</subsectionHeader>
<bodyText confidence="0.999631">
Our proposed dual-view co-training approach has
the following three advantages.
</bodyText>
<listItem confidence="0.744453">
(1) Effectively address the negation issue
</listItem>
<bodyText confidence="0.999523166666667">
We use the antonymous review as a view to effec-
tively address the negation issue. Let us revisit the
example in Section 3.1 and assume that the orig-
inal review (i.e., “The app doesn’t work well on
my phone. Disappointing. Do not recommend it.”)
is an unlabeled sample. Because the traditional
</bodyText>
<equation confidence="0.425863">
po(+|xteo ) + pa(−|xtea )
;
2
</equation>
<page confidence="0.94159">
1057
</page>
<bodyText confidence="0.9999246">
BOW model cannot well represent negative struc-
tures, the review is likely to be incorrectly labeled
as positive and then added into the labeled set.
In our proposed approach, the antonymous re-
view (i.e., “The app works well on my phone. Sat-
isfactory. Recommend it.”) removed all the neg-
ative structures, and is thus more suited for the
BOW representation. In this example, the antony-
mous review is also likely to be marked as positive.
Hence, in this case, both the original review and
its antonymous review will be labeled as positive,
which violates the principle of dual-view sentiment
consensus as mentioned in Section 3.2. As a result,
the unlabeled instance will not be added into the
labeled set.
Therefore, our approach can overcome the limi-
tations of the conventional methods in addressing
the negation issue and reduce the labeling error
rate (caused by the negative structures) during the
bootstrapping process.
</bodyText>
<listItem confidence="0.552924">
(2) Automatically learn the associations among
antonyms
</listItem>
<bodyText confidence="0.999901647058824">
In semi-supervised sentiment classification, only
limited association information between the words
and categories can be obtained from a small num-
ber of initial labeled data.
For instance, in the above example “disappoint-
ing” and “satisfactory” are a pair of antonyms.
From the initial labeled data, we may only learn
that “disappointing” is derogatory, but we cannot
infer that “satisfactory” is commendatory.
During the bootstrapping process in our
approach, when constructing the dual view rep-
resentation, the original view and its antonymous
view are required to have opposite class labels.
Hence we can automatically infer the relationship
between “satisfactory” and “disappointing” (e.g.,
one is positive and one is negative), thereby
improving the learning efficiency of the system.
</bodyText>
<listItem confidence="0.969904">
(3) Better meet two co-training requirements
</listItem>
<bodyText confidence="0.9998935">
Compared with traditional methods, our dual-view
co-training can better meet the two co-training re-
quirements: 1) sufficient condition (i.e., each view
is sufficient for classification); 2) complementary
condition (i.e., the two views are conditionally in-
dependent).
First, for the sufficient condition, we use a dif-
ferent view construction method. Most traditional
methods construct the two views by feature parti-
tioning (i.e., dividing the original feature set into
two subsets), while we use data expansion by gen-
erating antonymous reviews. We will demonstrate
in the experimental section (Section 4.6), that our
data expansion method can construct better views
than the feature partition method in terms of pre-
dicting the class labels from individual views.
Second, as we know, every coin has two sides
and the two sides are often complementary. In
our proposed approach, the original review and its
antonymous review (i.e., two sides of one review)
are used as two views for co-training and they can
better meet the complementary condition. We will
illustrate this point in Section 4.6 by calculating the
KL divergence between the two views.
</bodyText>
<sectionHeader confidence="0.999628" genericHeader="method">
4 Experimental Study
</sectionHeader>
<subsectionHeader confidence="0.992716">
4.1 Datasets and Experimental Settings
</subsectionHeader>
<bodyText confidence="0.958777909090909">
We conduct the experiments on the multi-domain
sentiment datasets, which were introduced in
(Blitzer et al., 2007) and have been widely used in
sentiment classification. It consists of four domains
(Book, DVD, Electronics, and Kitchen) of reviews
extracted from Amazon.com. Each of the four
datasets contains 1,000 positive and 1,000 negative
reviews. Following the experimental settings used
in (Li et al., 2010a), we randomly separate all the
reviews in each class into a labeled data set, a un-
labeled data set, and a test set, with a proportion of
10%, 70% and 20%, respectively. We report the av-
eraged results of 10-fold cross-validation in terms
of classification accuracy.
Note that our approach is a general framework
that allows different classification algorithms. Due
to the space limitation, we only report the results by
using logistic regression3. Note the similar conclu-
sions can be obtained by using the other algorithms
such as SVMs and naive Bayes. The LibLinear
toolkit4 is utilized, with a dual L2-regularized fac-
tor, and a default tradeoff parameter c. Similar to
(Wan , 2009; Li et al., 2010a), we carry out the ex-
periments with the unigram features without fea-
ture selection. Presence is used as the term weight-
ing scheme as it was reported in (Pang et al., 2002)
that it performed better than TF and TF-IDF. Fi-
nally, the paired t-test (Yang and Liu, 1999) is per-
formed to test the significance of the difference be-
3Logistic regression is quite similar to Maximum Entropy,
and has been proved to be more efficient in sentiment clas-
sification than some other classification algorithms including
naive Bayes and SVMs (Pang et al., 2002).
</bodyText>
<footnote confidence="0.9923165">
4http://www.csie.ntu.edu.tw/˜cjlin/
liblinear/
</footnote>
<page confidence="0.954117">
1058
</page>
<table confidence="0.999671727272727">
BOOK DVD ELEC KITC Avg.
Baseline 0.680 0.691 0.726 0.740 0.709
LP 0.681 0.676 0.697 0.722 0.694
T-SVM 0.671 0.677 0.716 0.729 0.698
EM 0.702 0.706 0.758 0.744 0.728
Self-Training 0.689 0.705 0.736 0.751 0.720
Self-Reserved 0.690 0.708 0.735 0.754 0.722
Co-Static 0.696 0.714 0.745 0.762 0.729
Co-Dynamic 0.701 0.725 0.756 0.767 0.737
Co-PI 0.702 0.716 0.746 0.769 0.733
Our approach 0.721 0.738 0.769 0.780 0.752
</table>
<tableCaption confidence="0.9767065">
Table 1: The semi-supervised classification accu-
racy of ten systems.
</tableCaption>
<bodyText confidence="0.8822085">
tween two systems, with a default significant level
of 0.05.
</bodyText>
<subsectionHeader confidence="0.985247">
4.2 Compared Systems
</subsectionHeader>
<bodyText confidence="0.999754">
We implement the following nine systems and
compare them with our approach:
</bodyText>
<listItem confidence="0.998415068965517">
• Baseline, the supervised baseline trained with
the initial labeled data only;
• Expectation Maximization (EM), with the
naive Bayes model proposed by Nigam et al.
(2000);
• Label Propagation (LP), a graph-based
semi-supervised learning method proposed by
Zhu and Ghahramani (2002);
• Transductive SVM (T-SVM), an extension
of SVM so that it can exploit unlabeled data in
semi-supervised learning ( Joachims, 1999);
• Self-Training, a bootstrapping model that
first trains a classifier, uses it to classify the
unlabeled data, and adds the most confident
data to the labeled set;
• Self-Reserved, a variation of self-training
proposed in (Liu et al., 2013),with a reserved
procedure to incorporate some less confident
examples;
• Co-Static, the co-training algorithm by using
two static partitions of feature set as two views
(Blum and Mitchell, 1998);
• Co-Dynamic, a variation of co-training that
uses dynamic feature space in each loop. It
was reported in (Li et al., 2011) that the Co-
Dynamic significantly outperforms Co-Static
significantly;
• Co-PI, another variation of co-training pro-
posed by (Li et al., 2010a), by using personal
</listItem>
<figure confidence="0.980198">
Performance across four datasets
0 100 200300 400 500 600 700 800 900
Number of new labeled data bootstrapped from unlabeled set
</figure>
<figureCaption confidence="0.996332">
Figure 3: Comparsion of different boostrapping
methods.
</figureCaption>
<bodyText confidence="0.836424">
and impersonal views for co-training.
</bodyText>
<subsectionHeader confidence="0.982868">
4.3 Performance Comparison
</subsectionHeader>
<bodyText confidence="0.999985892857143">
In table 1, we report the semi-supervised classifica-
tion accuracy of ten evaluated systems. We report
the results with 200 labeled, 1400 unlabeled and
400 test reviews. Note that the similar conclusions
can be obtained when the size of the initial labeled
data changes. We will discuss its influence later.
As can be seen, trained with only 200 labeled
data, the supervised baseline yields an average ac-
curacy of 0.709. Self-training gains an improve-
ment of 1.1%. Self-reserved does not show sig-
nificant priority against Self-training. Three co-
training systems (Co-static, Co-dynamic and Co-
PI) get significant improvements. They increase
the supervised baseline by 2.0%, 2.8% and 2.4%,
respectively.
It is somehow surprising that T-SVM and LP do
not outperform the supervised baseline, probably
because the supervised baseline is obtained by lo-
gistic regression, which was reported to be more ef-
fective than SVMs in sentiment classification (the
supervised result of SVMs is 0.695).
Our proposed approach significantly outper-
forms all the other methods. It gains the improve-
ment over the supervised baseline, Self-training,
Co-static, Co-dynamic and Co-PI by 4.3%, 3.2%,
2.3%, 1.5% and 1.9%, respectively. All of the im-
provements are significant according to the paired
t-test.
</bodyText>
<subsectionHeader confidence="0.999434">
4.4 Comparison of Bootstrapping Methods
</subsectionHeader>
<bodyText confidence="0.9995345">
In Figure 3, we further compare five bootstrap-
ping methods by drawing the accuracy curve dur-
</bodyText>
<figure confidence="0.981479409090909">
Accuracy
0.74
0.70
0.69
0.75
0.73
0.72
0.71
Self-training
Co-static
Co-PI
Co-dynamic
Our approach
1059
Supervised Co-Dynamic Co-PI Our Approach
0.8
0.75
0.7
0.65
0.6
0.55
20 50 100 150 200 300 400
</figure>
<figureCaption confidence="0.9806345">
Figure 4: Influence of the size of initial labeled
data.
</figureCaption>
<bodyText confidence="0.986939384615385">
ing the bootstrapping process. The x-axis denotes
the number of new labeled data bootstrapped from
the unlabeled data.
We can roughly rank five bootstrapping methods
as follows: Our approach » Co-dynamic &gt; Co-
PI &gt; Co-static » Self-training. Self-training gives
the worst performance. Co-static works better but
the effect is limited. Co-PI and Co-dynamic are
significantly better. Our proposed approach outper-
forms the other systems robustly, along with the in-
creased number of the new labeled data. It suggests
that our approach is very efficient in bootstrapping
the class labels from the unlabeled data.
</bodyText>
<subsectionHeader confidence="0.9344005">
4.5 Influence of the Size of the Initial Labeled
Set
</subsectionHeader>
<bodyText confidence="0.999978666666667">
The above results are obtained with 200 labeled,
1400 unlabeled and 400 test reviews. We now tune
the size of the initial labeled set (from 20 to 400),
and report its influence in Figure 4. For all the set-
tings, we fix the size of test set as 400. The x-axis
denotes the number of initial labeled set. For ex-
ample, “20” denotes the setting of 20 labeled and
1580 unlabeled data.
We can observe that our all methods improve as
the initial size increases. But the improvements be-
come limited when the size becomes larger. When
the initial size is 400, the semi-supervised perfor-
mance is close to the golden result obtained by the
supervised classifier trained with all 1600 labeled
data.
Our approach performs consistently the best
across different sizes of the initial sizes. The
smaller the initial size is, the more improvements
our approach can gain, in comparison with the
other methods. This confirms our analysis in Sec-
tion 3.3 that the technique of dual-view construc-
tion is very effective to boost the semi-supervised
classification performance, especially when the
size of the initial labeled set is small.
</bodyText>
<subsectionHeader confidence="0.7582995">
4.6 Discussion on the Two Co-training
Requirements
</subsectionHeader>
<bodyText confidence="0.999870428571429">
Ideally, co-training requires that each view is
sufficient for classification (sufficient condition)
and two views provide complementary informa-
tion of the instance,(complementary condition).In
this section, we answer the following question
empirically: whether our approach could meet the
two requirements?
</bodyText>
<listItem confidence="0.453715">
(1) Sufficient condition
</listItem>
<bodyText confidence="0.999906">
In Figure 5, we report the classification perfor-
mance obtained by the classifiers trained with dis-
tinct views and compared them with the two views
in Co-PI, on the DVD and Electronics datasets.
The observation in Book is similar to that in Elec-
tronics; the observation in DVD is similar to that in
Kitchen.
Seen from Figure 5, the classification perfor-
mance of both the original-view and antonymous-
view classifiers are satisfactory. It shows that in
</bodyText>
<page confidence="0.918012">
1060
</page>
<figure confidence="0.998956862068966">
DVD
ELEC
0.78
0.76
0.74
0.70
0.64
0 200 400 600 800 1000
Number of new labeled data bootstrapped from unlabeled set
0.74
0.70
0.68
0.60
0 200 400 600 800 1000
Number of new labeled data bootstrapped from unlabeled set
0.72
0.72
Co-PI-view1
Co-PI-view2
Co-PI
Original view
Antonymous view
Dual-view testing
Co-PI-view1
Co-PI-view2
Co-PI
Original view
Antonymous view
Dual-view testing
</figure>
<figureCaption confidence="0.9705565">
Figure 5: Comparison of different views on the
DVD and Electronics datasets.
</figureCaption>
<figure confidence="0.992717571428572">
Accuracy
Accuracy
0.68
0.66
0.66
0.64
0.62
</figure>
<bodyText confidence="0.999899538461538">
our approach, each individual view is sufficient to
predict the sentiment. In comparison with the two
views in Co-PI (i.e., the personal and impersonal
views), two views in our approach perform signifi-
cantly better.
As has been mentioned in Section 3.3, in tradi-
tional methods, such as Co-PI and Co-dynamic,
two views are created by data partition (or feature
partition). In comparison, the two views in our
approach are constructed in a manner of data ex-
pansion. By creating a new antonymous view, our
approach can provide more sufficient information
of the reviews than traditional methods.
</bodyText>
<sectionHeader confidence="0.673934" genericHeader="method">
(2) Complementary condition
</sectionHeader>
<bodyText confidence="0.9999498">
Since we have not found a direct measure of the
complementarity of two views, we instead calcu-
late the Kullback-Leibler (KL) divergence between
them, based on an assumption that two views with
higher KL divergence can provide more comple-
mentary information of the instance.
KL divergence is a widely used metric of statis-
tical distance. We assume that distribution of the
review text is multinomial, and calculate the K-L
divergence between two views as follows:
</bodyText>
<equation confidence="0.5153">
pi logCpi J
qi/
</equation>
<bodyText confidence="0.994579913043478">
where pi and qi are the probabilities of word ap-
pearing in two views, respectively. In our ex-
periments, we use information gain (IG) to select
a set of discriminative words with the dimension
V = 2000.
In Table 2, we report the results of three differ-
ent methods: 1) dataset random partition; 2) per-
sonal and impersonal views in Co-PI; 3) original
and antonymous views in our approach. We can
observe from Table 2 that, random partition has
the lowest KL divergence. It shows that the dis-
tributional distance between two randomly parti-
tioned views is very small. Co-PI is a higher value,
but it still does not have significant difference in
two views. By contrast, the KL divergence be-
tween the original view and the antonymous view
is much higher than both random partition and Co-
PI. It demonstrates that the distributions of two
views in our approach are significantly different.
We thereby infer that the two views constructed in
our approach can provide more complementary in-
formation than traditional methods. It is reason-
able since the antonymous view incorporates the
</bodyText>
<table confidence="0.99839925">
KL divergence
Random Partition 2.43
Co-PI 4.59
Our approach 12.33
</table>
<tableCaption confidence="0.936761">
Table 2: The average KL divergence between two
views across four datasets.
</tableCaption>
<bodyText confidence="0.993023">
antonyms that might have not appeared in the origi-
nal view (e.g., “satisfactory” in the example in Sec-
tion 3.2). These features might provide new infor-
mation about the instance.
</bodyText>
<subsectionHeader confidence="0.972405">
4.7 The Effect of Dual-view Testing
</subsectionHeader>
<bodyText confidence="0.999990772727273">
In Figure 5, we can further observe the effect of
dual-view testing. On the Electronics dataset, the
antonymous view performs better than the orig-
inal view. This suggests the advantage of the
antonymous view, as it removes the negations and
thus is more suitable for the BOW representa-
tion. On the DVD dataset, the original view is
slightly better. This is also reasonablel, because the
antonymous review is automatically created and its
quality might be limited in some cases. By tak-
ing two opposite views into a joint consideration,
our dual-view testing technique guarantees a satis-
factory classification performance across different
datasets.
Note that in the current version, the original-
view and antonymous-view classifiers have the
same predicting weight. We believe that by learn-
ing the tradeoff between two views in different set-
tings may further improve our approach’s perfor-
mance. For example, if the original view on the
Electronics dataset gets a relatively larger weight,
dual-view testing might gain more improvements.
</bodyText>
<sectionHeader confidence="0.999442" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999926461538462">
In this work, a review text is represented by a
pair of bags-of-words with opposite views (i.e., the
original and antonymous views). By making use
of two views in pairs, a dual-view co-training al-
gorithm is proposed for semi-supervised sentiment
classification. The dual-view representation is in a
good accordance with the two co-training require-
ments (i.e., sufficient condition and complemen-
tary condition). The experimental results demon-
strate the effect of our approach, in addressing the
negation problem and enhancing the bootstrapping
efficiency for semi-supervised sentiment classifica-
tion.
</bodyText>
<equation confidence="0.991695">
V
DKL(p||q) =
i=1
</equation>
<page confidence="0.973085">
1061
</page>
<sectionHeader confidence="0.981267" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.999108">
The work is supported by the Natural Science
Foundation of China (61305090), and the Jiangsu
Provincial Natural Science Foundation of China
(BK2012396).
</bodyText>
<sectionHeader confidence="0.999108" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999896589473684">
A. Aue and M. Gamon. 2005. Customizing Sentiment
Classifiers to New Domains: A Case Study. In Pro-
ceedings of Recent Advances in Natural Language
Processing.
J. Blitzer, M. Dredze, and F. Pereira. 2007. Biogra-
phies, Bollywood, Boom-boxes and Blenders: Do-
main Adaptation for Sentiment Classification. In
Proceedings of ACL .
A. Blum and T. Mitchell. 1998. Combining labeled
and unlabeled data with co-training. In Proceedings
of COLT.
S. Clark, J. R. Curran, and M. Osborne. 2003. Boot-
strapping POS taggers using unlabelled data. In Pro-
ceedings of CoNLL.
S. Dasgupta and V. Ng. 2009. Mine the Easy and Clas-
sify the Hard: Experiments with Automatic Senti-
ment Classification. In Proceedings ofACL-IJCNLP.
A. Goldberg and X. Zhu. 2006. Seeing stars when
there aren’t many stars: graph-based semi-supervised
learning for sentiment categorization. In Proceed-
ings of the Workshop on TextGraphs at HLT-NAACL.
M. Hu and B. Liu. 2004. Mining opinion features in
customer reviews. In Proceedings of the National
Conference on Artificial Intelligence (AAAI).
D. Ikeda, H. Takamura, L. Ratinov, and M. Okumura.
2008. Learning to Shift the Polarity of Words for
Sentiment Classification. In Proceedings of IJCNLP.
T. Joachims. 1999. Transductive Inference for Text
Classification using Support Vector Machines. In
Proceedings of ICML.
A. Kennedy and D. Inkpen. 2006. Sentiment classi-
fication of movie reviews using contextual valence
shifters. Computational Intelligence, 22:110–125.
LaTeX Error: File ‘url.sty’ not found. V. Ng and C.
Cardie. 2003. Weakly supervised natural language
learning without redundant views. In Proceedings of
HLT-NAACL.
K. Nigam, A. McCallum, S. Thrun, and T. Mitchell.
2000. Text classification from labeled and unlabeled
documents using EM. Machine Learning, 39(2/3):
103–134.
S. Li, C. Huang, G. Zhou, and S. Y. M. Lee. 2010a.
Employing personal/impersonal views in supervised
and semi-supervised sentiment classification. In Pro-
ceedings of the Annual Meeting of the Association for
Computational Linguistics (ACL) .
S. Li, S. Lee, Y. Chen, C. Huang, and G. Zhou. 2010b.
Sentiment Classification and Polarity Shifting. In
Proceeding of the International Conference on Com-
putational Linguistics (COLING).
S. Li, Z. Wang, G. Zhou, and S. Lee. 2011. Semi-
Supervised Learning for Imbalanced Sentiment Clas-
sification. In Proceedings of IJCAI.
S. Li. 2013. Sentiment classification using subjective
and objective views. International Journal of Com-
puter Applications, 80(7): 30–34.
Z. Liu, X. Dong, Y. Guan, and J. Yang. 2013. Reserved
Self-training: A Semi-supervised Sentiment Classifi-
cation Method for Chinese Microblogs. In Proceed-
ings of IJCNLP.
R. Mihalcea. 2004. Co-training and self-training
for word sense disambiguation. In Proceedings of
CoNLL.
J. Na, H. Sui, C. Khoo, S. Chan, and Y. Zhou. 2004.
Effectiveness of simple linguistic processing in au-
tomatic sentiment classification of product reviews.
In Proceeding of the Conference of the International
Society for Knowledge Organization.
S. Orimaye, S. Alhashmi, and E. Siew. 2012. Buy it -
don’t buy it: sentiment classification on Amazon re-
views using sentence polarity shift. In Proceedings
of the Pacific Rim International Conferences on Arti-
ficial Intelligence (PRICAI).
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up?: sentiment classification using machine learning
techniques. In Proceedings of EMNLP.
L. Polanyi and A. Zaenen. 2004. Contextual lexical
valence shifters. In Proceedings of the AAAI Spring
Symposium on Exploring Attitude and Affect in Text.
Y. Ren, N. Kaji, N. Yoshinaga, M. Toyoda, and M.
Kitsuregawa. 2011. Sentiment Classification in
Resource-Scarce Languages by using Label Prop-
agation. In Proceedings of the Pacific Asia Con-
ference on Language, Information and Computation
(PACLIC).
A. Sarkar. 2001. Applying cotraining methods to sta-
tistical parsing. In Proceedings of NAACL.
R. Socher, J. Pennington, E. H. Huang, and A. Y. 2012.
Semi-Supervised Recursive Autoencoders for Pre-
dicting Sentiment Distributions. In Proceedings of
EMNLP.
Y. Su, S. Li, S. Ju, G. Zhou and J. Li. 2012. 2012.
Multi-view Learning for Semi-supervised Sentiment
Classification. In Proceedings of the International
Conference on Asian Language Processing.
</reference>
<page confidence="0.542638">
1062
</page>
<reference confidence="0.999812333333333">
X. Wan. 2009. Co-Training for Cross-Lingual Senti-
ment Classification. In Proceedings ofACL-IJCNLP.
R. Xia, T. Wang, X. Hu, S. Li, and C. Zong. 2013. Dual
Training and Dual Prediction for Polarity Classifica-
tion. In Proceedings of ACL.
Y. Yang and X. Liu. 1999. A re-examination of text
categorization methods. In Proceedings SIGIR.
S. Zhou, Q. Chen, and X. Wang. 2010. Active Deep
Networks for Semi-Supervised Sentiment Classifica-
tion. In Proceedings of COLING.
X. Zhu and Z. Ghahramani. 2002. Learning
from labeled and unlabeled data with label prop-
agation. Technical Report CMU-CALD-02-107,
Carnegie Mellon University.
X. Zhu, Z. Ghahramani, and J. Lafferty. 2003. Semi-
supervised learning using Gaussian fields and har-
monic functions. In Proceddings of the International
Conference on Machine Learning (ICML).
</reference>
<page confidence="0.720745">
1063
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.763581">
<title confidence="0.992714">Co-training for Semi-supervised Sentiment Classification Based Dual-view Bags-of-words Representation</title>
<author confidence="0.994025">Cheng Xinyu</author>
<author confidence="0.994025">Tao</author>
<affiliation confidence="0.9684095">of Computer Science, Nanjing University of Science &amp; Technology, Key Laboratory for Novel Software Technology, Nanjing University, of Computer Science, Florida International University, USA of Computer Science, Nanjing University of Posts &amp; Telecommunications,</affiliation>
<address confidence="0.899263">rxia@njust.edu.cn, wangcheng1022@gmail.com,</address>
<email confidence="0.999751">daixinyu@nju.edu.cn,taoli@cs.fiu.edu</email>
<abstract confidence="0.999221086956522">A review text is normally represented as a bag-of-words (BOW) in sentiment classification. Such a simplified BOW model has fundamental deficiencies in modeling some complex linguistic phenomena such as negation. In this work, we propose a dual-view co-training algorithm based on BOW representation for semisupervised sentiment classification. dual-view BOW, we automatically construct antonymous reviews and model a review text by a pair of bags-of-words with opposite views. We make use of the original and antonymous views in pairs, in the training, bootstrapping and testing process, all based on a joint observation of two views. The experimental results demonstrate the advantages of our approach, in meeting the two co-training requirements, addressing the negation problem, and enhancing the semi-supervised sentiment classification efficiency.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Aue</author>
<author>M Gamon</author>
</authors>
<title>Customizing Sentiment Classifiers to New Domains: A Case Study.</title>
<date>2005</date>
<booktitle>In Proceedings of Recent Advances in Natural Language Processing.</booktitle>
<contexts>
<context position="3209" citStr="Aue and Gamon (2005)" startWordPosition="460" endWordPosition="464">rk well” are considered to be very similar by most statistical learning algorithms. In supervised sentiment classification, many approaches have been proposed in addressing the negation problem (Pang et al., 2002; Na et al., 2004; Polanyi and Zaenen , 2004; Kennedy and Inkpen, 2006; Ikeda et al., 2008; Li et al., 2010b; Orimaye et al., 2012; Xia et al., 2013). Nevertheless, in semi-supervised sentiment classification, most of the current approaches directly apply standard semi-supervised learning algorithms, without paying attention to appropriate representation for review texts. For example, Aue and Gamon (2005) applied the naive Bayes EM algorithm (Nigam et al., 2000). Goldberg and Zhu (2006) applied a graph-based semi-supervised learning algorithm by (Zhu et al., 2003). Wan (2009) employed a co-training approach for cross-language sentiment classification. Li et al. (2010a) employed cotraining with personal and impersonal views. Ren et al. (2011) explored the use of label propagation (Zhu and Ghahramani, 2002). As pointed by (Goldberg and Zhu, 2006): it is necessary to investigate better review text representations and similarity measures based on linguistic knowledge, as well as reviews’ sentiment</context>
<context position="5622" citStr="Aue and Gamon (2005)" startWordPosition="815" endWordPosition="818">eview is. In comparison with traditional methods, our dual-view co-training approach has the following advantages: • Effectively address the negation problem; • Automatically learn the associations among antonyms; • Better meet the two co-training requirements in (Blum and Mitchell, 1998). 2 Related Work The mainstream of the research in sentiment classification focused on supervised and unsupervised learning tasks. In comparison, semi-supervised sentiment classification has much less related studies. In this section, we focus on reviewing the work of semi-supervised sentiment classification. Aue and Gamon (2005) combined a small amount of labeled data with a large amount of unlabeled data in target domain for cross-domain sentiment classification based on the EM algorithm. Goldberg and Zhu (2006) presented a graphbased semi-supervised learning algorithm (Zhu et al., 2003) for the sentiment analysis task of rating inference. Dasgupta and Ng (2009) proposed a semi-supervised approach to mine the unambiguous reviews at first and then exploiting them to classify the ambiguous reviews, via a combination of active learning, transductive learning and ensemble learning. Ren et al. (2011) explored the use of </context>
</contexts>
<marker>Aue, Gamon, 2005</marker>
<rawString>A. Aue and M. Gamon. 2005. Customizing Sentiment Classifiers to New Domains: A Case Study. In Proceedings of Recent Advances in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Blitzer</author>
<author>M Dredze</author>
<author>F Pereira</author>
</authors>
<title>Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL .</booktitle>
<contexts>
<context position="19861" citStr="Blitzer et al., 2007" startWordPosition="3064" endWordPosition="3067">n terms of predicting the class labels from individual views. Second, as we know, every coin has two sides and the two sides are often complementary. In our proposed approach, the original review and its antonymous review (i.e., two sides of one review) are used as two views for co-training and they can better meet the complementary condition. We will illustrate this point in Section 4.6 by calculating the KL divergence between the two views. 4 Experimental Study 4.1 Datasets and Experimental Settings We conduct the experiments on the multi-domain sentiment datasets, which were introduced in (Blitzer et al., 2007) and have been widely used in sentiment classification. It consists of four domains (Book, DVD, Electronics, and Kitchen) of reviews extracted from Amazon.com. Each of the four datasets contains 1,000 positive and 1,000 negative reviews. Following the experimental settings used in (Li et al., 2010a), we randomly separate all the reviews in each class into a labeled data set, a unlabeled data set, and a test set, with a proportion of 10%, 70% and 20%, respectively. We report the averaged results of 10-fold cross-validation in terms of classification accuracy. Note that our approach is a general</context>
</contexts>
<marker>Blitzer, Dredze, Pereira, 2007</marker>
<rawString>J. Blitzer, M. Dredze, and F. Pereira. 2007. Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification. In Proceedings of ACL .</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Blum</author>
<author>T Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training.</title>
<date>1998</date>
<booktitle>In Proceedings of COLT.</booktitle>
<contexts>
<context position="5291" citStr="Blum and Mitchell, 1998" startWordPosition="768" endWordPosition="771">ite views. Based on such a dual-view representation, we design a dual-view co-training approach. The training, bootstrapping and testing processes are all performed by observing two opposite sides of one review. That is, we consider not only how positive/negative the original review is, but also how negative/positive the antonymous review is. In comparison with traditional methods, our dual-view co-training approach has the following advantages: • Effectively address the negation problem; • Automatically learn the associations among antonyms; • Better meet the two co-training requirements in (Blum and Mitchell, 1998). 2 Related Work The mainstream of the research in sentiment classification focused on supervised and unsupervised learning tasks. In comparison, semi-supervised sentiment classification has much less related studies. In this section, we focus on reviewing the work of semi-supervised sentiment classification. Aue and Gamon (2005) combined a small amount of labeled data with a large amount of unlabeled data in target domain for cross-domain sentiment classification based on the EM algorithm. Goldberg and Zhu (2006) presented a graphbased semi-supervised learning algorithm (Zhu et al., 2003) for</context>
<context position="22949" citStr="Blum and Mitchell, 1998" startWordPosition="3556" endWordPosition="3559"> learning method proposed by Zhu and Ghahramani (2002); • Transductive SVM (T-SVM), an extension of SVM so that it can exploit unlabeled data in semi-supervised learning ( Joachims, 1999); • Self-Training, a bootstrapping model that first trains a classifier, uses it to classify the unlabeled data, and adds the most confident data to the labeled set; • Self-Reserved, a variation of self-training proposed in (Liu et al., 2013),with a reserved procedure to incorporate some less confident examples; • Co-Static, the co-training algorithm by using two static partitions of feature set as two views (Blum and Mitchell, 1998); • Co-Dynamic, a variation of co-training that uses dynamic feature space in each loop. It was reported in (Li et al., 2011) that the CoDynamic significantly outperforms Co-Static significantly; • Co-PI, another variation of co-training proposed by (Li et al., 2010a), by using personal Performance across four datasets 0 100 200300 400 500 600 700 800 900 Number of new labeled data bootstrapped from unlabeled set Figure 3: Comparsion of different boostrapping methods. and impersonal views for co-training. 4.3 Performance Comparison In table 1, we report the semi-supervised classification accur</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>A. Blum and T. Mitchell. 1998. Combining labeled and unlabeled data with co-training. In Proceedings of COLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Clark</author>
<author>J R Curran</author>
<author>M Osborne</author>
</authors>
<title>Bootstrapping POS taggers using unlabelled data.</title>
<date>2003</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<contexts>
<context position="11450" citStr="Clark et al., 2003" startWordPosition="1709" endWordPosition="1712">ous views form two different views of one review text, it is natural to employ the co-training algorithm, which requires two views for semi-supervised classification. Co-training is a typical bootstrapping algorithm that first learns a separate classifier for each view using the labeled data. The most confident predictions of each classifier on the unlabeled data are then used to construct additional labeled training data iteratively. Co-training has been extensively used in NLP, including statistical parsing (Sarkar , 2001), reference resolution (Ng and Cardie, 2003), part-of-speech tagging (Clark et al., 2003), word sense disambiguation (Mihalcea, 2004), and sentiment classification (Wan , 2009; Li et al., 2010a). But it should be noted that the dual views in our approach are different from traditional views. One important property of our approach is that two views are opposite and therefore associated with opposite class labels. Figure 2 illustrates the process of dual-view co-training. Feature Original Antonymous Space View View 1 app phone 1 didn&apos;t work well 1 1 1 1 1 0 disappointing recommend satisfactory 1 1 0 1 1 0 1 1 Bootstrapping Original View Antonymous iView Unlabeled Original Reviews Or</context>
</contexts>
<marker>Clark, Curran, Osborne, 2003</marker>
<rawString>S. Clark, J. R. Curran, and M. Osborne. 2003. Bootstrapping POS taggers using unlabelled data. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Dasgupta</author>
<author>V Ng</author>
</authors>
<title>Mine the Easy and Classify the Hard: Experiments with Automatic Sentiment Classification.</title>
<date>2009</date>
<booktitle>In Proceedings ofACL-IJCNLP.</booktitle>
<contexts>
<context position="5963" citStr="Dasgupta and Ng (2009)" startWordPosition="870" endWordPosition="873"> sentiment classification focused on supervised and unsupervised learning tasks. In comparison, semi-supervised sentiment classification has much less related studies. In this section, we focus on reviewing the work of semi-supervised sentiment classification. Aue and Gamon (2005) combined a small amount of labeled data with a large amount of unlabeled data in target domain for cross-domain sentiment classification based on the EM algorithm. Goldberg and Zhu (2006) presented a graphbased semi-supervised learning algorithm (Zhu et al., 2003) for the sentiment analysis task of rating inference. Dasgupta and Ng (2009) proposed a semi-supervised approach to mine the unambiguous reviews at first and then exploiting them to classify the ambiguous reviews, via a combination of active learning, transductive learning and ensemble learning. Ren et al. (2011) explored the use of label propagation (LP) (Zhu and Ghahramani, 2002) in building a semi-supervised sentiment classifier, and compared their results with Transductive SVMs(T-SVM). LP and T-SVM are transductive learning methods where the test data should participate in the training process. Zhou et al. (2010) proposed a deep learning approach called active dee</context>
</contexts>
<marker>Dasgupta, Ng, 2009</marker>
<rawString>S. Dasgupta and V. Ng. 2009. Mine the Easy and Classify the Hard: Experiments with Automatic Sentiment Classification. In Proceedings ofACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Goldberg</author>
<author>X Zhu</author>
</authors>
<title>Seeing stars when there aren’t many stars: graph-based semi-supervised learning for sentiment categorization.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on TextGraphs at HLT-NAACL.</booktitle>
<contexts>
<context position="3292" citStr="Goldberg and Zhu (2006)" startWordPosition="475" endWordPosition="478">ms. In supervised sentiment classification, many approaches have been proposed in addressing the negation problem (Pang et al., 2002; Na et al., 2004; Polanyi and Zaenen , 2004; Kennedy and Inkpen, 2006; Ikeda et al., 2008; Li et al., 2010b; Orimaye et al., 2012; Xia et al., 2013). Nevertheless, in semi-supervised sentiment classification, most of the current approaches directly apply standard semi-supervised learning algorithms, without paying attention to appropriate representation for review texts. For example, Aue and Gamon (2005) applied the naive Bayes EM algorithm (Nigam et al., 2000). Goldberg and Zhu (2006) applied a graph-based semi-supervised learning algorithm by (Zhu et al., 2003). Wan (2009) employed a co-training approach for cross-language sentiment classification. Li et al. (2010a) employed cotraining with personal and impersonal views. Ren et al. (2011) explored the use of label propagation (Zhu and Ghahramani, 2002). As pointed by (Goldberg and Zhu, 2006): it is necessary to investigate better review text representations and similarity measures based on linguistic knowledge, as well as reviews’ sentiment patterns. However, to the best knowledge, such investigations are very scarce in t</context>
<context position="5810" citStr="Goldberg and Zhu (2006)" startWordPosition="846" endWordPosition="849">associations among antonyms; • Better meet the two co-training requirements in (Blum and Mitchell, 1998). 2 Related Work The mainstream of the research in sentiment classification focused on supervised and unsupervised learning tasks. In comparison, semi-supervised sentiment classification has much less related studies. In this section, we focus on reviewing the work of semi-supervised sentiment classification. Aue and Gamon (2005) combined a small amount of labeled data with a large amount of unlabeled data in target domain for cross-domain sentiment classification based on the EM algorithm. Goldberg and Zhu (2006) presented a graphbased semi-supervised learning algorithm (Zhu et al., 2003) for the sentiment analysis task of rating inference. Dasgupta and Ng (2009) proposed a semi-supervised approach to mine the unambiguous reviews at first and then exploiting them to classify the ambiguous reviews, via a combination of active learning, transductive learning and ensemble learning. Ren et al. (2011) explored the use of label propagation (LP) (Zhu and Ghahramani, 2002) in building a semi-supervised sentiment classifier, and compared their results with Transductive SVMs(T-SVM). LP and T-SVM are transductiv</context>
</contexts>
<marker>Goldberg, Zhu, 2006</marker>
<rawString>A. Goldberg and X. Zhu. 2006. Seeing stars when there aren’t many stars: graph-based semi-supervised learning for sentiment categorization. In Proceedings of the Workshop on TextGraphs at HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hu</author>
<author>B Liu</author>
</authors>
<title>Mining opinion features in customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the National Conference on Artificial Intelligence (AAAI).</booktitle>
<marker>Hu, Liu, 2004</marker>
<rawString>M. Hu and B. Liu. 2004. Mining opinion features in customer reviews. In Proceedings of the National Conference on Artificial Intelligence (AAAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Ikeda</author>
<author>H Takamura</author>
<author>L Ratinov</author>
<author>M Okumura</author>
</authors>
<title>Learning to Shift the Polarity of Words for Sentiment Classification.</title>
<date>2008</date>
<booktitle>In Proceedings of IJCNLP.</booktitle>
<contexts>
<context position="2891" citStr="Ikeda et al., 2008" startWordPosition="413" endWordPosition="416">supervised sentiment classification is known as the bag-of-words (BOW) model, which is difficult to meet the requirements for understanding the review text and dealing with complex linguistic structures such as negation. For example, the BOW representations of two opposite reviews “It works well” and “It doesn’t work well” are considered to be very similar by most statistical learning algorithms. In supervised sentiment classification, many approaches have been proposed in addressing the negation problem (Pang et al., 2002; Na et al., 2004; Polanyi and Zaenen , 2004; Kennedy and Inkpen, 2006; Ikeda et al., 2008; Li et al., 2010b; Orimaye et al., 2012; Xia et al., 2013). Nevertheless, in semi-supervised sentiment classification, most of the current approaches directly apply standard semi-supervised learning algorithms, without paying attention to appropriate representation for review texts. For example, Aue and Gamon (2005) applied the naive Bayes EM algorithm (Nigam et al., 2000). Goldberg and Zhu (2006) applied a graph-based semi-supervised learning algorithm by (Zhu et al., 2003). Wan (2009) employed a co-training approach for cross-language sentiment classification. Li et al. (2010a) employed cot</context>
</contexts>
<marker>Ikeda, Takamura, Ratinov, Okumura, 2008</marker>
<rawString>D. Ikeda, H. Takamura, L. Ratinov, and M. Okumura. 2008. Learning to Shift the Polarity of Words for Sentiment Classification. In Proceedings of IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Transductive Inference for Text Classification using Support Vector Machines.</title>
<date>1999</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="22512" citStr="Joachims, 1999" startWordPosition="3490" endWordPosition="3491">d classification accuracy of ten systems. tween two systems, with a default significant level of 0.05. 4.2 Compared Systems We implement the following nine systems and compare them with our approach: • Baseline, the supervised baseline trained with the initial labeled data only; • Expectation Maximization (EM), with the naive Bayes model proposed by Nigam et al. (2000); • Label Propagation (LP), a graph-based semi-supervised learning method proposed by Zhu and Ghahramani (2002); • Transductive SVM (T-SVM), an extension of SVM so that it can exploit unlabeled data in semi-supervised learning ( Joachims, 1999); • Self-Training, a bootstrapping model that first trains a classifier, uses it to classify the unlabeled data, and adds the most confident data to the labeled set; • Self-Reserved, a variation of self-training proposed in (Liu et al., 2013),with a reserved procedure to incorporate some less confident examples; • Co-Static, the co-training algorithm by using two static partitions of feature set as two views (Blum and Mitchell, 1998); • Co-Dynamic, a variation of co-training that uses dynamic feature space in each loop. It was reported in (Li et al., 2011) that the CoDynamic significantly outp</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>T. Joachims. 1999. Transductive Inference for Text Classification using Support Vector Machines. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kennedy</author>
<author>D Inkpen</author>
</authors>
<title>Sentiment classification of movie reviews using contextual valence shifters.</title>
<date>2006</date>
<journal>Computational Intelligence,</journal>
<pages>22--110</pages>
<contexts>
<context position="2871" citStr="Kennedy and Inkpen, 2006" startWordPosition="409" endWordPosition="412"> both supervised and semi-supervised sentiment classification is known as the bag-of-words (BOW) model, which is difficult to meet the requirements for understanding the review text and dealing with complex linguistic structures such as negation. For example, the BOW representations of two opposite reviews “It works well” and “It doesn’t work well” are considered to be very similar by most statistical learning algorithms. In supervised sentiment classification, many approaches have been proposed in addressing the negation problem (Pang et al., 2002; Na et al., 2004; Polanyi and Zaenen , 2004; Kennedy and Inkpen, 2006; Ikeda et al., 2008; Li et al., 2010b; Orimaye et al., 2012; Xia et al., 2013). Nevertheless, in semi-supervised sentiment classification, most of the current approaches directly apply standard semi-supervised learning algorithms, without paying attention to appropriate representation for review texts. For example, Aue and Gamon (2005) applied the naive Bayes EM algorithm (Nigam et al., 2000). Goldberg and Zhu (2006) applied a graph-based semi-supervised learning algorithm by (Zhu et al., 2003). Wan (2009) employed a co-training approach for cross-language sentiment classification. Li et al. </context>
</contexts>
<marker>Kennedy, Inkpen, 2006</marker>
<rawString>A. Kennedy and D. Inkpen. 2006. Sentiment classification of movie reviews using contextual valence shifters. Computational Intelligence, 22:110–125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>LaTeX</author>
</authors>
<title>Error: File ‘url.sty’ not</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<marker>LaTeX, 2003</marker>
<rawString>LaTeX Error: File ‘url.sty’ not found. V. Ng and C. Cardie. 2003. Weakly supervised natural language learning without redundant views. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Nigam</author>
<author>A McCallum</author>
<author>S Thrun</author>
<author>T Mitchell</author>
</authors>
<title>Text classification from labeled and unlabeled documents using EM.</title>
<date>2000</date>
<booktitle>Machine Learning,</booktitle>
<volume>39</volume>
<issue>2</issue>
<pages>103--134</pages>
<contexts>
<context position="3267" citStr="Nigam et al., 2000" startWordPosition="471" endWordPosition="474">cal learning algorithms. In supervised sentiment classification, many approaches have been proposed in addressing the negation problem (Pang et al., 2002; Na et al., 2004; Polanyi and Zaenen , 2004; Kennedy and Inkpen, 2006; Ikeda et al., 2008; Li et al., 2010b; Orimaye et al., 2012; Xia et al., 2013). Nevertheless, in semi-supervised sentiment classification, most of the current approaches directly apply standard semi-supervised learning algorithms, without paying attention to appropriate representation for review texts. For example, Aue and Gamon (2005) applied the naive Bayes EM algorithm (Nigam et al., 2000). Goldberg and Zhu (2006) applied a graph-based semi-supervised learning algorithm by (Zhu et al., 2003). Wan (2009) employed a co-training approach for cross-language sentiment classification. Li et al. (2010a) employed cotraining with personal and impersonal views. Ren et al. (2011) explored the use of label propagation (Zhu and Ghahramani, 2002). As pointed by (Goldberg and Zhu, 2006): it is necessary to investigate better review text representations and similarity measures based on linguistic knowledge, as well as reviews’ sentiment patterns. However, to the best knowledge, such investigat</context>
<context position="22268" citStr="Nigam et al. (2000)" startWordPosition="3452" endWordPosition="3455">.736 0.751 0.720 Self-Reserved 0.690 0.708 0.735 0.754 0.722 Co-Static 0.696 0.714 0.745 0.762 0.729 Co-Dynamic 0.701 0.725 0.756 0.767 0.737 Co-PI 0.702 0.716 0.746 0.769 0.733 Our approach 0.721 0.738 0.769 0.780 0.752 Table 1: The semi-supervised classification accuracy of ten systems. tween two systems, with a default significant level of 0.05. 4.2 Compared Systems We implement the following nine systems and compare them with our approach: • Baseline, the supervised baseline trained with the initial labeled data only; • Expectation Maximization (EM), with the naive Bayes model proposed by Nigam et al. (2000); • Label Propagation (LP), a graph-based semi-supervised learning method proposed by Zhu and Ghahramani (2002); • Transductive SVM (T-SVM), an extension of SVM so that it can exploit unlabeled data in semi-supervised learning ( Joachims, 1999); • Self-Training, a bootstrapping model that first trains a classifier, uses it to classify the unlabeled data, and adds the most confident data to the labeled set; • Self-Reserved, a variation of self-training proposed in (Liu et al., 2013),with a reserved procedure to incorporate some less confident examples; • Co-Static, the co-training algorithm by </context>
</contexts>
<marker>Nigam, McCallum, Thrun, Mitchell, 2000</marker>
<rawString>K. Nigam, A. McCallum, S. Thrun, and T. Mitchell. 2000. Text classification from labeled and unlabeled documents using EM. Machine Learning, 39(2/3): 103–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Li</author>
<author>C Huang</author>
<author>G Zhou</author>
<author>S Y M Lee</author>
</authors>
<title>Employing personal/impersonal views in supervised and semi-supervised sentiment classification.</title>
<date>2010</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL) .</booktitle>
<contexts>
<context position="2908" citStr="Li et al., 2010" startWordPosition="417" endWordPosition="420"> classification is known as the bag-of-words (BOW) model, which is difficult to meet the requirements for understanding the review text and dealing with complex linguistic structures such as negation. For example, the BOW representations of two opposite reviews “It works well” and “It doesn’t work well” are considered to be very similar by most statistical learning algorithms. In supervised sentiment classification, many approaches have been proposed in addressing the negation problem (Pang et al., 2002; Na et al., 2004; Polanyi and Zaenen , 2004; Kennedy and Inkpen, 2006; Ikeda et al., 2008; Li et al., 2010b; Orimaye et al., 2012; Xia et al., 2013). Nevertheless, in semi-supervised sentiment classification, most of the current approaches directly apply standard semi-supervised learning algorithms, without paying attention to appropriate representation for review texts. For example, Aue and Gamon (2005) applied the naive Bayes EM algorithm (Nigam et al., 2000). Goldberg and Zhu (2006) applied a graph-based semi-supervised learning algorithm by (Zhu et al., 2003). Wan (2009) employed a co-training approach for cross-language sentiment classification. Li et al. (2010a) employed cotraining with pers</context>
<context position="7489" citStr="Li et al. (2010" startWordPosition="1096" endWordPosition="1099"> on a considerable amount of unlabeled data to learn the representations and the inability to explicitly model the negation problem. One line of semi-supervised learning research is to bootstrap class labels using techniques like self-training, co-training and their variations. Wan (2009) proposed a co-training approach to address the cross-lingual sentiment classification problem. They made use of the machine translation service to produce two views (a English view and a Chinese view) for co-training a Chinese review sentiment classifier, based on English corpus and unlabeled Chinese corpus. Li et al. (2010a) proposed an unsupervised method at first to automatically separate the review text into a personal view and an impersonal view, based on which the standard cotraining algorithm is then applied to build a semisupervised sentiment classifier. Li et al. (2011) further studied semi-supervised learning for imbalanced sentiment classification by using a dynamic co-training approach. Su et al. (2012) proposed a multi-view learning approach to semi-supervised sentiment classification with both feature partition and language translation strategies (Wan , 2009). Following (Li et al., 2010a), Li (2013</context>
<context position="11553" citStr="Li et al., 2010" startWordPosition="1725" endWordPosition="1728">hich requires two views for semi-supervised classification. Co-training is a typical bootstrapping algorithm that first learns a separate classifier for each view using the labeled data. The most confident predictions of each classifier on the unlabeled data are then used to construct additional labeled training data iteratively. Co-training has been extensively used in NLP, including statistical parsing (Sarkar , 2001), reference resolution (Ng and Cardie, 2003), part-of-speech tagging (Clark et al., 2003), word sense disambiguation (Mihalcea, 2004), and sentiment classification (Wan , 2009; Li et al., 2010a). But it should be noted that the dual views in our approach are different from traditional views. One important property of our approach is that two views are opposite and therefore associated with opposite class labels. Figure 2 illustrates the process of dual-view co-training. Feature Original Antonymous Space View View 1 app phone 1 didn&apos;t work well 1 1 1 1 1 0 disappointing recommend satisfactory 1 1 0 1 1 0 1 1 Bootstrapping Original View Antonymous iView Unlabeled Original Reviews Original Sentiment Classifier Labeled Original Reviews Dual-view Sentiment Consensus Dual Sentiment Class</context>
<context position="20159" citStr="Li et al., 2010" startWordPosition="3109" endWordPosition="3112"> better meet the complementary condition. We will illustrate this point in Section 4.6 by calculating the KL divergence between the two views. 4 Experimental Study 4.1 Datasets and Experimental Settings We conduct the experiments on the multi-domain sentiment datasets, which were introduced in (Blitzer et al., 2007) and have been widely used in sentiment classification. It consists of four domains (Book, DVD, Electronics, and Kitchen) of reviews extracted from Amazon.com. Each of the four datasets contains 1,000 positive and 1,000 negative reviews. Following the experimental settings used in (Li et al., 2010a), we randomly separate all the reviews in each class into a labeled data set, a unlabeled data set, and a test set, with a proportion of 10%, 70% and 20%, respectively. We report the averaged results of 10-fold cross-validation in terms of classification accuracy. Note that our approach is a general framework that allows different classification algorithms. Due to the space limitation, we only report the results by using logistic regression3. Note the similar conclusions can be obtained by using the other algorithms such as SVMs and naive Bayes. The LibLinear toolkit4 is utilized, with a dua</context>
<context position="23215" citStr="Li et al., 2010" startWordPosition="3599" endWordPosition="3602">sify the unlabeled data, and adds the most confident data to the labeled set; • Self-Reserved, a variation of self-training proposed in (Liu et al., 2013),with a reserved procedure to incorporate some less confident examples; • Co-Static, the co-training algorithm by using two static partitions of feature set as two views (Blum and Mitchell, 1998); • Co-Dynamic, a variation of co-training that uses dynamic feature space in each loop. It was reported in (Li et al., 2011) that the CoDynamic significantly outperforms Co-Static significantly; • Co-PI, another variation of co-training proposed by (Li et al., 2010a), by using personal Performance across four datasets 0 100 200300 400 500 600 700 800 900 Number of new labeled data bootstrapped from unlabeled set Figure 3: Comparsion of different boostrapping methods. and impersonal views for co-training. 4.3 Performance Comparison In table 1, we report the semi-supervised classification accuracy of ten evaluated systems. We report the results with 200 labeled, 1400 unlabeled and 400 test reviews. Note that the similar conclusions can be obtained when the size of the initial labeled data changes. We will discuss its influence later. As can be seen, train</context>
</contexts>
<marker>Li, Huang, Zhou, Lee, 2010</marker>
<rawString>S. Li, C. Huang, G. Zhou, and S. Y. M. Lee. 2010a. Employing personal/impersonal views in supervised and semi-supervised sentiment classification. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL) .</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Li</author>
<author>S Lee</author>
<author>Y Chen</author>
<author>C Huang</author>
<author>G Zhou</author>
</authors>
<title>Sentiment Classification and Polarity Shifting.</title>
<date>2010</date>
<booktitle>In Proceeding of the International Conference on Computational Linguistics (COLING).</booktitle>
<contexts>
<context position="2908" citStr="Li et al., 2010" startWordPosition="417" endWordPosition="420"> classification is known as the bag-of-words (BOW) model, which is difficult to meet the requirements for understanding the review text and dealing with complex linguistic structures such as negation. For example, the BOW representations of two opposite reviews “It works well” and “It doesn’t work well” are considered to be very similar by most statistical learning algorithms. In supervised sentiment classification, many approaches have been proposed in addressing the negation problem (Pang et al., 2002; Na et al., 2004; Polanyi and Zaenen , 2004; Kennedy and Inkpen, 2006; Ikeda et al., 2008; Li et al., 2010b; Orimaye et al., 2012; Xia et al., 2013). Nevertheless, in semi-supervised sentiment classification, most of the current approaches directly apply standard semi-supervised learning algorithms, without paying attention to appropriate representation for review texts. For example, Aue and Gamon (2005) applied the naive Bayes EM algorithm (Nigam et al., 2000). Goldberg and Zhu (2006) applied a graph-based semi-supervised learning algorithm by (Zhu et al., 2003). Wan (2009) employed a co-training approach for cross-language sentiment classification. Li et al. (2010a) employed cotraining with pers</context>
<context position="7489" citStr="Li et al. (2010" startWordPosition="1096" endWordPosition="1099"> on a considerable amount of unlabeled data to learn the representations and the inability to explicitly model the negation problem. One line of semi-supervised learning research is to bootstrap class labels using techniques like self-training, co-training and their variations. Wan (2009) proposed a co-training approach to address the cross-lingual sentiment classification problem. They made use of the machine translation service to produce two views (a English view and a Chinese view) for co-training a Chinese review sentiment classifier, based on English corpus and unlabeled Chinese corpus. Li et al. (2010a) proposed an unsupervised method at first to automatically separate the review text into a personal view and an impersonal view, based on which the standard cotraining algorithm is then applied to build a semisupervised sentiment classifier. Li et al. (2011) further studied semi-supervised learning for imbalanced sentiment classification by using a dynamic co-training approach. Su et al. (2012) proposed a multi-view learning approach to semi-supervised sentiment classification with both feature partition and language translation strategies (Wan , 2009). Following (Li et al., 2010a), Li (2013</context>
<context position="11553" citStr="Li et al., 2010" startWordPosition="1725" endWordPosition="1728">hich requires two views for semi-supervised classification. Co-training is a typical bootstrapping algorithm that first learns a separate classifier for each view using the labeled data. The most confident predictions of each classifier on the unlabeled data are then used to construct additional labeled training data iteratively. Co-training has been extensively used in NLP, including statistical parsing (Sarkar , 2001), reference resolution (Ng and Cardie, 2003), part-of-speech tagging (Clark et al., 2003), word sense disambiguation (Mihalcea, 2004), and sentiment classification (Wan , 2009; Li et al., 2010a). But it should be noted that the dual views in our approach are different from traditional views. One important property of our approach is that two views are opposite and therefore associated with opposite class labels. Figure 2 illustrates the process of dual-view co-training. Feature Original Antonymous Space View View 1 app phone 1 didn&apos;t work well 1 1 1 1 1 0 disappointing recommend satisfactory 1 1 0 1 1 0 1 1 Bootstrapping Original View Antonymous iView Unlabeled Original Reviews Original Sentiment Classifier Labeled Original Reviews Dual-view Sentiment Consensus Dual Sentiment Class</context>
<context position="20159" citStr="Li et al., 2010" startWordPosition="3109" endWordPosition="3112"> better meet the complementary condition. We will illustrate this point in Section 4.6 by calculating the KL divergence between the two views. 4 Experimental Study 4.1 Datasets and Experimental Settings We conduct the experiments on the multi-domain sentiment datasets, which were introduced in (Blitzer et al., 2007) and have been widely used in sentiment classification. It consists of four domains (Book, DVD, Electronics, and Kitchen) of reviews extracted from Amazon.com. Each of the four datasets contains 1,000 positive and 1,000 negative reviews. Following the experimental settings used in (Li et al., 2010a), we randomly separate all the reviews in each class into a labeled data set, a unlabeled data set, and a test set, with a proportion of 10%, 70% and 20%, respectively. We report the averaged results of 10-fold cross-validation in terms of classification accuracy. Note that our approach is a general framework that allows different classification algorithms. Due to the space limitation, we only report the results by using logistic regression3. Note the similar conclusions can be obtained by using the other algorithms such as SVMs and naive Bayes. The LibLinear toolkit4 is utilized, with a dua</context>
<context position="23215" citStr="Li et al., 2010" startWordPosition="3599" endWordPosition="3602">sify the unlabeled data, and adds the most confident data to the labeled set; • Self-Reserved, a variation of self-training proposed in (Liu et al., 2013),with a reserved procedure to incorporate some less confident examples; • Co-Static, the co-training algorithm by using two static partitions of feature set as two views (Blum and Mitchell, 1998); • Co-Dynamic, a variation of co-training that uses dynamic feature space in each loop. It was reported in (Li et al., 2011) that the CoDynamic significantly outperforms Co-Static significantly; • Co-PI, another variation of co-training proposed by (Li et al., 2010a), by using personal Performance across four datasets 0 100 200300 400 500 600 700 800 900 Number of new labeled data bootstrapped from unlabeled set Figure 3: Comparsion of different boostrapping methods. and impersonal views for co-training. 4.3 Performance Comparison In table 1, we report the semi-supervised classification accuracy of ten evaluated systems. We report the results with 200 labeled, 1400 unlabeled and 400 test reviews. Note that the similar conclusions can be obtained when the size of the initial labeled data changes. We will discuss its influence later. As can be seen, train</context>
</contexts>
<marker>Li, Lee, Chen, Huang, Zhou, 2010</marker>
<rawString>S. Li, S. Lee, Y. Chen, C. Huang, and G. Zhou. 2010b. Sentiment Classification and Polarity Shifting. In Proceeding of the International Conference on Computational Linguistics (COLING).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Li</author>
<author>Z Wang</author>
<author>G Zhou</author>
<author>S Lee</author>
</authors>
<title>SemiSupervised Learning for Imbalanced Sentiment Classification.</title>
<date>2011</date>
<booktitle>In Proceedings of IJCAI.</booktitle>
<contexts>
<context position="7749" citStr="Li et al. (2011)" startWordPosition="1138" endWordPosition="1141">d their variations. Wan (2009) proposed a co-training approach to address the cross-lingual sentiment classification problem. They made use of the machine translation service to produce two views (a English view and a Chinese view) for co-training a Chinese review sentiment classifier, based on English corpus and unlabeled Chinese corpus. Li et al. (2010a) proposed an unsupervised method at first to automatically separate the review text into a personal view and an impersonal view, based on which the standard cotraining algorithm is then applied to build a semisupervised sentiment classifier. Li et al. (2011) further studied semi-supervised learning for imbalanced sentiment classification by using a dynamic co-training approach. Su et al. (2012) proposed a multi-view learning approach to semi-supervised sentiment classification with both feature partition and language translation strategies (Wan , 2009). Following (Li et al., 2010a), Li (2013) proposed a co-training approach which exploits subjective and objective views for semi-supervised sentiment classification. Our approach can also be viewed as a variation of co-training. The innovation of our approach is the dual-view construction technique </context>
<context position="23074" citStr="Li et al., 2011" startWordPosition="3578" endWordPosition="3581">led data in semi-supervised learning ( Joachims, 1999); • Self-Training, a bootstrapping model that first trains a classifier, uses it to classify the unlabeled data, and adds the most confident data to the labeled set; • Self-Reserved, a variation of self-training proposed in (Liu et al., 2013),with a reserved procedure to incorporate some less confident examples; • Co-Static, the co-training algorithm by using two static partitions of feature set as two views (Blum and Mitchell, 1998); • Co-Dynamic, a variation of co-training that uses dynamic feature space in each loop. It was reported in (Li et al., 2011) that the CoDynamic significantly outperforms Co-Static significantly; • Co-PI, another variation of co-training proposed by (Li et al., 2010a), by using personal Performance across four datasets 0 100 200300 400 500 600 700 800 900 Number of new labeled data bootstrapped from unlabeled set Figure 3: Comparsion of different boostrapping methods. and impersonal views for co-training. 4.3 Performance Comparison In table 1, we report the semi-supervised classification accuracy of ten evaluated systems. We report the results with 200 labeled, 1400 unlabeled and 400 test reviews. Note that the simi</context>
</contexts>
<marker>Li, Wang, Zhou, Lee, 2011</marker>
<rawString>S. Li, Z. Wang, G. Zhou, and S. Lee. 2011. SemiSupervised Learning for Imbalanced Sentiment Classification. In Proceedings of IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Li</author>
</authors>
<title>Sentiment classification using subjective and objective views.</title>
<date>2013</date>
<journal>International Journal of Computer Applications,</journal>
<volume>80</volume>
<issue>7</issue>
<pages>30--34</pages>
<contexts>
<context position="8090" citStr="Li (2013)" startWordPosition="1186" endWordPosition="1187">l. (2010a) proposed an unsupervised method at first to automatically separate the review text into a personal view and an impersonal view, based on which the standard cotraining algorithm is then applied to build a semisupervised sentiment classifier. Li et al. (2011) further studied semi-supervised learning for imbalanced sentiment classification by using a dynamic co-training approach. Su et al. (2012) proposed a multi-view learning approach to semi-supervised sentiment classification with both feature partition and language translation strategies (Wan , 2009). Following (Li et al., 2010a), Li (2013) proposed a co-training approach which exploits subjective and objective views for semi-supervised sentiment classification. Our approach can also be viewed as a variation of co-training. The innovation of our approach is the dual-view construction technique by incorporating antonymous reviews and the bootstrapping mechanism by observing two opposite sides of one review. 3 The Proposed Approach 3.1 Dual-view BOW Representation for Review Texts Every coin has two sizes. In this work, we are motivated to automatically construct the antonymous reviews, consider the original and antonymous reviews</context>
</contexts>
<marker>Li, 2013</marker>
<rawString>S. Li. 2013. Sentiment classification using subjective and objective views. International Journal of Computer Applications, 80(7): 30–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Liu</author>
<author>X Dong</author>
<author>Y Guan</author>
<author>J Yang</author>
</authors>
<title>Reserved Self-training: A Semi-supervised Sentiment Classification Method for Chinese Microblogs.</title>
<date>2013</date>
<booktitle>In Proceedings of IJCNLP.</booktitle>
<contexts>
<context position="22754" citStr="Liu et al., 2013" startWordPosition="3527" endWordPosition="3530">ned with the initial labeled data only; • Expectation Maximization (EM), with the naive Bayes model proposed by Nigam et al. (2000); • Label Propagation (LP), a graph-based semi-supervised learning method proposed by Zhu and Ghahramani (2002); • Transductive SVM (T-SVM), an extension of SVM so that it can exploit unlabeled data in semi-supervised learning ( Joachims, 1999); • Self-Training, a bootstrapping model that first trains a classifier, uses it to classify the unlabeled data, and adds the most confident data to the labeled set; • Self-Reserved, a variation of self-training proposed in (Liu et al., 2013),with a reserved procedure to incorporate some less confident examples; • Co-Static, the co-training algorithm by using two static partitions of feature set as two views (Blum and Mitchell, 1998); • Co-Dynamic, a variation of co-training that uses dynamic feature space in each loop. It was reported in (Li et al., 2011) that the CoDynamic significantly outperforms Co-Static significantly; • Co-PI, another variation of co-training proposed by (Li et al., 2010a), by using personal Performance across four datasets 0 100 200300 400 500 600 700 800 900 Number of new labeled data bootstrapped from un</context>
</contexts>
<marker>Liu, Dong, Guan, Yang, 2013</marker>
<rawString>Z. Liu, X. Dong, Y. Guan, and J. Yang. 2013. Reserved Self-training: A Semi-supervised Sentiment Classification Method for Chinese Microblogs. In Proceedings of IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
</authors>
<title>Co-training and self-training for word sense disambiguation.</title>
<date>2004</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<contexts>
<context position="11494" citStr="Mihalcea, 2004" startWordPosition="1716" endWordPosition="1717"> text, it is natural to employ the co-training algorithm, which requires two views for semi-supervised classification. Co-training is a typical bootstrapping algorithm that first learns a separate classifier for each view using the labeled data. The most confident predictions of each classifier on the unlabeled data are then used to construct additional labeled training data iteratively. Co-training has been extensively used in NLP, including statistical parsing (Sarkar , 2001), reference resolution (Ng and Cardie, 2003), part-of-speech tagging (Clark et al., 2003), word sense disambiguation (Mihalcea, 2004), and sentiment classification (Wan , 2009; Li et al., 2010a). But it should be noted that the dual views in our approach are different from traditional views. One important property of our approach is that two views are opposite and therefore associated with opposite class labels. Figure 2 illustrates the process of dual-view co-training. Feature Original Antonymous Space View View 1 app phone 1 didn&apos;t work well 1 1 1 1 1 0 disappointing recommend satisfactory 1 1 0 1 1 0 1 1 Bootstrapping Original View Antonymous iView Unlabeled Original Reviews Original Sentiment Classifier Labeled Original</context>
</contexts>
<marker>Mihalcea, 2004</marker>
<rawString>R. Mihalcea. 2004. Co-training and self-training for word sense disambiguation. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Na</author>
<author>H Sui</author>
<author>C Khoo</author>
<author>S Chan</author>
<author>Y Zhou</author>
</authors>
<title>Effectiveness of simple linguistic processing in automatic sentiment classification of product reviews.</title>
<date>2004</date>
<booktitle>In Proceeding of the Conference of the International Society for Knowledge Organization.</booktitle>
<contexts>
<context position="2818" citStr="Na et al., 2004" startWordPosition="400" endWordPosition="403">The dominating text representation method in both supervised and semi-supervised sentiment classification is known as the bag-of-words (BOW) model, which is difficult to meet the requirements for understanding the review text and dealing with complex linguistic structures such as negation. For example, the BOW representations of two opposite reviews “It works well” and “It doesn’t work well” are considered to be very similar by most statistical learning algorithms. In supervised sentiment classification, many approaches have been proposed in addressing the negation problem (Pang et al., 2002; Na et al., 2004; Polanyi and Zaenen , 2004; Kennedy and Inkpen, 2006; Ikeda et al., 2008; Li et al., 2010b; Orimaye et al., 2012; Xia et al., 2013). Nevertheless, in semi-supervised sentiment classification, most of the current approaches directly apply standard semi-supervised learning algorithms, without paying attention to appropriate representation for review texts. For example, Aue and Gamon (2005) applied the naive Bayes EM algorithm (Nigam et al., 2000). Goldberg and Zhu (2006) applied a graph-based semi-supervised learning algorithm by (Zhu et al., 2003). Wan (2009) employed a co-training approach fo</context>
</contexts>
<marker>Na, Sui, Khoo, Chan, Zhou, 2004</marker>
<rawString>J. Na, H. Sui, C. Khoo, S. Chan, and Y. Zhou. 2004. Effectiveness of simple linguistic processing in automatic sentiment classification of product reviews. In Proceeding of the Conference of the International Society for Knowledge Organization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Orimaye</author>
<author>S Alhashmi</author>
<author>E Siew</author>
</authors>
<title>Buy it -don’t buy it: sentiment classification on Amazon reviews using sentence polarity shift.</title>
<date>2012</date>
<booktitle>In Proceedings of the Pacific Rim International Conferences on Artificial Intelligence (PRICAI).</booktitle>
<contexts>
<context position="2931" citStr="Orimaye et al., 2012" startWordPosition="421" endWordPosition="424"> known as the bag-of-words (BOW) model, which is difficult to meet the requirements for understanding the review text and dealing with complex linguistic structures such as negation. For example, the BOW representations of two opposite reviews “It works well” and “It doesn’t work well” are considered to be very similar by most statistical learning algorithms. In supervised sentiment classification, many approaches have been proposed in addressing the negation problem (Pang et al., 2002; Na et al., 2004; Polanyi and Zaenen , 2004; Kennedy and Inkpen, 2006; Ikeda et al., 2008; Li et al., 2010b; Orimaye et al., 2012; Xia et al., 2013). Nevertheless, in semi-supervised sentiment classification, most of the current approaches directly apply standard semi-supervised learning algorithms, without paying attention to appropriate representation for review texts. For example, Aue and Gamon (2005) applied the naive Bayes EM algorithm (Nigam et al., 2000). Goldberg and Zhu (2006) applied a graph-based semi-supervised learning algorithm by (Zhu et al., 2003). Wan (2009) employed a co-training approach for cross-language sentiment classification. Li et al. (2010a) employed cotraining with personal and impersonal vie</context>
</contexts>
<marker>Orimaye, Alhashmi, Siew, 2012</marker>
<rawString>S. Orimaye, S. Alhashmi, and E. Siew. 2012. Buy it -don’t buy it: sentiment classification on Amazon reviews using sentence polarity shift. In Proceedings of the Pacific Rim International Conferences on Artificial Intelligence (PRICAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
<author>S Vaithyanathan</author>
</authors>
<title>Thumbs up?: sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="2801" citStr="Pang et al., 2002" startWordPosition="396" endWordPosition="399">unlabeled reviews. The dominating text representation method in both supervised and semi-supervised sentiment classification is known as the bag-of-words (BOW) model, which is difficult to meet the requirements for understanding the review text and dealing with complex linguistic structures such as negation. For example, the BOW representations of two opposite reviews “It works well” and “It doesn’t work well” are considered to be very similar by most statistical learning algorithms. In supervised sentiment classification, many approaches have been proposed in addressing the negation problem (Pang et al., 2002; Na et al., 2004; Polanyi and Zaenen , 2004; Kennedy and Inkpen, 2006; Ikeda et al., 2008; Li et al., 2010b; Orimaye et al., 2012; Xia et al., 2013). Nevertheless, in semi-supervised sentiment classification, most of the current approaches directly apply standard semi-supervised learning algorithms, without paying attention to appropriate representation for review texts. For example, Aue and Gamon (2005) applied the naive Bayes EM algorithm (Nigam et al., 2000). Goldberg and Zhu (2006) applied a graph-based semi-supervised learning algorithm by (Zhu et al., 2003). Wan (2009) employed a co-tra</context>
<context position="21032" citStr="Pang et al., 2002" startWordPosition="3259" endWordPosition="3262">n accuracy. Note that our approach is a general framework that allows different classification algorithms. Due to the space limitation, we only report the results by using logistic regression3. Note the similar conclusions can be obtained by using the other algorithms such as SVMs and naive Bayes. The LibLinear toolkit4 is utilized, with a dual L2-regularized factor, and a default tradeoff parameter c. Similar to (Wan , 2009; Li et al., 2010a), we carry out the experiments with the unigram features without feature selection. Presence is used as the term weighting scheme as it was reported in (Pang et al., 2002) that it performed better than TF and TF-IDF. Finally, the paired t-test (Yang and Liu, 1999) is performed to test the significance of the difference be3Logistic regression is quite similar to Maximum Entropy, and has been proved to be more efficient in sentiment classification than some other classification algorithms including naive Bayes and SVMs (Pang et al., 2002). 4http://www.csie.ntu.edu.tw/˜cjlin/ liblinear/ 1058 BOOK DVD ELEC KITC Avg. Baseline 0.680 0.691 0.726 0.740 0.709 LP 0.681 0.676 0.697 0.722 0.694 T-SVM 0.671 0.677 0.716 0.729 0.698 EM 0.702 0.706 0.758 0.744 0.728 Self-Train</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs up?: sentiment classification using machine learning techniques. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Polanyi</author>
<author>A Zaenen</author>
</authors>
<title>Contextual lexical valence shifters.</title>
<date>2004</date>
<booktitle>In Proceedings of the AAAI Spring Symposium on Exploring Attitude and Affect in Text.</booktitle>
<marker>Polanyi, Zaenen, 2004</marker>
<rawString>L. Polanyi and A. Zaenen. 2004. Contextual lexical valence shifters. In Proceedings of the AAAI Spring Symposium on Exploring Attitude and Affect in Text.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Ren</author>
<author>N Kaji</author>
<author>N Yoshinaga</author>
<author>M Toyoda</author>
<author>M Kitsuregawa</author>
</authors>
<title>Sentiment Classification in Resource-Scarce Languages by using Label Propagation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Pacific Asia Conference on Language, Information and Computation (PACLIC).</booktitle>
<contexts>
<context position="3552" citStr="Ren et al. (2011)" startWordPosition="515" endWordPosition="518">a et al., 2013). Nevertheless, in semi-supervised sentiment classification, most of the current approaches directly apply standard semi-supervised learning algorithms, without paying attention to appropriate representation for review texts. For example, Aue and Gamon (2005) applied the naive Bayes EM algorithm (Nigam et al., 2000). Goldberg and Zhu (2006) applied a graph-based semi-supervised learning algorithm by (Zhu et al., 2003). Wan (2009) employed a co-training approach for cross-language sentiment classification. Li et al. (2010a) employed cotraining with personal and impersonal views. Ren et al. (2011) explored the use of label propagation (Zhu and Ghahramani, 2002). As pointed by (Goldberg and Zhu, 2006): it is necessary to investigate better review text representations and similarity measures based on linguistic knowledge, as well as reviews’ sentiment patterns. However, to the best knowledge, such investigations are very scarce in the research of semi1054 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1054–1063, Beijing, China, July 26-31, 2015. c�2015 Association for</context>
<context position="6201" citStr="Ren et al. (2011)" startWordPosition="907" endWordPosition="910">t classification. Aue and Gamon (2005) combined a small amount of labeled data with a large amount of unlabeled data in target domain for cross-domain sentiment classification based on the EM algorithm. Goldberg and Zhu (2006) presented a graphbased semi-supervised learning algorithm (Zhu et al., 2003) for the sentiment analysis task of rating inference. Dasgupta and Ng (2009) proposed a semi-supervised approach to mine the unambiguous reviews at first and then exploiting them to classify the ambiguous reviews, via a combination of active learning, transductive learning and ensemble learning. Ren et al. (2011) explored the use of label propagation (LP) (Zhu and Ghahramani, 2002) in building a semi-supervised sentiment classifier, and compared their results with Transductive SVMs(T-SVM). LP and T-SVM are transductive learning methods where the test data should participate in the training process. Zhou et al. (2010) proposed a deep learning approach called active deep networks to address semi-supervised sentiment classification with active learning. Socher et al. (2012) introduced a deep learning framework called semi-supervised recursive autoencoders for predicting sentencelevel sentiment distributi</context>
</contexts>
<marker>Ren, Kaji, Yoshinaga, Toyoda, Kitsuregawa, 2011</marker>
<rawString>Y. Ren, N. Kaji, N. Yoshinaga, M. Toyoda, and M. Kitsuregawa. 2011. Sentiment Classification in Resource-Scarce Languages by using Label Propagation. In Proceedings of the Pacific Asia Conference on Language, Information and Computation (PACLIC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Sarkar</author>
</authors>
<title>Applying cotraining methods to statistical parsing.</title>
<date>2001</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<marker>Sarkar, 2001</marker>
<rawString>A. Sarkar. 2001. Applying cotraining methods to statistical parsing. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Socher</author>
<author>J Pennington</author>
<author>E H Huang</author>
<author>A Y</author>
</authors>
<title>Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="6668" citStr="Socher et al. (2012)" startWordPosition="976" endWordPosition="979">n exploiting them to classify the ambiguous reviews, via a combination of active learning, transductive learning and ensemble learning. Ren et al. (2011) explored the use of label propagation (LP) (Zhu and Ghahramani, 2002) in building a semi-supervised sentiment classifier, and compared their results with Transductive SVMs(T-SVM). LP and T-SVM are transductive learning methods where the test data should participate in the training process. Zhou et al. (2010) proposed a deep learning approach called active deep networks to address semi-supervised sentiment classification with active learning. Socher et al. (2012) introduced a deep learning framework called semi-supervised recursive autoencoders for predicting sentencelevel sentiment distributions. The limitation of deep learning approaches might be their dependence on a considerable amount of unlabeled data to learn the representations and the inability to explicitly model the negation problem. One line of semi-supervised learning research is to bootstrap class labels using techniques like self-training, co-training and their variations. Wan (2009) proposed a co-training approach to address the cross-lingual sentiment classification problem. They made</context>
</contexts>
<marker>Socher, Pennington, Huang, Y, 2012</marker>
<rawString>R. Socher, J. Pennington, E. H. Huang, and A. Y. 2012. Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Su</author>
<author>S Li</author>
<author>S Ju</author>
<author>G Zhou</author>
<author>J Li</author>
</authors>
<title>Multi-view Learning for Semi-supervised Sentiment Classification.</title>
<date>2012</date>
<booktitle>In Proceedings of the International Conference on Asian Language Processing.</booktitle>
<contexts>
<context position="7888" citStr="Su et al. (2012)" startWordPosition="1157" endWordPosition="1160"> of the machine translation service to produce two views (a English view and a Chinese view) for co-training a Chinese review sentiment classifier, based on English corpus and unlabeled Chinese corpus. Li et al. (2010a) proposed an unsupervised method at first to automatically separate the review text into a personal view and an impersonal view, based on which the standard cotraining algorithm is then applied to build a semisupervised sentiment classifier. Li et al. (2011) further studied semi-supervised learning for imbalanced sentiment classification by using a dynamic co-training approach. Su et al. (2012) proposed a multi-view learning approach to semi-supervised sentiment classification with both feature partition and language translation strategies (Wan , 2009). Following (Li et al., 2010a), Li (2013) proposed a co-training approach which exploits subjective and objective views for semi-supervised sentiment classification. Our approach can also be viewed as a variation of co-training. The innovation of our approach is the dual-view construction technique by incorporating antonymous reviews and the bootstrapping mechanism by observing two opposite sides of one review. 3 The Proposed Approach </context>
</contexts>
<marker>Su, Li, Ju, Zhou, Li, 2012</marker>
<rawString>Y. Su, S. Li, S. Ju, G. Zhou and J. Li. 2012. 2012. Multi-view Learning for Semi-supervised Sentiment Classification. In Proceedings of the International Conference on Asian Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Wan</author>
</authors>
<title>Co-Training for Cross-Lingual Sentiment Classification.</title>
<date>2009</date>
<booktitle>In Proceedings ofACL-IJCNLP.</booktitle>
<contexts>
<context position="3383" citStr="Wan (2009)" startWordPosition="492" endWordPosition="493">n problem (Pang et al., 2002; Na et al., 2004; Polanyi and Zaenen , 2004; Kennedy and Inkpen, 2006; Ikeda et al., 2008; Li et al., 2010b; Orimaye et al., 2012; Xia et al., 2013). Nevertheless, in semi-supervised sentiment classification, most of the current approaches directly apply standard semi-supervised learning algorithms, without paying attention to appropriate representation for review texts. For example, Aue and Gamon (2005) applied the naive Bayes EM algorithm (Nigam et al., 2000). Goldberg and Zhu (2006) applied a graph-based semi-supervised learning algorithm by (Zhu et al., 2003). Wan (2009) employed a co-training approach for cross-language sentiment classification. Li et al. (2010a) employed cotraining with personal and impersonal views. Ren et al. (2011) explored the use of label propagation (Zhu and Ghahramani, 2002). As pointed by (Goldberg and Zhu, 2006): it is necessary to investigate better review text representations and similarity measures based on linguistic knowledge, as well as reviews’ sentiment patterns. However, to the best knowledge, such investigations are very scarce in the research of semi1054 Proceedings of the 53rd Annual Meeting of the Association for Compu</context>
<context position="7163" citStr="Wan (2009)" startWordPosition="1046" endWordPosition="1047">active deep networks to address semi-supervised sentiment classification with active learning. Socher et al. (2012) introduced a deep learning framework called semi-supervised recursive autoencoders for predicting sentencelevel sentiment distributions. The limitation of deep learning approaches might be their dependence on a considerable amount of unlabeled data to learn the representations and the inability to explicitly model the negation problem. One line of semi-supervised learning research is to bootstrap class labels using techniques like self-training, co-training and their variations. Wan (2009) proposed a co-training approach to address the cross-lingual sentiment classification problem. They made use of the machine translation service to produce two views (a English view and a Chinese view) for co-training a Chinese review sentiment classifier, based on English corpus and unlabeled Chinese corpus. Li et al. (2010a) proposed an unsupervised method at first to automatically separate the review text into a personal view and an impersonal view, based on which the standard cotraining algorithm is then applied to build a semisupervised sentiment classifier. Li et al. (2011) further studi</context>
</contexts>
<marker>Wan, 2009</marker>
<rawString>X. Wan. 2009. Co-Training for Cross-Lingual Sentiment Classification. In Proceedings ofACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Xia</author>
<author>T Wang</author>
<author>X Hu</author>
<author>S Li</author>
<author>C Zong</author>
</authors>
<title>Dual Training and Dual Prediction for Polarity Classification.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="2950" citStr="Xia et al., 2013" startWordPosition="425" endWordPosition="428">ords (BOW) model, which is difficult to meet the requirements for understanding the review text and dealing with complex linguistic structures such as negation. For example, the BOW representations of two opposite reviews “It works well” and “It doesn’t work well” are considered to be very similar by most statistical learning algorithms. In supervised sentiment classification, many approaches have been proposed in addressing the negation problem (Pang et al., 2002; Na et al., 2004; Polanyi and Zaenen , 2004; Kennedy and Inkpen, 2006; Ikeda et al., 2008; Li et al., 2010b; Orimaye et al., 2012; Xia et al., 2013). Nevertheless, in semi-supervised sentiment classification, most of the current approaches directly apply standard semi-supervised learning algorithms, without paying attention to appropriate representation for review texts. For example, Aue and Gamon (2005) applied the naive Bayes EM algorithm (Nigam et al., 2000). Goldberg and Zhu (2006) applied a graph-based semi-supervised learning algorithm by (Zhu et al., 2003). Wan (2009) employed a co-training approach for cross-language sentiment classification. Li et al. (2010a) employed cotraining with personal and impersonal views. Ren et al. (201</context>
<context position="4237" citStr="Xia et al., 2013" startWordPosition="616" endWordPosition="619">s pointed by (Goldberg and Zhu, 2006): it is necessary to investigate better review text representations and similarity measures based on linguistic knowledge, as well as reviews’ sentiment patterns. However, to the best knowledge, such investigations are very scarce in the research of semi1054 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1054–1063, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics supervised sentiment classification. In (Xia et al., 2013), we have developed a dual sentiment analysis approach, which creates antonymous reviews and makes use of original and antonymous reviews together for supervised sentiment classification. In this work, we propose a dual-view co-training approach based on dualview BOW representation for semi-supervised sentiment classification. Specifically, we model both the original and antonymous reviews by a pair of bags-of-words with opposite views. Based on such a dual-view representation, we design a dual-view co-training approach. The training, bootstrapping and testing processes are all performed by ob</context>
</contexts>
<marker>Xia, Wang, Hu, Li, Zong, 2013</marker>
<rawString>R. Xia, T. Wang, X. Hu, S. Li, and C. Zong. 2013. Dual Training and Dual Prediction for Polarity Classification. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Yang</author>
<author>X Liu</author>
</authors>
<title>A re-examination of text categorization methods.</title>
<date>1999</date>
<booktitle>In Proceedings SIGIR.</booktitle>
<contexts>
<context position="21125" citStr="Yang and Liu, 1999" startWordPosition="3276" endWordPosition="3279">on algorithms. Due to the space limitation, we only report the results by using logistic regression3. Note the similar conclusions can be obtained by using the other algorithms such as SVMs and naive Bayes. The LibLinear toolkit4 is utilized, with a dual L2-regularized factor, and a default tradeoff parameter c. Similar to (Wan , 2009; Li et al., 2010a), we carry out the experiments with the unigram features without feature selection. Presence is used as the term weighting scheme as it was reported in (Pang et al., 2002) that it performed better than TF and TF-IDF. Finally, the paired t-test (Yang and Liu, 1999) is performed to test the significance of the difference be3Logistic regression is quite similar to Maximum Entropy, and has been proved to be more efficient in sentiment classification than some other classification algorithms including naive Bayes and SVMs (Pang et al., 2002). 4http://www.csie.ntu.edu.tw/˜cjlin/ liblinear/ 1058 BOOK DVD ELEC KITC Avg. Baseline 0.680 0.691 0.726 0.740 0.709 LP 0.681 0.676 0.697 0.722 0.694 T-SVM 0.671 0.677 0.716 0.729 0.698 EM 0.702 0.706 0.758 0.744 0.728 Self-Training 0.689 0.705 0.736 0.751 0.720 Self-Reserved 0.690 0.708 0.735 0.754 0.722 Co-Static 0.696</context>
</contexts>
<marker>Yang, Liu, 1999</marker>
<rawString>Y. Yang and X. Liu. 1999. A re-examination of text categorization methods. In Proceedings SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Zhou</author>
<author>Q Chen</author>
<author>X Wang</author>
</authors>
<title>Active Deep Networks for Semi-Supervised Sentiment Classification.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="6511" citStr="Zhou et al. (2010)" startWordPosition="954" endWordPosition="957">he sentiment analysis task of rating inference. Dasgupta and Ng (2009) proposed a semi-supervised approach to mine the unambiguous reviews at first and then exploiting them to classify the ambiguous reviews, via a combination of active learning, transductive learning and ensemble learning. Ren et al. (2011) explored the use of label propagation (LP) (Zhu and Ghahramani, 2002) in building a semi-supervised sentiment classifier, and compared their results with Transductive SVMs(T-SVM). LP and T-SVM are transductive learning methods where the test data should participate in the training process. Zhou et al. (2010) proposed a deep learning approach called active deep networks to address semi-supervised sentiment classification with active learning. Socher et al. (2012) introduced a deep learning framework called semi-supervised recursive autoencoders for predicting sentencelevel sentiment distributions. The limitation of deep learning approaches might be their dependence on a considerable amount of unlabeled data to learn the representations and the inability to explicitly model the negation problem. One line of semi-supervised learning research is to bootstrap class labels using techniques like self-tr</context>
</contexts>
<marker>Zhou, Chen, Wang, 2010</marker>
<rawString>S. Zhou, Q. Chen, and X. Wang. 2010. Active Deep Networks for Semi-Supervised Sentiment Classification. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Zhu</author>
<author>Z Ghahramani</author>
</authors>
<title>Learning from labeled and unlabeled data with label propagation.</title>
<date>2002</date>
<tech>Technical Report CMU-CALD-02-107,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="3617" citStr="Zhu and Ghahramani, 2002" startWordPosition="525" endWordPosition="528">nt classification, most of the current approaches directly apply standard semi-supervised learning algorithms, without paying attention to appropriate representation for review texts. For example, Aue and Gamon (2005) applied the naive Bayes EM algorithm (Nigam et al., 2000). Goldberg and Zhu (2006) applied a graph-based semi-supervised learning algorithm by (Zhu et al., 2003). Wan (2009) employed a co-training approach for cross-language sentiment classification. Li et al. (2010a) employed cotraining with personal and impersonal views. Ren et al. (2011) explored the use of label propagation (Zhu and Ghahramani, 2002). As pointed by (Goldberg and Zhu, 2006): it is necessary to investigate better review text representations and similarity measures based on linguistic knowledge, as well as reviews’ sentiment patterns. However, to the best knowledge, such investigations are very scarce in the research of semi1054 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1054–1063, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics supervised sentiment classification. I</context>
<context position="6271" citStr="Zhu and Ghahramani, 2002" startWordPosition="918" endWordPosition="922"> of labeled data with a large amount of unlabeled data in target domain for cross-domain sentiment classification based on the EM algorithm. Goldberg and Zhu (2006) presented a graphbased semi-supervised learning algorithm (Zhu et al., 2003) for the sentiment analysis task of rating inference. Dasgupta and Ng (2009) proposed a semi-supervised approach to mine the unambiguous reviews at first and then exploiting them to classify the ambiguous reviews, via a combination of active learning, transductive learning and ensemble learning. Ren et al. (2011) explored the use of label propagation (LP) (Zhu and Ghahramani, 2002) in building a semi-supervised sentiment classifier, and compared their results with Transductive SVMs(T-SVM). LP and T-SVM are transductive learning methods where the test data should participate in the training process. Zhou et al. (2010) proposed a deep learning approach called active deep networks to address semi-supervised sentiment classification with active learning. Socher et al. (2012) introduced a deep learning framework called semi-supervised recursive autoencoders for predicting sentencelevel sentiment distributions. The limitation of deep learning approaches might be their depende</context>
<context position="22379" citStr="Zhu and Ghahramani (2002)" startWordPosition="3467" endWordPosition="3470">ynamic 0.701 0.725 0.756 0.767 0.737 Co-PI 0.702 0.716 0.746 0.769 0.733 Our approach 0.721 0.738 0.769 0.780 0.752 Table 1: The semi-supervised classification accuracy of ten systems. tween two systems, with a default significant level of 0.05. 4.2 Compared Systems We implement the following nine systems and compare them with our approach: • Baseline, the supervised baseline trained with the initial labeled data only; • Expectation Maximization (EM), with the naive Bayes model proposed by Nigam et al. (2000); • Label Propagation (LP), a graph-based semi-supervised learning method proposed by Zhu and Ghahramani (2002); • Transductive SVM (T-SVM), an extension of SVM so that it can exploit unlabeled data in semi-supervised learning ( Joachims, 1999); • Self-Training, a bootstrapping model that first trains a classifier, uses it to classify the unlabeled data, and adds the most confident data to the labeled set; • Self-Reserved, a variation of self-training proposed in (Liu et al., 2013),with a reserved procedure to incorporate some less confident examples; • Co-Static, the co-training algorithm by using two static partitions of feature set as two views (Blum and Mitchell, 1998); • Co-Dynamic, a variation of</context>
</contexts>
<marker>Zhu, Ghahramani, 2002</marker>
<rawString>X. Zhu and Z. Ghahramani. 2002. Learning from labeled and unlabeled data with label propagation. Technical Report CMU-CALD-02-107, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Zhu</author>
<author>Z Ghahramani</author>
<author>J Lafferty</author>
</authors>
<title>Semisupervised learning using Gaussian fields and harmonic functions.</title>
<date>2003</date>
<booktitle>In Proceddings of the International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="3371" citStr="Zhu et al., 2003" startWordPosition="488" endWordPosition="491">ressing the negation problem (Pang et al., 2002; Na et al., 2004; Polanyi and Zaenen , 2004; Kennedy and Inkpen, 2006; Ikeda et al., 2008; Li et al., 2010b; Orimaye et al., 2012; Xia et al., 2013). Nevertheless, in semi-supervised sentiment classification, most of the current approaches directly apply standard semi-supervised learning algorithms, without paying attention to appropriate representation for review texts. For example, Aue and Gamon (2005) applied the naive Bayes EM algorithm (Nigam et al., 2000). Goldberg and Zhu (2006) applied a graph-based semi-supervised learning algorithm by (Zhu et al., 2003). Wan (2009) employed a co-training approach for cross-language sentiment classification. Li et al. (2010a) employed cotraining with personal and impersonal views. Ren et al. (2011) explored the use of label propagation (Zhu and Ghahramani, 2002). As pointed by (Goldberg and Zhu, 2006): it is necessary to investigate better review text representations and similarity measures based on linguistic knowledge, as well as reviews’ sentiment patterns. However, to the best knowledge, such investigations are very scarce in the research of semi1054 Proceedings of the 53rd Annual Meeting of the Associati</context>
<context position="5887" citStr="Zhu et al., 2003" startWordPosition="857" endWordPosition="860">m and Mitchell, 1998). 2 Related Work The mainstream of the research in sentiment classification focused on supervised and unsupervised learning tasks. In comparison, semi-supervised sentiment classification has much less related studies. In this section, we focus on reviewing the work of semi-supervised sentiment classification. Aue and Gamon (2005) combined a small amount of labeled data with a large amount of unlabeled data in target domain for cross-domain sentiment classification based on the EM algorithm. Goldberg and Zhu (2006) presented a graphbased semi-supervised learning algorithm (Zhu et al., 2003) for the sentiment analysis task of rating inference. Dasgupta and Ng (2009) proposed a semi-supervised approach to mine the unambiguous reviews at first and then exploiting them to classify the ambiguous reviews, via a combination of active learning, transductive learning and ensemble learning. Ren et al. (2011) explored the use of label propagation (LP) (Zhu and Ghahramani, 2002) in building a semi-supervised sentiment classifier, and compared their results with Transductive SVMs(T-SVM). LP and T-SVM are transductive learning methods where the test data should participate in the training pro</context>
</contexts>
<marker>Zhu, Ghahramani, Lafferty, 2003</marker>
<rawString>X. Zhu, Z. Ghahramani, and J. Lafferty. 2003. Semisupervised learning using Gaussian fields and harmonic functions. In Proceddings of the International Conference on Machine Learning (ICML).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>