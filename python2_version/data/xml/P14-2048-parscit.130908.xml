<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000415">
<title confidence="0.9981885">
Automatic Detection of Machine Translated Text and Translation Quality
Estimation
</title>
<author confidence="0.996734">
Roee Aharoni Moshe Koppel Yoav Goldberg
</author>
<affiliation confidence="0.990115">
Dept. of Computer Science Dept. of Computer Science Dept. of Computer Science
Bar Ilan University Bar Ilan University Bar Ilan University
</affiliation>
<address confidence="0.770249">
Ramat-Gan, Israel 52900 Ramat-Gan, Israel 52900 Ramat-Gan, Israel 52900
</address>
<email confidence="0.997435">
roee.aharoni@gmail.com moishk@gmail.com yoav.goldberg@gmail.com
</email>
<sectionHeader confidence="0.99387" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999753923076923">
We show that it is possible to automati-
cally detect machine translated text at sen-
tence level from monolingual corpora, us-
ing text classification methods. We show
further that the accuracy with which a
learned classifier can detect text as ma-
chine translated is strongly correlated with
the translation quality of the machine
translation system that generated it. Fi-
nally, we offer a generic machine transla-
tion quality estimation technique based on
this approach, which does not require ref-
erence sentences.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999976075">
The recent success and proliferation of statistical
machine translation (MT) systems raise a number
of important questions. Prominent among these
are how to evaluate the quality of such a system
efficiently and how to detect the output of such
systems (for example, to avoid using it circularly
as input for refining MT systems).
In this paper, we will answer both these ques-
tions. First, we will show that using style-related
linguistic features, such as frequencies of parts-
of-speech n-grams and function words, it is pos-
sible to learn classifiers that distinguish machine-
translated text from human-translated or native
English text. While this is a straightforward and
not entirely novel result, our main contribution is
to relativize the result. We will see that the suc-
cess of such classifiers are strongly correlated with
the quality of the underlying machine translation
system. Specifically, given a corpus consisting of
both machine-translated English text (English be-
ing the target language) and native English text
(not necessarily the reference translation of the
machine-translated text), we measure the accuracy
of the system in classifying the sentences in the
corpus as machine-translated or not. This accu-
racy will be shown to decrease as the quality of
the underlying MT system increases. In fact, the
correlation is strong enough that we propose that
this accuracy measure itself can be used as a mea-
sure of MT system quality, obviating the need for
a reference corpus, as for example is necessary for
BLEU (Papineni et al., 2001).
The paper is structured as follows: In the next
section, we review previous related work. In the
third section, we describe experiments regarding
the detection of machine translation and in the
fourth section we discuss the use of detection tech-
niques as a machine translation quality estimation
method. In the final section we offer conclusions
and suggestions for future work.
</bodyText>
<sectionHeader confidence="0.997235" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<subsectionHeader confidence="0.615643">
2.1 Translationese
</subsectionHeader>
<bodyText confidence="0.999964428571429">
The special features of translated texts have been
studied widely for many years. Attempts to de-
fine their characteristics, often called ”Translation
Universals”, include (Toury, 1980; Blum-Kulka
and Levenston, 1983; Baker, 1993; Gellerstam,
1986). The differences between native and trans-
lated texts found there go well beyond systematic
translation errors and point to a distinct ”Transla-
tionese” dialect.
Using automatic text classification methods in
the field of translation studies had many use cases
in recent years, mainly as an empirical method
of measuring, proving or contradicting translation
universals. Several works (Baroni and Bernar-
dini, 2006; Kurokawa et al., 2009; Ilisei et al.,
2010) used text classification techniques in order
to distinguish human translated text from native
language text at document or paragraph level, us-
ing features like word and POS n-grams, propor-
tion of grammatical words in the text, nouns, fi-
nite verbs, auxiliary verbs, adjectives, adverbs, nu-
</bodyText>
<page confidence="0.973994">
289
</page>
<bodyText confidence="0.892070333333333">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 289–295,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
merals, pronouns, prepositions, determiners, con-
junctions etc. Koppel and Ordan (2011) classi-
fied texts to original or translated, using a list
of 300 function words taken from LIWC (Pen-
nebaker et al., 2001) as features. Volanski et
al. (2013) also tested various hypotheses regarding
”Translationese”, using 32 different linguistically-
informed features, to assess the degree to which
different sets of features can distinguish between
translated and original texts.
</bodyText>
<subsectionHeader confidence="0.996887">
2.2 Machine Translation Detection
</subsectionHeader>
<bodyText confidence="0.999911222222222">
Regarding the detection of machine translated
text, Carter and Inkpen (2012) translated the
Hansards of the 36th Parliament of Canada us-
ing the Microsoft Bing MT web service, and
conducted three detection experiments at docu-
ment level, using unigrams, average token length,
and type-token ratio as features. Arase and
Zhou (2013) trained a sentence-level classifier to
distinguish machine translated text from human
generated text on English and Japanese web-page
corpora, translated by Google Translate, Bing and
an in-house SMT system. They achieved very high
detection accuracy using application-specific fea-
ture sets for this purpose, including indicators of
the ”Phrase Salad” (Lopez, 2008) phenomenon or
”Gappy-Phrases” (Bansal et al., 2011).
While Arase and Zhou (2013) considered MT
detection at sentence level, as we do in this pa-
per, they did not study the correlation between the
translation quality of the machine translated text
and the ability to detect it. We show below that
such detection is possible with very high accuracy
only on low-quality translations. We examine this
detection accuracy vs. quality correlation, with
various MT systems, such as rule-based and sta-
tistical MT, both commercial and in-house, using
various feature sets.
</bodyText>
<sectionHeader confidence="0.977338" genericHeader="method">
3 Detection Experiments
</sectionHeader>
<subsectionHeader confidence="0.920864">
3.1 Features
</subsectionHeader>
<bodyText confidence="0.999987294117647">
We wish to distinguish machine translated En-
glish sentences from either human-translated sen-
tences or native English sentences. Due to the
sparseness of the data at the sentence level, we
use common content-independent linguistic fea-
tures for the classification task. Our features are
binary, denoting the presence or absence of each
of a set of part-of-speech n-grams acquired using
the Stanford POS tagger (Toutanova et al., 2003),
as well as the presence or absence of each of 467
function words taken from LIWC (Pennebaker et
al., 2001). We consider only those entries that ap-
pear at least ten times in the entire corpus, in order
to reduce sparsity in the data. As our learning al-
gorithm we use SVM with sequential minimal op-
timization (SMO), taken from the WEKA machine
learning toolkit (Hall et al., 2009).
</bodyText>
<subsectionHeader confidence="0.999694">
3.2 Detecting Different MT Systems
</subsectionHeader>
<bodyText confidence="0.999982341463415">
In the first experiment set, we explore the ability
to detect outputs of machine translated text from
different MT systems, in an environment contain-
ing both human generated and machine translated
text. For this task, we use a portion of the Cana-
dian Hansard corpus (Germann, 2001), containing
48,914 parallel sentences from French to English.
We translate the French portion of the corpus using
several MT systems, respectively: Google Trans-
late, Systran, and five other commercial MT sys-
tems available at the http://itranslate4.eu website,
which enables to query example MT systems built
by several european MT companies. After trans-
lating the sentences, we take 20,000 sentences
from each engine output and conduct the detection
experiment by labeling those sentences as MT sen-
tences, and another 20,000 sentences, which are
the human reference translations, labeled as ref-
erence sentences. We conduct a 10-fold cross-
validation experiment on the entire 40,000 sen-
tence corpus. We also conduct the same exper-
iment using 20,000 random, non-reference sen-
tences from the same corpus, instead of the ref-
erence sentences. Using simple linear regression,
we also obtain an R2 value (coefficient of deter-
mination) over the measurements of detection ac-
curacy and BLEU score, for each of three feature
set combinations (function words, POS tags and
mixed) and the two data combinations (MT vs.
reference and MT vs. non reference sentences).
The detection and R2 results are shown in Table 1.
As can be seen, best detection results are ob-
tained using the full combined feature set. It can
also be seen that, as might be expected, it is easier
to distinguish machine-translated sentences from
a non-reference set than from the reference set. In
Figure 1, we show the relationship of the observed
detection accuracy for each system with the BLEU
score of that system. As is evident, regardless
of the feature set or non-MT sentences used, the
correlation between detection accuracy and BLEU
</bodyText>
<page confidence="0.974104">
290
</page>
<figure confidence="0.778499">
BLEU
</figure>
<figureCaption confidence="0.9597005">
Figure 1: Correlation between detection accu-
racy and BLEU score on commercial MT systems,
using POS, function words and mixed features
against reference and non-reference sentences.
</figureCaption>
<bodyText confidence="0.682965">
score is very high, as we can also see from the R2
values in Table 1.
</bodyText>
<subsectionHeader confidence="0.927956">
3.3 In-House SMT Systems
</subsectionHeader>
<table confidence="0.998705375">
Parallel Monolingual BLEU
SMT-1 2000k 2000k 28.54
SMT-2 1000k 1000k 27.76
SMT-3 500k 500k 29.18
SMT-4 100k 100k 23.83
SMT-5 50k 50k 24.34
SMT-6 25k 25k 22.46
SMT-7 10k 10k 20.72
</table>
<tableCaption confidence="0.993953">
Table 3: Details for Moses based SMT systems
</tableCaption>
<bodyText confidence="0.9999414">
In the second experiment set, we test our de-
tection method on SMT systems we created, in
which we have control over the training data and
the expected overall relative translation quality. In
order to do so, we use the Moses statistical ma-
chine translation toolkit (Koehn et al., 2007). To
train the systems, we take a portion of the Europarl
corpus (Koehn, 2005), creating 7 different SMT
systems, each using a different amount of train-
ing data, for both the translation model and lan-
guage model. We do this in order to create dif-
ferent quality translation systems, details of which
are described in Table 3. For purposes of classifi-
cation, we use the same content independent fea-
tures as in the previous experiment, based on func-
</bodyText>
<figure confidence="0.914230333333333">
R2 = 0.789
20 22 24 26 28 30
BLEU
</figure>
<figureCaption confidence="0.988883">
Figure 2: Correlation between detection accu-
</figureCaption>
<bodyText confidence="0.919454583333333">
racy and BLEU score on in-house Moses-based
SMT systems against non-reference sentences us-
ing content independent features.
tion words and POS tags, again with SMO-based
SVM as the classifier. For data, we use 20,000 ran-
dom, non reference sentences from the Hansard
corpus, against 20,000 sentences from one MT
system per experiment, again resulting in 40,000
sentence instances per experiment. The relation-
ship between the detection results for each MT
system and the BLEU score for that system, re-
sulting in R2 = 0.774, is shown in Figure 2.
</bodyText>
<sectionHeader confidence="0.997891" genericHeader="method">
4 Machine Translation Evaluation
</sectionHeader>
<subsectionHeader confidence="0.986353">
4.1 Human Evaluation Experiments
</subsectionHeader>
<bodyText confidence="0.9997796875">
As can be seen in the above experiments, there is
a strong correlation between the BLEU score and
the MT detection accuracy of our method. In fact,
results are linearly and negatively correlated with
BLEU, as can be seen both on commercial systems
and our in-house SMT systems. We also wish to
consider the relationship between detection accu-
racy and a human quality estimation score. To
do this, we use the French-English data from the
8th Workshop on Statistical Machine Translation
- WMT13’ (Bojar et al., 2013), containing out-
puts from 13 different MT systems and their hu-
man evaluations. We conduct the same classifi-
cation experiment as above, with features based
on function words and POS tags, and SMO-based
SVM as the classifier. We first use 3000 refer-
</bodyText>
<figure confidence="0.994017111111111">
10 20 30
90
80
60
70
mix-nr
mix-r
fw-nr
fw-r
pos-nr
pos-r
detection accuracy (%)
76
75
74
73
detection accuracy (%)
72
</figure>
<page confidence="0.989881">
291
</page>
<table confidence="0.998888714285714">
Features Data Google Moses Systran ProMT Linguatec Skycode Trident R2
mixed MT/non-ref 63.34 72.02 72.36 78.2 79.57 80.9 89.36 0.946
mixed MT/ref 59.51 69.47 69.77 75.86 78.11 79.24 88.85 0.944
func. w. MT/non-ref 60.43 69.17 69.87 69.78 71.38 75.46 84.97 0.798
func. w. MT/ref 57.27 66.05 67.48 67.06 68.58 73.37 84.79 0.779
POS MT/non-ref 60.32 64.39 66.61 73 73.9 74.33 79.6 0.978
POS MT/ref 57.21 65.55 64.12 70.29 73.06 73.04 78.84 0.948
</table>
<tableCaption confidence="0.604431333333333">
Table 1: Classifier performance, including the R2 coefficient describing the correlation with BLEU.
MT Engine Example
Google Translate ”These days, all but one were subject to a vote,
</tableCaption>
<bodyText confidence="0.991153153846154">
and all had a direct link to the post September 11th.”
Moses ”these days , except one were the subject of a vote,
and all had a direct link with the after 11 September .”
Systran ”From these days, all except one were the object of a vote,
and all were connected a direct link with after September 11th.”
Linguatec ”Of these days, all except one were making the object of a vote
and all had a straightforward tie with after September 11.”
ProMT ”These days, very safe one all made object a vote,
and had a direct link with after September 11th.”
Trident ”From these all days, except one operated object voting,
and all had a direct rope with after 11 septembre.”
Skycode ”In these days, all safe one made the object in a vote
and all had a direct connection with him after 11 of September.”
</bodyText>
<tableCaption confidence="0.820403">
Table 2: Outputs from several MT systems for the same source sentence (function words marked in bold)
human evaluation score
</tableCaption>
<figureCaption confidence="0.996418333333333">
Figure 3: Correlation between detection accuracy
and human evaluation scores on systems from
WMT13’ against reference sentences.
</figureCaption>
<figure confidence="0.96229075">
R2 = 0.556
detection accuracy (%) 77
76
75
74
73
0.3 0.4 0.5 0.6
human evaluation score
</figure>
<figureCaption confidence="0.992499333333333">
Figure 4: Correlation between detection accu-
racy and human evaluation scores on systems from
WMT 13’ against non-reference sentences.
</figureCaption>
<figure confidence="0.940264285714286">
R2 = 0.774
0.3 0.4 0.5 0.6
detection accuracy (%)
64
62
60
58
292
R2 = 0.829
66
64
62
0.3 0.4 0.5 0.6
human evaluation score
</figure>
<figureCaption confidence="0.994284">
Figure 5: Correlation between detection accu-
</figureCaption>
<bodyText confidence="0.988368076923077">
racy and human evaluation scores on systems from
WMT 13’ against non-reference sentences, using
the syntactic CFG features described in section 4.2
ence sentences from the WMT13’ English refer-
ence translations, against the matching 3000 out-
put sentences from one MT system at a time, re-
sulting in 6000 sentence instances per experiment.
As can be seen in Figure 3, the detection accuracy
is strongly correlated with the evaluations scores,
yielding R2 = 0.774. To provide another mea-
sure of correlation, we compared every pair of
data points in the experiment to get the proportion
of pairs ordered identically by the human evalu-
ators and our method, with a result of 0.846 (66
of 78). In the second experiment, we use 3000
random, non reference sentences from the new-
stest 2011-2012 corpora published in WMT12’
(Callison-Burch et al., 2012) against 3000 output
sentences from one MT system at a time, again re-
sulting in 6000 sentence instances per experiment.
While applying the same classification method as
with the reference sentences, the detection accu-
racy rises, while the correlation with the transla-
tion quality yields R2 = 0.556, as can be seen in
Figure 4. Here, the proportion of identically or-
dered pairs is 0.782 (61 of 78).
</bodyText>
<subsectionHeader confidence="0.997402">
4.2 Syntactic Features
</subsectionHeader>
<bodyText confidence="0.999972476190476">
We note that the second leftmost point in Figures
3, 4 is an outlier: that is, our method has a hard
time detecting sentences produced by this system
although it is not highly rated by human evalu-
ators. This point represents the Joshua (Post et
al., 2013) SMT system. This system is syntax-
based, which apparently confound our POS and
FW-based classifier, despite it’s low human evalu-
ation score. We hypothesize that the use of syntax-
based features might improve results. To ver-
ify this intuition, we create parse trees using the
Berkeley parser (Petrov and Klein, 2007) and ex-
tract the one-level CFG rules as features. Again,
we represent each sentence as a boolean vector,
in which each entry represents the presence or ab-
sence of the CFG rule in the parse-tree of the sen-
tence. Using these features alone, without the FW
and POS tag based features presented above, we
obtain an R2 = 0.829 with a proportion of iden-
tically ordered pairs at 0.923 (72 of 78), as shown
in Figure 5.
</bodyText>
<sectionHeader confidence="0.992293" genericHeader="conclusions">
5 Discussion and Future Work
</sectionHeader>
<bodyText confidence="0.9999781">
We have shown that it is possible to detect ma-
chine translation from monolingual corpora con-
taining both machine translated text and human
generated text, at sentence level. There is a strong
correlation between the detection accuracy that
can be obtained and the BLEU score or the human
evaluation score of the machine translation itself.
This correlation holds whether or not a reference
set is used. This suggests that our method might be
used as an unsupervised quality estimation method
when no reference sentences are available, such
as for resource-poor source languages. Further
work might include applying our methods to other
language pairs and domains, acquiring word-level
quality estimation or integrating our method in
a machine translation system. Furthermore, ad-
ditional features and feature selection techniques
can be applied, both for improving detection ac-
curacy and for strengthening the correlation with
human quality estimation.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.886704">
We would like to thank Noam Ordan and Shuly
Wintner for their help and feedback on the early
stages of this work. This research was funded in
part by the Intel Collaborative Research Institute
for Computational Intelligence.
detection accuracy (%)
</bodyText>
<page confidence="0.992436">
293
</page>
<sectionHeader confidence="0.988781" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995814476635514">
Yuki Arase and Ming Zhou. 2013. Machine transla-
tion detection from monolingual web-text. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 1597–1607, Sofia, Bulgaria, August.
Association for Computational Linguistics.
Mona Baker. 1993. Corpus linguistics and transla-
tion studies: Implications and applications. Text and
technology: in honour of John Sinclair, 233:250.
Mohit Bansal, Chris Quirk, and Robert C. Moore.
2011. Gappy phrasal alignment by agreement. In
Dekang Lin, Yuji Matsumoto, and Rada Mihalcea,
editors, ACL, pages 1308–1317. The Association for
Computer Linguistics.
Marco Baroni and Silvia Bernardini. 2006. A new
approach to the study of translationese: Machine-
learning the difference between original and trans-
lated text. LLC, 21(3):259–274.
Shoshana Blum-Kulka and Eddie A. Levenston. 1983.
Universals of lexical simplification. Strategies in In-
terlanguage Communication, pages 119–139.
Ondˇrej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Work-
shop on Statistical Machine Translation. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 1–44, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10–51, Montr´eal, Canada, June. Association for
Computational Linguistics.
Dave Carter and Diana Inkpen. 2012. Searching
for poor quality machine translated text: Learning
the difference between human writing and machine
translations. In Leila Kosseim and Diana Inkpen,
editors, Canadian Conference on AI, volume 7310
of Lecture Notes in Computer Science, pages 49–60.
Springer.
Martin Gellerstam. 1986. Translationese in swedish
novels translated from english. In Lars Wollin
and Hans Lindquist, editors, Translation Studies in
Scandinavia, pages 88–95.
Ulrich Germann. 2001. Aligned hansards of the 36th
parliament of canada release 2001-1a.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDDExplor. Newsl., 11(1):10–18.
Iustina Ilisei, Diana Inkpen, Gloria Corpas Pastor, and
Ruslan Mitkov. 2010. Identification of transla-
tionese: A machine learning approach. In Alexan-
der F. Gelbukh, editor, CICLing, volume 6008 of
Lecture Notes in Computer Science, pages 503–511.
Springer.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
ACL. The Association for Computer Linguistics.
Philipp Koehn. 2005. Europarl: A Parallel Corpus
for Statistical Machine Translation. In Conference
Proceedings: the tenth Machine Translation Sum-
mit, pages 79–86, Phuket, Thailand. AAMT, AAMT.
Moshe Koppel and Noam Ordan. 2011. Translationese
and its dialects. In Dekang Lin, Yuji Matsumoto,
and Rada Mihalcea, editors, ACL, pages 1318–1326.
The Association for Computer Linguistics.
David Kurokawa, Cyril Goutte, and Pierre Isabelle.
2009. Automatic Detection of Translated Text and
its Impact on Machine Translation. In Conference
Proceedings: the twelvth Machine Translation Sum-
mit.
Adam Lopez. 2008. Statistical machine translation.
ACM Computing Surveys (CSUR), 40(3):8.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic
evaluation of machine translation. Technical report,
IBM Research Report.
J.W. Pennebaker, M.E. Francis, and R.J. Booth. 2001.
Linguistic inquiry and word count: Liwc 2001.
Mahway: Lawrence Erlbaum Associates.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, Proceedings of the Main Confer-
ence, pages 404–411.
Marius Popescu. 2011. Studying translationese at
the character level. In Galia Angelova, Kalina
Bontcheva, Ruslan Mitkov, and Nicolas Nicolov, ed-
itors, RANLP, pages 634–639. RANLP 2011 Organ-
ising Committee.
Matt Post, Juri Ganitkevitch, Luke Orland, Jonathan
Weese, Yuan Cao, and Chris Callison-Burch. 2013.
Joshua 5.0: Sparser, better, faster, server. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, August 8-9, 2013., pages 206–
212. Association for Computational Linguistics.
Gideon Toury. 1980. In Search of a Theory of Transla-
tion.
</reference>
<page confidence="0.977029">
294
</page>
<reference confidence="0.998925818181818">
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In IN PROCEEDINGS OF HLT-NAACL, pages 252–
259.
Hans van Halteren. 2008. Source language markers
in europarl translations. In Donia Scott and Hans
Uszkoreit, editors, COLING, pages 937–944.
Vered Volansky, Noam Ordan, and Shuly Wintner.
2013. On the features of translationese. Literary
and Linguistic Computing.
</reference>
<page confidence="0.99856">
295
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.958411">
<title confidence="0.9970815">Automatic Detection of Machine Translated Text and Translation Quality Estimation</title>
<author confidence="0.997417">Roee Aharoni Moshe Koppel Yoav Goldberg</author>
<affiliation confidence="0.9974475">Dept. of Computer Science Dept. of Computer Science Dept. of Computer Science Bar Ilan University Bar Ilan University Bar Ilan University</affiliation>
<address confidence="0.999839">Ramat-Gan, Israel 52900 Ramat-Gan, Israel 52900 Ramat-Gan, Israel</address>
<email confidence="0.999649">roee.aharoni@gmail.commoishk@gmail.comyoav.goldberg@gmail.com</email>
<abstract confidence="0.997986142857143">We show that it is possible to automatically detect machine translated text at sentence level from monolingual corpora, using text classification methods. We show further that the accuracy with which a learned classifier can detect text as machine translated is strongly correlated with the translation quality of the machine translation system that generated it. Finally, we offer a generic machine translation quality estimation technique based on this approach, which does not require reference sentences.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yuki Arase</author>
<author>Ming Zhou</author>
</authors>
<title>Machine translation detection from monolingual web-text.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>1597--1607</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="4932" citStr="Arase and Zhou (2013)" startWordPosition="741" endWordPosition="744">s features. Volanski et al. (2013) also tested various hypotheses regarding ”Translationese”, using 32 different linguisticallyinformed features, to assess the degree to which different sets of features can distinguish between translated and original texts. 2.2 Machine Translation Detection Regarding the detection of machine translated text, Carter and Inkpen (2012) translated the Hansards of the 36th Parliament of Canada using the Microsoft Bing MT web service, and conducted three detection experiments at document level, using unigrams, average token length, and type-token ratio as features. Arase and Zhou (2013) trained a sentence-level classifier to distinguish machine translated text from human generated text on English and Japanese web-page corpora, translated by Google Translate, Bing and an in-house SMT system. They achieved very high detection accuracy using application-specific feature sets for this purpose, including indicators of the ”Phrase Salad” (Lopez, 2008) phenomenon or ”Gappy-Phrases” (Bansal et al., 2011). While Arase and Zhou (2013) considered MT detection at sentence level, as we do in this paper, they did not study the correlation between the translation quality of the machine tra</context>
</contexts>
<marker>Arase, Zhou, 2013</marker>
<rawString>Yuki Arase and Ming Zhou. 2013. Machine translation detection from monolingual web-text. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1597–1607, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mona Baker</author>
</authors>
<title>Corpus linguistics and translation studies: Implications and applications. Text and technology: in honour of John Sinclair,</title>
<date>1993</date>
<pages>233--250</pages>
<contexts>
<context position="3122" citStr="Baker, 1993" startWordPosition="476" endWordPosition="477">ows: In the next section, we review previous related work. In the third section, we describe experiments regarding the detection of machine translation and in the fourth section we discuss the use of detection techniques as a machine translation quality estimation method. In the final section we offer conclusions and suggestions for future work. 2 Previous Work 2.1 Translationese The special features of translated texts have been studied widely for many years. Attempts to define their characteristics, often called ”Translation Universals”, include (Toury, 1980; Blum-Kulka and Levenston, 1983; Baker, 1993; Gellerstam, 1986). The differences between native and translated texts found there go well beyond systematic translation errors and point to a distinct ”Translationese” dialect. Using automatic text classification methods in the field of translation studies had many use cases in recent years, mainly as an empirical method of measuring, proving or contradicting translation universals. Several works (Baroni and Bernardini, 2006; Kurokawa et al., 2009; Ilisei et al., 2010) used text classification techniques in order to distinguish human translated text from native language text at document or </context>
</contexts>
<marker>Baker, 1993</marker>
<rawString>Mona Baker. 1993. Corpus linguistics and translation studies: Implications and applications. Text and technology: in honour of John Sinclair, 233:250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohit Bansal</author>
<author>Chris Quirk</author>
<author>Robert C Moore</author>
</authors>
<title>Gappy phrasal alignment by agreement.</title>
<date>2011</date>
<pages>1308--1317</pages>
<editor>In Dekang Lin, Yuji Matsumoto, and Rada Mihalcea, editors, ACL,</editor>
<publisher>The Association for Computer Linguistics.</publisher>
<contexts>
<context position="5350" citStr="Bansal et al., 2011" startWordPosition="799" endWordPosition="802">Canada using the Microsoft Bing MT web service, and conducted three detection experiments at document level, using unigrams, average token length, and type-token ratio as features. Arase and Zhou (2013) trained a sentence-level classifier to distinguish machine translated text from human generated text on English and Japanese web-page corpora, translated by Google Translate, Bing and an in-house SMT system. They achieved very high detection accuracy using application-specific feature sets for this purpose, including indicators of the ”Phrase Salad” (Lopez, 2008) phenomenon or ”Gappy-Phrases” (Bansal et al., 2011). While Arase and Zhou (2013) considered MT detection at sentence level, as we do in this paper, they did not study the correlation between the translation quality of the machine translated text and the ability to detect it. We show below that such detection is possible with very high accuracy only on low-quality translations. We examine this detection accuracy vs. quality correlation, with various MT systems, such as rule-based and statistical MT, both commercial and in-house, using various feature sets. 3 Detection Experiments 3.1 Features We wish to distinguish machine translated English se</context>
</contexts>
<marker>Bansal, Quirk, Moore, 2011</marker>
<rawString>Mohit Bansal, Chris Quirk, and Robert C. Moore. 2011. Gappy phrasal alignment by agreement. In Dekang Lin, Yuji Matsumoto, and Rada Mihalcea, editors, ACL, pages 1308–1317. The Association for Computer Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
</authors>
<title>A new approach to the study of translationese: Machinelearning the difference between original and translated text.</title>
<date>2006</date>
<journal>LLC,</journal>
<volume>21</volume>
<issue>3</issue>
<contexts>
<context position="3553" citStr="Baroni and Bernardini, 2006" startWordPosition="536" endWordPosition="540">ed texts have been studied widely for many years. Attempts to define their characteristics, often called ”Translation Universals”, include (Toury, 1980; Blum-Kulka and Levenston, 1983; Baker, 1993; Gellerstam, 1986). The differences between native and translated texts found there go well beyond systematic translation errors and point to a distinct ”Translationese” dialect. Using automatic text classification methods in the field of translation studies had many use cases in recent years, mainly as an empirical method of measuring, proving or contradicting translation universals. Several works (Baroni and Bernardini, 2006; Kurokawa et al., 2009; Ilisei et al., 2010) used text classification techniques in order to distinguish human translated text from native language text at document or paragraph level, using features like word and POS n-grams, proportion of grammatical words in the text, nouns, finite verbs, auxiliary verbs, adjectives, adverbs, nu289 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 289–295, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics merals, pronouns, prepositions, determiners, conjun</context>
</contexts>
<marker>Baroni, Bernardini, 2006</marker>
<rawString>Marco Baroni and Silvia Bernardini. 2006. A new approach to the study of translationese: Machinelearning the difference between original and translated text. LLC, 21(3):259–274.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shoshana Blum-Kulka</author>
<author>Eddie A Levenston</author>
</authors>
<title>Universals of lexical simplification.</title>
<date>1983</date>
<booktitle>Strategies in Interlanguage Communication,</booktitle>
<pages>119--139</pages>
<contexts>
<context position="3109" citStr="Blum-Kulka and Levenston, 1983" startWordPosition="472" endWordPosition="475"> The paper is structured as follows: In the next section, we review previous related work. In the third section, we describe experiments regarding the detection of machine translation and in the fourth section we discuss the use of detection techniques as a machine translation quality estimation method. In the final section we offer conclusions and suggestions for future work. 2 Previous Work 2.1 Translationese The special features of translated texts have been studied widely for many years. Attempts to define their characteristics, often called ”Translation Universals”, include (Toury, 1980; Blum-Kulka and Levenston, 1983; Baker, 1993; Gellerstam, 1986). The differences between native and translated texts found there go well beyond systematic translation errors and point to a distinct ”Translationese” dialect. Using automatic text classification methods in the field of translation studies had many use cases in recent years, mainly as an empirical method of measuring, proving or contradicting translation universals. Several works (Baroni and Bernardini, 2006; Kurokawa et al., 2009; Ilisei et al., 2010) used text classification techniques in order to distinguish human translated text from native language text at</context>
</contexts>
<marker>Blum-Kulka, Levenston, 1983</marker>
<rawString>Shoshana Blum-Kulka and Eddie A. Levenston. 1983. Universals of lexical simplification. Strategies in Interlanguage Communication, pages 119–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondˇrej Bojar</author>
<author>Christian Buck</author>
<author>Chris Callison-Burch</author>
<author>Christian Federmann</author>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<date>2013</date>
<booktitle>Findings of the 2013 Workshop on Statistical Machine Translation. In Proceedings of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>1--44</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="11170" citStr="Bojar et al., 2013" startWordPosition="1761" endWordPosition="1764">g in R2 = 0.774, is shown in Figure 2. 4 Machine Translation Evaluation 4.1 Human Evaluation Experiments As can be seen in the above experiments, there is a strong correlation between the BLEU score and the MT detection accuracy of our method. In fact, results are linearly and negatively correlated with BLEU, as can be seen both on commercial systems and our in-house SMT systems. We also wish to consider the relationship between detection accuracy and a human quality estimation score. To do this, we use the French-English data from the 8th Workshop on Statistical Machine Translation - WMT13’ (Bojar et al., 2013), containing outputs from 13 different MT systems and their human evaluations. We conduct the same classification experiment as above, with features based on function words and POS tags, and SMO-based SVM as the classifier. We first use 3000 refer10 20 30 90 80 60 70 mix-nr mix-r fw-nr fw-r pos-nr pos-r detection accuracy (%) 76 75 74 73 detection accuracy (%) 72 291 Features Data Google Moses Systran ProMT Linguatec Skycode Trident R2 mixed MT/non-ref 63.34 72.02 72.36 78.2 79.57 80.9 89.36 0.946 mixed MT/ref 59.51 69.47 69.77 75.86 78.11 79.24 88.85 0.944 func. w. MT/non-ref 60.43 69.17 69.8</context>
</contexts>
<marker>Bojar, Buck, Callison-Burch, Federmann, Haddow, Koehn, Monz, Post, Soricut, Specia, 2013</marker>
<rawString>Ondˇrej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2013. Findings of the 2013 Workshop on Statistical Machine Translation. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 1–44, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<title>Findings of the 2012 workshop on statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>10--51</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="14443" citStr="Callison-Burch et al., 2012" startWordPosition="2323" endWordPosition="2326">ainst the matching 3000 output sentences from one MT system at a time, resulting in 6000 sentence instances per experiment. As can be seen in Figure 3, the detection accuracy is strongly correlated with the evaluations scores, yielding R2 = 0.774. To provide another measure of correlation, we compared every pair of data points in the experiment to get the proportion of pairs ordered identically by the human evaluators and our method, with a result of 0.846 (66 of 78). In the second experiment, we use 3000 random, non reference sentences from the newstest 2011-2012 corpora published in WMT12’ (Callison-Burch et al., 2012) against 3000 output sentences from one MT system at a time, again resulting in 6000 sentence instances per experiment. While applying the same classification method as with the reference sentences, the detection accuracy rises, while the correlation with the translation quality yields R2 = 0.556, as can be seen in Figure 4. Here, the proportion of identically ordered pairs is 0.782 (61 of 78). 4.2 Syntactic Features We note that the second leftmost point in Figures 3, 4 is an outlier: that is, our method has a hard time detecting sentences produced by this system although it is not highly rat</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Post, Soricut, Specia, 2012</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2012. Findings of the 2012 workshop on statistical machine translation. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 10–51, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dave Carter</author>
<author>Diana Inkpen</author>
</authors>
<title>Searching for poor quality machine translated text: Learning the difference between human writing and machine translations.</title>
<date>2012</date>
<booktitle>In Leila Kosseim and Diana Inkpen, editors, Canadian Conference on AI,</booktitle>
<volume>7310</volume>
<pages>49--60</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="4679" citStr="Carter and Inkpen (2012)" startWordPosition="701" endWordPosition="704">2014 Association for Computational Linguistics merals, pronouns, prepositions, determiners, conjunctions etc. Koppel and Ordan (2011) classified texts to original or translated, using a list of 300 function words taken from LIWC (Pennebaker et al., 2001) as features. Volanski et al. (2013) also tested various hypotheses regarding ”Translationese”, using 32 different linguisticallyinformed features, to assess the degree to which different sets of features can distinguish between translated and original texts. 2.2 Machine Translation Detection Regarding the detection of machine translated text, Carter and Inkpen (2012) translated the Hansards of the 36th Parliament of Canada using the Microsoft Bing MT web service, and conducted three detection experiments at document level, using unigrams, average token length, and type-token ratio as features. Arase and Zhou (2013) trained a sentence-level classifier to distinguish machine translated text from human generated text on English and Japanese web-page corpora, translated by Google Translate, Bing and an in-house SMT system. They achieved very high detection accuracy using application-specific feature sets for this purpose, including indicators of the ”Phrase S</context>
</contexts>
<marker>Carter, Inkpen, 2012</marker>
<rawString>Dave Carter and Diana Inkpen. 2012. Searching for poor quality machine translated text: Learning the difference between human writing and machine translations. In Leila Kosseim and Diana Inkpen, editors, Canadian Conference on AI, volume 7310 of Lecture Notes in Computer Science, pages 49–60. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Gellerstam</author>
</authors>
<title>Translationese in swedish novels translated from english.</title>
<date>1986</date>
<booktitle>In Lars Wollin and Hans Lindquist, editors, Translation Studies in Scandinavia,</booktitle>
<pages>88--95</pages>
<contexts>
<context position="3141" citStr="Gellerstam, 1986" startWordPosition="478" endWordPosition="479">ext section, we review previous related work. In the third section, we describe experiments regarding the detection of machine translation and in the fourth section we discuss the use of detection techniques as a machine translation quality estimation method. In the final section we offer conclusions and suggestions for future work. 2 Previous Work 2.1 Translationese The special features of translated texts have been studied widely for many years. Attempts to define their characteristics, often called ”Translation Universals”, include (Toury, 1980; Blum-Kulka and Levenston, 1983; Baker, 1993; Gellerstam, 1986). The differences between native and translated texts found there go well beyond systematic translation errors and point to a distinct ”Translationese” dialect. Using automatic text classification methods in the field of translation studies had many use cases in recent years, mainly as an empirical method of measuring, proving or contradicting translation universals. Several works (Baroni and Bernardini, 2006; Kurokawa et al., 2009; Ilisei et al., 2010) used text classification techniques in order to distinguish human translated text from native language text at document or paragraph level, us</context>
</contexts>
<marker>Gellerstam, 1986</marker>
<rawString>Martin Gellerstam. 1986. Translationese in swedish novels translated from english. In Lars Wollin and Hans Lindquist, editors, Translation Studies in Scandinavia, pages 88–95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulrich Germann</author>
</authors>
<title>Aligned hansards of the 36th parliament of canada release</title>
<date>2001</date>
<pages>2001--1</pages>
<contexts>
<context position="7028" citStr="Germann, 2001" startWordPosition="1076" endWordPosition="1077">Pennebaker et al., 2001). We consider only those entries that appear at least ten times in the entire corpus, in order to reduce sparsity in the data. As our learning algorithm we use SVM with sequential minimal optimization (SMO), taken from the WEKA machine learning toolkit (Hall et al., 2009). 3.2 Detecting Different MT Systems In the first experiment set, we explore the ability to detect outputs of machine translated text from different MT systems, in an environment containing both human generated and machine translated text. For this task, we use a portion of the Canadian Hansard corpus (Germann, 2001), containing 48,914 parallel sentences from French to English. We translate the French portion of the corpus using several MT systems, respectively: Google Translate, Systran, and five other commercial MT systems available at the http://itranslate4.eu website, which enables to query example MT systems built by several european MT companies. After translating the sentences, we take 20,000 sentences from each engine output and conduct the detection experiment by labeling those sentences as MT sentences, and another 20,000 sentences, which are the human reference translations, labeled as referenc</context>
</contexts>
<marker>Germann, 2001</marker>
<rawString>Ulrich Germann. 2001. Aligned hansards of the 36th parliament of canada release 2001-1a.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The weka data mining software: an update.</title>
<date>2009</date>
<journal>SIGKDDExplor. Newsl.,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="6710" citStr="Hall et al., 2009" startWordPosition="1022" endWordPosition="1025">mmon content-independent linguistic features for the classification task. Our features are binary, denoting the presence or absence of each of a set of part-of-speech n-grams acquired using the Stanford POS tagger (Toutanova et al., 2003), as well as the presence or absence of each of 467 function words taken from LIWC (Pennebaker et al., 2001). We consider only those entries that appear at least ten times in the entire corpus, in order to reduce sparsity in the data. As our learning algorithm we use SVM with sequential minimal optimization (SMO), taken from the WEKA machine learning toolkit (Hall et al., 2009). 3.2 Detecting Different MT Systems In the first experiment set, we explore the ability to detect outputs of machine translated text from different MT systems, in an environment containing both human generated and machine translated text. For this task, we use a portion of the Canadian Hansard corpus (Germann, 2001), containing 48,914 parallel sentences from French to English. We translate the French portion of the corpus using several MT systems, respectively: Google Translate, Systran, and five other commercial MT systems available at the http://itranslate4.eu website, which enables to quer</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The weka data mining software: an update. SIGKDDExplor. Newsl., 11(1):10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Iustina Ilisei</author>
</authors>
<title>Diana Inkpen, Gloria Corpas Pastor, and Ruslan Mitkov.</title>
<date>2010</date>
<booktitle>of Lecture Notes in Computer Science,</booktitle>
<volume>6008</volume>
<pages>503--511</pages>
<editor>In Alexander F. Gelbukh, editor, CICLing,</editor>
<publisher>Springer.</publisher>
<marker>Ilisei, 2010</marker>
<rawString>Iustina Ilisei, Diana Inkpen, Gloria Corpas Pastor, and Ruslan Mitkov. 2010. Identification of translationese: A machine learning approach. In Alexander F. Gelbukh, editor, CICLing, volume 6008 of Lecture Notes in Computer Science, pages 503–511. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation. In ACL. The Association for Computer Linguistics.</title>
<date>2007</date>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="9522" citStr="Koehn et al., 2007" startWordPosition="1483" endWordPosition="1486">ence sentences. score is very high, as we can also see from the R2 values in Table 1. 3.3 In-House SMT Systems Parallel Monolingual BLEU SMT-1 2000k 2000k 28.54 SMT-2 1000k 1000k 27.76 SMT-3 500k 500k 29.18 SMT-4 100k 100k 23.83 SMT-5 50k 50k 24.34 SMT-6 25k 25k 22.46 SMT-7 10k 10k 20.72 Table 3: Details for Moses based SMT systems In the second experiment set, we test our detection method on SMT systems we created, in which we have control over the training data and the expected overall relative translation quality. In order to do so, we use the Moses statistical machine translation toolkit (Koehn et al., 2007). To train the systems, we take a portion of the Europarl corpus (Koehn, 2005), creating 7 different SMT systems, each using a different amount of training data, for both the translation model and language model. We do this in order to create different quality translation systems, details of which are described in Table 3. For purposes of classification, we use the same content independent features as in the previous experiment, based on funcR2 = 0.789 20 22 24 26 28 30 BLEU Figure 2: Correlation between detection accuracy and BLEU score on in-house Moses-based SMT systems against non-referenc</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In ACL. The Association for Computer Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A Parallel Corpus for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In Conference Proceedings: the tenth Machine Translation Summit,</booktitle>
<pages>79--86</pages>
<location>Phuket, Thailand. AAMT, AAMT.</location>
<contexts>
<context position="9600" citStr="Koehn, 2005" startWordPosition="1499" endWordPosition="1500"> 3.3 In-House SMT Systems Parallel Monolingual BLEU SMT-1 2000k 2000k 28.54 SMT-2 1000k 1000k 27.76 SMT-3 500k 500k 29.18 SMT-4 100k 100k 23.83 SMT-5 50k 50k 24.34 SMT-6 25k 25k 22.46 SMT-7 10k 10k 20.72 Table 3: Details for Moses based SMT systems In the second experiment set, we test our detection method on SMT systems we created, in which we have control over the training data and the expected overall relative translation quality. In order to do so, we use the Moses statistical machine translation toolkit (Koehn et al., 2007). To train the systems, we take a portion of the Europarl corpus (Koehn, 2005), creating 7 different SMT systems, each using a different amount of training data, for both the translation model and language model. We do this in order to create different quality translation systems, details of which are described in Table 3. For purposes of classification, we use the same content independent features as in the previous experiment, based on funcR2 = 0.789 20 22 24 26 28 30 BLEU Figure 2: Correlation between detection accuracy and BLEU score on in-house Moses-based SMT systems against non-reference sentences using content independent features. tion words and POS tags, again</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A Parallel Corpus for Statistical Machine Translation. In Conference Proceedings: the tenth Machine Translation Summit, pages 79–86, Phuket, Thailand. AAMT, AAMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Moshe Koppel</author>
<author>Noam Ordan</author>
</authors>
<title>Translationese and its dialects.</title>
<date>2011</date>
<pages>1318--1326</pages>
<editor>In Dekang Lin, Yuji Matsumoto, and Rada Mihalcea, editors, ACL,</editor>
<publisher>The Association for Computer Linguistics.</publisher>
<contexts>
<context position="4188" citStr="Koppel and Ordan (2011)" startWordPosition="629" endWordPosition="632">t al., 2009; Ilisei et al., 2010) used text classification techniques in order to distinguish human translated text from native language text at document or paragraph level, using features like word and POS n-grams, proportion of grammatical words in the text, nouns, finite verbs, auxiliary verbs, adjectives, adverbs, nu289 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 289–295, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics merals, pronouns, prepositions, determiners, conjunctions etc. Koppel and Ordan (2011) classified texts to original or translated, using a list of 300 function words taken from LIWC (Pennebaker et al., 2001) as features. Volanski et al. (2013) also tested various hypotheses regarding ”Translationese”, using 32 different linguisticallyinformed features, to assess the degree to which different sets of features can distinguish between translated and original texts. 2.2 Machine Translation Detection Regarding the detection of machine translated text, Carter and Inkpen (2012) translated the Hansards of the 36th Parliament of Canada using the Microsoft Bing MT web service, and conduc</context>
</contexts>
<marker>Koppel, Ordan, 2011</marker>
<rawString>Moshe Koppel and Noam Ordan. 2011. Translationese and its dialects. In Dekang Lin, Yuji Matsumoto, and Rada Mihalcea, editors, ACL, pages 1318–1326. The Association for Computer Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Kurokawa</author>
<author>Cyril Goutte</author>
<author>Pierre Isabelle</author>
</authors>
<title>Automatic Detection of Translated Text and its Impact on Machine Translation.</title>
<date>2009</date>
<booktitle>In Conference Proceedings: the twelvth Machine Translation Summit.</booktitle>
<contexts>
<context position="3576" citStr="Kurokawa et al., 2009" startWordPosition="541" endWordPosition="544">dely for many years. Attempts to define their characteristics, often called ”Translation Universals”, include (Toury, 1980; Blum-Kulka and Levenston, 1983; Baker, 1993; Gellerstam, 1986). The differences between native and translated texts found there go well beyond systematic translation errors and point to a distinct ”Translationese” dialect. Using automatic text classification methods in the field of translation studies had many use cases in recent years, mainly as an empirical method of measuring, proving or contradicting translation universals. Several works (Baroni and Bernardini, 2006; Kurokawa et al., 2009; Ilisei et al., 2010) used text classification techniques in order to distinguish human translated text from native language text at document or paragraph level, using features like word and POS n-grams, proportion of grammatical words in the text, nouns, finite verbs, auxiliary verbs, adjectives, adverbs, nu289 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 289–295, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics merals, pronouns, prepositions, determiners, conjunctions etc. Koppel and </context>
</contexts>
<marker>Kurokawa, Goutte, Isabelle, 2009</marker>
<rawString>David Kurokawa, Cyril Goutte, and Pierre Isabelle. 2009. Automatic Detection of Translated Text and its Impact on Machine Translation. In Conference Proceedings: the twelvth Machine Translation Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Lopez</author>
</authors>
<title>Statistical machine translation.</title>
<date>2008</date>
<journal>ACM Computing Surveys (CSUR),</journal>
<volume>40</volume>
<issue>3</issue>
<contexts>
<context position="5298" citStr="Lopez, 2008" startWordPosition="794" endWordPosition="795">ated the Hansards of the 36th Parliament of Canada using the Microsoft Bing MT web service, and conducted three detection experiments at document level, using unigrams, average token length, and type-token ratio as features. Arase and Zhou (2013) trained a sentence-level classifier to distinguish machine translated text from human generated text on English and Japanese web-page corpora, translated by Google Translate, Bing and an in-house SMT system. They achieved very high detection accuracy using application-specific feature sets for this purpose, including indicators of the ”Phrase Salad” (Lopez, 2008) phenomenon or ”Gappy-Phrases” (Bansal et al., 2011). While Arase and Zhou (2013) considered MT detection at sentence level, as we do in this paper, they did not study the correlation between the translation quality of the machine translated text and the ability to detect it. We show below that such detection is possible with very high accuracy only on low-quality translations. We examine this detection accuracy vs. quality correlation, with various MT systems, such as rule-based and statistical MT, both commercial and in-house, using various feature sets. 3 Detection Experiments 3.1 Features </context>
</contexts>
<marker>Lopez, 2008</marker>
<rawString>Adam Lopez. 2008. Statistical machine translation. ACM Computing Surveys (CSUR), 40(3):8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2001</date>
<tech>Technical report, IBM Research Report.</tech>
<contexts>
<context position="2478" citStr="Papineni et al., 2001" startWordPosition="377" endWordPosition="380">f both machine-translated English text (English being the target language) and native English text (not necessarily the reference translation of the machine-translated text), we measure the accuracy of the system in classifying the sentences in the corpus as machine-translated or not. This accuracy will be shown to decrease as the quality of the underlying MT system increases. In fact, the correlation is strong enough that we propose that this accuracy measure itself can be used as a measure of MT system quality, obviating the need for a reference corpus, as for example is necessary for BLEU (Papineni et al., 2001). The paper is structured as follows: In the next section, we review previous related work. In the third section, we describe experiments regarding the detection of machine translation and in the fourth section we discuss the use of detection techniques as a machine translation quality estimation method. In the final section we offer conclusions and suggestions for future work. 2 Previous Work 2.1 Translationese The special features of translated texts have been studied widely for many years. Attempts to define their characteristics, often called ”Translation Universals”, include (Toury, 1980;</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2001. Bleu: a method for automatic evaluation of machine translation. Technical report, IBM Research Report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J W Pennebaker</author>
<author>M E Francis</author>
<author>R J Booth</author>
</authors>
<title>Linguistic inquiry and word count: Liwc</title>
<date>2001</date>
<contexts>
<context position="4309" citStr="Pennebaker et al., 2001" startWordPosition="650" endWordPosition="654"> native language text at document or paragraph level, using features like word and POS n-grams, proportion of grammatical words in the text, nouns, finite verbs, auxiliary verbs, adjectives, adverbs, nu289 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 289–295, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics merals, pronouns, prepositions, determiners, conjunctions etc. Koppel and Ordan (2011) classified texts to original or translated, using a list of 300 function words taken from LIWC (Pennebaker et al., 2001) as features. Volanski et al. (2013) also tested various hypotheses regarding ”Translationese”, using 32 different linguisticallyinformed features, to assess the degree to which different sets of features can distinguish between translated and original texts. 2.2 Machine Translation Detection Regarding the detection of machine translated text, Carter and Inkpen (2012) translated the Hansards of the 36th Parliament of Canada using the Microsoft Bing MT web service, and conducted three detection experiments at document level, using unigrams, average token length, and type-token ratio as features</context>
<context position="6438" citStr="Pennebaker et al., 2001" startWordPosition="973" endWordPosition="976">cial and in-house, using various feature sets. 3 Detection Experiments 3.1 Features We wish to distinguish machine translated English sentences from either human-translated sentences or native English sentences. Due to the sparseness of the data at the sentence level, we use common content-independent linguistic features for the classification task. Our features are binary, denoting the presence or absence of each of a set of part-of-speech n-grams acquired using the Stanford POS tagger (Toutanova et al., 2003), as well as the presence or absence of each of 467 function words taken from LIWC (Pennebaker et al., 2001). We consider only those entries that appear at least ten times in the entire corpus, in order to reduce sparsity in the data. As our learning algorithm we use SVM with sequential minimal optimization (SMO), taken from the WEKA machine learning toolkit (Hall et al., 2009). 3.2 Detecting Different MT Systems In the first experiment set, we explore the ability to detect outputs of machine translated text from different MT systems, in an environment containing both human generated and machine translated text. For this task, we use a portion of the Canadian Hansard corpus (Germann, 2001), containi</context>
</contexts>
<marker>Pennebaker, Francis, Booth, 2001</marker>
<rawString>J.W. Pennebaker, M.E. Francis, and R.J. Booth. 2001. Linguistic inquiry and word count: Liwc 2001. Mahway: Lawrence Erlbaum Associates.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics, Proceedings of the Main Conference,</booktitle>
<pages>404--411</pages>
<contexts>
<context position="15433" citStr="Petrov and Klein, 2007" startWordPosition="2492" endWordPosition="2495">red pairs is 0.782 (61 of 78). 4.2 Syntactic Features We note that the second leftmost point in Figures 3, 4 is an outlier: that is, our method has a hard time detecting sentences produced by this system although it is not highly rated by human evaluators. This point represents the Joshua (Post et al., 2013) SMT system. This system is syntaxbased, which apparently confound our POS and FW-based classifier, despite it’s low human evaluation score. We hypothesize that the use of syntaxbased features might improve results. To verify this intuition, we create parse trees using the Berkeley parser (Petrov and Klein, 2007) and extract the one-level CFG rules as features. Again, we represent each sentence as a boolean vector, in which each entry represents the presence or absence of the CFG rule in the parse-tree of the sentence. Using these features alone, without the FW and POS tag based features presented above, we obtain an R2 = 0.829 with a proportion of identically ordered pairs at 0.923 (72 of 78), as shown in Figure 5. 5 Discussion and Future Work We have shown that it is possible to detect machine translation from monolingual corpora containing both machine translated text and human generated text, at s</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics, Proceedings of the Main Conference, pages 404–411.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marius Popescu</author>
</authors>
<title>Studying translationese at the character level.</title>
<date>2011</date>
<booktitle>RANLP 2011 Organising Committee.</booktitle>
<pages>634--639</pages>
<editor>In Galia Angelova, Kalina Bontcheva, Ruslan Mitkov, and Nicolas Nicolov, editors, RANLP,</editor>
<marker>Popescu, 2011</marker>
<rawString>Marius Popescu. 2011. Studying translationese at the character level. In Galia Angelova, Kalina Bontcheva, Ruslan Mitkov, and Nicolas Nicolov, editors, RANLP, pages 634–639. RANLP 2011 Organising Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Post</author>
<author>Juri Ganitkevitch</author>
<author>Luke Orland</author>
<author>Jonathan Weese</author>
<author>Yuan Cao</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Joshua 5.0: Sparser, better, faster, server.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>206--212</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="15119" citStr="Post et al., 2013" startWordPosition="2441" endWordPosition="2444">e, again resulting in 6000 sentence instances per experiment. While applying the same classification method as with the reference sentences, the detection accuracy rises, while the correlation with the translation quality yields R2 = 0.556, as can be seen in Figure 4. Here, the proportion of identically ordered pairs is 0.782 (61 of 78). 4.2 Syntactic Features We note that the second leftmost point in Figures 3, 4 is an outlier: that is, our method has a hard time detecting sentences produced by this system although it is not highly rated by human evaluators. This point represents the Joshua (Post et al., 2013) SMT system. This system is syntaxbased, which apparently confound our POS and FW-based classifier, despite it’s low human evaluation score. We hypothesize that the use of syntaxbased features might improve results. To verify this intuition, we create parse trees using the Berkeley parser (Petrov and Klein, 2007) and extract the one-level CFG rules as features. Again, we represent each sentence as a boolean vector, in which each entry represents the presence or absence of the CFG rule in the parse-tree of the sentence. Using these features alone, without the FW and POS tag based features prese</context>
</contexts>
<marker>Post, Ganitkevitch, Orland, Weese, Cao, Callison-Burch, 2013</marker>
<rawString>Matt Post, Juri Ganitkevitch, Luke Orland, Jonathan Weese, Yuan Cao, and Chris Callison-Burch. 2013. Joshua 5.0: Sparser, better, faster, server. In Proceedings of the Eighth Workshop on Statistical Machine Translation, August 8-9, 2013., pages 206– 212. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gideon Toury</author>
</authors>
<title>In Search of a Theory of Translation.</title>
<date>1980</date>
<contexts>
<context position="3077" citStr="Toury, 1980" startWordPosition="470" endWordPosition="471">t al., 2001). The paper is structured as follows: In the next section, we review previous related work. In the third section, we describe experiments regarding the detection of machine translation and in the fourth section we discuss the use of detection techniques as a machine translation quality estimation method. In the final section we offer conclusions and suggestions for future work. 2 Previous Work 2.1 Translationese The special features of translated texts have been studied widely for many years. Attempts to define their characteristics, often called ”Translation Universals”, include (Toury, 1980; Blum-Kulka and Levenston, 1983; Baker, 1993; Gellerstam, 1986). The differences between native and translated texts found there go well beyond systematic translation errors and point to a distinct ”Translationese” dialect. Using automatic text classification methods in the field of translation studies had many use cases in recent years, mainly as an empirical method of measuring, proving or contradicting translation universals. Several works (Baroni and Bernardini, 2006; Kurokawa et al., 2009; Ilisei et al., 2010) used text classification techniques in order to distinguish human translated t</context>
</contexts>
<marker>Toury, 1980</marker>
<rawString>Gideon Toury. 1980. In Search of a Theory of Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-ofspeech tagging with a cyclic dependency network. In</title>
<date>2003</date>
<booktitle>IN PROCEEDINGS OF HLT-NAACL,</booktitle>
<pages>252--259</pages>
<contexts>
<context position="6330" citStr="Toutanova et al., 2003" startWordPosition="953" endWordPosition="956">curacy vs. quality correlation, with various MT systems, such as rule-based and statistical MT, both commercial and in-house, using various feature sets. 3 Detection Experiments 3.1 Features We wish to distinguish machine translated English sentences from either human-translated sentences or native English sentences. Due to the sparseness of the data at the sentence level, we use common content-independent linguistic features for the classification task. Our features are binary, denoting the presence or absence of each of a set of part-of-speech n-grams acquired using the Stanford POS tagger (Toutanova et al., 2003), as well as the presence or absence of each of 467 function words taken from LIWC (Pennebaker et al., 2001). We consider only those entries that appear at least ten times in the entire corpus, in order to reduce sparsity in the data. As our learning algorithm we use SVM with sequential minimal optimization (SMO), taken from the WEKA machine learning toolkit (Hall et al., 2009). 3.2 Detecting Different MT Systems In the first experiment set, we explore the ability to detect outputs of machine translated text from different MT systems, in an environment containing both human generated and machi</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-rich part-ofspeech tagging with a cyclic dependency network. In IN PROCEEDINGS OF HLT-NAACL, pages 252– 259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans van Halteren</author>
</authors>
<title>Source language markers in europarl translations.</title>
<date>2008</date>
<booktitle>In Donia Scott</booktitle>
<pages>937--944</pages>
<editor>and Hans Uszkoreit, editors, COLING,</editor>
<marker>van Halteren, 2008</marker>
<rawString>Hans van Halteren. 2008. Source language markers in europarl translations. In Donia Scott and Hans Uszkoreit, editors, COLING, pages 937–944.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vered Volansky</author>
<author>Noam Ordan</author>
<author>Shuly Wintner</author>
</authors>
<title>On the features of translationese. Literary and Linguistic Computing.</title>
<date>2013</date>
<marker>Volansky, Ordan, Wintner, 2013</marker>
<rawString>Vered Volansky, Noam Ordan, and Shuly Wintner. 2013. On the features of translationese. Literary and Linguistic Computing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>