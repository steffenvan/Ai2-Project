<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.041385">
<title confidence="0.9992705">
Leveraging Preposition Ambiguity to Assess Compositional Distributional
Models of Semantics
</title>
<author confidence="0.997935">
Samuel Ritter∗ Cotie Long Denis Paperno
</author>
<affiliation confidence="0.997726">
Princeton University Indiana University University of Trento
</affiliation>
<author confidence="0.996444">
Marco Baroni Matthew Botvinick Adele Goldberg
</author>
<affiliation confidence="0.999795">
University of Trento Princeton University Princeton University
</affiliation>
<sectionHeader confidence="0.988081" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99979224">
Complex interactions among the meanings of
words are important factors in the function
that maps word meanings to phrase meanings.
Recently, compositional distributional seman-
tics models (CDSM) have been designed with
the goal of emulating these complex interac-
tions; however, experimental results on the ef-
fectiveness of CDSM have been difficult to in-
terpret because the current metrics for assess-
ing them do not control for the confound of
lexical information. We present a new method
for assessing the degree to which CDSM cap-
ture semantic interactions that dissociates the
influences of lexical and compositional infor-
mation. We then provide a dataset for per-
forming this type of assessment and use it
to evaluate six compositional models using
both co-occurrence based and neural language
model input vectors. Results show that neural
language input vectors are consistently supe-
rior to co-occurrence based vectors, that sev-
eral CDSM capture substantial compositional
information, and that, surprisingly, vector ad-
dition matches and is in many cases superior
to purpose-built paramaterized models.
</bodyText>
<sectionHeader confidence="0.999336" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.986799548387097">
Consider the meanings of the following phrases:
“red apple,” “red hair,” and “red state.” The meaning
of the word “red” in each of these examples interacts
with the meaning of the noun it modifies, applying
∗Please address correspondence to the first author at swrit-
ter@princeton.edu
a different color to the first two and a political af-
filiation to the third. This is an example of a com-
mon phenomenon in natural language in which the
meaning of a whole expression is not derived from
a simple concatenation of its parts, but is composed
by interactions among their meanings.
Cognitive and computer scientists have pointed
out this complexity and proposed various models for
accommodating it (Kintsch, 2001; Mitchell and La-
pata, 2010; Socher et al., 2013). A dominant model-
ing approach seeks to learn functions that combine
word representations derived from the distributional
structure of large natural language corpora (Deer-
wester et al., 1990; Landauer and Dumais, 1997).
Because the word representations to be combined
and the compositional functions are generated based
on the distributions of words in corpora, these mod-
els have been dubbed compositional distributional
semantic models, or CDSM (Marelli et al., 2014).
CDSM produce fixed-dimensional vector represen-
tations of arbitrary sentences and phrases, and the
foundational principle of these models is, stated sim-
ply, that semantically similar phrases should have
vector representations that are close together in the
vector space.
</bodyText>
<subsectionHeader confidence="0.969189">
1.1 CDSM Assessment
</subsectionHeader>
<bodyText confidence="0.999784428571428">
Past studies have tested how well CDSM adhere to
this principle by comparing the vector similarity of
pairs of sentences with similarity ratings given by
humans. Many of these studies used datasets in
which the amount of lexical overlap between the
sentence pairs is not carefully controlled, e.g., the
datasets of Dolan and Brockett (2005) and Agirre
</bodyText>
<page confidence="0.98589">
199
</page>
<note confidence="0.43974">
Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 199–204,
Denver, Colorado, June 4–5, 2015.
</note>
<bodyText confidence="0.999965867924529">
et al. (2014). One such study obtained the influen-
tial result that on such a dataset, simple composition
models such as vector addition perform compara-
bly to a state-of-the-art composition model (Blacoe
and Lapata, 2012). The success of these simplis-
tic models led to the conjecture that these data sets
fail to assess critical aspects of language (Baroni et
al., 2014a) and leaves open the question of whether
CDSM would outperform simplistic models in a set-
ting in which lexical cues are uninformative.
In the present study, we develop a method for re-
moving the confound of lexical cues from CDSM as-
sessment. The method is to create a set of sentences
where each sentence fits into a semantic category
and where a sentence’s semantic category cannot be
determined based on any individual word in the sen-
tence. CDSM are then challenged to create a vector
space in which the representations for sentences in a
given category cluster together, even though the in-
dividual word vectors do not cluster together. This
clustering can be tested by training a simple linear
classifier on the CDSM representations, then testing
it on representations for held out sentences.
Here, we build a suitable test set by leveraging
the lexical ambiguity inherent in locative expres-
sions. Locative expressions are phrases that describe
a spatial relationship between two objects using two
nouns joined by a preposition; for example, “The
magnet is on the refrigerator”, which describes the
relationship of adhesion to a vertical surface. Cru-
cially, the spatial relationship between the two nouns
in a locative expression is undetermined by the spa-
tial preposition, and can only be determined based
on semantic interactions among the prepositions and
the two nouns (Herskovits, 1985).
For example, while “The magnet is on the refrig-
erator” describes the spatial relationship of adhesion
to a vertical surface, “The apple is on the refrigera-
tor” describes support by a horizontal surface. In or-
der to classify a new sentence, e.g., “The magnet is
on the papers”, into the correct category of support
by a horizontal surface, the CDSM vectors for the
three sentences must encode the fact that “The mag-
net is on the papers” shares a common spatial rela-
tionship with “The apple is on the refrigerator” and
not with “The magnet is on the refrigerator”, even
though the latter pair of sentences share more words
than the former.
Given this dissociation between lexical overlap
and spatial relationship, we were able to construct
a dataset wherein lexical information is uninforma-
tive, and models must rely on compositionality to
score well in classification.
</bodyText>
<subsectionHeader confidence="0.994079">
1.2 Relation to Past Work
</subsectionHeader>
<bodyText confidence="0.999984975609756">
This approach to CDSM assessment is similar to
a previous method wherein polysemous verbs are
paired with disambiguating nouns in transitive or
intransitive verb phrases. These phrases are then
matched with “landmark” verbs that are either sim-
ilar or not similar in meaning to the full phrase.
CDSM are then challenged to create representa-
tions of the phrases from which classifiers can de-
termine whether or not a phrase is similar to its
landmark verb (Kintsch, 2001; Mitchell and Lapata,
2008; Mitchell and Lapata, 2010; Grefenstette and
Sadrzadeh, 2011). Another notable CDSM assess-
ment task involves matching a phrase with a word
with a similar meaning, for example, matching a
short dictionary definition with the word it defines
(Kartsaklis et al., 2012; Turney, 2014).
While these methods are applicable only to simple
phrases that can be mapped reasonably to a single
word, the present method can, in principle, be ap-
plied to any type of phrase. This allowed us to build
a dataset that extends the current landmark word and
word matching datasets in at least two important
ways. First, it includes function words, specifically
prepositions. Second, it requires the characterization
of interactions among three words in each expres-
sion, whereas previous datasets had two words per
expression, or subsets of the words did not interact
in complex ways.
Other important approaches to CDSM assessment
include rating the similarity of sentence pairs, de-
termining whether two sentences are paraphrases
(Dolan and Brockett, 2005), classifying the entail-
ment relationship between two sentences (Marelli
et al., 2014), classifying the relationship between
two entities named in a sentence (Hendrickx et al.,
2009), and classifying the valence of the sentiment
expressed in a sentence (Socher et al., 2013). These
methods have primarily been aimed at assessing
CDSM on the full array of constructions inherent
in naturally generated language, while our method
aims to isolate a specific construction of interest.
</bodyText>
<page confidence="0.969052">
200
</page>
<figure confidence="0.380498">
Category Example
Adhesion to Vertical Surface “There is a magnet on the refrigerator.”
Support by Horizontal Surface “There is an apple on the refrigerator.”
Support from Above “There is an apple on the branch.”
Full Containment “There is an apple in the refrigerator.”
Partial Containment “There is an apple in the water.”
</figure>
<tableCaption confidence="0.990126">
Table 1: Categories and Example Sentences
</tableCaption>
<sectionHeader confidence="0.95073" genericHeader="method">
2 The Dataset
</sectionHeader>
<bodyText confidence="0.99998968">
A list of all of the spatial categories with examples
is given in Table 1. The authors chose the set of cat-
egories to produce the desired dissociation between
lexical meaning and phrase category, taking inspi-
ration from the observations of Herskovits (1985).
To produce a dataset of expressions fitting these cat-
egories, the first and second authors - both native
English speakers - generated a large set of locative
expressions, intending each expression for a specific
category. Then all of the expressions were indepen-
dently rated by the first two authors, and any expres-
sion for which the ratings disagreed were excluded
from the dataset. In order to achieve a balanced cat-
egory size, the second author then created additional
sentences intended for underrepresented categories.
All additional sentences were stripped of labels and
rated independently by the first author. If the first
and second authors’ categorizations did not match,
the sentence was not added to the dataset.
The dataset contains 500 sentences in total with
100 sentences per category. There is a large amount
of lexical variety in the set, with 242 distinct words
occurring in noun position one and 213 occurring in
noun position two. The dataset is publicly available
for download at www.princeton.edu/∼swritter.
</bodyText>
<sectionHeader confidence="0.997359" genericHeader="method">
3 Evaluation Setup
</sectionHeader>
<bodyText confidence="0.976840872340425">
Classification among the five categories was per-
formed using a naive Bayes classifier. Two of the
categories contained “in” as the preposition in all
sentences while the other three contained “on” in all
sentences. To be certain that the held out sentences
on which the classifier was tested did not contain
even a single category-informative noun, we oper-
ationally defined informativeness and relegated all
sentences with an informative noun to the training
set. A noun was deemed informative if it both oc-
curred more than once in the entire data set and it
occurred more frequently in one category than in
any other. This criterion yielded a set of 80 sen-
tences with no informative nouns, and a set of 420
sentences with at least one informative noun. By this
method, we ensured that no component of the mod-
els’ classification accuracy on the test set is due to
the recognition of individual nouns.
In addition to the CDSM, we included two non-
distributional models for comparison. The first, re-
ferred to as word overlap, consists of a binary feature
vector containing one feature per vocabulary item.
This model’s performance provides an upper-bound
on the performance that a model can achieve given
only the distribution of word tokens in the train-
ing set. The second model, inspired by Srikumar
and Roth (2013), contains binary features for Word-
net hypernyms (up to 4 levels) of each sense of the
noun and a binary feature for each preposition. This
model’s score provides an indication of the amount
of task-relevant information contained in the taxo-
nomic features of individual words.
We compared CDSM to a further control that con-
sisted of the concatenation of the word vectors. The
concatenated vectors contain a complete representa-
tion of all of the individual word information, so that
any performance the CDSM can achieve above the
concatenation score can be attributed to semantic in-
teraction information contained in the parameters of
the CDSM.1
1One other experiment we considered was to test the models
on the dataset phrases with prepositions removed. However, LF
and PLF are undefined for such an input, and the element-wise
models trivially perform better with the preposition included be-
cause the preposition is the only word that is not stripped of in-
formativeness by design of the task. As such, we excluded this
experiment from this report.
</bodyText>
<page confidence="0.995672">
201
</page>
<figureCaption confidence="0.973593666666667">
Figure 1: Naive Bayes accuracy scores for count and predict variants of several CDSM. Chance performance
on this task was 0.2. Overlap refers to the word overlap baseline. CW refers to the vectors from Collobert
and Weston (2008)
</figureCaption>
<figure confidence="0.995815666666667">
.
Proportion Correct
0.7
0.6
0.4
0.3
0.5
0.2
Word-Based
Predict
Count
CW
</figure>
<subsectionHeader confidence="0.99332">
3.1 Compositional Distributional Models
</subsectionHeader>
<bodyText confidence="0.999989703703704">
We compared six models that are currently promi-
nent in the CDSM literature: addition, multiplica-
tion (Mitchell and Lapata, 2008), lexical function
(LF) (Coecke et al., 2010), practical lexical func-
tion (PLF) (Paperno et al., 2014), full additive (FA)
(Guevara, 2010; Zanzotto et al., 2010), and the re-
cursive auto-encoder (RAE) (Socher et al., 2011).
The training data for LF, PLF, and FA was the
UKWAC+Wikipedia+BNC 2.8 billion word cor-
pus. In training LF, we followed Grefenstette et al.
(2013), employing a two-step training regime using
corpus-extracted vectors for noun–preposition–noun
combinations to estimate matrices of correspond-
ing prepositional phrases, which were in turn used
to estimate a three-way tensor of each preposition.
For PLF and FA, we learned separate matrices for
combining prepositions with each of the two nouns
in the construction, using corpus-based vectors of
prepositional phrases for training preposition–noun
combination. For training composition of the head
noun with the prepositional phrase, we used corpus-
extracted noun+preposition (for lexical matrices in
PLF) or attributive adjective+noun (for attributive
construction in FA) vectors. Phrase vectors for train-
ing were built as DISSECT ‘peripheral’ spaces from
phrase cooccurrence data in the count models. In the
predict models, phrase vectors were learned along
with word vectors in one pass, feeding all phrases of
the relevant type as single tokens.
The RAE vectors were computed using Socher et
al.’s implementation which is trained on a 150K sen-
tence subset of the NYT and AP sections of the Gi-
gaword corpus.
For all compositional models, we used as in-
put two varieties of word level representations: co-
occurrence based (Turney et al., 2010) and neural
language model (Mikolov et al., 2013). Following
Baroni et al. (2014b), we will refer to these variants
as count and predict models respectively. Both word
models were trained on the same corpus as those
used to train the compositional models. Count was
based on a 5 word window weighted with positive
PMI and was reduced to 300 dimensions via SVD,
while predict was based on a 5 word window using
Mikolov’s continuous bag of words approach with
negative sampling (Mikolov et al., 2013). These pa-
rameters were based on their strong performance in
the systematic evaluation by Baroni et al. (2014b).
Socher et al.’s RAE implementation composes neu-
ral language model vectors described by Collobert
and Weston (2008) and supplied by Turian et al.
(2010). For comparison with the RAE, we report re-
sults for addition, multiplication, and concatenation
of these same embeddings.
</bodyText>
<sectionHeader confidence="0.999905" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.9999555">
The naive Bayes accuracy scores for all models are
displayed in Figure 1. Addition, PLF, and the RAE
each substantially outperformed concatenation, in-
dicating that these models’ vectors contain informa-
</bodyText>
<page confidence="0.995234">
202
</page>
<bodyText confidence="0.99998276">
tion about the semantic interactions between phrase
constituents. Addition scored higher than PLF,
while the RAE achieved comparable performance
to its additive counterpart. In all cases except FA
in which predict and count vectors were compared,
predict achieved a higher score. This last result
shows that the superiority of predict vectors docu-
mented by Baroni et al. (2014b) extends to their use
in compositional models.
All of the models performed well above chance
accuracy of 0.2. The Wordnet based model achieved
accuracy substantially above word overlap using
hypernym information, indicating that although
each noun is uninformative, its membership in
higher level semantic categories is informative. All
of the distributional models outperform the non-
distributional models, except for LF and FA, which
also fail to outperform concatenations of their in-
put vectors. One explanation for the poor perfor-
mance of LF and FA is that the 2.8B word corpus
used to train them did not have sufficient relevant
information to specify their large sets of parameters.
This explanation is supported by the fact that PLF,
a model designed as a parameter-reduced version of
LF, performs well.
</bodyText>
<sectionHeader confidence="0.998278" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999994304347826">
The most important finding of this study is that, even
on a test painstakingly designed to exclusively as-
sess composition, vector addition matches or out-
performs sophisticated CDSM. This finding implies
that the structure of distributional vector spaces
admits the effective use of addition for modeling
complex interactions between meanings. This sug-
gests that future work should be concerned with un-
derstanding the properties of distributional vector
spaces that make this possible, as well as with un-
derstanding how these properties can be leveraged
by sophisticated models.
A further contribution of this work is that it serves
as a proof-of-concept for a new method for dissoci-
ating the influences of lexical and compositional in-
fluences on CDSM performance. Future work can
extend this approach by finding alternatives to loca-
tive expressions in order to test a wider variety of
constructions. More immediately, future work may
improve the locative expressions dataset by using
crowdsourcing to obtain naive participant ratings to
corroborate the expert ratings and to increase the
size of the dataset.
</bodyText>
<sectionHeader confidence="0.977297" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9923885">
Denis Paperno and Marco Baroni were sup-
ported by ERC 2011 Starting Independent Re-
search Grant n. 283554 (COMPOSES). Samuel
Ritter and Matthew Botvinick were supported by
Intelligence Advanced Research Projects Activity
(IARPA) Grant n. 102-01.
</bodyText>
<sectionHeader confidence="0.9984" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.917157868421053">
Eneko Agirre, Carmen Baneab, Claire Cardiec, Daniel
Cerd, Mona Diabe, Aitor Gonzalez-Agirre, Weiwei
Guof, Rada Mihalceab, German Rigaua, and Janyce
Wiebeg. 2014. Semeval-2014 task 10: Multilingual
semantic textual similarity. SemEval 2014, page 81.
Marco Baroni, Raffaela Bernardi, and Roberto Zampar-
elli. 2014a. Frege in space: A program of compo-
sitional distributional semantics. Linguistic Issues in
Language Technology, 9.
Marco Baroni, Georgiana Dinu, and Germ´an Kruszewski.
2014b. Dont count, predict! a systematic compari-
son of context-counting vs. context-predicting seman-
tic vectors. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguistics,
volume 1.
William Blacoe and Mirella Lapata. 2012. A comparison
of vector-based representations for semantic composi-
tion. In EMNLP, pages 546–556.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark.
2010. Mathematical foundations for a compositional
distributional model of meaning. arXiv preprint
arXiv:1003.4394.
Ronan Collobert and Jason Weston. 2008. A unified ar-
chitecture for natural language processing: Deep neu-
ral networks with multitask learning. In Proceedings
of the 25th international conference on Machine learn-
ing, pages 160–167. ACM.
Scott C. Deerwester, Susan T Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. JASIS,
41(6):391–407.
William B Dolan and Chris Brockett. 2005. Automat-
ically constructing a corpus of sentential paraphrases.
In Proc. of IWP.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical compositional
distributional model of meaning. In EMNLP, pages
1394–1404.
</reference>
<page confidence="0.994996">
203
</page>
<reference confidence="0.999425643678161">
Edward Grefenstette, Georgiana Dinu, Yao-Zhong
Zhang, Mehrnoosh Sadrzadeh, and Marco Baroni.
2013. Multi-step regression learning for composi-
tional distributional semantics. In Proceedings of
IWCS, pages 131–142, Potsdam, Germany.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of the 2010 Workshop on
Geometrical Models of Natural Language Semantics,
pages 33–37.
Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav
Nakov, Diarmuid O´ S´eaghdha, Sebastian Pad´o, Marco
Pennacchiotti, Lorenza Romano, and Stan Szpakow-
icz. 2009. Semeval-2010 task 8: Multi-way classifica-
tion of semantic relations between pairs of nominals.
In Proceedings of the Workshop on Semantic Evalu-
ations: Recent Achievements and Future Directions,
pages 94–99. Association for Computational Linguis-
tics.
Annette Herskovits. 1985. Semantics and pragmatics of
locative expressions*. Cognitive Science, 9(3):341–
378.
Dimitri Kartsaklis, Mehrnoosh Sadrzadeh, and Stephen
Pulman. 2012. A unified sentence space for cate-
gorical distributional-compositional semantics: The-
ory and experiments. In In Proceedings of COLING:
Posters. Citeseer.
Walter Kintsch. 2001. Predication. Cognitive Science,
25(2):173–202.
Thomas K Landauer and Susan T Dumais. 1997. A so-
lution to plato’s problem: The latent semantic analysis
theory of acquisition, induction, and representation of
knowledge. Psychological review, 104(2):211.
Marco Marelli, Luisa Bentivogli, Marco Baroni, Raf-
faella Bernardi, Stefano Menini, and Roberto Zampar-
elli. 2014. Semeval-2014 task 1: Evaluation of com-
positional distributional semantic models on full sen-
tences through semantic relatedness and textual entail-
ment. SemEval-2014.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word representa-
tions in vector space. arXiv preprint arXiv:1301.3781.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In ACL, pages 236–
244. Citeseer.
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive science,
34(8):1388–1429.
Denis Paperno, Nghia The Pham, and Marco Baroni.
2014. A practical and linguistically-motivated ap-
proach to compositional distributional semantics. In
Proceedings of the 52nd Annual Meeting of the Associ-
ation for Computational Linguistics (Volume 1: Long
Papers), pages 90–99, Baltimore, Maryland, June.
Richard Socher, Eric H. Huang, Jeffrey Pennington, An-
drew Y. Ng, and Christopher D. Manning. 2011. Dy-
namic Pooling and Unfolding Recursive Autoencoders
for Paraphrase Detection. In Advances in Neural In-
formation Processing Systems 24.
Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng, and
Christopher Potts. 2013. Recursive deep models for
semantic compositionality over a sentiment treebank.
In EMNLP, pages 1631–1642. Citeseer.
V. Srikumar and D. Roth. 2013. Modeling semantic rela-
tions expressed by prepositions. In Transactions of the
Association for Computational Linguistics, volume 1,
pages 231–242.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method for
semi-supervised learning. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 384–394. Association for Computa-
tional Linguistics.
Peter D Turney, Patrick Pantel, et al. 2010. From fre-
quency to meaning: Vector space models of semantics.
Journal of artificial intelligence research, 37(1):141–
188.
Peter D Turney. 2014. Semantic composition and de-
composition: From recognition to generation. arXiv
preprint arXiv:1405.7908.
Fabio Massimo Zanzotto, Ioannis Korkontzelos,
Francesca Fallucchi, and Suresh Manandhar. 2010.
Estimating linear models for compositional dis-
tributional semantics. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics, pages 1263–1271.
</reference>
<page confidence="0.998893">
204
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.942030">
<title confidence="0.991997">Leveraging Preposition Ambiguity to Assess Compositional Models of Semantics</title>
<author confidence="0.987597">Long Denis Paperno</author>
<affiliation confidence="0.986698">Princeton University Indiana University University of Trento</affiliation>
<author confidence="0.987313">Marco Baroni Matthew Botvinick Adele Goldberg</author>
<affiliation confidence="0.999917">University of Trento Princeton University Princeton University</affiliation>
<abstract confidence="0.999377038461538">Complex interactions among the meanings of words are important factors in the function that maps word meanings to phrase meanings. Recently, compositional distributional semantics models (CDSM) have been designed with the goal of emulating these complex interactions; however, experimental results on the effectiveness of CDSM have been difficult to interpret because the current metrics for assessing them do not control for the confound of lexical information. We present a new method for assessing the degree to which CDSM capture semantic interactions that dissociates the influences of lexical and compositional information. We then provide a dataset for performing this type of assessment and use it to evaluate six compositional models using both co-occurrence based and neural language model input vectors. Results show that neural language input vectors are consistently superior to co-occurrence based vectors, that several CDSM capture substantial compositional information, and that, surprisingly, vector addition matches and is in many cases superior to purpose-built paramaterized models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Carmen Baneab</author>
<author>Claire Cardiec</author>
<author>Daniel Cerd</author>
</authors>
<title>Mona Diabe, Aitor Gonzalez-Agirre, Weiwei Guof, Rada Mihalceab, German Rigaua, and Janyce Wiebeg.</title>
<date>2014</date>
<booktitle>Semeval-2014 task 10: Multilingual semantic textual similarity. SemEval 2014,</booktitle>
<pages>81</pages>
<marker>Agirre, Baneab, Cardiec, Cerd, 2014</marker>
<rawString>Eneko Agirre, Carmen Baneab, Claire Cardiec, Daniel Cerd, Mona Diabe, Aitor Gonzalez-Agirre, Weiwei Guof, Rada Mihalceab, German Rigaua, and Janyce Wiebeg. 2014. Semeval-2014 task 10: Multilingual semantic textual similarity. SemEval 2014, page 81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Raffaela Bernardi</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Frege in space: A program of compositional distributional semantics.</title>
<date>2014</date>
<journal>Linguistic Issues in Language Technology,</journal>
<volume>9</volume>
<contexts>
<context position="3813" citStr="Baroni et al., 2014" startWordPosition="576" endWordPosition="579">sentence pairs is not carefully controlled, e.g., the datasets of Dolan and Brockett (2005) and Agirre 199 Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 199–204, Denver, Colorado, June 4–5, 2015. et al. (2014). One such study obtained the influential result that on such a dataset, simple composition models such as vector addition perform comparably to a state-of-the-art composition model (Blacoe and Lapata, 2012). The success of these simplistic models led to the conjecture that these data sets fail to assess critical aspects of language (Baroni et al., 2014a) and leaves open the question of whether CDSM would outperform simplistic models in a setting in which lexical cues are uninformative. In the present study, we develop a method for removing the confound of lexical cues from CDSM assessment. The method is to create a set of sentences where each sentence fits into a semantic category and where a sentence’s semantic category cannot be determined based on any individual word in the sentence. CDSM are then challenged to create a vector space in which the representations for sentences in a given category cluster together, even though the individua</context>
<context position="14335" citStr="Baroni et al. (2014" startWordPosition="2267" endWordPosition="2270">ining were built as DISSECT ‘peripheral’ spaces from phrase cooccurrence data in the count models. In the predict models, phrase vectors were learned along with word vectors in one pass, feeding all phrases of the relevant type as single tokens. The RAE vectors were computed using Socher et al.’s implementation which is trained on a 150K sentence subset of the NYT and AP sections of the Gigaword corpus. For all compositional models, we used as input two varieties of word level representations: cooccurrence based (Turney et al., 2010) and neural language model (Mikolov et al., 2013). Following Baroni et al. (2014b), we will refer to these variants as count and predict models respectively. Both word models were trained on the same corpus as those used to train the compositional models. Count was based on a 5 word window weighted with positive PMI and was reduced to 300 dimensions via SVD, while predict was based on a 5 word window using Mikolov’s continuous bag of words approach with negative sampling (Mikolov et al., 2013). These parameters were based on their strong performance in the systematic evaluation by Baroni et al. (2014b). Socher et al.’s RAE implementation composes neural language model vec</context>
<context position="15731" citStr="Baroni et al. (2014" startWordPosition="2490" endWordPosition="2493">n of these same embeddings. 4 Results The naive Bayes accuracy scores for all models are displayed in Figure 1. Addition, PLF, and the RAE each substantially outperformed concatenation, indicating that these models’ vectors contain informa202 tion about the semantic interactions between phrase constituents. Addition scored higher than PLF, while the RAE achieved comparable performance to its additive counterpart. In all cases except FA in which predict and count vectors were compared, predict achieved a higher score. This last result shows that the superiority of predict vectors documented by Baroni et al. (2014b) extends to their use in compositional models. All of the models performed well above chance accuracy of 0.2. The Wordnet based model achieved accuracy substantially above word overlap using hypernym information, indicating that although each noun is uninformative, its membership in higher level semantic categories is informative. All of the distributional models outperform the nondistributional models, except for LF and FA, which also fail to outperform concatenations of their input vectors. One explanation for the poor performance of LF and FA is that the 2.8B word corpus used to train the</context>
</contexts>
<marker>Baroni, Bernardi, Zamparelli, 2014</marker>
<rawString>Marco Baroni, Raffaela Bernardi, and Roberto Zamparelli. 2014a. Frege in space: A program of compositional distributional semantics. Linguistic Issues in Language Technology, 9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Georgiana Dinu</author>
<author>Germ´an Kruszewski</author>
</authors>
<title>Dont count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<volume>1</volume>
<contexts>
<context position="3813" citStr="Baroni et al., 2014" startWordPosition="576" endWordPosition="579">sentence pairs is not carefully controlled, e.g., the datasets of Dolan and Brockett (2005) and Agirre 199 Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 199–204, Denver, Colorado, June 4–5, 2015. et al. (2014). One such study obtained the influential result that on such a dataset, simple composition models such as vector addition perform comparably to a state-of-the-art composition model (Blacoe and Lapata, 2012). The success of these simplistic models led to the conjecture that these data sets fail to assess critical aspects of language (Baroni et al., 2014a) and leaves open the question of whether CDSM would outperform simplistic models in a setting in which lexical cues are uninformative. In the present study, we develop a method for removing the confound of lexical cues from CDSM assessment. The method is to create a set of sentences where each sentence fits into a semantic category and where a sentence’s semantic category cannot be determined based on any individual word in the sentence. CDSM are then challenged to create a vector space in which the representations for sentences in a given category cluster together, even though the individua</context>
<context position="14335" citStr="Baroni et al. (2014" startWordPosition="2267" endWordPosition="2270">ining were built as DISSECT ‘peripheral’ spaces from phrase cooccurrence data in the count models. In the predict models, phrase vectors were learned along with word vectors in one pass, feeding all phrases of the relevant type as single tokens. The RAE vectors were computed using Socher et al.’s implementation which is trained on a 150K sentence subset of the NYT and AP sections of the Gigaword corpus. For all compositional models, we used as input two varieties of word level representations: cooccurrence based (Turney et al., 2010) and neural language model (Mikolov et al., 2013). Following Baroni et al. (2014b), we will refer to these variants as count and predict models respectively. Both word models were trained on the same corpus as those used to train the compositional models. Count was based on a 5 word window weighted with positive PMI and was reduced to 300 dimensions via SVD, while predict was based on a 5 word window using Mikolov’s continuous bag of words approach with negative sampling (Mikolov et al., 2013). These parameters were based on their strong performance in the systematic evaluation by Baroni et al. (2014b). Socher et al.’s RAE implementation composes neural language model vec</context>
<context position="15731" citStr="Baroni et al. (2014" startWordPosition="2490" endWordPosition="2493">n of these same embeddings. 4 Results The naive Bayes accuracy scores for all models are displayed in Figure 1. Addition, PLF, and the RAE each substantially outperformed concatenation, indicating that these models’ vectors contain informa202 tion about the semantic interactions between phrase constituents. Addition scored higher than PLF, while the RAE achieved comparable performance to its additive counterpart. In all cases except FA in which predict and count vectors were compared, predict achieved a higher score. This last result shows that the superiority of predict vectors documented by Baroni et al. (2014b) extends to their use in compositional models. All of the models performed well above chance accuracy of 0.2. The Wordnet based model achieved accuracy substantially above word overlap using hypernym information, indicating that although each noun is uninformative, its membership in higher level semantic categories is informative. All of the distributional models outperform the nondistributional models, except for LF and FA, which also fail to outperform concatenations of their input vectors. One explanation for the poor performance of LF and FA is that the 2.8B word corpus used to train the</context>
</contexts>
<marker>Baroni, Dinu, Kruszewski, 2014</marker>
<rawString>Marco Baroni, Georgiana Dinu, and Germ´an Kruszewski. 2014b. Dont count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, volume 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Blacoe</author>
<author>Mirella Lapata</author>
</authors>
<title>A comparison of vector-based representations for semantic composition.</title>
<date>2012</date>
<booktitle>In EMNLP,</booktitle>
<pages>546--556</pages>
<contexts>
<context position="3665" citStr="Blacoe and Lapata, 2012" startWordPosition="550" endWordPosition="553">ty of pairs of sentences with similarity ratings given by humans. Many of these studies used datasets in which the amount of lexical overlap between the sentence pairs is not carefully controlled, e.g., the datasets of Dolan and Brockett (2005) and Agirre 199 Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 199–204, Denver, Colorado, June 4–5, 2015. et al. (2014). One such study obtained the influential result that on such a dataset, simple composition models such as vector addition perform comparably to a state-of-the-art composition model (Blacoe and Lapata, 2012). The success of these simplistic models led to the conjecture that these data sets fail to assess critical aspects of language (Baroni et al., 2014a) and leaves open the question of whether CDSM would outperform simplistic models in a setting in which lexical cues are uninformative. In the present study, we develop a method for removing the confound of lexical cues from CDSM assessment. The method is to create a set of sentences where each sentence fits into a semantic category and where a sentence’s semantic category cannot be determined based on any individual word in the sentence. CDSM are</context>
</contexts>
<marker>Blacoe, Lapata, 2012</marker>
<rawString>William Blacoe and Mirella Lapata. 2012. A comparison of vector-based representations for semantic composition. In EMNLP, pages 546–556.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Coecke</author>
<author>Mehrnoosh Sadrzadeh</author>
<author>Stephen Clark</author>
</authors>
<title>Mathematical foundations for a compositional distributional model of meaning. arXiv preprint arXiv:1003.4394.</title>
<date>2010</date>
<contexts>
<context position="12690" citStr="Coecke et al., 2010" startWordPosition="2013" endWordPosition="2016">ed of informativeness by design of the task. As such, we excluded this experiment from this report. 201 Figure 1: Naive Bayes accuracy scores for count and predict variants of several CDSM. Chance performance on this task was 0.2. Overlap refers to the word overlap baseline. CW refers to the vectors from Collobert and Weston (2008) . Proportion Correct 0.7 0.6 0.4 0.3 0.5 0.2 Word-Based Predict Count CW 3.1 Compositional Distributional Models We compared six models that are currently prominent in the CDSM literature: addition, multiplication (Mitchell and Lapata, 2008), lexical function (LF) (Coecke et al., 2010), practical lexical function (PLF) (Paperno et al., 2014), full additive (FA) (Guevara, 2010; Zanzotto et al., 2010), and the recursive auto-encoder (RAE) (Socher et al., 2011). The training data for LF, PLF, and FA was the UKWAC+Wikipedia+BNC 2.8 billion word corpus. In training LF, we followed Grefenstette et al. (2013), employing a two-step training regime using corpus-extracted vectors for noun–preposition–noun combinations to estimate matrices of corresponding prepositional phrases, which were in turn used to estimate a three-way tensor of each preposition. For PLF and FA, we learned sepa</context>
</contexts>
<marker>Coecke, Sadrzadeh, Clark, 2010</marker>
<rawString>Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark. 2010. Mathematical foundations for a compositional distributional model of meaning. arXiv preprint arXiv:1003.4394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th international conference on Machine learning,</booktitle>
<pages>160--167</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="12403" citStr="Collobert and Weston (2008)" startWordPosition="1969" endWordPosition="1972"> experiment we considered was to test the models on the dataset phrases with prepositions removed. However, LF and PLF are undefined for such an input, and the element-wise models trivially perform better with the preposition included because the preposition is the only word that is not stripped of informativeness by design of the task. As such, we excluded this experiment from this report. 201 Figure 1: Naive Bayes accuracy scores for count and predict variants of several CDSM. Chance performance on this task was 0.2. Overlap refers to the word overlap baseline. CW refers to the vectors from Collobert and Weston (2008) . Proportion Correct 0.7 0.6 0.4 0.3 0.5 0.2 Word-Based Predict Count CW 3.1 Compositional Distributional Models We compared six models that are currently prominent in the CDSM literature: addition, multiplication (Mitchell and Lapata, 2008), lexical function (LF) (Coecke et al., 2010), practical lexical function (PLF) (Paperno et al., 2014), full additive (FA) (Guevara, 2010; Zanzotto et al., 2010), and the recursive auto-encoder (RAE) (Socher et al., 2011). The training data for LF, PLF, and FA was the UKWAC+Wikipedia+BNC 2.8 billion word corpus. In training LF, we followed Grefenstette et </context>
<context position="14980" citStr="Collobert and Weston (2008)" startWordPosition="2374" endWordPosition="2377">to these variants as count and predict models respectively. Both word models were trained on the same corpus as those used to train the compositional models. Count was based on a 5 word window weighted with positive PMI and was reduced to 300 dimensions via SVD, while predict was based on a 5 word window using Mikolov’s continuous bag of words approach with negative sampling (Mikolov et al., 2013). These parameters were based on their strong performance in the systematic evaluation by Baroni et al. (2014b). Socher et al.’s RAE implementation composes neural language model vectors described by Collobert and Weston (2008) and supplied by Turian et al. (2010). For comparison with the RAE, we report results for addition, multiplication, and concatenation of these same embeddings. 4 Results The naive Bayes accuracy scores for all models are displayed in Figure 1. Addition, PLF, and the RAE each substantially outperformed concatenation, indicating that these models’ vectors contain informa202 tion about the semantic interactions between phrase constituents. Addition scored higher than PLF, while the RAE achieved comparable performance to its additive counterpart. In all cases except FA in which predict and count v</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, pages 160–167. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott C Deerwester</author>
<author>Susan T Dumais</author>
<author>Thomas K Landauer</author>
<author>George W Furnas</author>
<author>Richard A Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>JASIS,</journal>
<volume>41</volume>
<issue>6</issue>
<contexts>
<context position="2375" citStr="Deerwester et al., 1990" startWordPosition="354" endWordPosition="358">a political affiliation to the third. This is an example of a common phenomenon in natural language in which the meaning of a whole expression is not derived from a simple concatenation of its parts, but is composed by interactions among their meanings. Cognitive and computer scientists have pointed out this complexity and proposed various models for accommodating it (Kintsch, 2001; Mitchell and Lapata, 2010; Socher et al., 2013). A dominant modeling approach seeks to learn functions that combine word representations derived from the distributional structure of large natural language corpora (Deerwester et al., 1990; Landauer and Dumais, 1997). Because the word representations to be combined and the compositional functions are generated based on the distributions of words in corpora, these models have been dubbed compositional distributional semantic models, or CDSM (Marelli et al., 2014). CDSM produce fixed-dimensional vector representations of arbitrary sentences and phrases, and the foundational principle of these models is, stated simply, that semantically similar phrases should have vector representations that are close together in the vector space. 1.1 CDSM Assessment Past studies have tested how w</context>
</contexts>
<marker>Deerwester, Dumais, Landauer, Furnas, Harshman, 1990</marker>
<rawString>Scott C. Deerwester, Susan T Dumais, Thomas K. Landauer, George W. Furnas, and Richard A. Harshman. 1990. Indexing by latent semantic analysis. JASIS, 41(6):391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William B Dolan</author>
<author>Chris Brockett</author>
</authors>
<title>Automatically constructing a corpus of sentential paraphrases.</title>
<date>2005</date>
<booktitle>In Proc. of IWP.</booktitle>
<contexts>
<context position="3285" citStr="Dolan and Brockett (2005)" startWordPosition="492" endWordPosition="495">produce fixed-dimensional vector representations of arbitrary sentences and phrases, and the foundational principle of these models is, stated simply, that semantically similar phrases should have vector representations that are close together in the vector space. 1.1 CDSM Assessment Past studies have tested how well CDSM adhere to this principle by comparing the vector similarity of pairs of sentences with similarity ratings given by humans. Many of these studies used datasets in which the amount of lexical overlap between the sentence pairs is not carefully controlled, e.g., the datasets of Dolan and Brockett (2005) and Agirre 199 Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 199–204, Denver, Colorado, June 4–5, 2015. et al. (2014). One such study obtained the influential result that on such a dataset, simple composition models such as vector addition perform comparably to a state-of-the-art composition model (Blacoe and Lapata, 2012). The success of these simplistic models led to the conjecture that these data sets fail to assess critical aspects of language (Baroni et al., 2014a) and leaves open the question of whether CDSM would outperform simplis</context>
<context position="7633" citStr="Dolan and Brockett, 2005" startWordPosition="1194" endWordPosition="1197">, in principle, be applied to any type of phrase. This allowed us to build a dataset that extends the current landmark word and word matching datasets in at least two important ways. First, it includes function words, specifically prepositions. Second, it requires the characterization of interactions among three words in each expression, whereas previous datasets had two words per expression, or subsets of the words did not interact in complex ways. Other important approaches to CDSM assessment include rating the similarity of sentence pairs, determining whether two sentences are paraphrases (Dolan and Brockett, 2005), classifying the entailment relationship between two sentences (Marelli et al., 2014), classifying the relationship between two entities named in a sentence (Hendrickx et al., 2009), and classifying the valence of the sentiment expressed in a sentence (Socher et al., 2013). These methods have primarily been aimed at assessing CDSM on the full array of constructions inherent in naturally generated language, while our method aims to isolate a specific construction of interest. 200 Category Example Adhesion to Vertical Surface “There is a magnet on the refrigerator.” Support by Horizontal Surfac</context>
</contexts>
<marker>Dolan, Brockett, 2005</marker>
<rawString>William B Dolan and Chris Brockett. 2005. Automatically constructing a corpus of sentential paraphrases. In Proc. of IWP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Grefenstette</author>
<author>Mehrnoosh Sadrzadeh</author>
</authors>
<title>Experimental support for a categorical compositional distributional model of meaning.</title>
<date>2011</date>
<booktitle>In EMNLP,</booktitle>
<pages>1394--1404</pages>
<contexts>
<context position="6660" citStr="Grefenstette and Sadrzadeh, 2011" startWordPosition="1040" endWordPosition="1043"> compositionality to score well in classification. 1.2 Relation to Past Work This approach to CDSM assessment is similar to a previous method wherein polysemous verbs are paired with disambiguating nouns in transitive or intransitive verb phrases. These phrases are then matched with “landmark” verbs that are either similar or not similar in meaning to the full phrase. CDSM are then challenged to create representations of the phrases from which classifiers can determine whether or not a phrase is similar to its landmark verb (Kintsch, 2001; Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Grefenstette and Sadrzadeh, 2011). Another notable CDSM assessment task involves matching a phrase with a word with a similar meaning, for example, matching a short dictionary definition with the word it defines (Kartsaklis et al., 2012; Turney, 2014). While these methods are applicable only to simple phrases that can be mapped reasonably to a single word, the present method can, in principle, be applied to any type of phrase. This allowed us to build a dataset that extends the current landmark word and word matching datasets in at least two important ways. First, it includes function words, specifically prepositions. Second,</context>
</contexts>
<marker>Grefenstette, Sadrzadeh, 2011</marker>
<rawString>Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011. Experimental support for a categorical compositional distributional model of meaning. In EMNLP, pages 1394–1404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Grefenstette</author>
<author>Georgiana Dinu</author>
<author>Yao-Zhong Zhang</author>
<author>Mehrnoosh Sadrzadeh</author>
<author>Marco Baroni</author>
</authors>
<title>Multi-step regression learning for compositional distributional semantics.</title>
<date>2013</date>
<booktitle>In Proceedings of IWCS,</booktitle>
<pages>131--142</pages>
<location>Potsdam, Germany.</location>
<contexts>
<context position="13013" citStr="Grefenstette et al. (2013)" startWordPosition="2066" endWordPosition="2069">nd Weston (2008) . Proportion Correct 0.7 0.6 0.4 0.3 0.5 0.2 Word-Based Predict Count CW 3.1 Compositional Distributional Models We compared six models that are currently prominent in the CDSM literature: addition, multiplication (Mitchell and Lapata, 2008), lexical function (LF) (Coecke et al., 2010), practical lexical function (PLF) (Paperno et al., 2014), full additive (FA) (Guevara, 2010; Zanzotto et al., 2010), and the recursive auto-encoder (RAE) (Socher et al., 2011). The training data for LF, PLF, and FA was the UKWAC+Wikipedia+BNC 2.8 billion word corpus. In training LF, we followed Grefenstette et al. (2013), employing a two-step training regime using corpus-extracted vectors for noun–preposition–noun combinations to estimate matrices of corresponding prepositional phrases, which were in turn used to estimate a three-way tensor of each preposition. For PLF and FA, we learned separate matrices for combining prepositions with each of the two nouns in the construction, using corpus-based vectors of prepositional phrases for training preposition–noun combination. For training composition of the head noun with the prepositional phrase, we used corpusextracted noun+preposition (for lexical matrices in </context>
</contexts>
<marker>Grefenstette, Dinu, Zhang, Sadrzadeh, Baroni, 2013</marker>
<rawString>Edward Grefenstette, Georgiana Dinu, Yao-Zhong Zhang, Mehrnoosh Sadrzadeh, and Marco Baroni. 2013. Multi-step regression learning for compositional distributional semantics. In Proceedings of IWCS, pages 131–142, Potsdam, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emiliano Guevara</author>
</authors>
<title>A regression model of adjective-noun compositionality in distributional semantics.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Workshop on Geometrical Models of Natural Language Semantics,</booktitle>
<pages>33--37</pages>
<contexts>
<context position="12782" citStr="Guevara, 2010" startWordPosition="2029" endWordPosition="2030"> 201 Figure 1: Naive Bayes accuracy scores for count and predict variants of several CDSM. Chance performance on this task was 0.2. Overlap refers to the word overlap baseline. CW refers to the vectors from Collobert and Weston (2008) . Proportion Correct 0.7 0.6 0.4 0.3 0.5 0.2 Word-Based Predict Count CW 3.1 Compositional Distributional Models We compared six models that are currently prominent in the CDSM literature: addition, multiplication (Mitchell and Lapata, 2008), lexical function (LF) (Coecke et al., 2010), practical lexical function (PLF) (Paperno et al., 2014), full additive (FA) (Guevara, 2010; Zanzotto et al., 2010), and the recursive auto-encoder (RAE) (Socher et al., 2011). The training data for LF, PLF, and FA was the UKWAC+Wikipedia+BNC 2.8 billion word corpus. In training LF, we followed Grefenstette et al. (2013), employing a two-step training regime using corpus-extracted vectors for noun–preposition–noun combinations to estimate matrices of corresponding prepositional phrases, which were in turn used to estimate a three-way tensor of each preposition. For PLF and FA, we learned separate matrices for combining prepositions with each of the two nouns in the construction, usi</context>
</contexts>
<marker>Guevara, 2010</marker>
<rawString>Emiliano Guevara. 2010. A regression model of adjective-noun compositionality in distributional semantics. In Proceedings of the 2010 Workshop on Geometrical Models of Natural Language Semantics, pages 33–37.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Iris Hendrickx</author>
<author>Su Nam Kim</author>
<author>Zornitsa Kozareva</author>
<author>Preslav Nakov</author>
<author>Diarmuid O´ S´eaghdha</author>
<author>Sebastian Pad´o</author>
<author>Marco Pennacchiotti</author>
<author>Lorenza Romano</author>
<author>Stan Szpakowicz</author>
</authors>
<title>Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions,</booktitle>
<pages>94--99</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Hendrickx, Kim, Kozareva, Nakov, S´eaghdha, Pad´o, Pennacchiotti, Romano, Szpakowicz, 2009</marker>
<rawString>Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav Nakov, Diarmuid O´ S´eaghdha, Sebastian Pad´o, Marco Pennacchiotti, Lorenza Romano, and Stan Szpakowicz. 2009. Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals. In Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 94–99. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Annette Herskovits</author>
</authors>
<title>Semantics and pragmatics of locative expressions*.</title>
<date>1985</date>
<journal>Cognitive Science,</journal>
<volume>9</volume>
<issue>3</issue>
<pages>378</pages>
<contexts>
<context position="5208" citStr="Herskovits, 1985" startWordPosition="803" endWordPosition="804">eld out sentences. Here, we build a suitable test set by leveraging the lexical ambiguity inherent in locative expressions. Locative expressions are phrases that describe a spatial relationship between two objects using two nouns joined by a preposition; for example, “The magnet is on the refrigerator”, which describes the relationship of adhesion to a vertical surface. Crucially, the spatial relationship between the two nouns in a locative expression is undetermined by the spatial preposition, and can only be determined based on semantic interactions among the prepositions and the two nouns (Herskovits, 1985). For example, while “The magnet is on the refrigerator” describes the spatial relationship of adhesion to a vertical surface, “The apple is on the refrigerator” describes support by a horizontal surface. In order to classify a new sentence, e.g., “The magnet is on the papers”, into the correct category of support by a horizontal surface, the CDSM vectors for the three sentences must encode the fact that “The magnet is on the papers” shares a common spatial relationship with “The apple is on the refrigerator” and not with “The magnet is on the refrigerator”, even though the latter pair of sent</context>
<context position="8755" citStr="Herskovits (1985)" startWordPosition="1373" endWordPosition="1374">esion to Vertical Surface “There is a magnet on the refrigerator.” Support by Horizontal Surface “There is an apple on the refrigerator.” Support from Above “There is an apple on the branch.” Full Containment “There is an apple in the refrigerator.” Partial Containment “There is an apple in the water.” Table 1: Categories and Example Sentences 2 The Dataset A list of all of the spatial categories with examples is given in Table 1. The authors chose the set of categories to produce the desired dissociation between lexical meaning and phrase category, taking inspiration from the observations of Herskovits (1985). To produce a dataset of expressions fitting these categories, the first and second authors - both native English speakers - generated a large set of locative expressions, intending each expression for a specific category. Then all of the expressions were independently rated by the first two authors, and any expression for which the ratings disagreed were excluded from the dataset. In order to achieve a balanced category size, the second author then created additional sentences intended for underrepresented categories. All additional sentences were stripped of labels and rated independently b</context>
</contexts>
<marker>Herskovits, 1985</marker>
<rawString>Annette Herskovits. 1985. Semantics and pragmatics of locative expressions*. Cognitive Science, 9(3):341– 378.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dimitri Kartsaklis</author>
<author>Mehrnoosh Sadrzadeh</author>
<author>Stephen Pulman</author>
</authors>
<title>A unified sentence space for categorical distributional-compositional semantics: Theory and experiments.</title>
<date>2012</date>
<booktitle>In In Proceedings of COLING:</booktitle>
<publisher>Posters. Citeseer.</publisher>
<contexts>
<context position="6863" citStr="Kartsaklis et al., 2012" startWordPosition="1073" endWordPosition="1076">tive or intransitive verb phrases. These phrases are then matched with “landmark” verbs that are either similar or not similar in meaning to the full phrase. CDSM are then challenged to create representations of the phrases from which classifiers can determine whether or not a phrase is similar to its landmark verb (Kintsch, 2001; Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Grefenstette and Sadrzadeh, 2011). Another notable CDSM assessment task involves matching a phrase with a word with a similar meaning, for example, matching a short dictionary definition with the word it defines (Kartsaklis et al., 2012; Turney, 2014). While these methods are applicable only to simple phrases that can be mapped reasonably to a single word, the present method can, in principle, be applied to any type of phrase. This allowed us to build a dataset that extends the current landmark word and word matching datasets in at least two important ways. First, it includes function words, specifically prepositions. Second, it requires the characterization of interactions among three words in each expression, whereas previous datasets had two words per expression, or subsets of the words did not interact in complex ways. O</context>
</contexts>
<marker>Kartsaklis, Sadrzadeh, Pulman, 2012</marker>
<rawString>Dimitri Kartsaklis, Mehrnoosh Sadrzadeh, and Stephen Pulman. 2012. A unified sentence space for categorical distributional-compositional semantics: Theory and experiments. In In Proceedings of COLING: Posters. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Kintsch</author>
</authors>
<date>2001</date>
<journal>Predication. Cognitive Science,</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="2136" citStr="Kintsch, 2001" startWordPosition="320" endWordPosition="321">meaning of the word “red” in each of these examples interacts with the meaning of the noun it modifies, applying ∗Please address correspondence to the first author at swritter@princeton.edu a different color to the first two and a political affiliation to the third. This is an example of a common phenomenon in natural language in which the meaning of a whole expression is not derived from a simple concatenation of its parts, but is composed by interactions among their meanings. Cognitive and computer scientists have pointed out this complexity and proposed various models for accommodating it (Kintsch, 2001; Mitchell and Lapata, 2010; Socher et al., 2013). A dominant modeling approach seeks to learn functions that combine word representations derived from the distributional structure of large natural language corpora (Deerwester et al., 1990; Landauer and Dumais, 1997). Because the word representations to be combined and the compositional functions are generated based on the distributions of words in corpora, these models have been dubbed compositional distributional semantic models, or CDSM (Marelli et al., 2014). CDSM produce fixed-dimensional vector representations of arbitrary sentences and </context>
<context position="6571" citStr="Kintsch, 2001" startWordPosition="1030" endWordPosition="1031">wherein lexical information is uninformative, and models must rely on compositionality to score well in classification. 1.2 Relation to Past Work This approach to CDSM assessment is similar to a previous method wherein polysemous verbs are paired with disambiguating nouns in transitive or intransitive verb phrases. These phrases are then matched with “landmark” verbs that are either similar or not similar in meaning to the full phrase. CDSM are then challenged to create representations of the phrases from which classifiers can determine whether or not a phrase is similar to its landmark verb (Kintsch, 2001; Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Grefenstette and Sadrzadeh, 2011). Another notable CDSM assessment task involves matching a phrase with a word with a similar meaning, for example, matching a short dictionary definition with the word it defines (Kartsaklis et al., 2012; Turney, 2014). While these methods are applicable only to simple phrases that can be mapped reasonably to a single word, the present method can, in principle, be applied to any type of phrase. This allowed us to build a dataset that extends the current landmark word and word matching datasets in at least </context>
</contexts>
<marker>Kintsch, 2001</marker>
<rawString>Walter Kintsch. 2001. Predication. Cognitive Science, 25(2):173–202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Susan T Dumais</author>
</authors>
<title>A solution to plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological review,</journal>
<volume>104</volume>
<issue>2</issue>
<contexts>
<context position="2403" citStr="Landauer and Dumais, 1997" startWordPosition="359" endWordPosition="362">o the third. This is an example of a common phenomenon in natural language in which the meaning of a whole expression is not derived from a simple concatenation of its parts, but is composed by interactions among their meanings. Cognitive and computer scientists have pointed out this complexity and proposed various models for accommodating it (Kintsch, 2001; Mitchell and Lapata, 2010; Socher et al., 2013). A dominant modeling approach seeks to learn functions that combine word representations derived from the distributional structure of large natural language corpora (Deerwester et al., 1990; Landauer and Dumais, 1997). Because the word representations to be combined and the compositional functions are generated based on the distributions of words in corpora, these models have been dubbed compositional distributional semantic models, or CDSM (Marelli et al., 2014). CDSM produce fixed-dimensional vector representations of arbitrary sentences and phrases, and the foundational principle of these models is, stated simply, that semantically similar phrases should have vector representations that are close together in the vector space. 1.1 CDSM Assessment Past studies have tested how well CDSM adhere to this prin</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Thomas K Landauer and Susan T Dumais. 1997. A solution to plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological review, 104(2):211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Marelli</author>
<author>Luisa Bentivogli</author>
<author>Marco Baroni</author>
<author>Raffaella Bernardi</author>
<author>Stefano Menini</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment.</title>
<date>2014</date>
<tech>SemEval-2014.</tech>
<contexts>
<context position="2653" citStr="Marelli et al., 2014" startWordPosition="396" endWordPosition="399">entists have pointed out this complexity and proposed various models for accommodating it (Kintsch, 2001; Mitchell and Lapata, 2010; Socher et al., 2013). A dominant modeling approach seeks to learn functions that combine word representations derived from the distributional structure of large natural language corpora (Deerwester et al., 1990; Landauer and Dumais, 1997). Because the word representations to be combined and the compositional functions are generated based on the distributions of words in corpora, these models have been dubbed compositional distributional semantic models, or CDSM (Marelli et al., 2014). CDSM produce fixed-dimensional vector representations of arbitrary sentences and phrases, and the foundational principle of these models is, stated simply, that semantically similar phrases should have vector representations that are close together in the vector space. 1.1 CDSM Assessment Past studies have tested how well CDSM adhere to this principle by comparing the vector similarity of pairs of sentences with similarity ratings given by humans. Many of these studies used datasets in which the amount of lexical overlap between the sentence pairs is not carefully controlled, e.g., the datas</context>
<context position="7719" citStr="Marelli et al., 2014" startWordPosition="1206" endWordPosition="1209">extends the current landmark word and word matching datasets in at least two important ways. First, it includes function words, specifically prepositions. Second, it requires the characterization of interactions among three words in each expression, whereas previous datasets had two words per expression, or subsets of the words did not interact in complex ways. Other important approaches to CDSM assessment include rating the similarity of sentence pairs, determining whether two sentences are paraphrases (Dolan and Brockett, 2005), classifying the entailment relationship between two sentences (Marelli et al., 2014), classifying the relationship between two entities named in a sentence (Hendrickx et al., 2009), and classifying the valence of the sentiment expressed in a sentence (Socher et al., 2013). These methods have primarily been aimed at assessing CDSM on the full array of constructions inherent in naturally generated language, while our method aims to isolate a specific construction of interest. 200 Category Example Adhesion to Vertical Surface “There is a magnet on the refrigerator.” Support by Horizontal Surface “There is an apple on the refrigerator.” Support from Above “There is an apple on th</context>
</contexts>
<marker>Marelli, Bentivogli, Baroni, Bernardi, Menini, Zamparelli, 2014</marker>
<rawString>Marco Marelli, Luisa Bentivogli, Marco Baroni, Raffaella Bernardi, Stefano Menini, and Roberto Zamparelli. 2014. Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment. SemEval-2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</title>
<date>2013</date>
<contexts>
<context position="14304" citStr="Mikolov et al., 2013" startWordPosition="2262" endWordPosition="2265">) vectors. Phrase vectors for training were built as DISSECT ‘peripheral’ spaces from phrase cooccurrence data in the count models. In the predict models, phrase vectors were learned along with word vectors in one pass, feeding all phrases of the relevant type as single tokens. The RAE vectors were computed using Socher et al.’s implementation which is trained on a 150K sentence subset of the NYT and AP sections of the Gigaword corpus. For all compositional models, we used as input two varieties of word level representations: cooccurrence based (Turney et al., 2010) and neural language model (Mikolov et al., 2013). Following Baroni et al. (2014b), we will refer to these variants as count and predict models respectively. Both word models were trained on the same corpus as those used to train the compositional models. Count was based on a 5 word window weighted with positive PMI and was reduced to 300 dimensions via SVD, while predict was based on a 5 word window using Mikolov’s continuous bag of words approach with negative sampling (Mikolov et al., 2013). These parameters were based on their strong performance in the systematic evaluation by Baroni et al. (2014b). Socher et al.’s RAE implementation com</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Vector-based models of semantic composition.</title>
<date>2008</date>
<booktitle>In ACL,</booktitle>
<pages>236--244</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="6598" citStr="Mitchell and Lapata, 2008" startWordPosition="1032" endWordPosition="1035"> information is uninformative, and models must rely on compositionality to score well in classification. 1.2 Relation to Past Work This approach to CDSM assessment is similar to a previous method wherein polysemous verbs are paired with disambiguating nouns in transitive or intransitive verb phrases. These phrases are then matched with “landmark” verbs that are either similar or not similar in meaning to the full phrase. CDSM are then challenged to create representations of the phrases from which classifiers can determine whether or not a phrase is similar to its landmark verb (Kintsch, 2001; Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Grefenstette and Sadrzadeh, 2011). Another notable CDSM assessment task involves matching a phrase with a word with a similar meaning, for example, matching a short dictionary definition with the word it defines (Kartsaklis et al., 2012; Turney, 2014). While these methods are applicable only to simple phrases that can be mapped reasonably to a single word, the present method can, in principle, be applied to any type of phrase. This allowed us to build a dataset that extends the current landmark word and word matching datasets in at least two important ways. First, </context>
<context position="12645" citStr="Mitchell and Lapata, 2008" startWordPosition="2006" endWordPosition="2009">the preposition is the only word that is not stripped of informativeness by design of the task. As such, we excluded this experiment from this report. 201 Figure 1: Naive Bayes accuracy scores for count and predict variants of several CDSM. Chance performance on this task was 0.2. Overlap refers to the word overlap baseline. CW refers to the vectors from Collobert and Weston (2008) . Proportion Correct 0.7 0.6 0.4 0.3 0.5 0.2 Word-Based Predict Count CW 3.1 Compositional Distributional Models We compared six models that are currently prominent in the CDSM literature: addition, multiplication (Mitchell and Lapata, 2008), lexical function (LF) (Coecke et al., 2010), practical lexical function (PLF) (Paperno et al., 2014), full additive (FA) (Guevara, 2010; Zanzotto et al., 2010), and the recursive auto-encoder (RAE) (Socher et al., 2011). The training data for LF, PLF, and FA was the UKWAC+Wikipedia+BNC 2.8 billion word corpus. In training LF, we followed Grefenstette et al. (2013), employing a two-step training regime using corpus-extracted vectors for noun–preposition–noun combinations to estimate matrices of corresponding prepositional phrases, which were in turn used to estimate a three-way tensor of each</context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. In ACL, pages 236– 244. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Composition in distributional models of semantics.</title>
<date>2010</date>
<journal>Cognitive science,</journal>
<volume>34</volume>
<issue>8</issue>
<contexts>
<context position="2163" citStr="Mitchell and Lapata, 2010" startWordPosition="322" endWordPosition="326">word “red” in each of these examples interacts with the meaning of the noun it modifies, applying ∗Please address correspondence to the first author at swritter@princeton.edu a different color to the first two and a political affiliation to the third. This is an example of a common phenomenon in natural language in which the meaning of a whole expression is not derived from a simple concatenation of its parts, but is composed by interactions among their meanings. Cognitive and computer scientists have pointed out this complexity and proposed various models for accommodating it (Kintsch, 2001; Mitchell and Lapata, 2010; Socher et al., 2013). A dominant modeling approach seeks to learn functions that combine word representations derived from the distributional structure of large natural language corpora (Deerwester et al., 1990; Landauer and Dumais, 1997). Because the word representations to be combined and the compositional functions are generated based on the distributions of words in corpora, these models have been dubbed compositional distributional semantic models, or CDSM (Marelli et al., 2014). CDSM produce fixed-dimensional vector representations of arbitrary sentences and phrases, and the foundation</context>
<context position="6625" citStr="Mitchell and Lapata, 2010" startWordPosition="1036" endWordPosition="1039">ve, and models must rely on compositionality to score well in classification. 1.2 Relation to Past Work This approach to CDSM assessment is similar to a previous method wherein polysemous verbs are paired with disambiguating nouns in transitive or intransitive verb phrases. These phrases are then matched with “landmark” verbs that are either similar or not similar in meaning to the full phrase. CDSM are then challenged to create representations of the phrases from which classifiers can determine whether or not a phrase is similar to its landmark verb (Kintsch, 2001; Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Grefenstette and Sadrzadeh, 2011). Another notable CDSM assessment task involves matching a phrase with a word with a similar meaning, for example, matching a short dictionary definition with the word it defines (Kartsaklis et al., 2012; Turney, 2014). While these methods are applicable only to simple phrases that can be mapped reasonably to a single word, the present method can, in principle, be applied to any type of phrase. This allowed us to build a dataset that extends the current landmark word and word matching datasets in at least two important ways. First, it includes function words,</context>
</contexts>
<marker>Mitchell, Lapata, 2010</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2010. Composition in distributional models of semantics. Cognitive science, 34(8):1388–1429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Denis Paperno</author>
<author>Nghia The Pham</author>
<author>Marco Baroni</author>
</authors>
<title>A practical and linguistically-motivated approach to compositional distributional semantics.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>90--99</pages>
<location>Baltimore, Maryland,</location>
<contexts>
<context position="12747" citStr="Paperno et al., 2014" startWordPosition="2022" endWordPosition="2025"> excluded this experiment from this report. 201 Figure 1: Naive Bayes accuracy scores for count and predict variants of several CDSM. Chance performance on this task was 0.2. Overlap refers to the word overlap baseline. CW refers to the vectors from Collobert and Weston (2008) . Proportion Correct 0.7 0.6 0.4 0.3 0.5 0.2 Word-Based Predict Count CW 3.1 Compositional Distributional Models We compared six models that are currently prominent in the CDSM literature: addition, multiplication (Mitchell and Lapata, 2008), lexical function (LF) (Coecke et al., 2010), practical lexical function (PLF) (Paperno et al., 2014), full additive (FA) (Guevara, 2010; Zanzotto et al., 2010), and the recursive auto-encoder (RAE) (Socher et al., 2011). The training data for LF, PLF, and FA was the UKWAC+Wikipedia+BNC 2.8 billion word corpus. In training LF, we followed Grefenstette et al. (2013), employing a two-step training regime using corpus-extracted vectors for noun–preposition–noun combinations to estimate matrices of corresponding prepositional phrases, which were in turn used to estimate a three-way tensor of each preposition. For PLF and FA, we learned separate matrices for combining prepositions with each of the</context>
</contexts>
<marker>Paperno, Pham, Baroni, 2014</marker>
<rawString>Denis Paperno, Nghia The Pham, and Marco Baroni. 2014. A practical and linguistically-motivated approach to compositional distributional semantics. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 90–99, Baltimore, Maryland, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Eric H Huang</author>
<author>Jeffrey Pennington</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection.</title>
<date>2011</date>
<booktitle>In Advances in Neural Information Processing Systems 24.</booktitle>
<contexts>
<context position="12866" citStr="Socher et al., 2011" startWordPosition="2041" endWordPosition="2044">several CDSM. Chance performance on this task was 0.2. Overlap refers to the word overlap baseline. CW refers to the vectors from Collobert and Weston (2008) . Proportion Correct 0.7 0.6 0.4 0.3 0.5 0.2 Word-Based Predict Count CW 3.1 Compositional Distributional Models We compared six models that are currently prominent in the CDSM literature: addition, multiplication (Mitchell and Lapata, 2008), lexical function (LF) (Coecke et al., 2010), practical lexical function (PLF) (Paperno et al., 2014), full additive (FA) (Guevara, 2010; Zanzotto et al., 2010), and the recursive auto-encoder (RAE) (Socher et al., 2011). The training data for LF, PLF, and FA was the UKWAC+Wikipedia+BNC 2.8 billion word corpus. In training LF, we followed Grefenstette et al. (2013), employing a two-step training regime using corpus-extracted vectors for noun–preposition–noun combinations to estimate matrices of corresponding prepositional phrases, which were in turn used to estimate a three-way tensor of each preposition. For PLF and FA, we learned separate matrices for combining prepositions with each of the two nouns in the construction, using corpus-based vectors of prepositional phrases for training preposition–noun combi</context>
</contexts>
<marker>Socher, Huang, Pennington, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Eric H. Huang, Jeffrey Pennington, Andrew Y. Ng, and Christopher D. Manning. 2011. Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection. In Advances in Neural Information Processing Systems 24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Y Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank. In</title>
<date>2013</date>
<booktitle>EMNLP,</booktitle>
<pages>1631--1642</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="2185" citStr="Socher et al., 2013" startWordPosition="327" endWordPosition="330"> examples interacts with the meaning of the noun it modifies, applying ∗Please address correspondence to the first author at swritter@princeton.edu a different color to the first two and a political affiliation to the third. This is an example of a common phenomenon in natural language in which the meaning of a whole expression is not derived from a simple concatenation of its parts, but is composed by interactions among their meanings. Cognitive and computer scientists have pointed out this complexity and proposed various models for accommodating it (Kintsch, 2001; Mitchell and Lapata, 2010; Socher et al., 2013). A dominant modeling approach seeks to learn functions that combine word representations derived from the distributional structure of large natural language corpora (Deerwester et al., 1990; Landauer and Dumais, 1997). Because the word representations to be combined and the compositional functions are generated based on the distributions of words in corpora, these models have been dubbed compositional distributional semantic models, or CDSM (Marelli et al., 2014). CDSM produce fixed-dimensional vector representations of arbitrary sentences and phrases, and the foundational principle of these </context>
<context position="7907" citStr="Socher et al., 2013" startWordPosition="1235" endWordPosition="1238">zation of interactions among three words in each expression, whereas previous datasets had two words per expression, or subsets of the words did not interact in complex ways. Other important approaches to CDSM assessment include rating the similarity of sentence pairs, determining whether two sentences are paraphrases (Dolan and Brockett, 2005), classifying the entailment relationship between two sentences (Marelli et al., 2014), classifying the relationship between two entities named in a sentence (Hendrickx et al., 2009), and classifying the valence of the sentiment expressed in a sentence (Socher et al., 2013). These methods have primarily been aimed at assessing CDSM on the full array of constructions inherent in naturally generated language, while our method aims to isolate a specific construction of interest. 200 Category Example Adhesion to Vertical Surface “There is a magnet on the refrigerator.” Support by Horizontal Surface “There is an apple on the refrigerator.” Support from Above “There is an apple on the branch.” Full Containment “There is an apple in the refrigerator.” Partial Containment “There is an apple in the water.” Table 1: Categories and Example Sentences 2 The Dataset A list of</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In EMNLP, pages 1631–1642. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Srikumar</author>
<author>D Roth</author>
</authors>
<title>Modeling semantic relations expressed by prepositions.</title>
<date>2013</date>
<booktitle>In Transactions of the Association for Computational Linguistics,</booktitle>
<volume>1</volume>
<pages>231--242</pages>
<contexts>
<context position="11121" citStr="Srikumar and Roth (2013)" startWordPosition="1759" endWordPosition="1762"> and a set of 420 sentences with at least one informative noun. By this method, we ensured that no component of the models’ classification accuracy on the test set is due to the recognition of individual nouns. In addition to the CDSM, we included two nondistributional models for comparison. The first, referred to as word overlap, consists of a binary feature vector containing one feature per vocabulary item. This model’s performance provides an upper-bound on the performance that a model can achieve given only the distribution of word tokens in the training set. The second model, inspired by Srikumar and Roth (2013), contains binary features for Wordnet hypernyms (up to 4 levels) of each sense of the noun and a binary feature for each preposition. This model’s score provides an indication of the amount of task-relevant information contained in the taxonomic features of individual words. We compared CDSM to a further control that consisted of the concatenation of the word vectors. The concatenated vectors contain a complete representation of all of the individual word information, so that any performance the CDSM can achieve above the concatenation score can be attributed to semantic interaction informati</context>
</contexts>
<marker>Srikumar, Roth, 2013</marker>
<rawString>V. Srikumar and D. Roth. 2013. Modeling semantic relations expressed by prepositions. In Transactions of the Association for Computational Linguistics, volume 1, pages 231–242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>384--394</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="15017" citStr="Turian et al. (2010)" startWordPosition="2381" endWordPosition="2384">s respectively. Both word models were trained on the same corpus as those used to train the compositional models. Count was based on a 5 word window weighted with positive PMI and was reduced to 300 dimensions via SVD, while predict was based on a 5 word window using Mikolov’s continuous bag of words approach with negative sampling (Mikolov et al., 2013). These parameters were based on their strong performance in the systematic evaluation by Baroni et al. (2014b). Socher et al.’s RAE implementation composes neural language model vectors described by Collobert and Weston (2008) and supplied by Turian et al. (2010). For comparison with the RAE, we report results for addition, multiplication, and concatenation of these same embeddings. 4 Results The naive Bayes accuracy scores for all models are displayed in Figure 1. Addition, PLF, and the RAE each substantially outperformed concatenation, indicating that these models’ vectors contain informa202 tion about the semantic interactions between phrase constituents. Addition scored higher than PLF, while the RAE achieved comparable performance to its additive counterpart. In all cases except FA in which predict and count vectors were compared, predict achieve</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 384–394. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of artificial intelligence research,</journal>
<volume>37</volume>
<issue>1</issue>
<pages>188</pages>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D Turney, Patrick Pantel, et al. 2010. From frequency to meaning: Vector space models of semantics. Journal of artificial intelligence research, 37(1):141– 188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Semantic composition and decomposition: From recognition to generation. arXiv preprint arXiv:1405.7908.</title>
<date>2014</date>
<contexts>
<context position="6878" citStr="Turney, 2014" startWordPosition="1077" endWordPosition="1078"> phrases. These phrases are then matched with “landmark” verbs that are either similar or not similar in meaning to the full phrase. CDSM are then challenged to create representations of the phrases from which classifiers can determine whether or not a phrase is similar to its landmark verb (Kintsch, 2001; Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Grefenstette and Sadrzadeh, 2011). Another notable CDSM assessment task involves matching a phrase with a word with a similar meaning, for example, matching a short dictionary definition with the word it defines (Kartsaklis et al., 2012; Turney, 2014). While these methods are applicable only to simple phrases that can be mapped reasonably to a single word, the present method can, in principle, be applied to any type of phrase. This allowed us to build a dataset that extends the current landmark word and word matching datasets in at least two important ways. First, it includes function words, specifically prepositions. Second, it requires the characterization of interactions among three words in each expression, whereas previous datasets had two words per expression, or subsets of the words did not interact in complex ways. Other important </context>
</contexts>
<marker>Turney, 2014</marker>
<rawString>Peter D Turney. 2014. Semantic composition and decomposition: From recognition to generation. arXiv preprint arXiv:1405.7908.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Massimo Zanzotto</author>
<author>Ioannis Korkontzelos</author>
<author>Francesca Fallucchi</author>
<author>Suresh Manandhar</author>
</authors>
<title>Estimating linear models for compositional distributional semantics.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>1263--1271</pages>
<contexts>
<context position="12806" citStr="Zanzotto et al., 2010" startWordPosition="2031" endWordPosition="2034">Naive Bayes accuracy scores for count and predict variants of several CDSM. Chance performance on this task was 0.2. Overlap refers to the word overlap baseline. CW refers to the vectors from Collobert and Weston (2008) . Proportion Correct 0.7 0.6 0.4 0.3 0.5 0.2 Word-Based Predict Count CW 3.1 Compositional Distributional Models We compared six models that are currently prominent in the CDSM literature: addition, multiplication (Mitchell and Lapata, 2008), lexical function (LF) (Coecke et al., 2010), practical lexical function (PLF) (Paperno et al., 2014), full additive (FA) (Guevara, 2010; Zanzotto et al., 2010), and the recursive auto-encoder (RAE) (Socher et al., 2011). The training data for LF, PLF, and FA was the UKWAC+Wikipedia+BNC 2.8 billion word corpus. In training LF, we followed Grefenstette et al. (2013), employing a two-step training regime using corpus-extracted vectors for noun–preposition–noun combinations to estimate matrices of corresponding prepositional phrases, which were in turn used to estimate a three-way tensor of each preposition. For PLF and FA, we learned separate matrices for combining prepositions with each of the two nouns in the construction, using corpus-based vectors </context>
</contexts>
<marker>Zanzotto, Korkontzelos, Fallucchi, Manandhar, 2010</marker>
<rawString>Fabio Massimo Zanzotto, Ioannis Korkontzelos, Francesca Fallucchi, and Suresh Manandhar. 2010. Estimating linear models for compositional distributional semantics. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 1263–1271.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>