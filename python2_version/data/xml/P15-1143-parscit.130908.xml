<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000037">
<title confidence="0.996361">
Graph parsing with s-graph grammars
</title>
<author confidence="0.996167">
Jonas Groschwitz and Alexander Koller and Christoph Teichmann
</author>
<affiliation confidence="0.997877">
Department of Linguistics
University of Potsdam
</affiliation>
<email confidence="0.388605">
firstname.lastname@uni-potsdam.de
</email>
<sectionHeader confidence="0.989366" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999954357142857">
A key problem in semantic parsing with
graph-based semantic representations is
graph parsing, i.e. computing all pos-
sible analyses of a given graph accord-
ing to a grammar. This problem arises
in training synchronous string-to-graph
grammars, and when generating strings
from them. We present two algorithms for
graph parsing (bottom-up and top-down)
with s-graph grammars. On the related
problem of graph parsing with hyperedge
replacement grammars, our implementa-
tions outperform the best previous system
by several orders of magnitude.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999786433962264">
The recent years have seen an increased interest
in semantic parsing, the problem of deriving a se-
mantic representation for natural-language expres-
sions with data-driven methods. With the recent
availability of graph-based meaning banks (Ba-
narescu et al., 2013; Oepen et al., 2014), much
work has focused on computing graph-based se-
mantic representations from strings (Jones et al.,
2012; Flanigan et al., 2014; Martins and Almeida,
2014).
One major approach to graph-based semantic
parsing is to learn an explicit synchronous gram-
mar which relates strings with graphs. One can
then apply methods from statistical parsing to
parse the string and read off the graph. Chiang et
al. (2013) and Quernheim and Knight (2012) rep-
resent this mapping of a (latent) syntactic struc-
ture to a graph with a grammar formalism called
hyperedge replacement grammar (HRG; (Drewes
et al., 1997)). As an alternative to HRG, Koller
(2015) introduced s-graph grammars and showed
that they support linguistically reasonable gram-
mars for graph-based semantics construction.
One problem that is only partially understood
in the context of semantic parsing with explicit
grammars is graph parsing, i.e. the computation
of the possible analyses the grammar assigns to
an input graph (as opposed to string). This prob-
lem arises whenever one tries to generate a string
from a graph (e.g., on the generation side of an MT
system), but also in the context of extracting and
training a synchronous grammar, e.g. in EM train-
ing. The state of the art is defined by the bottom-
up graph parsing algorithm for HRG by Chiang et
al. (2013), implemented in the Bolinas tool (An-
dreas et al., 2013).
We present two graph parsing algorithms (top-
down and bottom-up) for s-graph grammars. S-
graph grammars are equivalent to HRGs, but em-
ploy a more fine-grained perspective on graph-
combining operations. This simplifies the parsing
algorithms, and facilitates reasoning about them.
Our bottom-up algorithm is similar to Chiang et
al.’s, and derives the same asymptotic number of
rule instances. The top-down algorithm is novel,
and achieves the same asymptotic runtime as the
bottom-up algorithm by reasoning about the bi-
connected components of the graph. Our eval-
uation on the “Little Prince” graph-bank shows
that our implementations of both algorithms out-
perform Bolinas by several orders of magnitude.
Furthermore, the top-down algorithm can be more
memory-efficient in practice.
</bodyText>
<sectionHeader confidence="0.999931" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999632888888889">
The AMR-Bank (Banarescu et al., 2013) annotates
sentences with abstract meaning representations
(AMRs), like the one shown in Fig. 1(a). These
are graphs that represent the predicate-argument
structure of a sentence; notably, phenomena such
as control are represented by reentrancies in the
graph. Another major graph-bank is the SemEval-
2014 shared task on semantic dependency parsing
dataset (Oepen et al., 2014).
</bodyText>
<page confidence="0.932627">
1481
</page>
<note confidence="0.987968666666667">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1481–1490,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999709">
Figure 1: AMR (a) for ‘The boy wants to sleep’, and s-graphs. We call (b) SGwant and (c) SGsleep.
</figureCaption>
<figure confidence="0.99399425">
(a) (b)
(c)
(d)
(e)
(f)
boy
arg1
want
arg1 arg2
sleep
sleep R sleep O want
arg1 arg2 arg1 arg1 arg1 arg2
want
R
R
want
R
arg1
arg2
boy S O sleep
arg1
boy
S O
S
S
boy
sleep
arg1
</figure>
<bodyText confidence="0.999955592592592">
The primary grammar formalism currently in
use for synchronous graph grammars is hyper-
edge replacement grammar (HRG) (Drewes et al.,
1997), which we sketch in Section 4.3. An alterna-
tive is offered by Koller (2015), who introduced s-
graph grammars and showed that they lend them-
selves to manually written grammars for semantic
construction. In this paper, we show the equiv-
alence of HRG and s-graph grammars and work
out graph parsing for s-graph grammars.
The first polynomial graph parsing algorithm
for HRGs on graphs with limited connectivity was
presented by Lautemann (1988). Lautemann’s
original algorithm is a top-down parser, which is
presented at a rather abstract level that does not
directly support implementation or detailed com-
plexity analysis. We extend Lautemann’s work
by showing how new parse items can be repre-
sented and constructed efficiently. Finally, Chiang
et al. (2013) presented a bottom-up graph parser
for HRGs, in which the representation and con-
struction of items was worked out for the first time.
It produces O((n · 3d)k+1) instances of the rules
in a parsing schema, where n is the number of
nodes of the graph, d is the maximum degree of
any node, and k is a quantity called the tree-width
of the grammar.
</bodyText>
<sectionHeader confidence="0.927384" genericHeader="method">
3 An algebra of graphs
</sectionHeader>
<bodyText confidence="0.999924217391305">
We start by introducing the exact type of graphs
that our grammars and parsers manipulate, and by
developing some theory.
Throughout this paper, we define a graph G =
(V, E) as a directed graph with edge labels from
some label alphabet L. The graph consists of a
finite set V of nodes and a finite set E C_ V xV xL
of edges e = (u, v, l), where u and v are the nodes
connected by e, and l E L is the edge label. We
say that e is incident to both u and v, and call the
number of edges incident to a node its degree. We
write u e� v if either e = (u, v, l) or e = (v, u, l)
for some l; we drop the e if the identity of the edge
is irrelevant. Edges with u = v are called loops;
we use them here to encode node labels. Given a
graph G, we write n = |V |, m = |E|, and d for
the maximum degree of any node in V .
If f : A --� B and g : A --� B are partial func-
tions, we let the partial function f U g be defined
if for all a E A with both f(a) and g(a) defined,
we have f(a) = g(a). We then let (f U g)(a) be
f(a) if f(a) is defined; g(a) if g(a) is defined; and
undefined otherwise.
</bodyText>
<subsectionHeader confidence="0.997855">
3.1 The HR algebra of graphs with sources
</subsectionHeader>
<bodyText confidence="0.998551117647059">
Our grammars describe how to build graphs from
smaller pieces. They do this by accessing nodes
(called source nodes) which are assigned “public
names”. We define an s-graph (Courcelle and En-
gelfriet, 2012) as a pair SG = (G, 0) of a graph
G and a source assignment, i.e. a partial, injective
function 0 : S --� V that maps some source names
from a finite set S to the nodes of G. We call the
nodes in 0(S) the source nodes or sources of SG;
all other nodes are internal nodes. If 0 is defined
on the source name Q, we call 0(Q) the Q-source
of SG. Throughout, we let s = |S|.
Examples of s-graphs are given in Fig. 1. We
use numbers as node names and lowercase strings
for edge names (except in the concrete graphs of
Fig. 1, where the edges are marked with edge la-
bels instead). Source nodes are drawn in black,
with source names drawn on the inside. Fig. 1(b)
shows an s-graph SGwant with three nodes and
four edges. The three nodes are marked as the R-,
S-, and O-source, respectively. Likewise, the s-
graph SGsleep in (c) has two nodes (one of which
is an R-source and the other an S-source) and two
edges.
We can now apply operations to these graphs.
First, we can rename the R-source of (c) to an O-
source. The result, denoted SGd = SGsleep[R —*
O], is shown in (d). Next, we can merge SGd
with SGwant. This copies the edges and nodes
of SGd and SGwant into a new s-graph; but cru-
cially, for every source name Q the two s-graphs
have in common, the Q-sources of the graphs are
fused into a single node (and become a Q-source of
the result). We write  ||for the merge operation;
</bodyText>
<page confidence="0.991167">
1482
</page>
<bodyText confidence="0.999561055555556">
thus we obtain SGe = SGd  ||SGwant, shown
in (e). Finally, we can forget source names. The
graph SGf = fS(fp(SGe)), in which we forgot S
and O, is shown in (f). We refer to Courcelle and
Engelfriet (2012) for technical details.1
We can take the set of all s-graphs, together with
these operations, as an algebra of s-graphs. In ad-
dition to the binary merge operation and the unary
operations for forget and rename, we fix some fi-
nite set of atomic s-graphs and take them as con-
stants of the algebra which evaluate to themselves.
Following Courcelle and Engelfriet, we call this
algebra the HR algebra. We can evaluate any term
T consisting of these operation symbols into an s-
graph QT] as usual. For instance, the following
term encodes the merge, forget, and rename oper-
ations from the example above, and evaluates to
the s-graph in Fig. 1(f).
</bodyText>
<listItem confidence="0.495144">
(1) fS(fp(SGwant  ||SGsleep[R → O]))
</listItem>
<bodyText confidence="0.999936875">
The set of s-graphs that can be represented as
the value QT] of some term T over the HR alge-
bra depends on the source set S and on the con-
stants. For simplicity, we assume here that we
have a constant for each s-graph consisting of a
single labeled edge (or loop), and that the values
of all other constants can be expressed by combin-
ing these using merge, rename, and forget.
</bodyText>
<subsectionHeader confidence="0.999897">
3.2 S-components
</subsectionHeader>
<bodyText confidence="0.999789647058824">
A central question in graph parsing is how some
s-graph that is a subgraph of a larger s-graph SG
(a sub-s-graph) can be represented as the merge
of two smaller sub-s-graphs of SG. In general,
SG1  ||SG2 is defined for any two s-graphs SG1
and SG2. However, if we see SG1 and SG2 as
subgraphs of SG, SG1  ||SG2 may no longer be
a subgraph of SG. For instance, we cannot merge
the s-graphs (b) and (c) in Fig. 2 as part of the
graph (a): The startpoints of the edges a and d are
both A-sources and would thus become the same
node (unlike in (a)), and furthermore the edge d
would have to be duplicated. In graph parsing, we
already know the identity of all nodes and edges
in sub-s-graphs (as nodes and edges in SG), and
must thus pay attention that merge operations do
not accidentally fuse or duplicate them. In partic-
</bodyText>
<footnote confidence="0.99852">
1 Note that the rename operation of Courcelle and En-
gelfriet (2012) allows for swapping source assignments and
making multiple renames in one step. We simplify the pre-
sentation here, but all of our techniques extend easily.
</footnote>
<figureCaption confidence="0.978698666666667">
Figure 2: (a) An s-graph with (b,c) some sub-s-
graphs, (d) its BCCs, and (e) its block-cutpoint
graph.
</figureCaption>
<bodyText confidence="0.9724915">
ular, two sub-s-graphs cannot be merged if they
have edges in common.
We call a sub-s-graph SG1 of SG extensible if
there is another sub-s-graph SG2 of SG such that
SG1  ||SG2 contains the same edges as SG. An
example of a sub-s-graph that is not extensible is
the sub-s-graph (b) of the s-graph in (a) in Fig. 2.
Because sources can only be renamed or forgotten
by the algebra operations, but never introduced,
we can never attach the missing edge a: this can
only happen when 1 and 2 are sources. As a gen-
eral rule, a sub-s-graph can only be extensible if
it contains all edges that are adjacent to all of its
internal nodes in SG. Obviously, a graph parser
need only concern itself with sub-s-graphs that are
extensible.
We can further clarify the structure of extensible
sub-s-graphs by looking at the s-components of a
graph. Let U ⊆ V be some set of nodes. This
set splits the edges of G into equivalence classes
that are separated by U. We say that two edges
e, f ∈ E are equivalent with respect to U, e ∼U f,
e f
if there is a sequence v1 ↔ v2 ↔ ... vk−1 ↔ vk
with v2, ... , vk−1 ∈/ U, i.e. if we can reach f
from an endpoint of e without visiting a node in U.
We call the equivalence classes of E with respect
to ∼U the s-components of G and denote the s-
component that contains an edge e with [e]. In
Fig. 2(a), the edges a and f are equivalent with
respect to U = {4, 5}, but a and h are not. The s-
components are [a] = {a, b, c, d, e, f}, [g] = {g},
and [h] = {h}.
It can be shown that for any s-graph SG =
</bodyText>
<figure confidence="0.999699153846154">
1 2
1 B
a
2
3
b
B
c
d
A 4
d
A 4
(b) (c)
(a)
3 4e
e
a
1 4d
d
2
b
c
3
f
5f 5g 4g
g
{f}
(d) (e)
B
6
5
h
a
1 2
b
c
d
3 A 4
e
f
g
{e}
5h 6
h
{f}
{h}
{g}
{a,b,c,f,e} {d}
2 {d}
{a,b,c,d}
{a,b,c}
{e}
</figure>
<page confidence="0.961784">
1483
</page>
<bodyText confidence="0.999464882352941">
(G, 0), a sub-s-graph 5H with source nodes U
is extensible iff its edge set is the union of a set
of s-components of G with respect to U. We let
an s-component representation C = (C, 0) in the
s-graph 5G = (G, 0&apos;) consist of a source assign-
ment 0 : 5 V and a set C of s-components of
G with respect to the set VSC = 0(5) ⊆ V of
source nodes of 0. Then we can represent every
extensible sub-s-graph 5H = (H, 0) of 5G by
the s-component representation C = (C, 0) where
C is the set of s-components of which 5H con-
sists. Conversely, we write T (C) for the unique
extensible sub-s-graph of 5G represented by the
s-component representation C.
The utility of s-component representations de-
rives from the fact that merge can be evaluated on
these representations alone, as follows.
</bodyText>
<construct confidence="0.6957728">
Lemma 1. Let C = (C, 0), C1 = (C1, 01), C2 =
(C2, 02) be s-component representations in the s-
graph 5G. Then T (C) = T(C1)  ||T (C2) iff C =
C1]C2 (i.e., disjoint union) and 01∪02 is defined,
injective, and equal to 0.
</construct>
<subsectionHeader confidence="0.999728">
3.3 Boundary representations
</subsectionHeader>
<bodyText confidence="0.999073909090909">
If there is no C such that all conditions of Lemma 1
are satisfied, then T(C1)  ||T(C2) is not defined.
In order to check this efficiently in the bottom-up
parser, it will be useful to represent s-components
explicitly via their boundary.
Consider an s-component representation C =
(C, 0) in 5G and let E be the set of all edges that
are adjacent to a source node in VSC and contained
in an s-component in C. Then we let the bound-
ary representation (BR) β of C in the s-graph 5G
be the pair β = (E, 0). That is, β represents the
s-components through the in-boundary edges, i.e.
those edges inside the s-components (and thus the
sub-s-graph) which are adjacent to a source. The
BR β specifies C uniquely if the base graph 5G
is connected, so we write T(β) for T (C) and VSβ
for VSC.
In Fig. 2(a), the bold sub-s-graph is represented
by β = h{d, e, f, g}, {A:4, B:5}i, indicating that
it contains the A-source 4 and the B-source 5; and
further, that the edge set of the sub-s-graph is [d] ∪
[e] ∪ [f] ∪ [g] = {a, b, c, d, e, f, g}. The edge h
(which is also incident to 5) is not specified, and
therefore not in the sub-s-graph.
The following lemma can be shown about com-
puting merge on boundary representations. Intu-
itively, the conditions (b) and (c) guarantee that
the component sets are disjoint; the lemma then
follows from Lemma 1.
Lemma 2. Let 5G be an s-graph, and let β1 =
(E1, 01), β2 = (E2, 02) be two boundary repre-
sentations in 5G. Then T(β1)  ||T(β2) is defined
within 5G iff the following conditions hold:
</bodyText>
<listItem confidence="0.979177714285714">
(a) 01 ∪ 02 is defined and injective;
(b) the two BRs have no in-boundary edges in
common, i.e. E1 ∩ E2 = ∅;
(c) for every source node v of β1, the last edge
on the path in 5G from v to the closest source
node of β2 is not an in-boundary edge of β2,
and vice versa.
</listItem>
<equation confidence="0.933833333333333">
Furthermore, if these conditions hold, we have
T(β1  ||β2) = T(β1)  ||T(β2), where we define
β1  ||β2 = (E1 ∪ E2, 01 ∪ 02).
</equation>
<sectionHeader confidence="0.993351" genericHeader="method">
4 S-graph grammars
</sectionHeader>
<bodyText confidence="0.9998705">
We are now ready to define s-graph grammars,
which describe languages of s-graphs. We also
introduce graph parsing and relate s-graph gram-
mars to HRGs.
</bodyText>
<subsectionHeader confidence="0.953215">
4.1 Grammars for languages of s-graphs
</subsectionHeader>
<bodyText confidence="0.9902616875">
We use interpreted regular tree grammars (IRTGs;
Koller and Kuhlmann (2011)) to describe lan-
guages of s-graphs. IRTGs are a very general
mechanism for describing languages over and re-
lations between arbitrary algebras. They sepa-
rate conceptually the generation of a grammatical
derivation from its interpretation as a string, tree,
graph, or some other object.
Consider, as an example, the tiny grammar in
Fig. 3; see Koller (2015) for linguistically mean-
ingful grammars. The left column consists of a
regular tree grammar G (RTG; see e.g. Comon et
al. (2008)) with two rules. This RTG describes a
regular language L(G) of derivation trees (in gen-
eral, it may be infinite). In the example, we can
derive S ⇒ r1(VP) ⇒ r1(r2), therefore we have
</bodyText>
<equation confidence="0.852707">
t = r1(r2) ∈ L(G).
</equation>
<bodyText confidence="0.999584875">
We then use a tree homomorphism h to rewrite
the derivation trees into terms over an algebra; in
this case the HR algebra. In the example, the val-
ues h(r1) and h(r2) are specified in the second col-
umn of Fig. 3. We compute h(t) by substituting
the variable x1 in h(r1) with h(r2). The term h(t)
is thus the one shown in (1). It evaluates to the
s-graph 5Gf in Fig. 1(f).
</bodyText>
<page confidence="0.970609">
1484
</page>
<figure confidence="0.930417">
Rule of RTG G homomorphism h
fS(fO(SGwant  ||x1[R → O]))
SGsleep
</figure>
<figureCaption confidence="0.99882">
Figure 3: An example s-graph grammar.
</figureCaption>
<bodyText confidence="0.9997118">
In general, the IRTG G = (G, h, A) generates
the language L(G) = {Qh(t)�  |t ∈ L(G)}, where
Q· is evaluation in the algebra A. Thus, in the
example, we have L(G) = {SGf}.
In this paper, we focus on IRTGs that describe
languages L(G) ⊆ A of objects in an algebra;
specifically, of s-graphs in the HR algebra. How-
ever, IRTGs extend naturally to a synchronous
grammar formalism by adding more homomor-
phisms and algebras. For instance, the grammars
in Koller (2015) map each derivation tree simulta-
neously to a string and an s-graph, and therefore
describe a binary relation between strings and s-
graphs. We call IRTGs where at least one algebra
is the HR algebra, s-graph grammars.
</bodyText>
<subsectionHeader confidence="0.999385">
4.2 Parsing with s-graph grammars
</subsectionHeader>
<bodyText confidence="0.999940775510204">
In this paper, we are concerned with the pars-
ing problem of s-graph grammars. In the context
of IRTGs, parsing means that we are looking for
those derivation trees t that are (a) grammatically
correct, i.e. t ∈ L(G), and (b) match some given
input object a, i.e. h(t) evaluates to a in the al-
gebra. Because the set P of such derivation trees
may be large or infinite, we aim to compute an
RTG Ga such that L(Ga) = P. This RTG plays
the role of a parse chart, which represents the pos-
sible derivation trees compactly.
In order to compute Ga, we need to solve two
problems. First, we need to determine all the pos-
sible ways in which a can be represented by terms
T over the algebra A. This is familiar from string
parsing, where a CKY parse chart spells out all
the ways in which larger substrings can be decom-
posed into smaller parts by concatenation. Sec-
ond, we need to identify all those derivation trees
t ∈ L(G) that map to such a decomposition T,
i.e. for which h(t) evaluates to a. In string pars-
ing, this corresponds to retaining only such de-
compositions into substrings that are justified by
the grammar rules.
While any parsing algorithm must address both
of these issues, they are usually conflated, in that
parse items combine information about the de-
composition of a (such as a string span) with infor-
mation about grammaticality (such as nonterminal
symbols). In IRTG parsing, we take a different,
more generic approach. We assume that the set
D of all decompositions T, i.e. of all terms T that
evaluate to a in the algebra, can be represented
as the language D = L(Da) of a decomposition
grammar Da. Da is an RTG over the signature of
the algebra. Crucially, Da only depends on the al-
gebra and a itself, and not on G or h, because D
contains all terms that evaluate to a and not just
those that are licensed by the grammar. However,
we can compute Ga from Da efficiently by exploit-
ing the closure of regular tree languages under in-
tersection and inverse homomorphism; see Koller
and Kuhlmann (2011) for details.
In practice, this means that whenever we want
to apply IRTGs to a new algebra (as, in this pa-
per, to the HR algebra), we can obtain a parsing
algorithm by specifying how to compute decom-
position grammars over this algebra. This is the
topic of Section 5.
</bodyText>
<subsectionHeader confidence="0.993044">
4.3 Relationship to HRG
</subsectionHeader>
<bodyText confidence="0.99995524137931">
We close our exposition of s-graph grammars by
relating them to HRGs. It is known that the graph
languages that can be described with s-graph
grammars are the same as the HRG languages
(Courcelle and Engelfriet, 2012, Prop. 4.27). Here
we establish a more precise equivalence result, so
we can compare our asymptotic runtimes directly
to those of HRG parsers.
An HRG rule, such as the one shown in Fig. 4,
rewrites a nonterminal symbol into a graph. The
example rule constructs a graph for the nontermi-
nal S by combining the graph Gr in the middle
(with nodes 1, 2, 3 and edges e, f) with graphs GX
and GY that are recursively derived from the non-
terminals X and Y . The combination happens by
merging the external nodes of GX and GY with
nodes of Gr: the squiggly lines indicate that the
external node I of GX should be 1, and the ex-
ternal node II should be 2. Similarly the external
nodes of GY are unified with 1 and 3. Finally, the
external nodes I and II of the HRG rule for S itself,
shaded gray, are 1 and 3.
The fundamental idea of the HRG-to-IRTG
translation is to encode external nodes as sources,
and to use rename and merge to unify the nodes of
the different graphs. In the example, we might say
that the external nodes of GX and GY are repre-
sented using the source names I and II, and extend
Gr to an s-graph by saying that the nodes 1, 2, and
</bodyText>
<equation confidence="0.742843">
S → r1(VP)
VP → r2
</equation>
<page confidence="0.742444">
1485
</page>
<figure confidence="0.983181894736842">
X : 1,2
1,2
e : 1,2 f : 1,3
1,3
1,3
Y : 1,3
II
e f
II
Gr
1
I
I
I
X Y
2
II
3
3 are its I-source, III-source, and II-source respec-
</figure>
<bodyText confidence="0.417998">
tively. This results in the expression
</bodyText>
<listItem confidence="0.9414905">
(2) fIII((I) e, (III)  ||x1[II , III]
 ||(I) ,f (II)  ||x2)
</listItem>
<bodyText confidence="0.999581">
where we write “(I) e, (III)” for the s-graph con-
sisting of the edge e, with node 1 as I-source and
2 as III-source.
However, this requires the use of three source
names (I, II, and III). The following encoding of
the rule uses the sources more economically:
</bodyText>
<listItem confidence="0.820788">
(3) fII((I) e, (II)  ||x1)  ||(I) f → (II)  ||x2
</listItem>
<bodyText confidence="0.996909862068965">
This term uses only two source names. It forgets
II as soon as we are finished with the node 2, and
frees the name up for reuse for 3. The complete
encoding of the HRG rule consists of the RTG rule
S , r(X, Y) with h(r) = (3).
In the general case, one can “read off” possible
term encodings of a HRG rule from its tree decom-
positions; see Chiang et al. (2013) or Def. 2.80 of
Courcelle and Engelfriet (2012) for details. A tree
decomposition is a tree, each of whose nodes 7r is
labeled with a subset Vπ of the nodes in the HRG
rule. We can construct a term encoding from a tree
decomposition bottom-up. Leaves map to vari-
ables or constants; binary nodes introduce merge
operations; and we use rename and forget oper-
ations to ensure that the subterm for the node 7r
evaluates to an s-graph in which exactly the nodes
in Vπ are source nodes.2 In the example, we obtain
(3) from the tree decomposition in Fig. 4 like this.
The tree-width k of an HRG rule is measured
by finding the tree decomposition of the rule for
which the node sets have the lowest maximum size
s and setting k = s −1. It is a crucial measure be-
cause Chiang et al.’s parsing algorithm is exponen-
tial in k. The translation we just sketched uses s
source names. Thus we see that a HRG with rules
of tree-width ≤ k can be encoded into an s-graph
grammar with k + 1 source names. (The converse
is also true.)
</bodyText>
<sectionHeader confidence="0.982462" genericHeader="method">
5 Graph parsing with s-graph grammars
</sectionHeader>
<bodyText confidence="0.9988976">
Now we show how to compute decomposition
grammars for the s-graph algebra. As we ex-
plained in Section 4.2, we can then obtain a com-
plete parser for s-graph grammars through generic
methods.
</bodyText>
<footnote confidence="0.906781">
2This uses the swap operations mentioned in Footnote 1.
</footnote>
<note confidence="0.442694">
S ,
</note>
<figureCaption confidence="0.998593">
Figure 4: An HRG rule (left) with one of its tree
decompositions (right).
</figureCaption>
<bodyText confidence="0.999848166666667">
Given an s-graph SG, the language of the de-
composition grammar DSG is the set of all terms
over the HR algebra that evaluate to SG. For ex-
ample, the decomposition grammar for the graph
SG in Fig. 1(a) contains – among many others –
the following two rules:
</bodyText>
<listItem confidence="0.978224">
(4) SG , fR(SGf)
(5) SGe ,  ||(SGb, SGd),
</listItem>
<bodyText confidence="0.999847111111111">
where SGf, SGe, SGb, and SGd are the graphs
from Fig. 1 (see Section 3.1). In other words, DSG
keeps track of sub-s-graphs in the nonterminals,
and the rules spell out how “larger” sub-s-graphs
can be constructed from “smaller” sub-s-graphs
using the operations of the HR algebra. The al-
gorithms below represent sub-s-graphs compactly
using s-component and boundary representations.
Because the decomposition grammars in the s-
graph algebra can be very large (see Section 6),
we will not usually compute the entire decompo-
sition grammar explicitly. Instead, it is sufficient
to maintain a lazy representation of DSG, which
allows us to answer queries to the decomposition
grammar efficiently. During parsing, such queries
will be generated by the generic part of the pars-
ing algorithm. Specifically, we will show how to
answer the following types of query:
</bodyText>
<listItem confidence="0.974686769230769">
• Top-down: given an s-component represen-
tation C of some s-graph and an algebra
operation o, enumerate all the rules C ,
o(C1, ... , Ck) in DSG. This asks how a larger
sub-s-graph can be derived from other sub-s-
graphs using the operation o. In the example
above, a query for SG and fR(·) should yield,
among others, the rule in (4).
• Bottom-up: given boundary representations
Q1, ... , Qk and an algebra operation o, enu-
merate all the rules Q , o(Q1,...,Qk) in
DSG. This asks how smaller sub-s-graphs
can be combined into a bigger one using the
</listItem>
<page confidence="0.962711">
1486
</page>
<table confidence="0.99596225">
forget rename merge
bottom-up O(d + s) O(s) O(ds)
top-down O(ds) O(s) O(ds)
I = # rules O(ns2ds) O(ns2ds) O(ns3ds)
</table>
<tableCaption confidence="0.890671">
Table 1: Amortized per-rule runtimes T for the
different rule types.
</tableCaption>
<bodyText confidence="0.999353142857143">
operation o. In the example above, a merge
query for 5Gb and 5Gd should yield the rule
in (5). Unlike in the top-down case, every
bottom-up query returns at most one rule.
The runtime of the complete parsing algorithm
is bounded by the number I of different queries
to DSG that we receive, multiplied by the per-
rule runtime T that we need to answer each query.
The factor I is analogous to the number of rule
instances in schema-based parsing (Shieber et al.,
1995). The factor T is often ignored in the anal-
ysis of parsing algorithms, because in parsing
schemata for strings, we typically have T = O(1).
This need not be the case for graph parsers. In the
HRG parsing schema of Chiang et al. (2013), we
have I = O(nk+13d(k+1)), where k is the tree-
width of the HRG. In addition, each of their rule
instances takes time T = O(d(k + 1)) to actually
calculate the new item.
Below, we show how we can efficiently answer
both bottom-up and top-down queries to DSG. Ev-
ery s-graph grammar has an equivalent normal
form where every constant describes an s-graph
with a single edge. Assuming that the grammar
is in this normal form, queries of the form 0 —* g
(resp. C —* g), where g is a constant of the HR-
algebra, are trivial and we will not consider them
further. Table 1 summarizes our results.
</bodyText>
<subsectionHeader confidence="0.995283">
5.1 Bottom-up decomposition
</subsectionHeader>
<bodyText confidence="0.99634165625">
Forget and rename. Given a boundary repre-
sentation 0&apos; = (E&apos;, φ&apos;), answering the bottom-up
forget query 0 —* fA(0&apos;) amounts to verifying that
all edges incident to φ&apos;(A) are in-boundary in 0&apos;,
since otherwise the result would not be extensible.
This takes time O(d). We then let 0 = (E, φ),
where φ is like φ&apos; but undefined on A, and E is the
set of edges in E&apos; that are still incident to a source
in φ. Computing 0 thus takes time O(d + s).
The rename operation works similarly, but since
the edge set remains unmodified, the per-rule run-
time is O(s).
A BR is fully determined by specifying the node
and in-boundary edges for each source name, so
there are at most O ((n2d)s) different BRs. Since
the result of a forget or rename rule is determined
by the child 0&apos;, this is an upper bound for the num-
ber I of rule instances of forget or rename.
Merge. Now consider the bottom-up merge
query for the boundary representations 01 and 02.
As we saw in Section 3.3, T(01)  ||T(02) is not
always defined. But if it is, we can answer the
query with the rule (01  ||02) —*  ||(01, 02), with
01  ||02 defined as in Section 3.3. Computing this
BR takes time O(ds).
We can check whether T(01)  ||T(02) is de-
fined by going through the conditions of Lemma 2.
The only nontrivial condition is (c). In order to
check it efficiently, we precompute a data struc-
ture which contains, for any two nodes u, v E V ,
the length k of the shortest undirected path u =
v1 H ... H vk = v and the last edge a on this
</bodyText>
<subsubsectionHeader confidence="0.344983">
e
</subsubsectionHeader>
<bodyText confidence="0.998802705882353">
path. This can be done in time O(n3) using the
Floyd-Warshall algorithm. Checking (c) for every
source pair then takes time O(s2) per rule, but be-
cause sources that are common to both 01 and 02
automatically satisfy (c) due to (a), one can show
that the total runtime of checking (c) for all merge
rules of DSG is O(ns3dss).
Observe finally that there are I = O(ns3ds)
instances of the merge rule, because each of the
O(ds) edges that are incident to a source node can
be either in 01, in 02, or in neither. Therefore
the runtime for checking (c) amortizes to O(s) per
rule. The Floyd-Warshall step amortizes to O(1)
per rule for s &gt; 3; for s &lt; 2 the node table can
be computed in amortized O(1) using more spe-
cialized algorithms. This yields a total amortized
per-rule runtime T for bottom-up merge of O(ds).
</bodyText>
<subsectionHeader confidence="0.998351">
5.2 Top-down decomposition
</subsectionHeader>
<bodyText confidence="0.991787384615385">
For the top-down queries, we specify sub-s-graphs
in terms of their s-component representations. The
number I of instances of each rule type is the same
as in the bottom-up case because of the one-to-
one correspondence of s-component and bound-
ary representations. We focus on merge and forget
queries; rename is as above.
Merge. Given an s-component representation
C = (C, φ), a top-down merge query asks us to
enumerate the rules C —*  ||(C1, C2) such that
T(C1)  ||T(C2) = T(C). By Lemma 1, we
can do this by using every distribution of the s-
components in C over C1 and C2 and restricting φ
</bodyText>
<page confidence="0.987014">
1487
</page>
<bodyText confidence="0.998678205128206">
accordingly. This brings the per-rule time of top-
down merge to O(ds), the maximum number of
s-components in C.
Block-cutpoint graphs. The challenging query
to answer top-down is forget. We will first de-
scribe the problem and introduce a data structure
that supports efficient top-down forget queries.
Consider top-down forget queries on
the sub-s-graph 5G1 drawn in bold in
Fig. 2(a); its s-component representation is
({[a], [g]}, {A:4, B:5}). A top-down forget might
promote the node 3 to a C-source, yielding a
sub-s-graph 5G2 (that is, fC(5G2) is the orig-
inal s-graph 5G1). In 5G2, a, e, and f are
no longer equivalent; its s-component repre-
sentation is ({[a], [e], [f], [g]}, {A:4, B:5, C:3}).
Thus promoting 3 to a source splits the original
s-component into smaller parts.
By contrast, the same top-down forget might
also promote the node 1 to a C-source, yield-
ing a sub-s-graph 5G3; fC(5G3) is also 5G1.
However, all edges in [a] are still equiva-
lent in 5G3; its s-component representation is
({[a], [g]}, {A:4, B:5, C:1}).
An algorithm for top-down forget must be able
to determine whether promotion of a node splits
an s-component or not. To do this, let G be the in-
put graph. We create an undirected auxiliary graph
GU from G and a set U of (source) nodes. GU
contains all nodes in V \U, and for each edge e
that is incident to a node u E U, it contains a node
(u, e). Furthermore, GU contains undirected ver-
sions of all edges in G; if an edge e E E is incident
to a node u E U, it becomes incident to (u, e) in
GU instead. The auxiliary graph G14,51 for our
example graph is shown in Fig. 2(d).
Two edges are connected in GU if and only if
they are equivalent with respect to U in G. There-
fore, promotion of u splits s-components iff u is a
cutpoint in GU, i.e. a node whose removal discon-
nects the graph. Cutpoints can be characterized
as those nodes that belong to multiple biconnected
components (BCCs) of GU, i.e. the maximal sub-
graphs such that any node can be removed without
disconnecting a graph segment. In Fig. 2(d), the
BCCs are indicated by the dotted boxes. Observe
that 3 is a cutpoint and 1 is not.
For any given U, we can represent the structure
of the BCCs of GU in its block-cutpoint graph.
This is a bipartite graph whose nodes are the cut-
points and BCCs of GU, and a BCC is connected
to all of its cutpoints; see Fig. 2(e) for the block-
cutpoint graph of the example. Block-cutpoint
graphs are always forests, with the individual trees
representing the s-components of G. Promoting
a cutpoint u splits the s-component into smaller
parts, each corresponding to an incident edge of u.
We annotate each edge with that part.
Forget. We can now answer a top-down forget
query C → fA(C&apos;) efficiently from the block-
cutpoint graph for the sources of C = (C, φ). We
iterate over all components c E C, and then over
all internal nodes u of c. If u is not a cutpoint,
we simply let C&apos; = (C&apos;, φ&apos;) by making u an A-
source and letting C&apos; = C. Otherwise, we also
remove c from C and add the new s-components
on the edges adjacent to u in the block-cutpoint
graph. The query returns rules for all C&apos; that can
be constructed like this.
The per-rule runtime of top-down forget is
O(ds), the time needed to compute C&apos; in the cut-
point case. We furthermore precompute the block-
cutpoint graphs for the input graph with respect to
all sets U ⊆ V of nodes with SUS ≤ s − 1. For
each U, we can compute the block-cutpoint graph
and annotate its edges in time O(nd2s). Thus the
total time for the precomputation is O(ns · d2s),
which amortizes to O(1) per rule.
</bodyText>
<sectionHeader confidence="0.999648" genericHeader="evaluation">
6 Evaluation
</sectionHeader>
<bodyText confidence="0.999995590909091">
We evaluate the performance of our algorithms on
the “Little Prince” AMR-Bank version 1.4, avail-
able from amr.isi.edu. This graph-bank con-
sists of 1562 sentences manually annotated with
AMRs. We implemented our algorithms in Java
as part of the Alto parser for IRTGs (Alto Devel-
opers, 2015), and compared them to the Bolinas
HRG parser (Andreas et al., 2013). We measured
runtimes using Java 8 (for Alto) and Pypy 2.5.0
(for Bolinas) on an Intel Xeon E7-8857 CPU at 3
GHz, after warming up the JIT compilers.
As there are no freely available grammars for
this dataset, we created our own for the evalua-
tion, using Bayesian grammar induction roughly
along the lines of Cohn et al. (2010). We pro-
vide the grammars as supplementary material.
Around 64% of the AMRs in the graph-bank have
treewidth 1 and can thus be parsed using s = 2
source names. 98% have treewidth 1 or 2, corre-
sponding to s = 3 source names. All experiments
evaluated parser times on the same AMRs from
which the grammar was sampled.
</bodyText>
<page confidence="0.955398">
1488
</page>
<note confidence="0.432046">
1 10 100 10000 1e+06
</note>
<bodyText confidence="0.99655368">
Top-down versus bottom-up. Fig. 5 compares
the performance of the top-down and the bottom-
up algorithm, on a grammar with three source
names sampled from all 1261 graphs with up to
10 nodes. Each point in the figure is the geometric
mean of runtimes for all graphs with a given num-
ber of nodes; note the log-scale. We aborted the
top-down parser after its runtimes grew too large.
We observe that the bottom-up algorithm out-
performs the top-down algorithm, and yields prac-
tical runtimes even for nontrivial graphs. One pos-
sible explanation for the difference is that the top-
down algorithm spends more time analyzing un-
grammatical s-graphs, particularly subgraphs that
are not connected.
Comparison to Bolinas. We also compare our
implementations to Bolinas. Because Bolinas is
much slower than Alto, we restrict ourselves to
two source names (= treewidth 1) and sampled the
grammar from 30 randomly chosen AMRs each of
size 2 to 8, plus the 21 AMRs of size one.
Fig. 6 shows the runtimes. Our parsers are
generally much faster than in Fig. 5, due to the
decreased number of sources and grammar size.
They are also both much faster than Bolinas. Mea-
suring the total time for parsing all 231 AMRs,
our bottom-up algorithm outperforms Bolinas by a
factor of 6722. The top-down algorithm is slower,
but still outperforms Bolinas by a factor of 340.
Further analysis. In practice, memory use can
be a serious issue. For example, the decomposi-
tion grammar for s=3 for AMR #194 in the corpus
has over 300 million rules. However, many uses
of decomposition grammars, such as sampling for
grammar induction, can be phrased purely in terms
of top-down queries. The top-down algorithm can
answer these without computing the entire gram-
mar, alleviating the memory problem.
Finally, we analyzed the asymptotic runtimes in
Table 1 in terms of the maximum number d · s of
in-boundary edges. However, the top-down parser
does not manipulate individual edges, but entire
s-components. The maximum number D3 of s-
components into which a set of s sources can split
a graph is called the s-separability of G by Laute-
mann (1990). We can analyze the runtime of the
top-down parser more carefully as O(n33D3ds);
as the dotted line in Fig. 5 shows, this predicts
the runtime well. Interestingly, D3 is much lower
in practice than its theoretical maximum. In the
</bodyText>
<figureCaption confidence="0.912424333333333">
Figure 5: Runtimes of our parsers with s = 3.
Figure 6: Runtimes of our parsers and Bolinas
with s = 2.
</figureCaption>
<bodyText confidence="0.99756325">
“Little Prince” AMR-Bank, the mean of D3 is 6.0,
whereas the mean of 3 · d is 12.7. Thus exploit-
ing the s-component structure of the graph can im-
prove parsing times.
</bodyText>
<sectionHeader confidence="0.999172" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9995644">
We presented two new graph parsing algorithms
for s-graph grammars. These were framed in
terms of top-down and bottom-up queries to a de-
composition grammar for the HR algebra. Our
implementations outperform Bolinas, the previ-
ously best system, by several orders of magnitude.
We have made them available as part of the Alto
parser.
A challenge for grammar-based semantic pars-
ing is grammar induction from data. We will ex-
plore this problem in future work. Furthermore,
we will investigate methods for speeding up graph
parsing further, e.g. with different heuristics.
Acknowledgments. We thank the anonymous
reviewers for their comments, and Daniel Bauer
for his help with Bolinas. We received valuable
feedback at the 2015 Dagstuhl seminar on graph
grammars and the 2014 Johns Hopkins workshop
in Prague. This work was supported by the DFG
grant KO 2916/2-1.
</bodyText>
<figure confidence="0.999529433333333">
O(n3 &apos; 3D3 &apos; 3 &apos; d)
Bottom−up
Top−down
[ms]
●
●●
●
●
●
●
●
●
●
Node count
●
1 2 3 4 5 6 7 8 9 10
1 2 3 4 5 6 7 8
1 10 100 1000 1e+05
●
[ms]
●
● Bottom−up
Top−down
Bolinas
●
●
●
●
● ●
Node count
</figure>
<page confidence="0.9891">
1489
</page>
<sectionHeader confidence="0.997772" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999923066666667">
Alto Developers. 2015. Alto: algebraic language
toolkit for parsing and decoding with IRTGs. Avail-
able at https://bitbucket.org/tclup/
alto.
Jacob Andreas, Daniel Bauer, Karl Moritz Hermann,
Bevan Jones, Kevin Knight, and David Chiang.
2013. Bolinas graph processing package. Available
at http://www.isi.edu/publications/
licensed-sw/bolinas/. Downloaded in Jan-
uary 2015.
Laura Banarescu, Claire Bonial, Shu Cai, Madalina
Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider. 2013. Abstract meaning representation
for sembanking. In Proceedings of the 7th Lin-
guistic Annotation Workshop &amp; Interoperability with
Discourse, pages 178–186.
David Chiang, Jacob Andreas, Daniel Bauer,
Karl Moritz Hermann, Bevan Jones, and Kevin
Knight. 2013. Parsing graphs with hyperedge
replacement grammars. In Proceedings of the 51st
Annual Meeting of the Association for Computa-
tional Linguistics, pages 924–932.
Trevor Cohn, Phil Blunsom, and Sharon Goldwater.
2010. Inducing tree-substitution grammars. Journal
of Machine Learning Research (JMLR), 11:3053–
3096.
Hubert Comon, Max Dauchet, R´emi Gilleron, Flo-
rent Jacquemard, Denis Lugiez, Christof L¨oding,
Sophie Tison, and Marc Tommasi. 2008. Tree
automata techniques and applications. http://
tata.gforge.inria.fr/.
Bruno Courcelle and Joost Engelfriet. 2012. Graph
Structure and Monadic Second-Order Logic, vol-
ume 138 of Encyclopedia of Mathematics and its
Applications. Cambridge University Press.
Frank Drewes, Hans-J¨org Kreowski, and Annegret Ha-
bel. 1997. Hyperedge replacement graph gram-
mars. In Grzegorz Rozenberg, editor, Handbook of
Graph Grammars and Computing by Graph Trans-
formation, pages 95–162. World Scientific Publish-
ing Co., Inc.
Jeffrey Flanigan, Sam Thomson, Jamie Carbonell,
Chris Dyer, and Noah A. Smith. 2014. A discrim-
inative graph-based parser for the abstract meaning
representation. In Proceedings of the 52nd Annual
Meeting of the Association for Computational Lin-
guistics, pages 1426–1436, Baltimore, Maryland.
Bevan K. Jones, Jacob Andreas, Daniel Bauer,
Karl Moritz Hermann, and Kevin Knight. 2012. Se-
mantics – Based machine translation with hyperedge
replacement grammars. In Proceedings of COLING
2012: Technical Papers, pages 1359–1376.
Alexander Koller and Marco Kuhlmann. 2011. A gen-
eralized view on parsing and translation. In Pro-
ceedings of the 12th International Conference on
Parsing Technologies, pages 2–13.
Alexander Koller. 2015. Semantics construction with
graph grammars. In Proceedings of the 11th Inter-
national Conference on Computational Semantics
(IWCS), pages 228–238.
Clemens Lautemann. 1988. Decomposition trees:
Structured graph representation and efficient algo-
rithms. In Max Dauchet and Maurice Nivat, editors,
13th Colloquium on Trees in Algebra and Program-
ming, volume 299 of Lecture Notes in Computer Sci-
ence, pages 28–39. Springer Berlin Heidelberg.
Clemens Lautemann. 1990. The complexity of
graph languages generated by hyperedge replace-
ment. Acta Informatica, 27:399–421.
Andr´e F. T. Martins and Mariana S. C. Almeida. 2014.
Priberam: A turbo semantic parser with second or-
der features. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval 2014),
pages 471–476.
Stephan Oepen, Marco Kuhlmann, Yusuke Miyao,
Daniel Zeman, Dan Flickinger, Jan Hajic, Angelina
Ivanova, and Yi Zhang. 2014. SemEval 2014 Task
8: Broad-coverage semantic dependency parsing. In
Proceedings of the 8th International Workshop on
Semantic Evaluation (SemEval 2014), pages 63–72.
Daniel Quernheim and Kevin Knight. 2012. DAG-
GER: A toolkit for automata on directed acyclic
graphs. In Proceedings of the 10th International
Workshop on Finite State Methods and Natural Lan-
guage Processing, pages 40–44.
Stuart M. Shieber, Yves Schabes, and Fernando C. N.
Pereira. 1995. Principles and implementation of
deductive parsing. Journal of Logic Programming,
24(1–2):3–36.
</reference>
<page confidence="0.990028">
1490
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.970337">
<title confidence="0.998539">Graph parsing with s-graph grammars</title>
<author confidence="0.997153">Groschwitz Koller</author>
<affiliation confidence="0.999317">Department of University of</affiliation>
<email confidence="0.984413">firstname.lastname@uni-potsdam.de</email>
<abstract confidence="0.999391133333333">A key problem in semantic parsing with graph-based semantic representations is i.e. computing all possible analyses of a given graph according to a grammar. This problem arises in training synchronous string-to-graph grammars, and when generating strings from them. We present two algorithms for graph parsing (bottom-up and top-down) with s-graph grammars. On the related problem of graph parsing with hyperedge replacement grammars, our implementations outperform the best previous system by several orders of magnitude.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alto Developers</author>
</authors>
<title>Alto: algebraic language toolkit for parsing and decoding with IRTGs. Available at https://bitbucket.org/tclup/ alto.</title>
<date>2015</date>
<contexts>
<context position="33284" citStr="Developers, 2015" startWordPosition="6151" endWordPosition="6153">We furthermore precompute the blockcutpoint graphs for the input graph with respect to all sets U ⊆ V of nodes with SUS ≤ s − 1. For each U, we can compute the block-cutpoint graph and annotate its edges in time O(nd2s). Thus the total time for the precomputation is O(ns · d2s), which amortizes to O(1) per rule. 6 Evaluation We evaluate the performance of our algorithms on the “Little Prince” AMR-Bank version 1.4, available from amr.isi.edu. This graph-bank consists of 1562 sentences manually annotated with AMRs. We implemented our algorithms in Java as part of the Alto parser for IRTGs (Alto Developers, 2015), and compared them to the Bolinas HRG parser (Andreas et al., 2013). We measured runtimes using Java 8 (for Alto) and Pypy 2.5.0 (for Bolinas) on an Intel Xeon E7-8857 CPU at 3 GHz, after warming up the JIT compilers. As there are no freely available grammars for this dataset, we created our own for the evaluation, using Bayesian grammar induction roughly along the lines of Cohn et al. (2010). We provide the grammars as supplementary material. Around 64% of the AMRs in the graph-bank have treewidth 1 and can thus be parsed using s = 2 source names. 98% have treewidth 1 or 2, corresponding to </context>
</contexts>
<marker>Developers, 2015</marker>
<rawString>Alto Developers. 2015. Alto: algebraic language toolkit for parsing and decoding with IRTGs. Available at https://bitbucket.org/tclup/ alto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Andreas</author>
<author>Daniel Bauer</author>
<author>Karl Moritz Hermann</author>
<author>Bevan Jones</author>
<author>Kevin Knight</author>
<author>David Chiang</author>
</authors>
<title>Bolinas graph processing package. Available at http://www.isi.edu/publications/ licensed-sw/bolinas/. Downloaded in</title>
<date>2013</date>
<contexts>
<context position="2398" citStr="Andreas et al., 2013" startWordPosition="367" endWordPosition="371">s construction. One problem that is only partially understood in the context of semantic parsing with explicit grammars is graph parsing, i.e. the computation of the possible analyses the grammar assigns to an input graph (as opposed to string). This problem arises whenever one tries to generate a string from a graph (e.g., on the generation side of an MT system), but also in the context of extracting and training a synchronous grammar, e.g. in EM training. The state of the art is defined by the bottomup graph parsing algorithm for HRG by Chiang et al. (2013), implemented in the Bolinas tool (Andreas et al., 2013). We present two graph parsing algorithms (topdown and bottom-up) for s-graph grammars. Sgraph grammars are equivalent to HRGs, but employ a more fine-grained perspective on graphcombining operations. This simplifies the parsing algorithms, and facilitates reasoning about them. Our bottom-up algorithm is similar to Chiang et al.’s, and derives the same asymptotic number of rule instances. The top-down algorithm is novel, and achieves the same asymptotic runtime as the bottom-up algorithm by reasoning about the biconnected components of the graph. Our evaluation on the “Little Prince” graph-ban</context>
<context position="33352" citStr="Andreas et al., 2013" startWordPosition="6162" endWordPosition="6165"> graph with respect to all sets U ⊆ V of nodes with SUS ≤ s − 1. For each U, we can compute the block-cutpoint graph and annotate its edges in time O(nd2s). Thus the total time for the precomputation is O(ns · d2s), which amortizes to O(1) per rule. 6 Evaluation We evaluate the performance of our algorithms on the “Little Prince” AMR-Bank version 1.4, available from amr.isi.edu. This graph-bank consists of 1562 sentences manually annotated with AMRs. We implemented our algorithms in Java as part of the Alto parser for IRTGs (Alto Developers, 2015), and compared them to the Bolinas HRG parser (Andreas et al., 2013). We measured runtimes using Java 8 (for Alto) and Pypy 2.5.0 (for Bolinas) on an Intel Xeon E7-8857 CPU at 3 GHz, after warming up the JIT compilers. As there are no freely available grammars for this dataset, we created our own for the evaluation, using Bayesian grammar induction roughly along the lines of Cohn et al. (2010). We provide the grammars as supplementary material. Around 64% of the AMRs in the graph-bank have treewidth 1 and can thus be parsed using s = 2 source names. 98% have treewidth 1 or 2, corresponding to s = 3 source names. All experiments evaluated parser times on the sa</context>
</contexts>
<marker>Andreas, Bauer, Hermann, Jones, Knight, Chiang, 2013</marker>
<rawString>Jacob Andreas, Daniel Bauer, Karl Moritz Hermann, Bevan Jones, Kevin Knight, and David Chiang. 2013. Bolinas graph processing package. Available at http://www.isi.edu/publications/ licensed-sw/bolinas/. Downloaded in January 2015.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Banarescu</author>
<author>Claire Bonial</author>
<author>Shu Cai</author>
<author>Madalina Georgescu</author>
<author>Kira Griffitt</author>
<author>Ulf Hermjakob</author>
<author>Kevin Knight</author>
<author>Philipp Koehn</author>
<author>Martha Palmer</author>
<author>Nathan Schneider</author>
</authors>
<title>Abstract meaning representation for sembanking.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th Linguistic Annotation Workshop &amp; Interoperability with Discourse,</booktitle>
<pages>178--186</pages>
<contexts>
<context position="1002" citStr="Banarescu et al., 2013" startWordPosition="136" endWordPosition="140"> in training synchronous string-to-graph grammars, and when generating strings from them. We present two algorithms for graph parsing (bottom-up and top-down) with s-graph grammars. On the related problem of graph parsing with hyperedge replacement grammars, our implementations outperform the best previous system by several orders of magnitude. 1 Introduction The recent years have seen an increased interest in semantic parsing, the problem of deriving a semantic representation for natural-language expressions with data-driven methods. With the recent availability of graph-based meaning banks (Banarescu et al., 2013; Oepen et al., 2014), much work has focused on computing graph-based semantic representations from strings (Jones et al., 2012; Flanigan et al., 2014; Martins and Almeida, 2014). One major approach to graph-based semantic parsing is to learn an explicit synchronous grammar which relates strings with graphs. One can then apply methods from statistical parsing to parse the string and read off the graph. Chiang et al. (2013) and Quernheim and Knight (2012) represent this mapping of a (latent) syntactic structure to a graph with a grammar formalism called hyperedge replacement grammar (HRG; (Drew</context>
<context position="3231" citStr="Banarescu et al., 2013" startWordPosition="494" endWordPosition="497">lifies the parsing algorithms, and facilitates reasoning about them. Our bottom-up algorithm is similar to Chiang et al.’s, and derives the same asymptotic number of rule instances. The top-down algorithm is novel, and achieves the same asymptotic runtime as the bottom-up algorithm by reasoning about the biconnected components of the graph. Our evaluation on the “Little Prince” graph-bank shows that our implementations of both algorithms outperform Bolinas by several orders of magnitude. Furthermore, the top-down algorithm can be more memory-efficient in practice. 2 Related work The AMR-Bank (Banarescu et al., 2013) annotates sentences with abstract meaning representations (AMRs), like the one shown in Fig. 1(a). These are graphs that represent the predicate-argument structure of a sentence; notably, phenomena such as control are represented by reentrancies in the graph. Another major graph-bank is the SemEval2014 shared task on semantic dependency parsing dataset (Oepen et al., 2014). 1481 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1481–1490, Beijing, China, July 26-31, 2015. c�2</context>
</contexts>
<marker>Banarescu, Bonial, Cai, Georgescu, Griffitt, Hermjakob, Knight, Koehn, Palmer, Schneider, 2013</marker>
<rawString>Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. 2013. Abstract meaning representation for sembanking. In Proceedings of the 7th Linguistic Annotation Workshop &amp; Interoperability with Discourse, pages 178–186.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Jacob Andreas</author>
<author>Daniel Bauer</author>
<author>Karl Moritz Hermann</author>
<author>Bevan Jones</author>
<author>Kevin Knight</author>
</authors>
<title>Parsing graphs with hyperedge replacement grammars.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>924--932</pages>
<contexts>
<context position="1428" citStr="Chiang et al. (2013)" startWordPosition="206" endWordPosition="209">ng, the problem of deriving a semantic representation for natural-language expressions with data-driven methods. With the recent availability of graph-based meaning banks (Banarescu et al., 2013; Oepen et al., 2014), much work has focused on computing graph-based semantic representations from strings (Jones et al., 2012; Flanigan et al., 2014; Martins and Almeida, 2014). One major approach to graph-based semantic parsing is to learn an explicit synchronous grammar which relates strings with graphs. One can then apply methods from statistical parsing to parse the string and read off the graph. Chiang et al. (2013) and Quernheim and Knight (2012) represent this mapping of a (latent) syntactic structure to a graph with a grammar formalism called hyperedge replacement grammar (HRG; (Drewes et al., 1997)). As an alternative to HRG, Koller (2015) introduced s-graph grammars and showed that they support linguistically reasonable grammars for graph-based semantics construction. One problem that is only partially understood in the context of semantic parsing with explicit grammars is graph parsing, i.e. the computation of the possible analyses the grammar assigns to an input graph (as opposed to string). This </context>
<context position="5046" citStr="Chiang et al. (2013)" startWordPosition="787" endWordPosition="790">hemselves to manually written grammars for semantic construction. In this paper, we show the equivalence of HRG and s-graph grammars and work out graph parsing for s-graph grammars. The first polynomial graph parsing algorithm for HRGs on graphs with limited connectivity was presented by Lautemann (1988). Lautemann’s original algorithm is a top-down parser, which is presented at a rather abstract level that does not directly support implementation or detailed complexity analysis. We extend Lautemann’s work by showing how new parse items can be represented and constructed efficiently. Finally, Chiang et al. (2013) presented a bottom-up graph parser for HRGs, in which the representation and construction of items was worked out for the first time. It produces O((n · 3d)k+1) instances of the rules in a parsing schema, where n is the number of nodes of the graph, d is the maximum degree of any node, and k is a quantity called the tree-width of the grammar. 3 An algebra of graphs We start by introducing the exact type of graphs that our grammars and parsers manipulate, and by developing some theory. Throughout this paper, we define a graph G = (V, E) as a directed graph with edge labels from some label alph</context>
<context position="21913" citStr="Chiang et al. (2013)" startWordPosition="4063" endWordPosition="4066">h consisting of the edge e, with node 1 as I-source and 2 as III-source. However, this requires the use of three source names (I, II, and III). The following encoding of the rule uses the sources more economically: (3) fII((I) e, (II) ||x1) ||(I) f → (II) ||x2 This term uses only two source names. It forgets II as soon as we are finished with the node 2, and frees the name up for reuse for 3. The complete encoding of the HRG rule consists of the RTG rule S , r(X, Y) with h(r) = (3). In the general case, one can “read off” possible term encodings of a HRG rule from its tree decompositions; see Chiang et al. (2013) or Def. 2.80 of Courcelle and Engelfriet (2012) for details. A tree decomposition is a tree, each of whose nodes 7r is labeled with a subset Vπ of the nodes in the HRG rule. We can construct a term encoding from a tree decomposition bottom-up. Leaves map to variables or constants; binary nodes introduce merge operations; and we use rename and forget operations to ensure that the subterm for the node 7r evaluates to an s-graph in which exactly the nodes in Vπ are source nodes.2 In the example, we obtain (3) from the tree decomposition in Fig. 4 like this. The tree-width k of an HRG rule is mea</context>
<context position="25879" citStr="Chiang et al. (2013)" startWordPosition="4775" endWordPosition="4778">he rule in (5). Unlike in the top-down case, every bottom-up query returns at most one rule. The runtime of the complete parsing algorithm is bounded by the number I of different queries to DSG that we receive, multiplied by the perrule runtime T that we need to answer each query. The factor I is analogous to the number of rule instances in schema-based parsing (Shieber et al., 1995). The factor T is often ignored in the analysis of parsing algorithms, because in parsing schemata for strings, we typically have T = O(1). This need not be the case for graph parsers. In the HRG parsing schema of Chiang et al. (2013), we have I = O(nk+13d(k+1)), where k is the treewidth of the HRG. In addition, each of their rule instances takes time T = O(d(k + 1)) to actually calculate the new item. Below, we show how we can efficiently answer both bottom-up and top-down queries to DSG. Every s-graph grammar has an equivalent normal form where every constant describes an s-graph with a single edge. Assuming that the grammar is in this normal form, queries of the form 0 —* g (resp. C —* g), where g is a constant of the HRalgebra, are trivial and we will not consider them further. Table 1 summarizes our results. 5.1 Botto</context>
</contexts>
<marker>Chiang, Andreas, Bauer, Hermann, Jones, Knight, 2013</marker>
<rawString>David Chiang, Jacob Andreas, Daniel Bauer, Karl Moritz Hermann, Bevan Jones, and Kevin Knight. 2013. Parsing graphs with hyperedge replacement grammars. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 924–932.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Phil Blunsom</author>
<author>Sharon Goldwater</author>
</authors>
<title>Inducing tree-substitution grammars.</title>
<date>2010</date>
<journal>Journal of Machine Learning Research (JMLR),</journal>
<volume>11</volume>
<pages>3096</pages>
<contexts>
<context position="33680" citStr="Cohn et al. (2010)" startWordPosition="6221" endWordPosition="6224"> AMR-Bank version 1.4, available from amr.isi.edu. This graph-bank consists of 1562 sentences manually annotated with AMRs. We implemented our algorithms in Java as part of the Alto parser for IRTGs (Alto Developers, 2015), and compared them to the Bolinas HRG parser (Andreas et al., 2013). We measured runtimes using Java 8 (for Alto) and Pypy 2.5.0 (for Bolinas) on an Intel Xeon E7-8857 CPU at 3 GHz, after warming up the JIT compilers. As there are no freely available grammars for this dataset, we created our own for the evaluation, using Bayesian grammar induction roughly along the lines of Cohn et al. (2010). We provide the grammars as supplementary material. Around 64% of the AMRs in the graph-bank have treewidth 1 and can thus be parsed using s = 2 source names. 98% have treewidth 1 or 2, corresponding to s = 3 source names. All experiments evaluated parser times on the same AMRs from which the grammar was sampled. 1488 1 10 100 10000 1e+06 Top-down versus bottom-up. Fig. 5 compares the performance of the top-down and the bottomup algorithm, on a grammar with three source names sampled from all 1261 graphs with up to 10 nodes. Each point in the figure is the geometric mean of runtimes for all g</context>
</contexts>
<marker>Cohn, Blunsom, Goldwater, 2010</marker>
<rawString>Trevor Cohn, Phil Blunsom, and Sharon Goldwater. 2010. Inducing tree-substitution grammars. Journal of Machine Learning Research (JMLR), 11:3053– 3096.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hubert Comon</author>
<author>Max Dauchet</author>
</authors>
<title>R´emi Gilleron, Florent Jacquemard, Denis Lugiez, Christof L¨oding, Sophie Tison, and Marc Tommasi.</title>
<date>2008</date>
<marker>Comon, Dauchet, 2008</marker>
<rawString>Hubert Comon, Max Dauchet, R´emi Gilleron, Florent Jacquemard, Denis Lugiez, Christof L¨oding, Sophie Tison, and Marc Tommasi. 2008. Tree automata techniques and applications. http:// tata.gforge.inria.fr/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bruno Courcelle</author>
<author>Joost Engelfriet</author>
</authors>
<title>Graph Structure and Monadic Second-Order Logic,</title>
<date>2012</date>
<volume>138</volume>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="6742" citStr="Courcelle and Engelfriet, 2012" startWordPosition="1144" endWordPosition="1148">encode node labels. Given a graph G, we write n = |V |, m = |E|, and d for the maximum degree of any node in V . If f : A --� B and g : A --� B are partial functions, we let the partial function f U g be defined if for all a E A with both f(a) and g(a) defined, we have f(a) = g(a). We then let (f U g)(a) be f(a) if f(a) is defined; g(a) if g(a) is defined; and undefined otherwise. 3.1 The HR algebra of graphs with sources Our grammars describe how to build graphs from smaller pieces. They do this by accessing nodes (called source nodes) which are assigned “public names”. We define an s-graph (Courcelle and Engelfriet, 2012) as a pair SG = (G, 0) of a graph G and a source assignment, i.e. a partial, injective function 0 : S --� V that maps some source names from a finite set S to the nodes of G. We call the nodes in 0(S) the source nodes or sources of SG; all other nodes are internal nodes. If 0 is defined on the source name Q, we call 0(Q) the Q-source of SG. Throughout, we let s = |S|. Examples of s-graphs are given in Fig. 1. We use numbers as node names and lowercase strings for edge names (except in the concrete graphs of Fig. 1, where the edges are marked with edge labels instead). Source nodes are drawn in</context>
<context position="8326" citStr="Courcelle and Engelfriet (2012)" startWordPosition="1456" endWordPosition="1459">st, we can rename the R-source of (c) to an Osource. The result, denoted SGd = SGsleep[R —* O], is shown in (d). Next, we can merge SGd with SGwant. This copies the edges and nodes of SGd and SGwant into a new s-graph; but crucially, for every source name Q the two s-graphs have in common, the Q-sources of the graphs are fused into a single node (and become a Q-source of the result). We write ||for the merge operation; 1482 thus we obtain SGe = SGd ||SGwant, shown in (e). Finally, we can forget source names. The graph SGf = fS(fp(SGe)), in which we forgot S and O, is shown in (f). We refer to Courcelle and Engelfriet (2012) for technical details.1 We can take the set of all s-graphs, together with these operations, as an algebra of s-graphs. In addition to the binary merge operation and the unary operations for forget and rename, we fix some finite set of atomic s-graphs and take them as constants of the algebra which evaluate to themselves. Following Courcelle and Engelfriet, we call this algebra the HR algebra. We can evaluate any term T consisting of these operation symbols into an sgraph QT] as usual. For instance, the following term encodes the merge, forget, and rename operations from the example above, an</context>
<context position="10281" citStr="Courcelle and Engelfriet (2012)" startWordPosition="1814" endWordPosition="1818">er, if we see SG1 and SG2 as subgraphs of SG, SG1 ||SG2 may no longer be a subgraph of SG. For instance, we cannot merge the s-graphs (b) and (c) in Fig. 2 as part of the graph (a): The startpoints of the edges a and d are both A-sources and would thus become the same node (unlike in (a)), and furthermore the edge d would have to be duplicated. In graph parsing, we already know the identity of all nodes and edges in sub-s-graphs (as nodes and edges in SG), and must thus pay attention that merge operations do not accidentally fuse or duplicate them. In partic1 Note that the rename operation of Courcelle and Engelfriet (2012) allows for swapping source assignments and making multiple renames in one step. We simplify the presentation here, but all of our techniques extend easily. Figure 2: (a) An s-graph with (b,c) some sub-sgraphs, (d) its BCCs, and (e) its block-cutpoint graph. ular, two sub-s-graphs cannot be merged if they have edges in common. We call a sub-s-graph SG1 of SG extensible if there is another sub-s-graph SG2 of SG such that SG1 ||SG2 contains the same edges as SG. An example of a sub-s-graph that is not extensible is the sub-s-graph (b) of the s-graph in (a) in Fig. 2. Because sources can only be </context>
<context position="19859" citStr="Courcelle and Engelfriet, 2012" startWordPosition="3647" endWordPosition="3650">loiting the closure of regular tree languages under intersection and inverse homomorphism; see Koller and Kuhlmann (2011) for details. In practice, this means that whenever we want to apply IRTGs to a new algebra (as, in this paper, to the HR algebra), we can obtain a parsing algorithm by specifying how to compute decomposition grammars over this algebra. This is the topic of Section 5. 4.3 Relationship to HRG We close our exposition of s-graph grammars by relating them to HRGs. It is known that the graph languages that can be described with s-graph grammars are the same as the HRG languages (Courcelle and Engelfriet, 2012, Prop. 4.27). Here we establish a more precise equivalence result, so we can compare our asymptotic runtimes directly to those of HRG parsers. An HRG rule, such as the one shown in Fig. 4, rewrites a nonterminal symbol into a graph. The example rule constructs a graph for the nonterminal S by combining the graph Gr in the middle (with nodes 1, 2, 3 and edges e, f) with graphs GX and GY that are recursively derived from the nonterminals X and Y . The combination happens by merging the external nodes of GX and GY with nodes of Gr: the squiggly lines indicate that the external node I of GX shoul</context>
<context position="21961" citStr="Courcelle and Engelfriet (2012)" startWordPosition="4071" endWordPosition="4074"> 1 as I-source and 2 as III-source. However, this requires the use of three source names (I, II, and III). The following encoding of the rule uses the sources more economically: (3) fII((I) e, (II) ||x1) ||(I) f → (II) ||x2 This term uses only two source names. It forgets II as soon as we are finished with the node 2, and frees the name up for reuse for 3. The complete encoding of the HRG rule consists of the RTG rule S , r(X, Y) with h(r) = (3). In the general case, one can “read off” possible term encodings of a HRG rule from its tree decompositions; see Chiang et al. (2013) or Def. 2.80 of Courcelle and Engelfriet (2012) for details. A tree decomposition is a tree, each of whose nodes 7r is labeled with a subset Vπ of the nodes in the HRG rule. We can construct a term encoding from a tree decomposition bottom-up. Leaves map to variables or constants; binary nodes introduce merge operations; and we use rename and forget operations to ensure that the subterm for the node 7r evaluates to an s-graph in which exactly the nodes in Vπ are source nodes.2 In the example, we obtain (3) from the tree decomposition in Fig. 4 like this. The tree-width k of an HRG rule is measured by finding the tree decomposition of the r</context>
</contexts>
<marker>Courcelle, Engelfriet, 2012</marker>
<rawString>Bruno Courcelle and Joost Engelfriet. 2012. Graph Structure and Monadic Second-Order Logic, volume 138 of Encyclopedia of Mathematics and its Applications. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Drewes</author>
<author>Hans-J¨org Kreowski</author>
<author>Annegret Habel</author>
</authors>
<title>Hyperedge replacement graph grammars.</title>
<date>1997</date>
<booktitle>In Grzegorz Rozenberg, editor, Handbook of Graph Grammars and Computing by Graph Transformation,</booktitle>
<pages>95--162</pages>
<publisher>World Scientific Publishing Co., Inc.</publisher>
<contexts>
<context position="1618" citStr="Drewes et al., 1997" startWordPosition="237" endWordPosition="240">2013; Oepen et al., 2014), much work has focused on computing graph-based semantic representations from strings (Jones et al., 2012; Flanigan et al., 2014; Martins and Almeida, 2014). One major approach to graph-based semantic parsing is to learn an explicit synchronous grammar which relates strings with graphs. One can then apply methods from statistical parsing to parse the string and read off the graph. Chiang et al. (2013) and Quernheim and Knight (2012) represent this mapping of a (latent) syntactic structure to a graph with a grammar formalism called hyperedge replacement grammar (HRG; (Drewes et al., 1997)). As an alternative to HRG, Koller (2015) introduced s-graph grammars and showed that they support linguistically reasonable grammars for graph-based semantics construction. One problem that is only partially understood in the context of semantic parsing with explicit grammars is graph parsing, i.e. the computation of the possible analyses the grammar assigns to an input graph (as opposed to string). This problem arises whenever one tries to generate a string from a graph (e.g., on the generation side of an MT system), but also in the context of extracting and training a synchronous grammar, </context>
<context position="4290" citStr="Drewes et al., 1997" startWordPosition="667" endWordPosition="670">n for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1481–1490, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Figure 1: AMR (a) for ‘The boy wants to sleep’, and s-graphs. We call (b) SGwant and (c) SGsleep. (a) (b) (c) (d) (e) (f) boy arg1 want arg1 arg2 sleep sleep R sleep O want arg1 arg2 arg1 arg1 arg1 arg2 want R R want R arg1 arg2 boy S O sleep arg1 boy S O S S boy sleep arg1 The primary grammar formalism currently in use for synchronous graph grammars is hyperedge replacement grammar (HRG) (Drewes et al., 1997), which we sketch in Section 4.3. An alternative is offered by Koller (2015), who introduced sgraph grammars and showed that they lend themselves to manually written grammars for semantic construction. In this paper, we show the equivalence of HRG and s-graph grammars and work out graph parsing for s-graph grammars. The first polynomial graph parsing algorithm for HRGs on graphs with limited connectivity was presented by Lautemann (1988). Lautemann’s original algorithm is a top-down parser, which is presented at a rather abstract level that does not directly support implementation or detailed </context>
</contexts>
<marker>Drewes, Kreowski, Habel, 1997</marker>
<rawString>Frank Drewes, Hans-J¨org Kreowski, and Annegret Habel. 1997. Hyperedge replacement graph grammars. In Grzegorz Rozenberg, editor, Handbook of Graph Grammars and Computing by Graph Transformation, pages 95–162. World Scientific Publishing Co., Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Flanigan</author>
<author>Sam Thomson</author>
<author>Jamie Carbonell</author>
<author>Chris Dyer</author>
<author>Noah A Smith</author>
</authors>
<title>A discriminative graph-based parser for the abstract meaning representation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1426--1436</pages>
<location>Baltimore, Maryland.</location>
<contexts>
<context position="1152" citStr="Flanigan et al., 2014" startWordPosition="161" endWordPosition="164">p-down) with s-graph grammars. On the related problem of graph parsing with hyperedge replacement grammars, our implementations outperform the best previous system by several orders of magnitude. 1 Introduction The recent years have seen an increased interest in semantic parsing, the problem of deriving a semantic representation for natural-language expressions with data-driven methods. With the recent availability of graph-based meaning banks (Banarescu et al., 2013; Oepen et al., 2014), much work has focused on computing graph-based semantic representations from strings (Jones et al., 2012; Flanigan et al., 2014; Martins and Almeida, 2014). One major approach to graph-based semantic parsing is to learn an explicit synchronous grammar which relates strings with graphs. One can then apply methods from statistical parsing to parse the string and read off the graph. Chiang et al. (2013) and Quernheim and Knight (2012) represent this mapping of a (latent) syntactic structure to a graph with a grammar formalism called hyperedge replacement grammar (HRG; (Drewes et al., 1997)). As an alternative to HRG, Koller (2015) introduced s-graph grammars and showed that they support linguistically reasonable grammars</context>
</contexts>
<marker>Flanigan, Thomson, Carbonell, Dyer, Smith, 2014</marker>
<rawString>Jeffrey Flanigan, Sam Thomson, Jamie Carbonell, Chris Dyer, and Noah A. Smith. 2014. A discriminative graph-based parser for the abstract meaning representation. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1426–1436, Baltimore, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bevan K Jones</author>
<author>Jacob Andreas</author>
<author>Daniel Bauer</author>
<author>Karl Moritz Hermann</author>
<author>Kevin Knight</author>
</authors>
<title>Semantics – Based machine translation with hyperedge replacement grammars.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING 2012: Technical Papers,</booktitle>
<pages>1359--1376</pages>
<contexts>
<context position="1129" citStr="Jones et al., 2012" startWordPosition="157" endWordPosition="160">ng (bottom-up and top-down) with s-graph grammars. On the related problem of graph parsing with hyperedge replacement grammars, our implementations outperform the best previous system by several orders of magnitude. 1 Introduction The recent years have seen an increased interest in semantic parsing, the problem of deriving a semantic representation for natural-language expressions with data-driven methods. With the recent availability of graph-based meaning banks (Banarescu et al., 2013; Oepen et al., 2014), much work has focused on computing graph-based semantic representations from strings (Jones et al., 2012; Flanigan et al., 2014; Martins and Almeida, 2014). One major approach to graph-based semantic parsing is to learn an explicit synchronous grammar which relates strings with graphs. One can then apply methods from statistical parsing to parse the string and read off the graph. Chiang et al. (2013) and Quernheim and Knight (2012) represent this mapping of a (latent) syntactic structure to a graph with a grammar formalism called hyperedge replacement grammar (HRG; (Drewes et al., 1997)). As an alternative to HRG, Koller (2015) introduced s-graph grammars and showed that they support linguistica</context>
</contexts>
<marker>Jones, Andreas, Bauer, Hermann, Knight, 2012</marker>
<rawString>Bevan K. Jones, Jacob Andreas, Daniel Bauer, Karl Moritz Hermann, and Kevin Knight. 2012. Semantics – Based machine translation with hyperedge replacement grammars. In Proceedings of COLING 2012: Technical Papers, pages 1359–1376.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Koller</author>
<author>Marco Kuhlmann</author>
</authors>
<title>A generalized view on parsing and translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 12th International Conference on Parsing Technologies,</booktitle>
<pages>2--13</pages>
<contexts>
<context position="15463" citStr="Koller and Kuhlmann (2011)" startWordPosition="2852" endWordPosition="2855">two BRs have no in-boundary edges in common, i.e. E1 ∩ E2 = ∅; (c) for every source node v of β1, the last edge on the path in 5G from v to the closest source node of β2 is not an in-boundary edge of β2, and vice versa. Furthermore, if these conditions hold, we have T(β1 ||β2) = T(β1) ||T(β2), where we define β1 ||β2 = (E1 ∪ E2, 01 ∪ 02). 4 S-graph grammars We are now ready to define s-graph grammars, which describe languages of s-graphs. We also introduce graph parsing and relate s-graph grammars to HRGs. 4.1 Grammars for languages of s-graphs We use interpreted regular tree grammars (IRTGs; Koller and Kuhlmann (2011)) to describe languages of s-graphs. IRTGs are a very general mechanism for describing languages over and relations between arbitrary algebras. They separate conceptually the generation of a grammatical derivation from its interpretation as a string, tree, graph, or some other object. Consider, as an example, the tiny grammar in Fig. 3; see Koller (2015) for linguistically meaningful grammars. The left column consists of a regular tree grammar G (RTG; see e.g. Comon et al. (2008)) with two rules. This RTG describes a regular language L(G) of derivation trees (in general, it may be infinite). I</context>
<context position="19350" citStr="Koller and Kuhlmann (2011)" startWordPosition="3556" endWordPosition="3559">g, we take a different, more generic approach. We assume that the set D of all decompositions T, i.e. of all terms T that evaluate to a in the algebra, can be represented as the language D = L(Da) of a decomposition grammar Da. Da is an RTG over the signature of the algebra. Crucially, Da only depends on the algebra and a itself, and not on G or h, because D contains all terms that evaluate to a and not just those that are licensed by the grammar. However, we can compute Ga from Da efficiently by exploiting the closure of regular tree languages under intersection and inverse homomorphism; see Koller and Kuhlmann (2011) for details. In practice, this means that whenever we want to apply IRTGs to a new algebra (as, in this paper, to the HR algebra), we can obtain a parsing algorithm by specifying how to compute decomposition grammars over this algebra. This is the topic of Section 5. 4.3 Relationship to HRG We close our exposition of s-graph grammars by relating them to HRGs. It is known that the graph languages that can be described with s-graph grammars are the same as the HRG languages (Courcelle and Engelfriet, 2012, Prop. 4.27). Here we establish a more precise equivalence result, so we can compare our a</context>
</contexts>
<marker>Koller, Kuhlmann, 2011</marker>
<rawString>Alexander Koller and Marco Kuhlmann. 2011. A generalized view on parsing and translation. In Proceedings of the 12th International Conference on Parsing Technologies, pages 2–13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Koller</author>
</authors>
<title>Semantics construction with graph grammars.</title>
<date>2015</date>
<booktitle>In Proceedings of the 11th International Conference on Computational Semantics (IWCS),</booktitle>
<pages>228--238</pages>
<contexts>
<context position="1660" citStr="Koller (2015)" startWordPosition="246" endWordPosition="247">on computing graph-based semantic representations from strings (Jones et al., 2012; Flanigan et al., 2014; Martins and Almeida, 2014). One major approach to graph-based semantic parsing is to learn an explicit synchronous grammar which relates strings with graphs. One can then apply methods from statistical parsing to parse the string and read off the graph. Chiang et al. (2013) and Quernheim and Knight (2012) represent this mapping of a (latent) syntactic structure to a graph with a grammar formalism called hyperedge replacement grammar (HRG; (Drewes et al., 1997)). As an alternative to HRG, Koller (2015) introduced s-graph grammars and showed that they support linguistically reasonable grammars for graph-based semantics construction. One problem that is only partially understood in the context of semantic parsing with explicit grammars is graph parsing, i.e. the computation of the possible analyses the grammar assigns to an input graph (as opposed to string). This problem arises whenever one tries to generate a string from a graph (e.g., on the generation side of an MT system), but also in the context of extracting and training a synchronous grammar, e.g. in EM training. The state of the art </context>
<context position="4366" citStr="Koller (2015)" startWordPosition="683" endWordPosition="684">al Language Processing, pages 1481–1490, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Figure 1: AMR (a) for ‘The boy wants to sleep’, and s-graphs. We call (b) SGwant and (c) SGsleep. (a) (b) (c) (d) (e) (f) boy arg1 want arg1 arg2 sleep sleep R sleep O want arg1 arg2 arg1 arg1 arg1 arg2 want R R want R arg1 arg2 boy S O sleep arg1 boy S O S S boy sleep arg1 The primary grammar formalism currently in use for synchronous graph grammars is hyperedge replacement grammar (HRG) (Drewes et al., 1997), which we sketch in Section 4.3. An alternative is offered by Koller (2015), who introduced sgraph grammars and showed that they lend themselves to manually written grammars for semantic construction. In this paper, we show the equivalence of HRG and s-graph grammars and work out graph parsing for s-graph grammars. The first polynomial graph parsing algorithm for HRGs on graphs with limited connectivity was presented by Lautemann (1988). Lautemann’s original algorithm is a top-down parser, which is presented at a rather abstract level that does not directly support implementation or detailed complexity analysis. We extend Lautemann’s work by showing how new parse ite</context>
<context position="15819" citStr="Koller (2015)" startWordPosition="2911" endWordPosition="2912">w ready to define s-graph grammars, which describe languages of s-graphs. We also introduce graph parsing and relate s-graph grammars to HRGs. 4.1 Grammars for languages of s-graphs We use interpreted regular tree grammars (IRTGs; Koller and Kuhlmann (2011)) to describe languages of s-graphs. IRTGs are a very general mechanism for describing languages over and relations between arbitrary algebras. They separate conceptually the generation of a grammatical derivation from its interpretation as a string, tree, graph, or some other object. Consider, as an example, the tiny grammar in Fig. 3; see Koller (2015) for linguistically meaningful grammars. The left column consists of a regular tree grammar G (RTG; see e.g. Comon et al. (2008)) with two rules. This RTG describes a regular language L(G) of derivation trees (in general, it may be infinite). In the example, we can derive S ⇒ r1(VP) ⇒ r1(r2), therefore we have t = r1(r2) ∈ L(G). We then use a tree homomorphism h to rewrite the derivation trees into terms over an algebra; in this case the HR algebra. In the example, the values h(r1) and h(r2) are specified in the second column of Fig. 3. We compute h(t) by substituting the variable x1 in h(r1) </context>
<context position="17088" citStr="Koller (2015)" startWordPosition="3147" endWordPosition="3148">It evaluates to the s-graph 5Gf in Fig. 1(f). 1484 Rule of RTG G homomorphism h fS(fO(SGwant ||x1[R → O])) SGsleep Figure 3: An example s-graph grammar. In general, the IRTG G = (G, h, A) generates the language L(G) = {Qh(t)� |t ∈ L(G)}, where Q· is evaluation in the algebra A. Thus, in the example, we have L(G) = {SGf}. In this paper, we focus on IRTGs that describe languages L(G) ⊆ A of objects in an algebra; specifically, of s-graphs in the HR algebra. However, IRTGs extend naturally to a synchronous grammar formalism by adding more homomorphisms and algebras. For instance, the grammars in Koller (2015) map each derivation tree simultaneously to a string and an s-graph, and therefore describe a binary relation between strings and sgraphs. We call IRTGs where at least one algebra is the HR algebra, s-graph grammars. 4.2 Parsing with s-graph grammars In this paper, we are concerned with the parsing problem of s-graph grammars. In the context of IRTGs, parsing means that we are looking for those derivation trees t that are (a) grammatically correct, i.e. t ∈ L(G), and (b) match some given input object a, i.e. h(t) evaluates to a in the algebra. Because the set P of such derivation trees may be </context>
</contexts>
<marker>Koller, 2015</marker>
<rawString>Alexander Koller. 2015. Semantics construction with graph grammars. In Proceedings of the 11th International Conference on Computational Semantics (IWCS), pages 228–238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Clemens Lautemann</author>
</authors>
<title>Decomposition trees: Structured graph representation and efficient algorithms.</title>
<date>1988</date>
<booktitle>In Max Dauchet and Maurice Nivat, editors, 13th Colloquium on Trees in Algebra and Programming,</booktitle>
<volume>299</volume>
<pages>28--39</pages>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="4731" citStr="Lautemann (1988)" startWordPosition="741" endWordPosition="742">leep arg1 boy S O S S boy sleep arg1 The primary grammar formalism currently in use for synchronous graph grammars is hyperedge replacement grammar (HRG) (Drewes et al., 1997), which we sketch in Section 4.3. An alternative is offered by Koller (2015), who introduced sgraph grammars and showed that they lend themselves to manually written grammars for semantic construction. In this paper, we show the equivalence of HRG and s-graph grammars and work out graph parsing for s-graph grammars. The first polynomial graph parsing algorithm for HRGs on graphs with limited connectivity was presented by Lautemann (1988). Lautemann’s original algorithm is a top-down parser, which is presented at a rather abstract level that does not directly support implementation or detailed complexity analysis. We extend Lautemann’s work by showing how new parse items can be represented and constructed efficiently. Finally, Chiang et al. (2013) presented a bottom-up graph parser for HRGs, in which the representation and construction of items was worked out for the first time. It produces O((n · 3d)k+1) instances of the rules in a parsing schema, where n is the number of nodes of the graph, d is the maximum degree of any nod</context>
</contexts>
<marker>Lautemann, 1988</marker>
<rawString>Clemens Lautemann. 1988. Decomposition trees: Structured graph representation and efficient algorithms. In Max Dauchet and Maurice Nivat, editors, 13th Colloquium on Trees in Algebra and Programming, volume 299 of Lecture Notes in Computer Science, pages 28–39. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Clemens Lautemann</author>
</authors>
<title>The complexity of graph languages generated by hyperedge replacement.</title>
<date>1990</date>
<journal>Acta Informatica,</journal>
<pages>27--399</pages>
<contexts>
<context position="36127" citStr="Lautemann (1990)" startWordPosition="6645" endWordPosition="6647">ver 300 million rules. However, many uses of decomposition grammars, such as sampling for grammar induction, can be phrased purely in terms of top-down queries. The top-down algorithm can answer these without computing the entire grammar, alleviating the memory problem. Finally, we analyzed the asymptotic runtimes in Table 1 in terms of the maximum number d · s of in-boundary edges. However, the top-down parser does not manipulate individual edges, but entire s-components. The maximum number D3 of scomponents into which a set of s sources can split a graph is called the s-separability of G by Lautemann (1990). We can analyze the runtime of the top-down parser more carefully as O(n33D3ds); as the dotted line in Fig. 5 shows, this predicts the runtime well. Interestingly, D3 is much lower in practice than its theoretical maximum. In the Figure 5: Runtimes of our parsers with s = 3. Figure 6: Runtimes of our parsers and Bolinas with s = 2. “Little Prince” AMR-Bank, the mean of D3 is 6.0, whereas the mean of 3 · d is 12.7. Thus exploiting the s-component structure of the graph can improve parsing times. 7 Conclusion We presented two new graph parsing algorithms for s-graph grammars. These were framed </context>
</contexts>
<marker>Lautemann, 1990</marker>
<rawString>Clemens Lautemann. 1990. The complexity of graph languages generated by hyperedge replacement. Acta Informatica, 27:399–421.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e F T Martins</author>
<author>Mariana S C Almeida</author>
</authors>
<title>Priberam: A turbo semantic parser with second order features.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>471--476</pages>
<contexts>
<context position="1180" citStr="Martins and Almeida, 2014" startWordPosition="165" endWordPosition="168">ammars. On the related problem of graph parsing with hyperedge replacement grammars, our implementations outperform the best previous system by several orders of magnitude. 1 Introduction The recent years have seen an increased interest in semantic parsing, the problem of deriving a semantic representation for natural-language expressions with data-driven methods. With the recent availability of graph-based meaning banks (Banarescu et al., 2013; Oepen et al., 2014), much work has focused on computing graph-based semantic representations from strings (Jones et al., 2012; Flanigan et al., 2014; Martins and Almeida, 2014). One major approach to graph-based semantic parsing is to learn an explicit synchronous grammar which relates strings with graphs. One can then apply methods from statistical parsing to parse the string and read off the graph. Chiang et al. (2013) and Quernheim and Knight (2012) represent this mapping of a (latent) syntactic structure to a graph with a grammar formalism called hyperedge replacement grammar (HRG; (Drewes et al., 1997)). As an alternative to HRG, Koller (2015) introduced s-graph grammars and showed that they support linguistically reasonable grammars for graph-based semantics c</context>
</contexts>
<marker>Martins, Almeida, 2014</marker>
<rawString>Andr´e F. T. Martins and Mariana S. C. Almeida. 2014. Priberam: A turbo semantic parser with second order features. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 471–476.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Oepen</author>
<author>Marco Kuhlmann</author>
<author>Yusuke Miyao</author>
<author>Daniel Zeman</author>
<author>Dan Flickinger</author>
<author>Jan Hajic</author>
<author>Angelina Ivanova</author>
<author>Yi Zhang</author>
</authors>
<title>Task 8: Broad-coverage semantic dependency parsing.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>63--72</pages>
<note>SemEval</note>
<contexts>
<context position="1023" citStr="Oepen et al., 2014" startWordPosition="141" endWordPosition="144"> string-to-graph grammars, and when generating strings from them. We present two algorithms for graph parsing (bottom-up and top-down) with s-graph grammars. On the related problem of graph parsing with hyperedge replacement grammars, our implementations outperform the best previous system by several orders of magnitude. 1 Introduction The recent years have seen an increased interest in semantic parsing, the problem of deriving a semantic representation for natural-language expressions with data-driven methods. With the recent availability of graph-based meaning banks (Banarescu et al., 2013; Oepen et al., 2014), much work has focused on computing graph-based semantic representations from strings (Jones et al., 2012; Flanigan et al., 2014; Martins and Almeida, 2014). One major approach to graph-based semantic parsing is to learn an explicit synchronous grammar which relates strings with graphs. One can then apply methods from statistical parsing to parse the string and read off the graph. Chiang et al. (2013) and Quernheim and Knight (2012) represent this mapping of a (latent) syntactic structure to a graph with a grammar formalism called hyperedge replacement grammar (HRG; (Drewes et al., 1997)). As</context>
<context position="3607" citStr="Oepen et al., 2014" startWordPosition="549" endWordPosition="552"> graph-bank shows that our implementations of both algorithms outperform Bolinas by several orders of magnitude. Furthermore, the top-down algorithm can be more memory-efficient in practice. 2 Related work The AMR-Bank (Banarescu et al., 2013) annotates sentences with abstract meaning representations (AMRs), like the one shown in Fig. 1(a). These are graphs that represent the predicate-argument structure of a sentence; notably, phenomena such as control are represented by reentrancies in the graph. Another major graph-bank is the SemEval2014 shared task on semantic dependency parsing dataset (Oepen et al., 2014). 1481 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1481–1490, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Figure 1: AMR (a) for ‘The boy wants to sleep’, and s-graphs. We call (b) SGwant and (c) SGsleep. (a) (b) (c) (d) (e) (f) boy arg1 want arg1 arg2 sleep sleep R sleep O want arg1 arg2 arg1 arg1 arg1 arg2 want R R want R arg1 arg2 boy S O sleep arg1 boy S O S S boy sleep arg1 The primary grammar formalism currently in use for sync</context>
</contexts>
<marker>Oepen, Kuhlmann, Miyao, Zeman, Flickinger, Hajic, Ivanova, Zhang, 2014</marker>
<rawString>Stephan Oepen, Marco Kuhlmann, Yusuke Miyao, Daniel Zeman, Dan Flickinger, Jan Hajic, Angelina Ivanova, and Yi Zhang. 2014. SemEval 2014 Task 8: Broad-coverage semantic dependency parsing. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 63–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Quernheim</author>
<author>Kevin Knight</author>
</authors>
<title>DAGGER: A toolkit for automata on directed acyclic graphs.</title>
<date>2012</date>
<booktitle>In Proceedings of the 10th International Workshop on Finite State Methods and Natural Language Processing,</booktitle>
<pages>40--44</pages>
<contexts>
<context position="1460" citStr="Quernheim and Knight (2012)" startWordPosition="211" endWordPosition="214">ng a semantic representation for natural-language expressions with data-driven methods. With the recent availability of graph-based meaning banks (Banarescu et al., 2013; Oepen et al., 2014), much work has focused on computing graph-based semantic representations from strings (Jones et al., 2012; Flanigan et al., 2014; Martins and Almeida, 2014). One major approach to graph-based semantic parsing is to learn an explicit synchronous grammar which relates strings with graphs. One can then apply methods from statistical parsing to parse the string and read off the graph. Chiang et al. (2013) and Quernheim and Knight (2012) represent this mapping of a (latent) syntactic structure to a graph with a grammar formalism called hyperedge replacement grammar (HRG; (Drewes et al., 1997)). As an alternative to HRG, Koller (2015) introduced s-graph grammars and showed that they support linguistically reasonable grammars for graph-based semantics construction. One problem that is only partially understood in the context of semantic parsing with explicit grammars is graph parsing, i.e. the computation of the possible analyses the grammar assigns to an input graph (as opposed to string). This problem arises whenever one trie</context>
</contexts>
<marker>Quernheim, Knight, 2012</marker>
<rawString>Daniel Quernheim and Kevin Knight. 2012. DAGGER: A toolkit for automata on directed acyclic graphs. In Proceedings of the 10th International Workshop on Finite State Methods and Natural Language Processing, pages 40–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
<author>Yves Schabes</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Principles and implementation of deductive parsing.</title>
<date>1995</date>
<journal>Journal of Logic Programming,</journal>
<pages>24--1</pages>
<contexts>
<context position="25645" citStr="Shieber et al., 1995" startWordPosition="4731" endWordPosition="4734">p O(d + s) O(s) O(ds) top-down O(ds) O(s) O(ds) I = # rules O(ns2ds) O(ns2ds) O(ns3ds) Table 1: Amortized per-rule runtimes T for the different rule types. operation o. In the example above, a merge query for 5Gb and 5Gd should yield the rule in (5). Unlike in the top-down case, every bottom-up query returns at most one rule. The runtime of the complete parsing algorithm is bounded by the number I of different queries to DSG that we receive, multiplied by the perrule runtime T that we need to answer each query. The factor I is analogous to the number of rule instances in schema-based parsing (Shieber et al., 1995). The factor T is often ignored in the analysis of parsing algorithms, because in parsing schemata for strings, we typically have T = O(1). This need not be the case for graph parsers. In the HRG parsing schema of Chiang et al. (2013), we have I = O(nk+13d(k+1)), where k is the treewidth of the HRG. In addition, each of their rule instances takes time T = O(d(k + 1)) to actually calculate the new item. Below, we show how we can efficiently answer both bottom-up and top-down queries to DSG. Every s-graph grammar has an equivalent normal form where every constant describes an s-graph with a sing</context>
</contexts>
<marker>Shieber, Schabes, Pereira, 1995</marker>
<rawString>Stuart M. Shieber, Yves Schabes, and Fernando C. N. Pereira. 1995. Principles and implementation of deductive parsing. Journal of Logic Programming, 24(1–2):3–36.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>