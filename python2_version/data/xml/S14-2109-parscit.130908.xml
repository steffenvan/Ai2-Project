<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000049">
<title confidence="0.9976795">
TCDSCSS: Dimensionality Reduction to Evaluate Texts of Varying
Lengths - an IR Approach
</title>
<author confidence="0.996103">
Arun Jayapal Martin Emms John D.Kelleher
</author>
<affiliation confidence="0.9961255">
Dept of Computer Science Dept of Computer Science School of Computing
Trinity College Dublin Trinity College Dublin Dublin Institute of Technology
</affiliation>
<email confidence="0.9916">
jayapala@cs.tcd.ie martin.emms@cs.tcd.ie john.d.kelleher@dit.ie
</email>
<sectionHeader confidence="0.993743" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999720933333334">
This paper provides system description of
the cross-level semantic similarity task for
the SEMEVAL-2014 workshop. Cross-
level semantic similarity measures the de-
gree of relatedness between texts of vary-
ing lengths such as Paragraph to Sen-
tence and Sentence to Phrase. Latent Se-
mantic Analysis was used to evaluate the
cross-level semantic relatedness between
the texts to achieve above baseline scores,
tested on the training and test datasets. We
also tried using a bag-of-vectors approach
to evaluate the semantic relatedness. This
bag-of-vectors approach however did not
produced encouraging results.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999938833333333">
Semantic relatedness between texts have been
dealt with in multiple situations earlier. But it is
not usual to measure the semantic relatedness of
texts of varying lengths such as Paragraph to Sen-
tence (P2S) and Sentence to Phrase (S2P). This
task will be useful in natural language process-
ing applications such as paraphrasing and summa-
rization. The working principle of information re-
trieval system is the motivation for this task, where
the queries are not of equal lengths compared to
the documents in the index. We attempted two
ways to measure the semantic similarity for P2S
and S2P in a scale of 0 to 4, 4 meaning both texts
are similar and 0 being dissimilar. The first one
is Latent Semantic Analysis (LSA) and second, a
bag-of-vecors (BV) approach. An example of tar-
get similarity ratings for comparison type S2P is
provided in table 1.
</bodyText>
<footnote confidence="0.9751196">
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence de-
tails: http://creativecommons.org/licenses/
by/4.0/
</footnote>
<construct confidence="0.8106372">
Sentence: Schumacher was undoubtedly one of
the very greatest racing drivers there has ever
been, a man who was routinely, on every lap, able
to dance on a limit accessible to almost no-one
else.
</construct>
<table confidence="0.996969571428571">
Score Phrase
4 the unparalleled greatness
of Schumachers driving abilities
3 driving abilities
2 formula one racing
1 north-south highway
0 orthodontic insurance
</table>
<tableCaption confidence="0.987677">
Table 1: An Example - Sentence to Phrase simi-
larity ratings for each scale
</tableCaption>
<sectionHeader confidence="0.978217" genericHeader="introduction">
2 Data
</sectionHeader>
<bodyText confidence="0.999896090909091">
The task organizers provided training data, which
included 500 pairs of P2S, S2P, Phrase to Word
(P2W) and their similarity scores. The training
data for P2S and S2P included text from different
genres such as Newswire, Travel, Metaphoric and
Reviews. In the training data for P2S, newswire
text constituted 36% of the data, while reviews
constituted 10% of the data and rest of the three
genres shared 54% of the data.
Considering the different genres provided in the
training data, a chunk of data provided for NIST
TAC’s Knowledge Base Population was used for
building a term-by-document matrix on which
to base the LSA method. The data included
newswire text and web-text, where the web-text
included data mostly from blogs. We used 2343
documents from the NIST dataset1, which were
available in eXtended Markup Language format.
Further to the NIST dataset, all the paragraphs
in the training data2 of paragraph to sentence were
added to the dataset. To add these paragraphs to
the dataset, we converted each paragraph into a
</bodyText>
<footnote confidence="0.9999855">
1Distributed by LDC (Linguistic Data Consortium)
2provided by the SEMEVAL task-3 organizers
</footnote>
<page confidence="0.917709">
619
</page>
<note confidence="0.7409545">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 619–623,
Dublin, Ireland, August 23-24, 2014.
</note>
<bodyText confidence="0.820987166666667">
new document and the documents were added to
the corpus. The unique number of words identi-
fied in the corpus were approximately 40000.
word counts, and sometimes a thresholded or
‘boolean’ version, mapping all non-zero numbers
to 1.
</bodyText>
<sectionHeader confidence="0.952441" genericHeader="method">
3 System description
</sectionHeader>
<bodyText confidence="0.9997258">
We tried two different approaches for evaluating
the P2S and S2P. Latent Semantic Analysis (LSA)
using SVD worked better than the Bag-of-Vectors
(BV) approach. The description of both the ap-
proaches are discussed in this section.
</bodyText>
<subsectionHeader confidence="0.999903">
3.1 Latent Semantic Analysis
</subsectionHeader>
<bodyText confidence="0.999975833333333">
LSA has been used for information retrieval al-
lowing retrieval via vectors over latent, arguably
conceptual, dimensions, rather than over surface
word dimensions (Deerwester et al., 1990). It was
thought this would be of advantage for comparison
of texts of varying length.
</bodyText>
<subsectionHeader confidence="0.760896">
3.1.1 Representation
</subsectionHeader>
<bodyText confidence="0.996454">
The data corpus was converted into a mxn term-
by-document matrix, A, where the counts (cm,n)
of all terms (wm) in the corpus are represented
in rows and the respective documents (dn) in
columns:
</bodyText>
<equation confidence="0.91316525">
d1 d2 ··· dn
⎛w1 c1,1 c1,2 ··· c1,n
w2c2,1 c2,2 · · · c2,n
⎜ ⎜
.⎜ ...
. .⎝ . .. ....
. .
wm cm,1 cm,2 ··· cm,n
</equation>
<bodyText confidence="0.998784181818182">
The document indexing rules such as text tok-
enization, case standardization, stop words re-
moval, token stemming, and special characters and
punctuations removal were followed to get the ma-
trix A.
Singular Value Decomposition (SVD) decom-
poses the matrix into U, E and V matrices (ie.,
A = UEVT) such that U and V are orthonormal
matrices and E is a diagonal matrix with singular
values. Retaining just the first k columns of U and
V , gives an approximation of A
</bodyText>
<equation confidence="0.9367655">
A Pz Ak = UkEkV T (1)
k
</equation>
<bodyText confidence="0.985686666666667">
According to LSA, the columns of Uk are thought
of as representing latent, semantic dimensions,
and an arbitrary m-dimensional vector #»v can be
projected onto this semantic space by taking the
dot-product with each column of Uk; we will call
the result # »
vsem .
In the experiments reported later, the m-
dimensional vector #»v is sometimes a vector of
</bodyText>
<subsectionHeader confidence="0.633488">
3.1.2 Similarity Calculation
</subsectionHeader>
<bodyText confidence="0.999089875">
To evaluate the similarity of a paragraph, p, and a
sentence, s, first these are represented as vectors of
word counts, #»p and #»s , then these are projected in
the latent semantic space, to give # »
psem and # »
ssem ,
and then between these the cosine similarity met-
ric is calculated:
</bodyText>
<equation confidence="0.992774">
psem .# »
# »ssem» (2)
psem |. |ssem
</equation>
<bodyText confidence="0.999968666666667">
The cosine similarity metric provides a similarity
value in the range of 0 to 1, so to match the target
range of 0 to 4, the cosine values were multiplied
by 4. Exactly the same procedure is used for the
sentence to phrase comparison.
Further, the number of retained dimensions of
Uk was varied, giving different dimensionalities
of the LSA space. The results of testing at the re-
duced dimensions are discussed in 4.1
</bodyText>
<subsectionHeader confidence="0.999952">
3.2 Bag-of-Vectors
</subsectionHeader>
<bodyText confidence="0.999991">
Another method we experimented on could be
termed a ‘bag-of-vectors’ (BV) approach: each
word in an item to be compared is replaced by a
vector representing its co-occurrence behavior and
the obtained bags of vectors enter into the compar-
ison process.
</bodyText>
<subsectionHeader confidence="0.84291">
3.2.1 Representation
</subsectionHeader>
<bodyText confidence="0.999830666666667">
For the BV approach, the same data sources as was
used for the LSA approach is turned into a m x m
term-by-term co-occurrence matrix C:
</bodyText>
<equation confidence="0.957072090909091">
w1 w2 ··· wm
⎛w1 c1,1 c1,2 ··· c1,m
⎜
w2c2,1 c2,2 ··· c2,m
⎜ ⎜ ⎜
.⎜ ..
.
....
..
. ⎝ . . . .
wm cm,1 cm,2 · · · cm,m
</equation>
<bodyText confidence="0.998302125">
The same preprocessing steps as for the LSA ap-
proach applied (text tokenization, case standard-
ization, stop words removal, special characters and
punctuations removal). Via C, if one has a bag-
of-words representing a paragraph, sentence or
phrase, one can replace it by a bag-of-vectors, re-
placing each word wi by the corresponding row of
C – we will call these rows word-vectors.
</bodyText>
<equation confidence="0.655267">
A=
⎞
⎠ ⎟ ⎟ ⎟
,,„ »
cos (( �Nsem&gt; .ssem ) =
C =
⎞
⎠⎟⎟⎟⎟⎟
</equation>
<page confidence="0.949215">
620
</page>
<subsubsectionHeader confidence="0.528547">
3.2.2 Similarity Calculation
</subsubsectionHeader>
<bodyText confidence="0.993965909090909">
For calculating P2S similarity, the procedure is
as follows. The paragraph and sentence are tok-
enized, and stop-words were removed and are rep-
resented as two vectors p and s .
For each word pi from p , its word vector from
C is found, and this is compared to the word vector
for each word si in Is , via the cosine measure. The
highest similarity score for each word pi in p is
� �
stored in a vector Sp shown in (3). The overall
semantic similarity score between paragraph and
</bodyText>
<equation confidence="0.884508285714286">
� �
sentence is then the mean value of the vector Sp x
4 – see (4).
Sp = [Sp1 Sp2 ··· Spi] (3)
�ni=1 Spi
Ssim =
n
</equation>
<bodyText confidence="0.997634">
Exactly corresponding steps are carried out for the
S2P similarity. Although experiments were car-
ried out this particular BV approach, the results
were not encouraging. Details of the experiments
carried out are explained in 4.2.
</bodyText>
<sectionHeader confidence="0.999688" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999959818181818">
Different experiments were carried out using LSA
and BV systems described in sections 3.1 and 3.2
on the dataset described in section 2. Pearson
correlation and Spearman’s rank correlation were
the metrics used to evaluate the performance of
the systems. Pearson correlation provides the de-
gree of similarity between the system’s score for
each pair and the gold standard’s score for the said
pair while Spearman’s rank correlation provides
the degree of similarity between the rankings of
the pairs according to similarity.
</bodyText>
<subsectionHeader confidence="0.967979">
4.1 LSA
</subsectionHeader>
<bodyText confidence="0.999872">
The LSA model was used to evaluate the semantic
similarity between P2S and S2P.
</bodyText>
<subsectionHeader confidence="0.921551">
4.1.1 Paragraph to Sentence
</subsectionHeader>
<bodyText confidence="0.9999746">
An initial word-document matrix A was built by
extracting tokens just based on spaces, stop words
removed and tokens sorted in alphabetical order.
As described in 3.1.1, via the SVD of A, a ma-
trix Uk is obtained which can be used to project an
m dimensional vector into a k dimensional one.
In one setting the paragraph and sentence vec-
tors which are projected into the LSA space have
unique word counts for their dimensions. In an-
other setting before projection, these vectors are
</bodyText>
<table confidence="0.9862626">
Dimensions 100% 90% 50% 30% 10%
Basic word-doc representation 0.499 - 0.494 0.484 0.426
Evaluation-boolean counts 0.548 - 0.533 0.511 0.420
Constrained tokenization 0.368 0.564 0.540 0.516 0.480
Added data 0.461 0.602 0.568 0.517 0.522
</table>
<tableCaption confidence="0.998925">
Table 2: Pearson scores at different dimensions -
</tableCaption>
<note confidence="0.388157">
Paragraph to Sentence
</note>
<bodyText confidence="0.999538571428571">
thresholded into ‘boolean’ versions, with 1 for ev-
ery non-zero count.
The Pearson scores for these settings are in the
first and second rows of table 2. They show the
variation with the number of dimensions of the
LSA representation (that is the number of columns
of U that are kept)3. An observation is that the
usage of boolean values instead of word counts
showed improved results.
Further experiments were conducted, retaining
the boolean treatment of the vectors to be pro-
jected. In a new setting, further improvements
were made to the pre-processing step, creating a
new word-document matrix A using constrained
tokenization rules, removing unnecessary spaces
and tabs, and tokens stemmed4. The performance
of the similarity calculation is shown as the third
row of Table 2: there is a trend of increase in cor-
relation scores with respect to the increase in di-
mensionality up to a maximum of 0.564, reached
at 90% dimension.
</bodyText>
<subsectionHeader confidence="0.67344">
Percent Dimensions maintained
</subsectionHeader>
<figureCaption confidence="0.651500333333333">
Figure 1: Paragraph to Sentence - Pearson corre-
lation scores for four different experiments at dif-
ferent dimensions3 (represented in percent) of Uk
</figureCaption>
<bodyText confidence="0.773174">
Not convinced with the pearson scores, more
3Here, the dimension X% means k = (X/100) × N,
where N is the total number of columns in A in the unreduced
SVD.
</bodyText>
<footnote confidence="0.9579655">
4Stemmed using Porter Stemmer module availabe from
http://tartarus.org/∼martin/PorterStemmer/
</footnote>
<figure confidence="0.995920266666667">
0.7
0.4
0.35
0 20 40 60 80 100
0.65
Semantic similarity
0.6
0.55
0.5
0.45
Basic word−doc representation
Evaluation with Boolean values
Constrained Tokenization
Added data representation
x 4 (4)
</figure>
<page confidence="0.99477">
621
</page>
<bodyText confidence="0.999978785714286">
documents were added to the dataset to build a
new word-document matrix representation A. The
documents included all the paragraphs from the
training set. Each paragraph provided in the train-
ing set was added to the dataset as a separate docu-
ment. The experiment was performed maintaining
the settings from the previous experiment and the
results are shown in the fourth row of table 2. The
increase in trend of correlation scores with respect
to the increase in dimensionality is followed by the
new U produced from A after applying SVD. Fig-
ure 2 provides the distribution of similarity scores
evaluated at 90% dimension of the model with re-
spect to the gold standard.
Further to compare the performance of different
experiments, all the experiment results are plotted
in Figure 1. It can be observed that every subse-
quent model built has shown improvements in per-
formance. The first two experiments shown in the
first two rows of table 2 are shown in red and blue
lines in the figure. It can be observed that in both
the settings, the pearson correlation scores were
increasing as the the number of dimensions main-
tained also increased, whereas in the other two set-
tings, the pearson correlation scores reached their
maximum at 90% and came down at 100% di-
mension, which is unexpected and so is not jus-
tified. It is observed from Figure 2 that the scores
</bodyText>
<subsectionHeader confidence="0.762872">
Training data Examples
</subsectionHeader>
<figureCaption confidence="0.980291333333333">
Figure 2: Semantic similarity scores - Gold stan-
dard (Line plot) vs System scores (Scatter plot) for
examples in training data
</figureCaption>
<bodyText confidence="0.999633833333333">
of the system in scatter plot are not always clus-
tered around the gold standard scores, plotted as a
line. As the gold standard score goes up, the sys-
tem prediction accuracy has come down. One rea-
son for this pattern can be attributed to the train-
ing set which had data mostly data from Newswire
</bodyText>
<table confidence="0.995021444444444">
Dimensions 100% 90% 70% 50% 30% 10%
Basic word-doc 0.493 - - 0.435 0.423 0.366
representation
Evaluation 0.472 - - 0.449 0.430 0.363
boolean counts
Constrained 0.498 0.494 0.517 0.485 0.470 0.434
tokenization
Added 0.493 0.504 0.498 0.498 0.488 0.460
data
</table>
<tableCaption confidence="0.999791">
Table 3: Pearson scores at different dimensions3-
</tableCaption>
<note confidence="0.463075">
Sentence to Phrase
</note>
<bodyText confidence="0.9995786">
and webtext. Therefore, during evaluation all the
words from paragraph and/or sentence would not
have got a position while getting projected on the
latent semantic space, which we believe has pulled
down the accuracy.
</bodyText>
<subsectionHeader confidence="0.833348">
4.1.2 Sentence to Phrase
</subsectionHeader>
<bodyText confidence="0.999618666666667">
The experiments carried out for P2S provided in
4.1.1 were conducted for S2P examples as well.
The pearson scores produced by different experi-
ments at different dimensions are provided in ta-
ble 3. This table shows that the latest word-
document representation made with added docu-
ments, did not have any impact on the correlation
scores, while the earlier word-document represen-
tation provided in 3rd row, which used the original
dataset preprocessed with constrained tokeniza-
tion rules, removing unnecessary spaces and tabs,
and tokens stemmed, provided better correlation
score at 70% dimension. Further the comparison
of different experiments carried out at different
settings are plotted in Figure 3.
</bodyText>
<subsectionHeader confidence="0.4685">
Percent Dimensions maintained
</subsectionHeader>
<figureCaption confidence="0.944078">
Figure 3: Sentence to Phrase - Pearson correlation
scores for four different experiments at different
dimensions3 (represented in percentage) of Uk
</figureCaption>
<figure confidence="0.998735272727273">
4
3.5
3
1
0.5
0
0 100 200 300 400 500
2.5
2
Similarity scores
1.5
Semantic similarity
0.55
0.45
0.35
0 20 40 60 80 100
0.5
0.4
Basic word−doc representation
Evaluation with Boolean values
Constrained Tokenization
Added data representation
</figure>
<page confidence="0.959279">
622
</page>
<subsectionHeader confidence="0.993944">
4.2 Bag of Vectors
</subsectionHeader>
<bodyText confidence="0.999997777777778">
BV was tested in two different settings. The
first representation was created with bi-gram co-
occurance count as mentioned in section 3.2.1 and
experiments were carried out as mentioned in sec-
tion 3.2.2. This produced negative Pearson corre-
lation scores for P2S and S2P. Then we created an-
other representation by getting co-occurance count
in a window of 6 words in a sentence, on evalua-
tion produced correlation scores of 0.094 for P2S
and 0.145 for S2P. As BV showed strong negative
results, we did not continue using the method for
evaluating the test data. But we strongly believe
that the BV approach can produce better results if
we could compare the sentence to the paragraph
rather than the paragraph to the sentence as men-
tioned in section 3.2.2. During similarity calcula-
tion, when comparing sentence to the paragraph,
for each word in the sentence, we look for the best
semantic match from the paragraph, which would
increase the mean value by reducing the number of
divisions representing the number of words in the
sentence. In the current setting, it is believed that
while computing the similarity for the paragraph
to sentence, the words in the paragraph (longer
text) will consider a few words in the sentence to
be similar multiple times. This could not be right
when we compare the texts of varying lengths.
</bodyText>
<sectionHeader confidence="0.990184" genericHeader="conclusions">
5 Conclusion and Discussion
</sectionHeader>
<bodyText confidence="0.999977875">
On manual verification, it was identified that the
dataset used to build the representation did not
have documents related to the genres Metaphoric,
CQA and Travel. The original dataset mostly had
documents from Newswire text and blogs which
included reviews as well. Further, it can be identi-
fied from tables 2 and 3, the word-document rep-
resentation with added documents from the train-
ing set improved Pearson scores. This allowed to
assume that the dataset did not have completely
relevant set of documents to evaluate the training
set which included data from different genres. For
evaluation of the model on test data, we submitted
two runs and best of them reported Pearson score
of 0.607 and 0.552 on P2S and S2P respectively.
In the future work, we should be able to experi-
ment with more relevant data to build the model
using LSI and also use statistically strong unsu-
pervised classifier pLSI (Hofmann T, 2001) for the
same task. Further to this, as discussed in 4.2 we
would be able to experiment with the BV approach
by comparing the sentence to the paragraph, which
we believe will yield promising results to compare
the texts of varying lengths.
</bodyText>
<sectionHeader confidence="0.996454" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998608333333333">
Scott Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer and Richard Harshman
1990. Indexing by latent semantic analysis Jour-
nal of the American society for information science,
41(6):391–401
Thomas Hofmann 2001. Unsupervised Learning
by Probabilistic Latent Semantic Analysis Journal
Machine Learning, Volume 42 Issue 1-2, January-
February 2001 Pages 177 - 196
</reference>
<page confidence="0.99915">
623
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.696688">
<title confidence="0.9986545">TCDSCSS: Dimensionality Reduction to Evaluate Texts of Lengths an IR Approach</title>
<author confidence="0.998555">Arun Jayapal Martin Emms John D Kelleher</author>
<affiliation confidence="0.9977975">Dept of Computer Science Dept of Computer Science School of Computing Trinity College Dublin Trinity College Dublin Dublin Institute of Technology</affiliation>
<email confidence="0.71349">jayapala@cs.tcd.iemartin.emms@cs.tcd.iejohn.d.kelleher@dit.ie</email>
<abstract confidence="0.9987809375">This paper provides system description of the cross-level semantic similarity task for the SEMEVAL-2014 workshop. Crosslevel semantic similarity measures the degree of relatedness between texts of varylengths such as to Sento Se- Analysis used to evaluate the cross-level semantic relatedness between the texts to achieve above baseline scores, tested on the training and test datasets. We also tried using a bag-of-vectors approach to evaluate the semantic relatedness. This bag-of-vectors approach however did not produced encouraging results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Scott Deerwester</author>
<author>Susan T Dumais</author>
<author>George W Furnas</author>
<author>Thomas K Landauer</author>
<author>Richard Harshman</author>
</authors>
<title>Indexing by latent semantic analysis Journal of the American society for information science,</title>
<date>1990</date>
<pages>41--6</pages>
<contexts>
<context position="4431" citStr="Deerwester et al., 1990" startWordPosition="687" endWordPosition="690">f words identified in the corpus were approximately 40000. word counts, and sometimes a thresholded or ‘boolean’ version, mapping all non-zero numbers to 1. 3 System description We tried two different approaches for evaluating the P2S and S2P. Latent Semantic Analysis (LSA) using SVD worked better than the Bag-of-Vectors (BV) approach. The description of both the approaches are discussed in this section. 3.1 Latent Semantic Analysis LSA has been used for information retrieval allowing retrieval via vectors over latent, arguably conceptual, dimensions, rather than over surface word dimensions (Deerwester et al., 1990). It was thought this would be of advantage for comparison of texts of varying length. 3.1.1 Representation The data corpus was converted into a mxn termby-document matrix, A, where the counts (cm,n) of all terms (wm) in the corpus are represented in rows and the respective documents (dn) in columns: d1 d2 ··· dn ⎛w1 c1,1 c1,2 ··· c1,n w2c2,1 c2,2 · · · c2,n ⎜ ⎜ .⎜ ... . .⎝ . .. .... . . wm cm,1 cm,2 ··· cm,n The document indexing rules such as text tokenization, case standardization, stop words removal, token stemming, and special characters and punctuations removal were followed to get the m</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer and Richard Harshman 1990. Indexing by latent semantic analysis Journal of the American society for information science, 41(6):391–401</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
</authors>
<title>Unsupervised Learning by Probabilistic Latent Semantic Analysis</title>
<date>2001</date>
<booktitle>Journal Machine Learning, Volume 42 Issue 1-2, JanuaryFebruary</booktitle>
<pages>177--196</pages>
<marker>Hofmann, 2001</marker>
<rawString>Thomas Hofmann 2001. Unsupervised Learning by Probabilistic Latent Semantic Analysis Journal Machine Learning, Volume 42 Issue 1-2, JanuaryFebruary 2001 Pages 177 - 196</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>