<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003428">
<title confidence="0.983132">
Inside-Outside Estimation of a Lexicalized PCFG for German
</title>
<author confidence="0.962135">
Franz Beil, Glenn Carroll, Detlef Prescher, Stefan Riezler and Mats Rooth
</author>
<affiliation confidence="0.714154">
Institut fiir Maschinelle Sprachverarbeitung, University of Stuttgart
</affiliation>
<sectionHeader confidence="0.988296" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999935285714286">
The paper describes an extensive experiment in
inside-outside estimation of a lexicalized proba-
bilistic context free grammar for German verb-
final clauses. Grammar and formalism features
which make the experiment feasible are de-
scribed. Successive models are evaluated on pre-
cision and recall of phrase markup.
</bodyText>
<sectionHeader confidence="0.999645" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99967232">
Charniak (1995) and Carroll and Rooth (1998)
present head-lexicalized probabilistic context
free grammar formalisms, and show that they
can effectively be applied in inside-outside es-
timation of syntactic language models for En-
glish, the parameterization of which encodes
lexicalized rule probabilities and syntactically
conditioned word-word bigram collocates. The
present paper describes an experiment where a
slightly modified version of Carroll and Rooth&apos;s
model was applied in a systematic experiment
on German, which is a language with rich in-
flectional morphology and free word order (or
rather, compared to English, free-er phrase or-
der). We emphasize techniques which made it
practical to apply inside-outside estimation of
a lexicalized context free grammar to such a
language. These techniques relate to the treat-
ment of argument cancellation and scrambled
phrase order; to the treatment of case features in
category labels; to the category vocabulary for
nouns, articles, adjectives and their projections;
to lexicalization based on uninflected lemmata
rather than word forms; and to exploitation of
a parameter-tying feature.
</bodyText>
<sectionHeader confidence="0.896824" genericHeader="introduction">
2 Corpus and morphology
</sectionHeader>
<bodyText confidence="0.9999475">
The data for the experiment is a corpus of Ger-
man subordinate clauses extracted by regular
expression matching from a 200 million token
newspaper corpus. The clause length ranges be-
tween four and 12 words. Apart from infiniti-
val VPs as verbal arguments, there are no fur-
ther clausal embeddings, and the clauses do
not contain any punctuation except for a ter-
minal period. The corpus contains 4128873 to-
kens and 450526 clauses which yields an average
of 9.16456 tokens per clause. Tokens are auto-
matically annotated with a list of part-of-speech
(PoS) tags using a computational morpholog-
ical analyser based on finite-state technology
(Karttunen et al. (1994), Schiller and Stockert
(1995)).
A problem for practical inside-outside esti-
mation of an inflectional language like German
arises with the large number of terminal and
low-level non-terminal categories in the gram-
mar resulting from the morpho-syntactic fea-
tures of words. Apart from major class (noun,
adjective, and so forth) the analyser provides an
ambiguous word with a list of possible combina-
tions of inflectional features like gender, person,
number (cf. the top part of Fig. 1 for an exam-
ple ambiguous between nominal and adjectival
PoS; the PoS is indicated following the &apos;+&apos; sign).
In order to reduce the number of parameters
to be estimated, and to reduce the size of the
parse forest used in inside-outside estimation,
we collapsed the inflectional readings of adjec-
tives, adjective derived nouns, article words, and
pronouns to a single morphological feature (see
of Fig. 1 for an example). This reduced the num-
ber of low-level categories, as exemplified in Fig.
2: das has one reading as an article and one as
a demonstrative; westdeutschen has one reading
as an adjective, with its morphological feature
N indicating the inflectional suffix.
We use the special tag UNTAGGED indicating
that the analyser fails to provide a tag for the
word. The vast majority of UNTAGGED words are
proper names not recognized as such. These
gaps in the morphology have little effect on our
experiment.
</bodyText>
<sectionHeader confidence="0.998962" genericHeader="method">
3 Grammar
</sectionHeader>
<bodyText confidence="0.999599666666667">
The grammar is a manually developed headed
context-free phrase structure grammar for Ger-
man subordinate clauses with 5508 rules and
</bodyText>
<page confidence="0.995771">
269
</page>
<figure confidence="0.983998">
analyze&gt; Deutsche
1. deutsch-ADJ.Pos+NN.Fem.Akk.Sg
2. deutsch-ADJ.Pos+NN.Fem.Nom.Sg
3. deutsch-ADJ.Pos+NN.Masc.Nom.Sg.Sw
4. deutsch-ADJ.Pos+NN.Neut.Akk.Sg.Sw
5. deutsch-ADJ.Pos+NN.Neut.Nom.Sg.Sw
6. deutsch-ADJ.Pos+NN.NoGend.Akk.P1.St
7. deutsch-ADJ.Pos+NN.NoGend.Nom.P1.St
8. *deutsch+ADJ.Pos.Fem.Akk.Sg
9. *deutsch+ADJ.Pos.Fem.Nom.Sg
10. *deutsch+ADJ.Pos.Masc.Nom.Sg.Sw
11. *deutsch+ADJ.Pos.Neut.Akk.Sg.Sw
12. *deutsch+ADJ.Pos.Neut.Nom.Sg.Sw
13. *deutsch+ADJ.Pos.NoGend.Akk.P1.St
14. *deutsch+ADJ.Pos.NoGend.Nom.P1.St
==&gt; Deutsche f ADJ.E, NNADJ.E 1
</figure>
<figureCaption confidence="0.991794">
Figure 1: Collapsing Inflectional Features
</figureCaption>
<figure confidence="0.726014777777778">
w&amp;hrend f ADJ.Adv, ADJ.Pred, KOUS,
APPR.Dat, APPR.Gen 1
sich f PRF.Z
das f DEMS.Z, ART.Def.Z 1
Preisniveau f NN.Neut.NotGen.Sg
dem f DEMS.M, ART.Def.M 1
westdeutschen f ADJ.N 1
annahere f VVFIN
. f PER 1
</figure>
<figureCaption confidence="0.999338">
Figure 2: Corpus Clip
</figureCaption>
<bodyText confidence="0.992413121951219">
562 categories, 209 of which are terminal cat-
egories. The formalism is that of Carroll and
Rooth (1998), henceforth C+R:
mother -&gt; non-heads head&apos; non-heads (freq)
The rules are head marked with a prime. The
non-head sequences may be empty. f req is a
rule frequency, which is initialized randomly and
subsequently estimated by the inside outside-
algorithm. To handle systematic patterns re-
lated to features, rules were generated by Lisp
functions, rather than being written directly in
the above form. With very few exceptions (rules
for coordination, S-rule), the rules do not have
more than two daughters.
Grammar development is facilitated by a
chart browser that permits a quick and efficient
discovery of grammar bugs (Carroll, 1997a). Fig.
3 shows that the ambiguity in the chart is quite
considerable even though grammar and corpus
are restricted. For the entire corpus, we com-
puted an average 9202 trees per clause. In the
chart browser, the categories filling the cells in-
dicate the most probable category for that span
with their estimated frequencies. The pop-up
window under IP presents the ranked list of all
possible categories for the covered span. Rules
(chart edges) with frequencies can be viewed
with a further menu. In the chart browser, colors
are used to display frequencies (between 0 and 1)
estimated by the inside-outside algorithm. This
allows properties shared across tree analyses to
be checked at a glance; often grammar and es-
timation bugs can be detected without mouse
operations.
The grammar covers 88.5% of the clauses and
87.9% of the tokens contained in the corpus.
Parsing failures are mainly due to UNTAGGED
words contained in 6.6% of the failed clauses,
the pollution of the corpus by infinitival con-
structions (P-J1.3%), and a number of coordina-
tions not covered by the grammar (P--1.6%).
</bodyText>
<subsectionHeader confidence="0.999838">
3.1 Case features and agreement
</subsectionHeader>
<bodyText confidence="0.999524481481482">
On nominal categories, in addition to the four
cases Nom, Gen, Dat, and Akk, case features
with a disjunctive interpretation (such as Dir
for Nom or Akk) are used. The grammar is writ-
ten in such a way that non-disjunctive features
are introduced high up in the tree. This results
in some reduction in the size of the parse forest,
and some parameter pooling. Essentially the full
range of agreement inside the noun phrase is en-
forced. Agreement between the nominative NP
and the tensed verb (e.g. in number) is not en-
forced by the grammar, in order to control the
number of parameters and rules.
For noun phrases we employ Abney&apos;s chunk
grammar organization (Abney, 1996). The noun
chunk (NC) is an approximately non-recursive
projection that excludes post-head complements
and (adverbial) adjuncts introduced higher than
pre-head modifiers and determiners but in-
cludes participial pre-modifiers with their com-
plements. Since we perform complete context
free parsing, parse forest construction, and
inside-outside estimation, chunks are not moti-
vated by deterministic parsing. Rather, they fa-
cilitate evaluation and graphical debugging, by
tending to increase the span of constituents with
high estimated frequency.
</bodyText>
<page confidence="0.997025">
270
</page>
<figureCaption confidence="0.929508">
Figure 3: Chart browser
Word-by-word gloss of the clause: &apos;that Sarajevo over the airport with the essentials supplied will can&apos;
</figureCaption>
<figure confidence="0.988748666666667">
clan
1E 1 JOGE000 IF
0.661299 VPP.rap.np
0.125495 VPP.ri
0.104106 11F.K.n
0 .007692 VPP.dp.dp
0 020743 V PP.d
0 003556 V Plosid.nd
e.0110029 VPP
Saraiewe •
Veer
den
Flighafen
den%
NOtIlaten .
%realign
Werner&apos;.
Van •
class #
VPA 15
VPP 13
VPI 10
VPK 2
frame types
</figure>
<construct confidence="0.622949333333333">
n, na, nad, nai, nap, nar, nd, ndi,
ndp, ndr, ni, nir, np, npr, nr
d, di, dp, dr, i, ir, ii, nd, ni, np, p,
pr, r
a, ad, ap, ar, d, dp, dr, p, pr, r
i, n
</construct>
<figure confidence="0.994831666666667">
VPA.na.na
VPA.na.na
NP.Nom VPA.na.a
NP.Akk VPA.na.n
NP.Akk VPA.na
NP.Nom VPA.na
</figure>
<figureCaption confidence="0.8267535">
Figure 4: Number and types of verb frames
3.2 Subcategorisation frames of verbs
</figureCaption>
<bodyText confidence="0.995734380952381">
The grammar distinguishes four subcategorisa-
tion frame classes: active (VPA), passive (VPP),
infinitival (VPI) frames, and copula construc-
tions (VPK). A frame may have maximally three
arguments. Possible arguments in the frames are
nominative (n), dative (d) and accusative (a)
NPs, reflexive pronouns (r), PPs (p), and infini-
tival VPs (i). The grammar does not distinguish
plain infinitival VPs from zu-infinitival VPs. The
grammar is designed to partially distinguish dif-
ferent PP frames relative to the prepositional
head of the PP. A distinct category for the spe-
cific preposition becomes visible only when a
subcategorized preposition is cancelled from the
subcat list. This means that specific prepositions
do not figure in the evaluation discussed below.
The number and the types of frames in the dif-
ferent frame classes are given in figure 4.
German, being a language with comparatively
free phrase order, allows for scrambling of ar-
guments. Scrambling is reflected in the particu-
</bodyText>
<figureCaption confidence="0.581848071428571">
Figure 5: Coding of canonical and scrambled ar-
gument order
lar sequence in which the arguments of the verb
frame are saturated. Compare figure 5 for an ex-
ample of a canonical subject-object order in an
active transitive frame and its scrambled object-
subject order. The possibility of scrambling verb
arguments yields a substantial increase in the
number of rules in the grammar (e.g. 102 com-
binatorically possible argument rules for all in
VPA frames). Adverbs and non-subcategorized
PPs are introduced as adjuncts to VP categories
which do not saturate positions in the subcat
frame.
</figureCaption>
<bodyText confidence="0.9994626">
In earlier experiments, we employed a flat
clausal structure, with rules for all permutations
of complements. As the number of frames in-
creased, this produced prohibitively many rules,
particularly with the inclusion of adjuncts.
</bodyText>
<sectionHeader confidence="0.935215" genericHeader="method">
4 Parameters
</sectionHeader>
<bodyText confidence="0.996328666666667">
The parameterization is as in C+R, with one
significant modification. Parameters consist of
(i) rule parameters, corresponding to right hand
</bodyText>
<page confidence="0.983205">
271
</page>
<bodyText confidence="0.999988387096774">
sides conditioned by parent category and par-
ent head; (ii) lexical choice parameters for non-
head children, corresponding to child lemma
conditioned by child category, parent category,
and parent head lemma. See C+R or Charniak
(1995) for an explanation of how such parame-
ters define a probabilistic weighting of trees. The
change relative to C+R is that lexicalization is
by uninflected lemma rather than word form.
This reduces the number of lexical parameters,
giving more acceptable model sizes and elimi-
nating splitting of estimated frequencies among
inflectional forms. Inflected forms are generated
at the leaves of the tree, conditioned on termi-
nal category and lemma. This results in a third
family of parameters, though usually the choice
of inflected form is deterministic.
A parameter pooling feature is used for argu-
ment filling where all parent categories of the
form VP.x.y are mapped to a category VP.x in
defining lexical choice parameters. The conse-
quence is e.g. that an accusative daughter of a
nominative-accusative verb uses the same lexical
choice parameter, whether a default or scram-
bled word order is used. (This feature was used
by C+R for their phrase trigram grammar, not
in the linguistic part of their grammar.) Not all
desirable parameter pooling can be expressed in
this way, though; for instance rule parameters
are not pooled, and so get split when the parent
category bears an inflectional feature.
</bodyText>
<sectionHeader confidence="0.99906" genericHeader="method">
5 Estimation
</sectionHeader>
<bodyText confidence="0.999784055555555">
The training of our probabilistic CFG proceeds
in three steps: (i) unlexicalized training with
the supar parser, (ii) bootstrapping a lexical-
ized model from the trained unlexicalized one
with the ultra parser, and finally (iii) lexical-
ized training with the hypar parser (Carroll,
1997b). Each of the three parsers uses the inside-
outside algorithm. supar and ultra use an un-
lexicalized weighting of trees, while hypar uses a
lexicalized weighting of trees. ultra and hypar
both collect frequencies for lexicalized rule and
lexical choice events, while supar collects only
unlexicalized rule frequencies.
Our experiments have shown that training an
unlexicalized model first is worth the effort. De-
spite our use of a manually developed grammar
that does not have to be pruned of superfluous
rules like an automatically generated grammar,
</bodyText>
<table confidence="0.998118166666667">
A
1: 52.0199 1: 53.7654 1: 49.8165
2: 25.3652 2: 26.3184 23.1008
24.5905 3: 25.5035 3: 22.4479
. . . . . .
. . . . . .
. . . . . .
13: 24.2872 55: 25.0548 70: 22.1445
14: 24.2863 56: 25.0549 80: 22.1443
15: 24.2861 57: 25.0549 90: 22.1443
16: 24.2861 58: 25.0549 95: 22.1443
17: 24.2867 59: 25.055 96: 22.1444
</table>
<figureCaption confidence="0.8669915">
Figure 6: Overtraining (iteration: cross-entropy
on heldout data)
</figureCaption>
<bodyText confidence="0.998823615384615">
the lexicalized model is notably better when
preceded by unlexicalized training (see also Er-
san and Charniak (1995) for related observa-
tions). A comparison of immediate lexicalized
training (without prior training of an unlexical-
ized model) and our standard training regime
that involves preliminary unlexicalized training
speaks in favor of our strategy (cf. the differ-
ent &apos;lex 0&apos; and &apos;lex 2&apos; curves in figures 8 and 9).
However, the amount of unlexicalized training
has to be controlled in some way.
A standard criterion to measure overtraining
is to compare log-likelihood values on held-out
data of subsequent iterations. While the log-
likelihood value of the training data is theo-
retically guaranteed to converge through sub-
sequent iterations, a decreasing log-likelihood
value of the held-out data indicates over-
training. Instead of log-likelihood, we use the
inversely proportional cross-entropy measure.
Fig. 6 shows comparisons of different sizes of
training and heldout data (training/heldout):
(A) 50k/50k, (B) 500k/500k, (C) 4.1M/500k.
The overtraining effect is indicated by the in-
crease in cross-entropy from the penultimate to
the ultimate iteration in the tables. Overtraining
results for lexicalized models are not yet avail-
able.
However, a comparison of precision/recall
measures on categories of different complexity
through iterative unlexicalized training shows
that the mathematical criterion for overtraining
may lead to bad results from a linguistic point
of view. While we observed more or less con-
verging precision/recall measures for lower level
structures such as noun chunks, iterative unlexi-
calized training up to the overtraining threshold
turned out to be disastrous for the evaluation of
complex categories that depend on almost the
</bodyText>
<page confidence="0.927512">
272
</page>
<equation confidence="0.88616825">
O 0 0 0 0 0
O 0 0 0 0
O 0 0 0
O 0 0
</equation>
<figure confidence="0.981816428571429">
O0
000000000
00000000
0000000
dal)
0
er
slat
in
seiner
Haut
wehren
0
munO
0
O0
O0
O0
O 0
O0
O0
</figure>
<figureCaption confidence="0.999982">
Figure 7: Chart browser for manual NC labelling
</figureCaption>
<bodyText confidence="0.9999215">
entire span of the clause. The recognition of sub-
categorization frames through 60 iterations of
unlexicalized training shows a massive decrease
in precision/recall from the best to the last iter-
ation, even dropping below the results with the
randomly initialized grammar (see Fig. 9).
</bodyText>
<subsectionHeader confidence="0.998104">
5.1 Training regime
</subsectionHeader>
<bodyText confidence="0.999809857142857">
We compared lexicalized training with respect
to different starting points: a random unlexi-
calized model, the trained unlexicalized model
with the best precision/recall results, and an un-
lexicalized model that comes close to the cross-
entropy overtraining threshold. The details of
the training steps are as follows:
</bodyText>
<listItem confidence="0.989617">
(1) 0, 2 and 60 iterations of unlexicalized pars-
ing with supar;
(2) lexicalization with ultra using the entire
corpus;
(3) 23 iterations of lexicalized parsing with
hypar.
</listItem>
<bodyText confidence="0.999921">
The training was done on four machines (two
167 MHz UltraSPARC and two 296 MHz SUNW
UltraSPARC-II). Using the grammar described
here, one iteration of supar on the entire corpus
takes about 2.5 hours, lexicalization and gen-
erating an initial lexicalized model takes more
than six hours, and an iteration of lexicalized
parsing can be done in 5.5 hours.
</bodyText>
<sectionHeader confidence="0.998049" genericHeader="evaluation">
6 Evaluation
</sectionHeader>
<bodyText confidence="0.9999834375">
For the evaluation, a total of 600 randomly se-
lected clauses were manually annotated by two
labellers. Using a chart browser, the labellers
filled the appropriate cells with category names
of NCs and those of maximal VP projections
(cf. Figure 7 for an example of NC-labelling).
Subsequent alignment of the labellers decisions
resulted in a total of 1353 labelled NC categories
(with four different cases). The total of 584 la-
belled VP categories subdivides into 21 differ-
ent verb frames with 340 different lemma heads.
The dominant frames are active transitive (164
occurrences) and active intransitive (117 occur-
rences). They represent almost half of the an-
notated frames. Thirteen frames occur less than
ten times, five of which just once.
</bodyText>
<subsectionHeader confidence="0.974418">
6.1 Methodology
</subsectionHeader>
<bodyText confidence="0.9999835">
To evaluate iterative training, we extracted
maximum probability (Viterbi) trees for the 600
clause test set in each iteration of parsing. For
extraction of a maximal probability parse in
unlexicalized training, we used Schmid&apos;s lopar
parser (Schmid, 1999). Trees were mapped to
a database of parser generated markup guesses,
and we measured precision and recall against
the manually annotated category names and
spans. Precision gives the ratio of correct guesses
over all guesses, and recall the ratio of correct
guesses over the number of phrases identified by
human annotators. Here, we render only the pre-
cision/recall results on pairs of category names
and spans, neglecting less interesting measures
on spans alone. For the figures of adjusted re-
call, the number of unparsed misses has been
subtracted from the number of possibilities.
</bodyText>
<page confidence="0.995106">
273
</page>
<figure confidence="0.998302957446809">
precision/recall
-/-----------
-------------------- ------ -------------------
precision lex 02
precision unlex
precision lex 00
precision lex 60
recall lex 02
recall unlex -
recall lex 00
recall lex 60
0.72
0.7
0.68
0.66
0.64
0.62
0.6
0.58
0.56
0.54
0.52
0
precision lex 02 ----
precision unlex
precision lex 00
precision lex 60 --
90
BO 90
10 20
0.88
0.86
0.84
0.82
0.8
0.78
0.76
0.74
0.72
0.7
0.68
0
10 20 30 40 50 60 70
iteration
30 40 50 60 70
iteration *
80
</figure>
<figureCaption confidence="0.999907">
Figure 8: Precision/recall measures on NC cases Figure 9: Precision measures on all verb frames
</figureCaption>
<bodyText confidence="0.999838333333333">
In the following, we focus on the combination
of the best unlexicalized model and the lexical-
ized model that is grounded on the former.
</bodyText>
<subsectionHeader confidence="0.997937">
6.2 NC Evaluation
</subsectionHeader>
<bodyText confidence="0.999859534883721">
Figure 8 plots precision/recall for the training
runs described in section 5.1, with lexicalized
parsing starting after 0, 2, or 60 unlexicalized it-
erations. The best results are achieved by start-
ing with lexicalized training after two iterations
of unlexicalized training. Of a total of 1353 an-
notated NCs with case, 1103 are correctly recog-
nized in the best unlexicalized model and 1112
in the last lexicalized model. With a number
of 1295 guesses in the unlexicalized and 1288
guesses in the final lexicalized model, we gain
1.2% in precision (85.1% vs. 86.3%) and 0.6%
in recall (81.5% vs. 82.1%) through lexicalized
training. Adjustment to parsed clauses yields
88% vs. 89.2% in recall. As shown in Figure 8,
the gain is achieved already within the first it-
eration; it is equally distributed between correc-
tions of category boundaries and labels.
The comparatively small gain with lexical-
ized training could be viewed as evidence that
the chunking task is too simple for lexical infor-
mation to make a difference. However, we find
about 7% revised guesses from the unlexicalized
to the first lexicalized model. Currently, we do
not have a clear picture of the newly introduced
errors.
The plots labeled &amp;quot;00&amp;quot; are results for lexi-
calized training starting from a random initial
grammar. The precision measure of the first lex-
icalized model falls below that of the unlexi-
calized random model (74%), only recovering
through lexicalized training to equalize the pre-
cision measure of the random model (75.6%).
This indicates that some degree of unlexicalized
initialization is necessary, if a good lexicalized
model is to be obtained.
(Skut and Brants, 1998) report 84.4% recall
and 84.2% for NP and PP chunking without case
labels. While these are numbers for a simpler
problem and are slightly below ours, they are
figures for an experiment on unrestricted sen-
tences. A genuine comparison has to await ex-
tension of our model to free text.
</bodyText>
<subsectionHeader confidence="0.999458">
6.3 Verb Frame Evaluation
</subsectionHeader>
<bodyText confidence="0.999121076923077">
Figure 9 gives results for verb frame recogni-
tion under the same training conditions. Again,
we achieve best results by lexicalising the sec-
ond unlexicalized model. Of a total of 584 anno-
tated verb frames, 384 are correctly recognized
in the best unlexicalized model and 397 through
subsequent lexicalized training. Precision for the
best unlexicalized model is 68.4%. This is raised
by 2% to 70.4% through lexicalized training; re-
call is 65.7%/68%; adjustment by 41 unparsed
misses makes for 70.4%/72.8% in recall. The
rather small improvements are in contrast to
88 differences in parser markup, i.e. 15.7%, be-
tween the unlexicalized and second lexicalized
model. The main gain is observed within the
first two iterations (cf. Figure 9; for readability,
we dropped the recall curves when more or less
parallel to the precision curves).
Results for lexicalized training without prior
unlexicalized training are better than in the NC
evaluation, but fall short of our best results by
more than 2%.
The most notable observation in verb frame
evaluation is the decrease of precision of frame
recognition in unlexicalized training from the
second iteration onward. After several dozen it-
</bodyText>
<page confidence="0.991458">
274
</page>
<figure confidence="0.998786">
0.8
0.75
. 0.7
2 0.65
0.6
0.55
0.5
0 10 20 30 40 50 60 70 80 90
iteration #
</figure>
<figureCaption confidence="0.99791">
Figure 10: Precision measures on non-PP frames
</figureCaption>
<bodyText confidence="0.999984375">
erations, results are 5% below a random model
and 14% below the best model. The primary
reason for the decrease is the mistaken revi-
sion of adjoined PPs to argument PPs. E.g.
the required number of 164 transitive frames
is missed by 76, while the parser guesses 64
VPA.nap frames in the final iteration against
the annotator&apos;s baseline of 12. In contrast, lexi-
calized training generally stabilizes w.r.t. frame
recognition results after only few iterations.
The plot labeled &amp;quot;lex 60&amp;quot; gives precision for a
lexicalized training starting from the unlexical-
ized model obtained with 60 iterations, which
measured by linguistic criteria is a very poor
state. As far as we know, lexicalized EM esti-
mation never recovers from this bad state.
</bodyText>
<subsectionHeader confidence="0.995463">
6.4 Evaluation of non-PP Frames
</subsectionHeader>
<bodyText confidence="0.999969">
Because examination of individual cases showed
that PP attachments are responsible for many
errors, we did a separate evaluation of non-PP
frames. We filtered out all frames labelled with
a PP argument from both the maximal proba-
bility parses and the manually annotated frames
(91 filtered frames), measuring precision and re-
call against the remaining 493 labeller anno-
tated non-PP frames.
For the best lexicalized model, we find some-
what but not excessively better results than
those of the evaluation of the entire set of
frames. Of 527 guessed frames in parser markup,
382 are correct, i.e. a precision of 72.5%. The
recall figure of 77.5% is considerably better
since overgeneration of 34 guesses is neglected.
The differences with respect to different start-
ing points for lexicalization emulate those in the
evaluation of all frames.
The rather spectacular looking precision and
recall differences in unlexicalized training con-
firm what was observed for the full frame
set. From the first trained unlexicalized model
throughout unlexicalized training, we find a
steady increase in precision (70% first trained
model to 78% final model) against a sharp drop
in recall (78% peek in the second model vs.
50% in the final). Considering our above re-
marks on the difficulties of frame recognition
in unlexicalized training, the sharp drop in re-
call is to be expected: Since recall measures the
correct parser guesses against the annotator&apos;s
baseline, the tendency to favor PP arguments
over PP-adjuncts leads to a loss in guesses when
PP-frames are abandoned. Similarly, the rise in
precision is mainly explained by the decreas-
ing number of guesses when cutting out non-PP
frames. For further discussion of what happens
with individual frames, we refer the reader to
(Beil et al., 1998).
One systematic result in these plots is that
performance of lexicalized training stabilizes af-
ter a few iterations. This is consistent with
what happens with rule parameters for individ-
ual verbs, which are close to their final values
within five iterations.
</bodyText>
<sectionHeader confidence="0.999172" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999745916666667">
Our principal result is that scrambling-style
free-er phrase order, case morphology and sub-
categorization, and NP-internal gender, num-
ber and case agreement can be dealt with in
a head-lexicalized PFCG formalism by means
of carefully designed categories and rules which
limit the size of the packed parse forest and give
desirable pooling of parameters. Hedging this,
we point out that we made compromises in the
grammar (notably, in not enforcing nominative-
verb agreement) in order to control the number
of categories, rules, and parameters.
A second result is that iterative lexicalized
inside-outside estimation appears to be bene-
ficial, although the precision/recall increments
are small. We believe this is the first substan-
tial investigation of the utility of iterative lexi-
calized inside-outside estimation of a lexicalized
probabilistic grammar involving a carefully built
grammar where parses can be evaluated by lin-
guistic criteria.
A third result is that using too many unlexi-
calized iterations (more than two) is detrimen-
tal. A criterion using cross-entropy overtraining
</bodyText>
<figure confidence="0.660621125">
r &apos;
-
V...-
precision lex 02 ---
precision unlex
precision lex 00
precision lex 60 -
recall unlex
</figure>
<page confidence="0.995976">
275
</page>
<bodyText confidence="0.999987714285714">
on held-out data dictates many more unlexical-
ized iterations, and this criterion is therefore in-
appropriate.
Finally, we have clear cases of lexicalized
EM estimation being stuck in linguistically bad
states. As far as we know, the model which gave
the best results could also be stuck in a compar-
atively bad state. We plan to experiment with
other lexicalized training regimes, such as ones
which alternate between different training cor-
pora.
The experiments are made possible by im-
provements in parser and hardware speeds, the
carefully built grammar, and evaluation tools.
In combination, these provide a unique environ-
ment for investigating training regimes for lexi-
calized PCFGs. Much work remains to be done
in this area, and we feel that we are just begin-
ning to develop understanding of the time course
of parameter estimation, and of the general effi-
cacy of EM estimation of lexicalized PCFGs as
evaluated by linguistic criteria.
We believe our current grammar of Ger-
man could be extended to a robust free-text
chunk/phrase grammar in the style of the En-
glish grammar of Carroll and Rooth (1998)
with about a month&apos;s work, and to a free-text
grammar treating verb-second clauses and addi-
tional complementation structures (notably ex-
traposed clausal complements) with about one
year of additional grammar development and
experiment. These increments in the grammar
could easily double the number of rules. How-
ever this would probably not pose a problem for
the parsing and estimation software.
</bodyText>
<sectionHeader confidence="0.998817" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998417078947369">
Steven Abney. 1996. Chunk stylebook. Techni-
cal report, SfS, Universitat Tubingen.
Franz Beil, Glenn Carroll, Detlef Prescher, Ste-
fan Riezler, and Mats Rooth. 1998. Inside-
outside estimation of a lexicalized PCFG for
German. —Gold—. In Inducing Lexicons with
the EM Algorithm. AIMS Report 4(3), IMS,
Universitat Stuttgart.
Glenn Carroll and Mats Rooth. 1998. Valence
induction with a head-lexicalized PCFG. In
Proceedings of EMNLP-3, Granada.
Glenn Carroll, 1997a. Manual pages for charge,
hyparCharge, and tau. IMS, Universitat
Stuttgart.
Glenn Carroll, 1997b. Manual pages for supar,
ultra, hypar, and genDists. IMS, Stuttgart
University.
E. Charniak. 1995. Parsing with context-free
grammars and word statistics. Technical Re-
port CS-95-28, Department of Computer Sci-
ence, Brown University.
M. Ersan and E. Charniak. 1995. A statistical
syntactic disambiguation program and what
it learns. Technical Report CS-95-29, Depart-
ment of Computer Science, Brown University.
Lauri Karttunen, Todd Yampol, and Gregory
Grefenstette, 1994. INFL Morphological An-
alyzer/Generator 3.2.9 (3.6.4). Xerox Corpo-
ration.
Anne Schiller and Chris Stockert, 1995. DMOR.
IMS, Universitat Stuttgart.
Helmut Schmid, 1999. Manual page for lopar.
IMS, Universitat Stuttgart.
Wojciech Skut and Thorsten Brants. 1998.
A maximum-entropy partial parser for un-
restricted text. In Proceedings of the Sixth
Workshop on Very Large Corpora, Montréal,
Québec.
</reference>
<page confidence="0.998425">
276
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.884317">
<title confidence="0.999455">Inside-Outside Estimation of a Lexicalized PCFG for German</title>
<author confidence="0.99938">Franz Beil</author>
<author confidence="0.99938">Glenn Carroll</author>
<author confidence="0.99938">Detlef Prescher</author>
<author confidence="0.99938">Stefan Riezler</author>
<author confidence="0.99938">Mats Rooth</author>
<affiliation confidence="0.990169">Institut fiir Maschinelle Sprachverarbeitung, University of Stuttgart</affiliation>
<abstract confidence="0.986642375">The paper describes an extensive experiment in inside-outside estimation of a lexicalized probabilistic context free grammar for German verbfinal clauses. Grammar and formalism features which make the experiment feasible are described. Successive models are evaluated on precision and recall of phrase markup.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Abney</author>
</authors>
<title>Chunk stylebook.</title>
<date>1996</date>
<tech>Technical report, SfS, Universitat Tubingen.</tech>
<contexts>
<context position="7254" citStr="Abney, 1996" startWordPosition="1101" endWordPosition="1102">en, Dat, and Akk, case features with a disjunctive interpretation (such as Dir for Nom or Akk) are used. The grammar is written in such a way that non-disjunctive features are introduced high up in the tree. This results in some reduction in the size of the parse forest, and some parameter pooling. Essentially the full range of agreement inside the noun phrase is enforced. Agreement between the nominative NP and the tensed verb (e.g. in number) is not enforced by the grammar, in order to control the number of parameters and rules. For noun phrases we employ Abney&apos;s chunk grammar organization (Abney, 1996). The noun chunk (NC) is an approximately non-recursive projection that excludes post-head complements and (adverbial) adjuncts introduced higher than pre-head modifiers and determiners but includes participial pre-modifiers with their complements. Since we perform complete context free parsing, parse forest construction, and inside-outside estimation, chunks are not motivated by deterministic parsing. Rather, they facilitate evaluation and graphical debugging, by tending to increase the span of constituents with high estimated frequency. 270 Figure 3: Chart browser Word-by-word gloss of the c</context>
</contexts>
<marker>Abney, 1996</marker>
<rawString>Steven Abney. 1996. Chunk stylebook. Technical report, SfS, Universitat Tubingen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Beil</author>
<author>Glenn Carroll</author>
<author>Detlef Prescher</author>
<author>Stefan Riezler</author>
<author>Mats Rooth</author>
</authors>
<title>Insideoutside estimation of a lexicalized PCFG for German. —Gold—. In Inducing Lexicons with the EM Algorithm.</title>
<date>1998</date>
<tech>AIMS Report 4(3), IMS,</tech>
<institution>Universitat Stuttgart.</institution>
<contexts>
<context position="24385" citStr="Beil et al., 1998" startWordPosition="3877" endWordPosition="3880">call (78% peek in the second model vs. 50% in the final). Considering our above remarks on the difficulties of frame recognition in unlexicalized training, the sharp drop in recall is to be expected: Since recall measures the correct parser guesses against the annotator&apos;s baseline, the tendency to favor PP arguments over PP-adjuncts leads to a loss in guesses when PP-frames are abandoned. Similarly, the rise in precision is mainly explained by the decreasing number of guesses when cutting out non-PP frames. For further discussion of what happens with individual frames, we refer the reader to (Beil et al., 1998). One systematic result in these plots is that performance of lexicalized training stabilizes after a few iterations. This is consistent with what happens with rule parameters for individual verbs, which are close to their final values within five iterations. 7 Conclusion Our principal result is that scrambling-style free-er phrase order, case morphology and subcategorization, and NP-internal gender, number and case agreement can be dealt with in a head-lexicalized PFCG formalism by means of carefully designed categories and rules which limit the size of the packed parse forest and give desira</context>
</contexts>
<marker>Beil, Carroll, Prescher, Riezler, Rooth, 1998</marker>
<rawString>Franz Beil, Glenn Carroll, Detlef Prescher, Stefan Riezler, and Mats Rooth. 1998. Insideoutside estimation of a lexicalized PCFG for German. —Gold—. In Inducing Lexicons with the EM Algorithm. AIMS Report 4(3), IMS, Universitat Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Glenn Carroll</author>
<author>Mats Rooth</author>
</authors>
<title>Valence induction with a head-lexicalized PCFG.</title>
<date>1998</date>
<booktitle>In Proceedings of EMNLP-3,</booktitle>
<location>Granada.</location>
<contexts>
<context position="4826" citStr="Carroll and Rooth (1998)" startWordPosition="697" endWordPosition="700">k.Sg 9. *deutsch+ADJ.Pos.Fem.Nom.Sg 10. *deutsch+ADJ.Pos.Masc.Nom.Sg.Sw 11. *deutsch+ADJ.Pos.Neut.Akk.Sg.Sw 12. *deutsch+ADJ.Pos.Neut.Nom.Sg.Sw 13. *deutsch+ADJ.Pos.NoGend.Akk.P1.St 14. *deutsch+ADJ.Pos.NoGend.Nom.P1.St ==&gt; Deutsche f ADJ.E, NNADJ.E 1 Figure 1: Collapsing Inflectional Features w&amp;hrend f ADJ.Adv, ADJ.Pred, KOUS, APPR.Dat, APPR.Gen 1 sich f PRF.Z das f DEMS.Z, ART.Def.Z 1 Preisniveau f NN.Neut.NotGen.Sg dem f DEMS.M, ART.Def.M 1 westdeutschen f ADJ.N 1 annahere f VVFIN . f PER 1 Figure 2: Corpus Clip 562 categories, 209 of which are terminal categories. The formalism is that of Carroll and Rooth (1998), henceforth C+R: mother -&gt; non-heads head&apos; non-heads (freq) The rules are head marked with a prime. The non-head sequences may be empty. f req is a rule frequency, which is initialized randomly and subsequently estimated by the inside outsidealgorithm. To handle systematic patterns related to features, rules were generated by Lisp functions, rather than being written directly in the above form. With very few exceptions (rules for coordination, S-rule), the rules do not have more than two daughters. Grammar development is facilitated by a chart browser that permits a quick and efficient discov</context>
</contexts>
<marker>Carroll, Rooth, 1998</marker>
<rawString>Glenn Carroll and Mats Rooth. 1998. Valence induction with a head-lexicalized PCFG. In Proceedings of EMNLP-3, Granada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Glenn Carroll</author>
</authors>
<title>Manual pages for charge, hyparCharge, and tau. IMS,</title>
<date>1997</date>
<institution>Universitat Stuttgart.</institution>
<contexts>
<context position="5460" citStr="Carroll, 1997" startWordPosition="799" endWordPosition="800">her -&gt; non-heads head&apos; non-heads (freq) The rules are head marked with a prime. The non-head sequences may be empty. f req is a rule frequency, which is initialized randomly and subsequently estimated by the inside outsidealgorithm. To handle systematic patterns related to features, rules were generated by Lisp functions, rather than being written directly in the above form. With very few exceptions (rules for coordination, S-rule), the rules do not have more than two daughters. Grammar development is facilitated by a chart browser that permits a quick and efficient discovery of grammar bugs (Carroll, 1997a). Fig. 3 shows that the ambiguity in the chart is quite considerable even though grammar and corpus are restricted. For the entire corpus, we computed an average 9202 trees per clause. In the chart browser, the categories filling the cells indicate the most probable category for that span with their estimated frequencies. The pop-up window under IP presents the ranked list of all possible categories for the covered span. Rules (chart edges) with frequencies can be viewed with a further menu. In the chart browser, colors are used to display frequencies (between 0 and 1) estimated by the insid</context>
<context position="12202" citStr="Carroll, 1997" startWordPosition="1888" endWordPosition="1889">der is used. (This feature was used by C+R for their phrase trigram grammar, not in the linguistic part of their grammar.) Not all desirable parameter pooling can be expressed in this way, though; for instance rule parameters are not pooled, and so get split when the parent category bears an inflectional feature. 5 Estimation The training of our probabilistic CFG proceeds in three steps: (i) unlexicalized training with the supar parser, (ii) bootstrapping a lexicalized model from the trained unlexicalized one with the ultra parser, and finally (iii) lexicalized training with the hypar parser (Carroll, 1997b). Each of the three parsers uses the insideoutside algorithm. supar and ultra use an unlexicalized weighting of trees, while hypar uses a lexicalized weighting of trees. ultra and hypar both collect frequencies for lexicalized rule and lexical choice events, while supar collects only unlexicalized rule frequencies. Our experiments have shown that training an unlexicalized model first is worth the effort. Despite our use of a manually developed grammar that does not have to be pruned of superfluous rules like an automatically generated grammar, A 1: 52.0199 1: 53.7654 1: 49.8165 2: 25.3652 2:</context>
</contexts>
<marker>Carroll, 1997</marker>
<rawString>Glenn Carroll, 1997a. Manual pages for charge, hyparCharge, and tau. IMS, Universitat Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Glenn Carroll</author>
</authors>
<title>Manual pages for supar, ultra, hypar, and genDists. IMS,</title>
<date>1997</date>
<institution>Stuttgart University.</institution>
<contexts>
<context position="5460" citStr="Carroll, 1997" startWordPosition="799" endWordPosition="800">her -&gt; non-heads head&apos; non-heads (freq) The rules are head marked with a prime. The non-head sequences may be empty. f req is a rule frequency, which is initialized randomly and subsequently estimated by the inside outsidealgorithm. To handle systematic patterns related to features, rules were generated by Lisp functions, rather than being written directly in the above form. With very few exceptions (rules for coordination, S-rule), the rules do not have more than two daughters. Grammar development is facilitated by a chart browser that permits a quick and efficient discovery of grammar bugs (Carroll, 1997a). Fig. 3 shows that the ambiguity in the chart is quite considerable even though grammar and corpus are restricted. For the entire corpus, we computed an average 9202 trees per clause. In the chart browser, the categories filling the cells indicate the most probable category for that span with their estimated frequencies. The pop-up window under IP presents the ranked list of all possible categories for the covered span. Rules (chart edges) with frequencies can be viewed with a further menu. In the chart browser, colors are used to display frequencies (between 0 and 1) estimated by the insid</context>
<context position="12202" citStr="Carroll, 1997" startWordPosition="1888" endWordPosition="1889">der is used. (This feature was used by C+R for their phrase trigram grammar, not in the linguistic part of their grammar.) Not all desirable parameter pooling can be expressed in this way, though; for instance rule parameters are not pooled, and so get split when the parent category bears an inflectional feature. 5 Estimation The training of our probabilistic CFG proceeds in three steps: (i) unlexicalized training with the supar parser, (ii) bootstrapping a lexicalized model from the trained unlexicalized one with the ultra parser, and finally (iii) lexicalized training with the hypar parser (Carroll, 1997b). Each of the three parsers uses the insideoutside algorithm. supar and ultra use an unlexicalized weighting of trees, while hypar uses a lexicalized weighting of trees. ultra and hypar both collect frequencies for lexicalized rule and lexical choice events, while supar collects only unlexicalized rule frequencies. Our experiments have shown that training an unlexicalized model first is worth the effort. Despite our use of a manually developed grammar that does not have to be pruned of superfluous rules like an automatically generated grammar, A 1: 52.0199 1: 53.7654 1: 49.8165 2: 25.3652 2:</context>
</contexts>
<marker>Carroll, 1997</marker>
<rawString>Glenn Carroll, 1997b. Manual pages for supar, ultra, hypar, and genDists. IMS, Stuttgart University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>Parsing with context-free grammars and word statistics.</title>
<date>1995</date>
<tech>Technical Report CS-95-28,</tech>
<institution>Department of Computer Science, Brown University.</institution>
<contexts>
<context position="10703" citStr="Charniak (1995)" startWordPosition="1648" endWordPosition="1649">lier experiments, we employed a flat clausal structure, with rules for all permutations of complements. As the number of frames increased, this produced prohibitively many rules, particularly with the inclusion of adjuncts. 4 Parameters The parameterization is as in C+R, with one significant modification. Parameters consist of (i) rule parameters, corresponding to right hand 271 sides conditioned by parent category and parent head; (ii) lexical choice parameters for nonhead children, corresponding to child lemma conditioned by child category, parent category, and parent head lemma. See C+R or Charniak (1995) for an explanation of how such parameters define a probabilistic weighting of trees. The change relative to C+R is that lexicalization is by uninflected lemma rather than word form. This reduces the number of lexical parameters, giving more acceptable model sizes and eliminating splitting of estimated frequencies among inflectional forms. Inflected forms are generated at the leaves of the tree, conditioned on terminal category and lemma. This results in a third family of parameters, though usually the choice of inflected form is deterministic. A parameter pooling feature is used for argument </context>
<context position="13245" citStr="Charniak (1995)" startWordPosition="2065" endWordPosition="2066"> a manually developed grammar that does not have to be pruned of superfluous rules like an automatically generated grammar, A 1: 52.0199 1: 53.7654 1: 49.8165 2: 25.3652 2: 26.3184 23.1008 24.5905 3: 25.5035 3: 22.4479 . . . . . . . . . . . . . . . . . . 13: 24.2872 55: 25.0548 70: 22.1445 14: 24.2863 56: 25.0549 80: 22.1443 15: 24.2861 57: 25.0549 90: 22.1443 16: 24.2861 58: 25.0549 95: 22.1443 17: 24.2867 59: 25.055 96: 22.1444 Figure 6: Overtraining (iteration: cross-entropy on heldout data) the lexicalized model is notably better when preceded by unlexicalized training (see also Ersan and Charniak (1995) for related observations). A comparison of immediate lexicalized training (without prior training of an unlexicalized model) and our standard training regime that involves preliminary unlexicalized training speaks in favor of our strategy (cf. the different &apos;lex 0&apos; and &apos;lex 2&apos; curves in figures 8 and 9). However, the amount of unlexicalized training has to be controlled in some way. A standard criterion to measure overtraining is to compare log-likelihood values on held-out data of subsequent iterations. While the loglikelihood value of the training data is theoretically guaranteed to converg</context>
</contexts>
<marker>Charniak, 1995</marker>
<rawString>E. Charniak. 1995. Parsing with context-free grammars and word statistics. Technical Report CS-95-28, Department of Computer Science, Brown University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ersan</author>
<author>E Charniak</author>
</authors>
<title>A statistical syntactic disambiguation program and what it learns.</title>
<date>1995</date>
<tech>Technical Report CS-95-29,</tech>
<institution>Department of Computer Science, Brown University.</institution>
<contexts>
<context position="13245" citStr="Ersan and Charniak (1995)" startWordPosition="2062" endWordPosition="2066">our use of a manually developed grammar that does not have to be pruned of superfluous rules like an automatically generated grammar, A 1: 52.0199 1: 53.7654 1: 49.8165 2: 25.3652 2: 26.3184 23.1008 24.5905 3: 25.5035 3: 22.4479 . . . . . . . . . . . . . . . . . . 13: 24.2872 55: 25.0548 70: 22.1445 14: 24.2863 56: 25.0549 80: 22.1443 15: 24.2861 57: 25.0549 90: 22.1443 16: 24.2861 58: 25.0549 95: 22.1443 17: 24.2867 59: 25.055 96: 22.1444 Figure 6: Overtraining (iteration: cross-entropy on heldout data) the lexicalized model is notably better when preceded by unlexicalized training (see also Ersan and Charniak (1995) for related observations). A comparison of immediate lexicalized training (without prior training of an unlexicalized model) and our standard training regime that involves preliminary unlexicalized training speaks in favor of our strategy (cf. the different &apos;lex 0&apos; and &apos;lex 2&apos; curves in figures 8 and 9). However, the amount of unlexicalized training has to be controlled in some way. A standard criterion to measure overtraining is to compare log-likelihood values on held-out data of subsequent iterations. While the loglikelihood value of the training data is theoretically guaranteed to converg</context>
</contexts>
<marker>Ersan, Charniak, 1995</marker>
<rawString>M. Ersan and E. Charniak. 1995. A statistical syntactic disambiguation program and what it learns. Technical Report CS-95-29, Department of Computer Science, Brown University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauri Karttunen</author>
<author>Todd Yampol</author>
<author>Gregory Grefenstette</author>
</authors>
<date>1994</date>
<journal>INFL Morphological Analyzer/Generator</journal>
<volume>3</volume>
<pages>3--6</pages>
<publisher>Xerox Corporation.</publisher>
<contexts>
<context position="2358" citStr="Karttunen et al. (1994)" startWordPosition="348" endWordPosition="351">rpus of German subordinate clauses extracted by regular expression matching from a 200 million token newspaper corpus. The clause length ranges between four and 12 words. Apart from infinitival VPs as verbal arguments, there are no further clausal embeddings, and the clauses do not contain any punctuation except for a terminal period. The corpus contains 4128873 tokens and 450526 clauses which yields an average of 9.16456 tokens per clause. Tokens are automatically annotated with a list of part-of-speech (PoS) tags using a computational morphological analyser based on finite-state technology (Karttunen et al. (1994), Schiller and Stockert (1995)). A problem for practical inside-outside estimation of an inflectional language like German arises with the large number of terminal and low-level non-terminal categories in the grammar resulting from the morpho-syntactic features of words. Apart from major class (noun, adjective, and so forth) the analyser provides an ambiguous word with a list of possible combinations of inflectional features like gender, person, number (cf. the top part of Fig. 1 for an example ambiguous between nominal and adjectival PoS; the PoS is indicated following the &apos;+&apos; sign). In order</context>
</contexts>
<marker>Karttunen, Yampol, Grefenstette, 1994</marker>
<rawString>Lauri Karttunen, Todd Yampol, and Gregory Grefenstette, 1994. INFL Morphological Analyzer/Generator 3.2.9 (3.6.4). Xerox Corporation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anne Schiller</author>
<author>Chris Stockert</author>
</authors>
<date>1995</date>
<tech>DMOR. IMS,</tech>
<institution>Universitat Stuttgart.</institution>
<contexts>
<context position="2388" citStr="Schiller and Stockert (1995)" startWordPosition="352" endWordPosition="355">e clauses extracted by regular expression matching from a 200 million token newspaper corpus. The clause length ranges between four and 12 words. Apart from infinitival VPs as verbal arguments, there are no further clausal embeddings, and the clauses do not contain any punctuation except for a terminal period. The corpus contains 4128873 tokens and 450526 clauses which yields an average of 9.16456 tokens per clause. Tokens are automatically annotated with a list of part-of-speech (PoS) tags using a computational morphological analyser based on finite-state technology (Karttunen et al. (1994), Schiller and Stockert (1995)). A problem for practical inside-outside estimation of an inflectional language like German arises with the large number of terminal and low-level non-terminal categories in the grammar resulting from the morpho-syntactic features of words. Apart from major class (noun, adjective, and so forth) the analyser provides an ambiguous word with a list of possible combinations of inflectional features like gender, person, number (cf. the top part of Fig. 1 for an example ambiguous between nominal and adjectival PoS; the PoS is indicated following the &apos;+&apos; sign). In order to reduce the number of param</context>
</contexts>
<marker>Schiller, Stockert, 1995</marker>
<rawString>Anne Schiller and Chris Stockert, 1995. DMOR. IMS, Universitat Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Manual page for lopar. IMS,</title>
<date>1999</date>
<institution>Universitat Stuttgart.</institution>
<contexts>
<context position="17252" citStr="Schmid, 1999" startWordPosition="2702" endWordPosition="2703">es). The total of 584 labelled VP categories subdivides into 21 different verb frames with 340 different lemma heads. The dominant frames are active transitive (164 occurrences) and active intransitive (117 occurrences). They represent almost half of the annotated frames. Thirteen frames occur less than ten times, five of which just once. 6.1 Methodology To evaluate iterative training, we extracted maximum probability (Viterbi) trees for the 600 clause test set in each iteration of parsing. For extraction of a maximal probability parse in unlexicalized training, we used Schmid&apos;s lopar parser (Schmid, 1999). Trees were mapped to a database of parser generated markup guesses, and we measured precision and recall against the manually annotated category names and spans. Precision gives the ratio of correct guesses over all guesses, and recall the ratio of correct guesses over the number of phrases identified by human annotators. Here, we render only the precision/recall results on pairs of category names and spans, neglecting less interesting measures on spans alone. For the figures of adjusted recall, the number of unparsed misses has been subtracted from the number of possibilities. 273 precision</context>
</contexts>
<marker>Schmid, 1999</marker>
<rawString>Helmut Schmid, 1999. Manual page for lopar. IMS, Universitat Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wojciech Skut</author>
<author>Thorsten Brants</author>
</authors>
<title>A maximum-entropy partial parser for unrestricted text.</title>
<date>1998</date>
<booktitle>In Proceedings of the Sixth Workshop on Very Large Corpora,</booktitle>
<location>Montréal, Québec.</location>
<contexts>
<context position="20213" citStr="Skut and Brants, 1998" startWordPosition="3192" endWordPosition="3195"> about 7% revised guesses from the unlexicalized to the first lexicalized model. Currently, we do not have a clear picture of the newly introduced errors. The plots labeled &amp;quot;00&amp;quot; are results for lexicalized training starting from a random initial grammar. The precision measure of the first lexicalized model falls below that of the unlexicalized random model (74%), only recovering through lexicalized training to equalize the precision measure of the random model (75.6%). This indicates that some degree of unlexicalized initialization is necessary, if a good lexicalized model is to be obtained. (Skut and Brants, 1998) report 84.4% recall and 84.2% for NP and PP chunking without case labels. While these are numbers for a simpler problem and are slightly below ours, they are figures for an experiment on unrestricted sentences. A genuine comparison has to await extension of our model to free text. 6.3 Verb Frame Evaluation Figure 9 gives results for verb frame recognition under the same training conditions. Again, we achieve best results by lexicalising the second unlexicalized model. Of a total of 584 annotated verb frames, 384 are correctly recognized in the best unlexicalized model and 397 through subseque</context>
</contexts>
<marker>Skut, Brants, 1998</marker>
<rawString>Wojciech Skut and Thorsten Brants. 1998. A maximum-entropy partial parser for unrestricted text. In Proceedings of the Sixth Workshop on Very Large Corpora, Montréal, Québec.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>