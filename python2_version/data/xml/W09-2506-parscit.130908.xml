<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.094657">
<title confidence="0.996829">
Ranking Paraphrases in Context
</title>
<author confidence="0.973261">
Stefan Thater
</author>
<affiliation confidence="0.922768">
Universität des Saarlandes
</affiliation>
<email confidence="0.952658">
stth@coli.uni-sb.de
</email>
<author confidence="0.910461">
Georgiana Dinu
</author>
<affiliation confidence="0.866113">
Universität des Saarlandes
</affiliation>
<email confidence="0.931436">
dinu@coli.uni-sb.de
</email>
<author confidence="0.866061">
Manfred Pinkal
</author>
<affiliation confidence="0.826834">
Universität des Saarlandes
</affiliation>
<email confidence="0.958718">
pinkal@coli.uni-sb.de
</email>
<sectionHeader confidence="0.992737" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999950666666666">
We present a vector space model that sup-
ports the computation of appropriate vec-
tor representations for words in context,
and apply it to a paraphrase ranking task.
An evaluation on the SemEval 2007 lexical
substitution task data shows promising re-
sults: the model significantly outperforms
a current state of the art model, and our
treatment of context is effective.
</bodyText>
<sectionHeader confidence="0.999266" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999946615384616">
Knowledge about paraphrases is of central impor-
tance to textual inference modeling. Systems which
support automatic extraction of large repositories
of paraphrase or inference rules like Lin and Pantel
(2001) or Szpektor et al. (2004) thus form first-class
candidate resources to be leveraged for NLP tasks
like question answering, information extraction, or
summarization, and the meta-task of recognizing
textual entailment.
Existing knowledge bases still suffer a number
of limitations, making their use in applications
challenging. One of the most serious problems
is insensitivity to context. Natural-language infer-
ence is highly context-sensitive, the applicability
of inference rules depending on word sense and
even finer grained contextual distinctions in us-
age (Szpektor et al., 2007). Application of a rule
like “X shed Y q X throw Y” is appropriate in a
sentence like “a mouse study sheds light on the
mixed results,” but not in sentences like “the econ-
omy seems to be shedding fewer jobs” or “cats
do not shed the virus to other cats.” Systems like
the above-mentioned ones base the extraction of
inference rules on distributional similarity of words
rather than word senses, and apply unconditionally
whenever one side of the rule matches on the word
level, which may lead to considerable precision
problems (Geffet and Dagan, 2005) .
Some approaches address the problem of con-
text sensitivity by deriving inference rules whose
argument slots bear selectional preference infor-
mation (Pantel et al., 2007; Basili et al., 2007). A
different line of accounting for contextual variation
has been taken by Mitchell and Lapata (2008), who
propose a compositional approach, “contextualiz-
ing” the vector-space meaning representation of
predicates by combining the distributional proper-
ties of the predicate with those of its arguments.
A related approach has been proposed by Erk and
Padó (2008), who integrate selectional preferences
into the compositional picture. In this paper, we
propose a context-sensitive vector-space approach
which draws some important ideas from Erk and
Pado’s paper (“E&amp;P” in the following), but imple-
ments them in a different, more effective way: An
evaluation on the SemEval 2007 lexical substitu-
tion task data shows that our model significantly
outperforms E&amp;P in terms of average precision.
Plan of the paper. Section 2 presents our model
and briefly relates it to previous work. Section 3
describes the evaluation of our model on the lexical
substitution task data. Section 4 concludes.
</bodyText>
<sectionHeader confidence="0.778173" genericHeader="method">
2 A model for meaning in context
</sectionHeader>
<bodyText confidence="0.999965352941177">
We propose a dependency-based model whose di-
mensions reflect dependency relations, and distin-
guish two kinds or layers of lexical meaning: ar-
gument meaning and predicate meaning. The argu-
ment meaning of a word w is a vector representing
frequencies of all pairs (w&apos;,r&apos;) of predicate expres-
sions w&apos; and dependency relations r&apos; such that w&apos;
stands in relation r&apos; to w. Intuitively, argument
meaning is similar to E&amp;P’s “inverse selectional
preferences.” Argument meanings are used for two
purposes in our model: (i) to construct predicate
meanings, and (ii) to contextually constrain them.
For technical convenience, we will use a defini-
tional variant of argument meaning, by indexing
it with an “incoming” relation, which allows pred-
icate and argument meaning to be treated techni-
cally as vectors of the same type. Assuming a set
</bodyText>
<page confidence="0.993668">
44
</page>
<note confidence="0.985265">
Proceedings of the 2009 Workshop on Applied Textual Inference, ACL-IJCNLP 2009, pages 44–47,
Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.988095666666667">
R of role labels and a set W of words, we represent
both predicate and argument meaning as vectors
in a vector space V with a basis {ei}iER.R.W, i.e.,
a vector space whose dimensions correspond to
triples of two role labels and a word. The argument
meaning vr(w) of a word w is defined as follows:
</bodyText>
<equation confidence="0.9822945">
( &apos;
vr(w) w&apos;EW,r&apos;ER f lw , r&apos;,\ wJ - e(r,r&apos;,w&apos;), (1)
</equation>
<bodyText confidence="0.99803725">
where r is the “incoming” relation, and f (w&apos;,r&apos;,w)
denotes the frequency of w occurring in relation r&apos;
to w&apos; in a collection of dependency trees. To obtain
predicate meaning vP(w), we count the occurrences
of argument words w&apos; standing in relation r to w,
and compute the predicate meaning as the sum of
the argument meanings vr(w&apos;), weighted by these
co-occurrence frequencies:
</bodyText>
<equation confidence="0.996777">
vP(w) = ∑rER,w&apos;EW f (w,r,w&apos;) - vr(w&apos;) (2)
</equation>
<bodyText confidence="0.99946636">
That is, the meaning of a predicate is modelled by a
vector representing “second order” co-coccurrence
frequencies with other predicates.
In general, words have both a “downward look-
ing” predicate meaning and an “upward looking”
argument meaning. In our study, only one of them
will be relevant, since we will restrict ourselves
to local predicate-argument structures with verbal
heads and nominal arguments.
Computing meaning in context. Vectors repre-
senting predicate meaning are derived by collecting
co-occurrence frequencies for all uses of the pred-
icate, possibly resulting in vector representations
in which different meanings of the predicate are
combined. Given an instance of a predicate w that
has arguments w1,...,wk, we can now contextually
constrain the predicate meaning of w by the argu-
ment meanings of its arguments. Here, we propose
to simple “restrict” the predicate meaning to those
dimensions that have a non-zero value in at least
one of its argument meanings. More formally, we
write v|v&apos; to denote a vector that is identical to v
for all components that have a non-zero value in v&apos;,
zero otherwise. We compute predicate meaning in
context as follows:
</bodyText>
<equation confidence="0.913489">
vP(w)|∑1&lt;i&lt;k vri(wi), (3)
</equation>
<bodyText confidence="0.949976125">
where ri is the argument position filled by wi.
Parameters. To reduce the effect of noise and
provide a more fine-grained control over the ef-
fect of context, we can choose different thresholds
target subject object paraphrases
shed study light throw 3, reveal 2, shine 1
shed cat virus spread 2, pass 2, emit 1, transmit 2
shed you blood lose 3, spill 1, give 1
</bodyText>
<tableCaption confidence="0.981496">
Table 1: Lexical substitution task data set
</tableCaption>
<bodyText confidence="0.999950833333334">
for function f in the computation of predicate and
argument meaning. In Section 3, we obtain best
results if we consider only dependency relations
that occur at least 6 times in the British National
Corpus (BNC) for the computation of predicate
meaning, and relations occurring at least 15 times
for the computation of argument meanings when
predicate meaning is contextually constrained.
Related work. Our model is similar to the struc-
tured vector space model proposed by Erk and Padó
(2008) in that the representation of predicate mean-
ing is based on dependency relations, and that “in-
verse selectional preferences” play an important
role. However, inverse selectional preferences are
used in E&amp;P’s model mainly to compute mean-
ing in context, while they are directly “built into”
the vectors representing predicate meaning in our
model.
</bodyText>
<sectionHeader confidence="0.999147" genericHeader="evaluation">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.999843083333333">
We evaluate our model on a paraphrase ranking
task on a subset of the SemEval 2007 lexical substi-
tution task (McCarthy and Navigli, 2007) data, and
compare it to a random baseline and E&amp;P’s state
of the art model.
Dataset. The lexical substitution task dataset con-
tains 10 instances for 44 target verbs in different
sentential contexts. Systems that participated in
the task had to generate paraphrases for each of
these instances, which are evaluated against a gold
standard containing up to 9 possible paraphrases
for individual instances. Following Erk and Padó
(2008), we use the data in a different fashion: we
pool paraphrases for all instances of a verb in all
contexts, and use the models to rank these para-
phrase candidates in specific contexts.
Table 1 shows three instances of the target verb
shed together with its paraphrases in the gold stan-
dard as an expample. The paraphrases are attached
with weights, which correspond to the number of
times they have been given by different annotators.
To allow for a comparision with E&amp;P’s model,
we follow Erk and Padó (2008) and extract only
sentences from the dataset containing target verbs
</bodyText>
<page confidence="0.996673">
45
</page>
<bodyText confidence="0.995366523809524">
with overtly realized subject and object, and re-
move instances from the dataset for which the tar-
get verb or one of its arguments is not in the BNC.
We obtain a set of 162 instances for 34 different
verbs. We also remove paraphrases that are not
in the BNC. On average, target verbs have 20.5
paraphrase candidates, 3.9 of which are correct in
specific contexts.
Experimental setup. We parse the BNC using
MiniPar (Lin, 1993) and extract co-occurrence fre-
quencies, considering only dependency relations
for the most frequent 2000 verbs. We don’t use raw
frequency counts directly but reweight the vectors
by pointwise mutual information.
To rank paraphrases in context, we compute con-
textually constrained vectors for the verb in the
input sentence and all its paraphrase candidates
by taking the corresponding predicate vectors and
restricting them to the argument meanings of the
argument head nouns in the input sentence. The
restricted vectors for the paraphrase candidates are
then ranked by comparing them to the restricted
vector of the input verb using cosine similarity.
In order to compare our model with state of the
art, we reimplement E&amp;P’s structured vector space
model. We filter stop words, and compute lexical
vectors in a “syntactic” space using the most fre-
quent 2000 words from the BNC as basis. We also
consider a variant in which the basis corresponds
to words indexed by their grammatical roles. We
choose parameters that Erk and Padó (2009) report
to perform best, and use the method described in
Erk and Padó (2009) to compute vectors in context.
Evaluation metrics. As scoring methods, we
use both “precision out of ten” (Poot), which was
originally used in the lexical substitution task and
also used by E&amp;P, and generalized average preci-
sion (Kishida, 2005), a variant of average precision
which is frequently used in information extraction
tasks and has also been used in the PASCAL RTE
challenges (Dagan et al., 2006).
Poot can be defined as follows:
</bodyText>
<equation confidence="0.745844">
ΣscMnG f(s)
ΣscG f (s) ,
</equation>
<bodyText confidence="0.999507166666667">
where M is the list of 10 paraphrase candidates
top-ranked by the model, G is the corresponding
annotated gold data, and f (s) is the weight of the
individual paraphrases. Here, Poot is computed for
each target instance separately; below, we report
the average over all instances.
</bodyText>
<table confidence="0.999130142857143">
Model Poot GAP
Random baseline 54.25 26.03
E&amp;P (target only) 64.61 (63.31) 29.95 (32.02)
E&amp;P (add, object only) 66.20 (62.90) 29.93 (31.54)
E&amp;P (min, both) 64.86 (59.62) 32.22 (31.28)
TDP 63.32 36.54
TDP (target only) 62.60 33.04
</table>
<tableCaption confidence="0.999466">
Table 2: Results
</tableCaption>
<bodyText confidence="0.9997249">
Generalized average precision (GAP) is a more
precise measure than Poot: Applied to a ranking
task with about 20 candidates, Poot just gives the
percentage of good candidates found in the upper
half of the proposed ranking. Average precision
is sensitive to the relative position of correct and
incorrect candidates in the ranking, GAP moreover
rewards the correct order of positive cases w.r.t.
their gold standard weight.
We define average precision first:
</bodyText>
<equation confidence="0.98404">
R pi = Σik=1xk
i
</equation>
<bodyText confidence="0.999974375">
where xi is a binary variable indicating whether
the ith item as ranked by the model is in the gold
standard or not, R is the size of the gold standard,
and n the number of paraphrase candidates to be
ranked. If we take xi to be the gold standard weight
of the ith item or zero if it is not in the gold standard,
we can define generalized average precision as
follows:
</bodyText>
<equation confidence="0.9799105">
GAP = Σni=1I(xi) pi i R
R, R = Σi=1I(yi)yi
</equation>
<bodyText confidence="0.999986333333334">
where I(xi) = 1 if xi is larger than zero, zero oth-
erwise, and yi is the average weight of the ideal
ranked list y1,...,yi of paraphrases in the gold stan-
dard.
Results and discussion. Table 2 shows the re-
sults of our experiments for two variants of our
model (“TDP”), and compares them to a random
baseline and three instantiations (in two variants) of
E&amp;P’s model. The “target only” models don’t use
context information, i.e., paraphrases are ranked by
cosine similarity of predicate meaning only. The
other models take context into account. The “min”
E&amp;P model takes the component-wise minimum to
combine a lexical vector with context vectors and
considers both subject and object as context; it is
the best performing model in Erk and Padó (2009).
The “add” model uses vector addition and consid-
ers only objects as context; it is the best-performing
</bodyText>
<equation confidence="0.925873333333333">
Poot =
AP =
Σni=1xi pi
</equation>
<page confidence="0.996912">
46
</page>
<figureCaption confidence="0.999729">
Figure 1: “Precision out of n” for 1 &lt; n &lt; 10.
</figureCaption>
<bodyText confidence="0.999887071428572">
model (in terms of Poot) for our dataset. The num-
bers in brackets refer to variants of the E&amp;P models
in which the basis corresponds to words indexed
by their syntactic roles. Note that the results for the
E&amp;P models are better than the results published
in Erk and Padó (2009), which might be due to
slightly different datasets or lists of stop-words.
As can be seen, our model performs &gt; 10% bet-
ter than the random baseline. It performs &gt; 4%
better than the “min” E&amp;P model and &gt; 6% better
then the “add” model in terms of GAP if we use a
vectors space with words as basis. For the variants
of the E&amp;P models in which the basis corresponds
to words indexed by their syntactic role, we ob-
tain different results, but our model is still &gt; 4%
better than these variants. We can also see that
our treatment of context is effective, leading to a
&gt; 3% increase of GAP. A stratified shuffling-based
randomization test (Yeh, 2000) shows that the dif-
ferences are statistically significant (p &lt; 0.05).
In terms of Poot, the “add” E&amp;P model performs
better than our model, which might look surprising,
given its low GAP score. Fig. 1 gives a more fine-
grained comparison between the two models. It
displays the “precision out of n” of the two models
for varying n. As can be seen, our model performs
better for all n &lt; 10, and much better than the base-
line and E&amp;P for n &lt; 4.
</bodyText>
<sectionHeader confidence="0.999331" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.986751045454545">
In this paper, we have proposed a dependency-
based context-sensitive vector-space approach that
supports the computation of adequate vector-based
representations of predicate meaning in context.
An evaluation on a paraphrase ranking task using
a subset of the SemEval 2007 lexical substitution
task data shows promising results: our model per-
forms significantly better than a current state of the
art system (Erk and Padó, 2008), and our treatment
of context is effective.
Since the dataset we used for the evaluation is
relatively small, there is a potential danger for over-
fitting, and it remains to be seen whether the results
carry over to larger datasets. First experiments
indicate that this is actually the case.
We expect that our approach can be generalized
to arrive at a general compositional model, which
would allow to compute contextually appropriate
meaning representations for complex relational ex-
pressions rather than single lexical predicates.
Acknowledgements. We thank Katrin Erk and
Sebastian Padó for help and critical comments.
</bodyText>
<sectionHeader confidence="0.997504" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999874324324324">
R. Basili, D. De Cao, P. Marocco, and M. Pennacchiotti. 2007.
Learning selectional preferences for entailment or para-
phrasing rules. In Proc. of RANLP 2007.
I. Dagan, O. Glickman, and B. Magnini. 2006. The PASCAL
Recognising Textual Entailment Challenge. In Machine
Learning Challenges, volume 3944. Springer.
K. Erk and S. Padó. 2008. A structured vector space model
for word meaning in context. In Proc. of EMNLP.
K. Erk and S. Padó. 2009. Paraphrase assessment in struc-
tured vector space: Exploring parameters and datasets. In
Proc. of the Workshop on Geometrical Models of Natural
Language Semantics, Athens.
M. Geffet and I. Dagan. 2005. The distributional inclusion
hypotheses and lexical entailment. In Proc. of the ACL.
K. Kishida. 2005. Property of average precision and its
generalization: An examination of evaluation indicator for
information retrieval experiments. NII Technical Report.
D. Lin and P. Pantel. 2001. DIRT – Discovery of Inference
Rules from Text. In Proc. of the ACM Conference on
Knowledge Discovery and Data Mining, San Francisco.
D. Lin. 1993. Principle-based parsing without overgeneration.
In Proc. of ACL, Columbus.
D. McCarthy and R. Navigli. 2007. SemEval-2007 Task 10:
English Lexical Substitution Task. In Proc. of SemEval,
Prague.
J. Mitchell and M. Lapata. 2008. Vector-based models of se-
mantic composition. In Proc. of ACL-08: HLT, Columbus.
P. Pantel, R. Bhagat, B. Coppola, T. Chklovski, and E. Hovy.
2007. ISP: Learning inferential selectional preferences. In
Human Language Technologies 2007, Rochester.
I. Szpektor, H. Tanev, I. Dagan, and B. Coppola. 2004. Scal-
ing web-based acquisition of entailment relations. In Proc.
of EMNLP, Barcellona.
I. Szpektor, E. Shnarch, and I. Dagan. 2007. Instance-based
evaluation of entailment rule acquisition. In Proc. of ACL.
A. Yeh. 2000. More accurate tests for the statistical signifi-
cance of result differences. In Proc. of COLING.
</reference>
<figure confidence="0.999606416666666">
1 2 3 4 5 6 7 8 9 10
100
40
20
80
60
0
presision out of n
E&amp;P (add, object only)
present paper
baseline
upper bound
</figure>
<page confidence="0.990053">
47
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.397779">
<title confidence="0.999907">Ranking Paraphrases in Context</title>
<author confidence="0.976486">Stefan</author>
<affiliation confidence="0.955613">Universität des</affiliation>
<email confidence="0.767719">stth@coli.uni-sb.de</email>
<author confidence="0.787327">Georgiana</author>
<affiliation confidence="0.878813">Universität des</affiliation>
<email confidence="0.852066">dinu@coli.uni-sb.de</email>
<author confidence="0.823612">Manfred</author>
<affiliation confidence="0.917672">Universität des</affiliation>
<email confidence="0.921282">pinkal@coli.uni-sb.de</email>
<abstract confidence="0.9989613">We present a vector space model that supports the computation of appropriate vector representations for words in context, and apply it to a paraphrase ranking task. An evaluation on the SemEval 2007 lexical substitution task data shows promising results: the model significantly outperforms a current state of the art model, and our treatment of context is effective.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Basili</author>
<author>D De Cao</author>
<author>P Marocco</author>
<author>M Pennacchiotti</author>
</authors>
<title>Learning selectional preferences for entailment or paraphrasing rules.</title>
<date>2007</date>
<booktitle>In Proc. of RANLP</booktitle>
<marker>Basili, De Cao, Marocco, Pennacchiotti, 2007</marker>
<rawString>R. Basili, D. De Cao, P. Marocco, and M. Pennacchiotti. 2007. Learning selectional preferences for entailment or paraphrasing rules. In Proc. of RANLP 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>O Glickman</author>
<author>B Magnini</author>
</authors>
<title>The PASCAL Recognising Textual Entailment Challenge.</title>
<date>2006</date>
<booktitle>In Machine Learning Challenges,</booktitle>
<volume>volume</volume>
<pages>3944</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="10479" citStr="Dagan et al., 2006" startWordPosition="1686" endWordPosition="1689">ider a variant in which the basis corresponds to words indexed by their grammatical roles. We choose parameters that Erk and Padó (2009) report to perform best, and use the method described in Erk and Padó (2009) to compute vectors in context. Evaluation metrics. As scoring methods, we use both “precision out of ten” (Poot), which was originally used in the lexical substitution task and also used by E&amp;P, and generalized average precision (Kishida, 2005), a variant of average precision which is frequently used in information extraction tasks and has also been used in the PASCAL RTE challenges (Dagan et al., 2006). Poot can be defined as follows: ΣscMnG f(s) ΣscG f (s) , where M is the list of 10 paraphrase candidates top-ranked by the model, G is the corresponding annotated gold data, and f (s) is the weight of the individual paraphrases. Here, Poot is computed for each target instance separately; below, we report the average over all instances. Model Poot GAP Random baseline 54.25 26.03 E&amp;P (target only) 64.61 (63.31) 29.95 (32.02) E&amp;P (add, object only) 66.20 (62.90) 29.93 (31.54) E&amp;P (min, both) 64.86 (59.62) 32.22 (31.28) TDP 63.32 36.54 TDP (target only) 62.60 33.04 Table 2: Results Generalized a</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2006</marker>
<rawString>I. Dagan, O. Glickman, and B. Magnini. 2006. The PASCAL Recognising Textual Entailment Challenge. In Machine Learning Challenges, volume 3944. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Erk</author>
<author>S Padó</author>
</authors>
<title>A structured vector space model for word meaning in context.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="2511" citStr="Erk and Padó (2008)" startWordPosition="374" endWordPosition="377">lead to considerable precision problems (Geffet and Dagan, 2005) . Some approaches address the problem of context sensitivity by deriving inference rules whose argument slots bear selectional preference information (Pantel et al., 2007; Basili et al., 2007). A different line of accounting for contextual variation has been taken by Mitchell and Lapata (2008), who propose a compositional approach, “contextualizing” the vector-space meaning representation of predicates by combining the distributional properties of the predicate with those of its arguments. A related approach has been proposed by Erk and Padó (2008), who integrate selectional preferences into the compositional picture. In this paper, we propose a context-sensitive vector-space approach which draws some important ideas from Erk and Pado’s paper (“E&amp;P” in the following), but implements them in a different, more effective way: An evaluation on the SemEval 2007 lexical substitution task data shows that our model significantly outperforms E&amp;P in terms of average precision. Plan of the paper. Section 2 presents our model and briefly relates it to previous work. Section 3 describes the evaluation of our model on the lexical substitution task da</context>
<context position="7021" citStr="Erk and Padó (2008)" startWordPosition="1116" endWordPosition="1119"> virus spread 2, pass 2, emit 1, transmit 2 shed you blood lose 3, spill 1, give 1 Table 1: Lexical substitution task data set for function f in the computation of predicate and argument meaning. In Section 3, we obtain best results if we consider only dependency relations that occur at least 6 times in the British National Corpus (BNC) for the computation of predicate meaning, and relations occurring at least 15 times for the computation of argument meanings when predicate meaning is contextually constrained. Related work. Our model is similar to the structured vector space model proposed by Erk and Padó (2008) in that the representation of predicate meaning is based on dependency relations, and that “inverse selectional preferences” play an important role. However, inverse selectional preferences are used in E&amp;P’s model mainly to compute meaning in context, while they are directly “built into” the vectors representing predicate meaning in our model. 3 Evaluation We evaluate our model on a paraphrase ranking task on a subset of the SemEval 2007 lexical substitution task (McCarthy and Navigli, 2007) data, and compare it to a random baseline and E&amp;P’s state of the art model. Dataset. The lexical subst</context>
<context position="8460" citStr="Erk and Padó (2008)" startWordPosition="1353" endWordPosition="1356">a gold standard containing up to 9 possible paraphrases for individual instances. Following Erk and Padó (2008), we use the data in a different fashion: we pool paraphrases for all instances of a verb in all contexts, and use the models to rank these paraphrase candidates in specific contexts. Table 1 shows three instances of the target verb shed together with its paraphrases in the gold standard as an expample. The paraphrases are attached with weights, which correspond to the number of times they have been given by different annotators. To allow for a comparision with E&amp;P’s model, we follow Erk and Padó (2008) and extract only sentences from the dataset containing target verbs 45 with overtly realized subject and object, and remove instances from the dataset for which the target verb or one of its arguments is not in the BNC. We obtain a set of 162 instances for 34 different verbs. We also remove paraphrases that are not in the BNC. On average, target verbs have 20.5 paraphrase candidates, 3.9 of which are correct in specific contexts. Experimental setup. We parse the BNC using MiniPar (Lin, 1993) and extract co-occurrence frequencies, considering only dependency relations for the most frequent 200</context>
<context position="14685" citStr="Erk and Padó, 2008" startWordPosition="2431" endWordPosition="2434"> displays the “precision out of n” of the two models for varying n. As can be seen, our model performs better for all n &lt; 10, and much better than the baseline and E&amp;P for n &lt; 4. 4 Conclusion In this paper, we have proposed a dependencybased context-sensitive vector-space approach that supports the computation of adequate vector-based representations of predicate meaning in context. An evaluation on a paraphrase ranking task using a subset of the SemEval 2007 lexical substitution task data shows promising results: our model performs significantly better than a current state of the art system (Erk and Padó, 2008), and our treatment of context is effective. Since the dataset we used for the evaluation is relatively small, there is a potential danger for overfitting, and it remains to be seen whether the results carry over to larger datasets. First experiments indicate that this is actually the case. We expect that our approach can be generalized to arrive at a general compositional model, which would allow to compute contextually appropriate meaning representations for complex relational expressions rather than single lexical predicates. Acknowledgements. We thank Katrin Erk and Sebastian Padó for help</context>
</contexts>
<marker>Erk, Padó, 2008</marker>
<rawString>K. Erk and S. Padó. 2008. A structured vector space model for word meaning in context. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Erk</author>
<author>S Padó</author>
</authors>
<title>Paraphrase assessment in structured vector space: Exploring parameters and datasets.</title>
<date>2009</date>
<booktitle>In Proc. of the Workshop on Geometrical Models of Natural Language Semantics,</booktitle>
<location>Athens.</location>
<contexts>
<context position="9996" citStr="Erk and Padó (2009)" startWordPosition="1606" endWordPosition="1609">them to the argument meanings of the argument head nouns in the input sentence. The restricted vectors for the paraphrase candidates are then ranked by comparing them to the restricted vector of the input verb using cosine similarity. In order to compare our model with state of the art, we reimplement E&amp;P’s structured vector space model. We filter stop words, and compute lexical vectors in a “syntactic” space using the most frequent 2000 words from the BNC as basis. We also consider a variant in which the basis corresponds to words indexed by their grammatical roles. We choose parameters that Erk and Padó (2009) report to perform best, and use the method described in Erk and Padó (2009) to compute vectors in context. Evaluation metrics. As scoring methods, we use both “precision out of ten” (Poot), which was originally used in the lexical substitution task and also used by E&amp;P, and generalized average precision (Kishida, 2005), a variant of average precision which is frequently used in information extraction tasks and has also been used in the PASCAL RTE challenges (Dagan et al., 2006). Poot can be defined as follows: ΣscMnG f(s) ΣscG f (s) , where M is the list of 10 paraphrase candidates top-ranked</context>
<context position="12703" citStr="Erk and Padó (2009)" startWordPosition="2073" endWordPosition="2076">phrases in the gold standard. Results and discussion. Table 2 shows the results of our experiments for two variants of our model (“TDP”), and compares them to a random baseline and three instantiations (in two variants) of E&amp;P’s model. The “target only” models don’t use context information, i.e., paraphrases are ranked by cosine similarity of predicate meaning only. The other models take context into account. The “min” E&amp;P model takes the component-wise minimum to combine a lexical vector with context vectors and considers both subject and object as context; it is the best performing model in Erk and Padó (2009). The “add” model uses vector addition and considers only objects as context; it is the best-performing Poot = AP = Σni=1xi pi 46 Figure 1: “Precision out of n” for 1 &lt; n &lt; 10. model (in terms of Poot) for our dataset. The numbers in brackets refer to variants of the E&amp;P models in which the basis corresponds to words indexed by their syntactic roles. Note that the results for the E&amp;P models are better than the results published in Erk and Padó (2009), which might be due to slightly different datasets or lists of stop-words. As can be seen, our model performs &gt; 10% better than the random baseli</context>
</contexts>
<marker>Erk, Padó, 2009</marker>
<rawString>K. Erk and S. Padó. 2009. Paraphrase assessment in structured vector space: Exploring parameters and datasets. In Proc. of the Workshop on Geometrical Models of Natural Language Semantics, Athens.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Geffet</author>
<author>I Dagan</author>
</authors>
<title>The distributional inclusion hypotheses and lexical entailment.</title>
<date>2005</date>
<booktitle>In Proc. of the ACL.</booktitle>
<contexts>
<context position="1956" citStr="Geffet and Dagan, 2005" startWordPosition="290" endWordPosition="293"> finer grained contextual distinctions in usage (Szpektor et al., 2007). Application of a rule like “X shed Y q X throw Y” is appropriate in a sentence like “a mouse study sheds light on the mixed results,” but not in sentences like “the economy seems to be shedding fewer jobs” or “cats do not shed the virus to other cats.” Systems like the above-mentioned ones base the extraction of inference rules on distributional similarity of words rather than word senses, and apply unconditionally whenever one side of the rule matches on the word level, which may lead to considerable precision problems (Geffet and Dagan, 2005) . Some approaches address the problem of context sensitivity by deriving inference rules whose argument slots bear selectional preference information (Pantel et al., 2007; Basili et al., 2007). A different line of accounting for contextual variation has been taken by Mitchell and Lapata (2008), who propose a compositional approach, “contextualizing” the vector-space meaning representation of predicates by combining the distributional properties of the predicate with those of its arguments. A related approach has been proposed by Erk and Padó (2008), who integrate selectional preferences into </context>
</contexts>
<marker>Geffet, Dagan, 2005</marker>
<rawString>M. Geffet and I. Dagan. 2005. The distributional inclusion hypotheses and lexical entailment. In Proc. of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kishida</author>
</authors>
<title>Property of average precision and its generalization: An examination of evaluation indicator for information retrieval experiments.</title>
<date>2005</date>
<tech>NII Technical Report.</tech>
<contexts>
<context position="10317" citStr="Kishida, 2005" startWordPosition="1661" endWordPosition="1662"> space model. We filter stop words, and compute lexical vectors in a “syntactic” space using the most frequent 2000 words from the BNC as basis. We also consider a variant in which the basis corresponds to words indexed by their grammatical roles. We choose parameters that Erk and Padó (2009) report to perform best, and use the method described in Erk and Padó (2009) to compute vectors in context. Evaluation metrics. As scoring methods, we use both “precision out of ten” (Poot), which was originally used in the lexical substitution task and also used by E&amp;P, and generalized average precision (Kishida, 2005), a variant of average precision which is frequently used in information extraction tasks and has also been used in the PASCAL RTE challenges (Dagan et al., 2006). Poot can be defined as follows: ΣscMnG f(s) ΣscG f (s) , where M is the list of 10 paraphrase candidates top-ranked by the model, G is the corresponding annotated gold data, and f (s) is the weight of the individual paraphrases. Here, Poot is computed for each target instance separately; below, we report the average over all instances. Model Poot GAP Random baseline 54.25 26.03 E&amp;P (target only) 64.61 (63.31) 29.95 (32.02) E&amp;P (add,</context>
</contexts>
<marker>Kishida, 2005</marker>
<rawString>K. Kishida. 2005. Property of average precision and its generalization: An examination of evaluation indicator for information retrieval experiments. NII Technical Report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
<author>P Pantel</author>
</authors>
<title>DIRT – Discovery of Inference Rules from Text.</title>
<date>2001</date>
<booktitle>In Proc. of the ACM Conference on Knowledge Discovery and Data Mining,</booktitle>
<location>San Francisco.</location>
<contexts>
<context position="818" citStr="Lin and Pantel (2001)" startWordPosition="111" endWordPosition="114">inkal@coli.uni-sb.de Abstract We present a vector space model that supports the computation of appropriate vector representations for words in context, and apply it to a paraphrase ranking task. An evaluation on the SemEval 2007 lexical substitution task data shows promising results: the model significantly outperforms a current state of the art model, and our treatment of context is effective. 1 Introduction Knowledge about paraphrases is of central importance to textual inference modeling. Systems which support automatic extraction of large repositories of paraphrase or inference rules like Lin and Pantel (2001) or Szpektor et al. (2004) thus form first-class candidate resources to be leveraged for NLP tasks like question answering, information extraction, or summarization, and the meta-task of recognizing textual entailment. Existing knowledge bases still suffer a number of limitations, making their use in applications challenging. One of the most serious problems is insensitivity to context. Natural-language inference is highly context-sensitive, the applicability of inference rules depending on word sense and even finer grained contextual distinctions in usage (Szpektor et al., 2007). Application </context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>D. Lin and P. Pantel. 2001. DIRT – Discovery of Inference Rules from Text. In Proc. of the ACM Conference on Knowledge Discovery and Data Mining, San Francisco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Principle-based parsing without overgeneration.</title>
<date>1993</date>
<booktitle>In Proc. of ACL,</booktitle>
<location>Columbus.</location>
<contexts>
<context position="8957" citStr="Lin, 1993" startWordPosition="1442" endWordPosition="1443">ve been given by different annotators. To allow for a comparision with E&amp;P’s model, we follow Erk and Padó (2008) and extract only sentences from the dataset containing target verbs 45 with overtly realized subject and object, and remove instances from the dataset for which the target verb or one of its arguments is not in the BNC. We obtain a set of 162 instances for 34 different verbs. We also remove paraphrases that are not in the BNC. On average, target verbs have 20.5 paraphrase candidates, 3.9 of which are correct in specific contexts. Experimental setup. We parse the BNC using MiniPar (Lin, 1993) and extract co-occurrence frequencies, considering only dependency relations for the most frequent 2000 verbs. We don’t use raw frequency counts directly but reweight the vectors by pointwise mutual information. To rank paraphrases in context, we compute contextually constrained vectors for the verb in the input sentence and all its paraphrase candidates by taking the corresponding predicate vectors and restricting them to the argument meanings of the argument head nouns in the input sentence. The restricted vectors for the paraphrase candidates are then ranked by comparing them to the restri</context>
</contexts>
<marker>Lin, 1993</marker>
<rawString>D. Lin. 1993. Principle-based parsing without overgeneration. In Proc. of ACL, Columbus.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McCarthy</author>
<author>R Navigli</author>
</authors>
<date>2007</date>
<booktitle>SemEval-2007 Task 10: English Lexical Substitution Task. In Proc. of SemEval,</booktitle>
<location>Prague.</location>
<contexts>
<context position="7518" citStr="McCarthy and Navigli, 2007" startWordPosition="1195" endWordPosition="1198"> contextually constrained. Related work. Our model is similar to the structured vector space model proposed by Erk and Padó (2008) in that the representation of predicate meaning is based on dependency relations, and that “inverse selectional preferences” play an important role. However, inverse selectional preferences are used in E&amp;P’s model mainly to compute meaning in context, while they are directly “built into” the vectors representing predicate meaning in our model. 3 Evaluation We evaluate our model on a paraphrase ranking task on a subset of the SemEval 2007 lexical substitution task (McCarthy and Navigli, 2007) data, and compare it to a random baseline and E&amp;P’s state of the art model. Dataset. The lexical substitution task dataset contains 10 instances for 44 target verbs in different sentential contexts. Systems that participated in the task had to generate paraphrases for each of these instances, which are evaluated against a gold standard containing up to 9 possible paraphrases for individual instances. Following Erk and Padó (2008), we use the data in a different fashion: we pool paraphrases for all instances of a verb in all contexts, and use the models to rank these paraphrase candidates in s</context>
</contexts>
<marker>McCarthy, Navigli, 2007</marker>
<rawString>D. McCarthy and R. Navigli. 2007. SemEval-2007 Task 10: English Lexical Substitution Task. In Proc. of SemEval, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Mitchell</author>
<author>M Lapata</author>
</authors>
<title>Vector-based models of semantic composition.</title>
<date>2008</date>
<booktitle>In Proc. of ACL-08: HLT,</booktitle>
<location>Columbus.</location>
<contexts>
<context position="2251" citStr="Mitchell and Lapata (2008)" startWordPosition="336" endWordPosition="339">ot shed the virus to other cats.” Systems like the above-mentioned ones base the extraction of inference rules on distributional similarity of words rather than word senses, and apply unconditionally whenever one side of the rule matches on the word level, which may lead to considerable precision problems (Geffet and Dagan, 2005) . Some approaches address the problem of context sensitivity by deriving inference rules whose argument slots bear selectional preference information (Pantel et al., 2007; Basili et al., 2007). A different line of accounting for contextual variation has been taken by Mitchell and Lapata (2008), who propose a compositional approach, “contextualizing” the vector-space meaning representation of predicates by combining the distributional properties of the predicate with those of its arguments. A related approach has been proposed by Erk and Padó (2008), who integrate selectional preferences into the compositional picture. In this paper, we propose a context-sensitive vector-space approach which draws some important ideas from Erk and Pado’s paper (“E&amp;P” in the following), but implements them in a different, more effective way: An evaluation on the SemEval 2007 lexical substitution task</context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>J. Mitchell and M. Lapata. 2008. Vector-based models of semantic composition. In Proc. of ACL-08: HLT, Columbus.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pantel</author>
<author>R Bhagat</author>
<author>B Coppola</author>
<author>T Chklovski</author>
<author>E Hovy</author>
</authors>
<title>ISP: Learning inferential selectional preferences.</title>
<date>2007</date>
<booktitle>In Human Language Technologies 2007,</booktitle>
<location>Rochester.</location>
<contexts>
<context position="2127" citStr="Pantel et al., 2007" startWordPosition="316" endWordPosition="319">ds light on the mixed results,” but not in sentences like “the economy seems to be shedding fewer jobs” or “cats do not shed the virus to other cats.” Systems like the above-mentioned ones base the extraction of inference rules on distributional similarity of words rather than word senses, and apply unconditionally whenever one side of the rule matches on the word level, which may lead to considerable precision problems (Geffet and Dagan, 2005) . Some approaches address the problem of context sensitivity by deriving inference rules whose argument slots bear selectional preference information (Pantel et al., 2007; Basili et al., 2007). A different line of accounting for contextual variation has been taken by Mitchell and Lapata (2008), who propose a compositional approach, “contextualizing” the vector-space meaning representation of predicates by combining the distributional properties of the predicate with those of its arguments. A related approach has been proposed by Erk and Padó (2008), who integrate selectional preferences into the compositional picture. In this paper, we propose a context-sensitive vector-space approach which draws some important ideas from Erk and Pado’s paper (“E&amp;P” in the fol</context>
</contexts>
<marker>Pantel, Bhagat, Coppola, Chklovski, Hovy, 2007</marker>
<rawString>P. Pantel, R. Bhagat, B. Coppola, T. Chklovski, and E. Hovy. 2007. ISP: Learning inferential selectional preferences. In Human Language Technologies 2007, Rochester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Szpektor</author>
<author>H Tanev</author>
<author>I Dagan</author>
<author>B Coppola</author>
</authors>
<title>Scaling web-based acquisition of entailment relations.</title>
<date>2004</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<location>Barcellona.</location>
<contexts>
<context position="844" citStr="Szpektor et al. (2004)" startWordPosition="116" endWordPosition="119">ract We present a vector space model that supports the computation of appropriate vector representations for words in context, and apply it to a paraphrase ranking task. An evaluation on the SemEval 2007 lexical substitution task data shows promising results: the model significantly outperforms a current state of the art model, and our treatment of context is effective. 1 Introduction Knowledge about paraphrases is of central importance to textual inference modeling. Systems which support automatic extraction of large repositories of paraphrase or inference rules like Lin and Pantel (2001) or Szpektor et al. (2004) thus form first-class candidate resources to be leveraged for NLP tasks like question answering, information extraction, or summarization, and the meta-task of recognizing textual entailment. Existing knowledge bases still suffer a number of limitations, making their use in applications challenging. One of the most serious problems is insensitivity to context. Natural-language inference is highly context-sensitive, the applicability of inference rules depending on word sense and even finer grained contextual distinctions in usage (Szpektor et al., 2007). Application of a rule like “X shed Y q</context>
</contexts>
<marker>Szpektor, Tanev, Dagan, Coppola, 2004</marker>
<rawString>I. Szpektor, H. Tanev, I. Dagan, and B. Coppola. 2004. Scaling web-based acquisition of entailment relations. In Proc. of EMNLP, Barcellona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Szpektor</author>
<author>E Shnarch</author>
<author>I Dagan</author>
</authors>
<title>Instance-based evaluation of entailment rule acquisition.</title>
<date>2007</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="1404" citStr="Szpektor et al., 2007" startWordPosition="194" endWordPosition="197">nce rules like Lin and Pantel (2001) or Szpektor et al. (2004) thus form first-class candidate resources to be leveraged for NLP tasks like question answering, information extraction, or summarization, and the meta-task of recognizing textual entailment. Existing knowledge bases still suffer a number of limitations, making their use in applications challenging. One of the most serious problems is insensitivity to context. Natural-language inference is highly context-sensitive, the applicability of inference rules depending on word sense and even finer grained contextual distinctions in usage (Szpektor et al., 2007). Application of a rule like “X shed Y q X throw Y” is appropriate in a sentence like “a mouse study sheds light on the mixed results,” but not in sentences like “the economy seems to be shedding fewer jobs” or “cats do not shed the virus to other cats.” Systems like the above-mentioned ones base the extraction of inference rules on distributional similarity of words rather than word senses, and apply unconditionally whenever one side of the rule matches on the word level, which may lead to considerable precision problems (Geffet and Dagan, 2005) . Some approaches address the problem of contex</context>
</contexts>
<marker>Szpektor, Shnarch, Dagan, 2007</marker>
<rawString>I. Szpektor, E. Shnarch, and I. Dagan. 2007. Instance-based evaluation of entailment rule acquisition. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Yeh</author>
</authors>
<title>More accurate tests for the statistical significance of result differences.</title>
<date>2000</date>
<booktitle>In Proc. of COLING.</booktitle>
<contexts>
<context position="13803" citStr="Yeh, 2000" startWordPosition="2282" endWordPosition="2283">ferent datasets or lists of stop-words. As can be seen, our model performs &gt; 10% better than the random baseline. It performs &gt; 4% better than the “min” E&amp;P model and &gt; 6% better then the “add” model in terms of GAP if we use a vectors space with words as basis. For the variants of the E&amp;P models in which the basis corresponds to words indexed by their syntactic role, we obtain different results, but our model is still &gt; 4% better than these variants. We can also see that our treatment of context is effective, leading to a &gt; 3% increase of GAP. A stratified shuffling-based randomization test (Yeh, 2000) shows that the differences are statistically significant (p &lt; 0.05). In terms of Poot, the “add” E&amp;P model performs better than our model, which might look surprising, given its low GAP score. Fig. 1 gives a more finegrained comparison between the two models. It displays the “precision out of n” of the two models for varying n. As can be seen, our model performs better for all n &lt; 10, and much better than the baseline and E&amp;P for n &lt; 4. 4 Conclusion In this paper, we have proposed a dependencybased context-sensitive vector-space approach that supports the computation of adequate vector-based </context>
</contexts>
<marker>Yeh, 2000</marker>
<rawString>A. Yeh. 2000. More accurate tests for the statistical significance of result differences. In Proc. of COLING.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>