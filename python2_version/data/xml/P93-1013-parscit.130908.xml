<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.313449">
<title confidence="0.745069">
PLANNING MULTIMODAL DISCOURSE
</title>
<author confidence="0.956867">
Wolfgang Wahlster
</author>
<affiliation confidence="0.85793">
German Research Center for Artificial Intelligence (DFKI)
</affiliation>
<address confidence="0.727051">
Stuhlsatzenhausweg 3
D-6600 Saarbriicken 11, Germany
</address>
<email confidence="0.628624">
Internet: wahlsterÂ©dfki.uni-sb.de
</email>
<sectionHeader confidence="0.990003" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.996053285714286">
In this talk, we will,show how techniques for plan-
ning text and discourse can be generalized to plan
the structure and content of multimodal communi-
cations, that integrate natural language, pointing,
graphics, and animations. The central claim of
this talk is that the generation of multimodal dis-
course can be considered as an incremental plan-
ning process that aims to achieve a given commu-
nicative goal.
One of the surprises from our research is that
it is actually possible to extend and adapt many
of the fundamental concepts developed to date
in computatational linguistics in such a way that
they become useful for multimodal discourse as
well. This means that an interesting methodologi-
cal transfer from the area of natural language pro-
cessing to a much broader computational model of
multimodal communication is possible. In partic-
ular, semantic and pragmatic concepts like speech
acts, coherence, focus, communicative act, dis-
course model, reference, implicature, anaphora,
rhetorical relations and scope ambiguity take an
extended meaning in the context of multimodal
discourse.
It is an important goal of this research not
simply to merge the verbalization and visualiza-
tion results of mode-specific generators, but to
carefully coordinate them in such a way that they
generate a multiplicative improvement in commu-
nication capabilities. Allowing all of the modali-
ties to refer to and depend upon each other is a
key to the richness of multimodal communication.
A basic principle underlying our model is that
the various constituents of a multimodal commu-
nication should be generated from a common rep-
resentation of what is to be conveyed. This raises
the question of how to decompose a given com-
municative goal into subgoals to be realized by
the mode-specific generators, so that they com-
plement each other. To address this problem, we
explore computational models of the cognitive de-
cision process, coping with questions such as what
should go into text, what should go into graphics,
and which kinds of links between the verbal and
non-verbal fragments are necessary. In addition,
we deal with layout as a rhetorical force, influ-
encing the intentional and attentional state of the
discourse participants.
We have been engaged in work in the area of
multimodal communication for several years now,
starting with the HAM-ANS (Wahlster et al. 1983)
and VITRA systems (Wahlster 1989), which auto-
matically create natural language descriptions of
pictures and image sequences shown on the screen.
These projects resulted in a better understanding
of how perception interacts with language produc-
tion. Since then, we have been investigating ways
of integrating tactile pointing and graphics with
natural language understanding and generation
in the XTRA (Wahlster 1991) and WIP projects
(Wahlster et al. 1991).
The task of the knowledge-based presentation
system WIP is the context-sensitive generation of
a variety of multimodal communications from an
input including a presentation goal (Wahlster et
al. 1993a). The presentation goal is a formal repre-
sentation of the communicative intent specified by
a back-end application system. WIP is currently
able to generate simple multimodal explanations
in German and English on using an espresso ma-
chine, assembling a lawn-mower, or installing a
modem, demonstrating our claim of language and
application independence. WIP is a highly adap-
tive multimodal presentation system, since all of
its output is generated on the fly and customized
for the intended discourse situation. The quest for
adaptation is based on the fact that it is impos-
sible to anticipate the needs and requirements of
each potential dialog partner in an infinite number
of discourse situations. Thus all presentation deci-
sions are postponed until runtime. In contrast to
hypermedia-based approaches, WIP does not use
any preplanned texts or graphics. That is, each
presentation is designed from scratch by reasoning
</bodyText>
<page confidence="0.995088">
95
</page>
<bodyText confidence="0.999919514705882">
from first principles using commonsense presenta-
tion knowledge.
We approximate the fact that multimodal
communication is always situated by introducing
seven discourse parameters in our model. The cur-
rent system includes a choice between user stereo-
types (e.g. novice, expert), target languages (Ger-
man vs. English), layout formats (e.g. paper hard-
copy, slide, screen display), output modes (incre-
mental output vs. complete output only), pre-
ferred mode (e.g. text, graphics, or no prefer-
ence), and binary switches for space restrictions
and speech output. This set of parameters is used
to specify design constraints that must be satisfied
by the final presentation. The combinatorics of
WIP&apos;s contextual parameters can generate 576 al-
ternate multimodal presentations of the same con-
tent.
At the heart of the multimodal presentation
system WIP is a parallel top-down planner (Andre
and Rist 1993) and a constraint-based layout man-
ager. While the root of the hierarchical plan struc-
ture for a particular multimodal communication
corresponds to a complex communicative act such
as describing a process, the leaves are elementary
acts that verbalize and visualize information spec-
ified in the input from the back-end application
system.
In multimodal generation systems, three dif-
ferent processes are distinguished: a content plan-
ning process, a mode selection process and a con-
tent realization process. A sequential architec-
ture in which data only flow from the &amp;quot;what to
present&amp;quot; to the &amp;quot;how to present&amp;quot; part has proven
inappropriate because the components responsible
for selecting the contents would have to anticipate
all decisions of the realization components. This
problem is compounded if content realization is
done by separate components (e.g. for language,
pointing, graphics and animations) of which the
content planner has only limited knowledge.
It seems even inappropriate to sequentialize
content planning and mode selection. Selecting a
mode of presentation depends to a large extent on
the nature of the information to be conveyed.
On the other hand, content planning is
strongly influenced by previously selected mode
combinations. E.g., to graphically refer to a phys-
ical object (Rist and Andre 1992), we need visual
information that may be irrelevant to textual ref-
erences. In the WIP system, we interleave content
and mode selection. In contrast to this, presen-
tation planning and content realization are per-
formed by separate components to enable parallel
processing (Wahlster et al. 1993b).
In a follow-up project to WIP called PPP
(Personalized Plan-Based Presenter), we are cur-
rently addressing the additional problem of plan-
ning presentation acts such as pointing and coor-
dinated speech output during the display of the
multimodal material synthesized by WIP.
The insights and experience we gained from
the design and implementation of the multimodal
systems HAM-ANS, VITRA, XTRA and WIP
provide a good starting point for a deeper un-
derstanding of the interdependencies of language,
graphics, pointing, and animations in coordinated
multimodal discourse.
</bodyText>
<sectionHeader confidence="0.999699" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.999600333333333">
Andre, Elisabeth; and Rist, Thomas. 1993. The
Design of Illustrated Documents as a Planning
Task. Maybury, Mark (ed.). Intelligent Multime-
dia Interfaces, AAAI Press (to appear).
Rist, Thomas; and Andre, Elisabeth. 1992.
From Presentation Tasks to Pictures: Towards an
Approach to Automatic Graphics Design. Pro-
ceedings European Conference on Al (ECAI-92),
Vienna, Austria (1992) 764-768.
Wahlster, Wolfgang. 1989. One Word Says
more than a Thousand Pictures. On the Auto-
matic Verbalization of the Results of Image Se-
quence Analysis Systems. Computers and Artifi-
cial Intelligence, 8, 5: 479-492
Wahlster, Wolfgang. 1991. User and Dis-
course Models for Multimodal Communication.
in: Sullivan, J.W.; and Tyler, S.W.(eds.). Intel-
ligent User Interfaces, Reading: Addison-Wesley
(1991): 45-67.
Wahlster, Wolfgang; Marburger, Heinz; Jame-
son, Anthony; Busemann, Stephan. 1983. Over-
answering Yes-No Questions: Extended Responses
in a NL Interface to a Vision System. Proceedings
of IJCAI-83, Karlsruhe: 643-646.
Wahlster, Wolfgang; Andre, Elisabeth; Graf,
Winfried; and Rist, Thomas. 1991. Designing Il-
lustrated Texts: How Language Production is In-
fluenced by Graphics Generation. Proceedings Eu-
ropean ACL Conference, Berlin, Germany: 8-14.
Wahlster, Wolfgang; Andre, Elisabeth; Ban-
dyopadhyay, Som; Graf, Winfried; and Rist,
Thomas. 1993a. WIP: The Coordinated Gener-
ation of Multimodal Presentations from a Com-
mon Representation, in: Ortony, A.; Slack, J.; and
Stock, 0.(eds.). Communication from an Artifi-
cial Intelligence Perspective: Theoretical and Ap-
plied Issues, Springer: Heidelberg: 121-144.
Wahlster, Wolfgang; Andre, Elisabeth; Fin-
kler, Wolfgang; Profitlich, Hans-Jiirgen; and Rist,
Thomas. 1993b. Plan-Based Integration of Natu-
ral Language and Graphics Generation. Artificial
Intelligence Journal 26(3), (to appear).
</reference>
<page confidence="0.99845">
96
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.008821">
<title confidence="0.762884">PLANNING MULTIMODAL DISCOURSE Wolfgang Wahlster German Research Center for Artificial Intelligence (DFKI)</title>
<address confidence="0.810342">Stuhlsatzenhausweg 3 D-6600 Saarbriicken 11, Germany</address>
<email confidence="0.965121">Internet:wahlsterÂ©dfki.uni-sb.de</email>
<abstract confidence="0.999773383116883">In this talk, we will,show how techniques for planning text and discourse can be generalized to plan the structure and content of multimodal communications, that integrate natural language, pointing, graphics, and animations. The central claim of this talk is that the generation of multimodal discourse can be considered as an incremental planning process that aims to achieve a given communicative goal. One of the surprises from our research is that it is actually possible to extend and adapt many of the fundamental concepts developed to date in computatational linguistics in such a way that they become useful for multimodal discourse as well. This means that an interesting methodological transfer from the area of natural language processing to a much broader computational model of multimodal communication is possible. In particular, semantic and pragmatic concepts like speech acts, coherence, focus, communicative act, discourse model, reference, implicature, anaphora, rhetorical relations and scope ambiguity take an extended meaning in the context of multimodal discourse. It is an important goal of this research not simply to merge the verbalization and visualization results of mode-specific generators, but to carefully coordinate them in such a way that they generate a multiplicative improvement in communication capabilities. Allowing all of the modalities to refer to and depend upon each other is a key to the richness of multimodal communication. A basic principle underlying our model is that the various constituents of a multimodal communication should be generated from a common representation of what is to be conveyed. This raises the question of how to decompose a given communicative goal into subgoals to be realized by the mode-specific generators, so that they complement each other. To address this problem, we explore computational models of the cognitive decision process, coping with questions such as what should go into text, what should go into graphics, and which kinds of links between the verbal and non-verbal fragments are necessary. In addition, we deal with layout as a rhetorical force, influencing the intentional and attentional state of the discourse participants. We have been engaged in work in the area of multimodal communication for several years now, starting with the HAM-ANS (Wahlster et al. 1983) and VITRA systems (Wahlster 1989), which automatically create natural language descriptions of pictures and image sequences shown on the screen. These projects resulted in a better understanding of how perception interacts with language production. Since then, we have been investigating ways of integrating tactile pointing and graphics with natural language understanding and generation in the XTRA (Wahlster 1991) and WIP projects (Wahlster et al. 1991). The task of the knowledge-based presentation system WIP is the context-sensitive generation of a variety of multimodal communications from an input including a presentation goal (Wahlster et al. 1993a). The presentation goal is a formal representation of the communicative intent specified by a back-end application system. WIP is currently able to generate simple multimodal explanations in German and English on using an espresso machine, assembling a lawn-mower, or installing a modem, demonstrating our claim of language and application independence. WIP is a highly adaptive multimodal presentation system, since all of its output is generated on the fly and customized for the intended discourse situation. The quest for adaptation is based on the fact that it is impossible to anticipate the needs and requirements of each potential dialog partner in an infinite number of discourse situations. Thus all presentation decisions are postponed until runtime. In contrast to hypermedia-based approaches, WIP does not use any preplanned texts or graphics. That is, each presentation is designed from scratch by reasoning 95 from first principles using commonsense presentation knowledge. We approximate the fact that multimodal communication is always situated by introducing seven discourse parameters in our model. The current system includes a choice between user stereotypes (e.g. novice, expert), target languages (German vs. English), layout formats (e.g. paper hardcopy, slide, screen display), output modes (incremental output vs. complete output only), preferred mode (e.g. text, graphics, or no preference), and binary switches for space restrictions and speech output. This set of parameters is used to specify design constraints that must be satisfied by the final presentation. The combinatorics of WIP&apos;s contextual parameters can generate 576 alternate multimodal presentations of the same content. At the heart of the multimodal presentation system WIP is a parallel top-down planner (Andre and Rist 1993) and a constraint-based layout manager. While the root of the hierarchical plan structure for a particular multimodal communication corresponds to a complex communicative act such as describing a process, the leaves are elementary acts that verbalize and visualize information specified in the input from the back-end application system. In multimodal generation systems, three different processes are distinguished: a content planning process, a mode selection process and a content realization process. A sequential architecture in which data only flow from the &amp;quot;what to present&amp;quot; to the &amp;quot;how to present&amp;quot; part has proven inappropriate because the components responsible for selecting the contents would have to anticipate all decisions of the realization components. This problem is compounded if content realization is done by separate components (e.g. for language, pointing, graphics and animations) of which the content planner has only limited knowledge. It seems even inappropriate to sequentialize content planning and mode selection. Selecting a mode of presentation depends to a large extent on the nature of the information to be conveyed. On the other hand, content planning is strongly influenced by previously selected mode combinations. E.g., to graphically refer to a physical object (Rist and Andre 1992), we need visual information that may be irrelevant to textual references. In the WIP system, we interleave content and mode selection. In contrast to this, presentation planning and content realization are performed by separate components to enable parallel processing (Wahlster et al. 1993b). In a follow-up project to WIP called PPP (Personalized Plan-Based Presenter), we are currently addressing the additional problem of planning presentation acts such as pointing and coordinated speech output during the display of the multimodal material synthesized by WIP. The insights and experience we gained from the design and implementation of the multimodal systems HAM-ANS, VITRA, XTRA and WIP provide a good starting point for a deeper understanding of the interdependencies of language, graphics, pointing, and animations in coordinated multimodal discourse.</abstract>
<note confidence="0.870225953488372">REFERENCES Andre, Elisabeth; and Rist, Thomas. 1993. The Design of Illustrated Documents as a Planning Maybury, Mark (ed.). Multime- Interfaces, Press (to appear). Rist, Thomas; and Andre, Elisabeth. 1992. From Presentation Tasks to Pictures: Towards an Approach to Automatic Graphics Design. Proceedings European Conference on Al (ECAI-92), Vienna, Austria (1992) 764-768. Wahlster, Wolfgang. 1989. One Word Says more than a Thousand Pictures. On the Automatic Verbalization of the Results of Image Se- Analysis Systems. and Artifi- Intelligence, 5: 479-492 Wahlster, Wolfgang. 1991. User and Discourse Models for Multimodal Communication. Sullivan, J.W.; and Tyler, S.W.(eds.). Intel- User Interfaces, Addison-Wesley (1991): 45-67. Wahlster, Wolfgang; Marburger, Heinz; Jameson, Anthony; Busemann, Stephan. 1983. Overanswering Yes-No Questions: Extended Responses a NL Interface to a Vision System. IJCAI-83, 643-646. Wahlster, Wolfgang; Andre, Elisabeth; Graf, Winfried; and Rist, Thomas. 1991. Designing Illustrated Texts: How Language Production is Inby Graphics Generation. Eu- ACL Conference, Germany: 8-14. Wahlster, Wolfgang; Andre, Elisabeth; Bandyopadhyay, Som; Graf, Winfried; and Rist, Thomas. 1993a. WIP: The Coordinated Generation of Multimodal Presentations from a Common Representation, in: Ortony, A.; Slack, J.; and 0.(eds.). from an Artificial Intelligence Perspective: Theoretical and Ap- Issues, Heidelberg: 121-144. Wahlster, Wolfgang; Andre, Elisabeth; Finkler, Wolfgang; Profitlich, Hans-Jiirgen; and Rist, Thomas. 1993b. Plan-Based Integration of Natu- Language and Graphics Generation. Journal (to appear).</note>
<intro confidence="0.483374">96</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Elisabeth Andre</author>
<author>Thomas Rist</author>
</authors>
<title>The Design of Illustrated Documents as a Planning Task.</title>
<date>1993</date>
<booktitle>Intelligent Multimedia Interfaces,</booktitle>
<editor>Maybury, Mark (ed.).</editor>
<publisher>AAAI Press</publisher>
<note>(to appear).</note>
<contexts>
<context position="5043" citStr="Andre and Rist 1993" startWordPosition="772" endWordPosition="775">, target languages (German vs. English), layout formats (e.g. paper hardcopy, slide, screen display), output modes (incremental output vs. complete output only), preferred mode (e.g. text, graphics, or no preference), and binary switches for space restrictions and speech output. This set of parameters is used to specify design constraints that must be satisfied by the final presentation. The combinatorics of WIP&apos;s contextual parameters can generate 576 alternate multimodal presentations of the same content. At the heart of the multimodal presentation system WIP is a parallel top-down planner (Andre and Rist 1993) and a constraint-based layout manager. While the root of the hierarchical plan structure for a particular multimodal communication corresponds to a complex communicative act such as describing a process, the leaves are elementary acts that verbalize and visualize information specified in the input from the back-end application system. In multimodal generation systems, three different processes are distinguished: a content planning process, a mode selection process and a content realization process. A sequential architecture in which data only flow from the &amp;quot;what to present&amp;quot; to the &amp;quot;how to pre</context>
</contexts>
<marker>Andre, Rist, 1993</marker>
<rawString>Andre, Elisabeth; and Rist, Thomas. 1993. The Design of Illustrated Documents as a Planning Task. Maybury, Mark (ed.). Intelligent Multimedia Interfaces, AAAI Press (to appear).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Rist</author>
<author>Elisabeth Andre</author>
</authors>
<title>From Presentation Tasks to Pictures: Towards an Approach to Automatic Graphics Design.</title>
<date>1992</date>
<booktitle>Proceedings European Conference on Al (ECAI-92),</booktitle>
<pages>764--768</pages>
<location>Vienna, Austria</location>
<contexts>
<context position="6364" citStr="Rist and Andre 1992" startWordPosition="974" endWordPosition="977">ld have to anticipate all decisions of the realization components. This problem is compounded if content realization is done by separate components (e.g. for language, pointing, graphics and animations) of which the content planner has only limited knowledge. It seems even inappropriate to sequentialize content planning and mode selection. Selecting a mode of presentation depends to a large extent on the nature of the information to be conveyed. On the other hand, content planning is strongly influenced by previously selected mode combinations. E.g., to graphically refer to a physical object (Rist and Andre 1992), we need visual information that may be irrelevant to textual references. In the WIP system, we interleave content and mode selection. In contrast to this, presentation planning and content realization are performed by separate components to enable parallel processing (Wahlster et al. 1993b). In a follow-up project to WIP called PPP (Personalized Plan-Based Presenter), we are currently addressing the additional problem of planning presentation acts such as pointing and coordinated speech output during the display of the multimodal material synthesized by WIP. The insights and experience we ga</context>
</contexts>
<marker>Rist, Andre, 1992</marker>
<rawString>Rist, Thomas; and Andre, Elisabeth. 1992. From Presentation Tasks to Pictures: Towards an Approach to Automatic Graphics Design. Proceedings European Conference on Al (ECAI-92), Vienna, Austria (1992) 764-768.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Wahlster</author>
</authors>
<title>One Word Says more than a Thousand Pictures. On the Automatic Verbalization of the Results of Image Sequence Analysis Systems.</title>
<date>1989</date>
<journal>Computers and Artificial Intelligence,</journal>
<volume>8</volume>
<pages>479--492</pages>
<contexts>
<context position="2596" citStr="Wahlster 1989" startWordPosition="402" endWordPosition="403">ors, so that they complement each other. To address this problem, we explore computational models of the cognitive decision process, coping with questions such as what should go into text, what should go into graphics, and which kinds of links between the verbal and non-verbal fragments are necessary. In addition, we deal with layout as a rhetorical force, influencing the intentional and attentional state of the discourse participants. We have been engaged in work in the area of multimodal communication for several years now, starting with the HAM-ANS (Wahlster et al. 1983) and VITRA systems (Wahlster 1989), which automatically create natural language descriptions of pictures and image sequences shown on the screen. These projects resulted in a better understanding of how perception interacts with language production. Since then, we have been investigating ways of integrating tactile pointing and graphics with natural language understanding and generation in the XTRA (Wahlster 1991) and WIP projects (Wahlster et al. 1991). The task of the knowledge-based presentation system WIP is the context-sensitive generation of a variety of multimodal communications from an input including a presentation go</context>
</contexts>
<marker>Wahlster, 1989</marker>
<rawString>Wahlster, Wolfgang. 1989. One Word Says more than a Thousand Pictures. On the Automatic Verbalization of the Results of Image Sequence Analysis Systems. Computers and Artificial Intelligence, 8, 5: 479-492</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Wahlster</author>
</authors>
<title>User and Discourse Models for Multimodal Communication.</title>
<date>1991</date>
<booktitle>Intelligent User Interfaces,</booktitle>
<pages>45--67</pages>
<editor>in: Sullivan, J.W.; and Tyler, S.W.(eds.).</editor>
<publisher>Addison-Wesley</publisher>
<location>Reading:</location>
<contexts>
<context position="2979" citStr="Wahlster 1991" startWordPosition="457" endWordPosition="458">nal and attentional state of the discourse participants. We have been engaged in work in the area of multimodal communication for several years now, starting with the HAM-ANS (Wahlster et al. 1983) and VITRA systems (Wahlster 1989), which automatically create natural language descriptions of pictures and image sequences shown on the screen. These projects resulted in a better understanding of how perception interacts with language production. Since then, we have been investigating ways of integrating tactile pointing and graphics with natural language understanding and generation in the XTRA (Wahlster 1991) and WIP projects (Wahlster et al. 1991). The task of the knowledge-based presentation system WIP is the context-sensitive generation of a variety of multimodal communications from an input including a presentation goal (Wahlster et al. 1993a). The presentation goal is a formal representation of the communicative intent specified by a back-end application system. WIP is currently able to generate simple multimodal explanations in German and English on using an espresso machine, assembling a lawn-mower, or installing a modem, demonstrating our claim of language and application independence. WIP</context>
</contexts>
<marker>Wahlster, 1991</marker>
<rawString>Wahlster, Wolfgang. 1991. User and Discourse Models for Multimodal Communication. in: Sullivan, J.W.; and Tyler, S.W.(eds.). Intelligent User Interfaces, Reading: Addison-Wesley (1991): 45-67.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Wahlster</author>
<author>Heinz Marburger</author>
<author>Anthony Jameson</author>
<author>Stephan Busemann</author>
</authors>
<title>Overanswering Yes-No Questions: Extended Responses in a NL Interface to a Vision System.</title>
<date>1983</date>
<booktitle>Proceedings of IJCAI-83,</booktitle>
<pages>643--646</pages>
<location>Karlsruhe:</location>
<contexts>
<context position="2562" citStr="Wahlster et al. 1983" startWordPosition="395" endWordPosition="398"> be realized by the mode-specific generators, so that they complement each other. To address this problem, we explore computational models of the cognitive decision process, coping with questions such as what should go into text, what should go into graphics, and which kinds of links between the verbal and non-verbal fragments are necessary. In addition, we deal with layout as a rhetorical force, influencing the intentional and attentional state of the discourse participants. We have been engaged in work in the area of multimodal communication for several years now, starting with the HAM-ANS (Wahlster et al. 1983) and VITRA systems (Wahlster 1989), which automatically create natural language descriptions of pictures and image sequences shown on the screen. These projects resulted in a better understanding of how perception interacts with language production. Since then, we have been investigating ways of integrating tactile pointing and graphics with natural language understanding and generation in the XTRA (Wahlster 1991) and WIP projects (Wahlster et al. 1991). The task of the knowledge-based presentation system WIP is the context-sensitive generation of a variety of multimodal communications from an</context>
</contexts>
<marker>Wahlster, Marburger, Jameson, Busemann, 1983</marker>
<rawString>Wahlster, Wolfgang; Marburger, Heinz; Jameson, Anthony; Busemann, Stephan. 1983. Overanswering Yes-No Questions: Extended Responses in a NL Interface to a Vision System. Proceedings of IJCAI-83, Karlsruhe: 643-646.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Wahlster</author>
<author>Elisabeth Andre</author>
<author>Winfried Graf</author>
<author>Thomas Rist</author>
</authors>
<title>Designing Illustrated Texts: How Language Production is Influenced by Graphics Generation.</title>
<date>1991</date>
<booktitle>Proceedings European ACL Conference,</booktitle>
<pages>8--14</pages>
<location>Berlin, Germany:</location>
<contexts>
<context position="3019" citStr="Wahlster et al. 1991" startWordPosition="462" endWordPosition="465">discourse participants. We have been engaged in work in the area of multimodal communication for several years now, starting with the HAM-ANS (Wahlster et al. 1983) and VITRA systems (Wahlster 1989), which automatically create natural language descriptions of pictures and image sequences shown on the screen. These projects resulted in a better understanding of how perception interacts with language production. Since then, we have been investigating ways of integrating tactile pointing and graphics with natural language understanding and generation in the XTRA (Wahlster 1991) and WIP projects (Wahlster et al. 1991). The task of the knowledge-based presentation system WIP is the context-sensitive generation of a variety of multimodal communications from an input including a presentation goal (Wahlster et al. 1993a). The presentation goal is a formal representation of the communicative intent specified by a back-end application system. WIP is currently able to generate simple multimodal explanations in German and English on using an espresso machine, assembling a lawn-mower, or installing a modem, demonstrating our claim of language and application independence. WIP is a highly adaptive multimodal present</context>
</contexts>
<marker>Wahlster, Andre, Graf, Rist, 1991</marker>
<rawString>Wahlster, Wolfgang; Andre, Elisabeth; Graf, Winfried; and Rist, Thomas. 1991. Designing Illustrated Texts: How Language Production is Influenced by Graphics Generation. Proceedings European ACL Conference, Berlin, Germany: 8-14.</rawString>
</citation>
<citation valid="false">
<title>The Coordinated Generation of Multimodal Presentations from a Common Representation,</title>
<booktitle>Stock, 0.(eds.). Communication from an Artificial Intelligence Perspective: Theoretical and Applied Issues,</booktitle>
<pages>121--144</pages>
<editor>Wahlster, Wolfgang; Andre, Elisabeth; Bandyopadhyay, Som; Graf, Winfried; and Rist, Thomas. 1993a. WIP:</editor>
<publisher>Springer:</publisher>
<location>Heidelberg:</location>
<marker></marker>
<rawString>Wahlster, Wolfgang; Andre, Elisabeth; Bandyopadhyay, Som; Graf, Winfried; and Rist, Thomas. 1993a. WIP: The Coordinated Generation of Multimodal Presentations from a Common Representation, in: Ortony, A.; Slack, J.; and Stock, 0.(eds.). Communication from an Artificial Intelligence Perspective: Theoretical and Applied Issues, Springer: Heidelberg: 121-144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Wahlster</author>
<author>Elisabeth Andre</author>
<author>Wolfgang Finkler</author>
<author>Hans-Jiirgen Profitlich</author>
<author>Thomas Rist</author>
</authors>
<date>1993</date>
<journal>Plan-Based Integration of Natural Language and Graphics Generation. Artificial Intelligence Journal</journal>
<volume>26</volume>
<issue>3</issue>
<note>(to appear).</note>
<contexts>
<context position="3220" citStr="Wahlster et al. 1993" startWordPosition="491" endWordPosition="494">ich automatically create natural language descriptions of pictures and image sequences shown on the screen. These projects resulted in a better understanding of how perception interacts with language production. Since then, we have been investigating ways of integrating tactile pointing and graphics with natural language understanding and generation in the XTRA (Wahlster 1991) and WIP projects (Wahlster et al. 1991). The task of the knowledge-based presentation system WIP is the context-sensitive generation of a variety of multimodal communications from an input including a presentation goal (Wahlster et al. 1993a). The presentation goal is a formal representation of the communicative intent specified by a back-end application system. WIP is currently able to generate simple multimodal explanations in German and English on using an espresso machine, assembling a lawn-mower, or installing a modem, demonstrating our claim of language and application independence. WIP is a highly adaptive multimodal presentation system, since all of its output is generated on the fly and customized for the intended discourse situation. The quest for adaptation is based on the fact that it is impossible to anticipate the </context>
</contexts>
<marker>Wahlster, Andre, Finkler, Profitlich, Rist, 1993</marker>
<rawString>Wahlster, Wolfgang; Andre, Elisabeth; Finkler, Wolfgang; Profitlich, Hans-Jiirgen; and Rist, Thomas. 1993b. Plan-Based Integration of Natural Language and Graphics Generation. Artificial Intelligence Journal 26(3), (to appear).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>