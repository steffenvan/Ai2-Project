<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.012646">
<note confidence="0.816937">
SENSEVAL-3: Third International Workshop on the Evaluation of Systems
for the Semantic Analysis of Text, Barcelona, Spain, July 2004
</note>
<subsectionHeader confidence="0.775076">
Association for Computational Linguistics
</subsectionHeader>
<bodyText confidence="0.994346777777778">
the current content of the MEANING3 Multilin-
gual Central Repository (Mcn) (Atserias et al.,
2004) (i.e. SUMO, WordNet Domains, etc.).
Given a word from a gloss, each heuristic
votes for different synsets.
The program simply adds up the votes for
each word sense, selecting the most voted sense.
We have presented two different systems us-
ing two different preprocess.
</bodyText>
<listItem confidence="0.9996345">
• PRE-XWN (XWN preprocess) uses the
gloss segmentation and PoS tagging pro-
vided by the XWN.
• In PRE-TALP (TALP preprocess) first, the
</listItem>
<bodyText confidence="0.810816333333333">
sentences are tokenized and then passed
on to a multiword identification module.
Then, the output containing the multi-
words is POS tagged using Eric Brill&apos;s tag-
ger (Brill, 1995). Tagged words and mul-
tiword expressions are lemmatized using
WN.
PRE-XWN 00256298n the restoration#Ms) of run-
down#a(g) urban#a(n) areas#n(n) by the mid-
dle#a class#n(n) resulting#v(n) in the displace-
ment#n of lower#a(n) - income#Ms) people#Mn)
PRE-TALP 00256298n the restoration#n of run-
down#v urban-areas#n by the middle-class#n
resulting#v in the displacement#n of lower#a -
income#n people#n )
</bodyText>
<tableCaption confidence="0.978639">
Table 1: PRE-XWN and PRE-TALP Example
</tableCaption>
<bodyText confidence="0.993467111111111">
Table 1 shows an example of the different be-
haviours of both preprocessing systems. PRE-
TALP recognizes the Multi Word Expression
urban-area and middle-class, while XWN split
them in several words. On the other hand, the
tagger did not recognize run-down as an adjec-
tive. Obviously, different segmentation and tag-
ging preprocessing causes different word counts
and scoring.
</bodyText>
<subsectionHeader confidence="0.919366">
2.1 Heuristics
</subsectionHeader>
<bodyText confidence="0.9561135">
The main heuristics used in the disambiguation
process are:
</bodyText>
<listItem confidence="0.994074">
1. Monosemous: Applying a closed—world
assumption, this heuristic identifies
monosemous words.
2. Most Frequent: Based on WN2.0 sense
frequencies, this heuristic only selects those
</listItem>
<footnote confidence="0.595284">
3http://www.lsi.upc.es/-meaning
</footnote>
<bodyText confidence="0.958152">
synsets having frequencies higher than the
85% of the most frequent senses.
</bodyText>
<listItem confidence="0.995116894736842">
3. Hypernym: This method follows the hy-
pernym chain looking for words appearing
in the gloss (e.g. the genus term).
4. WordNet Relations: This heuristic fol-
lows any synset relation looking for words
appearing in its gloss. The method does
not only use direct relations, but also per-
forms a chaining search following all rela-
tions and stopping at distance five.
5. MultiWordNet Domains (Magnini and
Cavagli, 2000): Having a synset with a par-
ticular WN Domain label, this method se-
lects those synsets from the words of the
gloss having the same Domain label.
6. Patterns: This method uses the &amp;quot;One
sense per collocation&amp;quot; heuristic (Yarowsky,
1993), implementing those patterns ap-
pearing in (Novischi, 2002).
7. Lexical Parallelism: This heuristic iden-
tifies the words with the same PoS sepa-
rated by comas or conjunctions and marks
them, when possible, with senses that be-
long to the same hierarchy.
8. SUMO (Niles and Pease, 2001): Having
a synset with a particular SUMO label,
this method selects those synsets from the
words of the gloss having the same SUMO
label.
9. Category: Having a synset being con-
nected to a particular WN CATEGORY,
this method selects those synsets from the
words of the gloss connected the same
CATEGORY.
10. Bigram: This heuristic uses high fre-
quency word sense pairs occurring in Sem-
Cor.
11. Sense One: Finally, this heuristic always
assigns the first WN sense.
</listItem>
<sectionHeader confidence="0.515983" genericHeader="abstract">
3 Test Data
</sectionHeader>
<bodyText confidence="0.999100142857143">
The test set consists of 15,179 gold assigments
from 9,257 glosses taked directly from XwN2.0-
1.1 (see table 2).
However, this version is not free of errors
and inconsistencies. For instance, XWN has 724
word tagged senses not belonging to WN. Three
of them labelled as gold.
</bodyText>
<table confidence="0.998064666666667">
POS words gold
Noun 35539 10985
Verb 2863 2105
Adj 370 263
Adv 3719 1826
Total 42491 15179
</table>
<tableCaption confidence="0.99725">
Table 2: Test Senseval3 (9257 gloss)
</tableCaption>
<bodyText confidence="0.999888333333333">
Furthermore, as we can see in table 3 the
synset distributions of the test data and WN2.0
are very different. In particular for adjective
and adverbs. Being the test data not represen-
tative of WN2.0, this test set misleads the global
results and the final goal of the task.
</bodyText>
<table confidence="0.999863571428571">
WordNet 2.0 Test Data
POS Gloss % Gloss %
Noun 79689 69.0 6706 72.4
Verb 13508 11.7 773 8.4
Adj 18563 16.0 94 1.0
Adv 3664 3.2 1684 18.2
Total 115424 100 9257 100
</table>
<tableCaption confidence="0.997258">
Table 3: Synset distributions of WN2.0 and Test
</tableCaption>
<sectionHeader confidence="0.881023" genericHeader="keywords">
4 Results
</sectionHeader>
<table confidence="0.9998524375">
PRE-XWN
Noun Verb Adj Adv Total
Correct 7788 1191 134 1246 10363
Attempted 10981 2105 263 1717 15102
Total 10985 2105 263 1826 15179
Precision 70.9% 56.6% 51.0% 72.6% 68.6%
Recall 70.9% 56.6% 51.0% 68.2% 68.3%
% Attemp. 100% 100% 100% 94.0% 99.5%
PRE-TALP
Noun Verb Adj Adv Total
Correct 6076 979 130 1260 8466
Attempted 9339 2000 253 1746 14757
Total 10985 2105 263 1826 15179
Precision 65.1% 48.9% 51.4% 72.2% 57.4%
Recall 55.3% 46.5% 49.4% 69.0% 55.8%
% Attemp. 85.0% 95.0% 96.2% 95.6% 97.2%
</table>
<tableCaption confidence="0.7501875">
Table 4: Results of PRE-XWN and
PRE-TALP
</tableCaption>
<bodyText confidence="0.999603375">
The final results of both TALP systems are
presented in table 4. Obviously, the perfor-
mance of PRE-TALP is lower than PRE-XWN
because it uses a different preprocess. The dif-
ference in the amount of words is due to the pre-
process (different tokenization, PoS tagging).
From the ten systems presented at the task,
PRE-XWN has obtained the first position of
recall (10,363 correct gold assigments, 68.3%)
and PRE-TALP the third position (8,466 cor-
rect gold assigments, 55.8%).
Table 5 presents the final results per heuris-
tic. As expected, each heuristic has different
behaviour of (P) precision and (R) recall. How-
ever, none of them has higher performance than
its combination.
</bodyText>
<table confidence="0.999634230769231">
PRE-XWN
Heuristic Corr Attem P R Attem
Monos 131 140 93.6% 0.9% 0.9%
MostFre 7741 13903 55.7% 51.0% 91.6%
Hyper 2003 2271 88.2% 13.2% 15.0%
Relations 5243 8054 65.1% 34.5% 53.1%
Domains 2873 4119 69.7% 18.9% 27.1%
Pattern 709 753 94.2% 4.7% 5.0%
LexPar 756 1360 55.6% 5.0% 9.0%
Sumo 2334 4181 55.8% 15.4% 27.5%
Category 38 64 59.4% 0.3% 0.4%
Bigram 1903 3305 57.6% 12.5% 21.8%
SenseOne 8338 15093 55.2% 54.9% 99.4%
PRE-TALP
Heuristic Corr Attem P R Attem
Monos 8 86 9.3% 0.1% 0.6%
MostFre 6061 12340 49.1% 39.9% 81.3%
Hyper 1845 2082 88.6% 12.2% 13.7%
Relations 4538 7443 61.0% 29.9% 49.0%
Domains 1856 3096 59.9% 12.2% 20.4%
Pattern 712 757 94.1% 4.7% 5.0%
LexPar 590 1143 51.6% 3.9% 7.5%
Sumo 2172 3942 55.1% 14.3% 26.0%
Category 34 62 54.8% 0.2% 0.4%
Bigram 1361 2692 50.6% 9.0% 17.7%
SenseOne 6538 13403 48.8% 43.1% 88.3%
</table>
<tableCaption confidence="0.998059">
Table 5: Per heuristic results
</tableCaption>
<sectionHeader confidence="0.964726" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999977692307692">
It is our belief, following (McRoy, 1992) and
(Rigau et al., 1997), that full-fledged lexical am-
biguity resolution should integrate several in-
formation sources and techniques. Our heuris-
tics used most of the information content coher-
ently integrated within the Multilingual Central
Repository (Mcn) of MEANING (Atserias et al.,
2004), one of the richest and largest multilingual
lexical knowledge base in existence.
In order to improve the current systems, we
plan to enrich the current set of heuristics using
other knowledge uploaded into the MCR with a
more robust preprocessing schema.
</bodyText>
<sectionHeader confidence="0.956801" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998157">
This research has been partially funded by the
European Commission (Meaning Project, 1ST-
2001-34460), and by the Spanish Research Min-
istry (Hermes Project: TIC2000-0335-0O3-02),
Generalitat de Catalunya (2002FI 00648) and
Universidad TecnolOgica Metropolitana (Chile).
</bodyText>
<sectionHeader confidence="0.946992" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.917806963414634">
E. Agirre, 0. Ansa, X. Arregi, J.M. Arriola,
A. Diaz de Ilarraza, E. Pociello, and L. Uria.
2002. Methodological issues in the building of
the basque wordnet: quantitative and quali-
tative analysis. In Proceedings of the first In-
ternational WordNet Conference in Mysore,
India, 21-25 January.
J. Atserias, S. Climent, X. Farreres, G. Rigau,
and H. Rodriguez. 1997. Combining multi-
ple methods for the automatic construction
of multilingual wordnets. In Proceedings of
RANLP&apos;97, pages 143-149, Bulgaria.
Jordi Atserias, Luis Villarejo, German Rigau,
Eneko Agirre, John Carroll, Bernardo
Magnini, and Piek Vossen. 2004. The
meaning multilingual central repository.
In Proceedings of the Second International
Global WordNet Conference (GWC&apos;04),
Brno, Czech Republic, January. ISBN
80-210-3302-9.
E. Brill. 1995. Transformation-based error-
driven learning and natural language process-
ing: A case study in part of speech tagging.
Computational Linguistics, 21(4).
C. Fellbaum, editor. 1998. WordNet. An Elec-
tronic Lexical Database. The MIT Press.
A. Gangemi, R. Navigli, and P. Velardi. 2003.
Axiomatizing wordnet glosses in the on-
towordnet project. In Proceedings of 2nd In-
ternational Semantic Web Conference Work-
shop on Human Language Technology for the
Semantic Web and Web Services, Sanibel Is-
land, Florida.
S. Harabagiu, G. Miller, and D. Moldovan.
1999. Wordnet 2 - a morphologically and se-
mantically enhanced resource. In Proceedings
of ACL on Standardizing Lexical Resources
(SIGLEX&apos;99), Maryland, MD.
B. Magnini and G. Cavagli. 2000. Integrating
subject field codes into wordnet. In In Pro-
ceedings of the Second Internatgional Confer-
ence on Language Resources and Evaluation
LREC&apos;2000, Athens. Greece.
Susan Weber McRoy. 1992. Using multiple
knowledge sources for word sense discrimina-
tion. Computational Linguistics, 18(1):1-30.
R. Mihalcea and D. Moldovan. 2001. extended
wordnet: Progress report. In Proceedings of
NAACL Workshop on WordNet and Other
Lexical Resources, Pittsburgh, PA.
G. Miller, R. Beckwith, C. Fellbaum, D. Gross,
and K. Miller. 1990. Five Papers on Word-
Net. Special Issue of International Journal of
Lexicography, 3 (4 ) .
I. Niles and A. Pease. 2001. Towards a standard
upper ontology. In In Proceedings of the 2nd
International Conference on Formal Ontology
in Information Systems (FOIS-2001), pages
17-19. Chris Welty and Barry Smith, eds.
A. Novischi. 2002. Accurate semantic anno-
tations via pattern matching. In Florida
Artificial Intelligence Research Society
(FLAIRS&apos;02), Pensacola, Florida.
E. Pianta, L. Bentivogli, and C. Girardi. 2002.
Multiwordnet: developing an aligned multi-
lingual database. In First International Con-
ference on Global WordNet, Mysore, India.
G. Rigau, J. Atserias, and E. Agirre. 1997.
Combining unsupervised lexical knowledge
methods for word sense disambiguation. In
Proceedings of joint 35th Annual Meeting
of the Association for Computational Lin-
guistics and 8th Conference of the European
Chapter of the Association for Computational
Linguistics ACL/ EACL &apos;97, Madrid, Spain.
G. Rigau. 1998. Automatic Acquisition of Lexi-
cal Knowledge from MRDs. Ph.D. thesis, De-
partament de LSI. Universitat Politecnica de
Catalunya.
D. Yarowsky. 1993. One sense per colloca-
tion. In Proceedings, ARPA Human Lan-
guage Technology Workshop, Princeton.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.874728333333333">SENSEVAL-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, Barcelona, Spain, July 2004 Association for Computational Linguistics</note>
<abstract confidence="0.972078841121496">current content of the Multilingual Central Repository (Mcn) (Atserias et al., 2004) (i.e. SUMO, WordNet Domains, etc.). Given a word from a gloss, each heuristic votes for different synsets. The program simply adds up the votes for each word sense, selecting the most voted sense. We have presented two different systems using two different preprocess. • PRE-XWN (XWN preprocess) uses the gloss segmentation and PoS tagging provided by the XWN. • In PRE-TALP (TALP preprocess) first, the sentences are tokenized and then passed on to a multiword identification module. Then, the output containing the multiwords is POS tagged using Eric Brill&apos;s tagger (Brill, 1995). Tagged words and multiword expressions are lemmatized using WN. PRE-XWN 00256298n the restoration#Ms) of rundown#a(g) urban#a(n) areas#n(n) by the middle#a class#n(n) resulting#v(n) in the displacement#n of lower#a(n) income#Ms) people#Mn) PRE-TALP 00256298n the restoration#n of rundown#v urban-areas#n by the middle-class#n resulting#v in the displacement#n of lower#a income#n people#n ) Table 1: PRE-XWN and PRE-TALP Example Table 1 shows an example of the different behaviours of both preprocessing systems. PRE- TALP recognizes the Multi Word Expression XWN split them in several words. On the other hand, the did not recognize an adjective. Obviously, different segmentation and tagging preprocessing causes different word counts and scoring. 2.1 Heuristics The main heuristics used in the disambiguation process are: 1. Monosemous: Applying a closed—world assumption, this heuristic identifies monosemous words. 2. Most Frequent: Based on WN2.0 sense frequencies, this heuristic only selects those synsets having frequencies higher than the 85% of the most frequent senses. 3. Hypernym: This method follows the hypernym chain looking for words appearing in the gloss (e.g. the genus term). 4. WordNet Relations: This heuristic follows any synset relation looking for words appearing in its gloss. The method does not only use direct relations, but also performs a chaining search following all relations and stopping at distance five. 5. MultiWordNet Domains (Magnini and Cavagli, 2000): Having a synset with a particular WN Domain label, this method selects those synsets from the words of the gloss having the same Domain label. 6. Patterns: This method uses the &amp;quot;One sense per collocation&amp;quot; heuristic (Yarowsky, 1993), implementing those patterns appearing in (Novischi, 2002). 7. Lexical Parallelism: This heuristic identifies the words with the same PoS separated by comas or conjunctions and marks them, when possible, with senses that belong to the same hierarchy. 8. SUMO (Niles and Pease, 2001): Having a synset with a particular SUMO label, this method selects those synsets from the words of the gloss having the same SUMO label. 9. Category: Having a synset being connected to a particular WN CATEGORY, this method selects those synsets from the words of the gloss connected the same CATEGORY. 10. Bigram: This heuristic uses high frequency word sense pairs occurring in Sem- Cor. 11. Sense One: Finally, this heuristic always assigns the first WN sense. Data The test set consists of 15,179 gold assigments from 9,257 glosses taked directly from XwN2.0- 1.1 (see table 2). However, this version is not free of errors and inconsistencies. For instance, XWN has 724 word tagged senses not belonging to WN. Three of them labelled as gold. POS words gold Noun 35539 10985 Verb 2863 2105 Adj 370 263 Adv 3719 1826 Total 42491 15179 Table 2: Test Senseval3 (9257 gloss) as we can table 3 the synset distributions of the test data and WN2.0 are very different. In particular for adjective and adverbs. Being the test data not representative of WN2.0, this test set misleads the global results and the final goal of the task.</abstract>
<note confidence="0.873445423076923">WordNet 2.0 Test Data POS Gloss % Gloss % Noun 79689 69.0 6706 72.4 Verb 13508 11.7 773 8.4 Adj 18563 16.0 94 1.0 Adv 3664 3.2 1684 18.2 Total 115424 100 9257 100 Table 3: Synset distributions of WN2.0 and Test 4 Results PRE-XWN Noun Verb Adj Adv Total Correct 7788 1191 134 1246 10363 Attempted 10981 2105 263 1717 15102 Total 10985 2105 263 1826 15179 Precision 70.9% 56.6% 51.0% 72.6% 68.6% Recall 70.9% 56.6% 51.0% 68.2% 68.3% % Attemp. 100% 100% 100% 94.0% 99.5% PRE-TALP Noun Verb Adj Adv Total Correct 6076 979 130 1260 8466 Attempted 9339 2000 253 1746 14757 Total 10985 2105 263 1826 15179 Precision 65.1% 48.9% 51.4% 72.2% 57.4% Recall 55.3% 46.5% 49.4% 69.0% 55.8% % Attemp. 85.0% 95.0% 96.2% 95.6% 97.2% 4: Results of</note>
<abstract confidence="0.992022741379311">PRE-TALP final results of both TALP are presented in table 4. Obviously, the perfor- PRE-TALP is lower than PRE-XWN because it uses a different preprocess. The difference in the amount of words is due to the preprocess (different tokenization, PoS tagging). the ten presented at the task, PRE-XWN has obtained the first position of (10,363 correct gold 68.3%) and PRE-TALP the third position (8,466 correct gold assigments, 55.8%). Table 5 presents the final results per heuristic. As expected, each heuristic has different behaviour of (P) precision and (R) recall. However, none of them has higher performance than its combination. PRE-XWN Heuristic Corr Attem P R Attem Monos 131 140 93.6% 0.9% 0.9% MostFre 7741 13903 55.7% 51.0% 91.6% Hyper 2003 2271 88.2% 13.2% 15.0% Relations 5243 8054 65.1% 34.5% 53.1% Domains 2873 4119 69.7% 18.9% 27.1% Pattern 709 753 94.2% 4.7% 5.0% LexPar 756 1360 55.6% 5.0% 9.0% Sumo 2334 4181 55.8% 15.4% 27.5% Category 38 64 59.4% 0.3% 0.4% Bigram 1903 3305 57.6% 12.5% 21.8% SenseOne 8338 15093 55.2% 54.9% 99.4% PRE-TALP Heuristic Corr Attem P R Attem Monos 8 86 9.3% 0.1% 0.6% MostFre 6061 12340 49.1% 39.9% 81.3% Hyper 1845 2082 88.6% 12.2% 13.7% Relations 4538 7443 61.0% 29.9% 49.0% Domains 1856 3096 59.9% 12.2% 20.4% Pattern 712 757 94.1% 4.7% 5.0% LexPar 590 1143 51.6% 3.9% 7.5% Sumo 2172 3942 55.1% 14.3% 26.0% Category 34 62 54.8% 0.2% 0.4% Bigram 1361 2692 50.6% 9.0% 17.7% SenseOne 6538 13403 48.8% 43.1% 88.3% Table 5: Per heuristic results 5 Conclusions and Future Work It is our belief, following (McRoy, 1992) and (Rigau et al., 1997), that full-fledged lexical ambiguity resolution should integrate several information sources and techniques. Our heuristics used most of the information content coherently integrated within the Multilingual Central (Mcn) of et al., 2004), one of the richest and largest multilingual knowledge in order to improve the current plan to enrich the current set of heuristics using other knowledge uploaded into the MCR with a more robust preprocessing schema.</abstract>
<note confidence="0.87592725">Acknowledgments This research has been partially funded by the European Commission (Meaning Project, 1ST- 2001-34460), and by the Spanish Research Ministry (Hermes Project: TIC2000-0335-0O3-02), Generalitat de Catalunya (2002FI 00648) and Universidad TecnolOgica Metropolitana (Chile). References E. Agirre, 0. Ansa, X. Arregi, J.M. Arriola, A. Diaz de Ilarraza, E. Pociello, and L. Uria. 2002. Methodological issues in the building of the basque wordnet: quantitative and quali-</note>
<author confidence="0.392847">In of the first In-</author>
<affiliation confidence="0.546213">ternational WordNet Conference in Mysore,</affiliation>
<address confidence="0.637193">India, 21-25 January.</address>
<author confidence="0.615301">J Atserias</author>
<author confidence="0.615301">S Climent</author>
<author confidence="0.615301">X Farreres</author>
<author confidence="0.615301">G Rigau</author>
<abstract confidence="0.78922225">and H. Rodriguez. 1997. Combining multiple methods for the automatic construction multilingual wordnets. In of 143-149, Bulgaria.</abstract>
<note confidence="0.735693625">Jordi Atserias, Luis Villarejo, German Rigau, Eneko Agirre, John Carroll, Bernardo Magnini, and Piek Vossen. 2004. The meaning multilingual central repository. of the Second International Global WordNet Conference (GWC&apos;04), Brno, Czech Republic, January. ISBN 80-210-3302-9.</note>
<abstract confidence="0.851909333333333">E. Brill. 1995. Transformation-based errordriven learning and natural language processing: A case study in part of speech tagging.</abstract>
<note confidence="0.56828427027027">Linguistics, Fellbaum, editor. 1998. An Elec- Lexical Database. MIT Press. A. Gangemi, R. Navigli, and P. Velardi. 2003. Axiomatizing wordnet glosses in the onproject. In of 2nd International Semantic Web Conference Workshop on Human Language Technology for the Web and Web Services, Island, Florida. S. Harabagiu, G. Miller, and D. Moldovan. 1999. Wordnet 2 a morphologically and seenhanced resource. In of ACL on Standardizing Lexical Resources MD. B. Magnini and G. Cavagli. 2000. Integrating field codes into wordnet. In Proceedings of the Second Internatgional Conference on Language Resources and Evaluation Greece. Susan Weber McRoy. 1992. Using multiple knowledge sources for word sense discrimina- Linguistics, R. Mihalcea and D. Moldovan. 2001. extended Progress report. In of NAACL Workshop on WordNet and Other Resources, PA. G. Miller, R. Beckwith, C. Fellbaum, D. Gross, and K. Miller. 1990. Five Papers on Word- Issue of International Journal of (4 ) . I. Niles and A. Pease. 2001. Towards a standard ontology. In Proceedings of the 2nd International Conference on Formal Ontology Information Systems (FOIS-2001), 17-19. Chris Welty and Barry Smith, eds. A. Novischi. 2002. Accurate semantic anno-</note>
<author confidence="0.466112">In</author>
<affiliation confidence="0.744056">Artificial Intelligence Research Society</affiliation>
<address confidence="0.676878">Florida.</address>
<note confidence="0.926947263157895">E. Pianta, L. Bentivogli, and C. Girardi. 2002. Multiwordnet: developing an aligned multidatabase. In International Conon Global WordNet, India. G. Rigau, J. Atserias, and E. Agirre. 1997. Combining unsupervised lexical knowledge methods for word sense disambiguation. In Proceedings of joint 35th Annual the Association for Computational Linguistics and 8th Conference of the European Chapter of the Association for Computational ACL/ EACL &apos;97, Spain. Rigau. 1998. Acquisition of Lexi- Knowledge from MRDs. thesis, Departament de LSI. Universitat Politecnica de Catalunya. D. Yarowsky. 1993. One sense per colloca- In ARPA Human Lan- Technology Workshop,</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>X Arregi Ansa</author>
<author>J M Arriola</author>
<author>A Diaz de Ilarraza</author>
<author>E Pociello</author>
<author>L Uria</author>
</authors>
<title>Methodological issues in the building of the basque wordnet: quantitative and qualitative analysis.</title>
<date>2002</date>
<booktitle>In Proceedings of the first International WordNet Conference in Mysore, India,</booktitle>
<pages>21--25</pages>
<marker>Ansa, Arriola, de Ilarraza, Pociello, Uria, 2002</marker>
<rawString>E. Agirre, 0. Ansa, X. Arregi, J.M. Arriola, A. Diaz de Ilarraza, E. Pociello, and L. Uria. 2002. Methodological issues in the building of the basque wordnet: quantitative and qualitative analysis. In Proceedings of the first International WordNet Conference in Mysore, India, 21-25 January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Atserias</author>
<author>S Climent</author>
<author>X Farreres</author>
<author>G Rigau</author>
<author>H Rodriguez</author>
</authors>
<title>Combining multiple methods for the automatic construction of multilingual wordnets.</title>
<date>1997</date>
<booktitle>In Proceedings of RANLP&apos;97,</booktitle>
<pages>143--149</pages>
<marker>Atserias, Climent, Farreres, Rigau, Rodriguez, 1997</marker>
<rawString>J. Atserias, S. Climent, X. Farreres, G. Rigau, and H. Rodriguez. 1997. Combining multiple methods for the automatic construction of multilingual wordnets. In Proceedings of RANLP&apos;97, pages 143-149, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordi Atserias</author>
<author>Luis Villarejo</author>
<author>German Rigau</author>
<author>Eneko Agirre</author>
<author>John Carroll</author>
<author>Bernardo Magnini</author>
<author>Piek Vossen</author>
</authors>
<title>The meaning multilingual central repository.</title>
<date>2004</date>
<journal>ISBN</journal>
<booktitle>In Proceedings of the Second International Global WordNet Conference (GWC&apos;04),</booktitle>
<pages>80--210</pages>
<location>Brno, Czech Republic,</location>
<marker>Atserias, Villarejo, Rigau, Agirre, Carroll, Magnini, Vossen, 2004</marker>
<rawString>Jordi Atserias, Luis Villarejo, German Rigau, Eneko Agirre, John Carroll, Bernardo Magnini, and Piek Vossen. 2004. The meaning multilingual central repository. In Proceedings of the Second International Global WordNet Conference (GWC&apos;04), Brno, Czech Republic, January. ISBN 80-210-3302-9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>Transformation-based errordriven learning and natural language processing: A case study in part of speech tagging.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>4</issue>
<contexts>
<context position="854" citStr="Brill, 1995" startWordPosition="132" endWordPosition="133"> (Mcn) (Atserias et al., 2004) (i.e. SUMO, WordNet Domains, etc.). Given a word from a gloss, each heuristic votes for different synsets. The program simply adds up the votes for each word sense, selecting the most voted sense. We have presented two different systems using two different preprocess. • PRE-XWN (XWN preprocess) uses the gloss segmentation and PoS tagging provided by the XWN. • In PRE-TALP (TALP preprocess) first, the sentences are tokenized and then passed on to a multiword identification module. Then, the output containing the multiwords is POS tagged using Eric Brill&apos;s tagger (Brill, 1995). Tagged words and multiword expressions are lemmatized using WN. PRE-XWN 00256298n the restoration#Ms) of rundown#a(g) urban#a(n) areas#n(n) by the middle#a class#n(n) resulting#v(n) in the displacement#n of lower#a(n) - income#Ms) people#Mn) PRE-TALP 00256298n the restoration#n of rundown#v urban-areas#n by the middle-class#n resulting#v in the displacement#n of lower#a - income#n people#n ) Table 1: PRE-XWN and PRE-TALP Example Table 1 shows an example of the different behaviours of both preprocessing systems. PRETALP recognizes the Multi Word Expression urban-area and middle-class, while X</context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>E. Brill. 1995. Transformation-based errordriven learning and natural language processing: A case study in part of speech tagging. Computational Linguistics, 21(4).</rawString>
</citation>
<citation valid="true">
<title>WordNet. An Electronic Lexical Database.</title>
<date>1998</date>
<editor>C. Fellbaum, editor.</editor>
<publisher>The MIT Press.</publisher>
<marker>1998</marker>
<rawString>C. Fellbaum, editor. 1998. WordNet. An Electronic Lexical Database. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gangemi</author>
<author>R Navigli</author>
<author>P Velardi</author>
</authors>
<title>Axiomatizing wordnet glosses in the ontowordnet project.</title>
<date>2003</date>
<booktitle>In Proceedings of 2nd International Semantic Web Conference Workshop on Human Language Technology</booktitle>
<location>Sanibel Island, Florida.</location>
<marker>Gangemi, Navigli, Velardi, 2003</marker>
<rawString>A. Gangemi, R. Navigli, and P. Velardi. 2003. Axiomatizing wordnet glosses in the ontowordnet project. In Proceedings of 2nd International Semantic Web Conference Workshop on Human Language Technology for the Semantic Web and Web Services, Sanibel Island, Florida.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Harabagiu</author>
<author>G Miller</author>
<author>D Moldovan</author>
</authors>
<title>Wordnet 2 - a morphologically and semantically enhanced resource.</title>
<date>1999</date>
<booktitle>In Proceedings of ACL on Standardizing Lexical Resources (SIGLEX&apos;99),</booktitle>
<location>Maryland, MD.</location>
<marker>Harabagiu, Miller, Moldovan, 1999</marker>
<rawString>S. Harabagiu, G. Miller, and D. Moldovan. 1999. Wordnet 2 - a morphologically and semantically enhanced resource. In Proceedings of ACL on Standardizing Lexical Resources (SIGLEX&apos;99), Maryland, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Magnini</author>
<author>G Cavagli</author>
</authors>
<title>Integrating subject field codes into wordnet. In</title>
<date>2000</date>
<booktitle>In Proceedings of the Second Internatgional Conference on Language Resources and Evaluation LREC&apos;2000,</booktitle>
<location>Athens. Greece.</location>
<contexts>
<context position="2438" citStr="Magnini and Cavagli, 2000" startWordPosition="365" endWordPosition="368">s monosemous words. 2. Most Frequent: Based on WN2.0 sense frequencies, this heuristic only selects those 3http://www.lsi.upc.es/-meaning synsets having frequencies higher than the 85% of the most frequent senses. 3. Hypernym: This method follows the hypernym chain looking for words appearing in the gloss (e.g. the genus term). 4. WordNet Relations: This heuristic follows any synset relation looking for words appearing in its gloss. The method does not only use direct relations, but also performs a chaining search following all relations and stopping at distance five. 5. MultiWordNet Domains (Magnini and Cavagli, 2000): Having a synset with a particular WN Domain label, this method selects those synsets from the words of the gloss having the same Domain label. 6. Patterns: This method uses the &amp;quot;One sense per collocation&amp;quot; heuristic (Yarowsky, 1993), implementing those patterns appearing in (Novischi, 2002). 7. Lexical Parallelism: This heuristic identifies the words with the same PoS separated by comas or conjunctions and marks them, when possible, with senses that belong to the same hierarchy. 8. SUMO (Niles and Pease, 2001): Having a synset with a particular SUMO label, this method selects those synsets fr</context>
</contexts>
<marker>Magnini, Cavagli, 2000</marker>
<rawString>B. Magnini and G. Cavagli. 2000. Integrating subject field codes into wordnet. In In Proceedings of the Second Internatgional Conference on Language Resources and Evaluation LREC&apos;2000, Athens. Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan Weber McRoy</author>
</authors>
<title>Using multiple knowledge sources for word sense discrimination.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<pages>18--1</pages>
<contexts>
<context position="6468" citStr="McRoy, 1992" startWordPosition="1067" endWordPosition="1068">ory 38 64 59.4% 0.3% 0.4% Bigram 1903 3305 57.6% 12.5% 21.8% SenseOne 8338 15093 55.2% 54.9% 99.4% PRE-TALP Heuristic Corr Attem P R Attem Monos 8 86 9.3% 0.1% 0.6% MostFre 6061 12340 49.1% 39.9% 81.3% Hyper 1845 2082 88.6% 12.2% 13.7% Relations 4538 7443 61.0% 29.9% 49.0% Domains 1856 3096 59.9% 12.2% 20.4% Pattern 712 757 94.1% 4.7% 5.0% LexPar 590 1143 51.6% 3.9% 7.5% Sumo 2172 3942 55.1% 14.3% 26.0% Category 34 62 54.8% 0.2% 0.4% Bigram 1361 2692 50.6% 9.0% 17.7% SenseOne 6538 13403 48.8% 43.1% 88.3% Table 5: Per heuristic results 5 Conclusions and Future Work It is our belief, following (McRoy, 1992) and (Rigau et al., 1997), that full-fledged lexical ambiguity resolution should integrate several information sources and techniques. Our heuristics used most of the information content coherently integrated within the Multilingual Central Repository (Mcn) of MEANING (Atserias et al., 2004), one of the richest and largest multilingual lexical knowledge base in existence. In order to improve the current systems, we plan to enrich the current set of heuristics using other knowledge uploaded into the MCR with a more robust preprocessing schema. Acknowledgments This research has been partially fu</context>
</contexts>
<marker>McRoy, 1992</marker>
<rawString>Susan Weber McRoy. 1992. Using multiple knowledge sources for word sense discrimination. Computational Linguistics, 18(1):1-30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>D Moldovan</author>
</authors>
<title>extended wordnet: Progress report.</title>
<date>2001</date>
<booktitle>In Proceedings of NAACL Workshop on WordNet and Other Lexical Resources,</booktitle>
<location>Pittsburgh, PA.</location>
<marker>Mihalcea, Moldovan, 2001</marker>
<rawString>R. Mihalcea and D. Moldovan. 2001. extended wordnet: Progress report. In Proceedings of NAACL Workshop on WordNet and Other Lexical Resources, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
<author>R Beckwith</author>
<author>C Fellbaum</author>
<author>D Gross</author>
<author>K Miller</author>
</authors>
<title>Five Papers on WordNet.</title>
<date>1990</date>
<journal>Special Issue of International Journal of Lexicography,</journal>
<volume>3</volume>
<issue>4</issue>
<pages>.</pages>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1990</marker>
<rawString>G. Miller, R. Beckwith, C. Fellbaum, D. Gross, and K. Miller. 1990. Five Papers on WordNet. Special Issue of International Journal of Lexicography, 3 (4 ) .</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Niles</author>
<author>A Pease</author>
</authors>
<title>Towards a standard upper ontology. In</title>
<date>2001</date>
<booktitle>In Proceedings of the 2nd</booktitle>
<contexts>
<context position="2954" citStr="Niles and Pease, 2001" startWordPosition="452" endWordPosition="455">following all relations and stopping at distance five. 5. MultiWordNet Domains (Magnini and Cavagli, 2000): Having a synset with a particular WN Domain label, this method selects those synsets from the words of the gloss having the same Domain label. 6. Patterns: This method uses the &amp;quot;One sense per collocation&amp;quot; heuristic (Yarowsky, 1993), implementing those patterns appearing in (Novischi, 2002). 7. Lexical Parallelism: This heuristic identifies the words with the same PoS separated by comas or conjunctions and marks them, when possible, with senses that belong to the same hierarchy. 8. SUMO (Niles and Pease, 2001): Having a synset with a particular SUMO label, this method selects those synsets from the words of the gloss having the same SUMO label. 9. Category: Having a synset being connected to a particular WN CATEGORY, this method selects those synsets from the words of the gloss connected the same CATEGORY. 10. Bigram: This heuristic uses high frequency word sense pairs occurring in SemCor. 11. Sense One: Finally, this heuristic always assigns the first WN sense. 3 Test Data The test set consists of 15,179 gold assigments from 9,257 glosses taked directly from XwN2.0- 1.1 (see table 2). However, thi</context>
</contexts>
<marker>Niles, Pease, 2001</marker>
<rawString>I. Niles and A. Pease. 2001. Towards a standard upper ontology. In In Proceedings of the 2nd</rawString>
</citation>
<citation valid="false">
<booktitle>International Conference on Formal Ontology in Information Systems (FOIS-2001),</booktitle>
<pages>17--19</pages>
<editor>Chris Welty and Barry Smith, eds.</editor>
<marker></marker>
<rawString>International Conference on Formal Ontology in Information Systems (FOIS-2001), pages 17-19. Chris Welty and Barry Smith, eds.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Novischi</author>
</authors>
<title>Accurate semantic annotations via pattern matching.</title>
<date>2002</date>
<booktitle>In Florida Artificial Intelligence Research Society (FLAIRS&apos;02),</booktitle>
<location>Pensacola, Florida.</location>
<contexts>
<context position="2730" citStr="Novischi, 2002" startWordPosition="415" endWordPosition="416">oss (e.g. the genus term). 4. WordNet Relations: This heuristic follows any synset relation looking for words appearing in its gloss. The method does not only use direct relations, but also performs a chaining search following all relations and stopping at distance five. 5. MultiWordNet Domains (Magnini and Cavagli, 2000): Having a synset with a particular WN Domain label, this method selects those synsets from the words of the gloss having the same Domain label. 6. Patterns: This method uses the &amp;quot;One sense per collocation&amp;quot; heuristic (Yarowsky, 1993), implementing those patterns appearing in (Novischi, 2002). 7. Lexical Parallelism: This heuristic identifies the words with the same PoS separated by comas or conjunctions and marks them, when possible, with senses that belong to the same hierarchy. 8. SUMO (Niles and Pease, 2001): Having a synset with a particular SUMO label, this method selects those synsets from the words of the gloss having the same SUMO label. 9. Category: Having a synset being connected to a particular WN CATEGORY, this method selects those synsets from the words of the gloss connected the same CATEGORY. 10. Bigram: This heuristic uses high frequency word sense pairs occurring</context>
</contexts>
<marker>Novischi, 2002</marker>
<rawString>A. Novischi. 2002. Accurate semantic annotations via pattern matching. In Florida Artificial Intelligence Research Society (FLAIRS&apos;02), Pensacola, Florida.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Pianta</author>
<author>L Bentivogli</author>
<author>C Girardi</author>
</authors>
<title>Multiwordnet: developing an aligned multilingual database.</title>
<date>2002</date>
<booktitle>In First International Conference on Global WordNet, Mysore,</booktitle>
<marker>Pianta, Bentivogli, Girardi, 2002</marker>
<rawString>E. Pianta, L. Bentivogli, and C. Girardi. 2002. Multiwordnet: developing an aligned multilingual database. In First International Conference on Global WordNet, Mysore, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Rigau</author>
<author>J Atserias</author>
<author>E Agirre</author>
</authors>
<title>Combining unsupervised lexical knowledge methods for word sense disambiguation.</title>
<date>1997</date>
<booktitle>In Proceedings of joint 35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the European Chapter of the Association for Computational Linguistics ACL/ EACL &apos;97,</booktitle>
<location>Madrid,</location>
<contexts>
<context position="6493" citStr="Rigau et al., 1997" startWordPosition="1070" endWordPosition="1073">3% 0.4% Bigram 1903 3305 57.6% 12.5% 21.8% SenseOne 8338 15093 55.2% 54.9% 99.4% PRE-TALP Heuristic Corr Attem P R Attem Monos 8 86 9.3% 0.1% 0.6% MostFre 6061 12340 49.1% 39.9% 81.3% Hyper 1845 2082 88.6% 12.2% 13.7% Relations 4538 7443 61.0% 29.9% 49.0% Domains 1856 3096 59.9% 12.2% 20.4% Pattern 712 757 94.1% 4.7% 5.0% LexPar 590 1143 51.6% 3.9% 7.5% Sumo 2172 3942 55.1% 14.3% 26.0% Category 34 62 54.8% 0.2% 0.4% Bigram 1361 2692 50.6% 9.0% 17.7% SenseOne 6538 13403 48.8% 43.1% 88.3% Table 5: Per heuristic results 5 Conclusions and Future Work It is our belief, following (McRoy, 1992) and (Rigau et al., 1997), that full-fledged lexical ambiguity resolution should integrate several information sources and techniques. Our heuristics used most of the information content coherently integrated within the Multilingual Central Repository (Mcn) of MEANING (Atserias et al., 2004), one of the richest and largest multilingual lexical knowledge base in existence. In order to improve the current systems, we plan to enrich the current set of heuristics using other knowledge uploaded into the MCR with a more robust preprocessing schema. Acknowledgments This research has been partially funded by the European Comm</context>
</contexts>
<marker>Rigau, Atserias, Agirre, 1997</marker>
<rawString>G. Rigau, J. Atserias, and E. Agirre. 1997. Combining unsupervised lexical knowledge methods for word sense disambiguation. In Proceedings of joint 35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the European Chapter of the Association for Computational Linguistics ACL/ EACL &apos;97, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Rigau</author>
</authors>
<title>Automatic Acquisition of Lexical Knowledge from MRDs.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>Departament de LSI. Universitat Politecnica de Catalunya.</institution>
<marker>Rigau, 1998</marker>
<rawString>G. Rigau. 1998. Automatic Acquisition of Lexical Knowledge from MRDs. Ph.D. thesis, Departament de LSI. Universitat Politecnica de Catalunya.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>One sense per collocation.</title>
<date>1993</date>
<booktitle>In Proceedings, ARPA Human Language Technology Workshop,</booktitle>
<location>Princeton.</location>
<contexts>
<context position="2671" citStr="Yarowsky, 1993" startWordPosition="407" endWordPosition="408">ws the hypernym chain looking for words appearing in the gloss (e.g. the genus term). 4. WordNet Relations: This heuristic follows any synset relation looking for words appearing in its gloss. The method does not only use direct relations, but also performs a chaining search following all relations and stopping at distance five. 5. MultiWordNet Domains (Magnini and Cavagli, 2000): Having a synset with a particular WN Domain label, this method selects those synsets from the words of the gloss having the same Domain label. 6. Patterns: This method uses the &amp;quot;One sense per collocation&amp;quot; heuristic (Yarowsky, 1993), implementing those patterns appearing in (Novischi, 2002). 7. Lexical Parallelism: This heuristic identifies the words with the same PoS separated by comas or conjunctions and marks them, when possible, with senses that belong to the same hierarchy. 8. SUMO (Niles and Pease, 2001): Having a synset with a particular SUMO label, this method selects those synsets from the words of the gloss having the same SUMO label. 9. Category: Having a synset being connected to a particular WN CATEGORY, this method selects those synsets from the words of the gloss connected the same CATEGORY. 10. Bigram: Th</context>
</contexts>
<marker>Yarowsky, 1993</marker>
<rawString>D. Yarowsky. 1993. One sense per collocation. In Proceedings, ARPA Human Language Technology Workshop, Princeton.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>