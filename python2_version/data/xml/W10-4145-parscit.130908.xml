<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002475">
<title confidence="0.975051">
The SAU Report for the 1st CIPS-SIGHAN-ParsEval-2010
</title>
<author confidence="0.8926655">
Qiaoli Zhou Wenjing Yingying Yan Wang Dongfeng Cai
Lang Wang
</author>
<affiliation confidence="0.898633">
Knowledge Engineering Research Center,Shenyang Aerospace
University,Shenyang,China
</affiliation>
<email confidence="0.992321">
Qiaoli_z@yahoo.com.cn
</email>
<sectionHeader confidence="0.997248" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999940666666667">
This paper presents our work for
participation in the 2010 CIPS-SIGHAN
evaluation on two tasks which are Event
Description Sub-sentence (EDSs)
Analysis and Complete Sentence (CS)
Parsing in Chinese Parsing. The paper
describes the implementation of our
system as well as the results we have
achieved and the analysis.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999899095238095">
The paper describes the parsing system of SAU
in 1st CLPS-SIGHAN evaluation task 2. We
participate in two tasks - EDS Analysis and CS
Parsing. The testing set only provides
segmentation results, therefore, we divide our
system into the following subsystems: (1) Part-
of-Speech (POS) tagging system, we mainly
make use of Conditional Random Fields (CRFs)
model for POS tagging; (2) parsing system, the
paper adopts divide-and-conquer strategy to
parsing, which uses CCRFs model for parsing
and adopts searching algorithm to build trees in
decoding; (3) head recognition system, which
also makes use of CCRFs model.
The rest of the paper is organized as follows:
Section 2 describes the POS tagging system;
Section 3 describes the structure of our parsing
system; Section 4 describes head recognition
system in parsing tree; Section 5 presents the
results of our system and the analysis; Section 6
concludes the paper.
</bodyText>
<sectionHeader confidence="0.990028" genericHeader="method">
2 Part-of-Speech Tagging
</sectionHeader>
<bodyText confidence="0.999168428571429">
We use CRFs model and post-processing
method for POS tagging. In the first step, we tag
POS based on CRFs. The second step is the
post-processing after tagging, which is
correcting by using dictionary drawn from
training set. The system architecture of POS
tagging is shown in Figure 1.
</bodyText>
<sectionHeader confidence="0.712635" genericHeader="method">
2.1 Features
</sectionHeader>
<bodyText confidence="0.992456333333333">
Feature selection significantly influences the
performance of CRFs. We use the following
features in our system.
</bodyText>
<table confidence="0.992565909090909">
Atom Template
word(-2) , word(-1) , word(0) , word(1) , word(2)
prefix( word (0) ) ,suffix( word(0) )
includeDot1(word ( 0 ))
includeDot2(word ( 0 ))
Complex Template
word(-1)&amp; word(0) , word(0)&amp; word(1)
word(0)&amp; prefix( word (0) )
word(0)&amp; suffix( word(0) )
word(0)&amp; includeDot1(word ( 0 ))
word(0)&amp; includeDot2(word ( 0 ))
</table>
<tableCaption confidence="0.999062">
Table 1: Feature templates used in POS tagger.
</tableCaption>
<bodyText confidence="0.973353">
word(i) represents the ith word, prefix( word (i) )
represents the first character of the ith word,
suffix( word (i) ) represents the last character of
the ith word, ncludeDot1(word ( i)) represents
the ith word containing ‘· ’ or not, and
includeDot2(word ( i)) represnts the ith word
containing ‘.’ or not.
</bodyText>
<subsectionHeader confidence="0.999365">
2.2 Post-processing
</subsectionHeader>
<bodyText confidence="0.999155333333333">
The post-processing module adopts the
following processing by analyzing the errors
from tagging result based on CRFs. We firstly
need to build two dictionaries which are single
class word dictionary and ambiguity word
dictionary before the post-processing. The
single class word dictionary and ambiguity
word dictionary are built by drawing from
training set.
</bodyText>
<figureCaption confidence="0.999692">
Figure 1: System architecture of POS tagging
</figureCaption>
<figure confidence="0.995049785714286">
POS tagger based
on CRF
Features selection
Testing
corpus
CRF model
Primary reco-
gnition result
Post-processing
POS
Result
Training
corpus
Parameter estimation
</figure>
<bodyText confidence="0.93459775">
The single class word is the word having
single POS in training set, and the ambiguity
word is the word having multi POS in training
set. Besides, we build rules for words with
distinctive features aiming at correcting errors,
such as “的”, numbers and English characters,
etc.
Figure 2 shows the post-processing step after
POS tagging by CRFs model. As shown in
Figure 2, we respectively post-process single
class words and ambiguity words according to
CRF score.
</bodyText>
<listItem confidence="0.834846">
(1) Single class word processing module
</listItem>
<bodyText confidence="0.998492">
The post-processing of single class words
consults the single class word dictionary and
CRFs score. When the score from CRFs is
higher than 0.9, we take the POS from CRFs as
the final POS; otherwise, POS of the word is
corrected by the POS in the single class word
dictionary.
</bodyText>
<figure confidence="0.950724105263158">
CRF Primary result
End
Figure2: Post-processing architecture after CRF labeling
N
2
Single class word
Single class word
processing module
Word class?
3
Ambiguity word
Ambiguity word
processing module
Unknown word
Unknown word
processing module
1
Rule base
(2) Ambiguity word processing module
</figure>
<bodyText confidence="0.9967294">
The post-processing of ambiguity words
consults the ambiguity word dictionary and
CRFs score. When the POS from CRFs belongs
to the POS of the word in the ambiguity word
dictionary, we take the POS from CRFs as the
final POS; otherwise, we examine the score of
CRF, if the score is less than 0.4, the final POS
of the word is the POS who has the highest
score (has highest frequency), or else taking
POS from CRF as the final POS.
</bodyText>
<listItem confidence="0.737503">
(3) Unknown word processing module
</listItem>
<bodyText confidence="0.999859285714286">
The unknown words are the words not in
training set. By analyzing the examples, we find
that there are great deals of person names,
location names, organization names and
numbers, etc. And the words have
characteristics when building word, therefore,
we set up rules for processing.
</bodyText>
<subsectionHeader confidence="0.999089">
2.3 Experiment results
</subsectionHeader>
<bodyText confidence="0.9893475">
Table 2 shows the comparative experimental
results of POS tagging using two methods.
</bodyText>
<table confidence="0.997234">
Method EDSs CS
precision precision
CRF 92.83% 89.42%
CRF + 93.96% 91.05%
post-processing
</table>
<tableCaption confidence="0.999422">
Table 2: Comparative POS tagging results
</tableCaption>
<sectionHeader confidence="0.885168" genericHeader="method">
3 Parsing system
</sectionHeader>
<bodyText confidence="0.999957533333333">
The paper uses divide-and-conquer strategy
(Shiuan 1996 et al., Braun 2000 et al., Lyon
1997 et al.)for parsing. Firstly, we recognize
MNP for an input sentence, which divide the
sentence into two kinds of parts. One kind is
MNPs, and the other one is frame which is a
new sentence generating by replacing MNP
using its head word. Secondly, we use parsing
approach based on chunking (Abney, 1991, Erik
Tjong and Kim Sang, 2001) and a searching
algorithm in decoding. Thirdly, we combine the
parsing trees of MNPs and frame, which obtains
the full parsing tree of the original sentence.
Figure 3 shows the architecture of paring
system.
</bodyText>
<subsectionHeader confidence="0.994924">
3.1 MNP recognition
</subsectionHeader>
<bodyText confidence="0.998301714285714">
Maximal Noun Phrase (MNP) is the noun
phrase which is not contained by any other noun
phrases. We use Berkeley parser (2009 1.0) for
MNP recognition. We first use Berkeley parser
to parse sentences after POS tagging, and then
we tag MNPs from the parsing results. As the
following example:
Berkeley parser result: dj[ 中国/nS vp[ 重视/v
vp[ 发展/v np[ pp[ 与/p np[ 欧洲/nS 国家/n ] ] 的
/uJDE 关系/n ] ] ] ]
MNP recognition result: 中国/nS 重视/v 发展
/v np[ 与/p 欧洲/nS 国家/n 的/uJDE 关系/n ]
The results of MNP recognition EDSs
analysis and CS parsing are as table3:
</bodyText>
<table confidence="0.997258">
P R F
EDSs 85.3202% 85.998% 85.6578%
CS 77.7102% 79.2782% 78.4864%
</table>
<tableCaption confidence="0.999928">
Table 3: Results of MNP recognition
</tableCaption>
<subsectionHeader confidence="0.9151225">
3.2 Head recognition of MNP and
generation of frame
</subsectionHeader>
<bodyText confidence="0.999995074074074">
In this paper, the new sentence in which MNPs
are replaced by their head word is defined as the
sentence’s frame. The head of MNPs is
identified after MNP recognition and then they
are used to replace the original MNP, and
finally the sentence’s frame is formed. We use
the rules to recognize the head of MNP. Usually,
the last word of MNP is the head of the phrase,
which can represent the MNP in function. For
example: “[该/r 学派/n] 同样/ad 主张/v 消除/v
[ 干 预 /v 造 成 /v 的 /u 阻 碍 /n] 。 ” In this
sentence“ 该/r学派/n” and “ 干预/v 造成/v
的/u 阻碍/n” are MNPs. If we omit the
modifier in MNP, for example “[学派/n] 同样
/ad 主张/v 消除/v [阻碍/n]。”, the meaning of
the sentence will not be changed. Because the
head can represent the syntax function of MNP,
we can use the head for parsing, which can
avoid the effect of the modifier of MNP on
parsing and reduce the complexity of parsing.
However, the components of MNP are
complicated, not all of the last word of MNP
can be the head of MNP. The paper shows that
if MNP has parentheses, we can use the last
word before parentheses as the head. When the
last word of MNP is “等”, we use the second last
word as the head.
</bodyText>
<subsectionHeader confidence="0.999859">
3.3 Chunking with CRFs
</subsectionHeader>
<bodyText confidence="0.9995635">
The accuracy of chunk parsing is highly
dependent on the accuracy of each level of
</bodyText>
<figure confidence="0.98729225">
Chunk Model
Search
sentence
MNP Recognition
frame
MNPs
CRF Chunker
parsing tree
</figure>
<figureCaption confidence="0.878789">
Figure3: Parsing system architecture
</figureCaption>
<bodyText confidence="0.9995415">
chunking. This section describes our approach
to the chunking task. A common approach to
the chunking problem is to convert the problem
into a sequence tagging task by using the
“BIEO” (B for beginning, I for inside, E for
ending, and O for outside) representation.
This representation enables us to use the
linear chain CRF model to perform chunking,
since the task is simply assigning appropriate
labels to sequence.
</bodyText>
<sectionHeader confidence="0.818821" genericHeader="method">
3.3.1 Features
</sectionHeader>
<bodyText confidence="0.999188666666667">
Table 4 shows feature templates used in the
whole levels of chunking. In the whole levels of
chunking, we can use a rich set of features
because the chunker has access to the
information about the partial trees that have
been already created (Yoshimasa et al., 2009). It
uses the words and POS tags around the edges
of the covered by the current non-terminal
symbol.
</bodyText>
<table confidence="0.999852142857143">
Word Unigrams W-2 , W-1, W0, W1, W2,
Word Bigrams W-2W-1, W-1W0, W0W1,
W1W2, W0W-2, W0W2,
Word Trigrams W0W-1W-2, W0W1W2
POS Unigrams P-3, P-2 , P-1 , P0 , P1, P2, P3,
POS Bigrams P-3P-2, P-2P-1, P-1P0, P0P1,
P1P2, P2P3, P0P-2, P0P2,
POS Trigrams P-3P-2P-1, P-2P-1P0, P-1P0P1,
P0P1P2, P1P2P3
Word &amp; POS W0P0, W0P-1, W0P1,
Word &amp; WordCount W0C0
Word &amp; FirstWord W0F0 , W-1F0
Word &amp; LastWord W0L0, W1L0
Word &amp; Symbol W0S0
</table>
<tableCaption confidence="0.9497115">
Table 4: Feature templates used in parsing system.
W represents a word, P represents the part-of-speech
</tableCaption>
<bodyText confidence="0.990494857142857">
of the word, C represents the sum of the chunk
containing the word, F represents the first word of
the chunk containing the word, L represents the last
word of the chunk containing the word, S represents
that the word is a non-terminal symbol or not. Wj is
the current word; Wj-1 is the word preceding Wj, Wj+1
is the word following Wj.
</bodyText>
<subsectionHeader confidence="0.935877">
3.4 Searching for the Best Parse
</subsectionHeader>
<bodyText confidence="0.999700333333334">
The probability for an entire parsing tree is
computed as the product of the probabilities
output by the individual CRF chunkers:
</bodyText>
<equation confidence="0.962458">
h
score =∏ p
0
</equation>
<bodyText confidence="0.999715166666667">
We use a searching algorithm to find the highest
probability derivation. CRF can score each
chunker result by A* search algorithm,
therefore, we use the score as the probability of
each chunker. We do not give pseudo code, but
the basic idea is as figure 4.
</bodyText>
<listItem confidence="0.99504445">
1: inti parser(sent)
2: Parse(sent, 1, 0)
4: function Parse(sent, m, n)
5: if sent is chunked as a complete sentence
6: return m
7: H = Chunking(sent, m/n)
8: for h∈H do
9: r = m * h.probability
10: if r&gt;n then
11: sent2 = Update(sent, h)
12: s = Parse(sent2, r, n)
13: if s&gt;n then n = s
14: return n
15: function Chunking(sent, t)
16: perform chunking with a CRF chunker and
return a set of chunking hypotheses whose
17: probabilities are greater than t.
18: function Update(sent, h)
19: update sequence sent according to chunking
hypothesis h and return the updated sequence.
</listItem>
<figureCaption confidence="0.997613">
Figure 4: Searching algorithm for the best parse
</figureCaption>
<bodyText confidence="0.9999682">
It is straightforward to introduce beam search
in this search algorithm—we simply limit the
number of hypotheses generated by the CRF
chunker. We examine how the width of the
beam affects the parsing performance in the
</bodyText>
<equation confidence="0.9658422">
(y / )
x
i i
i
=
</equation>
<bodyText confidence="0.9998995">
experiments. We experiment beam width and
we adopt the beam width of 4 at last.
</bodyText>
<subsectionHeader confidence="0.98015">
3.5 Head Finding
</subsectionHeader>
<bodyText confidence="0.999975">
Head finding is a post process after parsing in
our system. The paper uses method combining
statistics and rules to find head. The selected
statistical method is CRF model. The first step
is to train a CRF classifier to classify each
context-free production into several categories.
Then a rule-based method is used to post
process the identification results and gets the
final recognition results. The rule-based post-
processing module mainly uses rule base and
case base to carry out post-processing.
</bodyText>
<subsectionHeader confidence="0.969917">
3.6 Head finding based on CRFs
</subsectionHeader>
<bodyText confidence="0.990451">
The head finding procedure proceeds in the
bottom-up fashion, so that the head words of
productions in lower layers could be used as
features for the productions of higher layers
(Xiao chen et al. 2009).
</bodyText>
<figureCaption confidence="0.998281">
Atom template Definition
CurPhraseTag The label of the current word
LCh Word The left most child
RCh Word The right most child
LCh Pos The POS of the left most child
MCh Pos The POS of the middle child
RCh Pos The POS of the right most child
NumCh The number of children
CurPhraseTag± 1 The labels of the former phrase
and the latter
</figureCaption>
<tableCaption confidence="0.7609215">
Table 5: Atom templates for Head finding
Complex Template
CurPhraseTag/ NumCh, CurPhraseTag/ LCh_Word,
CurPhraseTag/LCh_Pos,
CurPhraseTag/LCh_Pos/RCh_Pos,
CurPhraseTag/NumCh/LCh_Pos/ RCh_Pos,
CurPhraseTag/NumCh/LCh_Word/LCh_Pos/MCh_
Pos/RCh_Word/RCh_Pos,
LCh_Word/LCh_Pos, CurPhraseTag/MCh_Pos,
NumCh/LCh_Pos/ MCh_Pos/ RCh_Pos,
CurPhraseTag/ NumCh/ MCh_Pos,
CurPhraseTag/LCh_Word/LCh_Pos/MCh_Pos/RCh
_Word/RCh_Pos,
LCh_Word/ LCh_Pos, LCh_Pos/ MCh_Pos,
CurPhraseTag/NumCh, RCh_Word/RCh_Pos,
NumCh/LCh_Word/LCh_Pos/MCh_Pos/RCh_Word
/RCh Pos
Table 6: Complex templates for Head finding
</tableCaption>
<bodyText confidence="0.999912909090909">
The atom templates are not sufficient for
labeling context; therefore, we use some
complex templates by combining the upper
atom templates for more effectively describing
context. When the feature function is fixed, the
atom templates in complex templates are
instantiated, which will generate features.
The final feature templates are composed of
the atom templates and the complex templates.
The feature templates of the head recognition in
phrases contain 24 types.
</bodyText>
<subsectionHeader confidence="0.994834">
3.7 Head Finding based on rules
</subsectionHeader>
<bodyText confidence="0.999931090909091">
Through the analysis of error examples, we
found that some CRFs recognition results are
clearly inconsistent with the actual situation; we
can use rules to correct these errors, thus
forming a rule base. Example-base is a chunk-
based library built through analysis and
processing on the training corpus. The
Example-base is composed of all the bottom
chunk and high-level chunk in training corpus.
High-level phrases are the bottom chunk
replaced by heads.
</bodyText>
<subsectionHeader confidence="0.996209">
3.8 Experiment results of head finding
</subsectionHeader>
<bodyText confidence="0.9718625">
Table 7 shows the comparative experiment
results of head recognition.
</bodyText>
<sectionHeader confidence="0.868378" genericHeader="evaluation">
4 Experiment of parsing system
</sectionHeader>
<bodyText confidence="0.995516">
We perform experiments on the training set and
testing set of Tsinghua Treebank provided by
CIPS-SIGHAN-ParsEval-2010. For the direct
influence of parsing result by the length of
sentence, we count the length distribution of
corpus.
Table 8 shows that the length of training set
and testing set of EDSs is mostly less than 20
words. The length of training set of CS is evenly
distributed, while the length of testing set is
between 30 and 40 words.
The paper adopts divide-and-conquer strategy
to parsing; therefore, we conduct the
</bodyText>
<table confidence="0.979569333333333">
Total Wrong Precision
Num Num
CRFs 7035 93 98.68%
CRFs + 7035 74 98.95%
rule-base+
case-base
</table>
<tableCaption confidence="0.767414571428572">
Table7: Comparative results of head recognition
comparative experiment of MNP parsing and
frame parsing. In addition, the results of MNP
parsing and frame parsing depend on the length
largely, so we list the length distribution of
MNP and frame of EDSs and CS as table 9 and
table 10.
</tableCaption>
<table confidence="0.99854775">
EDSs CS
length training testing training testing
set set set set
[0, 10) 50.68% 64.30% 10.59% 0
[10,20) 37.27% 29.50% 27.55% 0
[20,30) 8.64% 5.40% 26.37% 79.9%
[30,40) 2.31% 0.60% 16.63% 20.1%
40≤ 1.10% 0.20% 18.86% 0
</table>
<tableCaption confidence="0.999348">
Table 8: Length distribution of EDSs and CS
</tableCaption>
<bodyText confidence="0.999201">
We define Simple MNP (SMNP) whose
length is less than 5 words and Complete MNP
(CMNP) whose length is more than 5 words.
</bodyText>
<table confidence="0.996860571428572">
EDSs CS
length training testing training testing
set set set set
[0,5) 55.30% 62.46% 55.42% 59.45%
[5,10) 32.66% 29.69% 32.57% 30.77%
[10,20) 10.03% 6.75% 10.03% 8.65%
20≤ 2.00% 1.09% 1.98% 1.12%
</table>
<tableCaption confidence="0.999303">
Table 9: Length distribution of MNP
</tableCaption>
<bodyText confidence="0.989788166666667">
Table 9 shows the length distribution of MNP
in training set and testing set of sub-sentence is
consistent in basic, but the SMNP distribution
of EDSs is 3.01% less than CS, which
illuminates the complexity of MNP in CS is
higher than in EDSs.
</bodyText>
<table confidence="0.984134714285714">
EDSs CS
length training testing training testing
set set set set
[0,5) 45.84% 47.20% 10.17% 1.00%
[5,10) 43.58% 44.00% 24.14% 10.80%
[10,20) 9.98% 8.70% 41.31% 62.20%
20≤ 0.60% 0.10% 24.38% 26.00%
</table>
<tableCaption confidence="0.998708">
Table 10: Length distribution of frame
</tableCaption>
<bodyText confidence="0.999452471698114">
Table 10 shows the length distribution of frame
in training set and testing set of EDSs is consistent
in basic, while the CS is non-consistent. For the
frame whose length is less than 5 words, the frame
length distribution of training set is 9.17% higher
than the testing set; for the frame whose length is
more than 5 words and less than 10 words, the
training set is 7.65% lower than testing; and for the
frame whose length is between 10 words and 20
words, the testing set is 20.09% higher compared
with the training set. From another aspect, in
testing set, CS is 46.2% lower compared with
EDSs for frame whose length is less than 5.
Therefore, the complexity of frame in CS is higher
than in EDSs.
As shown in Table 8, 9 and 10, the length
distribution of testing set shows that the paring unit
length of EDSs is reduced to less than 10 from less
than 20 in original sentence and CS is reduced to
less than 20 from between 30 and 40 after dividing
an original sentence into MNPs parts and frame
part. The above data indicate the divide-and-
conquer strategy reduces the complexity of
sentences significantly.
We can conclude that the parsing result of CS
is lower than EDSs from Table 11, which is due
to the higher complexity of MNP and frame in CS
compared with EDSs from the results of Table 9
and Table 10. In addition, we obtain about 1%
improvement compared with Berkeley parser in
MNP and Frame parsing result in EDSs from
Table 11 and Table 12, which indicates that our
method is effective for short length parsing units. In
particular, Table 12 shows that our result is 1.8%
higher than Berkeley parser in the frame parsing of
CS. Due to the non-consistent frame length
distribution of training set and testing set in CS
from Table 10, we find that Berkeley parser largely
depends on training set compared with our method.
To more fairly compare the performance of
our proposed method, the comparative results
are shown as Table 13, the first one (Model01)
is combination method of MNP pre-processing
and chunk-based, and the chunk-based result
which adopts CCRFs method with searching
algorithm; the second one (Berkeley) is the
parsing result of Berkeley parser; the third one
(Model02) also is combination method of MNP
pre-processing and chunk-based, and the chunk-
based result which adopts CCRFs method only;
and the lase one (Model03) is the chunk-based
result which adopts CCRFs method with
searching algorithm.
</bodyText>
<table confidence="0.9996164">
method P R F
EDSs Berkeley 87.5746% 87.8365% 87.7053%
Proposed Method 88.5752% 88.6341% 88.6047%
CS Berkeley 84.4755% 84.9182% 84.6963%
Proposed Method 84.7535% 85.046% 84.8995%
</table>
<tableCaption confidence="0.964475">
Table 11: Comparative results of MNP parsing
</tableCaption>
<table confidence="0.978960055555556">
method P R F
EDSs Berkeley 91.3411% 91.1823% 91.2617%
Proposed Method 92.4669% 92.0765% 92.2713%
CS Berkeley 85.4388% 85.3023% 85.3705%
Proposed Method 87.3357% 87.0357% 87.1854%
Table12: Comparative results of Frame parsing
P R F
Model 01 85.42% 85.35% 85.39%
Berkeley 84.56% 84.62% 84.59%
Models 02 85.31% 85.30% 85.31%
Models 03 83.99% 83.77% 83.88%
Table13: Comparative results of EDSs
dj constituent fj constituent overall F
P R P R F F F
Model 01 78.64% 78.73% 78.69% 70.22% 71.62% 70.91% 74.80%
Berkeley 78.37% 78.16% 78.26% 69.43% 72.42% 70.89% 74.58%
Models 02 78.18% 78.30% 78.24% 70.20% 70.98% 70.59% 74.41%
Models 03 77.38% 77.41% 77.39% 70.39% 70.01% 70.24% 73.82%
</table>
<tableCaption confidence="0.878674">
Table14: Comparative results of CS
</tableCaption>
<bodyText confidence="0.999872125">
From Table 13, we can see that Model01
performance in EDSs is improved by 0.08%
than Model02, and the searching algorithm
helps little in EDSs analysis. From Table 14, we
can see that Model01 performance in CS is
improved by 0.4% than Model02, better than
Berkeley parser result with search algorism.
Overall, in EDSs analysis, Model01
performance is improved by 0.8% than
Berkeley parser, and in overall F-measure of CS,
Model01 performance is 0.22% higher than
Berkeley parser. From Table 13 and 14, We can
see that Model01 performance in EDSs is
improved by 1.51% than Model03 and the
Model01 in CS is improved by 0.98% than
Model03, and the MNP pre-processing helps.
</bodyText>
<sectionHeader confidence="0.999872" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999904827586207">
We participate in two tasks - EDS Analysis
and CS Parsing in CLPS-SIGHAN- ParsEval-
2010. We use divide-and-conquer strategy for
parsing and a chunking-based discriminative
approach to full parsing by using CRF for
chunking. As we all know, CRF is effective for
chunking task. However, the chunking result in
the current level is based on the upper level in
the chunking-based parsing approach, which
will enhance ambiguity problems when the
input of the current level contains non-terminal
symbols, therefore, the features used in
chunking is crucial. This paper, for effectively
using the information of partial trees that have
been already created, keeps the terminal
symbols in the node containing non-terminal
symbols for features. Our experiments show
that these features are effective for ambiguity
problems.
We suppose that MNP pre-processing before
statistical model can significantly simplify the
analysis of complex sentences, which will have
more satisfatory results compared with using
statistical model singly. The current results
show that the MNP pre-processing does International Workshop on Parsing Technology.
simplify the complex sentences. However, the 1997. 215-222.
performance of MNP recognition and the
parsing of MNP need to be improved, which
will be our next work.
</bodyText>
<sectionHeader confidence="0.998454" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999646952380953">
Yoshimasa Tsuruoka, Jun’ichi Tsujii, Sophia
Anaiakou. 2009. Fast Full Parsing by Linear-
Chain Conditional Random Fields. In
Proceedings of EACL’09, pages 790-798.
Xiao chen, Changning Huang, Mu li, Chunyu Kit.
2009. Better Parser Combination. In CIPS-
ParsEval-2009, pages 81-90.
Abney, S.. 1991. Parsing by chunks, Principle-Based
Parsing, Kluwer Academic Publishers.
Erik Tjong, Kim Sang. 2000. Transforming a
chunker to a parser. In J.Veenstra W.daelemans,
K Sima’ an and J. Zavrek, editors, Computational
Linguistics in the Netherlands 2000, Rodopi, page
177-188.
P.L. Shiuan, C.T.H. Ann. 1996. A Divided-and-
Conquer Strategy for Parsing. In Proc. of the
ACL/SIGPARSE 5th International Workshop on
Parsing Technologies. Santa Cruz, USA, 1996,
pages 57-66
C. Braun, G. Neumann, J, Piskorski. 2000. A Divide-
and-Conquer Strategy for Shallow Parsing of
German Free Texts. In Proc. of ANLP-2000.
Seattle, Washington, 2000, pages 239-246.
C.Lyon, B.Dickerson. 1997. Reducing the
Complexity of Parsing by a Method of
Decomposition International Workshop on
Parsing Technology, 1997, pages 215-222.
Qiaoli Zhou, Xin Liu, Xiaona Ren, Wenjing Lang,
Dongfeng Cai. 2009. Statistical parsing based on
Maximal Noun Phrase pre-processing. In CIPS-
ParsEval-2209.
P.L. Shiuan, C.T.H. Ann. A Divide-and-Conquer
Strategy for Parsing. In: Proc. of the
ACL/SIGPARSE 5th International Workshop on
Parsing Technologies. Santa Cruz, USA, 1996.
57-66.
C. Braun, G. Neumann, J. Piskorski. A Divide-and-
Conquer Strategy for Shallow Parsing of German
Free Texts. In: Proc. of ANLP-2000. Seattle,
Washington, 2000. 239-246.
C. Lyon, B. Dickerson. Reducing the Complexity of
Parsing by a Method of Decomposition.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.225904">
<title confidence="0.997053">SAU Report for the</title>
<author confidence="0.9326715">Qiaoli Zhou Wenjing Yingying Yan Wang Dongfeng Cai Lang Wang</author>
<affiliation confidence="0.702027333333333">Knowledge Engineering Research Center,Shenyang University,Shenyang,China Qiaoli_z@yahoo.com.cn</affiliation>
<abstract confidence="0.9881161">This paper presents our work for participation in the 2010 CIPS-SIGHAN evaluation on two tasks which are Event Description Sub-sentence (EDSs) Analysis and Complete Sentence (CS) Parsing in Chinese Parsing. The paper describes the implementation of our system as well as the results we have achieved and the analysis.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoshimasa Tsuruoka</author>
</authors>
<title>Jun’ichi Tsujii, Sophia Anaiakou.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL’09,</booktitle>
<pages>790--798</pages>
<marker>Tsuruoka, 2009</marker>
<rawString>Yoshimasa Tsuruoka, Jun’ichi Tsujii, Sophia Anaiakou. 2009. Fast Full Parsing by LinearChain Conditional Random Fields. In Proceedings of EACL’09, pages 790-798.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiao chen</author>
<author>Changning Huang</author>
<author>Mu li</author>
<author>Chunyu Kit</author>
</authors>
<title>Better Parser Combination. In</title>
<date>2009</date>
<booktitle>CIPSParsEval-2009,</booktitle>
<pages>81--90</pages>
<contexts>
<context position="11773" citStr="chen et al. 2009" startWordPosition="1994" endWordPosition="1997">nd head. The selected statistical method is CRF model. The first step is to train a CRF classifier to classify each context-free production into several categories. Then a rule-based method is used to post process the identification results and gets the final recognition results. The rule-based postprocessing module mainly uses rule base and case base to carry out post-processing. 3.6 Head finding based on CRFs The head finding procedure proceeds in the bottom-up fashion, so that the head words of productions in lower layers could be used as features for the productions of higher layers (Xiao chen et al. 2009). Atom template Definition CurPhraseTag The label of the current word LCh Word The left most child RCh Word The right most child LCh Pos The POS of the left most child MCh Pos The POS of the middle child RCh Pos The POS of the right most child NumCh The number of children CurPhraseTag± 1 The labels of the former phrase and the latter Table 5: Atom templates for Head finding Complex Template CurPhraseTag/ NumCh, CurPhraseTag/ LCh_Word, CurPhraseTag/LCh_Pos, CurPhraseTag/LCh_Pos/RCh_Pos, CurPhraseTag/NumCh/LCh_Pos/ RCh_Pos, CurPhraseTag/NumCh/LCh_Word/LCh_Pos/MCh_ Pos/RCh_Word/RCh_Pos, LCh_Word/</context>
</contexts>
<marker>chen, Huang, li, Kit, 2009</marker>
<rawString>Xiao chen, Changning Huang, Mu li, Chunyu Kit. 2009. Better Parser Combination. In CIPSParsEval-2009, pages 81-90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Abney</author>
</authors>
<title>Parsing by chunks, Principle-Based Parsing,</title>
<date>1991</date>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="5653" citStr="Abney, 1991" startWordPosition="901" endWordPosition="902">e comparative experimental results of POS tagging using two methods. Method EDSs CS precision precision CRF 92.83% 89.42% CRF + 93.96% 91.05% post-processing Table 2: Comparative POS tagging results 3 Parsing system The paper uses divide-and-conquer strategy (Shiuan 1996 et al., Braun 2000 et al., Lyon 1997 et al.)for parsing. Firstly, we recognize MNP for an input sentence, which divide the sentence into two kinds of parts. One kind is MNPs, and the other one is frame which is a new sentence generating by replacing MNP using its head word. Secondly, we use parsing approach based on chunking (Abney, 1991, Erik Tjong and Kim Sang, 2001) and a searching algorithm in decoding. Thirdly, we combine the parsing trees of MNPs and frame, which obtains the full parsing tree of the original sentence. Figure 3 shows the architecture of paring system. 3.1 MNP recognition Maximal Noun Phrase (MNP) is the noun phrase which is not contained by any other noun phrases. We use Berkeley parser (2009 1.0) for MNP recognition. We first use Berkeley parser to parse sentences after POS tagging, and then we tag MNPs from the parsing results. As the following example: Berkeley parser result: dj[ 中国/nS vp[ 重视/v vp[ 发展</context>
</contexts>
<marker>Abney, 1991</marker>
<rawString>Abney, S.. 1991. Parsing by chunks, Principle-Based Parsing, Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik Tjong</author>
<author>Kim Sang</author>
</authors>
<title>Transforming a chunker to a parser. In</title>
<date>2000</date>
<booktitle>Computational Linguistics in the Netherlands 2000, Rodopi,</booktitle>
<pages>177--188</pages>
<editor>J.Veenstra W.daelemans, K Sima’ an and J. Zavrek, editors,</editor>
<marker>Tjong, Sang, 2000</marker>
<rawString>Erik Tjong, Kim Sang. 2000. Transforming a chunker to a parser. In J.Veenstra W.daelemans, K Sima’ an and J. Zavrek, editors, Computational Linguistics in the Netherlands 2000, Rodopi, page 177-188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P L Shiuan</author>
<author>C T H Ann</author>
</authors>
<title>A Divided-andConquer Strategy for Parsing.</title>
<date>1996</date>
<booktitle>In Proc. of the ACL/SIGPARSE 5th International Workshop on Parsing Technologies.</booktitle>
<pages>57--66</pages>
<location>Santa Cruz, USA,</location>
<marker>Shiuan, Ann, 1996</marker>
<rawString>P.L. Shiuan, C.T.H. Ann. 1996. A Divided-andConquer Strategy for Parsing. In Proc. of the ACL/SIGPARSE 5th International Workshop on Parsing Technologies. Santa Cruz, USA, 1996, pages 57-66</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Braun</author>
<author>G Neumann</author>
<author>J Piskorski</author>
</authors>
<title>A Divideand-Conquer Strategy for Shallow Parsing of German Free Texts.</title>
<date>2000</date>
<booktitle>In Proc. of ANLP-2000.</booktitle>
<pages>239--246</pages>
<location>Seattle, Washington,</location>
<marker>Braun, Neumann, Piskorski, 2000</marker>
<rawString>C. Braun, G. Neumann, J, Piskorski. 2000. A Divideand-Conquer Strategy for Shallow Parsing of German Free Texts. In Proc. of ANLP-2000. Seattle, Washington, 2000, pages 239-246.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Dickerson C Lyon</author>
</authors>
<title>Reducing the Complexity of Parsing by a Method of Decomposition International Workshop on Parsing Technology,</title>
<date>1997</date>
<pages>215--222</pages>
<contexts>
<context position="5350" citStr="Lyon 1997" startWordPosition="848" endWordPosition="849"> the words not in training set. By analyzing the examples, we find that there are great deals of person names, location names, organization names and numbers, etc. And the words have characteristics when building word, therefore, we set up rules for processing. 2.3 Experiment results Table 2 shows the comparative experimental results of POS tagging using two methods. Method EDSs CS precision precision CRF 92.83% 89.42% CRF + 93.96% 91.05% post-processing Table 2: Comparative POS tagging results 3 Parsing system The paper uses divide-and-conquer strategy (Shiuan 1996 et al., Braun 2000 et al., Lyon 1997 et al.)for parsing. Firstly, we recognize MNP for an input sentence, which divide the sentence into two kinds of parts. One kind is MNPs, and the other one is frame which is a new sentence generating by replacing MNP using its head word. Secondly, we use parsing approach based on chunking (Abney, 1991, Erik Tjong and Kim Sang, 2001) and a searching algorithm in decoding. Thirdly, we combine the parsing trees of MNPs and frame, which obtains the full parsing tree of the original sentence. Figure 3 shows the architecture of paring system. 3.1 MNP recognition Maximal Noun Phrase (MNP) is the nou</context>
</contexts>
<marker>Lyon, 1997</marker>
<rawString>C.Lyon, B.Dickerson. 1997. Reducing the Complexity of Parsing by a Method of Decomposition International Workshop on Parsing Technology, 1997, pages 215-222.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiaoli Zhou</author>
</authors>
<title>Xin Liu, Xiaona Ren, Wenjing Lang,</title>
<date>2009</date>
<booktitle>In CIPSParsEval-2209.</booktitle>
<location>Dongfeng Cai.</location>
<marker>Zhou, 2009</marker>
<rawString>Qiaoli Zhou, Xin Liu, Xiaona Ren, Wenjing Lang, Dongfeng Cai. 2009. Statistical parsing based on Maximal Noun Phrase pre-processing. In CIPSParsEval-2209.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P L Shiuan</author>
<author>C T H Ann</author>
</authors>
<title>A Divide-and-Conquer Strategy for Parsing. In:</title>
<date>1996</date>
<booktitle>Proc. of the ACL/SIGPARSE 5th International Workshop on Parsing Technologies.</booktitle>
<pages>57--66</pages>
<location>Santa Cruz, USA,</location>
<marker>Shiuan, Ann, 1996</marker>
<rawString>P.L. Shiuan, C.T.H. Ann. A Divide-and-Conquer Strategy for Parsing. In: Proc. of the ACL/SIGPARSE 5th International Workshop on Parsing Technologies. Santa Cruz, USA, 1996. 57-66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Braun</author>
<author>G Neumann</author>
<author>J Piskorski</author>
</authors>
<title>A Divide-andConquer Strategy for Shallow Parsing of German Free Texts. In:</title>
<date>2000</date>
<booktitle>Proc. of ANLP-2000.</booktitle>
<pages>239--246</pages>
<location>Seattle, Washington,</location>
<marker>Braun, Neumann, Piskorski, 2000</marker>
<rawString>C. Braun, G. Neumann, J. Piskorski. A Divide-andConquer Strategy for Shallow Parsing of German Free Texts. In: Proc. of ANLP-2000. Seattle, Washington, 2000. 239-246.</rawString>
</citation>
<citation valid="false">
<authors>
<author>C Lyon</author>
<author>B Dickerson</author>
</authors>
<title>Reducing the Complexity of Parsing by a Method of Decomposition.</title>
<marker>Lyon, Dickerson, </marker>
<rawString>C. Lyon, B. Dickerson. Reducing the Complexity of Parsing by a Method of Decomposition.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>