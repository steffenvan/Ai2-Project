<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.747152">
Improving Word Alignment Using Syntactic Dependencies
</title>
<author confidence="0.836154">
Yanjun Ma1 Sylwia Ozdowska1 Yanli Sun2 Andy Way1
</author>
<affiliation confidence="0.723712">
1 School of Computing, Dublin City University, Dublin, Ireland
</affiliation>
<email confidence="0.890102">
{yma,sozdowska,away}@computing.dcu.ie
</email>
<affiliation confidence="0.9559115">
2 School of Applied Language and Intercultural Studies,
Dublin City University, Dublin, Ireland
</affiliation>
<email confidence="0.983243">
yanli.sun2@mail.dcu.ie
</email>
<sectionHeader confidence="0.9965" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99924144">
We introduce a word alignment framework
that facilitates the incorporation of syntax en-
coded in bilingual dependency tree pairs. Our
model consists of two sub-models: an anchor
word alignment model which aims to find a set
of high-precision anchor links and a syntax-
enhanced word alignment model which fo-
cuses on aligning the remaining words relying
on dependency information invoked by the ac-
quired anchor links. We show that our syntax-
enhanced word alignment approach leads to a
10.32% and 5.57% relative decrease in align-
ment error rate compared to a generative word
alignment model and a syntax-proof discrim-
inative word alignment model respectively.
Furthermore, our approach is evaluated ex-
trinsically using a phrase-based statistical ma-
chine translation system. The results show
that SMT systems based on our word align-
ment approach tend to generate shorter out-
puts. Without length penalty, using our word
alignments yields statistically significant im-
provement in Chinese–English machine trans-
lation in comparison with the baseline word
alignment.
</bodyText>
<sectionHeader confidence="0.999166" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999628714285714">
Automatic word alignment can be defined as the
problem of determining translational correspon-
dences at word level given a parallel corpus of
aligned sentences. Bilingual word alignment is a
fundamental component of most approaches to sta-
tistical machine translation (SMT). Dominant ap-
proaches to word alignment can be classified into
</bodyText>
<page confidence="0.989696">
69
</page>
<bodyText confidence="0.999301676470588">
two main schools: generative and discriminative
word alignment models.
Generative word alignment models, initially de-
veloped at IBM (Brown et al., 1993), and then
augmented by an HMM-based model (Vogel et al.,
1996), have provided powerful modeling capability
for word alignment. However, it is very difficult to
incorporate new features into these models. Dis-
criminative word alignment models, based on dis-
criminative training of a set of features (Liu et al.,
2005; Moore, 2005), on the other hand, are more
flexible to incorporate new features, and feature se-
lection is essential to the performance of the system.
Syntactic annotation of bilingual corpora, which
can be obtained more efficiently and accurately with
the advances in monolingual language processing,
is a potential information source for word align-
ment tasks. For example, Part-of-Speech (POS) tags
of source and target words can be used to tackle
the data sparseness problem in discriminative word
alignment (Liu et al., 2005; Blunsom and Cohn,
2006). Shallow parsing has also been used to pro-
vide relevant information for alignment (Ren et al.,
2007; Sun et al., 2000). Deeper syntax, e.g. phrase
or dependency structures, has been shown useful in
generative models (Wang and Zhou, 2004; Lopez
and Resnik, 2005), heuristic-based models (Ayan et
al., 2004; Ozdowska, 2004) and even for syntac-
tically motivated models such as ITG (Wu, 1997;
Cherry and Lin, 2006).
In this paper, we introduce an approach to im-
prove word alignment by incorporating syntactic de-
pendencies. Our approach is motivated by the fact
that words tend to be dependent on each other. If
</bodyText>
<note confidence="0.9761375">
Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 69–77,
ACL-08: HLT, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.999166866666667">
we can first obtain a set of reliable anchor links, we
could take advantage of the syntactic dependencies
relating unaligned words to aligned anchor words to
expand the alignment. Figure 1 gives an illustrating
example. Note that the link (2, 4) can be easily iden-
tified, but the link involving the fourth Chinese word
(a function word denoting ‘time’) (4,4) is hard. In
such cases, we can make use of the dependency re-
lationship (‘tclause’) between c2 and c4 to help the
alignment process. Given such an observation, our
model is composed of two related alignment models.
The first one is an anchor alignment model which is
used to find a set of anchor links; the other one is a
syntax-enhanced alignment model aiming to process
the words left unaligned after anchoring.
</bodyText>
<figureCaption confidence="0.996511">
Figure 1: How syntactic dependencies can help word
alignment: an example
</figureCaption>
<bodyText confidence="0.999966444444444">
The remainder of this paper is organized as fol-
lows. In Section 2, we introduce our syntax-
enhanced discriminative word alignment approach.
The feature functions used are described in Sec-
tion 3. Experimental setting and results are pre-
sented in Section 4 and 5 respectively. In Section 6,
we compare our approach with other related word
alignment approaches. Section 7 concludes the pa-
per and gives avenues for future work.
</bodyText>
<sectionHeader confidence="0.983953" genericHeader="introduction">
2 Word Alignment Model
</sectionHeader>
<subsectionHeader confidence="0.877493">
2.1 Notation
</subsectionHeader>
<bodyText confidence="0.974479434782609">
While in this paper we focus on Chinese–English,
the method proposed is applicable to any language
pair. The notation will assume Chinese–English
word alignment and Chinese–English MT. Here we
adopt a notation similar to (Brown et al., 1993).
Given a Chinese sentence cJ1 consisting of J words
{c1, ..., cJ} and an English sentence eI1 consisting of
I words e1, ..., eI, we define the alignment A be-
tween cJ1 and eI1 as a subset of the Cartesian product
of the word positions:
A C {(j, i) : j = 1, ..., J; i = 1, ..., I}
Our alignment representation is restricted so that
each source word can only be aligned to one tar-
get word. The alignment A consists of associations
j → i = aj from a source position j to a target po-
sition i = aj. The ‘null’ alignment aj = 0 with the
‘empty’ word e0 is used to account for source words
that are not aligned to any target word.
We use AA to denote a subset of A. The indices of
the K source words involved in AA are represented
as AK1 and the corresponding target indices for Ak
are represented as aAk. The unaligned source words
are represented as A.
</bodyText>
<subsectionHeader confidence="0.925704">
2.2 General Model
</subsectionHeader>
<bodyText confidence="0.9980075">
Given a source sentence cJ1 and target sentence eI1,
we seek to find the optimum alignment A� such that:
</bodyText>
<equation confidence="0.743009">
A� = argmaxP(A|cJ1 , eI1) (1)
A
</equation>
<bodyText confidence="0.999939166666667">
We use a model (2) that directly models the link-
age between source and target words similarly to (It-
tycheriah and Roukos, 2005). We decompose this
model into an anchor alignment model (3) and a
syntax-enhanced model (4) by distinguishing the an-
chor alignment from the non-anchor alignment.
</bodyText>
<equation confidence="0.991906">
p(aj|cJ1 , eI1, aj−1
1 ) (2)
Z� pǫ(AA|cJ 1 , eI 1) &apos; (3)
p(aj|cJ 1 , eI 1, aj−1
1 , AA) (4)
</equation>
<subsectionHeader confidence="0.986267">
2.3 Anchor Alignment Model
</subsectionHeader>
<bodyText confidence="0.999972">
The anchor alignment model pǫ(AA) aims to find a
set of high precision links. Various approaches can
be used for this purpose. In this paper we adopted
the following two approaches.
</bodyText>
<subsectionHeader confidence="0.814008">
2.3.1 Heuristics-based Approach
</subsectionHeader>
<bodyText confidence="0.9986478">
The problem of word alignment is regarded as a
process of word linkage disambiguation, i.e. choos-
ing the correct links between words from all com-
peting hypothesis (Melamed, 2000; Deng and Gao,
2007).
</bodyText>
<equation confidence="0.9937618">
J
p(A|cJ1 , eI1) =
j=0
1
j∈ A�
</equation>
<page confidence="0.972981">
70
</page>
<bodyText confidence="0.9920065">
We constrain the link probabilities in such a way
that:
</bodyText>
<equation confidence="0.9999055">
∀i′ ∈ {1, ..., I}, i′ =6i : p((j, i)) &gt; ǫ1 (5)
p((j, i′))
∀j′ ∈ {1, ..., J}, j′ =6 j : p((j, i))
p((j′, i)) &gt; ǫ2 (6)
</equation>
<bodyText confidence="0.997182">
Condition (5) implies that for the source word cj,
the link with the target word ei is more probable
(with reliability threshold ǫ1) than the link with any
other target word. Condition (6) guarantees that for
the target word ei, cj is the only most probable (with
threshold ǫ2) source word to be linked to.
</bodyText>
<sectionHeader confidence="0.701426" genericHeader="method">
2.3.2 Intersected Generative Word Alignment
Models
</sectionHeader>
<bodyText confidence="0.999741">
We can use the asymmetric IBM models for bidi-
rectional word alignment and get the intersection.
</bodyText>
<subsectionHeader confidence="0.969185">
2.4 Syntax-Enhanced Word Alignment Model
</subsectionHeader>
<bodyText confidence="0.9993035">
The syntax-enhanced model is used to model the
alignment of the words left unaligned after anchor-
ing. We directly model the linkage between source
and target words using a discriminative word align-
ment framework where various features can be in-
corporated. Given a source word cj and the target
sentence eI1, we search for the alignment aj such
that:
</bodyText>
<equation confidence="0.940353">
aj = argmax
aj {pλ�1 (aj|cJ1, eI1, aj−1
1 , AA)} (7)
= argmax {�M J I j
m=1 λmhm(c1,e1,a1, AA, Tc, Te)}
aj
</equation>
<bodyText confidence="0.9999435">
In this decision rule, we assume that a set of highly
reliable anchor alignments AA has been obtained,
and Tc (resp. Te) is used to denote the dependency
structure for source (resp. target) language. In such
a framework, various machine learning techniques
can be used for parameter estimation.
</bodyText>
<sectionHeader confidence="0.9497755" genericHeader="method">
3 Feature Function for Syntax-Enhanced
Model
</sectionHeader>
<bodyText confidence="0.999891">
The various features used in our syntax-enhanced
model can be classified into three groups: statistics-
based features, syntactic features and relative distor-
tion features.
</bodyText>
<subsectionHeader confidence="0.7134435">
3.1 Statistics-based Features
3.1.1 IBM model 1 score
</subsectionHeader>
<bodyText confidence="0.9999795">
IBM model 1 is a position-independent word
alignment model which is often used to boot-
strap parameters for more complex models. Model
1 models the conditional distribution and uses a
uniform distribution for the dependencies between
source word positions and target word positions.
</bodyText>
<equation confidence="0.99241625">
p(J|I) H
p(cj|eaj) (8)
(I + 1)J
j=1
</equation>
<subsectionHeader confidence="0.407201">
3.1.2 Log-likelihood ratio
</subsectionHeader>
<bodyText confidence="0.9452387">
The log-likelihood ratio statistic has been found to
be accurate for modeling the associations between
rare events (Dunning, 1993). It has also been suc-
cessfully used to measure the associations between
word pairs (Melamed, 2000; Moore, 2005). Given
the following contingency table:
cj ¬cj
ei a b
¬ei c d
the log-likelihood ratio can be defined as:
</bodyText>
<equation confidence="0.997941">
G2c�&apos;Z) = —2lo e B(ala+b,p1)B(c|c + d,p2)
( 9 B(a|a + b,p)B(c|c + d,p)
</equation>
<bodyText confidence="0.999903">
where B(k|n, p) = (nk)pk(1 − p)n−k are binomial
probabilities. The probability parameters can be ob-
tained using maximum likelihood estimates:
</bodyText>
<equation confidence="0.856757">
c (9)
c + d
p= a + b + c + d
a + c (10)
3.1.3 POS translation probability
</equation>
<bodyText confidence="0.999959">
The POS tags can provide effective information
for addressing the data sparseness problem using the
lexical features (Liu et al., 2005; Blunsom and Cohn,
2006). The POS translation probability can be easily
obtained using maximum likelihood estimation from
an annotated corpus:
</bodyText>
<equation confidence="0.99687525">
COL(Tc,Te)
Pr(Tc|Te) = (11)
COF(Te)
J
Pr(cJ1 , aJ1 |eI1) =
a =
a + b,p2
p1 =
</equation>
<page confidence="0.961509">
71
</page>
<bodyText confidence="0.999983">
where Tc is a Chinese word’s POS tag and Te is an
English word’s POS tag. COL(Tc, Te) is the count
of Tc and Te being linked to each other in the corpus,
and COF(Te) is the frequency of Te in the corpus.
</bodyText>
<subsectionHeader confidence="0.999571">
3.2 Syntactic Features
</subsectionHeader>
<bodyText confidence="0.989661166666667">
The dependency relation Re (resp. Rc) between two
English (resp. Chinese) words ei and ei′ (resp. cj
and cj′) in the dependency tree of the English sen-
tence eI1 (resp. Chinese sentence cJ1) can be repre-
sented as a triple &lt;ei, Re, ei′&gt;(resp. &lt;cj, Rc, ej′&gt;).
Given cJ1 , eI1 and their syntactic dependency trees
Tc,, TeI, if ei is aligned to cj and ei′ aligned to
cj , according to the dependency correspondence as-
sumption (Hwa et al., 2002), there exists a triple
&lt;cj, Rc, cj′&gt;.
While we are not aiming to justify the feasibil-
ity of the dependency correspondence assumption
by proving to what extent Re = Rc under the con-
dition described above, we do believe that cj and cj′
are likely to be dependent on each other. Given the
anchor alignment AA, a candidate link (j, i) and the
dependency trees, we can design four classes of fea-
ture functions.
</bodyText>
<subsectionHeader confidence="0.856933">
3.2.1 Agreement features
</subsectionHeader>
<bodyText confidence="0.909766076923077">
The agreement features can be further classi-
fied into dependency agreement features and depen-
dency label agreement features. Given a candidate
link (j, i) and the anchor alignment AA, the depen-
dency agreement (DA) feature function is defined as
follows:
We can define the dependency label agreement fea-
ture1 as follows:
hDLA−1 = { 1 if ∃ &lt;cj, Rc, cj′&gt;, &lt;ei, Re, ei′&gt;
and (j′, i′) ∈ AA,Rc = Re,
0 otherwise.
Similarly we can obtain hDLA−2 by changing the
dependency direction.
</bodyText>
<subsectionHeader confidence="0.703825">
3.2.2 Source word dependency features
</subsectionHeader>
<bodyText confidence="0.999188125">
Given a candidate link (j, i) and anchor alignment
AA, source language dependency features are used
to capture the dependency label between a source
word cj and a source anchor word ck ∈ A. For
example, a feature function relating to dependency
type ‘PRD’ can be defined as:
By changing the direction we can obtain
hsrc−2−PRD.
</bodyText>
<subsectionHeader confidence="0.491832">
3.2.3 Target word dependency features
</subsectionHeader>
<bodyText confidence="0.999925333333333">
Target word dependency features can be defined
in a similar way as source word dependency fea-
tures.
</bodyText>
<subsectionHeader confidence="0.705188">
3.2.4 Target anchor feature
</subsectionHeader>
<bodyText confidence="0.9991265">
The target anchor feature defines whether the tar-
get word ei is an anchor word.
</bodyText>
<figure confidence="0.904727">
(14)
hsrc−1−PRD = { 1 if ∃ &lt;cj, Rc, cj′&gt; (15)
and Rc =‘PRD’,
0 otherwise.
hDA−1 = { 1 if ∃ &lt;cj, Rc, cj′&gt;, &lt;ei, Re, ei′&gt; (12) �
and (j′, i′) ∈ An,, 1 if i ∈ a�,
0 otherwise. hsrc−1−PRD = (16)
0 otherwise.
3.3 Relative distortion feature
</figure>
<bodyText confidence="0.698765333333333">
By changing the dependency direction between the
words cj and cj′, we can derive another dependency
agreement feature:
</bodyText>
<footnote confidence="0.427692666666667">
{ 1 if ∃ &lt;cj′, Rc, cj&gt;, &lt;ei′, Re, ei&gt;
and (j′, i′) ∈ An,,
0 otherwise.
</footnote>
<bodyText confidence="0.999658666666667">
We can design features encoding the relative dis-
tortion information which can be used to evaluate
a candidate link by computing its relative position
change with respect to the anchor alignment. The
relative position change of a candidate link l = (j, i)
is formally defined as follows:
</bodyText>
<footnote confidence="0.872168">
1Note that we used the same dependency parser for source
and target language parsing.
</footnote>
<equation confidence="0.972673833333333">
hDA−2 =
(13)
72
D(l) = min(|dL|, |dR|) (17)
dL = (j − jL) − (i − iL) (18)
dR = (j − jR) − (i − iR) (19)
</equation>
<bodyText confidence="0.958661666666667">
where (iL, jL) is the leftmost anchor link of l,
(iR, jR) is the rightmost anchor link of l. The less
the relative position changes, the more likely the
candidate link is. With a set of anchor alignments,
we can obtain the distribution of the relative posi-
tion changes from an annotated corpus using maxi-
mum likelihood estimation. In our experiments, we
used the following four probabilities: p(D = 0),
p(D = 1, 2), p(D = 3,4) and p(D &gt; 4).
</bodyText>
<sectionHeader confidence="0.998226" genericHeader="method">
4 Experimental Setting
</sectionHeader>
<subsectionHeader confidence="0.84776">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.99962925">
The experiments were carried out using the
Chinese–English datasets provided within the
IWSLT 2007 evaluation campaign (Fordyce, 2007),
extracted from the Basic Travel Expression Corpus
(BTEC) (Takezawa et al., 2002). This multilingual
speech corpus contains sentences similar to those
that are usually found in phrase-books for tourists
going abroad.
We tagged all the sentences in the training and de-
vset3 using a maximum entropy-based POS tagger–
MXPOST (Ratnaparkhi, 1996), trained on the Penn
English and Chinese Treebanks. Both Chinese and
English sentences are parsed using the Malt depen-
dency parser (Nivre et al., 2007), which achieved
84% and 88% labelled attachment scores for Chi-
nese and English respectively.
</bodyText>
<subsubsectionHeader confidence="0.49118">
4.1.1 Word Alignment
</subsubsectionHeader>
<bodyText confidence="0.999986333333333">
We manually annotated word alignments on de-
vset3. Since manual word alignment is an ambigu-
ous task, we also explicitly allow for ambiguous
alignments, i.e. the links are marked as sure (S) or
possible (P) (Och and Ney, 2003). IWSLT devset3
consists of 502 sentence pairs after cleaning. We
used the first 300 sentence pairs for training, the fol-
lowing 50 sentence pairs as validation set and the
last 152 sentence pairs for testing.
</bodyText>
<subsubsectionHeader confidence="0.848041">
4.1.2 Machine Translation
</subsubsectionHeader>
<bodyText confidence="0.999895333333333">
Training was performed using the default training
set (39,952 sentence pairs), to which we added the
set devset1 (506 sentence pairs).2 We used devset2
(506 sentence pairs, 16 references) to tune various
parameters in the MT system and IWSLT 2007 test
set (489 sentence pairs, 6 references) for testing.
</bodyText>
<subsectionHeader confidence="0.9996">
4.2 Alignment Training and Search
</subsectionHeader>
<bodyText confidence="0.999984263157895">
In our experiments, we treated anchor alignment and
syntax-enhanced alignment as separate processes in
a pipeline. The anchor alignments are kept fixed so
that the parameters in the syntax-enhanced model
can be optimized.3 We used the support vector ma-
chine (SVM) toolkit–SVM light4 to optimize the
parameters in (7). Our model is constrained in such
a way that each source word can only be aligned to
one target word. Therefore, in training, we trans-
form each possible link involving the words left un-
aligned after anchoring into an event. In testing, the
source words are consumed in sequence and the tar-
get words serve as states. The SVM dual variable
was used to measure the reliability of each candidate
link and the alignment link for each word is made
independently, which makes the alignment search
much easier. A threshold t was set as the minimal
reliability score for each link. t is optimized accord-
ing to alignment error rate (21) on the validation set.
</bodyText>
<subsectionHeader confidence="0.8281955">
4.3 Baselines
4.3.1 Word Alignment
</subsectionHeader>
<bodyText confidence="0.9999606">
We used the GIZA++ implementation of IBM
word alignment model 4 (Brown et al., 1993; Och
and Ney, 2003) for word alignment, and the heuris-
tics described in (Och and Ney, 2003) to derive the
intersection and refined alignment.
</bodyText>
<subsectionHeader confidence="0.605981">
4.3.2 Machine Translation
</subsectionHeader>
<bodyText confidence="0.999859">
We use a standard log-linear phrase-based SMT
(PB-SMT) model as a baseline: GIZA++ implemen-
tation of IBM word alignment model 4,5 the refine-
</bodyText>
<footnote confidence="0.707975">
2More specifically, we chose the first English reference from
the 16 references and the Chinese sentence to construct new
sentence pairs.
3Note our anchor alignment does not achieve 100% preci-
sion. Since we performed precision-oriented alignment for the
anchor alignment model, the errors in anchor alignment will not
bring much noise into the syntax-enhanced model.
4http://svmlight.joachims.org/
5More specifically, we performed 5 iterations of Model 1, 5
iterations of HMM, 3 iterations of Model 3, and 3 iterations of
Model 4.
</footnote>
<page confidence="0.999339">
73
</page>
<bodyText confidence="0.999157333333333">
ment and phrase-extraction heuristics described in
(Koehn et al., 2003), minimum-error-rate training
(Och, 2003), a trigram language model with Kneser-
Ney smoothing trained with SRILM (Stolcke, 2002)
on the English side of the training data, and Moses
(Koehn et al., 2007) to decode.
</bodyText>
<subsectionHeader confidence="0.981474">
4.4 Evaluation
</subsectionHeader>
<bodyText confidence="0.9999022">
We evaluate the intrinsic quality of predicted align-
ment A with precision, recall and alignment error
rate (AER). Slightly differently from (Och and Ney,
2003), we use possible alignments in computing re-
call.
</bodyText>
<sectionHeader confidence="0.992536" genericHeader="method">
5 Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.979773">
5.1 Word Alignment
</subsectionHeader>
<bodyText confidence="0.999837384615385">
We performed word alignment bidirectionally using
our approach to obtain the union and compared our
results with two strong baselines based on generative
word alignment models. The results are shown in
Table 1. We can see that both the syntax-enhanced
model based on HMM intersection anchors (Syntax-
HMM) and on IBM model 4 anchors (Syntax-Model
4) are better than the pure generative word alignment
models. Our approach is superior in precision with
a disadvantage in recall. The best result achieved
10.32% relative decrease in AER compared to the
baseline when we use IBM model 4 intersection to
obtain the set of anchor alignments.
</bodyText>
<table confidence="0.765137">
model precision recall f-score AER
HMM refined 0.8043 0.7592 0.7811 0.2059
Syntax-HMM 0.8744 0.7304 0.7959 0.1845
Model 4 refined 0.7941 0.7987 0.7964 0.1929
Syntax-Model4 0.8566 0.7685 0.8102 0.1730
</table>
<tableCaption confidence="0.9978575">
Table 1: Comparing syntax-enhanced approach with gen-
erative word alignment
</tableCaption>
<bodyText confidence="0.841940555555556">
5.1.1 The Influence of Anchor Alignment
Quality
As we can see in Table 2, our precision-oriented
approach to acquire anchor alignments was accom-
plished quite well. All four different anchor align-
ment models achieved high precision. However, the
recall differs dramatically, with model 4 achieving
the highest recall and the heuristics-based approach
receiving the lowest. To investigate the influence
</bodyText>
<table confidence="0.8665458">
anchor model precision recall f-measure AER
Heuristics 0.9774 0.4047 0.5724 0.3947
Model 1 0.9509 0.5011 0.6563 0.3157
HMM 0.9802 0.5327 0.6903 0.2809
Model 4 0.9777 0.5677 0.7179 0.2533
</table>
<tableCaption confidence="0.998674">
Table 2: Performance of anchor alignment
</tableCaption>
<bodyText confidence="0.998088777777778">
of the anchor alignment model, we first obtained
the intersection of the words left unaligned after an-
choring using each of the anchor alignment models.
We evaluate the alignment of these words against
the gold-standard alignments involving these words.
The influence of anchor alignment on the perfor-
mance of the syntax-enhanced model can be seen
in Table 3. The performance of the syntax-enhanced
model is closely related to that of the anchor align-
ment method. As can be seen from Table 2 and
3, HMM anchoring achieves the best precision and
so does the syntax-enhanced alignment; IBM model
4 achieves the best recall and so does the syntax-
enhanced alignment. Finally, the best alignment per-
formances are obtained with IBM model 4 anchor-
ing, with the difference in recall between HMM and
IBM model 4 anchoring being more significant than
the difference in precision.
</bodyText>
<table confidence="0.8517208">
anchor model precision recall f-score AER
Heuristics 0.4505 0.3270 0.3790 0.6210
Model 1 0.5538 0.3894 0.4573 0.5427
HMM 0.5932 0.3611 0.4489 0.5511
Model 4 0.5660 0.4216 0.4832 0.5168
</table>
<tableCaption confidence="0.954582">
Table 3: Influence of anchor alignment in syntax-
enhanced model
</tableCaption>
<equation confidence="0.5367525">
AER(S, P; A) = 1 _ |A n S |+ |A n P |(21)
|A |+ |S|
</equation>
<bodyText confidence="0.391478">
We also extrinsically measure the word alignment
quality via a Chinese–English translation task. The
translation output is measured using BLEU (Pap-
ineni et al., 2002).
</bodyText>
<subsubsectionHeader confidence="0.830153">
5.1.2 The Influence of Syntactic Dependencies
</subsubsectionHeader>
<bodyText confidence="0.813394">
on Word Alignment
The influence of incorporating syntactic depen-
dencies into the word alignment process is shown
</bodyText>
<figure confidence="0.7884286">
recall = |A n P |, precision = |A n P|
|
||A|
P
(20)
</figure>
<page confidence="0.993656">
74
</page>
<bodyText confidence="0.998549625">
in Table 4. Syntax plays a positive role in all differ-
ent anchor alignment configurations. The influence
grows proportionally to the strength of the anchor
alignment model. With the Model 4 intersection
used as the set of anchor alignments, adding syn-
tactic dependency features into the syntax-enhanced
alignment model yields a 5.57% relative decrease in
AER.
</bodyText>
<table confidence="0.996337923076923">
model precision recall f-score AER
Heuristics
no syntax 0.8362 0.6751 0.7470 0.2302
w. syntax 0.8376 0.6894 0.7563 0.2240
Model 1 0.8759 0.6902 0.7720 0.2045
no syntax
w. syntax 0.8542 0.7160 0.7790 0.2011
HMM 0.8655 0.7168 0.7841 0.1952
no syntax
w. syntax 0.8744 0.7304 0.7959 0.1845
Model 4 0.8697 0.7340 0.7961 0.1832
no syntax
w. syntax 0.8566 0.7685 0.8102 0.1730
</table>
<tableCaption confidence="0.981482">
Table 4: Influence of syntactic dependencies on word
alignment
</tableCaption>
<subsectionHeader confidence="0.846534">
5.1.3 Contribution of Different Feature Classes
</subsectionHeader>
<bodyText confidence="0.9990856">
We interpret the contribution of each feature in
terms of feature weights in SVM model training.
The weights for the most discriminative features in
each feature class in Chinese–English word align-
ment (using HMM intersection as anchor align-
ment) are shown in Table 5. As we can see, all
statistics-based features are informative. Two target
dependency features are informative: ‘PRD’ denot-
ing ‘predicative’ dependency, and ‘AMOD’ denot-
ing ‘adjective/adverb modifier’ dependency.
</bodyText>
<table confidence="0.997630777777778">
weight
Model 1 Score 0.1416
POS 0.0540
Log-likelihood Ratio 0.0856
relative distortion 0.0606
DA-1 0.0227
DLA-2 0.0927
tgt-1-PRD 0.0961
tgt-2-AMOD 0.0621
</table>
<tableCaption confidence="0.999682">
Table 5: Weights of some informative features
</tableCaption>
<subsectionHeader confidence="0.9957">
5.2 Machine Translation
</subsectionHeader>
<bodyText confidence="0.999951380952381">
Research has shown that an increase in AER does
not necessarily imply an improvement in translation
quality (Liang et al., 2006) and vice-versa (Vilar et
al., 2006). Hereafter, we used a Chinese–English
MT task to extrinsically evaluate the quality of our
word alignment.
Table 6 shows the influence of our word align-
ment approach on MT quality.6 On development set,
we achieved statistically significant improvement
using both our syntax-enhanced models—Syntax-
HMM (p&lt;0.002) and Syntax-Model 4 (p&lt;0.008).
On the test set, we observed that the MT output
based on our alignment model tends to be shorter
than the reference translations and the BLEU score
is considerably penalized. If we ignore the length
penalty (‘BP’ in Table 6) in significance testing, the
improvement on test set is also statistically signif-
icant: p&lt;0.04 for both Syntax-HMM and Syntax-
Model 4. However, an indepth manual analysis
needs to be carried out in order to determine the ex-
act nature of the shorter sentences derived.
</bodyText>
<table confidence="0.9983825">
dev. set test set
Baseline 0.5412 0.3510 (BP=0.96)
Syntax-HMM 0.6015 0.3409 (BP=0.86)
Syntax-Model4 0.5834 0.3585 (BP=0.91)
</table>
<tableCaption confidence="0.999535">
Table 6: The Influence of Word Alignment on MT
</tableCaption>
<sectionHeader confidence="0.690466" genericHeader="method">
6 Comparison with Previous Work
</sectionHeader>
<bodyText confidence="0.999877692307692">
Our syntax-enhanced model is a discriminative word
alignment model. Certain generative word align-
ment models (e.g. HMM or IBM 4) also take
the first-order dependencies into account. How-
ever, long distance dependencies between words are
hard to incorporate into these models because of
the explosive number of parameters. On the other
hand, like existing discriminative models, our ap-
proach uses a set of informative features based on
co-occurrence statistics, e.g. log-likelihood ratio
and DICE score. The advantage of our approach is
the mechanism by which syntactic features may be
incorporated.
</bodyText>
<footnote confidence="0.9951655">
6Note that the only difference between our MT system and
the baseline PB-SMT system is the word alignment component.
</footnote>
<page confidence="0.998923">
75
</page>
<bodyText confidence="0.999944133333333">
Some previous research also tried to make use
of syntax in word alignment. (Wang and Zhou,
2004) investigated the benefit of monolingual pars-
ing for alignment. They learned a generalized word
association measure (crosslingual word similarities)
based on monolingual dependency structures and
improved alignment performances over IBM model
2 and certain heuristic-based models. (Cherry and
Lin, 2006) used dependency structures as soft con-
straints to improve word alignment in an ITG frame-
work. Compared to these models, our approach di-
rectly takes advantage of dependency relations as
they are transformed into feature functions incorpo-
rated into a discriminative word alignment frame-
work.
</bodyText>
<sectionHeader confidence="0.992776" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999976111111111">
In this paper, we proposed a model that can facili-
tate the incorporation of syntax into word alignment
and measured the combination of a set of syntactic
features. Experimental results have shown that syn-
tax is useful in word alignment and especially effec-
tive in improving the recall. We have also observed
that in our word alignment framework, the two sub-
models are closely related and the quality of the an-
chor alignment model plays an important role in the
system performance.
The promising results will lead us to improve our
model in the following aspects. First, the two sub-
models in our approach are two separate processes
performed in pipeline. We plan to jointly optimize
the two models in one go. Second, some of our
experiments used complex IBM models, e.g. IBM
Model 4, to obtain anchor alignment. We plan to
boostrap the alignment using simple heuristics with-
out relying on complex IBM models. Third, the
alignment searching process assumed the alignment
link for each word is made independently. A feasible
markovian assumption will be tested for searching.
Fourth, a comparison with traditional discriminative
word alignment models is also necessary to justify
the merits of our approach. Finally, we also plan to
adapt our approach to larger data sets and more lan-
guage pairs.
</bodyText>
<sectionHeader confidence="0.995466" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999515">
This work is supported by Science Foundation Ire-
land (grant number OS/IN/1732). Prof. Rebecca
Hwa from University of Pittsburgh and Dr. Yang
Liu from the Institute of Computing Technology,
Chinese Academy of Sciences, are kindly acknowl-
edged for providing us with their word alignment
guidelines. We would also like to thank the anony-
mous reviewers for their insightful comments.
</bodyText>
<sectionHeader confidence="0.997812" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.962522097560976">
Necip Fazil Ayan, Bonnie Dorr, and Nizar Habash.
2004. Multi-align: Combining linguistic and statis-
tical techniques to improve alignments for adaptable
mt. In Proceedings of the 6th Conference of the AMTA
(AMTA-2004), pages 17–26, Washington DC.
Phil Blunsom and Trevor Cohn. 2006. Discrimina-
tive word alignment with conditional random fields.
In Proceedings of the 21st International Conference
on Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics,
pages 65–72, Sydney, Australia.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263–311.
Colin Cherry and Dekang Lin. 2006. Soft syntactic
constraints for word alignment through discriminative
training. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th Annual
Meeting ofthe Association for Computational Linguis-
tics, pages 105–112, Sydney, Australia.
Yonggang Deng and Yuqing Gao. 2007. Guiding sta-
tistical word alignment models with prior knowledge.
In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, pages 1–8,
Prague, Czech Republic.
Ted Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Linguis-
tics, 19(1):61–74.
Cameron Shaw Fordyce. 2007. Overview of the IWSLT
2007 Evaluation Campaign. In Proceedings of the In-
ternational Workshop on Spoken Language Transla-
tion, pages 1–12, Trento, Italy.
Rebecca Hwa, Philip Resnik, Amy Weinberg, and Okan
Kolak. 2002. Evaluating translational correspondence
using annotation projection. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics, pages 392–399, Philadelphia, PA.
Abraham Ittycheriah and Salim Roukos. 2005. A max-
imum entropy word aligner for Arabic-English ma-
chine translation. In Proceedings ofHuman Language
</reference>
<page confidence="0.951399">
76
</page>
<reference confidence="0.995493160377358">
Technology Conference and Conference on Empirical
Methods in Natural Language Processing, pages 89–
96, Vancouver, British Columbia, Canada.
Philipp Koehn, Franz Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 48–54, Edmonton, Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics Companion Volume Pro-
ceedings of the Demo and Poster Sessions, pages 177–
180, Prague, Czech Republic.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of Human Lan-
guage Technology Conference and Conference on
Empirical Methods in Natural Language Processing,
pages 104–111, New York, NY.
Yang Liu, Qun Liu, and Shouxun Lin. 2005. Log-linear
models for word alignment. In Proceedings of the
43rd Annual Meeting of the Association for Compu-
tational Linguistics, pages 459–466, Ann Arbor, MI.
Adam Lopez and Philip Resnik. 2005. Improved HMM
alignment models for languages with scarce resources.
In Proceedings of the ACL Workshop on Building and
Using Parallel Texts, pages 83–86, Ann Arbor, Michi-
gan, June.
I. Dan Melamed. 2000. Models of translational equiv-
alence among words. Computational Linguistics,
26(2):221–249.
Robert C. Moore. 2005. A discriminative framework for
bilingual word alignment. In Proceedings of Human
Language Technology Conference and Conference on
Empirical Methods in Natural Language Processing,
pages 81–88, Vancouver, British Columbia, Canada.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov,
and Ervin Marsi. 2007. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95–135.
Franz Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Com-
putational Linguistics, 29(1):19–51.
Franz Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics, pages 160–167, Sapporo, Japan.
Sylwia Ozdowska. 2004. Identifying correspondences
between words: an approach based on a bilingual syn-
tactic analysis of French/English parallel corpora. In
Proceedings of the COLING’04 Workshop on Multi-
lingual Linguistic Resources, pages 49–56, Geneva,
Switzerland.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics, pages 311–318, Philadelphia, PA.
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. In Eric Brill and Ken-
neth Church, editors, Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing, pages 133–142, Somerset, NJ.
Dengjun Ren, Hua Wu, and Haifeng Wang. 2007. Im-
proving statistical word alignment with various clues.
In Machine Translation Summit XI, pages 391–397,
Copenhagen, Denmark.
Andrea Stolcke. 2002. SRILM – An extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing,
pages 901–904, Denver, CO.
Le Sun, Youbing Jin, Lin Du, and Yufang Sun. 2000.
Word alignment of English-Chinese bilingual corpus
based on chunks. In Proceedings of the 2000 Joint
SIGDAT conference on Empirical Methods in Natural
Language Processing and very large corpora, pages
110–116.
Toshiyuki Takezawa, Eiichiro Sumita, Fumiaki Sugaya,
Hirofumi Yamamoto, and Seiichi Yamamoto. 2002.
Toward a broad-coverage bilingual corpus for speech
translation of travel conversations in the real world.
In Proceedings of Third International Conference on
Language Resources and Evaluation 2002, pages 147–
152, Las Palmas, Spain.
David Vilar, Maja Popovic, and Hermann Ney. 2006.
AER: Do we need to “improve” our alignments? In
Proceedings of the International Workshop on Spoken
Language Translation, pages 205–212, Kyoto, Japan.
Stefan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of the 16th International Con-
ference on Computational Linguistics, pages 836–841,
Copenhagen, Denmark.
Wei Wang and Ming Zhou. 2004. Improving word align-
ment models using structured monolingual corpora.
In Dekang Lin and Dekai Wu, editors, Proceedings
of Conference on Empirical Methods in Natural Lan-
guage Processing, pages 198–205, Barcelona, Spain.
Dekai Wu. 1997. Stochastic Inversion Transduction
Grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377–403.
</reference>
<page confidence="0.999129">
77
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.752523">
<title confidence="0.999944">Improving Word Alignment Using Syntactic Dependencies</title>
<author confidence="0.999404">Sylwia Yanli Andy</author>
<affiliation confidence="0.951587">of Computing, Dublin City University, Dublin, of Applied Language and Intercultural Dublin City University, Dublin,</affiliation>
<email confidence="0.947088">yanli.sun2@mail.dcu.ie</email>
<abstract confidence="0.996523423076923">We introduce a word alignment framework that facilitates the incorporation of syntax encoded in bilingual dependency tree pairs. Our model consists of two sub-models: an anchor word alignment model which aims to find a set of high-precision anchor links and a syntaxenhanced word alignment model which focuses on aligning the remaining words relying on dependency information invoked by the acquired anchor links. We show that our syntaxenhanced word alignment approach leads to a 10.32% and 5.57% relative decrease in alignment error rate compared to a generative word model and a discriminative word alignment model respectively. Furthermore, our approach is evaluated extrinsically using a phrase-based statistical machine translation system. The results show that SMT systems based on our word alignment approach tend to generate shorter outputs. Without length penalty, using our word alignments yields statistically significant improvement in Chinese–English machine translation in comparison with the baseline word alignment.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Necip Fazil Ayan</author>
<author>Bonnie Dorr</author>
<author>Nizar Habash</author>
</authors>
<title>Multi-align: Combining linguistic and statistical techniques to improve alignments for adaptable mt.</title>
<date>2004</date>
<booktitle>In Proceedings of the 6th Conference of the AMTA (AMTA-2004),</booktitle>
<pages>17--26</pages>
<location>Washington DC.</location>
<contexts>
<context position="3064" citStr="Ayan et al., 2004" startWordPosition="457" endWordPosition="460">rately with the advances in monolingual language processing, is a potential information source for word alignment tasks. For example, Part-of-Speech (POS) tags of source and target words can be used to tackle the data sparseness problem in discriminative word alignment (Liu et al., 2005; Blunsom and Cohn, 2006). Shallow parsing has also been used to provide relevant information for alignment (Ren et al., 2007; Sun et al., 2000). Deeper syntax, e.g. phrase or dependency structures, has been shown useful in generative models (Wang and Zhou, 2004; Lopez and Resnik, 2005), heuristic-based models (Ayan et al., 2004; Ozdowska, 2004) and even for syntactically motivated models such as ITG (Wu, 1997; Cherry and Lin, 2006). In this paper, we introduce an approach to improve word alignment by incorporating syntactic dependencies. Our approach is motivated by the fact that words tend to be dependent on each other. If Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 69–77, ACL-08: HLT, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics we can first obtain a set of reliable anchor links, we could take advantage of the syntact</context>
</contexts>
<marker>Ayan, Dorr, Habash, 2004</marker>
<rawString>Necip Fazil Ayan, Bonnie Dorr, and Nizar Habash. 2004. Multi-align: Combining linguistic and statistical techniques to improve alignments for adaptable mt. In Proceedings of the 6th Conference of the AMTA (AMTA-2004), pages 17–26, Washington DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
</authors>
<title>Discriminative word alignment with conditional random fields.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>65--72</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="2759" citStr="Blunsom and Cohn, 2006" startWordPosition="408" endWordPosition="411">ased on discriminative training of a set of features (Liu et al., 2005; Moore, 2005), on the other hand, are more flexible to incorporate new features, and feature selection is essential to the performance of the system. Syntactic annotation of bilingual corpora, which can be obtained more efficiently and accurately with the advances in monolingual language processing, is a potential information source for word alignment tasks. For example, Part-of-Speech (POS) tags of source and target words can be used to tackle the data sparseness problem in discriminative word alignment (Liu et al., 2005; Blunsom and Cohn, 2006). Shallow parsing has also been used to provide relevant information for alignment (Ren et al., 2007; Sun et al., 2000). Deeper syntax, e.g. phrase or dependency structures, has been shown useful in generative models (Wang and Zhou, 2004; Lopez and Resnik, 2005), heuristic-based models (Ayan et al., 2004; Ozdowska, 2004) and even for syntactically motivated models such as ITG (Wu, 1997; Cherry and Lin, 2006). In this paper, we introduce an approach to improve word alignment by incorporating syntactic dependencies. Our approach is motivated by the fact that words tend to be dependent on each ot</context>
<context position="9809" citStr="Blunsom and Cohn, 2006" startWordPosition="1632" endWordPosition="1635">asure the associations between word pairs (Melamed, 2000; Moore, 2005). Given the following contingency table: cj ¬cj ei a b ¬ei c d the log-likelihood ratio can be defined as: G2c�&apos;Z) = —2lo e B(ala+b,p1)B(c|c + d,p2) ( 9 B(a|a + b,p)B(c|c + d,p) where B(k|n, p) = (nk)pk(1 − p)n−k are binomial probabilities. The probability parameters can be obtained using maximum likelihood estimates: c (9) c + d p= a + b + c + d a + c (10) 3.1.3 POS translation probability The POS tags can provide effective information for addressing the data sparseness problem using the lexical features (Liu et al., 2005; Blunsom and Cohn, 2006). The POS translation probability can be easily obtained using maximum likelihood estimation from an annotated corpus: COL(Tc,Te) Pr(Tc|Te) = (11) COF(Te) J Pr(cJ1 , aJ1 |eI1) = a = a + b,p2 p1 = 71 where Tc is a Chinese word’s POS tag and Te is an English word’s POS tag. COL(Tc, Te) is the count of Tc and Te being linked to each other in the corpus, and COF(Te) is the frequency of Te in the corpus. 3.2 Syntactic Features The dependency relation Re (resp. Rc) between two English (resp. Chinese) words ei and ei′ (resp. cj and cj′) in the dependency tree of the English sentence eI1 (resp. Chines</context>
</contexts>
<marker>Blunsom, Cohn, 2006</marker>
<rawString>Phil Blunsom and Trevor Cohn. 2006. Discriminative word alignment with conditional random fields. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 65–72, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1892" citStr="Brown et al., 1993" startWordPosition="272" endWordPosition="275">lly significant improvement in Chinese–English machine translation in comparison with the baseline word alignment. 1 Introduction Automatic word alignment can be defined as the problem of determining translational correspondences at word level given a parallel corpus of aligned sentences. Bilingual word alignment is a fundamental component of most approaches to statistical machine translation (SMT). Dominant approaches to word alignment can be classified into 69 two main schools: generative and discriminative word alignment models. Generative word alignment models, initially developed at IBM (Brown et al., 1993), and then augmented by an HMM-based model (Vogel et al., 1996), have provided powerful modeling capability for word alignment. However, it is very difficult to incorporate new features into these models. Discriminative word alignment models, based on discriminative training of a set of features (Liu et al., 2005; Moore, 2005), on the other hand, are more flexible to incorporate new features, and feature selection is essential to the performance of the system. Syntactic annotation of bilingual corpora, which can be obtained more efficiently and accurately with the advances in monolingual langu</context>
<context position="5119" citStr="Brown et al., 1993" startWordPosition="794" endWordPosition="797">ur syntaxenhanced discriminative word alignment approach. The feature functions used are described in Section 3. Experimental setting and results are presented in Section 4 and 5 respectively. In Section 6, we compare our approach with other related word alignment approaches. Section 7 concludes the paper and gives avenues for future work. 2 Word Alignment Model 2.1 Notation While in this paper we focus on Chinese–English, the method proposed is applicable to any language pair. The notation will assume Chinese–English word alignment and Chinese–English MT. Here we adopt a notation similar to (Brown et al., 1993). Given a Chinese sentence cJ1 consisting of J words {c1, ..., cJ} and an English sentence eI1 consisting of I words e1, ..., eI, we define the alignment A between cJ1 and eI1 as a subset of the Cartesian product of the word positions: A C {(j, i) : j = 1, ..., J; i = 1, ..., I} Our alignment representation is restricted so that each source word can only be aligned to one target word. The alignment A consists of associations j → i = aj from a source position j to a target position i = aj. The ‘null’ alignment aj = 0 with the ‘empty’ word e0 is used to account for source words that are not alig</context>
<context position="16194" citStr="Brown et al., 1993" startWordPosition="2743" endWordPosition="2746">le link involving the words left unaligned after anchoring into an event. In testing, the source words are consumed in sequence and the target words serve as states. The SVM dual variable was used to measure the reliability of each candidate link and the alignment link for each word is made independently, which makes the alignment search much easier. A threshold t was set as the minimal reliability score for each link. t is optimized according to alignment error rate (21) on the validation set. 4.3 Baselines 4.3.1 Word Alignment We used the GIZA++ implementation of IBM word alignment model 4 (Brown et al., 1993; Och and Ney, 2003) for word alignment, and the heuristics described in (Och and Ney, 2003) to derive the intersection and refined alignment. 4.3.2 Machine Translation We use a standard log-linear phrase-based SMT (PB-SMT) model as a baseline: GIZA++ implementation of IBM word alignment model 4,5 the refine2More specifically, we chose the first English reference from the 16 references and the Chinese sentence to construct new sentence pairs. 3Note our anchor alignment does not achieve 100% precision. Since we performed precision-oriented alignment for the anchor alignment model, the errors in</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>Dekang Lin</author>
</authors>
<title>Soft syntactic constraints for word alignment through discriminative training.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting ofthe Association for Computational Linguistics,</booktitle>
<pages>105--112</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="3170" citStr="Cherry and Lin, 2006" startWordPosition="475" endWordPosition="478">d alignment tasks. For example, Part-of-Speech (POS) tags of source and target words can be used to tackle the data sparseness problem in discriminative word alignment (Liu et al., 2005; Blunsom and Cohn, 2006). Shallow parsing has also been used to provide relevant information for alignment (Ren et al., 2007; Sun et al., 2000). Deeper syntax, e.g. phrase or dependency structures, has been shown useful in generative models (Wang and Zhou, 2004; Lopez and Resnik, 2005), heuristic-based models (Ayan et al., 2004; Ozdowska, 2004) and even for syntactically motivated models such as ITG (Wu, 1997; Cherry and Lin, 2006). In this paper, we introduce an approach to improve word alignment by incorporating syntactic dependencies. Our approach is motivated by the fact that words tend to be dependent on each other. If Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 69–77, ACL-08: HLT, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics we can first obtain a set of reliable anchor links, we could take advantage of the syntactic dependencies relating unaligned words to aligned anchor words to expand the alignment. Figure 1 gives a</context>
<context position="24540" citStr="Cherry and Lin, 2006" startWordPosition="4044" endWordPosition="4047">core. The advantage of our approach is the mechanism by which syntactic features may be incorporated. 6Note that the only difference between our MT system and the baseline PB-SMT system is the word alignment component. 75 Some previous research also tried to make use of syntax in word alignment. (Wang and Zhou, 2004) investigated the benefit of monolingual parsing for alignment. They learned a generalized word association measure (crosslingual word similarities) based on monolingual dependency structures and improved alignment performances over IBM model 2 and certain heuristic-based models. (Cherry and Lin, 2006) used dependency structures as soft constraints to improve word alignment in an ITG framework. Compared to these models, our approach directly takes advantage of dependency relations as they are transformed into feature functions incorporated into a discriminative word alignment framework. 7 Conclusion and Future Work In this paper, we proposed a model that can facilitate the incorporation of syntax into word alignment and measured the combination of a set of syntactic features. Experimental results have shown that syntax is useful in word alignment and especially effective in improving the re</context>
</contexts>
<marker>Cherry, Lin, 2006</marker>
<rawString>Colin Cherry and Dekang Lin. 2006. Soft syntactic constraints for word alignment through discriminative training. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting ofthe Association for Computational Linguistics, pages 105–112, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yonggang Deng</author>
<author>Yuqing Gao</author>
</authors>
<title>Guiding statistical word alignment models with prior knowledge.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>1--8</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="6943" citStr="Deng and Gao, 2007" startWordPosition="1138" endWordPosition="1141">nhanced model (4) by distinguishing the anchor alignment from the non-anchor alignment. p(aj|cJ1 , eI1, aj−1 1 ) (2) Z� pǫ(AA|cJ 1 , eI 1) &apos; (3) p(aj|cJ 1 , eI 1, aj−1 1 , AA) (4) 2.3 Anchor Alignment Model The anchor alignment model pǫ(AA) aims to find a set of high precision links. Various approaches can be used for this purpose. In this paper we adopted the following two approaches. 2.3.1 Heuristics-based Approach The problem of word alignment is regarded as a process of word linkage disambiguation, i.e. choosing the correct links between words from all competing hypothesis (Melamed, 2000; Deng and Gao, 2007). J p(A|cJ1 , eI1) = j=0 1 j∈ A� 70 We constrain the link probabilities in such a way that: ∀i′ ∈ {1, ..., I}, i′ =6i : p((j, i)) &gt; ǫ1 (5) p((j, i′)) ∀j′ ∈ {1, ..., J}, j′ =6 j : p((j, i)) p((j′, i)) &gt; ǫ2 (6) Condition (5) implies that for the source word cj, the link with the target word ei is more probable (with reliability threshold ǫ1) than the link with any other target word. Condition (6) guarantees that for the target word ei, cj is the only most probable (with threshold ǫ2) source word to be linked to. 2.3.2 Intersected Generative Word Alignment Models We can use the asymmetric IBM mod</context>
</contexts>
<marker>Deng, Gao, 2007</marker>
<rawString>Yonggang Deng and Yuqing Gao. 2007. Guiding statistical word alignment models with prior knowledge. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 1–8, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Dunning</author>
</authors>
<title>Accurate methods for the statistics of surprise and coincidence.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="9144" citStr="Dunning, 1993" startWordPosition="1515" endWordPosition="1516">oups: statisticsbased features, syntactic features and relative distortion features. 3.1 Statistics-based Features 3.1.1 IBM model 1 score IBM model 1 is a position-independent word alignment model which is often used to bootstrap parameters for more complex models. Model 1 models the conditional distribution and uses a uniform distribution for the dependencies between source word positions and target word positions. p(J|I) H p(cj|eaj) (8) (I + 1)J j=1 3.1.2 Log-likelihood ratio The log-likelihood ratio statistic has been found to be accurate for modeling the associations between rare events (Dunning, 1993). It has also been successfully used to measure the associations between word pairs (Melamed, 2000; Moore, 2005). Given the following contingency table: cj ¬cj ei a b ¬ei c d the log-likelihood ratio can be defined as: G2c�&apos;Z) = —2lo e B(ala+b,p1)B(c|c + d,p2) ( 9 B(a|a + b,p)B(c|c + d,p) where B(k|n, p) = (nk)pk(1 − p)n−k are binomial probabilities. The probability parameters can be obtained using maximum likelihood estimates: c (9) c + d p= a + b + c + d a + c (10) 3.1.3 POS translation probability The POS tags can provide effective information for addressing the data sparseness problem usin</context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>Ted Dunning. 1993. Accurate methods for the statistics of surprise and coincidence. Computational Linguistics, 19(1):61–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cameron Shaw Fordyce</author>
</authors>
<title>Overview of the IWSLT</title>
<date>2007</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation,</booktitle>
<pages>1--12</pages>
<location>Trento, Italy.</location>
<contexts>
<context position="13703" citStr="Fordyce, 2007" startWordPosition="2337" endWordPosition="2338">) (19) where (iL, jL) is the leftmost anchor link of l, (iR, jR) is the rightmost anchor link of l. The less the relative position changes, the more likely the candidate link is. With a set of anchor alignments, we can obtain the distribution of the relative position changes from an annotated corpus using maximum likelihood estimation. In our experiments, we used the following four probabilities: p(D = 0), p(D = 1, 2), p(D = 3,4) and p(D &gt; 4). 4 Experimental Setting 4.1 Data The experiments were carried out using the Chinese–English datasets provided within the IWSLT 2007 evaluation campaign (Fordyce, 2007), extracted from the Basic Travel Expression Corpus (BTEC) (Takezawa et al., 2002). This multilingual speech corpus contains sentences similar to those that are usually found in phrase-books for tourists going abroad. We tagged all the sentences in the training and devset3 using a maximum entropy-based POS tagger– MXPOST (Ratnaparkhi, 1996), trained on the Penn English and Chinese Treebanks. Both Chinese and English sentences are parsed using the Malt dependency parser (Nivre et al., 2007), which achieved 84% and 88% labelled attachment scores for Chinese and English respectively. 4.1.1 Word A</context>
</contexts>
<marker>Fordyce, 2007</marker>
<rawString>Cameron Shaw Fordyce. 2007. Overview of the IWSLT 2007 Evaluation Campaign. In Proceedings of the International Workshop on Spoken Language Translation, pages 1–12, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
<author>Philip Resnik</author>
<author>Amy Weinberg</author>
<author>Okan Kolak</author>
</authors>
<title>Evaluating translational correspondence using annotation projection.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>392--399</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="10674" citStr="Hwa et al., 2002" startWordPosition="1793" endWordPosition="1796"> English word’s POS tag. COL(Tc, Te) is the count of Tc and Te being linked to each other in the corpus, and COF(Te) is the frequency of Te in the corpus. 3.2 Syntactic Features The dependency relation Re (resp. Rc) between two English (resp. Chinese) words ei and ei′ (resp. cj and cj′) in the dependency tree of the English sentence eI1 (resp. Chinese sentence cJ1) can be represented as a triple &lt;ei, Re, ei′&gt;(resp. &lt;cj, Rc, ej′&gt;). Given cJ1 , eI1 and their syntactic dependency trees Tc,, TeI, if ei is aligned to cj and ei′ aligned to cj , according to the dependency correspondence assumption (Hwa et al., 2002), there exists a triple &lt;cj, Rc, cj′&gt;. While we are not aiming to justify the feasibility of the dependency correspondence assumption by proving to what extent Re = Rc under the condition described above, we do believe that cj and cj′ are likely to be dependent on each other. Given the anchor alignment AA, a candidate link (j, i) and the dependency trees, we can design four classes of feature functions. 3.2.1 Agreement features The agreement features can be further classified into dependency agreement features and dependency label agreement features. Given a candidate link (j, i) and the ancho</context>
</contexts>
<marker>Hwa, Resnik, Weinberg, Kolak, 2002</marker>
<rawString>Rebecca Hwa, Philip Resnik, Amy Weinberg, and Okan Kolak. 2002. Evaluating translational correspondence using annotation projection. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 392–399, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abraham Ittycheriah</author>
<author>Salim Roukos</author>
</authors>
<title>A maximum entropy word aligner for Arabic-English machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings ofHuman Language</booktitle>
<contexts>
<context position="6249" citStr="Ittycheriah and Roukos, 2005" startWordPosition="1016" endWordPosition="1020">ll’ alignment aj = 0 with the ‘empty’ word e0 is used to account for source words that are not aligned to any target word. We use AA to denote a subset of A. The indices of the K source words involved in AA are represented as AK1 and the corresponding target indices for Ak are represented as aAk. The unaligned source words are represented as A. 2.2 General Model Given a source sentence cJ1 and target sentence eI1, we seek to find the optimum alignment A� such that: A� = argmaxP(A|cJ1 , eI1) (1) A We use a model (2) that directly models the linkage between source and target words similarly to (Ittycheriah and Roukos, 2005). We decompose this model into an anchor alignment model (3) and a syntax-enhanced model (4) by distinguishing the anchor alignment from the non-anchor alignment. p(aj|cJ1 , eI1, aj−1 1 ) (2) Z� pǫ(AA|cJ 1 , eI 1) &apos; (3) p(aj|cJ 1 , eI 1, aj−1 1 , AA) (4) 2.3 Anchor Alignment Model The anchor alignment model pǫ(AA) aims to find a set of high precision links. Various approaches can be used for this purpose. In this paper we adopted the following two approaches. 2.3.1 Heuristics-based Approach The problem of word alignment is regarded as a process of word linkage disambiguation, i.e. choosing the</context>
</contexts>
<marker>Ittycheriah, Roukos, 2005</marker>
<rawString>Abraham Ittycheriah and Salim Roukos. 2005. A maximum entropy word aligner for Arabic-English machine translation. In Proceedings ofHuman Language</rawString>
</citation>
<citation valid="false">
<booktitle>Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>89--96</pages>
<location>Vancouver, British Columbia, Canada.</location>
<marker></marker>
<rawString>Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 89– 96, Vancouver, British Columbia, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>48--54</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="17108" citStr="Koehn et al., 2003" startWordPosition="2884" endWordPosition="2887">4,5 the refine2More specifically, we chose the first English reference from the 16 references and the Chinese sentence to construct new sentence pairs. 3Note our anchor alignment does not achieve 100% precision. Since we performed precision-oriented alignment for the anchor alignment model, the errors in anchor alignment will not bring much noise into the syntax-enhanced model. 4http://svmlight.joachims.org/ 5More specifically, we performed 5 iterations of Model 1, 5 iterations of HMM, 3 iterations of Model 3, and 3 iterations of Model 4. 73 ment and phrase-extraction heuristics described in (Koehn et al., 2003), minimum-error-rate training (Och, 2003), a trigram language model with KneserNey smoothing trained with SRILM (Stolcke, 2002) on the English side of the training data, and Moses (Koehn et al., 2007) to decode. 4.4 Evaluation We evaluate the intrinsic quality of predicted alignment A with precision, recall and alignment error rate (AER). Slightly differently from (Och and Ney, 2003), we use possible alignments in computing recall. 5 Experimental Results 5.1 Word Alignment We performed word alignment bidirectionally using our approach to obtain the union and compared our results with two stron</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 48–54, Edmonton, Canada.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ondrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,</booktitle>
<pages>177--180</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="17308" citStr="Koehn et al., 2007" startWordPosition="2915" endWordPosition="2918"> precision. Since we performed precision-oriented alignment for the anchor alignment model, the errors in anchor alignment will not bring much noise into the syntax-enhanced model. 4http://svmlight.joachims.org/ 5More specifically, we performed 5 iterations of Model 1, 5 iterations of HMM, 3 iterations of Model 3, and 3 iterations of Model 4. 73 ment and phrase-extraction heuristics described in (Koehn et al., 2003), minimum-error-rate training (Och, 2003), a trigram language model with KneserNey smoothing trained with SRILM (Stolcke, 2002) on the English side of the training data, and Moses (Koehn et al., 2007) to decode. 4.4 Evaluation We evaluate the intrinsic quality of predicted alignment A with precision, recall and alignment error rate (AER). Slightly differently from (Och and Ney, 2003), we use possible alignments in computing recall. 5 Experimental Results 5.1 Word Alignment We performed word alignment bidirectionally using our approach to obtain the union and compared our results with two strong baselines based on generative word alignment models. The results are shown in Table 1. We can see that both the syntax-enhanced model based on HMM intersection anchors (SyntaxHMM) and on IBM model 4</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 177– 180, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Ben Taskar</author>
<author>Dan Klein</author>
</authors>
<title>Alignment by agreement.</title>
<date>2006</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>104--111</pages>
<location>New York, NY.</location>
<contexts>
<context position="22351" citStr="Liang et al., 2006" startWordPosition="3708" endWordPosition="3711">intersection as anchor alignment) are shown in Table 5. As we can see, all statistics-based features are informative. Two target dependency features are informative: ‘PRD’ denoting ‘predicative’ dependency, and ‘AMOD’ denoting ‘adjective/adverb modifier’ dependency. weight Model 1 Score 0.1416 POS 0.0540 Log-likelihood Ratio 0.0856 relative distortion 0.0606 DA-1 0.0227 DLA-2 0.0927 tgt-1-PRD 0.0961 tgt-2-AMOD 0.0621 Table 5: Weights of some informative features 5.2 Machine Translation Research has shown that an increase in AER does not necessarily imply an improvement in translation quality (Liang et al., 2006) and vice-versa (Vilar et al., 2006). Hereafter, we used a Chinese–English MT task to extrinsically evaluate the quality of our word alignment. Table 6 shows the influence of our word alignment approach on MT quality.6 On development set, we achieved statistically significant improvement using both our syntax-enhanced models—SyntaxHMM (p&lt;0.002) and Syntax-Model 4 (p&lt;0.008). On the test set, we observed that the MT output based on our alignment model tends to be shorter than the reference translations and the BLEU score is considerably penalized. If we ignore the length penalty (‘BP’ in Table 6</context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 104–111, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Log-linear models for word alignment.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>459--466</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="2206" citStr="Liu et al., 2005" startWordPosition="322" endWordPosition="325"> a fundamental component of most approaches to statistical machine translation (SMT). Dominant approaches to word alignment can be classified into 69 two main schools: generative and discriminative word alignment models. Generative word alignment models, initially developed at IBM (Brown et al., 1993), and then augmented by an HMM-based model (Vogel et al., 1996), have provided powerful modeling capability for word alignment. However, it is very difficult to incorporate new features into these models. Discriminative word alignment models, based on discriminative training of a set of features (Liu et al., 2005; Moore, 2005), on the other hand, are more flexible to incorporate new features, and feature selection is essential to the performance of the system. Syntactic annotation of bilingual corpora, which can be obtained more efficiently and accurately with the advances in monolingual language processing, is a potential information source for word alignment tasks. For example, Part-of-Speech (POS) tags of source and target words can be used to tackle the data sparseness problem in discriminative word alignment (Liu et al., 2005; Blunsom and Cohn, 2006). Shallow parsing has also been used to provide</context>
<context position="9784" citStr="Liu et al., 2005" startWordPosition="1628" endWordPosition="1631">ssfully used to measure the associations between word pairs (Melamed, 2000; Moore, 2005). Given the following contingency table: cj ¬cj ei a b ¬ei c d the log-likelihood ratio can be defined as: G2c�&apos;Z) = —2lo e B(ala+b,p1)B(c|c + d,p2) ( 9 B(a|a + b,p)B(c|c + d,p) where B(k|n, p) = (nk)pk(1 − p)n−k are binomial probabilities. The probability parameters can be obtained using maximum likelihood estimates: c (9) c + d p= a + b + c + d a + c (10) 3.1.3 POS translation probability The POS tags can provide effective information for addressing the data sparseness problem using the lexical features (Liu et al., 2005; Blunsom and Cohn, 2006). The POS translation probability can be easily obtained using maximum likelihood estimation from an annotated corpus: COL(Tc,Te) Pr(Tc|Te) = (11) COF(Te) J Pr(cJ1 , aJ1 |eI1) = a = a + b,p2 p1 = 71 where Tc is a Chinese word’s POS tag and Te is an English word’s POS tag. COL(Tc, Te) is the count of Tc and Te being linked to each other in the corpus, and COF(Te) is the frequency of Te in the corpus. 3.2 Syntactic Features The dependency relation Re (resp. Rc) between two English (resp. Chinese) words ei and ei′ (resp. cj and cj′) in the dependency tree of the English s</context>
</contexts>
<marker>Liu, Liu, Lin, 2005</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2005. Log-linear models for word alignment. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 459–466, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Lopez</author>
<author>Philip Resnik</author>
</authors>
<title>Improved HMM alignment models for languages with scarce resources.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Building and Using Parallel Texts,</booktitle>
<pages>83--86</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="3021" citStr="Lopez and Resnik, 2005" startWordPosition="451" endWordPosition="454">, which can be obtained more efficiently and accurately with the advances in monolingual language processing, is a potential information source for word alignment tasks. For example, Part-of-Speech (POS) tags of source and target words can be used to tackle the data sparseness problem in discriminative word alignment (Liu et al., 2005; Blunsom and Cohn, 2006). Shallow parsing has also been used to provide relevant information for alignment (Ren et al., 2007; Sun et al., 2000). Deeper syntax, e.g. phrase or dependency structures, has been shown useful in generative models (Wang and Zhou, 2004; Lopez and Resnik, 2005), heuristic-based models (Ayan et al., 2004; Ozdowska, 2004) and even for syntactically motivated models such as ITG (Wu, 1997; Cherry and Lin, 2006). In this paper, we introduce an approach to improve word alignment by incorporating syntactic dependencies. Our approach is motivated by the fact that words tend to be dependent on each other. If Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 69–77, ACL-08: HLT, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics we can first obtain a set of reliable anchor li</context>
</contexts>
<marker>Lopez, Resnik, 2005</marker>
<rawString>Adam Lopez and Philip Resnik. 2005. Improved HMM alignment models for languages with scarce resources. In Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 83–86, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
</authors>
<title>Models of translational equivalence among words.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>2</issue>
<contexts>
<context position="6922" citStr="Melamed, 2000" startWordPosition="1136" endWordPosition="1137"> and a syntax-enhanced model (4) by distinguishing the anchor alignment from the non-anchor alignment. p(aj|cJ1 , eI1, aj−1 1 ) (2) Z� pǫ(AA|cJ 1 , eI 1) &apos; (3) p(aj|cJ 1 , eI 1, aj−1 1 , AA) (4) 2.3 Anchor Alignment Model The anchor alignment model pǫ(AA) aims to find a set of high precision links. Various approaches can be used for this purpose. In this paper we adopted the following two approaches. 2.3.1 Heuristics-based Approach The problem of word alignment is regarded as a process of word linkage disambiguation, i.e. choosing the correct links between words from all competing hypothesis (Melamed, 2000; Deng and Gao, 2007). J p(A|cJ1 , eI1) = j=0 1 j∈ A� 70 We constrain the link probabilities in such a way that: ∀i′ ∈ {1, ..., I}, i′ =6i : p((j, i)) &gt; ǫ1 (5) p((j, i′)) ∀j′ ∈ {1, ..., J}, j′ =6 j : p((j, i)) p((j′, i)) &gt; ǫ2 (6) Condition (5) implies that for the source word cj, the link with the target word ei is more probable (with reliability threshold ǫ1) than the link with any other target word. Condition (6) guarantees that for the target word ei, cj is the only most probable (with threshold ǫ2) source word to be linked to. 2.3.2 Intersected Generative Word Alignment Models We can use t</context>
<context position="9242" citStr="Melamed, 2000" startWordPosition="1531" endWordPosition="1532">-based Features 3.1.1 IBM model 1 score IBM model 1 is a position-independent word alignment model which is often used to bootstrap parameters for more complex models. Model 1 models the conditional distribution and uses a uniform distribution for the dependencies between source word positions and target word positions. p(J|I) H p(cj|eaj) (8) (I + 1)J j=1 3.1.2 Log-likelihood ratio The log-likelihood ratio statistic has been found to be accurate for modeling the associations between rare events (Dunning, 1993). It has also been successfully used to measure the associations between word pairs (Melamed, 2000; Moore, 2005). Given the following contingency table: cj ¬cj ei a b ¬ei c d the log-likelihood ratio can be defined as: G2c�&apos;Z) = —2lo e B(ala+b,p1)B(c|c + d,p2) ( 9 B(a|a + b,p)B(c|c + d,p) where B(k|n, p) = (nk)pk(1 − p)n−k are binomial probabilities. The probability parameters can be obtained using maximum likelihood estimates: c (9) c + d p= a + b + c + d a + c (10) 3.1.3 POS translation probability The POS tags can provide effective information for addressing the data sparseness problem using the lexical features (Liu et al., 2005; Blunsom and Cohn, 2006). The POS translation probability</context>
</contexts>
<marker>Melamed, 2000</marker>
<rawString>I. Dan Melamed. 2000. Models of translational equivalence among words. Computational Linguistics, 26(2):221–249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
</authors>
<title>A discriminative framework for bilingual word alignment.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>81--88</pages>
<location>Vancouver, British Columbia, Canada.</location>
<contexts>
<context position="2220" citStr="Moore, 2005" startWordPosition="326" endWordPosition="327">ponent of most approaches to statistical machine translation (SMT). Dominant approaches to word alignment can be classified into 69 two main schools: generative and discriminative word alignment models. Generative word alignment models, initially developed at IBM (Brown et al., 1993), and then augmented by an HMM-based model (Vogel et al., 1996), have provided powerful modeling capability for word alignment. However, it is very difficult to incorporate new features into these models. Discriminative word alignment models, based on discriminative training of a set of features (Liu et al., 2005; Moore, 2005), on the other hand, are more flexible to incorporate new features, and feature selection is essential to the performance of the system. Syntactic annotation of bilingual corpora, which can be obtained more efficiently and accurately with the advances in monolingual language processing, is a potential information source for word alignment tasks. For example, Part-of-Speech (POS) tags of source and target words can be used to tackle the data sparseness problem in discriminative word alignment (Liu et al., 2005; Blunsom and Cohn, 2006). Shallow parsing has also been used to provide relevant info</context>
<context position="9256" citStr="Moore, 2005" startWordPosition="1533" endWordPosition="1534"> 3.1.1 IBM model 1 score IBM model 1 is a position-independent word alignment model which is often used to bootstrap parameters for more complex models. Model 1 models the conditional distribution and uses a uniform distribution for the dependencies between source word positions and target word positions. p(J|I) H p(cj|eaj) (8) (I + 1)J j=1 3.1.2 Log-likelihood ratio The log-likelihood ratio statistic has been found to be accurate for modeling the associations between rare events (Dunning, 1993). It has also been successfully used to measure the associations between word pairs (Melamed, 2000; Moore, 2005). Given the following contingency table: cj ¬cj ei a b ¬ei c d the log-likelihood ratio can be defined as: G2c�&apos;Z) = —2lo e B(ala+b,p1)B(c|c + d,p2) ( 9 B(a|a + b,p)B(c|c + d,p) where B(k|n, p) = (nk)pk(1 − p)n−k are binomial probabilities. The probability parameters can be obtained using maximum likelihood estimates: c (9) c + d p= a + b + c + d a + c (10) 3.1.3 POS translation probability The POS tags can provide effective information for addressing the data sparseness problem using the lexical features (Liu et al., 2005; Blunsom and Cohn, 2006). The POS translation probability can be easily</context>
</contexts>
<marker>Moore, 2005</marker>
<rawString>Robert C. Moore. 2005. A discriminative framework for bilingual word alignment. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 81–88, Vancouver, British Columbia, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Ervin Marsi.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="14197" citStr="Nivre et al., 2007" startWordPosition="2411" endWordPosition="2414">iments were carried out using the Chinese–English datasets provided within the IWSLT 2007 evaluation campaign (Fordyce, 2007), extracted from the Basic Travel Expression Corpus (BTEC) (Takezawa et al., 2002). This multilingual speech corpus contains sentences similar to those that are usually found in phrase-books for tourists going abroad. We tagged all the sentences in the training and devset3 using a maximum entropy-based POS tagger– MXPOST (Ratnaparkhi, 1996), trained on the Penn English and Chinese Treebanks. Both Chinese and English sentences are parsed using the Malt dependency parser (Nivre et al., 2007), which achieved 84% and 88% labelled attachment scores for Chinese and English respectively. 4.1.1 Word Alignment We manually annotated word alignments on devset3. Since manual word alignment is an ambiguous task, we also explicitly allow for ambiguous alignments, i.e. the links are marked as sure (S) or possible (P) (Och and Ney, 2003). IWSLT devset3 consists of 502 sentence pairs after cleaning. We used the first 300 sentence pairs for training, the following 50 sentence pairs as validation set and the last 152 sentence pairs for testing. 4.1.2 Machine Translation Training was performed usi</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Ervin Marsi. 2007. MaltParser: A languageindependent system for data-driven dependency parsing. Natural Language Engineering, 13(2):95–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="14536" citStr="Och and Ney, 2003" startWordPosition="2467" endWordPosition="2470">d. We tagged all the sentences in the training and devset3 using a maximum entropy-based POS tagger– MXPOST (Ratnaparkhi, 1996), trained on the Penn English and Chinese Treebanks. Both Chinese and English sentences are parsed using the Malt dependency parser (Nivre et al., 2007), which achieved 84% and 88% labelled attachment scores for Chinese and English respectively. 4.1.1 Word Alignment We manually annotated word alignments on devset3. Since manual word alignment is an ambiguous task, we also explicitly allow for ambiguous alignments, i.e. the links are marked as sure (S) or possible (P) (Och and Ney, 2003). IWSLT devset3 consists of 502 sentence pairs after cleaning. We used the first 300 sentence pairs for training, the following 50 sentence pairs as validation set and the last 152 sentence pairs for testing. 4.1.2 Machine Translation Training was performed using the default training set (39,952 sentence pairs), to which we added the set devset1 (506 sentence pairs).2 We used devset2 (506 sentence pairs, 16 references) to tune various parameters in the MT system and IWSLT 2007 test set (489 sentence pairs, 6 references) for testing. 4.2 Alignment Training and Search In our experiments, we trea</context>
<context position="16214" citStr="Och and Ney, 2003" startWordPosition="2747" endWordPosition="2750">e words left unaligned after anchoring into an event. In testing, the source words are consumed in sequence and the target words serve as states. The SVM dual variable was used to measure the reliability of each candidate link and the alignment link for each word is made independently, which makes the alignment search much easier. A threshold t was set as the minimal reliability score for each link. t is optimized according to alignment error rate (21) on the validation set. 4.3 Baselines 4.3.1 Word Alignment We used the GIZA++ implementation of IBM word alignment model 4 (Brown et al., 1993; Och and Ney, 2003) for word alignment, and the heuristics described in (Och and Ney, 2003) to derive the intersection and refined alignment. 4.3.2 Machine Translation We use a standard log-linear phrase-based SMT (PB-SMT) model as a baseline: GIZA++ implementation of IBM word alignment model 4,5 the refine2More specifically, we chose the first English reference from the 16 references and the Chinese sentence to construct new sentence pairs. 3Note our anchor alignment does not achieve 100% precision. Since we performed precision-oriented alignment for the anchor alignment model, the errors in anchor alignment wi</context>
<context position="17494" citStr="Och and Ney, 2003" startWordPosition="2944" endWordPosition="2947">//svmlight.joachims.org/ 5More specifically, we performed 5 iterations of Model 1, 5 iterations of HMM, 3 iterations of Model 3, and 3 iterations of Model 4. 73 ment and phrase-extraction heuristics described in (Koehn et al., 2003), minimum-error-rate training (Och, 2003), a trigram language model with KneserNey smoothing trained with SRILM (Stolcke, 2002) on the English side of the training data, and Moses (Koehn et al., 2007) to decode. 4.4 Evaluation We evaluate the intrinsic quality of predicted alignment A with precision, recall and alignment error rate (AER). Slightly differently from (Och and Ney, 2003), we use possible alignments in computing recall. 5 Experimental Results 5.1 Word Alignment We performed word alignment bidirectionally using our approach to obtain the union and compared our results with two strong baselines based on generative word alignment models. The results are shown in Table 1. We can see that both the syntax-enhanced model based on HMM intersection anchors (SyntaxHMM) and on IBM model 4 anchors (Syntax-Model 4) are better than the pure generative word alignment models. Our approach is superior in precision with a disadvantage in recall. The best result achieved 10.32% </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="17149" citStr="Och, 2003" startWordPosition="2890" endWordPosition="2891">st English reference from the 16 references and the Chinese sentence to construct new sentence pairs. 3Note our anchor alignment does not achieve 100% precision. Since we performed precision-oriented alignment for the anchor alignment model, the errors in anchor alignment will not bring much noise into the syntax-enhanced model. 4http://svmlight.joachims.org/ 5More specifically, we performed 5 iterations of Model 1, 5 iterations of HMM, 3 iterations of Model 3, and 3 iterations of Model 4. 73 ment and phrase-extraction heuristics described in (Koehn et al., 2003), minimum-error-rate training (Och, 2003), a trigram language model with KneserNey smoothing trained with SRILM (Stolcke, 2002) on the English side of the training data, and Moses (Koehn et al., 2007) to decode. 4.4 Evaluation We evaluate the intrinsic quality of predicted alignment A with precision, recall and alignment error rate (AER). Slightly differently from (Och and Ney, 2003), we use possible alignments in computing recall. 5 Experimental Results 5.1 Word Alignment We performed word alignment bidirectionally using our approach to obtain the union and compared our results with two strong baselines based on generative word alig</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 160–167, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sylwia Ozdowska</author>
</authors>
<title>Identifying correspondences between words: an approach based on a bilingual syntactic analysis of French/English parallel corpora.</title>
<date>2004</date>
<booktitle>In Proceedings of the COLING’04 Workshop on Multilingual Linguistic Resources,</booktitle>
<pages>49--56</pages>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="3081" citStr="Ozdowska, 2004" startWordPosition="461" endWordPosition="462">ances in monolingual language processing, is a potential information source for word alignment tasks. For example, Part-of-Speech (POS) tags of source and target words can be used to tackle the data sparseness problem in discriminative word alignment (Liu et al., 2005; Blunsom and Cohn, 2006). Shallow parsing has also been used to provide relevant information for alignment (Ren et al., 2007; Sun et al., 2000). Deeper syntax, e.g. phrase or dependency structures, has been shown useful in generative models (Wang and Zhou, 2004; Lopez and Resnik, 2005), heuristic-based models (Ayan et al., 2004; Ozdowska, 2004) and even for syntactically motivated models such as ITG (Wu, 1997; Cherry and Lin, 2006). In this paper, we introduce an approach to improve word alignment by incorporating syntactic dependencies. Our approach is motivated by the fact that words tend to be dependent on each other. If Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 69–77, ACL-08: HLT, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics we can first obtain a set of reliable anchor links, we could take advantage of the syntactic dependencies r</context>
</contexts>
<marker>Ozdowska, 2004</marker>
<rawString>Sylwia Ozdowska. 2004. Identifying correspondences between words: an approach based on a bilingual syntactic analysis of French/English parallel corpora. In Proceedings of the COLING’04 Workshop on Multilingual Linguistic Resources, pages 49–56, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="20461" citStr="Papineni et al., 2002" startWordPosition="3415" endWordPosition="3419">ed with IBM model 4 anchoring, with the difference in recall between HMM and IBM model 4 anchoring being more significant than the difference in precision. anchor model precision recall f-score AER Heuristics 0.4505 0.3270 0.3790 0.6210 Model 1 0.5538 0.3894 0.4573 0.5427 HMM 0.5932 0.3611 0.4489 0.5511 Model 4 0.5660 0.4216 0.4832 0.5168 Table 3: Influence of anchor alignment in syntaxenhanced model AER(S, P; A) = 1 _ |A n S |+ |A n P |(21) |A |+ |S| We also extrinsically measure the word alignment quality via a Chinese–English translation task. The translation output is measured using BLEU (Papineni et al., 2002). 5.1.2 The Influence of Syntactic Dependencies on Word Alignment The influence of incorporating syntactic dependencies into the word alignment process is shown recall = |A n P |, precision = |A n P| | ||A| P (20) 74 in Table 4. Syntax plays a positive role in all different anchor alignment configurations. The influence grows proportionally to the strength of the anchor alignment model. With the Model 4 intersection used as the set of anchor alignments, adding syntactic dependency features into the syntax-enhanced alignment model yields a 5.57% relative decrease in AER. model precision recall </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A maximum entropy model for part-of-speech tagging.</title>
<date>1996</date>
<booktitle>Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>133--142</pages>
<editor>In Eric Brill and Kenneth Church, editors,</editor>
<location>Somerset, NJ.</location>
<contexts>
<context position="14045" citStr="Ratnaparkhi, 1996" startWordPosition="2388" endWordPosition="2389">In our experiments, we used the following four probabilities: p(D = 0), p(D = 1, 2), p(D = 3,4) and p(D &gt; 4). 4 Experimental Setting 4.1 Data The experiments were carried out using the Chinese–English datasets provided within the IWSLT 2007 evaluation campaign (Fordyce, 2007), extracted from the Basic Travel Expression Corpus (BTEC) (Takezawa et al., 2002). This multilingual speech corpus contains sentences similar to those that are usually found in phrase-books for tourists going abroad. We tagged all the sentences in the training and devset3 using a maximum entropy-based POS tagger– MXPOST (Ratnaparkhi, 1996), trained on the Penn English and Chinese Treebanks. Both Chinese and English sentences are parsed using the Malt dependency parser (Nivre et al., 2007), which achieved 84% and 88% labelled attachment scores for Chinese and English respectively. 4.1.1 Word Alignment We manually annotated word alignments on devset3. Since manual word alignment is an ambiguous task, we also explicitly allow for ambiguous alignments, i.e. the links are marked as sure (S) or possible (P) (Och and Ney, 2003). IWSLT devset3 consists of 502 sentence pairs after cleaning. We used the first 300 sentence pairs for train</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A maximum entropy model for part-of-speech tagging. In Eric Brill and Kenneth Church, editors, Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 133–142, Somerset, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dengjun Ren</author>
<author>Hua Wu</author>
<author>Haifeng Wang</author>
</authors>
<title>Improving statistical word alignment with various clues.</title>
<date>2007</date>
<booktitle>In Machine Translation Summit XI,</booktitle>
<pages>391--397</pages>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="2859" citStr="Ren et al., 2007" startWordPosition="425" endWordPosition="428">e more flexible to incorporate new features, and feature selection is essential to the performance of the system. Syntactic annotation of bilingual corpora, which can be obtained more efficiently and accurately with the advances in monolingual language processing, is a potential information source for word alignment tasks. For example, Part-of-Speech (POS) tags of source and target words can be used to tackle the data sparseness problem in discriminative word alignment (Liu et al., 2005; Blunsom and Cohn, 2006). Shallow parsing has also been used to provide relevant information for alignment (Ren et al., 2007; Sun et al., 2000). Deeper syntax, e.g. phrase or dependency structures, has been shown useful in generative models (Wang and Zhou, 2004; Lopez and Resnik, 2005), heuristic-based models (Ayan et al., 2004; Ozdowska, 2004) and even for syntactically motivated models such as ITG (Wu, 1997; Cherry and Lin, 2006). In this paper, we introduce an approach to improve word alignment by incorporating syntactic dependencies. Our approach is motivated by the fact that words tend to be dependent on each other. If Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (S</context>
</contexts>
<marker>Ren, Wu, Wang, 2007</marker>
<rawString>Dengjun Ren, Hua Wu, and Haifeng Wang. 2007. Improving statistical word alignment with various clues. In Machine Translation Summit XI, pages 391–397, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Stolcke</author>
</authors>
<title>SRILM – An extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing,</booktitle>
<pages>901--904</pages>
<location>Denver, CO.</location>
<contexts>
<context position="17235" citStr="Stolcke, 2002" startWordPosition="2903" endWordPosition="2904">new sentence pairs. 3Note our anchor alignment does not achieve 100% precision. Since we performed precision-oriented alignment for the anchor alignment model, the errors in anchor alignment will not bring much noise into the syntax-enhanced model. 4http://svmlight.joachims.org/ 5More specifically, we performed 5 iterations of Model 1, 5 iterations of HMM, 3 iterations of Model 3, and 3 iterations of Model 4. 73 ment and phrase-extraction heuristics described in (Koehn et al., 2003), minimum-error-rate training (Och, 2003), a trigram language model with KneserNey smoothing trained with SRILM (Stolcke, 2002) on the English side of the training data, and Moses (Koehn et al., 2007) to decode. 4.4 Evaluation We evaluate the intrinsic quality of predicted alignment A with precision, recall and alignment error rate (AER). Slightly differently from (Och and Ney, 2003), we use possible alignments in computing recall. 5 Experimental Results 5.1 Word Alignment We performed word alignment bidirectionally using our approach to obtain the union and compared our results with two strong baselines based on generative word alignment models. The results are shown in Table 1. We can see that both the syntax-enhanc</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andrea Stolcke. 2002. SRILM – An extensible language modeling toolkit. In Proceedings of the International Conference on Spoken Language Processing, pages 901–904, Denver, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Youbing Jin Le Sun</author>
<author>Lin Du</author>
<author>Yufang Sun</author>
</authors>
<title>Word alignment of English-Chinese bilingual corpus based on chunks.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2000 Joint SIGDAT conference on Empirical Methods in Natural Language Processing and very large corpora,</booktitle>
<pages>110--116</pages>
<marker>Le Sun, Du, Sun, 2000</marker>
<rawString>Le Sun, Youbing Jin, Lin Du, and Yufang Sun. 2000. Word alignment of English-Chinese bilingual corpus based on chunks. In Proceedings of the 2000 Joint SIGDAT conference on Empirical Methods in Natural Language Processing and very large corpora, pages 110–116.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Toshiyuki Takezawa</author>
<author>Eiichiro Sumita</author>
<author>Fumiaki Sugaya</author>
<author>Hirofumi Yamamoto</author>
<author>Seiichi Yamamoto</author>
</authors>
<title>Toward a broad-coverage bilingual corpus for speech translation of travel conversations in the real world.</title>
<date>2002</date>
<booktitle>In Proceedings of Third International Conference on Language Resources and Evaluation</booktitle>
<pages>147--152</pages>
<location>Las Palmas,</location>
<contexts>
<context position="13785" citStr="Takezawa et al., 2002" startWordPosition="2347" endWordPosition="2350">ghtmost anchor link of l. The less the relative position changes, the more likely the candidate link is. With a set of anchor alignments, we can obtain the distribution of the relative position changes from an annotated corpus using maximum likelihood estimation. In our experiments, we used the following four probabilities: p(D = 0), p(D = 1, 2), p(D = 3,4) and p(D &gt; 4). 4 Experimental Setting 4.1 Data The experiments were carried out using the Chinese–English datasets provided within the IWSLT 2007 evaluation campaign (Fordyce, 2007), extracted from the Basic Travel Expression Corpus (BTEC) (Takezawa et al., 2002). This multilingual speech corpus contains sentences similar to those that are usually found in phrase-books for tourists going abroad. We tagged all the sentences in the training and devset3 using a maximum entropy-based POS tagger– MXPOST (Ratnaparkhi, 1996), trained on the Penn English and Chinese Treebanks. Both Chinese and English sentences are parsed using the Malt dependency parser (Nivre et al., 2007), which achieved 84% and 88% labelled attachment scores for Chinese and English respectively. 4.1.1 Word Alignment We manually annotated word alignments on devset3. Since manual word align</context>
</contexts>
<marker>Takezawa, Sumita, Sugaya, Yamamoto, Yamamoto, 2002</marker>
<rawString>Toshiyuki Takezawa, Eiichiro Sumita, Fumiaki Sugaya, Hirofumi Yamamoto, and Seiichi Yamamoto. 2002. Toward a broad-coverage bilingual corpus for speech translation of travel conversations in the real world. In Proceedings of Third International Conference on Language Resources and Evaluation 2002, pages 147– 152, Las Palmas, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vilar</author>
<author>Maja Popovic</author>
<author>Hermann Ney</author>
</authors>
<title>AER: Do we need to “improve” our alignments?</title>
<date>2006</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation,</booktitle>
<pages>205--212</pages>
<location>Kyoto, Japan.</location>
<contexts>
<context position="22387" citStr="Vilar et al., 2006" startWordPosition="3714" endWordPosition="3717">e shown in Table 5. As we can see, all statistics-based features are informative. Two target dependency features are informative: ‘PRD’ denoting ‘predicative’ dependency, and ‘AMOD’ denoting ‘adjective/adverb modifier’ dependency. weight Model 1 Score 0.1416 POS 0.0540 Log-likelihood Ratio 0.0856 relative distortion 0.0606 DA-1 0.0227 DLA-2 0.0927 tgt-1-PRD 0.0961 tgt-2-AMOD 0.0621 Table 5: Weights of some informative features 5.2 Machine Translation Research has shown that an increase in AER does not necessarily imply an improvement in translation quality (Liang et al., 2006) and vice-versa (Vilar et al., 2006). Hereafter, we used a Chinese–English MT task to extrinsically evaluate the quality of our word alignment. Table 6 shows the influence of our word alignment approach on MT quality.6 On development set, we achieved statistically significant improvement using both our syntax-enhanced models—SyntaxHMM (p&lt;0.002) and Syntax-Model 4 (p&lt;0.008). On the test set, we observed that the MT output based on our alignment model tends to be shorter than the reference translations and the BLEU score is considerably penalized. If we ignore the length penalty (‘BP’ in Table 6) in significance testing, the impro</context>
</contexts>
<marker>Vilar, Popovic, Ney, 2006</marker>
<rawString>David Vilar, Maja Popovic, and Hermann Ney. 2006. AER: Do we need to “improve” our alignments? In Proceedings of the International Workshop on Spoken Language Translation, pages 205–212, Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>HMM-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th International Conference on Computational Linguistics,</booktitle>
<pages>836--841</pages>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="1955" citStr="Vogel et al., 1996" startWordPosition="283" endWordPosition="286">ion in comparison with the baseline word alignment. 1 Introduction Automatic word alignment can be defined as the problem of determining translational correspondences at word level given a parallel corpus of aligned sentences. Bilingual word alignment is a fundamental component of most approaches to statistical machine translation (SMT). Dominant approaches to word alignment can be classified into 69 two main schools: generative and discriminative word alignment models. Generative word alignment models, initially developed at IBM (Brown et al., 1993), and then augmented by an HMM-based model (Vogel et al., 1996), have provided powerful modeling capability for word alignment. However, it is very difficult to incorporate new features into these models. Discriminative word alignment models, based on discriminative training of a set of features (Liu et al., 2005; Moore, 2005), on the other hand, are more flexible to incorporate new features, and feature selection is essential to the performance of the system. Syntactic annotation of bilingual corpora, which can be obtained more efficiently and accurately with the advances in monolingual language processing, is a potential information source for word alig</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>Stefan Vogel, Hermann Ney, and Christoph Tillmann. 1996. HMM-based word alignment in statistical translation. In Proceedings of the 16th International Conference on Computational Linguistics, pages 836–841, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Wang</author>
<author>Ming Zhou</author>
</authors>
<title>Improving word alignment models using structured monolingual corpora.</title>
<date>2004</date>
<booktitle>In Dekang Lin and Dekai Wu, editors, Proceedings of Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>198--205</pages>
<location>Barcelona, Spain.</location>
<contexts>
<context position="2996" citStr="Wang and Zhou, 2004" startWordPosition="447" endWordPosition="450"> of bilingual corpora, which can be obtained more efficiently and accurately with the advances in monolingual language processing, is a potential information source for word alignment tasks. For example, Part-of-Speech (POS) tags of source and target words can be used to tackle the data sparseness problem in discriminative word alignment (Liu et al., 2005; Blunsom and Cohn, 2006). Shallow parsing has also been used to provide relevant information for alignment (Ren et al., 2007; Sun et al., 2000). Deeper syntax, e.g. phrase or dependency structures, has been shown useful in generative models (Wang and Zhou, 2004; Lopez and Resnik, 2005), heuristic-based models (Ayan et al., 2004; Ozdowska, 2004) and even for syntactically motivated models such as ITG (Wu, 1997; Cherry and Lin, 2006). In this paper, we introduce an approach to improve word alignment by incorporating syntactic dependencies. Our approach is motivated by the fact that words tend to be dependent on each other. If Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 69–77, ACL-08: HLT, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics we can first obtain a </context>
<context position="24237" citStr="Wang and Zhou, 2004" startWordPosition="4004" endWordPosition="4007">g distance dependencies between words are hard to incorporate into these models because of the explosive number of parameters. On the other hand, like existing discriminative models, our approach uses a set of informative features based on co-occurrence statistics, e.g. log-likelihood ratio and DICE score. The advantage of our approach is the mechanism by which syntactic features may be incorporated. 6Note that the only difference between our MT system and the baseline PB-SMT system is the word alignment component. 75 Some previous research also tried to make use of syntax in word alignment. (Wang and Zhou, 2004) investigated the benefit of monolingual parsing for alignment. They learned a generalized word association measure (crosslingual word similarities) based on monolingual dependency structures and improved alignment performances over IBM model 2 and certain heuristic-based models. (Cherry and Lin, 2006) used dependency structures as soft constraints to improve word alignment in an ITG framework. Compared to these models, our approach directly takes advantage of dependency relations as they are transformed into feature functions incorporated into a discriminative word alignment framework. 7 Conc</context>
</contexts>
<marker>Wang, Zhou, 2004</marker>
<rawString>Wei Wang and Ming Zhou. 2004. Improving word alignment models using structured monolingual corpora. In Dekang Lin and Dekai Wu, editors, Proceedings of Conference on Empirical Methods in Natural Language Processing, pages 198–205, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic Inversion Transduction Grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="3147" citStr="Wu, 1997" startWordPosition="473" endWordPosition="474">ce for word alignment tasks. For example, Part-of-Speech (POS) tags of source and target words can be used to tackle the data sparseness problem in discriminative word alignment (Liu et al., 2005; Blunsom and Cohn, 2006). Shallow parsing has also been used to provide relevant information for alignment (Ren et al., 2007; Sun et al., 2000). Deeper syntax, e.g. phrase or dependency structures, has been shown useful in generative models (Wang and Zhou, 2004; Lopez and Resnik, 2005), heuristic-based models (Ayan et al., 2004; Ozdowska, 2004) and even for syntactically motivated models such as ITG (Wu, 1997; Cherry and Lin, 2006). In this paper, we introduce an approach to improve word alignment by incorporating syntactic dependencies. Our approach is motivated by the fact that words tend to be dependent on each other. If Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 69–77, ACL-08: HLT, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics we can first obtain a set of reliable anchor links, we could take advantage of the syntactic dependencies relating unaligned words to aligned anchor words to expand the alig</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic Inversion Transduction Grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–403.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>