<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004428">
<note confidence="0.814773">
In: Proceedings of CoNLL-2000 and LLL-2000, pages 154-156, Lisbon, Portugal, 2000.
</note>
<title confidence="0.956391">
Chunking with WPDV Models
</title>
<author confidence="0.528529">
Hans van Halteren
</author>
<affiliation confidence="0.422108">
Dept. of Language and Speech, Univ. of Nijmegen
</affiliation>
<address confidence="0.3675485">
P.O. Box 9103, 6500 HD Nijmegen
The Netherlands
</address>
<email confidence="0.494527">
hvhOlet.kun.n1
</email>
<sectionHeader confidence="0.995949" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999422272727273">
In this paper I describe the application of the
WPDV algorithm to the CoNLL-2000 shared
task, the identification of base chunks in English
text (Tjong Kim Sang and Buchholz, 2000). For
this task, I use a three-stage architecture: I
first run five different base chunkers, then com-
bine them and finally try to correct some recur-
ring errors. Except for one base chunker, which
uses the memory-based machine learning sys-
tem TiMBL,1 all modules are based on WPDV
models (van Halteren, 2000a).
</bodyText>
<sectionHeader confidence="0.961479" genericHeader="categories and subject descriptors">
2 Architecture components
</sectionHeader>
<bodyText confidence="0.8734040625">
The first stage of the chunking architecture con-
sists of five different base chunkers:
1) As a baseline, I use a stacked TiMBL
model. For the first level, following Daelemans
et al. (1999), I use as features all words and
tags in a window ranging from five tokens to
the left to three tokens to the right. For the
second level (cf. Tjong Kim Sang (2000)), I use
a smaller window, four left and two right, but
add the JOB suggestions made by the first level
for one token left and right (but not the focus).
2) The basic WPDV model uses as features
the words in a window ranging from one left to
one right, the tags in a window ranging from
three left to three right, and the JOB sugges-
tions for the previous two tokens.2
</bodyText>
<listItem confidence="0.994042333333333">
3) In the reverse WPDV model, the direction
of chunking is reversed, i.e. it chunks from the
end of each utterance towards the beginning.
4) In the R&amp;M WPDV model, Ramshaw and
Marcus&apos;s type of JOB-tags are used, i.e. starts
of chunks are tagged with a B-tag only if the
</listItem>
<footnote confidence="0.96774">
Cf. http: //ilk . kub n1/.
2For unseen data, i.e. while being applied, the JOB
suggestions used are of course those suggested by the
model itself, not the true ones.
</footnote>
<bodyText confidence="0.984152875">
preceding chunk is of the same type, and with
an I-tag otherwise.
5) In the LOB WPDV model, the Penn word-
class tags (as produced by the Brill tagger)
are replaced by the output of a WPDV tagger
trained on 90% of the LOB corpus (van Hal-
teren, 2000b).
For all WPDV models, the number of fea-
tures is too high to be handled comfortably by
the current WPDV implementation. For this
reason, I use a maximum feature subset size of
four and a threshold frequency of two.3
The second stage consists of a combination of
the outputs of the five base chunkers, using an-
other WPDV model. Each chunker contributes
a feature containing the JOB suggestions for the
previous, current and next token. In addition,
there is a feature for the word and a feature
combining the (Penn-style) wordclass tags of
the previous, current and next token. For the
combination model, I use no feature restrictions,
and the default hill-climbing procedure.
In the final stage, I apply corrective mea-
sures to systematic errors which are observed
in the output of leave-one-out experiments on
the training data. For now, I focus on the most
frequent phrase type, the NP, and especially on
one weak point: determination of the start po-
sition of NPs. I use separate WPDV models for
each of the following cases:
1) Should a token now marked I-NP start a
3Cf. van Halteren (2000a). Also, the difference be-
tween training and running (correct JOB-tags vs model
suggestions) leads to a low expected generalization qual-
ity of hill-climbing. I therefore stop climbing after a
single effective step, but using an alternative climbing
procedure, in which not only the single best multiplica-
tions/division is applied per step, but which during ev-
ery step applies all multiplications/divisions that yielded
improvements while the opposite operation did not.
</bodyText>
<page confidence="0.999143">
154
</page>
<table confidence="0.999885071428572">
Phrase Number in TiMBL WPDV Combination Corrective
type test set measures
basic reverse R&amp;M LOB
ADJP 438 64.99 71.14 76.18 70.52 69.83 74.55 74.52
ADVP 866 75.03 78.96 79.83 78.16 78.50 80.09 79.86
CONJP 9 36.36 45.45 18.18 20.69 58.82 42.11 42.11
INTJ 2 66.67 66.67 66.67 66.67 0.00 66.67 66.67
LST 5 0.00 0.00 0.00 0.00 0.00 0.00 0.00
NP 12422 91.85 92.65 92.56 92.00 92.35 93.72 93.84
PP 4811 95.66 96.53 96.85 96.06 96.65 97.09 97.10
PRT 106 63.10 73.63 68.60 74.07 73.45 74.31 74.31
SBAR 535 76.50 82.27 85.54 84.18 84.77 85.41 85.41
VP 4658 92.11 92.80 92.84 92.37 91.45 93.61 93.65
Overall 23852 91.15 92.29 92.47 91.72 91.90 93.26 93.32
</table>
<tableCaption confidence="0.998227">
Table 1: Fo=1 measurements for all systems (as described in the text). In addition we list the
</tableCaption>
<bodyText confidence="0.971104333333333">
number of occurrences of each phrase type in the test set.
new NP?4 Features used: the wordclass tag se-
quence within the NP up to the current token,
the wordclass sequence within the NP from the
current token, and the current, previous and
next word within the NP.
</bodyText>
<listItem confidence="0.828395181818182">
2) Should a token now marked B-NP con-
tinue a preceding NP? Features used: type and
structure (in terms of wordclass tags) of the cur-
rent and the preceding two chunks, and the final
word of the current and the preceding chunk.
3) Should (part of) a chunk now preceding
an NP be part of the NP? Features used: type
and structure (in wordclass tags) of the current,
preceding and next chunk (the latter being the
NP), and the final word of the current and next
chunk.
</listItem>
<bodyText confidence="0.9997268125">
For all three models, the number of different
features is large. Normally, this would force the
use of feature restrictions. The training sets are
very small, however, so that the need for feature
restrictions disappears and the full model can
be used. On the other hand, the limited size
of the training sets has as a disadvantage that
hill-climbing becomes practically useless. For
this reason, I do not use hill-climbing but simply
take the initial first order weight factors.
Each token is subjected to the appropriate
model, or, if not in any of the listed situations,
left untouched. To remove (some) resulting in-
consistencies, I let an AWK script then change
the I0B-tag of all comma&apos;s and coordinators
that now end an NP into 0.
</bodyText>
<footnote confidence="0.770032666666667">
4This cannot already be the first token of an NP,
as I-tags following a different type of chunk are always
immediately transformed to B-tags.
</footnote>
<sectionHeader confidence="0.998375" genericHeader="acknowledgments">
3 Results
</sectionHeader>
<bodyText confidence="0.961005794117647">
The F0,1 scores for all systems are listed in Ta-
ble 1. They vary greatly per phrase type, partly
because of the relative difficulty of the tasks but
also because of the variation in the number of
relevant training and test cases: the most fre-
quent phrase types (NP, PP and VP) also show
the best results. Note that three of the phrase
types (CONJP, INTJ and LST) are too infre-
quent to yield statistically sensible information.
The TiMBL results are worse than the ones
reported by Buchholz et al. (1999),5 but the lat-
ter were based on training on WSJ sections 00-
19 and testing on 20-24. When comparing with
the NP scores of Daelemans et al. (1999), we see
a comparable accuracy (actually slightly higher
because of the second level classification).
The WPDV accuracies are almost all much
higher. For NP, the basic and reverse model
produce accuracies which can compete with
the highest published non-combination accura-
cies so far. Interestingly, the reverse model
yields the best overall score. This can be ex-
plained by the observation that many choices,
e.g. PP/PRT and especially ADJP/part of NP,
are based mostly on the right context, about
which more information becomes available when
the text is handled from right to left. The
R&amp;M-type JOB-tags are generally less useful
than the standard ones, but still show excep-
tional quality for some phrase types, e.g. PRT.
The results for the LOB model are disappoint-
ing, given the overall quality of the tagger used
5FADjp=66.7, FADvP=77.9 FNp=92.3, Fpp=96.8,
Fvp=91.8
</bodyText>
<page confidence="0.997943">
155
</page>
<bodyText confidence="0.999976862745098">
(97.82% on the held-out 10% of LOB). I hypoth-
esize this to be due to: a) differences in text
type between LOB and WSJ, b) par tial incom-
patibility between the LOB tags and the WSJ
chunks and c) insufficiency of chunkier training
set size for the more varied LOB tags.
Combination, as in other tasks (e.g. van Hal-
teren et al. (To appear)), leads to an impressive
accuracy increase, especially for the three most
frequent phrase types, where there is a suffi-
cient number of cases to train the combination
model on. There are only two phrase types,
ADVP and SBAR, where a base chunker (re-
verse WPDV) manages to outperform the com-
bination. In both cases the four normal direc-
tion base chunkers outvote the better-informed
reverse chunker, probably because the combina-
tion system has insufficient training material to
recognize the higher information value of the re-
verse model (for these two phrase types). Even
though the results are already quite good, I ex-
pect that even more effective combination is
possible, with an increase in training set size
and the inclusion of more base chunkers, espe-
cially ones which differ substantially from the
current, still rather homogeneous, set.
The corrective measures yield further im-
provement, although less impressive. Unsur-
prisingly, the increase is found mostly for the
NP. The next most affected phrase type is the
ADJP, which can often be joined with or re-
moved from the NP. There is an increase in re-
call for ADJP (71.23% to 71.46%), but a de-
crease in precision (78.20% to 77.86%), leav-
ing the Fo=1 value practically unchanged. For
ADVP, there is a loss of accuracy, most likely
caused by the one-shot correction procedure.
This loss will probably disappear when a proce-
dure is used which is iterative and also targets
other phrase types than the NP. For VP, on the
other hand, there is an accuracy increase, prob-
ably due to a corrected inclusion/exclusion of
participles into/from NPs. The overall scores
show an increase, especially due to the per-type
increases for the very frequent NP and VP.
All scores for the chunking system as a whole,
including precision and recall percentages, are
listed in Table 2. For all phrase types, the
system yields substantially better results than
any previously published. I attribute the im-
provements primarily to the combination archi-
</bodyText>
<table confidence="0.999845833333333">
test data precision recall Fo=1
ADJP 77.86% 71.46% 74.52
ADVP 80.52% 79.21% 79.86
CONJP 40.00% 44.44% 42.11
INTJ 100.00% 50.00% 66.67
LST 0.00% 0.00% 0.00
NP 93.55% 94.13% 93.84
PP 96.43% 97.78% 97.10
PRT 72.32% 76.42% 74.31
SBAR 87.77% 83.18% 85.41
VP 93.36% 93.95% 93.65
all 93.13% 93.51% 93.32
</table>
<tableCaption confidence="0.940127">
Table 2: Final results per chunk type, i.e. af-
</tableCaption>
<bodyText confidence="0.9914889">
ter applying corrective measures to base chun-
ker combination.
tecture, with a smaller but yet valuable contri-
bution by the corrective measures. The choice
for WPDV proves a good one, as the WPDV
algorithm is able to cope well with all the mod-
eling tasks in the system. Whether it is the best
choice can only be determined by future experi-
ments, using other machine learning techniques
in the same architecture.
</bodyText>
<sectionHeader confidence="0.998093" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999910692307692">
Sabine Buchholz, Jorn Veenstra, and Walter Daele-
mans. 1999. Cascaded grammatical relation as-
signment. In Proceedings of EMNLP/VLC-99.
Association for Computational Linguistics.
W. Daelemans, S. Buchholz and J. Veenstra. 1999.
Memory-based shallow parsing. In Proceedings of
CoNLL, Bergen, Norway.
H. van Halteren. 2000a. A default first order family
weight determination procedure for WPDV mod-
els. In Proceedings of the CoNLL-2000. Associa-
tion for Computational Linguistics.
H. van Halteren. 2000b. The detection of inconsis-
tency in manually tagged text. In Proceedings of
LINC2000.
H. van Halteren, J. Zavrel, and W. Daelemans. To
appear. Improving accuracy in wordclass tagging
through combination of machine learning systems.
Computational Linguistics.
E. F. Tjong Kim Sang. 2000. Noun phrase recogni-
tion by system combination. In Proceedings of the
ANLP-NAACL 2000. Seattle, Washington, USA.
Morgan Kaufman Publishers.
E. F. Tjong Kim Sang and S. Buchholz. 2000. Intro-
duction to the CoNLL-2000 shared task: Chunk-
ing. In Proceedings of the CoNLL-2000. Associa-
tion for Computational Linguistics.
</reference>
<page confidence="0.998792">
156
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.239772">
<note confidence="0.985338">of CoNLL-2000 and LLL-2000, 154-156, Lisbon, Portugal, 2000.</note>
<title confidence="0.989226">Chunking with WPDV Models</title>
<author confidence="0.998891">Hans van</author>
<affiliation confidence="0.95592">Dept. of Language and Speech, Univ. of</affiliation>
<address confidence="0.646757">P.O. Box 9103, 6500 HD</address>
<note confidence="0.389663">The hvhOlet.kun.n1</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Jorn Veenstra</author>
<author>Walter Daelemans</author>
</authors>
<title>Cascaded grammatical relation assignment.</title>
<date>1999</date>
<booktitle>In Proceedings of EMNLP/VLC-99. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="6601" citStr="Buchholz et al. (1999)" startWordPosition="1151" endWordPosition="1154"> token of an NP, as I-tags following a different type of chunk are always immediately transformed to B-tags. 3 Results The F0,1 scores for all systems are listed in Table 1. They vary greatly per phrase type, partly because of the relative difficulty of the tasks but also because of the variation in the number of relevant training and test cases: the most frequent phrase types (NP, PP and VP) also show the best results. Note that three of the phrase types (CONJP, INTJ and LST) are too infrequent to yield statistically sensible information. The TiMBL results are worse than the ones reported by Buchholz et al. (1999),5 but the latter were based on training on WSJ sections 00- 19 and testing on 20-24. When comparing with the NP scores of Daelemans et al. (1999), we see a comparable accuracy (actually slightly higher because of the second level classification). The WPDV accuracies are almost all much higher. For NP, the basic and reverse model produce accuracies which can compete with the highest published non-combination accuracies so far. Interestingly, the reverse model yields the best overall score. This can be explained by the observation that many choices, e.g. PP/PRT and especially ADJP/part of NP, a</context>
</contexts>
<marker>Buchholz, Veenstra, Daelemans, 1999</marker>
<rawString>Sabine Buchholz, Jorn Veenstra, and Walter Daelemans. 1999. Cascaded grammatical relation assignment. In Proceedings of EMNLP/VLC-99. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>S Buchholz</author>
<author>J Veenstra</author>
</authors>
<title>Memory-based shallow parsing.</title>
<date>1999</date>
<booktitle>In Proceedings of CoNLL,</booktitle>
<location>Bergen,</location>
<contexts>
<context position="956" citStr="Daelemans et al. (1999)" startWordPosition="151" endWordPosition="154"> shared task, the identification of base chunks in English text (Tjong Kim Sang and Buchholz, 2000). For this task, I use a three-stage architecture: I first run five different base chunkers, then combine them and finally try to correct some recurring errors. Except for one base chunker, which uses the memory-based machine learning system TiMBL,1 all modules are based on WPDV models (van Halteren, 2000a). 2 Architecture components The first stage of the chunking architecture consists of five different base chunkers: 1) As a baseline, I use a stacked TiMBL model. For the first level, following Daelemans et al. (1999), I use as features all words and tags in a window ranging from five tokens to the left to three tokens to the right. For the second level (cf. Tjong Kim Sang (2000)), I use a smaller window, four left and two right, but add the JOB suggestions made by the first level for one token left and right (but not the focus). 2) The basic WPDV model uses as features the words in a window ranging from one left to one right, the tags in a window ranging from three left to three right, and the JOB suggestions for the previous two tokens.2 3) In the reverse WPDV model, the direction of chunking is reversed</context>
<context position="6747" citStr="Daelemans et al. (1999)" startWordPosition="1179" endWordPosition="1182">stems are listed in Table 1. They vary greatly per phrase type, partly because of the relative difficulty of the tasks but also because of the variation in the number of relevant training and test cases: the most frequent phrase types (NP, PP and VP) also show the best results. Note that three of the phrase types (CONJP, INTJ and LST) are too infrequent to yield statistically sensible information. The TiMBL results are worse than the ones reported by Buchholz et al. (1999),5 but the latter were based on training on WSJ sections 00- 19 and testing on 20-24. When comparing with the NP scores of Daelemans et al. (1999), we see a comparable accuracy (actually slightly higher because of the second level classification). The WPDV accuracies are almost all much higher. For NP, the basic and reverse model produce accuracies which can compete with the highest published non-combination accuracies so far. Interestingly, the reverse model yields the best overall score. This can be explained by the observation that many choices, e.g. PP/PRT and especially ADJP/part of NP, are based mostly on the right context, about which more information becomes available when the text is handled from right to left. The R&amp;M-type JOB</context>
</contexts>
<marker>Daelemans, Buchholz, Veenstra, 1999</marker>
<rawString>W. Daelemans, S. Buchholz and J. Veenstra. 1999. Memory-based shallow parsing. In Proceedings of CoNLL, Bergen, Norway.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H van Halteren</author>
</authors>
<title>A default first order family weight determination procedure for WPDV models.</title>
<date>2000</date>
<booktitle>In Proceedings of the CoNLL-2000. Association for Computational Linguistics.</booktitle>
<marker>van Halteren, 2000</marker>
<rawString>H. van Halteren. 2000a. A default first order family weight determination procedure for WPDV models. In Proceedings of the CoNLL-2000. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H van Halteren</author>
</authors>
<title>The detection of inconsistency in manually tagged text.</title>
<date>2000</date>
<booktitle>In Proceedings of LINC2000.</booktitle>
<marker>van Halteren, 2000</marker>
<rawString>H. van Halteren. 2000b. The detection of inconsistency in manually tagged text. In Proceedings of LINC2000.</rawString>
</citation>
<citation valid="false">
<authors>
<author>H van Halteren</author>
<author>J Zavrel</author>
<author>W Daelemans</author>
</authors>
<title>To appear. Improving accuracy in wordclass tagging through combination of machine learning systems.</title>
<journal>Computational Linguistics.</journal>
<marker>van Halteren, Zavrel, Daelemans, </marker>
<rawString>H. van Halteren, J. Zavrel, and W. Daelemans. To appear. Improving accuracy in wordclass tagging through combination of machine learning systems. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E F Tjong Kim Sang</author>
</authors>
<title>Noun phrase recognition by system combination.</title>
<date>2000</date>
<booktitle>In Proceedings of the ANLP-NAACL 2000.</booktitle>
<publisher>Morgan Kaufman Publishers.</publisher>
<location>Seattle, Washington, USA.</location>
<contexts>
<context position="1121" citStr="Sang (2000)" startWordPosition="186" endWordPosition="187">base chunkers, then combine them and finally try to correct some recurring errors. Except for one base chunker, which uses the memory-based machine learning system TiMBL,1 all modules are based on WPDV models (van Halteren, 2000a). 2 Architecture components The first stage of the chunking architecture consists of five different base chunkers: 1) As a baseline, I use a stacked TiMBL model. For the first level, following Daelemans et al. (1999), I use as features all words and tags in a window ranging from five tokens to the left to three tokens to the right. For the second level (cf. Tjong Kim Sang (2000)), I use a smaller window, four left and two right, but add the JOB suggestions made by the first level for one token left and right (but not the focus). 2) The basic WPDV model uses as features the words in a window ranging from one left to one right, the tags in a window ranging from three left to three right, and the JOB suggestions for the previous two tokens.2 3) In the reverse WPDV model, the direction of chunking is reversed, i.e. it chunks from the end of each utterance towards the beginning. 4) In the R&amp;M WPDV model, Ramshaw and Marcus&apos;s type of JOB-tags are used, i.e. starts of chunk</context>
</contexts>
<marker>Sang, 2000</marker>
<rawString>E. F. Tjong Kim Sang. 2000. Noun phrase recognition by system combination. In Proceedings of the ANLP-NAACL 2000. Seattle, Washington, USA. Morgan Kaufman Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E F Tjong Kim Sang</author>
<author>S Buchholz</author>
</authors>
<title>Introduction to the CoNLL-2000 shared task: Chunking.</title>
<date>2000</date>
<booktitle>In Proceedings of the CoNLL-2000. Association for Computational Linguistics.</booktitle>
<marker>Sang, Buchholz, 2000</marker>
<rawString>E. F. Tjong Kim Sang and S. Buchholz. 2000. Introduction to the CoNLL-2000 shared task: Chunking. In Proceedings of the CoNLL-2000. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>