<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000088">
<title confidence="0.968267">
Extensible Dependency Grammar: A New Methodology
</title>
<author confidence="0.748383">
Ralph Debusmann Denys Duchier Geert-Jan M. Kruijff
</author>
<affiliation confidence="0.5595725">
Programming Systems Lab ´Equipe Calligramme Computational Linguistics
Saarland University LORIA – UMR 7503 Saarland University
</affiliation>
<address confidence="0.554312333333333">
Postfach 15 11 50 Campus Scientifique, B. P. 239 Postfach 15 11 50
66041 Saarbr ucken 54506 Vandcruvre l`es Nancy CEDEX 66041 Saarbr ucken
Germany France Germany
</address>
<email confidence="0.991889">
rade@ps.uni-sb.de duchier@loria.fr gj@coli.uni-sb.de
</email>
<sectionHeader confidence="0.994602" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999955666666667">
This paper introduces the new grammar formalism
of Extensible Dependency Grammar (XDG), and
emphasizes the benefits of its methodology of ex-
plaining complex phenomena by interaction of sim-
ple principles on multiple dimensions of linguis-
tic description. This has the potential to increase
modularity with respect to linguistic description and
grammar engineering, and to facilitate concurrent
processing and the treatment of ambiguity.
</bodyText>
<sectionHeader confidence="0.998429" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99873446875">
We introduce the new grammar formalism of Exten-
sible Dependency Grammar (XDG). In XDG, com-
plex phenomena arise out of the interaction of sim-
ple principles on multiple dimensions of linguis-
tic description. In this paper, we point out how
this novel methodology positions XDG in between
multi-stratal approaches like LFG (Bresnan and Ka-
plan, 1982) and MTT (Mel’ˇcuk, 1988), see also
(Kahane, 2002), and mono-stratal ones like HPSG
(Pollard and Sag, 1994), attempting to combine
their benefits and avoid their problems.
It is the division of linguistic analyses into dif-
ferent dimensions which makes XDG multi-stratal.
On the other, XDG is mono-stratal in that its princi-
ples interact to constrain all dimensions simultane-
ously. XDG combines the benefits of these two po-
sitions, and attempts to circumvent their problems.
From multi-stratal approaches, XDG adopts a high
degree of modularity, both with respect to linguis-
tic description as well as for grammar engineering.
This also facilitates the statement of cross-linguistic
generalizations. XDG avoids the problem of placing
too high a burden on the interfaces, and allows in-
teractions between all and not only adjacent dimen-
sions. From mono-stratal approaches, XDG adopts
a high degree of integration, facilitating concurrent
processing and the treatment of ambiguity. At the
same time, XDG does not lose its modularity.
XDG is a descendant of Topological Depen-
dency Grammar (TDG) (Duchier and Debusmann,
2001), pushing the underlying methodology further
by generalizing it in two aspects:
</bodyText>
<listItem confidence="0.99498125">
• number of dimensions: two in TDG (ID and
LP), arbitrary many in XDG
• set of principles: fixed in TDG, extensible
principle library in XDG
</listItem>
<bodyText confidence="0.9998345">
The structure of this paper is as follows: In §2, we
introduce XDG and the XDG solver used for pars-
ing and generation. In §3, we introduce a number
of XDG principles informally, before making use of
them in an idealized example grammar in §4. In §5
we argue why XDG has the potential to be an im-
provement over multi-stratal and mono-stratal ap-
proaches, before we conclude in §6.
</bodyText>
<sectionHeader confidence="0.989259" genericHeader="method">
2 Extensible Dependency Grammar
</sectionHeader>
<bodyText confidence="0.999778666666667">
In this section, we introduce XDG formally and
mention briefly the constraint-based XDG solver for
parsing and generation.
</bodyText>
<subsectionHeader confidence="0.754415">
2.1 Formalization
</subsectionHeader>
<bodyText confidence="0.804349941176471">
Formally, an XDG grammar is built up of dimen-
sions, a lexicon and principles, and characterizes a
set of well-formed analyses.
A dimension is a tuple D = (Lab,Fea,Val,Pri) of
a set Lab of edge labels, a set Fea of features, a set
Val of feature values, and a set of one-dimensional
principles Pri. A lexicon for the dimension D is a
set Lex C Fea —* Val of total feature assignments
called lexical entries. An analysis on dimension
D is a triple (V,E,F) of a set V of nodes, a set
E C V xV x Lab of directed labeled edges, and an
assignment F : V —* (Fea —* Val) of lexical entries
to nodes. V and E form a graph. We write AnaD for
the set of all possible analyses on dimension D. The
principles characterize subsets of AnaD. We assume
that the elements of Pri are finite representations of
such subsets.
</bodyText>
<subsubsectionHeader confidence="0.548376">
AnXDGgrammar ((Labi,Feai,Vali,Prii)n i=1,Pri,
</subsubsectionHeader>
<bodyText confidence="0.984175523809524">
Lex) consists of n dimensions, multi-dimensional
principles Pri, and a lexicon Lex. An XDG analysis
(V,Ei,Fi)n i=1 is an element of Ana = Anal x ··· x
Anan where all dimensions share the same set of
nodes V. We call a dimension of a grammar gram-
mar dimension.
Multi-dimensional principles specify subsets of
Ana, i.e. of tuples of analyses for the individual di-
mensions. The lexicon Lex C_ Lexl x···xLexn con-
strains all dimensions at once, thereby synchroniz-
ing them. An XDG analysis is licensed by Lex iff
(F1(v),... ,Fn(v)) E Lex for every node v E V.
In order to compute analyses for a given input,
we employ a set of input constraints (Inp), which
again specify a subset of Ana. XDG solving then
amounts to finding elements of Ana that are licensed
by Lex, and consistent with Inp and Pri. The input
constraints determine whether XDG solving is to be
used for parsing or generation. For parsing, they
specify a sequence of words, and for generation, a
multiset of semantic literals.
</bodyText>
<subsectionHeader confidence="0.988642">
2.2 Solver
</subsectionHeader>
<bodyText confidence="0.999949304347826">
XDG solving has a natural reading as a constraint
satisfaction problem (CSP) on finite sets of integers,
where well-formed analyses correspond to the solu-
tions of the CSP (Duchier, 2003). We have imple-
mented an XDG solver using the Mozart-Oz pro-
gramming system.
XDG solving operates on all dimensions concur-
rently. This means that the solver can infer informa-
tion about one dimension from information on an-
other, if there is either a multi-dimensional principle
linking the two dimensions, or by the synchroniza-
tion induced by the lexical entries. For instance, not
only can syntactic information trigger inferences in
syntax, but also vice versa.
Because XDG allows us to write grammars with
completely free word order, XDG solving is an
NP-complete problem (Koller and Striegnitz, 2002).
This means that the worst-case complexity of the
solver is exponential. The average-case complexity
of many smaller-scale grammars that we have ex-
perimented with seems polynomial, but it remains
to be seen whether we can scale this up to large-
scale grammars.
</bodyText>
<sectionHeader confidence="0.994125" genericHeader="method">
3 Principles
</sectionHeader>
<bodyText confidence="0.999815777777778">
The well-formedness conditions of XDG analy-
ses are stipulated by principles. Principles are
parametrizable, e.g. by the dimensions on which
they are applied, or by lexical features. They can
be lexicalized or non-lexicalized, and can be one-
dimensional or multi-dimensional. Principles are
taken from an extensible principle library, and we
introduce some of the most important principles in
the following.
</bodyText>
<subsectionHeader confidence="0.999564">
3.1 Tree principle
</subsectionHeader>
<bodyText confidence="0.999882">
tree(i) The analysis on dimension i must be a tree.
The tree principle is non-lexicalized and
parametrized by the dimension i.
</bodyText>
<subsectionHeader confidence="0.99987">
3.2 Dag principle
</subsectionHeader>
<bodyText confidence="0.999835">
dag(i) The analysis on dimension i must be a di-
rected acyclic graph.
The dag principle is non-lexicalized and
parametrized by the dimension i.
</bodyText>
<subsectionHeader confidence="0.999696">
3.3 Valency principle
</subsectionHeader>
<bodyText confidence="0.999086235294118">
valency(i,ini,outi) All nodes on dimension i must
satisfy their in and out specifications.
The valency principle is lexicalized and serves
to lexically describe dependency graphs. It is
parametrized by the dimension i, the in specification
ini and the out specification outi. For each node, ini
stipulates the licensed incoming edges, and outi the
licensed outgoing edges.
In the example grammar lexicon part in Figure 1
below, the in specification is inID and outID is the
out specification on the ID dimension. For the com-
mon noun Roman, the in specification licenses zero
or one incoming edges labeled subj, and zero or one
incoming edges labeled obj ({subj?,obj?}), i.e. it
can be either a subject or an object. The out specifi-
cation requires precisely one outgoing edge labeled
det ({det!}), i.e. it requires a determiner.
</bodyText>
<subsectionHeader confidence="0.996406">
3.4 Government principle
</subsectionHeader>
<bodyText confidence="0.999162466666667">
government(i,casesi,governi) All edges in dimen-
sion i must satisfy the government specification of
the mother.
The government principle is lexicalized. Its pur-
pose is to constrain the case feature of a depen-
dent.&apos; It is parametrized by the dimension i, the
cases specification casesi and the government spec-
ification govern. cases assigns to each word a set of
possible cases, and govern a mapping from labels to
sets of cases.
In Figure 1, the cases specification for the deter-
miner den is {acc} (i.e. it can only be accusative).
By its government specification, the finite verb ver-
sucht requires its subject to exhibit nominative case
(subj H {nom}).
</bodyText>
<subsectionHeader confidence="0.995499">
3.5 Agreement principle
</subsectionHeader>
<bodyText confidence="0.985905588235294">
agreement(i,casesi,agreei) All edges in dimension
i must satisfy the agreement specification of the
mother.
1We restrict ourselves to the case feature only for simplicity.
In a fully-fledged grammar, the government principle would be
used to constrain also other morphological aspects like number,
person and gender.
The agreement principle is lexicalized. Its pur-
pose is to enforce the case agreement of a daugh-
ter.2 It is parametrized by dimension i, the lexical
cases specification casesi, assigning to each word a
set of possible cases, and the agreement specifica-
tion agreei, assigning to each word a set of labels.
As an example, in Figure 1, the agreement spec-
ification for the common noun Roman is {det}, i.e.
the case of the common noun must agree with its
determiner.
</bodyText>
<subsectionHeader confidence="0.995416">
3.6 Order principle.
</subsectionHeader>
<bodyText confidence="0.999286842105263">
order(i,oni,i) On dimension i, 1) each node must
satisfy its node labels specification, 2) the order of
the daughters of each node must be compatible with
i, and 3) the node itself must be ordered correctly
with respect to its daughters (using its node label).
The order principle is lexicalized. It is
parametrized by the dimension i, the node labels
specification oni mapping each node to set of labels
from Labi, and the total order i on Labi.
Assuming the node labels specification given in
Figure 2, and the total order in (5), the tree in (11)
satisfies the order principle.3 For instance for the
node versucht: 1) The node label of versucht is lbf,
satisfying the node labels specification. 2) The order
of the daughters Roman (under the edge labeled vf),
Peter (mf) and lesen (rbf) is compatible with the
total order prescribing vf  mf  rbf. 3) The node
versucht itself is ordered correctly with respect to its
daughters (the total order prescribes vf  lbf  mf).
</bodyText>
<subsectionHeader confidence="0.998006">
3.7 Projectivity principle
</subsectionHeader>
<bodyText confidence="0.9998056">
projectivity(i) The analysis on dimension i must be
projective.
The projectivity principle is non-lexicalized. Its
purpose is to exclude non-projective analyses.4 It is
parametrized by dimension i.
</bodyText>
<subsectionHeader confidence="0.999591">
3.8 Climbing principle
</subsectionHeader>
<bodyText confidence="0.99936875">
climbing(i, j) The graph on dimension i must be
flatter than the graph on dimension j.
The climbing principle is non-lexicalized and
two-dimensional. It is parametrized by the two di-
mensions i and j.
For instance, the tree in (11) is flatter than the
corresponding tree in (10). This concept was intro-
duced as lifting in (Kahane et al., 1998).
</bodyText>
<footnote confidence="0.998299">
2Again, we restrict ourselves to case for simplicity.
3The node labels are defined in (2) below.
4The projectivity principle of course only makes sense in
combination with the order principle.
</footnote>
<subsectionHeader confidence="0.998752">
3.9 Linking principle
</subsectionHeader>
<bodyText confidence="0.999055352941176">
linking(i, j,linki, j) All edges on dimension i must
satisfy the linking specification of the mother.
The linking principle is lexicalized and two-
dimensional. It is parametrized by the two dimen-
sions i and j, and by the linking specification linki,j,
mapping labels from Labi to sets of labels from
Labj. Its purpose is to specify how dependents on
dimension i are realized by (or linked to) dependents
on dimension j.
In the lexicon part in Figure 3, the linking spec-
ification for the transitive verb lesen requires that
its agent on the PA dimension must be realized by a
subject (ag  {subj}), and the patient by an object
(pat  {obj}).
The linking principle is oriented. Symmetric
linking could be gained simply by using the linking
principle twice (in both directions).
</bodyText>
<sectionHeader confidence="0.993764" genericHeader="method">
4 Example grammar
</sectionHeader>
<bodyText confidence="0.996422">
In this section, we elucidate XDG with an example
grammar fragment for German. With it, we demon-
strate three aspects of the methodology of XDG:
</bodyText>
<listItem confidence="0.982814333333333">
• How complex phenomena such as topicaliza-
tion and control arise by the interaction of sim-
ple principles on different dimensions of lin-
guistic description.
• How the high degree of integration helps to re-
duce ambiguity.
• How the high degree of modularity facilitates
the statement of cross-linguistic generaliza-
tions.
</listItem>
<bodyText confidence="0.999331375">
Note that this grammar fragment is an idealized ex-
ample, and does not make any claims about XDG as
a grammar theory. Its purpose is solely to substan-
tiate our points about XDG as a framework. More-
over, the grammar is fully lexicalized for simplicity.
However, XDG of course allows the grammar writer
to formulate lexical abstractions using inheritance
(like in HPSG) or crossings (Candito, 1996).
</bodyText>
<subsectionHeader confidence="0.947472">
4.1 Dimensions
</subsectionHeader>
<bodyText confidence="0.998796777777778">
The grammar fragment make use of two dimen-
sions: Immediate Dominance (ID) and Linear
Precedence (LP). The models on the ID dimension
are unordered, syntactic dependency trees whose
edge labels correspond to syntactic functions like
subject and object. On the LP dimension, the mod-
els are ordered, projective topological dependency
trees whose edge labels are topological fields like
Vorfeld and Mittelfeld.
</bodyText>
<subsectionHeader confidence="0.969039">
4.2 Labels
</subsectionHeader>
<bodyText confidence="0.982951">
The set LabID of labels on the ID dimension is:
</bodyText>
<equation confidence="0.960962">
LabID = {det,subj,obj,vinf,part} (1)
</equation>
<bodyText confidence="0.999939666666667">
These correspond resp. to determiner, subject, ob-
ject, infinitive verbal complement, and particle.
The set LabLP of labels on the LP dimension is:
</bodyText>
<equation confidence="0.999045">
LabLP = {detf,nounf,vf,lbf,mf,partf,rbf} (2)
</equation>
<bodyText confidence="0.999330666666667">
Corresponding resp. to determiner field, noun field,
Vorfeld, left bracket field, Mittelfeld, particle field,
and right bracket field.
</bodyText>
<subsectionHeader confidence="0.99484">
4.3 Principles
</subsectionHeader>
<bodyText confidence="0.986639">
On the ID dimension, we make use of the following
one-dimensional principles:
</bodyText>
<equation confidence="0.996319">
tree(ID)
valency(ID,inID,outID)
government(ID,casesID,governID)
agreement(ID,casesID,agreeID)
</equation>
<bodyText confidence="0.991891">
where the total order SLP is defined as:
</bodyText>
<equation confidence="0.842622">
detf � nounf � vf � lbf � mf � partf � rbf (5)
</equation>
<bodyText confidence="0.9804565">
We make use of the following multi-dimensional
principles:
</bodyText>
<equation confidence="0.955671">
climbing(LP, ID) (6)
linking(LP, ID)
</equation>
<subsectionHeader confidence="0.953572">
4.4 Lexicon
</subsectionHeader>
<bodyText confidence="0.99999175">
We split the lexicon into two parts. The ID and LP
parts are displayed resp. in Figure 15 and Figure 2.
The LP part includes also the linking specification
for the LP,ID-application of the linking principle.6
</bodyText>
<subsectionHeader confidence="0.978202">
4.5 Government and agreement
</subsectionHeader>
<bodyText confidence="0.998525">
Our first example is the following sentence:
</bodyText>
<subsubsectionHeader confidence="0.5793895">
Peter versucht einen Roman zu lesen.
Peter tries aacc novel to read. (7)
</subsubsectionHeader>
<bodyText confidence="0.903128071428571">
Peter tries to read a novel.
We display the ID analysis of the sentence below:
5Here, stands for “don’t care”, this means e.g. for the verb
versucht that it has unspecified case.
6We do not make use of the linking specification for the
German grammar fragment (the mappings are all empty), but
we will do so as we switch to Dutch in §4.8 below.
Here, Peter is the subject of versucht. lesen is the in-
finitival verbal complement of versucht, zu the parti-
cle of lesen, and Roman the object of lesen. Finally,
einen is the determiner of Roman.
Under our example grammar, the sentence is un-
ambiguous, i.e. the given ID tree is the only possible
one. Other ID trees are ruled out by the interaction
of the principles on the ID dimension. For instance,
the government and agreement principles conspire
to rule out the reading where Roman is the subject of
versucht (and Peter the object). How? By the agree-
ment principle, Roman must be accusative, since it
agrees with its accusative determiner einen. By the
government principle, the subject of versucht must
be nominative, and the object of lesen accusative.
Thus Roman, by virtue of being accusative, cannot
become the subject of versucht. The only other op-
tion for it is to become the object of lesen. Conse-
quently, Peter, which is unspecified for case, must
become the subject of versuchen (versuchen must
have a subject by the valency principle).
</bodyText>
<subsectionHeader confidence="0.970361">
4.6 Topicalization
</subsectionHeader>
<bodyText confidence="0.973826944444445">
Our second example is a case of topicalization,
where the object has moved into the Vorfeld, to the
left of the finite verb:
Einen Roman versucht Peter zu lesen. (9)
Here is the ID tree and the LP tree analysis:
The ID tree analysis is the same as before, except
that the words are shown in different positions. In
the LP tree, Roman is in the Vorfeld of versucht, Pe-
ter in the Mittelfeld, and lesen in the right bracket
field. versucht itself is (by its node label) in the left
bracket field. Moreover, Einen is in the determiner
field of Roman, and zu in the particle field of lesen.
Again, this is an example demonstrating how
complex phenomena (here: topicalization) are ex-
plained by the interaction of simple principles. Top-
icalization does not have to explicitly taken care of,
it is rather a consequence of the interacting princi-
ples. Here, the valency, projectivity and climbing
</bodyText>
<figure confidence="0.989945705882353">
The LP dimension uses the following principles:
tree(LP)
valency(LP,inLP,outLP)
order(LP,onLP,�LP)
projectivity(LP)
(4)
Peter versucht einen Roman zu lesen
(8)
detf
Einen Roman versucht Peter zu lesen
lbf
rbf
nounf
nounf
partf
Einen Roman versucht Peter zu lesen
(3)
</figure>
<table confidence="0.969693">
inID outID casesID governID agreeID
den {det?} {} {acc} {} {}
Roman {subj?,obj?} {det!} {nom,dat,acc} {} {det}
Peter {subj?,obj?} {} {nom,dat,acc} {} {}
versucht {} {subj!,vinf!} {subj  {nom}} {}
zu {part?} {} {} {}
lesen {vinf?} {obj!} {obj  {acc}} {}
</table>
<figureCaption confidence="0.591344777777778">
Figure 1: Lexicon for the example grammar fragment, ID part
inLP outLP onLP linkLP,ID
den {detf?} {} {detf} {}
Roman {vf?,mf?} {detf!} {nounf} {}
Peter {vf?,mf?} {} {nounf} {}
versucht {} {vf?,mf,rbf?} {lbf} {}
zu {partf?} {} {partf} {}
lesen {rbf?} {} {rbf} {}
Figure 2: Lexicon for the example grammar fragment, LP part
</figureCaption>
<bodyText confidence="0.999862461538462">
principles conspire to bring about the “climbing up”
of the NP Einen Roman from being the daughter of
lesen in the ID tree to being the daughter of versucht
in the LP tree: The out specification of lesen does
not license any outgoing edge. Hence, Roman must
become the daughter of another node. The only pos-
sibility is versucht. The determiner Einen must then
also “climb up” because Roman is its only possi-
ble mother. The result is an LP tree which is flat-
ter with respect to the ID tree. The LP tree is also
projective. If it were not be flatter, then it would
be non-projective, and ruled out by the projectivity
principle.
</bodyText>
<subsectionHeader confidence="0.969479">
4.7 Negative example
</subsectionHeader>
<bodyText confidence="0.9992845">
Our third example is a negative example, i.e. an un-
grammatical sentence:
</bodyText>
<subsectionHeader confidence="0.527284">
Peter einen Roman versucht zu lesen. (12)
</subsectionHeader>
<bodyText confidence="0.999881181818182">
This example is perfectly legal on the unordered ID
dimension, but has no model on the LP dimension.
Why? Because by its LP out specification, the finite
verb versucht allows only one dependent to the left
of it (in its Vorfeld), and here we have two. The
interesting aspect of this example is that although
we can find a well-formed ID tree for it, this ID tree
is never actually generated. The interactions of the
principles, viz. here of the principles on the LP di-
mension, rule out the sentence before any full ID
analysis has been found.
</bodyText>
<subsectionHeader confidence="0.968944">
4.8 From German to Dutch
</subsectionHeader>
<bodyText confidence="0.993381857142857">
For the fourth example, we switch from German to
Dutch. We will show how to use the lexicon to con-
cisely capture an important cross-linguistic general-
ization. We keep the same grammar as before, but
with two changes, arising from the lesser degree of
inflection and the higher reliance on word order in
Dutch:
</bodyText>
<listItem confidence="0.994905666666667">
• The determiner een is not case-marked but
can be either nominative, dative or accusative:
casesID = {nom,dat,acc}.
• The Vorfeld of the finite verb probeert cannot
be occupied by an object (but only by an ob-
ject): linkLP,ID = {vf H {subj}}.7
</listItem>
<bodyText confidence="0.98367245">
Now to the example, a Dutch translation of (7):
Peter probeert een roman te lezen.
Peter tries a novel to read. (13)
Peter tries to read a novel.
We get only one analysis on the ID dimension,
where Peter is the subject and roman the object.
An analysis where Peter is the object of lezen and
roman the subject of probeert is impossible, as in
the German example. The difference is, however,
how this analysis is excluded. In German, the ac-
cusative inflection of the determiner einen triggered
the agreement and the government principle to rule
it out. In Dutch, the determiner is not inflected.
The unwanted analysis is excluded on the grounds
of word order instead: By the linking principle, the
Vorfeld of probeert must be filled by a subject, and
not by an object. That means that Peter in the Vor-
feld (to the left of probeert) must be a subject, and
consequently, the only other choice for roman is that
it becomes the object of lezen.
</bodyText>
<subsectionHeader confidence="0.992728">
4.9 Predicate-Argument Structure
</subsectionHeader>
<bodyText confidence="0.89825">
Going towards semantics, we extend the grammar
with another dimension, Predicate-Argument Struc-
ture (PA), where the models are not trees but di-
rected acyclic graphs (dags), to model re-entrancies
</bodyText>
<footnote confidence="0.891362">
7Of course, this is an idealized assumption. In fact, given
the right stress, the Dutch Vorfeld can be filled by objects.
</footnote>
<bodyText confidence="0.96018075">
e.g. caused by control constructions. Thanks to the
modularity of XDG, the PA part of the grammar is
the same for German and Dutch.
The set LabPA of labels on the PA dimension is:
</bodyText>
<equation confidence="0.931636">
LabPA = {ag,pat,prop} (14)
</equation>
<bodyText confidence="0.99747025">
Corresponding resp. to agent, patient and proposi-
tion.
The PA dimension uses the following one-
dimensional principles:
</bodyText>
<equation confidence="0.9925325">
dag(PA) (15)
valency(PA,inPA,outPA)
</equation>
<bodyText confidence="0.99850625">
Note that we re-use the valency principle again, as
we did on the ID and LP dimensions.
And also the following multi-dimensional princi-
ples:
</bodyText>
<equation confidence="0.990713">
climbing(ID, PA) (16)
linking(PA, ID)
</equation>
<bodyText confidence="0.999608692307692">
Here, we re-use the climbing and linking princi-
ples. That is, we state that the ID tree is flatter
than the corresponding PA dag. This captures rais-
ing and control, where arguments of embedded infi-
nite verbs can “climb up” and become arguments of
a raising or control verb, in the same way as syntac-
tic arguments can “climb up” from ID to LP. We use
the linking principle to specify how semantic argu-
ments are to be realized syntactically (e.g. the agent
as a subject etc.). We display the PA part of the lex-
icon in Figure 3.8
Here is an example PA dag analysis of example
sentence (7):
</bodyText>
<subsubsectionHeader confidence="0.69209">
Peter versucht einen Roman zu lesen (17)
</subsubsectionHeader>
<bodyText confidence="0.995695045454546">
Here, Peter is the agent of versucht, and also the
agent of lesen. Furthermore, lesen is a proposition
dependent of versucht, and Roman is the patient of
lesen.
Notice that the PA dag is indeed a dag and not a
tree since Peter has two incoming edges: It is simul-
taneously the agent of versucht and of lesen. This
is enforced by by the valency principle: Both ver-
sucht and lesen require an agent. Peter is the only
word which can be the agent of both, because it is a
subject and the agents of versucht and lesen must
be subjects by the linking principle. The climb-
ing principle ensures that predicate arguments can
$Notice that we specify linking lexically, allowing us to
capture deviations from the typical linking patterns. Still, we
can also accommodate linking generalizations using lexical ab-
stractions.
be “raised” on the ID structure with respect to the
PA structure. Again, this example demonstrates that
XDG is able to reduce a complex phenomenon such
as control to the interaction of per se fairly simple
principles such as valency, climbing and linking.
</bodyText>
<sectionHeader confidence="0.996975" genericHeader="evaluation">
5 Comparison
</sectionHeader>
<bodyText confidence="0.999184723404256">
This section includes a more in-depth comparison
of XDG with purely multi- and mono-stratal ap-
proaches.
Contrary to multi-stratal approaches like LFG or
MTT, XDG is more integrated. For one, it places
a lighter burden the interfaces between the dimen-
sions. In LFG for instance, the φ-mapping from c-
structure to f-structure is rather specific, and has to
be specifically adapted to new c-structures, e.g. in
order to handle a new construction with a different
word order. That is, not only the grammar rules for
the c-structure need to be adapted, but also the inter-
face between c- and f-structure. In XDG, complex
phenomena arise out of the interaction of simple,
maximally general principles. To accommodate the
new construction, the grammar would ideally only
need to be adapted on the word order dimension.
Furthermore, XDG allows interactions of rela-
tional constraints between all dimensions, not only
between adjacent ones (like c- and f-structure),
and in all directions. For one, this gets us bi-
directionality for free. Secondly, the interactions
of XDG have the potential to help greatly in reduc-
ing ambiguity. In multi-stratal approaches, ambigu-
ity must be duplicated throughout the system. E.g.
suppose there are two candidate c-structures in LFG
parsing, but one is ill-formed semantically. Then
they can only be ruled out after duplicating the am-
biguity on the f-structure, and then filtering out the
ill-formed structure on the semantic σ-structure. In
XDG on the other hand, the semantic principles can
rule out the ill-formed analysis much earlier, typ-
ically on the basis of a partial syntactic analysis.
Thus, ill-formed analyses are never duplicated.
Contrary to mono-stratal ones, XDG is more
modular. For one, as (Oliva et al., 1999) note,
mono-stratal approaches like HPSG usually give
precedence to the syntactic tree structure, while
putting the description of other aspects of the anal-
ysis on the secondary level only, by means of fea-
tures spread over the nodes of the tree. As a result,
it becomes a hard task to modularize grammars. Be-
cause syntax is privileged, the phenomena ascribing
to semantics cannot be described independently, and
whenever the syntax part of the grammar changes,
the semantics part needs to be adapted. In XDG, no
dimension is privileged to another. Semantic phe-
</bodyText>
<table confidence="0.996050571428571">
inPA outPA linkPA,m
den {} {} {}
Roman {ag?,pat?} {} {}
Peter {ag?,pat?} {} {}
versucht {} {ag!,prop!} {ag  {subj},prop  {vinf}}
zu {} {} {}
lesen {prop?} {ag!,pat!} {ag  {subj},pat  {obj}}
</table>
<figureCaption confidence="0.99712">
Figure 3: Lexicon of the example grammar fragment, PA part
</figureCaption>
<bodyText confidence="0.998513142857143">
nomena can be described much more independently
from syntax. This facilitates grammar engineering,
and also the statement of cross-linguistic general-
izations. Assuming that the semantics part of a
grammar stay invariant for most natural languages,
in order to accommodate a new language, ideally
only the syntactic parts would need to be changed.
</bodyText>
<sectionHeader confidence="0.999649" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999992233333333">
In this paper, we introduced the XDG grammar
framework, and emphasized that its new methodol-
ogy places it in between the extremes of multi- and
mono-stratal approaches. By means of an idealized
example grammar, we demonstrated how complex
phenomena are explained as arising from the in-
teraction of simple principles on numerous dimen-
sions of linguistic description. On the one hand, this
methodology has the potential to modularize lin-
guistic description and grammar engineering, and
to facilitate the statement of linguistic generaliza-
tions. On the other hand, as XDG is a inherently
concurrent architecture, inferences from any dimen-
sion can help reduce the ambiguity on others.
XDG is a new grammar formalism, and still has
many open issues. Firstly, we need to continue work
on XDG as a framework. Here, one important goal
is to find out what criteria we can give to restrict the
principles. Secondly, we need to evolve the XDG
grammar theory, and in particular the XDG syntax-
semantics interface. Thirdly, for practical use, we
need to improve our knowledge about XDG solv-
ing (i.e. parsing and generation). So far, our only
good results are for smaller-scale handwritten gram-
mars, and we have not good results yet for larger-
scale grammars induced from treebanks (NEGRA,
PDT) or converted from other grammar formalisms
(XTAG). Finally, we need to incorporate statistics
into the picture, e.g. to guide the search for solu-
tions, in the vein of (Dienes et al., 2003).
</bodyText>
<sectionHeader confidence="0.998929" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.993585361702128">
Joan Bresnan and Ronald Kaplan. 1982. Lexical-
functional grammar: A formal system for gram-
matical representation. In Joan Bresnan, editor,
The Mental Representation of Grammatical Re-
lations, pages 173–281. The MIT Press, Cam-
bridge/USA.
Marie-H`el´ene Candito. 1996. A principle-based hi-
erarchical representation of LTAG. In Proceed-
ings of COLING 1996, Kopenhagen/DEN.
Peter Dienes, Alexander Koller, and Marco
Kuhlmann. 2003. Statistical A* Dependency
Parsing. In Prospects and Advances in the Syn-
tax/Semantics Interface, Nancy/FRA.
Denys Duchier and Ralph Debusmann. 2001.
Topological dependency trees: A constraint-
based account of linear precedence. In Proceed-
ings ofACL 2001, Toulouse/FRA.
Denys Duchier. 2003. Configuration of labeled
trees under lexicalized constraints and principles.
Research on Language and Computation, 1(3–
4):307–336.
Sylvain Kahane, Alexis Nasr, and Owen Ram-
bow. 1998. Pseudo-projectivity: a polynomi-
ally parsable non-projective dependency gram-
mar. In 36th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL 1998),
Montr´eal/CAN.
Sylvain Kahane. 2002. Grammaire d’Unification
Sens-Texte: Vers un mod`ele math´ematique ar-
ticul´e de la langue. Universit´e Paris 7. Docu-
ment de synth`ese de l’habilitation a` diriger les
recherches.
Alexander Koller and Kristina Striegnitz. 2002.
Generation as dependency parsing. In Proceed-
ings ofACL 2002, Philadelphia/USA.
Igor Mel’ˇcuk. 1988. Dependency Syntax: Theory
and Practice. State Univ. Press of New York, Al-
bany/USA.
Karel Oliva, M. Andrew Moshier, and Sabine
Lehmann. 1999. Grammar engineering for the
next millennium. In Proceedings of the 5th Natu-
ral Language Processing Pacific Rim Symposium
1999 “Closing the Millennium”, Beijing/CHI.
Tsinghua University Press.
Carl Pollard and Ivan A. Sag. 1994. Head-
Driven Phrase Structure Grammar. University of
Chicago Press, Chicago/USA.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.425572">
<title confidence="0.994687">Extensible Dependency Grammar: A New Methodology</title>
<author confidence="0.999816">Ralph Debusmann Denys Duchier Geert-Jan M Kruijff</author>
<affiliation confidence="0.9980485">Programming Systems Lab ´Equipe Calligramme Computational Linguistics Saarland University LORIA – UMR 7503 Saarland University</affiliation>
<address confidence="0.579485">Postfach 15 11 50 Campus Scientifique, B. P. 239 Postfach 15 11</address>
<author confidence="0.842237">Saarbr ucken Vandcruvre l`es Nancy Saarbr ucken Germany France Germany</author>
<email confidence="0.985858">rade@ps.uni-sb.deduchier@loria.frgj@coli.uni-sb.de</email>
<abstract confidence="0.9965465">This paper introduces the new grammar formalism of Extensible Dependency Grammar (XDG), and emphasizes the benefits of its methodology of explaining complex phenomena by interaction of simple principles on multiple dimensions of linguistic description. This has the potential to increase modularity with respect to linguistic description and grammar engineering, and to facilitate concurrent processing and the treatment of ambiguity.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Joan Bresnan</author>
<author>Ronald Kaplan</author>
</authors>
<title>Lexicalfunctional grammar: A formal system for grammatical representation.</title>
<date>1982</date>
<booktitle>The Mental Representation of Grammatical Relations,</booktitle>
<pages>173--281</pages>
<editor>In Joan Bresnan, editor,</editor>
<publisher>The MIT Press, Cambridge/USA.</publisher>
<contexts>
<context position="1246" citStr="Bresnan and Kaplan, 1982" startWordPosition="174" endWordPosition="178">interaction of simple principles on multiple dimensions of linguistic description. This has the potential to increase modularity with respect to linguistic description and grammar engineering, and to facilitate concurrent processing and the treatment of ambiguity. 1 Introduction We introduce the new grammar formalism of Extensible Dependency Grammar (XDG). In XDG, complex phenomena arise out of the interaction of simple principles on multiple dimensions of linguistic description. In this paper, we point out how this novel methodology positions XDG in between multi-stratal approaches like LFG (Bresnan and Kaplan, 1982) and MTT (Mel’ˇcuk, 1988), see also (Kahane, 2002), and mono-stratal ones like HPSG (Pollard and Sag, 1994), attempting to combine their benefits and avoid their problems. It is the division of linguistic analyses into different dimensions which makes XDG multi-stratal. On the other, XDG is mono-stratal in that its principles interact to constrain all dimensions simultaneously. XDG combines the benefits of these two positions, and attempts to circumvent their problems. From multi-stratal approaches, XDG adopts a high degree of modularity, both with respect to linguistic description as well as </context>
</contexts>
<marker>Bresnan, Kaplan, 1982</marker>
<rawString>Joan Bresnan and Ronald Kaplan. 1982. Lexicalfunctional grammar: A formal system for grammatical representation. In Joan Bresnan, editor, The Mental Representation of Grammatical Relations, pages 173–281. The MIT Press, Cambridge/USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-H`el´ene Candito</author>
</authors>
<title>A principle-based hierarchical representation of LTAG.</title>
<date>1996</date>
<booktitle>In Proceedings of COLING</booktitle>
<location>Kopenhagen/DEN.</location>
<contexts>
<context position="12551" citStr="Candito, 1996" startWordPosition="2049" endWordPosition="2050">ples on different dimensions of linguistic description. • How the high degree of integration helps to reduce ambiguity. • How the high degree of modularity facilitates the statement of cross-linguistic generalizations. Note that this grammar fragment is an idealized example, and does not make any claims about XDG as a grammar theory. Its purpose is solely to substantiate our points about XDG as a framework. Moreover, the grammar is fully lexicalized for simplicity. However, XDG of course allows the grammar writer to formulate lexical abstractions using inheritance (like in HPSG) or crossings (Candito, 1996). 4.1 Dimensions The grammar fragment make use of two dimensions: Immediate Dominance (ID) and Linear Precedence (LP). The models on the ID dimension are unordered, syntactic dependency trees whose edge labels correspond to syntactic functions like subject and object. On the LP dimension, the models are ordered, projective topological dependency trees whose edge labels are topological fields like Vorfeld and Mittelfeld. 4.2 Labels The set LabID of labels on the ID dimension is: LabID = {det,subj,obj,vinf,part} (1) These correspond resp. to determiner, subject, object, infinitive verbal complem</context>
</contexts>
<marker>Candito, 1996</marker>
<rawString>Marie-H`el´ene Candito. 1996. A principle-based hierarchical representation of LTAG. In Proceedings of COLING 1996, Kopenhagen/DEN.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Dienes</author>
<author>Alexander Koller</author>
<author>Marco Kuhlmann</author>
</authors>
<title>Statistical A* Dependency Parsing.</title>
<date>2003</date>
<booktitle>In Prospects and Advances in the Syntax/Semantics Interface,</booktitle>
<location>Nancy/FRA.</location>
<marker>Dienes, Koller, Kuhlmann, 2003</marker>
<rawString>Peter Dienes, Alexander Koller, and Marco Kuhlmann. 2003. Statistical A* Dependency Parsing. In Prospects and Advances in the Syntax/Semantics Interface, Nancy/FRA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Denys Duchier</author>
<author>Ralph Debusmann</author>
</authors>
<title>Topological dependency trees: A constraintbased account of linear precedence.</title>
<date>2001</date>
<booktitle>In Proceedings ofACL 2001,</booktitle>
<location>Toulouse/FRA.</location>
<contexts>
<context position="2364" citStr="Duchier and Debusmann, 2001" startWordPosition="347" endWordPosition="350">al approaches, XDG adopts a high degree of modularity, both with respect to linguistic description as well as for grammar engineering. This also facilitates the statement of cross-linguistic generalizations. XDG avoids the problem of placing too high a burden on the interfaces, and allows interactions between all and not only adjacent dimensions. From mono-stratal approaches, XDG adopts a high degree of integration, facilitating concurrent processing and the treatment of ambiguity. At the same time, XDG does not lose its modularity. XDG is a descendant of Topological Dependency Grammar (TDG) (Duchier and Debusmann, 2001), pushing the underlying methodology further by generalizing it in two aspects: • number of dimensions: two in TDG (ID and LP), arbitrary many in XDG • set of principles: fixed in TDG, extensible principle library in XDG The structure of this paper is as follows: In §2, we introduce XDG and the XDG solver used for parsing and generation. In §3, we introduce a number of XDG principles informally, before making use of them in an idealized example grammar in §4. In §5 we argue why XDG has the potential to be an improvement over multi-stratal and mono-stratal approaches, before we conclude in §6. </context>
</contexts>
<marker>Duchier, Debusmann, 2001</marker>
<rawString>Denys Duchier and Ralph Debusmann. 2001. Topological dependency trees: A constraintbased account of linear precedence. In Proceedings ofACL 2001, Toulouse/FRA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Denys Duchier</author>
</authors>
<title>Configuration of labeled trees under lexicalized constraints and principles.</title>
<date>2003</date>
<journal>Research on Language and Computation,</journal>
<volume>1</volume>
<issue>3</issue>
<pages>4--307</pages>
<contexts>
<context position="5172" citStr="Duchier, 2003" startWordPosition="843" endWordPosition="844">ute analyses for a given input, we employ a set of input constraints (Inp), which again specify a subset of Ana. XDG solving then amounts to finding elements of Ana that are licensed by Lex, and consistent with Inp and Pri. The input constraints determine whether XDG solving is to be used for parsing or generation. For parsing, they specify a sequence of words, and for generation, a multiset of semantic literals. 2.2 Solver XDG solving has a natural reading as a constraint satisfaction problem (CSP) on finite sets of integers, where well-formed analyses correspond to the solutions of the CSP (Duchier, 2003). We have implemented an XDG solver using the Mozart-Oz programming system. XDG solving operates on all dimensions concurrently. This means that the solver can infer information about one dimension from information on another, if there is either a multi-dimensional principle linking the two dimensions, or by the synchronization induced by the lexical entries. For instance, not only can syntactic information trigger inferences in syntax, but also vice versa. Because XDG allows us to write grammars with completely free word order, XDG solving is an NP-complete problem (Koller and Striegnitz, 200</context>
</contexts>
<marker>Duchier, 2003</marker>
<rawString>Denys Duchier. 2003. Configuration of labeled trees under lexicalized constraints and principles. Research on Language and Computation, 1(3– 4):307–336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sylvain Kahane</author>
<author>Alexis Nasr</author>
<author>Owen Rambow</author>
</authors>
<title>Pseudo-projectivity: a polynomially parsable non-projective dependency grammar.</title>
<date>1998</date>
<booktitle>In 36th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<location>Montr´eal/CAN.</location>
<contexts>
<context position="10680" citStr="Kahane et al., 1998" startWordPosition="1736" endWordPosition="1739">er prescribes vf  lbf  mf). 3.7 Projectivity principle projectivity(i) The analysis on dimension i must be projective. The projectivity principle is non-lexicalized. Its purpose is to exclude non-projective analyses.4 It is parametrized by dimension i. 3.8 Climbing principle climbing(i, j) The graph on dimension i must be flatter than the graph on dimension j. The climbing principle is non-lexicalized and two-dimensional. It is parametrized by the two dimensions i and j. For instance, the tree in (11) is flatter than the corresponding tree in (10). This concept was introduced as lifting in (Kahane et al., 1998). 2Again, we restrict ourselves to case for simplicity. 3The node labels are defined in (2) below. 4The projectivity principle of course only makes sense in combination with the order principle. 3.9 Linking principle linking(i, j,linki, j) All edges on dimension i must satisfy the linking specification of the mother. The linking principle is lexicalized and twodimensional. It is parametrized by the two dimensions i and j, and by the linking specification linki,j, mapping labels from Labi to sets of labels from Labj. Its purpose is to specify how dependents on dimension i are realized by (or li</context>
</contexts>
<marker>Kahane, Nasr, Rambow, 1998</marker>
<rawString>Sylvain Kahane, Alexis Nasr, and Owen Rambow. 1998. Pseudo-projectivity: a polynomially parsable non-projective dependency grammar. In 36th Annual Meeting of the Association for Computational Linguistics (ACL 1998), Montr´eal/CAN.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sylvain Kahane</author>
</authors>
<title>Grammaire d’Unification Sens-Texte: Vers un mod`ele math´ematique articul´e de la langue. Universit´e Paris 7. Document de synth`ese de l’habilitation a` diriger les recherches.</title>
<date>2002</date>
<contexts>
<context position="1296" citStr="Kahane, 2002" startWordPosition="185" endWordPosition="186">guistic description. This has the potential to increase modularity with respect to linguistic description and grammar engineering, and to facilitate concurrent processing and the treatment of ambiguity. 1 Introduction We introduce the new grammar formalism of Extensible Dependency Grammar (XDG). In XDG, complex phenomena arise out of the interaction of simple principles on multiple dimensions of linguistic description. In this paper, we point out how this novel methodology positions XDG in between multi-stratal approaches like LFG (Bresnan and Kaplan, 1982) and MTT (Mel’ˇcuk, 1988), see also (Kahane, 2002), and mono-stratal ones like HPSG (Pollard and Sag, 1994), attempting to combine their benefits and avoid their problems. It is the division of linguistic analyses into different dimensions which makes XDG multi-stratal. On the other, XDG is mono-stratal in that its principles interact to constrain all dimensions simultaneously. XDG combines the benefits of these two positions, and attempts to circumvent their problems. From multi-stratal approaches, XDG adopts a high degree of modularity, both with respect to linguistic description as well as for grammar engineering. This also facilitates the</context>
</contexts>
<marker>Kahane, 2002</marker>
<rawString>Sylvain Kahane. 2002. Grammaire d’Unification Sens-Texte: Vers un mod`ele math´ematique articul´e de la langue. Universit´e Paris 7. Document de synth`ese de l’habilitation a` diriger les recherches.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Koller</author>
<author>Kristina Striegnitz</author>
</authors>
<title>Generation as dependency parsing.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL 2002,</booktitle>
<location>Philadelphia/USA.</location>
<contexts>
<context position="5774" citStr="Koller and Striegnitz, 2002" startWordPosition="937" endWordPosition="940">of the CSP (Duchier, 2003). We have implemented an XDG solver using the Mozart-Oz programming system. XDG solving operates on all dimensions concurrently. This means that the solver can infer information about one dimension from information on another, if there is either a multi-dimensional principle linking the two dimensions, or by the synchronization induced by the lexical entries. For instance, not only can syntactic information trigger inferences in syntax, but also vice versa. Because XDG allows us to write grammars with completely free word order, XDG solving is an NP-complete problem (Koller and Striegnitz, 2002). This means that the worst-case complexity of the solver is exponential. The average-case complexity of many smaller-scale grammars that we have experimented with seems polynomial, but it remains to be seen whether we can scale this up to largescale grammars. 3 Principles The well-formedness conditions of XDG analyses are stipulated by principles. Principles are parametrizable, e.g. by the dimensions on which they are applied, or by lexical features. They can be lexicalized or non-lexicalized, and can be onedimensional or multi-dimensional. Principles are taken from an extensible principle li</context>
</contexts>
<marker>Koller, Striegnitz, 2002</marker>
<rawString>Alexander Koller and Kristina Striegnitz. 2002. Generation as dependency parsing. In Proceedings ofACL 2002, Philadelphia/USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor Mel’ˇcuk</author>
</authors>
<title>Dependency Syntax: Theory and Practice. State Univ.</title>
<date>1988</date>
<publisher>Press of</publisher>
<location>New York, Albany/USA.</location>
<marker>Mel’ˇcuk, 1988</marker>
<rawString>Igor Mel’ˇcuk. 1988. Dependency Syntax: Theory and Practice. State Univ. Press of New York, Albany/USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karel Oliva</author>
<author>M Andrew Moshier</author>
<author>Sabine Lehmann</author>
</authors>
<title>Grammar engineering for the next millennium.</title>
<date>1999</date>
<booktitle>In Proceedings of the 5th Natural Language Processing Pacific Rim Symposium 1999 “Closing the Millennium”, Beijing/CHI.</booktitle>
<publisher>Tsinghua University Press.</publisher>
<contexts>
<context position="24463" citStr="Oliva et al., 1999" startWordPosition="4051" endWordPosition="4054">al approaches, ambiguity must be duplicated throughout the system. E.g. suppose there are two candidate c-structures in LFG parsing, but one is ill-formed semantically. Then they can only be ruled out after duplicating the ambiguity on the f-structure, and then filtering out the ill-formed structure on the semantic σ-structure. In XDG on the other hand, the semantic principles can rule out the ill-formed analysis much earlier, typically on the basis of a partial syntactic analysis. Thus, ill-formed analyses are never duplicated. Contrary to mono-stratal ones, XDG is more modular. For one, as (Oliva et al., 1999) note, mono-stratal approaches like HPSG usually give precedence to the syntactic tree structure, while putting the description of other aspects of the analysis on the secondary level only, by means of features spread over the nodes of the tree. As a result, it becomes a hard task to modularize grammars. Because syntax is privileged, the phenomena ascribing to semantics cannot be described independently, and whenever the syntax part of the grammar changes, the semantics part needs to be adapted. In XDG, no dimension is privileged to another. Semantic pheinPA outPA linkPA,m den {} {} {} Roman {</context>
</contexts>
<marker>Oliva, Moshier, Lehmann, 1999</marker>
<rawString>Karel Oliva, M. Andrew Moshier, and Sabine Lehmann. 1999. Grammar engineering for the next millennium. In Proceedings of the 5th Natural Language Processing Pacific Rim Symposium 1999 “Closing the Millennium”, Beijing/CHI. Tsinghua University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan A Sag</author>
</authors>
<title>HeadDriven Phrase Structure Grammar.</title>
<date>1994</date>
<publisher>University of Chicago Press, Chicago/USA.</publisher>
<contexts>
<context position="1353" citStr="Pollard and Sag, 1994" startWordPosition="192" endWordPosition="195">ncrease modularity with respect to linguistic description and grammar engineering, and to facilitate concurrent processing and the treatment of ambiguity. 1 Introduction We introduce the new grammar formalism of Extensible Dependency Grammar (XDG). In XDG, complex phenomena arise out of the interaction of simple principles on multiple dimensions of linguistic description. In this paper, we point out how this novel methodology positions XDG in between multi-stratal approaches like LFG (Bresnan and Kaplan, 1982) and MTT (Mel’ˇcuk, 1988), see also (Kahane, 2002), and mono-stratal ones like HPSG (Pollard and Sag, 1994), attempting to combine their benefits and avoid their problems. It is the division of linguistic analyses into different dimensions which makes XDG multi-stratal. On the other, XDG is mono-stratal in that its principles interact to constrain all dimensions simultaneously. XDG combines the benefits of these two positions, and attempts to circumvent their problems. From multi-stratal approaches, XDG adopts a high degree of modularity, both with respect to linguistic description as well as for grammar engineering. This also facilitates the statement of cross-linguistic generalizations. XDG avoid</context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>Carl Pollard and Ivan A. Sag. 1994. HeadDriven Phrase Structure Grammar. University of Chicago Press, Chicago/USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>