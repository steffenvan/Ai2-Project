<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003632">
<note confidence="0.7209445">
In: Proceedings of CoNLL-2000 and LLL-2000, pages 194-198, Lisbon, Portugal, 2000.
Learning from Parsed Sentences with INTHELEX
F. Esposito and S. Ferilli and N. Fanizzi and G. Semeraro
Dipartimento di Informatica
Universita di Bari
via E. Orabona, 4 - 70126 Ban - Italia
</note>
<keyword confidence="0.268984">
fesposito, ferilli, fanizzi, semerarolOdi.uniba.it
</keyword>
<sectionHeader confidence="0.989262" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999913294117647">
In the context of language learning, we address a
logical approach to information extraction. The
system INTHELEX, used to carry out this task,
requires a logic representation of sentences to
run the learning algorithm. Hence, the need
for parsers to produce structured representa-
tions from raw text. This led us to develop a
prototypical Italian language parser, as a pre-
processor in order to obtain the structured rep-
resentation of sentences required for the sym-
bolic learner to work. A preliminary exper-
imentation proved that the logic approach to
learning from language is able to capture the
semantics underlying the kind of sentences that
were processed, even if a comparison with clas-
sical methods as regards efficiency has still to
be done.
</bodyText>
<sectionHeader confidence="0.998782" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999936534482759">
Language learning has gained growing attention
in the last years. Statistical approaches, so far
extensively used — see (Saitta and Neri, 1997)
for an overview of the research in this area —,
have severe limitations, whereas the flexibility
and expressivity of logical representations make
them highly suitable for natural language anal-
ysis (Cussens, 1999). Indeed, logical approaches
may have a relevant impact at the level of se-
mantic interpretation, where a logical represen-
tation of the meaning of a sentence is important
and useful (Mooney, 1999).
Logical approaches have been already em-
ployed in Text Categorization and/or Informa-
tion Extraction. Yet they try to use an expres-
sive representation language such as first-order
logic to define simple properties about tex-
tual sources, regarded, for instance, as bags of
words (Junker et al., 1999) or as semi-structured
texts (Freitag, 2000). These properties are of-
ten loosely related with the grammar of the
underlying language, often relying on extra-
grammatical features (Cohen, 1996). We intend
to exploit a logic representation for exploiting
the grammatical structure of texts, as it could
be detected using a proper parser. Indeed, a
more knowledge intensive technique is likely to
perform better applied on the tasks mentioned
above.
When no background knowledge about the
language structure is assumed to be available,
one of the fundamental problems with the adop-
tion of logic learning techniques is that a struc-
tured representation of sentences is required
on which the learning algorithm can be run.
Thus, the need arises for parsers that are able
to discover such a structure starting from raw,
unstructured text. Research in this field has
produced a variety of tools and techniques for
English, that cannot be applied to other lan-
guages, such as Italian, because of the differ-
ent, and sometimes much more complex, gram-
matical structure. Such considerations led us to
develop a prototypical Italian language parser,
that could serve as a pre-processor of texts in
order to obtain the structured representation of
sentences that is needed for the symbolic learner
to work. It is fundamental to note that the fo-
cus of this paper is not the parser, that does not
adopt sophisticated NLP techniques. The aim
here is investigating the feasibility of learning
semantic definitions for some kinds of sentences.
Even more so, the availability of a professional
parser will further enhance the performance of
the whole process.
Further problems in applying relational learn-
ing to language are due to the intrinsic compu-
tational complexity of these methods, as a draw-
</bodyText>
<page confidence="0.99811">
194
</page>
<bodyText confidence="0.999982647058823">
back of the expressive power gained through re-
lations. Moreover, another weakness of our ap-
proach could be the dependence on the quality
of the data coming from the preprocessing step:
it is possible that noise coming from wrongly
parsed sentences be present, thus having a neg-
ative influence towards the model to be induced.
After briefly presenting in Section 2 the
parser performance, in order to establish the de-
gree of reliability of the data on which the learn-
ing step is performed, Section 3 shows the re-
sults of applying the first-order learning system
INTHELEX (Esposito et al., 2000) for the in-
ference of some simple events related to foreign
commerce. Lastly, Section 4 draws some prelim-
inary conclusions on this research and outlines
future work issues to be addressed.
</bodyText>
<sectionHeader confidence="0.9683315" genericHeader="method">
2 A Stratified Parser for Italian
Language
</sectionHeader>
<bodyText confidence="0.999929870967742">
This section presents a parser for the Italian
language, based on context-free grammars and
designed to manage texts having a simple and
standard phrase structure (e.g., foreign com-
merce texts as opposed to poetry texts). It
is composed by 12 parsing levels and 106 pro-
duction rules, and uses the longest-match tech-
nique, which complies with the typical ambigu-
ity of Italian language. Syntactic lookahead is
used to overcome ambiguity and to prevent the
parsing from stopping in case of grammatically
wrong input.
The text is segmented in progressively larger
syntactic constructs. Subject, main verb, di-
rect or indirect object and clauses referring to
them are identified. Nested syntactic constructs
at the same abstraction level (e.g., expressions
including a sentence in parentheses) are sup-
ported.
Plain text documents are provided to a lexical
analyzer and a noun-recognizer (XEROX MUL-
TEXT), whose output is the document text
tagged with parts of speech to be fed to the
parser. Since Italian grammar is very differ-
ent from the English one, some terms do not
have an English equivalent and, hence, cannot
be translated.
The parser was validated on a set of 72
sentences drawn from a corpus of articles on
foreign commerce available on the Internet,
and the results obtained were evaluated with
</bodyText>
<table confidence="0.9997505">
Parsing Phase Precision Recall
Noun Groups 0.984 0.992
1st level NPs 0.994 0.994
2nd level NPs 0.983 0.983
PPs 0.951 0.951
Clauses 0.840 0.840
Refined clauses 0.913 0.913
Sentences 0.736 0.736
</table>
<tableCaption confidence="0.998302">
Table 1: Summary of parser validation results
</tableCaption>
<table confidence="0.999678444444444">
(Precision/Recall)
Parsing Phase Errorl Error2
Noun Groups 0.787% 0.793%
1st level NPs 0.596% 0.596%
2nd level NPs 1.666% 1.666%
PPs 4.918% 4.918%
Clauses 15.941% 15.941%
Refined clauses 8.695% 8.695%
Sentences 26.384% 26.384%
</table>
<tableCaption confidence="0.796532">
Table 2: Summary of parser validation results
(Errorl/Error2)
</tableCaption>
<bodyText confidence="0.9749465">
respect to precision, recall (reported in Table 1)
and two measures about error ratio:
</bodyText>
<equation confidence="0.6282785">
Errorl = # errors I# total constituents extracted
Error2 = # errors I# total correct constituents expected
</equation>
<bodyText confidence="0.868275">
(see Table 2).
</bodyText>
<sectionHeader confidence="0.991446" genericHeader="method">
3 Information extraction
</sectionHeader>
<bodyText confidence="0.999941705882353">
The grammar above was used to parse Italian
texts downloaded from the Internet, and con-
cerning foreign commerce. Through such pre-
processing, the aim was to obtain some struc-
ture for those texts that could then be trans-
lated in the input language of the learning sys-
tem INTHELEX (Esposito et al., 2000) in order
to make it learn simple events concerning that
domain.
INTHELEX (INcremental THEory Learner
from EXamples) is a fully incremental, multi-
conceptual closed loop learning system for the
induction of hierarchical theories from exam-
ples. In detail, full incrementality avoids the
need of a previously generated version of the
theory to be available, so that learning can start
from an empty theory and from the first exam-
</bodyText>
<page confidence="0.995718">
195
</page>
<bodyText confidence="0.9994594">
pie; multi-conceptual means that it can learn si-
multaneously various concepts, possibly related
to each other; a closed loop system is a system
in which the learned theory is checked to be
valid on any new example available, and in case
of failure a revision process is activated on it,
in order to restore the completeness and consis-
tency properties.
Incremental learning is necessary when either
incomplete information is available at the time
of initial theory generation, or the nature of the
concepts evolves dynamically. The latter situ-
ation is the most difficult to handle since time
evolution needs to be considered. In any case,
it is useful to consider learning as a closed loop
process, where feedback on performance is used
to activate the theory revision phase.
INTHELEX learns theories, from positive and
negative examples described in the same lan-
guage. It adopts a full memory storage strategy
— i.e., it retains all the available examples, thus
the learned theories are guaranteed to be valid
on the whole set of known examples.
In the formal representation of texts, we used
the following descriptors:
</bodyText>
<listItem confidence="0.9998204">
• sent (el ,e2) e2 is a sentence from el
• subj (el ,e2) e2 is the subject of el
• obj (el ,e2) e2 is the (direct) object of el
• indirect_obj (el ,e2) e2 is an indirect ob-
ject of el
• rel_subj (el , e2 is a clause related to
the subject el
• rel_obj (el ,e2) e2 is a clause related to
the object el
• verb (el ,e2) e2 is the verb of el
• lemma(e2) word e2 has lemma lemma
• infinite (e2) verb e2 is in an infinite
mood
• finite (e2) verb e2 is in a finite mood
• affirmative (e2) verb e2 is in an affirma-
tive mood
• negative (e2) verb e2 is in a negative
mood
• np (el ,e2) e2 is a 2nd level NP of el
• pp (el ,e2) e2 is a PP of el
</listItem>
<bodyText confidence="0.999970142857143">
where lemma is a meta-predicate. This allows
the system to exploit information about word
lemmas in generalizations/specializations, and
in the recognition of higher level concepts of
which lemma is an instance.
Thus, the following Horn clause is an instance
of an example:
</bodyText>
<equation confidence="0.836411913043478">
imports (example) &lt;—
sent (example , el) ,
subj(example,e2),
np(e2,e3),
impresa(e3),
rel_subj(el,e4),
verb(e4,e5),
specializzare(e5),
infinite(e5),
affirmative(e5),
pp(e4,e6),
distrubuzione(e6),
componente(e6),
verb(el,e7),
interessare(e7),
finite(e7),
affirmative(e7),
indirect_obj(el,e8),
pp(e8,e9),
importazione(e9),
macchina(e9),
produzione(e9),
ombrello(e9).
</equation>
<bodyText confidence="0.9999708">
A first experiment aimed at learning the con-
cept of specialization (of someone in some field).
The system was run on 40 examples, 24 posi-
tive and 16 negative. The resulting theory was
made up by 5 clauses, some of which differ just
in one literal (e.g., the lemma of the word in the
subject). By exploiting the background knowl-
edge that terms &apos;impresa&apos;, `societa&apos;, `ditta&apos; and
`agenzia&apos; are all instances of the concept &apos;per-
sona giuridica&apos;, i.e. clauses:
</bodyText>
<equation confidence="0.972454875">
persona_giuridica(X)
ditta(X).
persona_giuridica(X) +-
societa(X).
persona_giuridica(X) f-
impresa(X).
persona_giuridica(X) ÷-
agenzia(X).
</equation>
<page confidence="0.990096">
196
</page>
<bodyText confidence="0.987679">
the theory becomes more compact, yielding
the following rules:
</bodyText>
<equation confidence="0.910649368421053">
specialization(A) &lt;-
sent (A ,B) ,
subj(B,C), np(C,D) ,
persona_giuridica(D) ,
verb (B ,E) ,
specializzare (E)
finite (E) ,
affirmative (E) ,
indirect_obj (B,F) ,
pp (F ,_)
specialization(A)
sent (A ,B) ,
subj(B,C), np(C,D) ,
persona_giuridica(D) ,
rel_subj (B,E) ,
verb (E ,F) ,
specializzare (F) ,
affirmative (F) ,
pp (E , _) , verb (B , _) .
</equation>
<bodyText confidence="0.984242302631579">
Another experiment aimed at learning the
concept of &amp;quot;imports&amp;quot;. INTHELEX was run
starting from the empty theory, and was fed
with a total of 67 examples (39 positive and 28
negative). It should be noted that not all posi-
tive examples explicitly use verb &apos;importare&apos; (to
import): e.g., in the sentence &amp;quot;Societa belga,
specializzata nella lavorazione del legno, cerca
fornitori di legname&amp;quot; the imports event is char-
acterized by the noun `societa&apos; (society) as the
sentence subject, by the verb `cercare&apos; (to look
for) and by the object including the noun &apos;for-
nitore&apos; (provider). We obtained the following
results (in which the above background knowl-
edge was used to compress more rules into one,
too):
imports(A) E- sent (A ,B) ,
subj(B,C), np(C,D) ,
persona_giuridica(D) ,
verb (B , E) ,
cercare (E)
finite(E),
affirmative(E),
obj(B,F), np(F,G),
fornitore(G).
imports(A) ÷-- sent (A ,B) ,
subj(B,C), np(C,D) ,
persona_giuridica(D),
societa(D),
verb(B,E),
cercare(E),
finite(E),
affirmative(E),
obj(B,F), np(F,G),
distributore(G).
imports(A) +- sent(A,B),
subj(B,C), np(C,D),
persona_giuridica(D),
verb(B,E),
interessare(E),
finite(E),
affirmative(E),
indirect_obj(B,F),
pp(F,G),
importazione(G).
imports(A) +- sent(A,B),
subj(B,C), np(C,D),
persona_giuridica(D),
verb(B,E),
acquistare(E),
finite(E),
affirmative(E).
imports(A) +- sent (A ,B)
subj(B,C), np(C,D) ,
persona_giuridica(B),
impresa(D),
verb(B,E),
importare(E),
finite(E),
affirmative(E).
For instance, the third clause means: &amp;quot;Text A
deals with imports if it contains a sentence with
a subject composed by a NP containing a per-
sona giuridica, the verb of the main sentence
is interessare (to interest) in finite affirmative
mood, and the indirect object consists of a PP
containing the word importazione&amp;quot;. Note that,
by exploiting a background knowledge that rep-
resents a more complex ontology than the cur-
rent one, it would be possible to further merge
conceptual descriptors and, as a consequence,
clauses in the theory. For example, lornitore&apos;
(provider) and `distributore&apos; (distributor) could
be recognized as instances of a common higher
level concept; the same applies to `acquistare&apos;
(to buy) and &apos;importare&apos; (to import).
</bodyText>
<page confidence="0.997274">
197
</page>
<sectionHeader confidence="0.995107" genericHeader="conclusions">
4 Conclusions &amp; Future Work
</sectionHeader>
<bodyText confidence="0.999993357142857">
We have addressed the problem of learning logic
theories for information extraction so to bene-
fit by the semantic interpretation provided by
a logical approach. This has required struc-
tured sentences in a logic representation on
which to run our learning algorithms. Hence,
we needed a parser to produce structured repre-
sentations from raw unstructured text. Though
many techniques have been developed for En-
glish, they cannot be applied to other languages,
such as Italian, because of the different gram-
matical structure. This has led us to develop a
prototypical Italian language parser, as a pre-
processor in order to obtain the structured rep-
resentation of sentences needed for the symbolic
learner to work.
Future work will concern a more extensive ex-
perimentation, an empirical evaluation of our
approach, and application of the same kind of
experiments on English parsed texts. If good
results will be obtained, it is possible thinking
to carry out experiments that take advantage
also from the structure of semi-structured doc-
uments. Indeed, we are involved in the project
CDL (Esposito et al., 1998; Costabile et al.,
1999), that could profit by this kind of tech-
niques as regard semantic indexing of the stored
documents (cf. (Chanod, 1999)).
</bodyText>
<sectionHeader confidence="0.998289" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999871690476191">
J.-P. Chanod. 1999. Natural language processing
and digital libraries. In M.T. Pazienza, editor,
Information Extraction, volume 1714 of Lecture
Notes in Artificial Intelligence Tutorial, pages 17-
31. Springer.
W. Cohen. 1996. Learning to classify english text
with ilp methods. In Luc de Raedt, editor, Ad-
vances in Inductive Logic Programming, pages
124-143. IOS Press, Amsterdam, NL.
M.F. Costabile, F. Esposito, G. Semeraro, and
N. Fanizzi. 1999. An adaptive visual environment
for digital libraries. International Journal of Dig-
ital Libraries, 2:124-143.
J. Cussens, editor. 1999. Learning Language in
Logic. Workshop on Learning Language in Logic,
Workshop Notes.
F. Esposito, D. Malerba, G. Semeraro, N. Fanizzi,
and S. Ferilli. 1998. Adding machine learning
and knowledge intensive techniques to a digital
library service. International Journal of Digital
Libraries, 2(1):3-19.
F. Esposito, G. Semeraro, N. Fanizzi, and S. Ferilli.
2000. Multistrategy Theory Revision: Induction
and abduction in INTHELEX. Machine Learn-
ing, 38(1/2):133-156.
D. Freitag. 2000. Machine learning for information
extraction in informal domains. Machine Learn-
ing, 39(2/3):169-202.
M. Junker, M. Sintek, and M. Rinck. 1999. Learning
for text categorization and information extraction
with ILP. In James Cussens, editor, Proceedings
of the First International Workshop on Learning
Language in Logic - LLL99.
R. Mooney. 1999. Learning for semantic interpre-
tation: Scaling up without dumbing down. In
Cussens (Cussens, 1999), pages 7-15. Workshop
on Learning Language in Logic, Workshop Notes.
L. Saitta and F. Neri. 1997. Machine learning for
information extraction. In M.T. Pazienza, editor,
Information Extraction, volume 1299 of Lecture
Notes in Artificial Intelligence Tutorial, pages
171-191. Springer.
</reference>
<page confidence="0.997704">
198
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.565345">
<note confidence="0.970239">of CoNLL-2000 and LLL-2000, 194-198, Lisbon, Portugal, 2000.</note>
<title confidence="0.913383">Learning from Parsed Sentences with INTHELEX</title>
<author confidence="0.673755">F Esposito</author>
<author confidence="0.673755">S Ferilli</author>
<author confidence="0.673755">N Fanizzi</author>
<author confidence="0.673755">G</author>
<affiliation confidence="0.972129">Dipartimento di Universita di</affiliation>
<author confidence="0.915575">via E Orabona</author>
<email confidence="0.996734">fesposito,ferilli,fanizzi,semerarolOdi.uniba.it</email>
<abstract confidence="0.998251222222222">the context of learning, address a logical approach to information extraction. The system INTHELEX, used to carry out this task, requires a logic representation of sentences to run the learning algorithm. Hence, the need for parsers to produce structured representations from raw text. This led us to develop a prototypical Italian language parser, as a preprocessor in order to obtain the structured representation of sentences required for the symbolic learner to work. A preliminary experimentation proved that the logic approach to learning from language is able to capture the semantics underlying the kind of sentences that were processed, even if a comparison with classical methods as regards efficiency has still to be done.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J-P Chanod</author>
</authors>
<title>Natural language processing and digital libraries.</title>
<date>1999</date>
<journal>Information Extraction,</journal>
<booktitle>of Lecture Notes in Artificial Intelligence Tutorial,</booktitle>
<volume>1714</volume>
<pages>17--31</pages>
<editor>In M.T. Pazienza, editor,</editor>
<publisher>Springer.</publisher>
<marker>Chanod, 1999</marker>
<rawString>J.-P. Chanod. 1999. Natural language processing and digital libraries. In M.T. Pazienza, editor, Information Extraction, volume 1714 of Lecture Notes in Artificial Intelligence Tutorial, pages 17-31. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Cohen</author>
</authors>
<title>Learning to classify english text with ilp methods.</title>
<date>1996</date>
<booktitle>In Luc de Raedt, editor, Advances in Inductive Logic Programming,</booktitle>
<pages>124--143</pages>
<publisher>IOS Press,</publisher>
<location>Amsterdam, NL.</location>
<contexts>
<context position="2136" citStr="Cohen, 1996" startWordPosition="331" endWordPosition="332">semantic interpretation, where a logical representation of the meaning of a sentence is important and useful (Mooney, 1999). Logical approaches have been already employed in Text Categorization and/or Information Extraction. Yet they try to use an expressive representation language such as first-order logic to define simple properties about textual sources, regarded, for instance, as bags of words (Junker et al., 1999) or as semi-structured texts (Freitag, 2000). These properties are often loosely related with the grammar of the underlying language, often relying on extragrammatical features (Cohen, 1996). We intend to exploit a logic representation for exploiting the grammatical structure of texts, as it could be detected using a proper parser. Indeed, a more knowledge intensive technique is likely to perform better applied on the tasks mentioned above. When no background knowledge about the language structure is assumed to be available, one of the fundamental problems with the adoption of logic learning techniques is that a structured representation of sentences is required on which the learning algorithm can be run. Thus, the need arises for parsers that are able to discover such a structur</context>
</contexts>
<marker>Cohen, 1996</marker>
<rawString>W. Cohen. 1996. Learning to classify english text with ilp methods. In Luc de Raedt, editor, Advances in Inductive Logic Programming, pages 124-143. IOS Press, Amsterdam, NL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M F Costabile</author>
<author>F Esposito</author>
<author>G Semeraro</author>
<author>N Fanizzi</author>
</authors>
<title>An adaptive visual environment for digital libraries.</title>
<date>1999</date>
<journal>International Journal of Digital Libraries,</journal>
<pages>2--124</pages>
<marker>Costabile, Esposito, Semeraro, Fanizzi, 1999</marker>
<rawString>M.F. Costabile, F. Esposito, G. Semeraro, and N. Fanizzi. 1999. An adaptive visual environment for digital libraries. International Journal of Digital Libraries, 2:124-143.</rawString>
</citation>
<citation valid="true">
<date>1999</date>
<booktitle>Learning Language in Logic. Workshop on Learning Language in Logic, Workshop Notes.</booktitle>
<editor>J. Cussens, editor.</editor>
<marker>1999</marker>
<rawString>J. Cussens, editor. 1999. Learning Language in Logic. Workshop on Learning Language in Logic, Workshop Notes.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Esposito</author>
<author>D Malerba</author>
<author>G Semeraro</author>
<author>N Fanizzi</author>
<author>S Ferilli</author>
</authors>
<title>Adding machine learning and knowledge intensive techniques to a digital library service.</title>
<date>1998</date>
<journal>International Journal of Digital Libraries,</journal>
<pages>2--1</pages>
<marker>Esposito, Malerba, Semeraro, Fanizzi, Ferilli, 1998</marker>
<rawString>F. Esposito, D. Malerba, G. Semeraro, N. Fanizzi, and S. Ferilli. 1998. Adding machine learning and knowledge intensive techniques to a digital library service. International Journal of Digital Libraries, 2(1):3-19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Esposito</author>
<author>G Semeraro</author>
<author>N Fanizzi</author>
<author>S Ferilli</author>
</authors>
<title>Multistrategy Theory Revision: Induction and abduction in INTHELEX.</title>
<date>2000</date>
<booktitle>Machine Learning,</booktitle>
<pages>38--1</pages>
<contexts>
<context position="4317" citStr="Esposito et al., 2000" startWordPosition="689" endWordPosition="692"> these methods, as a draw194 back of the expressive power gained through relations. Moreover, another weakness of our approach could be the dependence on the quality of the data coming from the preprocessing step: it is possible that noise coming from wrongly parsed sentences be present, thus having a negative influence towards the model to be induced. After briefly presenting in Section 2 the parser performance, in order to establish the degree of reliability of the data on which the learning step is performed, Section 3 shows the results of applying the first-order learning system INTHELEX (Esposito et al., 2000) for the inference of some simple events related to foreign commerce. Lastly, Section 4 draws some preliminary conclusions on this research and outlines future work issues to be addressed. 2 A Stratified Parser for Italian Language This section presents a parser for the Italian language, based on context-free grammars and designed to manage texts having a simple and standard phrase structure (e.g., foreign commerce texts as opposed to poetry texts). It is composed by 12 parsing levels and 106 production rules, and uses the longest-match technique, which complies with the typical ambiguity of I</context>
<context position="6912" citStr="Esposito et al., 2000" startWordPosition="1106" endWordPosition="1109">entences 26.384% 26.384% Table 2: Summary of parser validation results (Errorl/Error2) respect to precision, recall (reported in Table 1) and two measures about error ratio: Errorl = # errors I# total constituents extracted Error2 = # errors I# total correct constituents expected (see Table 2). 3 Information extraction The grammar above was used to parse Italian texts downloaded from the Internet, and concerning foreign commerce. Through such preprocessing, the aim was to obtain some structure for those texts that could then be translated in the input language of the learning system INTHELEX (Esposito et al., 2000) in order to make it learn simple events concerning that domain. INTHELEX (INcremental THEory Learner from EXamples) is a fully incremental, multiconceptual closed loop learning system for the induction of hierarchical theories from examples. In detail, full incrementality avoids the need of a previously generated version of the theory to be available, so that learning can start from an empty theory and from the first exam195 pie; multi-conceptual means that it can learn simultaneously various concepts, possibly related to each other; a closed loop system is a system in which the learned theor</context>
</contexts>
<marker>Esposito, Semeraro, Fanizzi, Ferilli, 2000</marker>
<rawString>F. Esposito, G. Semeraro, N. Fanizzi, and S. Ferilli. 2000. Multistrategy Theory Revision: Induction and abduction in INTHELEX. Machine Learning, 38(1/2):133-156.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Freitag</author>
</authors>
<title>Machine learning for information extraction in informal domains.</title>
<date>2000</date>
<booktitle>Machine Learning,</booktitle>
<pages>39--2</pages>
<contexts>
<context position="1990" citStr="Freitag, 2000" startWordPosition="309" endWordPosition="310">ions make them highly suitable for natural language analysis (Cussens, 1999). Indeed, logical approaches may have a relevant impact at the level of semantic interpretation, where a logical representation of the meaning of a sentence is important and useful (Mooney, 1999). Logical approaches have been already employed in Text Categorization and/or Information Extraction. Yet they try to use an expressive representation language such as first-order logic to define simple properties about textual sources, regarded, for instance, as bags of words (Junker et al., 1999) or as semi-structured texts (Freitag, 2000). These properties are often loosely related with the grammar of the underlying language, often relying on extragrammatical features (Cohen, 1996). We intend to exploit a logic representation for exploiting the grammatical structure of texts, as it could be detected using a proper parser. Indeed, a more knowledge intensive technique is likely to perform better applied on the tasks mentioned above. When no background knowledge about the language structure is assumed to be available, one of the fundamental problems with the adoption of logic learning techniques is that a structured representatio</context>
</contexts>
<marker>Freitag, 2000</marker>
<rawString>D. Freitag. 2000. Machine learning for information extraction in informal domains. Machine Learning, 39(2/3):169-202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Junker</author>
<author>M Sintek</author>
<author>M Rinck</author>
</authors>
<title>Learning for text categorization and information extraction with ILP.</title>
<date>1999</date>
<booktitle>Proceedings of the First International Workshop on Learning Language in Logic - LLL99.</booktitle>
<editor>In James Cussens, editor,</editor>
<contexts>
<context position="1946" citStr="Junker et al., 1999" startWordPosition="301" endWordPosition="304">lexibility and expressivity of logical representations make them highly suitable for natural language analysis (Cussens, 1999). Indeed, logical approaches may have a relevant impact at the level of semantic interpretation, where a logical representation of the meaning of a sentence is important and useful (Mooney, 1999). Logical approaches have been already employed in Text Categorization and/or Information Extraction. Yet they try to use an expressive representation language such as first-order logic to define simple properties about textual sources, regarded, for instance, as bags of words (Junker et al., 1999) or as semi-structured texts (Freitag, 2000). These properties are often loosely related with the grammar of the underlying language, often relying on extragrammatical features (Cohen, 1996). We intend to exploit a logic representation for exploiting the grammatical structure of texts, as it could be detected using a proper parser. Indeed, a more knowledge intensive technique is likely to perform better applied on the tasks mentioned above. When no background knowledge about the language structure is assumed to be available, one of the fundamental problems with the adoption of logic learning t</context>
</contexts>
<marker>Junker, Sintek, Rinck, 1999</marker>
<rawString>M. Junker, M. Sintek, and M. Rinck. 1999. Learning for text categorization and information extraction with ILP. In James Cussens, editor, Proceedings of the First International Workshop on Learning Language in Logic - LLL99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mooney</author>
</authors>
<title>Learning for semantic interpretation: Scaling up without dumbing down. In Cussens (Cussens,</title>
<date>1999</date>
<booktitle>Workshop on Learning Language in Logic, Workshop Notes.</booktitle>
<pages>7--15</pages>
<contexts>
<context position="1647" citStr="Mooney, 1999" startWordPosition="256" endWordPosition="257">ds as regards efficiency has still to be done. 1 Introduction Language learning has gained growing attention in the last years. Statistical approaches, so far extensively used — see (Saitta and Neri, 1997) for an overview of the research in this area —, have severe limitations, whereas the flexibility and expressivity of logical representations make them highly suitable for natural language analysis (Cussens, 1999). Indeed, logical approaches may have a relevant impact at the level of semantic interpretation, where a logical representation of the meaning of a sentence is important and useful (Mooney, 1999). Logical approaches have been already employed in Text Categorization and/or Information Extraction. Yet they try to use an expressive representation language such as first-order logic to define simple properties about textual sources, regarded, for instance, as bags of words (Junker et al., 1999) or as semi-structured texts (Freitag, 2000). These properties are often loosely related with the grammar of the underlying language, often relying on extragrammatical features (Cohen, 1996). We intend to exploit a logic representation for exploiting the grammatical structure of texts, as it could be</context>
</contexts>
<marker>Mooney, 1999</marker>
<rawString>R. Mooney. 1999. Learning for semantic interpretation: Scaling up without dumbing down. In Cussens (Cussens, 1999), pages 7-15. Workshop on Learning Language in Logic, Workshop Notes.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Saitta</author>
<author>F Neri</author>
</authors>
<title>Machine learning for information extraction.</title>
<date>1997</date>
<journal>Information Extraction,</journal>
<booktitle>of Lecture Notes in Artificial Intelligence Tutorial,</booktitle>
<volume>1299</volume>
<pages>171--191</pages>
<editor>In M.T. Pazienza, editor,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="1239" citStr="Saitta and Neri, 1997" startWordPosition="190" endWordPosition="193"> from raw text. This led us to develop a prototypical Italian language parser, as a preprocessor in order to obtain the structured representation of sentences required for the symbolic learner to work. A preliminary experimentation proved that the logic approach to learning from language is able to capture the semantics underlying the kind of sentences that were processed, even if a comparison with classical methods as regards efficiency has still to be done. 1 Introduction Language learning has gained growing attention in the last years. Statistical approaches, so far extensively used — see (Saitta and Neri, 1997) for an overview of the research in this area —, have severe limitations, whereas the flexibility and expressivity of logical representations make them highly suitable for natural language analysis (Cussens, 1999). Indeed, logical approaches may have a relevant impact at the level of semantic interpretation, where a logical representation of the meaning of a sentence is important and useful (Mooney, 1999). Logical approaches have been already employed in Text Categorization and/or Information Extraction. Yet they try to use an expressive representation language such as first-order logic to def</context>
</contexts>
<marker>Saitta, Neri, 1997</marker>
<rawString>L. Saitta and F. Neri. 1997. Machine learning for information extraction. In M.T. Pazienza, editor, Information Extraction, volume 1299 of Lecture Notes in Artificial Intelligence Tutorial, pages 171-191. Springer.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>