<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.041815">
<title confidence="0.99193">
A Corpus of Sentence-level Revisions in Academic Writing:
A Step towards Understanding Statement Strength in Communication
</title>
<author confidence="0.99482">
Chenhao Tan
</author>
<affiliation confidence="0.9943555">
Dept. of Computer Science
Cornell University
</affiliation>
<email confidence="0.99814">
chenhao@cs.cornell.edu
</email>
<sectionHeader confidence="0.99388" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999938526315789">
The strength with which a statement is
made can have a significant impact on the
audience. For example, international rela-
tions can be strained by how the media in
one country describes an event in another;
and papers can be rejected because they
overstate or understate their findings. It is
thus important to understand the effects of
statement strength. A first step is to be able
to distinguish between strong and weak
statements. However, even this problem
is understudied, partly due to a lack of
data. Since strength is inherently relative,
revisions of texts that make claims are a
natural source of data on strength differ-
ences. In this paper, we introduce a corpus
of sentence-level revisions from academic
writing. We also describe insights gained
from our annotation efforts for this task.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999035">
It is important for authors and speakers to find the
appropriate “pitch” to convey a desired message
to the public. Indeed, sometimes heated debates
can arise around the choice of statement strength.
For instance, on March 1, 2014, an attack at Kun-
ming’s railway station left 29 people dead and
more than 140 others injured.1 In the aftermath,
Chinese media accused Western media of “soft-
pedaling the attack and failing to state clearly that
it was an act of terrorism”.2 In particular, regard-
ing the statement by the US embassy that referred
to this incident as the “terrible and senseless act
of violence in Kunming”, a Weibo user posted “If
you say that the Kunming attack is a ‘terrible and
</bodyText>
<footnote confidence="0.9421156">
1http://en.wikipedia.org/wiki/2014_
Kunming_attack
2http://sinosphere.blogs.nytimes.
com/2014/03/03/u-n-security-council-
condemns-terrorist-attack-in-kunming/
</footnote>
<author confidence="0.871291">
Lillian Lee
</author>
<affiliation confidence="0.96278">
Dept. of Computer Science
Cornell University
</affiliation>
<email confidence="0.989877">
llee@cs.cornell.edu
</email>
<bodyText confidence="0.999613710526316">
senseless act of violence’, then the 9/11 attack can
be called a ‘regrettable traffic incident”’.3
This example is striking but not an isolated case,
for settings in which one party is trying to con-
vince another are pervasive; scenarios range from
court trials to conference submissions. Since the
strength and scope of an argument can be a cru-
cial factor in its success, it is important to under-
stand the effects of statement strength in commu-
nication.
A first step towards addressing this question is
to be able to distinguish between strong and weak
statements. As strength is inherently relative, it is
natural to look at revisions that change statement
strength, which we refer to as “strength changes”.
Though careful and repeated revisions are presum-
ably ubiquitous in politics, legal systems, and jour-
nalism, it is not clear how to collect them; on the
other hand, revisions to research papers may be
more accessible, and many researchers spend sig-
nificant time on editing to convey the right mes-
sage regarding the strength of a project’s contribu-
tions, novelty, and limitations. Indeed, statement
strength in science communication matters to writ-
ers: understating contributions can affect whether
people recognize the true importance of the work;
at the same time, overclaiming can cause papers to
be rejected.
With the increasing popularity of e-print ser-
vices such as the arXiv4, strength changes in scien-
tific papers are becoming more readily available.
Since the arXiv started in 1991, it has become
“the standard repository for new papers in mathe-
matics, physics, statistics, computer science, biol-
ogy, and other disciplines” (Krantz, 2007). An in-
triguing observation is that many researchers sub-
mit multiple versions of the same paper on arXiv.
For instance, among the 70K papers submitted in
</bodyText>
<footnote confidence="0.99997">
3http://www.huffingtonpost.co.uk/2014/
03/03/china-kunming-911_n_4888748.html
4http://arxiv.org/
</footnote>
<page confidence="0.98378">
403
</page>
<bodyText confidence="0.718128">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 403–408,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</bodyText>
<listItem confidence="0.965251909090909">
ID Pairs
1 S1: The algorithm is studied in this paper.
S2: The algorithm is proposed in this paper .
2 S1: ... circadian pattern and burstiness in human communication activity .
S2:... circadian pattern and burstiness in mobile phone communication .
3 S1: ... using minhash techniques , at a significantly lower cost and with same privacy guarantees .
S2:... using minhash techniques, with lower costs.
4 S1: the rows and columns of the covariate matrix then have certain physical meanings ...
S2: the rows and columns of the covariate matrix could have different meanings ...
5 S1: they maximize the expected revenue of the seller but induce efficiency loss .
S2: they maximize the expected revenue of the seller but are inefficient.
</listItem>
<tableCaption confidence="0.997068">
Table 1: Examples of potential strength differences.
</tableCaption>
<bodyText confidence="0.999963818181818">
2011, almost 40% (27.7K) have multiple versions.
Many differences between these versions consti-
tute a source of valid and motivated strength dif-
ferences, as can be seen from the sentential revi-
sions in Table 1. Pair 1 makes the contribution
seem more impressive by replacing “studied” with
“proposed”. Pair 2 downgrades “human commu-
nication activity” to “mobile phone communica-
tion”. Pair 3 removes “significantly” and the em-
phasis on “same privacy guarantees”. Pair 4 shows
an insertion of hedging, a relatively well-known
type of strength reduction. Pair 5 is an interesting
case that shows the complexity of this problem: on
the one hand, S2 claims that something is “ineffi-
cient”, which is an absolute statement, compared
to “efficiency loss” in S1, where the possibility of
efficiency still exists; on the other hand, S1 em-
ploys an active tone that emphasizes a causal rela-
tionship.
The main contribution of this work is to provide
the first large-scale corpus of sentence-level revi-
sions for studying a broad range of variations in
statement strength. We collected labels for a sub-
set of these revisions. Given the possibility of all
kinds of disagreement, the fair level of agreement
(Fleiss’ Kappa) among our annotators was decent.
But in some cases, the labels differed from our ex-
pectations, indicating that the general public can
interpret the strength of scientific statements dif-
ferently from researchers. The participants’ com-
ments may further shed light on science commu-
nication and point to better ways to define and un-
derstand strength differences.
</bodyText>
<sectionHeader confidence="0.99842" genericHeader="related work">
2 Related Work and Data
</sectionHeader>
<bodyText confidence="0.999914235294118">
Hedging, which can lead to strength differences,
has received some attention in the study of science
communication (Salager-Meyer, 2011; Lewin,
1998; Hyland, 1998; Myers, 1990). The CoNLL
2010 Shared Task was devoted to hedge detection
(Farkas et al., 2010). Hedge detection was also
used to understand scientific framing in debates
over genetically-modified organisms in food (Choi
et al., 2012).
Revisions on Wikipedia have been shown use-
ful for various applications, including spelling
correction (Zesch, 2012), sentence compression
(Yamangil and Nelken, 2008), text simplification
(Yatskar et al., 2010), paraphrasing (Max and Wis-
niewski, 2010), and textual entailment (Zanzotto
and Pennacchiotti, 2010). But none of the cat-
egories of Wikipedia revisions previously exam-
ined (Daxenberger and Gurevych, 2013; Bronner
and Monz, 2012; Mola-Velasco, 2011; Potthast et
al., 2008; Daxenberger and Gurevych, 2012) re-
late to statement strength. After all, the objective
of editing on Wikipedia is to present neutral and
objective articles.
Public datasets of science communication are
available, such as the ACL Anthology,5 collec-
tions of NIPS papers,6 and so on. These datasets
are useful for understanding the progress of disci-
plines or the evolution of topics. But the lack of
edit histories or revisions makes them not imme-
diately suitable for studying strength differences.
Recently, there have been experiments with open
peer review.7 Records from open reviewing can
provide additional insights into the revision pro-
cess once enough data is collected.
</bodyText>
<footnote confidence="0.999961666666667">
5http://aclweb.org/anthology/
6http://nips.djvuzone.org/txt.html
7http://openreview.net
</footnote>
<page confidence="0.997266">
404
</page>
<figure confidence="0.985892111111111">
900000.0
800000.0
500000.0
400000.0
300000.0
200000.0
700000.0
600000.0
100000.0
0.0
deletion
typo
rewrite
57% 71%
65%
58%
62%
500000.0
400000.0
300000.0
200000.0
600000.0
100000.0
0.0
57%
61%
67% 56%
deletion
typo
rewrite
59%
number of changes per sentence
0.5
0.4
0.3
0.2
0.1
0.0
54%
58%
58%
deletion
typo
rewrite
56%
59%
(a) Number of changes vs sections.
“middle” refers to the sections be-
tween introduction and conclusion.
(b) Top 5 categories in number of
changes.
(c) Top 5 categories in number of
changes over the number of sen-
tences.
</figure>
<figureCaption confidence="0.999861">
Figure 1: In all figures, different colors indicate different types of changes.
</figureCaption>
<sectionHeader confidence="0.995848" genericHeader="method">
3 Dataset Description
</sectionHeader>
<bodyText confidence="0.998212">
Our main dataset was constructed from all papers
submitted in 2011 on the arXiv. We first extracted
the textual content from papers that have multiple
versions of tex source files. All mathematical en-
vironments were ignored. Section titles were not
included in the final texts but are used in align-
ment.
In order to align the first version and the fi-
nal version of the same paper, we first did macro
alignment of paper sections based on section titles.
Then, for micro alignment of sentences, we em-
ployed a dynamic programming algorithm similar
to that of Barzilay and Elhadad (2003). Instead of
cosine similarity, we used an idf-weighted longest-
common-subsequence algorithm to define the sim-
ilarity between two sentences, because changes in
word ordering can also be interesting. Formally,
the similarity score between sentence i and sen-
tence j is defined as
max(řwPS, idf(w), řwPSj idf(w)),
where Si and Sj refer to sentence i and sentence j.
Since it is likely that a new version adds or deletes
a large sequence of sentences, we did not impose a
skip penalty. We set the mismatch penalty to 0.1.8
In the end, there are 23K papers where the first
version was different from the last version.9 We
8We did not allow cross matching (i.e., i --* j—1, i—1 --*
j), since we thought matching this case as (i — 1, i) --* j or
i --* (j, j — 1) can provide context for annotation purposes.
But in the end, we focused on labeling very similar pairs.
This decision had little effect.
</bodyText>
<listItem confidence="0.994549153846154">
9 This differs from the number in Section 1 because arti-
cles may not have the tex source available, or the differences
between versions may be in non-textual content.
categorize sentential revisions into the following
three types:
• Deletion: we cannot find a match in the final
version.
• Typo: all sequences in a pair of matched sen-
tences are typos, where a sequence-level typo
is one where the edit distance between the
matched sequences is less than three.
• Rewrite: matched sentences that are not ty-
pos. This type is the focus of this study.
</listItem>
<bodyText confidence="0.989846666666666">
What kinds of changes are being made? One
might initially think that typo fixes represent a
large proportion of revisions, but this is not cor-
rect, as shown in Figure 1a. Deletions represent a
substantial fraction, especially in the middle sec-
tion of a paper. But it is clear that the majority of
changes are rewrites; thus revisions on the arXiv
indeed provide a great source for potential strength
differences.
Who makes changes? Figure 1b shows that the
Math subarchive makes the largest number of
changes. This is consistent with the mathematics
community’s custom of using the arXiv to get find-
ings out early. In terms of changes per sentence
(Figure 1c), statistics and quantitative studies are
the top subareas.
Further, Figure 2 shows the effect of the number
of authors. It is interesting that both in terms of
sheer number and percentage, single-authored pa-
pers have the most changes. This could be because
a single author enjoys greater freedom and has
stronger motivation to make changes, or because
multiple authors tend to submit a more polished
initial version. This echoes the finding in Posner
</bodyText>
<equation confidence="0.938415">
Sim(i, j) _
Weighted-LCS(Si, Sj)
</equation>
<page confidence="0.983747">
405
</page>
<bodyText confidence="0.859758">
You should mark S2 as Stronger if
</bodyText>
<listItem confidence="0.9626288">
• (R1) S2 strengthens the degree of some aspect of S1, for example, S1 has the word ”better”,
whereas S2 uses ”best”, or S2 removes the word ”possibly”
• (R2) S2 adds more evidence or justification (we don’t count adding details)
• (R3) S2 sounds more impressive in some other way: the authors’ work is more important/novel-
/elegant/applicable/etc.
</listItem>
<bodyText confidence="0.99633525">
If instead S1 is stronger than S2 according to the reasons above, select Weaker. If the changes
aren’t strengthenings or weakenings according to the reason above, select No Strength Change.
If there are both strengthenings and weakenings, or you find that it is really hard to tell whether the
change is stronger or weaker, then select I can’t tell.
</bodyText>
<tableCaption confidence="0.985462">
Table 2: Definition of labels in our labeling tasks.
</tableCaption>
<figure confidence="0.989867333333333">
(a) Number of changes vs (b) Percentage of changed
number of authors. sentences vs number of au-
thors.
</figure>
<figureCaption confidence="0.87899525">
Figure 2: Error bars represent standard error. (a):
up until 5 authors, a larger number of authors in-
dicates a smaller number of changes. (b): per-
centage is measured over the number of sentences
</figureCaption>
<bodyText confidence="0.99329225">
in the first version; there is an interior minimum
where 2 or 3 authors make the smallest percentage
of sentence changes on a paper.
and Baecker (1992) that the collaborative writing
process differs considerably from individual writ-
ing. Also, more than 25% of the first versions are
changed, which again shows that substantive edits
are being made in these resubmissions.
</bodyText>
<sectionHeader confidence="0.927051" genericHeader="method">
4 Annotating Strength Differences
</sectionHeader>
<bodyText confidence="0.99866988">
In order to study statement strength, reliable
strength-difference labels are needed. In this sec-
tion, we describe how we tried to define strength
differences, compiled labeling instructions, and
gathered labels using Amazon Mechanical Turk.
Label definition and collection procedure. We
focused on matched sentences from abstracts
and introductions to maximize the proportion of
strength differences (as opposed to factual/no
strength changes). We required pairs to have sim-
ilarity score larger than 0.5 in our labeling task to
make pairs more comparable. We also replaced
all math environments with “[MATH]”.10 We ob-
tained 108K pairs that satisfy the above condi-
tions, available at http://chenhaot.com/
pages/statement-strength.html. To
create the pool of pairs for labeling, we randomly
sampled 1000 pairs and then removed pairs that
we thought were processing errors.
We used Amazon Mechanical Turk. It may
initially seem surprising to have annotations of
technical statements not done by domain experts;
we did this intentionally because it is common to
communicate unfamiliar topics to the public in po-
litical and science communication (we comment
on non-expert rationales later). We use the follow-
ing set of labels: Stronger, Weaker, No Strength
Change, I can’t tell. Table 2 gives our definitions.
The instructions included 8 pairs as examples and
10 pairs to label as a training exercise. Partici-
pants were then asked to choose labels and write
mandatory comments for 50 pairs. According to
the comments written by participants, we believe
that they did the labeling in good faith.
Quantitative overview. We collected 9 labels
each for 500 pairs. Among the 500 pairs, Fleiss’
Kappa was 0.242, which indicates fair agreement
(Landis and Koch, 1977). We took a conserva-
tive approach and only considered pairs with an
absolute majority label, i.e., at least 5 of 9 label-
ers chose the same label. There are 386 pairs that
satisfy this requirement (93 weaker, 194 stronger,
99 no change). On this subset of pairs, Fleiss’
Kappa is 0.322, and 74.4% of pairs were strength
changes. Considering all the possible disagree-
ment, this result was acceptable.
Qualitative observations. We were excited
about the labels from these participants: despite
10These decisions were made based on the results and feed-
back that we got from graduate students in an initial labeling.
</bodyText>
<figure confidence="0.996023666666667">
number of changes 64.0 percentage of changes 30%
62.0 29%
60.0 28%
58.0 27%
56.0 26%
54.0
52.0
50.0
48.0
46.0
1 2 3 4 5 5 1 2 3 4 5 5
number of authors number of authors
</figure>
<page confidence="0.992617">
406
</page>
<table confidence="0.996858705882353">
ID Matched sentences and comments
1 S1: ... using data from numerics and experiments .
S2:... using data sets from numerics in the point particle limit and one experimental data set .
(stronger) S2 is more specific in its description which seems stronger.
(weaker) ”one experimental data set” weakens the sentence
2 S1: we also proved that if [MATH] is sufficiently homogeneous then ...
S2: we also proved that if [MATH] is not totally disconnected and sufficiently homogeneous then ...
(stronger) We have more detail/proof in S2
(stronger) the words ”not totally disconnected” made the sentence sound more impressive.
3 S1: we also show in general that vectors of products of jack vertex operators form a basis of symmetric functions .
S2: we also show in general that the images of products of jack vertex operators form a basis of symmetric functions .
(weaker) Vectors sounds more impressive than images
(weaker) sentence one is more specific
4 S1: in the current paper we discover several variants of qd algorithms for quasiseparable matrices .
S2: in the current paper we adapt several variants of qd algorithms to quasiseparable matrices .
(stronger) in S2 Adapt is stronger than just the word discover. adapt implies more of a proactive measure.
(stronger) s2 sounds as if they’re doing something with specifics already, rather than hunting for a way to do it
</table>
<tableCaption confidence="0.999861">
Table 3: Representative examples of surprising labels, together with selected labeler comments.
</tableCaption>
<bodyText confidence="0.999980179487179">
the apparent difficulty of the task, we found that
many labels for the 386 pairs were reasonable.
However, in some cases, the labels were counter-
intuitive. Table 3 shows some representative ex-
amples.
First, participants tend to take details as evi-
dence even when these details are not germane to
the statement. For pair 1, while one turker pointed
out the decline in number of experiments, most
turkers simply labeled it as stronger because it was
more specific. “Specific” turned out to be a com-
mon reason used in the comments, even though we
said in the instructions that only additional justifi-
cation and evidence matter. This echoes the find-
ing in Bell and Loftus (1989) that even unrelated
details influenced judgments of guilt.
Second, participants interpret constraints/condi-
tions not in strictly logical ways, seeming to care
little about scope at times. For instance, the ma-
jority labeled pair 2 as “stronger”. But in S2 for
that pair, the result holds for strictly fewer pos-
sible worlds. But it should be said that there
are cases that labelers interpreted logically, e.g.,
“compelling evidence” subsumes “compelling ex-
perimental evidence”.
Both of the above cases share the property that
they seem to be correlated with a tendency to
judge lengthier statements as stronger. Another
interesting case that does not share this character-
istic is that participants can have a different un-
derstanding of domain-specific terms. For pair 3,
the majority thought that “vectors” sounds more
impressive than “images”; for pair 4, the major-
ity considered “adapt” stronger than “discover”.
This issue is common when communicating new
topics to the public not only in science commu-
nication but also in politics and other scenarios. It
may partly explain miscommunications and misin-
terpretations of scientific studies in journalism.11
</bodyText>
<sectionHeader confidence="0.979282" genericHeader="method">
5 Looking ahead
</sectionHeader>
<bodyText confidence="0.99994">
Our observations regarding the annotation results
raise questions regarding what is a generalizable
way to define strength differences, how to use the
labels that we collected, and how to collect la-
bels in the future. We believe that this corpus of
sentence-level revisions, together with the labels
and comments from participants, can provide in-
sights into better ways to approach this problem
and help further understand strength of statements.
One interesting direction that this enables is a
potentially new kind of learning problem. The
comments indicate features that humans think
salient. Is it possible to automatically learn new
features from the comments?
The ultimate goal of our study is to understand
the effects of statement strength on the public,
which can lead to various applications in public
communication.
</bodyText>
<sectionHeader confidence="0.997486" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999785777777778">
We thank J. Baldridge, J. Boyd-Graber, C.
Callison-Burch, and the reviewers for helpful
comments; P. Ginsparg for providing data; and S.
Chen, E. Kozyri, M. Lee, I. Lenz, M. Ott, J. Park,
K. Raman, M. Reitblatt, S. Roy, A. Sharma, R.
Sipos, A. Swaminathan, L. Wang, W. Xie, B. Yang
and the anonymous annotators for all their label-
ing help. This work was supported in part by NSF
grant IIS-0910664 and a Google Research Grant.
</bodyText>
<footnote confidence="0.934648">
11http://www.phdcomics.com/comics/
archive.php?comicid=1174
</footnote>
<page confidence="0.996375">
407
</page>
<sectionHeader confidence="0.982976" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.962528659574468">
Regina Barzilay and Noemie Elhadad. 2003. Sentence
alignment for monolingual comparable corpora. In
Proceedings of the 2003 conference on Empirical
methods in natural language processing, pages 25–
32.
Brad E Bell and Elizabeth F Loftus. 1989. Trivial per-
suasion in the courtroom: The power of (a few) mi-
nor details. Journal of Personality and Social Psy-
chology, 56(5):669.
Amit Bronner and Christof Monz. 2012. User Edits
Classification Using Document Revision Histories.
In Proceedings of the 13th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, pages 356–366.
Eunsol Choi, Chenhao Tan, Lillian Lee, Cristian
Danescu-Niculescu-Mizil, and Jennifer Spindel.
2012. Hedge detection as a lens on framing in the
GMO debates: A position paper. In Proceedings
of the Workshop on Extra-Propositional Aspects of
Meaning in Computational Linguistics, pages 70–
79.
Johannes Daxenberger and Iryna Gurevych. 2012. A
Corpus-Based Study of Edit Categories in Featured
and Non-Featured Wikipedia Articles. In COLING,
pages 711–726.
Johannes Daxenberger and Iryna Gurevych. 2013.
Automatically Classifying Edit Categories in
Wikipedia Revisions. In Proceedings of the 2013
Conference on Empirical Methods in Natural
Language Processing.
Rich´ard Farkas, Veronika Vincze, Gy¨orgy M´ora, J´anos
Csirik, and Gy¨orgy Szarvas. 2010. The CoNLL-
2010 shared task: Learning to detect hedges and
their scope in natural language text. In CoNLL—
Shared Task, pages 1–12.
Ken Hyland. 1998. Hedging in scientific research
articles. John Benjamins Pub. Co., Amsterdam;
Philadelphia.
Steven G. Krantz. 2007. How to Write Your First Pa-
per. Notices of the AMS.
J. Richard Landis and Gary G. Koch. 1977. The
Measurement of Observer Agreement for Categor-
ical Data. Biometrics, 33(1):159–174.
Beverly A. Lewin. 1998. Hedging: Form and func-
tion in scientific research texts. In Genre Studies
in English for Academic Purposes, volume 9, pages
89–108. Universitat Jaume I.
Aurlien Max and Guillaume Wisniewski. 2010.
Mining Naturally-occurring Corrections and Para-
phrases from Wikipedia’s Revision History. In Pro-
ceedings of The seventh international conference on
Language Resources and Evaluation.
Santiago M Mola-Velasco. 2011. Wikipedia Vandal-
ism Detection. In Proceedings of the 20th Interna-
tional Conference Companion on World Wide Web,
pages 391–396.
Greg Myers. 1990. Writing biology: Texts in the social
construction of scientific knowledge. University of
Wisconsin Press, Madison, Wis.
Ilona R Posner and Ronald M Baecker. 1992. How
people write together [groupware]. In System
Sciences, 1992. Proceedings of the Twenty-Fifth
Hawaii International Conference on, pages 127–
138.
Martin Potthast, Benno Stein, and Robert Ger-
ling. 2008. Automatic Vandalism Detection in
Wikipedia. In Advances in Information Retrieval,
pages 663–668. Springer Berlin Heidelberg.
Franc¸oise Salager-Meyer. 2011. Scientific discourse
and contrastive linguistics: hedging. European Sci-
ence Editing, 37(2):35–37.
Elif Yamangil and Rani Nelken. 2008. Mining
Wikipedia Revision Histories for Improving Sen-
tence Compression. In Proceedings of the 46th
Annual Meeting of the Association for Computa-
tional Linguistics on Human Language Technolo-
gies: Short Papers, pages 137–140.
Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-
Mizil, and Lillian Lee. 2010. For the sake of sim-
plicity: Unsupervised extraction of lexical simplifi-
cations from Wikipedia. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 365–368.
Fabio Massimo Zanzotto and Marco Pennacchiotti.
2010. Expanding textual entailment corpora from
Wikipedia using co-training. In Proceedings of the
2nd Workshop on Collaboratively Constructed Se-
mantic Resources.
Torsten Zesch. 2012. Measuring Contextual Fitness
Using Error Contexts Extracted from the Wikipedia
Revision History. In Proceedings of the 13th Con-
ference of the European Chapter of the Association
for Computational Linguistics, pages 529–538.
</reference>
<page confidence="0.997835">
408
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.685155">
<title confidence="0.999492">A Corpus of Sentence-level Revisions in Academic Writing: A Step towards Understanding Statement Strength in Communication</title>
<author confidence="0.837082">Chenhao</author>
<affiliation confidence="0.9063565">Dept. of Computer Cornell</affiliation>
<email confidence="0.998106">chenhao@cs.cornell.edu</email>
<abstract confidence="0.996929">The strength with which a statement is made can have a significant impact on the audience. For example, international relations can be strained by how the media in one country describes an event in another; and papers can be rejected because they overstate or understate their findings. It is thus important to understand the effects of statement strength. A first step is to be able to distinguish between strong and weak statements. However, even this problem is understudied, partly due to a lack of data. Since strength is inherently relative, texts that make claims are a natural source of data on strength differences. In this paper, we introduce a corpus of sentence-level revisions from academic writing. We also describe insights gained from our annotation efforts for this task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Noemie Elhadad</author>
</authors>
<title>Sentence alignment for monolingual comparable corpora.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 conference on Empirical methods in natural language processing,</booktitle>
<pages>25--32</pages>
<contexts>
<context position="9331" citStr="Barzilay and Elhadad (2003)" startWordPosition="1441" endWordPosition="1444">ent types of changes. 3 Dataset Description Our main dataset was constructed from all papers submitted in 2011 on the arXiv. We first extracted the textual content from papers that have multiple versions of tex source files. All mathematical environments were ignored. Section titles were not included in the final texts but are used in alignment. In order to align the first version and the final version of the same paper, we first did macro alignment of paper sections based on section titles. Then, for micro alignment of sentences, we employed a dynamic programming algorithm similar to that of Barzilay and Elhadad (2003). Instead of cosine similarity, we used an idf-weighted longestcommon-subsequence algorithm to define the similarity between two sentences, because changes in word ordering can also be interesting. Formally, the similarity score between sentence i and sentence j is defined as max(řwPS, idf(w), řwPSj idf(w)), where Si and Sj refer to sentence i and sentence j. Since it is likely that a new version adds or deletes a large sequence of sentences, we did not impose a skip penalty. We set the mismatch penalty to 0.1.8 In the end, there are 23K papers where the first version was different from the la</context>
</contexts>
<marker>Barzilay, Elhadad, 2003</marker>
<rawString>Regina Barzilay and Noemie Elhadad. 2003. Sentence alignment for monolingual comparable corpora. In Proceedings of the 2003 conference on Empirical methods in natural language processing, pages 25– 32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brad E Bell</author>
<author>Elizabeth F Loftus</author>
</authors>
<title>Trivial persuasion in the courtroom: The power of (a few) minor details.</title>
<date>1989</date>
<journal>Journal of Personality and Social Psychology,</journal>
<volume>56</volume>
<issue>5</issue>
<contexts>
<context position="18087" citStr="Bell and Loftus (1989)" startWordPosition="2903" endWordPosition="2906">any labels for the 386 pairs were reasonable. However, in some cases, the labels were counterintuitive. Table 3 shows some representative examples. First, participants tend to take details as evidence even when these details are not germane to the statement. For pair 1, while one turker pointed out the decline in number of experiments, most turkers simply labeled it as stronger because it was more specific. “Specific” turned out to be a common reason used in the comments, even though we said in the instructions that only additional justification and evidence matter. This echoes the finding in Bell and Loftus (1989) that even unrelated details influenced judgments of guilt. Second, participants interpret constraints/conditions not in strictly logical ways, seeming to care little about scope at times. For instance, the majority labeled pair 2 as “stronger”. But in S2 for that pair, the result holds for strictly fewer possible worlds. But it should be said that there are cases that labelers interpreted logically, e.g., “compelling evidence” subsumes “compelling experimental evidence”. Both of the above cases share the property that they seem to be correlated with a tendency to judge lengthier statements as</context>
</contexts>
<marker>Bell, Loftus, 1989</marker>
<rawString>Brad E Bell and Elizabeth F Loftus. 1989. Trivial persuasion in the courtroom: The power of (a few) minor details. Journal of Personality and Social Psychology, 56(5):669.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Bronner</author>
<author>Christof Monz</author>
</authors>
<title>User Edits Classification Using Document Revision Histories.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>356--366</pages>
<contexts>
<context position="7291" citStr="Bronner and Monz, 2012" startWordPosition="1121" endWordPosition="1124">oted to hedge detection (Farkas et al., 2010). Hedge detection was also used to understand scientific framing in debates over genetically-modified organisms in food (Choi et al., 2012). Revisions on Wikipedia have been shown useful for various applications, including spelling correction (Zesch, 2012), sentence compression (Yamangil and Nelken, 2008), text simplification (Yatskar et al., 2010), paraphrasing (Max and Wisniewski, 2010), and textual entailment (Zanzotto and Pennacchiotti, 2010). But none of the categories of Wikipedia revisions previously examined (Daxenberger and Gurevych, 2013; Bronner and Monz, 2012; Mola-Velasco, 2011; Potthast et al., 2008; Daxenberger and Gurevych, 2012) relate to statement strength. After all, the objective of editing on Wikipedia is to present neutral and objective articles. Public datasets of science communication are available, such as the ACL Anthology,5 collections of NIPS papers,6 and so on. These datasets are useful for understanding the progress of disciplines or the evolution of topics. But the lack of edit histories or revisions makes them not immediately suitable for studying strength differences. Recently, there have been experiments with open peer review</context>
</contexts>
<marker>Bronner, Monz, 2012</marker>
<rawString>Amit Bronner and Christof Monz. 2012. User Edits Classification Using Document Revision Histories. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 356–366.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eunsol Choi</author>
<author>Chenhao Tan</author>
<author>Lillian Lee</author>
<author>Cristian Danescu-Niculescu-Mizil</author>
<author>Jennifer Spindel</author>
</authors>
<title>Hedge detection as a lens on framing in the GMO debates: A position paper.</title>
<date>2012</date>
<booktitle>In Proceedings of the Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics,</booktitle>
<pages>70--79</pages>
<contexts>
<context position="6853" citStr="Choi et al., 2012" startWordPosition="1060" endWordPosition="1063">cientific statements differently from researchers. The participants’ comments may further shed light on science communication and point to better ways to define and understand strength differences. 2 Related Work and Data Hedging, which can lead to strength differences, has received some attention in the study of science communication (Salager-Meyer, 2011; Lewin, 1998; Hyland, 1998; Myers, 1990). The CoNLL 2010 Shared Task was devoted to hedge detection (Farkas et al., 2010). Hedge detection was also used to understand scientific framing in debates over genetically-modified organisms in food (Choi et al., 2012). Revisions on Wikipedia have been shown useful for various applications, including spelling correction (Zesch, 2012), sentence compression (Yamangil and Nelken, 2008), text simplification (Yatskar et al., 2010), paraphrasing (Max and Wisniewski, 2010), and textual entailment (Zanzotto and Pennacchiotti, 2010). But none of the categories of Wikipedia revisions previously examined (Daxenberger and Gurevych, 2013; Bronner and Monz, 2012; Mola-Velasco, 2011; Potthast et al., 2008; Daxenberger and Gurevych, 2012) relate to statement strength. After all, the objective of editing on Wikipedia is to </context>
</contexts>
<marker>Choi, Tan, Lee, Danescu-Niculescu-Mizil, Spindel, 2012</marker>
<rawString>Eunsol Choi, Chenhao Tan, Lillian Lee, Cristian Danescu-Niculescu-Mizil, and Jennifer Spindel. 2012. Hedge detection as a lens on framing in the GMO debates: A position paper. In Proceedings of the Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics, pages 70– 79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johannes Daxenberger</author>
<author>Iryna Gurevych</author>
</authors>
<title>A Corpus-Based Study of Edit Categories in Featured and Non-Featured Wikipedia Articles. In</title>
<date>2012</date>
<booktitle>COLING,</booktitle>
<pages>711--726</pages>
<contexts>
<context position="7367" citStr="Daxenberger and Gurevych, 2012" startWordPosition="1131" endWordPosition="1134">also used to understand scientific framing in debates over genetically-modified organisms in food (Choi et al., 2012). Revisions on Wikipedia have been shown useful for various applications, including spelling correction (Zesch, 2012), sentence compression (Yamangil and Nelken, 2008), text simplification (Yatskar et al., 2010), paraphrasing (Max and Wisniewski, 2010), and textual entailment (Zanzotto and Pennacchiotti, 2010). But none of the categories of Wikipedia revisions previously examined (Daxenberger and Gurevych, 2013; Bronner and Monz, 2012; Mola-Velasco, 2011; Potthast et al., 2008; Daxenberger and Gurevych, 2012) relate to statement strength. After all, the objective of editing on Wikipedia is to present neutral and objective articles. Public datasets of science communication are available, such as the ACL Anthology,5 collections of NIPS papers,6 and so on. These datasets are useful for understanding the progress of disciplines or the evolution of topics. But the lack of edit histories or revisions makes them not immediately suitable for studying strength differences. Recently, there have been experiments with open peer review.7 Records from open reviewing can provide additional insights into the revi</context>
</contexts>
<marker>Daxenberger, Gurevych, 2012</marker>
<rawString>Johannes Daxenberger and Iryna Gurevych. 2012. A Corpus-Based Study of Edit Categories in Featured and Non-Featured Wikipedia Articles. In COLING, pages 711–726.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johannes Daxenberger</author>
<author>Iryna Gurevych</author>
</authors>
<title>Automatically Classifying Edit Categories in Wikipedia Revisions.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="7267" citStr="Daxenberger and Gurevych, 2013" startWordPosition="1117" endWordPosition="1120">e CoNLL 2010 Shared Task was devoted to hedge detection (Farkas et al., 2010). Hedge detection was also used to understand scientific framing in debates over genetically-modified organisms in food (Choi et al., 2012). Revisions on Wikipedia have been shown useful for various applications, including spelling correction (Zesch, 2012), sentence compression (Yamangil and Nelken, 2008), text simplification (Yatskar et al., 2010), paraphrasing (Max and Wisniewski, 2010), and textual entailment (Zanzotto and Pennacchiotti, 2010). But none of the categories of Wikipedia revisions previously examined (Daxenberger and Gurevych, 2013; Bronner and Monz, 2012; Mola-Velasco, 2011; Potthast et al., 2008; Daxenberger and Gurevych, 2012) relate to statement strength. After all, the objective of editing on Wikipedia is to present neutral and objective articles. Public datasets of science communication are available, such as the ACL Anthology,5 collections of NIPS papers,6 and so on. These datasets are useful for understanding the progress of disciplines or the evolution of topics. But the lack of edit histories or revisions makes them not immediately suitable for studying strength differences. Recently, there have been experimen</context>
</contexts>
<marker>Daxenberger, Gurevych, 2013</marker>
<rawString>Johannes Daxenberger and Iryna Gurevych. 2013. Automatically Classifying Edit Categories in Wikipedia Revisions. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rich´ard Farkas</author>
<author>Veronika Vincze</author>
<author>Gy¨orgy M´ora</author>
<author>J´anos Csirik</author>
<author>Gy¨orgy Szarvas</author>
</authors>
<title>The CoNLL2010 shared task: Learning to detect hedges and their scope in natural language text.</title>
<date>2010</date>
<booktitle>In CoNLL— Shared Task,</booktitle>
<pages>1--12</pages>
<marker>Farkas, Vincze, M´ora, Csirik, Szarvas, 2010</marker>
<rawString>Rich´ard Farkas, Veronika Vincze, Gy¨orgy M´ora, J´anos Csirik, and Gy¨orgy Szarvas. 2010. The CoNLL2010 shared task: Learning to detect hedges and their scope in natural language text. In CoNLL— Shared Task, pages 1–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken Hyland</author>
</authors>
<title>Hedging in scientific research articles.</title>
<date>1998</date>
<publisher>John Benjamins Pub. Co.,</publisher>
<location>Amsterdam; Philadelphia.</location>
<contexts>
<context position="6619" citStr="Hyland, 1998" startWordPosition="1026" endWordPosition="1027">kinds of disagreement, the fair level of agreement (Fleiss’ Kappa) among our annotators was decent. But in some cases, the labels differed from our expectations, indicating that the general public can interpret the strength of scientific statements differently from researchers. The participants’ comments may further shed light on science communication and point to better ways to define and understand strength differences. 2 Related Work and Data Hedging, which can lead to strength differences, has received some attention in the study of science communication (Salager-Meyer, 2011; Lewin, 1998; Hyland, 1998; Myers, 1990). The CoNLL 2010 Shared Task was devoted to hedge detection (Farkas et al., 2010). Hedge detection was also used to understand scientific framing in debates over genetically-modified organisms in food (Choi et al., 2012). Revisions on Wikipedia have been shown useful for various applications, including spelling correction (Zesch, 2012), sentence compression (Yamangil and Nelken, 2008), text simplification (Yatskar et al., 2010), paraphrasing (Max and Wisniewski, 2010), and textual entailment (Zanzotto and Pennacchiotti, 2010). But none of the categories of Wikipedia revisions pre</context>
</contexts>
<marker>Hyland, 1998</marker>
<rawString>Ken Hyland. 1998. Hedging in scientific research articles. John Benjamins Pub. Co., Amsterdam; Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven G Krantz</author>
</authors>
<title>How to Write Your First Paper. Notices of the AMS.</title>
<date>2007</date>
<contexts>
<context position="3613" citStr="Krantz, 2007" startWordPosition="558" endWordPosition="559">ect’s contributions, novelty, and limitations. Indeed, statement strength in science communication matters to writers: understating contributions can affect whether people recognize the true importance of the work; at the same time, overclaiming can cause papers to be rejected. With the increasing popularity of e-print services such as the arXiv4, strength changes in scientific papers are becoming more readily available. Since the arXiv started in 1991, it has become “the standard repository for new papers in mathematics, physics, statistics, computer science, biology, and other disciplines” (Krantz, 2007). An intriguing observation is that many researchers submit multiple versions of the same paper on arXiv. For instance, among the 70K papers submitted in 3http://www.huffingtonpost.co.uk/2014/ 03/03/china-kunming-911_n_4888748.html 4http://arxiv.org/ 403 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 403–408, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics ID Pairs 1 S1: The algorithm is studied in this paper. S2: The algorithm is proposed in this paper . 2 S1: ... circadian pattern and b</context>
</contexts>
<marker>Krantz, 2007</marker>
<rawString>Steven G. Krantz. 2007. How to Write Your First Paper. Notices of the AMS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Richard Landis</author>
<author>Gary G Koch</author>
</authors>
<title>The Measurement of Observer Agreement for Categorical Data.</title>
<date>1977</date>
<journal>Biometrics,</journal>
<volume>33</volume>
<issue>1</issue>
<contexts>
<context position="15163" citStr="Landis and Koch, 1977" startWordPosition="2407" endWordPosition="2410">ication (we comment on non-expert rationales later). We use the following set of labels: Stronger, Weaker, No Strength Change, I can’t tell. Table 2 gives our definitions. The instructions included 8 pairs as examples and 10 pairs to label as a training exercise. Participants were then asked to choose labels and write mandatory comments for 50 pairs. According to the comments written by participants, we believe that they did the labeling in good faith. Quantitative overview. We collected 9 labels each for 500 pairs. Among the 500 pairs, Fleiss’ Kappa was 0.242, which indicates fair agreement (Landis and Koch, 1977). We took a conservative approach and only considered pairs with an absolute majority label, i.e., at least 5 of 9 labelers chose the same label. There are 386 pairs that satisfy this requirement (93 weaker, 194 stronger, 99 no change). On this subset of pairs, Fleiss’ Kappa is 0.322, and 74.4% of pairs were strength changes. Considering all the possible disagreement, this result was acceptable. Qualitative observations. We were excited about the labels from these participants: despite 10These decisions were made based on the results and feedback that we got from graduate students in an initia</context>
</contexts>
<marker>Landis, Koch, 1977</marker>
<rawString>J. Richard Landis and Gary G. Koch. 1977. The Measurement of Observer Agreement for Categorical Data. Biometrics, 33(1):159–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beverly A Lewin</author>
</authors>
<title>Hedging: Form and function in scientific research texts.</title>
<date>1998</date>
<booktitle>In Genre Studies in English for Academic Purposes,</booktitle>
<volume>9</volume>
<pages>89--108</pages>
<note>Universitat Jaume I.</note>
<contexts>
<context position="6605" citStr="Lewin, 1998" startWordPosition="1024" endWordPosition="1025">ility of all kinds of disagreement, the fair level of agreement (Fleiss’ Kappa) among our annotators was decent. But in some cases, the labels differed from our expectations, indicating that the general public can interpret the strength of scientific statements differently from researchers. The participants’ comments may further shed light on science communication and point to better ways to define and understand strength differences. 2 Related Work and Data Hedging, which can lead to strength differences, has received some attention in the study of science communication (Salager-Meyer, 2011; Lewin, 1998; Hyland, 1998; Myers, 1990). The CoNLL 2010 Shared Task was devoted to hedge detection (Farkas et al., 2010). Hedge detection was also used to understand scientific framing in debates over genetically-modified organisms in food (Choi et al., 2012). Revisions on Wikipedia have been shown useful for various applications, including spelling correction (Zesch, 2012), sentence compression (Yamangil and Nelken, 2008), text simplification (Yatskar et al., 2010), paraphrasing (Max and Wisniewski, 2010), and textual entailment (Zanzotto and Pennacchiotti, 2010). But none of the categories of Wikipedia</context>
</contexts>
<marker>Lewin, 1998</marker>
<rawString>Beverly A. Lewin. 1998. Hedging: Form and function in scientific research texts. In Genre Studies in English for Academic Purposes, volume 9, pages 89–108. Universitat Jaume I.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aurlien Max</author>
<author>Guillaume Wisniewski</author>
</authors>
<title>Mining Naturally-occurring Corrections and Paraphrases from Wikipedia’s Revision History.</title>
<date>2010</date>
<booktitle>In Proceedings of The seventh international conference on Language Resources and Evaluation.</booktitle>
<contexts>
<context position="7105" citStr="Max and Wisniewski, 2010" startWordPosition="1093" endWordPosition="1097"> to strength differences, has received some attention in the study of science communication (Salager-Meyer, 2011; Lewin, 1998; Hyland, 1998; Myers, 1990). The CoNLL 2010 Shared Task was devoted to hedge detection (Farkas et al., 2010). Hedge detection was also used to understand scientific framing in debates over genetically-modified organisms in food (Choi et al., 2012). Revisions on Wikipedia have been shown useful for various applications, including spelling correction (Zesch, 2012), sentence compression (Yamangil and Nelken, 2008), text simplification (Yatskar et al., 2010), paraphrasing (Max and Wisniewski, 2010), and textual entailment (Zanzotto and Pennacchiotti, 2010). But none of the categories of Wikipedia revisions previously examined (Daxenberger and Gurevych, 2013; Bronner and Monz, 2012; Mola-Velasco, 2011; Potthast et al., 2008; Daxenberger and Gurevych, 2012) relate to statement strength. After all, the objective of editing on Wikipedia is to present neutral and objective articles. Public datasets of science communication are available, such as the ACL Anthology,5 collections of NIPS papers,6 and so on. These datasets are useful for understanding the progress of disciplines or the evolution</context>
</contexts>
<marker>Max, Wisniewski, 2010</marker>
<rawString>Aurlien Max and Guillaume Wisniewski. 2010. Mining Naturally-occurring Corrections and Paraphrases from Wikipedia’s Revision History. In Proceedings of The seventh international conference on Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Santiago M Mola-Velasco</author>
</authors>
<title>Wikipedia Vandalism Detection.</title>
<date>2011</date>
<booktitle>In Proceedings of the 20th International Conference Companion on World Wide Web,</booktitle>
<pages>391--396</pages>
<contexts>
<context position="7311" citStr="Mola-Velasco, 2011" startWordPosition="1125" endWordPosition="1126">(Farkas et al., 2010). Hedge detection was also used to understand scientific framing in debates over genetically-modified organisms in food (Choi et al., 2012). Revisions on Wikipedia have been shown useful for various applications, including spelling correction (Zesch, 2012), sentence compression (Yamangil and Nelken, 2008), text simplification (Yatskar et al., 2010), paraphrasing (Max and Wisniewski, 2010), and textual entailment (Zanzotto and Pennacchiotti, 2010). But none of the categories of Wikipedia revisions previously examined (Daxenberger and Gurevych, 2013; Bronner and Monz, 2012; Mola-Velasco, 2011; Potthast et al., 2008; Daxenberger and Gurevych, 2012) relate to statement strength. After all, the objective of editing on Wikipedia is to present neutral and objective articles. Public datasets of science communication are available, such as the ACL Anthology,5 collections of NIPS papers,6 and so on. These datasets are useful for understanding the progress of disciplines or the evolution of topics. But the lack of edit histories or revisions makes them not immediately suitable for studying strength differences. Recently, there have been experiments with open peer review.7 Records from open</context>
</contexts>
<marker>Mola-Velasco, 2011</marker>
<rawString>Santiago M Mola-Velasco. 2011. Wikipedia Vandalism Detection. In Proceedings of the 20th International Conference Companion on World Wide Web, pages 391–396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Myers</author>
</authors>
<title>Writing biology: Texts in the social construction of scientific knowledge.</title>
<date>1990</date>
<publisher>University of Wisconsin Press,</publisher>
<location>Madison, Wis.</location>
<contexts>
<context position="6633" citStr="Myers, 1990" startWordPosition="1028" endWordPosition="1029">reement, the fair level of agreement (Fleiss’ Kappa) among our annotators was decent. But in some cases, the labels differed from our expectations, indicating that the general public can interpret the strength of scientific statements differently from researchers. The participants’ comments may further shed light on science communication and point to better ways to define and understand strength differences. 2 Related Work and Data Hedging, which can lead to strength differences, has received some attention in the study of science communication (Salager-Meyer, 2011; Lewin, 1998; Hyland, 1998; Myers, 1990). The CoNLL 2010 Shared Task was devoted to hedge detection (Farkas et al., 2010). Hedge detection was also used to understand scientific framing in debates over genetically-modified organisms in food (Choi et al., 2012). Revisions on Wikipedia have been shown useful for various applications, including spelling correction (Zesch, 2012), sentence compression (Yamangil and Nelken, 2008), text simplification (Yatskar et al., 2010), paraphrasing (Max and Wisniewski, 2010), and textual entailment (Zanzotto and Pennacchiotti, 2010). But none of the categories of Wikipedia revisions previously examin</context>
</contexts>
<marker>Myers, 1990</marker>
<rawString>Greg Myers. 1990. Writing biology: Texts in the social construction of scientific knowledge. University of Wisconsin Press, Madison, Wis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilona R Posner</author>
<author>Ronald M Baecker</author>
</authors>
<title>How people write together [groupware].</title>
<date>1992</date>
<booktitle>In System Sciences,</booktitle>
<pages>127--138</pages>
<marker>Posner, Baecker, 1992</marker>
<rawString>Ilona R Posner and Ronald M Baecker. 1992. How people write together [groupware]. In System Sciences, 1992. Proceedings of the Twenty-Fifth Hawaii International Conference on, pages 127– 138.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Potthast</author>
<author>Benno Stein</author>
<author>Robert Gerling</author>
</authors>
<title>Automatic Vandalism Detection in Wikipedia.</title>
<date>2008</date>
<booktitle>In Advances in Information Retrieval,</booktitle>
<pages>663--668</pages>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="7334" citStr="Potthast et al., 2008" startWordPosition="1127" endWordPosition="1130">). Hedge detection was also used to understand scientific framing in debates over genetically-modified organisms in food (Choi et al., 2012). Revisions on Wikipedia have been shown useful for various applications, including spelling correction (Zesch, 2012), sentence compression (Yamangil and Nelken, 2008), text simplification (Yatskar et al., 2010), paraphrasing (Max and Wisniewski, 2010), and textual entailment (Zanzotto and Pennacchiotti, 2010). But none of the categories of Wikipedia revisions previously examined (Daxenberger and Gurevych, 2013; Bronner and Monz, 2012; Mola-Velasco, 2011; Potthast et al., 2008; Daxenberger and Gurevych, 2012) relate to statement strength. After all, the objective of editing on Wikipedia is to present neutral and objective articles. Public datasets of science communication are available, such as the ACL Anthology,5 collections of NIPS papers,6 and so on. These datasets are useful for understanding the progress of disciplines or the evolution of topics. But the lack of edit histories or revisions makes them not immediately suitable for studying strength differences. Recently, there have been experiments with open peer review.7 Records from open reviewing can provide </context>
</contexts>
<marker>Potthast, Stein, Gerling, 2008</marker>
<rawString>Martin Potthast, Benno Stein, and Robert Gerling. 2008. Automatic Vandalism Detection in Wikipedia. In Advances in Information Retrieval, pages 663–668. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franc¸oise Salager-Meyer</author>
</authors>
<title>Scientific discourse and contrastive linguistics: hedging.</title>
<date>2011</date>
<journal>European Science Editing,</journal>
<volume>37</volume>
<issue>2</issue>
<contexts>
<context position="6592" citStr="Salager-Meyer, 2011" startWordPosition="1022" endWordPosition="1023">ons. Given the possibility of all kinds of disagreement, the fair level of agreement (Fleiss’ Kappa) among our annotators was decent. But in some cases, the labels differed from our expectations, indicating that the general public can interpret the strength of scientific statements differently from researchers. The participants’ comments may further shed light on science communication and point to better ways to define and understand strength differences. 2 Related Work and Data Hedging, which can lead to strength differences, has received some attention in the study of science communication (Salager-Meyer, 2011; Lewin, 1998; Hyland, 1998; Myers, 1990). The CoNLL 2010 Shared Task was devoted to hedge detection (Farkas et al., 2010). Hedge detection was also used to understand scientific framing in debates over genetically-modified organisms in food (Choi et al., 2012). Revisions on Wikipedia have been shown useful for various applications, including spelling correction (Zesch, 2012), sentence compression (Yamangil and Nelken, 2008), text simplification (Yatskar et al., 2010), paraphrasing (Max and Wisniewski, 2010), and textual entailment (Zanzotto and Pennacchiotti, 2010). But none of the categories</context>
</contexts>
<marker>Salager-Meyer, 2011</marker>
<rawString>Franc¸oise Salager-Meyer. 2011. Scientific discourse and contrastive linguistics: hedging. European Science Editing, 37(2):35–37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elif Yamangil</author>
<author>Rani Nelken</author>
</authors>
<title>Mining Wikipedia Revision Histories for Improving Sentence Compression.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers,</booktitle>
<pages>137--140</pages>
<contexts>
<context position="7020" citStr="Yamangil and Nelken, 2008" startWordPosition="1082" endWordPosition="1085">e and understand strength differences. 2 Related Work and Data Hedging, which can lead to strength differences, has received some attention in the study of science communication (Salager-Meyer, 2011; Lewin, 1998; Hyland, 1998; Myers, 1990). The CoNLL 2010 Shared Task was devoted to hedge detection (Farkas et al., 2010). Hedge detection was also used to understand scientific framing in debates over genetically-modified organisms in food (Choi et al., 2012). Revisions on Wikipedia have been shown useful for various applications, including spelling correction (Zesch, 2012), sentence compression (Yamangil and Nelken, 2008), text simplification (Yatskar et al., 2010), paraphrasing (Max and Wisniewski, 2010), and textual entailment (Zanzotto and Pennacchiotti, 2010). But none of the categories of Wikipedia revisions previously examined (Daxenberger and Gurevych, 2013; Bronner and Monz, 2012; Mola-Velasco, 2011; Potthast et al., 2008; Daxenberger and Gurevych, 2012) relate to statement strength. After all, the objective of editing on Wikipedia is to present neutral and objective articles. Public datasets of science communication are available, such as the ACL Anthology,5 collections of NIPS papers,6 and so on. The</context>
</contexts>
<marker>Yamangil, Nelken, 2008</marker>
<rawString>Elif Yamangil and Rani Nelken. 2008. Mining Wikipedia Revision Histories for Improving Sentence Compression. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers, pages 137–140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Yatskar</author>
<author>Bo Pang</author>
<author>Cristian Danescu-NiculescuMizil</author>
<author>Lillian Lee</author>
</authors>
<title>For the sake of simplicity: Unsupervised extraction of lexical simplifications from Wikipedia.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>365--368</pages>
<contexts>
<context position="7064" citStr="Yatskar et al., 2010" startWordPosition="1088" endWordPosition="1091">Work and Data Hedging, which can lead to strength differences, has received some attention in the study of science communication (Salager-Meyer, 2011; Lewin, 1998; Hyland, 1998; Myers, 1990). The CoNLL 2010 Shared Task was devoted to hedge detection (Farkas et al., 2010). Hedge detection was also used to understand scientific framing in debates over genetically-modified organisms in food (Choi et al., 2012). Revisions on Wikipedia have been shown useful for various applications, including spelling correction (Zesch, 2012), sentence compression (Yamangil and Nelken, 2008), text simplification (Yatskar et al., 2010), paraphrasing (Max and Wisniewski, 2010), and textual entailment (Zanzotto and Pennacchiotti, 2010). But none of the categories of Wikipedia revisions previously examined (Daxenberger and Gurevych, 2013; Bronner and Monz, 2012; Mola-Velasco, 2011; Potthast et al., 2008; Daxenberger and Gurevych, 2012) relate to statement strength. After all, the objective of editing on Wikipedia is to present neutral and objective articles. Public datasets of science communication are available, such as the ACL Anthology,5 collections of NIPS papers,6 and so on. These datasets are useful for understanding the</context>
</contexts>
<marker>Yatskar, Pang, Danescu-NiculescuMizil, Lee, 2010</marker>
<rawString>Mark Yatskar, Bo Pang, Cristian Danescu-NiculescuMizil, and Lillian Lee. 2010. For the sake of simplicity: Unsupervised extraction of lexical simplifications from Wikipedia. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 365–368.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Massimo Zanzotto</author>
<author>Marco Pennacchiotti</author>
</authors>
<title>Expanding textual entailment corpora from Wikipedia using co-training.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2nd Workshop on Collaboratively Constructed Semantic Resources.</booktitle>
<contexts>
<context position="7164" citStr="Zanzotto and Pennacchiotti, 2010" startWordPosition="1101" endWordPosition="1104">on in the study of science communication (Salager-Meyer, 2011; Lewin, 1998; Hyland, 1998; Myers, 1990). The CoNLL 2010 Shared Task was devoted to hedge detection (Farkas et al., 2010). Hedge detection was also used to understand scientific framing in debates over genetically-modified organisms in food (Choi et al., 2012). Revisions on Wikipedia have been shown useful for various applications, including spelling correction (Zesch, 2012), sentence compression (Yamangil and Nelken, 2008), text simplification (Yatskar et al., 2010), paraphrasing (Max and Wisniewski, 2010), and textual entailment (Zanzotto and Pennacchiotti, 2010). But none of the categories of Wikipedia revisions previously examined (Daxenberger and Gurevych, 2013; Bronner and Monz, 2012; Mola-Velasco, 2011; Potthast et al., 2008; Daxenberger and Gurevych, 2012) relate to statement strength. After all, the objective of editing on Wikipedia is to present neutral and objective articles. Public datasets of science communication are available, such as the ACL Anthology,5 collections of NIPS papers,6 and so on. These datasets are useful for understanding the progress of disciplines or the evolution of topics. But the lack of edit histories or revisions mak</context>
</contexts>
<marker>Zanzotto, Pennacchiotti, 2010</marker>
<rawString>Fabio Massimo Zanzotto and Marco Pennacchiotti. 2010. Expanding textual entailment corpora from Wikipedia using co-training. In Proceedings of the 2nd Workshop on Collaboratively Constructed Semantic Resources.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Torsten Zesch</author>
</authors>
<title>Measuring Contextual Fitness Using Error Contexts Extracted from the Wikipedia Revision History.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>529--538</pages>
<contexts>
<context position="6970" citStr="Zesch, 2012" startWordPosition="1078" endWordPosition="1079">on and point to better ways to define and understand strength differences. 2 Related Work and Data Hedging, which can lead to strength differences, has received some attention in the study of science communication (Salager-Meyer, 2011; Lewin, 1998; Hyland, 1998; Myers, 1990). The CoNLL 2010 Shared Task was devoted to hedge detection (Farkas et al., 2010). Hedge detection was also used to understand scientific framing in debates over genetically-modified organisms in food (Choi et al., 2012). Revisions on Wikipedia have been shown useful for various applications, including spelling correction (Zesch, 2012), sentence compression (Yamangil and Nelken, 2008), text simplification (Yatskar et al., 2010), paraphrasing (Max and Wisniewski, 2010), and textual entailment (Zanzotto and Pennacchiotti, 2010). But none of the categories of Wikipedia revisions previously examined (Daxenberger and Gurevych, 2013; Bronner and Monz, 2012; Mola-Velasco, 2011; Potthast et al., 2008; Daxenberger and Gurevych, 2012) relate to statement strength. After all, the objective of editing on Wikipedia is to present neutral and objective articles. Public datasets of science communication are available, such as the ACL Antho</context>
</contexts>
<marker>Zesch, 2012</marker>
<rawString>Torsten Zesch. 2012. Measuring Contextual Fitness Using Error Contexts Extracted from the Wikipedia Revision History. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 529–538.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>