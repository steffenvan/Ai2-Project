<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.029990">
<title confidence="0.997804666666667">
NRC: A Machine Translation Approach to
Cross-Lingual Word Sense Disambiguation
(SemEval-2013 Task 10)
</title>
<author confidence="0.976965">
Marine Carpuat
</author>
<affiliation confidence="0.74299">
National Research Council
Ottawa, Canada
</affiliation>
<email confidence="0.993674">
Marine.Carpuat@nrc.gc.ca
</email>
<sectionHeader confidence="0.995442" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.996293125">
This paper describes the NRC submission to
the Spanish Cross-Lingual Word Sense Dis-
ambiguation task at SemEval-2013. Since this
word sense disambiguation task uses Spanish
translations of English words as gold annota-
tion, it can be cast as a machine translation
problem. We therefore submitted the output of
a standard phrase-based system as a baseline,
and investigated ways to improve its sense dis-
ambiguation performance. Using only local
context information and no linguistic analy-
sis beyond lemmatization, our machine trans-
lation system surprisingly yields top precision
score based on the best predictions. However,
its top 5 predictions are weaker than those
from other systems.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997530090909091">
This paper describes the systems submitted by the
National Research Council Canada (NRC) for the
Cross-Lingual Word Sense Disambiguation task at
SemEval 2013 (Lefever and Hoste, 2013). As in
the previous edition (Lefever and Hoste, 2010), this
word sense disambiguation task asks systems to dis-
ambiguate English words by providing translations
in other languages. It is therefore closely related to
machine translation. Our work aims to explore this
connection between machine translation and cross-
lingual word sense disambiguation, by providing a
machine translation baseline and investigating ways
to improve the sense disambiguation performance of
a standard machine translation system.
Machine Translation (MT) has often been used
indirectly for SemEval Word Sense Disambiguation
(WSD) tasks: as a tool to automatically create train-
ing data (Guo and Diab, 2010, for instance) ; as
a source of parallel data that can be used to train
WSD systems (Ng and Chan, 2007; van Gompel,
2010; Lefever et al., 2011); or as an application
which can use the predictions of WSD systems de-
veloped for SemEval tasks (Carpuat and Wu, 2005;
Chan et al., 2007; Carpuat and Wu, 2007). This Se-
mEval shared task gives us the opportunity to com-
pare the performance of machine translation systems
with other submissions which use very different ap-
proaches. Our goal is to provide machine transla-
tion output which is representative of state-of-the-art
approaches, and provide a basis for comparing its
strength and weaknesses with that of other systems
submitted to this task. We submitted two systems to
the Spanish Cross-Lingual WSD (CLWSD) task:
</bodyText>
<listItem confidence="0.9789242">
1. BASIC, a baseline machine translation system
trained on the parallel corpus used to define the
sense inventory;
2. ADAPT, a machine translation system that has
been adapted to perform better on this task.
</listItem>
<bodyText confidence="0.986592">
After describing these systems in Sections 2 and
3, we give an overview of the results in Section 4.
</bodyText>
<sectionHeader confidence="0.763622" genericHeader="method">
2 BASIC: A Baseline Phrase-Based
Machine Translation System
</sectionHeader>
<bodyText confidence="0.996254">
We use a phrase-based SMT (PBSMT) architec-
ture, and set-up our system to perform English-to-
Spanish translation. We use a standard SMT system
set-up, as for any translation task. The fact that this
PBSMT system is intended to be used for CLWSD
only influences data selection and pre-processing.
</bodyText>
<page confidence="0.955877">
188
</page>
<bodyText confidence="0.836191">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 188–192, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics
</bodyText>
<subsectionHeader confidence="0.997238">
2.1 Model and Implementation
</subsectionHeader>
<bodyText confidence="0.999980384615385">
In order to translate an English sentence e into Span-
ish, PBSMT first segments the English sentence into
phrases, which are simply sequences of consecutive
words. Each phrase is translated into Spanish ac-
cording to the translations available in a translation
lexicon called phrase-table. Spanish phrases can be
reordered to account for structural divergence be-
tween the two languages. This simple process can
be used to generate Spanish sentences, which are
scored according to translation, reordering and lan-
guage models learned from parallel corpora. The
score of a Spanish translation given an English input
sentence e segmented into J phrases is defined as
</bodyText>
<equation confidence="0.876106666666667">
follows: score(s, e) _ � �j AZlog(OZ(sj, ej)) +
Z
ALMOLM(s)
</equation>
<bodyText confidence="0.99878025">
Detailed feature definitions for phrase-based SMT
models can be found in Koehn (2010). In our sys-
tem, we use the following standard feature functions
O to score English-Spanish phrase pairs:
</bodyText>
<listItem confidence="0.998149076923077">
• 4 phrase-table scores, which are conditional
translation probabilities and HMM lexical
probabilities in both directions translation di-
rections (Chen et al., 2011)
• 6 hierarchical lexicalized reordering scores,
which represent the orientation of the current
phrase with respect to the previous block that
could have been translated as a single phrase
(Galley and Manning, 2008)
• a word penalty, which scores the length of the
output sentence
• a word-displacement distortion penalty, which
penalizes long-distance reorderings.
</listItem>
<bodyText confidence="0.9972693">
In addition, fluency of translation is ensured by a
monolingual Spanish language model OLM, which
is a 5-gram model with Kneser-Ney smoothing.
Phrase translations are extracted based on IBM-
4 alignments obtained with GIZA++ (Och and Ney,
2003). The A weights for these features are learned
using the batch lattice-MIRA algorithm (Cherry and
Foster, 2012) to optimize BLEU-4 (Papineni et al.,
2002) on a tuning set. We use PORTAGE, our inter-
nal PBSMT decoder for all experiments. PORTAGE
uses a standard phrasal beam-search algorithm with
cube pruning. The main differences between this
set-up and the popular open-source Moses system
(Koehn et al., 2007), are the use of hierarchical re-
ordering (Moses only supports non-hierarchical lex-
icalized reordering by default) and smoothed trans-
lation probabilities (Chen et al., 2011).
As a result, disambiguation decisions for the
CLWSD task are based on the following sources of
information:
</bodyText>
<listItem confidence="0.9935036">
• local source context, represented by source
phrases of length 1 to 7 from the translation and
reordering tables
• local target context, represented by the 5-gram
language model.
</listItem>
<bodyText confidence="0.9996144">
Each English sentence in the CLWSD task is
translated into Spanish using our PBSMT system.
We keep track of the phrasal segmentation used to
produce the translation hypothesis and identify the
Spanish translation of the English word of interest.
When the English word is translated into a multi-
word Spanish phrase, we output the Spanish word
within the phrase that has the highest IBM1 transla-
tion probability given the English target word.
For the BEST evaluation, we use this process
on the top PBSMT hypothesis to produce a single
CLWSD translation candidate. For the Out-Of-Five
evaluation, we produce up to five CLWSD transla-
tion candidates from the top 1000 PBSMT transla-
tion hypotheses.
</bodyText>
<subsectionHeader confidence="0.998046">
2.2 Data and Preprocessing
</subsectionHeader>
<bodyText confidence="0.999927933333333">
Training the PBSMT system requires a two-step pro-
cess with two distinct sets of parallel data.
First, the translation, reordering and language
models are learned on a large parallel corpus, the
training set. We use the sentence pairs extracted
from Europarl by the organizers for the purpose of
selecting translation candidates for the gold annota-
tion. Training the SMT system on the exact same
parallel corpus ensures that the system “knows” the
same translations as the human annotators who built
the gold standard. This corpus consists of about
900k sentence pairs.
Second, the feature weights A in the PBSMT are
learned on a smaller parallel corpus, the tuning set.
This corpus should ideally be drawn from the test
</bodyText>
<page confidence="0.995735">
189
</page>
<bodyText confidence="0.999761708333333">
domain. Since the CLWSD task does not provide
parallel data in the test domain, we construct the
tuning set using corpora publicly released for the
WMT2012 translation task1. Since sentences pro-
vided in the trial data appeared to come from a wide
variety of genres and domains, we decided to build
our tuning set using data from the news-commentary
domain, rather then the more narrow Europarl do-
main used for training. We selected the top 3000
sentence pairs from the WMT 2012 development
test sets, based on their distance to the CLWSD
trial and test sentences as measured by cross-entropy
(Moore and Lewis, 2010).
All Spanish and English corpora were processed
using FreeLing (Padr´o and Stanilovsky, 2012).
Since the CLWSD targets and gold translations
are lemmatized, we lemmatize all corpora. While
FreeLing can provide a much richer linguistic anal-
ysis of the input sentences, the PBSMT sytem only
makes use of their lemmatized representation. Our
systems therefore contrast with previous approaches
to CLWSD (van Gompel, 2010; Lefever et al., 2011,
for instance), which use richer sources of informa-
tion such as part-of-speech tags.
</bodyText>
<sectionHeader confidence="0.9916365" genericHeader="method">
3 ADAPT: Adapting the MT system to the
CLWSD task
</sectionHeader>
<bodyText confidence="0.999554368421053">
Our ADAPT system simply consists of two modifi-
cations to the BASIC PBSMT system.
First, it uses a shorter maximum English phrase
length. Instead of learning a translation lexicons for
phrases of length 1 to 7 as in the BASIC system,
the ADAPT system only uses phrases of length 1
and 2. While this dramatically reduces the amount
of source side context available for disambiguation,
it also reduces the amount of noise due to incorrect
word alignments. In addition, there is more evidence
to estimate reliable translation probabilities for short
phrase, since they tend to occur more frequently than
longer phrases.
Second, the ADAPT system is trained on larger
and more diverse data sets. Since MT systems are
known to perform better when they can learn from
larger amounts of relevant training data, we augment
our training set with additional parallel corpora from
the WMT-12 evaluations. We learn translation and
</bodyText>
<footnote confidence="0.898434">
1http://www.statmt.org/wmt12/translation-task.html
</footnote>
<bodyText confidence="0.999938043478261">
reordering models for (1) the Europarl subset used
by the CLWSD organizers (900k sentence pairs, as
in the BASIC system), and (2) the news commen-
tary corpus from WMT12 (which comprises 150k
sentence pairs). For the language model, we use the
Spanish side of these two corpora, as well as that of
the full Europarl corpus from WMT12 (which com-
prises 1.9M sentences). Models learned on different
data sets are combined using linear mixtures learned
on the tuning set (Foster and Kuhn, 2007).
We also attempted other variations on the BASIC
system which were not as successful. For instance,
we tried to update the PBSMT tuning objective to be
better suited to the CLWSD task. When producing
translation of entire sentences, the PBSMT system
is expected to produce hypotheses that are simulta-
neously fluent and adequate, as measured by BLEU
score. In contrast, CLWSD measures the adequacy
of the translation of a single word in a given sen-
tence. We therefore attempted to tune for BLEU-
1, which only uses unigram precision, and therefore
focuses on adequacy rather than fluency. However,
this did not improve CLWSD accuracy.
</bodyText>
<sectionHeader confidence="0.999944" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999805181818182">
Table 1 gives an overview of the results per tar-
get word for both systems, as measured by all of-
ficial metrics (see Lefever and Hoste (2010) for a
detailed description.) According to the BEST Pre-
cision scores, the ADAPT system outperforms the
BASIC system for almost all target words. Using
only the dominant translation picked by the human
annotators as a reference (Mode), the precision for
BEST scores yield more heterogeneous results. This
is not surprising since the ADAPT system uses more
heterogeneous training data, which might make it
harder to learn a reliable estimate of a single domi-
nant translation. When evaluating the precision out
of the top 5 candidates (OOF), all systems improve,
indicating that PBSMT systems can usually produce
some correct alternatives to their top hypothesis.
Table 2 lets us compare the average performance
of the BASIC and ADAPT systems with other par-
ticipating systems. The ADAPT system surprisingly
yields the top performance based on the Precision
BEST evaluation setting, suggesting that, even with
relatively poor models of context, a PBSMT sys-
</bodyText>
<page confidence="0.992982">
190
</page>
<table confidence="0.999953086956522">
Precision: Best Best Best Mode Best Mode OOF OOF OOF Mode OOF Mode
Systems: BASIC ADAPT BASIC ADAPT BASIC ADAPT BASIC ADAPT
coach 22.30 60.10 13.64 59.09 38.30 66.30 31.82 63.64
education 36.07 38.01 73.08 84.62 42.36 42.80 84.62 84.62
execution 41.07 41.07 32.00 32.00 41.57 41.57 36.00 36.00
figure 23.43 29.02 33.33 37.04 31.15 36.12 37.04 44.44
job 13.45 24.26 0.00 37.23 26.52 37.57 27.27 54.55
letter 35.35 37.23 66.67 64.10 37.22 41.20 66.67 66.67
match 15.07 16.53 2.94 2.94 20.70 20.90 5.88 8.82
mission 67.98 67.98 85.29 85.29 67.98 67.98 85.29 85.29
mood 7.18 8.97 0.00 0.00 26.99 29.90 11.11 11.11
paper 31.33 44.59 29.73 40.54 50.45 55.61 45.95 51.35
post 32.26 33.72 23.81 19.05 50.67 53.28 57.14 42.86
pot 34.20 36.63 35.00 32.50 36.12 37.13 32.50 25.00
range 5.41 7.56 10.00 0.00 10.39 17.47 10.00 20.00
rest 20.91 23.44 12.00 8.00 27.44 25.89 16.00 16.00
ring 15.87 10.10 18.92 10.81 42.80 43.14 48.65 45.95
scene 15.86 23.42 43.75 62.50 38.35 37.53 81.25 81.25
side 24.63 33.14 13.04 17.39 36.84 44.03 21.74 39.13
soil 43.88 43.63 66.67 66.67 51.73 57.15 66.67 66.67
strain 24.00 26.24 35.71 35.71 38.37 36.58 42.86 35.71
test 34.45 37.51 50.00 28.57 43.61 40.86 50.00 28.57
Average 27.24 32.16 32.28 36.20 37.98 41.65 42.92 45.38
</table>
<tableCaption confidence="0.996344">
Table 1: Precision scores by target word for the BASIC and ADAPT systems
</tableCaption>
<table confidence="0.999901166666667">
Precision: Best Best Mode OOF OOF Mode
System
Best 32.16 37.11 61.69 57.35
ADAPT 32.16 36.20 41.65 45.38
BASIC 27.24 32.28 37.98 42.92
Baseline 23.23 27.48 53.07 64.65
</table>
<tableCaption confidence="0.976881">
Table 2: Overview of official results: comparison of
</tableCaption>
<bodyText confidence="0.998515291666667">
the precision scores of the ADAPT and BASIC sys-
tems with the best system according to each metric
and with the official baseline
tem can succeed in learning useful disambiguating
information for its top candidate. Despite the prob-
lems stemming from learning good dominant trans-
lations from heterogeneous data, ADAPT ranks near
the top using the Best Mode metric. The rankings in
the out-of-five settings are strikingly different: the
difference between BEST and OOF precisions are
much smaller for BASIC and ADAPT than for all
other participating systems (including the baseline.)
This suggests that our PBSMT system only succeeds
in learning to disambiguate one or two candidates
per word, but does not do a good job of a estimating
the full translation probability distribution of a word
in context. As a result, there is potentially much to
be gained from combining PBSMT systems with the
approaches used by other systems, which typically
use richer feature representation and context mod-
els. Further exploration of the role of context in PB-
SMT performance and a comparison with dedicated
classifiers trained on the same word-aligned parallel
data can be found in (Carpuat, 2013).
</bodyText>
<sectionHeader confidence="0.993859" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999988">
We have described the two systems submitted by
the NRC to the Cross-Lingual Word Sense Disam-
biguation task at SemEval-2013. We used phrase-
based machine translation systems trained on lem-
matized parallel corpora. These systems are unsu-
pervised and do not use any linguistic analysis be-
yond lemmatization. Disambiguation decisions are
based on the local source context available in the
phrasal translation lexicon and the target n-gram
language model. This simple approach gives top
performance when measuring the precision of the
top predictions. However, the top 5 predictions are
interestingly not as good as those of other systems.
</bodyText>
<page confidence="0.994431">
191
</page>
<sectionHeader confidence="0.636394" genericHeader="acknowledgments">
(Carpuat, 2013)
References
</sectionHeader>
<reference confidence="0.99952378">
Marine Carpuat and Dekai Wu. 2005. Word Sense Dis-
ambiguation vs. Statistical Machine Translation. In
Proceedings of the Annual Meeting of the Association
for Computational Linguistics (ACL-05), pages 387–
394, Ann Arbor, Michigan.
Marine Carpuat and Dekai Wu. 2007. Improving Statis-
tical Machine Translation using Word Sense Disam-
biguation. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL 2007), pages 61–72, Prague,
June.
Marine Carpuat. 2013. A semantic evaluation of ma-
chine translation lexical choice. In Proceedings of the
7th Workshop on Syntax, Semantics and Structure in
Statistical Translation, Atlanta, USA, May.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word Sense Disambiguation improves Statistical Ma-
chine Translation. In 45th Annual Meeting of the
Association for Computational Linguistics (ACL-07),
Prague, June.
Boxing Chen, Roland Kuhn, George Foster, and Howard
Johnson. 2011. Unpacking and transforming feature
functions: New ways to smooth phrase tables. In Pro-
ceedings of Machine Translation Summit.
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of the 2012 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, pages 427–
436, Montr´eal, Canada, June.
George Foster and Roland Kuhn. 2007. Mixture-model
adaptation for SMT. In Proceedings of the Second
Workshop on Statistical Machine Translation, pages
128–135, Prague, Czech Republic, June.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP
’08, pages 848–856.
Weiwei Guo and Mona Diab. 2010. COLEPL and COL-
SLM: An unsupervised wsd approach to multilingual
lexical substitution, tasks 2 and 3 semeval 2010. In
Proceedings of the 5th International Workshop on Se-
mantic Evaluation, pages 129–133, Uppsala, Sweden,
July.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Annual
Meeting of the Association for Computational Linguis-
tics (ACL), demonstration session, Prague, Czech Re-
public, June.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press.
Els Lefever and V´eronique Hoste. 2010. Semeval-2010
task 3: Cross-lingual word sense disambiguation. In
Proceedings of the 5th International Workshop on Se-
mantic Evaluation, pages 15–20, Uppsala, Sweden,
July.
Els Lefever and V´eronique Hoste. 2013. Semeval-2013
task 10: Cross-lingual word sense disambiguation.
In Proceedings of the 7th International Workshop on
Semantic Evaluation (SemEval 2013), Atlanta, USA,
May.
Els Lefever, V´eronique Hoste, and Martine De Cock.
2011. Parasense or how to use parallel corpora for
word sense disambiguation. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 317–322, Portland, Oregon, USA, June.
Robert C. Moore and William Lewis. 2010. Intelli-
gent selection of language model training data. In
Proceedings of the ACL 2010 Conference Short Pa-
pers, ACLShort ’10, pages 220–224, Stroudsburg, PA,
USA.
Hwee Tou Ng and Yee Seng Chan. 2007. SemEval-2007
Task 11: English Lexical Sample Task via English-
Chinese Parallel Text. In Proceedings of the 4th In-
ternational Workshop on Semantic Evaluations (Se-
mEval 2007), pages 54–58, Prague, Czech Republic.
SIGLEX.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19–52.
Llu´ıs Padr´o and Evgeny Stanilovsky. 2012. FreeLing
3.0: Towards wider multilinguality. In Proceedings of
the Language Resources and Evaluation Conference
(LREC 2012), Istanbul, Turkey, May.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics, Philadelphia, PA, July.
Maarten van Gompel. 2010. Uvt-wsd1: A cross-lingual
word sense disambiguation system. In Proceedings of
the 5th International Workshop on Semantic Evalua-
tion, pages 238–241, Uppsala, Sweden, July.
</reference>
<page confidence="0.998186">
192
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.118245">
<title confidence="0.849844">NRC: A Machine Translation Approach Cross-Lingual Word Sense</title>
<author confidence="0.5223155">Marine</author>
<affiliation confidence="0.997856">National Research</affiliation>
<address confidence="0.504273">Ottawa,</address>
<email confidence="0.979225">Marine.Carpuat@nrc.gc.ca</email>
<abstract confidence="0.999522294117647">This paper describes the NRC submission to the Spanish Cross-Lingual Word Sense Disambiguation task at SemEval-2013. Since this word sense disambiguation task uses Spanish translations of English words as gold annotation, it can be cast as a machine translation problem. We therefore submitted the output of a standard phrase-based system as a baseline, and investigated ways to improve its sense disambiguation performance. Using only local context information and no linguistic analysis beyond lemmatization, our machine translation system surprisingly yields top precision score based on the best predictions. However, its top 5 predictions are weaker than those from other systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
<author>Dekai Wu</author>
</authors>
<title>Word Sense Disambiguation vs. Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL-05),</booktitle>
<pages>387--394</pages>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="2018" citStr="Carpuat and Wu, 2005" startWordPosition="300" endWordPosition="303"> word sense disambiguation, by providing a machine translation baseline and investigating ways to improve the sense disambiguation performance of a standard machine translation system. Machine Translation (MT) has often been used indirectly for SemEval Word Sense Disambiguation (WSD) tasks: as a tool to automatically create training data (Guo and Diab, 2010, for instance) ; as a source of parallel data that can be used to train WSD systems (Ng and Chan, 2007; van Gompel, 2010; Lefever et al., 2011); or as an application which can use the predictions of WSD systems developed for SemEval tasks (Carpuat and Wu, 2005; Chan et al., 2007; Carpuat and Wu, 2007). This SemEval shared task gives us the opportunity to compare the performance of machine translation systems with other submissions which use very different approaches. Our goal is to provide machine translation output which is representative of state-of-the-art approaches, and provide a basis for comparing its strength and weaknesses with that of other systems submitted to this task. We submitted two systems to the Spanish Cross-Lingual WSD (CLWSD) task: 1. BASIC, a baseline machine translation system trained on the parallel corpus used to define the</context>
</contexts>
<marker>Carpuat, Wu, 2005</marker>
<rawString>Marine Carpuat and Dekai Wu. 2005. Word Sense Disambiguation vs. Statistical Machine Translation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL-05), pages 387– 394, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
<author>Dekai Wu</author>
</authors>
<title>Improving Statistical Machine Translation using Word Sense Disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</booktitle>
<pages>61--72</pages>
<location>Prague,</location>
<contexts>
<context position="2060" citStr="Carpuat and Wu, 2007" startWordPosition="308" endWordPosition="311">a machine translation baseline and investigating ways to improve the sense disambiguation performance of a standard machine translation system. Machine Translation (MT) has often been used indirectly for SemEval Word Sense Disambiguation (WSD) tasks: as a tool to automatically create training data (Guo and Diab, 2010, for instance) ; as a source of parallel data that can be used to train WSD systems (Ng and Chan, 2007; van Gompel, 2010; Lefever et al., 2011); or as an application which can use the predictions of WSD systems developed for SemEval tasks (Carpuat and Wu, 2005; Chan et al., 2007; Carpuat and Wu, 2007). This SemEval shared task gives us the opportunity to compare the performance of machine translation systems with other submissions which use very different approaches. Our goal is to provide machine translation output which is representative of state-of-the-art approaches, and provide a basis for comparing its strength and weaknesses with that of other systems submitted to this task. We submitted two systems to the Spanish Cross-Lingual WSD (CLWSD) task: 1. BASIC, a baseline machine translation system trained on the parallel corpus used to define the sense inventory; 2. ADAPT, a machine tran</context>
</contexts>
<marker>Carpuat, Wu, 2007</marker>
<rawString>Marine Carpuat and Dekai Wu. 2007. Improving Statistical Machine Translation using Word Sense Disambiguation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL 2007), pages 61–72, Prague, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
</authors>
<title>A semantic evaluation of machine translation lexical choice.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th Workshop on Syntax, Semantics and Structure in Statistical Translation,</booktitle>
<location>Atlanta, USA,</location>
<contexts>
<context position="14581" citStr="Carpuat, 2013" startWordPosition="2318" endWordPosition="2319">e baseline.) This suggests that our PBSMT system only succeeds in learning to disambiguate one or two candidates per word, but does not do a good job of a estimating the full translation probability distribution of a word in context. As a result, there is potentially much to be gained from combining PBSMT systems with the approaches used by other systems, which typically use richer feature representation and context models. Further exploration of the role of context in PBSMT performance and a comparison with dedicated classifiers trained on the same word-aligned parallel data can be found in (Carpuat, 2013). 5 Conclusion We have described the two systems submitted by the NRC to the Cross-Lingual Word Sense Disambiguation task at SemEval-2013. We used phrasebased machine translation systems trained on lemmatized parallel corpora. These systems are unsupervised and do not use any linguistic analysis beyond lemmatization. Disambiguation decisions are based on the local source context available in the phrasal translation lexicon and the target n-gram language model. This simple approach gives top performance when measuring the precision of the top predictions. However, the top 5 predictions are inte</context>
</contexts>
<marker>Carpuat, 2013</marker>
<rawString>Marine Carpuat. 2013. A semantic evaluation of machine translation lexical choice. In Proceedings of the 7th Workshop on Syntax, Semantics and Structure in Statistical Translation, Atlanta, USA, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
<author>Hwee Tou Ng</author>
<author>David Chiang</author>
</authors>
<title>Word Sense Disambiguation improves Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In 45th Annual Meeting of the Association for Computational Linguistics (ACL-07),</booktitle>
<location>Prague,</location>
<contexts>
<context position="2037" citStr="Chan et al., 2007" startWordPosition="304" endWordPosition="307">tion, by providing a machine translation baseline and investigating ways to improve the sense disambiguation performance of a standard machine translation system. Machine Translation (MT) has often been used indirectly for SemEval Word Sense Disambiguation (WSD) tasks: as a tool to automatically create training data (Guo and Diab, 2010, for instance) ; as a source of parallel data that can be used to train WSD systems (Ng and Chan, 2007; van Gompel, 2010; Lefever et al., 2011); or as an application which can use the predictions of WSD systems developed for SemEval tasks (Carpuat and Wu, 2005; Chan et al., 2007; Carpuat and Wu, 2007). This SemEval shared task gives us the opportunity to compare the performance of machine translation systems with other submissions which use very different approaches. Our goal is to provide machine translation output which is representative of state-of-the-art approaches, and provide a basis for comparing its strength and weaknesses with that of other systems submitted to this task. We submitted two systems to the Spanish Cross-Lingual WSD (CLWSD) task: 1. BASIC, a baseline machine translation system trained on the parallel corpus used to define the sense inventory; 2</context>
</contexts>
<marker>Chan, Ng, Chiang, 2007</marker>
<rawString>Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007. Word Sense Disambiguation improves Statistical Machine Translation. In 45th Annual Meeting of the Association for Computational Linguistics (ACL-07), Prague, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Boxing Chen</author>
<author>Roland Kuhn</author>
<author>George Foster</author>
<author>Howard Johnson</author>
</authors>
<title>Unpacking and transforming feature functions: New ways to smooth phrase tables.</title>
<date>2011</date>
<booktitle>In Proceedings of Machine Translation Summit.</booktitle>
<contexts>
<context position="4544" citStr="Chen et al., 2011" startWordPosition="692" endWordPosition="695">scored according to translation, reordering and language models learned from parallel corpora. The score of a Spanish translation given an English input sentence e segmented into J phrases is defined as follows: score(s, e) _ � �j AZlog(OZ(sj, ej)) + Z ALMOLM(s) Detailed feature definitions for phrase-based SMT models can be found in Koehn (2010). In our system, we use the following standard feature functions O to score English-Spanish phrase pairs: • 4 phrase-table scores, which are conditional translation probabilities and HMM lexical probabilities in both directions translation directions (Chen et al., 2011) • 6 hierarchical lexicalized reordering scores, which represent the orientation of the current phrase with respect to the previous block that could have been translated as a single phrase (Galley and Manning, 2008) • a word penalty, which scores the length of the output sentence • a word-displacement distortion penalty, which penalizes long-distance reorderings. In addition, fluency of translation is ensured by a monolingual Spanish language model OLM, which is a 5-gram model with Kneser-Ney smoothing. Phrase translations are extracted based on IBM4 alignments obtained with GIZA++ (Och and Ne</context>
</contexts>
<marker>Chen, Kuhn, Foster, Johnson, 2011</marker>
<rawString>Boxing Chen, Roland Kuhn, George Foster, and Howard Johnson. 2011. Unpacking and transforming feature functions: New ways to smooth phrase tables. In Proceedings of Machine Translation Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>George Foster</author>
</authors>
<title>Batch tuning strategies for statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>427--436</pages>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="5263" citStr="Cherry and Foster, 2012" startWordPosition="801" endWordPosition="804"> phrase with respect to the previous block that could have been translated as a single phrase (Galley and Manning, 2008) • a word penalty, which scores the length of the output sentence • a word-displacement distortion penalty, which penalizes long-distance reorderings. In addition, fluency of translation is ensured by a monolingual Spanish language model OLM, which is a 5-gram model with Kneser-Ney smoothing. Phrase translations are extracted based on IBM4 alignments obtained with GIZA++ (Och and Ney, 2003). The A weights for these features are learned using the batch lattice-MIRA algorithm (Cherry and Foster, 2012) to optimize BLEU-4 (Papineni et al., 2002) on a tuning set. We use PORTAGE, our internal PBSMT decoder for all experiments. PORTAGE uses a standard phrasal beam-search algorithm with cube pruning. The main differences between this set-up and the popular open-source Moses system (Koehn et al., 2007), are the use of hierarchical reordering (Moses only supports non-hierarchical lexicalized reordering by default) and smoothed translation probabilities (Chen et al., 2011). As a result, disambiguation decisions for the CLWSD task are based on the following sources of information: • local source con</context>
</contexts>
<marker>Cherry, Foster, 2012</marker>
<rawString>Colin Cherry and George Foster. 2012. Batch tuning strategies for statistical machine translation. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 427– 436, Montr´eal, Canada, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Roland Kuhn</author>
</authors>
<title>Mixture-model adaptation for SMT.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>128--135</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="10117" citStr="Foster and Kuhn, 2007" startWordPosition="1583" endWordPosition="1586"> additional parallel corpora from the WMT-12 evaluations. We learn translation and 1http://www.statmt.org/wmt12/translation-task.html reordering models for (1) the Europarl subset used by the CLWSD organizers (900k sentence pairs, as in the BASIC system), and (2) the news commentary corpus from WMT12 (which comprises 150k sentence pairs). For the language model, we use the Spanish side of these two corpora, as well as that of the full Europarl corpus from WMT12 (which comprises 1.9M sentences). Models learned on different data sets are combined using linear mixtures learned on the tuning set (Foster and Kuhn, 2007). We also attempted other variations on the BASIC system which were not as successful. For instance, we tried to update the PBSMT tuning objective to be better suited to the CLWSD task. When producing translation of entire sentences, the PBSMT system is expected to produce hypotheses that are simultaneously fluent and adequate, as measured by BLEU score. In contrast, CLWSD measures the adequacy of the translation of a single word in a given sentence. We therefore attempted to tune for BLEU1, which only uses unigram precision, and therefore focuses on adequacy rather than fluency. However, this</context>
</contexts>
<marker>Foster, Kuhn, 2007</marker>
<rawString>George Foster and Roland Kuhn. 2007. Mixture-model adaptation for SMT. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 128–135, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>A simple and effective hierarchical phrase reordering model.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08,</booktitle>
<pages>848--856</pages>
<contexts>
<context position="4759" citStr="Galley and Manning, 2008" startWordPosition="725" endWordPosition="728">ws: score(s, e) _ � �j AZlog(OZ(sj, ej)) + Z ALMOLM(s) Detailed feature definitions for phrase-based SMT models can be found in Koehn (2010). In our system, we use the following standard feature functions O to score English-Spanish phrase pairs: • 4 phrase-table scores, which are conditional translation probabilities and HMM lexical probabilities in both directions translation directions (Chen et al., 2011) • 6 hierarchical lexicalized reordering scores, which represent the orientation of the current phrase with respect to the previous block that could have been translated as a single phrase (Galley and Manning, 2008) • a word penalty, which scores the length of the output sentence • a word-displacement distortion penalty, which penalizes long-distance reorderings. In addition, fluency of translation is ensured by a monolingual Spanish language model OLM, which is a 5-gram model with Kneser-Ney smoothing. Phrase translations are extracted based on IBM4 alignments obtained with GIZA++ (Och and Ney, 2003). The A weights for these features are learned using the batch lattice-MIRA algorithm (Cherry and Foster, 2012) to optimize BLEU-4 (Papineni et al., 2002) on a tuning set. We use PORTAGE, our internal PBSMT </context>
</contexts>
<marker>Galley, Manning, 2008</marker>
<rawString>Michel Galley and Christopher D. Manning. 2008. A simple and effective hierarchical phrase reordering model. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08, pages 848–856.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Guo</author>
<author>Mona Diab</author>
</authors>
<title>COLEPL and COLSLM: An unsupervised wsd approach to multilingual lexical substitution,</title>
<date>2010</date>
<journal>tasks</journal>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<volume>2</volume>
<pages>129--133</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="1757" citStr="Guo and Diab, 2010" startWordPosition="251" endWordPosition="254">rd sense disambiguation task asks systems to disambiguate English words by providing translations in other languages. It is therefore closely related to machine translation. Our work aims to explore this connection between machine translation and crosslingual word sense disambiguation, by providing a machine translation baseline and investigating ways to improve the sense disambiguation performance of a standard machine translation system. Machine Translation (MT) has often been used indirectly for SemEval Word Sense Disambiguation (WSD) tasks: as a tool to automatically create training data (Guo and Diab, 2010, for instance) ; as a source of parallel data that can be used to train WSD systems (Ng and Chan, 2007; van Gompel, 2010; Lefever et al., 2011); or as an application which can use the predictions of WSD systems developed for SemEval tasks (Carpuat and Wu, 2005; Chan et al., 2007; Carpuat and Wu, 2007). This SemEval shared task gives us the opportunity to compare the performance of machine translation systems with other submissions which use very different approaches. Our goal is to provide machine translation output which is representative of state-of-the-art approaches, and provide a basis f</context>
</contexts>
<marker>Guo, Diab, 2010</marker>
<rawString>Weiwei Guo and Mona Diab. 2010. COLEPL and COLSLM: An unsupervised wsd approach to multilingual lexical substitution, tasks 2 and 3 semeval 2010. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 129–133, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<location>Christine Moran, Richard</location>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, </marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer Zens</author>
<author>Ondrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: Open Source Toolkit for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Annual Meeting of the Association for Computational Linguistics (ACL), demonstration session,</booktitle>
<location>Prague, Czech Republic,</location>
<marker>Zens, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Annual Meeting of the Association for Computational Linguistics (ACL), demonstration session, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical Machine Translation.</title>
<date>2010</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="4274" citStr="Koehn (2010)" startWordPosition="654" endWordPosition="655">Spanish according to the translations available in a translation lexicon called phrase-table. Spanish phrases can be reordered to account for structural divergence between the two languages. This simple process can be used to generate Spanish sentences, which are scored according to translation, reordering and language models learned from parallel corpora. The score of a Spanish translation given an English input sentence e segmented into J phrases is defined as follows: score(s, e) _ � �j AZlog(OZ(sj, ej)) + Z ALMOLM(s) Detailed feature definitions for phrase-based SMT models can be found in Koehn (2010). In our system, we use the following standard feature functions O to score English-Spanish phrase pairs: • 4 phrase-table scores, which are conditional translation probabilities and HMM lexical probabilities in both directions translation directions (Chen et al., 2011) • 6 hierarchical lexicalized reordering scores, which represent the orientation of the current phrase with respect to the previous block that could have been translated as a single phrase (Galley and Manning, 2008) • a word penalty, which scores the length of the output sentence • a word-displacement distortion penalty, which p</context>
</contexts>
<marker>Koehn, 2010</marker>
<rawString>Philipp Koehn. 2010. Statistical Machine Translation. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Els Lefever</author>
<author>V´eronique Hoste</author>
</authors>
<title>Semeval-2010 task 3: Cross-lingual word sense disambiguation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>15--20</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="1130" citStr="Lefever and Hoste, 2010" startWordPosition="160" endWordPosition="163">dard phrase-based system as a baseline, and investigated ways to improve its sense disambiguation performance. Using only local context information and no linguistic analysis beyond lemmatization, our machine translation system surprisingly yields top precision score based on the best predictions. However, its top 5 predictions are weaker than those from other systems. 1 Introduction This paper describes the systems submitted by the National Research Council Canada (NRC) for the Cross-Lingual Word Sense Disambiguation task at SemEval 2013 (Lefever and Hoste, 2013). As in the previous edition (Lefever and Hoste, 2010), this word sense disambiguation task asks systems to disambiguate English words by providing translations in other languages. It is therefore closely related to machine translation. Our work aims to explore this connection between machine translation and crosslingual word sense disambiguation, by providing a machine translation baseline and investigating ways to improve the sense disambiguation performance of a standard machine translation system. Machine Translation (MT) has often been used indirectly for SemEval Word Sense Disambiguation (WSD) tasks: as a tool to automatically create traini</context>
<context position="10900" citStr="Lefever and Hoste (2010)" startWordPosition="1716" endWordPosition="1719"> suited to the CLWSD task. When producing translation of entire sentences, the PBSMT system is expected to produce hypotheses that are simultaneously fluent and adequate, as measured by BLEU score. In contrast, CLWSD measures the adequacy of the translation of a single word in a given sentence. We therefore attempted to tune for BLEU1, which only uses unigram precision, and therefore focuses on adequacy rather than fluency. However, this did not improve CLWSD accuracy. 4 Results Table 1 gives an overview of the results per target word for both systems, as measured by all official metrics (see Lefever and Hoste (2010) for a detailed description.) According to the BEST Precision scores, the ADAPT system outperforms the BASIC system for almost all target words. Using only the dominant translation picked by the human annotators as a reference (Mode), the precision for BEST scores yield more heterogeneous results. This is not surprising since the ADAPT system uses more heterogeneous training data, which might make it harder to learn a reliable estimate of a single dominant translation. When evaluating the precision out of the top 5 candidates (OOF), all systems improve, indicating that PBSMT systems can usuall</context>
</contexts>
<marker>Lefever, Hoste, 2010</marker>
<rawString>Els Lefever and V´eronique Hoste. 2010. Semeval-2010 task 3: Cross-lingual word sense disambiguation. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 15–20, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Els Lefever</author>
<author>V´eronique Hoste</author>
</authors>
<title>Semeval-2013 task 10: Cross-lingual word sense disambiguation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013),</booktitle>
<location>Atlanta, USA,</location>
<contexts>
<context position="1076" citStr="Lefever and Hoste, 2013" startWordPosition="151" endWordPosition="154">n problem. We therefore submitted the output of a standard phrase-based system as a baseline, and investigated ways to improve its sense disambiguation performance. Using only local context information and no linguistic analysis beyond lemmatization, our machine translation system surprisingly yields top precision score based on the best predictions. However, its top 5 predictions are weaker than those from other systems. 1 Introduction This paper describes the systems submitted by the National Research Council Canada (NRC) for the Cross-Lingual Word Sense Disambiguation task at SemEval 2013 (Lefever and Hoste, 2013). As in the previous edition (Lefever and Hoste, 2010), this word sense disambiguation task asks systems to disambiguate English words by providing translations in other languages. It is therefore closely related to machine translation. Our work aims to explore this connection between machine translation and crosslingual word sense disambiguation, by providing a machine translation baseline and investigating ways to improve the sense disambiguation performance of a standard machine translation system. Machine Translation (MT) has often been used indirectly for SemEval Word Sense Disambiguation</context>
</contexts>
<marker>Lefever, Hoste, 2013</marker>
<rawString>Els Lefever and V´eronique Hoste. 2013. Semeval-2013 task 10: Cross-lingual word sense disambiguation. In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013), Atlanta, USA, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Els Lefever</author>
<author>V´eronique Hoste</author>
<author>Martine De Cock</author>
</authors>
<title>Parasense or how to use parallel corpora for word sense disambiguation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>317--322</pages>
<location>Portland, Oregon, USA,</location>
<marker>Lefever, Hoste, De Cock, 2011</marker>
<rawString>Els Lefever, V´eronique Hoste, and Martine De Cock. 2011. Parasense or how to use parallel corpora for word sense disambiguation. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 317–322, Portland, Oregon, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
<author>William Lewis</author>
</authors>
<title>Intelligent selection of language model training data.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Conference Short Papers, ACLShort ’10,</booktitle>
<pages>220--224</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="8085" citStr="Moore and Lewis, 2010" startWordPosition="1260" endWordPosition="1263">st 189 domain. Since the CLWSD task does not provide parallel data in the test domain, we construct the tuning set using corpora publicly released for the WMT2012 translation task1. Since sentences provided in the trial data appeared to come from a wide variety of genres and domains, we decided to build our tuning set using data from the news-commentary domain, rather then the more narrow Europarl domain used for training. We selected the top 3000 sentence pairs from the WMT 2012 development test sets, based on their distance to the CLWSD trial and test sentences as measured by cross-entropy (Moore and Lewis, 2010). All Spanish and English corpora were processed using FreeLing (Padr´o and Stanilovsky, 2012). Since the CLWSD targets and gold translations are lemmatized, we lemmatize all corpora. While FreeLing can provide a much richer linguistic analysis of the input sentences, the PBSMT sytem only makes use of their lemmatized representation. Our systems therefore contrast with previous approaches to CLWSD (van Gompel, 2010; Lefever et al., 2011, for instance), which use richer sources of information such as part-of-speech tags. 3 ADAPT: Adapting the MT system to the CLWSD task Our ADAPT system simply </context>
</contexts>
<marker>Moore, Lewis, 2010</marker>
<rawString>Robert C. Moore and William Lewis. 2010. Intelligent selection of language model training data. In Proceedings of the ACL 2010 Conference Short Papers, ACLShort ’10, pages 220–224, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Yee Seng Chan</author>
</authors>
<title>SemEval-2007 Task 11: English Lexical Sample Task via EnglishChinese Parallel Text.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval</booktitle>
<pages>54--58</pages>
<publisher>SIGLEX.</publisher>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="1860" citStr="Ng and Chan, 2007" startWordPosition="272" endWordPosition="275">er languages. It is therefore closely related to machine translation. Our work aims to explore this connection between machine translation and crosslingual word sense disambiguation, by providing a machine translation baseline and investigating ways to improve the sense disambiguation performance of a standard machine translation system. Machine Translation (MT) has often been used indirectly for SemEval Word Sense Disambiguation (WSD) tasks: as a tool to automatically create training data (Guo and Diab, 2010, for instance) ; as a source of parallel data that can be used to train WSD systems (Ng and Chan, 2007; van Gompel, 2010; Lefever et al., 2011); or as an application which can use the predictions of WSD systems developed for SemEval tasks (Carpuat and Wu, 2005; Chan et al., 2007; Carpuat and Wu, 2007). This SemEval shared task gives us the opportunity to compare the performance of machine translation systems with other submissions which use very different approaches. Our goal is to provide machine translation output which is representative of state-of-the-art approaches, and provide a basis for comparing its strength and weaknesses with that of other systems submitted to this task. We submitte</context>
</contexts>
<marker>Ng, Chan, 2007</marker>
<rawString>Hwee Tou Ng and Yee Seng Chan. 2007. SemEval-2007 Task 11: English Lexical Sample Task via EnglishChinese Parallel Text. In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval 2007), pages 54–58, Prague, Czech Republic. SIGLEX.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="5152" citStr="Och and Ney, 2003" startWordPosition="784" endWordPosition="787">al., 2011) • 6 hierarchical lexicalized reordering scores, which represent the orientation of the current phrase with respect to the previous block that could have been translated as a single phrase (Galley and Manning, 2008) • a word penalty, which scores the length of the output sentence • a word-displacement distortion penalty, which penalizes long-distance reorderings. In addition, fluency of translation is ensured by a monolingual Spanish language model OLM, which is a 5-gram model with Kneser-Ney smoothing. Phrase translations are extracted based on IBM4 alignments obtained with GIZA++ (Och and Ney, 2003). The A weights for these features are learned using the batch lattice-MIRA algorithm (Cherry and Foster, 2012) to optimize BLEU-4 (Papineni et al., 2002) on a tuning set. We use PORTAGE, our internal PBSMT decoder for all experiments. PORTAGE uses a standard phrasal beam-search algorithm with cube pruning. The main differences between this set-up and the popular open-source Moses system (Koehn et al., 2007), are the use of hierarchical reordering (Moses only supports non-hierarchical lexicalized reordering by default) and smoothed translation probabilities (Chen et al., 2011). As a result, di</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29(1):19–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Llu´ıs Padr´o</author>
<author>Evgeny Stanilovsky</author>
</authors>
<title>FreeLing 3.0: Towards wider multilinguality.</title>
<date>2012</date>
<booktitle>In Proceedings of the Language Resources and Evaluation Conference (LREC 2012),</booktitle>
<location>Istanbul, Turkey,</location>
<marker>Padr´o, Stanilovsky, 2012</marker>
<rawString>Llu´ıs Padr´o and Evgeny Stanilovsky. 2012. FreeLing 3.0: Towards wider multilinguality. In Proceedings of the Language Resources and Evaluation Conference (LREC 2012), Istanbul, Turkey, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Philadelphia, PA,</location>
<contexts>
<context position="5306" citStr="Papineni et al., 2002" startWordPosition="808" endWordPosition="811">at could have been translated as a single phrase (Galley and Manning, 2008) • a word penalty, which scores the length of the output sentence • a word-displacement distortion penalty, which penalizes long-distance reorderings. In addition, fluency of translation is ensured by a monolingual Spanish language model OLM, which is a 5-gram model with Kneser-Ney smoothing. Phrase translations are extracted based on IBM4 alignments obtained with GIZA++ (Och and Ney, 2003). The A weights for these features are learned using the batch lattice-MIRA algorithm (Cherry and Foster, 2012) to optimize BLEU-4 (Papineni et al., 2002) on a tuning set. We use PORTAGE, our internal PBSMT decoder for all experiments. PORTAGE uses a standard phrasal beam-search algorithm with cube pruning. The main differences between this set-up and the popular open-source Moses system (Koehn et al., 2007), are the use of hierarchical reordering (Moses only supports non-hierarchical lexicalized reordering by default) and smoothed translation probabilities (Chen et al., 2011). As a result, disambiguation decisions for the CLWSD task are based on the following sources of information: • local source context, represented by source phrases of leng</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, Philadelphia, PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maarten van Gompel</author>
</authors>
<title>Uvt-wsd1: A cross-lingual word sense disambiguation system.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>238--241</pages>
<location>Uppsala, Sweden,</location>
<marker>van Gompel, 2010</marker>
<rawString>Maarten van Gompel. 2010. Uvt-wsd1: A cross-lingual word sense disambiguation system. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 238–241, Uppsala, Sweden, July.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>