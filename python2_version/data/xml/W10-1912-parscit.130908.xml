<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002059">
<title confidence="0.9965395">
Extraction of Disease-Treatment Semantic Relations from Biomedical
Sentences
</title>
<author confidence="0.993346">
Oana Frunza and Diana Inkpen
</author>
<affiliation confidence="0.99815">
School of Information Technology and Engineering
University of Ottawa Ottawa, ON, Canada, K1N 6N5
</affiliation>
<email confidence="0.996865">
{ofrunza,diana}@site.uottawa.ca
</email>
<sectionHeader confidence="0.996636" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.993006714285714">
This paper describes our study on identi-
fying semantic relations that exist between
diseases and treatments in biomedical sen-
tences. We focus on three semantic rela-
tions: Cure, Prevent, and Side Effect. The
contributions of this paper consists in the
fact that better results are obtained com-
pared to previous studies and the fact that
our research settings allow the integration
of biomedical and medical knowledge.
We obtain 98.55% F-measure for the Cure
relation, 100% F-measure for the Prevent
relation, and 88.89% F-measure for the
Side Effect relation.
</bodyText>
<sectionHeader confidence="0.998364" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999977512195122">
Research in the fields of life-science and bio-
medical domain has been the focus of the Natural
Language Processing (NLP) and Machine Learn-
ing (ML) community for some time now. This
trend goes very much inline with the direction
the medical healthcare system is moving to: the
electronic world. The research focus of scientists
that work in the filed of computational linguistics
and life science domains also followed the trends
of the medicine that is practiced today, an Evi-
dence Based Medicine (EBM). This new way of
medical practice is not only based on the experi-
ence a healthcare provider acquires as time
passes by, but on the latest discoveries as well.
We live in an information explosion era where it
is almost impossible to find that piece of relevant
information that we need. With easy and cheep
access to disk-space we sometimes even find
challenging to find our stored local documents. It
should come to no surprise that the global trend
in domains like biomedicine and not only is to
rely on technology to identify and upraise infor-
mation. The amount of publications and research
that is indexed in the life-science domain grows
almost exponentially (Hunter and Cohen (2006)
making the task of finding relevant information,
a hard and challenging task for NLP research.
The search for information in the life-science
domain is not only the focus of researchers that
work in these fields, but the focus of laypeople as
well. Studies reveal that people are searching the
web for medical-related articles to be better in-
formed about their health. Ginsberg et al. (2009)
show how a new outbreak of the influenza virus
can be detected from search engine query data.
The aim of this paper is to show which NLP
and ML techniques are suitable for the task of
identifying semantic relations between diseases
and treatments in short biomedical texts. The
value of our work stands in the results we obtain
and the new feature representation techniques.
</bodyText>
<sectionHeader confidence="0.999796" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.998874941176471">
The most relevant work for our study is the work
of Rosario and Hearst (2004). The authors of this
paper are the ones that created and distributed the
data set used in our research. The data set is an-
notated with disease and treatments entities and
with 8 semantic relations between diseases and
treatments. The main focus of their work is on
entity recognition – the task of identifying enti-
ties, diseases and treatments in biomedical text
sentences. The authors use Hidden Markov
Models and maximum entropy models to per-
form both the task of entity recognition and of
relation discrimination. Their representation
techniques are based on words in context, part-
of-speech information, phrases, and terms from
MeSH1, a medical lexical knowledge-base. Com-
pared to previous work, our research is focused
</bodyText>
<footnote confidence="0.884008">
1 http://www.nlm.nih.gov/mesh/meshhome.html
</footnote>
<page confidence="0.971021">
91
</page>
<note confidence="0.979354">
Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, ACL 2010, pages 91–98,
Uppsala, Sweden, 15 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999876333333333">
on different representation techniques, different
classification models, and most importantly in
obtaining improved results without using the an-
notations of the entities (new data will not have
them). In previous research, the best results were
obtained when the entities involved in the rela-
tions were identified and used as features.
The biomedical literature contains a wealth of
work on semantic relation extraction, mostly fo-
cused on more biology-specific tasks: subcellu-
lar-location (Craven 1999), gene-disorder asso-
ciation (Ray and Craven 2001), and diseases and
drugs relations (Srinivasan and Rindflesch 2002,
Ahlers et al., 2007).
Text classification techniques combined with a
Naïve Bayes classifier and relational learning
algorithms are methods used by Craven (1999).
Hidden Markov Models are used in Craven
(2001), but similarly to Rosario and Hearst
(2004), the research focus was entity recognition.
A context based approach using MeSH term
co-occurrences are used by Srinivasan and Rind-
flesch (2002) for relationship discrimination be-
tween diseases and drugs.
A lot of work is focused on building rules used
to extract relation. Feldman et al. (2002) use a
rule-based system to extract relations that are
focused on genes, proteins, drugs, and diseases.
Friedman et al. (2001) go deeper into building a
rule-based system by hand-crafting a semantic
grammar and a set of semantic constraints in or-
der to recognize a range of biological and mo-
lecular relations.
</bodyText>
<sectionHeader confidence="0.965474" genericHeader="method">
3 Task and Data Sets
</sectionHeader>
<bodyText confidence="0.997871026315789">
Our task is focused on identifying disease-
treatment relations in sentences. Three relations:
Cure, Prevent, and Side Effect, are the main ob-
jective of our work. We are tackling this task by
using techniques based on NLP and supervised
ML techniques. We decided to focus on these
three relations because these are the ones that are
better represented in the original data set and in
the end will allow us to draw more reliable con-
clusions. Also, looking at the meaning of all rela-
tions in the original data set, the three that we
focus on are the ones that could be useful for
wider research goals and are the ones that really
entail relations between two entities. In the su-
pervised ML settings the amount of training data
is a factor that influences the performance; sup-
port for this stands not only in the related work
performed on the same data set, but in the re-
search literature as well. The aim of this paper is
to focus on few relations of interest and try to
identify what predictive model and what repre-
sentation techniques bring the best results of
identifying semantic relations in short biomedi-
cal texts. We mostly focused on the value that
the research can bring, rather than on an incre-
mental research.
As mentioned in the previous section, the data
set that we use to run our experiments is the one
of Rosario and Hearst (2004). The entire data set
is collected from Medline2 2001 abstracts. Sen-
tences from titles and abstracts are annotated
with entities and with 8 relations, based only on
the information present in a certain sentence. The
first 100 titles and 40 abstracts from each of the
59 Medline 2001 files were used for annotation.
Table 1, presents the original data set, as pub-
lished in previous research. The numbers in pa-
renthesis represent the training and test set sizes.
</bodyText>
<table confidence="0.997889333333333">
Relationship Definition and Example
Cure TREAT cures DIS
810 (648, 162) Intravenous immune globulin for
recurrent spontaneous abortion
Only DIS TREAT not mentioned
616 (492, 124) Social ties and susceptibility to
the common cold
Only TREAT DIS not mentioned
166 (132, 34) Flucticasome propionate is safe in
recommended doses
Prevent TREAT prevents the DIS
63 (50, 13) Statins for prevention of stroke
Vague Very unclear relationship
36 (28, 8) Phenylbutazone and leukemia
Side Effect DIS is a result of a TREAT
29 (24, 5) Malignant mesodermal mixed
tumor of the uterus following
irradiation
NO Cure TREAT does not cure DIS
4 (3, 1) Evidence for double resistance to
permethrin and malathion in head
lice
Total relevant: 1724 (1377, 347)
Irrelevant Treat and DIS not present
1771 (1416, 355) Patients were followed up for 6
months
Total: 3495 (2793, 702)
</table>
<tableCaption confidence="0.998776">
Table 1. Original data set.
</tableCaption>
<bodyText confidence="0.997259142857143">
From this original data set, the sentences that are
annotated with Cure, Prevent, Side Effect, Only
DIS, Only TREAT, and Vague are the ones that
used in our current work. While our main focus
is on the Cure, Prevent, and Side Effect, we also
run experiments for all relations such that a di-
rect comparison with the previous work is done.
</bodyText>
<footnote confidence="0.947982">
2 http://medline.cos.com/
</footnote>
<page confidence="0.996935">
92
</page>
<bodyText confidence="0.999679727272727">
Table 2 describes the data sets that we created
from the original data and used in our experi-
ments. For each of the relations of interest we
have 3 labels attached: Positive, Negative, and
Neutral. The Positive label is given to sentences
that are annotated with the relation in question in
the original data; the Negative label is given to
the sentences labeled with Only DIS and Only
TREAT classes in the original data; Neutral label
is given to the sentences annotated with Vague
class in the original data set.
</bodyText>
<table confidence="0.9993617">
Relation Train
Positive Negative Neutral
Cure 554 531 25
Prevent 42 531 25
SideEffect 20 531 25
Relation Test
Positive Negative Neutral
Cure 276 266 12
Prevent 21 266 12
SideEffect 10 266 12
</table>
<tableCaption confidence="0.99966">
Table 2. Our data sets3.
</tableCaption>
<sectionHeader confidence="0.99579" genericHeader="method">
4 Methodology
</sectionHeader>
<bodyText confidence="0.989017151515151">
The experimental settings that we follow are
adapted to the domain of study (we integrate ad-
ditional medical knowledge), yielding for the
methods to bring improved performance.
The challenges that can be encountered while
working with NLP and ML techniques are: find-
ing the suitable model for prediction – since the
ML field offers a suite of predictive models (al-
gorithms), the task of finding the suitable one
relies heavily on empirical studies and knowl-
edge expertise; and finding the best data repre-
sentation – identifying the right and sufficient
features to represent the data is a crucial aspect.
These challenges are addressed by trying various
predictive algorithms based on different learning
techniques, and by using various textual repre-
sentation techniques that we consider suitable.
The task of identifying the three semantic rela-
tions is addressed in three ways:
Setting 1: build three models, each focused
on one relation that can distinguish sentences
that contain the relation – Positive label, from
other sentences that are neutral – Neutral label,
and from sentences that do not contain relevant
information – Negative label;
3 The number of sentences available for download is
not the same as the ones from the original data set,
published in Rosario and Hearst (‘04).
Setting 2: build three models, each focused on
one relation that can distinguish sentences that
contain the relation from sentences that do not
contain any relevant information. This setting is
similar to a two-class classification task in which
instances are labeled either with the relation in
question – Positive label, or with non-relevant
information – Negative label;
Setting 3: build one model that distinguishes the
three relations – a three-way classification task
where each sentence is labeled with one of the
semantic relations, using the data with all the
Positive labels.
The first set of experiments is influenced by
previous research done by Koppel and Schler
(2005). The authors claim that for polarity learn-
ing “neutral” examples help the learning algo-
rithms to better identify the two polarities. Their
research was done on a corpus of posts to chat
groups devoted to popular U.S. television and
posts to shopping.com’s product evaluation page.
As classification algorithms, a set of 6 repre-
sentative models: decision-based models (Deci-
sion trees – J48), probabilistic models (Naïve
Bayes and complement Naïve Bayes (CNB),
which is adapted for imbalanced class distribu-
tion), adaptive learning (AdaBoost), linear classi-
fier (support vector machine (SVM) with poly-
nomial kernel), and a classifier, ZeroR, that al-
ways predicts the majority class in the training
data used as a baseline. All classifiers are part of
a tool called Weka4.
As representation technique, we rely on fea-
tures such as the words in the context, the noun
and verb-phrases, and the detected biomedical
and medical entities. In the following subsec-
tions, we describe all the representation tech-
niques that we use.
</bodyText>
<subsectionHeader confidence="0.997426">
4.1 Bag-of-words representation
</subsectionHeader>
<bodyText confidence="0.998302833333333">
The bag-of-words (BOW) representation is
commonly used for text classification tasks. It is
a representation in which the features are chosen
among the words that are present in the training
data. Selection techniques are used in order to
identify the most suitable words as features. Af-
ter the feature space is identified, each training
and test instance is mapped into this feature rep-
resentation by giving values to each feature for a
certain instance. Two feature value representa-
tions are the most commonly used for the BOW
representation: binary feature values – the value
</bodyText>
<footnote confidence="0.887725">
4 http://www.cs.waikato.ac.nz/ml/weka/
</footnote>
<page confidence="0.997531">
93
</page>
<bodyText confidence="0.998689960784314">
of a feature is 1 if the feature is present in the
instance and 0 otherwise, or frequency feature
values – the feature value is the number of times
it appears in an instance, or 0 if it did not appear.
Taking into consideration the fact that an in-
stance is a sentence, the textual information is
relatively small. Therefore a frequency value
representation is chosen. The difference between
a binary value representation and a frequency
value representation is not always significant,
because sentences tend to be short. Nonetheless,
if a feature appears more than once in a sentence,
this means that it is important and the frequency
value representation captures this aspect.
The selected features are words (not lemma-
tized) delimited by spaces and simple punctua-
tion marks: space, ( , ) , [ , ] , . , &apos; , _ that ap-
peared at least three times in the training collec-
tion and contain at least an alpha-numeric char-
acter, are not part of an English list of stop
words5 and are longer than three characters. Stop
words are function words that appear in every
document (e.g., the, it, of, an) and therefore do
not help in classification. The frequency thresh-
old of three is commonly used for text collec-
tions because it removes non-informative fea-
tures and also strings of characters that might be
the result of a wrong tokenization when splitting
the text into words. Words that have length of
one or two characters are not considered as fea-
tures because of two reasons: possible incorrect
tokenization and problems with very short acro-
nyms in the medical domain that could be highly
ambiguous (could be a medical acronym or an
abbreviation of a common word).
4.2 NLP and biomedical concepts represen-
tation
The second type of representation is based on
NLP information – noun-phrases, verb-phrases
and biomedical concepts (Biomed). In order to
extract this type of information from the data, we
used the Genia6 tagger. The tagger analyzes Eng-
lish sentences and outputs the base forms, part-
of-speech tags, chunk tags, and named entity
tags. The tagger is specifically tuned for bio-
medical text such as Medline abstracts.
Figure 1 presents an output example by the
Genia tagger for the sentence: “Inhibition of NF-
kappaB activation reversed the anti-apoptotic
effect of isochamaejasmin.”. The tag O stands
for Outside, B for Beginning, and I for Inside.
</bodyText>
<footnote confidence="0.9788215">
5 http://www.site.uottawa.ca/~diana/csi5180/StopWords
6 http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/tagger/
</footnote>
<figureCaption confidence="0.988802">
Figure 1. Example of Genia tagger output
</figureCaption>
<table confidence="0.993233181818182">
Inhibition Inhibition NN B-NP O
of of IN B-PP O
NF-kappaB NF-kappaB NN B-NP B-protein
activation activation NN I-NP O
reversed reverse VBD B-VP O
the the DT B-NP O
anti-apoptotic anti-apoptotic JJ I-NP O
effect effect NN I-NP O
of of IN B-PP O
isochamaejasmin isochamaejasmin NN B-NP O
. . . O O
</table>
<bodyText confidence="0.984542256410257">
The noun-phrases and verb-phrases identified by
the tagger are considered as features for our sec-
ond representation technique. The following pre-
processing steps are applied before defining the
set of final features: remove features that contain
only punctuation, remove stop-words, and con-
sider valid features only the lemma-based forms
of the identified noun-phrases, verb-phrases and
biomedical concepts. The reason to do this is
because there are a lot of inflected forms (e.g.,
plural forms) for the same word and the lemma-
tized form (the base form of a word) will give us
the same base form for all the inflected forms.
4.3 Medical concepts (UMLS) representa-
tion
In order to work with a representation that pro-
vides features that are more general than the
words in the abstracts (used in the BOW repre-
sentation), we also used the unified medical lan-
guage system7 (here on UMLS) concept repre-
sentations. UMLS is a knowledge source devel-
oped at the U.S. National Library of Medicine
(here on NLM) and it contains a meta-thesaurus,
a semantic network, and the specialist lexicon for
biomedical domain. The meta-thesaurus is organ-
ized around concepts and meanings; it links al-
ternative names and views of the same concept
and identifies useful relationships between dif-
ferent concepts. UMLS contains over 1 million
medical concepts, and over 5 million concept
names which are hierarchical organized. Each
unique concept that is present in the thesaurus
has associated multiple text strings variants
(slight morphological variations of the concept).
All concepts are assigned at least one semantic
type from the semantic network providing a gen-
eralization of the existing relations between con-
cepts. There are 135 semantic types in the
knowledge base linked through 54 relationships.
</bodyText>
<footnote confidence="0.704278">
7 http://www.nlm.nih.gov/pubs/factsheets/umls.html
</footnote>
<page confidence="0.999381">
94
</page>
<bodyText confidence="0.999456833333333">
In addition to the UMLS knowledge base,
NLM created a set of tools that allow easier ac-
cess to the useful information. MetaMap8 is a
tool created by NLM that maps free text to medi-
cal concepts in the UMLS, or equivalently, it
discovers meta-thesaurus concepts in text. With
this software, text is processed through a series
of modules that in the end will give a ranked list
of all possible concept candidates for a particular
noun-phrase. For each of the noun phrases that
the system finds in the text, variant noun phrases
are generated. For each of the variant noun
phrases, candidate concepts (concepts that con-
tain the noun phrase variant) from the UMLS
meta-thesaurus are retrieved and evaluated. The
retrieved concepts are compared to the actual
phrase using a fit function that measures the text
overlap between the actual phrase and the candi-
date concept (it returns a numerical value). The
best of the candidates are then organized accord-
ing to the decreasing value of the fit function.
We used the top concept candidate for each iden-
tified phrase in an abstract as a feature. Figure 2
presents an example of the output of the Meta-
Map system for the phrase “to an increased
risk&amp;quot;. The information presented in the brackets,
the semantic type, “Qualitative Concept, Quanti-
tative Concept” for the candidate with the fit
function value 861 is the feature used for our
UMLS representation.
</bodyText>
<figureCaption confidence="0.999743">
Figure 2. Example of MetaMap system output
</figureCaption>
<figure confidence="0.9810395">
Meta Candidates (6)
861 Risk [Qualitative Concept, Quantitative Concept]
694 Increased (Increased (qualifier value)) [Func-
tional Concept]
623 Increase (Increase (qualifier value)) [Functional
Concept]
601 Acquired (Acquired (qualifier value)) [Temporal
Concept]
601 Obtained (Obtained (attribute)) [Functional Con-
cept]
</figure>
<subsectionHeader confidence="0.869022">
588 Increasing (Increasing (qualifier value)) [Func-
tional Concept]
</subsectionHeader>
<bodyText confidence="0.999841444444444">
Another reason to use a UMLS concept represen-
tation is the concept drift phenomenon that can
appear in a BOW representation. Especially in
the medical domain texts, this is a frequent prob-
lem as stated by Cohen et al. (2004). New arti-
cles that publish new research on a certain topic
bring with them new terms that might not match
the ones that were seen in the training process in
a certain moment of time.
</bodyText>
<footnote confidence="0.356019">
8 http://mmtx.nlm.nih.gov/
</footnote>
<bodyText confidence="0.999912833333333">
Experiments for the task tackled in our re-
search are performed with all the above-
mentioned representations, plus combinations of
them. We combine the BOW, UMLS and NLP
and biomedical concepts by putting all features
together to represent an instance.
</bodyText>
<sectionHeader confidence="0.99982" genericHeader="method">
5 Results
</sectionHeader>
<bodyText confidence="0.992011230769231">
This section presents the results obtained for the
task of identifying semantic relations with the
methods described above. As evaluation meas-
ures we report F-measure and accuracy values.
The main evaluation metric that we consider is
the F-measure9, since it is a suitable when the
data set is imbalanced. We report the accuracy
measure as well, because we want to compare
our results with previous work. Table A1 from
appendix A presents the results that we obtained
with our methods. The table contains F-measure
scores for all three semantic relations with the
three experimental settings proposed for all com-
binations of representation and classification al-
gorithms. In this section, since we cannot report
all the results for all the classification algorithms,
we decided to report the classifiers that obtained
the lower and upper margin of results for every
representation setting. More detailed descriptions
for the results are present in appendix A. We
consider as baseline a classifier that always pre-
dicts the majority class. For the relation Cure the
F-measure baseline is 66.51%, for Prevent and
Side Effect 0%.
The next three figures present the best results
obtained for the three experimental settings.
</bodyText>
<figureCaption confidence="0.426229666666667">
Figure 3. Best results for Setting 1.
9 F-measure represents the harmonic mean between
precision and recall. Precision represents the percent-
age of correctly classified sentences while recall
represents the percentage of sentences identified as
relevant by the classifier.
</figureCaption>
<figure confidence="0.993073444444444">
100.00%
40.00%
80.00%
60.00%
20.00%
0.00%
F-measure Results - Setting1
Cure - BOW +
NLP + Biomed+
UMLS - SMO
85.14%
Prevent -
UMLS + NLP +
Biomed - SVM
62.50%
SideEffect -
BOW- NB
34.48%
</figure>
<page confidence="0.855893">
95
</page>
<figureCaption confidence="0.9995565">
Figure 4. Best results for Setting 2.
Figure 5. Best results for Setting 3.
</figureCaption>
<sectionHeader confidence="0.996813" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999176894736842">
Our goal was to obtain high performance results
for the three semantic relations. The first set of
experiments was influenced by previous work on
a different task. The results obtained show that
this setting might not be suitable for the medical
domain, due to one of the following possible ex-
planations: the number of examples that are con-
sidered as being neutral is not sufficient or not
appropriate (the neutral examples are considered
sentences that are annotated with a Vague rela-
tion in the original data); or the negative exam-
ples are not appropriate (the negative examples
are considered sentences that talk about either
treatment or about diseases). The results of these
experiments are shown in Figure 3. As future
work, we want to run similar setting experiments
when considering negative examples sentences
that are not informative, labeled Irrelevant, from
the original data set, and the neutral examples the
ones that are considered negative in this current
experiments.
In Setting 2, the results are better than in the
previous setting, showing that the neutral exam-
ples used in the previous experiments confused
the algorithms and were not appropriate. These
results validate the fact that the previous setting
was not the best one for the task.
The best results for the task are obtained with
the third setting, when a model is built and
trained on a data set that contains all sentences
annotated with the three relations. The represen-
tation and the classification algorithms were able
to make the distinction between the relations and
obtained the best results for this task. The results
are: 98.55% F-measure for the Cure class, 100%
F-measure for the Prevent class, and 88.89% for
the Side Effect class.
Some important observations can be drawn
from the obtained results: probabilistic and linear
models combined with informative feature repre-
sentations bring the best results. They are consis-
tent in outperforming the other classifiers in all
the three settings. AdaBoost classifier was out-
performed by other classifiers, which is a little
surprising, taking into consideration the fact that
this classifier tends to work better on imbalanced
data. BOW is a representation technique that
even though it is simplistic, most of the times it
is really hard to outperform. One of the major
contributions of this work is the fact that the cur-
rent experiments show that additional informa-
tion used in the representation settings brings
improvements for the task. The task itself is a
knowledge-charged task and the experiments
show that classifiers can perform better when
richer information (e.g. concepts for medical
ontologies) is provided.
</bodyText>
<subsectionHeader confidence="0.999439">
6.1 Comparison to previous work
</subsectionHeader>
<bodyText confidence="0.999900214285714">
Even though our main focus is on the three rela-
tions mentioned earlier, in order to validate our
methodology, we also performed the 8-class
classification task, similar to the one done by
Rosario and Hearst (2004). Figure 3 presents a
graphical comparison of the results of our meth-
ods to the ones obtained in the previous work.
We report accuracy values for these experiments,
as it was done in the previous work.
In Figure 3, the first set of bar-results repre-
sents the best individual results for each relation.
The representation technique and classification
model that obtains the best results are the ones
described on the x-axis.
</bodyText>
<figure confidence="0.999213372093023">
Results - Setting 2
F-measure
100.00%
98.00%
96.00%
94.00%
92.00%
90.00%
88.00%
86.00%
84.00%
82.00%
Cure -
BOW +
NLP +
Biomed+
UMLS - NB
Prevent -
BOW +
NLP +
Biomed+
UMLS - NB
SideEffect
- BOW +
NLP +
Biomed+
UMLS -
CNB
98.55% 100%
88.89%
Results - Setting 3
F-measure
Cure - BOW + Prevent - SideEffect -
NLP + BOW + NLP + BOW + NLP +
Biomed+ Biomed+ Biomed+
UMLS - NB UMLS - NB UMLS - CNB
100.00%
95.00%
90.00%
85.00%
80.00%
98.55% 100%
88.89%
</figure>
<page confidence="0.781912">
96
</page>
<figureCaption confidence="0.999552">
Figure 3. Comparison of results.
</figureCaption>
<bodyText confidence="0.999980708333333">
The second series of results represents the
overall best model that is reported for each rela-
tion. The model reported here is a combination
of BOW, verb and noun-phrases, biomedical and
UMLS concepts, with a CNB classifier.
The third series of results represent the accu-
racy results obtained in previous work by Rosa-
rio and Hearst (2004). As we can see from the
figure, the best individual models have a major
improvement over previous results. When a sin-
gle model is used for all relations, our results
improve the previous ones in four relations with
the difference varying from: 3 percentage point
difference (Cure) to 23 percentage point differ-
ence (Prevent). We obtain the same results for
two semantic relations, No_Cure and Vague and
we believe that this is the case due to the fact that
these two classes are significantly under-
represented compared to the other ones involved
in the task. For the Treatment_Only relation our
results are outperformed with 1.5 percentage
points and for the Irrelevant relation with 0.1
percentage point, only when we use the same
model for all relations.
</bodyText>
<sectionHeader confidence="0.997076" genericHeader="method">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.9999595">
We can conclude that additional knowledge and
deeper analysis of the task and data in question
are required in order to obtain reliable results.
Probabilistic models are stable and reliable for
the classification of short texts in the medical
domain. The representation techniques highly
influence the results, common for the ML com-
munity, but more informative representations
where the ones that consistently obtained the best
results.
As future work, we would like to extend the
experimental methodology when the first setting
is applied, and to use additional sources of in-
formation as representation techniques.
</bodyText>
<sectionHeader confidence="0.998916" genericHeader="method">
References
</sectionHeader>
<reference confidence="0.998225815789474">
Ahlers C., Fiszman M., Fushman D., Lang F.-M.,
Rindflesch T. 2007. Extracting semantic predica-
tions from Medline citations for pharmacogenom-
ics. Pacific Symposium on Biocomputing, 12:209-
220.
Craven M. 1999. Learning to extract relations from
Medline. AAAI-99 Workshop on Machine Learn-
ing for Information Extraction.
Feldman R. Regev Y., Finkelstein-Landau M., Hur-
vitz E., and Kogan B. 2002. Mining biomedical lit-
erature using information extraction. Current Drug
Discovery.
Friedman C., Kra P., Yu H., Krauthammer M., and
Rzhetzky A. 2001. Genies: a natural-language
processing system for the extraction of molecular
pathways from journal articles. Bioinformatics,
17(1).
Ginsberg J., Mohebbi Matthew H., Rajan S. Patel,
Lynnette Brammer, Mark S. Smolinski &amp; Larry
Brilliant. 2009. Detecting influenza epidemics
using search engine query data. Nature 457,
1012-1014.
Hunter Lawrence and K. Bretonnel Cohen. 2006.
Biomedical Language Processing: What’s Beyond
PubMed? Molecular Cell 21, 589–594.
Ray S. and Craven M. 2001. Representing sentence
structure in Hidden Markov Models for informa-
tion extraction. Proceedings of IJCAI-2001.
Rosario B. and Marti A. Hearst. 2004. Classifying
semantic relations in bioscience text. Proceed-
ings of the 42nd Annual Meeting on Association
for Computational Linguistics, 430.
Koppel M. and J. Schler. 2005. Using Neutral Ex-
amples for Learning Polarity, Proceedings of
IJCAI, Edinburgh, Scotland.
Srinivasan P. and T. Rindflesch 2002. Exploring text
mining from Medline. Proceedings of the AMIA
Symposium.
</reference>
<figure confidence="0.999105846153846">
Accuracy
120.00%
100.00%
40.00%
20.00%
80.00%
60.00%
0.00%
Results for all semantic relations
Models
Best Models
Best Model
Previous Work
</figure>
<page confidence="0.9949">
97
</page>
<sectionHeader confidence="0.533933" genericHeader="method">
Appendix A. Detailed Results.
</sectionHeader>
<table confidence="0.999872159090909">
Relation Representation Classification Algorithm - F-Measure (%)
Setting1 Setting2 Setting3
Cure NLP+Biomed AdaB 32.22 AdaB 35.69 CNB 87.88
ZeroR 66.51 ZeroR 67.48 SVM 94.85
BOW AdaB 63.60 AdaB 67.23 CNB 92.57
CNB 79.22 SVM 81.43 NB 96.80
UMLS AdaB 61.08 AdaB 64.78 CNB 88.20
NB 74.73 NB 76.04 SVM 95.62
BOW+UMLS AdaB 56.07 AdaB 74.68 J48 96.13
CNB 84.54 NB 86.48 NB 97.50
NLP+Biomed AdaB 61.08 AdaB 64.78 CNB 90.87
+UMLS NB 75.18 NB 76.70 SVM 96.58
NLP+Biomed AdaB 53.04 AdaB 77.46 J48 96.14
+BOW SVM 78.98 CNB 81.86 NB 97.86
NLP+Biomed+ AdaB 53.04 AdaB 72.32 J48 96.32
BOW+UMLS SVM 85.14 SVM 87.10 NB 98.55
Prevent NLP+Biomed AdaB 0 AdaB,J48 0 Ada,J48 0
NB 17.02 NB 22.86 CNB 55.17
BOW CNB 31.78 J48 0 SVM 50
NB 50 NB 61.9 CNB 89.47
UMLS AdaB 0 J48 0 J48 0
NB 28.57 SVM 48.28 CNB 68.75
BOW+UMLS J48 39.02 J48 9.09 AdaB 60
NB 57.14 NB 75.68 CNB 89.47
NLP+Biomed AdaB 0 J48 16 J48 0
+UMLS SVM 62.50 SVM 57.69 CNB 97.56
NLP+Biomed SVM 35 J48 0 AdaB 64.52
+BOW NB 54.90 NB 66.67 CNB 92.31
NLP+Biomed+ J48 30.77 J48 0 AdaB,J48 64.52
BOW+UMLS NB 62.30 SVM 77.78 NB 100
Side NLP+Biomed AdaB 0 J48,SVM 0 AdaB,J48 0
Effect NB,CNB 7.69 AdaB 18.18 CNB 33.33
BOW AdaB 0 AdaB,J48 0 Ada,J48 0
NB 34.48 NB 50 CNB 66.67
UMLS AdaB,J48, 0 J48,SVM 0 AdaB,J48 0
SVM NB 22.22 NB 33.33 NB,CNB 46.15
BOW+UMLS AdaB,J48 0 J48 0 AdaB 0
NB 21.43 NB 47 CNB 75
NLP+Biomed+ AdaB,J48 0 J48 0 AdaB.J48 0
UMLS NB 19.35 NB 31.58 NB,CNB 46.15
NLP+Biomed+ AdaB,J48 0 J48 0 AdaB,J48 0
BOW NB 33.33 NB 55.56 CNB 88.89
NLP+Biomed+ AdaB,J48 0 J48 0 AdaB 0
BOW+UMLS NB 24 NB 46.15 CNB 88.89
</table>
<tableCaption confidence="0.998729">
Table A1. Results obtained with our methods.
</tableCaption>
<bodyText confidence="0.999524666666667">
The Representation column describes all the feature representation techniques that we tried. The acro-
nym NLP stands from verb and noun-phrase features put together and Biomed for bio-medical con-
cepts (the ones extracted by Genia tagger). The first line of results for every representation technique
presents the classier that obtained the lowest results, while the second line represents the classifier
with the best F-measure score. In bold we mark the best scores for all semantic relations in each of the
three settings.
</bodyText>
<page confidence="0.991007">
98
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.442189">
<title confidence="0.99842">Extraction of Disease-Treatment Semantic Relations from Biomedical Sentences</title>
<author confidence="0.980983">Oana Frunza</author>
<author confidence="0.980983">Diana</author>
<affiliation confidence="0.998891">School of Information Technology and Engineering</affiliation>
<address confidence="0.552573">University of Ottawa Ottawa, ON, Canada, K1N 6N5</address>
<email confidence="0.992544">ofrunza@site.uottawa.ca</email>
<email confidence="0.992544">diana@site.uottawa.ca</email>
<abstract confidence="0.995176642857143">This paper describes our study on identifying semantic relations that exist between diseases and treatments in biomedical sentences. We focus on three semantic relaand The contributions of this paper consists in the fact that better results are obtained compared to previous studies and the fact that our research settings allow the integration of biomedical and medical knowledge. obtain 98.55% F-measure for the 100% F-measure for the relation, and 88.89% F-measure for the</abstract>
<intro confidence="0.84058">Effect</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Ahlers</author>
<author>M Fiszman</author>
<author>D Fushman</author>
<author>F-M Lang</author>
<author>T Rindflesch</author>
</authors>
<title>Extracting semantic predications from Medline citations for pharmacogenomics. Pacific Symposium on Biocomputing,</title>
<date>2007</date>
<pages>12--209</pages>
<contexts>
<context position="4458" citStr="Ahlers et al., 2007" startWordPosition="698" endWordPosition="701">entation techniques, different classification models, and most importantly in obtaining improved results without using the annotations of the entities (new data will not have them). In previous research, the best results were obtained when the entities involved in the relations were identified and used as features. The biomedical literature contains a wealth of work on semantic relation extraction, mostly focused on more biology-specific tasks: subcellular-location (Craven 1999), gene-disorder association (Ray and Craven 2001), and diseases and drugs relations (Srinivasan and Rindflesch 2002, Ahlers et al., 2007). Text classification techniques combined with a Naïve Bayes classifier and relational learning algorithms are methods used by Craven (1999). Hidden Markov Models are used in Craven (2001), but similarly to Rosario and Hearst (2004), the research focus was entity recognition. A context based approach using MeSH term co-occurrences are used by Srinivasan and Rindflesch (2002) for relationship discrimination between diseases and drugs. A lot of work is focused on building rules used to extract relation. Feldman et al. (2002) use a rule-based system to extract relations that are focused on genes,</context>
</contexts>
<marker>Ahlers, Fiszman, Fushman, Lang, Rindflesch, 2007</marker>
<rawString>Ahlers C., Fiszman M., Fushman D., Lang F.-M., Rindflesch T. 2007. Extracting semantic predications from Medline citations for pharmacogenomics. Pacific Symposium on Biocomputing, 12:209-220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Craven</author>
</authors>
<title>Learning to extract relations from Medline.</title>
<date>1999</date>
<booktitle>AAAI-99 Workshop on Machine Learning for Information Extraction.</booktitle>
<contexts>
<context position="4321" citStr="Craven 1999" startWordPosition="680" endWordPosition="681">ssing, ACL 2010, pages 91–98, Uppsala, Sweden, 15 July 2010. c�2010 Association for Computational Linguistics on different representation techniques, different classification models, and most importantly in obtaining improved results without using the annotations of the entities (new data will not have them). In previous research, the best results were obtained when the entities involved in the relations were identified and used as features. The biomedical literature contains a wealth of work on semantic relation extraction, mostly focused on more biology-specific tasks: subcellular-location (Craven 1999), gene-disorder association (Ray and Craven 2001), and diseases and drugs relations (Srinivasan and Rindflesch 2002, Ahlers et al., 2007). Text classification techniques combined with a Naïve Bayes classifier and relational learning algorithms are methods used by Craven (1999). Hidden Markov Models are used in Craven (2001), but similarly to Rosario and Hearst (2004), the research focus was entity recognition. A context based approach using MeSH term co-occurrences are used by Srinivasan and Rindflesch (2002) for relationship discrimination between diseases and drugs. A lot of work is focused </context>
</contexts>
<marker>Craven, 1999</marker>
<rawString>Craven M. 1999. Learning to extract relations from Medline. AAAI-99 Workshop on Machine Learning for Information Extraction.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Feldman R Regev Y</author>
<author>M Finkelstein-Landau</author>
<author>E Hurvitz</author>
<author>B Kogan</author>
</authors>
<title>Mining biomedical literature using information extraction. Current Drug Discovery.</title>
<date>2002</date>
<marker>Y, Finkelstein-Landau, Hurvitz, Kogan, 2002</marker>
<rawString>Feldman R. Regev Y., Finkelstein-Landau M., Hurvitz E., and Kogan B. 2002. Mining biomedical literature using information extraction. Current Drug Discovery.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Friedman</author>
<author>P Kra</author>
<author>H Yu</author>
<author>M Krauthammer</author>
<author>A Rzhetzky</author>
</authors>
<title>Genies: a natural-language processing system for the extraction of molecular pathways from journal articles.</title>
<date>2001</date>
<journal>Bioinformatics,</journal>
<volume>17</volume>
<issue>1</issue>
<contexts>
<context position="5112" citStr="Friedman et al. (2001)" startWordPosition="799" endWordPosition="802"> combined with a Naïve Bayes classifier and relational learning algorithms are methods used by Craven (1999). Hidden Markov Models are used in Craven (2001), but similarly to Rosario and Hearst (2004), the research focus was entity recognition. A context based approach using MeSH term co-occurrences are used by Srinivasan and Rindflesch (2002) for relationship discrimination between diseases and drugs. A lot of work is focused on building rules used to extract relation. Feldman et al. (2002) use a rule-based system to extract relations that are focused on genes, proteins, drugs, and diseases. Friedman et al. (2001) go deeper into building a rule-based system by hand-crafting a semantic grammar and a set of semantic constraints in order to recognize a range of biological and molecular relations. 3 Task and Data Sets Our task is focused on identifying diseasetreatment relations in sentences. Three relations: Cure, Prevent, and Side Effect, are the main objective of our work. We are tackling this task by using techniques based on NLP and supervised ML techniques. We decided to focus on these three relations because these are the ones that are better represented in the original data set and in the end will </context>
</contexts>
<marker>Friedman, Kra, Yu, Krauthammer, Rzhetzky, 2001</marker>
<rawString>Friedman C., Kra P., Yu H., Krauthammer M., and Rzhetzky A. 2001. Genies: a natural-language processing system for the extraction of molecular pathways from journal articles. Bioinformatics, 17(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ginsberg</author>
<author>Mohebbi Matthew H</author>
<author>Rajan S Patel</author>
<author>Lynnette Brammer</author>
<author>Mark S Smolinski</author>
<author>Larry Brilliant</author>
</authors>
<title>Detecting influenza epidemics using search engine query data.</title>
<date>2009</date>
<journal>Nature</journal>
<volume>457</volume>
<pages>1012--1014</pages>
<contexts>
<context position="2401" citStr="Ginsberg et al. (2009)" startWordPosition="380" endWordPosition="383">ins like biomedicine and not only is to rely on technology to identify and upraise information. The amount of publications and research that is indexed in the life-science domain grows almost exponentially (Hunter and Cohen (2006) making the task of finding relevant information, a hard and challenging task for NLP research. The search for information in the life-science domain is not only the focus of researchers that work in these fields, but the focus of laypeople as well. Studies reveal that people are searching the web for medical-related articles to be better informed about their health. Ginsberg et al. (2009) show how a new outbreak of the influenza virus can be detected from search engine query data. The aim of this paper is to show which NLP and ML techniques are suitable for the task of identifying semantic relations between diseases and treatments in short biomedical texts. The value of our work stands in the results we obtain and the new feature representation techniques. 2 Related Work The most relevant work for our study is the work of Rosario and Hearst (2004). The authors of this paper are the ones that created and distributed the data set used in our research. The data set is annotated w</context>
</contexts>
<marker>Ginsberg, H, Patel, Brammer, Smolinski, Brilliant, 2009</marker>
<rawString>Ginsberg J., Mohebbi Matthew H., Rajan S. Patel, Lynnette Brammer, Mark S. Smolinski &amp; Larry Brilliant. 2009. Detecting influenza epidemics using search engine query data. Nature 457, 1012-1014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hunter Lawrence</author>
<author>K Bretonnel Cohen</author>
</authors>
<title>Biomedical Language Processing: What’s Beyond PubMed?</title>
<date>2006</date>
<journal>Molecular Cell</journal>
<volume>21</volume>
<pages>589--594</pages>
<marker>Lawrence, Cohen, 2006</marker>
<rawString>Hunter Lawrence and K. Bretonnel Cohen. 2006. Biomedical Language Processing: What’s Beyond PubMed? Molecular Cell 21, 589–594.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Ray</author>
<author>M Craven</author>
</authors>
<title>Representing sentence structure in Hidden Markov Models for information extraction.</title>
<date>2001</date>
<booktitle>Proceedings of IJCAI-2001.</booktitle>
<contexts>
<context position="4370" citStr="Ray and Craven 2001" startWordPosition="685" endWordPosition="688">eden, 15 July 2010. c�2010 Association for Computational Linguistics on different representation techniques, different classification models, and most importantly in obtaining improved results without using the annotations of the entities (new data will not have them). In previous research, the best results were obtained when the entities involved in the relations were identified and used as features. The biomedical literature contains a wealth of work on semantic relation extraction, mostly focused on more biology-specific tasks: subcellular-location (Craven 1999), gene-disorder association (Ray and Craven 2001), and diseases and drugs relations (Srinivasan and Rindflesch 2002, Ahlers et al., 2007). Text classification techniques combined with a Naïve Bayes classifier and relational learning algorithms are methods used by Craven (1999). Hidden Markov Models are used in Craven (2001), but similarly to Rosario and Hearst (2004), the research focus was entity recognition. A context based approach using MeSH term co-occurrences are used by Srinivasan and Rindflesch (2002) for relationship discrimination between diseases and drugs. A lot of work is focused on building rules used to extract relation. Feldm</context>
</contexts>
<marker>Ray, Craven, 2001</marker>
<rawString>Ray S. and Craven M. 2001. Representing sentence structure in Hidden Markov Models for information extraction. Proceedings of IJCAI-2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Rosario</author>
<author>Marti A Hearst</author>
</authors>
<title>Classifying semantic relations in bioscience text.</title>
<date>2004</date>
<booktitle>Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>430</pages>
<contexts>
<context position="2869" citStr="Rosario and Hearst (2004)" startWordPosition="462" endWordPosition="465">eople as well. Studies reveal that people are searching the web for medical-related articles to be better informed about their health. Ginsberg et al. (2009) show how a new outbreak of the influenza virus can be detected from search engine query data. The aim of this paper is to show which NLP and ML techniques are suitable for the task of identifying semantic relations between diseases and treatments in short biomedical texts. The value of our work stands in the results we obtain and the new feature representation techniques. 2 Related Work The most relevant work for our study is the work of Rosario and Hearst (2004). The authors of this paper are the ones that created and distributed the data set used in our research. The data set is annotated with disease and treatments entities and with 8 semantic relations between diseases and treatments. The main focus of their work is on entity recognition – the task of identifying entities, diseases and treatments in biomedical text sentences. The authors use Hidden Markov Models and maximum entropy models to perform both the task of entity recognition and of relation discrimination. Their representation techniques are based on words in context, partof-speech infor</context>
<context position="4690" citStr="Rosario and Hearst (2004)" startWordPosition="732" endWordPosition="735">btained when the entities involved in the relations were identified and used as features. The biomedical literature contains a wealth of work on semantic relation extraction, mostly focused on more biology-specific tasks: subcellular-location (Craven 1999), gene-disorder association (Ray and Craven 2001), and diseases and drugs relations (Srinivasan and Rindflesch 2002, Ahlers et al., 2007). Text classification techniques combined with a Naïve Bayes classifier and relational learning algorithms are methods used by Craven (1999). Hidden Markov Models are used in Craven (2001), but similarly to Rosario and Hearst (2004), the research focus was entity recognition. A context based approach using MeSH term co-occurrences are used by Srinivasan and Rindflesch (2002) for relationship discrimination between diseases and drugs. A lot of work is focused on building rules used to extract relation. Feldman et al. (2002) use a rule-based system to extract relations that are focused on genes, proteins, drugs, and diseases. Friedman et al. (2001) go deeper into building a rule-based system by hand-crafting a semantic grammar and a set of semantic constraints in order to recognize a range of biological and molecular relat</context>
<context position="6659" citStr="Rosario and Hearst (2004)" startWordPosition="1076" endWordPosition="1079"> data is a factor that influences the performance; support for this stands not only in the related work performed on the same data set, but in the research literature as well. The aim of this paper is to focus on few relations of interest and try to identify what predictive model and what representation techniques bring the best results of identifying semantic relations in short biomedical texts. We mostly focused on the value that the research can bring, rather than on an incremental research. As mentioned in the previous section, the data set that we use to run our experiments is the one of Rosario and Hearst (2004). The entire data set is collected from Medline2 2001 abstracts. Sentences from titles and abstracts are annotated with entities and with 8 relations, based only on the information present in a certain sentence. The first 100 titles and 40 abstracts from each of the 59 Medline 2001 files were used for annotation. Table 1, presents the original data set, as published in previous research. The numbers in parenthesis represent the training and test set sizes. Relationship Definition and Example Cure TREAT cures DIS 810 (648, 162) Intravenous immune globulin for recurrent spontaneous abortion Only</context>
<context position="24589" citStr="Rosario and Hearst (2004)" startWordPosition="3985" endWordPosition="3988">utperform. One of the major contributions of this work is the fact that the current experiments show that additional information used in the representation settings brings improvements for the task. The task itself is a knowledge-charged task and the experiments show that classifiers can perform better when richer information (e.g. concepts for medical ontologies) is provided. 6.1 Comparison to previous work Even though our main focus is on the three relations mentioned earlier, in order to validate our methodology, we also performed the 8-class classification task, similar to the one done by Rosario and Hearst (2004). Figure 3 presents a graphical comparison of the results of our methods to the ones obtained in the previous work. We report accuracy values for these experiments, as it was done in the previous work. In Figure 3, the first set of bar-results represents the best individual results for each relation. The representation technique and classification model that obtains the best results are the ones described on the x-axis. Results - Setting 2 F-measure 100.00% 98.00% 96.00% 94.00% 92.00% 90.00% 88.00% 86.00% 84.00% 82.00% Cure - BOW + NLP + Biomed+ UMLS - NB Prevent - BOW + NLP + Biomed+ UMLS - N</context>
<context position="25833" citStr="Rosario and Hearst (2004)" startWordPosition="4209" endWordPosition="4213">+ NLP + Biomed+ UMLS - CNB 98.55% 100% 88.89% Results - Setting 3 F-measure Cure - BOW + Prevent - SideEffect - NLP + BOW + NLP + BOW + NLP + Biomed+ Biomed+ Biomed+ UMLS - NB UMLS - NB UMLS - CNB 100.00% 95.00% 90.00% 85.00% 80.00% 98.55% 100% 88.89% 96 Figure 3. Comparison of results. The second series of results represents the overall best model that is reported for each relation. The model reported here is a combination of BOW, verb and noun-phrases, biomedical and UMLS concepts, with a CNB classifier. The third series of results represent the accuracy results obtained in previous work by Rosario and Hearst (2004). As we can see from the figure, the best individual models have a major improvement over previous results. When a single model is used for all relations, our results improve the previous ones in four relations with the difference varying from: 3 percentage point difference (Cure) to 23 percentage point difference (Prevent). We obtain the same results for two semantic relations, No_Cure and Vague and we believe that this is the case due to the fact that these two classes are significantly underrepresented compared to the other ones involved in the task. For the Treatment_Only relation our resu</context>
</contexts>
<marker>Rosario, Hearst, 2004</marker>
<rawString>Rosario B. and Marti A. Hearst. 2004. Classifying semantic relations in bioscience text. Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, 430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Koppel</author>
<author>J Schler</author>
</authors>
<title>Using Neutral Examples for Learning Polarity,</title>
<date>2005</date>
<booktitle>Proceedings of IJCAI,</booktitle>
<location>Edinburgh, Scotland.</location>
<contexts>
<context position="11087" citStr="Koppel and Schler (2005)" startWordPosition="1807" endWordPosition="1810">n that can distinguish sentences that contain the relation from sentences that do not contain any relevant information. This setting is similar to a two-class classification task in which instances are labeled either with the relation in question – Positive label, or with non-relevant information – Negative label; Setting 3: build one model that distinguishes the three relations – a three-way classification task where each sentence is labeled with one of the semantic relations, using the data with all the Positive labels. The first set of experiments is influenced by previous research done by Koppel and Schler (2005). The authors claim that for polarity learning “neutral” examples help the learning algorithms to better identify the two polarities. Their research was done on a corpus of posts to chat groups devoted to popular U.S. television and posts to shopping.com’s product evaluation page. As classification algorithms, a set of 6 representative models: decision-based models (Decision trees – J48), probabilistic models (Naïve Bayes and complement Naïve Bayes (CNB), which is adapted for imbalanced class distribution), adaptive learning (AdaBoost), linear classifier (support vector machine (SVM) with poly</context>
</contexts>
<marker>Koppel, Schler, 2005</marker>
<rawString>Koppel M. and J. Schler. 2005. Using Neutral Examples for Learning Polarity, Proceedings of IJCAI, Edinburgh, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Srinivasan</author>
<author>T Rindflesch</author>
</authors>
<title>Exploring text mining from Medline.</title>
<date>2002</date>
<booktitle>Proceedings of the AMIA Symposium.</booktitle>
<contexts>
<context position="4436" citStr="Srinivasan and Rindflesch 2002" startWordPosition="694" endWordPosition="697"> Linguistics on different representation techniques, different classification models, and most importantly in obtaining improved results without using the annotations of the entities (new data will not have them). In previous research, the best results were obtained when the entities involved in the relations were identified and used as features. The biomedical literature contains a wealth of work on semantic relation extraction, mostly focused on more biology-specific tasks: subcellular-location (Craven 1999), gene-disorder association (Ray and Craven 2001), and diseases and drugs relations (Srinivasan and Rindflesch 2002, Ahlers et al., 2007). Text classification techniques combined with a Naïve Bayes classifier and relational learning algorithms are methods used by Craven (1999). Hidden Markov Models are used in Craven (2001), but similarly to Rosario and Hearst (2004), the research focus was entity recognition. A context based approach using MeSH term co-occurrences are used by Srinivasan and Rindflesch (2002) for relationship discrimination between diseases and drugs. A lot of work is focused on building rules used to extract relation. Feldman et al. (2002) use a rule-based system to extract relations that</context>
</contexts>
<marker>Srinivasan, Rindflesch, 2002</marker>
<rawString>Srinivasan P. and T. Rindflesch 2002. Exploring text mining from Medline. Proceedings of the AMIA Symposium.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>