<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.076240">
<title confidence="0.996268">
´UFAL: Using Hand-crafted Rules in Aspect Based Sentiment Analysis
on Parsed Data
</title>
<author confidence="0.98585">
Kateˇrina Veselovsk´a, Aleˇs Tamchyna
</author>
<affiliation confidence="0.9811025">
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
</affiliation>
<address confidence="0.751967">
Malostransk´e n´amˇest´ı 25, Prague, Czech Republic
</address>
<email confidence="0.998897">
{veselovska,tamchyna}@ufal.mff.cuni.cz
</email>
<sectionHeader confidence="0.997388" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999829666666667">
This paper describes our submission to Se-
mEval 2014 Task 41 (aspect based senti-
ment analysis). The current work is based
on the assumption that it could be advan-
tageous to connect the subtasks into one
workflow, not necessarily following their
given order. We took part in all four sub-
tasks (aspect term extraction, aspect term
polarity, aspect category detection, aspect
category polarity), using polarity items de-
tection via various subjectivity lexicons
and employing a rule-based system ap-
plied on dependency data. To determine
aspect categories, we simply look up their
WordNet hypernyms. For such a basic
method using no machine learning tech-
niques, we consider the results rather sat-
isfactory.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999919857142857">
In a real-life scenario, we usually do not have any
golden aspects at our disposal. Therefore, it could
be practical to be able to extract both aspects and
their polarities at once. So we first parse the data,
bearing in mind that it is very difficult to detect
both sources/targets and their aspects on plain text
corpora. This holds especially for pro-drop lan-
guages, e.g. Czech (Veselovsk´a et al., 2014) but
the proposed method is still language independent
to some extent. Secondly, we detect the polar-
ity items in the parsed text using a union of two
different existing subjectivity lexicons (see Sec-
tion 2). Afterwards, we extract the aspect terms in
the dependency structures containing polarity ex-
</bodyText>
<footnote confidence="0.940859833333333">
1http://alt.qcri.org/semeval2014/
task4/
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
</footnote>
<bodyText confidence="0.999352875">
pressions. In this task, we employ several hand-
crafted rules detecting aspects based on syntactic
features of the evaluative sentences, inspired by
the method by Qiu et al. (2011). Finally, we iden-
tify aspect term categories with the help of the En-
glish WordNet and derive their polarities based on
the polarities of individual aspects. The obtained
results are discussed in Section 4.
</bodyText>
<sectionHeader confidence="0.999918" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99997872">
This work is related to polarity detection based on
a list of evaluative items, i.e. subjectivity lexi-
cons, generally described e.g. in Taboada et al.
(2011). The English ones we use are minutely de-
scribed in Wiebe et al. (2005) and several papers
by Bing Liu, starting with Hu and Liu (2004). In-
spired by Kobayashi et al. (2007), who make use
of evaluative expressions when learning syntac-
tic patterns obtained via pattern mining to extract
aspect-evaluation pairs, we use the opinion words
to detect evaluative structures in parsed data. The
issue of target extraction in sentiment analysis is
discussed in articles proposing different methods,
mainly tested on product review datasets (Popescu
and Etzioni, 2005; Mei et al., 2007; Scaffidi et al.,
2007). Some of the authors take into consideration
also product aspects (features), defined as prod-
uct components or product attributes (Liu, 2006).
Hu and Liu (2004) take as the feature candidates
all noun phrases found in the text. Stoyanov and
Cardie (2008) see the problem of target extraction
as part of a topic modelling problem, similarly to
Mei et al. (2007). In this contribution, we follow
the work of Qiu et al. (2011) who learn syntactic
relations from dependency trees.
</bodyText>
<sectionHeader confidence="0.999577" genericHeader="method">
3 Pipeline
</sectionHeader>
<bodyText confidence="0.990009">
Our workflow is illustrated in Figure 1. We first
pre-process the data, then mark all aspects seen in
the training data (still on plain text). The rest of
the pipeline is implemented in Treex (Popel and
</bodyText>
<page confidence="0.980193">
694
</page>
<note confidence="0.759963">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 694–698,
Dublin, Ireland, August 23-24, 2014.
</note>
<table confidence="0.999741">
Pattern Example sentence
Subjaspect Predcopula PAdj The food was great.
Subjaspect Predcopula PNoun The coconut juice is the MUST!
Subjaspect Pred Adveval The pizza tastes so good.
Attreval Nounaspect Nice value.
Subjaspect Predeval Their wine sucks.
Subjsource Predeval Objaspect I liked the beer selection.
</table>
<tableCaption confidence="0.997978">
Table 1: Syntactic rules.
</tableCaption>
<figureCaption confidence="0.972848">
Figure 1: Overall schema of our approach.
</figureCaption>
<bodyText confidence="0.999476833333333">
ˇZabokrtsk´y, 2010) and consists of linguistic anal-
ysis (tagging, dependency parsing), identification
of evaluative words, and application of syntactic
rules to find the evaluated aspects. Finally, for
restaurants, we also identify aspect categories and
their polarity.
</bodyText>
<subsectionHeader confidence="0.998563">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.999844">
We used the training and trial data provided by the
organizers. During system development, we used
the trial section as a held-out set. In the final sub-
mission, both datasets are utilized in training.
</bodyText>
<subsectionHeader confidence="0.999968">
3.2 Pre-processing
</subsectionHeader>
<bodyText confidence="0.992275142857143">
The main phase of pre-processing (apart from
parsing the input files and other simple tasks) is
running a spell-checker. As data for this task
comes from real-world reviews, it contains various
typos and other small errors. We therefore imple-
mented a statistical spell-checker which works in
two stages:
</bodyText>
<listItem confidence="0.99687825">
1. Run Aspell2 to detect typos and obtain sug-
gestions for them.
2. Select the appropriate suggestions using a
language model (LM).
</listItem>
<bodyText confidence="0.9998991">
We trained a trigram LM from the English side
of CzEng 1.0 (Bojar et al., 2012) using SRILM
(Stolcke, 2002). We binarized the LM and use
the Lazy decoder (Heafield et al., 2013) for select-
ing the suggestions that best fit the current context.
Our script is freely available for download.3
We created a list of exceptions (domain-specific
words, such as “netbook”, are unknown to As-
pell’s dictionary) which should not be corrected
and also skip named entities in spell-checking.
</bodyText>
<subsectionHeader confidence="0.999933">
3.3 Marking Known Aspects
</subsectionHeader>
<bodyText confidence="0.999996">
Before any linguistic processing, we mark all
words (and multiword expressions) which are
marked as aspects in the training data. For our fi-
nal submission, the list also includes aspects from
the provided development sets.
</bodyText>
<subsectionHeader confidence="0.999828">
3.4 Morphological Analysis and Parsing
</subsectionHeader>
<bodyText confidence="0.999961625">
Further, we lemmatize the data and parse it using
Treex (Popel and ˇZabokrtsk´y, 2010), a modular
framework for natural language processing (NLP).
Treex is focused primarily on dependency syntax
and includes blocks (wrappers) for taggers, parsers
and other NLP tools. Within Treex, we used the
Morˇce tagger (Hajiˇc et al., 2007) and the MST de-
pendency parser (McDonald et al., 2005).
</bodyText>
<subsectionHeader confidence="0.995399">
3.5 Finding Evaluative Words
</subsectionHeader>
<bodyText confidence="0.999907333333333">
In the obtained dependency data, we detect polar-
ity items using MPQA subjectivity lexicon (Wiebe
et al., 2005) and Bing Liu’s subjectivity clues.4
</bodyText>
<footnote confidence="0.9354332">
2http://aspell.net/
3https://redmine.ms.mff.cuni.cz/
projects/staspell
4http://www.cs.uic.edu/˜liub/FBS/
sentiment-analysis.html#lexicon
</footnote>
<figure confidence="0.904582">
Run tagger &amp; parser
Mark evaluative words
Treex
Apply syntactic rules
Mark aspect categories
Pre-process &amp; spellcheck
Plain text
Mark known aspects
</figure>
<page confidence="0.932153">
695
</page>
<table confidence="0.996199">
Task 1: aspect extraction Task 2: aspect polarity Task 3: category detection Task 4: category polarity
prec recall F-measure accuracy prec recall F-measure accuracy
UFAL 0.50 0.72 0.59 0.67 0.57 0.74 0.65 0.63
best 0.91 0.82 0.84 0.81 0.91 0.86 0.88 0.83
</table>
<tableCaption confidence="0.97176">
Table 2: Results of our system on the Restaurants dataset as evaluated by the task organizers.
</tableCaption>
<table confidence="0.995504">
Task 1: aspect extraction Task 2: aspect polarity
prec recall F-measure accuracy
UFAL 0.39 0.66 0.49 0.57
best 0.85 0.67 0.75 0.70
</table>
<tableCaption confidence="0.999934">
Table 3: Results of our system on the Laptops dataset as evaluated by the task organizers.
</tableCaption>
<bodyText confidence="0.999871333333333">
We lemmatize both lexicons and look first for
matching surface forms, then for matching lem-
mas. (English lemmas as output by Morˇce are
sometimes too coarse, eliminating e.g. negation
– we can mostly avoid their matching by looking
at surface forms first.)
</bodyText>
<subsectionHeader confidence="0.976645">
3.6 Syntactic Rules
</subsectionHeader>
<bodyText confidence="0.998832869565218">
Further, we created six basic rules for finding
aspects in sentences containing evaluative items
from the lexicons, e.g. “If you find an adjective
which is a part of a verbonominal predicate, the
subject of its governing verb should be an aspect.”,
see Table 1. Situational functions are marked with
subscript, PAdj and PNoun stand for adjectival and
nominal predicative expressions.
Moreover, we applied three more rules con-
cerning coordinations. We suppose that if we find
an aspect, every member of a given coordination
must be an aspect too.
The excellent mussels, puff pastry, goat cheese
and salad.
Concerning but-clauses, we expect that if
there is no other aspect in the second part of
the sentence, we assign the conflict value to the
identified aspect.
The food was pretty good, but a little flavorless.
If there are two aspects identified in the
but-coordination, they should be marked with
opposite polarity.
The place is cramped, but the food is fantastic!
</bodyText>
<subsectionHeader confidence="0.993611">
3.7 Aspect Categories
</subsectionHeader>
<bodyText confidence="0.999962882352941">
We collect a list of aspects from the training data
and find all their hypernyms in WordNet (Fell-
baum, 1998). We hand-craft a list of typical hy-
pernyms for each category (such as “cooking” or
“consumption” for the category “food”). More-
over, we look at the most frequent aspects in the
training data and add as exceptions those for which
our list would fail.
We rely on the output of aspect identification
for this subtask. For each aspect marked in the
sentence, we look up all its hypernyms in Word-
Net and compare them to our list. When we find
a known hypernym, we assign its category to the
aspect. Otherwise, we put the aspect in the “anec-
dotes/miscellaneous” category. For category po-
larity assignment, we combine the polarities of all
aspects in that category in the following way:
</bodyText>
<listItem confidence="0.999981">
• all positive → positive
• all negative → negative
• all neutral → neutral
• otherwise → conflict
</listItem>
<sectionHeader confidence="0.960696" genericHeader="evaluation">
4 Results and Discussion
</sectionHeader>
<bodyText confidence="0.99978375">
Table 2 and Table 3 summarize the results of our
submission. We do not achieve the best perfor-
mance in any particular task, our system overall
ranked in the middle.
We tend to do better in terms of recall than pre-
cision. This effect is mainly caused by our deci-
sion to also automatically mark all aspects seen in
the training data.
</bodyText>
<subsectionHeader confidence="0.99099">
4.1 Effect of the Spell-checker
</subsectionHeader>
<bodyText confidence="0.99887">
We evaluated the performance of our system with
and without the spell-checker. Overall, the impact
</bodyText>
<page confidence="0.998083">
696
</page>
<bodyText confidence="0.981816714285714">
is very small (f-measure stays within 2-decimal
rounding error). In some cases its corrections are
useful (“convienent” → “convenient parking”),
sometimes its limited vocabulary harms our sys-
tem (“fettucino alfredo” → “fitting Alfred”). This
issue could be mitigated by providing a custom
lexicon to Aspell.
</bodyText>
<subsectionHeader confidence="0.999757">
4.2 Sources of Errors
</subsectionHeader>
<bodyText confidence="0.99985675">
As we always extract aspects that were observed in
the training data, our system often marks them in
non-evaluative contexts, leading to a considerable
number of false positives. However, using this ap-
proach improves our f-measure score due to the
limited recall of the syntactic rules.
The usefulness of our rules is mainly limited by
the (i) sentiment lexicons and (ii) parsing errors.
</bodyText>
<listItem confidence="0.593789">
(i) Since we used the lexicons directly without
domain adaptation, many domain-specific terms
are missed (“flavorless”, “crowded”) and some are
matched incorrectly.
(ii) Parsing errors often confuse the rules and
negatively impact both recall and precision. Of-
</listItem>
<bodyText confidence="0.989833333333333">
ten, they prevented the system from taking nega-
tion into account, so some of the negated polarity
items were assigned incorrectly.
The “conflict” polarity value was rarely correct
– all aspects and their polarity values need to be
correctly discovered to assign this value. How-
ever, this type of polarity is infrequent in the data,
so the overall impact is small.
Having participated in all four tasks, our sys-
tem can be readily deployed as a complete solution
which covers the whole process from plain text to
aspects and aspect categories annotated with po-
larity. Considering the number of tasks covered
and the fact that our system is entirely rule-based,
the achieved results seem satisfactory.
</bodyText>
<sectionHeader confidence="0.986993" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999735083333333">
In our work, we developed a purely rule-based sys-
tem for aspect based sentiment analysis which can
both detect aspect terms (and categories) and as-
sign polarity values to them. We have shown that
even such a simple approach can achieve relatively
good results.
In the future, our main plan is to involve ma-
chine learning in our system. We expect that out-
puts of our rules can serve as useful indicator fea-
tures for a discriminative learning model, along
with standard features such as bag-of-words (lem-
mas) or n-grams.
</bodyText>
<sectionHeader confidence="0.993001" genericHeader="acknowledgments">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.9995335">
The research described herein has been supported
by the by SVV project number 260 140 and by the
LINDAT/CLARIN project funded by the Ministry
of Education, Youth and Sports of the Czech Re-
public, project No. LM2010013.
This work has been using language resources
developed and/or stored and/or distributed by the
LINDAT/CLARIN project of the Ministry of Ed-
ucation, Youth and Sports of the Czech Republic
(project LM2010013).
</bodyText>
<sectionHeader confidence="0.999391" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998937441176471">
Ond&amp;quot;rej Bojar, Zden&amp;quot;ek &amp;quot;Zabokrtsk´y, Ond&amp;quot;rej Du&amp;quot;sek, Pe-
tra Galu&amp;quot;s&amp;quot;c´akov´a, Martin Majli&amp;quot;s, David Mare&amp;quot;cek, Ji&amp;quot;rfMar&amp;quot;sfk, Michal Nov´ak, Martin Popel, and Ale&amp;quot;s Tam-
chyna. 2012. The Joy of Parallelism with CzEng
1.0. In Proc. of LREC, pages 3921–3928. ELRA.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
Jan Haji&amp;quot;c, Jan Votrubec, Pavel Krbec, Pavel Kv&amp;quot;eto&amp;quot;n,
et al. 2007. The best of two worlds: Cooperation of
statistical and rule-based taggers for czech. In Pro-
ceedings of the Workshop on Balto-Slavonic Natural
Language Processing: Information Extraction and
Enabling Technologies, pages 67–74.
Kenneth Heafield, Philipp Koehn, and Alon Lavie.
2013. Grouping language model boundary words to
speed k-best extraction from hypergraphs. In Pro-
ceedings of the 2013 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
958–968, Atlanta, Georgia, USA, June.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the Tenth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, KDD ’04, pages
168–177, New York, NY, USA. ACM.
Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto.
2007. Extracting aspect-evaluation and aspect-of re-
lations in opinion mining. In Proceedings of the
2007 Joint Conference on Empirical Methods in
Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL).
Bing Liu. 2006. Web Data Mining: Exploring Hyper-
links, Contents, and Usage Data (Data-Centric Sys-
tems and Applications). Springer-Verlag New York,
Inc., Secaucus, NJ, USA.
</reference>
<page confidence="0.989748">
697
</page>
<reference confidence="0.999046648148148">
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Haji&amp;quot;c. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of the conference on Human Language Technology
and Empirical Methods in Natural Language Pro-
cessing, pages 523–530.
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su,
and ChengXiang Zhai. 2007. Topic sentiment mix-
ture: Modeling facets and opinions in weblogs. In
Proceedings of the 16th International Conference on
World Wide Web, WWW ’07, pages 171–180, New
York, NY, USA. ACM.
Martin Popel and Zden&amp;quot;ek &amp;quot;Zabokrtsk´y. 2010. Tec-
toMT: Modular NLP Framework. In Hrafn Lofts-
son, Eirikur R¨ognvaldsson, and Sigrun Helgadottir,
editors, IceTAL 2010, volume 6233 of Lecture Notes
in Computer Science, pages 293–304. Iceland Cen-
tre for Language Technology (ICLT), Springer.
Ana-Maria Popescu and Oren Etzioni. 2005. Ex-
tracting product features and opinions from reviews.
In Proceedings of the Conference on Human Lan-
guage Technology and Empirical Methods in Natu-
ral Language Processing, HLT ’05, pages 339–346,
Stroudsburg, PA, USA.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2011. Opinion word expansion and target extrac-
tion through double propagation. Comput. Linguist.,
37(1):9–27, March.
Christopher Scaffidi, Kevin Bierhoff, Eric Chang,
Mikhael Felker, Herman Ng, and Chun Jin. 2007.
Red opal: Product-feature scoring from reviews. In
Proceedings of the 8th ACM Conference on Elec-
tronic Commerce, EC ’07, pages 182–191, New
York, NY, USA. ACM.
Andreas Stolcke. 2002. SRILM – An Extensible Lan-
guage Modeling Toolkit. In Proc. Intl. Conf. on
Spoken Language Processing, volume 2, pages 901–
904.
Veselin Stoyanov and Claire Cardie. 2008. Topic
identification for fine-grained opinion analysis. In
Proceedings of the 22Nd International Conference
on Computational Linguistics - Volume 1, COLING
’08, pages 817–824, Stroudsburg, PA, USA.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Comput. Lin-
guist., 37(2):267–307, June.
Kate&amp;quot;rina Veselovsk´a, Jan Mas&amp;quot;ek, and Vladislav Kubo&amp;quot;n.
2014. Sentiment detection and annotation in a tree-
bank.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language resources and evalua-
tion, 39(2-3):165–210.
</reference>
<page confidence="0.997279">
698
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.296272">
<title confidence="0.9954145">UFAL: Using Hand-crafted Rules in Aspect Based Sentiment on Parsed Data</title>
<author confidence="0.487699">Kateˇrina Veselovsk´a</author>
<author confidence="0.487699">Aleˇs</author>
<affiliation confidence="0.8327555">Charles University in Prague, Faculty of Mathematics and Institute of Formal and Applied</affiliation>
<address confidence="0.706294">Malostransk´e n´amˇest´ı 25, Prague, Czech</address>
<abstract confidence="0.989536052631579">This paper describes our submission to Se- 2014 Task (aspect based sentiment analysis). The current work is based on the assumption that it could be advantageous to connect the subtasks into one workflow, not necessarily following their given order. We took part in all four subtasks (aspect term extraction, aspect term polarity, aspect category detection, aspect category polarity), using polarity items detection via various subjectivity lexicons and employing a rule-based system applied on dependency data. To determine aspect categories, we simply look up their WordNet hypernyms. For such a basic method using no machine learning techniques, we consider the results rather satisfactory.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ondrej Bojar</author>
<author>Zdenek Zabokrtsk´y</author>
<author>Ondrej Dusek</author>
<author>Petra Galusc´akov´a</author>
<author>Martin Majlis</author>
<author>David Marecek</author>
<author>Michal Nov´ak JirfMarsfk</author>
<author>Martin Popel</author>
<author>Ales Tamchyna</author>
</authors>
<date>2012</date>
<booktitle>The Joy of Parallelism with CzEng 1.0. In Proc. of LREC,</booktitle>
<pages>3921--3928</pages>
<publisher>ELRA.</publisher>
<marker>Bojar, Zabokrtsk´y, Dusek, Galusc´akov´a, Majlis, Marecek, JirfMarsfk, Popel, Tamchyna, 2012</marker>
<rawString>Ond&amp;quot;rej Bojar, Zden&amp;quot;ek &amp;quot;Zabokrtsk´y, Ond&amp;quot;rej Du&amp;quot;sek, Petra Galu&amp;quot;s&amp;quot;c´akov´a, Martin Majli&amp;quot;s, David Mare&amp;quot;cek, Ji&amp;quot;rfMar&amp;quot;sfk, Michal Nov´ak, Martin Popel, and Ale&amp;quot;s Tamchyna. 2012. The Joy of Parallelism with CzEng 1.0. In Proc. of LREC, pages 3921–3928. ELRA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>Bradford Books.</publisher>
<contexts>
<context position="8870" citStr="Fellbaum, 1998" startWordPosition="1382" endWordPosition="1384"> every member of a given coordination must be an aspect too. The excellent mussels, puff pastry, goat cheese and salad. Concerning but-clauses, we expect that if there is no other aspect in the second part of the sentence, we assign the conflict value to the identified aspect. The food was pretty good, but a little flavorless. If there are two aspects identified in the but-coordination, they should be marked with opposite polarity. The place is cramped, but the food is fantastic! 3.7 Aspect Categories We collect a list of aspects from the training data and find all their hypernyms in WordNet (Fellbaum, 1998). We hand-craft a list of typical hypernyms for each category (such as “cooking” or “consumption” for the category “food”). Moreover, we look at the most frequent aspects in the training data and add as exceptions those for which our list would fail. We rely on the output of aspect identification for this subtask. For each aspect marked in the sentence, we look up all its hypernyms in WordNet and compare them to our list. When we find a known hypernym, we assign its category to the aspect. Otherwise, we put the aspect in the “anecdotes/miscellaneous” category. For category polarity assignment,</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. Bradford Books.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajic</author>
<author>Jan Votrubec</author>
<author>Pavel Krbec</author>
<author>Pavel Kveton</author>
</authors>
<title>The best of two worlds: Cooperation of statistical and rule-based taggers for czech.</title>
<date>2007</date>
<booktitle>In Proceedings of the Workshop on Balto-Slavonic Natural Language Processing: Information Extraction and Enabling Technologies,</booktitle>
<pages>67--74</pages>
<marker>Hajic, Votrubec, Krbec, Kveton, 2007</marker>
<rawString>Jan Haji&amp;quot;c, Jan Votrubec, Pavel Krbec, Pavel Kv&amp;quot;eto&amp;quot;n, et al. 2007. The best of two worlds: Cooperation of statistical and rule-based taggers for czech. In Proceedings of the Workshop on Balto-Slavonic Natural Language Processing: Information Extraction and Enabling Technologies, pages 67–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
<author>Philipp Koehn</author>
<author>Alon Lavie</author>
</authors>
<title>Grouping language model boundary words to speed k-best extraction from hypergraphs.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>958--968</pages>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="5482" citStr="Heafield et al., 2013" startWordPosition="852" endWordPosition="855">rocessing The main phase of pre-processing (apart from parsing the input files and other simple tasks) is running a spell-checker. As data for this task comes from real-world reviews, it contains various typos and other small errors. We therefore implemented a statistical spell-checker which works in two stages: 1. Run Aspell2 to detect typos and obtain suggestions for them. 2. Select the appropriate suggestions using a language model (LM). We trained a trigram LM from the English side of CzEng 1.0 (Bojar et al., 2012) using SRILM (Stolcke, 2002). We binarized the LM and use the Lazy decoder (Heafield et al., 2013) for selecting the suggestions that best fit the current context. Our script is freely available for download.3 We created a list of exceptions (domain-specific words, such as “netbook”, are unknown to Aspell’s dictionary) which should not be corrected and also skip named entities in spell-checking. 3.3 Marking Known Aspects Before any linguistic processing, we mark all words (and multiword expressions) which are marked as aspects in the training data. For our final submission, the list also includes aspects from the provided development sets. 3.4 Morphological Analysis and Parsing Further, we</context>
</contexts>
<marker>Heafield, Koehn, Lavie, 2013</marker>
<rawString>Kenneth Heafield, Philipp Koehn, and Alon Lavie. 2013. Grouping language model boundary words to speed k-best extraction from hypergraphs. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 958–968, Atlanta, Georgia, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’04,</booktitle>
<pages>168--177</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="2697" citStr="Hu and Liu (2004)" startWordPosition="413" endWordPosition="416">ts based on syntactic features of the evaluative sentences, inspired by the method by Qiu et al. (2011). Finally, we identify aspect term categories with the help of the English WordNet and derive their polarities based on the polarities of individual aspects. The obtained results are discussed in Section 4. 2 Related Work This work is related to polarity detection based on a list of evaluative items, i.e. subjectivity lexicons, generally described e.g. in Taboada et al. (2011). The English ones we use are minutely described in Wiebe et al. (2005) and several papers by Bing Liu, starting with Hu and Liu (2004). Inspired by Kobayashi et al. (2007), who make use of evaluative expressions when learning syntactic patterns obtained via pattern mining to extract aspect-evaluation pairs, we use the opinion words to detect evaluative structures in parsed data. The issue of target extraction in sentiment analysis is discussed in articles proposing different methods, mainly tested on product review datasets (Popescu and Etzioni, 2005; Mei et al., 2007; Scaffidi et al., 2007). Some of the authors take into consideration also product aspects (features), defined as product components or product attributes (Liu,</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’04, pages 168–177, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nozomi Kobayashi</author>
<author>Kentaro Inui</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Extracting aspect-evaluation and aspect-of relations in opinion mining.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL).</booktitle>
<contexts>
<context position="2734" citStr="Kobayashi et al. (2007)" startWordPosition="420" endWordPosition="423">of the evaluative sentences, inspired by the method by Qiu et al. (2011). Finally, we identify aspect term categories with the help of the English WordNet and derive their polarities based on the polarities of individual aspects. The obtained results are discussed in Section 4. 2 Related Work This work is related to polarity detection based on a list of evaluative items, i.e. subjectivity lexicons, generally described e.g. in Taboada et al. (2011). The English ones we use are minutely described in Wiebe et al. (2005) and several papers by Bing Liu, starting with Hu and Liu (2004). Inspired by Kobayashi et al. (2007), who make use of evaluative expressions when learning syntactic patterns obtained via pattern mining to extract aspect-evaluation pairs, we use the opinion words to detect evaluative structures in parsed data. The issue of target extraction in sentiment analysis is discussed in articles proposing different methods, mainly tested on product review datasets (Popescu and Etzioni, 2005; Mei et al., 2007; Scaffidi et al., 2007). Some of the authors take into consideration also product aspects (features), defined as product components or product attributes (Liu, 2006). Hu and Liu (2004) take as the</context>
</contexts>
<marker>Kobayashi, Inui, Matsumoto, 2007</marker>
<rawString>Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto. 2007. Extracting aspect-evaluation and aspect-of relations in opinion mining. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
</authors>
<title>Web Data Mining: Exploring Hyperlinks, Contents, and Usage Data (Data-Centric Systems and Applications).</title>
<date>2006</date>
<publisher>Springer-Verlag</publisher>
<location>New York, Inc., Secaucus, NJ, USA.</location>
<contexts>
<context position="3303" citStr="Liu, 2006" startWordPosition="507" endWordPosition="508">004). Inspired by Kobayashi et al. (2007), who make use of evaluative expressions when learning syntactic patterns obtained via pattern mining to extract aspect-evaluation pairs, we use the opinion words to detect evaluative structures in parsed data. The issue of target extraction in sentiment analysis is discussed in articles proposing different methods, mainly tested on product review datasets (Popescu and Etzioni, 2005; Mei et al., 2007; Scaffidi et al., 2007). Some of the authors take into consideration also product aspects (features), defined as product components or product attributes (Liu, 2006). Hu and Liu (2004) take as the feature candidates all noun phrases found in the text. Stoyanov and Cardie (2008) see the problem of target extraction as part of a topic modelling problem, similarly to Mei et al. (2007). In this contribution, we follow the work of Qiu et al. (2011) who learn syntactic relations from dependency trees. 3 Pipeline Our workflow is illustrated in Figure 1. We first pre-process the data, then mark all aspects seen in the training data (still on plain text). The rest of the pipeline is implemented in Treex (Popel and 694 Proceedings of the 8th International Workshop </context>
</contexts>
<marker>Liu, 2006</marker>
<rawString>Bing Liu. 2006. Web Data Mining: Exploring Hyperlinks, Contents, and Usage Data (Data-Centric Systems and Applications). Springer-Verlag New York, Inc., Secaucus, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
<author>Kiril Ribarov</author>
<author>Jan Hajic</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>523--530</pages>
<contexts>
<context position="6454" citStr="McDonald et al., 2005" startWordPosition="1004" endWordPosition="1007">rocessing, we mark all words (and multiword expressions) which are marked as aspects in the training data. For our final submission, the list also includes aspects from the provided development sets. 3.4 Morphological Analysis and Parsing Further, we lemmatize the data and parse it using Treex (Popel and ˇZabokrtsk´y, 2010), a modular framework for natural language processing (NLP). Treex is focused primarily on dependency syntax and includes blocks (wrappers) for taggers, parsers and other NLP tools. Within Treex, we used the Morˇce tagger (Hajiˇc et al., 2007) and the MST dependency parser (McDonald et al., 2005). 3.5 Finding Evaluative Words In the obtained dependency data, we detect polarity items using MPQA subjectivity lexicon (Wiebe et al., 2005) and Bing Liu’s subjectivity clues.4 2http://aspell.net/ 3https://redmine.ms.mff.cuni.cz/ projects/staspell 4http://www.cs.uic.edu/˜liub/FBS/ sentiment-analysis.html#lexicon Run tagger &amp; parser Mark evaluative words Treex Apply syntactic rules Mark aspect categories Pre-process &amp; spellcheck Plain text Mark known aspects 695 Task 1: aspect extraction Task 2: aspect polarity Task 3: category detection Task 4: category polarity prec recall F-measure accuracy</context>
</contexts>
<marker>McDonald, Pereira, Ribarov, Hajic, 2005</marker>
<rawString>Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Haji&amp;quot;c. 2005. Non-projective dependency parsing using spanning tree algorithms. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 523–530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiaozhu Mei</author>
<author>Xu Ling</author>
<author>Matthew Wondra</author>
<author>Hang Su</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Topic sentiment mixture: Modeling facets and opinions in weblogs.</title>
<date>2007</date>
<booktitle>In Proceedings of the 16th International Conference on World Wide Web, WWW ’07,</booktitle>
<pages>171--180</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="3137" citStr="Mei et al., 2007" startWordPosition="480" endWordPosition="483">y described e.g. in Taboada et al. (2011). The English ones we use are minutely described in Wiebe et al. (2005) and several papers by Bing Liu, starting with Hu and Liu (2004). Inspired by Kobayashi et al. (2007), who make use of evaluative expressions when learning syntactic patterns obtained via pattern mining to extract aspect-evaluation pairs, we use the opinion words to detect evaluative structures in parsed data. The issue of target extraction in sentiment analysis is discussed in articles proposing different methods, mainly tested on product review datasets (Popescu and Etzioni, 2005; Mei et al., 2007; Scaffidi et al., 2007). Some of the authors take into consideration also product aspects (features), defined as product components or product attributes (Liu, 2006). Hu and Liu (2004) take as the feature candidates all noun phrases found in the text. Stoyanov and Cardie (2008) see the problem of target extraction as part of a topic modelling problem, similarly to Mei et al. (2007). In this contribution, we follow the work of Qiu et al. (2011) who learn syntactic relations from dependency trees. 3 Pipeline Our workflow is illustrated in Figure 1. We first pre-process the data, then mark all a</context>
</contexts>
<marker>Mei, Ling, Wondra, Su, Zhai, 2007</marker>
<rawString>Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, and ChengXiang Zhai. 2007. Topic sentiment mixture: Modeling facets and opinions in weblogs. In Proceedings of the 16th International Conference on World Wide Web, WWW ’07, pages 171–180, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Popel</author>
<author>Zdenek Zabokrtsk´y</author>
</authors>
<title>TectoMT: Modular NLP Framework.</title>
<date>2010</date>
<booktitle>In Hrafn Loftsson, Eirikur R¨ognvaldsson, and Sigrun Helgadottir, editors, IceTAL 2010,</booktitle>
<volume>6233</volume>
<pages>293--304</pages>
<publisher>Technology (ICLT), Springer.</publisher>
<institution>Iceland Centre for Language</institution>
<marker>Popel, Zabokrtsk´y, 2010</marker>
<rawString>Martin Popel and Zden&amp;quot;ek &amp;quot;Zabokrtsk´y. 2010. TectoMT: Modular NLP Framework. In Hrafn Loftsson, Eirikur R¨ognvaldsson, and Sigrun Helgadottir, editors, IceTAL 2010, volume 6233 of Lecture Notes in Computer Science, pages 293–304. Iceland Centre for Language Technology (ICLT), Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana-Maria Popescu</author>
<author>Oren Etzioni</author>
</authors>
<title>Extracting product features and opinions from reviews.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05,</booktitle>
<pages>339--346</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3119" citStr="Popescu and Etzioni, 2005" startWordPosition="476" endWordPosition="479">ectivity lexicons, generally described e.g. in Taboada et al. (2011). The English ones we use are minutely described in Wiebe et al. (2005) and several papers by Bing Liu, starting with Hu and Liu (2004). Inspired by Kobayashi et al. (2007), who make use of evaluative expressions when learning syntactic patterns obtained via pattern mining to extract aspect-evaluation pairs, we use the opinion words to detect evaluative structures in parsed data. The issue of target extraction in sentiment analysis is discussed in articles proposing different methods, mainly tested on product review datasets (Popescu and Etzioni, 2005; Mei et al., 2007; Scaffidi et al., 2007). Some of the authors take into consideration also product aspects (features), defined as product components or product attributes (Liu, 2006). Hu and Liu (2004) take as the feature candidates all noun phrases found in the text. Stoyanov and Cardie (2008) see the problem of target extraction as part of a topic modelling problem, similarly to Mei et al. (2007). In this contribution, we follow the work of Qiu et al. (2011) who learn syntactic relations from dependency trees. 3 Pipeline Our workflow is illustrated in Figure 1. We first pre-process the dat</context>
</contexts>
<marker>Popescu, Etzioni, 2005</marker>
<rawString>Ana-Maria Popescu and Oren Etzioni. 2005. Extracting product features and opinions from reviews. In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05, pages 339–346, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guang Qiu</author>
<author>Bing Liu</author>
<author>Jiajun Bu</author>
<author>Chun Chen</author>
</authors>
<title>Opinion word expansion and target extraction through double propagation.</title>
<date>2011</date>
<journal>Comput. Linguist.,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="2183" citStr="Qiu et al. (2011)" startWordPosition="324" endWordPosition="327">he parsed text using a union of two different existing subjectivity lexicons (see Section 2). Afterwards, we extract the aspect terms in the dependency structures containing polarity ex1http://alt.qcri.org/semeval2014/ task4/ This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ pressions. In this task, we employ several handcrafted rules detecting aspects based on syntactic features of the evaluative sentences, inspired by the method by Qiu et al. (2011). Finally, we identify aspect term categories with the help of the English WordNet and derive their polarities based on the polarities of individual aspects. The obtained results are discussed in Section 4. 2 Related Work This work is related to polarity detection based on a list of evaluative items, i.e. subjectivity lexicons, generally described e.g. in Taboada et al. (2011). The English ones we use are minutely described in Wiebe et al. (2005) and several papers by Bing Liu, starting with Hu and Liu (2004). Inspired by Kobayashi et al. (2007), who make use of evaluative expressions when lea</context>
<context position="3585" citStr="Qiu et al. (2011)" startWordPosition="556" endWordPosition="559">raction in sentiment analysis is discussed in articles proposing different methods, mainly tested on product review datasets (Popescu and Etzioni, 2005; Mei et al., 2007; Scaffidi et al., 2007). Some of the authors take into consideration also product aspects (features), defined as product components or product attributes (Liu, 2006). Hu and Liu (2004) take as the feature candidates all noun phrases found in the text. Stoyanov and Cardie (2008) see the problem of target extraction as part of a topic modelling problem, similarly to Mei et al. (2007). In this contribution, we follow the work of Qiu et al. (2011) who learn syntactic relations from dependency trees. 3 Pipeline Our workflow is illustrated in Figure 1. We first pre-process the data, then mark all aspects seen in the training data (still on plain text). The rest of the pipeline is implemented in Treex (Popel and 694 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 694–698, Dublin, Ireland, August 23-24, 2014. Pattern Example sentence Subjaspect Predcopula PAdj The food was great. Subjaspect Predcopula PNoun The coconut juice is the MUST! Subjaspect Pred Adveval The pizza tastes so good. Attreval N</context>
</contexts>
<marker>Qiu, Liu, Bu, Chen, 2011</marker>
<rawString>Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2011. Opinion word expansion and target extraction through double propagation. Comput. Linguist., 37(1):9–27, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Scaffidi</author>
<author>Kevin Bierhoff</author>
<author>Eric Chang</author>
<author>Mikhael Felker</author>
<author>Herman Ng</author>
<author>Chun Jin</author>
</authors>
<title>Red opal: Product-feature scoring from reviews.</title>
<date>2007</date>
<booktitle>In Proceedings of the 8th ACM Conference on Electronic Commerce, EC ’07,</booktitle>
<pages>182--191</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="3161" citStr="Scaffidi et al., 2007" startWordPosition="484" endWordPosition="487">n Taboada et al. (2011). The English ones we use are minutely described in Wiebe et al. (2005) and several papers by Bing Liu, starting with Hu and Liu (2004). Inspired by Kobayashi et al. (2007), who make use of evaluative expressions when learning syntactic patterns obtained via pattern mining to extract aspect-evaluation pairs, we use the opinion words to detect evaluative structures in parsed data. The issue of target extraction in sentiment analysis is discussed in articles proposing different methods, mainly tested on product review datasets (Popescu and Etzioni, 2005; Mei et al., 2007; Scaffidi et al., 2007). Some of the authors take into consideration also product aspects (features), defined as product components or product attributes (Liu, 2006). Hu and Liu (2004) take as the feature candidates all noun phrases found in the text. Stoyanov and Cardie (2008) see the problem of target extraction as part of a topic modelling problem, similarly to Mei et al. (2007). In this contribution, we follow the work of Qiu et al. (2011) who learn syntactic relations from dependency trees. 3 Pipeline Our workflow is illustrated in Figure 1. We first pre-process the data, then mark all aspects seen in the train</context>
</contexts>
<marker>Scaffidi, Bierhoff, Chang, Felker, Ng, Jin, 2007</marker>
<rawString>Christopher Scaffidi, Kevin Bierhoff, Eric Chang, Mikhael Felker, Herman Ng, and Chun Jin. 2007. Red opal: Product-feature scoring from reviews. In Proceedings of the 8th ACM Conference on Electronic Commerce, EC ’07, pages 182–191, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM – An Extensible Language Modeling Toolkit.</title>
<date>2002</date>
<booktitle>In Proc. Intl. Conf. on Spoken Language Processing,</booktitle>
<volume>2</volume>
<pages>901--904</pages>
<contexts>
<context position="5412" citStr="Stolcke, 2002" startWordPosition="841" endWordPosition="842"> submission, both datasets are utilized in training. 3.2 Pre-processing The main phase of pre-processing (apart from parsing the input files and other simple tasks) is running a spell-checker. As data for this task comes from real-world reviews, it contains various typos and other small errors. We therefore implemented a statistical spell-checker which works in two stages: 1. Run Aspell2 to detect typos and obtain suggestions for them. 2. Select the appropriate suggestions using a language model (LM). We trained a trigram LM from the English side of CzEng 1.0 (Bojar et al., 2012) using SRILM (Stolcke, 2002). We binarized the LM and use the Lazy decoder (Heafield et al., 2013) for selecting the suggestions that best fit the current context. Our script is freely available for download.3 We created a list of exceptions (domain-specific words, such as “netbook”, are unknown to Aspell’s dictionary) which should not be corrected and also skip named entities in spell-checking. 3.3 Marking Known Aspects Before any linguistic processing, we mark all words (and multiword expressions) which are marked as aspects in the training data. For our final submission, the list also includes aspects from the provide</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM – An Extensible Language Modeling Toolkit. In Proc. Intl. Conf. on Spoken Language Processing, volume 2, pages 901– 904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veselin Stoyanov</author>
<author>Claire Cardie</author>
</authors>
<title>Topic identification for fine-grained opinion analysis.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22Nd International Conference on Computational Linguistics - Volume 1, COLING ’08,</booktitle>
<pages>817--824</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3416" citStr="Stoyanov and Cardie (2008)" startWordPosition="525" endWordPosition="528">yntactic patterns obtained via pattern mining to extract aspect-evaluation pairs, we use the opinion words to detect evaluative structures in parsed data. The issue of target extraction in sentiment analysis is discussed in articles proposing different methods, mainly tested on product review datasets (Popescu and Etzioni, 2005; Mei et al., 2007; Scaffidi et al., 2007). Some of the authors take into consideration also product aspects (features), defined as product components or product attributes (Liu, 2006). Hu and Liu (2004) take as the feature candidates all noun phrases found in the text. Stoyanov and Cardie (2008) see the problem of target extraction as part of a topic modelling problem, similarly to Mei et al. (2007). In this contribution, we follow the work of Qiu et al. (2011) who learn syntactic relations from dependency trees. 3 Pipeline Our workflow is illustrated in Figure 1. We first pre-process the data, then mark all aspects seen in the training data (still on plain text). The rest of the pipeline is implemented in Treex (Popel and 694 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 694–698, Dublin, Ireland, August 23-24, 2014. Pattern Example senten</context>
</contexts>
<marker>Stoyanov, Cardie, 2008</marker>
<rawString>Veselin Stoyanov and Claire Cardie. 2008. Topic identification for fine-grained opinion analysis. In Proceedings of the 22Nd International Conference on Computational Linguistics - Volume 1, COLING ’08, pages 817–824, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maite Taboada</author>
<author>Julian Brooke</author>
<author>Milan Tofiloski</author>
<author>Kimberly Voll</author>
<author>Manfred Stede</author>
</authors>
<title>Lexiconbased methods for sentiment analysis.</title>
<date>2011</date>
<journal>Comput. Linguist.,</journal>
<volume>37</volume>
<issue>2</issue>
<contexts>
<context position="2562" citStr="Taboada et al. (2011)" startWordPosition="387" endWordPosition="390">. Licence details: http://creativecommons.org/licenses/by/4.0/ pressions. In this task, we employ several handcrafted rules detecting aspects based on syntactic features of the evaluative sentences, inspired by the method by Qiu et al. (2011). Finally, we identify aspect term categories with the help of the English WordNet and derive their polarities based on the polarities of individual aspects. The obtained results are discussed in Section 4. 2 Related Work This work is related to polarity detection based on a list of evaluative items, i.e. subjectivity lexicons, generally described e.g. in Taboada et al. (2011). The English ones we use are minutely described in Wiebe et al. (2005) and several papers by Bing Liu, starting with Hu and Liu (2004). Inspired by Kobayashi et al. (2007), who make use of evaluative expressions when learning syntactic patterns obtained via pattern mining to extract aspect-evaluation pairs, we use the opinion words to detect evaluative structures in parsed data. The issue of target extraction in sentiment analysis is discussed in articles proposing different methods, mainly tested on product review datasets (Popescu and Etzioni, 2005; Mei et al., 2007; Scaffidi et al., 2007).</context>
</contexts>
<marker>Taboada, Brooke, Tofiloski, Voll, Stede, 2011</marker>
<rawString>Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly Voll, and Manfred Stede. 2011. Lexiconbased methods for sentiment analysis. Comput. Linguist., 37(2):267–307, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katerina Veselovsk´a</author>
<author>Jan Masek</author>
<author>Vladislav Kubon</author>
</authors>
<title>Sentiment detection and annotation in a treebank.</title>
<date>2014</date>
<marker>Veselovsk´a, Masek, Kubon, 2014</marker>
<rawString>Kate&amp;quot;rina Veselovsk´a, Jan Mas&amp;quot;ek, and Vladislav Kubo&amp;quot;n. 2014. Sentiment detection and annotation in a treebank.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Claire Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language. Language resources and evaluation,</title>
<date>2005</date>
<pages>39--2</pages>
<contexts>
<context position="2633" citStr="Wiebe et al. (2005)" startWordPosition="401" endWordPosition="404"> In this task, we employ several handcrafted rules detecting aspects based on syntactic features of the evaluative sentences, inspired by the method by Qiu et al. (2011). Finally, we identify aspect term categories with the help of the English WordNet and derive their polarities based on the polarities of individual aspects. The obtained results are discussed in Section 4. 2 Related Work This work is related to polarity detection based on a list of evaluative items, i.e. subjectivity lexicons, generally described e.g. in Taboada et al. (2011). The English ones we use are minutely described in Wiebe et al. (2005) and several papers by Bing Liu, starting with Hu and Liu (2004). Inspired by Kobayashi et al. (2007), who make use of evaluative expressions when learning syntactic patterns obtained via pattern mining to extract aspect-evaluation pairs, we use the opinion words to detect evaluative structures in parsed data. The issue of target extraction in sentiment analysis is discussed in articles proposing different methods, mainly tested on product review datasets (Popescu and Etzioni, 2005; Mei et al., 2007; Scaffidi et al., 2007). Some of the authors take into consideration also product aspects (feat</context>
<context position="6595" citStr="Wiebe et al., 2005" startWordPosition="1026" endWordPosition="1029">so includes aspects from the provided development sets. 3.4 Morphological Analysis and Parsing Further, we lemmatize the data and parse it using Treex (Popel and ˇZabokrtsk´y, 2010), a modular framework for natural language processing (NLP). Treex is focused primarily on dependency syntax and includes blocks (wrappers) for taggers, parsers and other NLP tools. Within Treex, we used the Morˇce tagger (Hajiˇc et al., 2007) and the MST dependency parser (McDonald et al., 2005). 3.5 Finding Evaluative Words In the obtained dependency data, we detect polarity items using MPQA subjectivity lexicon (Wiebe et al., 2005) and Bing Liu’s subjectivity clues.4 2http://aspell.net/ 3https://redmine.ms.mff.cuni.cz/ projects/staspell 4http://www.cs.uic.edu/˜liub/FBS/ sentiment-analysis.html#lexicon Run tagger &amp; parser Mark evaluative words Treex Apply syntactic rules Mark aspect categories Pre-process &amp; spellcheck Plain text Mark known aspects 695 Task 1: aspect extraction Task 2: aspect polarity Task 3: category detection Task 4: category polarity prec recall F-measure accuracy prec recall F-measure accuracy UFAL 0.50 0.72 0.59 0.67 0.57 0.74 0.65 0.63 best 0.91 0.82 0.84 0.81 0.91 0.86 0.88 0.83 Table 2: Results of</context>
</contexts>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emotions in language. Language resources and evaluation, 39(2-3):165–210.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>