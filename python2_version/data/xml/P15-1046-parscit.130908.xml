<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000087">
<title confidence="0.994811">
New Transfer Learning Techniques for Disparate Label Sets
</title>
<author confidence="0.998099">
Young-Bum Kim† Karl Stratos‡ Ruhi Sarikaya† Minwoo Jeong†
</author>
<affiliation confidence="0.980943">
†Microsoft Corporation, Redmond, WA
‡Columbia University, New York, NY
</affiliation>
<email confidence="0.9525765">
{ybkim, ruhi.sarikaya, minwoo.jeong}@microsoft.com
stratos@cs.columbia.edu
</email>
<sectionHeader confidence="0.997353" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99998035">
In natural language understanding (NLU),
a user utterance can be labeled differently
depending on the domain or application
(e.g., weather vs. calendar). Standard
domain adaptation techniques are not di-
rectly applicable to take advantage of the
existing annotations because they assume
that the label set is invariant. We propose
a solution based on label embeddings in-
duced from canonical correlation analysis
(CCA) that reduces the problem to a stan-
dard domain adaptation task and allows
use of a number of transfer learning tech-
niques. We also introduce a new trans-
fer learning technique based on pretrain-
ing of hidden-unit CRFs (HUCRFs). We
perform extensive experiments on slot tag-
ging on eight personal digital assistant do-
mains and demonstrate that the proposed
methods are superior to strong baselines.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999965297872341">
The main goal of NLU is to automatically extract
the meaning of spoken or typed queries. In recent
years, this task has become increasingly impor-
tant as more and more speech-based applications
have emerged. Recent releases of personal dig-
ital assistants such as Siri, Google Now, Dragon
Go and Cortana in smart phones provide natu-
ral language based interface for a variety of do-
mains (e.g. places, weather, communications, re-
minders). The NLU in these domains are based
on statistical machine learned models which re-
quire annotated training data. Typically each do-
main has its own schema to annotate the words and
queries. However the meaning of words and utter-
ances could be different in each domain. For ex-
ample, “sunny” is considered a weather condition
in the weather domain but it may be a song title in
a music domain. Thus every time a new applica-
tion is developed or a new domain is built, a sig-
nificant amount of resources is invested in creating
annotations specific to that application or domain.
One might attempt to apply existing techniques
(Blitzer et al., 2006; Daum´e III, 2007) in domain
adaption to this problem, but a straightforward ap-
plication is not possible because these techniques
assume that the label set is invariant.
In this work, we provide a simple and effec-
tive solution to this problem by abstracting the la-
bel types using the canonical correlation analysis
(CCA) by Hotelling (Hotelling, 1936) a powerful
and flexible statistical technique for dimensional-
ity reduction. We derive a low dimensional rep-
resentation for each label type that is maximally
correlated to the average context of that label via
CCA. These shared label representations, or label
embeddings, allow us to map label types across
different domains and reduce the setting to a stan-
dard domain adaptation problem. After the map-
ping, we can apply the standard transfer learning
techniques to solve the problem.
Additionally, we introduce a novel pretraining
technique for hidden-unit CRFs (HUCRFs) to ef-
fectively transfer knowledge from one domain to
another. In our experiments, we find that our
pretraining method is almost always superior to
strong baselines such as the popular domain adap-
tation method of Daum´e III (2007).
</bodyText>
<sectionHeader confidence="0.648355" genericHeader="introduction">
2 Problem description and related work
</sectionHeader>
<bodyText confidence="0.999861833333333">
Let D be the number of distinct domains. Let Xi
be the space of observed samples for the i-th do-
main. Let Yi be the space of possible labels for the
i-th domain. In most previous works in domain
adaptation (Blitzer et al., 2006; Daum´e III, 2007),
observed data samples may vary but label space is
</bodyText>
<page confidence="0.990406">
473
</page>
<note confidence="0.7615076">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 473–482,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
invariant1. That is,
Yi = Yj bi,j E {1... D}
</note>
<bodyText confidence="0.999316892857143">
but Xi =� Xj for some domains i and j. For exam-
ple, in part-of-speech (POS) tagging on newswire
and biomedical domains, the observed data sam-
ple may be radically different but the POS tag set
remains the same.
In practice, there are cases, where the same
query is labeled differently depending on the do-
main or application and the context. For example,
Fred Myer can be tagged differently; “send a text
message to Fred Myer” and “get me driving direc-
tion to Fred Myer ”. In the first case, Fred Myer is
person in user’s contact list but it is a grocery store
in the second one.
So, we relax the constraint that label spaces
must be the same. Instead, we assume that sur-
face forms (i.e words) are similar. This is a natu-
ral setting in developing multiple applications on
speech utterances; input spaces (service request
utterances) do not change drastically but output
spaces (slot tags) might.
Multi-task learning differs from our task. In
general multi-task learning aims to improve per-
formance across all domains while our domain
adaptation objective is to optimize the perfor-
mance of semantic slot tagger on the target do-
main.
Below, we review related work in domain adap-
tion and natural language understanding (NLU).
</bodyText>
<sectionHeader confidence="0.80181" genericHeader="related work">
2.1 Related Work
</sectionHeader>
<bodyText confidence="0.997333466666667">
Domain adaptation has been widely used in many
natural language processing (NLP) applications
including part-of-speech tagging (Schnabel and
Sch¨utze, 2014), parsing (McClosky et al., 2010),
and machine translation (Foster et al., 2010).
Most of the work can be classified either su-
pervised domain adaptation (Chelba and Acero,
2006; Blitzer et al., 2006; Daume III and Marcu,
2006; Daum´e III, 2007; Finkel and Manning,
2009; Chen et al., 2011) or semi-supervised adap-
tation (Ando and Zhang, 2005; Jiang and Zhai,
2007; Kumar et al., 2010; Huang and Yates, 2010).
Our problem setting falls into the former.
Multi-task learning has become popular in NLP.
Sutton and McCallum (2005) showed that joint
</bodyText>
<footnote confidence="0.6702535">
1Multilingual learning (Kim et al., 2011; Kim and Snyder,
2012; Kim and Snyder, 2013) has same setting.
</footnote>
<bodyText confidence="0.999964536585366">
learning and/or decoding of sub-tasks helps to im-
prove performance. Collobert and Weston (2008)
proved the similar claim in a deep learning archi-
tecture. While our problem resembles their set-
tings, there are two clear distinctions. First, we
aim to optimize performance on the target domain
by minimizing the gap between source and target
domain while multi-task learning jointly learns the
shared tasks. Second, in our problem the domains
are different, but they are closely related. On the
other hand, prior work focuses on multiple sub-
tasks of the same data.
Despite the increasing interest in NLU (De Mori
et al., 2008; Xu and Sarikaya, 2013; Sarikaya et
al., 2014; Xu and Sarikaya, 2014; Anastasakos et
al., 2014; El-Kahky et al., 2014; Liu and Sarikaya,
2014; Marin et al., 2014; Celikyilmaz et al., 2015;
Ma et al., 2015; Kim et al., 2015), transfer learn-
ing in the context of NLU has not been much ex-
plored. The most relevant previous work is Tur
(2006) and Li et al. (2011), which described both
the effectiveness of multi-task learning in the con-
text of NLU. For multi-task learning, they used
shared slots by associating each slot type with ag-
gregate active feature weight vector based on an
existing domain specific slot tagger. Our empiri-
cal results shows that these vector representation
might be helpful to find shared slots across do-
main, but cannot find bijective mapping between
domains.
Also, Jeong and Lee (2009) presented a transfer
learning approach in multi-domain NLU, where
the model jointly learns slot taggers in multiple
domains and simultaneously predicts domain de-
tection and slot tagging results.2 To share parame-
ters across domains, they added an additional node
for domain prediction on top of the slot sequence.
However, this framework also limited to a setting
in which the label set remains invariant. In con-
trast, our method is restricted to this setting with-
out any modification of models.
</bodyText>
<sectionHeader confidence="0.987353" genericHeader="method">
3 Sequence Modeling Technique
</sectionHeader>
<bodyText confidence="0.994349833333333">
The proposed techniques in Section 4 and 5 are
generic methodologies and not tied to any partic-
ular models such as any sequence models and in-
stanced based models. However, because of supe-
rior performance over CRF, we use a hidden unit
CRF (HUCRF) of Maaten et al. (2011).
</bodyText>
<footnote confidence="0.5987005">
2Jeong and Lee (2009) pointed out that if the domain is
given, their method is the same as that of Daum´e III (2007).
</footnote>
<page confidence="0.998018">
474
</page>
<figureCaption confidence="0.9833485">
Figure 1: Graphical representation of hidden unit
CRFs.
</figureCaption>
<bodyText confidence="0.999313833333333">
While popular and effective, a CRF is still a lin-
ear model. In contrast, a HUCRF benefits from
nonlinearity, leading to superior performance over
CRF (Maaten et al., 2011). Thus we will focus on
HUCRFs to demonstrate our techniques in experi-
ments.
</bodyText>
<subsectionHeader confidence="0.997369">
3.1 Hidden Unit CRF (HUCRF)
</subsectionHeader>
<bodyText confidence="0.9989965">
A HUCRF introduces a layer of binary-valued hid-
den units z = z1 ... zn E 10, 11 for each pair of
label sequence y = y1 ... yn and observation se-
quence x = x1 ... xn. A HUCRF parametrized by
θ E Rd and γ E Rd&apos; defines a joint probability of
y and z conditioned on x as follows:
</bodyText>
<equation confidence="0.9998502">
pe,-r(y,z|x) =
exp(θTΦ(x, z) + γTΨ(z, y))
� z&apos;E{0,11n exp(θTΦ(x, zl) + γTΨ(zl, yl))
y&apos;EY(x,z&apos;)
(1)
</equation>
<bodyText confidence="0.99654">
where Y(x, z) is the set of all possible label se-
quences for x and z, and Φ(x, z) E Rd and
Ψ(z, y) E Rd&apos; are global feature functions that de-
compose into local feature functions:
</bodyText>
<equation confidence="0.9973645">
n
Φ(x, z) = φ(x,j,zj)
j=1
n
Ψ(z, y) = ψ(zj,yj−1,yj)
j=1
</equation>
<bodyText confidence="0.9997836">
HUCRF forces the interaction between the obser-
vations and the labels at each position j to go
through a latent variable zj: see Figure 1 for illus-
tration. Then the probability of labels y is given
by marginalizing over the hidden units,
</bodyText>
<equation confidence="0.948316">
pe,-r(y|x) = � pe,-r(y, z|x)
zE{0,11n
</equation>
<bodyText confidence="0.999946714285714">
As in restricted Boltzmann machines (Larochelle
and Bengio, 2008), hidden units are conditionally
independent given observations and labels. This
allows for efficient inference with HUCRFs de-
spite their richness (see Maaten et al. (2011) for
details). We use a perceptron-style algorithm of
Maaten et al. (2011) for training HUCRFs.
</bodyText>
<sectionHeader confidence="0.8714285" genericHeader="method">
4 Transfer learning between domains
with different label sets
</sectionHeader>
<bodyText confidence="0.999979636363636">
In this section, we describe three methods for uti-
lizing annotations in domains with different la-
bel types. First two methods are about transfer-
ring features and last method is about transfer-
ring model parameters. Each of these methods re-
quires some sort of mapping for label types. A
fine-grained label type needs to be mapped to a
coarse one; a label type in one domain needs to be
mapped to the corresponding label type in another
domain. We will provide a solution to obtaining
these label mappings automatically in Section 5.
</bodyText>
<subsectionHeader confidence="0.978783">
4.1 Coarse-to-fine prediction
</subsectionHeader>
<bodyText confidence="0.999120304347826">
This approach has some similarities to the method
of Li et al. (2011) in that shared slots are used
to transfer information between domains. In this
two-stage approach, we train a model on the
source domain, make predictions on the target do-
main, and then use the predicted labels as addi-
tional features to train a final model on the target
domain. This can be helpful if there is some cor-
relation between the label types in the source do-
main and the label types in the target domain.
However, it is not desirable to directly use the
label types in the source domain since they can
be highly specific to that particular domain. An
effective way to combat this problem is to re-
duce the original label types such start-time,
contract-info, and restaurant as to a
set of coarse label types such as name, date,
time, and location that are universally shared
across all domains. By doing so, we can use
the first model to predict generic labels such as
time and then use the second model to use this
information to predict fine-grained labels such as
start-time and end-time.
</bodyText>
<subsectionHeader confidence="0.999871">
4.2 Method of Daum´e III (2007)
</subsectionHeader>
<bodyText confidence="0.99997">
In this popular technique for domain adapta-
tion, we train a model on the union of the
source domain data and the target domain data
</bodyText>
<page confidence="0.995749">
475
</page>
<bodyText confidence="0.999861964285714">
but with the following preprocessing step: each
feature is duplicated and the copy is conjoined
with a domain indicator. For example, in a
WEATHER domain dataset, a feature that indi-
cates the identity of the string “Sunny” will
generate both w(0) = Sunny and (w(0) =
Sunny) ∧ (domain = WEATHER) as fea-
ture types. This preprocessing allows the model
to utilize all data through the common features
and at the same time specialize to specific do-
mains through the domain specific features. This
is especially helpful when there is label ambigu-
ity on particular features (e.g., “Sunny” might be a
weather-condition in a WEATHER domain
dataset but a music-song-name in a MUSIC
domain dataset).
Note that a straightforward application of this
technique is in general not feasible in our situation.
This is because we have features conjoined with
label types and our domains do not share label
types. This breaks the sharing of features across
domains: many feature types in the source domain
are disjoint from those in the target domain due to
different labeling.
Thus it is necessary to first map source domain
label types to target domain label type. After the
mapping, features are shared across domains and
we can apply this technique.
</bodyText>
<subsectionHeader confidence="0.984208">
4.3 Transferring model parameter
</subsectionHeader>
<bodyText confidence="0.926578210526316">
In this approach, we train HUCRF on the source
domain and transfer the learned parameters to ini-
tialize the training process on the target domain.
This can be helpful for at least two reasons:
1. The resulting model will have parameters for
feature types observed in the source domain
as well as the target domain. Thus it has bet-
ter feature coverage.
2. If the training objective is non-convex, this
initialization can be helpful in avoiding bad
local optima.
Since the training objective of HUCRFs is non-
convex, both benefits can apply. We show in our
experiments that this is indeed the case: the model
benefits from both better feature coverage and bet-
ter initialization.
Note that in order to use this approach, we need
to map source domain label types to target domain
label type so that we know which parameter in
</bodyText>
<figureCaption confidence="0.895951">
Figure 2: Illustration of a pretraining scheme for
HUCRFs.
</figureCaption>
<bodyText confidence="0.98890025">
the source domain corresponds to which param-
eter in the target domain. This can be a many-to-
one, one-to-many, one-to-one mapping depending
on the label sets.
</bodyText>
<subsectionHeader confidence="0.91141">
4.3.1 Pretraining with HUCRFs
</subsectionHeader>
<bodyText confidence="0.988149285714286">
In fact, pretraining HUCRFs in the source domain
can be done in various ways. Recall that there are
two parameter types: θ ∈ Rd for scoring obser-
vations and hidden states and γ ∈ Rd&apos; for scoring
hidden states and labels (Eq. (1)). In pretraining,
we first train a model (θ1,γ1) on the source data
{(x(i)
</bodyText>
<equation confidence="0.972013333333333">
src, y(i)
src)}nsrc
i=1 :
</equation>
<bodyText confidence="0.7995865">
Then we train a model (θ2, γ2) on the target
data {(x(i)
</bodyText>
<equation confidence="0.935975">
trg, y(i)
trg)}ntrg
i=1 by initializing (θ2,γ2) ←
(θ1,γ1):
</equation>
<bodyText confidence="0.9999866">
Here, we can choose to initialize only θ2 ← θ1 and
discard the parameters for hidden states and labels
since they may not be the same. The θ1 parame-
ters model the hidden structures in the source do-
main data and serve as a good initialization point
for learning the θ2 parameters in the target domain.
This can be helpful if the mapping between the la-
bel types in the source data and the label types in
the target data is unreliable. This process is illus-
trated in Figure 2.
</bodyText>
<sectionHeader confidence="0.9967425" genericHeader="method">
5 Automatic generation of label
mappings
</sectionHeader>
<bodyText confidence="0.99944975">
All methods described in Section 4 require
a way to propagate the information in label
types across different domains. A straightfor-
ward solution would be to manually construct
</bodyText>
<equation confidence="0.961828444444444">
nsrc
(θ1, γ1) ≈ arg max
θ,γ i=1
log pθ,γ(y(i)
src|x(i)
src)
(θ2, γ2) ≈ arg max ntrg log pθ,γ(y(i)
θ,γ i=1 trg|x(i)
trg)
</equation>
<page confidence="0.988738">
476
</page>
<bodyText confidence="0.999951875">
such mappings by inspection. For instance, we
can specify that start-time and end-time
are grouped as the same label time, or that
the label public-transportation-route
in the PLACES domain maps to the label
implicit-location in the CALENDAR do-
main.
Instead, we propose a technique that automat-
ically generates the label mappings. We induce
vector representations for all label types through
canonical correlation analysis (CCA) — a pow-
erful and flexible technique for deriving low-
dimensional representation. We give a review of
CCA in Section 5.1 and describe how we use
the technique to construct label mappings in Sec-
tion 5.2.
</bodyText>
<subsectionHeader confidence="0.956628">
5.1 Canonical Correlation Analysis (CCA)
</subsectionHeader>
<bodyText confidence="0.999884375">
CCA is a general technique that operates on a
pair of multi-dimensional variables. CCA finds k
dimensions (k is a parameter to be specified) in
which these variables are maximally correlated.
Let x1 ... xn E Rd and y1 ... yn E Rd0 be n
samples of the two variables. For simplicity, as-
sume that these variables have zero mean. Then
CCA computes the following for i = 1... k:
</bodyText>
<table confidence="0.928764">
arg max En l=1(u� i xl)(v�i yl)
uiERd, viERd0:
u&gt;i ui0=0 bi0&lt;i
v&gt;i vi0=0 bi0&lt;i
✓En ✓En
l=1(u� i xl)2 l=1(v� i yl)2
</table>
<bodyText confidence="0.999619785714286">
In other words, each (ui, vi) is a pair of projec-
tion vectors such that the correlation between the
projected variables u xl and vz yl (now scalars) is
maximized, under the constraint that this projec-
tion is uncorrelated with the previous i − 1 pro-
jections.
This is a non-convex problem due to the inter-
action between ui and vi. Fortunately, a method
based on singular value decomposition (SVD) pro-
vides an efficient and exact solution to this prob-
lem (Hotelling, 1936). The resulting solution
u1 ... uk E Rd and v1 ... vk E Rd0 can be used
to project the variables from the original d- and
d&apos;-dimensional spaces to a k-dimensional space:
</bodyText>
<equation confidence="0.977592">
x E Rd −� x¯E Rk:¯xi= uz x
y E Rd0 −� y¯E Rk : ¯yi = vZ y
</equation>
<bodyText confidence="0.9994236">
The new k-dimensional representation of each
variable now contains information about the other
variable. The value of k is usually selected to be
much smaller than d or d&apos;, so the representation is
typically also low-dimensional.
</bodyText>
<subsectionHeader confidence="0.995976">
5.2 Inducing label embeddings
</subsectionHeader>
<bodyText confidence="0.999818909090909">
We now describe how to use CCA to induce vec-
tor representations for label types. Using the same
notation, let n be the number of instances of la-
bels in the entire data. Let x1 ... xn be the original
representations of the label samples and y1 ... yn
be the original representations of the associated
words set contained in the labels.
We employ the following definition for the orig-
inal representations for reasons we explain below.
Let d be the number of distinct label types and d&apos;
be the number of distinct word types.
</bodyText>
<listItem confidence="0.99783325">
• xl E Rd is a zero vector in which the entry
corresponding to the label type of the l-th in-
stance is set to 1.
•
</listItem>
<bodyText confidence="0.99896255">
yl E Rd0 is a zero vector in which the entries
corresponding to words spanned by the label
are set to 1.
The motivation for this definition is that similar
label types often have similar or same word.
For instance, consider two label types
start-time, (start time of a calendar event)
and end-time, meaning (the end time of a cal-
endar event). Each type is frequently associated
with phrases about time. The phrases {“9 pm”,
“7”, “8 am”} might be labeled as start-time;
the phrases {“9 am”, “7 pm”} might be labeled
as end-time. In these examples, both label
types share words “am”, “pm”, “9”, and “7” even
though phrases may not match exactly.
Figure 3 gives the CCA algorithm for inducing
label embeddings. It produces a k-dimensional
vector for each label type corresponding to the
CCA projection of the one-hot encoding of that
label.
</bodyText>
<subsectionHeader confidence="0.8322135">
5.3 Discussion on alternative label
representations
</subsectionHeader>
<bodyText confidence="0.99998325">
We point out that there are other options for in-
ducing label representations besides CCA. For
instance, one could simply use the sparse fea-
ture vector representation of each label. How-
ever, CCA’s low-dimensional projection is com-
putationally more convenient and arguably more
generalizable. One can also consider training a
predictive model similar to word2vec (Mikolov
</bodyText>
<page confidence="0.998293">
477
</page>
<figureCaption confidence="0.99458675">
Figure 4: Bijective mapping: labels in REMINDER domain (orange box) are mapped into those in
PLACES and ALARM domains.
Figure 3: CCA algorithm for inducing label em-
beddings.
</figureCaption>
<subsectionHeader confidence="0.51888">
5.4.1 Mapping to a coarse label set
</subsectionHeader>
<bodyText confidence="0.999928">
Given a domain and the label types that occur
in the domain, we can reduce the number of la-
bel types by simply clustering their vector repre-
sentations. For instance, if the embeddings for
start-time and end-time are closetogether,
they will be grouped as a single label type. We run
the k-means algorithm on the label embeddings to
obtain this coarse label set.
Table 1 shows examples of this clustering. It
demonstrates that the CCA representations ob-
tained by the procedure described in Section 5.2
are indeed informative of the labels’ properties.
</bodyText>
<table confidence="0.948746333333333">
Cluster Labels Cluster Labels
Time start time Person contact info
end time artist
original start time from contact name
travel time relationship name
Loc absolute loc Loc ATTR prefer route
leaving loc public trans route
from loc nearby
position ref distance
</table>
<tableCaption confidence="0.999692">
Table 1: Some of cluster examples
</tableCaption>
<figure confidence="0.973697642857143">
CCA-LABEL
Input: labeled sequences {(x(i), (i))}
yni= 1, dimension k
Output: label vector v E Rk for each label type
1. For each label type l E {1 ... d} and word type w E
{1 ... d} present in the sequences, calculate
• count(l) = number of times label l occurs
• count(w) = number of times word w occurs
• count(l, w) = number of times word w occurs
under label l
2. Define a matrix Ω E Rd×d� where:
Ωl,w =
,/count(l)count(w)
3. Perform rank-k SVD on Ω. Let U E Rd×k be a matrix
</figure>
<bodyText confidence="0.947813272727273">
where the i-th column is the left singular vector of Ω
corresponding to the i-th largest singular value.
4. For each label l, set the l-th normalized row of U to be
its vector representation.
count(l, w)
et al., 2013). But this requires significant efforts in
implementation and also very long training time.
In contrast, CCA is simple, efficient, and effec-
tive and can be readily implemented. Also, CCA
is theoretically well understood while methods in-
spired by neural networks are not.
</bodyText>
<subsectionHeader confidence="0.99363">
5.4 Constructing label mappings
</subsectionHeader>
<bodyText confidence="0.998314">
Vector representations of label types allow for nat-
ural solutions to the task of constructing label
mappings.
</bodyText>
<subsectionHeader confidence="0.652619">
5.4.2 Bijective mapping between label sets
</subsectionHeader>
<bodyText confidence="0.997123363636363">
Given a pair of domains and their label sets, we
can create a bijective label mapping by finding
the nearest neighbor of each label type. Figure 4
shows some actual examples of CCA-based bijec-
tive maps, where the label set in the REMINDER
domain is mapped to the PLACES and ALARM
domains. One particularly interesting example is
that move earlier time in REMINDER do-
main is mapped to Travel time in PLACES
and Duration in ALARM domain. This is a tag
used in a user utterance requesting to move an
</bodyText>
<page confidence="0.997179">
478
</page>
<table confidence="0.997677">
Domains # of label Source Training Test Description
Alarm 7 27865 3334 Set alarms
Calendar 20 50255 7017 Set appointments &amp; meetings in the calendar
Communication 18 104881 14484 Make calls, send texts, and communication related user request
Note 4 17445 2342 Note taking
Ondevice 7 60847 9704 Phone settings
Places 32 150348 20798 Find places &amp; get direction
Reminder 16 62664 8235 Setting time, person &amp; place based reminder
Weather 9 53096 9114 Weather forecasts &amp; historical information about weather patterns
</table>
<tableCaption confidence="0.971359">
Table 2: Size of number of label, labeled data set size and description for Alarm, Calendar, Communica-
tion, Note, Ondevice, Places, Reminder and Weather domains partitioned into training and test set.
</tableCaption>
<bodyText confidence="0.99929225">
appointment to an earlier time. For example, in
the query “move the dentist’s appointment up by
30 minutes.”, the phrase “30 minutes” is tagged
with move earlier time. The role of this tag
is very similar to the role of Travel time in
PLACES (not Time) and Duration in ALARMS
(not Start date), and CCA is able to recover
this relation.
</bodyText>
<sectionHeader confidence="0.9998" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999635666666667">
In this section, we turn to experimental findings to
provide empirical support for our proposed meth-
ods.
</bodyText>
<subsectionHeader confidence="0.99511">
6.1 Setup
</subsectionHeader>
<bodyText confidence="0.999941888888889">
To test the effectiveness of our approach, we apply
it to a suite of eight Cortana personal assistant do-
mains for slot sequence tagging tasks, where the
goal is to find the correct semantic tagging of the
words in a given user utterance.
The data statistics and short descriptions are
shown in Table 2. As the table indicates, the do-
mains have very different granularity and diverse
semantics.
</bodyText>
<subsectionHeader confidence="0.999809">
6.2 Baselines
</subsectionHeader>
<bodyText confidence="0.999897">
In all our experiments, we trained HUCRF and
only used n-gram features, including unigram, bi-
gram, and trigram within a window of five words
(f2 words) around the current word as binary fea-
ture functions. With these features, we compare
the following methods for slot tagging:
</bodyText>
<listItem confidence="0.996941666666667">
• NoAdapt: train only on target training data.
• Union: train on the union of source and target
training data.
• Daume: train with the feature duplication
method described in 4.2.
• C2F: train with the coarse-to-fine prediction
method described in 4.1.
• Pretrain: train with the pretraining method
described in 4.3.1.
</listItem>
<bodyText confidence="0.99985375">
To apply these methods except for Target, we
treat each of the eight domains in turn as the test
domain, with one of remaining seven domain as
the source domain. As in general domain adap-
tation setting, we assume that the source domain
has a sufficient amount of labeled data but the tar-
get domain has an insufficient amount of labeled
data. Specifically, For each test or target domain,
we only use 10% of the training examples to sim-
ulate data scarcity. In the following experiments,
we report the slot F-measure, using the standard
CoNLL evaluation script 3
</bodyText>
<subsectionHeader confidence="0.905127">
6.3 Results on mappings
</subsectionHeader>
<table confidence="0.9971415">
Mapping technique
Adaptation Manual Li et al. (2011) CCA
technique
Union 68.16 64.7 70.51
Daume 73.42 67.32 75.85
C2F 75.47 75.69 76.29
Pretrain 77.72 76.99 78.76
NoAdapt 75.13
</table>
<tableCaption confidence="0.999359">
Table 3: Comparison of slot F1 scores using
</tableCaption>
<bodyText confidence="0.922719090909091">
the proposed CCA-derived mapping versus other
mapping methods combined with different adap-
tation techniques.
To assess the quality of our automatic mapping
methods via CCA described in Section 5, we com-
pared against manually established mappings and
also the mapping method of Li et al. (2011). The
method of Li et al. (2011) is to associate each
slot type with the aggregate active feature weight
vectors based on an existing domain specific slot
tagger (a CRF). Manual mapping were performed
</bodyText>
<footnote confidence="0.952327">
3http://www.cnts.ua.ac.be/conll2000/chunking/output.html
</footnote>
<page confidence="0.994069">
479
</page>
<table confidence="0.999852818181818">
Target Source Minimum distance domain performance
Domain Nearest Domain NoAdapt Union Daume C2F Pretrain
Alarm Calendar 74.82 84.46 84.97 81.54 84.88
Calendar Reminder 70.51 73.94 73.07 72.82 77.08
Note Reminder 65.38 56.39 69.89 66.6 69.55
Ondevice Weather 70.86 66.66 71.17 71.49 73.5
Reminder Calendar 77.3 83.38 82.19 81.29 83.22
Communication Reminder 79.31 74.28 80.33 79.66 82.96
Places Weather 73.93 73.74 75.86 73.73 80.11
Weather Places 92.78 92.88 94.43 93.75 97.18
Average - 75.61 75.72 78.99 77.61 81.06
</table>
<tableCaption confidence="0.995119">
Table 4: Slot F1 scores on each target domain using adapted models from the nearest source domain.
</tableCaption>
<table confidence="0.999973775">
Target Alarm Calendar Note Ondevice Reminder Communication Places Weather Average
❤❤❤❤❤❤❤❤❤❤❤❤
Source
NoAdapt 74.82 70.51 65.38 70.86 77.3 79.31 73.93 92.78 75.61
Alarm Union - 72.26 59.92 67.32 79.45 77.91 73.78 92.67 74.76
Daume - 72.77 66.28 70.94 81.12 80.38 75.62 93.12 77.18
C2F - 70.59 64.06 71 78.8 79.5 74.29 92.75 75.86
Pretrain - 76.68 68.12 71.8 81.25 81.5 77.1 95.03 78.78
Calendar Union 84.46 - 50.64 64.7 83.38 75.02 71.13 93.2 74.65
Daume 84.97 - 65.43 70.12 82.19 79.78 75.21 93.1 78.69
C2F 81.54 - 66.08 71.22 81.29 80.11 73.75 93.18 78.17
Pretrain 84.88 - 69.21 72.3 83.22 82.75 77.89 95.8 80.86
Note Union 60.26 60.42 - 65.79 69.81 76.85 70.56 90.02 70.53
Daume 66.03 67.38 - 69.54 76.65 77.83 73.49 92.09 74.72
C2F 74.68 70.51 - 71.34 77.49 79.48 74.17 92.89 77.22
Pretrain 75.52 72.4 - 71.4 80.1 82.06 76.53 94.22 78.89
Ondevice Union 63.72 66.28 55.67 - 75.16 74.85 70.59 90.7 71.00
Daume 71.01 69.39 64.02 - 75.75 77.92 74.41 92.62 75.02
C2F 74.02 70.33 64.99 - 77.43 79.53 73.84 92.71 76.12
Pretrain 76.27 71.59 67.21 - 78.67 82.34 77.45 95.04 78.37
Reminder Union 84.74 73.94 56.39 61.27 - 74.28 68.14 92.22 73.00
Daume 84.66 73.07 69.89 67.94 - 80.33 73.36 93.19 77.49
C2F 80.42 72.82 66.6 71.36 - 79.66 74.35 92.38 76.80
Pretrain 84.75 77.08 69.55 71.9 - 82.96 78.57 95.37 80.03
Communication Union 58.25 54.69 65.28 62.95 63.98 - 68.16 87.13 65.78
Daume 70.4 67.41 69.14 69.26 77.67 - 73.33 92.82 74.29
C2F 74.54 70.84 65.48 70.81 77.68 - 74.15 92.79 75.18
Pretrain 76.04 74.01 68.76 73.2 80.74 - 76.83 94.58 77.74
Places Union 71.7 67.56 45.37 53.93 67.78 63.67 - 92.88 66.13
Daume 75.69 69.01 66.11 65.46 79.01 78.42 - 94.43 75.45
C2F 78.9 71.64 66.93 71.26 79.2 79.19 - 93.75 77.27
Pretrain 76.8 74.12 67.5 72.7 81 81.89 - 97.18 78.74
Weather Union 69.43 58.53 56.76 66.66 74.98 77.53 73.74 - 68.23
Daume 75 71.73 66.54 71.17 79.36 80.57 75.86 - 74.32
C2F 77.61 71.47 63.24 71.49 78.44 79.43 73.73 - 73.63
Pretrain 77.37 74.5 68.23 73.5 80.96 82.05 80.11 - 76.67
Average Union 70.37 64.81 55.72 63.23 73.51 74.3 70.87 91.26 70.51
Daume 75.4 70.23 66.77 69.2 78.32 79.32 74.47 93.05 75.85
C2F 77.39 71.17 65.4 71.21 78.62 79.56 74.04 92.92 76.29
Pretrain 78.80 74.34 68.37 72.40 80.85 82.22 77.78 95.32 78.76
</table>
<tableCaption confidence="0.804442">
Table 5: Slot F1 scores of using Union, Daume, Coarse-to-Fine and pretraining on all pairs of source and
target data. The numbers in boldface are the best performing adaptation technique in each pair.
</tableCaption>
<bodyText confidence="0.999581681818182">
by two experienced annotators who have PhD in
linguistics and machine learning. Each annotator
first assigned mapping slot labels independently
and then both annotators collaborated to reduce
disagreement of their mapping results. Initially,
the disagreement of their mapping rate between
two annotators was about 30% because labels of
slot tagging are very diverse; furthermore, in some
cases it is not clear for human annotators if there
exists a valid mapping.
The results are shown at Table 3. Vector repre-
sentation of Li et al. (2011) increases the F1 score
slightly from 75.13 to 75.69 in C2F, but it does not
help as much in cases that require bijective map-
ping: Daume, Union and Pretrain.
In contrast, the proposed CCA based technique
consistently outperforms the NoAdapt baselines
by significant margins. More importantly, it also
outperforms manual results under all conditions.
It is perhaps not so surprising – the CCA derived
mapping is completely data driven, while human
annotators have nothing but the prior linguistic
</bodyText>
<page confidence="0.992111">
480
</page>
<bodyText confidence="0.95836">
knowledge about the slot tags and the domain.
</bodyText>
<subsectionHeader confidence="0.988062">
6.4 Main Results
</subsectionHeader>
<bodyText confidence="0.999986636363636">
The full results are shown in Table 5, where all
pairs of source and target languages are consid-
ered for domain adaptation. It is clear from the ta-
ble that we can always achieve better results using
adaptation techniques than the non-adapted mod-
els trained only on the target data. Also, our pro-
posed pretraining method outperforms other types
of adaptation in most cases.
The overall result of our experiments are shown
in Table 4. In this experiment, we compare dif-
ferent adaptation techniques using our suggested
CCA-based mapping. Here, except for NoAdapt,
we use both the target and the nearest source do-
main data. To find the nearest domain, we first
map fine grained label set to coarse label set by
using the method described in Section 5.4.1 and
then count how many coarse labels are used in a
domain. And then we can find the nearest source
domain by calculating the 12 distance between the
multinomial distributions of the source domain
and the target domain over the set of coarse labels.
For example, for CALENDAR, we identify
REMINDER as the nearest domain and vice versa
because most of their labels are attributes related
to time. In all experiments, the domain adapted
models perform better than using only target do-
main data which achieves 75.1% F1 score. Sim-
ply combining source and target domain using our
automatically mapped slot labels performs slightly
better than baseline. C2F boosts the performance
up to 77.61% and Daume is able to reach 78.99%.4
Finally, our proposed method, pretrain achieves
nearly 81.02% F1 score.
</bodyText>
<sectionHeader confidence="0.998918" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9999415">
We presented an approach to take advantage of ex-
isting annotations when the data are similar but
the label sets are different. This approach was
based on label embeddings from CCA, which re-
duces the setting to a standard domain adapta-
tion problem. Combined with a novel pretrain-
ing scheme applied to hidden-unit CRFs, our ap-
proach is shown to be superior to strong baselines
in extensive experiments for slot tagging on eight
distinct personal assistant domains.
</bodyText>
<footnote confidence="0.834637666666667">
4It is known that Daume is less beneficial when the source
and target domains are similar due to the increased number of
features.
</footnote>
<sectionHeader confidence="0.96517" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99114004">
Tasos Anastasakos, Young-Bum Kim, and Anoop Deo-
ras. 2014. Task specific continuous word represen-
tations for mono and multi-lingual spoken language
understanding. In Proceeding of the ICASSP, pages
3246–3250. IEEE.
Rie Kubota Ando and Tong Zhang. 2005. A frame-
work for learning predictive structures from multiple
tasks and unlabeled data. The Journal of Machine
Learning Research, 6:1817–1853.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of the EMNLP,
pages 120–128. Association for Computational Lin-
guistics.
Asli Celikyilmaz, Dilek Hakkani-Tur, Panupong Pasu-
pat, and Ruhi Sarikaya. 2015. Enriching word em-
beddings using knowledge graph for semantic tag-
ging in conversational dialog systems. AAAI - As-
sociation for the Advancement of Artificial Intelli-
gence.
Ciprian Chelba and Alex Acero. 2006. Adaptation of
maximum entropy capitalizer: Little data can help a
lot. Computer Speech &amp; Language, 20(4):382–399.
Minmin Chen, Kilian Q Weinberger, and John Blitzer.
2011. Co-training for domain adaptation. In Ad-
vances in neural information processing systems,
pages 2456–2464.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the ICML, pages 160–167. ACM.
Hal Daume III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. Journal of Artificial
Intelligence Research, pages 101–126.
Hal Daum´e III. 2007. Frustratingly easy domain adap-
tation. proceedings of the ACL, page 256.
Renato De Mori, Fr´ed´eric Bechet, Dilek Hakkani-Tur,
Michael McTear, Giuseppe Riccardi, and Gokhan
Tur. 2008. Spoken language understanding. Sig-
nal Processing Magazine, IEEE, 25(3):50–58.
Ali El-Kahky, Derek Liu, Ruhi Sarikaya, Gokhan Tur,
Dilek Hakkani-Tur, and Larry Heck. 2014. Ex-
tending domain coverage of language understand-
ing systems via intent transfer between domains us-
ing knowledge graphs and search query click logs.
IEEE, Proceedings of the ICASSP.
Jenny Rose Finkel and Christopher D Manning. 2009.
Hierarchical bayesian domain adaptation. In Pro-
ceedings of theACL, pages 602–610. Association for
Computational Linguistics.
</reference>
<page confidence="0.98139">
481
</page>
<reference confidence="0.999929892156863">
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adap-
tation in statistical machine translation. In Proceed-
ings of the EMNLP, pages 451–459. Association for
Computational Linguistics.
Harold Hotelling. 1936. Relations between two sets of
variates. Biometrika, 28(3/4):321–377.
Fei Huang and Alexander Yates. 2010. Exploring
representation-learning approaches to domain adap-
tation. In Proceedings of the 2010 Workshop on Do-
main Adaptation for Natural Language Processing,
pages 23–30. Association for Computational Lin-
guistics.
Minwoo Jeong and Gary Geunbae Lee. 2009. Multi-
domain spoken language understanding with trans-
fer learning. Speech Communication, 51(5):412–
424.
Jing Jiang and ChengXiang Zhai. 2007. Instance
weighting for domain adaptation in nlp. In Proceed-
ings of the ACL, volume 7, pages 264–271. Associ-
ation for Computational Linguistics.
Young-Bum Kim and Benjamin Snyder. 2012. Univer-
sal grapheme-to-phoneme prediction over latin al-
phabets. In Proceedings of the EMNLP, pages 332–
343, Jeju Island, South Korea, July. Association for
Computational Linguistics.
Young-Bum Kim and Benjamin Snyder. 2013. Unsu-
pervised consonant-vowel prediction over hundreds
of languages. In Proceedings of the ACL, pages
1527–1536. Association for Computational Linguis-
tics.
Young-Bum Kim, Jo˜ao V Grac¸a, and Benjamin Sny-
der. 2011. Universal morphological analysis using
structured nearest neighbor prediction. In Proceed-
ings of the EMNLP, pages 322–332. Association for
Computational Linguistics.
Young-Bum Kim, Minwoo Jeong, Karl Stratos, and
Ruhi Sarikaya. 2015. Weakly supervised slot
tagging with partially labeled sequences from web
search click logs. In Proceedings of the NAACL. As-
sociation for Computational Linguistics.
Abhishek Kumar, Avishek Saha, and Hal Daume.
2010. Co-regularization based semi-supervised do-
main adaptation. In Advances in Neural Information
Processing Systems, pages 478–486.
Hugo Larochelle and Yoshua Bengio. 2008. Classifi-
cation using discriminative restricted boltzmann ma-
chines. In Proceedings of the ICML.
Xiao Li, Ye-Yi Wang, and G¨okhan T¨ur. 2011. Multi-
task learning for spoken language understanding
with shared slots. In Proceeding of the INTER-
SPEECH, pages 701–704. IEEE.
Xiaohu Liu and Ruhi Sarikaya. 2014. A discriminative
model based entity dictionary weighting approach
for spoken language understanding. IEEE Institute
of Electrical and Electronics Engineers.
Yi Ma, Paul A. Crook, Ruhi Sarikaya, and Eric Fosler-
Lussier. 2015. Knowledge graph inference for spo-
ken dialog systems. In Proceedings of the ICASSP.
IEEE.
Laurens Maaten, Max Welling, and Lawrence K Saul.
2011. Hidden-unit conditional random fields. In In-
ternational Conference on Artificial Intelligence and
Statistics.
Alex Marin, Roman Holenstein, Ruhi Sarikaya, and
Mari Ostendorf. 2014. Learning phrase patterns for
text classification using a knowledge graph and un-
labeled data. ISCA - International Speech Commu-
nication Association.
David McClosky, Eugene Charniak, and Mark John-
son. 2010. Automatic domain adaptation for pars-
ing. In Proceedings of the NAACL, pages 28–36.
Association for Computational Linguistics.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
Ruhi Sarikaya, Asli C, Anoop Deoras, and Minwoo
Jeong. 2014. Shrinkage based features for slot tag-
ging with conditional random fields. Proceeding of
ISCA - International Speech Communication Asso-
ciation, September.
Tobias Schnabel and Hinrich Sch¨utze. 2014. Flors:
Fast and simple domain adaptation for part-of-
speech tagging. Transactions of the Association for
Computational Linguistics, 2:15–26.
Charles Sutton and Andrew McCallum. 2005. Compo-
sition of conditional random fields for transfer learn-
ing. In Proceedings of the EMNLP, pages 748–754.
Association for Computational Linguistics.
Gokhan Tur. 2006. Multitask learning for spoken
language understanding. In Proceedings of the
ICASSP, Toulouse, France. IEEE.
Puyang Xu and Ruhi Sarikaya. 2013. Convolutional
neural network based triangular crf for joint in-
tent detection and slot filling. In Automatic Speech
Recognition and Understanding (ASRU), pages 78–
83. IEEE.
Puyang Xu and Ruhi Sarikaya. 2014. Targeted feature
dropout for robust slot filling in natural language un-
derstanding. ISCA - International Speech Commu-
nication Association.
</reference>
<page confidence="0.998464">
482
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.801705">
<title confidence="0.999781">New Transfer Learning Techniques for Disparate Label Sets</title>
<author confidence="0.999618">Karl Ruhi Minwoo</author>
<affiliation confidence="0.93794">Corporation, Redmond,</affiliation>
<address confidence="0.872998">University, New York,</address>
<email confidence="0.9922435">ruhi.sarikaya,stratos@cs.columbia.edu</email>
<abstract confidence="0.999848333333333">In natural language understanding (NLU), a user utterance can be labeled differently depending on the domain or application (e.g., weather vs. calendar). Standard domain adaptation techniques are not directly applicable to take advantage of the existing annotations because they assume that the label set is invariant. We propose a solution based on label embeddings induced from canonical correlation analysis (CCA) that reduces the problem to a standard domain adaptation task and allows use of a number of transfer learning techniques. We also introduce a new transfer learning technique based on pretraining of hidden-unit CRFs (HUCRFs). We perform extensive experiments on slot tagging on eight personal digital assistant domains and demonstrate that the proposed methods are superior to strong baselines.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Tasos Anastasakos</author>
<author>Young-Bum Kim</author>
<author>Anoop Deoras</author>
</authors>
<title>Task specific continuous word representations for mono and multi-lingual spoken language understanding.</title>
<date>2014</date>
<booktitle>In Proceeding of the ICASSP,</booktitle>
<pages>3246--3250</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="6734" citStr="Anastasakos et al., 2014" startWordPosition="1087" endWordPosition="1090">8) proved the similar claim in a deep learning architecture. While our problem resembles their settings, there are two clear distinctions. First, we aim to optimize performance on the target domain by minimizing the gap between source and target domain while multi-task learning jointly learns the shared tasks. Second, in our problem the domains are different, but they are closely related. On the other hand, prior work focuses on multiple subtasks of the same data. Despite the increasing interest in NLU (De Mori et al., 2008; Xu and Sarikaya, 2013; Sarikaya et al., 2014; Xu and Sarikaya, 2014; Anastasakos et al., 2014; El-Kahky et al., 2014; Liu and Sarikaya, 2014; Marin et al., 2014; Celikyilmaz et al., 2015; Ma et al., 2015; Kim et al., 2015), transfer learning in the context of NLU has not been much explored. The most relevant previous work is Tur (2006) and Li et al. (2011), which described both the effectiveness of multi-task learning in the context of NLU. For multi-task learning, they used shared slots by associating each slot type with aggregate active feature weight vector based on an existing domain specific slot tagger. Our empirical results shows that these vector representation might be helpfu</context>
</contexts>
<marker>Anastasakos, Kim, Deoras, 2014</marker>
<rawString>Tasos Anastasakos, Young-Bum Kim, and Anoop Deoras. 2014. Task specific continuous word representations for mono and multi-lingual spoken language understanding. In Proceeding of the ICASSP, pages 3246–3250. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rie Kubota Ando</author>
<author>Tong Zhang</author>
</authors>
<title>A framework for learning predictive structures from multiple tasks and unlabeled data.</title>
<date>2005</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>6--1817</pages>
<contexts>
<context position="5710" citStr="Ando and Zhang, 2005" startWordPosition="919" endWordPosition="922">target domain. Below, we review related work in domain adaption and natural language understanding (NLU). 2.1 Related Work Domain adaptation has been widely used in many natural language processing (NLP) applications including part-of-speech tagging (Schnabel and Sch¨utze, 2014), parsing (McClosky et al., 2010), and machine translation (Foster et al., 2010). Most of the work can be classified either supervised domain adaptation (Chelba and Acero, 2006; Blitzer et al., 2006; Daume III and Marcu, 2006; Daum´e III, 2007; Finkel and Manning, 2009; Chen et al., 2011) or semi-supervised adaptation (Ando and Zhang, 2005; Jiang and Zhai, 2007; Kumar et al., 2010; Huang and Yates, 2010). Our problem setting falls into the former. Multi-task learning has become popular in NLP. Sutton and McCallum (2005) showed that joint 1Multilingual learning (Kim et al., 2011; Kim and Snyder, 2012; Kim and Snyder, 2013) has same setting. learning and/or decoding of sub-tasks helps to improve performance. Collobert and Weston (2008) proved the similar claim in a deep learning architecture. While our problem resembles their settings, there are two clear distinctions. First, we aim to optimize performance on the target domain by</context>
</contexts>
<marker>Ando, Zhang, 2005</marker>
<rawString>Rie Kubota Ando and Tong Zhang. 2005. A framework for learning predictive structures from multiple tasks and unlabeled data. The Journal of Machine Learning Research, 6:1817–1853.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Domain adaptation with structural correspondence learning.</title>
<date>2006</date>
<booktitle>In Proceedings of the EMNLP,</booktitle>
<pages>120--128</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2173" citStr="Blitzer et al., 2006" startWordPosition="339" endWordPosition="342">ins are based on statistical machine learned models which require annotated training data. Typically each domain has its own schema to annotate the words and queries. However the meaning of words and utterances could be different in each domain. For example, “sunny” is considered a weather condition in the weather domain but it may be a song title in a music domain. Thus every time a new application is developed or a new domain is built, a significant amount of resources is invested in creating annotations specific to that application or domain. One might attempt to apply existing techniques (Blitzer et al., 2006; Daum´e III, 2007) in domain adaption to this problem, but a straightforward application is not possible because these techniques assume that the label set is invariant. In this work, we provide a simple and effective solution to this problem by abstracting the label types using the canonical correlation analysis (CCA) by Hotelling (Hotelling, 1936) a powerful and flexible statistical technique for dimensionality reduction. We derive a low dimensional representation for each label type that is maximally correlated to the average context of that label via CCA. These shared label representation</context>
<context position="3594" citStr="Blitzer et al., 2006" startWordPosition="572" endWordPosition="575">ng techniques to solve the problem. Additionally, we introduce a novel pretraining technique for hidden-unit CRFs (HUCRFs) to effectively transfer knowledge from one domain to another. In our experiments, we find that our pretraining method is almost always superior to strong baselines such as the popular domain adaptation method of Daum´e III (2007). 2 Problem description and related work Let D be the number of distinct domains. Let Xi be the space of observed samples for the i-th domain. Let Yi be the space of possible labels for the i-th domain. In most previous works in domain adaptation (Blitzer et al., 2006; Daum´e III, 2007), observed data samples may vary but label space is 473 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 473–482, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics invariant1. That is, Yi = Yj bi,j E {1... D} but Xi =� Xj for some domains i and j. For example, in part-of-speech (POS) tagging on newswire and biomedical domains, the observed data sample may be radically different but the POS tag set remains the same. In practi</context>
<context position="5567" citStr="Blitzer et al., 2006" startWordPosition="895" endWordPosition="898"> to improve performance across all domains while our domain adaptation objective is to optimize the performance of semantic slot tagger on the target domain. Below, we review related work in domain adaption and natural language understanding (NLU). 2.1 Related Work Domain adaptation has been widely used in many natural language processing (NLP) applications including part-of-speech tagging (Schnabel and Sch¨utze, 2014), parsing (McClosky et al., 2010), and machine translation (Foster et al., 2010). Most of the work can be classified either supervised domain adaptation (Chelba and Acero, 2006; Blitzer et al., 2006; Daume III and Marcu, 2006; Daum´e III, 2007; Finkel and Manning, 2009; Chen et al., 2011) or semi-supervised adaptation (Ando and Zhang, 2005; Jiang and Zhai, 2007; Kumar et al., 2010; Huang and Yates, 2010). Our problem setting falls into the former. Multi-task learning has become popular in NLP. Sutton and McCallum (2005) showed that joint 1Multilingual learning (Kim et al., 2011; Kim and Snyder, 2012; Kim and Snyder, 2013) has same setting. learning and/or decoding of sub-tasks helps to improve performance. Collobert and Weston (2008) proved the similar claim in a deep learning architectu</context>
</contexts>
<marker>Blitzer, McDonald, Pereira, 2006</marker>
<rawString>John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspondence learning. In Proceedings of the EMNLP, pages 120–128. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Asli Celikyilmaz</author>
<author>Dilek Hakkani-Tur</author>
<author>Panupong Pasupat</author>
<author>Ruhi Sarikaya</author>
</authors>
<title>Enriching word embeddings using knowledge graph for semantic tagging in conversational dialog systems. AAAI - Association for the Advancement of Artificial Intelligence.</title>
<date>2015</date>
<contexts>
<context position="6827" citStr="Celikyilmaz et al., 2015" startWordPosition="1103" endWordPosition="1106">r settings, there are two clear distinctions. First, we aim to optimize performance on the target domain by minimizing the gap between source and target domain while multi-task learning jointly learns the shared tasks. Second, in our problem the domains are different, but they are closely related. On the other hand, prior work focuses on multiple subtasks of the same data. Despite the increasing interest in NLU (De Mori et al., 2008; Xu and Sarikaya, 2013; Sarikaya et al., 2014; Xu and Sarikaya, 2014; Anastasakos et al., 2014; El-Kahky et al., 2014; Liu and Sarikaya, 2014; Marin et al., 2014; Celikyilmaz et al., 2015; Ma et al., 2015; Kim et al., 2015), transfer learning in the context of NLU has not been much explored. The most relevant previous work is Tur (2006) and Li et al. (2011), which described both the effectiveness of multi-task learning in the context of NLU. For multi-task learning, they used shared slots by associating each slot type with aggregate active feature weight vector based on an existing domain specific slot tagger. Our empirical results shows that these vector representation might be helpful to find shared slots across domain, but cannot find bijective mapping between domains. Also</context>
</contexts>
<marker>Celikyilmaz, Hakkani-Tur, Pasupat, Sarikaya, 2015</marker>
<rawString>Asli Celikyilmaz, Dilek Hakkani-Tur, Panupong Pasupat, and Ruhi Sarikaya. 2015. Enriching word embeddings using knowledge graph for semantic tagging in conversational dialog systems. AAAI - Association for the Advancement of Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>Alex Acero</author>
</authors>
<title>Adaptation of maximum entropy capitalizer: Little data can help a lot.</title>
<date>2006</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>20</volume>
<issue>4</issue>
<contexts>
<context position="5545" citStr="Chelba and Acero, 2006" startWordPosition="891" endWordPosition="894">multi-task learning aims to improve performance across all domains while our domain adaptation objective is to optimize the performance of semantic slot tagger on the target domain. Below, we review related work in domain adaption and natural language understanding (NLU). 2.1 Related Work Domain adaptation has been widely used in many natural language processing (NLP) applications including part-of-speech tagging (Schnabel and Sch¨utze, 2014), parsing (McClosky et al., 2010), and machine translation (Foster et al., 2010). Most of the work can be classified either supervised domain adaptation (Chelba and Acero, 2006; Blitzer et al., 2006; Daume III and Marcu, 2006; Daum´e III, 2007; Finkel and Manning, 2009; Chen et al., 2011) or semi-supervised adaptation (Ando and Zhang, 2005; Jiang and Zhai, 2007; Kumar et al., 2010; Huang and Yates, 2010). Our problem setting falls into the former. Multi-task learning has become popular in NLP. Sutton and McCallum (2005) showed that joint 1Multilingual learning (Kim et al., 2011; Kim and Snyder, 2012; Kim and Snyder, 2013) has same setting. learning and/or decoding of sub-tasks helps to improve performance. Collobert and Weston (2008) proved the similar claim in a de</context>
</contexts>
<marker>Chelba, Acero, 2006</marker>
<rawString>Ciprian Chelba and Alex Acero. 2006. Adaptation of maximum entropy capitalizer: Little data can help a lot. Computer Speech &amp; Language, 20(4):382–399.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minmin Chen</author>
<author>Kilian Q Weinberger</author>
<author>John Blitzer</author>
</authors>
<title>Co-training for domain adaptation. In Advances in neural information processing systems,</title>
<date>2011</date>
<pages>2456--2464</pages>
<contexts>
<context position="5658" citStr="Chen et al., 2011" startWordPosition="911" endWordPosition="914">ze the performance of semantic slot tagger on the target domain. Below, we review related work in domain adaption and natural language understanding (NLU). 2.1 Related Work Domain adaptation has been widely used in many natural language processing (NLP) applications including part-of-speech tagging (Schnabel and Sch¨utze, 2014), parsing (McClosky et al., 2010), and machine translation (Foster et al., 2010). Most of the work can be classified either supervised domain adaptation (Chelba and Acero, 2006; Blitzer et al., 2006; Daume III and Marcu, 2006; Daum´e III, 2007; Finkel and Manning, 2009; Chen et al., 2011) or semi-supervised adaptation (Ando and Zhang, 2005; Jiang and Zhai, 2007; Kumar et al., 2010; Huang and Yates, 2010). Our problem setting falls into the former. Multi-task learning has become popular in NLP. Sutton and McCallum (2005) showed that joint 1Multilingual learning (Kim et al., 2011; Kim and Snyder, 2012; Kim and Snyder, 2013) has same setting. learning and/or decoding of sub-tasks helps to improve performance. Collobert and Weston (2008) proved the similar claim in a deep learning architecture. While our problem resembles their settings, there are two clear distinctions. First, we</context>
</contexts>
<marker>Chen, Weinberger, Blitzer, 2011</marker>
<rawString>Minmin Chen, Kilian Q Weinberger, and John Blitzer. 2011. Co-training for domain adaptation. In Advances in neural information processing systems, pages 2456–2464.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proceedings of the ICML,</booktitle>
<pages>160--167</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="6112" citStr="Collobert and Weston (2008)" startWordPosition="983" endWordPosition="986"> either supervised domain adaptation (Chelba and Acero, 2006; Blitzer et al., 2006; Daume III and Marcu, 2006; Daum´e III, 2007; Finkel and Manning, 2009; Chen et al., 2011) or semi-supervised adaptation (Ando and Zhang, 2005; Jiang and Zhai, 2007; Kumar et al., 2010; Huang and Yates, 2010). Our problem setting falls into the former. Multi-task learning has become popular in NLP. Sutton and McCallum (2005) showed that joint 1Multilingual learning (Kim et al., 2011; Kim and Snyder, 2012; Kim and Snyder, 2013) has same setting. learning and/or decoding of sub-tasks helps to improve performance. Collobert and Weston (2008) proved the similar claim in a deep learning architecture. While our problem resembles their settings, there are two clear distinctions. First, we aim to optimize performance on the target domain by minimizing the gap between source and target domain while multi-task learning jointly learns the shared tasks. Second, in our problem the domains are different, but they are closely related. On the other hand, prior work focuses on multiple subtasks of the same data. Despite the increasing interest in NLU (De Mori et al., 2008; Xu and Sarikaya, 2013; Sarikaya et al., 2014; Xu and Sarikaya, 2014; An</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the ICML, pages 160–167. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daume</author>
<author>Daniel Marcu</author>
</authors>
<title>Domain adaptation for statistical classifiers.</title>
<date>2006</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>101--126</pages>
<marker>Daume, Marcu, 2006</marker>
<rawString>Hal Daume III and Daniel Marcu. 2006. Domain adaptation for statistical classifiers. Journal of Artificial Intelligence Research, pages 101–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
</authors>
<title>Frustratingly easy domain adaptation. proceedings of the ACL,</title>
<date>2007</date>
<pages>256</pages>
<marker>Daum´e, 2007</marker>
<rawString>Hal Daum´e III. 2007. Frustratingly easy domain adaptation. proceedings of the ACL, page 256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Renato De Mori</author>
<author>Fr´ed´eric Bechet</author>
<author>Dilek Hakkani-Tur</author>
<author>Michael McTear</author>
<author>Giuseppe Riccardi</author>
<author>Gokhan Tur</author>
</authors>
<date>2008</date>
<booktitle>Spoken language understanding. Signal Processing Magazine, IEEE,</booktitle>
<pages>25--3</pages>
<marker>De Mori, Bechet, Hakkani-Tur, McTear, Riccardi, Tur, 2008</marker>
<rawString>Renato De Mori, Fr´ed´eric Bechet, Dilek Hakkani-Tur, Michael McTear, Giuseppe Riccardi, and Gokhan Tur. 2008. Spoken language understanding. Signal Processing Magazine, IEEE, 25(3):50–58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ali El-Kahky</author>
<author>Derek Liu</author>
<author>Ruhi Sarikaya</author>
<author>Gokhan Tur</author>
<author>Dilek Hakkani-Tur</author>
<author>Larry Heck</author>
</authors>
<title>Extending domain coverage of language understanding systems via intent transfer between domains using knowledge graphs and search query click logs.</title>
<date>2014</date>
<booktitle>IEEE, Proceedings of the ICASSP.</booktitle>
<contexts>
<context position="6757" citStr="El-Kahky et al., 2014" startWordPosition="1091" endWordPosition="1094">m in a deep learning architecture. While our problem resembles their settings, there are two clear distinctions. First, we aim to optimize performance on the target domain by minimizing the gap between source and target domain while multi-task learning jointly learns the shared tasks. Second, in our problem the domains are different, but they are closely related. On the other hand, prior work focuses on multiple subtasks of the same data. Despite the increasing interest in NLU (De Mori et al., 2008; Xu and Sarikaya, 2013; Sarikaya et al., 2014; Xu and Sarikaya, 2014; Anastasakos et al., 2014; El-Kahky et al., 2014; Liu and Sarikaya, 2014; Marin et al., 2014; Celikyilmaz et al., 2015; Ma et al., 2015; Kim et al., 2015), transfer learning in the context of NLU has not been much explored. The most relevant previous work is Tur (2006) and Li et al. (2011), which described both the effectiveness of multi-task learning in the context of NLU. For multi-task learning, they used shared slots by associating each slot type with aggregate active feature weight vector based on an existing domain specific slot tagger. Our empirical results shows that these vector representation might be helpful to find shared slots </context>
</contexts>
<marker>El-Kahky, Liu, Sarikaya, Tur, Hakkani-Tur, Heck, 2014</marker>
<rawString>Ali El-Kahky, Derek Liu, Ruhi Sarikaya, Gokhan Tur, Dilek Hakkani-Tur, and Larry Heck. 2014. Extending domain coverage of language understanding systems via intent transfer between domains using knowledge graphs and search query click logs. IEEE, Proceedings of the ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Christopher D Manning</author>
</authors>
<title>Hierarchical bayesian domain adaptation.</title>
<date>2009</date>
<booktitle>In Proceedings of theACL,</booktitle>
<pages>602--610</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5638" citStr="Finkel and Manning, 2009" startWordPosition="907" endWordPosition="910">ion objective is to optimize the performance of semantic slot tagger on the target domain. Below, we review related work in domain adaption and natural language understanding (NLU). 2.1 Related Work Domain adaptation has been widely used in many natural language processing (NLP) applications including part-of-speech tagging (Schnabel and Sch¨utze, 2014), parsing (McClosky et al., 2010), and machine translation (Foster et al., 2010). Most of the work can be classified either supervised domain adaptation (Chelba and Acero, 2006; Blitzer et al., 2006; Daume III and Marcu, 2006; Daum´e III, 2007; Finkel and Manning, 2009; Chen et al., 2011) or semi-supervised adaptation (Ando and Zhang, 2005; Jiang and Zhai, 2007; Kumar et al., 2010; Huang and Yates, 2010). Our problem setting falls into the former. Multi-task learning has become popular in NLP. Sutton and McCallum (2005) showed that joint 1Multilingual learning (Kim et al., 2011; Kim and Snyder, 2012; Kim and Snyder, 2013) has same setting. learning and/or decoding of sub-tasks helps to improve performance. Collobert and Weston (2008) proved the similar claim in a deep learning architecture. While our problem resembles their settings, there are two clear dis</context>
</contexts>
<marker>Finkel, Manning, 2009</marker>
<rawString>Jenny Rose Finkel and Christopher D Manning. 2009. Hierarchical bayesian domain adaptation. In Proceedings of theACL, pages 602–610. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Cyril Goutte</author>
<author>Roland Kuhn</author>
</authors>
<title>Discriminative instance weighting for domain adaptation in statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the EMNLP,</booktitle>
<pages>451--459</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5449" citStr="Foster et al., 2010" startWordPosition="875" endWordPosition="878">ly but output spaces (slot tags) might. Multi-task learning differs from our task. In general multi-task learning aims to improve performance across all domains while our domain adaptation objective is to optimize the performance of semantic slot tagger on the target domain. Below, we review related work in domain adaption and natural language understanding (NLU). 2.1 Related Work Domain adaptation has been widely used in many natural language processing (NLP) applications including part-of-speech tagging (Schnabel and Sch¨utze, 2014), parsing (McClosky et al., 2010), and machine translation (Foster et al., 2010). Most of the work can be classified either supervised domain adaptation (Chelba and Acero, 2006; Blitzer et al., 2006; Daume III and Marcu, 2006; Daum´e III, 2007; Finkel and Manning, 2009; Chen et al., 2011) or semi-supervised adaptation (Ando and Zhang, 2005; Jiang and Zhai, 2007; Kumar et al., 2010; Huang and Yates, 2010). Our problem setting falls into the former. Multi-task learning has become popular in NLP. Sutton and McCallum (2005) showed that joint 1Multilingual learning (Kim et al., 2011; Kim and Snyder, 2012; Kim and Snyder, 2013) has same setting. learning and/or decoding of sub-</context>
</contexts>
<marker>Foster, Goutte, Kuhn, 2010</marker>
<rawString>George Foster, Cyril Goutte, and Roland Kuhn. 2010. Discriminative instance weighting for domain adaptation in statistical machine translation. In Proceedings of the EMNLP, pages 451–459. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harold Hotelling</author>
</authors>
<title>Relations between two sets of variates.</title>
<date>1936</date>
<journal>Biometrika,</journal>
<pages>28--3</pages>
<contexts>
<context position="2525" citStr="Hotelling, 1936" startWordPosition="398" endWordPosition="399">usic domain. Thus every time a new application is developed or a new domain is built, a significant amount of resources is invested in creating annotations specific to that application or domain. One might attempt to apply existing techniques (Blitzer et al., 2006; Daum´e III, 2007) in domain adaption to this problem, but a straightforward application is not possible because these techniques assume that the label set is invariant. In this work, we provide a simple and effective solution to this problem by abstracting the label types using the canonical correlation analysis (CCA) by Hotelling (Hotelling, 1936) a powerful and flexible statistical technique for dimensionality reduction. We derive a low dimensional representation for each label type that is maximally correlated to the average context of that label via CCA. These shared label representations, or label embeddings, allow us to map label types across different domains and reduce the setting to a standard domain adaptation problem. After the mapping, we can apply the standard transfer learning techniques to solve the problem. Additionally, we introduce a novel pretraining technique for hidden-unit CRFs (HUCRFs) to effectively transfer know</context>
<context position="17008" citStr="Hotelling, 1936" startWordPosition="2878" endWordPosition="2879">s the following for i = 1... k: arg max En l=1(u� i xl)(v�i yl) uiERd, viERd0: u&gt;i ui0=0 bi0&lt;i v&gt;i vi0=0 bi0&lt;i ✓En ✓En l=1(u� i xl)2 l=1(v� i yl)2 In other words, each (ui, vi) is a pair of projection vectors such that the correlation between the projected variables u xl and vz yl (now scalars) is maximized, under the constraint that this projection is uncorrelated with the previous i − 1 projections. This is a non-convex problem due to the interaction between ui and vi. Fortunately, a method based on singular value decomposition (SVD) provides an efficient and exact solution to this problem (Hotelling, 1936). The resulting solution u1 ... uk E Rd and v1 ... vk E Rd0 can be used to project the variables from the original d- and d&apos;-dimensional spaces to a k-dimensional space: x E Rd −� x¯E Rk:¯xi= uz x y E Rd0 −� y¯E Rk : ¯yi = vZ y The new k-dimensional representation of each variable now contains information about the other variable. The value of k is usually selected to be much smaller than d or d&apos;, so the representation is typically also low-dimensional. 5.2 Inducing label embeddings We now describe how to use CCA to induce vector representations for label types. Using the same notation, let n </context>
</contexts>
<marker>Hotelling, 1936</marker>
<rawString>Harold Hotelling. 1936. Relations between two sets of variates. Biometrika, 28(3/4):321–377.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Huang</author>
<author>Alexander Yates</author>
</authors>
<title>Exploring representation-learning approaches to domain adaptation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing,</booktitle>
<pages>23--30</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5776" citStr="Huang and Yates, 2010" startWordPosition="931" endWordPosition="934">and natural language understanding (NLU). 2.1 Related Work Domain adaptation has been widely used in many natural language processing (NLP) applications including part-of-speech tagging (Schnabel and Sch¨utze, 2014), parsing (McClosky et al., 2010), and machine translation (Foster et al., 2010). Most of the work can be classified either supervised domain adaptation (Chelba and Acero, 2006; Blitzer et al., 2006; Daume III and Marcu, 2006; Daum´e III, 2007; Finkel and Manning, 2009; Chen et al., 2011) or semi-supervised adaptation (Ando and Zhang, 2005; Jiang and Zhai, 2007; Kumar et al., 2010; Huang and Yates, 2010). Our problem setting falls into the former. Multi-task learning has become popular in NLP. Sutton and McCallum (2005) showed that joint 1Multilingual learning (Kim et al., 2011; Kim and Snyder, 2012; Kim and Snyder, 2013) has same setting. learning and/or decoding of sub-tasks helps to improve performance. Collobert and Weston (2008) proved the similar claim in a deep learning architecture. While our problem resembles their settings, there are two clear distinctions. First, we aim to optimize performance on the target domain by minimizing the gap between source and target domain while multi-t</context>
</contexts>
<marker>Huang, Yates, 2010</marker>
<rawString>Fei Huang and Alexander Yates. 2010. Exploring representation-learning approaches to domain adaptation. In Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing, pages 23–30. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minwoo Jeong</author>
<author>Gary Geunbae Lee</author>
</authors>
<title>Multidomain spoken language understanding with transfer learning.</title>
<date>2009</date>
<journal>Speech Communication,</journal>
<volume>51</volume>
<issue>5</issue>
<pages>424</pages>
<contexts>
<context position="7449" citStr="Jeong and Lee (2009)" startWordPosition="1210" endWordPosition="1213">Ma et al., 2015; Kim et al., 2015), transfer learning in the context of NLU has not been much explored. The most relevant previous work is Tur (2006) and Li et al. (2011), which described both the effectiveness of multi-task learning in the context of NLU. For multi-task learning, they used shared slots by associating each slot type with aggregate active feature weight vector based on an existing domain specific slot tagger. Our empirical results shows that these vector representation might be helpful to find shared slots across domain, but cannot find bijective mapping between domains. Also, Jeong and Lee (2009) presented a transfer learning approach in multi-domain NLU, where the model jointly learns slot taggers in multiple domains and simultaneously predicts domain detection and slot tagging results.2 To share parameters across domains, they added an additional node for domain prediction on top of the slot sequence. However, this framework also limited to a setting in which the label set remains invariant. In contrast, our method is restricted to this setting without any modification of models. 3 Sequence Modeling Technique The proposed techniques in Section 4 and 5 are generic methodologies and n</context>
</contexts>
<marker>Jeong, Lee, 2009</marker>
<rawString>Minwoo Jeong and Gary Geunbae Lee. 2009. Multidomain spoken language understanding with transfer learning. Speech Communication, 51(5):412– 424.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Jiang</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Instance weighting for domain adaptation in nlp.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<volume>7</volume>
<pages>264--271</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5732" citStr="Jiang and Zhai, 2007" startWordPosition="923" endWordPosition="926">we review related work in domain adaption and natural language understanding (NLU). 2.1 Related Work Domain adaptation has been widely used in many natural language processing (NLP) applications including part-of-speech tagging (Schnabel and Sch¨utze, 2014), parsing (McClosky et al., 2010), and machine translation (Foster et al., 2010). Most of the work can be classified either supervised domain adaptation (Chelba and Acero, 2006; Blitzer et al., 2006; Daume III and Marcu, 2006; Daum´e III, 2007; Finkel and Manning, 2009; Chen et al., 2011) or semi-supervised adaptation (Ando and Zhang, 2005; Jiang and Zhai, 2007; Kumar et al., 2010; Huang and Yates, 2010). Our problem setting falls into the former. Multi-task learning has become popular in NLP. Sutton and McCallum (2005) showed that joint 1Multilingual learning (Kim et al., 2011; Kim and Snyder, 2012; Kim and Snyder, 2013) has same setting. learning and/or decoding of sub-tasks helps to improve performance. Collobert and Weston (2008) proved the similar claim in a deep learning architecture. While our problem resembles their settings, there are two clear distinctions. First, we aim to optimize performance on the target domain by minimizing the gap be</context>
</contexts>
<marker>Jiang, Zhai, 2007</marker>
<rawString>Jing Jiang and ChengXiang Zhai. 2007. Instance weighting for domain adaptation in nlp. In Proceedings of the ACL, volume 7, pages 264–271. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Young-Bum Kim</author>
<author>Benjamin Snyder</author>
</authors>
<title>Universal grapheme-to-phoneme prediction over latin alphabets.</title>
<date>2012</date>
<booktitle>In Proceedings of the EMNLP,</booktitle>
<pages>332--343</pages>
<institution>Jeju Island, South Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="5975" citStr="Kim and Snyder, 2012" startWordPosition="962" endWordPosition="965">ch¨utze, 2014), parsing (McClosky et al., 2010), and machine translation (Foster et al., 2010). Most of the work can be classified either supervised domain adaptation (Chelba and Acero, 2006; Blitzer et al., 2006; Daume III and Marcu, 2006; Daum´e III, 2007; Finkel and Manning, 2009; Chen et al., 2011) or semi-supervised adaptation (Ando and Zhang, 2005; Jiang and Zhai, 2007; Kumar et al., 2010; Huang and Yates, 2010). Our problem setting falls into the former. Multi-task learning has become popular in NLP. Sutton and McCallum (2005) showed that joint 1Multilingual learning (Kim et al., 2011; Kim and Snyder, 2012; Kim and Snyder, 2013) has same setting. learning and/or decoding of sub-tasks helps to improve performance. Collobert and Weston (2008) proved the similar claim in a deep learning architecture. While our problem resembles their settings, there are two clear distinctions. First, we aim to optimize performance on the target domain by minimizing the gap between source and target domain while multi-task learning jointly learns the shared tasks. Second, in our problem the domains are different, but they are closely related. On the other hand, prior work focuses on multiple subtasks of the same da</context>
</contexts>
<marker>Kim, Snyder, 2012</marker>
<rawString>Young-Bum Kim and Benjamin Snyder. 2012. Universal grapheme-to-phoneme prediction over latin alphabets. In Proceedings of the EMNLP, pages 332– 343, Jeju Island, South Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Young-Bum Kim</author>
<author>Benjamin Snyder</author>
</authors>
<title>Unsupervised consonant-vowel prediction over hundreds of languages.</title>
<date>2013</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>1527--1536</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5998" citStr="Kim and Snyder, 2013" startWordPosition="966" endWordPosition="969">g (McClosky et al., 2010), and machine translation (Foster et al., 2010). Most of the work can be classified either supervised domain adaptation (Chelba and Acero, 2006; Blitzer et al., 2006; Daume III and Marcu, 2006; Daum´e III, 2007; Finkel and Manning, 2009; Chen et al., 2011) or semi-supervised adaptation (Ando and Zhang, 2005; Jiang and Zhai, 2007; Kumar et al., 2010; Huang and Yates, 2010). Our problem setting falls into the former. Multi-task learning has become popular in NLP. Sutton and McCallum (2005) showed that joint 1Multilingual learning (Kim et al., 2011; Kim and Snyder, 2012; Kim and Snyder, 2013) has same setting. learning and/or decoding of sub-tasks helps to improve performance. Collobert and Weston (2008) proved the similar claim in a deep learning architecture. While our problem resembles their settings, there are two clear distinctions. First, we aim to optimize performance on the target domain by minimizing the gap between source and target domain while multi-task learning jointly learns the shared tasks. Second, in our problem the domains are different, but they are closely related. On the other hand, prior work focuses on multiple subtasks of the same data. Despite the increas</context>
</contexts>
<marker>Kim, Snyder, 2013</marker>
<rawString>Young-Bum Kim and Benjamin Snyder. 2013. Unsupervised consonant-vowel prediction over hundreds of languages. In Proceedings of the ACL, pages 1527–1536. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Young-Bum Kim</author>
<author>Jo˜ao V Grac¸a</author>
<author>Benjamin Snyder</author>
</authors>
<title>Universal morphological analysis using structured nearest neighbor prediction.</title>
<date>2011</date>
<booktitle>In Proceedings of the EMNLP,</booktitle>
<pages>322--332</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Kim, Grac¸a, Snyder, 2011</marker>
<rawString>Young-Bum Kim, Jo˜ao V Grac¸a, and Benjamin Snyder. 2011. Universal morphological analysis using structured nearest neighbor prediction. In Proceedings of the EMNLP, pages 322–332. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Young-Bum Kim</author>
<author>Minwoo Jeong</author>
<author>Karl Stratos</author>
<author>Ruhi Sarikaya</author>
</authors>
<title>Weakly supervised slot tagging with partially labeled sequences from web search click logs.</title>
<date>2015</date>
<booktitle>In Proceedings of the NAACL. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="6863" citStr="Kim et al., 2015" startWordPosition="1111" endWordPosition="1114">s. First, we aim to optimize performance on the target domain by minimizing the gap between source and target domain while multi-task learning jointly learns the shared tasks. Second, in our problem the domains are different, but they are closely related. On the other hand, prior work focuses on multiple subtasks of the same data. Despite the increasing interest in NLU (De Mori et al., 2008; Xu and Sarikaya, 2013; Sarikaya et al., 2014; Xu and Sarikaya, 2014; Anastasakos et al., 2014; El-Kahky et al., 2014; Liu and Sarikaya, 2014; Marin et al., 2014; Celikyilmaz et al., 2015; Ma et al., 2015; Kim et al., 2015), transfer learning in the context of NLU has not been much explored. The most relevant previous work is Tur (2006) and Li et al. (2011), which described both the effectiveness of multi-task learning in the context of NLU. For multi-task learning, they used shared slots by associating each slot type with aggregate active feature weight vector based on an existing domain specific slot tagger. Our empirical results shows that these vector representation might be helpful to find shared slots across domain, but cannot find bijective mapping between domains. Also, Jeong and Lee (2009) presented a t</context>
</contexts>
<marker>Kim, Jeong, Stratos, Sarikaya, 2015</marker>
<rawString>Young-Bum Kim, Minwoo Jeong, Karl Stratos, and Ruhi Sarikaya. 2015. Weakly supervised slot tagging with partially labeled sequences from web search click logs. In Proceedings of the NAACL. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abhishek Kumar</author>
<author>Avishek Saha</author>
<author>Hal Daume</author>
</authors>
<title>Co-regularization based semi-supervised domain adaptation.</title>
<date>2010</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>478--486</pages>
<contexts>
<context position="5752" citStr="Kumar et al., 2010" startWordPosition="927" endWordPosition="930"> in domain adaption and natural language understanding (NLU). 2.1 Related Work Domain adaptation has been widely used in many natural language processing (NLP) applications including part-of-speech tagging (Schnabel and Sch¨utze, 2014), parsing (McClosky et al., 2010), and machine translation (Foster et al., 2010). Most of the work can be classified either supervised domain adaptation (Chelba and Acero, 2006; Blitzer et al., 2006; Daume III and Marcu, 2006; Daum´e III, 2007; Finkel and Manning, 2009; Chen et al., 2011) or semi-supervised adaptation (Ando and Zhang, 2005; Jiang and Zhai, 2007; Kumar et al., 2010; Huang and Yates, 2010). Our problem setting falls into the former. Multi-task learning has become popular in NLP. Sutton and McCallum (2005) showed that joint 1Multilingual learning (Kim et al., 2011; Kim and Snyder, 2012; Kim and Snyder, 2013) has same setting. learning and/or decoding of sub-tasks helps to improve performance. Collobert and Weston (2008) proved the similar claim in a deep learning architecture. While our problem resembles their settings, there are two clear distinctions. First, we aim to optimize performance on the target domain by minimizing the gap between source and tar</context>
</contexts>
<marker>Kumar, Saha, Daume, 2010</marker>
<rawString>Abhishek Kumar, Avishek Saha, and Hal Daume. 2010. Co-regularization based semi-supervised domain adaptation. In Advances in Neural Information Processing Systems, pages 478–486.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hugo Larochelle</author>
<author>Yoshua Bengio</author>
</authors>
<title>Classification using discriminative restricted boltzmann machines.</title>
<date>2008</date>
<booktitle>In Proceedings of the ICML.</booktitle>
<contexts>
<context position="9652" citStr="Larochelle and Bengio, 2008" startWordPosition="1605" endWordPosition="1608">E{0,11n exp(θTΦ(x, zl) + γTΨ(zl, yl)) y&apos;EY(x,z&apos;) (1) where Y(x, z) is the set of all possible label sequences for x and z, and Φ(x, z) E Rd and Ψ(z, y) E Rd&apos; are global feature functions that decompose into local feature functions: n Φ(x, z) = φ(x,j,zj) j=1 n Ψ(z, y) = ψ(zj,yj−1,yj) j=1 HUCRF forces the interaction between the observations and the labels at each position j to go through a latent variable zj: see Figure 1 for illustration. Then the probability of labels y is given by marginalizing over the hidden units, pe,-r(y|x) = � pe,-r(y, z|x) zE{0,11n As in restricted Boltzmann machines (Larochelle and Bengio, 2008), hidden units are conditionally independent given observations and labels. This allows for efficient inference with HUCRFs despite their richness (see Maaten et al. (2011) for details). We use a perceptron-style algorithm of Maaten et al. (2011) for training HUCRFs. 4 Transfer learning between domains with different label sets In this section, we describe three methods for utilizing annotations in domains with different label types. First two methods are about transferring features and last method is about transferring model parameters. Each of these methods requires some sort of mapping for </context>
</contexts>
<marker>Larochelle, Bengio, 2008</marker>
<rawString>Hugo Larochelle and Yoshua Bengio. 2008. Classification using discriminative restricted boltzmann machines. In Proceedings of the ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiao Li</author>
<author>Ye-Yi Wang</author>
<author>G¨okhan T¨ur</author>
</authors>
<title>Multitask learning for spoken language understanding with shared slots.</title>
<date>2011</date>
<booktitle>In Proceeding of the INTERSPEECH,</booktitle>
<pages>701--704</pages>
<publisher>IEEE.</publisher>
<marker>Li, Wang, T¨ur, 2011</marker>
<rawString>Xiao Li, Ye-Yi Wang, and G¨okhan T¨ur. 2011. Multitask learning for spoken language understanding with shared slots. In Proceeding of the INTERSPEECH, pages 701–704. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaohu Liu</author>
<author>Ruhi Sarikaya</author>
</authors>
<title>A discriminative model based entity dictionary weighting approach for spoken language understanding.</title>
<date>2014</date>
<journal>IEEE Institute of Electrical and Electronics Engineers.</journal>
<contexts>
<context position="6781" citStr="Liu and Sarikaya, 2014" startWordPosition="1095" endWordPosition="1098">chitecture. While our problem resembles their settings, there are two clear distinctions. First, we aim to optimize performance on the target domain by minimizing the gap between source and target domain while multi-task learning jointly learns the shared tasks. Second, in our problem the domains are different, but they are closely related. On the other hand, prior work focuses on multiple subtasks of the same data. Despite the increasing interest in NLU (De Mori et al., 2008; Xu and Sarikaya, 2013; Sarikaya et al., 2014; Xu and Sarikaya, 2014; Anastasakos et al., 2014; El-Kahky et al., 2014; Liu and Sarikaya, 2014; Marin et al., 2014; Celikyilmaz et al., 2015; Ma et al., 2015; Kim et al., 2015), transfer learning in the context of NLU has not been much explored. The most relevant previous work is Tur (2006) and Li et al. (2011), which described both the effectiveness of multi-task learning in the context of NLU. For multi-task learning, they used shared slots by associating each slot type with aggregate active feature weight vector based on an existing domain specific slot tagger. Our empirical results shows that these vector representation might be helpful to find shared slots across domain, but canno</context>
</contexts>
<marker>Liu, Sarikaya, 2014</marker>
<rawString>Xiaohu Liu and Ruhi Sarikaya. 2014. A discriminative model based entity dictionary weighting approach for spoken language understanding. IEEE Institute of Electrical and Electronics Engineers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yi Ma</author>
<author>Paul A Crook</author>
</authors>
<title>Ruhi Sarikaya, and Eric FoslerLussier.</title>
<date>2015</date>
<booktitle>In Proceedings of the ICASSP.</booktitle>
<publisher>IEEE.</publisher>
<marker>Ma, Crook, 2015</marker>
<rawString>Yi Ma, Paul A. Crook, Ruhi Sarikaya, and Eric FoslerLussier. 2015. Knowledge graph inference for spoken dialog systems. In Proceedings of the ICASSP. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laurens Maaten</author>
<author>Max Welling</author>
<author>Lawrence K Saul</author>
</authors>
<title>Hidden-unit conditional random fields.</title>
<date>2011</date>
<booktitle>In International Conference on Artificial Intelligence and Statistics.</booktitle>
<contexts>
<context position="8245" citStr="Maaten et al. (2011)" startWordPosition="1342" endWordPosition="1345">ot tagging results.2 To share parameters across domains, they added an additional node for domain prediction on top of the slot sequence. However, this framework also limited to a setting in which the label set remains invariant. In contrast, our method is restricted to this setting without any modification of models. 3 Sequence Modeling Technique The proposed techniques in Section 4 and 5 are generic methodologies and not tied to any particular models such as any sequence models and instanced based models. However, because of superior performance over CRF, we use a hidden unit CRF (HUCRF) of Maaten et al. (2011). 2Jeong and Lee (2009) pointed out that if the domain is given, their method is the same as that of Daum´e III (2007). 474 Figure 1: Graphical representation of hidden unit CRFs. While popular and effective, a CRF is still a linear model. In contrast, a HUCRF benefits from nonlinearity, leading to superior performance over CRF (Maaten et al., 2011). Thus we will focus on HUCRFs to demonstrate our techniques in experiments. 3.1 Hidden Unit CRF (HUCRF) A HUCRF introduces a layer of binary-valued hidden units z = z1 ... zn E 10, 11 for each pair of label sequence y = y1 ... yn and observation se</context>
<context position="9824" citStr="Maaten et al. (2011)" startWordPosition="1630" endWordPosition="1633">ctions that decompose into local feature functions: n Φ(x, z) = φ(x,j,zj) j=1 n Ψ(z, y) = ψ(zj,yj−1,yj) j=1 HUCRF forces the interaction between the observations and the labels at each position j to go through a latent variable zj: see Figure 1 for illustration. Then the probability of labels y is given by marginalizing over the hidden units, pe,-r(y|x) = � pe,-r(y, z|x) zE{0,11n As in restricted Boltzmann machines (Larochelle and Bengio, 2008), hidden units are conditionally independent given observations and labels. This allows for efficient inference with HUCRFs despite their richness (see Maaten et al. (2011) for details). We use a perceptron-style algorithm of Maaten et al. (2011) for training HUCRFs. 4 Transfer learning between domains with different label sets In this section, we describe three methods for utilizing annotations in domains with different label types. First two methods are about transferring features and last method is about transferring model parameters. Each of these methods requires some sort of mapping for label types. A fine-grained label type needs to be mapped to a coarse one; a label type in one domain needs to be mapped to the corresponding label type in another domain. </context>
</contexts>
<marker>Maaten, Welling, Saul, 2011</marker>
<rawString>Laurens Maaten, Max Welling, and Lawrence K Saul. 2011. Hidden-unit conditional random fields. In International Conference on Artificial Intelligence and Statistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Marin</author>
<author>Roman Holenstein</author>
<author>Ruhi Sarikaya</author>
<author>Mari Ostendorf</author>
</authors>
<title>Learning phrase patterns for text classification using a knowledge graph and unlabeled data. ISCA - International Speech Communication Association.</title>
<date>2014</date>
<contexts>
<context position="6801" citStr="Marin et al., 2014" startWordPosition="1099" endWordPosition="1102">oblem resembles their settings, there are two clear distinctions. First, we aim to optimize performance on the target domain by minimizing the gap between source and target domain while multi-task learning jointly learns the shared tasks. Second, in our problem the domains are different, but they are closely related. On the other hand, prior work focuses on multiple subtasks of the same data. Despite the increasing interest in NLU (De Mori et al., 2008; Xu and Sarikaya, 2013; Sarikaya et al., 2014; Xu and Sarikaya, 2014; Anastasakos et al., 2014; El-Kahky et al., 2014; Liu and Sarikaya, 2014; Marin et al., 2014; Celikyilmaz et al., 2015; Ma et al., 2015; Kim et al., 2015), transfer learning in the context of NLU has not been much explored. The most relevant previous work is Tur (2006) and Li et al. (2011), which described both the effectiveness of multi-task learning in the context of NLU. For multi-task learning, they used shared slots by associating each slot type with aggregate active feature weight vector based on an existing domain specific slot tagger. Our empirical results shows that these vector representation might be helpful to find shared slots across domain, but cannot find bijective map</context>
</contexts>
<marker>Marin, Holenstein, Sarikaya, Ostendorf, 2014</marker>
<rawString>Alex Marin, Roman Holenstein, Ruhi Sarikaya, and Mari Ostendorf. 2014. Learning phrase patterns for text classification using a knowledge graph and unlabeled data. ISCA - International Speech Communication Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Automatic domain adaptation for parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL,</booktitle>
<pages>28--36</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5402" citStr="McClosky et al., 2010" startWordPosition="868" endWordPosition="871">rvice request utterances) do not change drastically but output spaces (slot tags) might. Multi-task learning differs from our task. In general multi-task learning aims to improve performance across all domains while our domain adaptation objective is to optimize the performance of semantic slot tagger on the target domain. Below, we review related work in domain adaption and natural language understanding (NLU). 2.1 Related Work Domain adaptation has been widely used in many natural language processing (NLP) applications including part-of-speech tagging (Schnabel and Sch¨utze, 2014), parsing (McClosky et al., 2010), and machine translation (Foster et al., 2010). Most of the work can be classified either supervised domain adaptation (Chelba and Acero, 2006; Blitzer et al., 2006; Daume III and Marcu, 2006; Daum´e III, 2007; Finkel and Manning, 2009; Chen et al., 2011) or semi-supervised adaptation (Ando and Zhang, 2005; Jiang and Zhai, 2007; Kumar et al., 2010; Huang and Yates, 2010). Our problem setting falls into the former. Multi-task learning has become popular in NLP. Sutton and McCallum (2005) showed that joint 1Multilingual learning (Kim et al., 2011; Kim and Snyder, 2012; Kim and Snyder, 2013) has</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2010</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2010. Automatic domain adaptation for parsing. In Proceedings of the NAACL, pages 28–36. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</title>
<date>2013</date>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruhi Sarikaya</author>
<author>C Asli</author>
<author>Anoop Deoras</author>
<author>Minwoo Jeong</author>
</authors>
<title>Shrinkage based features for slot tagging with conditional random fields.</title>
<date>2014</date>
<journal>Proceeding of ISCA - International Speech Communication Association,</journal>
<contexts>
<context position="6685" citStr="Sarikaya et al., 2014" startWordPosition="1079" endWordPosition="1082">improve performance. Collobert and Weston (2008) proved the similar claim in a deep learning architecture. While our problem resembles their settings, there are two clear distinctions. First, we aim to optimize performance on the target domain by minimizing the gap between source and target domain while multi-task learning jointly learns the shared tasks. Second, in our problem the domains are different, but they are closely related. On the other hand, prior work focuses on multiple subtasks of the same data. Despite the increasing interest in NLU (De Mori et al., 2008; Xu and Sarikaya, 2013; Sarikaya et al., 2014; Xu and Sarikaya, 2014; Anastasakos et al., 2014; El-Kahky et al., 2014; Liu and Sarikaya, 2014; Marin et al., 2014; Celikyilmaz et al., 2015; Ma et al., 2015; Kim et al., 2015), transfer learning in the context of NLU has not been much explored. The most relevant previous work is Tur (2006) and Li et al. (2011), which described both the effectiveness of multi-task learning in the context of NLU. For multi-task learning, they used shared slots by associating each slot type with aggregate active feature weight vector based on an existing domain specific slot tagger. Our empirical results shows</context>
</contexts>
<marker>Sarikaya, Asli, Deoras, Jeong, 2014</marker>
<rawString>Ruhi Sarikaya, Asli C, Anoop Deoras, and Minwoo Jeong. 2014. Shrinkage based features for slot tagging with conditional random fields. Proceeding of ISCA - International Speech Communication Association, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tobias Schnabel</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Flors: Fast and simple domain adaptation for part-ofspeech tagging.</title>
<date>2014</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>2--15</pages>
<marker>Schnabel, Sch¨utze, 2014</marker>
<rawString>Tobias Schnabel and Hinrich Sch¨utze. 2014. Flors: Fast and simple domain adaptation for part-ofspeech tagging. Transactions of the Association for Computational Linguistics, 2:15–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Sutton</author>
<author>Andrew McCallum</author>
</authors>
<title>Composition of conditional random fields for transfer learning.</title>
<date>2005</date>
<booktitle>In Proceedings of the EMNLP,</booktitle>
<pages>748--754</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5894" citStr="Sutton and McCallum (2005)" startWordPosition="949" endWordPosition="952">language processing (NLP) applications including part-of-speech tagging (Schnabel and Sch¨utze, 2014), parsing (McClosky et al., 2010), and machine translation (Foster et al., 2010). Most of the work can be classified either supervised domain adaptation (Chelba and Acero, 2006; Blitzer et al., 2006; Daume III and Marcu, 2006; Daum´e III, 2007; Finkel and Manning, 2009; Chen et al., 2011) or semi-supervised adaptation (Ando and Zhang, 2005; Jiang and Zhai, 2007; Kumar et al., 2010; Huang and Yates, 2010). Our problem setting falls into the former. Multi-task learning has become popular in NLP. Sutton and McCallum (2005) showed that joint 1Multilingual learning (Kim et al., 2011; Kim and Snyder, 2012; Kim and Snyder, 2013) has same setting. learning and/or decoding of sub-tasks helps to improve performance. Collobert and Weston (2008) proved the similar claim in a deep learning architecture. While our problem resembles their settings, there are two clear distinctions. First, we aim to optimize performance on the target domain by minimizing the gap between source and target domain while multi-task learning jointly learns the shared tasks. Second, in our problem the domains are different, but they are closely r</context>
</contexts>
<marker>Sutton, McCallum, 2005</marker>
<rawString>Charles Sutton and Andrew McCallum. 2005. Composition of conditional random fields for transfer learning. In Proceedings of the EMNLP, pages 748–754. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gokhan Tur</author>
</authors>
<title>Multitask learning for spoken language understanding.</title>
<date>2006</date>
<booktitle>In Proceedings of the ICASSP,</booktitle>
<publisher>IEEE.</publisher>
<location>Toulouse, France.</location>
<contexts>
<context position="6978" citStr="Tur (2006)" startWordPosition="1135" endWordPosition="1136"> multi-task learning jointly learns the shared tasks. Second, in our problem the domains are different, but they are closely related. On the other hand, prior work focuses on multiple subtasks of the same data. Despite the increasing interest in NLU (De Mori et al., 2008; Xu and Sarikaya, 2013; Sarikaya et al., 2014; Xu and Sarikaya, 2014; Anastasakos et al., 2014; El-Kahky et al., 2014; Liu and Sarikaya, 2014; Marin et al., 2014; Celikyilmaz et al., 2015; Ma et al., 2015; Kim et al., 2015), transfer learning in the context of NLU has not been much explored. The most relevant previous work is Tur (2006) and Li et al. (2011), which described both the effectiveness of multi-task learning in the context of NLU. For multi-task learning, they used shared slots by associating each slot type with aggregate active feature weight vector based on an existing domain specific slot tagger. Our empirical results shows that these vector representation might be helpful to find shared slots across domain, but cannot find bijective mapping between domains. Also, Jeong and Lee (2009) presented a transfer learning approach in multi-domain NLU, where the model jointly learns slot taggers in multiple domains and </context>
</contexts>
<marker>Tur, 2006</marker>
<rawString>Gokhan Tur. 2006. Multitask learning for spoken language understanding. In Proceedings of the ICASSP, Toulouse, France. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Puyang Xu</author>
<author>Ruhi Sarikaya</author>
</authors>
<title>Convolutional neural network based triangular crf for joint intent detection and slot filling.</title>
<date>2013</date>
<booktitle>In Automatic Speech Recognition and Understanding (ASRU),</booktitle>
<pages>78--83</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="6662" citStr="Xu and Sarikaya, 2013" startWordPosition="1075" endWordPosition="1078"> of sub-tasks helps to improve performance. Collobert and Weston (2008) proved the similar claim in a deep learning architecture. While our problem resembles their settings, there are two clear distinctions. First, we aim to optimize performance on the target domain by minimizing the gap between source and target domain while multi-task learning jointly learns the shared tasks. Second, in our problem the domains are different, but they are closely related. On the other hand, prior work focuses on multiple subtasks of the same data. Despite the increasing interest in NLU (De Mori et al., 2008; Xu and Sarikaya, 2013; Sarikaya et al., 2014; Xu and Sarikaya, 2014; Anastasakos et al., 2014; El-Kahky et al., 2014; Liu and Sarikaya, 2014; Marin et al., 2014; Celikyilmaz et al., 2015; Ma et al., 2015; Kim et al., 2015), transfer learning in the context of NLU has not been much explored. The most relevant previous work is Tur (2006) and Li et al. (2011), which described both the effectiveness of multi-task learning in the context of NLU. For multi-task learning, they used shared slots by associating each slot type with aggregate active feature weight vector based on an existing domain specific slot tagger. Our </context>
</contexts>
<marker>Xu, Sarikaya, 2013</marker>
<rawString>Puyang Xu and Ruhi Sarikaya. 2013. Convolutional neural network based triangular crf for joint intent detection and slot filling. In Automatic Speech Recognition and Understanding (ASRU), pages 78– 83. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Puyang Xu</author>
<author>Ruhi Sarikaya</author>
</authors>
<title>Targeted feature dropout for robust slot filling in natural language understanding.</title>
<date>2014</date>
<journal>ISCA - International Speech Communication Association.</journal>
<contexts>
<context position="6708" citStr="Xu and Sarikaya, 2014" startWordPosition="1083" endWordPosition="1086">llobert and Weston (2008) proved the similar claim in a deep learning architecture. While our problem resembles their settings, there are two clear distinctions. First, we aim to optimize performance on the target domain by minimizing the gap between source and target domain while multi-task learning jointly learns the shared tasks. Second, in our problem the domains are different, but they are closely related. On the other hand, prior work focuses on multiple subtasks of the same data. Despite the increasing interest in NLU (De Mori et al., 2008; Xu and Sarikaya, 2013; Sarikaya et al., 2014; Xu and Sarikaya, 2014; Anastasakos et al., 2014; El-Kahky et al., 2014; Liu and Sarikaya, 2014; Marin et al., 2014; Celikyilmaz et al., 2015; Ma et al., 2015; Kim et al., 2015), transfer learning in the context of NLU has not been much explored. The most relevant previous work is Tur (2006) and Li et al. (2011), which described both the effectiveness of multi-task learning in the context of NLU. For multi-task learning, they used shared slots by associating each slot type with aggregate active feature weight vector based on an existing domain specific slot tagger. Our empirical results shows that these vector repr</context>
</contexts>
<marker>Xu, Sarikaya, 2014</marker>
<rawString>Puyang Xu and Ruhi Sarikaya. 2014. Targeted feature dropout for robust slot filling in natural language understanding. ISCA - International Speech Communication Association.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>